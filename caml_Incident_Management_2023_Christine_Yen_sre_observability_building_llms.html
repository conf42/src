<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Leveraging SRE and Observability Techniques for the Wild World of Building on LLMs</title>
    <meta name="description" content="Squash your bugs before they grow!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Christine%20Yen_incident.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Leveraging SRE and Observability Techniques for the Wild World of Building on LLMs | Conf42"/>
    <meta property="og:description" content="Building on LLMs is magicalâ€”but maintaining LLM-backed code is tricky: how do you ensure perf/correctness for something probabilistic?  It's like building on any black box (eg APIs/DBs). We'll cover instrumentation techniques, uses for observability best practices, and even using SLOs early in dev."/>
    <meta property="og:url" content="https://conf42.com/Incident_Management_2023_Christine_Yen_sre_observability_building_llms"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/LLM2024_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Large Language Models (LLMs) 2024
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2024-04-11
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/llms2024" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/aiml2024">
                            Artificial Intelligence & Machine Learning (AI & ML)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/olly2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.sreday.com/">
                            SREday London
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="./support">
                            Support us
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/DnyHgrC7jC" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Subscribe for FREE
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #C44B4B;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Incident Management 2023 - Online
            </h1>

            <h2 class="text-white">
              
              Content unlocked! Welcome to the community!
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- Squash your bugs before they grow!
 -->
              <script>
                const event_date = new Date("2023-10-19T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2023-10-19T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "bAdIoLpL2Ek"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <a class="btn btn-info lift mb-3" data-bigpicture='{"ytSrc": "q2MuScM0Ru4"}' href="#">
                <i class="fe fe-eye me-2"></i>
                Watch Premiere
              </a>
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://youtube.com/playlist?list=PLIuxSyKxlQrB44Zc1LD3Yrp1Uv2opzokS&si=9WZppv3SU0nwCQPJ" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Thanks for joining me today. I\u0027m Christine", "timestamp": "00:02:08,710", "timestamp_s": 128.0}, {"text": "and I\u0027m going to start with a disclaimer. Honeycomb is", "timestamp": "00:02:12,598", "timestamp_s": 132.0}, {"text": "an observability tool, but the techniques that I\u0027ll describe today should", "timestamp": "00:02:15,888", "timestamp_s": 135.0}, {"text": "be transferable to your tool of about this", "timestamp": "00:02:19,728", "timestamp_s": 139.0}, {"text": "will draw from our experience of building on llms,", "timestamp": "00:02:23,872", "timestamp_s": 143.0}, {"text": "but should apply to whatever LLM and observability stack", "timestamp": "00:02:27,942", "timestamp_s": 147.0}, {"text": "you\u0027re using today. All right,", "timestamp": "00:02:32,358", "timestamp_s": 152.0}, {"text": "it software in 2023 feels more like", "timestamp": "00:02:36,048", "timestamp_s": 156.0}, {"text": "magic than it ever has before. There are llms everywhere", "timestamp": "00:02:39,752", "timestamp_s": 159.0}, {"text": "with a cheap API call to your provider of choice.", "timestamp": "00:02:43,534", "timestamp_s": 163.0}, {"text": "It feels like every CEO or even CEO", "timestamp": "00:02:47,454", "timestamp_s": 167.0}, {"text": "is now turning to their teams and asking how llms can be incorporated into", "timestamp": "00:02:51,810", "timestamp_s": 171.0}, {"text": "their core product. Many are reaching to define an AI strategy,", "timestamp": "00:02:55,788", "timestamp_s": 175.0}, {"text": "and there\u0027s lots to be excited about here. It\u0027s cool to be squarely in", "timestamp": "00:03:00,570", "timestamp_s": 180.0}, {"text": "the middle of a phase change in progress where everything is new to", "timestamp": "00:03:04,144", "timestamp_s": 184.0}, {"text": "everyone altogether. But there\u0027s also a reality check to trying", "timestamp": "00:03:07,792", "timestamp_s": 187.0}, {"text": "to suddenly incorporate all this new technology into our products.", "timestamp": "00:03:11,952", "timestamp_s": 191.0}, {"text": "There\u0027s suddenly a lot more demand for AI functionality than", "timestamp": "00:03:15,764", "timestamp_s": 195.0}, {"text": "there are people who carry expertise for it and software engineering teams", "timestamp": "00:03:19,428", "timestamp_s": 199.0}, {"text": "everywhere sre often just diving in to figure it out", "timestamp": "00:03:23,994", "timestamp_s": 203.0}, {"text": "because we\u0027re the only ones left. Which to be clear,", "timestamp": "00:03:27,876", "timestamp_s": 207.0}, {"text": "is just fine by me. As someone who used to identify as a generalist software", "timestamp": "00:03:30,952", "timestamp_s": 210.0}, {"text": "engineer, the fewer silos we can build in this industry,", "timestamp": "00:03:34,158", "timestamp_s": 214.0}, {"text": "the better. Because on one know,", "timestamp": "00:03:37,726", "timestamp_s": 217.0}, {"text": "using a large language model through an API is like any other", "timestamp": "00:03:41,404", "timestamp_s": 221.0}, {"text": "black box you interact with via API. Lots of", "timestamp": "00:03:45,052", "timestamp_s": 225.0}, {"text": "consistent expectations we can set about how we make sense of these", "timestamp": "00:03:48,428", "timestamp_s": 228.0}, {"text": "APIs, how we send parameters", "timestamp": "00:03:51,888", "timestamp_s": 231.0}, {"text": "to the API,", "timestamp": "00:03:55,062", "timestamp_s": 235.0}, {"text": "what types and scopes those inputs will", "timestamp": "00:03:58,110", "timestamp_s": 238.0}, {"text": "be, what we\u0027ll get back from those APIs,", "timestamp": "00:04:02,032", "timestamp_s": 242.0}, {"text": "and it\u0027s usually done over a standard protocol. And so all of these", "timestamp": "00:04:06,130", "timestamp_s": 246.0}, {"text": "properties make working with APIs,", "timestamp": "00:04:09,748", "timestamp_s": 249.0}, {"text": "these black boxes of logic,", "timestamp": "00:04:13,490", "timestamp_s": 253.0}, {"text": "into something that is testable and mockable,", "timestamp": "00:04:16,314", "timestamp_s": 256.0}, {"text": "a pretty reliable component in our system. But there\u0027s one key difference", "timestamp": "00:04:20,550", "timestamp_s": 260.0}, {"text": "between having your application behavior relied on an LLM versus,", "timestamp": "00:04:24,872", "timestamp_s": 264.0}, {"text": "say, a payments provider. That difference is how predictable", "timestamp": "00:04:29,570", "timestamp_s": 269.0}, {"text": "the behavior of that black box is, which then in turn influences", "timestamp": "00:04:33,810", "timestamp_s": 273.0}, {"text": "how testable or how mockable it is. And that", "timestamp": "00:04:38,146", "timestamp_s": 278.0}, {"text": "difference ends up breaking apart all the different techniques", "timestamp": "00:04:41,388", "timestamp_s": 281.0}, {"text": "that we\u0027ve built up along the years for making sense of these complex systems.", "timestamp": "00:04:45,014", "timestamp_s": 285.0}, {"text": "With normal APIs, you can write unit best.", "timestamp": "00:04:49,310", "timestamp_s": 289.0}, {"text": "With an API you can conceivably scope or", "timestamp": "00:04:54,290", "timestamp_s": 294.0}, {"text": "predict the full range of inputs in a useful way. On the LLMs", "timestamp": "00:04:57,492", "timestamp_s": 297.0}, {"text": "side, you\u0027re not working", "timestamp": "00:05:01,642", "timestamp_s": 301.0}, {"text": "with just the full range of negative and positive numbers.", "timestamp": "00:05:05,556", "timestamp_s": 305.0}, {"text": "You\u0027ve got a long tail of literally what we\u0027re soliciting", "timestamp": "00:05:09,624", "timestamp_s": 309.0}, {"text": "is free form natural language input from users.", "timestamp": "00:05:13,550", "timestamp_s": 313.0}, {"text": "We\u0027re not going to be able to have a reasonable test suite that we", "timestamp": "00:05:17,590", "timestamp_s": 317.0}, {"text": "can run reliably for reproducibility", "timestamp": "00:05:21,228", "timestamp_s": 321.0}, {"text": "APIs. Again, especially if it\u0027s", "timestamp": "00:05:26,730", "timestamp_s": 326.0}, {"text": "a software, as a service, it\u0027s something very consistent. You have", "timestamp": "00:05:30,114", "timestamp_s": 330.0}, {"text": "a payments service, typically when you say debit $5", "timestamp": "00:05:34,044", "timestamp_s": 334.0}, {"text": "from my bank account, the balance goes down by $5. It\u0027s predictable.", "timestamp": "00:05:38,032", "timestamp_s": 338.0}, {"text": "Ideally it\u0027s item potent, where if you\u0027re doing the same", "timestamp": "00:05:43,070", "timestamp_s": 343.0}, {"text": "transaction, bank account aside,", "timestamp": "00:05:46,276", "timestamp_s": 346.0}, {"text": "there\u0027s no additional strange side effects on", "timestamp": "00:05:49,730", "timestamp_s": 349.0}, {"text": "the LLM side. The way that many of these public", "timestamp": "00:05:53,428", "timestamp_s": 353.0}, {"text": "APIs are set up, usage by the public is", "timestamp": "00:05:57,432", "timestamp_s": 357.0}, {"text": "teaching the model itself additional behavior. And so", "timestamp": "00:06:01,112", "timestamp_s": 361.0}, {"text": "you have these API level regressions that", "timestamp": "00:06:04,376", "timestamp_s": 364.0}, {"text": "are happening, you can\u0027t control. And as", "timestamp": "00:06:08,328", "timestamp_s": 368.0}, {"text": "software engineers using that LLM, you need to adapt your prompts.", "timestamp": "00:06:12,188", "timestamp_s": 372.0}, {"text": "So again, not mockable, not reproducible.", "timestamp": "00:06:16,098", "timestamp_s": 376.0}, {"text": "And again, with a normal API, you can kind of reason", "timestamp": "00:06:19,610", "timestamp_s": 379.0}, {"text": "what it\u0027s supposed to be doing and whether the problem is on the API\u0027s", "timestamp": "00:06:23,264", "timestamp_s": 383.0}, {"text": "side or your application logic side, because there\u0027s", "timestamp": "00:06:27,062", "timestamp_s": 387.0}, {"text": "a spec, because it\u0027s explainable and you\u0027re", "timestamp": "00:06:30,854", "timestamp_s": 390.0}, {"text": "able to fit it in your head. On the LLM side,", "timestamp": "00:06:34,438", "timestamp_s": 394.0}, {"text": "it\u0027s really hard to make sense of some of these changes programmatically, because llms", "timestamp": "00:06:37,940", "timestamp_s": 397.0}, {"text": "are meant to almost simulate human behaviors.", "timestamp": "00:06:42,282", "timestamp_s": 402.0}, {"text": "It\u0027s kind of the point. And so a thing that we can see is", "timestamp": "00:06:45,594", "timestamp_s": 405.0}, {"text": "that very small changes to the prompt can yield very dramatic", "timestamp": "00:06:48,872", "timestamp_s": 408.0}, {"text": "changes to the results in ways that, again, make it hard", "timestamp": "00:06:52,926", "timestamp_s": 412.0}, {"text": "for humans to explain and debug,", "timestamp": "00:06:57,590", "timestamp_s": 417.0}, {"text": "and sort of build a mental model of how it\u0027s supposed to behave.", "timestamp": "00:07:01,362", "timestamp_s": 421.0}, {"text": "Now, these three techniques", "timestamp": "00:07:04,890", "timestamp_s": 424.0}, {"text": "on the left are ways that we have traditionally", "timestamp": "00:07:08,866", "timestamp_s": 428.0}, {"text": "tried to ensure correctness of our software.", "timestamp": "00:07:12,694", "timestamp_s": 432.0}, {"text": "And if you ask an ML team, the right", "timestamp": "00:07:16,750", "timestamp_s": 436.0}, {"text": "way to ensure correctness of something like", "timestamp": "00:07:20,576", "timestamp_s": 440.0}, {"text": "an LLM feature is to build an evaluation system to", "timestamp": "00:07:24,624", "timestamp_s": 444.0}, {"text": "evaluate the effectiveness of the model or the prompt.", "timestamp": "00:07:28,532", "timestamp_s": 448.0}, {"text": "But most of us trying to make sense of llms aren\u0027t ML engineers.", "timestamp": "00:07:32,370", "timestamp_s": 452.0}, {"text": "And the promise of llms exposed via APIs is that we shouldn\u0027t", "timestamp": "00:07:36,610", "timestamp_s": 456.0}, {"text": "have to be to fold these new capabilities into our software.", "timestamp": "00:07:39,822", "timestamp_s": 459.0}, {"text": "There\u0027s even one more layer of unpredictability", "timestamp": "00:07:44,150", "timestamp_s": 464.0}, {"text": "that llms introduce. There\u0027s a concept", "timestamp": "00:07:47,870", "timestamp_s": 467.0}, {"text": "of, I don\u0027t know how familiar everyone is with this piece,", "timestamp": "00:07:51,378", "timestamp_s": 471.0}, {"text": "but there\u0027s an acronym that is used in this world,", "timestamp": "00:07:54,556", "timestamp_s": 474.0}, {"text": "rag rags or retrieval augmented generation.", "timestamp": "00:07:58,204", "timestamp_s": 478.0}, {"text": "Effectively, it\u0027s a practice of pulling in", "timestamp": "00:08:01,746", "timestamp_s": 481.0}, {"text": "additional context within your domain to", "timestamp": "00:08:05,360", "timestamp_s": 485.0}, {"text": "help your llms return better results.", "timestamp": "00:08:08,912", "timestamp_s": 488.0}, {"text": "If you think about using Chat GPT prompt, it\u0027s where you", "timestamp": "00:08:12,270", "timestamp_s": 492.0}, {"text": "say, oh, do this but in this style, or do this", "timestamp": "00:08:16,212", "timestamp_s": 496.0}, {"text": "but in", "timestamp": "00:08:19,540", "timestamp_s": 499.0}, {"text": "a certain voice. All that extra context", "timestamp": "00:08:22,868", "timestamp_s": 502.0}, {"text": "helps make sure the LLM returns the result that you\u0027re looking for.", "timestamp": "00:08:26,266", "timestamp_s": 506.0}, {"text": "But it is because of the way that these", "timestamp": "00:08:30,550", "timestamp_s": 510.0}, {"text": "RaE Rag pipelines end up being built.", "timestamp": "00:08:35,096", "timestamp_s": 515.0}, {"text": "Really, it means that your app is pulling in even more dynamic", "timestamp": "00:08:38,712", "timestamp_s": 518.0}, {"text": "content and context that can", "timestamp": "00:08:42,178", "timestamp_s": 522.0}, {"text": "again create and result in big changes in how the LLM", "timestamp": "00:08:45,612", "timestamp_s": 525.0}, {"text": "is built, how the LLM is responding, and so", "timestamp": "00:08:49,778", "timestamp_s": 529.0}, {"text": "you have even more unpredictability in trying", "timestamp": "00:08:52,928", "timestamp_s": 532.0}, {"text": "to figure out why is my user", "timestamp": "00:08:56,288", "timestamp_s": 536.0}, {"text": "not having the experience that I want them to have?", "timestamp": "00:08:59,702", "timestamp_s": 539.0}, {"text": "So this turning upside down of our worldview is happening", "timestamp": "00:09:04,030", "timestamp_s": 544.0}, {"text": "on a literal software engineering and systems engineering level.", "timestamp": "00:09:07,828", "timestamp_s": 547.0}, {"text": "We know these black boxes aren\u0027t testable or debuggable in a traditional sense,", "timestamp": "00:09:11,730", "timestamp_s": 551.0}, {"text": "so there\u0027s no solid sense of correct behavior", "timestamp": "00:09:16,004", "timestamp_s": 556.0}, {"text": "that we can fall back to. It\u0027s also true from", "timestamp": "00:09:19,262", "timestamp_s": 559.0}, {"text": "a meta level where there\u0027s no environment within which we can conduct our", "timestamp": "00:09:22,568", "timestamp_s": 562.0}, {"text": "tests and feel confident in the results.", "timestamp": "00:09:26,328", "timestamp_s": 566.0}, {"text": "There\u0027s no creating a staging environment where we can be sure that the LLMs experience", "timestamp": "00:09:31,610", "timestamp_s": 571.0}, {"text": "or feature that we\u0027re building behaves correctly or", "timestamp": "00:09:35,868", "timestamp_s": 575.0}, {"text": "does what the user wants. Going even", "timestamp": "00:09:39,292", "timestamp_s": 579.0}, {"text": "one step further, even product development", "timestamp": "00:09:43,392", "timestamp_s": 583.0}, {"text": "or release practices are turned a little bit inside out.", "timestamp": "00:09:46,758", "timestamp_s": 586.0}, {"text": "Instead of being able to start with early access and then", "timestamp": "00:09:50,592", "timestamp_s": 590.0}, {"text": "putting your product through its paces and then feeling confident in a later or broader", "timestamp": "00:09:54,272", "timestamp_s": 594.0}, {"text": "release, early access programs are inherently going to fail", "timestamp": "00:09:57,818", "timestamp_s": 597.0}, {"text": "to capture that full range of user behavior and edge cases.", "timestamp": "00:10:01,882", "timestamp_s": 601.0}, {"text": "All these programs do is delay the inevitable failures that you\u0027ll encounter", "timestamp": "00:10:06,370", "timestamp_s": 606.0}, {"text": "when you have an uncontrolled and unprompted group of group of users doing", "timestamp": "00:10:10,286", "timestamp_s": 610.0}, {"text": "things that you never expected them to do.", "timestamp": "00:10:14,440", "timestamp_s": 614.0}, {"text": "So at this point,", "timestamp": "00:10:17,510", "timestamp_s": 617.0}, {"text": "do we just give up on everything we\u0027ve learned about building and operating software systems", "timestamp": "00:10:21,260", "timestamp_s": 621.0}, {"text": "and embrace the rise of prompt engineer as an entirely", "timestamp": "00:10:25,850", "timestamp_s": 625.0}, {"text": "separate skill set. Well, if you\u0027ve been paying attention to the title of this talk,", "timestamp": "00:10:29,218", "timestamp_s": 629.0}, {"text": "the answer is obviously not,", "timestamp": "00:10:33,440", "timestamp_s": 633.0}, {"text": "because we already have a model for how to measure and debug and move the", "timestamp": "00:10:36,830", "timestamp_s": 636.0}, {"text": "needle on an unpredictable qualitative experience.", "timestamp": "00:10:40,144", "timestamp_s": 640.0}, {"text": "Observability. And I\u0027ll say this term", "timestamp": "00:10:44,270", "timestamp_s": 644.0}, {"text": "has become so commonplace today, it\u0027s fallen out of fashion to define", "timestamp": "00:10:48,038", "timestamp_s": 648.0}, {"text": "it. But as someone who\u0027s been talking about all of this since before it was", "timestamp": "00:10:51,274", "timestamp_s": 651.0}, {"text": "cool, humor me. I think it\u0027ll help some pieces click into place.", "timestamp": "00:10:54,932", "timestamp_s": 654.0}, {"text": "This here is the formal Wikipedia definition", "timestamp": "00:11:00,230", "timestamp_s": 660.0}, {"text": "of observability. It comes from control theory. It\u0027s about", "timestamp": "00:11:03,758", "timestamp_s": 663.0}, {"text": "looking at a system based on the inputs and outputs and using", "timestamp": "00:11:07,496", "timestamp_s": 667.0}, {"text": "that to model what this system is doing. Black box", "timestamp": "00:11:10,876", "timestamp_s": 670.0}, {"text": "and it feels a little overly formal when talking about production systems.", "timestamp": "00:11:15,170", "timestamp_s": 675.0}, {"text": "Software systems still applies to,", "timestamp": "00:11:19,314", "timestamp_s": 679.0}, {"text": "but it feels like really applicable to a system like", "timestamp": "00:11:22,192", "timestamp_s": 682.0}, {"text": "an LLM, like this thing that\u0027s changing over time because", "timestamp": "00:11:26,048", "timestamp_s": 686.0}, {"text": "it can\u0027t be monitored or simulated with traditional techniques.", "timestamp": "00:11:30,672", "timestamp_s": 690.0}, {"text": "Another way I like to think about this is that less formally,", "timestamp": "00:11:35,410", "timestamp_s": 695.0}, {"text": "observability is a way of comparing what you expect in", "timestamp": "00:11:39,114", "timestamp_s": 699.0}, {"text": "your head versus the actual behavior,", "timestamp": "00:11:43,428", "timestamp_s": 703.0}, {"text": "but in live systems. And so let\u0027s", "timestamp": "00:11:46,790", "timestamp_s": 706.0}, {"text": "take a look at what this means for a standard web app.", "timestamp": "00:11:50,478", "timestamp_s": 710.0}, {"text": "Well, you\u0027re looking at this box has your application.", "timestamp": "00:11:53,432", "timestamp_s": 713.0}, {"text": "Because it\u0027s our application, we actually get to instrument it and we can capture what", "timestamp": "00:11:57,910", "timestamp_s": 717.0}, {"text": "arguments were sent to it. On any given HTTP request, we can", "timestamp": "00:12:01,720", "timestamp_s": 721.0}, {"text": "capture some metadata about how the app was running and we can", "timestamp": "00:12:05,084", "timestamp_s": 725.0}, {"text": "capture data about what was returned. This lets us reason about the behavior", "timestamp": "00:12:08,348", "timestamp_s": 728.0}, {"text": "we can expect for a given user and endpoint", "timestamp": "00:12:12,242", "timestamp_s": 732.0}, {"text": "and set of parameters. And it lets us debug and reproduce the issue", "timestamp": "00:12:15,782", "timestamp_s": 735.0}, {"text": "if the actual behavior we see deviates from that expectation.", "timestamp": "00:12:19,296", "timestamp_s": 739.0}, {"text": "Again, lots of parallels to best, but on live", "timestamp": "00:12:23,238", "timestamp_s": 743.0}, {"text": "data. What about this payment service over here on the right?", "timestamp": "00:12:27,088", "timestamp_s": 747.0}, {"text": "It\u0027s that black box that the app depends on. It\u0027s out of my control.", "timestamp": "00:12:30,964", "timestamp_s": 750.0}, {"text": "Might be another company entirely. And even", "timestamp": "00:12:34,868", "timestamp_s": 754.0}, {"text": "if I wanted to, because of that, I couldn\u0027t go and shove instrumentation inside", "timestamp": "00:12:38,052", "timestamp_s": 758.0}, {"text": "of it. You can think of this like a database too, right? You\u0027re not going", "timestamp": "00:12:41,832", "timestamp_s": 761.0}, {"text": "to go and fork Mysql and shove your own instrumentation in there.", "timestamp": "00:12:44,664", "timestamp_s": 764.0}, {"text": "But I know what requests my app has sent to it.", "timestamp": "00:12:48,472", "timestamp_s": 768.0}, {"text": "I know where those requests are coming from in the code", "timestamp": "00:12:51,832", "timestamp_s": 771.0}, {"text": "and on behalf of which user. And then I know how long it took", "timestamp": "00:12:55,052", "timestamp_s": 775.0}, {"text": "to respond from the app\u0027s perspective, whether it was successful,", "timestamp": "00:12:58,348", "timestamp_s": 778.0}, {"text": "and probably some other metadata. By capturing all of", "timestamp": "00:13:01,698", "timestamp_s": 781.0}, {"text": "that I can again start to reason, or at least have a paper trail,", "timestamp": "00:13:05,264", "timestamp_s": 785.0}, {"text": "to understand how these inputs impact the outputs", "timestamp": "00:13:09,190", "timestamp_s": 789.0}, {"text": "of my black box and then how the choices my application makes and the", "timestamp": "00:13:12,598", "timestamp_s": 792.0}, {"text": "inputs into that application impacts all of that.", "timestamp": "00:13:16,468", "timestamp_s": 796.0}, {"text": "And that approach becomes the same for llms, as unpredictable and", "timestamp": "00:13:19,652", "timestamp_s": 799.0}, {"text": "nondeterministic as they are. We know how a user interacts", "timestamp": "00:13:23,508", "timestamp_s": 803.0}, {"text": "with the app, we know how the app turns that into", "timestamp": "00:13:27,258", "timestamp_s": 807.0}, {"text": "parameters for the black box, and we know", "timestamp": "00:13:30,616", "timestamp_s": 810.0}, {"text": "how they respond. It\u0027s a blanket statement that in complex systems,", "timestamp": "00:13:33,688", "timestamp_s": 813.0}, {"text": "software usage patterns will become unpredictable and change over time.", "timestamp": "00:13:37,710", "timestamp_s": 817.0}, {"text": "With llms, that assertion becomes a guarantee.", "timestamp": "00:13:41,610", "timestamp_s": 821.0}, {"text": "If you use llms, as many of us are,", "timestamp": "00:13:44,818", "timestamp_s": 824.0}, {"text": "your data set is going to be unpredictable and will absolutely", "timestamp": "00:13:48,890", "timestamp_s": 828.0}, {"text": "change over time. So the key to operating sanely", "timestamp": "00:13:52,620", "timestamp_s": 832.0}, {"text": "on top of that magical foundation is having a way of", "timestamp": "00:13:56,054", "timestamp_s": 836.0}, {"text": "gathering, aggregating and exploring that data in a way", "timestamp": "00:13:59,968", "timestamp_s": 839.0}, {"text": "that captures what the user experienced as expressively", "timestamp": "00:14:03,392", "timestamp_s": 843.0}, {"text": "as possible. That\u0027s what lets you build and reason", "timestamp": "00:14:07,098", "timestamp_s": 847.0}, {"text": "and ensure a quality experience on top of llms, the ability to", "timestamp": "00:14:10,436", "timestamp_s": 850.0}, {"text": "understand from the outside why your user got a certain", "timestamp": "00:14:13,988", "timestamp_s": 853.0}, {"text": "response from your llmbacked application.", "timestamp": "00:14:17,752", "timestamp_s": 857.0}, {"text": "Observability creates these feedback loops to let", "timestamp": "00:14:20,790", "timestamp_s": 860.0}, {"text": "you learn from what\u0027s really happening with your code,", "timestamp": "00:14:23,992", "timestamp_s": 863.0}, {"text": "the same way we\u0027ve all learned how to work iteratively with tests.", "timestamp": "00:14:27,048", "timestamp_s": 867.0}, {"text": "Observability enables us all to ship sooner,", "timestamp": "00:14:31,906", "timestamp_s": 871.0}, {"text": "observe those results in the wild, and then wrap those observations back", "timestamp": "00:14:34,890", "timestamp_s": 874.0}, {"text": "into the development process. With llms rapidly", "timestamp": "00:14:38,508", "timestamp_s": 878.0}, {"text": "becoming some piece of every software", "timestamp": "00:14:41,778", "timestamp_s": 881.0}, {"text": "system, we all get to learn some new skills.", "timestamp": "00:14:45,158", "timestamp_s": 885.0}, {"text": "SRes who are used to thinking of APIs as black boxes that can be modeled", "timestamp": "00:14:48,214", "timestamp_s": 888.0}, {"text": "and asserted on, now have to get used to drift and peeling", "timestamp": "00:14:51,814", "timestamp_s": 891.0}, {"text": "back a layer to examine that emergent behavior.", "timestamp": "00:14:55,418", "timestamp_s": 895.0}, {"text": "Software engineers who are used to boolean logic and discrete math", "timestamp": "00:14:58,690", "timestamp_s": 898.0}, {"text": "and correctness and test driven development now", "timestamp": "00:15:02,218", "timestamp_s": 902.0}, {"text": "have to think about data quality, probabilistic systems and", "timestamp": "00:15:06,132", "timestamp_s": 906.0}, {"text": "representivity, or how well your model test", "timestamp": "00:15:10,648", "timestamp_s": 910.0}, {"text": "environment or your staging environment,", "timestamp": "00:15:15,160", "timestamp_s": 915.0}, {"text": "or your mental code represents the production system.", "timestamp": "00:15:18,110", "timestamp_s": 918.0}, {"text": "And everyone in engineering needs to reorient themselves around", "timestamp": "00:15:22,010", "timestamp_s": 922.0}, {"text": "what this LLM thing is", "timestamp": "00:15:25,628", "timestamp_s": 925.0}, {"text": "trying to achieve, what the business goals are, what the product use cases are,", "timestamp": "00:15:29,244", "timestamp_s": 929.0}, {"text": "what the ideal user experience is, instead of", "timestamp": "00:15:32,752", "timestamp_s": 932.0}, {"text": "sterile concepts like correctness, reliability or availability.", "timestamp": "00:15:36,432", "timestamp_s": 936.0}, {"text": "Those last three are still important. But ultimately,", "timestamp": "00:15:39,950", "timestamp_s": 939.0}, {"text": "when you bring in this thing that is so free form that", "timestamp": "00:15:43,286", "timestamp_s": 943.0}, {"text": "the human on the other end is going to have their own opinion of whether", "timestamp": "00:15:47,812", "timestamp_s": 947.0}, {"text": "your LLM feature was useful or not, we all need", "timestamp": "00:15:51,620", "timestamp_s": 951.0}, {"text": "to think expand our mental models of what it means to provide a great", "timestamp": "00:15:54,868", "timestamp_s": 954.0}, {"text": "service to include that definition as well.", "timestamp": "00:15:58,708", "timestamp_s": 958.0}, {"text": "So, okay, why am I up here talking about this and why should", "timestamp": "00:16:01,990", "timestamp_s": 961.0}, {"text": "you believe me? I\u0027m going to tell you a little bit about a", "timestamp": "00:16:05,144", "timestamp_s": 965.0}, {"text": "feature that we released and our experience building it,", "timestamp": "00:16:09,228", "timestamp_s": 969.0}, {"text": "trying to ensure that it would be a great experience, and maintaining it", "timestamp": "00:16:13,804", "timestamp_s": 973.0}, {"text": "going forward. So earlier this year we", "timestamp": "00:16:17,468", "timestamp_s": 977.0}, {"text": "released our query assistant in May 2023.", "timestamp": "00:16:21,212", "timestamp_s": 981.0}, {"text": "Took about six weeks of development super fast,", "timestamp": "00:16:24,750", "timestamp_s": 984.0}, {"text": "and we spent another eight weeks iterating on it.", "timestamp": "00:16:28,750", "timestamp_s": 988.0}, {"text": "And to give you a little bit of an overview of what it was trying", "timestamp": "00:16:31,808", "timestamp_s": 991.0}, {"text": "to do, Honeycomb as an observability tool", "timestamp": "00:16:34,928", "timestamp_s": 994.0}, {"text": "lets our users work with a lot of data. Our product has a visual query", "timestamp": "00:16:38,910", "timestamp_s": 998.0}, {"text": "interface. We believe that point and click is always going to be easier for someone", "timestamp": "00:16:42,202", "timestamp_s": 1002.0}, {"text": "to learn and play around with than an open text box. But even", "timestamp": "00:16:45,652", "timestamp_s": 1005.0}, {"text": "so, there\u0027s a learning curve to the user\u0027s interface and we were really", "timestamp": "00:16:49,188", "timestamp_s": 1009.0}, {"text": "excited about being able to use llms as a translation layer from", "timestamp": "00:16:52,472", "timestamp_s": 1012.0}, {"text": "what the human is trying to do over here on the right of this slide", "timestamp": "00:16:56,744", "timestamp_s": 1016.0}, {"text": "into the UI. And so we added this little experimental", "timestamp": "00:17:00,126", "timestamp_s": 1020.0}, {"text": "piece to the query. Building collapsed most of the time, but people could expand", "timestamp": "00:17:05,122", "timestamp_s": 1025.0}, {"text": "it and we let people type in in English", "timestamp": "00:17:08,658", "timestamp_s": 1028.0}, {"text": "what they were hoping to SRE. And we also another thing that was", "timestamp": "00:17:12,498", "timestamp_s": 1032.0}, {"text": "important to us is that we preserve the editability and explorability that\u0027s sort of", "timestamp": "00:17:16,304", "timestamp_s": 1036.0}, {"text": "inherent in our product. The same way that we", "timestamp": "00:17:19,904", "timestamp_s": 1039.0}, {"text": "all as consumers have gotten used to being able to edit or iterate on", "timestamp": "00:17:24,208", "timestamp_s": 1044.0}, {"text": "our response with Chat GPT. We wanted users to be able", "timestamp": "00:17:27,524", "timestamp_s": 1047.0}, {"text": "to get the output honeycomb would", "timestamp": "00:17:30,692", "timestamp_s": 1050.0}, {"text": "building the query for them, but be able to tweak", "timestamp": "00:17:34,068", "timestamp_s": 1054.0}, {"text": "and iterate on it. Because we wanted to encourage that iteration,", "timestamp": "00:17:37,438", "timestamp_s": 1057.0}, {"text": "we realized that there would be no concrete and", "timestamp": "00:17:41,830", "timestamp_s": 1061.0}, {"text": "quantitative result we could rely on that would cleanly", "timestamp": "00:17:45,512", "timestamp_s": 1065.0}, {"text": "describe whether the feature itself was good. If users ran", "timestamp": "00:17:49,858", "timestamp_s": 1069.0}, {"text": "more queries, maybe it was good, maybe we were just consistently", "timestamp": "00:17:53,218", "timestamp_s": 1073.0}, {"text": "being not useful. Maybe fewer queries were good,", "timestamp": "00:17:58,250", "timestamp_s": 1078.0}, {"text": "but maybe they just weren\u0027t using the product or they didn\u0027t understand what was going", "timestamp": "00:18:02,444", "timestamp_s": 1082.0}, {"text": "on. So we knew we would need to capture this qualitative feedback,", "timestamp": "00:18:06,112", "timestamp_s": 1086.0}, {"text": "the yes no, I\u0027m not sure buttons, so that we", "timestamp": "00:18:10,118", "timestamp_s": 1090.0}, {"text": "could understand from the user\u0027s perspective whether", "timestamp": "00:18:13,712", "timestamp_s": 1093.0}, {"text": "this thing that we tried to sre them was actually helpful or not.", "timestamp": "00:18:17,796", "timestamp_s": 1097.0}, {"text": "And then we could posit some higher level product goals, like product retention for", "timestamp": "00:18:21,268", "timestamp_s": 1101.0}, {"text": "new uses, to layer on top of that as", "timestamp": "00:18:25,188", "timestamp_s": 1105.0}, {"text": "a spoiler, we hit these goals. We were thrilled, but we did a lot of", "timestamp": "00:18:29,172", "timestamp_s": 1109.0}, {"text": "stumbling around in the dark along the way.", "timestamp": "00:18:32,264", "timestamp_s": 1112.0}, {"text": "And today, six months later, it\u0027s so much more common for", "timestamp": "00:18:35,270", "timestamp_s": 1115.0}, {"text": "us to meet someone playing around with llms than someone", "timestamp": "00:18:39,096", "timestamp_s": 1119.0}, {"text": "whose product has actual LLM functionality deployed", "timestamp": "00:18:43,404", "timestamp_s": 1123.0}, {"text": "in production. And we think that a lot of this is rooted", "timestamp": "00:18:46,514", "timestamp_s": 1126.0}, {"text": "in the fact that our teams have really embraced observability techniques", "timestamp": "00:18:50,498", "timestamp_s": 1130.0}, {"text": "in how we ship software, period. And those were key to", "timestamp": "00:18:54,354", "timestamp_s": 1134.0}, {"text": "building the confidence to ship this thing fast", "timestamp": "00:18:57,872", "timestamp_s": 1137.0}, {"text": "and iterate live and really just understand that", "timestamp": "00:19:01,840", "timestamp_s": 1141.0}, {"text": "we were going to have to react", "timestamp": "00:19:05,428", "timestamp_s": 1145.0}, {"text": "based on how the broader user base", "timestamp": "00:19:08,970", "timestamp_s": 1148.0}, {"text": "used the product.", "timestamp": "00:19:12,612", "timestamp_s": 1152.0}, {"text": "These were some learnings that we had fairly early on.", "timestamp": "00:19:16,850", "timestamp_s": 1156.0}, {"text": "There\u0027s a great blog post that this is excerpted from. You should", "timestamp": "00:19:20,120", "timestamp_s": 1160.0}, {"text": "check it out if you\u0027re again in the phase of building on llms", "timestamp": "00:19:23,688", "timestamp_s": 1163.0}, {"text": "but it\u0027s all about things are going to fall apart.", "timestamp": "00:19:27,430", "timestamp_s": 1167.0}, {"text": "It\u0027s not a question of how to prevent failures from happening,", "timestamp": "00:19:31,450", "timestamp_s": 1171.0}, {"text": "it\u0027s a question of can you detect it quickly", "timestamp": "00:19:34,860", "timestamp_s": 1174.0}, {"text": "enough? Because you just can\u0027t predict what a user is", "timestamp": "00:19:38,316", "timestamp_s": 1178.0}, {"text": "going to type into that freeform text box. You will ship", "timestamp": "00:19:41,728", "timestamp_s": 1181.0}, {"text": "something that breaks something else, and it\u0027s okay.", "timestamp": "00:19:45,302", "timestamp_s": 1185.0}, {"text": "And again, you can\u0027t predict. You can\u0027t rely", "timestamp": "00:19:48,640", "timestamp_s": 1188.0}, {"text": "on your test frameworks, you can\u0027t rely on your CI pipelines.", "timestamp": "00:19:53,238", "timestamp_s": 1193.0}, {"text": "So how do you react quickly enough? How do you capture the information", "timestamp": "00:19:56,850", "timestamp_s": 1196.0}, {"text": "that you need in order to come in and debug and improve", "timestamp": "00:20:00,356", "timestamp_s": 1200.0}, {"text": "going forward? So let\u0027s get a little bit,", "timestamp": "00:20:04,922", "timestamp_s": 1204.0}, {"text": "go one level deeper. How do we go forward? Well, talked a lot about capturing", "timestamp": "00:20:08,260", "timestamp_s": 1208.0}, {"text": "instrumentation, leaving this paper trail for how and why your", "timestamp": "00:20:12,606", "timestamp_s": 1212.0}, {"text": "code behaves a certain way. I think of instrumentation,", "timestamp": "00:20:15,688", "timestamp_s": 1215.0}, {"text": "frankly, like documentation and tests,", "timestamp": "00:20:19,522", "timestamp_s": 1219.0}, {"text": "they sre all ways to try to get your code", "timestamp": "00:20:23,634", "timestamp_s": 1223.0}, {"text": "to explain itself back to you. And instrumentation is", "timestamp": "00:20:27,740", "timestamp_s": 1227.0}, {"text": "like capturing debug statements and breakpoints in your production code,", "timestamp": "00:20:31,792", "timestamp_s": 1231.0}, {"text": "as much in the language of your application and", "timestamp": "00:20:36,350", "timestamp_s": 1236.0}, {"text": "the unique business logic of your product and domain", "timestamp": "00:20:40,016", "timestamp_s": 1240.0}, {"text": "as possible. In a normal software system, this can let you", "timestamp": "00:20:43,254", "timestamp_s": 1243.0}, {"text": "do things as simple as figure out quickly which", "timestamp": "00:20:46,964", "timestamp_s": 1246.0}, {"text": "individual user or account is associated with that unexpected behavior.", "timestamp": "00:20:50,612", "timestamp_s": 1250.0}, {"text": "It can let you do things as complex as deploy a", "timestamp": "00:20:54,850", "timestamp_s": 1254.0}, {"text": "few different implementations of a given np complete problem,", "timestamp": "00:20:58,308", "timestamp_s": 1258.0}, {"text": "get it behind a given feature flag, compare the results of each approach,", "timestamp": "00:21:01,832", "timestamp_s": 1261.0}, {"text": "and pick the implementation that behaves best on live data.", "timestamp": "00:21:05,678", "timestamp_s": 1265.0}, {"text": "When you have rich data that you need to", "timestamp": "00:21:09,430", "timestamp_s": 1269.0}, {"text": "tease apart all the different parameters that you\u0027re varying in your experiment,", "timestamp": "00:21:14,170", "timestamp_s": 1274.0}, {"text": "you\u0027re able to then validate your hypothesis much more quickly and flexibly", "timestamp": "00:21:18,418", "timestamp_s": 1278.0}, {"text": "along the way. And so in the LLM world, this is how", "timestamp": "00:21:22,754", "timestamp_s": 1282.0}, {"text": "we applied those principles. You want to capture as much as you can about", "timestamp": "00:21:26,144", "timestamp_s": 1286.0}, {"text": "what your users are doing in your system in a format that lets you view", "timestamp": "00:21:29,808", "timestamp_s": 1289.0}, {"text": "overarching performance, and then also debug any", "timestamp": "00:21:33,552", "timestamp_s": 1293.0}, {"text": "individual transaction. Over here on the right is actually a", "timestamp": "00:21:37,332", "timestamp_s": 1297.0}, {"text": "screenshot of a real trace that we have for how we sre", "timestamp": "00:21:40,884", "timestamp_s": 1300.0}, {"text": "building up a request to our", "timestamp": "00:21:45,570", "timestamp_s": 1305.0}, {"text": "LLM provider. This goes from user click through", "timestamp": "00:21:49,192", "timestamp_s": 1309.0}, {"text": "the dynamic prompt building to the actual LLM request", "timestamp": "00:21:52,648", "timestamp_s": 1312.0}, {"text": "response parsing, response validation and the query execution in", "timestamp": "00:21:56,222", "timestamp_s": 1316.0}, {"text": "our product. And having all of this", "timestamp": "00:21:59,688", "timestamp_s": 1319.0}, {"text": "full trace and then lots of metadata on each of those individual spans", "timestamp": "00:22:02,972", "timestamp_s": 1322.0}, {"text": "lets us ask high level questions about the end user", "timestamp": "00:22:07,290", "timestamp_s": 1327.0}, {"text": "experience. Here you can see the results", "timestamp": "00:22:10,738", "timestamp_s": 1330.0}, {"text": "of that yes, those yes no I\u0027m not sure buttons in a way that", "timestamp": "00:22:13,794", "timestamp_s": 1333.0}, {"text": "lets us quantitatively ask questions and tricky progress,", "timestamp": "00:22:17,408", "timestamp_s": 1337.0}, {"text": "but always be able to get back to okay for", "timestamp": "00:22:21,014", "timestamp_s": 1341.0}, {"text": "this one interaction where someone said no,", "timestamp": "00:22:25,010", "timestamp_s": 1345.0}, {"text": "it didn\u0027t answer their question, what was their input?", "timestamp": "00:22:28,868", "timestamp_s": 1348.0}, {"text": "What did we try to do? How could we build up that prompt? Better to", "timestamp": "00:22:32,698", "timestamp_s": 1352.0}, {"text": "make sure that their intent gets passed to the LLM and reflected", "timestamp": "00:22:36,792", "timestamp_s": 1356.0}, {"text": "in our product as effectively as possible.", "timestamp": "00:22:40,558", "timestamp_s": 1360.0}, {"text": "Let us ask high level questions about things like", "timestamp": "00:22:44,390", "timestamp_s": 1364.0}, {"text": "trends in the latency of actual LLM request and", "timestamp": "00:22:48,040", "timestamp_s": 1368.0}, {"text": "response calls, and then let", "timestamp": "00:22:51,528", "timestamp_s": 1371.0}, {"text": "us take those metrics and group them on really fine", "timestamp": "00:22:55,148", "timestamp_s": 1375.0}, {"text": "grained characteristics of each request. And this lets us then", "timestamp": "00:22:58,636", "timestamp_s": 1378.0}, {"text": "draw conclusions about how certain parameters for", "timestamp": "00:23:02,368", "timestamp_s": 1382.0}, {"text": "a given team, for a given column data set,", "timestamp": "00:23:05,728", "timestamp_s": 1385.0}, {"text": "whatever might impact the actual LLM operation.", "timestamp": "00:23:08,976", "timestamp_s": 1388.0}, {"text": "Again, you can think of that was an e commerce site having", "timestamp": "00:23:12,830", "timestamp_s": 1392.0}, {"text": "things like shopping cart id or number of items in the cart as", "timestamp": "00:23:16,212", "timestamp_s": 1396.0}, {"text": "parameters here. But by capturing all of this related", "timestamp": "00:23:20,116", "timestamp_s": 1400.0}, {"text": "to the LLM, I am now armed to deal with whoa,", "timestamp": "00:23:24,122", "timestamp_s": 1404.0}, {"text": "something weird started happening with llms with our LLM response.", "timestamp": "00:23:28,682", "timestamp_s": 1408.0}, {"text": "What changed? Why? What\u0027s different", "timestamp": "00:23:33,102", "timestamp_s": 1413.0}, {"text": "about that one account that is having a dramatically different experience", "timestamp": "00:23:36,440", "timestamp_s": 1416.0}, {"text": "than everyone else, and then what\u0027s intended?", "timestamp": "00:23:39,752", "timestamp_s": 1419.0}, {"text": "We were also able to really closely capture and track errors,", "timestamp": "00:23:45,530", "timestamp_s": 1425.0}, {"text": "but in a flexible, not everything marked an error is", "timestamp": "00:23:48,946", "timestamp_s": 1428.0}, {"text": "necessarily an error kind of way. It\u0027s early. We don\u0027t know", "timestamp": "00:23:52,396", "timestamp_s": 1432.0}, {"text": "which errors to take seriously and which ones don\u0027t. I think a", "timestamp": "00:23:55,728", "timestamp_s": 1435.0}, {"text": "principle I go by is not every exception is exceptional.", "timestamp": "00:23:59,184", "timestamp_s": 1439.0}, {"text": "Not everything exceptional is captured as an exception.", "timestamp": "00:24:03,222", "timestamp_s": 1443.0}, {"text": "And so we wanted to capture things that were fairly open ended, that always let", "timestamp": "00:24:06,118", "timestamp_s": 1446.0}, {"text": "us correlate back to, okay, well, what was the user actually trying to do?", "timestamp": "00:24:09,188", "timestamp_s": 1449.0}, {"text": "What did they see? And we captured this all in", "timestamp": "00:24:13,092", "timestamp_s": 1453.0}, {"text": "one trace. So we had the full context for what", "timestamp": "00:24:16,212", "timestamp_s": 1456.0}, {"text": "went into a given response to a user. This blue", "timestamp": "00:24:19,592", "timestamp_s": 1459.0}, {"text": "span I\u0027ve highlighted at the bottom, it\u0027s tiny text,", "timestamp": "00:24:23,598", "timestamp_s": 1463.0}, {"text": "but if you squint, you can see that this finally is", "timestamp": "00:24:26,856", "timestamp_s": 1466.0}, {"text": "our call to OpenAI. All the spans above it", "timestamp": "00:24:31,228", "timestamp_s": 1471.0}, {"text": "are work that we are doing inside the application to build the best prompt that", "timestamp": "00:24:34,764", "timestamp_s": 1474.0}, {"text": "we can. Which also means there are that many", "timestamp": "00:24:37,964", "timestamp_s": 1477.0}, {"text": "possible things that could go wrong that could result in a poor response", "timestamp": "00:24:41,916", "timestamp_s": 1481.0}, {"text": "from OpenAI or whatever llms you\u0027re using.", "timestamp": "00:24:45,462", "timestamp_s": 1485.0}, {"text": "And so as we were building this feature, and as we", "timestamp": "00:24:48,670", "timestamp_s": 1488.0}, {"text": "knew we wanted to iterate, we\u0027d need all this context if we had any", "timestamp": "00:24:52,064", "timestamp_s": 1492.0}, {"text": "hope of figuring out why things were going to go wrong and", "timestamp": "00:24:55,952", "timestamp_s": 1495.0}, {"text": "how to iterate towards a better future. Now,", "timestamp": "00:24:59,412", "timestamp_s": 1499.0}, {"text": "a lot of these behaviors have been on the rise for a while,", "timestamp": "00:25:02,548", "timestamp_s": 1502.0}, {"text": "may already be practiced by your team.", "timestamp": "00:25:06,132", "timestamp_s": 1506.0}, {"text": "I think that\u0027s an awesome thing. As a baby software engineer,", "timestamp": "00:25:09,330", "timestamp_s": 1509.0}, {"text": "I took a lot of pride in just shipping really fast, and I wrote", "timestamp": "00:25:13,102", "timestamp_s": 1513.0}, {"text": "lots of tests along the way, of course, because I was an accepted and celebrated", "timestamp": "00:25:17,038", "timestamp_s": 1517.0}, {"text": "part of shipping good code. But in the last decade or", "timestamp": "00:25:20,606", "timestamp_s": 1520.0}, {"text": "so, we\u0027ve seen a bit of a shift in the conversation.", "timestamp": "00:25:24,092", "timestamp_s": 1524.0}, {"text": "Instead of just writing lots of code being a sign of a good developer,", "timestamp": "00:25:27,690", "timestamp_s": 1527.0}, {"text": "there\u0027s phrases like service ownership, putting developers on call,", "timestamp": "00:25:31,570", "timestamp_s": 1531.0}, {"text": "testing in production. And as these phrases have entered our", "timestamp": "00:25:35,920", "timestamp_s": 1535.0}, {"text": "collective consciousness, it has shifted", "timestamp": "00:25:39,392", "timestamp_s": 1539.0}, {"text": "the domain, I think, of a developer from", "timestamp": "00:25:43,810", "timestamp_s": 1543.0}, {"text": "purely thinking about development to also thinking about production.", "timestamp": "00:25:47,940", "timestamp_s": 1547.0}, {"text": "And I\u0027m really excited about this because a lot of these, the shift that", "timestamp": "00:25:52,690", "timestamp_s": 1552.0}, {"text": "is already kind of underway of taking what", "timestamp": "00:25:56,548", "timestamp_s": 1556.0}, {"text": "we do in its TDD world and", "timestamp": "00:25:59,988", "timestamp_s": 1559.0}, {"text": "recognizing they can apply to production as well through Ollie or observability.", "timestamp": "00:26:04,616", "timestamp_s": 1564.0}, {"text": "We\u0027re just taking these behaviors that we know as developers", "timestamp": "00:26:09,430", "timestamp_s": 1569.0}, {"text": "and applying it under a different name in development", "timestamp": "00:26:12,626", "timestamp_s": 1572.0}, {"text": "or in the test environment. We\u0027re identifying the levers that impact", "timestamp": "00:26:16,482", "timestamp_s": 1576.0}, {"text": "logical branches in the code for debug ability and reproducibility, and making", "timestamp": "00:26:19,762", "timestamp_s": 1579.0}, {"text": "sure to exercise those in a test in observability.", "timestamp": "00:26:23,552", "timestamp_s": 1583.0}, {"text": "You\u0027re instrumenting code with intention so that you can do the same in production.", "timestamp": "00:26:27,310", "timestamp_s": 1587.0}, {"text": "When you\u0027re writing a test, you\u0027re thinking about what you", "timestamp": "00:26:31,470", "timestamp_s": 1591.0}, {"text": "expect and you\u0027re asserting on what", "timestamp": "00:26:35,012", "timestamp_s": 1595.0}, {"text": "you\u0027ll actually get with observability and looking", "timestamp": "00:26:38,484", "timestamp_s": 1598.0}, {"text": "at your systems in production. You\u0027re just inspecting results after", "timestamp": "00:26:42,052", "timestamp_s": 1602.0}, {"text": "the changes have been rolled out and you\u0027re watching for deviations when", "timestamp": "00:26:45,908", "timestamp_s": 1605.0}, {"text": "you\u0027re writing tests, especially if you\u0027re practicing real TDD,", "timestamp": "00:26:49,768", "timestamp_s": 1609.0}, {"text": "I know not everyone does. You\u0027re embracing these fast fail loops,", "timestamp": "00:26:54,310", "timestamp_s": 1614.0}, {"text": "fast feedback loops. You are expecting", "timestamp": "00:26:58,510", "timestamp_s": 1618.0}, {"text": "to act on the output of these feedback loops to make your code better.", "timestamp": "00:27:03,170", "timestamp_s": 1623.0}, {"text": "And that\u0027s all observability is all about.", "timestamp": "00:27:07,050", "timestamp_s": 1627.0}, {"text": "It\u0027s shipping to production quickly through your", "timestamp": "00:27:09,884", "timestamp_s": 1629.0}, {"text": "CI CD pipeline or through feature flags, and then expecting", "timestamp": "00:27:13,312", "timestamp_s": 1633.0}, {"text": "to iterate even on code that you think is shipped. And it\u0027s", "timestamp": "00:27:17,238", "timestamp_s": 1637.0}, {"text": "exciting that these are guardrails that we\u0027ve generalized for", "timestamp": "00:27:20,758", "timestamp_s": 1640.0}, {"text": "building and maintaining and supporting complex software systems that", "timestamp": "00:27:24,852", "timestamp_s": 1644.0}, {"text": "actually are pretty transferable to llms and maybe to", "timestamp": "00:27:29,172", "timestamp_s": 1649.0}, {"text": "greater effect for everything that we\u0027ve talked about here, where again with", "timestamp": "00:27:32,692", "timestamp_s": 1652.0}, {"text": "the unpredictability of llms, test driven development was all about", "timestamp": "00:27:36,404", "timestamp_s": 1656.0}, {"text": "the practice of helping software engineers build the habit of", "timestamp": "00:27:40,296", "timestamp_s": 1660.0}, {"text": "checking our mental models while we wrote code. Observability is", "timestamp": "00:27:43,368", "timestamp_s": 1663.0}, {"text": "all about the practice of helping software engineers and sres or", "timestamp": "00:27:47,432", "timestamp_s": 1667.0}, {"text": "DevOps teams have a backstop to and sanity check for our mental", "timestamp": "00:27:50,588", "timestamp_s": 1670.0}, {"text": "models when we ship code and this ability to", "timestamp": "00:27:54,658", "timestamp_s": 1674.0}, {"text": "sanity check is just so necessary for llms,", "timestamp": "00:27:58,268", "timestamp_s": 1678.0}, {"text": "where our mental models are never going to be accurate enough to rely on entirely.", "timestamp": "00:28:01,282", "timestamp_s": 1681.0}, {"text": "This is a truth I couldn\u0027t help but put in here.", "timestamp": "00:28:07,790", "timestamp_s": 1687.0}, {"text": "That has always been true that software", "timestamp": "00:28:11,230", "timestamp_s": 1691.0}, {"text": "behaves in unpredictable and emergent ways, especially as you put it out", "timestamp": "00:28:14,822", "timestamp_s": 1694.0}, {"text": "there in front of users that aren\u0027t you. But it\u0027s never", "timestamp": "00:28:18,164", "timestamp_s": 1698.0}, {"text": "been more true than with llms that the most important part", "timestamp": "00:28:21,716", "timestamp_s": 1701.0}, {"text": "is seeing and tracking and leveraging about how your user", "timestamp": "00:28:25,652", "timestamp_s": 1705.0}, {"text": "SRE using it as it\u0027s running in production in order", "timestamp": "00:28:28,734", "timestamp_s": 1708.0}, {"text": "to make it better incrementally.", "timestamp": "00:28:32,152", "timestamp_s": 1712.0}, {"text": "Now, before we wrap, I want to highlight one very specific example", "timestamp": "00:28:35,590", "timestamp_s": 1715.0}, {"text": "of a concept popularized through the rise of SRE,", "timestamp": "00:28:39,340", "timestamp_s": 1719.0}, {"text": "most commonly associated with ensuring consistent performance", "timestamp": "00:28:43,074", "timestamp_s": 1723.0}, {"text": "of production systems service level objectives are slos.", "timestamp": "00:28:46,546", "timestamp_s": 1726.0}, {"text": "Given the audience and this conference, I will assume that most of you are familiar", "timestamp": "00:28:51,050", "timestamp_s": 1731.0}, {"text": "with what they are. But in the hopes that this talk is shareable with a", "timestamp": "00:28:54,434", "timestamp_s": 1734.0}, {"text": "wider audience, I\u0027m going to do a little bit of background.", "timestamp": "00:28:57,424", "timestamp_s": 1737.0}, {"text": "Slos, I think are frankly really good for", "timestamp": "00:29:01,390", "timestamp_s": 1741.0}, {"text": "forcing product and service owners to align on a definition of what it means", "timestamp": "00:29:04,692", "timestamp_s": 1744.0}, {"text": "to provide great service to users.", "timestamp": "00:29:08,676", "timestamp_s": 1748.0}, {"text": "And it\u0027s intentionally thinking about from", "timestamp": "00:29:12,290", "timestamp_s": 1752.0}, {"text": "the client or user perspective rather than, oh,", "timestamp": "00:29:15,812", "timestamp_s": 1755.0}, {"text": "cpu or latency or things that we are used to when we think from the", "timestamp": "00:29:19,268", "timestamp_s": 1759.0}, {"text": "systems perspective. Often slos are used as a way to set a baseline", "timestamp": "00:29:22,664", "timestamp_s": 1762.0}, {"text": "and measure degradation over time of a key product workflow. You hear", "timestamp": "00:29:27,590", "timestamp_s": 1767.0}, {"text": "them associated a lot with uptime or performance or SRE metrics,", "timestamp": "00:29:31,772", "timestamp_s": 1771.0}, {"text": "and being alerted and going and acting", "timestamp": "00:29:36,170", "timestamp_s": 1776.0}, {"text": "if slos burn through", "timestamp": "00:29:40,482", "timestamp_s": 1780.0}, {"text": "an error budget. But you remember this slide when", "timestamp": "00:29:44,128", "timestamp_s": 1784.0}, {"text": "the LLM landscape is moving this quickly and best practices", "timestamp": "00:29:47,728", "timestamp_s": 1787.0}, {"text": "are still emerging, that degradation is guaranteed.", "timestamp": "00:29:51,222", "timestamp_s": 1791.0}, {"text": "You will break one thing when you think you\u0027re fixing another,", "timestamp": "00:29:55,070", "timestamp_s": 1795.0}, {"text": "and having slos over the top of your product,", "timestamp": "00:29:58,244", "timestamp_s": 1798.0}, {"text": "measuring that user experience are especially well", "timestamp": "00:30:01,410", "timestamp_s": 1801.0}, {"text": "suited to helping with this. And so what our team did", "timestamp": "00:30:04,868", "timestamp_s": 1804.0}, {"text": "after these six weeks, from like first line of code to having fully", "timestamp": "00:30:08,744", "timestamp_s": 1808.0}, {"text": "featured out the door, the team chose to uses slos", "timestamp": "00:30:12,254", "timestamp_s": 1812.0}, {"text": "to set a baseline at release and then track how their", "timestamp": "00:30:16,542", "timestamp_s": 1816.0}, {"text": "incremental work would move the needle. They expected this to go up over time because", "timestamp": "00:30:20,488", "timestamp_s": 1820.0}, {"text": "they were actively working on it, and they initially set this SLO", "timestamp": "00:30:23,564", "timestamp_s": 1823.0}, {"text": "to track the proportion of requests that complete without an error,", "timestamp": "00:30:27,714", "timestamp_s": 1827.0}, {"text": "because again, early days we weren\u0027t sure what the", "timestamp": "00:30:31,250", "timestamp_s": 1831.0}, {"text": "LLM API would accept from us and what uses would put in.", "timestamp": "00:30:35,008", "timestamp_s": 1835.0}, {"text": "And unlike most slos,", "timestamp": "00:30:38,830", "timestamp_s": 1838.0}, {"text": "which usually have to include lots of nines to be considered good,", "timestamp": "00:30:41,542", "timestamp_s": 1841.0}, {"text": "the team set their initial baseline at 75%.", "timestamp": "00:30:45,300", "timestamp_s": 1845.0}, {"text": "This is released as an experimental feature after all,", "timestamp": "00:30:49,010", "timestamp_s": 1849.0}, {"text": "and they aimed to iterate upwards. Today we\u0027re closer", "timestamp": "00:30:52,370", "timestamp_s": 1852.0}, {"text": "to 95% compliance.", "timestamp": "00:30:55,674", "timestamp_s": 1855.0}, {"text": "This little inset here on the bottom right is", "timestamp": "00:30:59,190", "timestamp_s": 1859.0}, {"text": "an example of what you can do with slos once", "timestamp": "00:31:03,016", "timestamp_s": 1863.0}, {"text": "you start measuring them, once you are able to cleanly separate out.", "timestamp": "00:31:06,696", "timestamp_s": 1866.0}, {"text": "These are requests that did not complete successfully versus the ones that did.", "timestamp": "00:31:10,888", "timestamp_s": 1870.0}, {"text": "You can go in and take all of this rich metadata", "timestamp": "00:31:15,052", "timestamp_s": 1875.0}, {"text": "you\u0027ve captured along the way and find outliers and then prioritize", "timestamp": "00:31:18,994", "timestamp_s": 1878.0}, {"text": "what work has the highest impact on. Yours is having a great experience.", "timestamp": "00:31:24,330", "timestamp_s": 1884.0}, {"text": "This sort of telemetry and analysis over time.", "timestamp": "00:31:28,350", "timestamp_s": 1888.0}, {"text": "This is a seven day view. There\u0027s 30 day views. Whatever your tool", "timestamp": "00:31:31,984", "timestamp_s": 1891.0}, {"text": "will have different time windows. But being able to", "timestamp": "00:31:35,252", "timestamp_s": 1895.0}, {"text": "track this historical compliance is what allows the team to iterate", "timestamp": "00:31:38,948", "timestamp_s": 1898.0}, {"text": "fast and confidently. Remember, the core", "timestamp": "00:31:42,730", "timestamp_s": 1902.0}, {"text": "of this is that llms are unpredictable and hard to model through traditional testing approaches.", "timestamp": "00:31:46,234", "timestamp_s": 1906.0}, {"text": "And so the team here chose to measure from the outside in", "timestamp": "00:31:50,878", "timestamp_s": 1910.0}, {"text": "to start with the measurements that mattered, users being", "timestamp": "00:31:55,430", "timestamp_s": 1915.0}, {"text": "able to use the feature period and have a good experience,", "timestamp": "00:31:58,808", "timestamp_s": 1918.0}, {"text": "and then debug as necessary and", "timestamp": "00:32:02,490", "timestamp_s": 1922.0}, {"text": "improve iteratively. I\u0027ll leave you with two other stories.", "timestamp": "00:32:05,852", "timestamp_s": 1925.0}, {"text": "So you believe that it\u0027s not just us. As we were building our feature,", "timestamp": "00:32:09,602", "timestamp_s": 1929.0}, {"text": "we actually learned that two of our customers were using honeycomb", "timestamp": "00:32:13,062", "timestamp_s": 1933.0}, {"text": "for a very similar thing.", "timestamp": "00:32:17,238", "timestamp_s": 1937.0}, {"text": "Duolingo language learning app care", "timestamp": "00:32:21,150", "timestamp_s": 1941.0}, {"text": "a lot about latency. With their LLMS features being", "timestamp": "00:32:25,476", "timestamp_s": 1945.0}, {"text": "heavily mobile, they really wanted to make sure that whatever they", "timestamp": "00:32:29,780", "timestamp_s": 1949.0}, {"text": "introduced felt fast. And so", "timestamp": "00:32:33,092", "timestamp_s": 1953.0}, {"text": "they captured all this. Metadata only shown", "timestamp": "00:32:36,520", "timestamp_s": 1956.0}, {"text": "two examples, and they wanted", "timestamp": "00:32:40,622", "timestamp_s": 1960.0}, {"text": "to really closely measure what would", "timestamp": "00:32:44,392", "timestamp_s": 1964.0}, {"text": "impact the llms being slos and the overall user experience", "timestamp": "00:32:48,616", "timestamp_s": 1968.0}, {"text": "being slow. And what they found, actually,", "timestamp": "00:32:52,220", "timestamp_s": 1972.0}, {"text": "the total latency was influenced way more by the things that they controlled", "timestamp": "00:32:55,884", "timestamp_s": 1975.0}, {"text": "in that long trace, that building up that prompt and then capturing additional", "timestamp": "00:33:00,410", "timestamp_s": 1980.0}, {"text": "context. That was where the bulk of the time was being spent,", "timestamp": "00:33:04,646", "timestamp_s": 1984.0}, {"text": "not the LLM call itself. And so again,", "timestamp": "00:33:08,198", "timestamp_s": 1988.0}, {"text": "their unpredictability happened in a different way. But in using", "timestamp": "00:33:12,030", "timestamp_s": 1992.0}, {"text": "these new technologies, you won\u0027t know where the potholes will", "timestamp": "00:33:15,732", "timestamp_s": 1995.0}, {"text": "be. And they were able to be confident", "timestamp": "00:33:19,748", "timestamp_s": 1999.0}, {"text": "by capturing this rich data, by capturing telemetry", "timestamp": "00:33:23,738", "timestamp_s": 2003.0}, {"text": "from the user\u0027s perspective that, okay, this is where we need to focus to", "timestamp": "00:33:28,078", "timestamp_s": 2008.0}, {"text": "make the whole feature fast.", "timestamp": "00:33:31,864", "timestamp_s": 2011.0}, {"text": "Second story I\u0027ll have for you is intercom.", "timestamp": "00:33:36,950", "timestamp_s": 2016.0}, {"text": "Intercom is a sort of a messaging application for", "timestamp": "00:33:41,370", "timestamp_s": 2021.0}, {"text": "businesses to message with their users.", "timestamp": "00:33:45,692", "timestamp_s": 2025.0}, {"text": "And they were rapidly iterating on", "timestamp": "00:33:48,570", "timestamp_s": 2028.0}, {"text": "a few different approaches to their LLM backed chatbot,", "timestamp": "00:33:52,460", "timestamp_s": 2032.0}, {"text": "I believe. And they really wanted to keep tabs on the user experience,", "timestamp": "00:33:56,838", "timestamp_s": 2036.0}, {"text": "even though there was all this change to the plumbing using on underneath.", "timestamp": "00:34:01,150", "timestamp_s": 2041.0}, {"text": "And so they tracked tons of", "timestamp": "00:34:05,466", "timestamp_s": 2045.0}, {"text": "pieces of metadata for each user interaction.", "timestamp": "00:34:09,172", "timestamp_s": 2049.0}, {"text": "They captured what was happening in the application, they captured all these different", "timestamp": "00:34:13,330", "timestamp_s": 2053.0}, {"text": "timings, time to first token, time to first usable token, how long it took", "timestamp": "00:34:17,412", "timestamp_s": 2057.0}, {"text": "to get to the end user, how long the overall latency was, everything.", "timestamp": "00:34:20,808", "timestamp_s": 2060.0}, {"text": "Then they tracked everything that they were changing along the way", "timestamp": "00:34:24,230", "timestamp_s": 2064.0}, {"text": "version of the algorithm, which model they were using, the type", "timestamp": "00:34:27,880", "timestamp_s": 2067.0}, {"text": "of metadata they were getting back. And critically, this was traced", "timestamp": "00:34:31,308", "timestamp_s": 2071.0}, {"text": "with everything else happening inside their application. They needed the", "timestamp": "00:34:35,362", "timestamp_s": 2075.0}, {"text": "full picture of the user experience to be confident in", "timestamp": "00:34:39,228", "timestamp_s": 2079.0}, {"text": "understanding that they pull one lever over here,", "timestamp": "00:34:43,052", "timestamp_s": 2083.0}, {"text": "they see the result over here, and they recognize that", "timestamp": "00:34:47,070", "timestamp_s": 2087.0}, {"text": "using an LLM is just one piece of understanding this user experience", "timestamp": "00:34:50,624", "timestamp_s": 2090.0}, {"text": "through telemetry of your application, not something to be siloed", "timestamp": "00:34:54,900", "timestamp_s": 2094.0}, {"text": "over there with an ML team or something else.", "timestamp": "00:34:58,618", "timestamp_s": 2098.0}, {"text": "So in the end, LLMs break many of", "timestamp": "00:35:02,050", "timestamp_s": 2102.0}, {"text": "our existing tools and techniques that we use to rely on", "timestamp": "00:35:06,132", "timestamp_s": 2106.0}, {"text": "ensuring correctness and a good user experience.", "timestamp": "00:35:09,496", "timestamp_s": 2109.0}, {"text": "Observability can help. Think about the problem from the outside in.", "timestamp": "00:35:13,350", "timestamp_s": 2113.0}, {"text": "Capture all the metadata so that you have that paper trail to debug and figure", "timestamp": "00:35:17,830", "timestamp_s": 2117.0}, {"text": "out what was going on with this weird LLM box", "timestamp": "00:35:21,832", "timestamp_s": 2121.0}, {"text": "and embrace the unpredictability.", "timestamp": "00:35:25,590", "timestamp_s": 2125.0}, {"text": "Get out to production quickly, get in front of user yours and plan", "timestamp": "00:35:28,710", "timestamp_s": 2128.0}, {"text": "to iterate fast. Plan to be reactive and embrace", "timestamp": "00:35:32,112", "timestamp_s": 2132.0}, {"text": "that as a good thing instead of a stressful piece instead.", "timestamp": "00:35:35,638", "timestamp_s": 2135.0}, {"text": "Thanks for your attention so far. If you want to learn", "timestamp": "00:35:39,070", "timestamp_s": 2139.0}, {"text": "more about this, we\u0027ve got a bunch of blog posts that go into much greater", "timestamp": "00:35:42,512", "timestamp_s": 2142.0}, {"text": "detail than I was able to in the time we had together.", "timestamp": "00:35:45,894", "timestamp_s": 2145.0}, {"text": "But thanks for your time. Enjoy the rest of the conference.", "timestamp": "00:35:49,150", "timestamp_s": 2149.0}, {"text": "Bye.", "timestamp": "00:35:53,790", "timestamp_s": 2153.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'bAdIoLpL2Ek',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Leveraging SRE and Observability Techniques for the Wild World of Building on LLMs
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>Building on LLMs is magicalâ€”but maintaining LLM-backed code is tricky: how do you ensure perf/correctness for something probabilistic?</p>
<p>It&rsquo;s like building on any black box (eg APIs/DBs). We&rsquo;ll cover instrumentation techniques, uses for observability best practices, and even using SLOs early in dev.</p>
<!-- End Text -->
          </div>

          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
              <li>
                The techniques that I'll describe today should be transferable to your tool of about this will draw from our experience of building on llms, but should apply to whatever LLM and observability stack you're using today. There's suddenly a lot more demand for AI functionality than there are people who carry expertise for it.

              </li>
              
              <li>
                RaE Rag is a practice of pulling in additional context within your domain to help your llms return better results. Even product development or release practices are turned a little bit inside out. Observability is a way of comparing what you expect in your head versus the actual behavior, but in live data.

              </li>
              
              <li>
                Earlier this year we released our query assistant in May 2023. Took about six weeks of development super fast, and we spent another eight weeks iterating on it. Six months later, it's so much more common for us to meet someone playing around with llms than someone whose product has actual LLM functionality deployed in production.

              </li>
              
              <li>
                The concept most commonly associated with ensuring consistent performance of production systems service level objectives are slos. Often used as a way to set a baseline and measure degradation over time of a key product workflow. Being able to track historical compliance is what allows the team to iterate fast and confidently.
              </li>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/assemblyai/bAdIoLpL2Ek.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:02:08,710'); seek(128.0)">
              Thanks for joining me today. I'm Christine
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:02:12,598'); seek(132.0)">
              and I'm going to start with a disclaimer. Honeycomb is
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:02:15,888'); seek(135.0)">
              an observability tool, but the techniques that I'll describe today should
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:02:19,728'); seek(139.0)">
              be transferable to your tool of about this
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:02:23,872'); seek(143.0)">
              will draw from our experience of building on llms,
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:02:27,942'); seek(147.0)">
              but should apply to whatever LLM and observability stack
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:02:32,358'); seek(152.0)">
              you're using today. All right,
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:02:36,048'); seek(156.0)">
              it software in 2023 feels more like
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:02:39,752'); seek(159.0)">
              magic than it ever has before. There are llms everywhere
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:02:43,534'); seek(163.0)">
              with a cheap API call to your provider of choice.
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:02:47,454'); seek(167.0)">
              It feels like every CEO or even CEO
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:02:51,810'); seek(171.0)">
              is now turning to their teams and asking how llms can be incorporated into
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:02:55,788'); seek(175.0)">
              their core product. Many are reaching to define an AI strategy,
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:03:00,570'); seek(180.0)">
              and there's lots to be excited about here. It's cool to be squarely in
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:03:04,144'); seek(184.0)">
              the middle of a phase change in progress where everything is new to
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:03:07,792'); seek(187.0)">
              everyone altogether. But there's also a reality check to trying
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:03:11,952'); seek(191.0)">
              to suddenly incorporate all this new technology into our products.
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:03:15,764'); seek(195.0)">
              There's suddenly a lot more demand for AI functionality than
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:03:19,428'); seek(199.0)">
              there are people who carry expertise for it and software engineering teams
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:03:23,994'); seek(203.0)">
              everywhere sre often just diving in to figure it out
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:03:27,876'); seek(207.0)">
              because we're the only ones left. Which to be clear,
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:03:30,952'); seek(210.0)">
              is just fine by me. As someone who used to identify as a generalist software
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:03:34,158'); seek(214.0)">
              engineer, the fewer silos we can build in this industry,
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:03:37,726'); seek(217.0)">
              the better. Because on one know,
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:03:41,404'); seek(221.0)">
              using a large language model through an API is like any other
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:03:45,052'); seek(225.0)">
              black box you interact with via API. Lots of
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:03:48,428'); seek(228.0)">
              consistent expectations we can set about how we make sense of these
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:03:51,888'); seek(231.0)">
              APIs, how we send parameters
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:03:55,062'); seek(235.0)">
              to the API,
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:03:58,110'); seek(238.0)">
              what types and scopes those inputs will
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:04:02,032'); seek(242.0)">
              be, what we'll get back from those APIs,
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:04:06,130'); seek(246.0)">
              and it's usually done over a standard protocol. And so all of these
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:04:09,748'); seek(249.0)">
              properties make working with APIs,
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:04:13,490'); seek(253.0)">
              these black boxes of logic,
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:04:16,314'); seek(256.0)">
              into something that is testable and mockable,
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:04:20,550'); seek(260.0)">
              a pretty reliable component in our system. But there's one key difference
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:04:24,872'); seek(264.0)">
              between having your application behavior relied on an LLM versus,
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:04:29,570'); seek(269.0)">
              say, a payments provider. That difference is how predictable
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:04:33,810'); seek(273.0)">
              the behavior of that black box is, which then in turn influences
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:04:38,146'); seek(278.0)">
              how testable or how mockable it is. And that
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:04:41,388'); seek(281.0)">
              difference ends up breaking apart all the different techniques
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:04:45,014'); seek(285.0)">
              that we've built up along the years for making sense of these complex systems.
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:04:49,310'); seek(289.0)">
              With normal APIs, you can write unit best.
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:04:54,290'); seek(294.0)">
              With an API you can conceivably scope or
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:04:57,492'); seek(297.0)">
              predict the full range of inputs in a useful way. On the LLMs
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:05:01,642'); seek(301.0)">
              side, you're not working
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:05:05,556'); seek(305.0)">
              with just the full range of negative and positive numbers.
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:05:09,624'); seek(309.0)">
              You've got a long tail of literally what we're soliciting
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:05:13,550'); seek(313.0)">
              is free form natural language input from users.
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:05:17,590'); seek(317.0)">
              We're not going to be able to have a reasonable test suite that we
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:05:21,228'); seek(321.0)">
              can run reliably for reproducibility
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:05:26,730'); seek(326.0)">
              APIs. Again, especially if it's
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:05:30,114'); seek(330.0)">
              a software, as a service, it's something very consistent. You have
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:05:34,044'); seek(334.0)">
              a payments service, typically when you say debit $5
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:05:38,032'); seek(338.0)">
              from my bank account, the balance goes down by $5. It's predictable.
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:05:43,070'); seek(343.0)">
              Ideally it's item potent, where if you're doing the same
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:05:46,276'); seek(346.0)">
              transaction, bank account aside,
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:05:49,730'); seek(349.0)">
              there's no additional strange side effects on
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:05:53,428'); seek(353.0)">
              the LLM side. The way that many of these public
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:05:57,432'); seek(357.0)">
              APIs are set up, usage by the public is
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:06:01,112'); seek(361.0)">
              teaching the model itself additional behavior. And so
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:06:04,376'); seek(364.0)">
              you have these API level regressions that
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:06:08,328'); seek(368.0)">
              are happening, you can't control. And as
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:06:12,188'); seek(372.0)">
              software engineers using that LLM, you need to adapt your prompts.
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:06:16,098'); seek(376.0)">
              So again, not mockable, not reproducible.
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:06:19,610'); seek(379.0)">
              And again, with a normal API, you can kind of reason
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:06:23,264'); seek(383.0)">
              what it's supposed to be doing and whether the problem is on the API's
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:06:27,062'); seek(387.0)">
              side or your application logic side, because there's
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:06:30,854'); seek(390.0)">
              a spec, because it's explainable and you're
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:06:34,438'); seek(394.0)">
              able to fit it in your head. On the LLM side,
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:06:37,940'); seek(397.0)">
              it's really hard to make sense of some of these changes programmatically, because llms
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:06:42,282'); seek(402.0)">
              are meant to almost simulate human behaviors.
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:06:45,594'); seek(405.0)">
              It's kind of the point. And so a thing that we can see is
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:06:48,872'); seek(408.0)">
              that very small changes to the prompt can yield very dramatic
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:06:52,926'); seek(412.0)">
              changes to the results in ways that, again, make it hard
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:06:57,590'); seek(417.0)">
              for humans to explain and debug,
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:07:01,362'); seek(421.0)">
              and sort of build a mental model of how it's supposed to behave.
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:07:04,890'); seek(424.0)">
              Now, these three techniques
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:07:08,866'); seek(428.0)">
              on the left are ways that we have traditionally
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:07:12,694'); seek(432.0)">
              tried to ensure correctness of our software.
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:07:16,750'); seek(436.0)">
              And if you ask an ML team, the right
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:07:20,576'); seek(440.0)">
              way to ensure correctness of something like
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:07:24,624'); seek(444.0)">
              an LLM feature is to build an evaluation system to
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:07:28,532'); seek(448.0)">
              evaluate the effectiveness of the model or the prompt.
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:07:32,370'); seek(452.0)">
              But most of us trying to make sense of llms aren't ML engineers.
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:07:36,610'); seek(456.0)">
              And the promise of llms exposed via APIs is that we shouldn't
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:07:39,822'); seek(459.0)">
              have to be to fold these new capabilities into our software.
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:07:44,150'); seek(464.0)">
              There's even one more layer of unpredictability
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:07:47,870'); seek(467.0)">
              that llms introduce. There's a concept
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:07:51,378'); seek(471.0)">
              of, I don't know how familiar everyone is with this piece,
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:07:54,556'); seek(474.0)">
              but there's an acronym that is used in this world,
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:07:58,204'); seek(478.0)">
              rag rags or retrieval augmented generation.
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:08:01,746'); seek(481.0)">
              Effectively, it's a practice of pulling in
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:08:05,360'); seek(485.0)">
              additional context within your domain to
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:08:08,912'); seek(488.0)">
              help your llms return better results.
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:08:12,270'); seek(492.0)">
              If you think about using Chat GPT prompt, it's where you
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:08:16,212'); seek(496.0)">
              say, oh, do this but in this style, or do this
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:08:19,540'); seek(499.0)">
              but in
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:08:22,868'); seek(502.0)">
              a certain voice. All that extra context
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:08:26,266'); seek(506.0)">
              helps make sure the LLM returns the result that you're looking for.
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:08:30,550'); seek(510.0)">
              But it is because of the way that these
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:08:35,096'); seek(515.0)">
              RaE Rag pipelines end up being built.
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:08:38,712'); seek(518.0)">
              Really, it means that your app is pulling in even more dynamic
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:08:42,178'); seek(522.0)">
              content and context that can
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:08:45,612'); seek(525.0)">
              again create and result in big changes in how the LLM
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:08:49,778'); seek(529.0)">
              is built, how the LLM is responding, and so
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:08:52,928'); seek(532.0)">
              you have even more unpredictability in trying
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:08:56,288'); seek(536.0)">
              to figure out why is my user
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:08:59,702'); seek(539.0)">
              not having the experience that I want them to have?
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:09:04,030'); seek(544.0)">
              So this turning upside down of our worldview is happening
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:09:07,828'); seek(547.0)">
              on a literal software engineering and systems engineering level.
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:09:11,730'); seek(551.0)">
              We know these black boxes aren't testable or debuggable in a traditional sense,
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:09:16,004'); seek(556.0)">
              so there's no solid sense of correct behavior
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:09:19,262'); seek(559.0)">
              that we can fall back to. It's also true from
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:09:22,568'); seek(562.0)">
              a meta level where there's no environment within which we can conduct our
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:09:26,328'); seek(566.0)">
              tests and feel confident in the results.
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:09:31,610'); seek(571.0)">
              There's no creating a staging environment where we can be sure that the LLMs experience
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:09:35,868'); seek(575.0)">
              or feature that we're building behaves correctly or
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:09:39,292'); seek(579.0)">
              does what the user wants. Going even
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:09:43,392'); seek(583.0)">
              one step further, even product development
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:09:46,758'); seek(586.0)">
              or release practices are turned a little bit inside out.
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:09:50,592'); seek(590.0)">
              Instead of being able to start with early access and then
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:09:54,272'); seek(594.0)">
              putting your product through its paces and then feeling confident in a later or broader
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:09:57,818'); seek(597.0)">
              release, early access programs are inherently going to fail
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:10:01,882'); seek(601.0)">
              to capture that full range of user behavior and edge cases.
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:10:06,370'); seek(606.0)">
              All these programs do is delay the inevitable failures that you'll encounter
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:10:10,286'); seek(610.0)">
              when you have an uncontrolled and unprompted group of group of users doing
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:10:14,440'); seek(614.0)">
              things that you never expected them to do.
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:10:17,510'); seek(617.0)">
              So at this point,
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:10:21,260'); seek(621.0)">
              do we just give up on everything we've learned about building and operating software systems
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:10:25,850'); seek(625.0)">
              and embrace the rise of prompt engineer as an entirely
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:10:29,218'); seek(629.0)">
              separate skill set. Well, if you've been paying attention to the title of this talk,
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:10:33,440'); seek(633.0)">
              the answer is obviously not,
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:10:36,830'); seek(636.0)">
              because we already have a model for how to measure and debug and move the
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:10:40,144'); seek(640.0)">
              needle on an unpredictable qualitative experience.
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:10:44,270'); seek(644.0)">
              Observability. And I'll say this term
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:10:48,038'); seek(648.0)">
              has become so commonplace today, it's fallen out of fashion to define
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:10:51,274'); seek(651.0)">
              it. But as someone who's been talking about all of this since before it was
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:10:54,932'); seek(654.0)">
              cool, humor me. I think it'll help some pieces click into place.
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:11:00,230'); seek(660.0)">
              This here is the formal Wikipedia definition
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:11:03,758'); seek(663.0)">
              of observability. It comes from control theory. It's about
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:11:07,496'); seek(667.0)">
              looking at a system based on the inputs and outputs and using
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:11:10,876'); seek(670.0)">
              that to model what this system is doing. Black box
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:11:15,170'); seek(675.0)">
              and it feels a little overly formal when talking about production systems.
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:11:19,314'); seek(679.0)">
              Software systems still applies to,
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:11:22,192'); seek(682.0)">
              but it feels like really applicable to a system like
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:11:26,048'); seek(686.0)">
              an LLM, like this thing that's changing over time because
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:11:30,672'); seek(690.0)">
              it can't be monitored or simulated with traditional techniques.
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:11:35,410'); seek(695.0)">
              Another way I like to think about this is that less formally,
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:11:39,114'); seek(699.0)">
              observability is a way of comparing what you expect in
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:11:43,428'); seek(703.0)">
              your head versus the actual behavior,
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:11:46,790'); seek(706.0)">
              but in live systems. And so let's
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:11:50,478'); seek(710.0)">
              take a look at what this means for a standard web app.
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:11:53,432'); seek(713.0)">
              Well, you're looking at this box has your application.
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:11:57,910'); seek(717.0)">
              Because it's our application, we actually get to instrument it and we can capture what
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:12:01,720'); seek(721.0)">
              arguments were sent to it. On any given HTTP request, we can
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:12:05,084'); seek(725.0)">
              capture some metadata about how the app was running and we can
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:12:08,348'); seek(728.0)">
              capture data about what was returned. This lets us reason about the behavior
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:12:12,242'); seek(732.0)">
              we can expect for a given user and endpoint
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:12:15,782'); seek(735.0)">
              and set of parameters. And it lets us debug and reproduce the issue
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:12:19,296'); seek(739.0)">
              if the actual behavior we see deviates from that expectation.
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:12:23,238'); seek(743.0)">
              Again, lots of parallels to best, but on live
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:12:27,088'); seek(747.0)">
              data. What about this payment service over here on the right?
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:12:30,964'); seek(750.0)">
              It's that black box that the app depends on. It's out of my control.
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:12:34,868'); seek(754.0)">
              Might be another company entirely. And even
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:12:38,052'); seek(758.0)">
              if I wanted to, because of that, I couldn't go and shove instrumentation inside
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:12:41,832'); seek(761.0)">
              of it. You can think of this like a database too, right? You're not going
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:12:44,664'); seek(764.0)">
              to go and fork Mysql and shove your own instrumentation in there.
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:12:48,472'); seek(768.0)">
              But I know what requests my app has sent to it.
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:12:51,832'); seek(771.0)">
              I know where those requests are coming from in the code
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:12:55,052'); seek(775.0)">
              and on behalf of which user. And then I know how long it took
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:12:58,348'); seek(778.0)">
              to respond from the app's perspective, whether it was successful,
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:13:01,698'); seek(781.0)">
              and probably some other metadata. By capturing all of
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:13:05,264'); seek(785.0)">
              that I can again start to reason, or at least have a paper trail,
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:13:09,190'); seek(789.0)">
              to understand how these inputs impact the outputs
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:13:12,598'); seek(792.0)">
              of my black box and then how the choices my application makes and the
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:13:16,468'); seek(796.0)">
              inputs into that application impacts all of that.
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:13:19,652'); seek(799.0)">
              And that approach becomes the same for llms, as unpredictable and
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:13:23,508'); seek(803.0)">
              nondeterministic as they are. We know how a user interacts
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:13:27,258'); seek(807.0)">
              with the app, we know how the app turns that into
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:13:30,616'); seek(810.0)">
              parameters for the black box, and we know
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:13:33,688'); seek(813.0)">
              how they respond. It's a blanket statement that in complex systems,
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:13:37,710'); seek(817.0)">
              software usage patterns will become unpredictable and change over time.
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:13:41,610'); seek(821.0)">
              With llms, that assertion becomes a guarantee.
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:13:44,818'); seek(824.0)">
              If you use llms, as many of us are,
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:13:48,890'); seek(828.0)">
              your data set is going to be unpredictable and will absolutely
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:13:52,620'); seek(832.0)">
              change over time. So the key to operating sanely
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:13:56,054'); seek(836.0)">
              on top of that magical foundation is having a way of
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:13:59,968'); seek(839.0)">
              gathering, aggregating and exploring that data in a way
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:14:03,392'); seek(843.0)">
              that captures what the user experienced as expressively
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:14:07,098'); seek(847.0)">
              as possible. That's what lets you build and reason
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:14:10,436'); seek(850.0)">
              and ensure a quality experience on top of llms, the ability to
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:14:13,988'); seek(853.0)">
              understand from the outside why your user got a certain
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:14:17,752'); seek(857.0)">
              response from your llmbacked application.
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:14:20,790'); seek(860.0)">
              Observability creates these feedback loops to let
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:14:23,992'); seek(863.0)">
              you learn from what's really happening with your code,
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:14:27,048'); seek(867.0)">
              the same way we've all learned how to work iteratively with tests.
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:14:31,906'); seek(871.0)">
              Observability enables us all to ship sooner,
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:14:34,890'); seek(874.0)">
              observe those results in the wild, and then wrap those observations back
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:14:38,508'); seek(878.0)">
              into the development process. With llms rapidly
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:14:41,778'); seek(881.0)">
              becoming some piece of every software
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:14:45,158'); seek(885.0)">
              system, we all get to learn some new skills.
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:14:48,214'); seek(888.0)">
              SRes who are used to thinking of APIs as black boxes that can be modeled
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:14:51,814'); seek(891.0)">
              and asserted on, now have to get used to drift and peeling
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:14:55,418'); seek(895.0)">
              back a layer to examine that emergent behavior.
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:14:58,690'); seek(898.0)">
              Software engineers who are used to boolean logic and discrete math
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:15:02,218'); seek(902.0)">
              and correctness and test driven development now
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:15:06,132'); seek(906.0)">
              have to think about data quality, probabilistic systems and
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:15:10,648'); seek(910.0)">
              representivity, or how well your model test
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:15:15,160'); seek(915.0)">
              environment or your staging environment,
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:15:18,110'); seek(918.0)">
              or your mental code represents the production system.
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:15:22,010'); seek(922.0)">
              And everyone in engineering needs to reorient themselves around
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:15:25,628'); seek(925.0)">
              what this LLM thing is
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:15:29,244'); seek(929.0)">
              trying to achieve, what the business goals are, what the product use cases are,
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:15:32,752'); seek(932.0)">
              what the ideal user experience is, instead of
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:15:36,432'); seek(936.0)">
              sterile concepts like correctness, reliability or availability.
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:15:39,950'); seek(939.0)">
              Those last three are still important. But ultimately,
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:15:43,286'); seek(943.0)">
              when you bring in this thing that is so free form that
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:15:47,812'); seek(947.0)">
              the human on the other end is going to have their own opinion of whether
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:15:51,620'); seek(951.0)">
              your LLM feature was useful or not, we all need
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:15:54,868'); seek(954.0)">
              to think expand our mental models of what it means to provide a great
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:15:58,708'); seek(958.0)">
              service to include that definition as well.
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:16:01,990'); seek(961.0)">
              So, okay, why am I up here talking about this and why should
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:16:05,144'); seek(965.0)">
              you believe me? I'm going to tell you a little bit about a
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:16:09,228'); seek(969.0)">
              feature that we released and our experience building it,
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:16:13,804'); seek(973.0)">
              trying to ensure that it would be a great experience, and maintaining it
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:16:17,468'); seek(977.0)">
              going forward. So earlier this year we
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:16:21,212'); seek(981.0)">
              released our query assistant in May 2023.
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:16:24,750'); seek(984.0)">
              Took about six weeks of development super fast,
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:16:28,750'); seek(988.0)">
              and we spent another eight weeks iterating on it.
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:16:31,808'); seek(991.0)">
              And to give you a little bit of an overview of what it was trying
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:16:34,928'); seek(994.0)">
              to do, Honeycomb as an observability tool
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:16:38,910'); seek(998.0)">
              lets our users work with a lot of data. Our product has a visual query
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:16:42,202'); seek(1002.0)">
              interface. We believe that point and click is always going to be easier for someone
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:16:45,652'); seek(1005.0)">
              to learn and play around with than an open text box. But even
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:16:49,188'); seek(1009.0)">
              so, there's a learning curve to the user's interface and we were really
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:16:52,472'); seek(1012.0)">
              excited about being able to use llms as a translation layer from
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:16:56,744'); seek(1016.0)">
              what the human is trying to do over here on the right of this slide
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:17:00,126'); seek(1020.0)">
              into the UI. And so we added this little experimental
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:17:05,122'); seek(1025.0)">
              piece to the query. Building collapsed most of the time, but people could expand
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:17:08,658'); seek(1028.0)">
              it and we let people type in in English
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:17:12,498'); seek(1032.0)">
              what they were hoping to SRE. And we also another thing that was
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:17:16,304'); seek(1036.0)">
              important to us is that we preserve the editability and explorability that's sort of
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:17:19,904'); seek(1039.0)">
              inherent in our product. The same way that we
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:17:24,208'); seek(1044.0)">
              all as consumers have gotten used to being able to edit or iterate on
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:17:27,524'); seek(1047.0)">
              our response with Chat GPT. We wanted users to be able
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:17:30,692'); seek(1050.0)">
              to get the output honeycomb would
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:17:34,068'); seek(1054.0)">
              building the query for them, but be able to tweak
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:17:37,438'); seek(1057.0)">
              and iterate on it. Because we wanted to encourage that iteration,
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:17:41,830'); seek(1061.0)">
              we realized that there would be no concrete and
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:17:45,512'); seek(1065.0)">
              quantitative result we could rely on that would cleanly
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:17:49,858'); seek(1069.0)">
              describe whether the feature itself was good. If users ran
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:17:53,218'); seek(1073.0)">
              more queries, maybe it was good, maybe we were just consistently
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:17:58,250'); seek(1078.0)">
              being not useful. Maybe fewer queries were good,
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:18:02,444'); seek(1082.0)">
              but maybe they just weren't using the product or they didn't understand what was going
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:18:06,112'); seek(1086.0)">
              on. So we knew we would need to capture this qualitative feedback,
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:18:10,118'); seek(1090.0)">
              the yes no, I'm not sure buttons, so that we
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:18:13,712'); seek(1093.0)">
              could understand from the user's perspective whether
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:18:17,796'); seek(1097.0)">
              this thing that we tried to sre them was actually helpful or not.
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:18:21,268'); seek(1101.0)">
              And then we could posit some higher level product goals, like product retention for
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:18:25,188'); seek(1105.0)">
              new uses, to layer on top of that as
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:18:29,172'); seek(1109.0)">
              a spoiler, we hit these goals. We were thrilled, but we did a lot of
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:18:32,264'); seek(1112.0)">
              stumbling around in the dark along the way.
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:18:35,270'); seek(1115.0)">
              And today, six months later, it's so much more common for
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:18:39,096'); seek(1119.0)">
              us to meet someone playing around with llms than someone
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:18:43,404'); seek(1123.0)">
              whose product has actual LLM functionality deployed
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:18:46,514'); seek(1126.0)">
              in production. And we think that a lot of this is rooted
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:18:50,498'); seek(1130.0)">
              in the fact that our teams have really embraced observability techniques
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:18:54,354'); seek(1134.0)">
              in how we ship software, period. And those were key to
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:18:57,872'); seek(1137.0)">
              building the confidence to ship this thing fast
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:19:01,840'); seek(1141.0)">
              and iterate live and really just understand that
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:19:05,428'); seek(1145.0)">
              we were going to have to react
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:19:08,970'); seek(1148.0)">
              based on how the broader user base
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:19:12,612'); seek(1152.0)">
              used the product.
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:19:16,850'); seek(1156.0)">
              These were some learnings that we had fairly early on.
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:19:20,120'); seek(1160.0)">
              There's a great blog post that this is excerpted from. You should
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:19:23,688'); seek(1163.0)">
              check it out if you're again in the phase of building on llms
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:19:27,430'); seek(1167.0)">
              but it's all about things are going to fall apart.
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:19:31,450'); seek(1171.0)">
              It's not a question of how to prevent failures from happening,
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:19:34,860'); seek(1174.0)">
              it's a question of can you detect it quickly
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:19:38,316'); seek(1178.0)">
              enough? Because you just can't predict what a user is
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:19:41,728'); seek(1181.0)">
              going to type into that freeform text box. You will ship
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:19:45,302'); seek(1185.0)">
              something that breaks something else, and it's okay.
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:19:48,640'); seek(1188.0)">
              And again, you can't predict. You can't rely
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:19:53,238'); seek(1193.0)">
              on your test frameworks, you can't rely on your CI pipelines.
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:19:56,850'); seek(1196.0)">
              So how do you react quickly enough? How do you capture the information
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:20:00,356'); seek(1200.0)">
              that you need in order to come in and debug and improve
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:20:04,922'); seek(1204.0)">
              going forward? So let's get a little bit,
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:20:08,260'); seek(1208.0)">
              go one level deeper. How do we go forward? Well, talked a lot about capturing
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:20:12,606'); seek(1212.0)">
              instrumentation, leaving this paper trail for how and why your
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:20:15,688'); seek(1215.0)">
              code behaves a certain way. I think of instrumentation,
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:20:19,522'); seek(1219.0)">
              frankly, like documentation and tests,
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:20:23,634'); seek(1223.0)">
              they sre all ways to try to get your code
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:20:27,740'); seek(1227.0)">
              to explain itself back to you. And instrumentation is
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:20:31,792'); seek(1231.0)">
              like capturing debug statements and breakpoints in your production code,
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:20:36,350'); seek(1236.0)">
              as much in the language of your application and
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:20:40,016'); seek(1240.0)">
              the unique business logic of your product and domain
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:20:43,254'); seek(1243.0)">
              as possible. In a normal software system, this can let you
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:20:46,964'); seek(1246.0)">
              do things as simple as figure out quickly which
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:20:50,612'); seek(1250.0)">
              individual user or account is associated with that unexpected behavior.
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:20:54,850'); seek(1254.0)">
              It can let you do things as complex as deploy a
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:20:58,308'); seek(1258.0)">
              few different implementations of a given np complete problem,
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:21:01,832'); seek(1261.0)">
              get it behind a given feature flag, compare the results of each approach,
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:21:05,678'); seek(1265.0)">
              and pick the implementation that behaves best on live data.
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:21:09,430'); seek(1269.0)">
              When you have rich data that you need to
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:21:14,170'); seek(1274.0)">
              tease apart all the different parameters that you're varying in your experiment,
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:21:18,418'); seek(1278.0)">
              you're able to then validate your hypothesis much more quickly and flexibly
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:21:22,754'); seek(1282.0)">
              along the way. And so in the LLM world, this is how
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:21:26,144'); seek(1286.0)">
              we applied those principles. You want to capture as much as you can about
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:21:29,808'); seek(1289.0)">
              what your users are doing in your system in a format that lets you view
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:21:33,552'); seek(1293.0)">
              overarching performance, and then also debug any
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:21:37,332'); seek(1297.0)">
              individual transaction. Over here on the right is actually a
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:21:40,884'); seek(1300.0)">
              screenshot of a real trace that we have for how we sre
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:21:45,570'); seek(1305.0)">
              building up a request to our
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:21:49,192'); seek(1309.0)">
              LLM provider. This goes from user click through
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:21:52,648'); seek(1312.0)">
              the dynamic prompt building to the actual LLM request
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:21:56,222'); seek(1316.0)">
              response parsing, response validation and the query execution in
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:21:59,688'); seek(1319.0)">
              our product. And having all of this
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:22:02,972'); seek(1322.0)">
              full trace and then lots of metadata on each of those individual spans
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:22:07,290'); seek(1327.0)">
              lets us ask high level questions about the end user
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:22:10,738'); seek(1330.0)">
              experience. Here you can see the results
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:22:13,794'); seek(1333.0)">
              of that yes, those yes no I'm not sure buttons in a way that
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:22:17,408'); seek(1337.0)">
              lets us quantitatively ask questions and tricky progress,
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:22:21,014'); seek(1341.0)">
              but always be able to get back to okay for
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:22:25,010'); seek(1345.0)">
              this one interaction where someone said no,
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:22:28,868'); seek(1348.0)">
              it didn't answer their question, what was their input?
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:22:32,698'); seek(1352.0)">
              What did we try to do? How could we build up that prompt? Better to
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:22:36,792'); seek(1356.0)">
              make sure that their intent gets passed to the LLM and reflected
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:22:40,558'); seek(1360.0)">
              in our product as effectively as possible.
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:22:44,390'); seek(1364.0)">
              Let us ask high level questions about things like
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:22:48,040'); seek(1368.0)">
              trends in the latency of actual LLM request and
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:22:51,528'); seek(1371.0)">
              response calls, and then let
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:22:55,148'); seek(1375.0)">
              us take those metrics and group them on really fine
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:22:58,636'); seek(1378.0)">
              grained characteristics of each request. And this lets us then
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:23:02,368'); seek(1382.0)">
              draw conclusions about how certain parameters for
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:23:05,728'); seek(1385.0)">
              a given team, for a given column data set,
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:23:08,976'); seek(1388.0)">
              whatever might impact the actual LLM operation.
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:23:12,830'); seek(1392.0)">
              Again, you can think of that was an e commerce site having
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:23:16,212'); seek(1396.0)">
              things like shopping cart id or number of items in the cart as
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:23:20,116'); seek(1400.0)">
              parameters here. But by capturing all of this related
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:23:24,122'); seek(1404.0)">
              to the LLM, I am now armed to deal with whoa,
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:23:28,682'); seek(1408.0)">
              something weird started happening with llms with our LLM response.
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:23:33,102'); seek(1413.0)">
              What changed? Why? What's different
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:23:36,440'); seek(1416.0)">
              about that one account that is having a dramatically different experience
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:23:39,752'); seek(1419.0)">
              than everyone else, and then what's intended?
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:23:45,530'); seek(1425.0)">
              We were also able to really closely capture and track errors,
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:23:48,946'); seek(1428.0)">
              but in a flexible, not everything marked an error is
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:23:52,396'); seek(1432.0)">
              necessarily an error kind of way. It's early. We don't know
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:23:55,728'); seek(1435.0)">
              which errors to take seriously and which ones don't. I think a
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:23:59,184'); seek(1439.0)">
              principle I go by is not every exception is exceptional.
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:24:03,222'); seek(1443.0)">
              Not everything exceptional is captured as an exception.
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:24:06,118'); seek(1446.0)">
              And so we wanted to capture things that were fairly open ended, that always let
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:24:09,188'); seek(1449.0)">
              us correlate back to, okay, well, what was the user actually trying to do?
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:24:13,092'); seek(1453.0)">
              What did they see? And we captured this all in
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:24:16,212'); seek(1456.0)">
              one trace. So we had the full context for what
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:24:19,592'); seek(1459.0)">
              went into a given response to a user. This blue
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:24:23,598'); seek(1463.0)">
              span I've highlighted at the bottom, it's tiny text,
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:24:26,856'); seek(1466.0)">
              but if you squint, you can see that this finally is
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:24:31,228'); seek(1471.0)">
              our call to OpenAI. All the spans above it
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:24:34,764'); seek(1474.0)">
              are work that we are doing inside the application to build the best prompt that
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:24:37,964'); seek(1477.0)">
              we can. Which also means there are that many
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:24:41,916'); seek(1481.0)">
              possible things that could go wrong that could result in a poor response
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:24:45,462'); seek(1485.0)">
              from OpenAI or whatever llms you're using.
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:24:48,670'); seek(1488.0)">
              And so as we were building this feature, and as we
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:24:52,064'); seek(1492.0)">
              knew we wanted to iterate, we'd need all this context if we had any
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:24:55,952'); seek(1495.0)">
              hope of figuring out why things were going to go wrong and
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:24:59,412'); seek(1499.0)">
              how to iterate towards a better future. Now,
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:25:02,548'); seek(1502.0)">
              a lot of these behaviors have been on the rise for a while,
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:25:06,132'); seek(1506.0)">
              may already be practiced by your team.
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:25:09,330'); seek(1509.0)">
              I think that's an awesome thing. As a baby software engineer,
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:25:13,102'); seek(1513.0)">
              I took a lot of pride in just shipping really fast, and I wrote
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:25:17,038'); seek(1517.0)">
              lots of tests along the way, of course, because I was an accepted and celebrated
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:25:20,606'); seek(1520.0)">
              part of shipping good code. But in the last decade or
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:25:24,092'); seek(1524.0)">
              so, we've seen a bit of a shift in the conversation.
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:25:27,690'); seek(1527.0)">
              Instead of just writing lots of code being a sign of a good developer,
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:25:31,570'); seek(1531.0)">
              there's phrases like service ownership, putting developers on call,
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:25:35,920'); seek(1535.0)">
              testing in production. And as these phrases have entered our
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:25:39,392'); seek(1539.0)">
              collective consciousness, it has shifted
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:25:43,810'); seek(1543.0)">
              the domain, I think, of a developer from
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:25:47,940'); seek(1547.0)">
              purely thinking about development to also thinking about production.
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:25:52,690'); seek(1552.0)">
              And I'm really excited about this because a lot of these, the shift that
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:25:56,548'); seek(1556.0)">
              is already kind of underway of taking what
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:25:59,988'); seek(1559.0)">
              we do in its TDD world and
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:26:04,616'); seek(1564.0)">
              recognizing they can apply to production as well through Ollie or observability.
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:26:09,430'); seek(1569.0)">
              We're just taking these behaviors that we know as developers
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:26:12,626'); seek(1572.0)">
              and applying it under a different name in development
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:26:16,482'); seek(1576.0)">
              or in the test environment. We're identifying the levers that impact
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:26:19,762'); seek(1579.0)">
              logical branches in the code for debug ability and reproducibility, and making
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:26:23,552'); seek(1583.0)">
              sure to exercise those in a test in observability.
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:26:27,310'); seek(1587.0)">
              You're instrumenting code with intention so that you can do the same in production.
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:26:31,470'); seek(1591.0)">
              When you're writing a test, you're thinking about what you
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:26:35,012'); seek(1595.0)">
              expect and you're asserting on what
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:26:38,484'); seek(1598.0)">
              you'll actually get with observability and looking
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:26:42,052'); seek(1602.0)">
              at your systems in production. You're just inspecting results after
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:26:45,908'); seek(1605.0)">
              the changes have been rolled out and you're watching for deviations when
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:26:49,768'); seek(1609.0)">
              you're writing tests, especially if you're practicing real TDD,
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:26:54,310'); seek(1614.0)">
              I know not everyone does. You're embracing these fast fail loops,
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:26:58,510'); seek(1618.0)">
              fast feedback loops. You are expecting
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:27:03,170'); seek(1623.0)">
              to act on the output of these feedback loops to make your code better.
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:27:07,050'); seek(1627.0)">
              And that's all observability is all about.
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:27:09,884'); seek(1629.0)">
              It's shipping to production quickly through your
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:27:13,312'); seek(1633.0)">
              CI CD pipeline or through feature flags, and then expecting
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:27:17,238'); seek(1637.0)">
              to iterate even on code that you think is shipped. And it's
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:27:20,758'); seek(1640.0)">
              exciting that these are guardrails that we've generalized for
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:27:24,852'); seek(1644.0)">
              building and maintaining and supporting complex software systems that
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:27:29,172'); seek(1649.0)">
              actually are pretty transferable to llms and maybe to
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:27:32,692'); seek(1652.0)">
              greater effect for everything that we've talked about here, where again with
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:27:36,404'); seek(1656.0)">
              the unpredictability of llms, test driven development was all about
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:27:40,296'); seek(1660.0)">
              the practice of helping software engineers build the habit of
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:27:43,368'); seek(1663.0)">
              checking our mental models while we wrote code. Observability is
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:27:47,432'); seek(1667.0)">
              all about the practice of helping software engineers and sres or
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:27:50,588'); seek(1670.0)">
              DevOps teams have a backstop to and sanity check for our mental
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:27:54,658'); seek(1674.0)">
              models when we ship code and this ability to
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:27:58,268'); seek(1678.0)">
              sanity check is just so necessary for llms,
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:28:01,282'); seek(1681.0)">
              where our mental models are never going to be accurate enough to rely on entirely.
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:28:07,790'); seek(1687.0)">
              This is a truth I couldn't help but put in here.
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:28:11,230'); seek(1691.0)">
              That has always been true that software
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:28:14,822'); seek(1694.0)">
              behaves in unpredictable and emergent ways, especially as you put it out
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:28:18,164'); seek(1698.0)">
              there in front of users that aren't you. But it's never
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:28:21,716'); seek(1701.0)">
              been more true than with llms that the most important part
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:28:25,652'); seek(1705.0)">
              is seeing and tracking and leveraging about how your user
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:28:28,734'); seek(1708.0)">
              SRE using it as it's running in production in order
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:28:32,152'); seek(1712.0)">
              to make it better incrementally.
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:28:35,590'); seek(1715.0)">
              Now, before we wrap, I want to highlight one very specific example
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:28:39,340'); seek(1719.0)">
              of a concept popularized through the rise of SRE,
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:28:43,074'); seek(1723.0)">
              most commonly associated with ensuring consistent performance
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:28:46,546'); seek(1726.0)">
              of production systems service level objectives are slos.
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:28:51,050'); seek(1731.0)">
              Given the audience and this conference, I will assume that most of you are familiar
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:28:54,434'); seek(1734.0)">
              with what they are. But in the hopes that this talk is shareable with a
            </span>
            
            <span id="chunk-429" class="transcript-chunks" onclick="console.log('00:28:57,424'); seek(1737.0)">
              wider audience, I'm going to do a little bit of background.
            </span>
            
            <span id="chunk-430" class="transcript-chunks" onclick="console.log('00:29:01,390'); seek(1741.0)">
              Slos, I think are frankly really good for
            </span>
            
            <span id="chunk-431" class="transcript-chunks" onclick="console.log('00:29:04,692'); seek(1744.0)">
              forcing product and service owners to align on a definition of what it means
            </span>
            
            <span id="chunk-432" class="transcript-chunks" onclick="console.log('00:29:08,676'); seek(1748.0)">
              to provide great service to users.
            </span>
            
            <span id="chunk-433" class="transcript-chunks" onclick="console.log('00:29:12,290'); seek(1752.0)">
              And it's intentionally thinking about from
            </span>
            
            <span id="chunk-434" class="transcript-chunks" onclick="console.log('00:29:15,812'); seek(1755.0)">
              the client or user perspective rather than, oh,
            </span>
            
            <span id="chunk-435" class="transcript-chunks" onclick="console.log('00:29:19,268'); seek(1759.0)">
              cpu or latency or things that we are used to when we think from the
            </span>
            
            <span id="chunk-436" class="transcript-chunks" onclick="console.log('00:29:22,664'); seek(1762.0)">
              systems perspective. Often slos are used as a way to set a baseline
            </span>
            
            <span id="chunk-437" class="transcript-chunks" onclick="console.log('00:29:27,590'); seek(1767.0)">
              and measure degradation over time of a key product workflow. You hear
            </span>
            
            <span id="chunk-438" class="transcript-chunks" onclick="console.log('00:29:31,772'); seek(1771.0)">
              them associated a lot with uptime or performance or SRE metrics,
            </span>
            
            <span id="chunk-439" class="transcript-chunks" onclick="console.log('00:29:36,170'); seek(1776.0)">
              and being alerted and going and acting
            </span>
            
            <span id="chunk-440" class="transcript-chunks" onclick="console.log('00:29:40,482'); seek(1780.0)">
              if slos burn through
            </span>
            
            <span id="chunk-441" class="transcript-chunks" onclick="console.log('00:29:44,128'); seek(1784.0)">
              an error budget. But you remember this slide when
            </span>
            
            <span id="chunk-442" class="transcript-chunks" onclick="console.log('00:29:47,728'); seek(1787.0)">
              the LLM landscape is moving this quickly and best practices
            </span>
            
            <span id="chunk-443" class="transcript-chunks" onclick="console.log('00:29:51,222'); seek(1791.0)">
              are still emerging, that degradation is guaranteed.
            </span>
            
            <span id="chunk-444" class="transcript-chunks" onclick="console.log('00:29:55,070'); seek(1795.0)">
              You will break one thing when you think you're fixing another,
            </span>
            
            <span id="chunk-445" class="transcript-chunks" onclick="console.log('00:29:58,244'); seek(1798.0)">
              and having slos over the top of your product,
            </span>
            
            <span id="chunk-446" class="transcript-chunks" onclick="console.log('00:30:01,410'); seek(1801.0)">
              measuring that user experience are especially well
            </span>
            
            <span id="chunk-447" class="transcript-chunks" onclick="console.log('00:30:04,868'); seek(1804.0)">
              suited to helping with this. And so what our team did
            </span>
            
            <span id="chunk-448" class="transcript-chunks" onclick="console.log('00:30:08,744'); seek(1808.0)">
              after these six weeks, from like first line of code to having fully
            </span>
            
            <span id="chunk-449" class="transcript-chunks" onclick="console.log('00:30:12,254'); seek(1812.0)">
              featured out the door, the team chose to uses slos
            </span>
            
            <span id="chunk-450" class="transcript-chunks" onclick="console.log('00:30:16,542'); seek(1816.0)">
              to set a baseline at release and then track how their
            </span>
            
            <span id="chunk-451" class="transcript-chunks" onclick="console.log('00:30:20,488'); seek(1820.0)">
              incremental work would move the needle. They expected this to go up over time because
            </span>
            
            <span id="chunk-452" class="transcript-chunks" onclick="console.log('00:30:23,564'); seek(1823.0)">
              they were actively working on it, and they initially set this SLO
            </span>
            
            <span id="chunk-453" class="transcript-chunks" onclick="console.log('00:30:27,714'); seek(1827.0)">
              to track the proportion of requests that complete without an error,
            </span>
            
            <span id="chunk-454" class="transcript-chunks" onclick="console.log('00:30:31,250'); seek(1831.0)">
              because again, early days we weren't sure what the
            </span>
            
            <span id="chunk-455" class="transcript-chunks" onclick="console.log('00:30:35,008'); seek(1835.0)">
              LLM API would accept from us and what uses would put in.
            </span>
            
            <span id="chunk-456" class="transcript-chunks" onclick="console.log('00:30:38,830'); seek(1838.0)">
              And unlike most slos,
            </span>
            
            <span id="chunk-457" class="transcript-chunks" onclick="console.log('00:30:41,542'); seek(1841.0)">
              which usually have to include lots of nines to be considered good,
            </span>
            
            <span id="chunk-458" class="transcript-chunks" onclick="console.log('00:30:45,300'); seek(1845.0)">
              the team set their initial baseline at 75%.
            </span>
            
            <span id="chunk-459" class="transcript-chunks" onclick="console.log('00:30:49,010'); seek(1849.0)">
              This is released as an experimental feature after all,
            </span>
            
            <span id="chunk-460" class="transcript-chunks" onclick="console.log('00:30:52,370'); seek(1852.0)">
              and they aimed to iterate upwards. Today we're closer
            </span>
            
            <span id="chunk-461" class="transcript-chunks" onclick="console.log('00:30:55,674'); seek(1855.0)">
              to 95% compliance.
            </span>
            
            <span id="chunk-462" class="transcript-chunks" onclick="console.log('00:30:59,190'); seek(1859.0)">
              This little inset here on the bottom right is
            </span>
            
            <span id="chunk-463" class="transcript-chunks" onclick="console.log('00:31:03,016'); seek(1863.0)">
              an example of what you can do with slos once
            </span>
            
            <span id="chunk-464" class="transcript-chunks" onclick="console.log('00:31:06,696'); seek(1866.0)">
              you start measuring them, once you are able to cleanly separate out.
            </span>
            
            <span id="chunk-465" class="transcript-chunks" onclick="console.log('00:31:10,888'); seek(1870.0)">
              These are requests that did not complete successfully versus the ones that did.
            </span>
            
            <span id="chunk-466" class="transcript-chunks" onclick="console.log('00:31:15,052'); seek(1875.0)">
              You can go in and take all of this rich metadata
            </span>
            
            <span id="chunk-467" class="transcript-chunks" onclick="console.log('00:31:18,994'); seek(1878.0)">
              you've captured along the way and find outliers and then prioritize
            </span>
            
            <span id="chunk-468" class="transcript-chunks" onclick="console.log('00:31:24,330'); seek(1884.0)">
              what work has the highest impact on. Yours is having a great experience.
            </span>
            
            <span id="chunk-469" class="transcript-chunks" onclick="console.log('00:31:28,350'); seek(1888.0)">
              This sort of telemetry and analysis over time.
            </span>
            
            <span id="chunk-470" class="transcript-chunks" onclick="console.log('00:31:31,984'); seek(1891.0)">
              This is a seven day view. There's 30 day views. Whatever your tool
            </span>
            
            <span id="chunk-471" class="transcript-chunks" onclick="console.log('00:31:35,252'); seek(1895.0)">
              will have different time windows. But being able to
            </span>
            
            <span id="chunk-472" class="transcript-chunks" onclick="console.log('00:31:38,948'); seek(1898.0)">
              track this historical compliance is what allows the team to iterate
            </span>
            
            <span id="chunk-473" class="transcript-chunks" onclick="console.log('00:31:42,730'); seek(1902.0)">
              fast and confidently. Remember, the core
            </span>
            
            <span id="chunk-474" class="transcript-chunks" onclick="console.log('00:31:46,234'); seek(1906.0)">
              of this is that llms are unpredictable and hard to model through traditional testing approaches.
            </span>
            
            <span id="chunk-475" class="transcript-chunks" onclick="console.log('00:31:50,878'); seek(1910.0)">
              And so the team here chose to measure from the outside in
            </span>
            
            <span id="chunk-476" class="transcript-chunks" onclick="console.log('00:31:55,430'); seek(1915.0)">
              to start with the measurements that mattered, users being
            </span>
            
            <span id="chunk-477" class="transcript-chunks" onclick="console.log('00:31:58,808'); seek(1918.0)">
              able to use the feature period and have a good experience,
            </span>
            
            <span id="chunk-478" class="transcript-chunks" onclick="console.log('00:32:02,490'); seek(1922.0)">
              and then debug as necessary and
            </span>
            
            <span id="chunk-479" class="transcript-chunks" onclick="console.log('00:32:05,852'); seek(1925.0)">
              improve iteratively. I'll leave you with two other stories.
            </span>
            
            <span id="chunk-480" class="transcript-chunks" onclick="console.log('00:32:09,602'); seek(1929.0)">
              So you believe that it's not just us. As we were building our feature,
            </span>
            
            <span id="chunk-481" class="transcript-chunks" onclick="console.log('00:32:13,062'); seek(1933.0)">
              we actually learned that two of our customers were using honeycomb
            </span>
            
            <span id="chunk-482" class="transcript-chunks" onclick="console.log('00:32:17,238'); seek(1937.0)">
              for a very similar thing.
            </span>
            
            <span id="chunk-483" class="transcript-chunks" onclick="console.log('00:32:21,150'); seek(1941.0)">
              Duolingo language learning app care
            </span>
            
            <span id="chunk-484" class="transcript-chunks" onclick="console.log('00:32:25,476'); seek(1945.0)">
              a lot about latency. With their LLMS features being
            </span>
            
            <span id="chunk-485" class="transcript-chunks" onclick="console.log('00:32:29,780'); seek(1949.0)">
              heavily mobile, they really wanted to make sure that whatever they
            </span>
            
            <span id="chunk-486" class="transcript-chunks" onclick="console.log('00:32:33,092'); seek(1953.0)">
              introduced felt fast. And so
            </span>
            
            <span id="chunk-487" class="transcript-chunks" onclick="console.log('00:32:36,520'); seek(1956.0)">
              they captured all this. Metadata only shown
            </span>
            
            <span id="chunk-488" class="transcript-chunks" onclick="console.log('00:32:40,622'); seek(1960.0)">
              two examples, and they wanted
            </span>
            
            <span id="chunk-489" class="transcript-chunks" onclick="console.log('00:32:44,392'); seek(1964.0)">
              to really closely measure what would
            </span>
            
            <span id="chunk-490" class="transcript-chunks" onclick="console.log('00:32:48,616'); seek(1968.0)">
              impact the llms being slos and the overall user experience
            </span>
            
            <span id="chunk-491" class="transcript-chunks" onclick="console.log('00:32:52,220'); seek(1972.0)">
              being slow. And what they found, actually,
            </span>
            
            <span id="chunk-492" class="transcript-chunks" onclick="console.log('00:32:55,884'); seek(1975.0)">
              the total latency was influenced way more by the things that they controlled
            </span>
            
            <span id="chunk-493" class="transcript-chunks" onclick="console.log('00:33:00,410'); seek(1980.0)">
              in that long trace, that building up that prompt and then capturing additional
            </span>
            
            <span id="chunk-494" class="transcript-chunks" onclick="console.log('00:33:04,646'); seek(1984.0)">
              context. That was where the bulk of the time was being spent,
            </span>
            
            <span id="chunk-495" class="transcript-chunks" onclick="console.log('00:33:08,198'); seek(1988.0)">
              not the LLM call itself. And so again,
            </span>
            
            <span id="chunk-496" class="transcript-chunks" onclick="console.log('00:33:12,030'); seek(1992.0)">
              their unpredictability happened in a different way. But in using
            </span>
            
            <span id="chunk-497" class="transcript-chunks" onclick="console.log('00:33:15,732'); seek(1995.0)">
              these new technologies, you won't know where the potholes will
            </span>
            
            <span id="chunk-498" class="transcript-chunks" onclick="console.log('00:33:19,748'); seek(1999.0)">
              be. And they were able to be confident
            </span>
            
            <span id="chunk-499" class="transcript-chunks" onclick="console.log('00:33:23,738'); seek(2003.0)">
              by capturing this rich data, by capturing telemetry
            </span>
            
            <span id="chunk-500" class="transcript-chunks" onclick="console.log('00:33:28,078'); seek(2008.0)">
              from the user's perspective that, okay, this is where we need to focus to
            </span>
            
            <span id="chunk-501" class="transcript-chunks" onclick="console.log('00:33:31,864'); seek(2011.0)">
              make the whole feature fast.
            </span>
            
            <span id="chunk-502" class="transcript-chunks" onclick="console.log('00:33:36,950'); seek(2016.0)">
              Second story I'll have for you is intercom.
            </span>
            
            <span id="chunk-503" class="transcript-chunks" onclick="console.log('00:33:41,370'); seek(2021.0)">
              Intercom is a sort of a messaging application for
            </span>
            
            <span id="chunk-504" class="transcript-chunks" onclick="console.log('00:33:45,692'); seek(2025.0)">
              businesses to message with their users.
            </span>
            
            <span id="chunk-505" class="transcript-chunks" onclick="console.log('00:33:48,570'); seek(2028.0)">
              And they were rapidly iterating on
            </span>
            
            <span id="chunk-506" class="transcript-chunks" onclick="console.log('00:33:52,460'); seek(2032.0)">
              a few different approaches to their LLM backed chatbot,
            </span>
            
            <span id="chunk-507" class="transcript-chunks" onclick="console.log('00:33:56,838'); seek(2036.0)">
              I believe. And they really wanted to keep tabs on the user experience,
            </span>
            
            <span id="chunk-508" class="transcript-chunks" onclick="console.log('00:34:01,150'); seek(2041.0)">
              even though there was all this change to the plumbing using on underneath.
            </span>
            
            <span id="chunk-509" class="transcript-chunks" onclick="console.log('00:34:05,466'); seek(2045.0)">
              And so they tracked tons of
            </span>
            
            <span id="chunk-510" class="transcript-chunks" onclick="console.log('00:34:09,172'); seek(2049.0)">
              pieces of metadata for each user interaction.
            </span>
            
            <span id="chunk-511" class="transcript-chunks" onclick="console.log('00:34:13,330'); seek(2053.0)">
              They captured what was happening in the application, they captured all these different
            </span>
            
            <span id="chunk-512" class="transcript-chunks" onclick="console.log('00:34:17,412'); seek(2057.0)">
              timings, time to first token, time to first usable token, how long it took
            </span>
            
            <span id="chunk-513" class="transcript-chunks" onclick="console.log('00:34:20,808'); seek(2060.0)">
              to get to the end user, how long the overall latency was, everything.
            </span>
            
            <span id="chunk-514" class="transcript-chunks" onclick="console.log('00:34:24,230'); seek(2064.0)">
              Then they tracked everything that they were changing along the way
            </span>
            
            <span id="chunk-515" class="transcript-chunks" onclick="console.log('00:34:27,880'); seek(2067.0)">
              version of the algorithm, which model they were using, the type
            </span>
            
            <span id="chunk-516" class="transcript-chunks" onclick="console.log('00:34:31,308'); seek(2071.0)">
              of metadata they were getting back. And critically, this was traced
            </span>
            
            <span id="chunk-517" class="transcript-chunks" onclick="console.log('00:34:35,362'); seek(2075.0)">
              with everything else happening inside their application. They needed the
            </span>
            
            <span id="chunk-518" class="transcript-chunks" onclick="console.log('00:34:39,228'); seek(2079.0)">
              full picture of the user experience to be confident in
            </span>
            
            <span id="chunk-519" class="transcript-chunks" onclick="console.log('00:34:43,052'); seek(2083.0)">
              understanding that they pull one lever over here,
            </span>
            
            <span id="chunk-520" class="transcript-chunks" onclick="console.log('00:34:47,070'); seek(2087.0)">
              they see the result over here, and they recognize that
            </span>
            
            <span id="chunk-521" class="transcript-chunks" onclick="console.log('00:34:50,624'); seek(2090.0)">
              using an LLM is just one piece of understanding this user experience
            </span>
            
            <span id="chunk-522" class="transcript-chunks" onclick="console.log('00:34:54,900'); seek(2094.0)">
              through telemetry of your application, not something to be siloed
            </span>
            
            <span id="chunk-523" class="transcript-chunks" onclick="console.log('00:34:58,618'); seek(2098.0)">
              over there with an ML team or something else.
            </span>
            
            <span id="chunk-524" class="transcript-chunks" onclick="console.log('00:35:02,050'); seek(2102.0)">
              So in the end, LLMs break many of
            </span>
            
            <span id="chunk-525" class="transcript-chunks" onclick="console.log('00:35:06,132'); seek(2106.0)">
              our existing tools and techniques that we use to rely on
            </span>
            
            <span id="chunk-526" class="transcript-chunks" onclick="console.log('00:35:09,496'); seek(2109.0)">
              ensuring correctness and a good user experience.
            </span>
            
            <span id="chunk-527" class="transcript-chunks" onclick="console.log('00:35:13,350'); seek(2113.0)">
              Observability can help. Think about the problem from the outside in.
            </span>
            
            <span id="chunk-528" class="transcript-chunks" onclick="console.log('00:35:17,830'); seek(2117.0)">
              Capture all the metadata so that you have that paper trail to debug and figure
            </span>
            
            <span id="chunk-529" class="transcript-chunks" onclick="console.log('00:35:21,832'); seek(2121.0)">
              out what was going on with this weird LLM box
            </span>
            
            <span id="chunk-530" class="transcript-chunks" onclick="console.log('00:35:25,590'); seek(2125.0)">
              and embrace the unpredictability.
            </span>
            
            <span id="chunk-531" class="transcript-chunks" onclick="console.log('00:35:28,710'); seek(2128.0)">
              Get out to production quickly, get in front of user yours and plan
            </span>
            
            <span id="chunk-532" class="transcript-chunks" onclick="console.log('00:35:32,112'); seek(2132.0)">
              to iterate fast. Plan to be reactive and embrace
            </span>
            
            <span id="chunk-533" class="transcript-chunks" onclick="console.log('00:35:35,638'); seek(2135.0)">
              that as a good thing instead of a stressful piece instead.
            </span>
            
            <span id="chunk-534" class="transcript-chunks" onclick="console.log('00:35:39,070'); seek(2139.0)">
              Thanks for your attention so far. If you want to learn
            </span>
            
            <span id="chunk-535" class="transcript-chunks" onclick="console.log('00:35:42,512'); seek(2142.0)">
              more about this, we've got a bunch of blog posts that go into much greater
            </span>
            
            <span id="chunk-536" class="transcript-chunks" onclick="console.log('00:35:45,894'); seek(2145.0)">
              detail than I was able to in the time we had together.
            </span>
            
            <span id="chunk-537" class="transcript-chunks" onclick="console.log('00:35:49,150'); seek(2149.0)">
              But thanks for your time. Enjoy the rest of the conference.
            </span>
            
            <span id="chunk-538" class="transcript-chunks" onclick="console.log('00:35:53,790'); seek(2153.0)">
              Bye.
            </span>
            
            </div>
          </div>
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/Conf42%20Incident%20Management%202023%20-%20Christine%20Yen.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/Conf42%20Incident%20Management%202023%20-%20Christine%20Yen.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #C44B4B;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/im2023" class="btn btn-sm btn-danger shadow lift" style="background-color: #C44B4B;">
                <i class="fe fe-grid me-2"></i>
                See all 10 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Christine%20Yen_incident.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Christine Yen
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    CEO @ Honeycomb
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/christineyen/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Christine Yen's LinkedIn account" />
                  </a>
                  
                  
                  <a href="https://twitter.com/@cyen" target="_blank">
                    <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="Christine Yen's twitter account" />
                  </a>
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by @cyen"
                  data-url="https://www.conf42.com/im2023"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/im2023"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>





  <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
    </script>
    <!-- SUBSCRIBE -->
    <section class="pt-8 pt-md-11 bg-gradient-light-white" id="register">
        <div class="container">
          <div class="row align-items-center justify-content-between mb-8 mb-md-11">
            <div class="col-12 col-md-6 order-md-2" data-aos="fade-left">
  
              <!-- Heading -->
              <h2>
                Awesome tech events for <br>
                <span class="text-success"><span data-typed='{"strings": ["software engineers.", "tech leaders.", "SREs.", "DevOps.", "CTOs.",  "managers.", "architects.", "QAs.", "developers.", "coders.", "founders.", "CEOs.", "students.", "geeks.", "ethical hackers.", "educators.", "enthusiasts.", "directors.", "researchers.", "PHDs.", "evangelists.", "tech authors."]}'></span></span>
              </h2>
  
              <!-- Text -->
              <p class="fs-lg text-muted mb-6">
  
              </p>
  
              <!-- List -->
              <div class="row">
                <div class="col-12 col-lg-12">
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <!-- Text -->
                    <p class="text-success">
                      Priority access to all content
                    </p>
                  </div>
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Video hallway track
                    </p>
                  </div>

                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Community chat
                    </p>
                  </div>
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Exclusive promotions and giveaways
                    </p>
                  </div>
  
                </div>
              </div> <!-- / .row -->
  
            </div>
            <div class="col-12 col-md-6 col-lg-5 order-md-1">
  
              <!-- Card -->
              <div class="card shadow-light-lg">
  
                <!-- Body -->
                <div class="card-body">
  
                  <!-- Form -->
                  <link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
                  <p class="emailoctopus-success-message text-success"></p>
                  <p class="emailoctopus-error-message text-danger"></p>
                  <form
                    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
                    method="post"
                    data-message-success="Thanks! Check your email for further directions!"
                    data-message-missing-email-address="Your email address is required."
                    data-message-invalid-email-address="Your email address looks incorrect, please try again."
                    data-message-bot-submission-error="This doesn't look like a human submission."
                    data-message-consent-required="Please check the checkbox to indicate your consent."
                    data-message-invalid-parameters-error="This form has missing or invalid fields."
                    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
                    class="emailoctopus-form"
                    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
                  >
                    <div class="form-floating emailoctopus-form-row">
                      <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
                      <label for="field_0">Email address</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
                      <label for="field_1">First Name</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
                      <label for="field_2">Last Name</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
                      <label for="field_4">Company</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
                      <label for="field_5">Job Title</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
                      <label for="field_3">Phone Number</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
                        oninput="updateCountry()"
                      >
                        <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
                      </select>
                      <label for="field_7">Country</label>
                    </div>
                    <input id="country-destination" name="field_7" type="hidden">
                    <input id="tz-country" name="field_8" type="hidden">
                    
                    <input
                      name="field_6"
                      type="hidden"
                      value="Incident Management"
                    >
                    
                    <div class="emailoctopus-form-row-consent">
                      <input
                        type="checkbox"
                        id="consent"
                        name="consent"
                      >
                      <label for="consent">
                        I consent to the following terms:
                      </label>
                      <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
                        Terms and Conditions
                      </a>
                      &amp;
                      <a href="./code-of-conduct" target="_blank">
                        Code of Conduct
                      </a>
                    </div>
                    <div
                      aria-hidden="true"
                      class="emailoctopus-form-row-hp"
                    >
                      <input
                        type="text"
                        name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
                        tabindex="-1"
                        autocomplete="nope"
                      >
                    </div>
                    <div class="mt-6 emailoctopus-form-row-subscribe">
                      <input
                        type="hidden"
                        name="successRedirectUrl"
                      >
                      <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
                        Subscribe
                      </button>
                    </div>
                  </form>
  
                </div>
  
              </div>
  
            </div>
  
          </div> <!-- / .row -->
        </div> <!-- / .container -->
      </section>

      <!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
      <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/aiml2024">
                  Artificial Intelligence & Machine Learning (AI & ML) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/olly2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.sreday.com/">
                  SREday London 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

  </body>
</html>