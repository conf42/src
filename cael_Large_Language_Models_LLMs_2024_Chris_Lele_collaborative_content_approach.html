<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Transforming Content Creation with Collaborative AI: A Groundbreaking Approach</title>
    <meta name="description" content="One model, extra large, please!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Chris%20Lele_llm.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Transforming Content Creation with Collaborative AI: A Groundbreaking Approach | Conf42"/>
    <meta property="og:description" content="Discover a new paradigm in content creation: collaborative AI. Learn how leveraging multiple LLMs in an iterative feedback loop led to outputs surpassing the original human-created contentâ€”all in mere minutes. Unlock the transformative potential of collaborative AI to achieve content perfection."/>
    <meta property="og:url" content="https://conf42.com/Large_Language_Models_LLMs_2024_Chris_Lele_collaborative_content_approach"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/PROMPT2024_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Prompt Engineering 2024
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2024-11-14
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/prompt2024" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2025">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2025">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2025">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2025">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2025">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2025">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2025">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2025">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2025">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/robotics2025">
                            Robotics
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2025">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2025">
                            Internet of Things (IoT)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="./support">
                            Support us
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Subscribe for FREE
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #CCB87B;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Large Language Models (LLMs) 2024 - Online
            </h1>

            <h2 class="text-white">
              
              Content unlocked! Welcome to the community!
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- One model, extra large, please!
 -->
              <script>
                const event_date = new Date("2024-04-11T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2024-04-11T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "3LT9pB4IB5U"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <a class="btn btn-info lift mb-3" data-bigpicture='{"ytSrc": "TQwxk0c4sh0"}' href="#">
                <i class="fe fe-eye me-2"></i>
                Watch Premiere
              </a>
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrBjR6ZR0g0LRq9Fp8c_4HrI" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Hi, today I\u0027m going to talk about a creative approach I\u0027ve", "timestamp": "00:00:21,280", "timestamp_s": 21.0}, {"text": "devised that harnesses multiple llms", "timestamp": "00:00:24,974", "timestamp_s": 24.0}, {"text": "in order to achieve a high quality output.", "timestamp": "00:00:28,210", "timestamp_s": 28.0}, {"text": "It\u0027s something I call collaborative AI, and my", "timestamp": "00:00:31,866", "timestamp_s": 31.0}, {"text": "hope today is that you will be able to unlock its", "timestamp": "00:00:35,170", "timestamp_s": 35.0}, {"text": "potential to really push the upper limits of what is", "timestamp": "00:00:38,626", "timestamp_s": 38.0}, {"text": "possible in terms of content quality and LLM", "timestamp": "00:00:42,122", "timestamp_s": 42.0}, {"text": "output. Today I\u0027m going to use Chachi PT", "timestamp": "00:00:46,018", "timestamp_s": 46.0}, {"text": "four and cloud three opus to showcase the power of", "timestamp": "00:00:49,986", "timestamp_s": 49.0}, {"text": "collaborative AI. Before I do so, I wanted to talk a little", "timestamp": "00:00:54,302", "timestamp_s": 54.0}, {"text": "bit about how content creation pre AI has", "timestamp": "00:00:57,894", "timestamp_s": 57.0}, {"text": "typically played out in my field. Online learning. The bar for", "timestamp": "00:01:01,342", "timestamp_s": 61.0}, {"text": "creating educational content is usually very, very high", "timestamp": "00:01:05,062", "timestamp_s": 65.0}, {"text": "when it comes to factuality. We hire what are known", "timestamp": "00:01:08,998", "timestamp_s": 68.0}, {"text": "as SME\u0027s or subject matter experts, and their job", "timestamp": "00:01:12,230", "timestamp_s": 72.0}, {"text": "is to be as accurate as possible, both in writing and and in reviewing", "timestamp": "00:01:15,606", "timestamp_s": 75.0}, {"text": "content. They essentially are the domain experts", "timestamp": "00:01:19,336", "timestamp_s": 79.0}, {"text": "of their field, be it art history or upper", "timestamp": "00:01:22,504", "timestamp_s": 82.0}, {"text": "division calculus. What they are not oftentimes", "timestamp": "00:01:26,000", "timestamp_s": 86.0}, {"text": "is professionally trained writers. As a result,", "timestamp": "00:01:29,432", "timestamp_s": 89.0}, {"text": "their writing, while grammatically sound, factually accurate,", "timestamp": "00:01:32,880", "timestamp_s": 92.0}, {"text": "can sometimes come across as a little bit dry,", "timestamp": "00:01:36,776", "timestamp_s": 96.0}, {"text": "unengaging, and a bit repetitive. But this isn\u0027t helped", "timestamp": "00:01:39,720", "timestamp_s": 99.0}, {"text": "by the fact that the amount of content often needed by online providers is", "timestamp": "00:01:43,288", "timestamp_s": 103.0}, {"text": "staggering, and SME\u0027s have to work under a tight deadline.", "timestamp": "00:01:46,976", "timestamp_s": 106.0}, {"text": "Engaging writing with memorable examples, smooth transitions,", "timestamp": "00:01:50,864", "timestamp_s": 110.0}, {"text": "and that writerly touch are oftentimes out of reach", "timestamp": "00:01:54,880", "timestamp_s": 114.0}, {"text": "for SME\u0027s, even those with professional training in writing.", "timestamp": "00:01:58,272", "timestamp_s": 118.0}, {"text": "With AI, we are now able to get content out much faster,", "timestamp": "00:02:02,176", "timestamp_s": 122.0}, {"text": "but not without potential pitfalls, for one,", "timestamp": "00:02:05,600", "timestamp_s": 125.0}, {"text": "hallucinations where the LLM generates inaccuracies", "timestamp": "00:02:09,248", "timestamp_s": 129.0}, {"text": "and even outright falsehoods. There\u0027s this fear that for first time learners,", "timestamp": "00:02:13,456", "timestamp_s": 133.0}, {"text": "they might end up thinking that the civil war happened only last century.", "timestamp": "00:02:17,750", "timestamp_s": 137.0}, {"text": "While such glaring falsehoods aren\u0027t necessarily that", "timestamp": "00:02:21,934", "timestamp_s": 141.0}, {"text": "common, smaller inaccuracies do occur.", "timestamp": "00:02:25,502", "timestamp_s": 145.0}, {"text": "Then there\u0027s the question of writing do AI models", "timestamp": "00:02:28,846", "timestamp_s": 148.0}, {"text": "have the ability to take otherwise potentially dry educational", "timestamp": "00:02:32,414", "timestamp_s": 152.0}, {"text": "content and make it exciting and interesting while", "timestamp": "00:02:36,726", "timestamp_s": 156.0}, {"text": "still being accurate and and able to convey a sense of authority? The PowerPoint", "timestamp": "00:02:40,574", "timestamp_s": 160.0}, {"text": "presentation that follows I\u0027m going to take a piece of educational", "timestamp": "00:02:44,710", "timestamp_s": 164.0}, {"text": "content that I\u0027m going to have Claude three opus and Chachi Pt", "timestamp": "00:02:48,654", "timestamp_s": 168.0}, {"text": "four evaluate, and then I\u0027m going to have them generate", "timestamp": "00:02:52,278", "timestamp_s": 172.0}, {"text": "their own versions. But I\u0027ll go further than that,", "timestamp": "00:02:55,526", "timestamp_s": 175.0}, {"text": "leveraging collaborative AI as I take inputs and outputs from one", "timestamp": "00:02:58,526", "timestamp_s": 178.0}, {"text": "LLM and feed them into the other, using collaborative", "timestamp": "00:03:02,566", "timestamp_s": 182.0}, {"text": "AI to improve upon those outputs so that the", "timestamp": "00:03:06,606", "timestamp_s": 186.0}, {"text": "final product is greater, better than", "timestamp": "00:03:09,822", "timestamp_s": 189.0}, {"text": "anything that either LLM could have generated by itself.", "timestamp": "00:03:13,054", "timestamp_s": 193.0}, {"text": "So let\u0027s dive in and see collaborative AI in action. So here we", "timestamp": "00:03:16,518", "timestamp_s": 196.0}, {"text": "are with transforming content creation with collaborative AI.", "timestamp": "00:03:19,982", "timestamp_s": 199.0}, {"text": "The first thing I did was create a little experiment.", "timestamp": "00:03:23,638", "timestamp_s": 203.0}, {"text": "The idea here was that we needed to take some baseline,", "timestamp": "00:03:27,222", "timestamp_s": 207.0}, {"text": "some standard content that the llms could improve upon,", "timestamp": "00:03:31,166", "timestamp_s": 211.0}, {"text": "and we needed to make sure that there was some scoring around that", "timestamp": "00:03:35,954", "timestamp_s": 215.0}, {"text": "baseline sample. Otherwise it would be difficult to say whether and how much", "timestamp": "00:03:40,042", "timestamp_s": 220.0}, {"text": "the other LLM generated outputs improved by.", "timestamp": "00:03:44,258", "timestamp_s": 224.0}, {"text": "So first off, we needed a piece of education content, something that", "timestamp": "00:03:47,954", "timestamp_s": 227.0}, {"text": "could serve as our baseline. And of course we needed to choose a", "timestamp": "00:03:51,994", "timestamp_s": 231.0}, {"text": "topic as well. And then we needed to define the criteria", "timestamp": "00:03:55,370", "timestamp_s": 235.0}, {"text": "of quality, like what made this a strong or not strong", "timestamp": "00:03:59,690", "timestamp_s": 239.0}, {"text": "piece of writing, and what did we want the two llms, in this case", "timestamp": "00:04:03,386", "timestamp_s": 243.0}, {"text": "chachi pt four and clot three opus, to focus", "timestamp": "00:04:07,498", "timestamp_s": 247.0}, {"text": "on when generating a high quality sample.", "timestamp": "00:04:10,922", "timestamp_s": 250.0}, {"text": "So that again speaks to the idea of to establish a quality", "timestamp": "00:04:15,234", "timestamp_s": 255.0}, {"text": "baseline. So what I did is I had chat GPT four actually write", "timestamp": "00:04:18,978", "timestamp_s": 258.0}, {"text": "a chapter, and then later clod three", "timestamp": "00:04:22,570", "timestamp_s": 262.0}, {"text": "scored that chapter. Now I\u0027m going to go through each one of these parts,", "timestamp": "00:04:26,050", "timestamp_s": 266.0}, {"text": "starting with the piece of education content and then ending with more details", "timestamp": "00:04:29,746", "timestamp_s": 269.0}, {"text": "about establishing that quality baseline.", "timestamp": "00:04:33,890", "timestamp_s": 273.0}, {"text": "First off, the piece of content, I decided it was going", "timestamp": "00:04:36,784", "timestamp_s": 276.0}, {"text": "to be a 500 to 600 word article on large language models,", "timestamp": "00:04:40,564", "timestamp_s": 280.0}, {"text": "and I think that makes sense given the target audience.", "timestamp": "00:04:44,256", "timestamp_s": 284.0}, {"text": "But I didn\u0027t just have it write the large language model.", "timestamp": "00:04:48,664", "timestamp_s": 288.0}, {"text": "Instead, I fed Claude 33 online learning excerpts", "timestamp": "00:04:51,560", "timestamp_s": 291.0}, {"text": "from different topics, different areas,", "timestamp": "00:04:55,192", "timestamp_s": 295.0}, {"text": "something that it could model when it", "timestamp": "00:04:58,560", "timestamp_s": 298.0}, {"text": "actually generated its own article, and I chose samples that", "timestamp": "00:05:02,072", "timestamp_s": 302.0}, {"text": "were indicative of more average online", "timestamp": "00:05:06,506", "timestamp_s": 306.0}, {"text": "learning content. So there\u0027s a lot of great online learning content out there,", "timestamp": "00:05:09,834", "timestamp_s": 309.0}, {"text": "and I definitely don\u0027t want to cast aspersions upon the field, but I was going", "timestamp": "00:05:13,506", "timestamp_s": 313.0}, {"text": "for something a little bit more average, something that if someone was", "timestamp": "00:05:16,850", "timestamp_s": 316.0}, {"text": "under a deadline, they might end up creating. And so I", "timestamp": "00:05:20,402", "timestamp_s": 320.0}, {"text": "fed that to Claude three and had it actually characterized", "timestamp": "00:05:24,178", "timestamp_s": 324.0}, {"text": "the writing, which is a step I like to do with llms. It\u0027s a reflection,", "timestamp": "00:05:28,170", "timestamp_s": 328.0}, {"text": "sort of step in between before they actually generate an", "timestamp": "00:05:32,178", "timestamp_s": 332.0}, {"text": "article. Now, you don\u0027t have to do this, but it\u0027s something that I did before", "timestamp": "00:05:35,760", "timestamp_s": 335.0}, {"text": "it actually generated the writing. And in doing so, it identified", "timestamp": "00:05:40,624", "timestamp_s": 340.0}, {"text": "eight characteristics from these excerpts and also was a sanity", "timestamp": "00:05:44,408", "timestamp_s": 344.0}, {"text": "check just to make sure that what I thought wasn\u0027t great writing, that it could", "timestamp": "00:05:47,896", "timestamp_s": 347.0}, {"text": "back me up there as well. And indeed it did here.", "timestamp": "00:05:50,960", "timestamp_s": 350.0}, {"text": "Came up with a total of eight. I\u0027ve only posted six and", "timestamp": "00:05:54,376", "timestamp_s": 354.0}, {"text": "a half here, but it gives you an idea. The point here isn\u0027t to read", "timestamp": "00:05:57,978", "timestamp_s": 357.0}, {"text": "through each one of these, but that there definitely are", "timestamp": "00:06:01,202", "timestamp_s": 361.0}, {"text": "lapses in quality. And now once the LLM has", "timestamp": "00:06:04,938", "timestamp_s": 364.0}, {"text": "that, it can generate this piece of content here, which again is a 500", "timestamp": "00:06:08,906", "timestamp_s": 368.0}, {"text": "or 600 word chapter on llms. And I actually", "timestamp": "00:06:12,610", "timestamp_s": 372.0}, {"text": "used my editorial eye just a little bit and looked at those", "timestamp": "00:06:16,354", "timestamp_s": 376.0}, {"text": "eight characteristics as well, courtesy of cloud three. And I changed,", "timestamp": "00:06:19,490", "timestamp_s": 379.0}, {"text": "tweaked just a few things, but nothing major.", "timestamp": "00:06:23,674", "timestamp_s": 383.0}, {"text": "And this is what we had, or what we ended up with.", "timestamp": "00:06:26,774", "timestamp_s": 386.0}, {"text": "Again, not going to pause here too long,", "timestamp": "00:06:30,582", "timestamp_s": 390.0}, {"text": "I don\u0027t think the point here is to really read this. In fact, I only", "timestamp": "00:06:34,182", "timestamp_s": 394.0}, {"text": "excerpted it because this is clearly not 500 to 600 words,", "timestamp": "00:06:36,438", "timestamp_s": 396.0}, {"text": "but the actual thing from which, or the text from which this is exerted", "timestamp": "00:06:39,526", "timestamp_s": 399.0}, {"text": "was around the 500 mark. So usually llms aren\u0027t that great at counting,", "timestamp": "00:06:43,502", "timestamp_s": 403.0}, {"text": "but they did a pretty good job here. But what is mediocre", "timestamp": "00:06:47,190", "timestamp_s": 407.0}, {"text": "about this? Let\u0027s just really quickly look at that first sentence where it says llms", "timestamp": "00:06:50,510", "timestamp_s": 410.0}, {"text": "are a type of AI, artificial intelligence. Then notice the", "timestamp": "00:06:53,742", "timestamp_s": 413.0}, {"text": "second sentence llms use. So it repeats", "timestamp": "00:06:56,880", "timestamp_s": 416.0}, {"text": "that exact same noun, and that gives rise to a repetitive,", "timestamp": "00:07:00,560", "timestamp_s": 420.0}, {"text": "dry kind of writing. And if you dive in here a little bit more,", "timestamp": "00:07:04,368", "timestamp_s": 424.0}, {"text": "you\u0027ll see that as well. Third paragraph starts off with llms,", "timestamp": "00:07:07,384", "timestamp_s": 427.0}, {"text": "but the idea here is it\u0027s the quality of writing that we\u0027re going", "timestamp": "00:07:11,096", "timestamp_s": 431.0}, {"text": "for, and it just doesn\u0027t hit the mark. Next, we wanted to", "timestamp": "00:07:14,304", "timestamp_s": 434.0}, {"text": "define criteria that we were looking for in", "timestamp": "00:07:17,688", "timestamp_s": 437.0}, {"text": "good writing. So when we have the llms create quality output,", "timestamp": "00:07:21,528", "timestamp_s": 441.0}, {"text": "what are we defining as quality? So we marked here some criteria. The LLM,", "timestamp": "00:07:26,438", "timestamp_s": 446.0}, {"text": "in this case cloud three, was able to come up with five categories", "timestamp": "00:07:30,870", "timestamp_s": 450.0}, {"text": "here, engaging language and storytelling, relatable examples, thought for broken", "timestamp": "00:07:34,822", "timestamp_s": 454.0}, {"text": "questions, sentence structure, clarity, et cetera. And this", "timestamp": "00:07:38,222", "timestamp_s": 458.0}, {"text": "is what it identified. And what I agreed were", "timestamp": "00:07:42,070", "timestamp_s": 462.0}, {"text": "hallmarks of strong, engaging educational", "timestamp": "00:07:46,142", "timestamp_s": 466.0}, {"text": "content. So again, in establishing the baseline,", "timestamp": "00:07:49,334", "timestamp_s": 469.0}, {"text": "we got a score out of four out of ten. But I didn\u0027t want to", "timestamp": "00:07:53,366", "timestamp_s": 473.0}, {"text": "just stop there. I asked myself, what if we just asked", "timestamp": "00:07:56,222", "timestamp_s": 476.0}, {"text": "the LLM in a one shot prompt to come", "timestamp": "00:08:00,326", "timestamp_s": 480.0}, {"text": "up with a chapter for an education course, online learning", "timestamp": "00:08:04,262", "timestamp_s": 484.0}, {"text": "course on llms, what would it come out with? And it", "timestamp": "00:08:08,174", "timestamp_s": 488.0}, {"text": "came out with something, this one shot prompt, and it got a seven out of", "timestamp": "00:08:11,910", "timestamp_s": 491.0}, {"text": "ten. Now, I\u0027m not going to paste that here, but I\u0027ll say it was high", "timestamp": "00:08:15,910", "timestamp_s": 495.0}, {"text": "level, generic, typical AI stuff. This was a good baseline for", "timestamp": "00:08:18,624", "timestamp_s": 498.0}, {"text": "me, because if we use collaborative AI here,", "timestamp": "00:08:21,880", "timestamp_s": 501.0}, {"text": "or if I use collaborative AI and it turns out I also get", "timestamp": "00:08:25,552", "timestamp_s": 505.0}, {"text": "a seven, then there doesn\u0027t seem to be much point in collaborative AI", "timestamp": "00:08:28,800", "timestamp_s": 508.0}, {"text": "when you can just do a one shot prompt that will get you a decent", "timestamp": "00:08:32,224", "timestamp_s": 512.0}, {"text": "seven out of ten score. But let\u0027s see what actually happens when", "timestamp": "00:08:35,376", "timestamp_s": 515.0}, {"text": "we use collaborative AI now. You\u0027ll notice it says", "timestamp": "00:08:39,232", "timestamp_s": 519.0}, {"text": "pre step one, so we\u0027re not quite there, and sorry", "timestamp": "00:08:42,952", "timestamp_s": 522.0}, {"text": "to be teasing you on this, but we\u0027re almost there in the", "timestamp": "00:08:46,184", "timestamp_s": 526.0}, {"text": "next slide. For now, though, the pre step prompt I did was", "timestamp": "00:08:50,024", "timestamp_s": 530.0}, {"text": "I asked cloud three and chat GPT four to identify characteristics of the", "timestamp": "00:08:53,512", "timestamp_s": 533.0}, {"text": "original sample and score it. That\u0027s that reflection piece that we did earlier on.", "timestamp": "00:08:57,168", "timestamp_s": 537.0}, {"text": "So this isn\u0027t an integral part of collaborative AI, but just something nice to", "timestamp": "00:09:01,336", "timestamp_s": 541.0}, {"text": "do. And then the second pre step was to", "timestamp": "00:09:04,720", "timestamp_s": 544.0}, {"text": "ask it to generate a sample as close to a ten out of ten as", "timestamp": "00:09:08,224", "timestamp_s": 548.0}, {"text": "possible. And this is where the collaborative AI", "timestamp": "00:09:11,072", "timestamp_s": 551.0}, {"text": "process and machinery now starts with step one.", "timestamp": "00:09:15,080", "timestamp_s": 555.0}, {"text": "Here, what I did was I input a version one from each LLM", "timestamp": "00:09:18,526", "timestamp_s": 558.0}, {"text": "into the other one, asking to evaluate it on a score from one through ten.", "timestamp": "00:09:22,430", "timestamp_s": 562.0}, {"text": "So, for example, clot three created a version", "timestamp": "00:09:26,230", "timestamp_s": 566.0}, {"text": "on that pre step number two just a second ago, created that version one,", "timestamp": "00:09:30,102", "timestamp_s": 570.0}, {"text": "and then I fed that version into chat GPT.", "timestamp": "00:09:33,806", "timestamp_s": 573.0}, {"text": "But look at that second part there, where it says other LLM", "timestamp": "00:09:38,374", "timestamp_s": 578.0}, {"text": "comma. That\u0027s the second part asking to evaluate.", "timestamp": "00:09:41,822", "timestamp_s": 581.0}, {"text": "So I didn\u0027t just input the version, but I actually asked it to", "timestamp": "00:09:45,476", "timestamp_s": 585.0}, {"text": "rate it and score it, much the way a teacher or a professional would", "timestamp": "00:09:49,108", "timestamp_s": 589.0}, {"text": "do. And so with that evaluation in hand, then go", "timestamp": "00:09:53,052", "timestamp_s": 593.0}, {"text": "to the next step here, which is take the version one evaluation", "timestamp": "00:09:56,740", "timestamp_s": 596.0}, {"text": "from an LLM, or from one of the llms, and put it back into the", "timestamp": "00:10:00,764", "timestamp_s": 600.0}, {"text": "other LLM. I know this can get a little crisscrossy, but to give you an", "timestamp": "00:10:04,620", "timestamp_s": 604.0}, {"text": "example here, the chat GPT\u0027s evaluation,", "timestamp": "00:10:07,748", "timestamp_s": 607.0}, {"text": "which was on version one of cloud three. I then put it back", "timestamp": "00:10:11,834", "timestamp_s": 611.0}, {"text": "into cloud three, but then there\u0027s a second part to", "timestamp": "00:10:15,882", "timestamp_s": 615.0}, {"text": "step two, which is inputting it into the first LLM", "timestamp": "00:10:19,802", "timestamp_s": 619.0}, {"text": "for rewrite. And that brings us to step number three, where I take the rewrite,", "timestamp": "00:10:23,386", "timestamp_s": 623.0}, {"text": "which we\u0027re now calling version two, and I input it back", "timestamp": "00:10:26,770", "timestamp_s": 626.0}, {"text": "into the other LLM for evaluation and scoring.", "timestamp": "00:10:30,002", "timestamp_s": 630.0}, {"text": "And so the thing is, step back for a moment. We can think of it", "timestamp": "00:10:33,770", "timestamp_s": 633.0}, {"text": "as I gave cloud three an opportunity to", "timestamp": "00:10:37,114", "timestamp_s": 637.0}, {"text": "do a rewrite the way we would in a classroom, and then we get feedback", "timestamp": "00:10:41,450", "timestamp_s": 641.0}, {"text": "from a teacher. And version two is its rewrite based on this", "timestamp": "00:10:44,322", "timestamp_s": 644.0}, {"text": "evaluation and scoring. And then at", "timestamp": "00:10:48,426", "timestamp_s": 648.0}, {"text": "that point, was there a difference between version one and", "timestamp": "00:10:51,810", "timestamp_s": 651.0}, {"text": "two in terms of score? Now you can carry this process", "timestamp": "00:10:55,562", "timestamp_s": 655.0}, {"text": "on and on. You could have a version three, a version", "timestamp": "00:10:59,610", "timestamp_s": 659.0}, {"text": "four, version five. But I think at a certain point there are diminishing returns.", "timestamp": "00:11:03,210", "timestamp_s": 663.0}, {"text": "And so what we\u0027re trying to see here in this little experiment is,", "timestamp": "00:11:07,002", "timestamp_s": 667.0}, {"text": "was there an improvement between version one and version two?", "timestamp": "00:11:10,538", "timestamp_s": 670.0}, {"text": "So let\u0027s see what happens before we get too excited.", "timestamp": "00:11:14,162", "timestamp_s": 674.0}, {"text": "We have a step number for it. I think it\u0027s very important is to check", "timestamp": "00:11:17,098", "timestamp_s": 677.0}, {"text": "for hallucinations by inputting version two into the other LLM.", "timestamp": "00:11:20,130", "timestamp_s": 680.0}, {"text": "So essentially, we\u0027re using collaborative AI to do", "timestamp": "00:11:23,834", "timestamp_s": 683.0}, {"text": "hallucination checks. I know I threw a lot of text and", "timestamp": "00:11:27,370", "timestamp_s": 687.0}, {"text": "words at you, but if you pause here for a moment, you can see this", "timestamp": "00:11:31,264", "timestamp_s": 691.0}, {"text": "collaborative AI structure use here, spread out here for each", "timestamp": "00:11:35,392", "timestamp_s": 695.0}, {"text": "one of the steps. Again, there could be more steps if you wanted", "timestamp": "00:11:39,352", "timestamp_s": 699.0}, {"text": "to do more than just two versions. But this is the bare bones,", "timestamp": "00:11:42,752", "timestamp_s": 702.0}, {"text": "basic little experiment version that we are doing here. So maybe you\u0027re", "timestamp": "00:11:47,272", "timestamp_s": 707.0}, {"text": "curious now, what was chat DPT in cloud three\u0027s first", "timestamp": "00:11:50,872", "timestamp_s": 710.0}, {"text": "version, and how was that scored? I\u0027m happy you asked.", "timestamp": "00:11:54,200", "timestamp_s": 714.0}, {"text": "Let\u0027s dive in here. The chat GT\u0027s first version,", "timestamp": "00:11:57,650", "timestamp_s": 717.0}, {"text": "we can see that it gets a seven out of ten based on", "timestamp": "00:12:01,466", "timestamp_s": 721.0}, {"text": "criteria of engaging writing, etcetera, which isn\u0027t great", "timestamp": "00:12:05,746", "timestamp_s": 725.0}, {"text": "given that the one shot prompt also got us a seven", "timestamp": "00:12:09,378", "timestamp_s": 729.0}, {"text": "out of ten. But at least it\u0027s a starting point.", "timestamp": "00:12:12,826", "timestamp_s": 732.0}, {"text": "Hopefully the second version will be better.", "timestamp": "00:12:16,354", "timestamp_s": 736.0}, {"text": "And how did Claude three do? Let\u0027s see what teacher Chachi Pt", "timestamp": "00:12:19,674", "timestamp_s": 739.0}, {"text": "four has to say. They give it. If you look there at the bottom", "timestamp": "00:12:23,354", "timestamp_s": 743.0}, {"text": "of the first paragraph, it says, I would rate this version an eight out of", "timestamp": "00:12:26,872", "timestamp_s": 746.0}, {"text": "ten. So it did a little bit better. But for", "timestamp": "00:12:30,056", "timestamp_s": 750.0}, {"text": "now, this is enough to give you an idea of how this works.", "timestamp": "00:12:33,312", "timestamp_s": 753.0}, {"text": "So here\u0027s the second step. We feed the evaluations from", "timestamp": "00:12:36,328", "timestamp_s": 756.0}, {"text": "one LLM back into the other for a rewrite.", "timestamp": "00:12:39,640", "timestamp_s": 759.0}, {"text": "Now, in this case, what I did was I actually", "timestamp": "00:12:42,984", "timestamp_s": 762.0}, {"text": "exerted the entire thing, and I did that for a reason. I think it\u0027s important", "timestamp": "00:12:46,376", "timestamp_s": 766.0}, {"text": "to see just how detailed these evaluations", "timestamp": "00:12:50,112", "timestamp_s": 770.0}, {"text": "are. So when the LLM is getting its feedback, you can think of", "timestamp": "00:12:53,950", "timestamp_s": 773.0}, {"text": "it as a prompt. Imagine writing a prompt that", "timestamp": "00:12:57,366", "timestamp_s": 777.0}, {"text": "is this long and, aha. Even longer.", "timestamp": "00:13:01,310", "timestamp_s": 781.0}, {"text": "So that\u0027s not necessarily bad thing, given that LLMs often thrive off", "timestamp": "00:13:06,494", "timestamp_s": 786.0}, {"text": "of this level of specificity, and there", "timestamp": "00:13:10,510", "timestamp_s": 790.0}, {"text": "is a lot of specificity going on, does it actually amount", "timestamp": "00:13:14,022", "timestamp_s": 794.0}, {"text": "to anything in version two? Meaning,", "timestamp": "00:13:18,558", "timestamp_s": 798.0}, {"text": "will the LLms write a better version of", "timestamp": "00:13:21,518", "timestamp_s": 801.0}, {"text": "the chapter? I\u0027m happy you asked, because now we\u0027re", "timestamp": "00:13:24,902", "timestamp_s": 804.0}, {"text": "at the point where we can ask it, come up with a version that", "timestamp": "00:13:28,558", "timestamp_s": 808.0}, {"text": "gets a perfect tense, so we\u0027ve definitely raised the bar, but there\u0027s a", "timestamp": "00:13:32,430", "timestamp_s": 812.0}, {"text": "lot of specificity. And that, of course, is what", "timestamp": "00:13:35,774", "timestamp_s": 815.0}, {"text": "makes collaborative AI so powerful. How does", "timestamp": "00:13:39,462", "timestamp_s": 819.0}, {"text": "this rate we get this is the version two from", "timestamp": "00:13:44,102", "timestamp_s": 824.0}, {"text": "Claude, and this is the version two from", "timestamp": "00:13:47,766", "timestamp_s": 827.0}, {"text": "ChatGpt. And Drumroll. Their scores", "timestamp": "00:13:51,496", "timestamp_s": 831.0}, {"text": "a 9.5 out of ten. So you can see this", "timestamp": "00:13:56,064", "timestamp_s": 836.0}, {"text": "is Claude rating chat GPT. So even though chat GPT\u0027s first attempt", "timestamp": "00:13:59,848", "timestamp_s": 839.0}, {"text": "was a measly baseline seven, this one got close to", "timestamp": "00:14:03,408", "timestamp_s": 843.0}, {"text": "a ten, and Claude\u0027s version got a solid", "timestamp": "00:14:07,200", "timestamp_s": 847.0}, {"text": "nine, if you look at the last or the bottom of the first paragraph.", "timestamp": "00:14:11,080", "timestamp_s": 851.0}, {"text": "But in both cases, the version two was much better.", "timestamp": "00:14:14,560", "timestamp_s": 854.0}, {"text": "So we can see the third step scores here. Version one,", "timestamp": "00:14:19,376", "timestamp_s": 859.0}, {"text": "seven for chachi, BT for claude, eight. Version two,", "timestamp": "00:14:23,472", "timestamp_s": 863.0}, {"text": "9.5 and nine. So both market improvements simply", "timestamp": "00:14:26,984", "timestamp_s": 866.0}, {"text": "by using just one round of collaborative AI.", "timestamp": "00:14:31,808", "timestamp_s": 871.0}, {"text": "Now, you might be asking, well, what about the", "timestamp": "00:14:36,184", "timestamp_s": 876.0}, {"text": "human in the loop? In this case me? Did I agree with these versions?", "timestamp": "00:14:40,308", "timestamp_s": 880.0}, {"text": "And so, yes, I did read these versions, and I", "timestamp": "00:14:43,580", "timestamp_s": 883.0}, {"text": "agreed with them in terms of the improvement. They were", "timestamp": "00:14:47,564", "timestamp_s": 887.0}, {"text": "both much better than the first versions.", "timestamp": "00:14:50,940", "timestamp_s": 890.0}, {"text": "And in fact, based on the criteria that established, I essentially agreed with these scores.", "timestamp": "00:14:54,036", "timestamp_s": 894.0}, {"text": "The reason I hesitate in saying I wholeheartedly agree", "timestamp": "00:14:58,364", "timestamp_s": 898.0}, {"text": "with them was I felt at times they were maybe trying to be a little", "timestamp": "00:15:01,964", "timestamp_s": 901.0}, {"text": "too engaging, a little bit too fun.", "timestamp": "00:15:05,820", "timestamp_s": 905.0}, {"text": "But that wasn\u0027t necessarily part of the evaluation criteria. So that\u0027s something", "timestamp": "00:15:09,364", "timestamp_s": 909.0}, {"text": "that, as the human in the loop, I can ask in a version three", "timestamp": "00:15:13,148", "timestamp_s": 913.0}, {"text": "just to tweak that. So it doesn\u0027t sound like you\u0027re trying to be", "timestamp": "00:15:16,588", "timestamp_s": 916.0}, {"text": "someone\u0027s pal and trying to be too relatable.", "timestamp": "00:15:20,348", "timestamp_s": 920.0}, {"text": "But again, everything besides that was at a much higher level,", "timestamp": "00:15:23,644", "timestamp_s": 923.0}, {"text": "making it solid educational content that I think would", "timestamp": "00:15:27,156", "timestamp_s": 927.0}, {"text": "really pull in audiences and make learning so much more", "timestamp": "00:15:30,852", "timestamp_s": 930.0}, {"text": "fun and enjoyable. Before we go on, though, I want us", "timestamp": "00:15:34,332", "timestamp_s": 934.0}, {"text": "to compare here to the baseline scores just one", "timestamp": "00:15:37,780", "timestamp_s": 937.0}, {"text": "last time, just to see where we came from, you know, speaking about education content", "timestamp": "00:15:41,436", "timestamp_s": 941.0}, {"text": "being more fun and enjoyable. If you\u0027re going from a four out of ten,", "timestamp": "00:15:45,308", "timestamp_s": 945.0}, {"text": "again, not all education content that\u0027s human created is", "timestamp": "00:15:48,356", "timestamp_s": 948.0}, {"text": "a four out of ten. But if kind of the average ish", "timestamp": "00:15:51,828", "timestamp_s": 951.0}, {"text": "is, then to go from four to nine to 9.5 is a huge", "timestamp": "00:15:55,924", "timestamp_s": 955.0}, {"text": "improvement. And the fact that collaborative AI, at least just with this one", "timestamp": "00:16:00,654", "timestamp_s": 960.0}, {"text": "round, is something that doesn\u0027t take long at all compared to some of these editorial", "timestamp": "00:16:04,262", "timestamp_s": 964.0}, {"text": "and content creation processes that involve multiple", "timestamp": "00:16:08,134", "timestamp_s": 968.0}, {"text": "SME\u0027s, both creators and reviewers, several rounds,", "timestamp": "00:16:11,606", "timestamp_s": 971.0}, {"text": "and someone oftentimes overseeing that entire process, you can", "timestamp": "00:16:15,590", "timestamp_s": 975.0}, {"text": "see that this can be costly and again, if that", "timestamp": "00:16:19,478", "timestamp_s": 979.0}, {"text": "standard is only a four, then we\u0027re also getting a huge,", "timestamp": "00:16:23,422", "timestamp_s": 983.0}, {"text": "huge bump in quality. Finally, there\u0027s that hallucination check.", "timestamp": "00:16:26,738", "timestamp_s": 986.0}, {"text": "Both pieces passed. I think, though, it\u0027s always", "timestamp": "00:16:31,394", "timestamp_s": 991.0}, {"text": "super important to have a human in the loop, especially for something like education content,", "timestamp": "00:16:34,834", "timestamp_s": 994.0}, {"text": "where you do not want to have facts that are incorrect no matter", "timestamp": "00:16:38,898", "timestamp_s": 998.0}, {"text": "what. Now for the broader implications.", "timestamp": "00:16:42,458", "timestamp_s": 1002.0}, {"text": "Quality of content and speed is vital.", "timestamp": "00:16:46,074", "timestamp_s": 1006.0}, {"text": "Collaborative AI can be a huge addition to whatever AI they\u0027re", "timestamp": "00:16:49,626", "timestamp_s": 1009.0}, {"text": "currently using. Now, if they\u0027re not using any AI, then they enter AI or", "timestamp": "00:16:53,072", "timestamp_s": 1013.0}, {"text": "LLMs at a much higher level than they would with one shot,", "timestamp": "00:16:56,824", "timestamp_s": 1016.0}, {"text": "prompting digital marketing, PR, and corporate communications.", "timestamp": "00:17:00,336", "timestamp_s": 1020.0}, {"text": "These are just a few here of the areas where high", "timestamp": "00:17:04,576", "timestamp_s": 1024.0}, {"text": "quality and engaging content that will really help people learn", "timestamp": "00:17:08,976", "timestamp_s": 1028.0}, {"text": "something. For instance, the case of healthcare communication. If something is dry", "timestamp": "00:17:12,392", "timestamp_s": 1032.0}, {"text": "and drought, patients aren\u0027t likely to remember that, make it engaging", "timestamp": "00:17:16,439", "timestamp_s": 1036.0}, {"text": "at that nine 9.5 level, then suddenly it\u0027s something", "timestamp": "00:17:20,001", "timestamp_s": 1040.0}, {"text": "that\u0027s a lot easier for them to learn and pay attention to, something that\u0027s really", "timestamp": "00:17:23,465", "timestamp_s": 1043.0}, {"text": "important in health. But again, coming back to that hallucination thing,", "timestamp": "00:17:26,913", "timestamp_s": 1046.0}, {"text": "always a human in the loop for many of these different industries.", "timestamp": "00:17:30,729", "timestamp_s": 1050.0}, {"text": "So now for the closing thoughts. This is an interesting", "timestamp": "00:17:34,921", "timestamp_s": 1054.0}, {"text": "one. It\u0027s the idea that a 9.5 out of ten for one article", "timestamp": "00:17:38,321", "timestamp_s": 1058.0}, {"text": "isn\u0027t quite the same as a corpus or a body of articles.", "timestamp": "00:17:42,209", "timestamp_s": 1062.0}, {"text": "Why? Well, imagine that 9.5 that we saw,", "timestamp": "00:17:46,012", "timestamp_s": 1066.0}, {"text": "and I believe that was the one that Chachi bt outputted.", "timestamp": "00:17:49,964", "timestamp_s": 1069.0}, {"text": "Imagine that that was the exact same one over", "timestamp": "00:17:52,724", "timestamp_s": 1072.0}, {"text": "and over again. Now I just use that word imagine twice", "timestamp": "00:17:56,828", "timestamp_s": 1076.0}, {"text": "in a row, almost as a joke. And the reason that was a", "timestamp": "00:18:00,844", "timestamp_s": 1080.0}, {"text": "joke is back earlier in the presentation.", "timestamp": "00:18:04,228", "timestamp_s": 1084.0}, {"text": "It might not have been obvious because I didn\u0027t focus on it, but two", "timestamp": "00:18:07,724", "timestamp_s": 1087.0}, {"text": "of the articles use the word imagine a", "timestamp": "00:18:11,216", "timestamp_s": 1091.0}, {"text": "world, or imagine something to start off,", "timestamp": "00:18:14,864", "timestamp_s": 1094.0}, {"text": "and that alone is a little bit confusing.", "timestamp": "00:18:17,800", "timestamp_s": 1097.0}, {"text": "And it shows that if you had that for, say, 50 articles,", "timestamp": "00:18:21,232", "timestamp_s": 1101.0}, {"text": "and maybe 20 of them had that phraseology. So correcting for", "timestamp": "00:18:24,744", "timestamp_s": 1104.0}, {"text": "something like this at scale is something that\u0027s important to keep in mind", "timestamp": "00:18:28,224", "timestamp_s": 1108.0}, {"text": "early on when you are creating these pieces. And again, having that", "timestamp": "00:18:32,360", "timestamp_s": 1112.0}, {"text": "human in the loop, someone who can really wield the AI and wield something like", "timestamp": "00:18:36,072", "timestamp_s": 1116.0}, {"text": "collaborative AI, will make it less likely that you\u0027re going to see very", "timestamp": "00:18:39,872", "timestamp_s": 1119.0}, {"text": "common or similar opening lines or really similar writing", "timestamp": "00:18:43,922", "timestamp_s": 1123.0}, {"text": "throughout. But that said, civil writing is part of AI,", "timestamp": "00:18:47,810", "timestamp_s": 1127.0}, {"text": "and there\u0027s almost this AI speak. After all, we have the GPT", "timestamp": "00:18:51,082", "timestamp_s": 1131.0}, {"text": "zeros of the world that can identify AI generated language or", "timestamp": "00:18:54,714", "timestamp_s": 1134.0}, {"text": "text for a reason. It\u0027s because there is a certain pattern that", "timestamp": "00:18:58,682", "timestamp_s": 1138.0}, {"text": "makes it slightly different from human generated text.", "timestamp": "00:19:02,242", "timestamp_s": 1142.0}, {"text": "One way around this is to actually feed models with high caliber human", "timestamp": "00:19:06,414", "timestamp_s": 1146.0}, {"text": "text for inspiration. And so if you", "timestamp": "00:19:10,110", "timestamp_s": 1150.0}, {"text": "want to make it sound even more like a person, more relatable,", "timestamp": "00:19:13,230", "timestamp_s": 1153.0}, {"text": "perhaps make it so that it\u0027s not always saying, imagine a", "timestamp": "00:19:16,782", "timestamp_s": 1156.0}, {"text": "world where and using some of those other giveaways", "timestamp": "00:19:19,870", "timestamp_s": 1159.0}, {"text": "of AI speak, then using", "timestamp": "00:19:23,374", "timestamp_s": 1163.0}, {"text": "high caliber human pros that you want it to model is a good way around", "timestamp": "00:19:26,798", "timestamp_s": 1166.0}, {"text": "that. These are just a few ideas of improving the", "timestamp": "00:19:30,206", "timestamp_s": 1170.0}, {"text": "AI using collaborative AI, but in general, lots of", "timestamp": "00:19:34,314", "timestamp_s": 1174.0}, {"text": "different ways that we can use collaborative AI. Not just coming up with", "timestamp": "00:19:38,002", "timestamp_s": 1178.0}, {"text": "subsequent versions, one after the other, but maybe even leveraging more than", "timestamp": "00:19:41,634", "timestamp_s": 1181.0}, {"text": "two models, having three models, having a model judge its", "timestamp": "00:19:44,930", "timestamp_s": 1184.0}, {"text": "own writing in a different thread and then compare it to what the other LLM", "timestamp": "00:19:48,634", "timestamp_s": 1188.0}, {"text": "said. See how similar those are. There\u0027s so much", "timestamp": "00:19:52,226", "timestamp_s": 1192.0}, {"text": "you can do here again, and to use AI speak in", "timestamp": "00:19:55,922", "timestamp_s": 1195.0}, {"text": "a world where the possibilities are limitless,", "timestamp": "00:19:59,378", "timestamp_s": 1199.0}, {"text": "collaborative AI could be a game changer.", "timestamp": "00:20:02,482", "timestamp_s": 1202.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: '3LT9pB4IB5U',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Transforming Content Creation with Collaborative AI: A Groundbreaking Approach
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>Discover a new paradigm in content creation: collaborative AI. Learn how leveraging multiple LLMs in an iterative feedback loop led to outputs surpassing the original human-created contentâ€”all in mere minutes. Unlock the transformative potential of collaborative AI to achieve content perfection.</p>
<!-- End Text -->
          </div>

          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
              <li>
                Collaborative AI harnesses multiple llms in order to achieve a high quality output. The bar for creating educational content is usually very, very high when it comes to factuality. With AI, we are now able to get content out much faster, but not without potential pitfalls.

              </li>
              
              <li>
                Next, we wanted to define criteria that we were looking for in good writing. And then the second pre step was to ask it to generate a sample as close to a ten out of ten as possible. Finally, we asked cloud three and chat GPT four to identify characteristics of the original sample and score it.

              </li>
              
              <li>
                Both versions were much better than the first versions. Both market improvements simply by using just one round of collaborative AI. The human in the loop agreed with them in terms of the improvement. But at times they were maybe trying to be a little too engaging, a little bit too fun.

              </li>
              
              <li>
                 Collaborative AI can be a huge addition to whatever AI they're currently using. Quality of content and speed is vital. High quality and engaging content that will really help people learn something.

              </li>
              
              <li>
                A 9.5 out of ten for one article isn't quite the same as a corpus or a body of articles. One way around this is to feed models with high caliber human text for inspiration. To use AI speak in a world where the possibilities are limitless, collaborative AI could be a game changer.
              </li>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./assemblyai/3LT9pB4IB5U.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:21,280'); seek(21.0)">
              Hi, today I'm going to talk about a creative approach I've
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:24,974'); seek(24.0)">
              devised that harnesses multiple llms
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:28,210'); seek(28.0)">
              in order to achieve a high quality output.
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:31,866'); seek(31.0)">
              It's something I call collaborative AI, and my
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:35,170'); seek(35.0)">
              hope today is that you will be able to unlock its
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:38,626'); seek(38.0)">
              potential to really push the upper limits of what is
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:42,122'); seek(42.0)">
              possible in terms of content quality and LLM
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:46,018'); seek(46.0)">
              output. Today I'm going to use Chachi PT
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:49,986'); seek(49.0)">
              four and cloud three opus to showcase the power of
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:54,302'); seek(54.0)">
              collaborative AI. Before I do so, I wanted to talk a little
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:00:57,894'); seek(57.0)">
              bit about how content creation pre AI has
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:01:01,342'); seek(61.0)">
              typically played out in my field. Online learning. The bar for
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:01:05,062'); seek(65.0)">
              creating educational content is usually very, very high
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:01:08,998'); seek(68.0)">
              when it comes to factuality. We hire what are known
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:01:12,230'); seek(72.0)">
              as SME's or subject matter experts, and their job
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:01:15,606'); seek(75.0)">
              is to be as accurate as possible, both in writing and and in reviewing
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:19,336'); seek(79.0)">
              content. They essentially are the domain experts
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:22,504'); seek(82.0)">
              of their field, be it art history or upper
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:26,000'); seek(86.0)">
              division calculus. What they are not oftentimes
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:29,432'); seek(89.0)">
              is professionally trained writers. As a result,
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:32,880'); seek(92.0)">
              their writing, while grammatically sound, factually accurate,
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:36,776'); seek(96.0)">
              can sometimes come across as a little bit dry,
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:39,720'); seek(99.0)">
              unengaging, and a bit repetitive. But this isn't helped
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:43,288'); seek(103.0)">
              by the fact that the amount of content often needed by online providers is
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:46,976'); seek(106.0)">
              staggering, and SME's have to work under a tight deadline.
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:50,864'); seek(110.0)">
              Engaging writing with memorable examples, smooth transitions,
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:01:54,880'); seek(114.0)">
              and that writerly touch are oftentimes out of reach
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:01:58,272'); seek(118.0)">
              for SME's, even those with professional training in writing.
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:02:02,176'); seek(122.0)">
              With AI, we are now able to get content out much faster,
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:02:05,600'); seek(125.0)">
              but not without potential pitfalls, for one,
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:02:09,248'); seek(129.0)">
              hallucinations where the LLM generates inaccuracies
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:02:13,456'); seek(133.0)">
              and even outright falsehoods. There's this fear that for first time learners,
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:02:17,750'); seek(137.0)">
              they might end up thinking that the civil war happened only last century.
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:21,934'); seek(141.0)">
              While such glaring falsehoods aren't necessarily that
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:25,502'); seek(145.0)">
              common, smaller inaccuracies do occur.
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:28,846'); seek(148.0)">
              Then there's the question of writing do AI models
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:32,414'); seek(152.0)">
              have the ability to take otherwise potentially dry educational
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:36,726'); seek(156.0)">
              content and make it exciting and interesting while
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:40,574'); seek(160.0)">
              still being accurate and and able to convey a sense of authority? The PowerPoint
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:44,710'); seek(164.0)">
              presentation that follows I'm going to take a piece of educational
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:48,654'); seek(168.0)">
              content that I'm going to have Claude three opus and Chachi Pt
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:02:52,278'); seek(172.0)">
              four evaluate, and then I'm going to have them generate
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:02:55,526'); seek(175.0)">
              their own versions. But I'll go further than that,
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:02:58,526'); seek(178.0)">
              leveraging collaborative AI as I take inputs and outputs from one
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:03:02,566'); seek(182.0)">
              LLM and feed them into the other, using collaborative
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:03:06,606'); seek(186.0)">
              AI to improve upon those outputs so that the
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:09,822'); seek(189.0)">
              final product is greater, better than
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:13,054'); seek(193.0)">
              anything that either LLM could have generated by itself.
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:16,518'); seek(196.0)">
              So let's dive in and see collaborative AI in action. So here we
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:19,982'); seek(199.0)">
              are with transforming content creation with collaborative AI.
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:23,638'); seek(203.0)">
              The first thing I did was create a little experiment.
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:27,222'); seek(207.0)">
              The idea here was that we needed to take some baseline,
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:31,166'); seek(211.0)">
              some standard content that the llms could improve upon,
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:35,954'); seek(215.0)">
              and we needed to make sure that there was some scoring around that
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:40,042'); seek(220.0)">
              baseline sample. Otherwise it would be difficult to say whether and how much
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:44,258'); seek(224.0)">
              the other LLM generated outputs improved by.
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:47,954'); seek(227.0)">
              So first off, we needed a piece of education content, something that
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:03:51,994'); seek(231.0)">
              could serve as our baseline. And of course we needed to choose a
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:03:55,370'); seek(235.0)">
              topic as well. And then we needed to define the criteria
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:03:59,690'); seek(239.0)">
              of quality, like what made this a strong or not strong
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:04:03,386'); seek(243.0)">
              piece of writing, and what did we want the two llms, in this case
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:04:07,498'); seek(247.0)">
              chachi pt four and clot three opus, to focus
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:10,922'); seek(250.0)">
              on when generating a high quality sample.
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:15,234'); seek(255.0)">
              So that again speaks to the idea of to establish a quality
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:18,978'); seek(258.0)">
              baseline. So what I did is I had chat GPT four actually write
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:22,570'); seek(262.0)">
              a chapter, and then later clod three
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:26,050'); seek(266.0)">
              scored that chapter. Now I'm going to go through each one of these parts,
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:29,746'); seek(269.0)">
              starting with the piece of education content and then ending with more details
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:33,890'); seek(273.0)">
              about establishing that quality baseline.
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:36,784'); seek(276.0)">
              First off, the piece of content, I decided it was going
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:40,564'); seek(280.0)">
              to be a 500 to 600 word article on large language models,
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:44,256'); seek(284.0)">
              and I think that makes sense given the target audience.
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:04:48,664'); seek(288.0)">
              But I didn't just have it write the large language model.
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:04:51,560'); seek(291.0)">
              Instead, I fed Claude 33 online learning excerpts
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:04:55,192'); seek(295.0)">
              from different topics, different areas,
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:04:58,560'); seek(298.0)">
              something that it could model when it
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:05:02,072'); seek(302.0)">
              actually generated its own article, and I chose samples that
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:05:06,506'); seek(306.0)">
              were indicative of more average online
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:05:09,834'); seek(309.0)">
              learning content. So there's a lot of great online learning content out there,
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:13,506'); seek(313.0)">
              and I definitely don't want to cast aspersions upon the field, but I was going
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:16,850'); seek(316.0)">
              for something a little bit more average, something that if someone was
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:20,402'); seek(320.0)">
              under a deadline, they might end up creating. And so I
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:24,178'); seek(324.0)">
              fed that to Claude three and had it actually characterized
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:28,170'); seek(328.0)">
              the writing, which is a step I like to do with llms. It's a reflection,
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:32,178'); seek(332.0)">
              sort of step in between before they actually generate an
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:35,760'); seek(335.0)">
              article. Now, you don't have to do this, but it's something that I did before
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:40,624'); seek(340.0)">
              it actually generated the writing. And in doing so, it identified
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:44,408'); seek(344.0)">
              eight characteristics from these excerpts and also was a sanity
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:05:47,896'); seek(347.0)">
              check just to make sure that what I thought wasn't great writing, that it could
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:05:50,960'); seek(350.0)">
              back me up there as well. And indeed it did here.
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:05:54,376'); seek(354.0)">
              Came up with a total of eight. I've only posted six and
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:05:57,978'); seek(357.0)">
              a half here, but it gives you an idea. The point here isn't to read
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:06:01,202'); seek(361.0)">
              through each one of these, but that there definitely are
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:06:04,938'); seek(364.0)">
              lapses in quality. And now once the LLM has
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:08,906'); seek(368.0)">
              that, it can generate this piece of content here, which again is a 500
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:12,610'); seek(372.0)">
              or 600 word chapter on llms. And I actually
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:16,354'); seek(376.0)">
              used my editorial eye just a little bit and looked at those
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:19,490'); seek(379.0)">
              eight characteristics as well, courtesy of cloud three. And I changed,
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:06:23,674'); seek(383.0)">
              tweaked just a few things, but nothing major.
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:06:26,774'); seek(386.0)">
              And this is what we had, or what we ended up with.
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:06:30,582'); seek(390.0)">
              Again, not going to pause here too long,
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:06:34,182'); seek(394.0)">
              I don't think the point here is to really read this. In fact, I only
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:36,438'); seek(396.0)">
              excerpted it because this is clearly not 500 to 600 words,
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:06:39,526'); seek(399.0)">
              but the actual thing from which, or the text from which this is exerted
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:06:43,502'); seek(403.0)">
              was around the 500 mark. So usually llms aren't that great at counting,
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:06:47,190'); seek(407.0)">
              but they did a pretty good job here. But what is mediocre
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:06:50,510'); seek(410.0)">
              about this? Let's just really quickly look at that first sentence where it says llms
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:06:53,742'); seek(413.0)">
              are a type of AI, artificial intelligence. Then notice the
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:06:56,880'); seek(416.0)">
              second sentence llms use. So it repeats
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:07:00,560'); seek(420.0)">
              that exact same noun, and that gives rise to a repetitive,
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:04,368'); seek(424.0)">
              dry kind of writing. And if you dive in here a little bit more,
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:07:07,384'); seek(427.0)">
              you'll see that as well. Third paragraph starts off with llms,
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:07:11,096'); seek(431.0)">
              but the idea here is it's the quality of writing that we're going
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:07:14,304'); seek(434.0)">
              for, and it just doesn't hit the mark. Next, we wanted to
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:07:17,688'); seek(437.0)">
              define criteria that we were looking for in
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:07:21,528'); seek(441.0)">
              good writing. So when we have the llms create quality output,
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:07:26,438'); seek(446.0)">
              what are we defining as quality? So we marked here some criteria. The LLM,
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:07:30,870'); seek(450.0)">
              in this case cloud three, was able to come up with five categories
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:07:34,822'); seek(454.0)">
              here, engaging language and storytelling, relatable examples, thought for broken
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:07:38,222'); seek(458.0)">
              questions, sentence structure, clarity, et cetera. And this
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:07:42,070'); seek(462.0)">
              is what it identified. And what I agreed were
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:07:46,142'); seek(466.0)">
              hallmarks of strong, engaging educational
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:07:49,334'); seek(469.0)">
              content. So again, in establishing the baseline,
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:07:53,366'); seek(473.0)">
              we got a score out of four out of ten. But I didn't want to
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:07:56,222'); seek(476.0)">
              just stop there. I asked myself, what if we just asked
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:08:00,326'); seek(480.0)">
              the LLM in a one shot prompt to come
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:08:04,262'); seek(484.0)">
              up with a chapter for an education course, online learning
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:08,174'); seek(488.0)">
              course on llms, what would it come out with? And it
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:08:11,910'); seek(491.0)">
              came out with something, this one shot prompt, and it got a seven out of
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:08:15,910'); seek(495.0)">
              ten. Now, I'm not going to paste that here, but I'll say it was high
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:08:18,624'); seek(498.0)">
              level, generic, typical AI stuff. This was a good baseline for
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:08:21,880'); seek(501.0)">
              me, because if we use collaborative AI here,
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:08:25,552'); seek(505.0)">
              or if I use collaborative AI and it turns out I also get
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:08:28,800'); seek(508.0)">
              a seven, then there doesn't seem to be much point in collaborative AI
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:08:32,224'); seek(512.0)">
              when you can just do a one shot prompt that will get you a decent
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:08:35,376'); seek(515.0)">
              seven out of ten score. But let's see what actually happens when
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:08:39,232'); seek(519.0)">
              we use collaborative AI now. You'll notice it says
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:08:42,952'); seek(522.0)">
              pre step one, so we're not quite there, and sorry
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:08:46,184'); seek(526.0)">
              to be teasing you on this, but we're almost there in the
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:08:50,024'); seek(530.0)">
              next slide. For now, though, the pre step prompt I did was
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:08:53,512'); seek(533.0)">
              I asked cloud three and chat GPT four to identify characteristics of the
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:08:57,168'); seek(537.0)">
              original sample and score it. That's that reflection piece that we did earlier on.
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:09:01,336'); seek(541.0)">
              So this isn't an integral part of collaborative AI, but just something nice to
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:09:04,720'); seek(544.0)">
              do. And then the second pre step was to
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:09:08,224'); seek(548.0)">
              ask it to generate a sample as close to a ten out of ten as
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:09:11,072'); seek(551.0)">
              possible. And this is where the collaborative AI
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:09:15,080'); seek(555.0)">
              process and machinery now starts with step one.
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:09:18,526'); seek(558.0)">
              Here, what I did was I input a version one from each LLM
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:09:22,430'); seek(562.0)">
              into the other one, asking to evaluate it on a score from one through ten.
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:09:26,230'); seek(566.0)">
              So, for example, clot three created a version
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:09:30,102'); seek(570.0)">
              on that pre step number two just a second ago, created that version one,
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:09:33,806'); seek(573.0)">
              and then I fed that version into chat GPT.
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:09:38,374'); seek(578.0)">
              But look at that second part there, where it says other LLM
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:09:41,822'); seek(581.0)">
              comma. That's the second part asking to evaluate.
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:09:45,476'); seek(585.0)">
              So I didn't just input the version, but I actually asked it to
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:09:49,108'); seek(589.0)">
              rate it and score it, much the way a teacher or a professional would
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:09:53,052'); seek(593.0)">
              do. And so with that evaluation in hand, then go
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:09:56,740'); seek(596.0)">
              to the next step here, which is take the version one evaluation
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:10:00,764'); seek(600.0)">
              from an LLM, or from one of the llms, and put it back into the
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:10:04,620'); seek(604.0)">
              other LLM. I know this can get a little crisscrossy, but to give you an
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:10:07,748'); seek(607.0)">
              example here, the chat GPT's evaluation,
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:10:11,834'); seek(611.0)">
              which was on version one of cloud three. I then put it back
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:10:15,882'); seek(615.0)">
              into cloud three, but then there's a second part to
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:10:19,802'); seek(619.0)">
              step two, which is inputting it into the first LLM
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:10:23,386'); seek(623.0)">
              for rewrite. And that brings us to step number three, where I take the rewrite,
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:10:26,770'); seek(626.0)">
              which we're now calling version two, and I input it back
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:10:30,002'); seek(630.0)">
              into the other LLM for evaluation and scoring.
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:10:33,770'); seek(633.0)">
              And so the thing is, step back for a moment. We can think of it
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:10:37,114'); seek(637.0)">
              as I gave cloud three an opportunity to
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:10:41,450'); seek(641.0)">
              do a rewrite the way we would in a classroom, and then we get feedback
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:10:44,322'); seek(644.0)">
              from a teacher. And version two is its rewrite based on this
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:10:48,426'); seek(648.0)">
              evaluation and scoring. And then at
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:10:51,810'); seek(651.0)">
              that point, was there a difference between version one and
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:10:55,562'); seek(655.0)">
              two in terms of score? Now you can carry this process
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:10:59,610'); seek(659.0)">
              on and on. You could have a version three, a version
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:11:03,210'); seek(663.0)">
              four, version five. But I think at a certain point there are diminishing returns.
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:11:07,002'); seek(667.0)">
              And so what we're trying to see here in this little experiment is,
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:11:10,538'); seek(670.0)">
              was there an improvement between version one and version two?
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:11:14,162'); seek(674.0)">
              So let's see what happens before we get too excited.
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:11:17,098'); seek(677.0)">
              We have a step number for it. I think it's very important is to check
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:11:20,130'); seek(680.0)">
              for hallucinations by inputting version two into the other LLM.
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:11:23,834'); seek(683.0)">
              So essentially, we're using collaborative AI to do
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:11:27,370'); seek(687.0)">
              hallucination checks. I know I threw a lot of text and
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:11:31,264'); seek(691.0)">
              words at you, but if you pause here for a moment, you can see this
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:11:35,392'); seek(695.0)">
              collaborative AI structure use here, spread out here for each
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:11:39,352'); seek(699.0)">
              one of the steps. Again, there could be more steps if you wanted
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:11:42,752'); seek(702.0)">
              to do more than just two versions. But this is the bare bones,
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:11:47,272'); seek(707.0)">
              basic little experiment version that we are doing here. So maybe you're
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:11:50,872'); seek(710.0)">
              curious now, what was chat DPT in cloud three's first
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:11:54,200'); seek(714.0)">
              version, and how was that scored? I'm happy you asked.
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:11:57,650'); seek(717.0)">
              Let's dive in here. The chat GT's first version,
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:12:01,466'); seek(721.0)">
              we can see that it gets a seven out of ten based on
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:12:05,746'); seek(725.0)">
              criteria of engaging writing, etcetera, which isn't great
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:12:09,378'); seek(729.0)">
              given that the one shot prompt also got us a seven
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:12:12,826'); seek(732.0)">
              out of ten. But at least it's a starting point.
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:12:16,354'); seek(736.0)">
              Hopefully the second version will be better.
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:12:19,674'); seek(739.0)">
              And how did Claude three do? Let's see what teacher Chachi Pt
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:12:23,354'); seek(743.0)">
              four has to say. They give it. If you look there at the bottom
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:12:26,872'); seek(746.0)">
              of the first paragraph, it says, I would rate this version an eight out of
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:12:30,056'); seek(750.0)">
              ten. So it did a little bit better. But for
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:12:33,312'); seek(753.0)">
              now, this is enough to give you an idea of how this works.
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:12:36,328'); seek(756.0)">
              So here's the second step. We feed the evaluations from
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:12:39,640'); seek(759.0)">
              one LLM back into the other for a rewrite.
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:12:42,984'); seek(762.0)">
              Now, in this case, what I did was I actually
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:12:46,376'); seek(766.0)">
              exerted the entire thing, and I did that for a reason. I think it's important
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:12:50,112'); seek(770.0)">
              to see just how detailed these evaluations
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:12:53,950'); seek(773.0)">
              are. So when the LLM is getting its feedback, you can think of
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:12:57,366'); seek(777.0)">
              it as a prompt. Imagine writing a prompt that
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:13:01,310'); seek(781.0)">
              is this long and, aha. Even longer.
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:13:06,494'); seek(786.0)">
              So that's not necessarily bad thing, given that LLMs often thrive off
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:13:10,510'); seek(790.0)">
              of this level of specificity, and there
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:13:14,022'); seek(794.0)">
              is a lot of specificity going on, does it actually amount
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:13:18,558'); seek(798.0)">
              to anything in version two? Meaning,
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:13:21,518'); seek(801.0)">
              will the LLms write a better version of
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:13:24,902'); seek(804.0)">
              the chapter? I'm happy you asked, because now we're
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:13:28,558'); seek(808.0)">
              at the point where we can ask it, come up with a version that
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:13:32,430'); seek(812.0)">
              gets a perfect tense, so we've definitely raised the bar, but there's a
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:13:35,774'); seek(815.0)">
              lot of specificity. And that, of course, is what
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:13:39,462'); seek(819.0)">
              makes collaborative AI so powerful. How does
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:13:44,102'); seek(824.0)">
              this rate we get this is the version two from
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:13:47,766'); seek(827.0)">
              Claude, and this is the version two from
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:13:51,496'); seek(831.0)">
              ChatGpt. And Drumroll. Their scores
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:13:56,064'); seek(836.0)">
              a 9.5 out of ten. So you can see this
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:13:59,848'); seek(839.0)">
              is Claude rating chat GPT. So even though chat GPT's first attempt
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:14:03,408'); seek(843.0)">
              was a measly baseline seven, this one got close to
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:14:07,200'); seek(847.0)">
              a ten, and Claude's version got a solid
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:14:11,080'); seek(851.0)">
              nine, if you look at the last or the bottom of the first paragraph.
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:14:14,560'); seek(854.0)">
              But in both cases, the version two was much better.
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:14:19,376'); seek(859.0)">
              So we can see the third step scores here. Version one,
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:14:23,472'); seek(863.0)">
              seven for chachi, BT for claude, eight. Version two,
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:14:26,984'); seek(866.0)">
              9.5 and nine. So both market improvements simply
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:14:31,808'); seek(871.0)">
              by using just one round of collaborative AI.
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:14:36,184'); seek(876.0)">
              Now, you might be asking, well, what about the
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:14:40,308'); seek(880.0)">
              human in the loop? In this case me? Did I agree with these versions?
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:14:43,580'); seek(883.0)">
              And so, yes, I did read these versions, and I
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:14:47,564'); seek(887.0)">
              agreed with them in terms of the improvement. They were
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:14:50,940'); seek(890.0)">
              both much better than the first versions.
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:14:54,036'); seek(894.0)">
              And in fact, based on the criteria that established, I essentially agreed with these scores.
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:14:58,364'); seek(898.0)">
              The reason I hesitate in saying I wholeheartedly agree
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:15:01,964'); seek(901.0)">
              with them was I felt at times they were maybe trying to be a little
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:15:05,820'); seek(905.0)">
              too engaging, a little bit too fun.
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:15:09,364'); seek(909.0)">
              But that wasn't necessarily part of the evaluation criteria. So that's something
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:15:13,148'); seek(913.0)">
              that, as the human in the loop, I can ask in a version three
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:15:16,588'); seek(916.0)">
              just to tweak that. So it doesn't sound like you're trying to be
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:15:20,348'); seek(920.0)">
              someone's pal and trying to be too relatable.
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:15:23,644'); seek(923.0)">
              But again, everything besides that was at a much higher level,
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:15:27,156'); seek(927.0)">
              making it solid educational content that I think would
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:15:30,852'); seek(930.0)">
              really pull in audiences and make learning so much more
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:15:34,332'); seek(934.0)">
              fun and enjoyable. Before we go on, though, I want us
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:15:37,780'); seek(937.0)">
              to compare here to the baseline scores just one
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:15:41,436'); seek(941.0)">
              last time, just to see where we came from, you know, speaking about education content
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:15:45,308'); seek(945.0)">
              being more fun and enjoyable. If you're going from a four out of ten,
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:15:48,356'); seek(948.0)">
              again, not all education content that's human created is
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:15:51,828'); seek(951.0)">
              a four out of ten. But if kind of the average ish
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:15:55,924'); seek(955.0)">
              is, then to go from four to nine to 9.5 is a huge
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:16:00,654'); seek(960.0)">
              improvement. And the fact that collaborative AI, at least just with this one
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:16:04,262'); seek(964.0)">
              round, is something that doesn't take long at all compared to some of these editorial
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:16:08,134'); seek(968.0)">
              and content creation processes that involve multiple
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:16:11,606'); seek(971.0)">
              SME's, both creators and reviewers, several rounds,
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:16:15,590'); seek(975.0)">
              and someone oftentimes overseeing that entire process, you can
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:16:19,478'); seek(979.0)">
              see that this can be costly and again, if that
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:16:23,422'); seek(983.0)">
              standard is only a four, then we're also getting a huge,
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:16:26,738'); seek(986.0)">
              huge bump in quality. Finally, there's that hallucination check.
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:16:31,394'); seek(991.0)">
              Both pieces passed. I think, though, it's always
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:16:34,834'); seek(994.0)">
              super important to have a human in the loop, especially for something like education content,
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:16:38,898'); seek(998.0)">
              where you do not want to have facts that are incorrect no matter
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:16:42,458'); seek(1002.0)">
              what. Now for the broader implications.
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:16:46,074'); seek(1006.0)">
              Quality of content and speed is vital.
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:16:49,626'); seek(1009.0)">
              Collaborative AI can be a huge addition to whatever AI they're
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:16:53,072'); seek(1013.0)">
              currently using. Now, if they're not using any AI, then they enter AI or
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:16:56,824'); seek(1016.0)">
              LLMs at a much higher level than they would with one shot,
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:17:00,336'); seek(1020.0)">
              prompting digital marketing, PR, and corporate communications.
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:17:04,576'); seek(1024.0)">
              These are just a few here of the areas where high
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:17:08,976'); seek(1028.0)">
              quality and engaging content that will really help people learn
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:17:12,392'); seek(1032.0)">
              something. For instance, the case of healthcare communication. If something is dry
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:17:16,439'); seek(1036.0)">
              and drought, patients aren't likely to remember that, make it engaging
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:17:20,001'); seek(1040.0)">
              at that nine 9.5 level, then suddenly it's something
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:17:23,465'); seek(1043.0)">
              that's a lot easier for them to learn and pay attention to, something that's really
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:17:26,913'); seek(1046.0)">
              important in health. But again, coming back to that hallucination thing,
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:17:30,729'); seek(1050.0)">
              always a human in the loop for many of these different industries.
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:17:34,921'); seek(1054.0)">
              So now for the closing thoughts. This is an interesting
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:17:38,321'); seek(1058.0)">
              one. It's the idea that a 9.5 out of ten for one article
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:17:42,209'); seek(1062.0)">
              isn't quite the same as a corpus or a body of articles.
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:17:46,012'); seek(1066.0)">
              Why? Well, imagine that 9.5 that we saw,
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:17:49,964'); seek(1069.0)">
              and I believe that was the one that Chachi bt outputted.
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:17:52,724'); seek(1072.0)">
              Imagine that that was the exact same one over
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:17:56,828'); seek(1076.0)">
              and over again. Now I just use that word imagine twice
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:18:00,844'); seek(1080.0)">
              in a row, almost as a joke. And the reason that was a
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:18:04,228'); seek(1084.0)">
              joke is back earlier in the presentation.
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:18:07,724'); seek(1087.0)">
              It might not have been obvious because I didn't focus on it, but two
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:18:11,216'); seek(1091.0)">
              of the articles use the word imagine a
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:18:14,864'); seek(1094.0)">
              world, or imagine something to start off,
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:18:17,800'); seek(1097.0)">
              and that alone is a little bit confusing.
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:18:21,232'); seek(1101.0)">
              And it shows that if you had that for, say, 50 articles,
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:18:24,744'); seek(1104.0)">
              and maybe 20 of them had that phraseology. So correcting for
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:18:28,224'); seek(1108.0)">
              something like this at scale is something that's important to keep in mind
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:18:32,360'); seek(1112.0)">
              early on when you are creating these pieces. And again, having that
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:18:36,072'); seek(1116.0)">
              human in the loop, someone who can really wield the AI and wield something like
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:18:39,872'); seek(1119.0)">
              collaborative AI, will make it less likely that you're going to see very
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:18:43,922'); seek(1123.0)">
              common or similar opening lines or really similar writing
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:18:47,810'); seek(1127.0)">
              throughout. But that said, civil writing is part of AI,
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:18:51,082'); seek(1131.0)">
              and there's almost this AI speak. After all, we have the GPT
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:18:54,714'); seek(1134.0)">
              zeros of the world that can identify AI generated language or
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:18:58,682'); seek(1138.0)">
              text for a reason. It's because there is a certain pattern that
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:19:02,242'); seek(1142.0)">
              makes it slightly different from human generated text.
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:19:06,414'); seek(1146.0)">
              One way around this is to actually feed models with high caliber human
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:19:10,110'); seek(1150.0)">
              text for inspiration. And so if you
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:19:13,230'); seek(1153.0)">
              want to make it sound even more like a person, more relatable,
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:19:16,782'); seek(1156.0)">
              perhaps make it so that it's not always saying, imagine a
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:19:19,870'); seek(1159.0)">
              world where and using some of those other giveaways
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:19:23,374'); seek(1163.0)">
              of AI speak, then using
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:19:26,798'); seek(1166.0)">
              high caliber human pros that you want it to model is a good way around
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:19:30,206'); seek(1170.0)">
              that. These are just a few ideas of improving the
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:19:34,314'); seek(1174.0)">
              AI using collaborative AI, but in general, lots of
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:19:38,002'); seek(1178.0)">
              different ways that we can use collaborative AI. Not just coming up with
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:19:41,634'); seek(1181.0)">
              subsequent versions, one after the other, but maybe even leveraging more than
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:19:44,930'); seek(1184.0)">
              two models, having three models, having a model judge its
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:19:48,634'); seek(1188.0)">
              own writing in a different thread and then compare it to what the other LLM
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:19:52,226'); seek(1192.0)">
              said. See how similar those are. There's so much
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:19:55,922'); seek(1195.0)">
              you can do here again, and to use AI speak in
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:19:59,378'); seek(1199.0)">
              a world where the possibilities are limitless,
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:20:02,482'); seek(1202.0)">
              collaborative AI could be a game changer.
            </span>
            
            </div>
          </div>
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/Chris%20Lele%20-%20Conf42%20Large%20Language%20Models%20%28LLMs%29%202024.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/Chris%20Lele%20-%20Conf42%20Large%20Language%20Models%20%28LLMs%29%202024.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #CCB87B;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/llms2024" class="btn btn-sm btn-danger shadow lift" style="background-color: #CCB87B;">
                <i class="fe fe-grid me-2"></i>
                See all 28 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Chris%20Lele_llm.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Chris Lele
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Data Science and Machine Learning Fellow @ ElevateAICoaching.com
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/chris-lele-095005a0/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Chris Lele's LinkedIn account" />
                  </a>
                  
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by Chris Lele"
                  data-url="https://www.conf42.com/llms2024"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/llms2024"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>





  <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
    </script>
    <!-- SUBSCRIBE -->
    <section class="pt-8 pt-md-11 bg-gradient-light-white" id="register">
        <div class="container">
          <div class="row align-items-center justify-content-between mb-8 mb-md-11">
            <div class="col-12 col-md-6 order-md-2" data-aos="fade-left">
  
              <!-- Heading -->
              <h2>
                Awesome tech events for <br>
                <span class="text-success"><span data-typed='{"strings": ["software engineers.", "tech leaders.", "SREs.", "DevOps.", "CTOs.",  "managers.", "architects.", "QAs.", "developers.", "coders.", "founders.", "CEOs.", "students.", "geeks.", "ethical hackers.", "educators.", "enthusiasts.", "directors.", "researchers.", "PHDs.", "evangelists.", "tech authors."]}'></span></span>
              </h2>
  
              <!-- Text -->
              <p class="fs-lg text-muted mb-6">
  
              </p>
  
              <!-- List -->
              <div class="row">
                <div class="col-12 col-lg-12">
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <!-- Text -->
                    <p class="text-success">
                      Priority access to all content
                    </p>
                  </div>
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Video hallway track
                    </p>
                  </div>

                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Community chat
                    </p>
                  </div>
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Exclusive promotions and giveaways
                    </p>
                  </div>
  
                </div>
              </div> <!-- / .row -->
  
            </div>
            <div class="col-12 col-md-6 col-lg-5 order-md-1">
  
              <!-- Card -->
              <div class="card shadow-light-lg">
  
                <!-- Body -->
                <div class="card-body">
  
                  <!-- Form -->
                  <link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
                  <p class="emailoctopus-success-message text-success"></p>
                  <p class="emailoctopus-error-message text-danger"></p>
                  <form
                    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
                    method="post"
                    data-message-success="Thanks! Check your email for further directions!"
                    data-message-missing-email-address="Your email address is required."
                    data-message-invalid-email-address="Your email address looks incorrect, please try again."
                    data-message-bot-submission-error="This doesn't look like a human submission."
                    data-message-consent-required="Please check the checkbox to indicate your consent."
                    data-message-invalid-parameters-error="This form has missing or invalid fields."
                    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
                    class="emailoctopus-form"
                    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
                  >
                    <div class="form-floating emailoctopus-form-row">
                      <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
                      <label for="field_0">Email address</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
                      <label for="field_1">First Name</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
                      <label for="field_2">Last Name</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
                      <label for="field_4">Company</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
                      <label for="field_5">Job Title</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
                      <label for="field_3">Phone Number</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
                        oninput="updateCountry()"
                      >
                        <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
                      </select>
                      <label for="field_7">Country</label>
                    </div>
                    <input id="country-destination" name="field_7" type="hidden">
                    <input id="tz-country" name="field_8" type="hidden">
                    
                    <input
                      name="field_6"
                      type="hidden"
                      value="Large Language Models"
                    >
                    
                    <div class="emailoctopus-form-row-consent">
                      <input
                        type="checkbox"
                        id="consent"
                        name="consent"
                      >
                      <label for="consent">
                        I consent to the following terms:
                      </label>
                      <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
                        Terms and Conditions
                      </a>
                      &amp;
                      <a href="./code-of-conduct" target="_blank">
                        Code of Conduct
                      </a>
                    </div>
                    <div
                      aria-hidden="true"
                      class="emailoctopus-form-row-hp"
                    >
                      <input
                        type="text"
                        name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
                        tabindex="-1"
                        autocomplete="nope"
                      >
                    </div>
                    <div class="mt-6 emailoctopus-form-row-subscribe">
                      <input
                        type="hidden"
                        name="successRedirectUrl"
                      >
                      <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
                        Subscribe
                      </button>
                    </div>
                  </form>
  
                </div>
  
              </div>
  
            </div>
  
          </div> <!-- / .row -->
        </div> <!-- / .container -->
      </section>

      <!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
      <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2025">
                  Observability 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2025">
                  Quantum Computing 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2025">
                  Rustlang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2025">
                  Platform Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2025">
                  MLOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2025">
                  Incident Management 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2025">
                  Kube Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2025">
                  JavaScript 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2025">
                  Prompt Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/robotics2025">
                  Robotics 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2025">
                  DevSecOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2025">
                  Internet of Things (IoT) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>