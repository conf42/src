<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Building AI Applications with LLMs: Practical Tips and Best Practices</title>
    <meta name="description" content="Get inspired by fellow Pythonistas, Snakes and Pandas united!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Sourabh%20Gawande_python.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Building AI Applications with LLMs: Practical Tips and Best Practices | Conf42"/>
    <meta property="og:description" content="Understand the problems and trade-offs you'll encounter when building an AI application on top of LLMs based on my learnings of building KushoAI, an AI agent used by 5000+ engineers to make API testing completely autonomous by leveraging LLMs."/>
    <meta property="og:url" content="https://conf42.com/Python_2025_Sourabh_Gawande_ai_applications_practical"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/MLOPS2025_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        MLOps 2025
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2025-09-18
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/mlops2025" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2025">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2025">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2025">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2025">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2025">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2025">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2025">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2025">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2025">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/robotics2025">
                            Robotics
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2025">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2025">
                            Internet of Things (IoT)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="https://conf42.circle.so/">
                            <b>Community platform login</b>
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Join the community!
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #69811f;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Python 2025 - Online
            </h1>

            <h2 class="text-white">
              
              <time datetime="2025-02-06">February 06 2025</time>
              
              - premiere 5PM GMT
              
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- Get inspired by fellow Pythonistas, Snakes and Pandas united!
 -->
              <script>
                const event_date = new Date("2025-02-06T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2025-02-06T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              <a href="#register" class="btn btn-primary shadow lift me-1 mb-3">
                <i class="fe fe-user-check me-2"></i>
                Subscribe to watch
              </a>
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "vI1-ATx7tls"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrBo176Is4wP2F6UCB0yEkWO" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Hello, everyone.", "timestamp": "00:00:00,010", "timestamp_s": 0.0}, {"text": "My name is Saurabh.", "timestamp": "00:00:00,780", "timestamp_s": 0.0}, {"text": "I am the co founder and CTO of Kusho AI.", "timestamp": "00:00:01,620", "timestamp_s": 1.0}, {"text": "And today I\u0027m going to talk about practical tips for building AI", "timestamp": "00:00:04,010", "timestamp_s": 4.0}, {"text": "applications or AI agents using LLMs.", "timestamp": "00:00:07,840", "timestamp_s": 7.0}, {"text": "At Kusho AI, we have been, working on building AI applications.", "timestamp": "00:00:11,419", "timestamp_s": 11.0}, {"text": "And, agents for the last, 18 months.", "timestamp": "00:00:15,899", "timestamp_s": 15.0}, {"text": "And, during our journey, we have identified a bunch of unique", "timestamp": "00:00:18,749", "timestamp_s": 18.0}, {"text": "problems that people generally face.", "timestamp": "00:00:23,919", "timestamp_s": 23.0}, {"text": "And, we have also faced, those same problems specifically while", "timestamp": "00:00:26,899", "timestamp_s": 26.0}, {"text": "building, applications using LLMs.", "timestamp": "00:00:30,749", "timestamp_s": 30.0}, {"text": "And, the, agenda for today\u0027s talk is that, we want to, educate devs", "timestamp": "00:00:33,329", "timestamp_s": 33.0}, {"text": "about these problems, so that.", "timestamp": "00:00:39,994", "timestamp_s": 39.0}, {"text": "when they are building apps on top of LLMs, they are aware of these", "timestamp": "00:00:42,874", "timestamp_s": 42.0}, {"text": "problems and, also discuss what are, the solutions that work for us", "timestamp": "00:00:46,694", "timestamp_s": 46.0}, {"text": "and, the dev tools or tooling that we use to solve these problems.", "timestamp": "00:00:50,884", "timestamp_s": 50.0}, {"text": "and, by, by sharing this information, we, we want to save time, when", "timestamp": "00:00:54,254", "timestamp_s": 54.0}, {"text": "devs are, building applications on top of LLMs for the first time.", "timestamp": "00:00:58,815", "timestamp_s": 58.0}, {"text": "The first thing that you will need to solve when you start building", "timestamp": "00:01:03,294", "timestamp_s": 63.0}, {"text": "apps on top of, LLMs is, how to handle LLM inconsistencies.", "timestamp": "00:01:06,154", "timestamp_s": 66.0}, {"text": "you, if you have some experience building, applications using", "timestamp": "00:01:11,259", "timestamp_s": 71.0}, {"text": "normal APIs, you would have seen that they don\u0027t, fail that often.", "timestamp": "00:01:15,019", "timestamp_s": 75.0}, {"text": "So, by building, general applications, you don\u0027t really worry about,", "timestamp": "00:01:18,219", "timestamp_s": 78.0}, {"text": "inconsistencies or failures, that much.", "timestamp": "00:01:22,429", "timestamp_s": 82.0}, {"text": "if an API fails, you just let the API fail.", "timestamp": "00:01:25,359", "timestamp_s": 85.0}, {"text": "And, the user, when they refresh their page, you make another API call", "timestamp": "00:01:27,929", "timestamp_s": 87.0}, {"text": "and it will most probably succeed.", "timestamp": "00:01:31,549", "timestamp_s": 91.0}, {"text": "but in case of LLMs.", "timestamp": "00:01:33,069", "timestamp_s": 93.0}, {"text": "this is the first thing that you\u0027ll probably need to solve, when you\u0027re", "timestamp": "00:01:34,849", "timestamp_s": 94.0}, {"text": "actually building an application because, LLMs have a much higher", "timestamp": "00:01:37,809", "timestamp_s": 97.0}, {"text": "error rate, than your normal APIs.", "timestamp": "00:01:41,749", "timestamp_s": 101.0}, {"text": "And unless you solve this particular thing, your application will have", "timestamp": "00:01:44,424", "timestamp_s": 104.0}, {"text": "a terrible UX, or user experience.", "timestamp": "00:01:48,674", "timestamp_s": 108.0}, {"text": "Because, in your application, you\u0027ll generally use the LLM", "timestamp": "00:01:52,324", "timestamp_s": 112.0}, {"text": "response, somewhere else.", "timestamp": "00:01:55,254", "timestamp_s": 115.0}, {"text": "And every time the LLM gives you a wrong output, your application will also crash.", "timestamp": "00:01:57,064", "timestamp_s": 117.0}, {"text": "So, this is the first, problem that, you should solve, while", "timestamp": "00:02:02,434", "timestamp_s": 122.0}, {"text": "building LLM, applications.", "timestamp": "00:02:07,854", "timestamp_s": 127.0}, {"text": "Now, before we get into, like why, how to solve this particular problem, let\u0027s", "timestamp": "00:02:10,274", "timestamp_s": 130.0}, {"text": "just talk about why this even occurs, in, in these, in these specific applications.", "timestamp": "00:02:16,984", "timestamp_s": 136.0}, {"text": "so like I mentioned earlier, if you are, if you are working with,", "timestamp": "00:02:22,604", "timestamp_s": 142.0}, {"text": "normal APIs, you generally don\u0027t worry, much about the error rate.", "timestamp": "00:02:25,744", "timestamp_s": 145.0}, {"text": "in like fairly stable, APIs, but even the most stable LLMs give you a much", "timestamp": "00:02:31,379", "timestamp_s": 151.0}, {"text": "higher error rate, than your normal APIs.", "timestamp": "00:02:36,170", "timestamp_s": 156.0}, {"text": "And the reason for this is LLMs are inherently non deterministic.", "timestamp": "00:02:38,710", "timestamp_s": 158.0}, {"text": "so what do you mean by that?", "timestamp": "00:02:44,930", "timestamp_s": 164.0}, {"text": "so if you, if you, if you look at an LLM under the hood, they are essentially", "timestamp": "00:02:46,380", "timestamp_s": 166.0}, {"text": "statistical machines, that produce token after token based on, the input", "timestamp": "00:02:50,260", "timestamp_s": 170.0}, {"text": "prompt and whatever tokens have been generated previously, Statistical", "timestamp": "00:02:56,180", "timestamp_s": 176.0}, {"text": "machines are basically probabilistic.", "timestamp": "00:02:59,860", "timestamp_s": 179.0}, {"text": "And as soon as you bring probability into software, you\u0027re going to", "timestamp": "00:03:02,650", "timestamp_s": 182.0}, {"text": "get something non deterministic.", "timestamp": "00:03:06,390", "timestamp_s": 186.0}, {"text": "Now, what do we mean by non deterministic?", "timestamp": "00:03:08,240", "timestamp_s": 188.0}, {"text": "you basically, we\u0027ll get a different output for the same input.", "timestamp": "00:03:11,520", "timestamp_s": 191.0}, {"text": "Every time, You, ask LLM for a response?", "timestamp": "00:03:15,400", "timestamp_s": 195.0}, {"text": "you could, I, and I\u0027m, I\u0027m pretty sure like you are, you would have", "timestamp": "00:03:19,020", "timestamp_s": 199.0}, {"text": "seen this problem while using, all the different, chat bots that", "timestamp": "00:03:23,450", "timestamp_s": 203.0}, {"text": "are available, like chat, GPT or.", "timestamp": "00:03:26,820", "timestamp_s": 206.0}, {"text": "the deep seek or cloud, chat, you would have noticed that, every time", "timestamp": "00:03:29,270", "timestamp_s": 209.0}, {"text": "you give, give, given input, for the same input, every time you hit", "timestamp": "00:03:33,600", "timestamp_s": 213.0}, {"text": "retry, you\u0027ll get a different output.", "timestamp": "00:03:37,460", "timestamp_s": 217.0}, {"text": "that\u0027s the same thing that will happen with, the LLM", "timestamp": "00:03:39,120", "timestamp_s": 219.0}, {"text": "responses in your applications.", "timestamp": "00:03:41,370", "timestamp_s": 221.0}, {"text": "you most of the time don\u0027t, have a lot of control or, will you", "timestamp": "00:03:43,530", "timestamp_s": 223.0}, {"text": "get the exact output or not?", "timestamp": "00:03:47,360", "timestamp_s": 227.0}, {"text": "Now, because of this particular problem, which is, LLMs being non deterministic,", "timestamp": "00:03:49,100", "timestamp_s": 229.0}, {"text": "every time you give it an input, you\u0027ll not always get the response that you want.", "timestamp": "00:03:55,680", "timestamp_s": 235.0}, {"text": "Like, for example, for example, if you ask an LLM API to generate,", "timestamp": "00:04:00,360", "timestamp_s": 240.0}, {"text": "JSON, which is a structured output.", "timestamp": "00:04:06,150", "timestamp_s": 246.0}, {"text": "You might get more fields than, what you asked for.", "timestamp": "00:04:08,470", "timestamp_s": 248.0}, {"text": "Sometimes you might get less fields.", "timestamp": "00:04:11,870", "timestamp_s": 251.0}, {"text": "sometimes you might have, a bracket missing.", "timestamp": "00:04:13,670", "timestamp_s": 253.0}, {"text": "Based on what we have seen, if, if you have like a normal stable API.", "timestamp": "00:04:16,960", "timestamp_s": 256.0}, {"text": "you will see an error rate of something like 0.", "timestamp": "00:04:22,375", "timestamp_s": 262.0}, {"text": "1%, but if you are working with an LLM, even the most stable, the LLMs,", "timestamp": "00:04:26,555", "timestamp_s": 266.0}, {"text": "which have been, here for the longest amount of time, they\u0027ll give you an", "timestamp": "00:04:31,835", "timestamp_s": 271.0}, {"text": "error rate of, something like one to 5 percent based on what kind of", "timestamp": "00:04:35,375", "timestamp_s": 275.0}, {"text": "task you\u0027re asking it to perform.", "timestamp": "00:04:38,695", "timestamp_s": 278.0}, {"text": "And if you\u0027re working with chained LLM responses, like, basically you.", "timestamp": "00:04:40,415", "timestamp_s": 280.0}, {"text": "provide, the LLM API with a prompt, you take that response, and then", "timestamp": "00:04:45,245", "timestamp_s": 285.0}, {"text": "you provide it with another prompt, using the response, that you got", "timestamp": "00:04:49,685", "timestamp_s": 289.0}, {"text": "earlier, this is basically a chained, LLM responses, you will see that", "timestamp": "00:04:54,175", "timestamp_s": 294.0}, {"text": "your error rate gets compounded.", "timestamp": "00:04:59,435", "timestamp_s": 299.0}, {"text": "And, this particular thing, will probably not have a solution, in the", "timestamp": "00:05:00,885", "timestamp_s": 300.0}, {"text": "LLMs because of LLMs, like I mentioned, are inherently non deterministic,", "timestamp": "00:05:06,636", "timestamp_s": 306.0}, {"text": "like that is, how the architecture is.", "timestamp": "00:05:11,539", "timestamp_s": 311.0}, {"text": "So this is something that needs to be solved, in your application.", "timestamp": "00:05:14,749", "timestamp_s": 314.0}, {"text": "you can\u0027t really wait for, like LLMs to get better and, start", "timestamp": "00:05:19,759", "timestamp_s": 319.0}, {"text": "providing better responses.", "timestamp": "00:05:23,829", "timestamp_s": 323.0}, {"text": "Like, I mean, they will definitely, get better and, reduce the error rate.", "timestamp": "00:05:24,819", "timestamp_s": 324.0}, {"text": "But, I think as an application developer, it\u0027s your responsibility", "timestamp": "00:05:28,969", "timestamp_s": 328.0}, {"text": "to take care of this issue within your application as well.", "timestamp": "00:05:32,089", "timestamp_s": 332.0}, {"text": "So what are your options?", "timestamp": "00:05:35,879", "timestamp_s": 335.0}, {"text": "The first thing that you should definitely try out is, retries and timeouts.", "timestamp": "00:05:38,414", "timestamp_s": 338.0}, {"text": "Now, these are not new concepts.", "timestamp": "00:05:43,584", "timestamp_s": 343.0}, {"text": "if you have worked in software development for a while now, you would know what", "timestamp": "00:05:45,954", "timestamp_s": 345.0}, {"text": "a retry is basically, when an API, gives you a wrong response, you try", "timestamp": "00:05:49,304", "timestamp_s": 349.0}, {"text": "it again with some, cool down period, or, maybe not depending on how the", "timestamp": "00:05:54,134", "timestamp_s": 354.0}, {"text": "rate limits are, retry is basically, you make an API call, the API fails,", "timestamp": "00:05:59,394", "timestamp_s": 359.0}, {"text": "you wait for a while, and then you retry it again, as simple as that.", "timestamp": "00:06:04,804", "timestamp_s": 364.0}, {"text": "Now, when you are developing, general applications, I think retries and", "timestamp": "00:06:09,269", "timestamp_s": 369.0}, {"text": "timeouts are something that, are not the first thing that you would implement", "timestamp": "00:06:15,649", "timestamp_s": 375.0}, {"text": "because, you just assume you just go with the assumption that the API response", "timestamp": "00:06:21,129", "timestamp_s": 381.0}, {"text": "rate is going to be fairly reasonable.", "timestamp": "00:06:25,889", "timestamp_s": 385.0}, {"text": "So.", "timestamp": "00:06:27,749", "timestamp_s": 387.0}, {"text": "they will most of the time work and, like not adding APIs will, not really", "timestamp": "00:06:28,779", "timestamp_s": 388.0}, {"text": "degrade the application performance.", "timestamp": "00:06:35,369", "timestamp_s": 395.0}, {"text": "unless like you are working with very critical, applications like,", "timestamp": "00:06:37,479", "timestamp_s": 397.0}, {"text": "something in finance or health where, the operation has to finish,", "timestamp": "00:06:41,479", "timestamp_s": 401.0}, {"text": "in which case you will, definitely start with the retries and timeouts.", "timestamp": "00:06:45,429", "timestamp_s": 405.0}, {"text": "But, in our general experience, if you\u0027re working with normal APIs, you", "timestamp": "00:06:48,469", "timestamp_s": 408.0}, {"text": "don\u0027t really worry about these things.", "timestamp": "00:06:51,739", "timestamp_s": 411.0}, {"text": "but because LLM APIs specifically have a higher error rate, retries", "timestamp": "00:06:53,639", "timestamp_s": 413.0}, {"text": "and timeouts are something that, need to be implemented from day one.", "timestamp": "00:06:58,249", "timestamp_s": 418.0}, {"text": "So, timeouts again, I think, I don\u0027t need to get into this.", "timestamp": "00:07:03,659", "timestamp_s": 423.0}, {"text": "A timeout is basically.", "timestamp": "00:07:06,399", "timestamp_s": 426.0}, {"text": "You make an API call and you wait for X seconds for the API to return.", "timestamp": "00:07:08,029", "timestamp_s": 428.0}, {"text": "If it doesn\u0027t return in X seconds for whatever reason, you dominate that", "timestamp": "00:07:14,179", "timestamp_s": 434.0}, {"text": "particular API and you try again.", "timestamp": "00:07:18,629", "timestamp_s": 438.0}, {"text": "this basically is protection against, the server, the API server being down and,", "timestamp": "00:07:20,479", "timestamp_s": 440.0}, {"text": "So like, if you don\u0027t do this, and if the API takes like a minute to respond,", "timestamp": "00:07:27,299", "timestamp_s": 447.0}, {"text": "you, your application is also stuck for a minute and, so are your users.", "timestamp": "00:07:31,279", "timestamp_s": 451.0}, {"text": "So a timeout is basically protection.", "timestamp": "00:07:35,589", "timestamp_s": 455.0}, {"text": "So that if an API doesn\u0027t return in like a reasonable amount of time, you cancel that", "timestamp": "00:07:39,354", "timestamp_s": 459.0}, {"text": "API call and you retry that API again.", "timestamp": "00:07:44,004", "timestamp_s": 464.0}, {"text": "that\u0027s where timeout comes into picture.", "timestamp": "00:07:46,694", "timestamp_s": 466.0}, {"text": "So, cool.", "timestamp": "00:07:48,744", "timestamp_s": 468.0}, {"text": "So how do you implement this into your application?", "timestamp": "00:07:49,474", "timestamp_s": 469.0}, {"text": "I would suggest don\u0027t, write the board for retries and timeouts from", "timestamp": "00:07:52,554", "timestamp_s": 472.0}, {"text": "scratch, because there are a bunch of, battle tested libraries available", "timestamp": "00:07:56,014", "timestamp_s": 476.0}, {"text": "in every language that you can use.", "timestamp": "00:08:00,034", "timestamp_s": 480.0}, {"text": "And, with a few lines of code, add these, behaviors to your application.", "timestamp": "00:08:02,964", "timestamp_s": 482.0}, {"text": "So let\u0027s look at a few examples.", "timestamp": "00:08:07,194", "timestamp_s": 487.0}, {"text": "the one that we actually use in production is, this one called", "timestamp": "00:08:10,284", "timestamp_s": 490.0}, {"text": "Tenacity by, it\u0027s a Python library.", "timestamp": "00:08:13,624", "timestamp_s": 493.0}, {"text": "And, it allows you to add retry to your functions by simply doing this.", "timestamp": "00:08:16,184", "timestamp_s": 496.0}, {"text": "you add a decorator, which is provided by the, by this particular library,", "timestamp": "00:08:21,544", "timestamp_s": 501.0}, {"text": "you add it to a function and this, this function will be retried.", "timestamp": "00:08:25,664", "timestamp_s": 505.0}, {"text": "whenever there is an exception or error in this particular function,", "timestamp": "00:08:29,864", "timestamp_s": 509.0}, {"text": "now, you\u0027d ideally want more control over, how many retries to do, how,", "timestamp": "00:08:34,364", "timestamp_s": 514.0}, {"text": "like how long to wait after, every retry and those kinds of things.", "timestamp": "00:08:40,884", "timestamp_s": 520.0}, {"text": "So, those all options are present in this library.", "timestamp": "00:08:44,754", "timestamp_s": 524.0}, {"text": "You can, give it stopping conditions where you want to stop after three retries.", "timestamp": "00:08:48,424", "timestamp_s": 528.0}, {"text": "You want to stop, stop after like.", "timestamp": "00:08:52,844", "timestamp_s": 532.0}, {"text": "retrying, you can add a wait time before every retry.", "timestamp": "00:08:54,404", "timestamp_s": 534.0}, {"text": "you can add a fixed wait time.", "timestamp": "00:08:59,854", "timestamp_s": 539.0}, {"text": "You can add a random wait time, all these, different kinds of, behaviors", "timestamp": "00:09:00,964", "timestamp_s": 540.0}, {"text": "can We added using this library, with like a few lines, of course.", "timestamp": "00:09:05,324", "timestamp_s": 545.0}, {"text": "So, if you are working in Python, this is our choice, has been working,", "timestamp": "00:09:09,089", "timestamp_s": 549.0}, {"text": "very well for us in production.", "timestamp": "00:09:14,029", "timestamp_s": 554.0}, {"text": "so, this is what we, have been using for a very long time.", "timestamp": "00:09:15,969", "timestamp_s": 555.0}, {"text": "So I would recommend this, if you\u0027re working in JS, there is a similar library", "timestamp": "00:09:18,529", "timestamp_s": 558.0}, {"text": "called retract, very conveniently.", "timestamp": "00:09:23,009", "timestamp_s": 563.0}, {"text": "that you can, that is available on NPM.", "timestamp": "00:09:25,039", "timestamp_s": 565.0}, {"text": "So, similar type of functionalities.", "timestamp": "00:09:27,279", "timestamp_s": 567.0}, {"text": "It gives you, retries and timeouts.", "timestamp": "00:09:29,669", "timestamp_s": 569.0}, {"text": "Oh, by the way, tenacity also has, timeout related decorators.", "timestamp": "00:09:32,349", "timestamp_s": 572.0}, {"text": "works the same way.", "timestamp": "00:09:36,149", "timestamp_s": 576.0}, {"text": "if you want to add a timeout to particular function, you just add", "timestamp": "00:09:36,859", "timestamp_s": 576.0}, {"text": "that decorator, specify the timeout.", "timestamp": "00:09:40,014", "timestamp_s": 580.0}, {"text": "yeah, back to.", "timestamp": "00:09:42,129", "timestamp_s": 582.0}, {"text": "This library, if you are working with the JS application, retry is, our choice.", "timestamp": "00:09:43,599", "timestamp_s": 583.0}, {"text": "the third option is, basically, a lot of people who are, developing", "timestamp": "00:09:49,049", "timestamp_s": 589.0}, {"text": "LLM applications are using these frameworks, LLM frameworks to handle, JS.", "timestamp": "00:09:54,109", "timestamp_s": 594.0}, {"text": "API calls, retries, and like a bunch of different things.", "timestamp": "00:09:58,479", "timestamp_s": 598.0}, {"text": "So the most famous LLM framework, framework, which a lot of", "timestamp": "00:10:02,009", "timestamp_s": 602.0}, {"text": "people are using is Lanchain.", "timestamp": "00:10:05,589", "timestamp_s": 605.0}, {"text": "And if you are working on top of Lanchain.", "timestamp": "00:10:07,699", "timestamp_s": 607.0}, {"text": "Lanshan provides you, basically, some, mechanism to retry, out of the box.", "timestamp": "00:10:10,739", "timestamp_s": 610.0}, {"text": "So, it\u0027s called, the retry output parser.", "timestamp": "00:10:16,719", "timestamp_s": 616.0}, {"text": "where, you can use this to make the LLM calls.", "timestamp": "00:10:20,504", "timestamp_s": 620.0}, {"text": "And whenever the LLM call fails, this parser will basically, handle retry", "timestamp": "00:10:22,954", "timestamp_s": 622.0}, {"text": "on your behalf, by passing, the prompt again and, also the previous output,", "timestamp": "00:10:28,224", "timestamp_s": 628.0}, {"text": "so that the, L-L-M-A-P has a better idea that, okay, the last out failed.", "timestamp": "00:10:32,534", "timestamp_s": 632.0}, {"text": "And, I\u0027m not supposed to, give this response again.", "timestamp": "00:10:35,984", "timestamp_s": 635.0}, {"text": "So if you\u0027re on , then it\u0027s already sorted out for you.", "timestamp": "00:10:38,864", "timestamp_s": 638.0}, {"text": "You use the retry out pass.", "timestamp": "00:10:41,584", "timestamp_s": 641.0}, {"text": "All right.", "timestamp": "00:10:43,404", "timestamp_s": 643.0}, {"text": "So this sorts out how to implement retries and timeouts.", "timestamp": "00:10:43,854", "timestamp_s": 643.0}, {"text": "The next most common reason for a failure or LLM inconsistency is when", "timestamp": "00:10:47,894", "timestamp_s": 647.0}, {"text": "you\u0027re working with structured outputs.", "timestamp": "00:10:54,064", "timestamp_s": 654.0}, {"text": "So when I say structured output, I mean, something like you asked.", "timestamp": "00:10:56,094", "timestamp_s": 656.0}, {"text": "The LLM to generate a JSON or XML CSV, even list arrays, those kinds of things.", "timestamp": "00:10:59,554", "timestamp_s": 659.0}, {"text": "So, whenever you\u0027re asking an LLM to generate a structured output, there", "timestamp": "00:11:04,974", "timestamp_s": 664.0}, {"text": "is a slight chance that, there\u0027ll be something wrong with that structure.", "timestamp": "00:11:08,994", "timestamp_s": 668.0}, {"text": "Maybe there are some fields missing, the extra fields, in case of like JSONs, XMLs,", "timestamp": "00:11:12,184", "timestamp_s": 672.0}, {"text": "there are brackets missing, might happen.", "timestamp": "00:11:16,844", "timestamp_s": 676.0}, {"text": "So how do you handle that?", "timestamp": "00:11:19,294", "timestamp_s": 679.0}, {"text": "The simplest way to do that is to, is to integrate a schema library instead", "timestamp": "00:11:22,094", "timestamp_s": 682.0}, {"text": "of like doing it on your own every time.", "timestamp": "00:11:28,024", "timestamp_s": 688.0}, {"text": "So, a schema library could be something like Pydantic.", "timestamp": "00:11:29,984", "timestamp_s": 689.0}, {"text": "And, this is what we use, in our production.", "timestamp": "00:11:33,414", "timestamp_s": 693.0}, {"text": "So, Pydantic is basically, the most.", "timestamp": "00:11:38,124", "timestamp_s": 698.0}, {"text": "commonly used data validation library in Python.", "timestamp": "00:11:40,904", "timestamp_s": 700.0}, {"text": "And what it does is it allows you to, create classes.", "timestamp": "00:11:43,904", "timestamp_s": 703.0}, {"text": "in which you describe the structure of your response, and then you use", "timestamp": "00:11:49,369", "timestamp_s": 709.0}, {"text": "this particular class to, check whether the LRM response fits,", "timestamp": "00:11:54,459", "timestamp_s": 714.0}, {"text": "this particular structure or not.", "timestamp": "00:11:59,319", "timestamp_s": 719.0}, {"text": "it will check for, fields, extra fields or less fields that will check for data", "timestamp": "00:12:01,099", "timestamp_s": 721.0}, {"text": "types, and a bunch of other options.", "timestamp": "00:12:06,499", "timestamp_s": 726.0}, {"text": "So on Python, just go for Pydantic, it is a tried and tested library.", "timestamp": "00:12:09,129", "timestamp_s": 729.0}, {"text": "and, it will make the data validation part when you\u0027re working with", "timestamp": "00:12:14,169", "timestamp_s": 734.0}, {"text": "structured outputs, hassle free.", "timestamp": "00:12:17,899", "timestamp_s": 737.0}, {"text": "similarly, if you\u0027re working with NPM, there\u0027s something called EOP, same", "timestamp": "00:12:19,619", "timestamp_s": 739.0}, {"text": "stuff as Pydantic, data validation.", "timestamp": "00:12:23,149", "timestamp_s": 743.0}, {"text": "you essentially, Define the shape of your output.", "timestamp": "00:12:25,809", "timestamp_s": 745.0}, {"text": "And, you basically use that shape, which is essentially a class, JS class,", "timestamp": "00:12:29,389", "timestamp_s": 749.0}, {"text": "or JS object to, check or enforce, the structure of your L responses.", "timestamp": "00:12:34,609", "timestamp_s": 754.0}, {"text": "and the idea is to use, these, these, data validation libraries,", "timestamp": "00:12:40,029", "timestamp_s": 760.0}, {"text": "along with, retries and timers.", "timestamp": "00:12:44,409", "timestamp_s": 764.0}, {"text": "So.", "timestamp": "00:12:46,199", "timestamp_s": 766.0}, {"text": "what you basically do is when you make an LLM API call, and you get", "timestamp": "00:12:47,049", "timestamp_s": 767.0}, {"text": "a response, you pass it through Pydantic or YUP or, whatever data", "timestamp": "00:12:50,389", "timestamp_s": 770.0}, {"text": "validation, library you are using.", "timestamp": "00:12:55,139", "timestamp_s": 775.0}, {"text": "And if you get an error, you use the retry, to like, like basically let the", "timestamp": "00:12:57,319", "timestamp_s": 777.0}, {"text": "LLM generate that structured output again.", "timestamp": "00:13:01,389", "timestamp_s": 781.0}, {"text": "most of the time, you will see that, a couple of retries sorts it out.", "timestamp": "00:13:03,809", "timestamp_s": 783.0}, {"text": "Like, it\u0027s not like every API call will fail in the same way.", "timestamp": "00:13:09,289", "timestamp_s": 789.0}, {"text": "So if let\u0027s say there are a few things missing, you can in your structured output", "timestamp": "00:13:12,849", "timestamp_s": 792.0}, {"text": "the first time when you do a retry, the next time you\u0027ll get the correct output.", "timestamp": "00:13:17,224", "timestamp_s": 797.0}, {"text": "but just, just as a, as a, general advice, if you see that there are", "timestamp": "00:13:21,384", "timestamp_s": 801.0}, {"text": "particular kind of issues happening again and again, you should mention", "timestamp": "00:13:27,604", "timestamp_s": 807.0}, {"text": "that, instruction in the prompt itself.", "timestamp": "00:13:31,304", "timestamp_s": 811.0}, {"text": "Because what happens is that, when you do retry, like, an API call, which.", "timestamp": "00:13:34,014", "timestamp_s": 814.0}, {"text": "Was supposed to take five seconds might end up taking 15 to 20", "timestamp": "00:13:39,314", "timestamp_s": 819.0}, {"text": "seconds and, it will make your, make your application feel laggy.", "timestamp": "00:13:42,124", "timestamp_s": 822.0}, {"text": "because, at, at the end of that API call, you\u0027re going to provide", "timestamp": "00:13:46,789", "timestamp_s": 826.0}, {"text": "some output to your users and, they\u0027re waiting for that output.", "timestamp": "00:13:50,229", "timestamp_s": 830.0}, {"text": "so if, you see that there are particular kind of, problems that are happening", "timestamp": "00:13:53,449", "timestamp_s": 833.0}, {"text": "again and again, like for example, if, if you are, generating JSON, using.", "timestamp": "00:13:57,869", "timestamp_s": 837.0}, {"text": "JSON using an LLM API.", "timestamp": "00:14:03,579", "timestamp_s": 843.0}, {"text": "And you\u0027ll see that, like the LLM is always using single quotes", "timestamp": "00:14:05,339", "timestamp_s": 845.0}, {"text": "instead of double quotes, which will generally cause issues.", "timestamp": "00:14:08,099", "timestamp_s": 848.0}, {"text": "you should specify that as an important point in your prompt so", "timestamp": "00:14:11,009", "timestamp_s": 851.0}, {"text": "that, you get the correct output in the first attempt itself.", "timestamp": "00:14:14,089", "timestamp_s": 854.0}, {"text": "this is just like, an additional level of check, but the idea is", "timestamp": "00:14:18,459", "timestamp_s": 858.0}, {"text": "that the first response should itself give you the correct output.", "timestamp": "00:14:21,989", "timestamp_s": 861.0}, {"text": "So.", "timestamp": "00:14:25,859", "timestamp_s": 865.0}, {"text": "anything that is, that is known should be mentioned in the prompt", "timestamp": "00:14:26,764", "timestamp_s": 866.0}, {"text": "as a special instruction, so that you don\u0027t keep retrying and you use", "timestamp": "00:14:30,014", "timestamp_s": 870.0}, {"text": "this and not waiting for an output.", "timestamp": "00:14:33,414", "timestamp_s": 873.0}, {"text": "one, one additional option worth, one special mention here is, the structured", "timestamp": "00:14:34,984", "timestamp_s": 874.0}, {"text": "output capabilities provided by OpenAI.", "timestamp": "00:14:40,504", "timestamp_s": 880.0}, {"text": "So, if you\u0027re using, GPT models, and OpenAI APIs, what you can do", "timestamp": "00:14:43,284", "timestamp_s": 883.0}, {"text": "is there is a response format field where you can specify a PyDynamic", "timestamp": "00:14:49,309", "timestamp_s": 889.0}, {"text": "class and, the OpenAI APIs themselves will, try to enforce the structure.", "timestamp": "00:14:52,529", "timestamp_s": 892.0}, {"text": "but this one problem here, which is if you want to switch out.", "timestamp": "00:14:59,249", "timestamp_s": 899.0}, {"text": "The model and use something else like Claude or Grok, then you have", "timestamp": "00:15:03,299", "timestamp_s": 903.0}, {"text": "basically, lost the structured output capabilities because those are not", "timestamp": "00:15:07,989", "timestamp_s": 907.0}, {"text": "available right now in other LLM APIs.", "timestamp": "00:15:12,369", "timestamp_s": 912.0}, {"text": "So, my suggestion is to just handle the, schema enforcing and checking", "timestamp": "00:15:15,899", "timestamp_s": 915.0}, {"text": "in your application itself so that like it\u0027s easy for you to switch out", "timestamp": "00:15:20,149", "timestamp_s": 920.0}, {"text": "the models and use different models.", "timestamp": "00:15:23,129", "timestamp_s": 923.0}, {"text": "That\u0027s all for handling LLM inconsistencies.", "timestamp": "00:15:25,759", "timestamp_s": 925.0}, {"text": "Two main things, retries and timeouts.", "timestamp": "00:15:29,979", "timestamp_s": 929.0}, {"text": "Use them from the start.", "timestamp": "00:15:32,509", "timestamp_s": 932.0}, {"text": "If you are working with structured outputs, use a data validation", "timestamp": "00:15:35,239", "timestamp_s": 935.0}, {"text": "library to check the structure.", "timestamp": "00:15:39,319", "timestamp_s": 939.0}, {"text": "You see the options here.", "timestamp": "00:15:42,099", "timestamp_s": 942.0}, {"text": "Any of these are good.", "timestamp": "00:15:43,629", "timestamp_s": 943.0}, {"text": "The next thing that you should, start thinking about is how to implement", "timestamp": "00:15:44,879", "timestamp_s": 944.0}, {"text": "streaming in your LLM application.", "timestamp": "00:15:48,959", "timestamp_s": 948.0}, {"text": "generally when you develop APIs, you, you implement, you implement normal request", "timestamp": "00:15:51,789", "timestamp_s": 951.0}, {"text": "response, like, you get an API call.", "timestamp": "00:15:56,849", "timestamp_s": 956.0}, {"text": "And, the server does some work and then you, then you return", "timestamp": "00:15:58,744", "timestamp_s": 958.0}, {"text": "the entire response in one go.", "timestamp": "00:16:02,764", "timestamp_s": 962.0}, {"text": "in, in case of, LLMs, what happens is sometimes it might take a long time", "timestamp": "00:16:04,734", "timestamp_s": 964.0}, {"text": "for the LLM to generate a response.", "timestamp": "00:16:10,654", "timestamp_s": 970.0}, {"text": "That\u0027s where streaming comes into picture.", "timestamp": "00:16:12,694", "timestamp_s": 972.0}, {"text": "Streaming, your responses allow you to start returning partial", "timestamp": "00:16:14,254", "timestamp_s": 974.0}, {"text": "responses, to the client.", "timestamp": "00:16:18,624", "timestamp_s": 978.0}, {"text": "Even when, the LLM is not done.", "timestamp": "00:16:20,894", "timestamp_s": 980.0}, {"text": "done with the generation.", "timestamp": "00:16:24,009", "timestamp_s": 984.0}, {"text": "So, let\u0027s look at why, streaming is so important while building NLM applications.", "timestamp": "00:16:25,749", "timestamp_s": 985.0}, {"text": "like I mentioned, NLMs might take long time for, for generation, to complete.", "timestamp": "00:16:32,729", "timestamp_s": 992.0}, {"text": "Now, when, when your user is, using your application, Most", "timestamp": "00:16:39,209", "timestamp_s": 999.0}, {"text": "users are very impatient.", "timestamp": "00:16:43,209", "timestamp_s": 1003.0}, {"text": "you, can\u0027t ask them to wait for, seconds.", "timestamp": "00:16:44,319", "timestamp_s": 1004.0}, {"text": "Like I\u0027m not even talking about minutes.", "timestamp": "00:16:48,649", "timestamp_s": 1008.0}, {"text": "if you have like a 10 second delay in showing the response,", "timestamp": "00:16:50,489", "timestamp_s": 1010.0}, {"text": "you might see a lot of drop off.", "timestamp": "00:16:53,619", "timestamp_s": 1013.0}, {"text": "so, and, and like most of the LLMs that you would work with would", "timestamp": "00:16:55,309", "timestamp_s": 1015.0}, {"text": "take like 5 to 10 seconds for even, you know, The simplest prompts.", "timestamp": "00:16:58,089", "timestamp_s": 1018.0}, {"text": "So how do you improve the UX?", "timestamp": "00:17:03,484", "timestamp_s": 1023.0}, {"text": "and make sure that your users don\u0027t drop off.", "timestamp": "00:17:07,474", "timestamp_s": 1027.0}, {"text": "that\u0027s where streaming comes into picture.", "timestamp": "00:17:10,184", "timestamp_s": 1030.0}, {"text": "what streaming allows you to do is, NLMs generate.", "timestamp": "00:17:12,204", "timestamp_s": 1032.0}, {"text": "The response is token by token, like they\u0027ll generate it word by word.", "timestamp": "00:17:16,264", "timestamp_s": 1036.0}, {"text": "And what streaming allows you to do is you don\u0027t need to wait", "timestamp": "00:17:20,124", "timestamp_s": 1040.0}, {"text": "for the LLM response or output.", "timestamp": "00:17:23,484", "timestamp_s": 1043.0}, {"text": "What you can do is as soon as it is done generating a few words,", "timestamp": "00:17:27,154", "timestamp_s": 1047.0}, {"text": "you can send them to the client and start displaying them on the UI.", "timestamp": "00:17:30,154", "timestamp_s": 1050.0}, {"text": "Or, whatever client you\u0027re using, in this way, the, the user, doesn\u0027t really feel", "timestamp": "00:17:33,839", "timestamp_s": 1053.0}, {"text": "the lag, that, LLM generation results in, what they see is that, as soon as", "timestamp": "00:17:39,759", "timestamp_s": 1059.0}, {"text": "they type out a prompt, immediately they start seeing some response", "timestamp": "00:17:45,779", "timestamp_s": 1065.0}, {"text": "and they can start reading it out.", "timestamp": "00:17:49,369", "timestamp_s": 1069.0}, {"text": "This is a very common pattern in any chat or LLM application", "timestamp": "00:17:51,024", "timestamp_s": 1071.0}, {"text": "that you would have used.", "timestamp": "00:17:56,174", "timestamp_s": 1076.0}, {"text": "As soon as you type something out or you do an action, you start", "timestamp": "00:17:57,454", "timestamp_s": 1077.0}, {"text": "seeing partial results on your UI.", "timestamp": "00:18:00,134", "timestamp_s": 1080.0}, {"text": "That is implemented through streaming.", "timestamp": "00:18:02,604", "timestamp_s": 1082.0}, {"text": "the most common or the most, used way.", "timestamp": "00:18:04,474", "timestamp_s": 1084.0}, {"text": "The other way to, to implement streaming is WebSockets.", "timestamp": "00:18:08,104", "timestamp_s": 1088.0}, {"text": "WebSockets allow you to send, generated tokens or vaults in real time.", "timestamp": "00:18:11,284", "timestamp_s": 1091.0}, {"text": "the connection is established, between client and server.", "timestamp": "00:18:16,694", "timestamp_s": 1096.0}, {"text": "And then, until the entire generation is completed or, as long as, the user", "timestamp": "00:18:20,354", "timestamp_s": 1100.0}, {"text": "is, live on the UI, you can just like reuse that connection to keep sending a", "timestamp": "00:18:25,244", "timestamp_s": 1105.0}, {"text": "response, as and when it gets generated.", "timestamp": "00:18:30,214", "timestamp_s": 1110.0}, {"text": "this is also a bidirectional, communication method.", "timestamp": "00:18:34,034", "timestamp_s": 1114.0}, {"text": "So you can use the same method to get some input from the client.", "timestamp": "00:18:37,054", "timestamp_s": 1117.0}, {"text": "Also, you know, one drawback of, WebSockets is that they need", "timestamp": "00:18:42,474", "timestamp_s": 1122.0}, {"text": "some custom, Implementation.", "timestamp": "00:18:46,684", "timestamp_s": 1126.0}, {"text": "you can\u0027t just take like your simple HTTP, REST server and,", "timestamp": "00:18:48,714", "timestamp_s": 1128.0}, {"text": "convert it into WebSockets.", "timestamp": "00:18:53,354", "timestamp_s": 1133.0}, {"text": "You\u0027ll need to redo your implementation, use new libraries, probably", "timestamp": "00:18:54,474", "timestamp_s": 1134.0}, {"text": "even new, use a new language.", "timestamp": "00:18:59,614", "timestamp_s": 1139.0}, {"text": "Like, for example, if you are working on Python, Python is not very, efficient,", "timestamp": "00:19:01,074", "timestamp_s": 1141.0}, {"text": "way for implementing WebSockets.", "timestamp": "00:19:07,064", "timestamp_s": 1147.0}, {"text": "You probably want to move to a different language, which handles, threads or", "timestamp": "00:19:08,574", "timestamp_s": 1148.0}, {"text": "multiprocessing in a much better way than Python, like Golang or Java or even C", "timestamp": "00:19:14,034", "timestamp_s": 1154.0}, {"text": "so generally WebSocket implementation.", "timestamp": "00:19:20,074", "timestamp_s": 1160.0}, {"text": "is a considerable effort.", "timestamp": "00:19:23,359", "timestamp_s": 1163.0}, {"text": "And, if all you want to do is stream LLM responses, it probably", "timestamp": "00:19:25,129", "timestamp_s": 1165.0}, {"text": "is not the best way to do it.", "timestamp": "00:19:29,069", "timestamp_s": 1169.0}, {"text": "there is another, solution for streaming, over HTTP, which", "timestamp": "00:19:30,999", "timestamp_s": 1170.0}, {"text": "is called server side events.", "timestamp": "00:19:36,439", "timestamp_s": 1176.0}, {"text": "which basically uses, your.", "timestamp": "00:19:38,319", "timestamp_s": 1178.0}, {"text": "your, server itself, like basically if you are on Python and you\u0027re using flask or", "timestamp": "00:19:42,039", "timestamp_s": 1182.0}, {"text": "fast API, you won\u0027t need to do a lot of changes to start streaming, using server", "timestamp": "00:19:46,109", "timestamp_s": 1186.0}, {"text": "sentiments, code wise or implementation wise, this is a minimal effort.", "timestamp": "00:19:51,869", "timestamp_s": 1191.0}, {"text": "what this essentially does is, it will use the same.", "timestamp": "00:19:55,439", "timestamp_s": 1195.0}, {"text": "HTTP, connection, which your STPA call utilizes, but instead of, sending the", "timestamp": "00:19:59,464", "timestamp_s": 1199.0}, {"text": "entire response in one shot, you can send the response in chunks and, on", "timestamp": "00:20:05,714", "timestamp_s": 1205.0}, {"text": "your client side, you can, receive it in chunks and start displaying.", "timestamp": "00:20:11,154", "timestamp_s": 1211.0}, {"text": "Now, this is a unidirectional, flow.", "timestamp": "00:20:14,904", "timestamp_s": 1214.0}, {"text": "It works exactly as a REST API call, but instead of, the client", "timestamp": "00:20:18,444", "timestamp_s": 1218.0}, {"text": "waiting for the entire response.", "timestamp": "00:20:22,334", "timestamp_s": 1222.0}, {"text": "to come, the client starts showing chunks that have been sent from", "timestamp": "00:20:25,909", "timestamp_s": 1225.0}, {"text": "server, using server sent events, implementation wise, it\u0027s very simple,", "timestamp": "00:20:30,469", "timestamp_s": 1230.0}, {"text": "like you, just need to maybe implement a generator, if you\u0027re using Python", "timestamp": "00:20:35,469", "timestamp_s": 1235.0}, {"text": "and, maybe add a couple of headers.", "timestamp": "00:20:40,539", "timestamp_s": 1240.0}, {"text": "we won\u0027t get into specific details because these are, things that you", "timestamp": "00:20:43,299", "timestamp_s": 1243.0}, {"text": "can easily Google and, find out.", "timestamp": "00:20:46,089", "timestamp_s": 1246.0}, {"text": "But, Our recommendation, if you want to implement streaming in your", "timestamp": "00:20:47,999", "timestamp_s": 1247.0}, {"text": "application and you already have a REST, setup ready on the backend, just", "timestamp": "00:20:51,809", "timestamp_s": 1251.0}, {"text": "go for server side events, much, faster implementation, also much easier to", "timestamp": "00:20:55,689", "timestamp_s": 1255.0}, {"text": "implement, WebSockets is a bit heavy and unless you have like a specific", "timestamp": "00:21:00,359", "timestamp_s": 1260.0}, {"text": "use case, For, WebSockets, I won\u0027t recommend, going that, that, on that path.", "timestamp": "00:21:05,029", "timestamp_s": 1265.0}, {"text": "Streaming is a good solution if, the particular task that an LLM", "timestamp": "00:21:11,799", "timestamp_s": 1271.0}, {"text": "is handling, gets over in a few seconds, like 5 to 10 seconds.", "timestamp": "00:21:17,329", "timestamp_s": 1277.0}, {"text": "but if your task is going to take minutes, streaming, Probably is not a good option.", "timestamp": "00:21:20,279", "timestamp_s": 1280.0}, {"text": "that\u0027s where background jobs come into picture.", "timestamp": "00:21:26,629", "timestamp_s": 1286.0}, {"text": "So, if you have a task which can be done in like five to 10 seconds,", "timestamp": "00:21:29,719", "timestamp_s": 1289.0}, {"text": "probably you streaming and, it\u0027s a good way to start showing, an", "timestamp": "00:21:34,089", "timestamp_s": 1294.0}, {"text": "output, to the user on client side.", "timestamp": "00:21:37,449", "timestamp_s": 1297.0}, {"text": "but if you have a task, which is going to take minutes.", "timestamp": "00:21:40,469", "timestamp_s": 1300.0}, {"text": "It is better to handle it asynchronously instead of synchronously", "timestamp": "00:21:43,519", "timestamp_s": 1303.0}, {"text": "in your backend server and background jobs help you do that.", "timestamp": "00:21:47,119", "timestamp_s": 1307.0}, {"text": "So what are these particular use cases where you might want to use", "timestamp": "00:21:51,259", "timestamp_s": 1311.0}, {"text": "background jobs instead of streaming?", "timestamp": "00:21:57,869", "timestamp_s": 1317.0}, {"text": "think of it this way.", "timestamp": "00:21:59,689", "timestamp_s": 1319.0}, {"text": "Let\u0027s say if you, if you are building something like an essay", "timestamp": "00:22:00,849", "timestamp_s": 1320.0}, {"text": "generator and you allow the user to enter essay topics in bulk.", "timestamp": "00:22:06,899", "timestamp_s": 1326.0}, {"text": "So if someone, gives you a single essay topic, probably, you\u0027ll finish", "timestamp": "00:22:11,864", "timestamp_s": 1331.0}, {"text": "the generation in a few seconds and, streaming is the way to go.", "timestamp": "00:22:15,824", "timestamp_s": 1335.0}, {"text": "But let\u0027s say if someone, gives you a hundred essay topics, for, for generation", "timestamp": "00:22:19,204", "timestamp_s": 1339.0}, {"text": "and that this particular task, doesn\u0027t matter how fast the LLM is, is going to", "timestamp": "00:22:24,624", "timestamp_s": 1344.0}, {"text": "take minutes at, at least a few minutes.", "timestamp": "00:22:28,464", "timestamp_s": 1348.0}, {"text": "And, if you use streaming for this, streaming will do all the", "timestamp": "00:22:31,524", "timestamp_s": 1351.0}, {"text": "work in your backend server.", "timestamp": "00:22:35,614", "timestamp_s": 1355.0}, {"text": "and, until this particular task is completed, which is", "timestamp": "00:22:37,714", "timestamp_s": 1357.0}, {"text": "going to be a few minutes.", "timestamp": "00:22:40,764", "timestamp_s": 1360.0}, {"text": "your backend server resources are going to get, hop or are going to be, tied up", "timestamp": "00:22:42,209", "timestamp_s": 1362.0}, {"text": "in this particular task, which is very inefficient because, like your backend", "timestamp": "00:22:49,019", "timestamp_s": 1369.0}, {"text": "servers job is basically take a request.", "timestamp": "00:22:53,629", "timestamp_s": 1373.0}, {"text": "Process it in a few seconds and send it back to the client.", "timestamp": "00:22:56,649", "timestamp_s": 1376.0}, {"text": "If you start doing things which take minutes, you will see that if you have", "timestamp": "00:23:00,639", "timestamp_s": 1380.0}, {"text": "a lot of concurrent users, your backend server will be busy and it will not", "timestamp": "00:23:05,849", "timestamp_s": 1385.0}, {"text": "be able to handle tasks which take a few seconds and your APIs will start", "timestamp": "00:23:10,789", "timestamp_s": 1390.0}, {"text": "getting blocked and your application performance will start to degrade.", "timestamp": "00:23:16,199", "timestamp_s": 1396.0}, {"text": "So what\u0027s the solution here?", "timestamp": "00:23:20,979", "timestamp_s": 1400.0}, {"text": "You, the solution is, you don\u0027t handle.", "timestamp": "00:23:23,499", "timestamp_s": 1403.0}, {"text": "Long running tasks in backend server synchronously.", "timestamp": "00:23:26,669", "timestamp_s": 1406.0}, {"text": "You handle them in background jobs asynchronously.", "timestamp": "00:23:30,719", "timestamp_s": 1410.0}, {"text": "Basically, when a user gives you a task, which is going to take", "timestamp": "00:23:33,849", "timestamp_s": 1413.0}, {"text": "minutes, you log it in a database, a background job will pick that task up.", "timestamp": "00:23:36,869", "timestamp_s": 1416.0}, {"text": "Till then, you tell the, you basically communicate to the user that, okay,", "timestamp": "00:23:41,689", "timestamp_s": 1421.0}, {"text": "this is going to take a few minutes.", "timestamp": "00:23:44,889", "timestamp_s": 1424.0}, {"text": "once.", "timestamp": "00:23:46,659", "timestamp_s": 1426.0}, {"text": "The task is completed.", "timestamp": "00:23:47,634", "timestamp_s": 1427.0}, {"text": "You will get a notification, probably as an email, or on slack.", "timestamp": "00:23:49,194", "timestamp_s": 1429.0}, {"text": "And, what do you do is you use a background job to, pick up the task,", "timestamp": "00:23:53,554", "timestamp_s": 1433.0}, {"text": "process it, And once it\u0027s ready, send out a notification, easiest", "timestamp": "00:23:57,834", "timestamp_s": 1437.0}, {"text": "way to implement this is cron jobs.", "timestamp": "00:24:01,999", "timestamp_s": 1441.0}, {"text": "cron jobs have been here for, I don\u0027t know, for a very long time.", "timestamp": "00:24:04,349", "timestamp_s": 1444.0}, {"text": "very easy to implement, on any Unix based, server, which is", "timestamp": "00:24:08,339", "timestamp_s": 1448.0}, {"text": "probably, what will be used in most of, production backend servers.", "timestamp": "00:24:13,529", "timestamp_s": 1453.0}, {"text": "all you need to do is set up a cron job, which does the processing.", "timestamp": "00:24:17,299", "timestamp_s": 1457.0}, {"text": "and the cron job runs every few minutes, checks the database", "timestamp": "00:24:21,689", "timestamp_s": 1461.0}, {"text": "if there are any pending tasks.", "timestamp": "00:24:24,469", "timestamp_s": 1464.0}, {"text": "now when your user.", "timestamp": "00:24:26,149", "timestamp_s": 1466.0}, {"text": "comes to you with, with a task, you just put it in a DB and, market as pending.", "timestamp": "00:24:27,914", "timestamp_s": 1467.0}, {"text": "when the cron job wakes up in a few minutes, it will check for any pending", "timestamp": "00:24:35,304", "timestamp_s": 1475.0}, {"text": "tasks and start the processing.", "timestamp": "00:24:38,594", "timestamp_s": 1478.0}, {"text": "And, on the US side, you can probably, implement some sort of polling.", "timestamp": "00:24:40,814", "timestamp_s": 1480.0}, {"text": "To check if the task is completed or not.", "timestamp": "00:24:45,214", "timestamp_s": 1485.0}, {"text": "And once it is completed, you can display that on the UI.", "timestamp": "00:24:46,944", "timestamp_s": 1486.0}, {"text": "But, this is an optional thing.", "timestamp": "00:24:50,154", "timestamp_s": 1490.0}, {"text": "Ideally, if you\u0027re using background jobs, you should also, sorry, you should also,", "timestamp": "00:24:51,334", "timestamp_s": 1491.0}, {"text": "separately communicate, that the task is completed with the user, because,", "timestamp": "00:24:57,034", "timestamp_s": 1497.0}, {"text": "the general, idea is that, when you, when, when a task is going to take a few", "timestamp": "00:25:01,294", "timestamp_s": 1501.0}, {"text": "minutes, your users will probably come to your platform, submit that task and", "timestamp": "00:25:05,054", "timestamp_s": 1505.0}, {"text": "they will move away from your platform.", "timestamp": "00:25:09,804", "timestamp_s": 1509.0}, {"text": "So they\u0027re not looking at, the UI of your application.", "timestamp": "00:25:11,794", "timestamp_s": 1511.0}, {"text": "So you should probably communicate that the task is completed through", "timestamp": "00:25:15,014", "timestamp_s": 1515.0}, {"text": "an email or a Slack notification.", "timestamp": "00:25:19,154", "timestamp_s": 1519.0}, {"text": "so that the users who have moved away from, the, you also know that, okay, that,", "timestamp": "00:25:21,534", "timestamp_s": 1521.0}, {"text": "that, that generation has been completed.", "timestamp": "00:25:25,304", "timestamp_s": 1525.0}, {"text": "this works very well, minimal setup, nothing new that", "timestamp": "00:25:28,764", "timestamp_s": 1528.0}, {"text": "you\u0027ll probably need to learn.", "timestamp": "00:25:31,894", "timestamp_s": 1531.0}, {"text": "Nothing new that you need to install, for the initial stages of your LLM", "timestamp": "00:25:33,014", "timestamp_s": 1533.0}, {"text": "application, just go for a cron job.", "timestamp": "00:25:37,394", "timestamp_s": 1537.0}, {"text": "what happens is that as your, application grows, you\u0027ll probably need to scale this.", "timestamp": "00:25:39,684", "timestamp_s": 1539.0}, {"text": "Now, if you run multiple cron jobs.", "timestamp": "00:25:46,684", "timestamp_s": 1546.0}, {"text": "you need to handle which cron job, picks up which task you need to implement some", "timestamp": "00:25:48,824", "timestamp_s": 1548.0}, {"text": "sort of, distributed locking and, all those complexities come into picture.", "timestamp": "00:25:53,784", "timestamp_s": 1553.0}, {"text": "Basically, cron jobs are good for the initial stages, but, like", "timestamp": "00:25:58,414", "timestamp_s": 1558.0}, {"text": "we also started with cron jobs.", "timestamp": "00:26:03,544", "timestamp_s": 1563.0}, {"text": "we still use cron jobs for some simple tasks, but there will be", "timestamp": "00:26:05,244", "timestamp_s": 1565.0}, {"text": "a stage, when you\u0027ll need to move away from cron jobs for scalability.", "timestamp": "00:26:08,904", "timestamp_s": 1568.0}, {"text": "and for, better retrying mechanisms, that\u0027s where task", "timestamp": "00:26:13,269", "timestamp_s": 1573.0}, {"text": "queues come into picture.", "timestamp": "00:26:17,789", "timestamp_s": 1577.0}, {"text": "So basically think of task queues as cron jobs with like more intelligence, where", "timestamp": "00:26:19,269", "timestamp_s": 1579.0}, {"text": "all the, task management that needs to be done, is handled by the task queue itself.", "timestamp": "00:26:24,639", "timestamp_s": 1584.0}, {"text": "when I say task management, on a very high level, what I It means is that,", "timestamp": "00:26:30,369", "timestamp_s": 1590.0}, {"text": "you submit a task to the task queue.", "timestamp": "00:26:35,454", "timestamp_s": 1595.0}, {"text": "generally a task queue is backed by some storage like Redis or some other cache.", "timestamp": "00:26:37,464", "timestamp_s": 1597.0}, {"text": "the task is stored over there and then the task queue handles, basically", "timestamp": "00:26:42,894", "timestamp_s": 1602.0}, {"text": "a task queue will have a bunch of workers running and, the, the task", "timestamp": "00:26:47,544", "timestamp_s": 1607.0}, {"text": "queue will then handle, how to allocate that work to which worker based on", "timestamp": "00:26:51,154", "timestamp_s": 1611.0}, {"text": "like a bunch of different mechanisms.", "timestamp": "00:26:56,814", "timestamp_s": 1616.0}, {"text": "Like you can have.", "timestamp": "00:26:58,264", "timestamp_s": 1618.0}, {"text": "Yeah.", "timestamp": "00:26:59,074", "timestamp_s": 1619.0}, {"text": "priority queues, you can have a bunch of different retry", "timestamp": "00:26:59,514", "timestamp_s": 1619.0}, {"text": "mechanisms, and all those things.", "timestamp": "00:27:02,824", "timestamp_s": 1622.0}, {"text": "So, two good things about using task queues.", "timestamp": "00:27:05,124", "timestamp_s": 1625.0}, {"text": "task queues are much easier to scale.", "timestamp": "00:27:07,934", "timestamp_s": 1627.0}, {"text": "in a cron job, if you go from one to two to 10 cron jobs, you have to", "timestamp": "00:27:09,704", "timestamp_s": 1629.0}, {"text": "handle, A bunch of, locking related stuff yourself, in task queues,", "timestamp": "00:27:13,086", "timestamp_s": 1633.0}, {"text": "it\u0027s already, implemented for you.", "timestamp": "00:27:17,007", "timestamp_s": 1637.0}, {"text": "So all you can do is increase the number of workers in a task queue.", "timestamp": "00:27:19,027", "timestamp_s": 1639.0}, {"text": "And if you start getting more, tasks or workload, the, you can", "timestamp": "00:27:22,807", "timestamp_s": 1642.0}, {"text": "just like, it\u0027s as easy as just changing the number on a dashboard,", "timestamp": "00:27:28,677", "timestamp_s": 1648.0}, {"text": "to increase the number of workers.", "timestamp": "00:27:31,985", "timestamp_s": 1651.0}, {"text": "again, like all the, additional handling of race conditions, retries,", "timestamp": "00:27:33,775", "timestamp_s": 1653.0}, {"text": "timeouts, it\u0027s already taken care of.", "timestamp": "00:27:38,525", "timestamp_s": 1658.0}, {"text": "All you need to do is, provide some configuration.", "timestamp": "00:27:40,625", "timestamp_s": 1660.0}, {"text": "you also get better monitoring with task queues.", "timestamp": "00:27:43,605", "timestamp_s": 1663.0}, {"text": "you, every task queue comes with some sort of, monitoring mechanism", "timestamp": "00:27:45,825", "timestamp_s": 1665.0}, {"text": "or a dashboard where you can see what are the tasks currently running,", "timestamp": "00:27:51,015", "timestamp_s": 1671.0}, {"text": "how much resources there are.", "timestamp": "00:27:55,325", "timestamp_s": 1675.0}, {"text": "Eating up, which tasks are failing, start or restart tasks", "timestamp": "00:27:56,845", "timestamp_s": 1676.0}, {"text": "and all those kinds of things.", "timestamp": "00:28:00,765", "timestamp_s": 1680.0}, {"text": "So, once you start scaling your application, go for task queues.", "timestamp": "00:28:02,305", "timestamp_s": 1682.0}, {"text": "The task queue that we use in our production is called RQ,", "timestamp": "00:28:06,495", "timestamp_s": 1686.0}, {"text": "which stands for Redis Queue.", "timestamp": "00:28:11,995", "timestamp_s": 1691.0}, {"text": "And, as the name suggests, it\u0027s backed by Redis and it\u0027s a very simple,", "timestamp": "00:28:14,030", "timestamp_s": 1694.0}, {"text": "library for queuing and processing background jobs with workers.", "timestamp": "00:28:19,760", "timestamp_s": 1699.0}, {"text": "very easy setup, hardly takes 15 minutes to set it up.", "timestamp": "00:28:24,160", "timestamp_s": 1704.0}, {"text": "If you already have a Redis, you don\u0027t even need to, set, set up a Redis.", "timestamp": "00:28:27,320", "timestamp_s": 1707.0}, {"text": "for RQ and, very simple, mechanism for queuing and processing.", "timestamp": "00:28:32,355", "timestamp_s": 1712.0}, {"text": "All you need to do is create a queue, provide a red connection so that,", "timestamp": "00:28:39,345", "timestamp_s": 1719.0}, {"text": "it has a place to store the tasks.", "timestamp": "00:28:43,005", "timestamp_s": 1723.0}, {"text": "when you get a task queue nq, you can, and, this is basically a function", "timestamp": "00:28:45,415", "timestamp_s": 1725.0}, {"text": "which is going to get called in the worker to process your tasks.", "timestamp": "00:28:49,635", "timestamp_s": 1729.0}, {"text": "So, it\u0027s this simple and you can also provide some arguments for that function.", "timestamp": "00:28:53,125", "timestamp_s": 1733.0}, {"text": "And", "timestamp": "00:28:57,330", "timestamp_s": 1737.0}, {"text": "the worker.", "timestamp": "00:28:58,700", "timestamp_s": 1738.0}, {"text": "For the worker, you just need to start it like this, on your command line.", "timestamp": "00:29:00,320", "timestamp_s": 1740.0}, {"text": "And, it consumes tasks from Redis and, process them.", "timestamp": "00:29:04,770", "timestamp_s": 1744.0}, {"text": "If you, want to increase the number of workers, you just like, start 10", "timestamp": "00:29:08,380", "timestamp_s": 1748.0}, {"text": "different workers, connect them to the same Redis, and, RQ will itself", "timestamp": "00:29:12,990", "timestamp_s": 1752.0}, {"text": "handle all the, all the complexities.", "timestamp": "00:29:17,840", "timestamp_s": 1757.0}, {"text": "of, managing which worker gets what task, and all those kinds of things.", "timestamp": "00:29:21,080", "timestamp_s": 1761.0}, {"text": "So, if you\u0027re on Python, RQ is the way to go.", "timestamp": "00:29:25,590", "timestamp_s": 1765.0}, {"text": "Celery provides you with a similar, functionality, but we just found", "timestamp": "00:29:30,090", "timestamp_s": 1770.0}, {"text": "that, there were a bunch of things in celery, which we did not really need.", "timestamp": "00:29:35,440", "timestamp_s": 1775.0}, {"text": "and it seemed like an overkill.", "timestamp": "00:29:39,600", "timestamp_s": 1779.0}, {"text": "so we decided to go with RQ, which was much simpler to set up on our end.", "timestamp": "00:29:41,405", "timestamp_s": 1781.0}, {"text": "Prompted at what inputs are not working, what models are working, what models", "timestamp": "00:29:45,235", "timestamp_s": 1785.0}, {"text": "are not working and, things like that.", "timestamp": "00:29:48,105", "timestamp_s": 1788.0}, {"text": "So, if you, if you want an analogy, you can think of evals as unit testing.", "timestamp": "00:29:50,085", "timestamp_s": 1790.0}, {"text": "So, think of it as unit testing for your prompts.", "timestamp": "00:29:56,745", "timestamp_s": 1796.0}, {"text": "So this allows you to take a prompt template and individually", "timestamp": "00:29:59,725", "timestamp_s": 1799.0}, {"text": "just test out that template with a bunch of different, Values.", "timestamp": "00:30:04,795", "timestamp_s": 1804.0}, {"text": "and, you can, there are, there are a bunch of, reasons why you should ideally,", "timestamp": "00:30:08,475", "timestamp_s": 1808.0}, {"text": "use evals with your prompt templates.", "timestamp": "00:30:14,475", "timestamp_s": 1814.0}, {"text": "one, it allows you to just test out the prompts in isolation,", "timestamp": "00:30:16,985", "timestamp_s": 1816.0}, {"text": "which makes it very fast.", "timestamp": "00:30:20,235", "timestamp_s": 1820.0}, {"text": "the same way unit tests are fast, because you are just checking one function", "timestamp": "00:30:21,255", "timestamp_s": 1821.0}, {"text": "against different types of inputs.", "timestamp": "00:30:24,495", "timestamp_s": 1824.0}, {"text": "using prompts, using, sorry, I\u0027m sorry.", "timestamp": "00:30:26,285", "timestamp_s": 1826.0}, {"text": "using evals, You will be able to figure out different things like which", "timestamp": "00:30:28,345", "timestamp_s": 1828.0}, {"text": "input works, which input doesn\u0027t work, which model works for a particular", "timestamp": "00:30:33,905", "timestamp_s": 1833.0}, {"text": "task, which model does not work.", "timestamp": "00:30:37,845", "timestamp_s": 1837.0}, {"text": "you\u0027ll be able to compare, costs of different models for different", "timestamp": "00:30:39,855", "timestamp_s": 1839.0}, {"text": "types of inputs and so on.", "timestamp": "00:30:43,465", "timestamp_s": 1843.0}, {"text": "an additional, benefit of using evals is that.", "timestamp": "00:30:45,735", "timestamp_s": 1845.0}, {"text": "You can directly, integrate them with your CI CD pipeline so that,", "timestamp": "00:30:50,045", "timestamp_s": 1850.0}, {"text": "you don\u0027t need to manually keep checking before every release.", "timestamp": "00:30:55,075", "timestamp_s": 1855.0}, {"text": "If your prompts are still working the way they\u0027re working just like unit test.", "timestamp": "00:30:57,815", "timestamp_s": 1857.0}, {"text": "You just, hook it up to your CI CD pipeline.", "timestamp": "00:31:01,865", "timestamp_s": 1861.0}, {"text": "And, before every commit or, I\u0027m sorry, after every commit or, after", "timestamp": "00:31:04,395", "timestamp_s": 1864.0}, {"text": "every build, you, straight up run the evals and, similar to, assertions in", "timestamp": "00:31:08,345", "timestamp_s": 1868.0}, {"text": "unit as evals also have assertions or checks where you can check the", "timestamp": "00:31:13,955", "timestamp_s": 1873.0}, {"text": "response, and, specify whether it is as expected or not as expected.", "timestamp": "00:31:18,345", "timestamp_s": 1878.0}, {"text": "And pass or fail an eval.", "timestamp": "00:31:23,800", "timestamp_s": 1883.0}, {"text": "So, that\u0027s how on a very high level evals work.", "timestamp": "00:31:25,570", "timestamp_s": 1885.0}, {"text": "we have tried out a bunch of different, eval libraries.", "timestamp": "00:31:30,510", "timestamp_s": 1890.0}, {"text": "the one we like the most is promptful.", "timestamp": "00:31:34,970", "timestamp_s": 1894.0}, {"text": "very easy to set up.", "timestamp": "00:31:37,720", "timestamp_s": 1897.0}, {"text": "simply works using YAML files, basically you, you create a YAML", "timestamp": "00:31:39,430", "timestamp_s": 1899.0}, {"text": "file where you specify your prompt template and you press specify a bunch", "timestamp": "00:31:46,000", "timestamp_s": 1906.0}, {"text": "of inputs, for that prompt template.", "timestamp": "00:31:50,150", "timestamp_s": 1910.0}, {"text": "And, using, so, Promfo is an open source, tool, so you can just like, straight up", "timestamp": "00:31:52,800", "timestamp_s": 1912.0}, {"text": "install it from NPM, run it in your CLI.", "timestamp": "00:31:58,680", "timestamp_s": 1918.0}, {"text": "and at the end of the event, you get a nice, graph like this.", "timestamp": "00:32:01,850", "timestamp_s": 1921.0}, {"text": "which will show you for different types of inputs, whether the", "timestamp": "00:32:06,705", "timestamp_s": 1926.0}, {"text": "output has passed the condition.", "timestamp": "00:32:11,175", "timestamp_s": 1931.0}, {"text": "it will also allow you to compare different, models and, there is", "timestamp": "00:32:13,415", "timestamp_s": 1933.0}, {"text": "some way to compare cost as well.", "timestamp": "00:32:18,665", "timestamp_s": 1938.0}, {"text": "I don\u0027t think they have displayed it here, but yeah, cost comparison", "timestamp": "00:32:20,445", "timestamp_s": 1940.0}, {"text": "is also something that you will get in the same dashboard.", "timestamp": "00:32:23,095", "timestamp_s": 1943.0}, {"text": "And, you can start off with the open source version of promptful, but they", "timestamp": "00:32:25,755", "timestamp_s": 1945.0}, {"text": "also have a cloud hosted version.", "timestamp": "00:32:30,175", "timestamp_s": 1950.0}, {"text": "So if you want more reliability or don\u0027t want to manage your own instance.", "timestamp": "00:32:31,945", "timestamp_s": 1951.0}, {"text": "that option is also available.", "timestamp": "00:32:35,970", "timestamp_s": 1955.0}, {"text": "before we end the stock, let\u0027s do a quick walkthrough of all the different", "timestamp": "00:32:37,970", "timestamp_s": 1957.0}, {"text": "foundational models, or foundational model APIs that are available for public use.", "timestamp": "00:32:41,840", "timestamp_s": 1961.0}, {"text": "the reason for doing this is basically, this landscape is changing very fast.", "timestamp": "00:32:46,980", "timestamp_s": 1966.0}, {"text": "So the last time you had gone over all the available models, so I\u0027m pretty sure that.", "timestamp": "00:32:51,860", "timestamp_s": 1971.0}, {"text": "By now, the list of models and also their comparisons have changed.", "timestamp": "00:32:58,475", "timestamp_s": 1978.0}, {"text": "Probably the models you thought are not that great have", "timestamp": "00:33:03,885", "timestamp_s": 1983.0}, {"text": "become very good and so on.", "timestamp": "00:33:07,815", "timestamp_s": 1987.0}, {"text": "So let\u0027s do a quick run through of all the available models.", "timestamp": "00:33:09,705", "timestamp_s": 1989.0}, {"text": "What are they good at?", "timestamp": "00:33:13,115", "timestamp_s": 1993.0}, {"text": "What are They\u0027re not good at what kind of use cases?", "timestamp": "00:33:13,855", "timestamp_s": 1993.0}, {"text": "you, what case, what kind of use cases work with a particular kind of model?", "timestamp": "00:33:16,490", "timestamp_s": 1996.0}, {"text": "let\u0027s start with the oldest player OpenAI.", "timestamp": "00:33:21,490", "timestamp_s": 2001.0}, {"text": "OpenAI has, three main families of models, which is GPT 4 0 4 O Mini, and,", "timestamp": "00:33:23,865", "timestamp_s": 2003.0}, {"text": "Owen, which are available for public use.", "timestamp": "00:33:29,840", "timestamp_s": 2009.0}, {"text": "I think they\u0027ve deprecated their three and 3.5 models.", "timestamp": "00:33:32,400", "timestamp_s": 2012.0}, {"text": "so these are the models that are available right now.", "timestamp": "00:33:35,210", "timestamp_s": 2015.0}, {"text": "If you don\u0027t know what to use, just go with OpenAI.", "timestamp": "00:33:38,450", "timestamp_s": 2018.0}, {"text": "These are the most versatile models.", "timestamp": "00:33:42,860", "timestamp_s": 2022.0}, {"text": "They work very well with a wide variety of tasks, within these models, between", "timestamp": "00:33:46,010", "timestamp_s": 2026.0}, {"text": "Foro and, Foro Mini, the difference is mainly, the trade off between,", "timestamp": "00:33:54,210", "timestamp_s": 2034.0}, {"text": "cost and latency versus accuracy.", "timestamp": "00:33:59,000", "timestamp_s": 2039.0}, {"text": "So if you have a complex task or something that requires a bit", "timestamp": "00:34:01,470", "timestamp_s": 2041.0}, {"text": "more of reasoning, go for Foro.", "timestamp": "00:34:05,730", "timestamp_s": 2045.0}, {"text": "if you are worried about cost, or if you\u0027re worried about, how", "timestamp": "00:34:08,329", "timestamp_s": 2048.0}, {"text": "fast the response is going to be, go for 4 O Mini, but, it will", "timestamp": "00:34:11,209", "timestamp_s": 2051.0}, {"text": "basically, give you lesser accuracy.", "timestamp": "00:34:14,689", "timestamp_s": 2054.0}, {"text": "O L is something that I have not tried out.", "timestamp": "00:34:17,739", "timestamp_s": 2057.0}, {"text": "these are supposed to be, open air flagship models.", "timestamp": "00:34:19,529", "timestamp_s": 2059.0}, {"text": "but from, What I\u0027ve heard, these are like fairly new.", "timestamp": "00:34:23,249", "timestamp_s": 2063.0}, {"text": "So, before you put it in production, maybe, test them out", "timestamp": "00:34:25,999", "timestamp_s": 2065.0}, {"text": "thoroughly, Foro and Foro Mini have been around for a while now.", "timestamp": "00:34:29,929", "timestamp_s": 2069.0}, {"text": "So I think, you should not see a lot of problems, with them.", "timestamp": "00:34:33,719", "timestamp_s": 2073.0}, {"text": "Also, like reliability wise, as, according to us, OpenAI APIs", "timestamp": "00:34:36,679", "timestamp_s": 2076.0}, {"text": "have been the most reliable.", "timestamp": "00:34:41,889", "timestamp_s": 2081.0}, {"text": "so you don\u0027t need to worry about downtime or, having to.", "timestamp": "00:34:43,429", "timestamp_s": 2083.0}, {"text": "handle switching models because, this provider is not working.", "timestamp": "00:34:48,864", "timestamp_s": 2088.0}, {"text": "The next provider is, and stopping.", "timestamp": "00:34:52,194", "timestamp_s": 2092.0}, {"text": "I think for a while, these guys were working mostly on", "timestamp": "00:34:56,814", "timestamp_s": 2096.0}, {"text": "the, chat, the, the, the.", "timestamp": "00:35:00,524", "timestamp_s": 2100.0}, {"text": "APIs were not publicly available as far as I know, but, I think in the", "timestamp": "00:35:02,759", "timestamp_s": 2102.0}, {"text": "last few months, I think that has changed, the APIs are available.", "timestamp": "00:35:07,329", "timestamp_s": 2107.0}, {"text": "You can just directly, and they\u0027re completely self serve.", "timestamp": "00:35:10,239", "timestamp_s": 2110.0}, {"text": "You can just directly go, on, anthropics, to Anthropx console", "timestamp": "00:35:12,619", "timestamp_s": 2112.0}, {"text": "and create an API key, load up some credit and get started with it.", "timestamp": "00:35:16,649", "timestamp_s": 2116.0}, {"text": "If you have any coding related use case, cloud APIs are your best choice.", "timestamp": "00:35:21,209", "timestamp_s": 2121.0}, {"text": "I think, as far as coding is concerned, coding as a particular task, cloud,", "timestamp": "00:35:26,629", "timestamp_s": 2126.0}, {"text": "works much better than, all the other models, which is also why you would have", "timestamp": "00:35:32,049", "timestamp_s": 2132.0}, {"text": "seen that, everyone is using cloud with, their, code editors as well, like cursor.", "timestamp": "00:35:36,119", "timestamp_s": 2136.0}, {"text": "so yeah, if code is what you want, work with Claude.", "timestamp": "00:35:41,449", "timestamp_s": 2141.0}, {"text": "Next up is Grok, not to be confused with XAI\u0027s Grok.", "timestamp": "00:35:44,939", "timestamp_s": 2144.0}, {"text": "So Grok is, essentially, a company that is building, special purpose chips.", "timestamp": "00:35:49,689", "timestamp_s": 2149.0}, {"text": "They call them LPUs, for running LLMs, which, makes,", "timestamp": "00:35:56,989", "timestamp_s": 2156.0}, {"text": "their inference time on LLMs.", "timestamp": "00:36:00,779", "timestamp_s": 2160.0}, {"text": "Very low, probably, even the inference cost.", "timestamp": "00:36:02,129", "timestamp_s": 2162.0}, {"text": "so if latency is what you\u0027re trying to optimize, tryout, grok, grok, cloud,", "timestamp": "00:36:05,189", "timestamp_s": 2165.0}, {"text": "which is their API, which are their, LLM APIs, they generally host, most of.", "timestamp": "00:36:12,599", "timestamp_s": 2172.0}, {"text": "Commonly used open source models.", "timestamp": "00:36:17,429", "timestamp_s": 2177.0}, {"text": "So you have Lama, Mixtel, Gemma available, apart from that, a bunch of other things.", "timestamp": "00:36:19,719", "timestamp_s": 2179.0}, {"text": "Latency wise, they are much faster than all the other model providers.", "timestamp": "00:36:24,459", "timestamp_s": 2184.0}, {"text": "So if you are optimizing for latency and these models work for", "timestamp": "00:36:31,709", "timestamp_s": 2191.0}, {"text": "your particular task, go for it.", "timestamp": "00:36:36,309", "timestamp_s": 2196.0}, {"text": "All right.", "timestamp": "00:36:38,609", "timestamp_s": 2198.0}, {"text": "So AWS, mainly works kind of like rock.", "timestamp": "00:36:39,059", "timestamp_s": 2199.0}, {"text": "They host a lot of open source models on, and along with that, I think", "timestamp": "00:36:43,999", "timestamp_s": 2203.0}, {"text": "they also have, their own models, which we have not tried out yet.", "timestamp": "00:36:48,369", "timestamp_s": 2208.0}, {"text": "but the biggest USP of using AWS, bedrock would be if you\u0027re already", "timestamp": "00:36:52,399", "timestamp_s": 2212.0}, {"text": "in the AWS ecosystem and you are, worried about, your sensitive data,", "timestamp": "00:36:58,779", "timestamp_s": 2218.0}, {"text": "getting out of your infra and you don\u0027t want to like, send it to open AI", "timestamp": "00:37:04,519", "timestamp_s": 2224.0}, {"text": "or cloud or any other model provider.", "timestamp": "00:37:08,449", "timestamp_s": 2228.0}, {"text": "in that case, Bedrock should be your choice.", "timestamp": "00:37:10,559", "timestamp_s": 2230.0}, {"text": "One good thing is Bedrock also hosts cloud APIs.", "timestamp": "00:37:13,259", "timestamp_s": 2233.0}, {"text": "so, the limits are lower, as far as I know, I think you\u0027ll need", "timestamp": "00:37:16,819", "timestamp_s": 2236.0}, {"text": "to talk to the support and, get your service quotas increased.", "timestamp": "00:37:21,499", "timestamp_s": 2241.0}, {"text": "But, if you are worried about, sensitive data and you\u0027re okay with cloud,", "timestamp": "00:37:24,029", "timestamp_s": 2244.0}, {"text": "Bedrock should work for you very well.", "timestamp": "00:37:28,549", "timestamp_s": 2248.0}, {"text": "and along with that, they also host Lama and Mixtel.", "timestamp": "00:37:30,321", "timestamp_s": 2250.0}, {"text": "And a few other, APIs, multimodal APIs.", "timestamp": "00:37:32,822", "timestamp_s": 2252.0}, {"text": "Azure is, the last time I checked Azure is hosting GPT models.", "timestamp": "00:37:36,072", "timestamp_s": 2256.0}, {"text": "separately, like, the hosting, which open AI does is separate from Azure.", "timestamp": "00:37:41,592", "timestamp_s": 2261.0}, {"text": "And, the last time we checked, Azure GPT APIs were a bit more faster than open air.", "timestamp": "00:37:45,702", "timestamp_s": 2265.0}, {"text": "So again, like, Oh, if you want to use open API APIs and you, want, a slightly", "timestamp": "00:37:51,642", "timestamp_s": 2271.0}, {"text": "better latency, try out Azure, but they\u0027ll make you fill a bunch of forms.", "timestamp": "00:37:57,972", "timestamp_s": 2277.0}, {"text": "I think these APIs or these models are not publicly available on Azure for everyone.", "timestamp": "00:38:02,442", "timestamp_s": 2282.0}, {"text": "GCP, I\u0027ve not tried out.", "timestamp": "00:38:07,262", "timestamp_s": 2287.0}, {"text": "so, again, like, I mean, I think the setup was a bit complex.", "timestamp": "00:38:09,072", "timestamp_s": 2289.0}, {"text": "So, we didn\u0027t get a chance to give it a try, but from what we\u0027ve heard, the", "timestamp": "00:38:12,022", "timestamp_s": 2292.0}, {"text": "developer experience is much better now.", "timestamp": "00:38:16,492", "timestamp_s": 2296.0}, {"text": "So someday we\u0027ll give it a try again.", "timestamp": "00:38:19,212", "timestamp_s": 2299.0}, {"text": "But GCP has a Gemini and the.", "timestamp": "00:38:21,682", "timestamp_s": 2301.0}, {"text": "Latest, the newest kid on the block is DeepSync, if you are active on", "timestamp": "00:38:25,382", "timestamp_s": 2305.0}, {"text": "Twitter, you would have already heard, about DeepSync, APIs, from", "timestamp": "00:38:30,492", "timestamp_s": 2310.0}, {"text": "the chatter, it seems as if they are at par with own APIs, again,", "timestamp": "00:38:35,442", "timestamp_s": 2315.0}, {"text": "haven\u0027t tried it out, give it a try.", "timestamp": "00:38:40,942", "timestamp_s": 2320.0}, {"text": "one concern could be, the hosting, which is in China,", "timestamp": "00:38:43,077", "timestamp_s": 2323.0}, {"text": "but, definitely give it a try.", "timestamp": "00:38:46,277", "timestamp_s": 2326.0}, {"text": "probably you might find it, to be a good fit for your use case.", "timestamp": "00:38:48,067", "timestamp_s": 2328.0}, {"text": "And, one more thing, deep seeks models are also open source, so", "timestamp": "00:38:51,997", "timestamp_s": 2331.0}, {"text": "you can host them on your own.", "timestamp": "00:38:56,047", "timestamp_s": 2336.0}, {"text": "And that\u0027s all from me.", "timestamp": "00:38:57,517", "timestamp_s": 2337.0}, {"text": "I hope you find the information shared in the stock useful, and it speeds up", "timestamp": "00:38:58,967", "timestamp_s": 2338.0}, {"text": "your development process when you\u0027re building LLM applications and, AI agents.", "timestamp": "00:39:05,157", "timestamp_s": 2345.0}, {"text": "if you have any, queries or if you want to, talk more", "timestamp": "00:39:10,067", "timestamp_s": 2350.0}, {"text": "about this, drop us an email.", "timestamp": "00:39:13,297", "timestamp_s": 2353.0}, {"text": "You can find our email.", "timestamp": "00:39:14,707", "timestamp_s": 2354.0}, {"text": "on Kusho AI\u0027s landing page, or just, send me a message on LinkedIn.", "timestamp": "00:39:16,287", "timestamp_s": 2356.0}, {"text": "I\u0027m happy to chat about this and, go build something awesome.", "timestamp": "00:39:21,587", "timestamp_s": 2361.0}, {"text": "Bye.", "timestamp": "00:39:25,737", "timestamp_s": 2365.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'vI1-ATx7tls',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Building AI Applications with LLMs: Practical Tips and Best Practices
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>Understand the problems and trade-offs you&rsquo;ll encounter when building an AI application on top of LLMs based on my learnings of building KushoAI, an AI agent used by 5000+ engineers to make API testing completely autonomous by leveraging LLMs.</p>
<!-- End Text -->
          </div>

          
          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./srt/python2025_Sourabh_Gawande.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:00,010'); seek(0.0)">
              Hello, everyone.
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:00,780'); seek(0.0)">
              My name is Saurabh.
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:01,620'); seek(1.0)">
              I am the co founder and CTO of Kusho AI.
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:04,010'); seek(4.0)">
              And today I'm going to talk about practical tips for building AI
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:07,840'); seek(7.0)">
              applications or AI agents using LLMs.
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:11,419'); seek(11.0)">
              At Kusho AI, we have been, working on building AI applications.
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:15,899'); seek(15.0)">
              And, agents for the last, 18 months.
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:18,749'); seek(18.0)">
              And, during our journey, we have identified a bunch of unique
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:23,919'); seek(23.0)">
              problems that people generally face.
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:26,899'); seek(26.0)">
              And, we have also faced, those same problems specifically while
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:00:30,749'); seek(30.0)">
              building, applications using LLMs.
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:00:33,329'); seek(33.0)">
              And, the, agenda for today's talk is that, we want to, educate devs
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:00:39,994'); seek(39.0)">
              about these problems, so that.
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:00:42,874'); seek(42.0)">
              when they are building apps on top of LLMs, they are aware of these
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:00:46,694'); seek(46.0)">
              problems and, also discuss what are, the solutions that work for us
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:00:50,884'); seek(50.0)">
              and, the dev tools or tooling that we use to solve these problems.
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:00:54,254'); seek(54.0)">
              and, by, by sharing this information, we, we want to save time, when
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:00:58,815'); seek(58.0)">
              devs are, building applications on top of LLMs for the first time.
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:03,294'); seek(63.0)">
              The first thing that you will need to solve when you start building
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:06,154'); seek(66.0)">
              apps on top of, LLMs is, how to handle LLM inconsistencies.
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:11,259'); seek(71.0)">
              you, if you have some experience building, applications using
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:15,019'); seek(75.0)">
              normal APIs, you would have seen that they don't, fail that often.
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:18,219'); seek(78.0)">
              So, by building, general applications, you don't really worry about,
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:22,429'); seek(82.0)">
              inconsistencies or failures, that much.
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:25,359'); seek(85.0)">
              if an API fails, you just let the API fail.
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:27,929'); seek(87.0)">
              And, the user, when they refresh their page, you make another API call
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:01:31,549'); seek(91.0)">
              and it will most probably succeed.
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:01:33,069'); seek(93.0)">
              but in case of LLMs.
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:01:34,849'); seek(94.0)">
              this is the first thing that you'll probably need to solve, when you're
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:01:37,809'); seek(97.0)">
              actually building an application because, LLMs have a much higher
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:01:41,749'); seek(101.0)">
              error rate, than your normal APIs.
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:01:44,424'); seek(104.0)">
              And unless you solve this particular thing, your application will have
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:01:48,674'); seek(108.0)">
              a terrible UX, or user experience.
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:01:52,324'); seek(112.0)">
              Because, in your application, you'll generally use the LLM
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:01:55,254'); seek(115.0)">
              response, somewhere else.
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:01:57,064'); seek(117.0)">
              And every time the LLM gives you a wrong output, your application will also crash.
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:02,434'); seek(122.0)">
              So, this is the first, problem that, you should solve, while
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:07,854'); seek(127.0)">
              building LLM, applications.
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:10,274'); seek(130.0)">
              Now, before we get into, like why, how to solve this particular problem, let's
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:16,984'); seek(136.0)">
              just talk about why this even occurs, in, in these, in these specific applications.
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:22,604'); seek(142.0)">
              so like I mentioned earlier, if you are, if you are working with,
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:02:25,744'); seek(145.0)">
              normal APIs, you generally don't worry, much about the error rate.
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:02:31,379'); seek(151.0)">
              in like fairly stable, APIs, but even the most stable LLMs give you a much
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:02:36,170'); seek(156.0)">
              higher error rate, than your normal APIs.
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:02:38,710'); seek(158.0)">
              And the reason for this is LLMs are inherently non deterministic.
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:02:44,930'); seek(164.0)">
              so what do you mean by that?
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:02:46,380'); seek(166.0)">
              so if you, if you, if you look at an LLM under the hood, they are essentially
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:02:50,260'); seek(170.0)">
              statistical machines, that produce token after token based on, the input
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:02:56,180'); seek(176.0)">
              prompt and whatever tokens have been generated previously, Statistical
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:02:59,860'); seek(179.0)">
              machines are basically probabilistic.
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:02,650'); seek(182.0)">
              And as soon as you bring probability into software, you're going to
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:06,390'); seek(186.0)">
              get something non deterministic.
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:08,240'); seek(188.0)">
              Now, what do we mean by non deterministic?
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:11,520'); seek(191.0)">
              you basically, we'll get a different output for the same input.
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:15,400'); seek(195.0)">
              Every time, You, ask LLM for a response?
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:19,020'); seek(199.0)">
              you could, I, and I'm, I'm pretty sure like you are, you would have
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:23,450'); seek(203.0)">
              seen this problem while using, all the different, chat bots that
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:03:26,820'); seek(206.0)">
              are available, like chat, GPT or.
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:03:29,270'); seek(209.0)">
              the deep seek or cloud, chat, you would have noticed that, every time
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:03:33,600'); seek(213.0)">
              you give, give, given input, for the same input, every time you hit
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:03:37,460'); seek(217.0)">
              retry, you'll get a different output.
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:03:39,120'); seek(219.0)">
              that's the same thing that will happen with, the LLM
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:03:41,370'); seek(221.0)">
              responses in your applications.
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:03:43,530'); seek(223.0)">
              you most of the time don't, have a lot of control or, will you
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:03:47,360'); seek(227.0)">
              get the exact output or not?
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:03:49,100'); seek(229.0)">
              Now, because of this particular problem, which is, LLMs being non deterministic,
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:03:55,680'); seek(235.0)">
              every time you give it an input, you'll not always get the response that you want.
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:00,360'); seek(240.0)">
              Like, for example, for example, if you ask an LLM API to generate,
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:06,150'); seek(246.0)">
              JSON, which is a structured output.
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:08,470'); seek(248.0)">
              You might get more fields than, what you asked for.
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:11,870'); seek(251.0)">
              Sometimes you might get less fields.
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:13,670'); seek(253.0)">
              sometimes you might have, a bracket missing.
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:04:16,960'); seek(256.0)">
              Based on what we have seen, if, if you have like a normal stable API.
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:04:22,375'); seek(262.0)">
              you will see an error rate of something like 0.
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:04:26,555'); seek(266.0)">
              1%, but if you are working with an LLM, even the most stable, the LLMs,
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:04:31,835'); seek(271.0)">
              which have been, here for the longest amount of time, they'll give you an
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:04:35,375'); seek(275.0)">
              error rate of, something like one to 5 percent based on what kind of
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:04:38,695'); seek(278.0)">
              task you're asking it to perform.
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:04:40,415'); seek(280.0)">
              And if you're working with chained LLM responses, like, basically you.
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:04:45,245'); seek(285.0)">
              provide, the LLM API with a prompt, you take that response, and then
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:04:49,685'); seek(289.0)">
              you provide it with another prompt, using the response, that you got
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:04:54,175'); seek(294.0)">
              earlier, this is basically a chained, LLM responses, you will see that
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:04:59,435'); seek(299.0)">
              your error rate gets compounded.
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:00,885'); seek(300.0)">
              And, this particular thing, will probably not have a solution, in the
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:06,636'); seek(306.0)">
              LLMs because of LLMs, like I mentioned, are inherently non deterministic,
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:11,539'); seek(311.0)">
              like that is, how the architecture is.
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:14,749'); seek(314.0)">
              So this is something that needs to be solved, in your application.
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:19,759'); seek(319.0)">
              you can't really wait for, like LLMs to get better and, start
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:05:23,829'); seek(323.0)">
              providing better responses.
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:05:24,819'); seek(324.0)">
              Like, I mean, they will definitely, get better and, reduce the error rate.
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:05:28,969'); seek(328.0)">
              But, I think as an application developer, it's your responsibility
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:05:32,089'); seek(332.0)">
              to take care of this issue within your application as well.
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:05:35,879'); seek(335.0)">
              So what are your options?
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:05:38,414'); seek(338.0)">
              The first thing that you should definitely try out is, retries and timeouts.
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:05:43,584'); seek(343.0)">
              Now, these are not new concepts.
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:05:45,954'); seek(345.0)">
              if you have worked in software development for a while now, you would know what
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:05:49,304'); seek(349.0)">
              a retry is basically, when an API, gives you a wrong response, you try
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:05:54,134'); seek(354.0)">
              it again with some, cool down period, or, maybe not depending on how the
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:05:59,394'); seek(359.0)">
              rate limits are, retry is basically, you make an API call, the API fails,
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:06:04,804'); seek(364.0)">
              you wait for a while, and then you retry it again, as simple as that.
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:06:09,269'); seek(369.0)">
              Now, when you are developing, general applications, I think retries and
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:06:15,649'); seek(375.0)">
              timeouts are something that, are not the first thing that you would implement
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:21,129'); seek(381.0)">
              because, you just assume you just go with the assumption that the API response
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:06:25,889'); seek(385.0)">
              rate is going to be fairly reasonable.
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:06:27,749'); seek(387.0)">
              So.
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:06:28,779'); seek(388.0)">
              they will most of the time work and, like not adding APIs will, not really
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:06:35,369'); seek(395.0)">
              degrade the application performance.
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:06:37,479'); seek(397.0)">
              unless like you are working with very critical, applications like,
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:06:41,479'); seek(401.0)">
              something in finance or health where, the operation has to finish,
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:06:45,429'); seek(405.0)">
              in which case you will, definitely start with the retries and timeouts.
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:06:48,469'); seek(408.0)">
              But, in our general experience, if you're working with normal APIs, you
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:06:51,739'); seek(411.0)">
              don't really worry about these things.
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:06:53,639'); seek(413.0)">
              but because LLM APIs specifically have a higher error rate, retries
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:06:58,249'); seek(418.0)">
              and timeouts are something that, need to be implemented from day one.
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:07:03,659'); seek(423.0)">
              So, timeouts again, I think, I don't need to get into this.
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:07:06,399'); seek(426.0)">
              A timeout is basically.
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:07:08,029'); seek(428.0)">
              You make an API call and you wait for X seconds for the API to return.
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:07:14,179'); seek(434.0)">
              If it doesn't return in X seconds for whatever reason, you dominate that
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:07:18,629'); seek(438.0)">
              particular API and you try again.
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:07:20,479'); seek(440.0)">
              this basically is protection against, the server, the API server being down and,
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:07:27,299'); seek(447.0)">
              So like, if you don't do this, and if the API takes like a minute to respond,
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:07:31,279'); seek(451.0)">
              you, your application is also stuck for a minute and, so are your users.
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:07:35,589'); seek(455.0)">
              So a timeout is basically protection.
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:07:39,354'); seek(459.0)">
              So that if an API doesn't return in like a reasonable amount of time, you cancel that
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:07:44,004'); seek(464.0)">
              API call and you retry that API again.
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:07:46,694'); seek(466.0)">
              that's where timeout comes into picture.
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:07:48,744'); seek(468.0)">
              So, cool.
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:07:49,474'); seek(469.0)">
              So how do you implement this into your application?
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:07:52,554'); seek(472.0)">
              I would suggest don't, write the board for retries and timeouts from
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:07:56,014'); seek(476.0)">
              scratch, because there are a bunch of, battle tested libraries available
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:08:00,034'); seek(480.0)">
              in every language that you can use.
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:08:02,964'); seek(482.0)">
              And, with a few lines of code, add these, behaviors to your application.
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:08:07,194'); seek(487.0)">
              So let's look at a few examples.
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:08:10,284'); seek(490.0)">
              the one that we actually use in production is, this one called
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:08:13,624'); seek(493.0)">
              Tenacity by, it's a Python library.
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:08:16,184'); seek(496.0)">
              And, it allows you to add retry to your functions by simply doing this.
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:08:21,544'); seek(501.0)">
              you add a decorator, which is provided by the, by this particular library,
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:08:25,664'); seek(505.0)">
              you add it to a function and this, this function will be retried.
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:08:29,864'); seek(509.0)">
              whenever there is an exception or error in this particular function,
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:08:34,364'); seek(514.0)">
              now, you'd ideally want more control over, how many retries to do, how,
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:08:40,884'); seek(520.0)">
              like how long to wait after, every retry and those kinds of things.
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:08:44,754'); seek(524.0)">
              So, those all options are present in this library.
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:08:48,424'); seek(528.0)">
              You can, give it stopping conditions where you want to stop after three retries.
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:08:52,844'); seek(532.0)">
              You want to stop, stop after like.
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:08:54,404'); seek(534.0)">
              retrying, you can add a wait time before every retry.
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:08:59,854'); seek(539.0)">
              you can add a fixed wait time.
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:09:00,964'); seek(540.0)">
              You can add a random wait time, all these, different kinds of, behaviors
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:09:05,324'); seek(545.0)">
              can We added using this library, with like a few lines, of course.
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:09:09,089'); seek(549.0)">
              So, if you are working in Python, this is our choice, has been working,
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:09:14,029'); seek(554.0)">
              very well for us in production.
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:09:15,969'); seek(555.0)">
              so, this is what we, have been using for a very long time.
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:09:18,529'); seek(558.0)">
              So I would recommend this, if you're working in JS, there is a similar library
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:09:23,009'); seek(563.0)">
              called retract, very conveniently.
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:09:25,039'); seek(565.0)">
              that you can, that is available on NPM.
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:09:27,279'); seek(567.0)">
              So, similar type of functionalities.
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:09:29,669'); seek(569.0)">
              It gives you, retries and timeouts.
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:09:32,349'); seek(572.0)">
              Oh, by the way, tenacity also has, timeout related decorators.
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:09:36,149'); seek(576.0)">
              works the same way.
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:09:36,859'); seek(576.0)">
              if you want to add a timeout to particular function, you just add
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:09:40,014'); seek(580.0)">
              that decorator, specify the timeout.
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:09:42,129'); seek(582.0)">
              yeah, back to.
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:09:43,599'); seek(583.0)">
              This library, if you are working with the JS application, retry is, our choice.
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:09:49,049'); seek(589.0)">
              the third option is, basically, a lot of people who are, developing
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:09:54,109'); seek(594.0)">
              LLM applications are using these frameworks, LLM frameworks to handle, JS.
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:09:58,479'); seek(598.0)">
              API calls, retries, and like a bunch of different things.
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:10:02,009'); seek(602.0)">
              So the most famous LLM framework, framework, which a lot of
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:10:05,589'); seek(605.0)">
              people are using is Lanchain.
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:10:07,699'); seek(607.0)">
              And if you are working on top of Lanchain.
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:10:10,739'); seek(610.0)">
              Lanshan provides you, basically, some, mechanism to retry, out of the box.
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:10:16,719'); seek(616.0)">
              So, it's called, the retry output parser.
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:10:20,504'); seek(620.0)">
              where, you can use this to make the LLM calls.
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:10:22,954'); seek(622.0)">
              And whenever the LLM call fails, this parser will basically, handle retry
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:10:28,224'); seek(628.0)">
              on your behalf, by passing, the prompt again and, also the previous output,
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:10:32,534'); seek(632.0)">
              so that the, L-L-M-A-P has a better idea that, okay, the last out failed.
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:10:35,984'); seek(635.0)">
              And, I'm not supposed to, give this response again.
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:10:38,864'); seek(638.0)">
              So if you're on , then it's already sorted out for you.
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:10:41,584'); seek(641.0)">
              You use the retry out pass.
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:10:43,404'); seek(643.0)">
              All right.
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:10:43,854'); seek(643.0)">
              So this sorts out how to implement retries and timeouts.
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:10:47,894'); seek(647.0)">
              The next most common reason for a failure or LLM inconsistency is when
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:10:54,064'); seek(654.0)">
              you're working with structured outputs.
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:10:56,094'); seek(656.0)">
              So when I say structured output, I mean, something like you asked.
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:10:59,554'); seek(659.0)">
              The LLM to generate a JSON or XML CSV, even list arrays, those kinds of things.
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:11:04,974'); seek(664.0)">
              So, whenever you're asking an LLM to generate a structured output, there
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:11:08,994'); seek(668.0)">
              is a slight chance that, there'll be something wrong with that structure.
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:11:12,184'); seek(672.0)">
              Maybe there are some fields missing, the extra fields, in case of like JSONs, XMLs,
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:11:16,844'); seek(676.0)">
              there are brackets missing, might happen.
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:11:19,294'); seek(679.0)">
              So how do you handle that?
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:11:22,094'); seek(682.0)">
              The simplest way to do that is to, is to integrate a schema library instead
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:11:28,024'); seek(688.0)">
              of like doing it on your own every time.
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:11:29,984'); seek(689.0)">
              So, a schema library could be something like Pydantic.
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:11:33,414'); seek(693.0)">
              And, this is what we use, in our production.
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:11:38,124'); seek(698.0)">
              So, Pydantic is basically, the most.
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:11:40,904'); seek(700.0)">
              commonly used data validation library in Python.
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:11:43,904'); seek(703.0)">
              And what it does is it allows you to, create classes.
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:11:49,369'); seek(709.0)">
              in which you describe the structure of your response, and then you use
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:11:54,459'); seek(714.0)">
              this particular class to, check whether the LRM response fits,
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:11:59,319'); seek(719.0)">
              this particular structure or not.
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:12:01,099'); seek(721.0)">
              it will check for, fields, extra fields or less fields that will check for data
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:12:06,499'); seek(726.0)">
              types, and a bunch of other options.
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:12:09,129'); seek(729.0)">
              So on Python, just go for Pydantic, it is a tried and tested library.
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:12:14,169'); seek(734.0)">
              and, it will make the data validation part when you're working with
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:12:17,899'); seek(737.0)">
              structured outputs, hassle free.
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:12:19,619'); seek(739.0)">
              similarly, if you're working with NPM, there's something called EOP, same
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:12:23,149'); seek(743.0)">
              stuff as Pydantic, data validation.
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:12:25,809'); seek(745.0)">
              you essentially, Define the shape of your output.
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:12:29,389'); seek(749.0)">
              And, you basically use that shape, which is essentially a class, JS class,
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:12:34,609'); seek(754.0)">
              or JS object to, check or enforce, the structure of your L responses.
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:12:40,029'); seek(760.0)">
              and the idea is to use, these, these, data validation libraries,
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:12:44,409'); seek(764.0)">
              along with, retries and timers.
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:12:46,199'); seek(766.0)">
              So.
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:12:47,049'); seek(767.0)">
              what you basically do is when you make an LLM API call, and you get
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:12:50,389'); seek(770.0)">
              a response, you pass it through Pydantic or YUP or, whatever data
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:12:55,139'); seek(775.0)">
              validation, library you are using.
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:12:57,319'); seek(777.0)">
              And if you get an error, you use the retry, to like, like basically let the
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:13:01,389'); seek(781.0)">
              LLM generate that structured output again.
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:13:03,809'); seek(783.0)">
              most of the time, you will see that, a couple of retries sorts it out.
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:13:09,289'); seek(789.0)">
              Like, it's not like every API call will fail in the same way.
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:13:12,849'); seek(792.0)">
              So if let's say there are a few things missing, you can in your structured output
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:13:17,224'); seek(797.0)">
              the first time when you do a retry, the next time you'll get the correct output.
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:13:21,384'); seek(801.0)">
              but just, just as a, as a, general advice, if you see that there are
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:13:27,604'); seek(807.0)">
              particular kind of issues happening again and again, you should mention
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:13:31,304'); seek(811.0)">
              that, instruction in the prompt itself.
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:13:34,014'); seek(814.0)">
              Because what happens is that, when you do retry, like, an API call, which.
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:13:39,314'); seek(819.0)">
              Was supposed to take five seconds might end up taking 15 to 20
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:13:42,124'); seek(822.0)">
              seconds and, it will make your, make your application feel laggy.
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:13:46,789'); seek(826.0)">
              because, at, at the end of that API call, you're going to provide
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:13:50,229'); seek(830.0)">
              some output to your users and, they're waiting for that output.
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:13:53,449'); seek(833.0)">
              so if, you see that there are particular kind of, problems that are happening
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:13:57,869'); seek(837.0)">
              again and again, like for example, if, if you are, generating JSON, using.
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:14:03,579'); seek(843.0)">
              JSON using an LLM API.
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:14:05,339'); seek(845.0)">
              And you'll see that, like the LLM is always using single quotes
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:14:08,099'); seek(848.0)">
              instead of double quotes, which will generally cause issues.
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:14:11,009'); seek(851.0)">
              you should specify that as an important point in your prompt so
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:14:14,089'); seek(854.0)">
              that, you get the correct output in the first attempt itself.
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:14:18,459'); seek(858.0)">
              this is just like, an additional level of check, but the idea is
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:14:21,989'); seek(861.0)">
              that the first response should itself give you the correct output.
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:14:25,859'); seek(865.0)">
              So.
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:14:26,764'); seek(866.0)">
              anything that is, that is known should be mentioned in the prompt
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:14:30,014'); seek(870.0)">
              as a special instruction, so that you don't keep retrying and you use
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:14:33,414'); seek(873.0)">
              this and not waiting for an output.
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:14:34,984'); seek(874.0)">
              one, one additional option worth, one special mention here is, the structured
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:14:40,504'); seek(880.0)">
              output capabilities provided by OpenAI.
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:14:43,284'); seek(883.0)">
              So, if you're using, GPT models, and OpenAI APIs, what you can do
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:14:49,309'); seek(889.0)">
              is there is a response format field where you can specify a PyDynamic
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:14:52,529'); seek(892.0)">
              class and, the OpenAI APIs themselves will, try to enforce the structure.
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:14:59,249'); seek(899.0)">
              but this one problem here, which is if you want to switch out.
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:15:03,299'); seek(903.0)">
              The model and use something else like Claude or Grok, then you have
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:15:07,989'); seek(907.0)">
              basically, lost the structured output capabilities because those are not
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:15:12,369'); seek(912.0)">
              available right now in other LLM APIs.
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:15:15,899'); seek(915.0)">
              So, my suggestion is to just handle the, schema enforcing and checking
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:15:20,149'); seek(920.0)">
              in your application itself so that like it's easy for you to switch out
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:15:23,129'); seek(923.0)">
              the models and use different models.
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:15:25,759'); seek(925.0)">
              That's all for handling LLM inconsistencies.
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:15:29,979'); seek(929.0)">
              Two main things, retries and timeouts.
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:15:32,509'); seek(932.0)">
              Use them from the start.
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:15:35,239'); seek(935.0)">
              If you are working with structured outputs, use a data validation
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:15:39,319'); seek(939.0)">
              library to check the structure.
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:15:42,099'); seek(942.0)">
              You see the options here.
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:15:43,629'); seek(943.0)">
              Any of these are good.
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:15:44,879'); seek(944.0)">
              The next thing that you should, start thinking about is how to implement
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:15:48,959'); seek(948.0)">
              streaming in your LLM application.
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:15:51,789'); seek(951.0)">
              generally when you develop APIs, you, you implement, you implement normal request
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:15:56,849'); seek(956.0)">
              response, like, you get an API call.
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:15:58,744'); seek(958.0)">
              And, the server does some work and then you, then you return
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:16:02,764'); seek(962.0)">
              the entire response in one go.
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:16:04,734'); seek(964.0)">
              in, in case of, LLMs, what happens is sometimes it might take a long time
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:16:10,654'); seek(970.0)">
              for the LLM to generate a response.
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:16:12,694'); seek(972.0)">
              That's where streaming comes into picture.
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:16:14,254'); seek(974.0)">
              Streaming, your responses allow you to start returning partial
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:16:18,624'); seek(978.0)">
              responses, to the client.
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:16:20,894'); seek(980.0)">
              Even when, the LLM is not done.
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:16:24,009'); seek(984.0)">
              done with the generation.
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:16:25,749'); seek(985.0)">
              So, let's look at why, streaming is so important while building NLM applications.
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:16:32,729'); seek(992.0)">
              like I mentioned, NLMs might take long time for, for generation, to complete.
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:16:39,209'); seek(999.0)">
              Now, when, when your user is, using your application, Most
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:16:43,209'); seek(1003.0)">
              users are very impatient.
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:16:44,319'); seek(1004.0)">
              you, can't ask them to wait for, seconds.
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:16:48,649'); seek(1008.0)">
              Like I'm not even talking about minutes.
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:16:50,489'); seek(1010.0)">
              if you have like a 10 second delay in showing the response,
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:16:53,619'); seek(1013.0)">
              you might see a lot of drop off.
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:16:55,309'); seek(1015.0)">
              so, and, and like most of the LLMs that you would work with would
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:16:58,089'); seek(1018.0)">
              take like 5 to 10 seconds for even, you know, The simplest prompts.
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:17:03,484'); seek(1023.0)">
              So how do you improve the UX?
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:17:07,474'); seek(1027.0)">
              and make sure that your users don't drop off.
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:17:10,184'); seek(1030.0)">
              that's where streaming comes into picture.
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:17:12,204'); seek(1032.0)">
              what streaming allows you to do is, NLMs generate.
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:17:16,264'); seek(1036.0)">
              The response is token by token, like they'll generate it word by word.
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:17:20,124'); seek(1040.0)">
              And what streaming allows you to do is you don't need to wait
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:17:23,484'); seek(1043.0)">
              for the LLM response or output.
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:17:27,154'); seek(1047.0)">
              What you can do is as soon as it is done generating a few words,
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:17:30,154'); seek(1050.0)">
              you can send them to the client and start displaying them on the UI.
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:17:33,839'); seek(1053.0)">
              Or, whatever client you're using, in this way, the, the user, doesn't really feel
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:17:39,759'); seek(1059.0)">
              the lag, that, LLM generation results in, what they see is that, as soon as
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:17:45,779'); seek(1065.0)">
              they type out a prompt, immediately they start seeing some response
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:17:49,369'); seek(1069.0)">
              and they can start reading it out.
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:17:51,024'); seek(1071.0)">
              This is a very common pattern in any chat or LLM application
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:17:56,174'); seek(1076.0)">
              that you would have used.
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:17:57,454'); seek(1077.0)">
              As soon as you type something out or you do an action, you start
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:18:00,134'); seek(1080.0)">
              seeing partial results on your UI.
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:18:02,604'); seek(1082.0)">
              That is implemented through streaming.
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:18:04,474'); seek(1084.0)">
              the most common or the most, used way.
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:18:08,104'); seek(1088.0)">
              The other way to, to implement streaming is WebSockets.
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:18:11,284'); seek(1091.0)">
              WebSockets allow you to send, generated tokens or vaults in real time.
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:18:16,694'); seek(1096.0)">
              the connection is established, between client and server.
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:18:20,354'); seek(1100.0)">
              And then, until the entire generation is completed or, as long as, the user
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:18:25,244'); seek(1105.0)">
              is, live on the UI, you can just like reuse that connection to keep sending a
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:18:30,214'); seek(1110.0)">
              response, as and when it gets generated.
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:18:34,034'); seek(1114.0)">
              this is also a bidirectional, communication method.
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:18:37,054'); seek(1117.0)">
              So you can use the same method to get some input from the client.
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:18:42,474'); seek(1122.0)">
              Also, you know, one drawback of, WebSockets is that they need
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:18:46,684'); seek(1126.0)">
              some custom, Implementation.
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:18:48,714'); seek(1128.0)">
              you can't just take like your simple HTTP, REST server and,
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:18:53,354'); seek(1133.0)">
              convert it into WebSockets.
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:18:54,474'); seek(1134.0)">
              You'll need to redo your implementation, use new libraries, probably
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:18:59,614'); seek(1139.0)">
              even new, use a new language.
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:19:01,074'); seek(1141.0)">
              Like, for example, if you are working on Python, Python is not very, efficient,
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:19:07,064'); seek(1147.0)">
              way for implementing WebSockets.
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:19:08,574'); seek(1148.0)">
              You probably want to move to a different language, which handles, threads or
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:19:14,034'); seek(1154.0)">
              multiprocessing in a much better way than Python, like Golang or Java or even C
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:19:20,074'); seek(1160.0)">
              so generally WebSocket implementation.
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:19:23,359'); seek(1163.0)">
              is a considerable effort.
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:19:25,129'); seek(1165.0)">
              And, if all you want to do is stream LLM responses, it probably
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:19:29,069'); seek(1169.0)">
              is not the best way to do it.
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:19:30,999'); seek(1170.0)">
              there is another, solution for streaming, over HTTP, which
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:19:36,439'); seek(1176.0)">
              is called server side events.
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:19:38,319'); seek(1178.0)">
              which basically uses, your.
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:19:42,039'); seek(1182.0)">
              your, server itself, like basically if you are on Python and you're using flask or
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:19:46,109'); seek(1186.0)">
              fast API, you won't need to do a lot of changes to start streaming, using server
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:19:51,869'); seek(1191.0)">
              sentiments, code wise or implementation wise, this is a minimal effort.
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:19:55,439'); seek(1195.0)">
              what this essentially does is, it will use the same.
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:19:59,464'); seek(1199.0)">
              HTTP, connection, which your STPA call utilizes, but instead of, sending the
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:20:05,714'); seek(1205.0)">
              entire response in one shot, you can send the response in chunks and, on
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:20:11,154'); seek(1211.0)">
              your client side, you can, receive it in chunks and start displaying.
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:20:14,904'); seek(1214.0)">
              Now, this is a unidirectional, flow.
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:20:18,444'); seek(1218.0)">
              It works exactly as a REST API call, but instead of, the client
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:20:22,334'); seek(1222.0)">
              waiting for the entire response.
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:20:25,909'); seek(1225.0)">
              to come, the client starts showing chunks that have been sent from
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:20:30,469'); seek(1230.0)">
              server, using server sent events, implementation wise, it's very simple,
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:20:35,469'); seek(1235.0)">
              like you, just need to maybe implement a generator, if you're using Python
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:20:40,539'); seek(1240.0)">
              and, maybe add a couple of headers.
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:20:43,299'); seek(1243.0)">
              we won't get into specific details because these are, things that you
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:20:46,089'); seek(1246.0)">
              can easily Google and, find out.
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:20:47,999'); seek(1247.0)">
              But, Our recommendation, if you want to implement streaming in your
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:20:51,809'); seek(1251.0)">
              application and you already have a REST, setup ready on the backend, just
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:20:55,689'); seek(1255.0)">
              go for server side events, much, faster implementation, also much easier to
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:21:00,359'); seek(1260.0)">
              implement, WebSockets is a bit heavy and unless you have like a specific
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:21:05,029'); seek(1265.0)">
              use case, For, WebSockets, I won't recommend, going that, that, on that path.
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:21:11,799'); seek(1271.0)">
              Streaming is a good solution if, the particular task that an LLM
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:21:17,329'); seek(1277.0)">
              is handling, gets over in a few seconds, like 5 to 10 seconds.
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:21:20,279'); seek(1280.0)">
              but if your task is going to take minutes, streaming, Probably is not a good option.
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:21:26,629'); seek(1286.0)">
              that's where background jobs come into picture.
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:21:29,719'); seek(1289.0)">
              So, if you have a task which can be done in like five to 10 seconds,
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:21:34,089'); seek(1294.0)">
              probably you streaming and, it's a good way to start showing, an
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:21:37,449'); seek(1297.0)">
              output, to the user on client side.
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:21:40,469'); seek(1300.0)">
              but if you have a task, which is going to take minutes.
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:21:43,519'); seek(1303.0)">
              It is better to handle it asynchronously instead of synchronously
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:21:47,119'); seek(1307.0)">
              in your backend server and background jobs help you do that.
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:21:51,259'); seek(1311.0)">
              So what are these particular use cases where you might want to use
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:21:57,869'); seek(1317.0)">
              background jobs instead of streaming?
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:21:59,689'); seek(1319.0)">
              think of it this way.
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:22:00,849'); seek(1320.0)">
              Let's say if you, if you are building something like an essay
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:22:06,899'); seek(1326.0)">
              generator and you allow the user to enter essay topics in bulk.
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:22:11,864'); seek(1331.0)">
              So if someone, gives you a single essay topic, probably, you'll finish
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:22:15,824'); seek(1335.0)">
              the generation in a few seconds and, streaming is the way to go.
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:22:19,204'); seek(1339.0)">
              But let's say if someone, gives you a hundred essay topics, for, for generation
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:22:24,624'); seek(1344.0)">
              and that this particular task, doesn't matter how fast the LLM is, is going to
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:22:28,464'); seek(1348.0)">
              take minutes at, at least a few minutes.
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:22:31,524'); seek(1351.0)">
              And, if you use streaming for this, streaming will do all the
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:22:35,614'); seek(1355.0)">
              work in your backend server.
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:22:37,714'); seek(1357.0)">
              and, until this particular task is completed, which is
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:22:40,764'); seek(1360.0)">
              going to be a few minutes.
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:22:42,209'); seek(1362.0)">
              your backend server resources are going to get, hop or are going to be, tied up
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:22:49,019'); seek(1369.0)">
              in this particular task, which is very inefficient because, like your backend
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:22:53,629'); seek(1373.0)">
              servers job is basically take a request.
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:22:56,649'); seek(1376.0)">
              Process it in a few seconds and send it back to the client.
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:23:00,639'); seek(1380.0)">
              If you start doing things which take minutes, you will see that if you have
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:23:05,849'); seek(1385.0)">
              a lot of concurrent users, your backend server will be busy and it will not
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:23:10,789'); seek(1390.0)">
              be able to handle tasks which take a few seconds and your APIs will start
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:23:16,199'); seek(1396.0)">
              getting blocked and your application performance will start to degrade.
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:23:20,979'); seek(1400.0)">
              So what's the solution here?
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:23:23,499'); seek(1403.0)">
              You, the solution is, you don't handle.
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:23:26,669'); seek(1406.0)">
              Long running tasks in backend server synchronously.
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:23:30,719'); seek(1410.0)">
              You handle them in background jobs asynchronously.
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:23:33,849'); seek(1413.0)">
              Basically, when a user gives you a task, which is going to take
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:23:36,869'); seek(1416.0)">
              minutes, you log it in a database, a background job will pick that task up.
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:23:41,689'); seek(1421.0)">
              Till then, you tell the, you basically communicate to the user that, okay,
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:23:44,889'); seek(1424.0)">
              this is going to take a few minutes.
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:23:46,659'); seek(1426.0)">
              once.
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:23:47,634'); seek(1427.0)">
              The task is completed.
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:23:49,194'); seek(1429.0)">
              You will get a notification, probably as an email, or on slack.
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:23:53,554'); seek(1433.0)">
              And, what do you do is you use a background job to, pick up the task,
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:23:57,834'); seek(1437.0)">
              process it, And once it's ready, send out a notification, easiest
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:24:01,999'); seek(1441.0)">
              way to implement this is cron jobs.
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:24:04,349'); seek(1444.0)">
              cron jobs have been here for, I don't know, for a very long time.
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:24:08,339'); seek(1448.0)">
              very easy to implement, on any Unix based, server, which is
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:24:13,529'); seek(1453.0)">
              probably, what will be used in most of, production backend servers.
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:24:17,299'); seek(1457.0)">
              all you need to do is set up a cron job, which does the processing.
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:24:21,689'); seek(1461.0)">
              and the cron job runs every few minutes, checks the database
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:24:24,469'); seek(1464.0)">
              if there are any pending tasks.
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:24:26,149'); seek(1466.0)">
              now when your user.
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:24:27,914'); seek(1467.0)">
              comes to you with, with a task, you just put it in a DB and, market as pending.
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:24:35,304'); seek(1475.0)">
              when the cron job wakes up in a few minutes, it will check for any pending
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:24:38,594'); seek(1478.0)">
              tasks and start the processing.
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:24:40,814'); seek(1480.0)">
              And, on the US side, you can probably, implement some sort of polling.
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:24:45,214'); seek(1485.0)">
              To check if the task is completed or not.
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:24:46,944'); seek(1486.0)">
              And once it is completed, you can display that on the UI.
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:24:50,154'); seek(1490.0)">
              But, this is an optional thing.
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:24:51,334'); seek(1491.0)">
              Ideally, if you're using background jobs, you should also, sorry, you should also,
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:24:57,034'); seek(1497.0)">
              separately communicate, that the task is completed with the user, because,
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:25:01,294'); seek(1501.0)">
              the general, idea is that, when you, when, when a task is going to take a few
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:25:05,054'); seek(1505.0)">
              minutes, your users will probably come to your platform, submit that task and
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:25:09,804'); seek(1509.0)">
              they will move away from your platform.
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:25:11,794'); seek(1511.0)">
              So they're not looking at, the UI of your application.
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:25:15,014'); seek(1515.0)">
              So you should probably communicate that the task is completed through
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:25:19,154'); seek(1519.0)">
              an email or a Slack notification.
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:25:21,534'); seek(1521.0)">
              so that the users who have moved away from, the, you also know that, okay, that,
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:25:25,304'); seek(1525.0)">
              that, that generation has been completed.
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:25:28,764'); seek(1528.0)">
              this works very well, minimal setup, nothing new that
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:25:31,894'); seek(1531.0)">
              you'll probably need to learn.
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:25:33,014'); seek(1533.0)">
              Nothing new that you need to install, for the initial stages of your LLM
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:25:37,394'); seek(1537.0)">
              application, just go for a cron job.
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:25:39,684'); seek(1539.0)">
              what happens is that as your, application grows, you'll probably need to scale this.
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:25:46,684'); seek(1546.0)">
              Now, if you run multiple cron jobs.
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:25:48,824'); seek(1548.0)">
              you need to handle which cron job, picks up which task you need to implement some
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:25:53,784'); seek(1553.0)">
              sort of, distributed locking and, all those complexities come into picture.
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:25:58,414'); seek(1558.0)">
              Basically, cron jobs are good for the initial stages, but, like
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:26:03,544'); seek(1563.0)">
              we also started with cron jobs.
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:26:05,244'); seek(1565.0)">
              we still use cron jobs for some simple tasks, but there will be
            </span>
            
            <span id="chunk-429" class="transcript-chunks" onclick="console.log('00:26:08,904'); seek(1568.0)">
              a stage, when you'll need to move away from cron jobs for scalability.
            </span>
            
            <span id="chunk-430" class="transcript-chunks" onclick="console.log('00:26:13,269'); seek(1573.0)">
              and for, better retrying mechanisms, that's where task
            </span>
            
            <span id="chunk-431" class="transcript-chunks" onclick="console.log('00:26:17,789'); seek(1577.0)">
              queues come into picture.
            </span>
            
            <span id="chunk-432" class="transcript-chunks" onclick="console.log('00:26:19,269'); seek(1579.0)">
              So basically think of task queues as cron jobs with like more intelligence, where
            </span>
            
            <span id="chunk-433" class="transcript-chunks" onclick="console.log('00:26:24,639'); seek(1584.0)">
              all the, task management that needs to be done, is handled by the task queue itself.
            </span>
            
            <span id="chunk-434" class="transcript-chunks" onclick="console.log('00:26:30,369'); seek(1590.0)">
              when I say task management, on a very high level, what I It means is that,
            </span>
            
            <span id="chunk-435" class="transcript-chunks" onclick="console.log('00:26:35,454'); seek(1595.0)">
              you submit a task to the task queue.
            </span>
            
            <span id="chunk-436" class="transcript-chunks" onclick="console.log('00:26:37,464'); seek(1597.0)">
              generally a task queue is backed by some storage like Redis or some other cache.
            </span>
            
            <span id="chunk-437" class="transcript-chunks" onclick="console.log('00:26:42,894'); seek(1602.0)">
              the task is stored over there and then the task queue handles, basically
            </span>
            
            <span id="chunk-438" class="transcript-chunks" onclick="console.log('00:26:47,544'); seek(1607.0)">
              a task queue will have a bunch of workers running and, the, the task
            </span>
            
            <span id="chunk-439" class="transcript-chunks" onclick="console.log('00:26:51,154'); seek(1611.0)">
              queue will then handle, how to allocate that work to which worker based on
            </span>
            
            <span id="chunk-440" class="transcript-chunks" onclick="console.log('00:26:56,814'); seek(1616.0)">
              like a bunch of different mechanisms.
            </span>
            
            <span id="chunk-441" class="transcript-chunks" onclick="console.log('00:26:58,264'); seek(1618.0)">
              Like you can have.
            </span>
            
            <span id="chunk-442" class="transcript-chunks" onclick="console.log('00:26:59,074'); seek(1619.0)">
              Yeah.
            </span>
            
            <span id="chunk-443" class="transcript-chunks" onclick="console.log('00:26:59,514'); seek(1619.0)">
              priority queues, you can have a bunch of different retry
            </span>
            
            <span id="chunk-444" class="transcript-chunks" onclick="console.log('00:27:02,824'); seek(1622.0)">
              mechanisms, and all those things.
            </span>
            
            <span id="chunk-445" class="transcript-chunks" onclick="console.log('00:27:05,124'); seek(1625.0)">
              So, two good things about using task queues.
            </span>
            
            <span id="chunk-446" class="transcript-chunks" onclick="console.log('00:27:07,934'); seek(1627.0)">
              task queues are much easier to scale.
            </span>
            
            <span id="chunk-447" class="transcript-chunks" onclick="console.log('00:27:09,704'); seek(1629.0)">
              in a cron job, if you go from one to two to 10 cron jobs, you have to
            </span>
            
            <span id="chunk-448" class="transcript-chunks" onclick="console.log('00:27:13,086'); seek(1633.0)">
              handle, A bunch of, locking related stuff yourself, in task queues,
            </span>
            
            <span id="chunk-449" class="transcript-chunks" onclick="console.log('00:27:17,007'); seek(1637.0)">
              it's already, implemented for you.
            </span>
            
            <span id="chunk-450" class="transcript-chunks" onclick="console.log('00:27:19,027'); seek(1639.0)">
              So all you can do is increase the number of workers in a task queue.
            </span>
            
            <span id="chunk-451" class="transcript-chunks" onclick="console.log('00:27:22,807'); seek(1642.0)">
              And if you start getting more, tasks or workload, the, you can
            </span>
            
            <span id="chunk-452" class="transcript-chunks" onclick="console.log('00:27:28,677'); seek(1648.0)">
              just like, it's as easy as just changing the number on a dashboard,
            </span>
            
            <span id="chunk-453" class="transcript-chunks" onclick="console.log('00:27:31,985'); seek(1651.0)">
              to increase the number of workers.
            </span>
            
            <span id="chunk-454" class="transcript-chunks" onclick="console.log('00:27:33,775'); seek(1653.0)">
              again, like all the, additional handling of race conditions, retries,
            </span>
            
            <span id="chunk-455" class="transcript-chunks" onclick="console.log('00:27:38,525'); seek(1658.0)">
              timeouts, it's already taken care of.
            </span>
            
            <span id="chunk-456" class="transcript-chunks" onclick="console.log('00:27:40,625'); seek(1660.0)">
              All you need to do is, provide some configuration.
            </span>
            
            <span id="chunk-457" class="transcript-chunks" onclick="console.log('00:27:43,605'); seek(1663.0)">
              you also get better monitoring with task queues.
            </span>
            
            <span id="chunk-458" class="transcript-chunks" onclick="console.log('00:27:45,825'); seek(1665.0)">
              you, every task queue comes with some sort of, monitoring mechanism
            </span>
            
            <span id="chunk-459" class="transcript-chunks" onclick="console.log('00:27:51,015'); seek(1671.0)">
              or a dashboard where you can see what are the tasks currently running,
            </span>
            
            <span id="chunk-460" class="transcript-chunks" onclick="console.log('00:27:55,325'); seek(1675.0)">
              how much resources there are.
            </span>
            
            <span id="chunk-461" class="transcript-chunks" onclick="console.log('00:27:56,845'); seek(1676.0)">
              Eating up, which tasks are failing, start or restart tasks
            </span>
            
            <span id="chunk-462" class="transcript-chunks" onclick="console.log('00:28:00,765'); seek(1680.0)">
              and all those kinds of things.
            </span>
            
            <span id="chunk-463" class="transcript-chunks" onclick="console.log('00:28:02,305'); seek(1682.0)">
              So, once you start scaling your application, go for task queues.
            </span>
            
            <span id="chunk-464" class="transcript-chunks" onclick="console.log('00:28:06,495'); seek(1686.0)">
              The task queue that we use in our production is called RQ,
            </span>
            
            <span id="chunk-465" class="transcript-chunks" onclick="console.log('00:28:11,995'); seek(1691.0)">
              which stands for Redis Queue.
            </span>
            
            <span id="chunk-466" class="transcript-chunks" onclick="console.log('00:28:14,030'); seek(1694.0)">
              And, as the name suggests, it's backed by Redis and it's a very simple,
            </span>
            
            <span id="chunk-467" class="transcript-chunks" onclick="console.log('00:28:19,760'); seek(1699.0)">
              library for queuing and processing background jobs with workers.
            </span>
            
            <span id="chunk-468" class="transcript-chunks" onclick="console.log('00:28:24,160'); seek(1704.0)">
              very easy setup, hardly takes 15 minutes to set it up.
            </span>
            
            <span id="chunk-469" class="transcript-chunks" onclick="console.log('00:28:27,320'); seek(1707.0)">
              If you already have a Redis, you don't even need to, set, set up a Redis.
            </span>
            
            <span id="chunk-470" class="transcript-chunks" onclick="console.log('00:28:32,355'); seek(1712.0)">
              for RQ and, very simple, mechanism for queuing and processing.
            </span>
            
            <span id="chunk-471" class="transcript-chunks" onclick="console.log('00:28:39,345'); seek(1719.0)">
              All you need to do is create a queue, provide a red connection so that,
            </span>
            
            <span id="chunk-472" class="transcript-chunks" onclick="console.log('00:28:43,005'); seek(1723.0)">
              it has a place to store the tasks.
            </span>
            
            <span id="chunk-473" class="transcript-chunks" onclick="console.log('00:28:45,415'); seek(1725.0)">
              when you get a task queue nq, you can, and, this is basically a function
            </span>
            
            <span id="chunk-474" class="transcript-chunks" onclick="console.log('00:28:49,635'); seek(1729.0)">
              which is going to get called in the worker to process your tasks.
            </span>
            
            <span id="chunk-475" class="transcript-chunks" onclick="console.log('00:28:53,125'); seek(1733.0)">
              So, it's this simple and you can also provide some arguments for that function.
            </span>
            
            <span id="chunk-476" class="transcript-chunks" onclick="console.log('00:28:57,330'); seek(1737.0)">
              And
            </span>
            
            <span id="chunk-477" class="transcript-chunks" onclick="console.log('00:28:58,700'); seek(1738.0)">
              the worker.
            </span>
            
            <span id="chunk-478" class="transcript-chunks" onclick="console.log('00:29:00,320'); seek(1740.0)">
              For the worker, you just need to start it like this, on your command line.
            </span>
            
            <span id="chunk-479" class="transcript-chunks" onclick="console.log('00:29:04,770'); seek(1744.0)">
              And, it consumes tasks from Redis and, process them.
            </span>
            
            <span id="chunk-480" class="transcript-chunks" onclick="console.log('00:29:08,380'); seek(1748.0)">
              If you, want to increase the number of workers, you just like, start 10
            </span>
            
            <span id="chunk-481" class="transcript-chunks" onclick="console.log('00:29:12,990'); seek(1752.0)">
              different workers, connect them to the same Redis, and, RQ will itself
            </span>
            
            <span id="chunk-482" class="transcript-chunks" onclick="console.log('00:29:17,840'); seek(1757.0)">
              handle all the, all the complexities.
            </span>
            
            <span id="chunk-483" class="transcript-chunks" onclick="console.log('00:29:21,080'); seek(1761.0)">
              of, managing which worker gets what task, and all those kinds of things.
            </span>
            
            <span id="chunk-484" class="transcript-chunks" onclick="console.log('00:29:25,590'); seek(1765.0)">
              So, if you're on Python, RQ is the way to go.
            </span>
            
            <span id="chunk-485" class="transcript-chunks" onclick="console.log('00:29:30,090'); seek(1770.0)">
              Celery provides you with a similar, functionality, but we just found
            </span>
            
            <span id="chunk-486" class="transcript-chunks" onclick="console.log('00:29:35,440'); seek(1775.0)">
              that, there were a bunch of things in celery, which we did not really need.
            </span>
            
            <span id="chunk-487" class="transcript-chunks" onclick="console.log('00:29:39,600'); seek(1779.0)">
              and it seemed like an overkill.
            </span>
            
            <span id="chunk-488" class="transcript-chunks" onclick="console.log('00:29:41,405'); seek(1781.0)">
              so we decided to go with RQ, which was much simpler to set up on our end.
            </span>
            
            <span id="chunk-489" class="transcript-chunks" onclick="console.log('00:29:45,235'); seek(1785.0)">
              Prompted at what inputs are not working, what models are working, what models
            </span>
            
            <span id="chunk-490" class="transcript-chunks" onclick="console.log('00:29:48,105'); seek(1788.0)">
              are not working and, things like that.
            </span>
            
            <span id="chunk-491" class="transcript-chunks" onclick="console.log('00:29:50,085'); seek(1790.0)">
              So, if you, if you want an analogy, you can think of evals as unit testing.
            </span>
            
            <span id="chunk-492" class="transcript-chunks" onclick="console.log('00:29:56,745'); seek(1796.0)">
              So, think of it as unit testing for your prompts.
            </span>
            
            <span id="chunk-493" class="transcript-chunks" onclick="console.log('00:29:59,725'); seek(1799.0)">
              So this allows you to take a prompt template and individually
            </span>
            
            <span id="chunk-494" class="transcript-chunks" onclick="console.log('00:30:04,795'); seek(1804.0)">
              just test out that template with a bunch of different, Values.
            </span>
            
            <span id="chunk-495" class="transcript-chunks" onclick="console.log('00:30:08,475'); seek(1808.0)">
              and, you can, there are, there are a bunch of, reasons why you should ideally,
            </span>
            
            <span id="chunk-496" class="transcript-chunks" onclick="console.log('00:30:14,475'); seek(1814.0)">
              use evals with your prompt templates.
            </span>
            
            <span id="chunk-497" class="transcript-chunks" onclick="console.log('00:30:16,985'); seek(1816.0)">
              one, it allows you to just test out the prompts in isolation,
            </span>
            
            <span id="chunk-498" class="transcript-chunks" onclick="console.log('00:30:20,235'); seek(1820.0)">
              which makes it very fast.
            </span>
            
            <span id="chunk-499" class="transcript-chunks" onclick="console.log('00:30:21,255'); seek(1821.0)">
              the same way unit tests are fast, because you are just checking one function
            </span>
            
            <span id="chunk-500" class="transcript-chunks" onclick="console.log('00:30:24,495'); seek(1824.0)">
              against different types of inputs.
            </span>
            
            <span id="chunk-501" class="transcript-chunks" onclick="console.log('00:30:26,285'); seek(1826.0)">
              using prompts, using, sorry, I'm sorry.
            </span>
            
            <span id="chunk-502" class="transcript-chunks" onclick="console.log('00:30:28,345'); seek(1828.0)">
              using evals, You will be able to figure out different things like which
            </span>
            
            <span id="chunk-503" class="transcript-chunks" onclick="console.log('00:30:33,905'); seek(1833.0)">
              input works, which input doesn't work, which model works for a particular
            </span>
            
            <span id="chunk-504" class="transcript-chunks" onclick="console.log('00:30:37,845'); seek(1837.0)">
              task, which model does not work.
            </span>
            
            <span id="chunk-505" class="transcript-chunks" onclick="console.log('00:30:39,855'); seek(1839.0)">
              you'll be able to compare, costs of different models for different
            </span>
            
            <span id="chunk-506" class="transcript-chunks" onclick="console.log('00:30:43,465'); seek(1843.0)">
              types of inputs and so on.
            </span>
            
            <span id="chunk-507" class="transcript-chunks" onclick="console.log('00:30:45,735'); seek(1845.0)">
              an additional, benefit of using evals is that.
            </span>
            
            <span id="chunk-508" class="transcript-chunks" onclick="console.log('00:30:50,045'); seek(1850.0)">
              You can directly, integrate them with your CI CD pipeline so that,
            </span>
            
            <span id="chunk-509" class="transcript-chunks" onclick="console.log('00:30:55,075'); seek(1855.0)">
              you don't need to manually keep checking before every release.
            </span>
            
            <span id="chunk-510" class="transcript-chunks" onclick="console.log('00:30:57,815'); seek(1857.0)">
              If your prompts are still working the way they're working just like unit test.
            </span>
            
            <span id="chunk-511" class="transcript-chunks" onclick="console.log('00:31:01,865'); seek(1861.0)">
              You just, hook it up to your CI CD pipeline.
            </span>
            
            <span id="chunk-512" class="transcript-chunks" onclick="console.log('00:31:04,395'); seek(1864.0)">
              And, before every commit or, I'm sorry, after every commit or, after
            </span>
            
            <span id="chunk-513" class="transcript-chunks" onclick="console.log('00:31:08,345'); seek(1868.0)">
              every build, you, straight up run the evals and, similar to, assertions in
            </span>
            
            <span id="chunk-514" class="transcript-chunks" onclick="console.log('00:31:13,955'); seek(1873.0)">
              unit as evals also have assertions or checks where you can check the
            </span>
            
            <span id="chunk-515" class="transcript-chunks" onclick="console.log('00:31:18,345'); seek(1878.0)">
              response, and, specify whether it is as expected or not as expected.
            </span>
            
            <span id="chunk-516" class="transcript-chunks" onclick="console.log('00:31:23,800'); seek(1883.0)">
              And pass or fail an eval.
            </span>
            
            <span id="chunk-517" class="transcript-chunks" onclick="console.log('00:31:25,570'); seek(1885.0)">
              So, that's how on a very high level evals work.
            </span>
            
            <span id="chunk-518" class="transcript-chunks" onclick="console.log('00:31:30,510'); seek(1890.0)">
              we have tried out a bunch of different, eval libraries.
            </span>
            
            <span id="chunk-519" class="transcript-chunks" onclick="console.log('00:31:34,970'); seek(1894.0)">
              the one we like the most is promptful.
            </span>
            
            <span id="chunk-520" class="transcript-chunks" onclick="console.log('00:31:37,720'); seek(1897.0)">
              very easy to set up.
            </span>
            
            <span id="chunk-521" class="transcript-chunks" onclick="console.log('00:31:39,430'); seek(1899.0)">
              simply works using YAML files, basically you, you create a YAML
            </span>
            
            <span id="chunk-522" class="transcript-chunks" onclick="console.log('00:31:46,000'); seek(1906.0)">
              file where you specify your prompt template and you press specify a bunch
            </span>
            
            <span id="chunk-523" class="transcript-chunks" onclick="console.log('00:31:50,150'); seek(1910.0)">
              of inputs, for that prompt template.
            </span>
            
            <span id="chunk-524" class="transcript-chunks" onclick="console.log('00:31:52,800'); seek(1912.0)">
              And, using, so, Promfo is an open source, tool, so you can just like, straight up
            </span>
            
            <span id="chunk-525" class="transcript-chunks" onclick="console.log('00:31:58,680'); seek(1918.0)">
              install it from NPM, run it in your CLI.
            </span>
            
            <span id="chunk-526" class="transcript-chunks" onclick="console.log('00:32:01,850'); seek(1921.0)">
              and at the end of the event, you get a nice, graph like this.
            </span>
            
            <span id="chunk-527" class="transcript-chunks" onclick="console.log('00:32:06,705'); seek(1926.0)">
              which will show you for different types of inputs, whether the
            </span>
            
            <span id="chunk-528" class="transcript-chunks" onclick="console.log('00:32:11,175'); seek(1931.0)">
              output has passed the condition.
            </span>
            
            <span id="chunk-529" class="transcript-chunks" onclick="console.log('00:32:13,415'); seek(1933.0)">
              it will also allow you to compare different, models and, there is
            </span>
            
            <span id="chunk-530" class="transcript-chunks" onclick="console.log('00:32:18,665'); seek(1938.0)">
              some way to compare cost as well.
            </span>
            
            <span id="chunk-531" class="transcript-chunks" onclick="console.log('00:32:20,445'); seek(1940.0)">
              I don't think they have displayed it here, but yeah, cost comparison
            </span>
            
            <span id="chunk-532" class="transcript-chunks" onclick="console.log('00:32:23,095'); seek(1943.0)">
              is also something that you will get in the same dashboard.
            </span>
            
            <span id="chunk-533" class="transcript-chunks" onclick="console.log('00:32:25,755'); seek(1945.0)">
              And, you can start off with the open source version of promptful, but they
            </span>
            
            <span id="chunk-534" class="transcript-chunks" onclick="console.log('00:32:30,175'); seek(1950.0)">
              also have a cloud hosted version.
            </span>
            
            <span id="chunk-535" class="transcript-chunks" onclick="console.log('00:32:31,945'); seek(1951.0)">
              So if you want more reliability or don't want to manage your own instance.
            </span>
            
            <span id="chunk-536" class="transcript-chunks" onclick="console.log('00:32:35,970'); seek(1955.0)">
              that option is also available.
            </span>
            
            <span id="chunk-537" class="transcript-chunks" onclick="console.log('00:32:37,970'); seek(1957.0)">
              before we end the stock, let's do a quick walkthrough of all the different
            </span>
            
            <span id="chunk-538" class="transcript-chunks" onclick="console.log('00:32:41,840'); seek(1961.0)">
              foundational models, or foundational model APIs that are available for public use.
            </span>
            
            <span id="chunk-539" class="transcript-chunks" onclick="console.log('00:32:46,980'); seek(1966.0)">
              the reason for doing this is basically, this landscape is changing very fast.
            </span>
            
            <span id="chunk-540" class="transcript-chunks" onclick="console.log('00:32:51,860'); seek(1971.0)">
              So the last time you had gone over all the available models, so I'm pretty sure that.
            </span>
            
            <span id="chunk-541" class="transcript-chunks" onclick="console.log('00:32:58,475'); seek(1978.0)">
              By now, the list of models and also their comparisons have changed.
            </span>
            
            <span id="chunk-542" class="transcript-chunks" onclick="console.log('00:33:03,885'); seek(1983.0)">
              Probably the models you thought are not that great have
            </span>
            
            <span id="chunk-543" class="transcript-chunks" onclick="console.log('00:33:07,815'); seek(1987.0)">
              become very good and so on.
            </span>
            
            <span id="chunk-544" class="transcript-chunks" onclick="console.log('00:33:09,705'); seek(1989.0)">
              So let's do a quick run through of all the available models.
            </span>
            
            <span id="chunk-545" class="transcript-chunks" onclick="console.log('00:33:13,115'); seek(1993.0)">
              What are they good at?
            </span>
            
            <span id="chunk-546" class="transcript-chunks" onclick="console.log('00:33:13,855'); seek(1993.0)">
              What are They're not good at what kind of use cases?
            </span>
            
            <span id="chunk-547" class="transcript-chunks" onclick="console.log('00:33:16,490'); seek(1996.0)">
              you, what case, what kind of use cases work with a particular kind of model?
            </span>
            
            <span id="chunk-548" class="transcript-chunks" onclick="console.log('00:33:21,490'); seek(2001.0)">
              let's start with the oldest player OpenAI.
            </span>
            
            <span id="chunk-549" class="transcript-chunks" onclick="console.log('00:33:23,865'); seek(2003.0)">
              OpenAI has, three main families of models, which is GPT 4 0 4 O Mini, and,
            </span>
            
            <span id="chunk-550" class="transcript-chunks" onclick="console.log('00:33:29,840'); seek(2009.0)">
              Owen, which are available for public use.
            </span>
            
            <span id="chunk-551" class="transcript-chunks" onclick="console.log('00:33:32,400'); seek(2012.0)">
              I think they've deprecated their three and 3.5 models.
            </span>
            
            <span id="chunk-552" class="transcript-chunks" onclick="console.log('00:33:35,210'); seek(2015.0)">
              so these are the models that are available right now.
            </span>
            
            <span id="chunk-553" class="transcript-chunks" onclick="console.log('00:33:38,450'); seek(2018.0)">
              If you don't know what to use, just go with OpenAI.
            </span>
            
            <span id="chunk-554" class="transcript-chunks" onclick="console.log('00:33:42,860'); seek(2022.0)">
              These are the most versatile models.
            </span>
            
            <span id="chunk-555" class="transcript-chunks" onclick="console.log('00:33:46,010'); seek(2026.0)">
              They work very well with a wide variety of tasks, within these models, between
            </span>
            
            <span id="chunk-556" class="transcript-chunks" onclick="console.log('00:33:54,210'); seek(2034.0)">
              Foro and, Foro Mini, the difference is mainly, the trade off between,
            </span>
            
            <span id="chunk-557" class="transcript-chunks" onclick="console.log('00:33:59,000'); seek(2039.0)">
              cost and latency versus accuracy.
            </span>
            
            <span id="chunk-558" class="transcript-chunks" onclick="console.log('00:34:01,470'); seek(2041.0)">
              So if you have a complex task or something that requires a bit
            </span>
            
            <span id="chunk-559" class="transcript-chunks" onclick="console.log('00:34:05,730'); seek(2045.0)">
              more of reasoning, go for Foro.
            </span>
            
            <span id="chunk-560" class="transcript-chunks" onclick="console.log('00:34:08,329'); seek(2048.0)">
              if you are worried about cost, or if you're worried about, how
            </span>
            
            <span id="chunk-561" class="transcript-chunks" onclick="console.log('00:34:11,209'); seek(2051.0)">
              fast the response is going to be, go for 4 O Mini, but, it will
            </span>
            
            <span id="chunk-562" class="transcript-chunks" onclick="console.log('00:34:14,689'); seek(2054.0)">
              basically, give you lesser accuracy.
            </span>
            
            <span id="chunk-563" class="transcript-chunks" onclick="console.log('00:34:17,739'); seek(2057.0)">
              O L is something that I have not tried out.
            </span>
            
            <span id="chunk-564" class="transcript-chunks" onclick="console.log('00:34:19,529'); seek(2059.0)">
              these are supposed to be, open air flagship models.
            </span>
            
            <span id="chunk-565" class="transcript-chunks" onclick="console.log('00:34:23,249'); seek(2063.0)">
              but from, What I've heard, these are like fairly new.
            </span>
            
            <span id="chunk-566" class="transcript-chunks" onclick="console.log('00:34:25,999'); seek(2065.0)">
              So, before you put it in production, maybe, test them out
            </span>
            
            <span id="chunk-567" class="transcript-chunks" onclick="console.log('00:34:29,929'); seek(2069.0)">
              thoroughly, Foro and Foro Mini have been around for a while now.
            </span>
            
            <span id="chunk-568" class="transcript-chunks" onclick="console.log('00:34:33,719'); seek(2073.0)">
              So I think, you should not see a lot of problems, with them.
            </span>
            
            <span id="chunk-569" class="transcript-chunks" onclick="console.log('00:34:36,679'); seek(2076.0)">
              Also, like reliability wise, as, according to us, OpenAI APIs
            </span>
            
            <span id="chunk-570" class="transcript-chunks" onclick="console.log('00:34:41,889'); seek(2081.0)">
              have been the most reliable.
            </span>
            
            <span id="chunk-571" class="transcript-chunks" onclick="console.log('00:34:43,429'); seek(2083.0)">
              so you don't need to worry about downtime or, having to.
            </span>
            
            <span id="chunk-572" class="transcript-chunks" onclick="console.log('00:34:48,864'); seek(2088.0)">
              handle switching models because, this provider is not working.
            </span>
            
            <span id="chunk-573" class="transcript-chunks" onclick="console.log('00:34:52,194'); seek(2092.0)">
              The next provider is, and stopping.
            </span>
            
            <span id="chunk-574" class="transcript-chunks" onclick="console.log('00:34:56,814'); seek(2096.0)">
              I think for a while, these guys were working mostly on
            </span>
            
            <span id="chunk-575" class="transcript-chunks" onclick="console.log('00:35:00,524'); seek(2100.0)">
              the, chat, the, the, the.
            </span>
            
            <span id="chunk-576" class="transcript-chunks" onclick="console.log('00:35:02,759'); seek(2102.0)">
              APIs were not publicly available as far as I know, but, I think in the
            </span>
            
            <span id="chunk-577" class="transcript-chunks" onclick="console.log('00:35:07,329'); seek(2107.0)">
              last few months, I think that has changed, the APIs are available.
            </span>
            
            <span id="chunk-578" class="transcript-chunks" onclick="console.log('00:35:10,239'); seek(2110.0)">
              You can just directly, and they're completely self serve.
            </span>
            
            <span id="chunk-579" class="transcript-chunks" onclick="console.log('00:35:12,619'); seek(2112.0)">
              You can just directly go, on, anthropics, to Anthropx console
            </span>
            
            <span id="chunk-580" class="transcript-chunks" onclick="console.log('00:35:16,649'); seek(2116.0)">
              and create an API key, load up some credit and get started with it.
            </span>
            
            <span id="chunk-581" class="transcript-chunks" onclick="console.log('00:35:21,209'); seek(2121.0)">
              If you have any coding related use case, cloud APIs are your best choice.
            </span>
            
            <span id="chunk-582" class="transcript-chunks" onclick="console.log('00:35:26,629'); seek(2126.0)">
              I think, as far as coding is concerned, coding as a particular task, cloud,
            </span>
            
            <span id="chunk-583" class="transcript-chunks" onclick="console.log('00:35:32,049'); seek(2132.0)">
              works much better than, all the other models, which is also why you would have
            </span>
            
            <span id="chunk-584" class="transcript-chunks" onclick="console.log('00:35:36,119'); seek(2136.0)">
              seen that, everyone is using cloud with, their, code editors as well, like cursor.
            </span>
            
            <span id="chunk-585" class="transcript-chunks" onclick="console.log('00:35:41,449'); seek(2141.0)">
              so yeah, if code is what you want, work with Claude.
            </span>
            
            <span id="chunk-586" class="transcript-chunks" onclick="console.log('00:35:44,939'); seek(2144.0)">
              Next up is Grok, not to be confused with XAI's Grok.
            </span>
            
            <span id="chunk-587" class="transcript-chunks" onclick="console.log('00:35:49,689'); seek(2149.0)">
              So Grok is, essentially, a company that is building, special purpose chips.
            </span>
            
            <span id="chunk-588" class="transcript-chunks" onclick="console.log('00:35:56,989'); seek(2156.0)">
              They call them LPUs, for running LLMs, which, makes,
            </span>
            
            <span id="chunk-589" class="transcript-chunks" onclick="console.log('00:36:00,779'); seek(2160.0)">
              their inference time on LLMs.
            </span>
            
            <span id="chunk-590" class="transcript-chunks" onclick="console.log('00:36:02,129'); seek(2162.0)">
              Very low, probably, even the inference cost.
            </span>
            
            <span id="chunk-591" class="transcript-chunks" onclick="console.log('00:36:05,189'); seek(2165.0)">
              so if latency is what you're trying to optimize, tryout, grok, grok, cloud,
            </span>
            
            <span id="chunk-592" class="transcript-chunks" onclick="console.log('00:36:12,599'); seek(2172.0)">
              which is their API, which are their, LLM APIs, they generally host, most of.
            </span>
            
            <span id="chunk-593" class="transcript-chunks" onclick="console.log('00:36:17,429'); seek(2177.0)">
              Commonly used open source models.
            </span>
            
            <span id="chunk-594" class="transcript-chunks" onclick="console.log('00:36:19,719'); seek(2179.0)">
              So you have Lama, Mixtel, Gemma available, apart from that, a bunch of other things.
            </span>
            
            <span id="chunk-595" class="transcript-chunks" onclick="console.log('00:36:24,459'); seek(2184.0)">
              Latency wise, they are much faster than all the other model providers.
            </span>
            
            <span id="chunk-596" class="transcript-chunks" onclick="console.log('00:36:31,709'); seek(2191.0)">
              So if you are optimizing for latency and these models work for
            </span>
            
            <span id="chunk-597" class="transcript-chunks" onclick="console.log('00:36:36,309'); seek(2196.0)">
              your particular task, go for it.
            </span>
            
            <span id="chunk-598" class="transcript-chunks" onclick="console.log('00:36:38,609'); seek(2198.0)">
              All right.
            </span>
            
            <span id="chunk-599" class="transcript-chunks" onclick="console.log('00:36:39,059'); seek(2199.0)">
              So AWS, mainly works kind of like rock.
            </span>
            
            <span id="chunk-600" class="transcript-chunks" onclick="console.log('00:36:43,999'); seek(2203.0)">
              They host a lot of open source models on, and along with that, I think
            </span>
            
            <span id="chunk-601" class="transcript-chunks" onclick="console.log('00:36:48,369'); seek(2208.0)">
              they also have, their own models, which we have not tried out yet.
            </span>
            
            <span id="chunk-602" class="transcript-chunks" onclick="console.log('00:36:52,399'); seek(2212.0)">
              but the biggest USP of using AWS, bedrock would be if you're already
            </span>
            
            <span id="chunk-603" class="transcript-chunks" onclick="console.log('00:36:58,779'); seek(2218.0)">
              in the AWS ecosystem and you are, worried about, your sensitive data,
            </span>
            
            <span id="chunk-604" class="transcript-chunks" onclick="console.log('00:37:04,519'); seek(2224.0)">
              getting out of your infra and you don't want to like, send it to open AI
            </span>
            
            <span id="chunk-605" class="transcript-chunks" onclick="console.log('00:37:08,449'); seek(2228.0)">
              or cloud or any other model provider.
            </span>
            
            <span id="chunk-606" class="transcript-chunks" onclick="console.log('00:37:10,559'); seek(2230.0)">
              in that case, Bedrock should be your choice.
            </span>
            
            <span id="chunk-607" class="transcript-chunks" onclick="console.log('00:37:13,259'); seek(2233.0)">
              One good thing is Bedrock also hosts cloud APIs.
            </span>
            
            <span id="chunk-608" class="transcript-chunks" onclick="console.log('00:37:16,819'); seek(2236.0)">
              so, the limits are lower, as far as I know, I think you'll need
            </span>
            
            <span id="chunk-609" class="transcript-chunks" onclick="console.log('00:37:21,499'); seek(2241.0)">
              to talk to the support and, get your service quotas increased.
            </span>
            
            <span id="chunk-610" class="transcript-chunks" onclick="console.log('00:37:24,029'); seek(2244.0)">
              But, if you are worried about, sensitive data and you're okay with cloud,
            </span>
            
            <span id="chunk-611" class="transcript-chunks" onclick="console.log('00:37:28,549'); seek(2248.0)">
              Bedrock should work for you very well.
            </span>
            
            <span id="chunk-612" class="transcript-chunks" onclick="console.log('00:37:30,321'); seek(2250.0)">
              and along with that, they also host Lama and Mixtel.
            </span>
            
            <span id="chunk-613" class="transcript-chunks" onclick="console.log('00:37:32,822'); seek(2252.0)">
              And a few other, APIs, multimodal APIs.
            </span>
            
            <span id="chunk-614" class="transcript-chunks" onclick="console.log('00:37:36,072'); seek(2256.0)">
              Azure is, the last time I checked Azure is hosting GPT models.
            </span>
            
            <span id="chunk-615" class="transcript-chunks" onclick="console.log('00:37:41,592'); seek(2261.0)">
              separately, like, the hosting, which open AI does is separate from Azure.
            </span>
            
            <span id="chunk-616" class="transcript-chunks" onclick="console.log('00:37:45,702'); seek(2265.0)">
              And, the last time we checked, Azure GPT APIs were a bit more faster than open air.
            </span>
            
            <span id="chunk-617" class="transcript-chunks" onclick="console.log('00:37:51,642'); seek(2271.0)">
              So again, like, Oh, if you want to use open API APIs and you, want, a slightly
            </span>
            
            <span id="chunk-618" class="transcript-chunks" onclick="console.log('00:37:57,972'); seek(2277.0)">
              better latency, try out Azure, but they'll make you fill a bunch of forms.
            </span>
            
            <span id="chunk-619" class="transcript-chunks" onclick="console.log('00:38:02,442'); seek(2282.0)">
              I think these APIs or these models are not publicly available on Azure for everyone.
            </span>
            
            <span id="chunk-620" class="transcript-chunks" onclick="console.log('00:38:07,262'); seek(2287.0)">
              GCP, I've not tried out.
            </span>
            
            <span id="chunk-621" class="transcript-chunks" onclick="console.log('00:38:09,072'); seek(2289.0)">
              so, again, like, I mean, I think the setup was a bit complex.
            </span>
            
            <span id="chunk-622" class="transcript-chunks" onclick="console.log('00:38:12,022'); seek(2292.0)">
              So, we didn't get a chance to give it a try, but from what we've heard, the
            </span>
            
            <span id="chunk-623" class="transcript-chunks" onclick="console.log('00:38:16,492'); seek(2296.0)">
              developer experience is much better now.
            </span>
            
            <span id="chunk-624" class="transcript-chunks" onclick="console.log('00:38:19,212'); seek(2299.0)">
              So someday we'll give it a try again.
            </span>
            
            <span id="chunk-625" class="transcript-chunks" onclick="console.log('00:38:21,682'); seek(2301.0)">
              But GCP has a Gemini and the.
            </span>
            
            <span id="chunk-626" class="transcript-chunks" onclick="console.log('00:38:25,382'); seek(2305.0)">
              Latest, the newest kid on the block is DeepSync, if you are active on
            </span>
            
            <span id="chunk-627" class="transcript-chunks" onclick="console.log('00:38:30,492'); seek(2310.0)">
              Twitter, you would have already heard, about DeepSync, APIs, from
            </span>
            
            <span id="chunk-628" class="transcript-chunks" onclick="console.log('00:38:35,442'); seek(2315.0)">
              the chatter, it seems as if they are at par with own APIs, again,
            </span>
            
            <span id="chunk-629" class="transcript-chunks" onclick="console.log('00:38:40,942'); seek(2320.0)">
              haven't tried it out, give it a try.
            </span>
            
            <span id="chunk-630" class="transcript-chunks" onclick="console.log('00:38:43,077'); seek(2323.0)">
              one concern could be, the hosting, which is in China,
            </span>
            
            <span id="chunk-631" class="transcript-chunks" onclick="console.log('00:38:46,277'); seek(2326.0)">
              but, definitely give it a try.
            </span>
            
            <span id="chunk-632" class="transcript-chunks" onclick="console.log('00:38:48,067'); seek(2328.0)">
              probably you might find it, to be a good fit for your use case.
            </span>
            
            <span id="chunk-633" class="transcript-chunks" onclick="console.log('00:38:51,997'); seek(2331.0)">
              And, one more thing, deep seeks models are also open source, so
            </span>
            
            <span id="chunk-634" class="transcript-chunks" onclick="console.log('00:38:56,047'); seek(2336.0)">
              you can host them on your own.
            </span>
            
            <span id="chunk-635" class="transcript-chunks" onclick="console.log('00:38:57,517'); seek(2337.0)">
              And that's all from me.
            </span>
            
            <span id="chunk-636" class="transcript-chunks" onclick="console.log('00:38:58,967'); seek(2338.0)">
              I hope you find the information shared in the stock useful, and it speeds up
            </span>
            
            <span id="chunk-637" class="transcript-chunks" onclick="console.log('00:39:05,157'); seek(2345.0)">
              your development process when you're building LLM applications and, AI agents.
            </span>
            
            <span id="chunk-638" class="transcript-chunks" onclick="console.log('00:39:10,067'); seek(2350.0)">
              if you have any, queries or if you want to, talk more
            </span>
            
            <span id="chunk-639" class="transcript-chunks" onclick="console.log('00:39:13,297'); seek(2353.0)">
              about this, drop us an email.
            </span>
            
            <span id="chunk-640" class="transcript-chunks" onclick="console.log('00:39:14,707'); seek(2354.0)">
              You can find our email.
            </span>
            
            <span id="chunk-641" class="transcript-chunks" onclick="console.log('00:39:16,287'); seek(2356.0)">
              on Kusho AI's landing page, or just, send me a message on LinkedIn.
            </span>
            
            <span id="chunk-642" class="transcript-chunks" onclick="console.log('00:39:21,587'); seek(2361.0)">
              I'm happy to chat about this and, go build something awesome.
            </span>
            
            <span id="chunk-643" class="transcript-chunks" onclick="console.log('00:39:25,737'); seek(2365.0)">
              Bye.
            </span>
            
            </div>
          </div>
          
          

          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/python2025" class="btn btn-sm btn-danger shadow lift" style="background-color: #69811f;">
                <i class="fe fe-grid me-2"></i>
                See all 53 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Sourabh%20Gawande_python.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Sourabh Gawande
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Co-founder @ Kusho
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/sourabhgawande/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Sourabh Gawande's LinkedIn account" />
                  </a>
                  
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by Sourabh Gawande"
                  data-url="https://www.conf42.com/python2025"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/python2025"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>






    <!-- WELCOME -->
    <section class="pt-8 pt-md-11 pb-10 pb-md-15 bg-info" id="register">

      <!-- Shape -->
      <div class="shape shape-blur-3 text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>      </div>

      <!-- Content -->
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 col-md-10 col-lg-8 text-center">

            <!-- Heading -->
            <h1 class="display-2 text-white">
              Join the community!
            </h1>

            <!-- Text -->
            <p class="lead text-white text-opacity-80 mb-6 mb-md-8">
              Learn for free, join the best tech learning community 
              for a <a class="text-white" href="https://www.reddit.com/r/sanfrancisco/comments/1bz90f6/why_are_coffee_shops_in_sf_so_expensive/" target="_blank">price of a pumpkin latte</a>.
            </p>

            <!-- Form -->
            <form class="d-flex align-items-center justify-content-center mb-7 mb-md-9">

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Annual
              </span>

              <!-- Switch -->
              <div class="form-check form-check-dark form-switch mx-3">
                <input class="form-check-input" type="checkbox" id="billingSwitch" data-toggle="price" data-target=".price">
              </div>

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Monthly
              </span>

            </form>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->

    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- PRICING -->
    <section class="mt-n8 mt-md-n15">
      <div class="container">
        <div class="row gx-4">
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-1">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Newsletter</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="0" data-monthly="0">0</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Event notifications, weekly newsletter
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Delayed access</b> to all content
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Immediate access to Keynotes & Panels
                  </p>
                </div>
              
              
              </div>
            </div>

            <!-- Card -->
            <div class="card shadow-lg mb-6 border border-success">
              <div class="card-body">

                <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
</script>

<!-- Form -->
<link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
<p class="emailoctopus-success-message text-success"></p>
<p class="emailoctopus-error-message text-danger"></p>
<form
    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
    method="post"
    data-message-success="Thanks! Check your email for further directions!"
    data-message-missing-email-address="Your email address is required."
    data-message-invalid-email-address="Your email address looks incorrect, please try again."
    data-message-bot-submission-error="This doesn't look like a human submission."
    data-message-consent-required="Please check the checkbox to indicate your consent."
    data-message-invalid-parameters-error="This form has missing or invalid fields."
    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
    class="emailoctopus-form"
    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
>
<div class="form-floating emailoctopus-form-row">
    <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
    <label for="field_0">Email address</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
    <label for="field_1">First Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
    <label for="field_2">Last Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
    <label for="field_4">Company</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
    <label for="field_5">Job Title</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
    <label for="field_3">Phone Number</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
    oninput="updateCountry()"
    >
    <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
    </select>
    <label for="field_7">Country</label>
</div>
<input id="country-destination" name="field_7" type="hidden">
<input id="tz-country" name="field_8" type="hidden">

<input
    name="field_6"
    type="hidden"
    value="Python"
>

<div class="emailoctopus-form-row-consent">
    <input
    type="checkbox"
    id="consent"
    name="consent"
    >
    <label for="consent">
    I consent to the following terms:
    </label>
    <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
    Terms and Conditions
    </a>
    &amp;
    <a href="./code-of-conduct" target="_blank">
    Code of Conduct
    </a>
</div>
<div
    aria-hidden="true"
    class="emailoctopus-form-row-hp"
>
    <input
    type="text"
    name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
    tabindex="-1"
    autocomplete="nope"
    >
</div>
<div class="mt-6 emailoctopus-form-row-subscribe">
    <input
    type="hidden"
    name="successRedirectUrl"
    >
    <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
    Subscribe to free newsletter <i class="fe fe-arrow-right ms-3"></i>
    </button>
</div>
</form>

<!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
<script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

              </div>
            </div>
          </div>
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-0">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Community</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="8.34" data-monthly="10">8.34</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Access to <a href="https://conf42.circle.so/">Circle community platform</a>
                  </p>
                </div>

                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Immediate access</b> to all content
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank"><b>Live events!</b></a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank">Regular office hours, Q&As, CV reviews</a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Courses, quizes & certificates
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Community chats
                  </p>
                </div>
                

                <!-- Button -->
                <a href="https://conf42.circle.so/checkout/subscribe" class="btn w-100 btn-primary">
                  Join the community (7 day free trial)<i class="fe fe-arrow-right ms-3"></i>
                </a>

              </div>
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2025">
                  Observability 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2025">
                  Quantum Computing 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2025">
                  Rustlang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2025">
                  Platform Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2025">
                  MLOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2025">
                  Incident Management 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2025">
                  Kube Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2025">
                  JavaScript 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2025">
                  Prompt Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/robotics2025">
                  Robotics 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2025">
                  DevSecOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2025">
                  Internet of Things (IoT) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>