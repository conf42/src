<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Self-hosted LLMs across all your devices and GPUs</title>
    <meta name="description" content="One model, extra large, please!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Michael%20Yuan_llm.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Self-hosted LLMs across all your devices and GPUs | Conf42"/>
    <meta property="og:description" content="Fast and Portable Llama2 Inference on the Heterogeneous Edge: An alternative to Python. Compared with Python, Rust+Wasm apps could be 1/100 of the size, 100 times the speed, and, most importantly, securely run everywhere at full hardware acceleration without any change to the binary code."/>
    <meta property="og:url" content="https://conf42.com/Large_Language_Models_LLMs_2024_Michael_Yuan_Selfhost_LLMs_on_Macacross_devices_Fast_and_Efficient"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/GOLANG2025_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Golang 2025
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2025-04-03
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/golang2025" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2025">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2025">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2025">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2025">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2025">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2025">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2025">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2025">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2025">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/robotics2025">
                            Robotics
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2025">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2025">
                            Internet of Things (IoT)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="https://conf42.circle.so/">
                            <b>Community platform login</b>
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Join the community!
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #CCB87B;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Large Language Models (LLMs) 2024 - Online
            </h1>

            <h2 class="text-white">
              
              <time datetime="2024-04-11">April 11 2024</time>
              
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- One model, extra large, please!
 -->
              <script>
                const event_date = new Date("2024-04-11T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2024-04-11T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              <a href="#register" class="btn btn-primary shadow lift me-1 mb-3">
                <i class="fe fe-user-check me-2"></i>
                Subscribe to watch
              </a>
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "O0TRkjrDh78"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <a class="btn btn-info lift mb-3" data-bigpicture='{"ytSrc": "TQwxk0c4sh0"}' href="#">
                <i class="fe fe-eye me-2"></i>
                Watch Premiere
              </a>
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrBjR6ZR0g0LRq9Fp8c_4HrI" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Hello, welcome to my talk. The title is self hosted arms", "timestamp": "00:00:20,760", "timestamp_s": 20.0}, {"text": "across all your devices and GPU\u0027s. So in this talk we\u0027re", "timestamp": "00:00:25,102", "timestamp_s": 25.0}, {"text": "going to talk about self hosted oems and we\u0027re going", "timestamp": "00:00:29,038", "timestamp_s": 29.0}, {"text": "to primarily use three open source projects.", "timestamp": "00:00:32,958", "timestamp_s": 32.0}, {"text": "One is called was match and this is the CNC project that", "timestamp": "00:00:36,510", "timestamp_s": 36.0}, {"text": "I founded and it is a webassembly", "timestamp": "00:00:40,286", "timestamp_s": 40.0}, {"text": "runtime that we see great adoption in", "timestamp": "00:00:44,094", "timestamp_s": 44.0}, {"text": "AI and the large language model space. And then the", "timestamp": "00:00:47,670", "timestamp_s": 47.0}, {"text": "second project is called Lama Edge. Lama Edge is open source project that build", "timestamp": "00:00:50,838", "timestamp_s": 50.0}, {"text": "upon was matched. So it provides tools", "timestamp": "00:00:54,406", "timestamp_s": 54.0}, {"text": "and SDKs and programming primaries for building", "timestamp": "00:00:58,532", "timestamp_s": 58.0}, {"text": "large language model applications, especially around the llama two model on", "timestamp": "00:01:02,748", "timestamp_s": 62.0}, {"text": "the WASM edge app runtime itself. So when it\u0027s a runtime you", "timestamp": "00:01:06,588", "timestamp_s": 66.0}, {"text": "can see this as operating system. And then Llamaedge is infrastructure", "timestamp": "00:01:10,116", "timestamp_s": 70.0}, {"text": "that specifically for building web, for building arm applications on", "timestamp": "00:01:14,764", "timestamp_s": 74.0}, {"text": "top of WASM edge. And then the third project is Gaia. Net. Gaynet further", "timestamp": "00:01:18,188", "timestamp_s": 78.0}, {"text": "builds upon Lama Edge to provide what we", "timestamp": "00:01:22,388", "timestamp_s": 82.0}, {"text": "call a rag application server, meaning that", "timestamp": "00:01:25,690", "timestamp_s": 85.0}, {"text": "we supplement the large language model with real world knowledge", "timestamp": "00:01:29,026", "timestamp_s": 89.0}, {"text": "base, either from public data, from public knowledge, or from our proprietary knowledge.", "timestamp": "00:01:33,122", "timestamp_s": 93.0}, {"text": "And that just makes AI models and applications", "timestamp": "00:01:37,538", "timestamp_s": 97.0}, {"text": "that associated with that much more useful. So it\u0027s designed", "timestamp": "00:01:41,666", "timestamp_s": 101.0}, {"text": "for use with AI agent applications and such.", "timestamp": "00:01:45,522", "timestamp_s": 105.0}, {"text": "Well, start off the talk with a demo of how easy it is to run", "timestamp": "00:01:51,574", "timestamp_s": 111.0}, {"text": "a chatbot using the open source arm on your own device.", "timestamp": "00:01:55,814", "timestamp_s": 115.0}, {"text": "Using Lama H in this demo", "timestamp": "00:01:58,854", "timestamp_s": 118.0}, {"text": "I\u0027ll show you the easiest way to run a large language model on your own", "timestamp": "00:02:02,358", "timestamp_s": 122.0}, {"text": "computer. First go to the Lamaedge Llamaedge", "timestamp": "00:02:05,590", "timestamp_s": 125.0}, {"text": "repository on GitHub. It is an entirely open source project.", "timestamp": "00:02:09,822", "timestamp_s": 129.0}, {"text": "So in the readme file while you are here just", "timestamp": "00:02:13,654", "timestamp_s": 133.0}, {"text": "give it a start and in the readme file", "timestamp": "00:02:17,648", "timestamp_s": 137.0}, {"text": "you will see a quick start guide and there\u0027s just one line of command.", "timestamp": "00:02:21,224", "timestamp_s": 141.0}, {"text": "Copy this and then go to", "timestamp": "00:02:25,064", "timestamp_s": 145.0}, {"text": "your Mac or Linux computer or even windows with WSl", "timestamp": "00:02:28,248", "timestamp_s": 148.0}, {"text": "to run this command. What it does is that", "timestamp": "00:02:31,920", "timestamp_s": 151.0}, {"text": "can install a large language model runtime called the Wasmatch", "timestamp": "00:02:35,448", "timestamp_s": 155.0}, {"text": "plugins that required to run was to", "timestamp": "00:02:40,344", "timestamp_s": 160.0}, {"text": "run the large language model and it goes", "timestamp": "00:02:43,582", "timestamp_s": 163.0}, {"text": "fast here. But as you can see it downloads a large", "timestamp": "00:02:46,622", "timestamp_s": 166.0}, {"text": "language model called Gamma two B which is one of Google\u0027s", "timestamp": "00:02:50,494", "timestamp_s": 170.0}, {"text": "large language models. And you can actually run any", "timestamp": "00:02:54,558", "timestamp_s": 174.0}, {"text": "large language model that you find on hugging", "timestamp": "00:02:59,078", "timestamp_s": 179.0}, {"text": "face. There\u0027s over 5000 of those. And just", "timestamp": "00:03:02,310", "timestamp_s": 182.0}, {"text": "go back to the readme and, and look for the parameters that", "timestamp": "00:03:06,406", "timestamp_s": 186.0}, {"text": "you can pass to the script where you can specify the model which even as", "timestamp": "00:03:09,930", "timestamp_s": 189.0}, {"text": "a hugging based URL. And once the model is started it says", "timestamp": "00:03:13,258", "timestamp_s": 193.0}, {"text": "the application starts a local web server on your own device,", "timestamp": "00:03:18,434", "timestamp_s": 198.0}, {"text": "and it\u0027s started on port 8080. So what you can do", "timestamp": "00:03:22,722", "timestamp_s": 202.0}, {"text": "is that you can go to that server and", "timestamp": "00:03:26,586", "timestamp_s": 206.0}, {"text": "it would load a URL. And it\u0027s", "timestamp": "00:03:30,090", "timestamp_s": 210.0}, {"text": "the URL said lama h chat. Now you can start to chat with it.", "timestamp": "00:03:34,230", "timestamp_s": 214.0}, {"text": "Say, where is the capital", "timestamp": "00:03:37,294", "timestamp_s": 217.0}, {"text": "of Japan?", "timestamp": "00:03:41,942", "timestamp_s": 221.0}, {"text": "The first time it is a little slow because it needs to load the", "timestamp": "00:03:46,934", "timestamp_s": 226.0}, {"text": "model into memory, but it\u0027s fast enough. The capital of Japan is Tokyo,", "timestamp": "00:03:50,238", "timestamp_s": 230.0}, {"text": "and it\u0027s now you can ask what", "timestamp": "00:03:54,134", "timestamp_s": 234.0}, {"text": "about the USA? And it clearly shows", "timestamp": "00:03:59,912", "timestamp_s": 239.0}, {"text": "that the model understands", "timestamp": "00:04:04,032", "timestamp_s": 244.0}, {"text": "the conversation. Right? Because the second question I did not tell", "timestamp": "00:04:07,408", "timestamp_s": 247.0}, {"text": "to find the capital of the USA. I said, what about the USA? It knows", "timestamp": "00:04:10,696", "timestamp_s": 250.0}, {"text": "about Washington DC because", "timestamp": "00:04:14,392", "timestamp_s": 254.0}, {"text": "it knows about the previous conversation.", "timestamp": "00:04:17,768", "timestamp_s": 257.0}, {"text": "If we go back to the log, we can see that it\u0027s generating", "timestamp": "00:04:21,352", "timestamp_s": 261.0}, {"text": "with the gamma two b. It was generating over 60 tokens", "timestamp": "00:04:25,816", "timestamp_s": 265.0}, {"text": "per second. That\u0027s a lot faster than a human can speak.", "timestamp": "00:04:29,096", "timestamp_s": 269.0}, {"text": "A human typically speaks when I\u0027m speaking. Now it\u0027s about three", "timestamp": "00:04:32,382", "timestamp_s": 272.0}, {"text": "to five tokens per second. So it\u0027s ten times faster than human", "timestamp": "00:04:36,422", "timestamp_s": 276.0}, {"text": "can speak. Now I can ask a longer question.", "timestamp": "00:04:39,654", "timestamp_s": 279.0}, {"text": "Plan me a one day trip there, meaning one", "timestamp": "00:04:48,294", "timestamp_s": 288.0}, {"text": "day trip to Japan. Okay, so it\u0027s,", "timestamp": "00:04:53,174", "timestamp_s": 293.0}, {"text": "I should have said Washington DC, but you know, that\u0027s close enough.", "timestamp": "00:04:56,784", "timestamp_s": 296.0}, {"text": "You know, that\u0027s, oh, it\u0027s giving me two days as well. But this", "timestamp": "00:05:00,768", "timestamp_s": 300.0}, {"text": "is one of, because this is a very small, it\u0027s a two b", "timestamp": "00:05:04,472", "timestamp_s": 304.0}, {"text": "model. If you change it to a seven B model or even", "timestamp": "00:05:07,752", "timestamp_s": 307.0}, {"text": "larger model, you\u0027re going to see. You\u0027re going to see it respond a", "timestamp": "00:05:11,184", "timestamp_s": 311.0}, {"text": "lot better. But even so, it is still pretty good. It gave me", "timestamp": "00:05:14,688", "timestamp_s": 314.0}, {"text": "itinerary of on day four. Now it\u0027s maybe", "timestamp": "00:05:18,416", "timestamp_s": 318.0}, {"text": "a week in Japan. Right? So that\u0027s it.", "timestamp": "00:05:21,792", "timestamp_s": 321.0}, {"text": "That\u0027s how you run a large language model on your own computer. This is,", "timestamp": "00:05:24,796", "timestamp_s": 324.0}, {"text": "I\u0027m running it on the Mac and I\u0027m recording this video while running it.", "timestamp": "00:05:27,844", "timestamp_s": 327.0}, {"text": "And so it\u0027s even then I", "timestamp": "00:05:31,532", "timestamp_s": 331.0}, {"text": "can get 60 tokens per second.", "timestamp": "00:05:34,876", "timestamp_s": 334.0}, {"text": "So it is one of the easiest way and the fastest way for", "timestamp": "00:05:38,140", "timestamp_s": 338.0}, {"text": "you to run again. Yeah, it gave me the whole", "timestamp": "00:05:41,828", "timestamp_s": 341.0}, {"text": "whole week. So again,", "timestamp": "00:05:45,364", "timestamp_s": 345.0}, {"text": "the URL is GitHub Lama edge.", "timestamp": "00:05:49,428", "timestamp_s": 349.0}, {"text": "Lama Edge. And they\u0027re just one line", "timestamp": "00:05:52,638", "timestamp_s": 352.0}, {"text": "of command. And once you get started running, you can run other models by reading", "timestamp": "00:05:56,310", "timestamp_s": 356.0}, {"text": "the rest of the readme. And you can use that as an API server and", "timestamp": "00:05:59,662", "timestamp_s": 359.0}, {"text": "you can do a lot of things with it. Okay, now you have seen the", "timestamp": "00:06:03,318", "timestamp_s": 363.0}, {"text": "demo. So what\u0027s it all for? We have seen", "timestamp": "00:06:06,598", "timestamp_s": 366.0}, {"text": "that you can run a large language model application, you can chat with it", "timestamp": "00:06:10,142", "timestamp_s": 370.0}, {"text": "on your own device, but what\u0027s it for? Most people just use", "timestamp": "00:06:13,782", "timestamp_s": 373.0}, {"text": "OpenAI for this purpose and OpenAI also provided SDK API,", "timestamp": "00:06:17,494", "timestamp_s": 377.0}, {"text": "right? So you can build applications around it as well. So why is there", "timestamp": "00:06:21,348", "timestamp_s": 381.0}, {"text": "need to run those large language, open source, large language models on your", "timestamp": "00:06:25,092", "timestamp_s": 385.0}, {"text": "own device? Well, so there are a couple of reasons. The first,", "timestamp": "00:06:28,748", "timestamp_s": 388.0}, {"text": "biggest reason really is that OpenAI or any other larger", "timestamp": "00:06:32,908", "timestamp_s": 392.0}, {"text": "language providers, basically it takes a one size fits all approach.", "timestamp": "00:06:37,068", "timestamp_s": 397.0}, {"text": "So basically they\u0027re using the largest model for the smallest tasks. So even", "timestamp": "00:06:40,788", "timestamp_s": 400.0}, {"text": "if you ask for something that can be easily handled by a smaller model,", "timestamp": "00:06:44,324", "timestamp_s": 404.0}, {"text": "it will still use the largest model, which generates a lot of waste.", "timestamp": "00:06:47,364", "timestamp_s": 407.0}, {"text": "It was very expensive and also makes the models very difficult", "timestamp": "00:06:51,564", "timestamp_s": 411.0}, {"text": "to fine tune because it\u0027s much harder to fine tune a large model than", "timestamp": "00:06:54,980", "timestamp_s": 414.0}, {"text": "a smaller model and because they\u0027re running on other people\u0027s server,", "timestamp": "00:06:58,508", "timestamp_s": 418.0}, {"text": "because you can\u0027t, because the model is too big to run on server. So it\u0027s", "timestamp": "00:07:02,004", "timestamp_s": 422.0}, {"text": "lack of privacy and control. And perhaps more interesting,", "timestamp": "00:07:04,612", "timestamp_s": 424.0}, {"text": "and I think people are noticing this more and more, is censorship and bias because", "timestamp": "00:07:08,156", "timestamp_s": 428.0}, {"text": "those companies like Openi or Microsoft all", "timestamp": "00:07:12,900", "timestamp_s": 432.0}, {"text": "have their own political stand and agendas. So you would", "timestamp": "00:07:16,500", "timestamp_s": 436.0}, {"text": "find a lot of questions that those models refuse to answer.", "timestamp": "00:07:20,228", "timestamp_s": 440.0}, {"text": "There\u0027s very compelling needs for", "timestamp": "00:07:24,644", "timestamp_s": 444.0}, {"text": "enterprises or users to run their own models, preferably on", "timestamp": "00:07:28,220", "timestamp_s": 448.0}, {"text": "their own devices or in the cloud.", "timestamp": "00:07:31,900", "timestamp_s": 451.0}, {"text": "So then that raise the second question.", "timestamp": "00:07:35,524", "timestamp_s": 455.0}, {"text": "The llama edge server probably is not the first or the", "timestamp": "00:07:39,164", "timestamp_s": 459.0}, {"text": "only open source software that can run large", "timestamp": "00:07:42,252", "timestamp_s": 462.0}, {"text": "language models. So for instance llama CPP can run and Allama can run it,", "timestamp": "00:07:45,804", "timestamp_s": 465.0}, {"text": "arm studio can run it. So why is it that we choose the", "timestamp": "00:07:49,580", "timestamp_s": 469.0}, {"text": "llama edge was stacked to run our API", "timestamp": "00:07:53,958", "timestamp_s": 473.0}, {"text": "server around those models as we have shown in the previous example?", "timestamp": "00:07:58,014", "timestamp_s": 478.0}, {"text": "First, I think there are several features that distinguish llama", "timestamp": "00:08:03,094", "timestamp_s": 483.0}, {"text": "edge from other products or", "timestamp": "00:08:06,606", "timestamp_s": 486.0}, {"text": "other open source projects in the space. The first", "timestamp": "00:08:09,990", "timestamp_s": 489.0}, {"text": "is to support a wide variety of models. The lamo edge", "timestamp": "00:08:13,326", "timestamp_s": 493.0}, {"text": "application server supports over 6000 large language models,", "timestamp": "00:08:17,182", "timestamp_s": 497.0}, {"text": "including all the ones", "timestamp": "00:08:21,094", "timestamp_s": 501.0}, {"text": "that you have heard of. And there\u0027s not just the base models and", "timestamp": "00:08:24,548", "timestamp_s": 504.0}, {"text": "chat models, but also embedding models which are essential and very important", "timestamp": "00:08:28,468", "timestamp_s": 508.0}, {"text": "for running sophisticated applications as we are seeing like RAC applications.", "timestamp": "00:08:32,516", "timestamp_s": 512.0}, {"text": "It even support larger models like X AI squawk model which is", "timestamp": "00:08:36,948", "timestamp_s": 516.0}, {"text": "I think 314 billion parameters. And it", "timestamp": "00:08:41,060", "timestamp_s": 521.0}, {"text": "requires top of the line, say a Mac studio or,", "timestamp": "00:08:44,636", "timestamp_s": 524.0}, {"text": "or two H 100 chips graphic", "timestamp": "00:08:49,494", "timestamp_s": 529.0}, {"text": "cards to run those models. But they can run using", "timestamp": "00:08:53,942", "timestamp_s": 533.0}, {"text": "Lama H on the other hand.", "timestamp": "00:08:58,006", "timestamp_s": 538.0}, {"text": "On one hand that supports a wide variety of models. But on the", "timestamp": "00:09:01,654", "timestamp_s": 541.0}, {"text": "other hand, it also supports a wide range of devices, because once", "timestamp": "00:09:05,086", "timestamp_s": 545.0}, {"text": "we start to run on our own devices, in our own", "timestamp": "00:09:08,502", "timestamp_s": 548.0}, {"text": "cloud, we have to deal with many different types of different devices,", "timestamp": "00:09:11,590", "timestamp_s": 551.0}, {"text": "different gpu\u0027s, different cpu\u0027s and different graphic", "timestamp": "00:09:15,174", "timestamp_s": 555.0}, {"text": "cards, different accelerators and all that. So the", "timestamp": "00:09:19,262", "timestamp_s": 559.0}, {"text": "architecture of Lama H, because it\u0027s based on the wash runtime which we\u0027ll get into", "timestamp": "00:09:23,374", "timestamp_s": 563.0}, {"text": "in a minute, it provides abstraction layer that", "timestamp": "00:09:27,390", "timestamp_s": 567.0}, {"text": "abstracts the application from the underlying runtime", "timestamp": "00:09:30,894", "timestamp_s": 570.0}, {"text": "and hardware. So that allows it to the same application", "timestamp": "00:09:34,934", "timestamp_s": 574.0}, {"text": "to run on a wide range of devices and drivers,", "timestamp": "00:09:38,446", "timestamp_s": 578.0}, {"text": "ranging from Nvidia to AMD to ARM devices,", "timestamp": "00:09:42,158", "timestamp_s": 582.0}, {"text": "to Intel AMD CPU", "timestamp": "00:09:46,446", "timestamp_s": 586.0}, {"text": "and GPU devices, Apple devices and all of those.", "timestamp": "00:09:50,706", "timestamp_s": 590.0}, {"text": "So in a minute. So I just showed you it runs on", "timestamp": "00:09:54,322", "timestamp_s": 594.0}, {"text": "a Mac device and I\u0027m going to show you how it runs on Nvidia device.", "timestamp": "00:09:57,922", "timestamp_s": 597.0}, {"text": "And we also have tutorials on how to run, say on raspberry PI", "timestamp": "00:10:01,786", "timestamp_s": 601.0}, {"text": "or the Jetson devices. All those,", "timestamp": "00:10:05,202", "timestamp_s": 605.0}, {"text": "I would say, devices more", "timestamp": "00:10:08,498", "timestamp_s": 608.0}, {"text": "commonly find in the edge cloud.", "timestamp": "00:10:12,250", "timestamp_s": 612.0}, {"text": "The Lama Edge API server also provides enough flexibility to", "timestamp": "00:10:15,734", "timestamp_s": 615.0}, {"text": "run multiple models in the same server. So, meaning that I can run a", "timestamp": "00:10:19,174", "timestamp_s": 619.0}, {"text": "chat model to chat with people and run the embedding model in the", "timestamp": "00:10:22,598", "timestamp_s": 622.0}, {"text": "same server, so that the user can upload", "timestamp": "00:10:25,958", "timestamp_s": 625.0}, {"text": "or process their external knowledge base with the same application server.", "timestamp": "00:10:29,894", "timestamp_s": 629.0}, {"text": "It can also do things like function calling by forcing the output to", "timestamp": "00:10:34,422", "timestamp_s": 634.0}, {"text": "be JSON. That\u0027s something that needs coordination between both", "timestamp": "00:10:37,678", "timestamp_s": 637.0}, {"text": "the model itself, the prompting, and the model runtime, meaning that the", "timestamp": "00:10:41,574", "timestamp_s": 641.0}, {"text": "model runtime would have to force the output to meet certain grammar checks,", "timestamp": "00:10:45,318", "timestamp_s": 645.0}, {"text": "to pass certain grammar check, for instance, that\u0027s provided JSON. Right. And as", "timestamp": "00:10:49,102", "timestamp_s": 649.0}, {"text": "you can see, it\u0027s very easy to install in a run. It\u0027s just one line", "timestamp": "00:10:52,878", "timestamp_s": 652.0}, {"text": "of command, and I highly suggest recommend you to try it out.", "timestamp": "00:10:56,470", "timestamp_s": 656.0}, {"text": "And it\u0027s also very lightweight because it\u0027s the entire", "timestamp": "00:11:00,134", "timestamp_s": 660.0}, {"text": "runtime wash. Plus the application itself", "timestamp": "00:11:03,886", "timestamp_s": 663.0}, {"text": "is less than say 30 megabytes. That compared to say,", "timestamp": "00:11:07,366", "timestamp_s": 667.0}, {"text": "the Pytorch Docker image we talked, you know, the Pytorch", "timestamp": "00:11:11,414", "timestamp_s": 671.0}, {"text": "Docker image is 4gb by itself. So that\u0027s", "timestamp": "00:11:14,652", "timestamp_s": 674.0}, {"text": "why you, you know, even when running", "timestamp": "00:11:19,660", "timestamp_s": 679.0}, {"text": "a large language model and put the API server in front of it, put a", "timestamp": "00:11:23,356", "timestamp_s": 683.0}, {"text": "chatbot in front of it, we use Lama edge, right? But all those", "timestamp": "00:11:26,196", "timestamp_s": 686.0}, {"text": "benefits actually come from one most important", "timestamp": "00:11:29,652", "timestamp_s": 689.0}, {"text": "attributes of the llama edge runtime, which is it is", "timestamp": "00:11:33,388", "timestamp_s": 693.0}, {"text": "not really designed to be, to be a standalone", "timestamp": "00:11:37,044", "timestamp_s": 697.0}, {"text": "server to run large language model. It is", "timestamp": "00:11:40,960", "timestamp_s": 700.0}, {"text": "actually a developer platform. You can write your own code to", "timestamp": "00:11:44,552", "timestamp_s": 704.0}, {"text": "extend it and to customize and build your own applications on", "timestamp": "00:11:48,424", "timestamp_s": 708.0}, {"text": "top of it. If you think about how large language", "timestamp": "00:11:52,248", "timestamp_s": 712.0}, {"text": "model applications are built today, typically you have", "timestamp": "00:11:56,040", "timestamp_s": 716.0}, {"text": "an application server that provides API.", "timestamp": "00:11:59,128", "timestamp_s": 719.0}, {"text": "It\u0027s either OpenAI or some other SaaS provider,", "timestamp": "00:12:02,088", "timestamp_s": 722.0}, {"text": "or you can use your own, you know, like using Lama H, you provide open", "timestamp": "00:12:05,344", "timestamp_s": 725.0}, {"text": "compatible API server and then you build an application that consumes", "timestamp": "00:12:09,302", "timestamp_s": 729.0}, {"text": "this API in say in a relatively", "timestamp": "00:12:12,910", "timestamp_s": 732.0}, {"text": "heavyweight stack like in Python or LAN chain or lamba index.", "timestamp": "00:12:16,350", "timestamp_s": 736.0}, {"text": "There are lots of tools out there to do that. And you would also have", "timestamp": "00:12:20,038", "timestamp_s": 740.0}, {"text": "a UI, have a web server, have all those", "timestamp": "00:12:22,758", "timestamp_s": 742.0}, {"text": "components either contained in a docker file,", "timestamp": "00:12:27,014", "timestamp_s": 747.0}, {"text": "something like that, and have them all tied together and build", "timestamp": "00:12:30,718", "timestamp_s": 750.0}, {"text": "an entire application like that. But while this provides", "timestamp": "00:12:34,230", "timestamp_s": 754.0}, {"text": "flexibility and easy allows people, allow developers to experiment", "timestamp": "00:12:37,880", "timestamp_s": 757.0}, {"text": "with different parts of the stack, it is also making it really difficult", "timestamp": "00:12:42,176", "timestamp_s": 762.0}, {"text": "to deploy those applications because they have too much dependencies.", "timestamp": "00:12:45,992", "timestamp_s": 765.0}, {"text": "They are huge because of the python dependency and they", "timestamp": "00:12:49,824", "timestamp_s": 769.0}, {"text": "are often very slow. Llama Edge", "timestamp": "00:12:53,008", "timestamp_s": 773.0}, {"text": "is a rust based SDK. Essentially it allows you", "timestamp": "00:12:57,144", "timestamp_s": 777.0}, {"text": "to build a single portable and deployable app. So basically you can write all", "timestamp": "00:13:00,840", "timestamp_s": 780.0}, {"text": "your logic into one single application. There\u0027s no need to write part", "timestamp": "00:13:04,560", "timestamp_s": 784.0}, {"text": "of the logic in C, part of that in Python,", "timestamp": "00:13:08,100", "timestamp_s": 788.0}, {"text": "and then use the HTTP API to connect them together. You don\u0027t need", "timestamp": "00:13:11,204", "timestamp_s": 791.0}, {"text": "to do any of those. So he improves efficiency, make it a", "timestamp": "00:13:14,508", "timestamp_s": 794.0}, {"text": "lot easier to deploy and it simplifies", "timestamp": "00:13:18,188", "timestamp_s": 798.0}, {"text": "the workflow. And again, there\u0027s no python dependency. A lot of", "timestamp": "00:13:21,564", "timestamp_s": 801.0}, {"text": "people, if you haven\u0027t really worked in the large language model", "timestamp": "00:13:25,076", "timestamp_s": 805.0}, {"text": "space in the past, that Python dependency is actually a", "timestamp": "00:13:28,652", "timestamp_s": 808.0}, {"text": "nightmare even for very experienced people in this field.", "timestamp": "00:13:32,300", "timestamp_s": 812.0}, {"text": "You can use rust or you can use JavaScript", "timestamp": "00:13:37,084", "timestamp_s": 817.0}, {"text": "that allows us to build applications that are similar to", "timestamp": "00:13:41,124", "timestamp_s": 821.0}, {"text": "say the latest advance in OpenAI. So if you look at OpenAI,", "timestamp": "00:13:45,116", "timestamp_s": 825.0}, {"text": "they have a systems API, they have a stateful API that", "timestamp": "00:13:48,436", "timestamp_s": 828.0}, {"text": "build a lot of functionality into the API server itself.", "timestamp": "00:13:51,972", "timestamp_s": 831.0}, {"text": "So the Lama H platform allows you to develop your", "timestamp": "00:13:54,748", "timestamp_s": 834.0}, {"text": "own API server to serve whatever", "timestamp": "00:13:58,460", "timestamp_s": 838.0}, {"text": "the front end that you want to do. So that", "timestamp": "00:14:02,042", "timestamp_s": 842.0}, {"text": "gives you a lot of flexibility in terms of building", "timestamp": "00:14:05,706", "timestamp_s": 845.0}, {"text": "advanced applications if you pay", "timestamp": "00:14:09,402", "timestamp_s": 849.0}, {"text": "close attention. There\u0027s a word that I put in bold here that", "timestamp": "00:14:12,698", "timestamp_s": 852.0}, {"text": "is single portable and the deployable application,", "timestamp": "00:14:16,258", "timestamp_s": 856.0}, {"text": "meaning you are talking about a developer platform that is based", "timestamp": "00:14:19,602", "timestamp_s": 859.0}, {"text": "on rust that has no python dependency. One of the biggest issues", "timestamp": "00:14:23,362", "timestamp_s": 863.0}, {"text": "people are going to ask is that how about cross platform compatibility?", "timestamp": "00:14:27,058", "timestamp_s": 867.0}, {"text": "Because for highly efficient", "timestamp": "00:14:31,248", "timestamp_s": 871.0}, {"text": "native applications like that, if you write it", "timestamp": "00:14:34,568", "timestamp_s": 874.0}, {"text": "on one platform, it\u0027s likely you are not going to be able to deploy it", "timestamp": "00:14:38,744", "timestamp_s": 878.0}, {"text": "on a different platform. That sort of makes", "timestamp": "00:14:41,912", "timestamp_s": 881.0}, {"text": "it really difficult for developers because I have a Mac or Windows,", "timestamp": "00:14:45,912", "timestamp_s": 885.0}, {"text": "but my application that I compiled and tested there,", "timestamp": "00:14:49,336", "timestamp_s": 889.0}, {"text": "is that really true that I can\u0027t deploy it on a media device in the", "timestamp": "00:14:53,744", "timestamp_s": 893.0}, {"text": "cloud? So in the next demo I\u0027m going to show you how", "timestamp": "00:14:56,648", "timestamp_s": 896.0}, {"text": "the was manage and the laminage stack address this problem", "timestamp": "00:15:00,754", "timestamp_s": 900.0}, {"text": "that we allow you to build truly portable large", "timestamp": "00:15:04,226", "timestamp_s": 904.0}, {"text": "language model applications. So here\u0027s a demo. In this demo I\u0027ll", "timestamp": "00:15:08,194", "timestamp_s": 908.0}, {"text": "show you a key benefit of using webassembly", "timestamp": "00:15:12,082", "timestamp_s": 912.0}, {"text": "or wasm to run ARM applications. Portability.", "timestamp": "00:15:15,186", "timestamp_s": 915.0}, {"text": "The webassembly application is a binary application that is directly", "timestamp": "00:15:18,634", "timestamp_s": 918.0}, {"text": "portable across different operating systems,", "timestamp": "00:15:22,202", "timestamp_s": 922.0}, {"text": "hardwares and drivers. So on this screen I have", "timestamp": "00:15:25,058", "timestamp_s": 925.0}, {"text": "opened a terminal to", "timestamp": "00:15:29,186", "timestamp_s": 929.0}, {"text": "my remote machine on Microsoft Azure. That machine runs Linux,", "timestamp": "00:15:33,338", "timestamp_s": 933.0}, {"text": "has Nvidia tiffo card and have the CudA twelve", "timestamp": "00:15:36,906", "timestamp_s": 936.0}, {"text": "driver installed on it. If I see what\u0027s in here,", "timestamp": "00:15:40,546", "timestamp_s": 940.0}, {"text": "you can see I have downloaded the large language model,", "timestamp": "00:15:45,514", "timestamp_s": 945.0}, {"text": "the Gamma two B model in GguI format,", "timestamp": "00:15:48,778", "timestamp_s": 948.0}, {"text": "and then a bunch of HTML and JavaScript files in the chatbot UI.", "timestamp": "00:15:52,234", "timestamp_s": 952.0}, {"text": "I have also installed the was image runtime here. So if I say wash,", "timestamp": "00:15:56,524", "timestamp_s": 956.0}, {"text": "it\u0027s was 13.5. So now we have the large language", "timestamp": "00:16:00,236", "timestamp_s": 960.0}, {"text": "model and we have the runtime for the large language model. We are", "timestamp": "00:16:04,076", "timestamp_s": 964.0}, {"text": "still missing the application. The application is something that", "timestamp": "00:16:07,148", "timestamp_s": 967.0}, {"text": "takes a user input, runs a large language model, prompting the larger language,", "timestamp": "00:16:11,156", "timestamp_s": 971.0}, {"text": "runs a large language model, generate a response and then send it back to the", "timestamp": "00:16:14,836", "timestamp_s": 974.0}, {"text": "user. So you can call it a chatbot application, it could be a web", "timestamp": "00:16:17,628", "timestamp_s": 977.0}, {"text": "application, it could be a discord, or it could be asian", "timestamp": "00:16:21,332", "timestamp_s": 981.0}, {"text": "application that connects to other applications that takes the large language", "timestamp": "00:16:25,056", "timestamp_s": 985.0}, {"text": "model output to do things. So the application", "timestamp": "00:16:28,984", "timestamp_s": 988.0}, {"text": "is a key part that as developers that you would write in", "timestamp": "00:16:34,504", "timestamp_s": 994.0}, {"text": "the past, you wouldn\u0027t expect this application to be portable because", "timestamp": "00:16:38,536", "timestamp_s": 998.0}, {"text": "when you develop this application you are probably on the Mac or Windows machine", "timestamp": "00:16:43,144", "timestamp_s": 1003.0}, {"text": "and you compared to say Apple silicon and use the metal", "timestamp": "00:16:46,600", "timestamp_s": 1006.0}, {"text": "framework, you have all this built in. And you wouldn\u0027t", "timestamp": "00:16:50,192", "timestamp_s": 1010.0}, {"text": "think that by just copying this binary application to", "timestamp": "00:16:54,002", "timestamp_s": 1014.0}, {"text": "Nvidia machine it would just run there, right? And of course", "timestamp": "00:16:58,026", "timestamp_s": 1018.0}, {"text": "there are ways to make it easier. For instance, if you use Python, Python has", "timestamp": "00:17:01,546", "timestamp_s": 1021.0}, {"text": "a lot of abstractions that allows you to write", "timestamp": "00:17:04,802", "timestamp_s": 1024.0}, {"text": "at a fairly high level so that you can write a Python script on", "timestamp": "00:17:09,834", "timestamp_s": 1029.0}, {"text": "Mac and then try to run this same script on", "timestamp": "00:17:13,970", "timestamp_s": 1033.0}, {"text": "the Linux machine. However, as you would imagine Python,", "timestamp": "00:17:18,715", "timestamp_s": 1038.0}, {"text": "in order to achieve that, Python have a huge amount of dependencies.", "timestamp": "00:17:21,963", "timestamp_s": 1041.0}, {"text": "Like if you go to the Pytorch official docker image, it is", "timestamp": "00:17:25,715", "timestamp_s": 1045.0}, {"text": "4gb right there. So the Pytorch docker image", "timestamp": "00:17:29,395", "timestamp_s": 1049.0}, {"text": "itself is 4gb and it\u0027s platform dependent,", "timestamp": "00:17:32,995", "timestamp_s": 1052.0}, {"text": "so you have to install the right Python version. And within Python", "timestamp": "00:17:36,379", "timestamp_s": 1056.0}, {"text": "you also have oftentimes you need to specify what is the underlying architecture.", "timestamp": "00:17:39,723", "timestamp_s": 1059.0}, {"text": "So it is huge dependency and it\u0027s not really that portable", "timestamp": "00:17:43,611", "timestamp_s": 1063.0}, {"text": "for any other languages like rust or go, or if you", "timestamp": "00:17:47,003", "timestamp_s": 1067.0}, {"text": "write your application in any of those other languages,", "timestamp": "00:17:50,688", "timestamp_s": 1070.0}, {"text": "you would not imagine that it would be portable. That\u0027s because the", "timestamp": "00:17:53,984", "timestamp_s": 1073.0}, {"text": "underlying GPU and CPU architectures are entirely different. What WASM", "timestamp": "00:17:58,200", "timestamp_s": 1078.0}, {"text": "does is that it provides abstraction for those applications", "timestamp": "00:18:01,904", "timestamp_s": 1081.0}, {"text": "so that it can run smoothly across all different platforms. In order to", "timestamp": "00:18:06,280", "timestamp_s": 1086.0}, {"text": "demonstrate that, I switch to a window. This is my,", "timestamp": "00:18:09,648", "timestamp_s": 1089.0}, {"text": "this is on my local machine, which is a MacBook,", "timestamp": "00:18:13,514", "timestamp_s": 1093.0}, {"text": "and what I have already downloaded one", "timestamp": "00:18:16,874", "timestamp_s": 1096.0}, {"text": "of the API server applications from the Lama Edge project which is a rust", "timestamp": "00:18:20,338", "timestamp_s": 1100.0}, {"text": "application. And I compiled it on my Mac and I tested it on", "timestamp": "00:18:24,050", "timestamp_s": 1104.0}, {"text": "my Mac, right? So now what I\u0027m going to do is I\u0027m going to just", "timestamp": "00:18:27,458", "timestamp_s": 1107.0}, {"text": "scp this entire file to the remote azure machine.", "timestamp": "00:18:31,954", "timestamp_s": 1111.0}, {"text": "So as you can see, the entire file is only nine megabytes and", "timestamp": "00:18:36,082", "timestamp_s": 1116.0}, {"text": "we didn\u0027t package it in any way, we didn\u0027t have a docker", "timestamp": "00:18:40,838", "timestamp_s": 1120.0}, {"text": "image around, you know, wrapped around it.", "timestamp": "00:18:44,430", "timestamp_s": 1124.0}, {"text": "We just scp the whole thing to another machine and", "timestamp": "00:18:47,894", "timestamp_s": 1127.0}, {"text": "with entirely different architecture in both hardware and software and", "timestamp": "00:18:52,070", "timestamp_s": 1132.0}, {"text": "expect to run there. Can we run there? So let\u0027s see. So we use", "timestamp": "00:18:55,510", "timestamp_s": 1135.0}, {"text": "the WASM edge runtime to run it.", "timestamp": "00:18:59,078", "timestamp_s": 1139.0}, {"text": "So the WASM edge runtime starts instantly. It\u0027s because", "timestamp": "00:19:02,534", "timestamp_s": 1142.0}, {"text": "it\u0027s an application, so it loads the large language model and then", "timestamp": "00:19:06,254", "timestamp_s": 1146.0}, {"text": "it starts an API server. The web server is actually accessible", "timestamp": "00:19:09,902", "timestamp_s": 1149.0}, {"text": "through port 8080. So if you have this machine,", "timestamp": "00:19:14,174", "timestamp_s": 1154.0}, {"text": "the port open public, you would be able to load up browser and go", "timestamp": "00:19:16,974", "timestamp_s": 1156.0}, {"text": "to port 880 and see it. But for", "timestamp": "00:19:20,358", "timestamp_s": 1160.0}, {"text": "now we want to just stay on this machine because we have it under a", "timestamp": "00:19:23,742", "timestamp_s": 1163.0}, {"text": "firewall. What are we going to try is that we\u0027re going to do API", "timestamp": "00:19:27,006", "timestamp_s": 1167.0}, {"text": "request, because this API server also takes open AI style", "timestamp": "00:19:30,286", "timestamp_s": 1170.0}, {"text": "API requests that allows us to integrate with all the", "timestamp": "00:19:34,430", "timestamp_s": 1174.0}, {"text": "openi ecosystem tools. So here\u0027s", "timestamp": "00:19:38,416", "timestamp_s": 1178.0}, {"text": "how the request looks like. So as you can see,", "timestamp": "00:19:42,184", "timestamp_s": 1182.0}, {"text": "we request at the localhost and then we send a", "timestamp": "00:19:46,312", "timestamp_s": 1186.0}, {"text": "message with a row of user and say where\u0027s Paris? We ask the model,", "timestamp": "00:19:49,640", "timestamp_s": 1189.0}, {"text": "where\u0027s Paris? Right. If I do this, it does the", "timestamp": "00:19:53,512", "timestamp_s": 1193.0}, {"text": "inference, its result come back before I can finish speaking.", "timestamp": "00:19:56,880", "timestamp_s": 1196.0}, {"text": "So the result is, the role of the result is", "timestamp": "00:20:00,848", "timestamp_s": 1200.0}, {"text": "system and the content is Paris located in hardware friends, blah, blah, blah.", "timestamp": "00:20:04,292", "timestamp_s": 1204.0}, {"text": "So now we have achieved something, I think very interesting,", "timestamp": "00:20:08,140", "timestamp_s": 1208.0}, {"text": "that we compiled a rust application on the", "timestamp": "00:20:11,196", "timestamp_s": 1211.0}, {"text": "Mac and fully taking advantage of the Mac GPU", "timestamp": "00:20:14,500", "timestamp_s": 1214.0}, {"text": "and the metal framework. And then we just copied this", "timestamp": "00:20:18,140", "timestamp_s": 1218.0}, {"text": "WASM application into a remote Linux machine", "timestamp": "00:20:21,700", "timestamp_s": 1221.0}, {"text": "running on Nvidia. And we can see this application,", "timestamp": "00:20:25,396", "timestamp_s": 1225.0}, {"text": "that\u0027s which the role of the application start server and interact", "timestamp": "00:20:28,460", "timestamp_s": 1228.0}, {"text": "with the underlying large", "timestamp": "00:20:32,106", "timestamp_s": 1232.0}, {"text": "language model runs just perfectly on", "timestamp": "00:20:35,882", "timestamp_s": 1235.0}, {"text": "the new hardware, fully utilizing the", "timestamp": "00:20:39,098", "timestamp_s": 1239.0}, {"text": "media GPU capabilities to accelerate. Without the GPU", "timestamp": "00:20:42,962", "timestamp_s": 1242.0}, {"text": "capabilities, you would not be able to have nearly 100", "timestamp": "00:20:47,074", "timestamp_s": 1247.0}, {"text": "tokens per second speed on this", "timestamp": "00:20:51,378", "timestamp_s": 1251.0}, {"text": "Linux machine. If you\u0027re just doing the cpu, you\u0027ll be more like one tokens", "timestamp": "00:20:55,186", "timestamp_s": 1255.0}, {"text": "per second, you know, it would be two orders,", "timestamp": "00:20:58,602", "timestamp_s": 1258.0}, {"text": "magnitude\u0027s difference. So that\u0027s it, that\u0027s what", "timestamp": "00:21:02,334", "timestamp_s": 1262.0}, {"text": "we have shown, that the WASM application is truly portable.", "timestamp": "00:21:06,182", "timestamp_s": 1266.0}, {"text": "All right, to recap the demo,", "timestamp": "00:21:09,694", "timestamp_s": 1269.0}, {"text": "there\u0027s so for the longest time we", "timestamp": "00:21:13,262", "timestamp_s": 1273.0}, {"text": "have platform engineering, or DevOps,", "timestamp": "00:21:17,062", "timestamp_s": 1277.0}, {"text": "that combines the role of developer and Ops. But with the new", "timestamp": "00:21:20,502", "timestamp_s": 1280.0}, {"text": "hardware, with more and more different devices and different drivers,", "timestamp": "00:21:24,574", "timestamp_s": 1284.0}, {"text": "all that stuff, that\u0027s that coming along for AI applications.", "timestamp": "00:21:28,102", "timestamp_s": 1288.0}, {"text": "And I think it\u0027s time to separate the dev and Ops role all", "timestamp": "00:21:31,578", "timestamp_s": 1291.0}, {"text": "over again. So the way it works is that", "timestamp": "00:21:35,450", "timestamp_s": 1295.0}, {"text": "Webassembly is a virtual machine format. You can think of it", "timestamp": "00:21:39,466", "timestamp_s": 1299.0}, {"text": "like a Java Java bytecode, so it provides abstraction over", "timestamp": "00:21:43,442", "timestamp_s": 1303.0}, {"text": "the real hardware. And for developers, you just need to", "timestamp": "00:21:47,090", "timestamp_s": 1307.0}, {"text": "write to the Webassembly interface. In our case, you write", "timestamp": "00:21:50,882", "timestamp_s": 1310.0}, {"text": "to the Lama edge SDK interface and it", "timestamp": "00:21:54,282", "timestamp_s": 1314.0}, {"text": "tells the SDK to say load a model and", "timestamp": "00:21:57,722", "timestamp_s": 1317.0}, {"text": "do the inference. And the", "timestamp": "00:22:01,442", "timestamp_s": 1321.0}, {"text": "developer only need to write application this way. So if the application,", "timestamp": "00:22:05,450", "timestamp_s": 1325.0}, {"text": "once the application is compiled to WASM, the developer\u0027s", "timestamp": "00:22:08,906", "timestamp_s": 1328.0}, {"text": "job is done. He or she can ship the application anywhere that", "timestamp": "00:22:12,410", "timestamp_s": 1332.0}, {"text": "they want and lets the runtime takes", "timestamp": "00:22:16,010", "timestamp_s": 1336.0}, {"text": "over the rest. So it\u0027s the Ops people that", "timestamp": "00:22:19,890", "timestamp_s": 1339.0}, {"text": "needs to install the correct runtime and driver for each device.", "timestamp": "00:22:23,482", "timestamp_s": 1343.0}, {"text": "So for instance, on a Mac we want to install the Mac version of was", "timestamp": "00:22:26,690", "timestamp_s": 1346.0}, {"text": "made. It\u0027s sort of like Java. On the Mac you need to install the Mac", "timestamp": "00:22:29,956", "timestamp_s": 1349.0}, {"text": "version of Java, right, the JVM, right. You know, so it\u0027s the", "timestamp": "00:22:33,612", "timestamp_s": 1353.0}, {"text": "same thing here. So you want to install the Mac version of the was runtime.", "timestamp": "00:22:36,916", "timestamp_s": 1356.0}, {"text": "If you have Nvidia device, you need to Ubuntu with CUDA twelve,", "timestamp": "00:22:40,924", "timestamp_s": 1360.0}, {"text": "you need to install the appropriate was runtime", "timestamp": "00:22:45,020", "timestamp_s": 1365.0}, {"text": "in there as well. So for the Ops guys, they can,", "timestamp": "00:22:48,660", "timestamp_s": 1368.0}, {"text": "once they take care of that, they would be able to just run", "timestamp": "00:22:52,844", "timestamp_s": 1372.0}, {"text": "that wasm application without any modification.", "timestamp": "00:22:55,996", "timestamp_s": 1375.0}, {"text": "Because the WASM edge runtime has a contract, has a", "timestamp": "00:22:59,250", "timestamp_s": 1379.0}, {"text": "standard API that\u0027s, that is defined in the", "timestamp": "00:23:02,650", "timestamp_s": 1382.0}, {"text": "llama HSDK, right? You know, so once it sees those instructions to", "timestamp": "00:23:06,058", "timestamp_s": 1386.0}, {"text": "say, load a model and you know, send some text to the model", "timestamp": "00:23:10,082", "timestamp_s": 1390.0}, {"text": "for inference, and it sees those instructions,", "timestamp": "00:23:13,570", "timestamp_s": 1393.0}, {"text": "the byte code, you automatically run those code and it would translate those code", "timestamp": "00:23:17,074", "timestamp_s": 1397.0}, {"text": "into that\u0027s instructions that are appropriate for the", "timestamp": "00:23:21,250", "timestamp_s": 1401.0}, {"text": "underlying accelerator and the drivers. So it allows developers", "timestamp": "00:23:24,622", "timestamp_s": 1404.0}, {"text": "to write truly portable applications that can be deployed anywhere,", "timestamp": "00:23:28,942", "timestamp_s": 1408.0}, {"text": "and it can be managed by tools like kubernetes and", "timestamp": "00:23:32,718", "timestamp_s": 1412.0}, {"text": "let ops people worry about installing the right driver and", "timestamp": "00:23:36,814", "timestamp_s": 1416.0}, {"text": "installing the right was match runtime on every single node or", "timestamp": "00:23:40,574", "timestamp_s": 1420.0}, {"text": "every single edge devices that you have in your cluster.", "timestamp": "00:23:44,702", "timestamp_s": 1424.0}, {"text": "So that leads us to our last demo, because we", "timestamp": "00:23:48,634", "timestamp_s": 1428.0}, {"text": "have talked about Lama Edge being a developer platform,", "timestamp": "00:23:52,378", "timestamp_s": 1432.0}, {"text": "and one of the most popular applications people do with Python, at least", "timestamp": "00:23:56,170", "timestamp_s": 1436.0}, {"text": "today, is what they call a rack application, meaning that", "timestamp": "00:23:59,986", "timestamp_s": 1439.0}, {"text": "you use a standard large language model, a fine tuned large", "timestamp": "00:24:03,394", "timestamp_s": 1443.0}, {"text": "language model, but you feed it with your proprietary", "timestamp": "00:24:07,026", "timestamp_s": 1447.0}, {"text": "knowledge base. The knowledge base was divided,", "timestamp": "00:24:11,058", "timestamp_s": 1451.0}, {"text": "it\u0027s typically a text, a PDF or image, or you", "timestamp": "00:24:15,142", "timestamp_s": 1455.0}, {"text": "know, or a text file, and it was divided into segments and each", "timestamp": "00:24:18,566", "timestamp_s": 1458.0}, {"text": "segment was turned into a vector and stored in a vector database.", "timestamp": "00:24:22,614", "timestamp_s": 1462.0}, {"text": "And when the user", "timestamp": "00:24:26,382", "timestamp_s": 1466.0}, {"text": "asks a new question from the API, the application", "timestamp": "00:24:30,222", "timestamp_s": 1470.0}, {"text": "would take, would take that question, turn that into", "timestamp": "00:24:33,854", "timestamp_s": 1473.0}, {"text": "vector as well, and perform a vector search in the database to find out", "timestamp": "00:24:37,422", "timestamp_s": 1477.0}, {"text": "which other, which texts in the knowledge base are most related", "timestamp": "00:24:41,206", "timestamp_s": 1481.0}, {"text": "to the question. And then it would add the context that retracted", "timestamp": "00:24:47,054", "timestamp_s": 1487.0}, {"text": "from the knowledge base and the new question into the prompt,", "timestamp": "00:24:51,382", "timestamp_s": 1491.0}, {"text": "and asks the large language model to give an answer. As you", "timestamp": "00:24:54,382", "timestamp_s": 1494.0}, {"text": "can see, this is a fairly complex process, and it involves", "timestamp": "00:24:57,886", "timestamp_s": 1497.0}, {"text": "not just the large language model, not just the runtime, but also", "timestamp": "00:25:01,958", "timestamp_s": 1501.0}, {"text": "things like the vector database, the embedding model and all", "timestamp": "00:25:05,334", "timestamp_s": 1505.0}, {"text": "that. Things that you have to tie together,", "timestamp": "00:25:08,662", "timestamp_s": 1508.0}, {"text": "as we talked about previously in this talk, is that", "timestamp": "00:25:12,974", "timestamp_s": 1512.0}, {"text": "things like that was traditionally done by say fairly complicated", "timestamp": "00:25:17,126", "timestamp_s": 1517.0}, {"text": "Python programs that does the orchestration, the queries,", "timestamp": "00:25:21,670", "timestamp_s": 1521.0}, {"text": "the turn into act with embeddings and all that stuff,", "timestamp": "00:25:25,150", "timestamp_s": 1525.0}, {"text": "and then you attach another UI in front of it. So it\u0027s a fairly involved", "timestamp": "00:25:28,774", "timestamp_s": 1528.0}, {"text": "and a complicated process. But with Lama Edge, we will", "timestamp": "00:25:32,822", "timestamp_s": 1532.0}, {"text": "be able to build a single application that can", "timestamp": "00:25:36,414", "timestamp_s": 1536.0}, {"text": "talk to the vector database, call the embeddings when it\u0027s needed,", "timestamp": "00:25:41,094", "timestamp_s": 1541.0}, {"text": "and perform the vector search, and then at", "timestamp": "00:25:45,194", "timestamp_s": 1545.0}, {"text": "the end prompts a large language model to get an answer.", "timestamp": "00:25:49,330", "timestamp_s": 1549.0}, {"text": "So this project, we call it an", "timestamp": "00:25:52,786", "timestamp_s": 1552.0}, {"text": "integrated assistant API server, which because it looks", "timestamp": "00:25:55,890", "timestamp_s": 1555.0}, {"text": "a lot like open eyes assistant API and", "timestamp": "00:25:59,770", "timestamp_s": 1559.0}, {"text": "we call this project the Garnet. And here\u0027s a demo for that.", "timestamp": "00:26:04,130", "timestamp_s": 1564.0}, {"text": "Hi. In this demo I\u0027ll show you the easiest way to", "timestamp": "00:26:08,964", "timestamp_s": 1568.0}, {"text": "run reg or RaC API server", "timestamp": "00:26:12,692", "timestamp_s": 1572.0}, {"text": "for a large language model. So if you are familiar with RAC,", "timestamp": "00:26:16,740", "timestamp_s": 1576.0}, {"text": "it is, it is a way to add knowledge.", "timestamp": "00:26:21,108", "timestamp_s": 1581.0}, {"text": "It could be additional public knowledge or proprietary", "timestamp": "00:26:25,748", "timestamp_s": 1585.0}, {"text": "knowledge that you don\u0027t want other people to see to an existing", "timestamp": "00:26:29,700", "timestamp_s": 1589.0}, {"text": "large language model, so that the larger language model can answer questions and chat", "timestamp": "00:26:32,924", "timestamp_s": 1592.0}, {"text": "with people based on additional context that you provide to it.", "timestamp": "00:26:36,652", "timestamp_s": 1596.0}, {"text": "So in order to do that, a typical rack", "timestamp": "00:26:40,444", "timestamp_s": 1600.0}, {"text": "setup requires a fairly complex setup that requires say", "timestamp": "00:26:43,940", "timestamp_s": 1603.0}, {"text": "install digital vector database UI, how to upload", "timestamp": "00:26:48,924", "timestamp_s": 1608.0}, {"text": "the knowledge and the tools like Lanchain", "timestamp": "00:26:52,452", "timestamp_s": 1612.0}, {"text": "to orchestrate how to manage retrieve data from the vector", "timestamp": "00:26:56,356", "timestamp_s": 1616.0}, {"text": "database and how to prompt application.", "timestamp": "00:27:00,060", "timestamp_s": 1620.0}, {"text": "So in our approach that we want to introduce a project called", "timestamp": "00:27:04,364", "timestamp_s": 1624.0}, {"text": "Gayanet and the Garnet is an application that build on was", "timestamp": "00:27:08,320", "timestamp_s": 1628.0}, {"text": "matched and also using the Lama edge", "timestamp": "00:27:11,888", "timestamp_s": 1631.0}, {"text": "framework. So what it does is that it allows", "timestamp": "00:27:15,592", "timestamp_s": 1635.0}, {"text": "applications, you can write simple", "timestamp": "00:27:19,312", "timestamp_s": 1639.0}, {"text": "rock applications using rust and then compile it into a single", "timestamp": "00:27:23,168", "timestamp_s": 1643.0}, {"text": "wasm binary with zero python dependency and", "timestamp": "00:27:27,704", "timestamp_s": 1647.0}, {"text": "run it very efficiently at the server. So let\u0027s see how", "timestamp": "00:27:31,224", "timestamp_s": 1651.0}, {"text": "it works. If you go to the Garnet GitHub repository, by the way,", "timestamp": "00:27:34,656", "timestamp_s": 1654.0}, {"text": "if you are here, just give us a star.", "timestamp": "00:27:38,150", "timestamp_s": 1658.0}, {"text": "And here is a quick start guide and", "timestamp": "00:27:41,534", "timestamp_s": 1661.0}, {"text": "this is called Gaia node. And there is a one line", "timestamp": "00:27:46,462", "timestamp_s": 1666.0}, {"text": "of command which you can use to install Gaia which let\u0027s", "timestamp": "00:27:49,950", "timestamp_s": 1669.0}, {"text": "do that and explain what it does. So this is on my local Mac machine.", "timestamp": "00:27:55,030", "timestamp_s": 1675.0}, {"text": "And if I say install, what it does is that it installs the", "timestamp": "00:27:59,230", "timestamp_s": 1679.0}, {"text": "was match runtime which is required to run a", "timestamp": "00:28:02,558", "timestamp_s": 1682.0}, {"text": "large language model. It installs a quadrant vector", "timestamp": "00:28:05,906", "timestamp_s": 1685.0}, {"text": "database which is required, and it\u0027s download a chat", "timestamp": "00:28:09,722", "timestamp_s": 1689.0}, {"text": "model which it has already downloaded. We call it a", "timestamp": "00:28:13,314", "timestamp_s": 1693.0}, {"text": "standard Lama two seven b chat model and download the embedding model", "timestamp": "00:28:17,090", "timestamp_s": 1697.0}, {"text": "which used to process the vector.", "timestamp": "00:28:20,770", "timestamp_s": 1700.0}, {"text": "But here what\u0027s really interesting, you can ignore the error here,", "timestamp": "00:28:24,794", "timestamp_s": 1704.0}, {"text": "what\u0027s really interesting is that it also downloads a knowledge collection", "timestamp": "00:28:28,082", "timestamp_s": 1708.0}, {"text": "which is a knowledge base we vectorized into", "timestamp": "00:28:31,618", "timestamp_s": 1711.0}, {"text": "the quadrant format. You can read in the documentation how", "timestamp": "00:28:34,736", "timestamp_s": 1714.0}, {"text": "we do that, and we have a separate rust application that", "timestamp": "00:28:38,032", "timestamp_s": 1718.0}, {"text": "helps you to do that. And then it", "timestamp": "00:28:42,232", "timestamp_s": 1722.0}, {"text": "installs that snapshot into the quantum database.", "timestamp": "00:28:46,536", "timestamp_s": 1726.0}, {"text": "So what it does is that it creates a Gaia net", "timestamp": "00:28:51,384", "timestamp_s": 1731.0}, {"text": "directory in your home directory", "timestamp": "00:28:55,400", "timestamp_s": 1735.0}, {"text": "and put everything in there and including this wasm file", "timestamp": "00:28:59,056", "timestamp_s": 1739.0}, {"text": "that starts up the rack application server. And here", "timestamp": "00:29:02,556", "timestamp_s": 1742.0}, {"text": "if you look at the config JSon,", "timestamp": "00:29:07,468", "timestamp_s": 1747.0}, {"text": "you will be able to see the chat model that\u0027s being used.", "timestamp": "00:29:11,244", "timestamp_s": 1751.0}, {"text": "The parameters for the chat model and the snapshot", "timestamp": "00:29:14,284", "timestamp_s": 1754.0}, {"text": "is knowledge", "timestamp": "00:29:18,524", "timestamp_s": 1758.0}, {"text": "base that vertebrates knowledge base and the prompt and you can change,", "timestamp": "00:29:22,364", "timestamp_s": 1762.0}, {"text": "you can modify any of the things that you want, use a different model,", "timestamp": "00:29:26,084", "timestamp_s": 1766.0}, {"text": "use a different knowledge base and rerun install. Right.", "timestamp": "00:29:29,068", "timestamp_s": 1769.0}, {"text": "So once you have the, once you have everything installed that", "timestamp": "00:29:32,584", "timestamp_s": 1772.0}, {"text": "you would be able to say run", "timestamp": "00:29:36,120", "timestamp_s": 1776.0}, {"text": "the start script. What it does is that it\u0027s going to start the vector", "timestamp": "00:29:41,160", "timestamp_s": 1781.0}, {"text": "database like we said, and they\u0027re going to start the was image application", "timestamp": "00:29:44,600", "timestamp_s": 1784.0}, {"text": "server and it\u0027s going to start a domain server which give it", "timestamp": "00:29:47,976", "timestamp_s": 1787.0}, {"text": "the public access for domain for the server.", "timestamp": "00:29:51,160", "timestamp_s": 1791.0}, {"text": "Right. So what are we going to do is that we\u0027re going to open while", "timestamp": "00:29:54,184", "timestamp_s": 1794.0}, {"text": "this is runs on our local machine. I can use localhost, but I can", "timestamp": "00:29:58,464", "timestamp_s": 1798.0}, {"text": "because it gives me a publicly accessible domain. So what I\u0027m going to do is", "timestamp": "00:30:02,328", "timestamp_s": 1802.0}, {"text": "that I\u0027m going to go to load this public accessible domain.", "timestamp": "00:30:05,608", "timestamp_s": 1805.0}, {"text": "So it gives me how to run access API.", "timestamp": "00:30:10,024", "timestamp_s": 1810.0}, {"text": "But what I would do is that I would just chat with the node while", "timestamp": "00:30:13,104", "timestamp_s": 1813.0}, {"text": "it\u0027s loading. I would come back here and tell you and open", "timestamp": "00:30:16,240", "timestamp_s": 1816.0}, {"text": "up the log here.", "timestamp": "00:30:20,184", "timestamp_s": 1820.0}, {"text": "So this is a lama edge startup log which", "timestamp": "00:30:31,284", "timestamp_s": 1831.0}, {"text": "logs all the interactions with large language model,", "timestamp": "00:30:34,748", "timestamp_s": 1834.0}, {"text": "right? So now it\u0027s,", "timestamp": "00:30:37,876", "timestamp_s": 1837.0}, {"text": "the UI has come up. I want to demonstrate to", "timestamp": "00:30:41,180", "timestamp_s": 1841.0}, {"text": "you that it does use a knowledge base to chat with us. So if I", "timestamp": "00:30:44,508", "timestamp_s": 1844.0}, {"text": "say where\u0027s Paris? Because the knowledge base I\u0027m using right", "timestamp": "00:30:48,188", "timestamp_s": 1848.0}, {"text": "now by default is a", "timestamp": "00:30:52,750", "timestamp_s": 1852.0}, {"text": "guide, the knowledge comes from Paris guidebook.", "timestamp": "00:30:56,710", "timestamp_s": 1856.0}, {"text": "So I vectorized that Paris guidebook and put that into the vector", "timestamp": "00:31:00,870", "timestamp_s": 1860.0}, {"text": "database so that the large language model", "timestamp": "00:31:04,382", "timestamp_s": 1864.0}, {"text": "can generate answers for me. So it says thank you for the", "timestamp": "00:31:08,030", "timestamp_s": 1868.0}, {"text": "question based on the context. Paris located at north central part", "timestamp": "00:31:11,550", "timestamp_s": 1871.0}, {"text": "of France, blah blah, blah blah. How do", "timestamp": "00:31:14,670", "timestamp_s": 1874.0}, {"text": "we know that this is actually using", "timestamp": "00:31:17,866", "timestamp_s": 1877.0}, {"text": "our context? How does we know that our RAC", "timestamp": "00:31:21,362", "timestamp_s": 1881.0}, {"text": "application server works? We go back to the log and we can", "timestamp": "00:31:24,922", "timestamp_s": 1884.0}, {"text": "see the actual prompt. So you can see this is the", "timestamp": "00:31:28,466", "timestamp_s": 1888.0}, {"text": "actual prompt the application server sends to the database. So you", "timestamp": "00:31:32,818", "timestamp_s": 1892.0}, {"text": "are helpful assistant. And then here\u0027s the context. If you don\u0027t", "timestamp": "00:31:35,898", "timestamp_s": 1895.0}, {"text": "know if the answer is not in the context, you don\u0027t answer it. So those", "timestamp": "00:31:39,714", "timestamp_s": 1899.0}, {"text": "three paragraph context come from our vector database.", "timestamp": "00:31:42,786", "timestamp_s": 1902.0}, {"text": "That\u0027s come from the Paris guidebook. It\u0027s about general information about", "timestamp": "00:31:46,954", "timestamp_s": 1906.0}, {"text": "Paris. And then we ask question, where\u0027s Paris? Right. You know,", "timestamp": "00:31:50,994", "timestamp_s": 1910.0}, {"text": "so it\u0027s answers based on that, based on that context.", "timestamp": "00:31:54,442", "timestamp_s": 1914.0}, {"text": "So you can ask follow up questions. Of course you can plan when they trip", "timestamp": "00:31:58,914", "timestamp_s": 1918.0}, {"text": "or whatever, but. And you can also use it as open AI", "timestamp": "00:32:02,074", "timestamp_s": 1922.0}, {"text": "API server. As you can imagine, you can have your knowledge base", "timestamp": "00:32:06,082", "timestamp_s": 1926.0}, {"text": "about a code repository and then use", "timestamp": "00:32:09,930", "timestamp_s": 1929.0}, {"text": "a fine tuned model that generates code or jSon, and have this", "timestamp": "00:32:13,270", "timestamp_s": 1933.0}, {"text": "packaged together as an API server that connects to a chatbot or", "timestamp": "00:32:17,190", "timestamp_s": 1937.0}, {"text": "something like that. That allows you to build", "timestamp": "00:32:21,454", "timestamp_s": 1941.0}, {"text": "an entirely portable application that can", "timestamp": "00:32:25,014", "timestamp_s": 1945.0}, {"text": "run across many different GPU architectures and drivers without", "timestamp": "00:32:29,014", "timestamp_s": 1949.0}, {"text": "any need for large dependencies like Python.", "timestamp": "00:32:32,886", "timestamp_s": 1952.0}, {"text": "All right, so that brings the end to our", "timestamp": "00:32:37,094", "timestamp_s": 1957.0}, {"text": "talk. I think we\u0027re going to have 30 minutes. And so we", "timestamp": "00:32:41,362", "timestamp_s": 1961.0}, {"text": "have done three different demos, from the", "timestamp": "00:32:46,258", "timestamp_s": 1966.0}, {"text": "easy to the most sophisticated. And I hope you", "timestamp": "00:32:49,682", "timestamp_s": 1969.0}, {"text": "would have time to at least install Lama", "timestamp": "00:32:52,938", "timestamp_s": 1972.0}, {"text": "edge on your own computer and install large language models so you can play with", "timestamp": "00:32:56,754", "timestamp_s": 1976.0}, {"text": "it. And if you find it interesting, you could install garnet", "timestamp": "00:32:59,650", "timestamp_s": 1979.0}, {"text": "node and build your own knowledge base and have", "timestamp": "00:33:04,044", "timestamp_s": 1984.0}, {"text": "a large language model on your own device, answer questions in the way that you", "timestamp": "00:33:07,604", "timestamp_s": 1987.0}, {"text": "want. So yeah,", "timestamp": "00:33:10,884", "timestamp_s": 1990.0}, {"text": "there are a lot more that we can get into. So for instance,", "timestamp": "00:33:14,364", "timestamp_s": 1994.0}, {"text": "what\u0027s the rust application look like, how does SDK look like? And things", "timestamp": "00:33:17,596", "timestamp_s": 1997.0}, {"text": "like that. But I don\u0027t think we have time for that at this moment.", "timestamp": "00:33:21,044", "timestamp_s": 2001.0}, {"text": "And if you\u0027re interested, we have those three open", "timestamp": "00:33:24,100", "timestamp_s": 2004.0}, {"text": "source GitHub repositories that you can,", "timestamp": "00:33:27,900", "timestamp_s": 2007.0}, {"text": "that you can go to.", "timestamp": "00:33:31,754", "timestamp_s": 2011.0}, {"text": "And please feel free to raise the issue and find", "timestamp": "00:33:34,514", "timestamp_s": 2014.0}, {"text": "us there and chat with us. All right, thank you so much.", "timestamp": "00:33:37,882", "timestamp_s": 2017.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'O0TRkjrDh78',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Self-hosted LLMs across all your devices and GPUs
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>Fast and Portable Llama2 Inference on the Heterogeneous Edge: An alternative to Python. Compared with Python, Rust+Wasm apps could be 1/100 of the size, 100 times the speed, and, most importantly, securely run everywhere at full hardware acceleration without any change to the binary code.</p>
<!-- End Text -->
          </div>

          
          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
              <li>
                The title is self hosted arms across all your devices and GPU's. We're going to primarily use three open source projects. Lama Edge is open source project that build upon was matched. Gaia Net. Gaynet further builds upon Lama Edge to provide what we call a rag application server.

              </li>
              
              <li>
                Using Lama H in this demo I'll show you the easiest way to run a large language model on your own computer. You can actually run any large language models that you find on hugging face. And you can use that as an API server to do a lot of things with it.

              </li>
              
              <li>
                The lamo edge application server supports over 6000 large language models. It also provides enough flexibility to run multiple models in the same server. It is not really designed to be, to be a standalone server to run large language model. You can write your own code to extend it and build your own applications on top of it.

              </li>
              
              <li>
                Using webassembly or wasm to run ARM applications. A binary application that is directly portable across different operating systems, hardwares and drivers. What WASM does is that it provides abstraction for those applications so that it can run smoothly across all different platforms.

              </li>
              
              <li>
                Lama Edge is a developer platform for Python. It allows applications to talk to a large language model using a knowledge base. Here's a demo showing the easiest way to run an API server for such a model.

              </li>
              
              <li>
                All right, so that brings the end to our talk. We have done three different demos, from the easy to the most sophisticated. And I hope you would have time to at least install Lama edge on your own computer and install large language models so you can play with it. There are a lot more that we can get into.
              </li>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./assemblyai/O0TRkjrDh78.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:20,760'); seek(20.0)">
              Hello, welcome to my talk. The title is self hosted arms
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:25,102'); seek(25.0)">
              across all your devices and GPU's. So in this talk we're
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:29,038'); seek(29.0)">
              going to talk about self hosted oems and we're going
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:32,958'); seek(32.0)">
              to primarily use three open source projects.
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:36,510'); seek(36.0)">
              One is called was match and this is the CNC project that
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:40,286'); seek(40.0)">
              I founded and it is a webassembly
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:44,094'); seek(44.0)">
              runtime that we see great adoption in
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:47,670'); seek(47.0)">
              AI and the large language model space. And then the
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:50,838'); seek(50.0)">
              second project is called Lama Edge. Lama Edge is open source project that build
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:54,406'); seek(54.0)">
              upon was matched. So it provides tools
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:00:58,532'); seek(58.0)">
              and SDKs and programming primaries for building
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:01:02,748'); seek(62.0)">
              large language model applications, especially around the llama two model on
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:01:06,588'); seek(66.0)">
              the WASM edge app runtime itself. So when it's a runtime you
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:01:10,116'); seek(70.0)">
              can see this as operating system. And then Llamaedge is infrastructure
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:01:14,764'); seek(74.0)">
              that specifically for building web, for building arm applications on
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:01:18,188'); seek(78.0)">
              top of WASM edge. And then the third project is Gaia. Net. Gaynet further
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:22,388'); seek(82.0)">
              builds upon Lama Edge to provide what we
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:25,690'); seek(85.0)">
              call a rag application server, meaning that
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:29,026'); seek(89.0)">
              we supplement the large language model with real world knowledge
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:33,122'); seek(93.0)">
              base, either from public data, from public knowledge, or from our proprietary knowledge.
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:37,538'); seek(97.0)">
              And that just makes AI models and applications
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:41,666'); seek(101.0)">
              that associated with that much more useful. So it's designed
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:45,522'); seek(105.0)">
              for use with AI agent applications and such.
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:51,574'); seek(111.0)">
              Well, start off the talk with a demo of how easy it is to run
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:55,814'); seek(115.0)">
              a chatbot using the open source arm on your own device.
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:58,854'); seek(118.0)">
              Using Lama H in this demo
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:02:02,358'); seek(122.0)">
              I'll show you the easiest way to run a large language model on your own
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:02:05,590'); seek(125.0)">
              computer. First go to the Lamaedge Llamaedge
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:02:09,822'); seek(129.0)">
              repository on GitHub. It is an entirely open source project.
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:02:13,654'); seek(133.0)">
              So in the readme file while you are here just
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:02:17,648'); seek(137.0)">
              give it a start and in the readme file
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:02:21,224'); seek(141.0)">
              you will see a quick start guide and there's just one line of command.
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:02:25,064'); seek(145.0)">
              Copy this and then go to
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:28,248'); seek(148.0)">
              your Mac or Linux computer or even windows with WSl
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:31,920'); seek(151.0)">
              to run this command. What it does is that
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:35,448'); seek(155.0)">
              can install a large language model runtime called the Wasmatch
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:40,344'); seek(160.0)">
              plugins that required to run was to
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:43,582'); seek(163.0)">
              run the large language model and it goes
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:46,622'); seek(166.0)">
              fast here. But as you can see it downloads a large
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:50,494'); seek(170.0)">
              language model called Gamma two B which is one of Google's
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:54,558'); seek(174.0)">
              large language models. And you can actually run any
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:02:59,078'); seek(179.0)">
              large language model that you find on hugging
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:03:02,310'); seek(182.0)">
              face. There's over 5000 of those. And just
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:03:06,406'); seek(186.0)">
              go back to the readme and, and look for the parameters that
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:03:09,930'); seek(189.0)">
              you can pass to the script where you can specify the model which even as
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:03:13,258'); seek(193.0)">
              a hugging based URL. And once the model is started it says
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:18,434'); seek(198.0)">
              the application starts a local web server on your own device,
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:22,722'); seek(202.0)">
              and it's started on port 8080. So what you can do
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:26,586'); seek(206.0)">
              is that you can go to that server and
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:30,090'); seek(210.0)">
              it would load a URL. And it's
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:34,230'); seek(214.0)">
              the URL said lama h chat. Now you can start to chat with it.
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:37,294'); seek(217.0)">
              Say, where is the capital
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:41,942'); seek(221.0)">
              of Japan?
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:46,934'); seek(226.0)">
              The first time it is a little slow because it needs to load the
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:50,238'); seek(230.0)">
              model into memory, but it's fast enough. The capital of Japan is Tokyo,
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:54,134'); seek(234.0)">
              and it's now you can ask what
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:59,912'); seek(239.0)">
              about the USA? And it clearly shows
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:04:04,032'); seek(244.0)">
              that the model understands
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:04:07,408'); seek(247.0)">
              the conversation. Right? Because the second question I did not tell
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:04:10,696'); seek(250.0)">
              to find the capital of the USA. I said, what about the USA? It knows
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:04:14,392'); seek(254.0)">
              about Washington DC because
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:04:17,768'); seek(257.0)">
              it knows about the previous conversation.
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:21,352'); seek(261.0)">
              If we go back to the log, we can see that it's generating
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:25,816'); seek(265.0)">
              with the gamma two b. It was generating over 60 tokens
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:29,096'); seek(269.0)">
              per second. That's a lot faster than a human can speak.
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:32,382'); seek(272.0)">
              A human typically speaks when I'm speaking. Now it's about three
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:36,422'); seek(276.0)">
              to five tokens per second. So it's ten times faster than human
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:39,654'); seek(279.0)">
              can speak. Now I can ask a longer question.
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:48,294'); seek(288.0)">
              Plan me a one day trip there, meaning one
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:53,174'); seek(293.0)">
              day trip to Japan. Okay, so it's,
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:56,784'); seek(296.0)">
              I should have said Washington DC, but you know, that's close enough.
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:05:00,768'); seek(300.0)">
              You know, that's, oh, it's giving me two days as well. But this
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:05:04,472'); seek(304.0)">
              is one of, because this is a very small, it's a two b
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:05:07,752'); seek(307.0)">
              model. If you change it to a seven B model or even
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:05:11,184'); seek(311.0)">
              larger model, you're going to see. You're going to see it respond a
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:05:14,688'); seek(314.0)">
              lot better. But even so, it is still pretty good. It gave me
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:05:18,416'); seek(318.0)">
              itinerary of on day four. Now it's maybe
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:05:21,792'); seek(321.0)">
              a week in Japan. Right? So that's it.
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:05:24,796'); seek(324.0)">
              That's how you run a large language model on your own computer. This is,
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:27,844'); seek(327.0)">
              I'm running it on the Mac and I'm recording this video while running it.
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:31,532'); seek(331.0)">
              And so it's even then I
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:34,876'); seek(334.0)">
              can get 60 tokens per second.
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:38,140'); seek(338.0)">
              So it is one of the easiest way and the fastest way for
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:41,828'); seek(341.0)">
              you to run again. Yeah, it gave me the whole
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:45,364'); seek(345.0)">
              whole week. So again,
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:49,428'); seek(349.0)">
              the URL is GitHub Lama edge.
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:52,638'); seek(352.0)">
              Lama Edge. And they're just one line
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:56,310'); seek(356.0)">
              of command. And once you get started running, you can run other models by reading
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:05:59,662'); seek(359.0)">
              the rest of the readme. And you can use that as an API server and
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:06:03,318'); seek(363.0)">
              you can do a lot of things with it. Okay, now you have seen the
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:06:06,598'); seek(366.0)">
              demo. So what's it all for? We have seen
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:06:10,142'); seek(370.0)">
              that you can run a large language model application, you can chat with it
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:06:13,782'); seek(373.0)">
              on your own device, but what's it for? Most people just use
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:06:17,494'); seek(377.0)">
              OpenAI for this purpose and OpenAI also provided SDK API,
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:21,348'); seek(381.0)">
              right? So you can build applications around it as well. So why is there
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:25,092'); seek(385.0)">
              need to run those large language, open source, large language models on your
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:28,748'); seek(388.0)">
              own device? Well, so there are a couple of reasons. The first,
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:32,908'); seek(392.0)">
              biggest reason really is that OpenAI or any other larger
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:06:37,068'); seek(397.0)">
              language providers, basically it takes a one size fits all approach.
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:06:40,788'); seek(400.0)">
              So basically they're using the largest model for the smallest tasks. So even
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:06:44,324'); seek(404.0)">
              if you ask for something that can be easily handled by a smaller model,
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:06:47,364'); seek(407.0)">
              it will still use the largest model, which generates a lot of waste.
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:51,564'); seek(411.0)">
              It was very expensive and also makes the models very difficult
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:06:54,980'); seek(414.0)">
              to fine tune because it's much harder to fine tune a large model than
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:06:58,508'); seek(418.0)">
              a smaller model and because they're running on other people's server,
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:07:02,004'); seek(422.0)">
              because you can't, because the model is too big to run on server. So it's
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:07:04,612'); seek(424.0)">
              lack of privacy and control. And perhaps more interesting,
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:07:08,156'); seek(428.0)">
              and I think people are noticing this more and more, is censorship and bias because
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:07:12,900'); seek(432.0)">
              those companies like Openi or Microsoft all
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:07:16,500'); seek(436.0)">
              have their own political stand and agendas. So you would
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:20,228'); seek(440.0)">
              find a lot of questions that those models refuse to answer.
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:07:24,644'); seek(444.0)">
              There's very compelling needs for
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:07:28,220'); seek(448.0)">
              enterprises or users to run their own models, preferably on
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:07:31,900'); seek(451.0)">
              their own devices or in the cloud.
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:07:35,524'); seek(455.0)">
              So then that raise the second question.
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:07:39,164'); seek(459.0)">
              The llama edge server probably is not the first or the
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:07:42,252'); seek(462.0)">
              only open source software that can run large
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:07:45,804'); seek(465.0)">
              language models. So for instance llama CPP can run and Allama can run it,
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:07:49,580'); seek(469.0)">
              arm studio can run it. So why is it that we choose the
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:07:53,958'); seek(473.0)">
              llama edge was stacked to run our API
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:07:58,014'); seek(478.0)">
              server around those models as we have shown in the previous example?
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:08:03,094'); seek(483.0)">
              First, I think there are several features that distinguish llama
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:08:06,606'); seek(486.0)">
              edge from other products or
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:08:09,990'); seek(489.0)">
              other open source projects in the space. The first
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:08:13,326'); seek(493.0)">
              is to support a wide variety of models. The lamo edge
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:08:17,182'); seek(497.0)">
              application server supports over 6000 large language models,
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:08:21,094'); seek(501.0)">
              including all the ones
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:24,548'); seek(504.0)">
              that you have heard of. And there's not just the base models and
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:08:28,468'); seek(508.0)">
              chat models, but also embedding models which are essential and very important
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:08:32,516'); seek(512.0)">
              for running sophisticated applications as we are seeing like RAC applications.
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:08:36,948'); seek(516.0)">
              It even support larger models like X AI squawk model which is
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:08:41,060'); seek(521.0)">
              I think 314 billion parameters. And it
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:08:44,636'); seek(524.0)">
              requires top of the line, say a Mac studio or,
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:08:49,494'); seek(529.0)">
              or two H 100 chips graphic
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:08:53,942'); seek(533.0)">
              cards to run those models. But they can run using
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:08:58,006'); seek(538.0)">
              Lama H on the other hand.
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:09:01,654'); seek(541.0)">
              On one hand that supports a wide variety of models. But on the
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:09:05,086'); seek(545.0)">
              other hand, it also supports a wide range of devices, because once
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:09:08,502'); seek(548.0)">
              we start to run on our own devices, in our own
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:09:11,590'); seek(551.0)">
              cloud, we have to deal with many different types of different devices,
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:09:15,174'); seek(555.0)">
              different gpu's, different cpu's and different graphic
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:09:19,262'); seek(559.0)">
              cards, different accelerators and all that. So the
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:09:23,374'); seek(563.0)">
              architecture of Lama H, because it's based on the wash runtime which we'll get into
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:09:27,390'); seek(567.0)">
              in a minute, it provides abstraction layer that
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:09:30,894'); seek(570.0)">
              abstracts the application from the underlying runtime
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:09:34,934'); seek(574.0)">
              and hardware. So that allows it to the same application
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:09:38,446'); seek(578.0)">
              to run on a wide range of devices and drivers,
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:09:42,158'); seek(582.0)">
              ranging from Nvidia to AMD to ARM devices,
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:09:46,446'); seek(586.0)">
              to Intel AMD CPU
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:09:50,706'); seek(590.0)">
              and GPU devices, Apple devices and all of those.
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:09:54,322'); seek(594.0)">
              So in a minute. So I just showed you it runs on
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:09:57,922'); seek(597.0)">
              a Mac device and I'm going to show you how it runs on Nvidia device.
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:10:01,786'); seek(601.0)">
              And we also have tutorials on how to run, say on raspberry PI
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:10:05,202'); seek(605.0)">
              or the Jetson devices. All those,
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:10:08,498'); seek(608.0)">
              I would say, devices more
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:10:12,250'); seek(612.0)">
              commonly find in the edge cloud.
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:10:15,734'); seek(615.0)">
              The Lama Edge API server also provides enough flexibility to
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:10:19,174'); seek(619.0)">
              run multiple models in the same server. So, meaning that I can run a
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:10:22,598'); seek(622.0)">
              chat model to chat with people and run the embedding model in the
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:10:25,958'); seek(625.0)">
              same server, so that the user can upload
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:10:29,894'); seek(629.0)">
              or process their external knowledge base with the same application server.
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:10:34,422'); seek(634.0)">
              It can also do things like function calling by forcing the output to
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:10:37,678'); seek(637.0)">
              be JSON. That's something that needs coordination between both
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:10:41,574'); seek(641.0)">
              the model itself, the prompting, and the model runtime, meaning that the
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:10:45,318'); seek(645.0)">
              model runtime would have to force the output to meet certain grammar checks,
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:10:49,102'); seek(649.0)">
              to pass certain grammar check, for instance, that's provided JSON. Right. And as
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:10:52,878'); seek(652.0)">
              you can see, it's very easy to install in a run. It's just one line
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:10:56,470'); seek(656.0)">
              of command, and I highly suggest recommend you to try it out.
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:11:00,134'); seek(660.0)">
              And it's also very lightweight because it's the entire
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:11:03,886'); seek(663.0)">
              runtime wash. Plus the application itself
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:11:07,366'); seek(667.0)">
              is less than say 30 megabytes. That compared to say,
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:11:11,414'); seek(671.0)">
              the Pytorch Docker image we talked, you know, the Pytorch
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:11:14,652'); seek(674.0)">
              Docker image is 4gb by itself. So that's
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:11:19,660'); seek(679.0)">
              why you, you know, even when running
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:11:23,356'); seek(683.0)">
              a large language model and put the API server in front of it, put a
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:11:26,196'); seek(686.0)">
              chatbot in front of it, we use Lama edge, right? But all those
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:11:29,652'); seek(689.0)">
              benefits actually come from one most important
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:11:33,388'); seek(693.0)">
              attributes of the llama edge runtime, which is it is
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:11:37,044'); seek(697.0)">
              not really designed to be, to be a standalone
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:11:40,960'); seek(700.0)">
              server to run large language model. It is
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:11:44,552'); seek(704.0)">
              actually a developer platform. You can write your own code to
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:11:48,424'); seek(708.0)">
              extend it and to customize and build your own applications on
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:11:52,248'); seek(712.0)">
              top of it. If you think about how large language
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:11:56,040'); seek(716.0)">
              model applications are built today, typically you have
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:11:59,128'); seek(719.0)">
              an application server that provides API.
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:12:02,088'); seek(722.0)">
              It's either OpenAI or some other SaaS provider,
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:12:05,344'); seek(725.0)">
              or you can use your own, you know, like using Lama H, you provide open
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:12:09,302'); seek(729.0)">
              compatible API server and then you build an application that consumes
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:12:12,910'); seek(732.0)">
              this API in say in a relatively
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:12:16,350'); seek(736.0)">
              heavyweight stack like in Python or LAN chain or lamba index.
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:12:20,038'); seek(740.0)">
              There are lots of tools out there to do that. And you would also have
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:12:22,758'); seek(742.0)">
              a UI, have a web server, have all those
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:12:27,014'); seek(747.0)">
              components either contained in a docker file,
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:12:30,718'); seek(750.0)">
              something like that, and have them all tied together and build
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:12:34,230'); seek(754.0)">
              an entire application like that. But while this provides
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:12:37,880'); seek(757.0)">
              flexibility and easy allows people, allow developers to experiment
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:12:42,176'); seek(762.0)">
              with different parts of the stack, it is also making it really difficult
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:12:45,992'); seek(765.0)">
              to deploy those applications because they have too much dependencies.
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:12:49,824'); seek(769.0)">
              They are huge because of the python dependency and they
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:12:53,008'); seek(773.0)">
              are often very slow. Llama Edge
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:12:57,144'); seek(777.0)">
              is a rust based SDK. Essentially it allows you
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:13:00,840'); seek(780.0)">
              to build a single portable and deployable app. So basically you can write all
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:13:04,560'); seek(784.0)">
              your logic into one single application. There's no need to write part
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:13:08,100'); seek(788.0)">
              of the logic in C, part of that in Python,
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:13:11,204'); seek(791.0)">
              and then use the HTTP API to connect them together. You don't need
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:13:14,508'); seek(794.0)">
              to do any of those. So he improves efficiency, make it a
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:13:18,188'); seek(798.0)">
              lot easier to deploy and it simplifies
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:13:21,564'); seek(801.0)">
              the workflow. And again, there's no python dependency. A lot of
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:13:25,076'); seek(805.0)">
              people, if you haven't really worked in the large language model
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:13:28,652'); seek(808.0)">
              space in the past, that Python dependency is actually a
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:13:32,300'); seek(812.0)">
              nightmare even for very experienced people in this field.
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:13:37,084'); seek(817.0)">
              You can use rust or you can use JavaScript
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:13:41,124'); seek(821.0)">
              that allows us to build applications that are similar to
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:13:45,116'); seek(825.0)">
              say the latest advance in OpenAI. So if you look at OpenAI,
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:13:48,436'); seek(828.0)">
              they have a systems API, they have a stateful API that
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:13:51,972'); seek(831.0)">
              build a lot of functionality into the API server itself.
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:13:54,748'); seek(834.0)">
              So the Lama H platform allows you to develop your
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:13:58,460'); seek(838.0)">
              own API server to serve whatever
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:14:02,042'); seek(842.0)">
              the front end that you want to do. So that
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:14:05,706'); seek(845.0)">
              gives you a lot of flexibility in terms of building
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:14:09,402'); seek(849.0)">
              advanced applications if you pay
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:14:12,698'); seek(852.0)">
              close attention. There's a word that I put in bold here that
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:14:16,258'); seek(856.0)">
              is single portable and the deployable application,
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:14:19,602'); seek(859.0)">
              meaning you are talking about a developer platform that is based
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:14:23,362'); seek(863.0)">
              on rust that has no python dependency. One of the biggest issues
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:14:27,058'); seek(867.0)">
              people are going to ask is that how about cross platform compatibility?
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:14:31,248'); seek(871.0)">
              Because for highly efficient
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:14:34,568'); seek(874.0)">
              native applications like that, if you write it
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:14:38,744'); seek(878.0)">
              on one platform, it's likely you are not going to be able to deploy it
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:14:41,912'); seek(881.0)">
              on a different platform. That sort of makes
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:14:45,912'); seek(885.0)">
              it really difficult for developers because I have a Mac or Windows,
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:14:49,336'); seek(889.0)">
              but my application that I compiled and tested there,
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:14:53,744'); seek(893.0)">
              is that really true that I can't deploy it on a media device in the
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:14:56,648'); seek(896.0)">
              cloud? So in the next demo I'm going to show you how
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:15:00,754'); seek(900.0)">
              the was manage and the laminage stack address this problem
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:15:04,226'); seek(904.0)">
              that we allow you to build truly portable large
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:15:08,194'); seek(908.0)">
              language model applications. So here's a demo. In this demo I'll
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:15:12,082'); seek(912.0)">
              show you a key benefit of using webassembly
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:15:15,186'); seek(915.0)">
              or wasm to run ARM applications. Portability.
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:15:18,634'); seek(918.0)">
              The webassembly application is a binary application that is directly
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:15:22,202'); seek(922.0)">
              portable across different operating systems,
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:15:25,058'); seek(925.0)">
              hardwares and drivers. So on this screen I have
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:15:29,186'); seek(929.0)">
              opened a terminal to
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:15:33,338'); seek(933.0)">
              my remote machine on Microsoft Azure. That machine runs Linux,
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:15:36,906'); seek(936.0)">
              has Nvidia tiffo card and have the CudA twelve
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:15:40,546'); seek(940.0)">
              driver installed on it. If I see what's in here,
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:15:45,514'); seek(945.0)">
              you can see I have downloaded the large language model,
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:15:48,778'); seek(948.0)">
              the Gamma two B model in GguI format,
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:15:52,234'); seek(952.0)">
              and then a bunch of HTML and JavaScript files in the chatbot UI.
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:15:56,524'); seek(956.0)">
              I have also installed the was image runtime here. So if I say wash,
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:16:00,236'); seek(960.0)">
              it's was 13.5. So now we have the large language
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:16:04,076'); seek(964.0)">
              model and we have the runtime for the large language model. We are
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:16:07,148'); seek(967.0)">
              still missing the application. The application is something that
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:16:11,156'); seek(971.0)">
              takes a user input, runs a large language model, prompting the larger language,
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:16:14,836'); seek(974.0)">
              runs a large language model, generate a response and then send it back to the
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:16:17,628'); seek(977.0)">
              user. So you can call it a chatbot application, it could be a web
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:16:21,332'); seek(981.0)">
              application, it could be a discord, or it could be asian
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:16:25,056'); seek(985.0)">
              application that connects to other applications that takes the large language
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:16:28,984'); seek(988.0)">
              model output to do things. So the application
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:16:34,504'); seek(994.0)">
              is a key part that as developers that you would write in
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:16:38,536'); seek(998.0)">
              the past, you wouldn't expect this application to be portable because
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:16:43,144'); seek(1003.0)">
              when you develop this application you are probably on the Mac or Windows machine
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:16:46,600'); seek(1006.0)">
              and you compared to say Apple silicon and use the metal
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:16:50,192'); seek(1010.0)">
              framework, you have all this built in. And you wouldn't
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:16:54,002'); seek(1014.0)">
              think that by just copying this binary application to
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:16:58,026'); seek(1018.0)">
              Nvidia machine it would just run there, right? And of course
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:17:01,546'); seek(1021.0)">
              there are ways to make it easier. For instance, if you use Python, Python has
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:17:04,802'); seek(1024.0)">
              a lot of abstractions that allows you to write
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:17:09,834'); seek(1029.0)">
              at a fairly high level so that you can write a Python script on
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:17:13,970'); seek(1033.0)">
              Mac and then try to run this same script on
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:17:18,715'); seek(1038.0)">
              the Linux machine. However, as you would imagine Python,
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:17:21,963'); seek(1041.0)">
              in order to achieve that, Python have a huge amount of dependencies.
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:17:25,715'); seek(1045.0)">
              Like if you go to the Pytorch official docker image, it is
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:17:29,395'); seek(1049.0)">
              4gb right there. So the Pytorch docker image
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:17:32,995'); seek(1052.0)">
              itself is 4gb and it's platform dependent,
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:17:36,379'); seek(1056.0)">
              so you have to install the right Python version. And within Python
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:17:39,723'); seek(1059.0)">
              you also have oftentimes you need to specify what is the underlying architecture.
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:17:43,611'); seek(1063.0)">
              So it is huge dependency and it's not really that portable
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:17:47,003'); seek(1067.0)">
              for any other languages like rust or go, or if you
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:17:50,688'); seek(1070.0)">
              write your application in any of those other languages,
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:17:53,984'); seek(1073.0)">
              you would not imagine that it would be portable. That's because the
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:17:58,200'); seek(1078.0)">
              underlying GPU and CPU architectures are entirely different. What WASM
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:18:01,904'); seek(1081.0)">
              does is that it provides abstraction for those applications
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:18:06,280'); seek(1086.0)">
              so that it can run smoothly across all different platforms. In order to
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:18:09,648'); seek(1089.0)">
              demonstrate that, I switch to a window. This is my,
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:18:13,514'); seek(1093.0)">
              this is on my local machine, which is a MacBook,
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:18:16,874'); seek(1096.0)">
              and what I have already downloaded one
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:18:20,338'); seek(1100.0)">
              of the API server applications from the Lama Edge project which is a rust
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:18:24,050'); seek(1104.0)">
              application. And I compiled it on my Mac and I tested it on
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:18:27,458'); seek(1107.0)">
              my Mac, right? So now what I'm going to do is I'm going to just
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:18:31,954'); seek(1111.0)">
              scp this entire file to the remote azure machine.
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:18:36,082'); seek(1116.0)">
              So as you can see, the entire file is only nine megabytes and
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:18:40,838'); seek(1120.0)">
              we didn't package it in any way, we didn't have a docker
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:18:44,430'); seek(1124.0)">
              image around, you know, wrapped around it.
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:18:47,894'); seek(1127.0)">
              We just scp the whole thing to another machine and
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:18:52,070'); seek(1132.0)">
              with entirely different architecture in both hardware and software and
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:18:55,510'); seek(1135.0)">
              expect to run there. Can we run there? So let's see. So we use
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:18:59,078'); seek(1139.0)">
              the WASM edge runtime to run it.
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:19:02,534'); seek(1142.0)">
              So the WASM edge runtime starts instantly. It's because
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:19:06,254'); seek(1146.0)">
              it's an application, so it loads the large language model and then
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:19:09,902'); seek(1149.0)">
              it starts an API server. The web server is actually accessible
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:19:14,174'); seek(1154.0)">
              through port 8080. So if you have this machine,
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:19:16,974'); seek(1156.0)">
              the port open public, you would be able to load up browser and go
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:19:20,358'); seek(1160.0)">
              to port 880 and see it. But for
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:19:23,742'); seek(1163.0)">
              now we want to just stay on this machine because we have it under a
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:19:27,006'); seek(1167.0)">
              firewall. What are we going to try is that we're going to do API
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:19:30,286'); seek(1170.0)">
              request, because this API server also takes open AI style
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:19:34,430'); seek(1174.0)">
              API requests that allows us to integrate with all the
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:19:38,416'); seek(1178.0)">
              openi ecosystem tools. So here's
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:19:42,184'); seek(1182.0)">
              how the request looks like. So as you can see,
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:19:46,312'); seek(1186.0)">
              we request at the localhost and then we send a
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:19:49,640'); seek(1189.0)">
              message with a row of user and say where's Paris? We ask the model,
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:19:53,512'); seek(1193.0)">
              where's Paris? Right. If I do this, it does the
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:19:56,880'); seek(1196.0)">
              inference, its result come back before I can finish speaking.
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:20:00,848'); seek(1200.0)">
              So the result is, the role of the result is
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:20:04,292'); seek(1204.0)">
              system and the content is Paris located in hardware friends, blah, blah, blah.
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:20:08,140'); seek(1208.0)">
              So now we have achieved something, I think very interesting,
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:20:11,196'); seek(1211.0)">
              that we compiled a rust application on the
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:20:14,500'); seek(1214.0)">
              Mac and fully taking advantage of the Mac GPU
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:20:18,140'); seek(1218.0)">
              and the metal framework. And then we just copied this
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:20:21,700'); seek(1221.0)">
              WASM application into a remote Linux machine
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:20:25,396'); seek(1225.0)">
              running on Nvidia. And we can see this application,
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:20:28,460'); seek(1228.0)">
              that's which the role of the application start server and interact
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:20:32,106'); seek(1232.0)">
              with the underlying large
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:20:35,882'); seek(1235.0)">
              language model runs just perfectly on
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:20:39,098'); seek(1239.0)">
              the new hardware, fully utilizing the
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:20:42,962'); seek(1242.0)">
              media GPU capabilities to accelerate. Without the GPU
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:20:47,074'); seek(1247.0)">
              capabilities, you would not be able to have nearly 100
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:20:51,378'); seek(1251.0)">
              tokens per second speed on this
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:20:55,186'); seek(1255.0)">
              Linux machine. If you're just doing the cpu, you'll be more like one tokens
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:20:58,602'); seek(1258.0)">
              per second, you know, it would be two orders,
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:21:02,334'); seek(1262.0)">
              magnitude's difference. So that's it, that's what
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:21:06,182'); seek(1266.0)">
              we have shown, that the WASM application is truly portable.
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:21:09,694'); seek(1269.0)">
              All right, to recap the demo,
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:21:13,262'); seek(1273.0)">
              there's so for the longest time we
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:21:17,062'); seek(1277.0)">
              have platform engineering, or DevOps,
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:21:20,502'); seek(1280.0)">
              that combines the role of developer and Ops. But with the new
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:21:24,574'); seek(1284.0)">
              hardware, with more and more different devices and different drivers,
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:21:28,102'); seek(1288.0)">
              all that stuff, that's that coming along for AI applications.
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:21:31,578'); seek(1291.0)">
              And I think it's time to separate the dev and Ops role all
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:21:35,450'); seek(1295.0)">
              over again. So the way it works is that
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:21:39,466'); seek(1299.0)">
              Webassembly is a virtual machine format. You can think of it
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:21:43,442'); seek(1303.0)">
              like a Java Java bytecode, so it provides abstraction over
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:21:47,090'); seek(1307.0)">
              the real hardware. And for developers, you just need to
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:21:50,882'); seek(1310.0)">
              write to the Webassembly interface. In our case, you write
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:21:54,282'); seek(1314.0)">
              to the Lama edge SDK interface and it
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:21:57,722'); seek(1317.0)">
              tells the SDK to say load a model and
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:22:01,442'); seek(1321.0)">
              do the inference. And the
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:22:05,450'); seek(1325.0)">
              developer only need to write application this way. So if the application,
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:22:08,906'); seek(1328.0)">
              once the application is compiled to WASM, the developer's
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:22:12,410'); seek(1332.0)">
              job is done. He or she can ship the application anywhere that
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:22:16,010'); seek(1336.0)">
              they want and lets the runtime takes
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:22:19,890'); seek(1339.0)">
              over the rest. So it's the Ops people that
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:22:23,482'); seek(1343.0)">
              needs to install the correct runtime and driver for each device.
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:22:26,690'); seek(1346.0)">
              So for instance, on a Mac we want to install the Mac version of was
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:22:29,956'); seek(1349.0)">
              made. It's sort of like Java. On the Mac you need to install the Mac
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:22:33,612'); seek(1353.0)">
              version of Java, right, the JVM, right. You know, so it's the
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:22:36,916'); seek(1356.0)">
              same thing here. So you want to install the Mac version of the was runtime.
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:22:40,924'); seek(1360.0)">
              If you have Nvidia device, you need to Ubuntu with CUDA twelve,
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:22:45,020'); seek(1365.0)">
              you need to install the appropriate was runtime
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:22:48,660'); seek(1368.0)">
              in there as well. So for the Ops guys, they can,
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:22:52,844'); seek(1372.0)">
              once they take care of that, they would be able to just run
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:22:55,996'); seek(1375.0)">
              that wasm application without any modification.
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:22:59,250'); seek(1379.0)">
              Because the WASM edge runtime has a contract, has a
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:23:02,650'); seek(1382.0)">
              standard API that's, that is defined in the
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:23:06,058'); seek(1386.0)">
              llama HSDK, right? You know, so once it sees those instructions to
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:23:10,082'); seek(1390.0)">
              say, load a model and you know, send some text to the model
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:23:13,570'); seek(1393.0)">
              for inference, and it sees those instructions,
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:23:17,074'); seek(1397.0)">
              the byte code, you automatically run those code and it would translate those code
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:23:21,250'); seek(1401.0)">
              into that's instructions that are appropriate for the
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:23:24,622'); seek(1404.0)">
              underlying accelerator and the drivers. So it allows developers
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:23:28,942'); seek(1408.0)">
              to write truly portable applications that can be deployed anywhere,
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:23:32,718'); seek(1412.0)">
              and it can be managed by tools like kubernetes and
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:23:36,814'); seek(1416.0)">
              let ops people worry about installing the right driver and
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:23:40,574'); seek(1420.0)">
              installing the right was match runtime on every single node or
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:23:44,702'); seek(1424.0)">
              every single edge devices that you have in your cluster.
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:23:48,634'); seek(1428.0)">
              So that leads us to our last demo, because we
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:23:52,378'); seek(1432.0)">
              have talked about Lama Edge being a developer platform,
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:23:56,170'); seek(1436.0)">
              and one of the most popular applications people do with Python, at least
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:23:59,986'); seek(1439.0)">
              today, is what they call a rack application, meaning that
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:24:03,394'); seek(1443.0)">
              you use a standard large language model, a fine tuned large
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:24:07,026'); seek(1447.0)">
              language model, but you feed it with your proprietary
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:24:11,058'); seek(1451.0)">
              knowledge base. The knowledge base was divided,
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:24:15,142'); seek(1455.0)">
              it's typically a text, a PDF or image, or you
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:24:18,566'); seek(1458.0)">
              know, or a text file, and it was divided into segments and each
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:24:22,614'); seek(1462.0)">
              segment was turned into a vector and stored in a vector database.
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:24:26,382'); seek(1466.0)">
              And when the user
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:24:30,222'); seek(1470.0)">
              asks a new question from the API, the application
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:24:33,854'); seek(1473.0)">
              would take, would take that question, turn that into
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:24:37,422'); seek(1477.0)">
              vector as well, and perform a vector search in the database to find out
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:24:41,206'); seek(1481.0)">
              which other, which texts in the knowledge base are most related
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:24:47,054'); seek(1487.0)">
              to the question. And then it would add the context that retracted
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:24:51,382'); seek(1491.0)">
              from the knowledge base and the new question into the prompt,
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:24:54,382'); seek(1494.0)">
              and asks the large language model to give an answer. As you
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:24:57,886'); seek(1497.0)">
              can see, this is a fairly complex process, and it involves
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:25:01,958'); seek(1501.0)">
              not just the large language model, not just the runtime, but also
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:25:05,334'); seek(1505.0)">
              things like the vector database, the embedding model and all
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:25:08,662'); seek(1508.0)">
              that. Things that you have to tie together,
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:25:12,974'); seek(1512.0)">
              as we talked about previously in this talk, is that
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:25:17,126'); seek(1517.0)">
              things like that was traditionally done by say fairly complicated
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:25:21,670'); seek(1521.0)">
              Python programs that does the orchestration, the queries,
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:25:25,150'); seek(1525.0)">
              the turn into act with embeddings and all that stuff,
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:25:28,774'); seek(1528.0)">
              and then you attach another UI in front of it. So it's a fairly involved
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:25:32,822'); seek(1532.0)">
              and a complicated process. But with Lama Edge, we will
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:25:36,414'); seek(1536.0)">
              be able to build a single application that can
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:25:41,094'); seek(1541.0)">
              talk to the vector database, call the embeddings when it's needed,
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:25:45,194'); seek(1545.0)">
              and perform the vector search, and then at
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:25:49,330'); seek(1549.0)">
              the end prompts a large language model to get an answer.
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:25:52,786'); seek(1552.0)">
              So this project, we call it an
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:25:55,890'); seek(1555.0)">
              integrated assistant API server, which because it looks
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:25:59,770'); seek(1559.0)">
              a lot like open eyes assistant API and
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:26:04,130'); seek(1564.0)">
              we call this project the Garnet. And here's a demo for that.
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:26:08,964'); seek(1568.0)">
              Hi. In this demo I'll show you the easiest way to
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:26:12,692'); seek(1572.0)">
              run reg or RaC API server
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:26:16,740'); seek(1576.0)">
              for a large language model. So if you are familiar with RAC,
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:26:21,108'); seek(1581.0)">
              it is, it is a way to add knowledge.
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:26:25,748'); seek(1585.0)">
              It could be additional public knowledge or proprietary
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:26:29,700'); seek(1589.0)">
              knowledge that you don't want other people to see to an existing
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:26:32,924'); seek(1592.0)">
              large language model, so that the larger language model can answer questions and chat
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:26:36,652'); seek(1596.0)">
              with people based on additional context that you provide to it.
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:26:40,444'); seek(1600.0)">
              So in order to do that, a typical rack
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:26:43,940'); seek(1603.0)">
              setup requires a fairly complex setup that requires say
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:26:48,924'); seek(1608.0)">
              install digital vector database UI, how to upload
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:26:52,452'); seek(1612.0)">
              the knowledge and the tools like Lanchain
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:26:56,356'); seek(1616.0)">
              to orchestrate how to manage retrieve data from the vector
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:27:00,060'); seek(1620.0)">
              database and how to prompt application.
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:27:04,364'); seek(1624.0)">
              So in our approach that we want to introduce a project called
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:27:08,320'); seek(1628.0)">
              Gayanet and the Garnet is an application that build on was
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:27:11,888'); seek(1631.0)">
              matched and also using the Lama edge
            </span>
            
            <span id="chunk-429" class="transcript-chunks" onclick="console.log('00:27:15,592'); seek(1635.0)">
              framework. So what it does is that it allows
            </span>
            
            <span id="chunk-430" class="transcript-chunks" onclick="console.log('00:27:19,312'); seek(1639.0)">
              applications, you can write simple
            </span>
            
            <span id="chunk-431" class="transcript-chunks" onclick="console.log('00:27:23,168'); seek(1643.0)">
              rock applications using rust and then compile it into a single
            </span>
            
            <span id="chunk-432" class="transcript-chunks" onclick="console.log('00:27:27,704'); seek(1647.0)">
              wasm binary with zero python dependency and
            </span>
            
            <span id="chunk-433" class="transcript-chunks" onclick="console.log('00:27:31,224'); seek(1651.0)">
              run it very efficiently at the server. So let's see how
            </span>
            
            <span id="chunk-434" class="transcript-chunks" onclick="console.log('00:27:34,656'); seek(1654.0)">
              it works. If you go to the Garnet GitHub repository, by the way,
            </span>
            
            <span id="chunk-435" class="transcript-chunks" onclick="console.log('00:27:38,150'); seek(1658.0)">
              if you are here, just give us a star.
            </span>
            
            <span id="chunk-436" class="transcript-chunks" onclick="console.log('00:27:41,534'); seek(1661.0)">
              And here is a quick start guide and
            </span>
            
            <span id="chunk-437" class="transcript-chunks" onclick="console.log('00:27:46,462'); seek(1666.0)">
              this is called Gaia node. And there is a one line
            </span>
            
            <span id="chunk-438" class="transcript-chunks" onclick="console.log('00:27:49,950'); seek(1669.0)">
              of command which you can use to install Gaia which let's
            </span>
            
            <span id="chunk-439" class="transcript-chunks" onclick="console.log('00:27:55,030'); seek(1675.0)">
              do that and explain what it does. So this is on my local Mac machine.
            </span>
            
            <span id="chunk-440" class="transcript-chunks" onclick="console.log('00:27:59,230'); seek(1679.0)">
              And if I say install, what it does is that it installs the
            </span>
            
            <span id="chunk-441" class="transcript-chunks" onclick="console.log('00:28:02,558'); seek(1682.0)">
              was match runtime which is required to run a
            </span>
            
            <span id="chunk-442" class="transcript-chunks" onclick="console.log('00:28:05,906'); seek(1685.0)">
              large language model. It installs a quadrant vector
            </span>
            
            <span id="chunk-443" class="transcript-chunks" onclick="console.log('00:28:09,722'); seek(1689.0)">
              database which is required, and it's download a chat
            </span>
            
            <span id="chunk-444" class="transcript-chunks" onclick="console.log('00:28:13,314'); seek(1693.0)">
              model which it has already downloaded. We call it a
            </span>
            
            <span id="chunk-445" class="transcript-chunks" onclick="console.log('00:28:17,090'); seek(1697.0)">
              standard Lama two seven b chat model and download the embedding model
            </span>
            
            <span id="chunk-446" class="transcript-chunks" onclick="console.log('00:28:20,770'); seek(1700.0)">
              which used to process the vector.
            </span>
            
            <span id="chunk-447" class="transcript-chunks" onclick="console.log('00:28:24,794'); seek(1704.0)">
              But here what's really interesting, you can ignore the error here,
            </span>
            
            <span id="chunk-448" class="transcript-chunks" onclick="console.log('00:28:28,082'); seek(1708.0)">
              what's really interesting is that it also downloads a knowledge collection
            </span>
            
            <span id="chunk-449" class="transcript-chunks" onclick="console.log('00:28:31,618'); seek(1711.0)">
              which is a knowledge base we vectorized into
            </span>
            
            <span id="chunk-450" class="transcript-chunks" onclick="console.log('00:28:34,736'); seek(1714.0)">
              the quadrant format. You can read in the documentation how
            </span>
            
            <span id="chunk-451" class="transcript-chunks" onclick="console.log('00:28:38,032'); seek(1718.0)">
              we do that, and we have a separate rust application that
            </span>
            
            <span id="chunk-452" class="transcript-chunks" onclick="console.log('00:28:42,232'); seek(1722.0)">
              helps you to do that. And then it
            </span>
            
            <span id="chunk-453" class="transcript-chunks" onclick="console.log('00:28:46,536'); seek(1726.0)">
              installs that snapshot into the quantum database.
            </span>
            
            <span id="chunk-454" class="transcript-chunks" onclick="console.log('00:28:51,384'); seek(1731.0)">
              So what it does is that it creates a Gaia net
            </span>
            
            <span id="chunk-455" class="transcript-chunks" onclick="console.log('00:28:55,400'); seek(1735.0)">
              directory in your home directory
            </span>
            
            <span id="chunk-456" class="transcript-chunks" onclick="console.log('00:28:59,056'); seek(1739.0)">
              and put everything in there and including this wasm file
            </span>
            
            <span id="chunk-457" class="transcript-chunks" onclick="console.log('00:29:02,556'); seek(1742.0)">
              that starts up the rack application server. And here
            </span>
            
            <span id="chunk-458" class="transcript-chunks" onclick="console.log('00:29:07,468'); seek(1747.0)">
              if you look at the config JSon,
            </span>
            
            <span id="chunk-459" class="transcript-chunks" onclick="console.log('00:29:11,244'); seek(1751.0)">
              you will be able to see the chat model that's being used.
            </span>
            
            <span id="chunk-460" class="transcript-chunks" onclick="console.log('00:29:14,284'); seek(1754.0)">
              The parameters for the chat model and the snapshot
            </span>
            
            <span id="chunk-461" class="transcript-chunks" onclick="console.log('00:29:18,524'); seek(1758.0)">
              is knowledge
            </span>
            
            <span id="chunk-462" class="transcript-chunks" onclick="console.log('00:29:22,364'); seek(1762.0)">
              base that vertebrates knowledge base and the prompt and you can change,
            </span>
            
            <span id="chunk-463" class="transcript-chunks" onclick="console.log('00:29:26,084'); seek(1766.0)">
              you can modify any of the things that you want, use a different model,
            </span>
            
            <span id="chunk-464" class="transcript-chunks" onclick="console.log('00:29:29,068'); seek(1769.0)">
              use a different knowledge base and rerun install. Right.
            </span>
            
            <span id="chunk-465" class="transcript-chunks" onclick="console.log('00:29:32,584'); seek(1772.0)">
              So once you have the, once you have everything installed that
            </span>
            
            <span id="chunk-466" class="transcript-chunks" onclick="console.log('00:29:36,120'); seek(1776.0)">
              you would be able to say run
            </span>
            
            <span id="chunk-467" class="transcript-chunks" onclick="console.log('00:29:41,160'); seek(1781.0)">
              the start script. What it does is that it's going to start the vector
            </span>
            
            <span id="chunk-468" class="transcript-chunks" onclick="console.log('00:29:44,600'); seek(1784.0)">
              database like we said, and they're going to start the was image application
            </span>
            
            <span id="chunk-469" class="transcript-chunks" onclick="console.log('00:29:47,976'); seek(1787.0)">
              server and it's going to start a domain server which give it
            </span>
            
            <span id="chunk-470" class="transcript-chunks" onclick="console.log('00:29:51,160'); seek(1791.0)">
              the public access for domain for the server.
            </span>
            
            <span id="chunk-471" class="transcript-chunks" onclick="console.log('00:29:54,184'); seek(1794.0)">
              Right. So what are we going to do is that we're going to open while
            </span>
            
            <span id="chunk-472" class="transcript-chunks" onclick="console.log('00:29:58,464'); seek(1798.0)">
              this is runs on our local machine. I can use localhost, but I can
            </span>
            
            <span id="chunk-473" class="transcript-chunks" onclick="console.log('00:30:02,328'); seek(1802.0)">
              because it gives me a publicly accessible domain. So what I'm going to do is
            </span>
            
            <span id="chunk-474" class="transcript-chunks" onclick="console.log('00:30:05,608'); seek(1805.0)">
              that I'm going to go to load this public accessible domain.
            </span>
            
            <span id="chunk-475" class="transcript-chunks" onclick="console.log('00:30:10,024'); seek(1810.0)">
              So it gives me how to run access API.
            </span>
            
            <span id="chunk-476" class="transcript-chunks" onclick="console.log('00:30:13,104'); seek(1813.0)">
              But what I would do is that I would just chat with the node while
            </span>
            
            <span id="chunk-477" class="transcript-chunks" onclick="console.log('00:30:16,240'); seek(1816.0)">
              it's loading. I would come back here and tell you and open
            </span>
            
            <span id="chunk-478" class="transcript-chunks" onclick="console.log('00:30:20,184'); seek(1820.0)">
              up the log here.
            </span>
            
            <span id="chunk-479" class="transcript-chunks" onclick="console.log('00:30:31,284'); seek(1831.0)">
              So this is a lama edge startup log which
            </span>
            
            <span id="chunk-480" class="transcript-chunks" onclick="console.log('00:30:34,748'); seek(1834.0)">
              logs all the interactions with large language model,
            </span>
            
            <span id="chunk-481" class="transcript-chunks" onclick="console.log('00:30:37,876'); seek(1837.0)">
              right? So now it's,
            </span>
            
            <span id="chunk-482" class="transcript-chunks" onclick="console.log('00:30:41,180'); seek(1841.0)">
              the UI has come up. I want to demonstrate to
            </span>
            
            <span id="chunk-483" class="transcript-chunks" onclick="console.log('00:30:44,508'); seek(1844.0)">
              you that it does use a knowledge base to chat with us. So if I
            </span>
            
            <span id="chunk-484" class="transcript-chunks" onclick="console.log('00:30:48,188'); seek(1848.0)">
              say where's Paris? Because the knowledge base I'm using right
            </span>
            
            <span id="chunk-485" class="transcript-chunks" onclick="console.log('00:30:52,750'); seek(1852.0)">
              now by default is a
            </span>
            
            <span id="chunk-486" class="transcript-chunks" onclick="console.log('00:30:56,710'); seek(1856.0)">
              guide, the knowledge comes from Paris guidebook.
            </span>
            
            <span id="chunk-487" class="transcript-chunks" onclick="console.log('00:31:00,870'); seek(1860.0)">
              So I vectorized that Paris guidebook and put that into the vector
            </span>
            
            <span id="chunk-488" class="transcript-chunks" onclick="console.log('00:31:04,382'); seek(1864.0)">
              database so that the large language model
            </span>
            
            <span id="chunk-489" class="transcript-chunks" onclick="console.log('00:31:08,030'); seek(1868.0)">
              can generate answers for me. So it says thank you for the
            </span>
            
            <span id="chunk-490" class="transcript-chunks" onclick="console.log('00:31:11,550'); seek(1871.0)">
              question based on the context. Paris located at north central part
            </span>
            
            <span id="chunk-491" class="transcript-chunks" onclick="console.log('00:31:14,670'); seek(1874.0)">
              of France, blah blah, blah blah. How do
            </span>
            
            <span id="chunk-492" class="transcript-chunks" onclick="console.log('00:31:17,866'); seek(1877.0)">
              we know that this is actually using
            </span>
            
            <span id="chunk-493" class="transcript-chunks" onclick="console.log('00:31:21,362'); seek(1881.0)">
              our context? How does we know that our RAC
            </span>
            
            <span id="chunk-494" class="transcript-chunks" onclick="console.log('00:31:24,922'); seek(1884.0)">
              application server works? We go back to the log and we can
            </span>
            
            <span id="chunk-495" class="transcript-chunks" onclick="console.log('00:31:28,466'); seek(1888.0)">
              see the actual prompt. So you can see this is the
            </span>
            
            <span id="chunk-496" class="transcript-chunks" onclick="console.log('00:31:32,818'); seek(1892.0)">
              actual prompt the application server sends to the database. So you
            </span>
            
            <span id="chunk-497" class="transcript-chunks" onclick="console.log('00:31:35,898'); seek(1895.0)">
              are helpful assistant. And then here's the context. If you don't
            </span>
            
            <span id="chunk-498" class="transcript-chunks" onclick="console.log('00:31:39,714'); seek(1899.0)">
              know if the answer is not in the context, you don't answer it. So those
            </span>
            
            <span id="chunk-499" class="transcript-chunks" onclick="console.log('00:31:42,786'); seek(1902.0)">
              three paragraph context come from our vector database.
            </span>
            
            <span id="chunk-500" class="transcript-chunks" onclick="console.log('00:31:46,954'); seek(1906.0)">
              That's come from the Paris guidebook. It's about general information about
            </span>
            
            <span id="chunk-501" class="transcript-chunks" onclick="console.log('00:31:50,994'); seek(1910.0)">
              Paris. And then we ask question, where's Paris? Right. You know,
            </span>
            
            <span id="chunk-502" class="transcript-chunks" onclick="console.log('00:31:54,442'); seek(1914.0)">
              so it's answers based on that, based on that context.
            </span>
            
            <span id="chunk-503" class="transcript-chunks" onclick="console.log('00:31:58,914'); seek(1918.0)">
              So you can ask follow up questions. Of course you can plan when they trip
            </span>
            
            <span id="chunk-504" class="transcript-chunks" onclick="console.log('00:32:02,074'); seek(1922.0)">
              or whatever, but. And you can also use it as open AI
            </span>
            
            <span id="chunk-505" class="transcript-chunks" onclick="console.log('00:32:06,082'); seek(1926.0)">
              API server. As you can imagine, you can have your knowledge base
            </span>
            
            <span id="chunk-506" class="transcript-chunks" onclick="console.log('00:32:09,930'); seek(1929.0)">
              about a code repository and then use
            </span>
            
            <span id="chunk-507" class="transcript-chunks" onclick="console.log('00:32:13,270'); seek(1933.0)">
              a fine tuned model that generates code or jSon, and have this
            </span>
            
            <span id="chunk-508" class="transcript-chunks" onclick="console.log('00:32:17,190'); seek(1937.0)">
              packaged together as an API server that connects to a chatbot or
            </span>
            
            <span id="chunk-509" class="transcript-chunks" onclick="console.log('00:32:21,454'); seek(1941.0)">
              something like that. That allows you to build
            </span>
            
            <span id="chunk-510" class="transcript-chunks" onclick="console.log('00:32:25,014'); seek(1945.0)">
              an entirely portable application that can
            </span>
            
            <span id="chunk-511" class="transcript-chunks" onclick="console.log('00:32:29,014'); seek(1949.0)">
              run across many different GPU architectures and drivers without
            </span>
            
            <span id="chunk-512" class="transcript-chunks" onclick="console.log('00:32:32,886'); seek(1952.0)">
              any need for large dependencies like Python.
            </span>
            
            <span id="chunk-513" class="transcript-chunks" onclick="console.log('00:32:37,094'); seek(1957.0)">
              All right, so that brings the end to our
            </span>
            
            <span id="chunk-514" class="transcript-chunks" onclick="console.log('00:32:41,362'); seek(1961.0)">
              talk. I think we're going to have 30 minutes. And so we
            </span>
            
            <span id="chunk-515" class="transcript-chunks" onclick="console.log('00:32:46,258'); seek(1966.0)">
              have done three different demos, from the
            </span>
            
            <span id="chunk-516" class="transcript-chunks" onclick="console.log('00:32:49,682'); seek(1969.0)">
              easy to the most sophisticated. And I hope you
            </span>
            
            <span id="chunk-517" class="transcript-chunks" onclick="console.log('00:32:52,938'); seek(1972.0)">
              would have time to at least install Lama
            </span>
            
            <span id="chunk-518" class="transcript-chunks" onclick="console.log('00:32:56,754'); seek(1976.0)">
              edge on your own computer and install large language models so you can play with
            </span>
            
            <span id="chunk-519" class="transcript-chunks" onclick="console.log('00:32:59,650'); seek(1979.0)">
              it. And if you find it interesting, you could install garnet
            </span>
            
            <span id="chunk-520" class="transcript-chunks" onclick="console.log('00:33:04,044'); seek(1984.0)">
              node and build your own knowledge base and have
            </span>
            
            <span id="chunk-521" class="transcript-chunks" onclick="console.log('00:33:07,604'); seek(1987.0)">
              a large language model on your own device, answer questions in the way that you
            </span>
            
            <span id="chunk-522" class="transcript-chunks" onclick="console.log('00:33:10,884'); seek(1990.0)">
              want. So yeah,
            </span>
            
            <span id="chunk-523" class="transcript-chunks" onclick="console.log('00:33:14,364'); seek(1994.0)">
              there are a lot more that we can get into. So for instance,
            </span>
            
            <span id="chunk-524" class="transcript-chunks" onclick="console.log('00:33:17,596'); seek(1997.0)">
              what's the rust application look like, how does SDK look like? And things
            </span>
            
            <span id="chunk-525" class="transcript-chunks" onclick="console.log('00:33:21,044'); seek(2001.0)">
              like that. But I don't think we have time for that at this moment.
            </span>
            
            <span id="chunk-526" class="transcript-chunks" onclick="console.log('00:33:24,100'); seek(2004.0)">
              And if you're interested, we have those three open
            </span>
            
            <span id="chunk-527" class="transcript-chunks" onclick="console.log('00:33:27,900'); seek(2007.0)">
              source GitHub repositories that you can,
            </span>
            
            <span id="chunk-528" class="transcript-chunks" onclick="console.log('00:33:31,754'); seek(2011.0)">
              that you can go to.
            </span>
            
            <span id="chunk-529" class="transcript-chunks" onclick="console.log('00:33:34,514'); seek(2014.0)">
              And please feel free to raise the issue and find
            </span>
            
            <span id="chunk-530" class="transcript-chunks" onclick="console.log('00:33:37,882'); seek(2017.0)">
              us there and chat with us. All right, thank you so much.
            </span>
            
            </div>
          </div>
          
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/Michael%20Yuan%20-%20Conf42%20Large%20Language%20Models%20%28LLMs%29%202024.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/Michael%20Yuan%20-%20Conf42%20Large%20Language%20Models%20%28LLMs%29%202024.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #CCB87B;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/llms2024" class="btn btn-sm btn-danger shadow lift" style="background-color: #CCB87B;">
                <i class="fe fe-grid me-2"></i>
                See all 28 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Michael%20Yuan_llm.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Michael Yuan
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Co-founder @ Second State & WasmEdge
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  
                  <a href="https://twitter.com/@juntao" target="_blank">
                    <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="Michael Yuan's twitter account" />
                  </a>
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by @juntao"
                  data-url="https://www.conf42.com/llms2024"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/llms2024"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>






    <!-- WELCOME -->
    <section class="pt-8 pt-md-11 pb-10 pb-md-15 bg-info" id="register">

      <!-- Shape -->
      <div class="shape shape-blur-3 text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>      </div>

      <!-- Content -->
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 col-md-10 col-lg-8 text-center">

            <!-- Heading -->
            <h1 class="display-2 text-white">
              Join the community!
            </h1>

            <!-- Text -->
            <p class="lead text-white text-opacity-80 mb-6 mb-md-8">
              Learn for free, join the best tech learning community 
              for a <a class="text-white" href="https://www.reddit.com/r/sanfrancisco/comments/1bz90f6/why_are_coffee_shops_in_sf_so_expensive/" target="_blank">price of a pumpkin latte</a>.
            </p>

            <!-- Form -->
            <form class="d-flex align-items-center justify-content-center mb-7 mb-md-9">

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Annual
              </span>

              <!-- Switch -->
              <div class="form-check form-check-dark form-switch mx-3">
                <input class="form-check-input" type="checkbox" id="billingSwitch" data-toggle="price" data-target=".price">
              </div>

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Monthly
              </span>

            </form>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->

    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- PRICING -->
    <section class="mt-n8 mt-md-n15">
      <div class="container">
        <div class="row gx-4">
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-1">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Newsletter</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="0" data-monthly="0">0</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Event notifications, weekly newsletter
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Delayed access</b> to all content
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Immediate access to Keynotes & Panels
                  </p>
                </div>
              
              
              </div>
            </div>

            <!-- Card -->
            <div class="card shadow-lg mb-6 border border-success">
              <div class="card-body">

                <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
</script>

<!-- Form -->
<link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
<p class="emailoctopus-success-message text-success"></p>
<p class="emailoctopus-error-message text-danger"></p>
<form
    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
    method="post"
    data-message-success="Thanks! Check your email for further directions!"
    data-message-missing-email-address="Your email address is required."
    data-message-invalid-email-address="Your email address looks incorrect, please try again."
    data-message-bot-submission-error="This doesn't look like a human submission."
    data-message-consent-required="Please check the checkbox to indicate your consent."
    data-message-invalid-parameters-error="This form has missing or invalid fields."
    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
    class="emailoctopus-form"
    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
>
<div class="form-floating emailoctopus-form-row">
    <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
    <label for="field_0">Email address</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
    <label for="field_1">First Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
    <label for="field_2">Last Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
    <label for="field_4">Company</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
    <label for="field_5">Job Title</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
    <label for="field_3">Phone Number</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
    oninput="updateCountry()"
    >
    <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
    </select>
    <label for="field_7">Country</label>
</div>
<input id="country-destination" name="field_7" type="hidden">
<input id="tz-country" name="field_8" type="hidden">

<input
    name="field_6"
    type="hidden"
    value="Large Language Models"
>

<div class="emailoctopus-form-row-consent">
    <input
    type="checkbox"
    id="consent"
    name="consent"
    >
    <label for="consent">
    I consent to the following terms:
    </label>
    <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
    Terms and Conditions
    </a>
    &amp;
    <a href="./code-of-conduct" target="_blank">
    Code of Conduct
    </a>
</div>
<div
    aria-hidden="true"
    class="emailoctopus-form-row-hp"
>
    <input
    type="text"
    name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
    tabindex="-1"
    autocomplete="nope"
    >
</div>
<div class="mt-6 emailoctopus-form-row-subscribe">
    <input
    type="hidden"
    name="successRedirectUrl"
    >
    <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
    Subscribe to free newsletter <i class="fe fe-arrow-right ms-3"></i>
    </button>
</div>
</form>

<!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
<script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

              </div>
            </div>
          </div>
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-0">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Community</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="8.34" data-monthly="10">8.34</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Access to <a href="https://conf42.circle.so/">Circle community platform</a>
                  </p>
                </div>

                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Immediate access</b> to all content
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank"><b>Live events!</b></a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank">Regular office hours, Q&As, CV reviews</a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Courses, quizes & certificates
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Community chats
                  </p>
                </div>
                

                <!-- Button -->
                <a href="https://conf42.circle.so/checkout/subscribe" class="btn w-100 btn-primary">
                  Join the community (7 day free trial)<i class="fe fe-arrow-right ms-3"></i>
                </a>

              </div>
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2025">
                  Observability 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2025">
                  Quantum Computing 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2025">
                  Rustlang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2025">
                  Platform Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2025">
                  MLOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2025">
                  Incident Management 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2025">
                  Kube Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2025">
                  JavaScript 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2025">
                  Prompt Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/robotics2025">
                  Robotics 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2025">
                  DevSecOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2025">
                  Internet of Things (IoT) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>