1
00:00:00,500 --> 00:00:01,160
Hello everyone.

2
00:00:01,339 --> 00:00:04,960
My name is, I am a solutions
engineer at Symbiotic LLC.

3
00:00:05,710 --> 00:00:09,249
I have just over six years of experience
in warehouse automation engineering,

4
00:00:09,729 --> 00:00:13,629
where I design, implement large
scale automated distribution centers.

5
00:00:14,200 --> 00:00:18,210
My work spans fortune hundred
retailers across grocery, healthcare

6
00:00:18,570 --> 00:00:20,415
app, RL, and paid food sectors.

7
00:00:20,915 --> 00:00:24,980
I help them transform supply chain
operations with robotics, ai.

8
00:00:25,685 --> 00:00:28,355
And advance ML operation practices.

9
00:00:29,105 --> 00:00:33,245
Today we'll be discussing why AI
system that look flawless in the lab

10
00:00:33,305 --> 00:00:38,045
often fail in real world warehouses
or experiences, and most importantly,

11
00:00:38,315 --> 00:00:40,775
how we can prevent those failures.

12
00:00:41,274 --> 00:00:42,774
So let me begin with Paradox.

13
00:00:43,274 --> 00:00:48,464
The autonomous mobile market, mobile
robot market is surging worldwide.

14
00:00:48,964 --> 00:00:53,464
Warehouses are inventing investing
millions into AI systems that

15
00:00:53,464 --> 00:00:56,524
are in the lab and deliver
success rates of above 90%.

16
00:00:57,364 --> 00:01:00,904
But there is reality when these
systems enter real warehouses,

17
00:01:00,964 --> 00:01:02,765
performances often collapses.

18
00:01:03,455 --> 00:01:08,464
We have seen projects delayed for months,
millions wasted, and operations disrupted.

19
00:01:09,335 --> 00:01:10,354
Why does this happen?

20
00:01:11,225 --> 00:01:12,705
That's the story I will
share with you today.

21
00:01:13,204 --> 00:01:17,925
So here is what I'll cover in next
20 minutes for devastating failure

22
00:01:17,925 --> 00:01:23,485
patterns in ml in real world that
are not considered, that issues

23
00:01:23,485 --> 00:01:25,335
were not considered in labs.

24
00:01:25,995 --> 00:01:31,695
The blind spot in the ML operations
that caused them performance breakdowns,

25
00:01:31,965 --> 00:01:33,675
game changing strategies for success.

26
00:01:34,175 --> 00:01:37,525
My goal is my goal is to leave
you a practical playbook that

27
00:01:37,525 --> 00:01:39,805
you can take back to your teams.

28
00:01:40,305 --> 00:01:44,465
So first failure pattern is the
physical reality gap in the lab.

29
00:01:44,885 --> 00:01:49,535
Robotic picking bot looks flawless
as you can see in the picture.

30
00:01:49,985 --> 00:01:53,165
But what happens when the
system meets the real world?

31
00:01:53,565 --> 00:01:55,484
Scenarios like transparent shrink wrap.

32
00:01:55,984 --> 00:02:02,555
When the pallet is built completely by
robots and they wrapped with plastic.

33
00:02:03,055 --> 00:02:07,645
Then second one is irregular shape
products, and third one is dust

34
00:02:07,645 --> 00:02:12,154
interfering with vision sensors which are
built, or which are, which have placed

35
00:02:12,154 --> 00:02:17,904
on the robots, the accuracy plunges,
sometimes from 95% down to early 60%,

36
00:02:18,404 --> 00:02:18,624
and.

37
00:02:19,434 --> 00:02:21,684
What are the blind spots do you think of?

38
00:02:22,075 --> 00:02:24,174
I think blind spots are clear here.

39
00:02:24,954 --> 00:02:29,124
First of all, if I was mentioning
about lab, that means training data.

40
00:02:29,484 --> 00:02:35,484
That lab is considering lacked messy real
world cases or real world experiences.

41
00:02:35,484 --> 00:02:41,244
I would say second one is validation
happen only in labs, not on life floors.

42
00:02:41,744 --> 00:02:46,939
And when errors happen, no feedback
loop pushed back into retraining.

43
00:02:47,439 --> 00:02:50,859
So teams optimized for
benchmarks instead of reality.

44
00:02:51,579 --> 00:02:56,999
They set their benchmark and they aim high
in lab, but in reality it was different.

45
00:02:57,449 --> 00:03:02,679
So the results was brittle systems
I, so second failure pattern I would

46
00:03:02,949 --> 00:03:07,769
say is forecasting models predicted
demand perfectly until market

47
00:03:07,769 --> 00:03:09,779
volatility hit volatility, hit.

48
00:03:10,559 --> 00:03:16,139
Suddenly we had overstocking of wrong
SKUs, empty shelves during peak demand

49
00:03:16,639 --> 00:03:19,039
and error rates, spiraling, unchecked.

50
00:03:19,539 --> 00:03:20,949
And what other root cause do you think of?

51
00:03:21,449 --> 00:03:26,999
So root cause first one is no robust
drift detection, outdated ground

52
00:03:26,999 --> 00:03:29,779
truth, and no human in loop oversight.

53
00:03:30,260 --> 00:03:34,339
That means human in the loop verification
points led to cascading errors.

54
00:03:34,839 --> 00:03:39,689
When error cost, and fourth one is
no fallback plan when things went

55
00:03:39,689 --> 00:03:45,599
wrong, there should be some plan B.
When we think of going system in wrong

56
00:03:46,409 --> 00:03:49,099
direction, it's like driving blind.

57
00:03:49,754 --> 00:03:52,894
The model keeps staring even
as it heads off the clip.

58
00:03:53,394 --> 00:03:56,664
Third pattern is similar,
I would say to everyone.

59
00:03:57,024 --> 00:04:01,754
Have you ever had a deployment
where warehouses ran seven to eight

60
00:04:01,754 --> 00:04:06,865
middleware systems, and each one seemed
fine in isolation, but the moment

61
00:04:06,865 --> 00:04:11,784
traffic spiked, the latency cascade
across them like falling dominoes?

62
00:04:12,745 --> 00:04:16,825
The single schema mismatch
become single point of failure,

63
00:04:17,544 --> 00:04:19,104
the entire deploy deployment.

64
00:04:19,779 --> 00:04:24,480
Collapsed like house of cards
and the root cause issues.

65
00:04:24,980 --> 00:04:28,940
The root cause issues are monitoring
was not shared across teams.

66
00:04:29,030 --> 00:04:34,120
When we are monitoring any issues or we
are finding any, we are, if we are, we

67
00:04:34,120 --> 00:04:38,050
have new findings, we should always share
across the teams and get their feedback.

68
00:04:38,550 --> 00:04:42,090
Second one is testing only covered
components, not the full system,

69
00:04:42,840 --> 00:04:44,755
and no graceful degradation plans.

70
00:04:45,450 --> 00:04:50,690
There should be some plan B always
in term when we know there might

71
00:04:50,690 --> 00:04:52,640
be some chances of failures.

72
00:04:53,140 --> 00:04:57,520
And the last failure I would
think of is human factor.

73
00:04:58,020 --> 00:05:02,685
You can design most advanced systems, but
if workers don't trust, they won't use it.

74
00:05:03,185 --> 00:05:07,150
Think of this what happens when early
failures cause operators to bypass

75
00:05:07,150 --> 00:05:10,175
the automation I have dealt with.

76
00:05:10,850 --> 00:05:16,190
Operators with 20 years of experience,
they are doing things manually.

77
00:05:16,250 --> 00:05:21,260
And when it comes, when the point comes of
automation implementation and using those,

78
00:05:21,650 --> 00:05:28,750
the operators are hesitant to use the new
advanced systems due to fear of losing job

79
00:05:28,990 --> 00:05:31,990
or they are not ready to use advancement.

80
00:05:32,200 --> 00:05:35,290
They think they are more productive
the way they're doing the things.

81
00:05:36,040 --> 00:05:39,010
That's exactly what
happened at one autonomous.

82
00:05:39,790 --> 00:05:45,530
Parts distributor, they invested
approximate 7 million system and, but the

83
00:05:45,530 --> 00:05:47,930
no work, but the workforce rejected it.

84
00:05:48,380 --> 00:05:54,185
The entire system was abandoned
and they wasted $7 million.

85
00:05:54,685 --> 00:05:57,895
So what are the game changing
strategies you think of here?

86
00:05:58,395 --> 00:06:01,395
So I think first one is phase deployment.

87
00:06:01,895 --> 00:06:06,425
Start with small control pilots
running shadow mode alongside existing

88
00:06:06,425 --> 00:06:11,075
operations Scale progressively
when metrics are validated.

89
00:06:11,765 --> 00:06:15,335
So I have been working closely
with one of the customer a

90
00:06:15,335 --> 00:06:17,495
retailer biggest customer in USA.

91
00:06:17,795 --> 00:06:21,575
They have implementing their
warehouse automation into.

92
00:06:22,099 --> 00:06:22,880
Different phases.

93
00:06:22,880 --> 00:06:28,099
For one of the FA Warehouse, they
have three to four phases and they

94
00:06:28,099 --> 00:06:29,870
will work on only one phase at a time.

95
00:06:30,560 --> 00:06:33,469
And alongside they will have
their existing operations running.

96
00:06:34,099 --> 00:06:38,510
And this help that organization
drastically, which reduce

97
00:06:38,510 --> 00:06:42,710
integration risk by nearly 40%
compared to Big Bang launches.

98
00:06:43,280 --> 00:06:46,280
So obviously phase
deployment always helps.

99
00:06:46,460 --> 00:06:47,630
This is the first strategy.

100
00:06:47,840 --> 00:06:49,805
I would think of and I
would suggest definitely.

101
00:06:50,305 --> 00:06:53,005
The second one is comprehensive
testing protocols.

102
00:06:53,505 --> 00:06:56,445
We should have a comprehensive
testing protocols use advise

103
00:06:56,745 --> 00:06:58,875
testing, stress test edge cases.

104
00:06:58,935 --> 00:07:02,835
Always use the worst case
scenarios for your testing.

105
00:07:03,405 --> 00:07:08,530
Use chaos engineering deliberately
break things and see how your system

106
00:07:09,030 --> 00:07:11,310
approaches or deals with these things.

107
00:07:12,060 --> 00:07:14,190
Augment training with
synthetic edge cases.

108
00:07:14,790 --> 00:07:17,910
And test, test the whole
integrated system under peak load.

109
00:07:18,410 --> 00:07:22,760
And one other thing is always this
system we should test in peak modes,

110
00:07:22,760 --> 00:07:27,650
which is like when you have festivals
like Christmas, black Friday, sales.

111
00:07:28,130 --> 00:07:33,780
So we should consider those peak times
and test your systems so that you get a

112
00:07:33,780 --> 00:07:37,410
hundred percent results and you understand
where the failure might come from.

113
00:07:37,910 --> 00:07:41,630
Those who adopted these methods
cut failures drastic dramatically.

114
00:07:42,130 --> 00:07:45,910
Third strategy, I would
think of human AI approaches.

115
00:07:45,940 --> 00:07:47,200
Hybrid human AI approach.

116
00:07:47,700 --> 00:07:49,410
Define clear human in loop.

117
00:07:49,440 --> 00:07:54,630
In the loop protocols, there should
be always some protocols to inter

118
00:07:55,050 --> 00:07:59,340
intervene humans in the automation
and they should work side by side.

119
00:07:59,840 --> 00:08:02,390
Feed human corrections
back into the training.

120
00:08:02,750 --> 00:08:06,820
If there are any correction errors made
by human, we should always feed them

121
00:08:06,820 --> 00:08:10,540
into training so that those errors are
not, have, will not happen in future.

122
00:08:11,440 --> 00:08:14,560
And design AI to augment
humans, not to replace them.

123
00:08:15,340 --> 00:08:19,200
The best outcomes I have,
the best outcomes I have

124
00:08:19,200 --> 00:08:21,240
seen were not just accuracy.

125
00:08:21,945 --> 00:08:25,755
Some warehouses achieved 99 plus
percent of inventory accuracy

126
00:08:25,815 --> 00:08:27,795
and improved workforce morality.

127
00:08:28,295 --> 00:08:31,265
So as I promised, there
will be a playbook.

128
00:08:31,265 --> 00:08:33,035
So here's the playbook I would think of.

129
00:08:33,245 --> 00:08:35,075
First, I would say it's validation.

130
00:08:35,735 --> 00:08:42,035
Validate the in the new invention in
real world condition, not just in labs.

131
00:08:42,125 --> 00:08:46,420
Always use worst case
scenarios and test your.

132
00:08:46,920 --> 00:08:50,760
Operations, monitor
everything across all systems.

133
00:08:51,000 --> 00:08:54,360
Integrate humans in the loop
and take their feedback and

134
00:08:55,020 --> 00:08:56,580
use that data to retrain them.

135
00:08:57,080 --> 00:08:59,720
Deploy incrementally, not all at once.

136
00:09:00,220 --> 00:09:03,190
Plan for resilience, graceful
degradation, and fallback modes.

137
00:09:03,190 --> 00:09:07,510
There should be always some plan
beef when it comes or when you

138
00:09:07,510 --> 00:09:09,040
know that the system might fail,

139
00:09:09,540 --> 00:09:10,260
For closing.

140
00:09:10,320 --> 00:09:14,730
And the key takeaways I would think
of is the most successful warehouse

141
00:09:14,730 --> 00:09:18,540
automation projects don't always
have most advanced algorithms.

142
00:09:19,350 --> 00:09:23,850
They succeed because they follow
machine learning operation practices.

143
00:09:24,000 --> 00:09:28,380
So remember, the Lab of Success
is not operational success.

144
00:09:28,880 --> 00:09:30,740
Monitor everything and plan for it.

145
00:09:30,920 --> 00:09:35,470
Plan for the drift test systems
holistically, not just in parts that.

146
00:09:36,090 --> 00:09:39,120
Test them in parts as well
as a hundred percent with a

147
00:09:39,120 --> 00:09:40,620
hundred percent implementation.

148
00:09:41,460 --> 00:09:43,200
Never underestimate the human factor.

149
00:09:43,440 --> 00:09:49,200
Always consider humans operators when
implementing a new warehouse automation,

150
00:09:49,950 --> 00:09:51,780
because they're the one will be working.

151
00:09:51,900 --> 00:09:53,190
Ask them questions.

152
00:09:53,730 --> 00:09:58,650
Ask or take their feedback in the
loop and apply when you are designing

153
00:09:58,710 --> 00:10:00,810
or when you're implementing things.

154
00:10:01,310 --> 00:10:07,670
Also make sure or feel them, let them or
try to make them comfortable with the new

155
00:10:07,670 --> 00:10:12,810
machineries or new system so that they
don't feel that they're losing their jobs

156
00:10:13,650 --> 00:10:15,720
and phase deployments to manage risk.

157
00:10:16,230 --> 00:10:22,065
Always deploy these new automation
in different phases, not whole

158
00:10:22,680 --> 00:10:24,480
warehouse system in one go.

159
00:10:24,980 --> 00:10:25,580
So that's all.

160
00:10:26,510 --> 00:10:27,260
Thank you everyone.

161
00:10:27,770 --> 00:10:28,220
Thank you for your.

