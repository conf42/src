1
00:00:00,500 --> 00:00:04,039
Deploying infrastructure is always
a mix of planning and firefighting.

2
00:00:04,430 --> 00:00:06,260
Sometimes everything works perfectly.

3
00:00:06,380 --> 00:00:10,640
Other times you are troubleshooting
at 2:00 AM wondering what went wrong.

4
00:00:11,480 --> 00:00:12,920
Hey guys, my name is Prade Gadi.

5
00:00:13,340 --> 00:00:14,870
I'm a senior DevOps engineer.

6
00:00:15,370 --> 00:00:20,200
My agenda today covers a little bit
about my journey and my experiences

7
00:00:20,200 --> 00:00:23,230
with incidents, tools and SRE world.

8
00:00:23,730 --> 00:00:28,529
Coming from banking and financial
services sector, I started my journey

9
00:00:28,529 --> 00:00:33,989
working with legacy systems such as web
spear, tomcat, and manual deployments.

10
00:00:34,489 --> 00:00:40,459
Custom scripts helped, but they introduced
a margin for error that made me appreciate

11
00:00:40,459 --> 00:00:45,290
the value of standardized tools like
Terraform Helm and CI CD pipelines.

12
00:00:46,010 --> 00:00:49,219
This talk is a collection
of real world SRD failures.

13
00:00:49,549 --> 00:00:53,119
Lessons learned and the
unexpected challenges that come

14
00:00:53,119 --> 00:00:55,309
with managing infrastructure.

15
00:00:55,699 --> 00:00:59,440
So let me go over my
experiences in real world.

16
00:01:00,400 --> 00:01:02,680
Let me start with a data center incident.

17
00:01:03,180 --> 00:01:03,930
Let me tell you this.

18
00:01:04,560 --> 00:01:09,060
Make sure your cloud provider service
terms aligned with your requirements,

19
00:01:09,120 --> 00:01:11,010
especially around RPOs and.

20
00:01:11,510 --> 00:01:15,920
When we signed up for when we signed
up for GCP, we used Google Cloud

21
00:01:15,920 --> 00:01:18,320
storage with dual region buckets.

22
00:01:19,070 --> 00:01:23,450
We assumed this meant our data would
be replicated instantly between

23
00:01:23,450 --> 00:01:25,700
regions, between different buckets.

24
00:01:26,200 --> 00:01:30,550
Then a data center fire hit and
objects in our GCS bucket where

25
00:01:30,580 --> 00:01:32,290
unavailable for up to two hours.

26
00:01:32,290 --> 00:01:32,380
Yes.

27
00:01:32,880 --> 00:01:40,740
We later discovered that gcps default
replication guarantees only 99.9% of newly

28
00:01:40,770 --> 00:01:46,110
written objects replicated within one
hour, a hundred percent within 12 hours.

29
00:01:46,200 --> 00:01:51,240
Meaning if a data center disaster happens,
there is a chance that some of your

30
00:01:51,240 --> 00:01:55,440
objects might be replicated, can take up
to 12 hours to replicate it to region.

31
00:01:55,940 --> 00:02:00,320
They later recommended enabling turbo
replication, which actually guarantees

32
00:02:00,320 --> 00:02:04,460
a hundred percent replication to
both regions in under 15 minutes.

33
00:02:04,979 --> 00:02:10,070
Regardless of object size, which
we would know this before that way

34
00:02:10,070 --> 00:02:13,589
we would enable this feature and we
didn't have to face this situation.

35
00:02:14,099 --> 00:02:15,299
So what's that lesson here?

36
00:02:15,509 --> 00:02:20,399
Always research if a product
meets your S-L-A-S-L-A needs

37
00:02:20,429 --> 00:02:21,659
before you go to production.

38
00:02:22,454 --> 00:02:25,454
Read the conditions,
apply details seriously.

39
00:02:25,674 --> 00:02:30,285
That's gonna save you from incidents
of this kind if such features exist,

40
00:02:30,645 --> 00:02:32,174
so you wouldn't face these issues.

41
00:02:32,535 --> 00:02:34,395
So research the product
before you sign up.

42
00:02:34,895 --> 00:02:39,090
Let me go over my experience and
incidents with respect to certificates.

43
00:02:39,845 --> 00:02:45,484
So SSL certificates, certificate issues
are more common than you would expect.

44
00:02:46,069 --> 00:02:49,729
A few years ago, public certificate
validated dropped from three years

45
00:02:49,729 --> 00:02:53,029
to one year, which means you have to
renew your certificates every one year.

46
00:02:53,529 --> 00:02:57,070
Especially the public wild card
certificates are often reused

47
00:02:57,070 --> 00:03:00,399
across multiple places, which
makes tracking them harder.

48
00:03:00,670 --> 00:03:01,719
So how do you track them?

49
00:03:01,959 --> 00:03:05,584
So let's say if you use commercial
products like Datadog look for

50
00:03:05,584 --> 00:03:07,325
features like synthetic monitoring.

51
00:03:07,910 --> 00:03:13,400
We, which you could use to track the
endpoints and get the certificate expiry

52
00:03:13,400 --> 00:03:15,500
data and renew the certificates on time.

53
00:03:16,040 --> 00:03:20,000
And if you're using open source such
as Grafana, look for Infinity Plugin.

54
00:03:20,750 --> 00:03:24,299
That way you could query the endpoint
and get the certificate information

55
00:03:24,299 --> 00:03:26,010
and keep track of the certificates.

56
00:03:26,399 --> 00:03:31,169
Also, proper doc certificate documentation
and the locations where it installed,

57
00:03:31,529 --> 00:03:33,390
they're installed is very important.

58
00:03:34,229 --> 00:03:34,979
It sounds basic.

59
00:03:35,489 --> 00:03:39,149
But certificate related outages
can be seriously embarrassing.

60
00:03:39,649 --> 00:03:40,820
Lemme tell you an example.

61
00:03:41,239 --> 00:03:45,679
At one of the company, we had an outage
because it was a financial client because

62
00:03:45,920 --> 00:03:47,869
an intermediate certificate was missing.

63
00:03:48,369 --> 00:03:50,869
Let me explain you
certificate terminology.

64
00:03:50,869 --> 00:03:51,649
In simple terms.

65
00:03:52,549 --> 00:03:57,290
The server certificate is signed by
an intermediate certificate, and the

66
00:03:57,290 --> 00:03:59,480
intermediate is signed by a trusted route.

67
00:03:59,979 --> 00:04:04,329
If the chain is, the complete
chain isn't complete, visitors

68
00:04:04,479 --> 00:04:07,249
may see an incomplete chain error.

69
00:04:08,059 --> 00:04:09,619
So how do you overcome this?

70
00:04:10,614 --> 00:04:15,444
The moment you renew any certificate,
go to open source online tools like

71
00:04:15,475 --> 00:04:19,765
SSL Labs to verify the complete
certificate change and make sure

72
00:04:19,765 --> 00:04:24,114
you have your route intermediate
and the server certificate in place.

73
00:04:24,614 --> 00:04:29,974
Also if you have open SSL utility on
your command line, just hit the end point

74
00:04:29,974 --> 00:04:31,894
and use the option called show search.

75
00:04:31,924 --> 00:04:34,964
It's gonna show you all the
certificate complete chain.

76
00:04:35,464 --> 00:04:38,134
Also, you could look for
something called verify written

77
00:04:38,134 --> 00:04:40,564
code to be zero at the bottom.

78
00:04:40,834 --> 00:04:44,254
That tells you like your certificate
is good and the chain is complete.

79
00:04:44,754 --> 00:04:47,594
Let me talk my experience
with logging and monitoring.

80
00:04:48,429 --> 00:04:53,449
Logs as an SRE logs are your lifetime
lifeline during incident, right?

81
00:04:53,869 --> 00:04:54,619
They're your lifeline.

82
00:04:55,369 --> 00:04:58,669
So if an incident happens, you
always look for logs and metrics.

83
00:04:59,169 --> 00:05:02,679
So ensure that all systems have
proper logging enabled before you

84
00:05:02,679 --> 00:05:06,109
even pro before you even promote
them to production and make sure like

85
00:05:06,109 --> 00:05:09,499
those logs are actually forwarded
to a centralized logging platform.

86
00:05:10,334 --> 00:05:14,034
This should be actually part of
your pre-deployment checklist.

87
00:05:14,534 --> 00:05:17,624
So let me describe a recent incident.

88
00:05:17,924 --> 00:05:21,155
So we had this incident where some
of the records were missing from a

89
00:05:21,155 --> 00:05:26,745
database and that database wasn't
configured properly to emit logging.

90
00:05:27,405 --> 00:05:30,825
So there many logs in that database.

91
00:05:31,275 --> 00:05:31,995
Also, the.

92
00:05:32,685 --> 00:05:36,195
Virtual missions, which we are
connecting to, that database had

93
00:05:36,195 --> 00:05:39,095
logs, but they weren't forwarded
to a central logging system.

94
00:05:39,185 --> 00:05:44,135
So we had to look for logs at multiple
places, which delayed our investigation.

95
00:05:44,635 --> 00:05:49,465
So it's always a good idea to, first of
all, have proper logging in place and

96
00:05:49,465 --> 00:05:54,745
second forward those logs to a centralized
system like Datadog or, Elasticsearch.

97
00:05:55,670 --> 00:05:56,210
If you, if.

98
00:05:56,710 --> 00:06:00,460
You don't wanna use commercial products
due to budget consent constraints.

99
00:06:00,490 --> 00:06:02,920
You could always set up open
source like Elasticsearch.

100
00:06:03,420 --> 00:06:06,480
Likewise, monitoring
plays an important role.

101
00:06:06,990 --> 00:06:10,990
Let me give you an in a simple incident,
happened at my previous company.

102
00:06:10,990 --> 00:06:15,740
So I did set up Elasticsearch logging
system, I didn't, I couldn't set up

103
00:06:15,740 --> 00:06:19,940
the monitoring for the disc due to
various reason and time constraints.

104
00:06:20,480 --> 00:06:21,680
It sounds simple, right?

105
00:06:21,740 --> 00:06:23,390
Being an sorry, how could I miss that?

106
00:06:23,960 --> 00:06:27,840
But, sometimes it's all about
the time and priorities.

107
00:06:28,710 --> 00:06:33,120
So I miss set setting up the dis alert.

108
00:06:33,330 --> 00:06:39,000
And moreover, elastic search wouldn't
ingest logs when it reaches 80%

109
00:06:39,000 --> 00:06:41,370
disc due to the watermark setup.

110
00:06:41,870 --> 00:06:43,730
So it's always better to set up.

111
00:06:44,599 --> 00:06:47,559
These alerts the moment you
install these software or the

112
00:06:47,559 --> 00:06:48,939
moment you install infrastructure.

113
00:06:49,330 --> 00:06:53,650
So the best way to best way is to use
some configuration management tools

114
00:06:53,650 --> 00:06:57,609
like Ansible or Puppet to ensure,
like these monitoring or logging

115
00:06:57,609 --> 00:07:01,239
agents are always installed as part
of your infrastructure provisioning.

116
00:07:02,109 --> 00:07:04,020
You all learn from our mistakes, isn't it?

117
00:07:04,520 --> 00:07:08,350
So the takeaway here is, always have
a centralized logging and monitoring

118
00:07:08,350 --> 00:07:11,800
system, and then that way it's easier
to track issues and fix them quickly.

119
00:07:12,300 --> 00:07:14,100
Let me brief my experience with Terraform.

120
00:07:14,820 --> 00:07:19,800
I didn't have any real incidents
with Terraform, but I absorbed things

121
00:07:19,800 --> 00:07:24,560
could be improved at a workspace,
by following few recommendations.

122
00:07:25,060 --> 00:07:30,010
So if possible, use public Terraform
modules provided by cloud vendors

123
00:07:30,070 --> 00:07:32,350
unless your use case is truly unique.

124
00:07:32,950 --> 00:07:37,150
I spent a lot of time writing
custom modules that I later replaced

125
00:07:37,150 --> 00:07:41,570
with standardized, well-maintained
modules from the Terraform, registry.

126
00:07:41,810 --> 00:07:44,990
It saved time and reduced maintenance.

127
00:07:45,680 --> 00:07:48,740
For example, let's say if
you wanna spin up a GCP vm.

128
00:07:49,715 --> 00:07:50,645
There are a couple of options.

129
00:07:50,675 --> 00:07:55,295
You could write all the modules and
then the resources by yourself, or if

130
00:07:55,295 --> 00:08:01,945
it's, or if your is a simple use case,
just try to use the preexisting public

131
00:08:01,945 --> 00:08:03,535
modules from the Terraform Registry.

132
00:08:04,035 --> 00:08:08,325
And then my second recommendation
would be, if possible, try to

133
00:08:08,835 --> 00:08:11,415
integrate your Terraform with CICD.

134
00:08:11,915 --> 00:08:15,365
In my previous workplace, we were
using, we were all using Terraform,

135
00:08:15,365 --> 00:08:16,975
running terraform from our workstation.

136
00:08:17,785 --> 00:08:22,585
You know that sometimes there is a
chance, like you set up some infra

137
00:08:22,585 --> 00:08:26,915
your provision infrastructure, but
that doesn't that doesn't have any

138
00:08:26,915 --> 00:08:28,775
approvals from any of your teammates.

139
00:08:29,105 --> 00:08:32,585
It's always better to have approval
from your teammates when you provision

140
00:08:32,585 --> 00:08:34,625
an infrastructure or, run any code.

141
00:08:35,150 --> 00:08:40,230
So having Terraform plan and apply
steps in your CSCD enables your

142
00:08:40,230 --> 00:08:43,260
teammates to review them before
the infrastructure is provisioned.

143
00:08:43,760 --> 00:08:49,660
Also, you could also use some security
tools like VI to scan your Terraform code.

144
00:08:49,840 --> 00:08:53,590
It catches misconfigurations
exposed secrets and comprehensive

145
00:08:53,590 --> 00:08:56,040
issues before the hit production.

146
00:08:56,540 --> 00:08:58,820
The takeaway here is reuse.

147
00:08:58,880 --> 00:08:59,840
What's reliable?

148
00:08:59,870 --> 00:09:02,420
Shift Terraform into your CICD workflow.

149
00:09:02,920 --> 00:09:04,240
Expect the unexpected.

150
00:09:05,080 --> 00:09:09,160
See, SRE is just SRE
isn't just about tools.

151
00:09:09,580 --> 00:09:12,640
It's about resilience,
adaptability, and curiosity.

152
00:09:13,360 --> 00:09:16,030
Sometimes you are solving infra shoes.

153
00:09:16,090 --> 00:09:18,970
Other times you are
wearing four hats at once.

154
00:09:19,570 --> 00:09:25,120
Yes, but every incident, every mistake,
and every fix is a chance to grow.

155
00:09:25,620 --> 00:09:30,090
In my last company, I did set up whole
monitoring system and I thought, I'm done.

156
00:09:30,390 --> 00:09:34,410
But no, the data which ha, which
was ingested into Prometheus

157
00:09:34,410 --> 00:09:36,180
was coming in different formats.

158
00:09:36,230 --> 00:09:39,650
Some data was coming in proto
of format, so I had to convert.

159
00:09:39,980 --> 00:09:43,010
I had to write a Python code and
deploy it in a cloud function to

160
00:09:43,010 --> 00:09:47,240
convert that proto of data into a
format which Prometheus understands.

161
00:09:47,870 --> 00:09:53,210
So it was an unexpected, crash course
across languages, formats, and platforms.

162
00:09:53,450 --> 00:09:54,725
That's all SRE about.

163
00:09:55,225 --> 00:09:59,105
And one, one other time I ha
we have chosen ela elastic

164
00:09:59,105 --> 00:10:00,425
for monitoring and logging.

165
00:10:00,455 --> 00:10:03,935
Assuming it's a free tire, you know
that the free tire would be enough.

166
00:10:04,385 --> 00:10:07,235
Turns out Slack alerts were
locked behind a paywall.

167
00:10:07,655 --> 00:10:11,170
Basically we ha we, we have
to upgrade to premium version.

168
00:10:11,440 --> 00:10:15,650
If we wanted to send any alerts
to the Slack, then we ended up

169
00:10:15,650 --> 00:10:19,040
writing our own Pi Python scripts
to send those alerts ourselves.

170
00:10:19,540 --> 00:10:24,580
So my point here is, always
pilot tools in dev environments

171
00:10:24,670 --> 00:10:26,200
before moving to productions.

172
00:10:26,890 --> 00:10:32,410
Because if you know what your requirements
better before you even move to production,

173
00:10:32,860 --> 00:10:34,860
then you would avoid a lot of manual work.

174
00:10:35,670 --> 00:10:38,480
You even know, if that product
is really worked for you.

175
00:10:38,980 --> 00:10:42,320
So the key take takeaway
here is, always pilot things.

176
00:10:42,695 --> 00:10:47,325
And then, try to learn new tech switch
hats and fix things under pressure.

177
00:10:47,355 --> 00:10:49,995
You had to be ready for that as an SRE.

178
00:10:49,995 --> 00:10:51,735
Not everything goes according to plan.

179
00:10:51,735 --> 00:10:55,815
Sometimes you are learning, piloting,
implementing, and fixing all in one day.

180
00:10:56,505 --> 00:10:59,295
It's intense, but that's
where the real growth happens.

181
00:10:59,795 --> 00:11:03,240
Thank you very much for listening
to my talk and thank you very much.

182
00:11:03,740 --> 00:11:05,510
This opportunity, please feel.

