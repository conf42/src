1
00:00:00,500 --> 00:00:01,100
All right.

2
00:00:01,670 --> 00:00:02,239
Hey guys.

3
00:00:02,900 --> 00:00:03,290
Good day.

4
00:00:03,830 --> 00:00:07,640
I'm here to talk about observability
with Kafka, and how do you make sure that

5
00:00:07,910 --> 00:00:11,810
you treat observability as a first class
citizen when you're dealing with Kafka,

6
00:00:12,080 --> 00:00:13,850
and how do you ensure visibility at scale?

7
00:00:14,420 --> 00:00:18,520
Kafka, as we know, is a backbone
for modern data streaming and any

8
00:00:18,520 --> 00:00:20,290
messaging ecosystems at this point.

9
00:00:20,740 --> 00:00:23,470
So anything as soon as.

10
00:00:23,970 --> 00:00:26,919
If you talk about Kafka, you'll
want to talk about observability

11
00:00:26,919 --> 00:00:29,799
because if your Kafka cluster fails.

12
00:00:30,299 --> 00:00:31,439
It's gonna be a good problem.

13
00:00:31,709 --> 00:00:36,459
So let's talk about how you can add
observability to your Kafka ecosystem.

14
00:00:37,129 --> 00:00:44,119
Using the MELT Stack as how I like
to call it, melt is an acronym for

15
00:00:44,449 --> 00:00:46,940
metrics, events, logs, and traces.

16
00:00:46,940 --> 00:00:49,160
And we are gonna talk
about all four of those.

17
00:00:49,819 --> 00:00:53,749
For Kafka let's start why
Kafka needs observability.

18
00:00:54,399 --> 00:00:57,309
As you can see here, server
rack in the photo here.

19
00:00:57,819 --> 00:01:02,589
Basically what this is trying to tell
you is that Kafka is not just one server.

20
00:01:02,829 --> 00:01:06,249
It's almost always a conglomerate
of servers working together.

21
00:01:06,749 --> 00:01:11,559
As a cluster to ensure high
availability and reliability and

22
00:01:11,679 --> 00:01:13,449
replication for your data sets.

23
00:01:14,069 --> 00:01:18,479
It has become a critical infrastructure
for a lot of data streaming systems.

24
00:01:19,049 --> 00:01:23,159
So an early detection is really critical.

25
00:01:23,679 --> 00:01:26,709
You wanna understand if there are,
there is gonna be a broker crash.

26
00:01:26,709 --> 00:01:30,189
If there is gonna, there is a lot of
memory pressure if there is a replication

27
00:01:30,189 --> 00:01:31,989
lag, if their consumers are lagging.

28
00:01:32,259 --> 00:01:32,949
Things like that.

29
00:01:33,539 --> 00:01:36,239
It also helps with fast
troubleshooting, right?

30
00:01:36,329 --> 00:01:40,799
If you have proper observability set
up for your Kafka clusters and your

31
00:01:40,799 --> 00:01:45,084
producers and consumers it makes
it a little easier for anybody and

32
00:01:45,084 --> 00:01:47,244
everyone to actually troubleshoot.

33
00:01:47,889 --> 00:01:52,839
The problems when they occur, and
maybe sometimes even before they occur.

34
00:01:53,460 --> 00:01:54,779
Yeah, troubleshooting.

35
00:01:54,839 --> 00:01:58,659
And then because you have all those
metrics, you will be able to dig

36
00:01:58,659 --> 00:02:02,409
deep into the performance insights
for the cluster and understand

37
00:02:02,409 --> 00:02:03,999
how your clusters performing.

38
00:02:04,050 --> 00:02:05,310
Is it doing really well?

39
00:02:05,310 --> 00:02:06,360
Where are the bottlenecks?

40
00:02:06,360 --> 00:02:07,200
Where are the issues?

41
00:02:07,230 --> 00:02:09,979
What the heat map looks
like, things like that.

42
00:02:10,460 --> 00:02:11,540
So let's get into it.

43
00:02:11,840 --> 00:02:13,400
Let's talk about the meld stack, right?

44
00:02:13,400 --> 00:02:17,210
We, I just introduced it earlier and we
are gonna talk about it a little more.

45
00:02:17,690 --> 00:02:21,120
In this pyramid, if you see the
basis is the metrics, right?

46
00:02:21,700 --> 00:02:24,760
We wanna make sure that we gather all
the metrics from Kafka and we are gonna

47
00:02:24,760 --> 00:02:26,630
talk about those what the metrics are.

48
00:02:26,660 --> 00:02:28,520
These are basically
numerical measurements.

49
00:02:29,020 --> 00:02:31,150
Next is the events, and we
are gonna talk about all of

50
00:02:31,150 --> 00:02:32,980
those in a little more detail.

51
00:02:33,580 --> 00:02:36,310
So bear with me while I
go through these things.

52
00:02:37,300 --> 00:02:42,790
Events are things like, actually,
let's talk about metrics first.

53
00:02:43,240 --> 00:02:46,870
Metrics are the numerical
measurements of what, of whatever

54
00:02:46,870 --> 00:02:48,310
is happening within your system.

55
00:02:48,820 --> 00:02:53,271
Those are metrics which are
exposed via the, Kafka brokers.

56
00:02:53,361 --> 00:02:59,121
They Kafka has a lot of, metrics that
it exposes out of the box by default.

57
00:02:59,631 --> 00:03:04,581
And you can use things like Prometheus
and Prometheus exporters to convert them

58
00:03:04,581 --> 00:03:10,011
into a proper open, telemetric compatible
formats or Prometheus compatible formats

59
00:03:10,011 --> 00:03:11,781
and expose all those MBE outside.

60
00:03:12,171 --> 00:03:14,181
Those are just the metrics
that you want to share.

61
00:03:14,181 --> 00:03:18,081
See, and those will give you
the performance traits, the

62
00:03:18,531 --> 00:03:21,531
functionality traits, and what's
going on within your ecosystem.

63
00:03:21,861 --> 00:03:23,571
Events are something like.

64
00:03:23,976 --> 00:03:25,596
Hey, my broker crashed.

65
00:03:25,806 --> 00:03:26,766
It's restarting.

66
00:03:26,766 --> 00:03:28,686
Something happened in the cluster.

67
00:03:29,306 --> 00:03:31,676
My disc is around 70% full.

68
00:03:31,736 --> 00:03:34,826
I should probably generate an
alert so that I can add more

69
00:03:35,186 --> 00:03:36,506
disc space to the cluster.

70
00:03:37,176 --> 00:03:41,016
My network throughput
is probably saturated.

71
00:03:41,076 --> 00:03:43,506
I need to think about either
adding more servers and

72
00:03:43,506 --> 00:03:45,886
rebalancing on the server servers.

73
00:03:46,216 --> 00:03:50,616
Or I have to think about how do
I scale up the network eco the

74
00:03:50,616 --> 00:03:52,116
network end points or the switches.

75
00:03:52,746 --> 00:03:54,546
So all of those could be events.

76
00:03:55,046 --> 00:03:59,236
Generally, the events from Kafka
are not exposed by the brokers.

77
00:03:59,236 --> 00:04:02,026
They'll probably be part
of the logs and metrics.

78
00:04:02,176 --> 00:04:06,746
So there will be no explicit discussion
around those that we make, but.

79
00:04:07,676 --> 00:04:08,816
Think about it, right?

80
00:04:08,816 --> 00:04:11,326
That there are certain things
that could be care categorized

81
00:04:11,326 --> 00:04:12,496
as events within Kafka.

82
00:04:13,256 --> 00:04:17,096
Nowadays people are even deploying Kafka
on Kubernetes, so all the Kubernetes

83
00:04:17,096 --> 00:04:18,596
events could be related as well.

84
00:04:18,656 --> 00:04:21,686
When there is a network change, when
the pod moves, when the pod crashes,

85
00:04:21,746 --> 00:04:25,766
when the pod restarts, all those events
might be correlatable to your Kafka.

86
00:04:26,556 --> 00:04:30,096
Performance and impacts on
their day-to-day working of

87
00:04:30,096 --> 00:04:31,506
that Kafka cluster as well.

88
00:04:31,876 --> 00:04:36,626
Event events are really helpful
optional, but helpful logs

89
00:04:37,166 --> 00:04:38,426
not at all optional, right?

90
00:04:38,456 --> 00:04:41,066
You'll want logs from your
Kafka brokers no matter what.

91
00:04:41,546 --> 00:04:46,866
You'll want your Kafka logs from somewhere
your in your client codes as well.

92
00:04:47,231 --> 00:04:49,061
You want all of them talk?

93
00:04:49,871 --> 00:04:52,070
Take a look at my older
older talks that I have.

94
00:04:52,521 --> 00:04:56,450
You will want all those logs
aggregated into an aggregation system.

95
00:04:56,450 --> 00:05:00,591
Instead of having five aggregated
systems, or five, five different

96
00:05:00,591 --> 00:05:03,260
systems for logs have maybe one.

97
00:05:03,811 --> 00:05:07,170
Where you get all that stuff
back and you can actually track

98
00:05:07,170 --> 00:05:08,610
them and search them easily.

99
00:05:09,030 --> 00:05:12,190
Logs are critical, super critical traces.

100
00:05:12,520 --> 00:05:15,911
So these function, not at the
Kafka broker level, but at the

101
00:05:15,916 --> 00:05:17,286
client level, more or less.

102
00:05:18,011 --> 00:05:22,271
The whole point of these things existing
and traces are more like, Hey, I'm

103
00:05:22,271 --> 00:05:26,801
gonna add a watermark to every one of
my message, and I'll see that wherever

104
00:05:26,801 --> 00:05:30,641
I see that message flowing, I'll be able
to track the entire workflow from there.

105
00:05:31,091 --> 00:05:32,741
And that's the whole purpose of traces.

106
00:05:33,251 --> 00:05:36,221
One, it gives you how much time
did it spend in particular.

107
00:05:36,821 --> 00:05:40,691
Specific client if it, if there
was some processing going on.

108
00:05:41,051 --> 00:05:44,801
Second, it also traces, like I
said, it traces the entire workflow

109
00:05:44,801 --> 00:05:48,821
for you, where was all, where
all was this message located?

110
00:05:48,851 --> 00:05:52,661
What all hops did it take and
how did it end up eventually?

111
00:05:53,021 --> 00:05:58,291
So traces optional, but again, good
to have a detailed view of what

112
00:05:58,291 --> 00:06:02,281
your functional system is doing
instead of the operat, instead of the

113
00:06:02,281 --> 00:06:04,321
operational system, how that's doing.

114
00:06:04,891 --> 00:06:09,841
That will give you your pyramid of the
melt stack, the top end, the complete

115
00:06:10,051 --> 00:06:14,821
observability for 360 degree visibility
in your Kafka cluster ecosystem.

116
00:06:15,321 --> 00:06:19,211
Tool chain recommendations are
generally hard and I don't like to

117
00:06:19,211 --> 00:06:21,341
make tool chain recommendations.

118
00:06:21,946 --> 00:06:25,416
But this is where I'm diverging
because I love Prometheus.

119
00:06:25,626 --> 00:06:31,586
Open source community sponsored, community
run, community driven, log aggregation.

120
00:06:31,616 --> 00:06:37,326
TSDB platform, not log, but metrics,
aggregation, TSDB time series database.

121
00:06:37,916 --> 00:06:38,756
Open telemetry.

122
00:06:38,756 --> 00:06:41,936
Again, as the name says, open,
that's a standard at this point.

123
00:06:42,056 --> 00:06:43,376
Everybody wants to use that.

124
00:06:43,406 --> 00:06:44,636
Everybody uses that.

125
00:06:44,696 --> 00:06:49,346
And because it's a standard, it's
easily you can move from Promeus

126
00:06:49,346 --> 00:06:52,526
to Prometheus to something else
in future if you really need to.

127
00:06:52,961 --> 00:06:56,111
And still say compatible if they
support the open telemetry formats.

128
00:06:56,531 --> 00:07:00,081
So that's something which I'll
say is critical that you want

129
00:07:00,081 --> 00:07:01,581
to have in your ecosystem.

130
00:07:02,481 --> 00:07:05,451
Grafana is one of my recommendations
just because I love Grafana.

131
00:07:06,276 --> 00:07:07,146
That's all right.

132
00:07:07,146 --> 00:07:11,106
It has nothing to do with the open
source, although they have an open

133
00:07:11,106 --> 00:07:14,636
source view and they have an open
source component that they allow

134
00:07:14,636 --> 00:07:16,716
you everybody to use in the world.

135
00:07:17,206 --> 00:07:18,676
But I just love Grafana.

136
00:07:18,676 --> 00:07:19,726
It's easy.

137
00:07:19,726 --> 00:07:21,116
It's much more manageable.

138
00:07:21,116 --> 00:07:24,716
It directly talks to Prometheus, it
talks to Elastic if it really needed to.

139
00:07:25,046 --> 00:07:28,626
There are like multiple ecosystems that
it can plug in with and still display

140
00:07:29,046 --> 00:07:31,086
your beautiful graphs as you need them.

141
00:07:31,186 --> 00:07:35,095
Grafana, it's, let's d start
diving into the meld stack now.

142
00:07:35,275 --> 00:07:38,395
Metrics system, health
indicators, what are they?

143
00:07:38,495 --> 00:07:41,925
As I just referencing earlier, these
are like numerical measurements that

144
00:07:41,925 --> 00:07:46,885
indicate your health of the system,
the performance of the system, what

145
00:07:46,885 --> 00:07:48,926
it's doing at that point, all those.

146
00:07:49,426 --> 00:07:52,915
Points of interest that you
would have in the ecosystem.

147
00:07:53,035 --> 00:07:54,985
Those are co, those are basically metrics.

148
00:07:55,325 --> 00:07:56,795
How many requests did I process?

149
00:07:57,205 --> 00:07:58,736
Is my disc back pressure?

150
00:07:58,825 --> 00:08:04,046
Is my network thread count enough to
tolerate the incoming brush of requests

151
00:08:04,046 --> 00:08:05,666
from the producers and consumers?

152
00:08:06,116 --> 00:08:09,506
That's where, that's the
data that you want, and those

153
00:08:09,506 --> 00:08:10,736
are all coming from metrics.

154
00:08:11,096 --> 00:08:15,326
Replication, lags metrics, consumer
lag metrics, all that stuff is metrics.

155
00:08:16,046 --> 00:08:20,905
They matter because they fail
from the basis of your view

156
00:08:20,905 --> 00:08:22,976
and observability into Kafka.

157
00:08:23,366 --> 00:08:26,816
So they are super important forms.

158
00:08:26,816 --> 00:08:29,876
The baseline foundations offer
pyramid of observability.

159
00:08:30,265 --> 00:08:33,116
So yeah, make sure you
have metrics available.

160
00:08:34,036 --> 00:08:34,606
Events.

161
00:08:34,816 --> 00:08:38,076
These are the second things which we want,
which we briefly talked about, right?

162
00:08:38,526 --> 00:08:40,116
Discrete state changes, right?

163
00:08:40,596 --> 00:08:43,536
For Kubernetes or crashes or restarts.

164
00:08:43,566 --> 00:08:45,516
There is a rollout, there
is a new deployment.

165
00:08:46,026 --> 00:08:47,826
500 different things can happen.

166
00:08:48,126 --> 00:08:48,936
Those are all events.

167
00:08:49,311 --> 00:08:50,841
Similar for Kafka, right?

168
00:08:50,991 --> 00:08:55,521
You could have a broker crash, you
could have a deployment, you could

169
00:08:55,521 --> 00:08:58,790
have a maintenance window where
your Kafka cluster is rolling.

170
00:08:58,911 --> 00:09:02,601
So all that stuff are
events for you, right?

171
00:09:02,661 --> 00:09:06,561
And those will provide you the
context to why your metrics are

172
00:09:06,561 --> 00:09:08,121
spiking at some point of time.

173
00:09:08,571 --> 00:09:12,581
So for example, let's say you
started a initiated a rollout for

174
00:09:12,581 --> 00:09:16,091
your cluster and you're rolling one
broker by after the other, right?

175
00:09:16,121 --> 00:09:18,971
So there will be latency spikes,
which you'll see from your producers,

176
00:09:18,971 --> 00:09:22,781
and then twin latencies will suffer
and they are suffering because

177
00:09:22,781 --> 00:09:23,921
you're clear rolling the cluster.

178
00:09:23,981 --> 00:09:25,571
There's no other correlation.

179
00:09:25,571 --> 00:09:26,951
More, more or less, right?

180
00:09:27,731 --> 00:09:28,181
And.

181
00:09:28,681 --> 00:09:32,881
If you see metrics by themselves in
that window, you'll probably not be

182
00:09:32,881 --> 00:09:34,801
able to understand why that's happening.

183
00:09:35,281 --> 00:09:39,751
Once you add an event to it, the
mixture, you'll be able to see, oh, okay.

184
00:09:39,811 --> 00:09:42,331
The latency spiked because
my cluster was rolling.

185
00:09:42,361 --> 00:09:45,781
I had a new power restart, and it
took up some time for it to get

186
00:09:45,901 --> 00:09:47,701
pro promoted itself to leaders.

187
00:09:48,181 --> 00:09:52,461
Things happened in, and the latency kicked
up a notch for a few milliseconds, right?

188
00:09:52,731 --> 00:09:54,111
That's where it, that's important.

189
00:09:54,411 --> 00:09:58,880
Logs like we talked, it's, they're
timestamped, they're detailed records of

190
00:09:59,180 --> 00:10:00,980
activities that the system was performing.

191
00:10:01,220 --> 00:10:06,830
So yeah, if there is any error, if there
is a error stack, Java error stack, that

192
00:10:06,830 --> 00:10:09,560
all comes out through your server logs.

193
00:10:10,130 --> 00:10:13,190
Things like authentications,
authorizations, whatever is

194
00:10:13,190 --> 00:10:14,300
happening within your system.

195
00:10:14,645 --> 00:10:16,385
Those will be tracked through the logs.

196
00:10:16,895 --> 00:10:19,385
Depends on the velocity of
the logs that you choose.

197
00:10:19,415 --> 00:10:25,085
But yeah, again, you can go as granular
as you want and keep or keep it as

198
00:10:25,085 --> 00:10:28,255
informatic, informational, as you want.

199
00:10:29,025 --> 00:10:35,175
They matter a lot because metrics, plus
logs, plus events, those are the way how

200
00:10:35,175 --> 00:10:39,465
you determine what your system is doing,
why your system is doing all that stuff.

201
00:10:39,855 --> 00:10:41,475
And it's important for Kafka as well.

202
00:10:42,075 --> 00:10:45,075
A big example of this could be,
hey, if there was a controller

203
00:10:45,075 --> 00:10:46,365
election, what happened?

204
00:10:46,765 --> 00:10:50,036
If there is frequent client
disconnections, what's going on?

205
00:10:50,575 --> 00:10:55,195
If there is a producer loop, when the
producer is producing, if the network

206
00:10:55,195 --> 00:11:00,385
who was backing up, if your disc was
backing up, what was going on, those kind

207
00:11:00,385 --> 00:11:02,395
of things can be exposed via the logs.

208
00:11:02,755 --> 00:11:04,885
Some of it is visible through the metrics.

209
00:11:04,975 --> 00:11:05,395
Sure.

210
00:11:05,786 --> 00:11:10,195
But once you get, get to see
the symptom, you'll want to see

211
00:11:10,195 --> 00:11:11,455
why that symptom's occurring.

212
00:11:11,455 --> 00:11:15,395
And that's gonna happen with the
logs, only with the logs, traces as

213
00:11:15,395 --> 00:11:18,635
we talked about it's at the functional
layer, so you may consider it as

214
00:11:18,635 --> 00:11:21,845
optional, but you'll want to think
about an end-to-end request flow,

215
00:11:21,935 --> 00:11:23,885
and that's where traces shine, right?

216
00:11:23,885 --> 00:11:24,935
It's not about.

217
00:11:25,250 --> 00:11:26,090
Kafka anymore.

218
00:11:26,090 --> 00:11:28,670
This is more like your application
and your business logic.

219
00:11:29,131 --> 00:11:33,540
A lot of customers do use it and
generally they had add headers

220
00:11:33,540 --> 00:11:35,431
to the Kafka message itself.

221
00:11:35,701 --> 00:11:37,801
So Kafka headers, every
message has a header.

222
00:11:37,801 --> 00:11:41,171
You can probably basically
add a header header row in

223
00:11:41,171 --> 00:11:42,941
there and have that watermark.

224
00:11:43,281 --> 00:11:46,461
Persists throughout the scope
of that life, of that message.

225
00:11:46,761 --> 00:11:49,311
And wherever it goes, wherever
it's read, that Kafka header can

226
00:11:49,311 --> 00:11:52,016
basically just say, Hey I came here.

227
00:11:52,136 --> 00:11:54,866
So that watermark will persist
in all those processing systems.

228
00:11:54,866 --> 00:11:59,726
Whatever you use on top of Kafka, they
will tell you that this message came here.

229
00:12:00,386 --> 00:12:03,506
So yeah, it helps with
transaction flows specifically.

230
00:12:03,806 --> 00:12:07,316
Like I said, you would want to
use all four of the MEL signals.

231
00:12:07,316 --> 00:12:09,476
They have complimentary perspectives.

232
00:12:10,076 --> 00:12:14,586
Not neither one of them
are enough by themselves.

233
00:12:15,291 --> 00:12:18,411
Right metrics will help you
determine anomalies in real time.

234
00:12:19,011 --> 00:12:23,631
Events will tie those anomalies to
specific changes if there are logs, will

235
00:12:23,631 --> 00:12:27,531
provide you deep details for debugging
and understanding if there was no

236
00:12:27,531 --> 00:12:31,371
anomaly, what happened in the system
so that you can see what's going on.

237
00:12:31,881 --> 00:12:36,411
And traces will actually help you
visualize the end-to-end paths above

238
00:12:36,411 --> 00:12:40,011
Kafka at some point of time in the
business layer so that you can understand

239
00:12:40,011 --> 00:12:44,061
what's going on to give you a holistic
view of what's going on in your system.

240
00:12:44,496 --> 00:12:48,246
Yeah, you want complete picture,
you want complimentary perspectives.

241
00:12:48,606 --> 00:12:51,756
That's why all signals are
more or less important.

242
00:12:52,266 --> 00:12:54,126
You choose which ones
are the best ones to use.

243
00:12:54,126 --> 00:12:57,786
To start with, my recommendation would
be start with metrics at a bare minimum

244
00:12:57,786 --> 00:13:01,776
and logs at a bare minimum, and then
graduate from there into the events

245
00:13:01,926 --> 00:13:04,146
and the tracing systems beyond that.

246
00:13:04,646 --> 00:13:09,806
Alright, let's talk about, let's dig a
little deeper and into the Kafka metrics.

247
00:13:10,306 --> 00:13:13,606
This is one of the big goals for
observability and you will wanna make

248
00:13:13,606 --> 00:13:15,466
sure that you have enough Kafka metrics.

249
00:13:16,126 --> 00:13:19,606
But Kafka can sometimes overwhelm
you with a lot of metrics.

250
00:13:20,536 --> 00:13:24,166
The last time I was counting and
it was like a fair two, three years

251
00:13:24,166 --> 00:13:30,286
ago, Kafka had about 3,500 MBS
that had exposed almost 3,500 mbs.

252
00:13:30,796 --> 00:13:36,106
Again, permutation and combinations
included not just the unique MBE names,

253
00:13:36,106 --> 00:13:38,026
but yeah, permutations for all the data.

254
00:13:38,026 --> 00:13:41,386
And that was very bare minimum cluster
that I was running on my home server.

255
00:13:41,986 --> 00:13:46,636
So yeah, it has a lot of data that it
spits out a lot of time series that will

256
00:13:46,636 --> 00:13:48,256
create, and you will have to make sure.

257
00:13:48,736 --> 00:13:51,046
That you are cherry picking
the right ones that you need.

258
00:13:51,466 --> 00:13:55,966
Not everything is as important
as something like replication

259
00:13:55,966 --> 00:13:56,986
lag, for example, right?

260
00:13:57,076 --> 00:13:59,776
Or CPU consumption or network IO load.

261
00:13:59,946 --> 00:14:01,686
How much is the network thread usage?

262
00:14:01,716 --> 00:14:05,636
Those kind of things are really
important to start with, the biggest

263
00:14:05,636 --> 00:14:06,896
key indicators, like I said, right?

264
00:14:06,896 --> 00:14:09,326
Replication, health,
partition distribution, how

265
00:14:09,536 --> 00:14:10,556
well are they distributed?

266
00:14:10,556 --> 00:14:12,086
How well are the leaders distributed?

267
00:14:12,086 --> 00:14:14,336
So you'll want to make
sure you understand those.

268
00:14:14,756 --> 00:14:19,376
You should not have a very hot
broker where a lot of partitions

269
00:14:19,406 --> 00:14:22,346
just concentrate together as a leader
and the others are sitting idle.

270
00:14:22,436 --> 00:14:25,496
You'll want those leadership to
get distributed, and you can use

271
00:14:25,496 --> 00:14:26,666
various tools for all that, right?

272
00:14:26,696 --> 00:14:30,866
This is not a Kafka admin talk, which
we are talking about, but you'll want

273
00:14:30,866 --> 00:14:32,546
to make sure that you have enough.

274
00:14:33,176 --> 00:14:38,576
Capacity available on each of these
brokers and those capacity being utilized

275
00:14:38,996 --> 00:14:44,916
almost evenly as much as possible so
that your cluster can shine the way

276
00:14:45,156 --> 00:14:49,566
it should as a cluster and should
not get throttled by a single broker.

277
00:14:49,846 --> 00:14:51,796
Error rates very important.

278
00:14:52,096 --> 00:14:53,506
How many requests failed?

279
00:14:53,506 --> 00:14:55,066
How many fetch requests failed?

280
00:14:55,066 --> 00:14:57,496
How many produced requests failed?

281
00:14:57,946 --> 00:14:59,746
Maybe something failed in replication.

282
00:14:59,776 --> 00:15:00,796
I wanna understand that.

283
00:15:01,216 --> 00:15:04,546
What are the error indicators
and what all those things mean?

284
00:15:04,966 --> 00:15:09,556
Probably something more important, but
you'll prob you'll want to understand

285
00:15:09,556 --> 00:15:13,696
those things to get more clear picture
of how your Kafka cluster is performing.

286
00:15:13,996 --> 00:15:17,326
I have mentioned network utilization
like multiple times already, right?

287
00:15:17,356 --> 00:15:21,496
Network utilization is by far the
most precious resource that you'll

288
00:15:21,496 --> 00:15:23,236
want to monitor for a Kafka cluster.

289
00:15:23,986 --> 00:15:25,876
Everything's network right in the end.

290
00:15:26,086 --> 00:15:28,876
Kafka is replicating a data
back in the background.

291
00:15:29,131 --> 00:15:33,040
Between multiple brokers across
the replica across the replicas

292
00:15:33,040 --> 00:15:35,800
that you allow it to, the number
of replicas that you configure it

293
00:15:35,800 --> 00:15:38,440
for those will need network, right?

294
00:15:38,440 --> 00:15:39,820
Enough network capacity.

295
00:15:40,060 --> 00:15:44,320
So if you're producing at a thousand
wags per second, or 125 megabytes

296
00:15:44,320 --> 00:15:50,471
per second, you are probably already
using most of the capacity available

297
00:15:50,471 --> 00:15:52,371
in the one gigabit ethernet port.

298
00:15:53,151 --> 00:15:57,591
You would probably want a 10 gigabit
that SFP port on your Kafka brokers.

299
00:15:57,621 --> 00:16:00,411
Again, not getting into the
hardware, but you have to understand

300
00:16:00,411 --> 00:16:01,551
the capacity requirements.

301
00:16:02,051 --> 00:16:04,121
At what scale your Kafka
cluster is operating.

302
00:16:04,121 --> 00:16:06,101
If it's a small scale cluster,
it doesn't matter, right?

303
00:16:06,131 --> 00:16:09,761
One gigabit per second ethernet
port probably will work.

304
00:16:10,151 --> 00:16:12,131
Probably need a bonded
port for that, right?

305
00:16:12,131 --> 00:16:14,681
And network one gigabit
per second bonded port.

306
00:16:14,771 --> 00:16:18,191
That might give you around
101, 1.5 gigabits per second.

307
00:16:18,701 --> 00:16:20,111
But it depends, right?

308
00:16:20,141 --> 00:16:25,151
Again, your use case, your scenarios, how
much your utilization is that is gonna

309
00:16:25,151 --> 00:16:29,561
be told by, or that is the answer which
you're gonna get from your metrics, right?

310
00:16:29,771 --> 00:16:31,151
So make sure you have those.

311
00:16:31,451 --> 00:16:32,321
How do you instrument those?

312
00:16:32,591 --> 00:16:34,181
So this is way more important.

313
00:16:34,301 --> 00:16:36,521
Now I've been talking about
why those are important, but

314
00:16:36,551 --> 00:16:37,511
how do you instrument those?

315
00:16:37,961 --> 00:16:41,891
Kafka exposes all those mbe,
they're called mbs, and those

316
00:16:41,891 --> 00:16:44,571
MBS are exposed via GMX.

317
00:16:45,221 --> 00:16:48,591
So you enable GMX metrics on
all the brokers and clients.

318
00:16:48,621 --> 00:16:51,741
Technically if you want the client
metrics, but brokers you definitely

319
00:16:51,741 --> 00:16:53,961
want, deploy a metrics scraper, right?

320
00:16:53,961 --> 00:16:54,981
Something like a Prometheus.

321
00:16:54,981 --> 00:16:56,721
JMX exporter is awesome.

322
00:16:56,891 --> 00:17:00,521
Recently it has been getting a
lot more attention than it was

323
00:17:00,521 --> 00:17:01,631
getting like three years ago.

324
00:17:02,121 --> 00:17:03,141
So it's improving a lot.

325
00:17:03,621 --> 00:17:09,351
It's basically the whole point of
A-A-G-M-X exporter is to convert

326
00:17:09,351 --> 00:17:16,541
the JMXM beans and convert them
from a Java centric format to more.

327
00:17:17,201 --> 00:17:20,081
TTP compatible format, if I may, right?

328
00:17:20,531 --> 00:17:22,391
It's a very weird analogy, but yeah.

329
00:17:22,841 --> 00:17:27,091
The JMX gets converted into a metrics
endpoint, which is an s ttp STTP

330
00:17:27,761 --> 00:17:32,381
endpoint on a specific port where it
generates a single page and all those

331
00:17:32,381 --> 00:17:34,751
lines it generates in there are metrics.

332
00:17:34,871 --> 00:17:38,601
That's it right now when you deploy those.

333
00:17:39,021 --> 00:17:42,891
The Kafka brokers will expose those
metrics, which are scrapable right

334
00:17:43,131 --> 00:17:46,791
now, your Prometheus endpoint or
Prometheus server or some other server,

335
00:17:46,791 --> 00:17:49,641
whatever you're using, they'll have
to go and scrape all those endpoints

336
00:17:49,641 --> 00:17:54,111
from all the brokers and gather
them into one single server, right?

337
00:17:54,381 --> 00:17:55,941
So you'll want to make sure that is done.

338
00:17:56,681 --> 00:18:01,031
For clients specifically, and this is
where, newer versions of Kafka Shines.

339
00:18:01,631 --> 00:18:05,561
Kafka added the feature where
clients can actually ship their

340
00:18:05,561 --> 00:18:07,811
metrics, some of their metrics.

341
00:18:08,251 --> 00:18:12,651
The really important ones, at least to
the Kafka broker for exposition, right?

342
00:18:12,921 --> 00:18:18,441
The, that's called Kip seven 14, and it
was available in Kafka 3.7 and above.

343
00:18:18,681 --> 00:18:20,871
So if you're using 3.7 and above.

344
00:18:21,371 --> 00:18:23,921
You have Kip seven 14 already available.

345
00:18:24,341 --> 00:18:26,381
We'll probably wanna implement that one.

346
00:18:26,891 --> 00:18:30,131
One abstract class that they
provided, and then it'll be able

347
00:18:30,131 --> 00:18:33,941
to expose it via open telemetry or
Prometheus based exposition formats.

348
00:18:34,391 --> 00:18:38,706
So you'll be able to centrally collect
the client metrics coming from all

349
00:18:38,706 --> 00:18:40,826
your clients directly from the brokers.

350
00:18:40,886 --> 00:18:44,711
So you would not have to
instrument Kafka and all its

351
00:18:44,711 --> 00:18:47,321
consumers or all its producers.

352
00:18:47,681 --> 00:18:52,841
With JMX, you'll probably just have
to do it for the Kafka cluster and

353
00:18:52,841 --> 00:18:55,631
it'll give you the most important
metrics from your clients as well.

354
00:18:55,991 --> 00:18:59,721
Pretty recent change, but this
has an awesome game changer event.

355
00:19:00,221 --> 00:19:04,811
So yeah, simple Flow Kafka, it
exposes the metrics in JMX format.

356
00:19:04,931 --> 00:19:08,621
You go through the the telemetry
collectors, open Telemetry or Prometheus,

357
00:19:08,621 --> 00:19:12,251
GMX, they are interoperable with
the open telemetry format as well.

358
00:19:12,661 --> 00:19:12,991
Prometheus.

359
00:19:13,651 --> 00:19:18,751
Scrapes, the Kafka endpoint, the Kafka
Open Telemetry, or the Prometheus Exporter

360
00:19:18,751 --> 00:19:23,041
endpoint and stores that time series
data within itself, Grafana basically

361
00:19:23,041 --> 00:19:27,601
connects to Prometheus and exposes that
data as pretty graphs or dashboards.

362
00:19:27,901 --> 00:19:31,121
We are not gonna talk about events
too much because events are in

363
00:19:31,121 --> 00:19:36,161
between the metrics and the logs
where some of the events can be.

364
00:19:36,986 --> 00:19:40,566
Decrypted for lack of a better
term, from the metrics itself.

365
00:19:40,986 --> 00:19:45,276
So basically, if your system is not
available or your broker's not online,

366
00:19:45,426 --> 00:19:49,116
it could mean that it either crashed
or it went down, or this is an event

367
00:19:49,116 --> 00:19:52,296
for a broker role of some kind, right?

368
00:19:52,836 --> 00:19:57,826
Similarly there are things like there is a
replication issue, probably logs are gonna

369
00:19:57,826 --> 00:19:59,966
tell you all that stuff much more cleaner.

370
00:20:00,086 --> 00:20:01,616
There is no inherent.

371
00:20:02,036 --> 00:20:05,576
It built way to expose
Kafka events as such.

372
00:20:06,206 --> 00:20:10,286
So you'll have to rely on a mixture
of logs and metrics to get some of

373
00:20:10,286 --> 00:20:12,626
those event related features available.

374
00:20:13,106 --> 00:20:17,126
So we, I'm gonna talk about logs
directly and just wanna tell you why

375
00:20:17,126 --> 00:20:19,256
events is not present in the slide deck.

376
00:20:19,556 --> 00:20:20,216
Kafka logs.

377
00:20:20,216 --> 00:20:22,986
This is the observability another
observability goal, right?

378
00:20:23,046 --> 00:20:27,626
Again, and one of the more important
ones gives you detailed troubleshooting.

379
00:20:27,776 --> 00:20:33,056
Logs are really rich and this, they
will tell you why the errors occurred,

380
00:20:33,056 --> 00:20:34,616
what you're seeing in the metrics.

381
00:20:34,646 --> 00:20:37,286
And so these are really important systems.

382
00:20:38,006 --> 00:20:41,306
Whenever there's an error, whenever
there's an issue, any stack trace,

383
00:20:41,426 --> 00:20:44,756
every stack trace is important, and
this is where logs will help you.

384
00:20:45,626 --> 00:20:47,876
RCAs cannot be completed without logs.

385
00:20:48,356 --> 00:20:50,696
Don't even think about it if you
don't have the logs available.

386
00:20:50,816 --> 00:20:52,646
So yeah, they are really important.

387
00:20:53,746 --> 00:20:56,916
And another thing which I would say is

388
00:20:57,916 --> 00:20:59,416
when you are exposing logs.

389
00:21:00,106 --> 00:21:04,941
Probably expose them as chase on values
if you can with and I think I'm probably

390
00:21:04,941 --> 00:21:06,861
gonna talk about that in a moment.

391
00:21:06,861 --> 00:21:07,521
Yeah, there you go.

392
00:21:07,881 --> 00:21:08,241
Alright.

393
00:21:08,311 --> 00:21:09,181
I'll get to that in a minute.

394
00:21:09,451 --> 00:21:09,781
Minute.

395
00:21:10,321 --> 00:21:13,451
So when you're talking about the logs
they'll give you the error details,

396
00:21:13,511 --> 00:21:16,391
they'll give you the state changes
again, goes back to the events.

397
00:21:16,571 --> 00:21:19,841
Controller elections, partition
movements, leadership changes, all

398
00:21:19,841 --> 00:21:24,461
that stuff are, could be technically
labeled as events, so you'll want to

399
00:21:24,461 --> 00:21:25,871
extract them from the logs itself.

400
00:21:26,261 --> 00:21:27,611
Hence logs are important.

401
00:21:28,041 --> 00:21:31,611
Client activity, it will tell you
what the client activity looks like.

402
00:21:32,041 --> 00:21:36,061
If you're connecting to older brokers
or older clients per se, those things,

403
00:21:36,331 --> 00:21:37,561
one of those big things, right?

404
00:21:37,561 --> 00:21:41,251
Then they'll come up as
the logs also as well.

405
00:21:41,791 --> 00:21:43,411
So yeah, all those things are important.

406
00:21:43,881 --> 00:21:44,571
Instrumentation.

407
00:21:45,231 --> 00:21:48,321
So you want your logs to be
really high quality, right?

408
00:21:48,531 --> 00:21:50,841
Log four J was the standard for Kafka.

409
00:21:51,341 --> 00:21:54,931
If I remember correctly with 4.0 and
above I think they moved to lock four

410
00:21:54,931 --> 00:22:00,481
J two, but even the lock four J was
patched version for Kafka specifically,

411
00:22:00,481 --> 00:22:04,531
it was not the standard lock four J
version, which had the big CV a while ago.

412
00:22:05,191 --> 00:22:08,431
Use the JSON if you can structure
it format like js ON because

413
00:22:08,461 --> 00:22:12,421
they are really helpful for any
machine to parse those messages.

414
00:22:12,481 --> 00:22:14,611
Otherwise, you'll have to
create all those parsers.

415
00:22:15,091 --> 00:22:16,771
Parcel formatting is not that easy.

416
00:22:17,386 --> 00:22:20,806
So I, believe me, I've tried
to do that in metricbeat and

417
00:22:20,806 --> 00:22:22,186
things like that, or Logbeat.

418
00:22:22,756 --> 00:22:26,986
It takes a while to go through
and ensure every use case is

419
00:22:26,986 --> 00:22:28,576
actually handled and enough.

420
00:22:29,076 --> 00:22:31,716
So yeah, set a log, four
J properties well enough.

421
00:22:32,046 --> 00:22:36,346
If you have JSON available use JSO,
it'll be much more easier to par for

422
00:22:36,346 --> 00:22:38,236
the machines and get that stuff done.

423
00:22:38,416 --> 00:22:41,176
Yes, it'll swell up your log volume.

424
00:22:41,176 --> 00:22:41,596
Yes.

425
00:22:41,836 --> 00:22:44,896
But it helps your machines and
to understand all that stuff.

426
00:22:45,706 --> 00:22:48,676
Deploy something like
filebeat, definitely right.

427
00:22:48,816 --> 00:22:52,446
You will want the collectors to
stream it to a central location so

428
00:22:52,446 --> 00:22:55,086
that they don't have to go to each
broker every time there is an issue.

429
00:22:55,716 --> 00:22:57,216
That will not be sustainable.

430
00:22:57,316 --> 00:23:01,726
Once you go beyond a single cluster, as
soon as even if you have two clusters,

431
00:23:01,786 --> 00:23:03,556
it's gonna be untenable, right?

432
00:23:03,646 --> 00:23:07,226
You want them to be shipped to
a central location filterable

433
00:23:07,256 --> 00:23:09,926
by the brokers, the clusters.

434
00:23:10,436 --> 00:23:12,371
All that stuff has to filterable, right?

435
00:23:12,371 --> 00:23:13,286
So it should be available.

436
00:23:14,026 --> 00:23:15,436
Retention strategy is important.

437
00:23:15,676 --> 00:23:17,926
Don't set a retention
strategy to seven days.

438
00:23:18,616 --> 00:23:19,396
It's not enough.

439
00:23:19,606 --> 00:23:24,786
I can tell you very confidently because
I've done that did not bode well for me.

440
00:23:24,936 --> 00:23:27,126
So yeah, seven days not a good idea.

441
00:23:27,726 --> 00:23:31,716
And that was like few years ago
for me, so I was still naive.

442
00:23:31,716 --> 00:23:33,366
Hey, who needs more than seven days?

443
00:23:33,366 --> 00:23:33,966
Both the logs.

444
00:23:34,326 --> 00:23:35,286
That's not the way how it goes.

445
00:23:35,736 --> 00:23:38,256
You'll need logs to be
more than seven days.

446
00:23:38,256 --> 00:23:38,826
Definitely.

447
00:23:38,946 --> 00:23:42,126
A good way to understand how
long will you need the logs

448
00:23:42,126 --> 00:23:43,536
would be something like, Hey.

449
00:23:44,036 --> 00:23:47,426
What time do I allow my application
teams to actually come back to me

450
00:23:47,426 --> 00:23:52,826
and ask for issues and referrals
and debugs into the issues, right?

451
00:23:52,826 --> 00:23:54,086
Deep dives into the issues.

452
00:23:54,596 --> 00:23:58,076
So if it's something like around 30
days, you'll probably want like 45

453
00:23:58,076 --> 00:24:01,466
days worth of logs to be retained
and then phased out after that.

454
00:24:02,066 --> 00:24:05,606
There are ways to do maybe
keep some of them in the heart.

455
00:24:06,461 --> 00:24:10,331
Storage, keep some in the warm
storage so that, and maybe export

456
00:24:10,331 --> 00:24:13,031
them out to the cold storage after
a while so that you don't have to

457
00:24:13,031 --> 00:24:15,341
delete all of them, but reduce costs.

458
00:24:15,611 --> 00:24:19,001
So that's also possible, but again,
establish a retention strategy.

459
00:24:19,001 --> 00:24:20,201
You'll want to think about it.

460
00:24:20,201 --> 00:24:24,011
There are great talks on the internet
around how do you want to talk about

461
00:24:24,011 --> 00:24:27,531
the retention strategy for logs
and cost implications around that.

462
00:24:28,041 --> 00:24:28,941
So check those out.

463
00:24:29,241 --> 00:24:33,351
Simple implementation would be Kafka,
log files, structure logs, json if you

464
00:24:33,351 --> 00:24:38,021
can something like an Open Telemetry
collector or a filebeat or a, yeah,

465
00:24:38,111 --> 00:24:39,731
Filebeat is probably the right one.

466
00:24:40,351 --> 00:24:40,471
Grafana.

467
00:24:40,771 --> 00:24:42,061
Loki, I love Grafana.

468
00:24:42,061 --> 00:24:43,681
As I said, as a company.

469
00:24:43,681 --> 00:24:45,751
They introduced another
product called Grafana.

470
00:24:45,751 --> 00:24:46,171
Loki.

471
00:24:46,411 --> 00:24:49,891
This is more like a Grafana, but not
for metrics, but for logs, right?

472
00:24:50,521 --> 00:24:51,241
Excellent product.

473
00:24:51,661 --> 00:24:52,891
Check that out if you want.

474
00:24:53,311 --> 00:24:57,931
And then Grafana ui of course, to
search and filter through those logs.

475
00:24:58,231 --> 00:25:00,841
This is not to say that you
have to use this exact stack.

476
00:25:01,201 --> 00:25:04,311
You can mix and match how you,
you can replace these components

477
00:25:04,311 --> 00:25:06,861
with something else that you
already have or your enterprise.

478
00:25:07,431 --> 00:25:08,901
So please feel free.

479
00:25:08,901 --> 00:25:11,271
Do not think that these are.

480
00:25:11,871 --> 00:25:14,961
The set guidelines or parameters
that you're gonna follow, right?

481
00:25:14,991 --> 00:25:17,227
You can change and mix and
match how you, however you like.

482
00:25:17,477 --> 00:25:21,947
Traces, like I said, is the
last part of this thing, right?

483
00:25:21,977 --> 00:25:22,787
The whole workflow.

484
00:25:23,147 --> 00:25:26,687
But this is more at the application
level, tracking, not at the Kafka level

485
00:25:26,687 --> 00:25:30,167
tracking, but it gives you end-to-end
visibility of what's happening, where

486
00:25:30,167 --> 00:25:34,557
each message is going, how much is the
latency if there is bottleneck, which

487
00:25:34,617 --> 00:25:38,847
process, which service is causing all
that bottleneck if there is a failure.

488
00:25:39,237 --> 00:25:40,107
Where is it?

489
00:25:40,947 --> 00:25:43,317
Where is that failure system, right?

490
00:25:43,317 --> 00:25:47,937
So that you can actually understand what
is going on in your e ecosystem, Rob?

491
00:25:49,097 --> 00:25:49,967
The message journey, right?

492
00:25:49,997 --> 00:25:52,007
Like I said, producer produces a message.

493
00:25:52,037 --> 00:25:53,087
It should have a watermark.

494
00:25:53,517 --> 00:25:54,747
It goes to the Kafka broker.

495
00:25:54,807 --> 00:25:56,937
Broker persists it, it keeps it forever.

496
00:25:56,967 --> 00:25:58,947
There could be one consumer
receiving that message.

497
00:25:58,947 --> 00:26:01,047
There could be 10 consumers
receiving that message.

498
00:26:01,332 --> 00:26:05,512
So that can it could be a one to
one to many flow, or one-to-one

499
00:26:05,512 --> 00:26:06,832
flow, or one to three flow.

500
00:26:06,832 --> 00:26:09,562
It doesn't matter the fan, or it could
be as large as you want with Kafka.

501
00:26:09,922 --> 00:26:10,792
So that's the beauty.

502
00:26:11,332 --> 00:26:14,572
But with watermarking, you'll be able to
actually trace all those paths as well.

503
00:26:14,812 --> 00:26:18,742
So really important there if you are
into those kinds of things and your

504
00:26:19,192 --> 00:26:21,082
company and the organization wants that.

505
00:26:21,342 --> 00:26:24,882
From an instrumentation perspective,
like what we talked about, you have

506
00:26:24,882 --> 00:26:26,472
to have that context propagation.

507
00:26:26,742 --> 00:26:30,792
So they will attach identifiers,
watermarks to the message headers

508
00:26:31,182 --> 00:26:34,632
and consumers will continue
to continue the same trace.

509
00:26:34,932 --> 00:26:38,862
They'll use the same identifier
and add whatever need they

510
00:26:38,862 --> 00:26:40,572
need to add for the span.

511
00:26:40,872 --> 00:26:43,632
There is an auto instrumentation
available with open telemetry.

512
00:26:43,752 --> 00:26:46,962
I don't wanna go into detail around
that, but there is a really good blog

513
00:26:46,962 --> 00:26:48,882
about that in the Open Telemetry website.

514
00:26:49,182 --> 00:26:49,992
So check that out.

515
00:26:50,492 --> 00:26:54,112
So instrument clients with open telemetry
collector will actually receive all

516
00:26:54,112 --> 00:26:56,572
that spans via are OTLP protocol.

517
00:26:57,092 --> 00:27:01,692
Things like jeager, zipkin or
similar systems can actually store

518
00:27:01,692 --> 00:27:03,282
and process those trace data sets.

519
00:27:03,852 --> 00:27:09,402
And Grafana in the etiquette can, again,
visualize those traces with span details.

520
00:27:10,032 --> 00:27:14,382
So again, on you, however you
want to implement it, these are

521
00:27:14,562 --> 00:27:15,852
options which are available.

522
00:27:15,852 --> 00:27:18,552
You can go and find better
options if you have.

523
00:27:18,982 --> 00:27:22,062
Please don't think that Grafana
is the only option in the world.

524
00:27:22,062 --> 00:27:25,482
I, as I said, these are my
preferences, which I use, but

525
00:27:25,482 --> 00:27:26,562
you can use whatever you like.

526
00:27:26,862 --> 00:27:29,802
Another example, so this is
a good example for a trace.

527
00:27:29,802 --> 00:27:33,162
I just wanted to include it, that,
hey, when you're tracing what happens

528
00:27:33,162 --> 00:27:34,752
right when you produce a message?

529
00:27:34,932 --> 00:27:37,902
Five milliseconds spent there
in that trace, that span

530
00:27:37,902 --> 00:27:39,192
will say five milliseconds.

531
00:27:39,792 --> 00:27:43,092
It stayed within the Kafka
queue for about 50 milliseconds.

532
00:27:43,092 --> 00:27:44,442
We message waiting in the broker.

533
00:27:44,772 --> 00:27:47,412
Then a consumer came in and
it consumed the message.

534
00:27:47,412 --> 00:27:49,572
It pulled it and it started processing it.

535
00:27:50,052 --> 00:27:53,922
Maybe 10 seconds to consume,
maybe 200 seconds to process, 200

536
00:27:53,922 --> 00:27:55,152
milliseconds to process, sorry.

537
00:27:55,212 --> 00:27:56,622
So all that stuff adds up.

538
00:27:56,922 --> 00:28:00,432
Those spans will tell you where
the hot area is, and if you have

539
00:28:00,432 --> 00:28:03,672
some processing performance issues,
that's where you look at it.

540
00:28:04,172 --> 00:28:07,437
So melt, going back to the whole thing.

541
00:28:07,887 --> 00:28:09,417
Metrics shows you symptoms.

542
00:28:09,807 --> 00:28:14,277
Latencies, dropping latencies,
replication, statuses, all that stuff.

543
00:28:14,517 --> 00:28:16,047
Metrics, events.

544
00:28:16,767 --> 00:28:18,027
Has my broker restarted?

545
00:28:18,387 --> 00:28:19,617
Is the leadership changed?

546
00:28:19,797 --> 00:28:21,867
Is there a partition reassignment ongoing?

547
00:28:22,017 --> 00:28:23,307
Is there a replication ongoing?

548
00:28:23,397 --> 00:28:28,617
All that stuff are events, logs, error
messages, what's going on in my system?

549
00:28:28,707 --> 00:28:29,937
Are there client disconnections?

550
00:28:29,967 --> 00:28:32,557
Are there issues related to replication?

551
00:28:32,607 --> 00:28:36,357
Is there an issue related
to the disc logs, traces?

552
00:28:36,972 --> 00:28:37,842
Where's my message?

553
00:28:37,902 --> 00:28:39,342
What's my message doing?

554
00:28:39,792 --> 00:28:43,392
Where is it all being consumed
from and produced from?

555
00:28:43,782 --> 00:28:46,692
I wanna know all that stuff
that's traces for you.

556
00:28:46,992 --> 00:28:50,652
A practical implementation, again,
my example, my preferred way,

557
00:28:50,652 --> 00:28:53,382
because I've been dealing with these
systems for a really long time.

558
00:28:53,862 --> 00:28:56,982
You can choose your, pick
your poison, if I may.

559
00:28:57,052 --> 00:28:58,667
Open telemetry when neutral.

560
00:28:58,927 --> 00:29:01,447
So it's basically just default.

561
00:29:02,202 --> 00:29:05,622
For me at this point, 'cause it's
vendor agnostic, it doesn't care.

562
00:29:06,022 --> 00:29:07,462
Prometheus has support for that.

563
00:29:08,032 --> 00:29:13,562
Prometheus also has A GMX exporter,
which the community built pretty good.

564
00:29:13,712 --> 00:29:19,232
It's it has really good query language,
it has really good alerting as well.

565
00:29:19,622 --> 00:29:24,162
And it's a really nice TSTB, which
scales really well and performs

566
00:29:24,162 --> 00:29:25,452
really well under pressure as well.

567
00:29:26,097 --> 00:29:27,417
Grafana Unified Dashboarding.

568
00:29:27,747 --> 00:29:31,677
Any signals, alerts can also go
into Grafana, so pick your poison.

569
00:29:31,917 --> 00:29:32,877
Promeus can do it.

570
00:29:33,207 --> 00:29:33,897
Grafana can do it.

571
00:29:33,957 --> 00:29:35,667
Choose whichever you one you like.

572
00:29:35,967 --> 00:29:41,187
Using Melt for Kafka, it'll get you
better detection of errors, quicker

573
00:29:41,187 --> 00:29:43,287
analysis for root cal root causes.

574
00:29:43,677 --> 00:29:46,827
Probably up to around 40% fewer
incidents depending on how you

575
00:29:46,827 --> 00:29:50,007
instrument it, how you alert on it,
and all those things considered.

576
00:29:50,697 --> 00:29:54,747
And definitely way more confidence
when you are managing, not one, not

577
00:29:54,747 --> 00:29:56,787
two, but many Kafka clusters at scale.

578
00:29:57,087 --> 00:29:57,387
Yeah.

579
00:29:57,597 --> 00:30:02,127
So key takeaways for me is always
been the synergy of signals.

580
00:30:02,307 --> 00:30:03,597
Use the combination, right?

581
00:30:03,987 --> 00:30:05,697
The melt stack is important.

582
00:30:05,997 --> 00:30:09,087
Use the ones which are most important
for you, and then instrument

583
00:30:09,087 --> 00:30:11,157
the other ones as needed, right?

584
00:30:11,847 --> 00:30:13,227
I love open source tooling.

585
00:30:13,407 --> 00:30:15,627
You can pick your
choices however you like.

586
00:30:16,287 --> 00:30:19,827
It gives you much better visibility if
you instrument the entire stack, but

587
00:30:19,887 --> 00:30:25,047
you can choose or start with metrics
and logs and that will give you more

588
00:30:25,047 --> 00:30:29,367
than enough to start off and manage
your Kafka clusters with confidence.

589
00:30:29,627 --> 00:30:32,507
These are some of the helpful resources,
which are just, this is more like an

590
00:30:32,567 --> 00:30:34,407
appendix but I just wanted to call out.

591
00:30:34,527 --> 00:30:38,007
There is a JMX monitoring stat that me
and one of some of my friends built.

592
00:30:38,517 --> 00:30:43,947
It's been, it has really good Kafka
dashboards available and really

593
00:30:43,947 --> 00:30:48,597
good Kafka meets configurations,
which are tuned for performance.

594
00:30:48,972 --> 00:30:52,737
Tuned for dashboarding and most
critical signals and things like that.

595
00:30:53,007 --> 00:30:54,477
Take a look if that helps.

596
00:30:54,477 --> 00:30:54,867
Good.

597
00:30:55,017 --> 00:30:58,257
But there are other numerous dashboards,
numerous Prometheus configurations

598
00:30:58,257 --> 00:30:59,537
available in the interwebs.

599
00:30:59,867 --> 00:31:03,317
So you can use your, choose your
pick, whichever one you like.

600
00:31:03,577 --> 00:31:06,307
Some of the metrics that you wanna
consider from a broker perspective,

601
00:31:06,307 --> 00:31:10,117
these are probably the ones which I
consider as the most important metrics

602
00:31:10,147 --> 00:31:12,517
of all types for our brokers, right?

603
00:31:12,577 --> 00:31:15,337
There could be others, there
are numerous other metrics, yes.

604
00:31:15,637 --> 00:31:19,197
But these are the things which I
see is as the most important ones.

605
00:31:19,857 --> 00:31:22,575
Take a look, go through it, pause
the video here and go through that.

606
00:31:22,875 --> 00:31:25,815
These are the common client metrics,
which I say again, the most important

607
00:31:25,815 --> 00:31:27,195
ones, which you should have.

608
00:31:27,435 --> 00:31:31,035
It is a method out of the producer,
the consumer, the connect, all three

609
00:31:31,035 --> 00:31:32,845
of them actually EMS out those metrics.

610
00:31:32,895 --> 00:31:36,415
Probably get those as a baseline
if you're building your own right.

611
00:31:36,445 --> 00:31:40,795
So all these metrics are important if you
are building your own right, if you're

612
00:31:40,795 --> 00:31:42,745
not using the JMX monitoring stack.

613
00:31:43,385 --> 00:31:46,865
JMX monitoring stacks already has most
of these already in building, right?

614
00:31:47,045 --> 00:31:50,165
So if you go through that good,
you're already set, right?

615
00:31:50,195 --> 00:31:54,275
But if you want to use your own, you
want to use your own or roll your own

616
00:31:54,695 --> 00:31:57,195
and will learn, this is how you start.

617
00:31:57,885 --> 00:32:00,285
So coming back to the common client
metrics, these are the ones which

618
00:32:00,285 --> 00:32:02,095
you'll want producer metrics.

619
00:32:02,335 --> 00:32:04,705
These are the ones which I
consider as the most important.

620
00:32:04,705 --> 00:32:08,225
There are more more metrics than these.

621
00:32:08,585 --> 00:32:11,795
So yeah choose accordingly and enough.

622
00:32:12,525 --> 00:32:13,275
Consumer metrics.

623
00:32:13,335 --> 00:32:15,225
This is a little small, but yeah.

624
00:32:15,255 --> 00:32:18,165
These are the ones which I'll
want you to start off with.

625
00:32:18,885 --> 00:32:24,195
Don't hesitate to co collect more from the
clients because they don't have so many.

626
00:32:24,735 --> 00:32:26,325
For the Kafka brokers, there's too many.

627
00:32:26,505 --> 00:32:27,885
So just be careful about that.

628
00:32:28,385 --> 00:32:31,415
And that's the, that's me guys.

629
00:32:31,835 --> 00:32:34,355
Thanks a lot for listening,
and if you have any questions,

630
00:32:34,355 --> 00:32:36,065
feel free to reach out to me.

631
00:32:36,155 --> 00:32:36,605
Thank you.

