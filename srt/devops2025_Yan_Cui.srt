1
00:00:00,280 --> 00:00:00,730
Hi, everyone.

2
00:00:00,740 --> 00:00:03,490
Thank you for taking the time out
of your busy day to join us on this

3
00:00:03,500 --> 00:00:08,590
session about how to save money on
your 80 percent serverless environment.

4
00:00:08,990 --> 00:00:13,140
so without further ado, let me
just switch over to my slide and

5
00:00:13,230 --> 00:00:15,140
we can get started right away.

6
00:00:15,610 --> 00:00:17,549
So a quick word about myself.

7
00:00:17,820 --> 00:00:19,930
I'm AWS Serverless Hero.

8
00:00:19,969 --> 00:00:25,029
My name is Yen Chui, and I've been
doing stuff on AWS since 2010.

9
00:00:25,030 --> 00:00:28,040
Nowadays, I spend half my
time working with Lumigo.

10
00:00:28,325 --> 00:00:32,134
As a developer advocate and the other
half my time I work as an independent

11
00:00:32,134 --> 00:00:37,635
consultant when I help other people, other
companies adopt serverless technologies.

12
00:00:37,945 --> 00:00:42,274
And one thing I like to do is
collecting tips around how to save

13
00:00:42,274 --> 00:00:44,305
money on your AWS environment.

14
00:00:44,595 --> 00:00:46,894
And so I've got lots of ideas
to share with you today.

15
00:00:46,915 --> 00:00:49,915
So I hope you are ready to
drink from the fire hose.

16
00:00:50,175 --> 00:00:53,375
As, as we go through a number of
different ways, you can save money

17
00:00:53,395 --> 00:00:55,855
on your AWS serverless environment.

18
00:00:56,355 --> 00:00:58,664
So we're going to, so we're going
to start with something that's very

19
00:00:58,664 --> 00:01:02,374
simple as I think that everybody should
be doing is even if you are new to

20
00:01:02,374 --> 00:01:05,944
AWS, this is probably the first thing
you should do when you create an AWS

21
00:01:05,944 --> 00:01:08,155
account is to set up billing alarms.

22
00:01:08,545 --> 00:01:09,405
they're not perfect.

23
00:01:09,455 --> 00:01:11,135
They're usually a few hours behind.

24
00:01:11,394 --> 00:01:15,464
but it's much better to find out that
you've got a problem a few hours late.

25
00:01:15,604 --> 00:01:20,024
Then say a few weeks late when your bill
finally arrived and you've got a much

26
00:01:20,064 --> 00:01:24,244
bigger amount that you have to pay than
if you were to, you know, found out that

27
00:01:24,264 --> 00:01:26,284
you've got an issue a few hours into it.

28
00:01:26,864 --> 00:01:30,995
So Luke, and his team at the
PostNL, are pretty experienced with

29
00:01:30,995 --> 00:01:35,034
AWS and serverless, but they were
still caught by a mistake, which,

30
00:01:35,074 --> 00:01:37,714
triggered their AWS cost to spiral.

31
00:01:37,984 --> 00:01:41,204
And in this particular case, they
It was, because of one line of

32
00:01:41,204 --> 00:01:45,644
change in their code, which caused
the lambda functions to make a lot

33
00:01:45,644 --> 00:01:47,824
of API calls to secrets manager.

34
00:01:48,054 --> 00:01:51,894
And the reason for it was because that
one line of change broke their caching.

35
00:01:52,144 --> 00:01:56,644
So instead of making a call to
secrets manager, At cold start and

36
00:01:56,644 --> 00:02:00,314
then caching the secret, they will
make a call to secret manager on

37
00:02:00,334 --> 00:02:04,484
every single invocation, which at
their scale, they are the national,

38
00:02:04,694 --> 00:02:06,575
delivery service for the Netherlands.

39
00:02:06,745 --> 00:02:10,585
You can imagine that's hundreds of
millions of requests per day, which

40
00:02:10,615 --> 00:02:15,775
can add up pretty quickly, which is why
within a few days, they got, an alert

41
00:02:16,015 --> 00:02:20,564
that triggered, the, the billing alarm
in the AWS, which I think was something

42
00:02:20,584 --> 00:02:22,644
like 2, 000 or something like that.

43
00:02:22,645 --> 00:02:22,854
Yeah.

44
00:02:22,994 --> 00:02:26,174
Which is their monthly budget
for, for their whole AWS

45
00:02:26,174 --> 00:02:27,544
environment for their team.

46
00:02:28,004 --> 00:02:31,944
So luckily they were able to find out
that this problem was happening within

47
00:02:31,944 --> 00:02:36,594
a few days, as opposed to say, within
the, after a few weeks, when the, when

48
00:02:36,594 --> 00:02:38,214
the damage could have been much bigger.

49
00:02:38,564 --> 00:02:40,034
So the learning from here is that.

50
00:02:40,249 --> 00:02:45,359
Yes, billing is not a perfect, there are a
few hours behind, but it's much better to

51
00:02:45,359 --> 00:02:50,409
find out that you've got a problem early
on into the issue than to say much later.

52
00:02:50,619 --> 00:02:52,249
And the billing alarms does work.

53
00:02:52,439 --> 00:02:56,369
So in this case, yeah, it was, no,
they did have, did suffer some damage,

54
00:02:56,529 --> 00:03:00,059
for a few thousand dollars, but it
could have been a lot worse if they

55
00:03:00,059 --> 00:03:03,989
didn't find out and only found out
when say the finance team comes

56
00:03:04,209 --> 00:03:05,644
knocking on their door to say, Okay.

57
00:03:05,694 --> 00:03:06,804
What else is going on guys?

58
00:03:06,834 --> 00:03:11,174
So your bills are much bigger now
compared to what it was last month.

59
00:03:12,014 --> 00:03:13,644
So billing systems work.

60
00:03:13,694 --> 00:03:15,644
it's just that, is there not perfect?

61
00:03:16,144 --> 00:03:21,164
So when it comes to billing and the cost
of AWS, one of the biggest offenders,

62
00:03:21,194 --> 00:03:25,189
something, something, something that I
see probably always come up as number

63
00:03:25,189 --> 00:03:27,384
one or two on the, my customers.

64
00:03:27,454 --> 00:03:29,494
It is bill is a cloud watch.

65
00:03:29,984 --> 00:03:34,874
Specifically around the cloud watch
logs, and it's very often as I work with

66
00:03:34,874 --> 00:03:39,014
my consulting clients that I see cloud
watch costing maybe, you know, a few

67
00:03:39,014 --> 00:03:42,924
times, maybe even 10 times more than
the actual application itself when you

68
00:03:42,964 --> 00:03:47,824
consider the cost of, say, API gateway
and lambda functions and down DB tables,

69
00:03:48,104 --> 00:03:53,634
cloud watch is often much, much higher in
terms of that cost, and also keep in mind

70
00:03:53,634 --> 00:03:58,394
that as your cost goes up because you're
collecting more and more logs, more and

71
00:03:58,414 --> 00:04:03,114
more data in CloudWatch, the value you get
from those logs actually goes down because

72
00:04:03,144 --> 00:04:07,144
now you're getting more and more noise
you have to deal with, and it's harder for

73
00:04:07,144 --> 00:04:11,574
you to then find the piece of information
you actually need to debug some kind

74
00:04:11,574 --> 00:04:13,594
of problems you have in production.

75
00:04:14,134 --> 00:04:18,944
So with CloudWatch, it's just also
not very good at surfacing the really

76
00:04:18,944 --> 00:04:23,824
valuable and actionable information from
all this data that you're collecting.

77
00:04:24,224 --> 00:04:27,584
So the number one thing I like
to do to keep my CloudWatch logs

78
00:04:27,584 --> 00:04:31,484
cost under control is to make
sure that I do structured logging.

79
00:04:32,274 --> 00:04:36,014
And I want to pay really close
attention every time I write a log

80
00:04:36,014 --> 00:04:40,444
message as to which log level this
log message should be recorded as.

81
00:04:40,774 --> 00:04:43,844
Because when it comes to
production, you don't need all

82
00:04:43,864 --> 00:04:45,774
the different debug, debug logs.

83
00:04:45,864 --> 00:04:49,824
When there's so much requests happening at
the same time, instead in production, you

84
00:04:49,824 --> 00:04:54,444
probably just need to record everything
that's info level or above so that

85
00:04:54,444 --> 00:04:58,324
you don't have all of these debug logs
that doesn't give you a lot of value.

86
00:04:58,374 --> 00:05:01,714
But at the same time, there is,
you know, you're going to be paying

87
00:05:01,714 --> 00:05:05,384
for every single log message that
is collected by cloud watch logs.

88
00:05:06,294 --> 00:05:10,454
However, sometimes, especially
when things go wrong.

89
00:05:10,739 --> 00:05:14,519
These are debug logs can be really,
really useful in terms of helping

90
00:05:14,519 --> 00:05:16,629
you figure out what the problem was.

91
00:05:17,129 --> 00:05:21,579
So even though in production you want
to disable debug logging, you want to

92
00:05:21,609 --> 00:05:25,999
be able to sample some percentage of
your debug logs in production such that

93
00:05:26,239 --> 00:05:31,489
hopefully say, Now, if you collect 10
percent or 5 percent of the debug logs

94
00:05:31,499 --> 00:05:36,109
for all of the invocations for lambda
function, you have enough debug logs

95
00:05:36,109 --> 00:05:38,179
that covers every single code path.

96
00:05:38,399 --> 00:05:42,449
So that when there's a problem happening
in production, you have got some debug

97
00:05:42,469 --> 00:05:44,029
logs that can tell you what to do.

98
00:05:44,169 --> 00:05:47,839
What the problem was to help you figure
out what the issue was, so that you don't

99
00:05:47,839 --> 00:05:53,229
have to say, go back to your code, re
enable debug logging, and then redeploy

100
00:05:53,229 --> 00:05:58,079
to production, wait some time for the
debug logs to be collected, and then

101
00:05:58,079 --> 00:06:01,599
figure out what the problem was, and then
disable debug logging and all of that.

102
00:06:01,879 --> 00:06:06,669
Instead, you want to be always sampling
some percentage of debug logs, such that

103
00:06:06,689 --> 00:06:10,924
you've got enough information to help you
figure out what Problems in production,

104
00:06:11,174 --> 00:06:16,254
but not so much that you end up paying a
disproportional amount of money for debug

105
00:06:16,294 --> 00:06:18,034
logs that doesn't add a lot of value.

106
00:06:18,534 --> 00:06:21,814
Another thing to keep in mind when it
comes to working with CloudWatch logs is

107
00:06:21,814 --> 00:06:27,734
that by default, log retention is set to
never expire, because CloudWatch doesn't

108
00:06:27,734 --> 00:06:32,344
want to delete your data without you
telling them that you're okay with that.

109
00:06:33,164 --> 00:06:38,664
This is fine from their perspective, but
from my perspective, the value of the

110
00:06:38,664 --> 00:06:43,354
logs reduced goes down as the time goes
by, and there's no reason for me to keep

111
00:06:43,364 --> 00:06:44,804
logs that's older than, say, three years.

112
00:06:45,084 --> 00:06:45,744
30 days.

113
00:06:46,044 --> 00:06:50,504
especially as the application continued
to evolve and change, those logs becomes

114
00:06:50,654 --> 00:06:53,004
more and more outdated as time goes by.

115
00:06:53,494 --> 00:06:56,554
So you want to also change your
log retention to something that's

116
00:06:56,554 --> 00:07:00,914
more reasonable, such as seven
days or 14 days or 30 days.

117
00:07:01,874 --> 00:07:07,054
And also you don't want to pay the storage
cost of 3 cents per gigabyte on all the

118
00:07:07,054 --> 00:07:11,564
logs that you've ever produced, forever,
especially when those logs are gonna.

119
00:07:11,954 --> 00:07:15,864
You essentially become useless after a
few weeks, and also another thing to keep

120
00:07:15,864 --> 00:07:20,974
in mind is that many of you are not using
cloud watch logs to query your data.

121
00:07:20,974 --> 00:07:24,424
You are shipping your logs from
cloud watch to some other third party

122
00:07:24,424 --> 00:07:26,534
service, and then you can query them.

123
00:07:26,754 --> 00:07:30,349
So in this case, As the lambda
function produces those logs, you

124
00:07:30,349 --> 00:07:34,999
afforded them to say logs IO or
some other platform, and then you'd

125
00:07:34,999 --> 00:07:37,069
analyze them in those platforms.

126
00:07:37,399 --> 00:07:40,749
In this case, you still end up
having to ingest logs in CloudWatch

127
00:07:40,759 --> 00:07:44,589
first, and therefore you still have
to pay that 50 cents per gigabyte

128
00:07:44,629 --> 00:07:46,819
of ingestion cost for CloudWatch.

129
00:07:47,249 --> 00:07:50,909
And so in this case, you probably
end up paying twice for the ingestion

130
00:07:50,929 --> 00:07:54,859
and processing of those logs, which
of course is going to be a waste.

131
00:07:55,129 --> 00:08:00,989
So nowadays, when it comes to Lambda, you
can use the Lambda extensions to ship all

132
00:08:00,999 --> 00:08:03,399
of your logs to some third party provider.

133
00:08:03,684 --> 00:08:08,494
via the telemetry API with, which
basically allows Lambda extensions,

134
00:08:08,514 --> 00:08:14,144
which are like sidecars to your main
Lambda runtime to access the logs from

135
00:08:14,154 --> 00:08:19,044
your function and is able to then send
those logs to some third party vendor.

136
00:08:19,324 --> 00:08:23,254
And you can do this without having to
go through CloudWatch logs first, which

137
00:08:23,264 --> 00:08:28,169
means once you've done that, you can then
also add This bit of IAM permission to

138
00:08:28,169 --> 00:08:32,599
your Lambda functions IAM role to then
disable your function from being able

139
00:08:32,609 --> 00:08:37,619
to send those logs to CloudWatch so that
you don't end up paying for the ingestion

140
00:08:37,629 --> 00:08:40,629
for the same logs at CloudWatch as well.

141
00:08:41,569 --> 00:08:44,829
So this is a much better way to
send your logs information to

142
00:08:44,969 --> 00:08:48,609
another vendor instead of having to
go through CloudWatch logs first.

143
00:08:48,854 --> 00:08:50,274
And then process them from there.

144
00:08:51,094 --> 00:08:54,694
And if you're looking to move away from
cloud watch logs and you're looking

145
00:08:54,694 --> 00:08:58,744
for another vendor that's more cost
efficient and allow you to do more with

146
00:08:58,744 --> 00:09:02,954
your log information, then the checkout
Lumigo, the log management system is

147
00:09:02,954 --> 00:09:04,734
a lot cheaper compared to cloud watch.

148
00:09:04,944 --> 00:09:09,244
And also they treat every single log
message as a event so that you can

149
00:09:09,244 --> 00:09:14,754
then on demand to create a arbitrary
metrics and then alerts on top of that.

150
00:09:14,984 --> 00:09:16,594
And it's all included in the price.

151
00:09:16,869 --> 00:09:21,459
For the ingestion of those logs so you
don't pay separate for logs and alerts

152
00:09:21,509 --> 00:09:26,299
and dashboards and so on So staying with
car wash logs Another thing I want to

153
00:09:26,439 --> 00:09:30,979
mention is that remember those system
messages that you get So after every

154
00:09:30,979 --> 00:09:34,819
single invocation, you get a number of
Logs in your lambda functions, cloud

155
00:09:34,819 --> 00:09:38,989
watch logs, which for most of you, it's
probably not going to matter very much.

156
00:09:39,149 --> 00:09:42,379
but if you're running a scale, for
example, you're doing billions of

157
00:09:42,379 --> 00:09:46,039
invocations per month, like the
guys at the far from analytics.

158
00:09:46,269 --> 00:09:49,079
then those are system messages
can actually end up being.

159
00:09:49,354 --> 00:09:52,915
you know, costing a non trivial
amount of, of, of dollars per month.

160
00:09:53,225 --> 00:09:56,055
And of course, they don't
really give you a lot of value

161
00:09:56,225 --> 00:09:58,065
in return for that investment.

162
00:09:58,315 --> 00:10:02,915
So nowadays there's a way for you to
control what information is included and

163
00:10:02,924 --> 00:10:04,715
how much of those system log messages.

164
00:10:05,165 --> 00:10:06,775
It gets produced by Lambda.

165
00:10:07,395 --> 00:10:11,485
So with Lambda, now you have this,
login config setting on your Lambda

166
00:10:11,485 --> 00:10:16,834
function, which you can configure through
the CLI, through CloudFormation, and,

167
00:10:16,835 --> 00:10:21,134
through CDK and other tools that uses
CloudFormation as well, whereby you can

168
00:10:21,134 --> 00:10:26,565
set the log format for your function,
so that, by default, this is still gonna

169
00:10:26,565 --> 00:10:30,205
just, output plain text, so whatever
your function is writing to standard

170
00:10:30,215 --> 00:10:34,790
out, I'm It's going to keep it as a
plain text, but you can also switch to

171
00:10:34,790 --> 00:10:39,650
JSON so that the Lambda runtime is going
to capture whatever information that

172
00:10:39,650 --> 00:10:45,320
you're sending to standard out and then
format it into a JSON blob so that you

173
00:10:45,320 --> 00:10:49,329
get, you kind of get a structured login
without you actually doing any structured

174
00:10:49,329 --> 00:10:50,854
logging in your application code.

175
00:10:51,354 --> 00:10:55,794
Now, where it gets interesting is that
once you set the log format to JSON,

176
00:10:56,134 --> 00:11:01,214
you can now also configure a system
log level, which controls which of

177
00:11:01,214 --> 00:11:05,734
the system log messages are actually
produced by the Lambda runtime.

178
00:11:05,734 --> 00:11:08,084
This is not in the official
documentations, but I did some

179
00:11:08,094 --> 00:11:12,514
experimentation to find out which
of the log messages are produced at

180
00:11:12,524 --> 00:11:14,494
which of the log system log levels.

181
00:11:14,844 --> 00:11:16,944
So if you just want to know.

182
00:11:17,229 --> 00:11:19,909
Whether or not there's a
like a unhandled exception.

183
00:11:19,939 --> 00:11:22,339
And so, you know, your application
code didn't in a capture.

184
00:11:22,399 --> 00:11:25,559
I didn't capture any of the
errors and you bubble up to the

185
00:11:25,559 --> 00:11:27,339
Lambda runtime and the brew up.

186
00:11:27,549 --> 00:11:30,409
And so you can just set
the log level to one.

187
00:11:30,679 --> 00:11:33,199
And that way you will still
get those unhandled exceptions

188
00:11:33,219 --> 00:11:34,789
in the in the, in the logs.

189
00:11:34,959 --> 00:11:38,229
but none of the other system messages
for when your function starts, when

190
00:11:38,309 --> 00:11:42,119
When it finishes, how much memory
gets used and so on and so forth.

191
00:11:42,819 --> 00:11:46,679
So CloudWatch is one of those services
that everyone should really know if

192
00:11:46,679 --> 00:11:50,099
they're going to be using AWS, especially
if you're going to be using Lambda.

193
00:11:50,399 --> 00:11:55,299
So I recommend this book by Sandra and
Tobias, who does a really good job of

194
00:11:55,299 --> 00:11:59,219
explaining how CloudWatch works, even
if you don't end up using CloudWatch

195
00:11:59,239 --> 00:12:03,169
for your day to day, you know, querying
your logs and the things like that.

196
00:12:03,319 --> 00:12:06,129
It's still worth understanding
how it works and all the different

197
00:12:06,149 --> 00:12:08,759
things that the cloud watch
gives you nowadays as well.

198
00:12:09,259 --> 00:12:12,439
Okay, so moving on, next thing
we're going to talk about is still

199
00:12:12,439 --> 00:12:16,069
around Lambda, but specifically
around the cost of Lambda functions.

200
00:12:16,359 --> 00:12:19,859
And, those of you who have used Lambda
before, you probably know this already

201
00:12:19,899 --> 00:12:24,479
that with Lambda, you have basically
one level to control the performance

202
00:12:24,509 --> 00:12:26,439
and the cost for your function.

203
00:12:27,054 --> 00:12:29,244
through how much memory that
you allocate to the function.

204
00:12:29,724 --> 00:12:34,614
More memory equals more CPU and
equals more network bandwidth as well.

205
00:12:34,944 --> 00:12:39,454
But also the more memory you allocate,
it also means that the more you're

206
00:12:39,454 --> 00:12:43,044
going to spend per millisecond of
execution time for your function.

207
00:12:43,394 --> 00:12:46,124
And that cost is proportional
to the amount of memory that

208
00:12:46,134 --> 00:12:47,794
you allocate to the function.

209
00:12:48,214 --> 00:12:52,634
Which on the one hand means that it's
really easy to just give your function

210
00:12:52,634 --> 00:12:57,154
more power so that you can process
whatever requests are faster, but at

211
00:12:57,154 --> 00:13:02,184
the same time, it's also very easy
to be wrong by an order of magnitude.

212
00:13:02,504 --> 00:13:06,654
So even though it probably does not going
to happen very often, but when it does,

213
00:13:06,864 --> 00:13:11,614
those over provision functions can hit you
pretty hard on the finance side of things.

214
00:13:12,479 --> 00:13:15,529
And so this actually happened to
a client of mine a while back.

215
00:13:15,749 --> 00:13:20,889
So they produce videos for those kind
of hand drawn, sort of tutorial videos

216
00:13:20,889 --> 00:13:22,479
that you sometimes see on YouTube.

217
00:13:22,789 --> 00:13:25,619
And so they use a lambda function
to do some of that rendering.

218
00:13:26,619 --> 00:13:30,999
Now the team understood that more power,
more memory equals more CPU power.

219
00:13:30,999 --> 00:13:34,759
And so they want to reduce the amount
of time it takes to do the rendering.

220
00:13:34,999 --> 00:13:38,669
And so they decided, Oh, let's just
allocate, with the maximum amount

221
00:13:38,669 --> 00:13:40,329
of memory to the lambda function.

222
00:13:40,534 --> 00:13:42,244
So it gave me the full 10 gig.

223
00:13:42,514 --> 00:13:45,384
And so at the end of the month,
they found out, Oh, wait a minute.

224
00:13:45,584 --> 00:13:48,124
So suddenly we've got this one rendering.

225
00:13:48,144 --> 00:13:52,324
So Lambda function now costing us
something like 10, 000 a month.

226
00:13:52,584 --> 00:13:54,424
So something is clearly wrong there.

227
00:13:54,724 --> 00:13:58,244
And so the reason is for that is
because, well, the allocated the full

228
00:13:58,244 --> 00:14:01,764
10 gig of memory to the function,
but it didn't reduce the amount of

229
00:14:01,954 --> 00:14:03,914
the rendering time proportionally.

230
00:14:04,244 --> 00:14:08,094
And the reason for that is
because while with Lambda, yes,

231
00:14:08,114 --> 00:14:09,834
more memory equals more CPU.

232
00:14:10,119 --> 00:14:12,209
But once you get to 1.

233
00:14:12,210 --> 00:14:15,629
8 gigabytes of memory, you
unlock a second CPU core.

234
00:14:15,999 --> 00:14:19,749
And so by the time you hit the 10 full
10 gig of memory allocation for the

235
00:14:19,749 --> 00:14:22,269
function, you actually have six CPU cores.

236
00:14:22,579 --> 00:14:26,359
So to take full advantage of all
the CPU you have, you have to

237
00:14:26,359 --> 00:14:27,699
write application in such a way.

238
00:14:27,944 --> 00:14:32,814
That allows you to process things in
parallel using multiple CPU cores.

239
00:14:33,024 --> 00:14:35,294
Unfortunately for them,
they were using Node.

240
00:14:35,294 --> 00:14:37,304
js for the rendering part of things.

241
00:14:37,524 --> 00:14:40,044
And so by default, Node.

242
00:14:40,044 --> 00:14:41,454
js runs on an event loop.

243
00:14:41,494 --> 00:14:46,324
And so you have to write your application
specifically using workers and, and

244
00:14:46,324 --> 00:14:48,344
child processes to all in order to.

245
00:14:48,754 --> 00:14:52,254
Paralyze the way you are doing and
take full advantage of the fact that

246
00:14:52,254 --> 00:14:56,634
you've got six CPU cores instead
of one very, very big CPU core.

247
00:14:56,874 --> 00:14:58,054
And they weren't doing that.

248
00:14:58,104 --> 00:15:02,044
So even though they were spending and
paying for 10 gigabytes of memory and

249
00:15:02,044 --> 00:15:06,154
all the CPU power that comes with that,
they were only able to use essentially 1.

250
00:15:06,184 --> 00:15:06,654
8 gig.

251
00:15:06,789 --> 00:15:11,249
Of, of the memory that they were paying
for, and that's why, you know, when

252
00:15:11,249 --> 00:15:14,279
it comes to helping these clients,
we just made a very simple decision

253
00:15:14,279 --> 00:15:16,369
to reduce memory allocation to 1.

254
00:15:16,369 --> 00:15:20,719
8 gig, so they get a full advantage
of what they are paying for, so much

255
00:15:20,729 --> 00:15:25,199
more efficient, you know, process for
rendering, but, not for, All the actual

256
00:15:25,239 --> 00:15:28,959
amount of CPU and power that they
were paying for, but not really using.

257
00:15:29,459 --> 00:15:32,899
And as the great Donald Knuth once
said, that we should forget about small

258
00:15:32,899 --> 00:15:38,379
efficiencies, say about 97 percent of the
time, that premature optimization is the

259
00:15:38,379 --> 00:15:42,799
root of all evil, but we should not pass
up our opportunities in that critical 3%.

260
00:15:43,599 --> 00:15:46,139
And that's where, for that one client, 99.

261
00:15:46,139 --> 00:15:50,479
9 percent of their lambda functions
was not doing anything significant

262
00:15:50,519 --> 00:15:54,549
in terms of cost, but that one
single function Was, was accounting

263
00:15:54,549 --> 00:15:58,779
for over 99 percent of the actual
cost for the serverless environment.

264
00:15:58,839 --> 00:16:03,869
So identifying those critical 3 percent
and really focus on how to improve the

265
00:16:03,869 --> 00:16:07,929
performance and efficiency for that
critical 3 percent is super important.

266
00:16:08,159 --> 00:16:12,319
So again, in Domingo, you can see all
the functions you have across all the

267
00:16:12,339 --> 00:16:16,139
different regions, and you can sort
them by cost so that you can really,

268
00:16:16,669 --> 00:16:20,919
really quickly identified outliers
in your environment, whether or not

269
00:16:20,919 --> 00:16:22,189
you've got lambda functions that are.

270
00:16:22,689 --> 00:16:27,289
Say, disproportionately represented in
terms of your cost allocation, and you

271
00:16:27,289 --> 00:16:30,099
can look at, you know, how much your
memory is allocated to the function,

272
00:16:30,339 --> 00:16:36,599
how much memory is being used on average
for processing when running this lambda

273
00:16:36,599 --> 00:16:40,779
function, and how often this function
runs as well, to give you an idea which

274
00:16:40,789 --> 00:16:45,389
function requires some more special
attention in terms of optimization to

275
00:16:45,579 --> 00:16:49,839
right size of memory allocation, And
the save the cost for those lambda

276
00:16:49,839 --> 00:16:54,959
functions in terms of actually, reducing
and the right sizing, the memory

277
00:16:54,959 --> 00:16:56,369
setting for your lambda function.

278
00:16:56,599 --> 00:17:00,999
I think the best tool you can use is
the lambda power tuning tool from Alex

279
00:17:01,029 --> 00:17:03,799
Casaboni, who used to work at AWS.

280
00:17:04,049 --> 00:17:08,754
And so you can Basically use the lambda
power tuning tool, which is a step

281
00:17:08,754 --> 00:17:12,784
function state machine that takes your
function and produces the different copies

282
00:17:12,784 --> 00:17:16,704
of it with different memory settings,
and then run a number of executions

283
00:17:16,704 --> 00:17:21,454
against those different variants and
find out based on the performance

284
00:17:21,454 --> 00:17:24,984
and, and then cost of those functions,
where is the sweet spot that gives you

285
00:17:24,984 --> 00:17:29,024
the most bang for your buck in terms
of the cost of your, of the function

286
00:17:29,194 --> 00:17:31,024
and how much performance you get.

287
00:17:31,524 --> 00:17:35,414
Another good way to reduce the cost
of lambda functions is to use the

288
00:17:35,444 --> 00:17:40,504
ARM architecture instead of x86
because on a per millisecond execution

289
00:17:40,524 --> 00:17:45,424
time, ARM architecture is actually
25 percent cheaper compared to x86.

290
00:17:46,024 --> 00:17:50,434
However, Performance gonna differ
depending on what it is you're doing.

291
00:17:50,734 --> 00:17:55,304
And, some people may report that, for
their workload, ARM is actually faster

292
00:17:55,524 --> 00:17:57,544
and also cheaper per millisecond.

293
00:17:57,824 --> 00:18:02,234
But for some of the things that I've
tested on, ARM can sometimes be 60

294
00:18:02,234 --> 00:18:08,594
percent more, I guess slower than the X
86 for the same workload in that case.

295
00:18:08,774 --> 00:18:13,064
Now, if you are saving 25% per
millisecond, but you're spending

296
00:18:13,064 --> 00:18:16,904
60% more milliseconds to process
the same thing, then you actually

297
00:18:16,904 --> 00:18:19,484
spend more money on arm than X 86.

298
00:18:19,934 --> 00:18:24,244
So in that case, that's not a good idea,
but we, I find the arm is really good,

299
00:18:24,244 --> 00:18:28,624
is, where you've got functions, they
have to talk to third party services and,

300
00:18:28,624 --> 00:18:30,179
maybe those third party services are.

301
00:18:30,679 --> 00:18:31,509
Quite slow.

302
00:18:31,509 --> 00:18:34,499
So you spend a lot of time waiting for IO.

303
00:18:34,739 --> 00:18:36,989
So in that case, well,
you're just going to wait.

304
00:18:37,029 --> 00:18:41,069
Then there might as well switch to ARM
where the cost per millisecond is cheaper.

305
00:18:41,069 --> 00:18:45,459
So the cost of those wait time
is going to be less as well.

306
00:18:45,959 --> 00:18:47,999
And still staying with
the lambda functions.

307
00:18:48,289 --> 00:18:52,264
Another, I guess, a big thing, and
this is probably the number one, I

308
00:18:52,659 --> 00:18:56,519
guess, anti pattern when it comes to
serverless environments around the

309
00:18:56,519 --> 00:19:01,879
Lambda functions is, you know, direct
synchronous Lambda to Lambda invocations.

310
00:19:02,339 --> 00:19:05,439
Now, one thing to keep in mind
about Lambda is that, Every single

311
00:19:05,459 --> 00:19:09,299
invocation of a lambda function
goes through its invoke API.

312
00:19:09,449 --> 00:19:12,489
And there are different
ways you can invoke this.

313
00:19:12,829 --> 00:19:16,659
Then that's why there's an invocation
type attribute here, where you can say,

314
00:19:16,939 --> 00:19:22,319
I want a request response invocation,
in which case the caller calls the

315
00:19:22,319 --> 00:19:26,039
lambda function, and they have to wait
for the whole invocation to finish.

316
00:19:26,309 --> 00:19:28,249
And get the output from the invocation.

317
00:19:28,599 --> 00:19:31,829
And that becomes a response
from the invoke API call.

318
00:19:32,009 --> 00:19:34,449
So that's a synchronous invocation.

319
00:19:34,449 --> 00:19:37,179
But you can also have an
asynchronous invocation by

320
00:19:37,199 --> 00:19:39,149
setting invocation type as event.

321
00:19:39,369 --> 00:19:42,939
In which case, as a caller, I
call the invoke function with

322
00:19:42,949 --> 00:19:46,629
its invoke API, and I'm going to
get a response back right away.

323
00:19:46,879 --> 00:19:48,444
But the function is not
going to run right away.

324
00:19:48,554 --> 00:19:49,014
Right there.

325
00:19:49,014 --> 00:19:51,894
And then it's going to run at
some point, and it's going to

326
00:19:51,934 --> 00:19:53,254
go through an internal queue.

327
00:19:53,474 --> 00:19:57,384
And so I don't get the output from
the function from calling the invoke

328
00:19:57,394 --> 00:20:02,884
API, but the function is going to
actually execute asynchronously.

329
00:20:03,394 --> 00:20:08,569
And when it comes to synchronous lambda
to lambda invocations, It's pretty much

330
00:20:08,689 --> 00:20:14,719
always a sign of bad design, and it
has also got cost implications because

331
00:20:15,229 --> 00:20:19,609
when the first function runs and calls
the second function synchronously, it

332
00:20:19,619 --> 00:20:23,469
has to wait for the second function
to finish executing to get its output

333
00:20:23,699 --> 00:20:28,359
in the response, which means for the
entire duration of the second function.

334
00:20:28,984 --> 00:20:31,744
The first function is still
running and just waiting.

335
00:20:31,984 --> 00:20:36,724
So you're actually paying for execution
time twice for both the first function,

336
00:20:36,934 --> 00:20:41,244
the caller, as well as the second
function, the callee, which is again,

337
00:20:41,664 --> 00:20:45,364
I don't mind spending money, but I want
to get value for what I'm spending on.

338
00:20:45,544 --> 00:20:46,834
I hate the waste.

339
00:20:46,834 --> 00:20:50,039
I hate the painful things that
just Doesn't provide any value.

340
00:20:50,069 --> 00:20:54,169
In this case, I'm not really getting
any value from having two functions all

341
00:20:54,179 --> 00:20:59,149
running at the same time, and especially
when you've got, say, two functions, one

342
00:20:59,149 --> 00:21:04,919
calling another inside the same service
boundary, whereby I own both functions,

343
00:21:05,129 --> 00:21:08,569
I actually don't need them to be running
as two separate lambda functions.

344
00:21:08,789 --> 00:21:09,629
I can just.

345
00:21:09,719 --> 00:21:13,589
Get rid of a second function and do
whatever it needs to do inside that first

346
00:21:13,589 --> 00:21:18,719
function because again, I own everything,
so I can just reorganize things and I

347
00:21:18,719 --> 00:21:22,679
can still have that modularity at the
code level without having to also have

348
00:21:22,679 --> 00:21:28,099
that modularity at the lamb function or
infrastructure level as well and mean.

349
00:21:28,099 --> 00:21:30,769
The thing is this, things are
called lamb functions, but you

350
00:21:30,769 --> 00:21:32,239
shouldn't have to confuse them.

351
00:21:32,574 --> 00:21:36,764
With Lambda functions in programming,
you can still have, you know, modularity

352
00:21:36,764 --> 00:21:40,224
in your code level without having
to again, you know, enforce the same

353
00:21:40,224 --> 00:21:42,254
modularity at the infrastructure level.

354
00:21:42,824 --> 00:21:46,954
But what if your Lambda to Lambda
calls are across service boundaries?

355
00:21:46,964 --> 00:21:50,674
So now this would be one service
providing some capabilities to

356
00:21:50,704 --> 00:21:54,664
other teams and other services by
exposing a Lambda function that other

357
00:21:54,664 --> 00:21:57,064
can just call and invoke directly.

358
00:21:57,784 --> 00:22:04,359
This is even a worse idea because now
you are Binding your consumers, your API

359
00:22:04,409 --> 00:22:10,729
or service consumers to implementation
details that can easily change the fact

360
00:22:10,739 --> 00:22:15,139
that you are using a Lambda function
and what region you're in and what your

361
00:22:15,139 --> 00:22:19,309
function is called, all of these things
are implementation details, and you should

362
00:22:19,319 --> 00:22:24,079
be able to change any and all of them
without forcing some downstream systems

363
00:22:24,249 --> 00:22:28,479
to change as well, so that if you want
to give them some capabilities, please.

364
00:22:28,644 --> 00:22:30,744
Just give them an API that they can call.

365
00:22:31,044 --> 00:22:33,864
So the fact that you are using
a Lambda function behind it, the

366
00:22:33,864 --> 00:22:36,064
API, is just implementation detail.

367
00:22:36,264 --> 00:22:40,014
maybe today Lambda makes sense for
you, but maybe tomorrow you are, you

368
00:22:40,044 --> 00:22:43,064
know, you're handling such a high
traffic that it makes more sense

369
00:22:43,074 --> 00:22:46,644
to take your workload and move it
into, say, a Fargate container.

370
00:22:46,894 --> 00:22:50,474
You can do that, but it doesn't
impact your callers because they are

371
00:22:50,474 --> 00:22:52,674
still talking to the same HTTP API.

372
00:22:52,674 --> 00:22:57,054
They're using the same HTTP API
contract to talk to your service.

373
00:22:57,054 --> 00:22:59,399
So, And, no, that's a stable interface.

374
00:22:59,569 --> 00:23:03,389
What you do behind the HTTP
API is entirely your business.

375
00:23:03,549 --> 00:23:06,689
It's implementation details
that you can change and control.

376
00:23:07,239 --> 00:23:10,349
Okay, so we've established that
synchronous lambda to lambda

377
00:23:10,349 --> 00:23:11,889
calls is the anti pattern.

378
00:23:11,919 --> 00:23:14,879
But what about asynchronous invocations?

379
00:23:15,189 --> 00:23:18,389
Because we talked about earlier that
when you invoke a function, you can

380
00:23:18,399 --> 00:23:22,219
do so synchronously, but you can
also do it asynchronously as well.

381
00:23:23,004 --> 00:23:26,704
Well, there are some legit use
cases I think is a good idea for

382
00:23:26,704 --> 00:23:28,274
using asynchronous invocations.

383
00:23:28,544 --> 00:23:33,984
For example, if I've got a user facing
API that handles the user request,

384
00:23:34,294 --> 00:23:39,364
and if the user request is to say save
my profile updates in the database,

385
00:23:39,394 --> 00:23:41,094
okay, my function is going to do that.

386
00:23:41,374 --> 00:23:45,814
But I maybe have some other secondary
responsibility that I want to do.

387
00:23:46,239 --> 00:23:50,039
Such as, tracking some events for
business analytics and then what

388
00:23:50,049 --> 00:23:53,919
have you, those are not things
that the user really cares about.

389
00:23:54,009 --> 00:23:58,359
So I don't want to do them while
the user is waiting for response.

390
00:23:58,649 --> 00:24:02,819
So what I can do is to take those
secondary responsibilities and move

391
00:24:02,819 --> 00:24:06,959
them into a second function and invoke
the second function asynchronously.

392
00:24:07,089 --> 00:24:08,829
So I don't have to wait for it to finish.

393
00:24:09,119 --> 00:24:13,139
And so this way allows me to build
a better user experience because my

394
00:24:13,149 --> 00:24:17,759
user facing API function can respond
to the caller faster without having

395
00:24:17,759 --> 00:24:20,809
to wait for all those secondary
responsibilities to complete.

396
00:24:21,179 --> 00:24:24,449
And it also allows me to build
more robust error handling as

397
00:24:24,449 --> 00:24:28,119
well when it comes to dealing with
those secondary responsibilities.

398
00:24:28,179 --> 00:24:32,229
Because when it comes to asynchronous
invocations, Lambda functions gives you

399
00:24:32,229 --> 00:24:36,939
built in, two retries out of the box
as well as the Delta Q support as well.

400
00:24:37,119 --> 00:24:41,429
So that if something goes wrong and I
care about, you know, making sure those

401
00:24:41,449 --> 00:24:45,739
things happen, I can then use the data
queue to capture any failed invocations.

402
00:24:46,009 --> 00:24:51,149
And so I can then retry them later when,
say, maybe I'm talking to a third party

403
00:24:51,149 --> 00:24:53,379
service that had a temporary downtime.

404
00:24:53,379 --> 00:24:57,449
And so I can then use the data
queue to capture the events that

405
00:24:57,449 --> 00:25:00,729
failed and then reprocess them
when the system comes back online.

406
00:25:01,229 --> 00:25:04,634
And that's with the assumption
that what I've got here is

407
00:25:04,844 --> 00:25:06,334
within the same service boundary.

408
00:25:06,334 --> 00:25:09,914
So whatever I'm doing in the second
function are things that I would

409
00:25:09,924 --> 00:25:11,324
have done in the first function.

410
00:25:11,554 --> 00:25:14,074
That's part of my API service already.

411
00:25:14,424 --> 00:25:18,864
And what about when you have asynchronous
implications across service boundaries?

412
00:25:19,194 --> 00:25:21,074
This is still going to be a bad idea.

413
00:25:21,294 --> 00:25:26,974
You never want to expose capabilities
to other services as in the form of a

414
00:25:26,974 --> 00:25:28,724
lambda function that someone can call.

415
00:25:29,164 --> 00:25:32,934
directly, either synchronously or
asynchronously, because again, you are

416
00:25:32,934 --> 00:25:37,324
exposing them, you're tying them to
implementation details on your side.

417
00:25:38,214 --> 00:25:42,094
So in general, are async lambda
to lambda invocations okay?

418
00:25:42,254 --> 00:25:44,634
well, it really depends on
what it is you're doing.

419
00:25:44,644 --> 00:25:47,344
And as I said, there are some
legit use cases for it, which

420
00:25:47,344 --> 00:25:48,724
I think they are a good idea.

421
00:25:49,154 --> 00:25:53,024
But I think it's really important, to
follow a principle that, where, you know,

422
00:25:53,024 --> 00:25:57,034
when you look at every single component
in your architecture, They should all

423
00:25:57,044 --> 00:26:00,684
serve a purpose and they should all
provide some return on investment.

424
00:26:00,944 --> 00:26:03,064
And you shouldn't just
do it for the sake of it.

425
00:26:03,564 --> 00:26:06,474
And talking about looking at the
architectural components and making

426
00:26:06,474 --> 00:26:07,554
sure that everything has a good.

427
00:26:07,864 --> 00:26:11,044
I'm making sure that everything has
a return on investment and then the

428
00:26:11,044 --> 00:26:15,264
good, I guess, a real world antidote
from the, the far from guys is that,

429
00:26:15,344 --> 00:26:19,404
they had this system whereby, you know,
they've got an ingestion API, which is

430
00:26:19,424 --> 00:26:22,844
backed by lambda function that will put
something into a queue and then they'll

431
00:26:22,844 --> 00:26:25,474
process it with another SQS function.

432
00:26:25,769 --> 00:26:30,079
And so what they found was that as
they simplify this setup and remove the

433
00:26:30,079 --> 00:26:34,089
queue from the, from the architecture
and the second SQS function from the

434
00:26:34,089 --> 00:26:38,069
architecture, and just did everything
in that first function at the ingestion

435
00:26:38,289 --> 00:26:40,989
API, they actually save a lot of cost.

436
00:26:41,219 --> 00:26:44,419
And they also improve the performance
of the system because there's

437
00:26:44,449 --> 00:26:47,339
less things that, you know, less
moving parts in architecture.

438
00:26:47,669 --> 00:26:52,319
And they also save quite a bit
of cost associated with SQS and

439
00:26:52,339 --> 00:26:53,799
also that second lambda function.

440
00:26:54,034 --> 00:26:59,274
So by simplifying architecture, you can
also find that you can sometimes save on

441
00:26:59,274 --> 00:27:03,874
cost as well, especially in a serverless
environment where you're paying for every

442
00:27:03,874 --> 00:27:05,714
single request that you're processing.

443
00:27:06,214 --> 00:27:11,314
And when it comes to cost efficiency and
scalability and performance, caching is

444
00:27:11,444 --> 00:27:17,004
probably the one of the most powerful
and probably underutilized tool.

445
00:27:17,274 --> 00:27:20,454
For me, it's almost like a cheat
code for building performance

446
00:27:20,574 --> 00:27:21,944
and scalable applications.

447
00:27:22,339 --> 00:27:27,229
And when talking about your typical,
I guess the web APIs, there are so

448
00:27:27,229 --> 00:27:30,909
many different ways and do different
places where you can apply caching.

449
00:27:31,409 --> 00:27:34,649
So consider your typical,
I guess, user facing API.

450
00:27:34,919 --> 00:27:40,789
You've got your client talking to, an
API behind some CDN like CloudFront

451
00:27:41,049 --> 00:27:45,039
and API is backed by a Lambda
function and some DynamDB table.

452
00:27:45,214 --> 00:27:49,984
You can do some client side caching
for static assets or configurations

453
00:27:49,984 --> 00:27:51,394
that doesn't really change.

454
00:27:51,654 --> 00:27:55,994
and you can also do some API response
caching at the edge with a CloudFront

455
00:27:55,994 --> 00:27:57,424
in front of your API gateway.

456
00:27:57,794 --> 00:28:02,064
And then you can also know if there's
anything that's the computationally

457
00:28:02,074 --> 00:28:04,474
expensive that your application is doing.

458
00:28:04,654 --> 00:28:08,674
You can also do application level
caching in the Lambda function, perhaps

459
00:28:08,674 --> 00:28:10,354
using something like Elastic Cache.

460
00:28:10,584 --> 00:28:15,384
So that you can, you know, share the
same, cash results across multiple

461
00:28:15,404 --> 00:28:19,654
instances of the same function, or maybe
even across multiple functions as well.

462
00:28:20,024 --> 00:28:23,354
And since I don't like paying for
uptime that, you know, for things

463
00:28:23,354 --> 00:28:27,084
that I don't use, elastic cash
forces me to pay for uptime instead

464
00:28:27,084 --> 00:28:29,244
of paying for just what I use.

465
00:28:29,464 --> 00:28:31,564
actually nowadays I prefer
to use Memento as well.

466
00:28:32,074 --> 00:28:36,034
which gives you a really good, really
scalable and cost efficient serverless

467
00:28:36,064 --> 00:28:39,844
cache that you only pay for the number
of requests you make, as opposed

468
00:28:39,854 --> 00:28:42,274
to for the uptime for the cluster.

469
00:28:42,774 --> 00:28:46,104
Another form of caching that people
don't talk about enough, and at

470
00:28:46,104 --> 00:28:51,334
scale can also still bite you,
is the Route 53, or DNS caching.

471
00:28:51,734 --> 00:28:55,894
With Route 53, you are charged for
the number of requests, or number of

472
00:28:55,924 --> 00:29:00,694
DNS requests that Route 53 handles
for you, and depending on your TTL,

473
00:29:00,904 --> 00:29:04,624
you know, if you have got a short
TTL, then that means you may get

474
00:29:04,624 --> 00:29:07,994
more requests coming in, and so when
you're running a single route, scale.

475
00:29:08,174 --> 00:29:10,814
those requests, again, those
requests and those, those costs

476
00:29:10,814 --> 00:29:12,594
can still to add up pretty quickly.

477
00:29:12,824 --> 00:29:16,764
And so if you know, you've got a domain
that's quite stable, you're not just

478
00:29:16,764 --> 00:29:21,244
going to be changing it the regularly,
then just use a very, very long TTL,

479
00:29:21,454 --> 00:29:25,644
maybe like a couple of hours or a day,
or even a week for things that are really

480
00:29:25,644 --> 00:29:27,794
stable and really unlikely to change.

481
00:29:28,054 --> 00:29:28,444
so.

482
00:29:28,804 --> 00:29:32,974
You can do that and suddenly you can
cut down your 53 costs by a significant

483
00:29:32,974 --> 00:29:34,824
amount when you're running a scale.

484
00:29:35,324 --> 00:29:39,674
When it comes to DNS and rapid API
requests, there's another one that

485
00:29:39,824 --> 00:29:43,994
oftentimes can add some hidden
costs that you don't think about,

486
00:29:44,634 --> 00:29:48,704
which is the cost associated with
making cross origin requests.

487
00:29:49,254 --> 00:29:54,734
So with API gateway, It's really
easy to enable course support.

488
00:29:54,914 --> 00:29:57,934
I just basically have to turn
a setting on and that's it.

489
00:29:58,294 --> 00:30:03,814
And API Gateway is going to generate those
options endpoints for your API endpoints.

490
00:30:03,814 --> 00:30:07,594
But because API Gateway charges
you for every single request,

491
00:30:07,854 --> 00:30:10,784
it's also going to charge you for
those course requests as well.

492
00:30:11,334 --> 00:30:14,324
And, You may think, oh, okay,
that's easy problem to fix.

493
00:30:14,334 --> 00:30:19,034
You just slap a CDN in front of it
and use caching and everything's fine.

494
00:30:19,534 --> 00:30:23,994
However, there has been a long standing
bug with how API gateways, the options

495
00:30:24,024 --> 00:30:28,824
and points handles the cache headers,
when you're using authorization headers.

496
00:30:29,044 --> 00:30:33,624
So this will be when you're using say,
API gateway with a cognitive authorizer.

497
00:30:34,114 --> 00:30:37,944
So the cognitive tokens have to come
in through the authorization header.

498
00:30:38,324 --> 00:30:41,004
And if you're using the authorization
header, then it's going to break

499
00:30:41,004 --> 00:30:44,234
the cache headers that API gateways
options endpoints going to return,

500
00:30:44,484 --> 00:30:48,614
which gonna just break your caching
for those options endpoints.

501
00:30:48,874 --> 00:30:51,744
So there's a good chance that you
may be double paying for those user

502
00:30:51,754 --> 00:30:56,184
requests because caching is not working
properly for the options endpoints.

503
00:30:56,784 --> 00:31:00,394
Your solution is to either to
roll your own options endpoints

504
00:31:00,594 --> 00:31:02,334
Which of course is not great.

505
00:31:02,334 --> 00:31:04,284
It's a lot of actual work you have to do.

506
00:31:04,674 --> 00:31:07,884
Or if you have control over
the DN settings and domain

507
00:31:07,884 --> 00:31:09,624
settings for your application.

508
00:31:09,874 --> 00:31:15,784
instead of, putting your applications,
APIs on a subdomain like api.example.com,

509
00:31:15,994 --> 00:31:19,354
if you move them to a sub path or the
same domain where the front end is

510
00:31:19,354 --> 00:31:25,804
hosted, like example.com/api, then
those don't count as, cross origin.

511
00:31:26,154 --> 00:31:29,694
Whereas requests to subdomains
count as cross origin.

512
00:31:29,704 --> 00:31:32,874
So by making a very simple domain
configuration change, you can

513
00:31:32,884 --> 00:31:36,124
actually remove the need for
course requests altogether.

514
00:31:36,624 --> 00:31:40,464
So moving on to, I guess, a more
architectural level of decisions.

515
00:31:40,744 --> 00:31:45,434
And when it comes to AWS and in the
cloud, every architectural decision

516
00:31:45,454 --> 00:31:47,254
is essentially a buying decision.

517
00:31:47,454 --> 00:31:50,374
You're deciding what services
you're going to buy and use.

518
00:31:50,704 --> 00:31:53,564
And making the wrong choice
here and using the wrong service

519
00:31:53,854 --> 00:31:55,444
Can be very, very costly.

520
00:31:56,194 --> 00:31:59,184
Take, for example, you're building
some kind of event driven architecture.

521
00:31:59,204 --> 00:32:02,614
And so you need to use some kind
of a messaging service between

522
00:32:02,614 --> 00:32:04,074
maybe different Lambda functions.

523
00:32:04,364 --> 00:32:07,924
And, you know, if you're running at
the one request per second, then you

524
00:32:07,924 --> 00:32:09,494
may think, Oh, you know, look at that.

525
00:32:09,744 --> 00:32:14,624
Kinesis is quite expensive compared
to SNS and SQS and the event bridge,

526
00:32:14,884 --> 00:32:18,364
but it's always important to take your
specific contacts and stripper into

527
00:32:18,364 --> 00:32:22,314
consideration, because if we were to
dial this up to a thousand requests per

528
00:32:22,314 --> 00:32:28,944
second, Then suddenly Kinesis is much,
much cheaper compared to EventBridge,

529
00:32:29,174 --> 00:32:35,184
because even though Kinesis charges you
for uptime for the shards, however, it's

530
00:32:35,214 --> 00:32:38,024
cost per million requests is much lower.

531
00:32:38,194 --> 00:32:43,004
So as the number of requests goes
up, your cost for Kinesis actually

532
00:32:43,174 --> 00:32:48,904
goes up much more slowly compared
to the cost for say SNS and SQS and

533
00:32:48,904 --> 00:32:50,874
EventBridge, which only charges you for.

534
00:32:51,119 --> 00:32:55,109
The request and doesn't have an
uptime cost associated with them.

535
00:32:55,769 --> 00:32:59,419
Same thing plays out with
say API gateway versus ALB.

536
00:32:59,659 --> 00:33:04,989
Whereas ALB has got a kind of complex
calculation for determining how many load

537
00:33:04,989 --> 00:33:10,699
balancer units you use and, and charges
you an uptime hourly cost for that.

538
00:33:10,709 --> 00:33:15,059
So at a very low throughput, API
gateway is much cheaper, whether it's

539
00:33:15,059 --> 00:33:20,969
the, whether you're using the REST
APIs or HTTP APIs compared to ALB.

540
00:33:21,329 --> 00:33:24,949
But as you dial up the number of requests
per second, so the total number of

541
00:33:24,949 --> 00:33:28,919
requests you're processing in a month,
suddenly API gateway is going to be

542
00:33:28,919 --> 00:33:31,344
a lot more expensive compared to ALB.

543
00:33:31,844 --> 00:33:36,544
So a good rule of thumb to have around
AWS is that the services that charges you

544
00:33:36,574 --> 00:33:41,604
based on uptime is going to be much, much
more cost efficient compared to services

545
00:33:41,604 --> 00:33:43,674
that only charges you for the requests.

546
00:33:44,204 --> 00:33:47,434
So that's a good rule of thumb to
have, but you still have to understand

547
00:33:47,434 --> 00:33:51,964
the specific cost dimensions of the
individual services that you work with.

548
00:33:52,514 --> 00:33:55,674
The example I showed you earlier
with the assumption that every single

549
00:33:55,674 --> 00:33:59,114
request is about one kilobyte in size.

550
00:33:59,484 --> 00:34:06,014
But what happens if you were to dial it
up to one megabyte of payload per request?

551
00:34:06,424 --> 00:34:11,329
Because The bytes process is also
one of the dimensions that ALB

552
00:34:11,339 --> 00:34:15,099
uses to calculate how much load
balancer unit you are charged for.

553
00:34:15,519 --> 00:34:20,729
So at a much bigger payload size, ALB is
going to be a lot more expensive compared

554
00:34:20,729 --> 00:34:25,529
to API gateway in this kind of a niche
situation where maybe you're sending large

555
00:34:25,569 --> 00:34:28,449
documents or video files or binary data.

556
00:34:28,859 --> 00:34:33,259
And so at one megabytes per request,
ALB is going to charge you a lot

557
00:34:33,259 --> 00:34:34,889
more compared to API gateway.

558
00:34:35,294 --> 00:34:38,044
So it's always important for you
to understand the specific cost

559
00:34:38,044 --> 00:34:40,074
dimensions of the services that you use.

560
00:34:40,574 --> 00:34:45,364
At the reInvent 2023, Werner gave
us the frugal architect, and he gave

561
00:34:45,364 --> 00:34:50,394
us, seven different rules or laws of
the, of the frugal architect on AWS.

562
00:34:50,684 --> 00:34:55,144
And specifically, I really like this
too, where he says that, architecting

563
00:34:55,174 --> 00:34:58,034
is basically a series of trade offs.

564
00:34:58,064 --> 00:35:02,224
And also we want to find systems
that aligns with the business

565
00:35:02,224 --> 00:35:04,724
revenue model for our application.

566
00:35:04,774 --> 00:35:09,404
This is so that, you know, as our business
grows and our revenue grows, the cost is

567
00:35:09,404 --> 00:35:11,354
going to grow along the same dimension.

568
00:35:11,354 --> 00:35:13,804
So as we make more
money, our cost goes up.

569
00:35:13,984 --> 00:35:17,534
we don't have a situation where, you
know, we're not making money, but, our

570
00:35:17,534 --> 00:35:21,854
cost is still going up because the,
the cost dimensions is misaligned with

571
00:35:21,874 --> 00:35:24,224
the business revenue, of our business.

572
00:35:24,724 --> 00:35:29,054
And I've built a lot of, WebSocket
based applications, and on AWS,

573
00:35:29,064 --> 00:35:32,254
you've got multiple services
that give you WebSockets,

574
00:35:32,324 --> 00:35:33,664
without having to manage servers.

575
00:35:33,934 --> 00:35:37,394
So API Gateway has got managed
WebSockets, so does AppSync with

576
00:35:37,464 --> 00:35:41,454
AppSync subscriptions, and IoT Core
also gives you WebSockets as well.

577
00:35:41,834 --> 00:35:44,164
And imagine you're building
some kind of a social network.

578
00:35:44,164 --> 00:35:48,254
So as more users signs up, and you're
using WebSockets to give them real time

579
00:35:48,274 --> 00:35:52,734
updates whenever someone send them a
message, or replies on their tweets

580
00:35:52,734 --> 00:35:54,934
or retweets or likes or what have you.

581
00:35:55,204 --> 00:35:59,084
So your connection time for the WebSocket
connections is going to go up as well

582
00:35:59,124 --> 00:36:03,624
with the number of users, but your
revenue doesn't go up at the same, the

583
00:36:03,624 --> 00:36:07,654
same dimension or at the same rate as
the connection time, because, Revenue

584
00:36:07,654 --> 00:36:09,894
goes up when users are more engaged.

585
00:36:10,084 --> 00:36:12,934
So when they're doing more
stuff, then they're more likely

586
00:36:12,934 --> 00:36:14,624
to turn into a paying user.

587
00:36:15,004 --> 00:36:18,234
So revenue goes up with
the amount of engagement.

588
00:36:18,534 --> 00:36:23,874
And so I want to find the services that
I can use where as far as the WebSocket

589
00:36:23,884 --> 00:36:28,064
is concerned, I'm going to be paying for
activity and not for connection time.

590
00:36:28,464 --> 00:36:32,364
Unfortunately, for all the services we
looked at earlier, One element or one

591
00:36:32,394 --> 00:36:34,674
dimension of the cost is connection time.

592
00:36:34,954 --> 00:36:38,764
And that's why when it comes to building
WebSocket based applications, I also very

593
00:36:38,764 --> 00:36:44,204
much like Memento as well, which gives
you WebSocket through its Memento topics.

594
00:36:44,444 --> 00:36:47,584
And the nice thing about Memento
topics is that they don't

595
00:36:47,584 --> 00:36:48,884
charge you for connection time.

596
00:36:49,224 --> 00:36:52,044
They only charge you for the
number of messages that you send.

597
00:36:52,284 --> 00:36:54,594
So that, you know,
compared to other services.

598
00:36:55,219 --> 00:36:58,489
They only charge you for messages
sent and not for connection time.

599
00:36:58,509 --> 00:37:02,969
And therefore, as my business
grows, activity is going to go up.

600
00:37:02,989 --> 00:37:07,299
And, and only when people are doing things
and sending messages and retweeting and

601
00:37:07,299 --> 00:37:09,839
whatnot, that's when I get the engagement.

602
00:37:09,849 --> 00:37:12,879
And that's when, that's, and that's the
thing that's going to drive my revenue.

603
00:37:12,879 --> 00:37:16,769
And so my cost is going to
go up with engagement and

604
00:37:16,769 --> 00:37:18,149
not just for connection time.

605
00:37:18,939 --> 00:37:23,879
And speaking of, picking now services that
are cost efficient to work with, You may

606
00:37:23,879 --> 00:37:28,909
have came across this last year when a
social network actually blew up and went

607
00:37:28,909 --> 00:37:33,249
from something like a few hundred users
to half a million daily active users.

608
00:37:33,469 --> 00:37:36,899
And they ran on Vercel and
suddenly they got slapped with

609
00:37:36,939 --> 00:37:38,839
something like a 100, 000 bill.

610
00:37:39,189 --> 00:37:44,759
And this is because even though Vercel is
built on top of AWS and, you know, every

611
00:37:44,759 --> 00:37:48,219
layer have to add more, I guess, margins.

612
00:37:48,219 --> 00:37:51,959
And so As, you know, you are using
the cell, they're actually slapping

613
00:37:51,959 --> 00:37:56,099
a big margin, a big markup on top
of the, their a s cost as well.

614
00:37:56,129 --> 00:38:01,449
And so in the, in this specific example,
they actually have a seven x markup on

615
00:38:01,449 --> 00:38:03,849
the underlying cost of Lambda functions.

616
00:38:04,059 --> 00:38:08,269
So Versal is, I think a lot of, I
guess the, cost overhead in terms

617
00:38:08,269 --> 00:38:09,909
of, what you end up paying for.

618
00:38:10,229 --> 00:38:13,189
And they also add in more and more
dimensions in terms of, you know,

619
00:38:13,189 --> 00:38:15,839
different things that they're going
to charge you for, including now,

620
00:38:16,049 --> 00:38:20,329
to Jane, the loss from your cell
functions so that you can query them

621
00:38:20,339 --> 00:38:22,069
elsewhere in a third party platform.

622
00:38:22,269 --> 00:38:27,039
So when you think of a cell, do keep in
mind that there's a huge markup in terms

623
00:38:27,039 --> 00:38:31,959
of the cost of running, the cell functions
versus just underlying lambda functions.

624
00:38:32,229 --> 00:38:34,809
and so when you are starting to have
something that's, really running at a.

625
00:38:34,909 --> 00:38:35,699
pretty big scale.

626
00:38:35,899 --> 00:38:37,589
then, it can be really, really costly.

627
00:38:37,619 --> 00:38:41,329
And I've seen so many stories about, a
set of customers, find, being surprised

628
00:38:41,329 --> 00:38:44,869
by a big, bill, when they suddenly
get some traction in the application.

629
00:38:45,369 --> 00:38:48,919
we talked about this one earlier, that,
simplifying your architecture can also pay

630
00:38:48,919 --> 00:38:50,939
off in terms of, cost efficiency as well.

631
00:38:51,329 --> 00:38:55,009
And a good rule of thumb to
have is to avoid any unnecessary

632
00:38:55,019 --> 00:38:56,839
moving parts in your architecture.

633
00:38:57,609 --> 00:39:00,569
We talked about this one earlier
where when you have a synchronous

634
00:39:00,799 --> 00:39:05,329
lambda to lambda invocation, then just
simplify by combining everything you're

635
00:39:05,349 --> 00:39:07,149
doing into a single lambda function.

636
00:39:07,994 --> 00:39:11,574
but what about the way you have
asynchronous Lambda to Lambda invocations?

637
00:39:11,624 --> 00:39:15,134
We talked about some particular use
cases that are, that's a good idea,

638
00:39:15,434 --> 00:39:18,714
but oftentimes what I see instead
is, you know, I've got, folks that

639
00:39:18,724 --> 00:39:23,374
are putting, essentially like a SNS,
topic, before, between two different

640
00:39:23,384 --> 00:39:26,734
Lambda functions, just so that they
are avoiding these Lambda to Lambda

641
00:39:26,734 --> 00:39:31,724
invocation, but they're not actually
using SNS to provide any fan out.

642
00:39:31,934 --> 00:39:32,504
It's just.

643
00:39:32,619 --> 00:39:36,299
Purely there for, I guess, for
cosmetics where they want to avoid

644
00:39:36,299 --> 00:39:37,759
the Lambda, Lambda invocations.

645
00:39:38,049 --> 00:39:41,644
I think again, for that, you may
as well just do the direct Lambda,

646
00:39:41,644 --> 00:39:44,089
Lambda invocations asynchronously.

647
00:39:44,239 --> 00:39:47,599
As we talk about, there are some use
cases for that, especially in this case,

648
00:39:47,639 --> 00:39:51,919
if you're not doing a fan out to multiple
targets, you don't really need SNS and

649
00:39:51,919 --> 00:39:54,839
you're still going to get all the benefits
in terms of having multiple targets.

650
00:39:54,889 --> 00:40:00,719
built in retries but also DLQ and
the Lambda destination support

651
00:40:00,719 --> 00:40:04,379
as well, for when those async
invocations fail after a few times.

652
00:40:04,589 --> 00:40:06,219
So you're not going to lose data.

653
00:40:06,479 --> 00:40:10,019
you get the same thing with, SNS
to Lambda, but again, you don't

654
00:40:10,029 --> 00:40:13,189
need to have the SNS topic if
you're not doing any fan out.

655
00:40:13,479 --> 00:40:15,409
So, you know, don't avoid.

656
00:40:15,639 --> 00:40:19,119
Don't put any SNS between them and
functions just to avoid the direct

657
00:40:19,119 --> 00:40:20,879
Lambda to Lambda async invocations.

658
00:40:20,909 --> 00:40:25,299
Because again, async invocations
has got some valid use cases.

659
00:40:25,519 --> 00:40:27,919
It's a synchronous Lambda
to Lambda invocations that

660
00:40:27,919 --> 00:40:28,919
you want to watch out for.

661
00:40:29,419 --> 00:40:33,499
As I mentioned earlier, every component in
your architecture should serve a purpose.

662
00:40:33,529 --> 00:40:36,286
So when you look at your
architecture diagram, always pay

663
00:40:36,286 --> 00:40:37,744
attention to what you're doing.

664
00:40:37,744 --> 00:40:41,344
What each component is providing
is, are they adding any value?

665
00:40:41,554 --> 00:40:43,354
Is there any reason for them to be there?

666
00:40:43,594 --> 00:40:48,034
If not, consider whether or not I can
just remove that, architecture component.

667
00:40:48,504 --> 00:40:51,714
And, as Grace Hopper once said, the
most dangerous phrase in the language

668
00:40:51,714 --> 00:40:53,334
is that we've always done it this way.

669
00:40:53,514 --> 00:40:56,394
Of course, people know this quote,
so they don't say that anymore.

670
00:40:56,584 --> 00:40:59,044
but instead, you know, it is
just, it's still reflected,

671
00:40:59,044 --> 00:41:00,214
in a lot of the behaviors.

672
00:41:01,029 --> 00:41:04,149
Yeah, we know we feel more comfortable
with this because it's, you know,

673
00:41:04,159 --> 00:41:05,349
we're familiar with this pattern.

674
00:41:05,349 --> 00:41:08,569
So again, be very critical when you
look at your architecture and see

675
00:41:08,809 --> 00:41:12,809
what each part is doing and what value
you're getting from each component

676
00:41:13,009 --> 00:41:14,289
in your architecture diagram.

677
00:41:14,999 --> 00:41:18,679
So in terms of, simplifying architecture,
there's different things you can do.

678
00:41:18,819 --> 00:41:22,339
For example, lambda function
URLs nowadays is quite useful.

679
00:41:22,624 --> 00:41:28,184
Alternative to using API gateway
to build a serverless APIs.

680
00:41:28,524 --> 00:41:31,954
so instead of having API gateway in
front of Lambda function just to serve

681
00:41:31,954 --> 00:41:35,634
as a HP API endpoint that someone
can call, you can actually have just

682
00:41:35,634 --> 00:41:39,184
Lambda function exposing itself,
you know, with the function URL.

683
00:41:39,474 --> 00:41:41,814
So this is very useful
if you're not using.

684
00:41:41,954 --> 00:41:45,674
All of the features that API gateway
gives you in terms of having a cognitive

685
00:41:46,294 --> 00:41:50,424
authorizer, or the fact that it has
support for request models and can

686
00:41:51,234 --> 00:41:57,384
validate post bodies for you, or is able
to directly integrate with other services.

687
00:41:57,744 --> 00:42:00,864
Or maybe you know you're
hitting some API gateway.

688
00:42:01,034 --> 00:42:06,784
Specific limits, such as the 29 seconds
timeout for integration endpoints, or the

689
00:42:06,784 --> 00:42:10,044
fact that it doesn't support the response
streaming, which is becoming more and

690
00:42:10,044 --> 00:42:14,564
more important when it comes to supporting
AI workloads and, building chatbots

691
00:42:14,574 --> 00:42:16,794
that can, stream response to the caller.

692
00:42:16,834 --> 00:42:20,404
And as responses come
back from the LM model.

693
00:42:20,904 --> 00:42:24,494
The downside with using lambda function
URL is that you are kind of forced

694
00:42:24,494 --> 00:42:28,224
into writing lambda lifts, which is
basically a single lambda function

695
00:42:28,234 --> 00:42:32,554
that handles all of the routes for
an API and does this internal routing

696
00:42:32,554 --> 00:42:34,464
for different parts of the code.

697
00:42:34,844 --> 00:42:38,214
and, the main downside here
is that, you also lose the per

698
00:42:38,274 --> 00:42:40,464
endpoint metrics and the alerts.

699
00:42:40,764 --> 00:42:45,814
And you're also not able to implement fine
grained access control and use different

700
00:42:46,094 --> 00:42:48,274
access control for each of the endpoints.

701
00:42:48,564 --> 00:42:51,214
And also a lot of frameworks
that you may want to use.

702
00:42:51,474 --> 00:42:54,654
They are not designed for lambdas
constrained execution environment.

703
00:42:54,654 --> 00:42:58,364
So they're often quite big and
bulky and they can really affect,

704
00:42:58,594 --> 00:43:01,834
they can really affect the costar
performance of your function.

705
00:43:02,464 --> 00:43:07,719
In terms of security, You can either
have a public URL or you can use AWS

706
00:43:07,759 --> 00:43:09,779
IAM as the authentication mechanism.

707
00:43:10,009 --> 00:43:14,449
So I find that they're best for
either public APIs or internal

708
00:43:14,479 --> 00:43:19,719
microservices APIs where you're going
to use AWS IAM as the authentication

709
00:43:19,739 --> 00:43:21,059
and authorization mechanism.

710
00:43:21,949 --> 00:43:26,579
However, having said that, if you
want to build a user facing API that's

711
00:43:26,579 --> 00:43:29,009
authenticated, you can still do that.

712
00:43:29,179 --> 00:43:33,769
You can just validate, say, Cognito token
inside your Lambda function, and turns

713
00:43:33,769 --> 00:43:35,519
out it's actually pretty quick to do that.

714
00:43:35,759 --> 00:43:38,269
so in terms of performance,
there's no downside to doing that.

715
00:43:38,489 --> 00:43:42,869
The main consideration here is that when
it comes to unauthorized requests, you

716
00:43:42,869 --> 00:43:44,719
only find out when your function runs.

717
00:43:44,969 --> 00:43:49,519
as opposed to with API gateway, where
API gateway can validate the token.

718
00:43:49,539 --> 00:43:53,519
And if it's invalid, it's going to reject
the request and you don't pay for those

719
00:43:53,569 --> 00:43:55,639
unauthorized requests with API gateway.

720
00:43:56,139 --> 00:43:59,079
We, if we do that with a Lambda
function, then you have to always pay

721
00:43:59,079 --> 00:43:59,819
for the Lambda function invocation.

722
00:44:00,639 --> 00:44:04,199
And however much, And however much
time it takes your function to

723
00:44:04,199 --> 00:44:08,359
realize that this is a unauthorized
request, the token is invalid, or

724
00:44:08,359 --> 00:44:10,069
the token is missing, and so on.

725
00:44:10,569 --> 00:44:13,679
Interestingly, you can also
switch the other way in terms of

726
00:44:13,679 --> 00:44:16,409
simplifying your architecture.

727
00:44:16,709 --> 00:44:17,859
You can go functionless.

728
00:44:18,359 --> 00:44:21,849
So instead of having an API gateway
in front of a Lambda function, just

729
00:44:21,849 --> 00:44:25,939
so that you can make a call to, say,
DynamDB table, you can actually just

730
00:44:26,089 --> 00:44:30,579
remove the Lambda function and have API
gateway Integrate directly with, say,

731
00:44:31,079 --> 00:44:33,719
DynamDB by making a call to DynamDB.

732
00:44:33,719 --> 00:44:37,739
An API gateway can pretty much integrate
with every single other AWS service

733
00:44:38,279 --> 00:44:41,949
directly without needing a Lambda
function if all your function is doing

734
00:44:41,969 --> 00:44:45,059
is using the SDK to call another service.

735
00:44:45,989 --> 00:44:50,139
In this case, by removing the Lambda
function, you're going to remove any cold

736
00:44:50,149 --> 00:44:54,519
start performance hits that's associated
with having that Lambda function, as

737
00:44:54,519 --> 00:44:59,419
well as any costs related to the Lambda
invocation, the duration, and so on.

738
00:44:59,969 --> 00:45:02,249
You can actually go pretty
far with this approach.

739
00:45:02,439 --> 00:45:05,219
for example, you know, once
your API gateway, writes

740
00:45:05,219 --> 00:45:06,889
the, Some data to Dynam db.

741
00:45:07,099 --> 00:45:10,609
You can also use the event bridge
pipes to capture the data changes

742
00:45:10,849 --> 00:45:14,779
and then send them as event
to event bridge, to event bus.

743
00:45:15,019 --> 00:45:19,339
And, those are gonna be dynam
DB events for data inserted,

744
00:45:19,369 --> 00:45:20,959
updated, removed, and so on.

745
00:45:21,229 --> 00:45:24,499
They're not really domain events that
specific to your business domain.

746
00:45:24,499 --> 00:45:28,729
And so what you can do in that case is
use a land function to transform the

747
00:45:28,729 --> 00:45:33,159
event payload so that you can turn,
say, like an insert event into, I

748
00:45:33,159 --> 00:45:37,439
don't know, user created event that's
more akin to your business domain.

749
00:45:37,759 --> 00:45:40,939
So, and, a good, adage that we had
at the start of the whole serverless

750
00:45:40,979 --> 00:45:45,349
movement was to use Lambda functions
to transform data, not to transport

751
00:45:45,359 --> 00:45:46,999
data from one place to another.

752
00:45:47,599 --> 00:45:50,639
So in that case, you know, if you've
got data that's in DynamoDB that you

753
00:45:50,639 --> 00:45:54,699
want to make searchable with OpenSearch,
you can also do that nowadays without

754
00:45:54,709 --> 00:45:56,019
having to write a custom code.

755
00:45:56,109 --> 00:45:56,949
Lambda function.

756
00:45:57,119 --> 00:46:01,479
You can use the no code ETL process
now to synchronize data from DynamoDB

757
00:46:01,489 --> 00:46:04,469
to OpenSearch to make it searchable.

758
00:46:04,969 --> 00:46:07,159
And it's not just API
gateway that can do this.

759
00:46:07,169 --> 00:46:11,729
You can also use AppSync and use the
step functions, which can all integrate

760
00:46:11,729 --> 00:46:16,139
with other AWS services directly
without needing you to write custom

761
00:46:16,159 --> 00:46:20,619
code in the Lambda function, just to
make a SDK call to some other service.

762
00:46:21,179 --> 00:46:25,299
Again, Every single component in your
architecture should serve a purpose.

763
00:46:25,569 --> 00:46:28,889
If your lambda function is not doing
any business logic, and all it's

764
00:46:28,889 --> 00:46:32,969
doing is transporting data from one
place to another, and if the service

765
00:46:32,969 --> 00:46:36,189
that's sitting in front of the lambda
function can do that already, then

766
00:46:36,309 --> 00:46:39,509
you've got to think critically about
whether or not you need this lambda

767
00:46:39,509 --> 00:46:41,069
function to be there in the first place.

768
00:46:41,569 --> 00:46:45,109
Speaking of which, what if you
just let the front end talk

769
00:46:45,229 --> 00:46:47,559
directly to the AWS services?

770
00:46:47,769 --> 00:46:51,369
So instead of having API gateway
from Lambda and in front, and then

771
00:46:51,369 --> 00:46:54,369
in front of your down db table,
you can just have the front end

772
00:46:54,369 --> 00:46:56,459
talk to the down db table directly.

773
00:46:56,719 --> 00:46:59,729
And before you call me crazy,
you can actually do this securely

774
00:46:59,729 --> 00:47:01,179
and safely and still maintain.

775
00:47:01,484 --> 00:47:03,114
tenant isolation as well.

776
00:47:03,614 --> 00:47:07,734
The way you do that is, have the user
logged into, say, Cognito, and then

777
00:47:07,734 --> 00:47:12,024
exchange the Cognito issued token,
into some temporary AWS credentials

778
00:47:12,214 --> 00:47:14,194
using, Cognito identity pool.

779
00:47:14,854 --> 00:47:18,144
And then with the temporary credentials,
you can then allow the front end

780
00:47:18,284 --> 00:47:20,054
to talk to DynamoDB directly.

781
00:47:20,454 --> 00:47:24,664
And the trick here, or rather the,
the, the important part here is that

782
00:47:24,999 --> 00:47:29,439
In that IAM role that you, that you
configure with Cognito Identity Pool,

783
00:47:29,629 --> 00:47:34,829
you add this condition here, which
means that the issued credentials only

784
00:47:34,829 --> 00:47:40,269
allow the caller or the frontend to
interact with DynamoDB against where the

785
00:47:40,909 --> 00:47:48,369
primary key matches the subject in the
Cognito Identity or the token that has

786
00:47:48,369 --> 00:47:50,159
been issued by the Cognito User Pool.

787
00:47:50,389 --> 00:47:53,309
So that way, the frontend can
only interact with DynamoDB.

788
00:47:53,479 --> 00:47:58,669
With the user's own data, but not any
other users data, of course there's

789
00:47:58,669 --> 00:48:00,649
a lot of ways that this can go wrong.

790
00:48:00,709 --> 00:48:04,739
So this is not an approach that I
would advise for Everybody in fact

791
00:48:04,789 --> 00:48:08,329
for most people I would say don't
do this because that's Way more

792
00:48:08,329 --> 00:48:12,389
reason not to do this than there are
reason to do this Is that definitely

793
00:48:12,389 --> 00:48:16,739
falls into the high risk high reward
category in terms of things you can do?

794
00:48:17,239 --> 00:48:21,179
and the main reason to do this would be
you're building a side project or side

795
00:48:21,189 --> 00:48:26,169
hustle and You are really cost conscious
and you want to minimize your cost to

796
00:48:26,534 --> 00:48:31,014
Now as much as possible and so in that
case by removing all the API layer in

797
00:48:31,014 --> 00:48:35,094
front of your data store and allow the
front end talk to the database directly

798
00:48:35,124 --> 00:48:39,614
and the way that's safe and secure and
you know what you're doing then this is a

799
00:48:39,614 --> 00:48:41,884
potential solution that you can explore.

800
00:48:42,384 --> 00:48:46,144
So that's 12 different things you can
do or start different ways to save

801
00:48:46,304 --> 00:48:50,254
money, or rather cutting out waste
in your serverless architecture.

802
00:48:50,514 --> 00:48:53,234
And if you want to learn how
to actually build a serverless

803
00:48:53,234 --> 00:48:56,284
architecture that's production
worthy, then check out my upcoming

804
00:48:56,284 --> 00:48:58,284
workshop, production ready serverless.

805
00:48:58,454 --> 00:49:02,444
Which has just started and you can still
sign up now for the next couple of days.

806
00:49:02,714 --> 00:49:05,214
And if you've got any questions,
please feel free to reach out to

807
00:49:05,214 --> 00:49:06,894
me and respond to the comment.

808
00:49:07,124 --> 00:49:09,254
And I'll try to answer your
questions as much as possible.

809
00:49:09,504 --> 00:49:12,444
Okay, thank you so much for your time
and enjoy the rest of the conference.

810
00:49:12,484 --> 00:49:13,184
Okay, bye bye.

