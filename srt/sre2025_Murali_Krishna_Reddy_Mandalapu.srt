1
00:00:00,660 --> 00:00:01,320
Hello everyone.

2
00:00:01,860 --> 00:00:02,430
Good morning.

3
00:00:02,760 --> 00:00:03,390
Good evening.

4
00:00:03,890 --> 00:00:05,060
My name is Murli Kdi Manou.

5
00:00:05,560 --> 00:00:09,700
Welcome to Con 42 Site
level Engineering 2025.

6
00:00:10,200 --> 00:00:15,809
Today we'll talk about scalable
interconnect strategies for

7
00:00:15,809 --> 00:00:18,269
GPU accelerated HPC clusters,

8
00:00:18,769 --> 00:00:19,944
GPU Interconnect Challenge.

9
00:00:20,444 --> 00:00:24,674
The problem, GPU computational
capacity has evolved exponentially

10
00:00:25,305 --> 00:00:29,685
outpacing interconnect bandwidth
improvements by three to four

11
00:00:29,685 --> 00:00:31,515
times per hardware generation.

12
00:00:32,265 --> 00:00:36,765
This widening gap creates severe
performance bottleneck in large scale

13
00:00:36,765 --> 00:00:42,045
distributor systems limiting the effective
throughput of multi GPU computations.

14
00:00:42,545 --> 00:00:45,905
So how does it impact in
modern HPC applications?

15
00:00:46,775 --> 00:00:48,095
Interconnect latency.

16
00:00:48,305 --> 00:00:52,925
A network congestion can consume 30
to 50% of the total execution time.

17
00:00:53,825 --> 00:00:57,785
The substantial overhead persist
even in meticulously optimized

18
00:00:57,785 --> 00:01:02,555
computational computation intensity
workloads significantly reducing

19
00:01:02,555 --> 00:01:03,875
overall system efficiency.

20
00:01:04,375 --> 00:01:06,265
So what do we need to do next?

21
00:01:06,265 --> 00:01:11,095
Generation solutions must address
three fundamental challenges,

22
00:01:11,845 --> 00:01:14,274
bandwidth saturation at extreme scale.

23
00:01:14,919 --> 00:01:19,839
Topology aware routing inefficiencies
and the substantial synchronization

24
00:01:19,839 --> 00:01:25,270
overhead of collective operations
across thousands of distributed GPUs.

25
00:01:25,770 --> 00:01:29,460
Let's look at some of the
traditional interconnect limitations.

26
00:01:29,960 --> 00:01:32,190
First, bandwidth saturation.

27
00:01:32,690 --> 00:01:39,500
GPUs generate data at rates that
overwhelm network capacity causing.

28
00:01:39,995 --> 00:01:44,645
Memory buffer congestion and forcing
computational pipelines to stall

29
00:01:44,795 --> 00:01:47,495
across distributed compute nodes.

30
00:01:47,995 --> 00:01:50,935
The second routing inefficiencies.

31
00:01:51,435 --> 00:01:55,785
Static routing protocols cannot
adapt to realtime network congestion,

32
00:01:56,385 --> 00:02:00,525
creating traffic bottlenecks,
enforcing data through suboptimal

33
00:02:00,525 --> 00:02:02,985
paths during high throughput workloads.

34
00:02:03,485 --> 00:02:05,390
Third synchronization overhead.

35
00:02:05,890 --> 00:02:11,230
Multi GPU collective operations require
precise barrier synchronization,

36
00:02:11,890 --> 00:02:17,740
where even nanosecond latency
variations compound exponentially,

37
00:02:18,240 --> 00:02:23,070
severely degrading performance as
systems scale to thousands of nodes.

38
00:02:23,570 --> 00:02:27,595
Let's dive into some of the top
policy innovations that are.

39
00:02:28,295 --> 00:02:31,934
In the marketplace First
Factory architecture.

40
00:02:32,534 --> 00:02:38,144
This network topology architecture
delivers non-blocking communication

41
00:02:38,384 --> 00:02:42,784
with full BI section and bandwidth
and deterministic latency scales

42
00:02:42,814 --> 00:02:47,224
effectively to thousands of nodes,
but requires exponentially increasing

43
00:02:47,244 --> 00:02:49,784
switch count at higher radis.

44
00:02:50,284 --> 00:02:52,354
3D Taurus Network topology.

45
00:02:52,854 --> 00:02:57,504
This topology implements a mesh like
structure with wrapped around connections,

46
00:02:57,924 --> 00:03:02,874
minimizing I wiring complexity
while maintaining low hop counts.

47
00:03:03,804 --> 00:03:06,624
Optimized for nearest neighbor
communication patterns.

48
00:03:07,374 --> 00:03:11,809
Common in physics simulations,
dragonfly network topology.

49
00:03:12,649 --> 00:03:13,884
This topology leverages.

50
00:03:14,569 --> 00:03:20,539
Hierarchical organization with high
erratics routers to minimize network

51
00:03:20,539 --> 00:03:26,499
diameter and cable length achieves
near optimal trade off between local

52
00:03:26,499 --> 00:03:30,429
and global bandwidth while reducing
the cost and power consumption.

53
00:03:30,929 --> 00:03:35,774
Let's understand RDMA
direct access efficiency.

54
00:03:36,274 --> 00:03:42,695
RDMA enables CPU bypass, which enables
direct memory to memory transfers

55
00:03:43,114 --> 00:03:48,294
across the network fabric without
CPU intervention, which dramatically

56
00:03:48,294 --> 00:03:53,185
re reducing crossing overhead
and system resource consumption.

57
00:03:53,685 --> 00:03:55,844
This also enables low latency.

58
00:03:56,344 --> 00:04:00,844
Achieves up to 55% reduction in
end-to-end communication latency,

59
00:04:01,565 --> 00:04:06,125
allowing near instantaneous data
sharing between distributed GP nodes

60
00:04:06,425 --> 00:04:08,794
in computing intensive applications.

61
00:04:09,294 --> 00:04:15,404
This also provides higher efficiency,
which delivers 97% protocol efficiency

62
00:04:15,404 --> 00:04:20,074
for medium sized message transfers,
virtually eliminating network overhead.

63
00:04:20,119 --> 00:04:25,399
And maximizing effective bandwidth
utilization across the cluster.

64
00:04:25,899 --> 00:04:27,579
Let's look at some case studies.

65
00:04:28,079 --> 00:04:31,099
First one, frontier supercomputer,

66
00:04:31,599 --> 00:04:37,624
20 point 24.8% gain was realized
by dynamic adaptive routing

67
00:04:37,629 --> 00:04:42,059
algorithms significantly outperformed
traditional static approaches.

68
00:04:42,574 --> 00:04:44,614
In high traffic simulation workloads.

69
00:04:45,114 --> 00:04:49,754
This also uses slingshot interconnect
proprietary congestion control mechanisms

70
00:04:50,254 --> 00:04:55,354
dynamically adjust packet priorities
and pathways to maintain optical

71
00:04:55,954 --> 00:04:58,684
optimal throughput under extreme loads.

72
00:04:59,184 --> 00:05:04,134
This also employs dragon fry to
policy as we reviewed previous slides.

73
00:05:04,634 --> 00:05:09,254
Enhanced network architecture
delivers near uniform global bandwidth

74
00:05:09,254 --> 00:05:14,684
distribution with minimal diameter,
reducing worst case latency by 37%.

75
00:05:15,184 --> 00:05:18,280
Let's look at another case
study, which is Nvidia Seline.

76
00:05:18,780 --> 00:05:21,020
This uses Infinity Band, HDR.

77
00:05:21,520 --> 00:05:25,385
This is revolutionary to 200
gigabit per second interconnect.

78
00:05:26,170 --> 00:05:29,950
Between compute nodes with full
bisection bandwidth architecture,

79
00:05:30,880 --> 00:05:34,690
eliminating network congestion and
ensuring seamless parallel communication.

80
00:05:35,190 --> 00:05:40,890
This also implements R-D-A-R-D-M-A,
which delivers exceptional 97%

81
00:05:40,890 --> 00:05:45,330
protocol efficiency for medium-sized
data transfers while slashing

82
00:05:45,510 --> 00:05:47,730
end-to-end latency by up to 55%.

83
00:05:48,230 --> 00:05:51,500
Dramatically outperforming
conventional networking approaches.

84
00:05:52,000 --> 00:05:54,940
This also implements NCCL optimization.

85
00:05:55,440 --> 00:06:00,570
Precision engineered GPU to GPU collective
operations maximize throughput with

86
00:06:00,840 --> 00:06:05,670
advanced topology aware algorithms
that intelligently adopt communication

87
00:06:05,670 --> 00:06:08,010
patterns based on workload demands.

88
00:06:08,510 --> 00:06:08,690
Okay.

89
00:06:08,960 --> 00:06:13,400
With that, let's get into some of
the software optimization strategies

90
00:06:13,900 --> 00:06:15,670
that are being used in the industry.

91
00:06:16,170 --> 00:06:20,550
First one, adaptive routing
intelligently reconfigures network

92
00:06:20,550 --> 00:06:26,820
pathways in real time based on traffic
analysis, reducing congestion by up

93
00:06:26,820 --> 00:06:31,530
to 40% and delivering sub microsecond
latency across complex workloads.

94
00:06:31,590 --> 00:06:31,620
Okay.

95
00:06:32,120 --> 00:06:37,190
Topology aware algorithms,
sophisticated communication frameworks

96
00:06:37,670 --> 00:06:42,140
that precisely map data, exchange
patterns to the physical network

97
00:06:42,140 --> 00:06:49,670
architecture, reducing network diameter,
traversals by 60%, and MA minimizing

98
00:06:49,670 --> 00:06:51,980
cross switch traffic overhead.

99
00:06:52,480 --> 00:06:57,460
N-C-C-L-Q, advanced customization
of GPU collective operations.

100
00:06:57,900 --> 00:07:02,910
That orchestrates communication patterns
with nanosecond precision, eliminating

101
00:07:02,910 --> 00:07:07,650
t transfers and achieving near the
theoretical bandwidth utilization

102
00:07:08,150 --> 00:07:12,461
load, balancing sophisticated
traffic distribution algorithms that

103
00:07:12,461 --> 00:07:18,181
dynamically allocate bandwidth across
multiple pathways, preventing resource

104
00:07:18,181 --> 00:07:22,921
contention and maintaining consistent
95% plus throughput efficiency.

105
00:07:23,566 --> 00:07:25,635
Under extreme computational demands

106
00:07:26,135 --> 00:07:31,555
future interconnect technologies
that are evolving integrated

107
00:07:31,555 --> 00:07:32,845
network processing units.

108
00:07:33,685 --> 00:07:37,876
These are purpose built silicon accelerate
packet processing and routing operations

109
00:07:37,895 --> 00:07:42,336
at line rate, complete off offloading
of communication protocols from GPUs.

110
00:07:42,836 --> 00:07:46,496
This liberates computational
resources for core workloads.

111
00:07:46,996 --> 00:07:52,545
Photonic interconnects, silicone photonics
enables multi tera data transfers using

112
00:07:52,635 --> 00:07:58,126
wavelength, using multiplexing power
consumption decreases by 65% compared to

113
00:07:58,126 --> 00:08:02,925
the electrical interconnects while sub
nanosecond latencies become achievable.

114
00:08:03,425 --> 00:08:07,925
Third in package integration,
high bandwidth network interfaces.

115
00:08:08,270 --> 00:08:11,720
Co-pack with the GPU substrate
using advanced chip plate

116
00:08:11,720 --> 00:08:16,881
architectures, drastically reduced
signal paths, minimize propagation

117
00:08:16,881 --> 00:08:20,720
delays, and unlock unprecedented
GPU to network through output.

118
00:08:21,220 --> 00:08:24,490
Let's look at some of the
performance metrics and benchmarks.

119
00:08:24,990 --> 00:08:29,005
Traditional ethernet maximum
is 12.5 gigabytes per second.

120
00:08:29,790 --> 00:08:31,110
End to end latency.

121
00:08:31,680 --> 00:08:32,700
10 microseconds.

122
00:08:33,120 --> 00:08:36,060
Peak performance utilization, 65%.

123
00:08:36,560 --> 00:08:37,370
InfiniBand, HDR.

124
00:08:37,970 --> 00:08:40,310
Maximum dropout is 25
gigabytes per second.

125
00:08:40,550 --> 00:08:43,130
End-to-end latency 3.5 microseconds.

126
00:08:43,700 --> 00:08:49,060
Peak performance utilization
is 85% NVIDIA and NVLink.

127
00:08:49,560 --> 00:08:51,900
Maximum throughput is
50 gigabytes per second.

128
00:08:52,110 --> 00:08:55,080
End-to-end latency, 1.8 microseconds.

129
00:08:55,740 --> 00:08:57,150
Peak performance utilization.

130
00:08:57,946 --> 00:08:58,906
93%

131
00:08:59,406 --> 00:09:00,876
future photonics.

132
00:09:01,146 --> 00:09:04,506
Maximum throughput is a hundred
gigabytes per second end-to-end

133
00:09:04,506 --> 00:09:06,425
latency 0.5 microseconds.

134
00:09:06,846 --> 00:09:09,621
Peak performance utilization up to 98%.

135
00:09:10,121 --> 00:09:13,361
Let's review key takeaways and next steps.

136
00:09:13,861 --> 00:09:15,991
Analyze your application
communication patterns.

137
00:09:16,651 --> 00:09:20,911
Conduct comprehensive workload profiling
to identify critical data, movement,

138
00:09:20,911 --> 00:09:23,101
bottlenecks and communication hotspots.

139
00:09:23,601 --> 00:09:27,261
Select appropriate network
topology, strategically align

140
00:09:27,261 --> 00:09:30,741
infrastructure investments with
specific application requirements to

141
00:09:30,741 --> 00:09:32,721
maximize performance to cost ratio.

142
00:09:33,221 --> 00:09:37,391
Implement software optimizations,
fine tune collective communication

143
00:09:37,391 --> 00:09:41,591
operations specifically for
your network architecture.

144
00:09:41,651 --> 00:09:47,611
To eliminate redundant data transfers,
pre prepare for emerging technologies,

145
00:09:48,111 --> 00:09:52,881
develop modular systems and abstraction
layers that can seamlessly incorporate

146
00:09:52,941 --> 00:09:54,921
next generation interconnect advancements.

147
00:09:55,421 --> 00:09:55,772
Thank you.

148
00:09:56,272 --> 00:09:59,842
If you have any questions, yeah,
please reach out to me on my LinkedIn.

