1
00:00:00,500 --> 00:00:02,929
Hi, this is Kisha Kumar IPOing.

2
00:00:03,620 --> 00:00:06,529
I'm a CPQ Solution Architect.

3
00:00:06,830 --> 00:00:12,080
I have more than 15 years of experience
in building the CPK applications

4
00:00:12,080 --> 00:00:14,200
for various enterprise orgs.

5
00:00:14,230 --> 00:00:14,260
Okay.

6
00:00:15,010 --> 00:00:20,020
And I worked in the very fortune
companies led to the manufacturing

7
00:00:20,020 --> 00:00:25,435
semiconductor and also the energy
and power utility services built.

8
00:00:26,200 --> 00:00:32,199
The CPQ ecosystem and helped in generation
of the proposal, quotation and complex

9
00:00:32,199 --> 00:00:34,360
pricing related solutions to them.

10
00:00:35,110 --> 00:00:41,559
So coming to this today topic, where I
was trying to present to the external

11
00:00:41,559 --> 00:00:48,279
audience is how can we utilize the
CPQ application or the CPQ systems

12
00:00:48,339 --> 00:00:51,040
by using the AI powered platforms?

13
00:00:51,610 --> 00:00:55,840
The reason being now, nowadays,
we keep hearing everywhere the AI

14
00:00:56,110 --> 00:00:58,090
implement, the AI in every technology.

15
00:00:58,570 --> 00:01:03,430
So that's where we are trying
to see, hey, how can we leverage

16
00:01:04,000 --> 00:01:06,910
the AI tools to the CPQ?

17
00:01:06,910 --> 00:01:11,230
How, what is the benefits that could
be bring up to the CPQ applications

18
00:01:11,440 --> 00:01:16,900
by using the AI platform, having the
microservices architecture kind of stuff.

19
00:01:17,110 --> 00:01:18,730
So we'll go in detail.

20
00:01:19,195 --> 00:01:20,155
How it is going.

21
00:01:20,255 --> 00:01:24,664
How it looks like, how the
deployments, how the configurations,

22
00:01:24,664 --> 00:01:28,895
what is the ROI we get it from, be
utilizing this corresponding feature.

23
00:01:29,464 --> 00:01:30,904
And what are the current challenges?

24
00:01:30,904 --> 00:01:37,335
Use the, the implementations and also
the sales guys, how issues in generating

25
00:01:37,335 --> 00:01:42,225
the quotations, getting the dynamic
pricing, the complex configuration,

26
00:01:42,225 --> 00:01:44,324
taking how much time for them to bill.

27
00:01:44,714 --> 00:01:49,815
All this information we encapsulated
in such a way by giving a

28
00:01:49,815 --> 00:01:54,734
microservices kind of an operation
by using the AI as a base platform.

29
00:01:55,695 --> 00:01:57,884
Okay, so coming to this one, right?

30
00:01:57,884 --> 00:02:01,764
So as I mentioned, a system process
in this particular architecture

31
00:02:01,764 --> 00:02:09,495
standpoint that a hundred care requests
monthly could be coming up with 99.9%.

32
00:02:09,524 --> 00:02:10,785
There will be an uptime means.

33
00:02:11,310 --> 00:02:12,450
There are no issue.

34
00:02:12,464 --> 00:02:19,195
The, the key complex AI workloads
will be at the enterprise level scale.

35
00:02:19,614 --> 00:02:23,274
We have engineered the AI in such a way
that even though you've tried to hit

36
00:02:23,364 --> 00:02:27,834
thousand K per requests immediately,
the trained models will able to

37
00:02:27,834 --> 00:02:32,304
generate the right configuration with
accuracy, pricing, and accuracy coating.

38
00:02:32,515 --> 00:02:37,224
So that's where this corresponding
architecture, the tool will looks like.

39
00:02:37,724 --> 00:02:40,094
So going to the next slide.

40
00:02:40,594 --> 00:02:40,804
Okay.

41
00:02:41,344 --> 00:02:43,624
So as I mentioned there's a interaction.

42
00:02:43,624 --> 00:02:48,994
So CPQ has much more demand
coming to this CPQ means its

43
00:02:48,994 --> 00:02:51,454
configuration, pricing, and coating.

44
00:02:51,994 --> 00:02:56,014
So how can we do this is complex
configurations with the real time

45
00:02:56,014 --> 00:03:00,334
data I know by using the real
time data, configure the products,

46
00:03:00,544 --> 00:03:02,044
accurate pricing decisions.

47
00:03:02,569 --> 00:03:05,929
And when, if you try to give more
discounts, the approval process

48
00:03:05,929 --> 00:03:07,669
should be in the dynamic nature.

49
00:03:07,999 --> 00:03:12,639
Hey, this is the, most of the customers
are asking this much of discount.

50
00:03:12,669 --> 00:03:15,789
Okay, we can able to approve this
particular code with the same discount.

51
00:03:16,029 --> 00:03:20,229
So those kind of a training model
and the dynamic and the forecasting.

52
00:03:20,259 --> 00:03:23,199
Okay, these are the list of
codes within the quarter.

53
00:03:23,229 --> 00:03:24,849
How can we forecast that one?

54
00:03:25,089 --> 00:03:26,619
What is the expected revenue?

55
00:03:27,459 --> 00:03:30,159
And when I were trying to use
a, trying to configure a model.

56
00:03:30,659 --> 00:03:32,429
We do an upselling kind of a thing.

57
00:03:32,429 --> 00:03:36,269
Hey guy, our guided selling, Hey, if
you try to buy this particular product,

58
00:03:36,299 --> 00:03:40,499
okay, so you get this particular product
with certain price, with less price.

59
00:03:40,679 --> 00:03:44,219
If you combine into a bundle, you
have much more discounts a thing.

60
00:03:44,759 --> 00:03:50,649
But this is all like, and the air will
train the corresponding backend system and

61
00:03:50,649 --> 00:03:54,909
suggest the user saying that, Hey, this
is the configuration you're trying to do.

62
00:03:55,209 --> 00:03:57,999
Most of the customers are trying
to buy this corresponding product.

63
00:03:58,524 --> 00:04:00,504
And also you'll save this much of amount.

64
00:04:00,684 --> 00:04:03,954
So these kind of ai, there are
microservices behind the scenes,

65
00:04:04,194 --> 00:04:08,695
which will try to solve and help you
in getting the right configuration

66
00:04:08,695 --> 00:04:09,834
with the right pricing too.

67
00:04:10,464 --> 00:04:14,274
And coming to this, the response
time by building this microservices

68
00:04:14,274 --> 00:04:16,404
is, was 200 milliseconds.

69
00:04:16,404 --> 00:04:19,570
It's not even 0.2 second
in the response time.

70
00:04:20,070 --> 00:04:25,290
And we address by utilizing this
corresponding platform technology, we

71
00:04:25,290 --> 00:04:29,550
address the challenges like, okay, what
are the providing the abstractions,

72
00:04:29,550 --> 00:04:33,960
the tooling necessary to deploy the
manage the AI workloads at a scale?

73
00:04:34,410 --> 00:04:39,180
And this particular article presence,
the approach, how can we build the

74
00:04:39,180 --> 00:04:44,790
microservices based CPQ platform that
seamlessly integrate with enterprise grade

75
00:04:44,790 --> 00:04:48,795
reliability and developer productivity
means this microservice can be plugin.

76
00:04:49,560 --> 00:04:53,760
And within that AI tools, and
then that will be trained and

77
00:04:53,760 --> 00:04:58,950
get utilized by the CPQ platform
for the better coding process.

78
00:04:59,450 --> 00:05:01,400
So here is a high level architecture.

79
00:05:01,550 --> 00:05:05,030
So one is the user request
means, for example, users trying

80
00:05:05,030 --> 00:05:06,650
to input some user request.

81
00:05:06,680 --> 00:05:06,920
Okay?

82
00:05:06,950 --> 00:05:10,340
Capturing the configuration,
input, whatever he's trying to want

83
00:05:10,340 --> 00:05:12,005
to buy, he select those inputs.

84
00:05:12,710 --> 00:05:13,310
And everything.

85
00:05:13,310 --> 00:05:16,580
After selecting those
configuration, whatever is required.

86
00:05:16,909 --> 00:05:20,510
So what happen, it'll go to
call an API, which is like a

87
00:05:20,510 --> 00:05:26,200
background app, API interface
that will call the microservices.

88
00:05:26,500 --> 00:05:30,099
Microservices already build in
such a way that it take the mission

89
00:05:30,099 --> 00:05:34,539
learning languages, AI tools,
everything will betray, generate

90
00:05:34,539 --> 00:05:36,550
the product rules and the data.

91
00:05:36,610 --> 00:05:39,695
So for example, configuration,
it'll generate hey.

92
00:05:39,984 --> 00:05:41,634
For this configuration.

93
00:05:41,634 --> 00:05:45,514
These are the, bill of materials, we
can say it or it could be a product.

94
00:05:45,904 --> 00:05:48,064
Those are the products
that could be offered.

95
00:05:48,364 --> 00:05:51,754
And also this is a price
that could be optimal.

96
00:05:51,754 --> 00:05:54,424
Price also will be generated
for the users so that they can

97
00:05:54,424 --> 00:05:56,554
able to generate a quotation.

98
00:05:56,704 --> 00:06:00,224
So it is the, it's all
included in the microservices.

99
00:06:00,809 --> 00:06:04,619
The way of getting the request from
the user, it will orchestrate and

100
00:06:04,619 --> 00:06:09,179
give a flexibility and to, and for the
enterprise work, I mean ensuring that

101
00:06:09,569 --> 00:06:11,369
we are handling the ING mechanism.

102
00:06:11,369 --> 00:06:16,469
We are handling the multiple request,
multi treading, all those interfaces,

103
00:06:16,769 --> 00:06:20,460
which will again interact with the
microservices AI interface services and

104
00:06:20,460 --> 00:06:22,619
supporting the infrastructure components.

105
00:06:23,340 --> 00:06:28,530
And we do have a synchronous API calls and
also the synchronous event, streamlining

106
00:06:28,590 --> 00:06:30,150
on the case by case requirements.

107
00:06:30,629 --> 00:06:35,729
Real time ca pricing calculations
and also ensure there's no latency

108
00:06:35,939 --> 00:06:38,849
and that sometimes it requires a
batch processing and model training.

109
00:06:39,239 --> 00:06:42,929
Real time model training will
also be taken care as part of this

110
00:06:42,929 --> 00:06:47,549
overall architecture standpoint
and next to this one, right?

111
00:06:47,549 --> 00:06:52,590
If it is about the AI and the machine
learning, the service integration.

112
00:06:52,949 --> 00:06:57,989
So what is do is we do have three level of
architecture of the integration pattern.

113
00:06:58,379 --> 00:07:02,340
One is a gradient Boosting engines means
it's handling the pricing observation

114
00:07:02,699 --> 00:07:05,969
with a standard API interfaces.

115
00:07:06,210 --> 00:07:07,590
The second is the neural network.

116
00:07:07,590 --> 00:07:11,129
It's a power configuration recommendations
to dedicated microservices.

117
00:07:11,549 --> 00:07:12,840
The third one is the real time.

118
00:07:12,840 --> 00:07:15,499
Real time is nothing but okay,
having the information, so

119
00:07:15,499 --> 00:07:19,219
dynamic prices, adjustments
based on the market conditions.

120
00:07:19,489 --> 00:07:23,479
So based upon, for example, if
you're trying the module of the

121
00:07:23,479 --> 00:07:25,460
train model will suggest a price.

122
00:07:25,794 --> 00:07:31,129
At the same time, it'll interact with the
several services microservices in the on

123
00:07:31,129 --> 00:07:34,009
demand, on the current market conditions.

124
00:07:34,099 --> 00:07:38,659
And it'll also adjust the pricing,
say pricing dynamic pricing.

125
00:07:39,049 --> 00:07:42,469
So it'll, it's not only the
easy infrastructure is not

126
00:07:42,469 --> 00:07:43,879
just for each static thing.

127
00:07:44,359 --> 00:07:47,959
It can be leveraged to any industry
standard frameworks, including the

128
00:07:47,989 --> 00:07:51,919
tens of flow serving mission flow,
and the customer interface service

129
00:07:51,919 --> 00:07:53,329
builds that kind of information.

130
00:07:54,124 --> 00:07:57,335
So the data science team
will to choose the when.

131
00:07:57,335 --> 00:08:00,184
Now we are trying to sell this
corresponding product of the architecture.

132
00:08:00,184 --> 00:08:03,954
The data science team can choose
the appropriate serving solutions

133
00:08:04,254 --> 00:08:07,524
for the specific model types and
the performance requirements.

134
00:08:07,704 --> 00:08:10,914
So where they can also provide
a unified monitoring and

135
00:08:10,914 --> 00:08:16,134
management CAPA capabilities
across the all serving frameworks,

136
00:08:16,134 --> 00:08:17,584
ensuring the consistency in the.

137
00:08:18,294 --> 00:08:21,294
Operational practices regardless
of underlying technology.

138
00:08:21,594 --> 00:08:26,664
So it really comes as a package, as a
service so that this microservices can

139
00:08:26,664 --> 00:08:31,704
be utilized to any industry framework
is what the high level, the integration

140
00:08:32,005 --> 00:08:33,145
standpoint we are trying to build.

141
00:08:33,745 --> 00:08:38,954
That is the microservices service
integration and coming to this

142
00:08:38,985 --> 00:08:40,454
event driven communication band.

143
00:08:40,515 --> 00:08:43,110
The next level is about, one is we
talk about the integration patterns.

144
00:08:44,084 --> 00:08:47,774
And ML thing, there's an even
based, so we now we're trying to,

145
00:08:47,834 --> 00:08:51,164
okay, there's some triggers or the
even driven based architecture.

146
00:08:51,164 --> 00:08:53,324
We now there some changes is happening.

147
00:08:53,684 --> 00:08:57,514
So then lose couple, so for example,
the end up data is not getting it.

148
00:08:57,514 --> 00:09:02,014
So you lose couple of between the services
while maintaining the data consistency and

149
00:09:02,014 --> 00:09:04,234
also enabling the real time approaches.

150
00:09:04,354 --> 00:09:08,254
So we use the car Kafka serves
as a central event, maintaining

151
00:09:08,524 --> 00:09:11,999
handles the millions of pricing
events so that it's event driven.

152
00:09:12,619 --> 00:09:16,639
So a lot of pricing, even when now we
try to hit the request, the millions

153
00:09:16,639 --> 00:09:20,869
of pricing events will directly hit
to the Kafka services and standardize

154
00:09:20,869 --> 00:09:25,759
the schema, enabling the backward
compatibility as a platform evolve, so the

155
00:09:25,759 --> 00:09:28,279
events also be taken into consideration.

156
00:09:28,549 --> 00:09:33,679
So it's keep on evolving the microservices
and also the get the data, train

157
00:09:33,679 --> 00:09:35,839
the model, all those information.

158
00:09:36,169 --> 00:09:39,289
So even and sourcing the caps, the
complete history of the pricing

159
00:09:39,289 --> 00:09:41,599
decisions, the configuration changes.

160
00:09:41,974 --> 00:09:47,074
And provides then audit trails and then
also, okay, what type of price happened in

161
00:09:47,134 --> 00:09:51,484
past, like last few days and what is the
history of the overall pricing and okay,

162
00:09:51,484 --> 00:09:55,054
what type of products configured by the
users in the order the period after time.

163
00:09:55,654 --> 00:10:00,364
All those audit trails will also be
available and provide for the analytics,

164
00:10:01,264 --> 00:10:05,404
this events to maintains the immediate
records, debugging, compliance,

165
00:10:05,404 --> 00:10:07,444
analytical purpose, all those information.

166
00:10:07,969 --> 00:10:11,209
So this highly, we are trying
to talk about the event driven

167
00:10:11,209 --> 00:10:15,769
communication patterns and coming
to the next one is the what.

168
00:10:16,339 --> 00:10:16,549
Okay.

169
00:10:16,549 --> 00:10:21,119
We, what is how we can, what is
infrastructure, how, what is a

170
00:10:21,119 --> 00:10:24,959
deployment cycle looks like on
this particular microservices?

171
00:10:25,019 --> 00:10:27,944
One is this is a multi-cloud
infrastructure management.

172
00:10:28,544 --> 00:10:31,364
So Terraform infrastructure as a code.

173
00:10:31,454 --> 00:10:31,724
Okay.

174
00:10:31,724 --> 00:10:33,014
That is one structure.

175
00:10:33,284 --> 00:10:35,624
The other one is about the
cross cloud networking.

176
00:10:35,894 --> 00:10:39,434
So coming to the Terraform is pretty
much like entire infrastructure with

177
00:10:39,434 --> 00:10:44,144
the modular configurations and different
cloud providers, including the AWS

178
00:10:44,534 --> 00:10:45,974
Azure and Google Cloud platform.

179
00:10:46,304 --> 00:10:49,184
So these are the standardized
patterns for the common components.

180
00:10:49,394 --> 00:10:52,664
It has a build in security
controls and also caught up cost

181
00:10:52,664 --> 00:10:53,794
optimization settings as well.

182
00:10:54,689 --> 00:10:58,739
And coming to the cross cloud
networking, it'll be a ME technology,

183
00:10:58,829 --> 00:11:03,389
so it to provide a secure, encrypted
communication between the services.

184
00:11:03,659 --> 00:11:08,369
It's an advanced traffic management,
candid deployments, and the secured

185
00:11:08,369 --> 00:11:10,089
braking and automatic tires.

186
00:11:10,269 --> 00:11:13,539
So we leverage both of these
infrastructures based upon,

187
00:11:13,589 --> 00:11:14,784
avoiding the vendor locking.

188
00:11:15,174 --> 00:11:17,424
Then this infrastructure code
undergoes the same rigorous

189
00:11:17,424 --> 00:11:19,494
review and also the testing.

190
00:11:19,809 --> 00:11:24,579
Have the ability process as application
code, ensuring the real reliability and

191
00:11:24,579 --> 00:11:26,499
the security at the infrastructure layer.

192
00:11:27,069 --> 00:11:29,649
So we use the multi-cloud
infrastructure management on the

193
00:11:29,649 --> 00:11:31,149
high level we talking about it.

194
00:11:31,649 --> 00:11:31,949
Yeah.

195
00:11:32,069 --> 00:11:34,889
Coming to the scalability and
the performance optimization.

196
00:11:34,889 --> 00:11:38,849
So any tool, anywhere
performance is very key.

197
00:11:39,059 --> 00:11:42,029
And also on the top of it, what
is a scalability standpoint?

198
00:11:42,419 --> 00:11:46,739
So it's not just okay, one time you buy
it or one year, one time you install it.

199
00:11:47,324 --> 00:11:51,134
We cannot, we need to ensure,
because now the world is keep on

200
00:11:51,134 --> 00:11:53,594
evolving, changing very, quite often.

201
00:11:53,954 --> 00:11:57,124
We don't want, okay,
this is the static thing.

202
00:11:57,124 --> 00:11:58,264
We cannot change anything.

203
00:11:58,474 --> 00:11:59,704
And the performance is like that.

204
00:11:59,704 --> 00:12:01,714
We cannot do improve the performance.

205
00:12:01,744 --> 00:12:02,494
It's not like that.

206
00:12:02,494 --> 00:12:03,964
This is about the performance.

207
00:12:03,964 --> 00:12:09,124
We ensure that the services are
more scalable and keep on, it could

208
00:12:09,124 --> 00:12:12,934
be evolved as much as we can and
you can make the changes as, but

209
00:12:12,934 --> 00:12:15,019
the industry standards and all.

210
00:12:15,709 --> 00:12:20,839
And coming to these auto-scaling policies
respond to the both traditional metrics

211
00:12:20,899 --> 00:12:23,179
and also the AI specific indicators.

212
00:12:23,239 --> 00:12:23,509
Okay.

213
00:12:23,579 --> 00:12:27,599
And also horizontal ports, autoscaler
configurations, and machine learning

214
00:12:27,599 --> 00:12:31,649
services, including custom metrics,
exported from the models, serving the

215
00:12:31,649 --> 00:12:36,149
frameworks, ensuring the scaling decisions
reflect actual model performance.

216
00:12:36,569 --> 00:12:36,689
And.

217
00:12:37,380 --> 00:12:39,420
I end up coming to the
performance optimization.

218
00:12:39,420 --> 00:12:42,420
That is one of the key characteristics
without having the performance

219
00:12:42,420 --> 00:12:47,120
intelligence that we added to the way of
implements we do that is the intelligent

220
00:12:47,120 --> 00:12:50,540
node selection algorithm that's
considered both cost and performance

221
00:12:50,540 --> 00:12:52,850
characteristics when adding a capacity.

222
00:12:53,150 --> 00:12:57,830
So GPU enabled nodes are provision only
when neural network workloads require them

223
00:12:58,205 --> 00:13:02,300
optimizing the infrastructure course while
maintaining the perfor, while maintain the

224
00:13:02,300 --> 00:13:06,860
performance as a. Is and high level, it
is a scalable and performance optimized

225
00:13:06,860 --> 00:13:09,530
as well and coming to the deployments.

226
00:13:09,560 --> 00:13:12,645
Okay, so everyone knows we
do have a GI repository.

227
00:13:12,650 --> 00:13:17,090
Also, the Microsoft is can also be
handled by the GI repository and the

228
00:13:17,090 --> 00:13:22,130
configuration deployment manifests stored
in the gates and the bluegreen deployment.

229
00:13:22,250 --> 00:13:26,480
This is a new model version that is
fully bombed up so that you can have the

230
00:13:26,810 --> 00:13:31,070
algo series sync automatically syncs the
design state from gate with Accu actual.

231
00:13:31,760 --> 00:13:35,180
Cluster state, automated testing,
all those simple, the deployment

232
00:13:35,180 --> 00:13:39,469
pipelines, it has a zero time
down downtime for the deployments.

233
00:13:39,469 --> 00:13:42,469
For example, if you're trying to
enhance a new features to the M,

234
00:13:43,280 --> 00:13:44,750
you don't need quite a downtime.

235
00:13:45,500 --> 00:13:50,150
It'll, the pipeline is in such a way
that it will automatically disconnect

236
00:13:50,329 --> 00:13:51,739
and connect to another models.

237
00:13:52,099 --> 00:13:57,650
Inside that is a well sophisticatedly
connected platform and manages

238
00:13:57,650 --> 00:13:59,150
the traffic also be shifted.

239
00:13:59,525 --> 00:14:04,535
From one tric to another metrics and
detect any degradation is happening.

240
00:14:04,535 --> 00:14:08,074
The performance in the model
performance, it'll alert and say

241
00:14:08,074 --> 00:14:09,665
that, Hey, there's some issues.

242
00:14:09,665 --> 00:14:10,594
We have to take a look.

243
00:14:10,895 --> 00:14:13,415
So it was very strongly connected.

244
00:14:13,474 --> 00:14:16,415
GI ops and the continuous deployment.

245
00:14:16,915 --> 00:14:17,204
Okay.

246
00:14:17,639 --> 00:14:20,040
The second thing is the developer
experience and tooling, right?

247
00:14:20,040 --> 00:14:22,889
There's the one is the deployments,
which we already discussed.

248
00:14:22,949 --> 00:14:24,989
What should be the developer experience?

249
00:14:25,500 --> 00:14:28,800
What type of APIs that will be
supported for the microservices

250
00:14:28,800 --> 00:14:31,555
and SDKs, so standardized APIs.

251
00:14:31,560 --> 00:14:32,970
Okay, so there are two types.

252
00:14:33,030 --> 00:14:36,570
Typically we look, one is the
standardized and the custom APIs.

253
00:14:36,600 --> 00:14:40,680
But by using the self services,
APIs provide the standard interfaces

254
00:14:40,680 --> 00:14:44,190
for common operations like model
deployment and also the feature more

255
00:14:44,850 --> 00:14:46,560
engineering and performance monitoring.

256
00:14:46,950 --> 00:14:50,910
These APIs will extract
abstract the complexity of

257
00:14:50,910 --> 00:14:52,230
the underlying infrastructure.

258
00:14:52,650 --> 00:14:55,290
Allowing the developers to
focus on business logic rather

259
00:14:55,290 --> 00:14:57,780
than the operational concepts.

260
00:14:58,170 --> 00:15:01,230
And the other one is the
software development kits.

261
00:15:01,469 --> 00:15:05,790
I like the Python, Java, and
GoPro idea medic interfaces for

262
00:15:05,790 --> 00:15:07,319
interacting the platform services.

263
00:15:07,380 --> 00:15:10,439
STK can also used data scientist tooling.

264
00:15:10,740 --> 00:15:14,400
So the other one is, we know
this APIs, we know the about the

265
00:15:14,400 --> 00:15:15,959
languages, and then the data.

266
00:15:16,285 --> 00:15:19,980
Data is data scientist that will be,
these environments helps libraries.

267
00:15:20,430 --> 00:15:24,990
The common tasks of feature extraction,
model evaluation, and the deployments.

268
00:15:25,290 --> 00:15:29,940
This is about the developer experience
and the tools we use coming to that once

269
00:15:29,940 --> 00:15:31,180
we develop, the other thing is okay.

270
00:15:31,770 --> 00:15:33,689
Where we have to do that development.

271
00:15:33,839 --> 00:15:34,020
Okay.

272
00:15:34,020 --> 00:15:36,150
We understand that this is very scalable.

273
00:15:36,150 --> 00:15:39,719
SDA SDKs and also APIs
and the data science.

274
00:15:39,989 --> 00:15:41,939
We can do local development, okay?

275
00:15:42,209 --> 00:15:44,849
And also replicate the production
platform architecture with the

276
00:15:44,849 --> 00:15:48,689
lightweight alternate use like
Docker composed configuration models.

277
00:15:49,185 --> 00:15:51,104
Including mock ML models for common.

278
00:15:51,464 --> 00:15:55,124
These approaches enable developers
to test complex workflows

279
00:15:55,544 --> 00:15:57,044
entirely on their local mission.

280
00:15:57,104 --> 00:16:00,185
They don't, see, because we're trying
to take the backup of the production,

281
00:16:00,635 --> 00:16:04,385
it is a very simple mechanism or
lightweight, alternate use, and

282
00:16:04,385 --> 00:16:08,315
we can reproduce any issues or
challenges in the complex workflows and

283
00:16:08,315 --> 00:16:09,785
increasing the development velocity.

284
00:16:10,325 --> 00:16:13,355
And also the platform includes a
sophisticated testing frameworks

285
00:16:13,355 --> 00:16:16,265
for ML workloads, addressing
the unique challenges of

286
00:16:16,265 --> 00:16:18,185
testing probabilistic system.

287
00:16:18,800 --> 00:16:20,260
And the testing, yes.

288
00:16:20,260 --> 00:16:24,310
Continuous integration pipelines we have
with automated testing is available.

289
00:16:24,580 --> 00:16:26,740
We can have the full test lifecycle suit.

290
00:16:26,740 --> 00:16:30,760
We can create it so that once it
is pulled from the production code

291
00:16:30,760 --> 00:16:34,850
base, you can run the test test
cases and see what is a benchmark.

292
00:16:35,180 --> 00:16:38,300
The performance benchmark and
integration test, we can do it.

293
00:16:38,600 --> 00:16:42,285
The c the kind of maintains the
historical performance data.

294
00:16:43,095 --> 00:16:46,395
Automatically flag integrations in
more accuracy or interference speed.

295
00:16:46,895 --> 00:16:47,165
Okay.

296
00:16:47,585 --> 00:16:49,595
Now, integrated development environments.

297
00:16:49,595 --> 00:16:51,755
Right now we have your local environment.

298
00:16:51,755 --> 00:16:52,805
We can do the testing.

299
00:16:53,075 --> 00:16:56,405
How can we connected with, how can
we connected with the production

300
00:16:56,405 --> 00:16:59,405
environment and what are the tools
are necessary kind of a thing.

301
00:16:59,735 --> 00:17:02,585
So cloud based environments that
includes all the necessary tools,

302
00:17:02,585 --> 00:17:07,385
configurations for production deployment,
GitHub code basis, and pre-configured

303
00:17:07,445 --> 00:17:09,695
work workspaces accessible for any.

304
00:17:10,070 --> 00:17:13,130
So integration with popular
ideas through remote development

305
00:17:13,130 --> 00:17:17,750
extensions, NFD developers, and while
more benefiting from cloud-based

306
00:17:17,750 --> 00:17:19,850
compute process, this is high level.

307
00:17:19,850 --> 00:17:23,840
We are trying to explain that this is
an integrated development environment,

308
00:17:24,080 --> 00:17:29,510
which is in easily integrated from
our test dev stage and production with

309
00:17:29,720 --> 00:17:36,160
having the CICD platform coming to the
observ and see one is, yes, deployed

310
00:17:36,160 --> 00:17:38,050
is done after once it's go live.

311
00:17:38,455 --> 00:17:41,755
Okay, how can we calibrate the metrics?

312
00:17:41,815 --> 00:17:44,515
Okay, so what is the metrics collection?

313
00:17:44,665 --> 00:17:46,975
Okay, we are what is how
the usage is happening?

314
00:17:47,305 --> 00:17:47,485
Okay?

315
00:17:47,485 --> 00:17:49,855
What are the, any error is
happening in the application?

316
00:17:50,095 --> 00:17:52,975
How we get notified, so what
are the metrics collection?

317
00:17:53,035 --> 00:17:57,835
How will we distribute that tracing
visualization in the dashboard,

318
00:17:58,105 --> 00:18:00,925
provide a realtime visibility
hearing system, behavior system,

319
00:18:01,225 --> 00:18:02,575
or holds the system performance.

320
00:18:02,905 --> 00:18:04,075
What is a motor behavior?

321
00:18:04,510 --> 00:18:06,040
And also the intelligent alerting.

322
00:18:06,310 --> 00:18:11,950
So ML algorithms analyze the historical
metric data to establish a dynamic

323
00:18:11,950 --> 00:18:13,750
baseline and detect anonymous.

324
00:18:14,080 --> 00:18:16,570
And the other one is that
the metrics collection pro,

325
00:18:16,720 --> 00:18:20,140
this captures the traditional
application metrics and instrument.

326
00:18:20,140 --> 00:18:24,850
So typically this microservice
has an ability once we go live, to

327
00:18:24,850 --> 00:18:29,350
provide the observability and also
the reliability as it kind of stuff.

328
00:18:29,770 --> 00:18:32,495
So we know the metrics we
visualize in the dashboard.

329
00:18:33,235 --> 00:18:36,865
The racing thing and also intelligence
alerting if there are any issues as

330
00:18:36,865 --> 00:18:44,055
well coming to the SLI or SLO framework,
okay, this is 99.9% availability

331
00:18:44,625 --> 00:18:51,885
with, 0.1, I mean I can say close
to the 99 point 900%, it is up.

332
00:18:52,095 --> 00:18:56,205
There's no downtime per month,
so try approximately 33 minutes

333
00:18:56,205 --> 00:18:58,365
of downtime per month is max.

334
00:18:58,920 --> 00:19:00,330
And also the response time.

335
00:19:00,330 --> 00:19:03,930
The other one is very, see
the services are not down.

336
00:19:03,930 --> 00:19:08,040
The how the response looks that is,
that's what we talk in previous slides

337
00:19:08,370 --> 00:19:11,310
about the performance standpoint
and the scalability standpoint.

338
00:19:11,640 --> 00:19:12,810
It's coming to the performance.

339
00:19:12,840 --> 00:19:19,230
The events is 200 milliseconds means
the accuracy is more important also, and

340
00:19:19,230 --> 00:19:20,790
also the speed is also more important.

341
00:19:20,790 --> 00:19:25,620
95%. The pricing calculation
completed within the 200 milliseconds

342
00:19:25,620 --> 00:19:26,970
by using this microservices.

343
00:19:27,465 --> 00:19:30,915
Maintaining the response to, so
user not even feel any slowness.

344
00:19:31,395 --> 00:19:35,955
And 50, the ML interface, because
we are using an, the average inter

345
00:19:36,555 --> 00:19:39,495
interference of the latency, enabling
the real time pricing without

346
00:19:39,495 --> 00:19:42,165
predictability delay is 50 milliseconds.

347
00:19:42,615 --> 00:19:46,515
The, these are the SLI service
level indicators beyond traditional

348
00:19:46,515 --> 00:19:50,025
availability and latency metrics
to include AI specific measures.

349
00:19:50,265 --> 00:19:50,895
The key SL.

350
00:19:50,895 --> 00:19:54,150
S includes the modern model
interference and accuracy.

351
00:19:54,990 --> 00:19:58,710
Prediction consistency across
replicas and future freshness

352
00:19:58,710 --> 00:20:00,630
for real time predictions.

353
00:20:00,930 --> 00:20:04,350
So on the high level, the
downtime is very minimal.

354
00:20:04,740 --> 00:20:08,895
The response time is very quick, and
the accuracy or suggestion to the

355
00:20:08,895 --> 00:20:11,505
customers is almost all very quick.

356
00:20:12,075 --> 00:20:16,695
And the ML interference is
just, it's 50 milliseconds.

357
00:20:16,785 --> 00:20:18,735
So it's all very, it's negligible.

358
00:20:18,975 --> 00:20:20,955
So that user will not see any slowness.

359
00:20:21,600 --> 00:20:24,210
And also there will always be available.

360
00:20:24,240 --> 00:20:28,130
That's how the framework looks like
coming to the incident response.

361
00:20:28,400 --> 00:20:31,940
So we talk about the development first.

362
00:20:31,940 --> 00:20:34,580
We talk about the infrastructure,
we talk about the development.

363
00:20:34,580 --> 00:20:39,140
What are the APIs that will be
utilized, and also the pipeline

364
00:20:39,140 --> 00:20:40,640
integration to the production.

365
00:20:41,000 --> 00:20:43,790
And then we observed, we looked into it.

366
00:20:43,820 --> 00:20:46,400
What is the pa? Also
any alerts are a thing.

367
00:20:46,985 --> 00:20:51,035
And also how the downtime and looks
like and now coming to this, okay,

368
00:20:51,035 --> 00:20:54,725
in the worst case, if there's an
incident happen, what is a response?

369
00:20:54,725 --> 00:20:57,545
What is a recovery and
how can we ensure that?

370
00:20:57,595 --> 00:21:01,524
So because there's a, some issue
happen in the circuit breakers

371
00:21:01,524 --> 00:21:04,645
preventing cascading failures or
automatically it's not getting that much.

372
00:21:04,975 --> 00:21:07,615
So there is an automated
runbook guide troubleshooting.

373
00:21:07,705 --> 00:21:11,820
So we can get created so that
incident response team procedures

374
00:21:11,820 --> 00:21:12,930
leverage the extension.

375
00:21:13,335 --> 00:21:16,815
Even for those instance, we do
have automated tools so that which

376
00:21:16,815 --> 00:21:18,645
will be okay for this response.

377
00:21:18,645 --> 00:21:22,035
For this one, you have to fix
this score or this responding.

378
00:21:22,335 --> 00:21:26,145
And also we have an automated charge, GPT
integration, that instrument management

379
00:21:26,505 --> 00:21:30,435
communication platforms where user can
also see the moment they get an error.

380
00:21:30,795 --> 00:21:35,805
That error will all be translated in the
backend and guide the user, Hey, there was

381
00:21:35,805 --> 00:21:38,055
some issue with X, Y, Z. And just wait.

382
00:21:38,055 --> 00:21:38,595
We are working.

383
00:21:38,595 --> 00:21:41,504
We created an incident
automatically just called off.

384
00:21:41,800 --> 00:21:43,600
And also the disaster recovery, right?

385
00:21:43,630 --> 00:21:44,560
Nowadays, okay.

386
00:21:45,220 --> 00:21:48,010
All of a sudden, what will
happen out to our critical data?

387
00:21:48,100 --> 00:21:51,790
So it's all running in the ml.
We are trying to gather the data,

388
00:21:51,820 --> 00:21:56,020
train the microservices, and get
the information and share the data.

389
00:21:56,410 --> 00:21:59,290
So what is the disaster recovery process?

390
00:21:59,350 --> 00:21:59,530
Okay.

391
00:21:59,530 --> 00:22:03,820
We are always take regular backups,
regular restoration tests, do

392
00:22:03,820 --> 00:22:05,980
BA and all if anything happen.

393
00:22:06,190 --> 00:22:09,550
So we always have a restore
mechanism from our backup services.

394
00:22:09,925 --> 00:22:14,035
And automated DNS updates as
well for the traffic redirection.

395
00:22:14,995 --> 00:22:15,445
Yes.

396
00:22:15,505 --> 00:22:18,175
And coming to this important
thing is about the data

397
00:22:18,175 --> 00:22:19,435
platform integration, right?

398
00:22:19,825 --> 00:22:24,495
So one task is about how the
Microsoft architectural front-end

399
00:22:24,495 --> 00:22:26,655
looks like the backbone of this.

400
00:22:26,655 --> 00:22:29,895
Everything is about the data
without having the data.

401
00:22:29,895 --> 00:22:32,325
Even though if you try to
build the microservices Yes.

402
00:22:32,385 --> 00:22:33,764
Output will not be kept.

403
00:22:34,285 --> 00:22:35,215
That is the major thing.

404
00:22:35,215 --> 00:22:37,195
There's a Kafka backbone, is there?

405
00:22:37,555 --> 00:22:39,505
What are the sources we
are trying to consume?

406
00:22:39,505 --> 00:22:43,405
And the cons were the consumers
was how we streamline the data

407
00:22:43,405 --> 00:22:45,055
based on the consumer information.

408
00:22:45,415 --> 00:22:49,825
So every year we process the millions
of the pricing events daily, and the

409
00:22:49,825 --> 00:22:54,415
topics are organized by businesses
domain, which generalized naming

410
00:22:54,415 --> 00:22:56,425
conventions and retention process.

411
00:22:56,725 --> 00:23:00,725
So it includes the timestamps
and correlation IDs, portion

412
00:23:00,725 --> 00:23:04,505
information enabling the detail
traceability, and the debugging cap.

413
00:23:05,005 --> 00:23:08,335
So that is about the, one of the
important data platform architecture.

414
00:23:08,385 --> 00:23:11,175
The strain, even stream
streaming architecture system.

415
00:23:12,014 --> 00:23:15,735
Coming to the other one is about the
feature store and ML data management.

416
00:23:16,215 --> 00:23:21,524
So the Future store serves as a central
repository for ML features, ensuring

417
00:23:21,524 --> 00:23:25,125
the consistency between the training
and also the serving environments

418
00:23:25,485 --> 00:23:28,365
built on top of the Apache fist.

419
00:23:28,844 --> 00:23:31,304
This is the feature store
in the ML data management.

420
00:23:31,304 --> 00:23:32,725
So we had a batch feature.

421
00:23:32,725 --> 00:23:36,745
Suppose the model training workflows
while online features enable the low

422
00:23:36,745 --> 00:23:39,024
latency interval during the interference.

423
00:23:39,385 --> 00:23:43,345
So Apache Spark for the large scale data
processing with optimized algorithms

424
00:23:43,345 --> 00:23:48,235
for the common transformations and
also coming to the data line is right.

425
00:23:48,264 --> 00:23:52,254
So we know the tracking provides
a complete visibility into

426
00:23:52,254 --> 00:23:53,665
the future generation process.

427
00:23:53,875 --> 00:23:57,115
From raw data services through
transformation steps to final.

428
00:23:57,475 --> 00:23:58,435
Future values.

429
00:23:58,735 --> 00:24:02,365
This transparency proves invaluable
for debugging model issues, and

430
00:24:02,365 --> 00:24:06,355
also ensuring the compliance with
data governance requirements.

431
00:24:07,345 --> 00:24:12,355
And the coming to the other picture
is about security and compliance.

432
00:24:12,865 --> 00:24:14,485
This is very important aspect.

433
00:24:14,514 --> 00:24:14,845
Okay.

434
00:24:14,905 --> 00:24:19,975
So because we are dealing with the data
of the organization, and also nowadays

435
00:24:19,975 --> 00:24:24,655
AI is trying to capture all the data
and to guide us, but at the same time.

436
00:24:25,195 --> 00:24:28,589
We need to protect our data and
also what is the security measures?

437
00:24:28,589 --> 00:24:29,549
What is the compliance?

438
00:24:29,849 --> 00:24:34,529
When you're trying to input a request,
the request has to be reside within that

439
00:24:34,529 --> 00:24:39,509
our architectural and also the compliance,
guidance, and standpoint so that it

440
00:24:39,509 --> 00:24:41,489
should not be below outside of our world.

441
00:24:42,389 --> 00:24:46,319
So that's why we doing the mutual
TLS, all the service to service

442
00:24:46,319 --> 00:24:49,079
communications across over mutual t ls.

443
00:24:49,350 --> 00:24:53,040
It's certificate rotation automated
through certificate manager so

444
00:24:53,040 --> 00:24:56,879
that it has a TLS mutual TLS
certificates and the network policies.

445
00:24:56,879 --> 00:24:59,850
The moment when you're trying to
send the request, the stringent

446
00:25:00,000 --> 00:25:03,600
segmentation between the services,
limiting the OCA communication too,

447
00:25:03,929 --> 00:25:05,520
explicitly authorize the paths.

448
00:25:05,730 --> 00:25:10,275
And the other thing is the identity
management could be open ID or 2.0.

449
00:25:10,365 --> 00:25:11,035
Definitely.

450
00:25:11,090 --> 00:25:12,560
It's a key based kind of thing.

451
00:25:12,560 --> 00:25:15,360
Or the client ID kind of, or rules.

452
00:25:15,360 --> 00:25:16,050
We use it.

453
00:25:16,080 --> 00:25:20,070
There's nothing like a. Layman, a
username and password kind of thing.

454
00:25:20,070 --> 00:25:23,610
It's all will be a secure
identity management of what?

455
00:25:23,639 --> 00:25:27,600
2.1 the Open ID connects and
the secrets management, right?

456
00:25:27,600 --> 00:25:29,879
We store everything in the vault.

457
00:25:30,239 --> 00:25:32,760
Even though those keys, you
cannot easily capture them.

458
00:25:32,760 --> 00:25:36,959
Those will be the secured wall
and it'll be the vault to access

459
00:25:36,959 --> 00:25:38,370
the vault every 15 minutes.

460
00:25:38,370 --> 00:25:39,899
It's keep on changing your password.

461
00:25:40,199 --> 00:25:43,290
You need to connect to the VPN
and you need to connect to the

462
00:25:43,620 --> 00:25:45,320
world management with your short.

463
00:25:46,139 --> 00:25:48,840
We can save one type of a
password to get the key.

464
00:25:48,840 --> 00:25:53,789
And again, then this kind of a rotation
will also be happen in the database.

465
00:25:53,879 --> 00:25:57,509
So these are the very key important
aspects for the security and compliance.

466
00:25:57,719 --> 00:26:01,649
And ensure that every layer of the
platform, zero trust principles, that

467
00:26:01,649 --> 00:26:05,580
in assume no implicit trust between
the components because end of the

468
00:26:05,580 --> 00:26:10,340
day it is, will be supporting with
single signon, with multifactor and

469
00:26:10,340 --> 00:26:15,050
hazard capacity to utilize this,
I mean backend device services.

470
00:26:15,379 --> 00:26:20,090
So that they cannot tam the data
and and also very strong mechanism

471
00:26:20,570 --> 00:26:22,399
to, for the security standpoint.

472
00:26:23,300 --> 00:26:23,540
Yeah.

473
00:26:23,720 --> 00:26:26,720
Coming to the data privacy one,
we talk about the security, right?

474
00:26:26,720 --> 00:26:31,250
How we can control, we have those
mechanisms of TLS open ID and also

475
00:26:31,250 --> 00:26:33,999
the what kind of a thing and okay.

476
00:26:34,029 --> 00:26:34,539
Privacy.

477
00:26:34,899 --> 00:26:36,369
So we need to have certain data.

478
00:26:36,369 --> 00:26:36,609
Okay.

479
00:26:36,609 --> 00:26:39,699
Certain persons only can sit
and see the certain data within

480
00:26:39,699 --> 00:26:41,289
even the organization as well.

481
00:26:41,559 --> 00:26:43,329
There could be industry
specific requirements.

482
00:26:43,334 --> 00:26:43,344
Hey.

483
00:26:44,154 --> 00:26:48,024
He can see that XY amount of
price, which is the roll of

484
00:26:48,024 --> 00:26:49,674
case, should not see that price.

485
00:26:49,914 --> 00:26:53,154
So those kind of things also
be classified in such a way.

486
00:26:53,154 --> 00:26:55,764
But the rules a thing also be embedded.

487
00:26:56,004 --> 00:26:59,814
Hey, okay, this particular user is
just dedicated to configure the model.

488
00:27:00,144 --> 00:27:03,534
The price is not there because it belongs
to the engineering department a thing.

489
00:27:03,955 --> 00:27:06,955
And also the privacy
preserving techniques also.

490
00:27:07,314 --> 00:27:11,695
So if we add that control noise to
maintain the data and federated the.

491
00:27:12,550 --> 00:27:17,950
Learning across distributed data sets
right to be forgotten implementation.

492
00:27:17,950 --> 00:27:20,950
So at the moment when we've done
this diagnosis, there's a right

493
00:27:20,950 --> 00:27:23,000
to be forgotten implementation.

494
00:27:23,179 --> 00:27:28,040
The data retention and also the deletion
policies automatically remove the expired

495
00:27:28,040 --> 00:27:31,700
data so that you don't need to locate
access according to the regulatory

496
00:27:31,700 --> 00:27:33,530
requirements and the business policies.

497
00:27:33,980 --> 00:27:36,860
And the platform maintains the
detailed audit logs for off all

498
00:27:36,860 --> 00:27:38,300
data taxes and modifications.

499
00:27:38,794 --> 00:27:43,735
Support compliance and reporting and
forensic analysis and coming to the

500
00:27:43,735 --> 00:27:47,995
other important gain, the performance
optimization, we did, we lightly touch

501
00:27:47,995 --> 00:27:51,504
in the previous slide, but coming
to the multi-layer caching, so it's

502
00:27:51,504 --> 00:27:55,095
nothing but you keep on hitting the
same request, it will be cached and it

503
00:27:55,095 --> 00:27:59,264
gives the response very quickly and very
intelligently with, even though you're

504
00:27:59,264 --> 00:28:02,415
trying to make the small parameter
change, you get it very quickly.

505
00:28:02,895 --> 00:28:04,814
And the other one is
database optimization.

506
00:28:04,814 --> 00:28:05,655
So it's now.

507
00:28:06,210 --> 00:28:09,840
Now we nowadays it's like Postgres
SQL instances use that once.

508
00:28:09,840 --> 00:28:14,640
Features including the ING pallet query
execution, just in time compilation.

509
00:28:15,330 --> 00:28:19,110
NoQ databases, including Mongo, d,
p, and Cassandra, handle specific

510
00:28:19,110 --> 00:28:20,580
workloads and they coming to the queue.

511
00:28:20,580 --> 00:28:24,990
Resource utilization, spot instances
handling batch processing workloads

512
00:28:24,990 --> 00:28:30,960
with automated failover or to on-demand
instances when sport capacities available.

513
00:28:31,410 --> 00:28:34,050
ML training jobs run
during the off peak hours.

514
00:28:34,395 --> 00:28:37,004
In which compute the
resource are less expensive.

515
00:28:37,004 --> 00:28:40,575
I know there are very less resources
that the training jobs will be trained

516
00:28:40,635 --> 00:28:45,555
and get it done so, and these content
delivery network integration accelerate

517
00:28:45,825 --> 00:28:49,905
the delivery of the static assets and
also the A PR responses to global users,

518
00:28:50,205 --> 00:28:54,495
edge location, catch pricing catalogs, and
the configuration data, which will reduce

519
00:28:54,495 --> 00:28:57,585
the latency for international customers.

520
00:28:58,004 --> 00:29:02,000
That is mainly, we are working on the
performance optimization and the strategy.

521
00:29:02,325 --> 00:29:04,635
Is coming to the case studies, right?

522
00:29:04,635 --> 00:29:04,875
What?

523
00:29:04,875 --> 00:29:05,235
Okay.

524
00:29:05,295 --> 00:29:06,255
We are, we're all good.

525
00:29:06,825 --> 00:29:08,505
We have the microservice
that we talk about.

526
00:29:08,535 --> 00:29:12,705
How can we utilize it in the CPQ,
what are the deployment strategies?

527
00:29:12,705 --> 00:29:14,415
What are the APIs behind the scenes?

528
00:29:14,415 --> 00:29:15,435
What is the performance?

529
00:29:15,475 --> 00:29:18,355
Optimization and also scalability.

530
00:29:18,615 --> 00:29:21,045
We are talking a lot of
good things in the slides.

531
00:29:21,465 --> 00:29:21,765
Okay.

532
00:29:22,125 --> 00:29:26,715
How can we ensure this all
will be work and what are the

533
00:29:27,435 --> 00:29:29,055
kind of thing or the results?

534
00:29:29,085 --> 00:29:29,295
Okay.

535
00:29:29,850 --> 00:29:34,290
We did a high level analysis of
this taking into small microservices

536
00:29:34,290 --> 00:29:37,050
built in the system and trying
to understand the capacity.

537
00:29:37,500 --> 00:29:41,910
So 10 time you will request the capacity
and increase from 10,000 to over a

538
00:29:42,210 --> 00:29:46,290
hundred K monthly request to horizonal
scaling and the performance optimization

539
00:29:46,860 --> 00:29:52,680
so that even though you try to increase
the 10 times, the responsive time is 75%.

540
00:29:52,680 --> 00:29:53,760
Respondent is very quick.

541
00:29:53,760 --> 00:29:56,925
That is between the 800
milliseconds to under.

542
00:29:57,840 --> 00:29:59,550
200. 200 milliseconds.

543
00:29:59,940 --> 00:30:04,920
It signed 65% time to market decrease
for new features measured for production.

544
00:30:04,920 --> 00:30:09,810
So the newer products evaluation
will be very quicker and very good.

545
00:30:10,440 --> 00:30:13,620
Building the scalable AI power platform
request a care will consideration

546
00:30:13,620 --> 00:30:17,430
of architecture, operations, and
developer experience for Microsoft.

547
00:30:17,430 --> 00:30:24,720
Visual based CPQ platforms demonstrates
the enterprise great reliability and AI

548
00:30:24,720 --> 00:30:26,560
innovation can coexist when supporting by.

549
00:30:27,090 --> 00:30:31,430
Appropriate platform engineering
practices all in one go.

550
00:30:31,430 --> 00:30:36,210
What I'm trying to tell is everywhere
we are using the ai but AI is giving a

551
00:30:36,210 --> 00:30:41,370
very positive result and taking that as
an advantage kind of a thing, or taking

552
00:30:41,370 --> 00:30:44,250
that as a source for that AI platforms.

553
00:30:44,670 --> 00:30:50,340
Plugging into the CPQ technologies
and trying to evolve, how can we

554
00:30:50,430 --> 00:30:56,970
do a configuration pricing co. Very
efficient, very responsive, very

555
00:30:56,970 --> 00:30:59,730
optimization way, and also very accurate.

556
00:31:00,090 --> 00:31:05,700
So Synchron and more driven with
the train the models and get the

557
00:31:05,700 --> 00:31:10,440
import robust platform engineering
that will grow by using this AI

558
00:31:10,440 --> 00:31:12,090
to drive the business decisions.

559
00:31:12,540 --> 00:31:14,310
That's concluding myself.

560
00:31:14,580 --> 00:31:17,785
This is just focusing on that
scalability, reliability.

561
00:31:18,465 --> 00:31:21,825
And also the greater experience
the organization can create.

562
00:31:21,825 --> 00:31:25,065
The platforms not only meet the current
needs, but also adapt to the future

563
00:31:25,065 --> 00:31:29,955
challenges because the landscape is
keep on evolving, changing day by in the

564
00:31:29,955 --> 00:31:34,635
industry standpoint by utilize, utilize
this AI powered MEChA enterprise systems.

565
00:31:35,295 --> 00:31:41,145
That's the major history and overall the
presentation I was provided to you guys.

566
00:31:41,595 --> 00:31:44,115
Hope you guys have
enjoyed the presentation.

567
00:31:44,700 --> 00:31:46,380
And thank you everyone.

568
00:31:46,530 --> 00:31:47,550
Have a great day.

569
00:31:47,640 --> 00:31:48,240
Good night.

570
00:31:49,080 --> 00:31:49,470
See you.

571
00:31:49,860 --> 00:31:50,130
Bye.

