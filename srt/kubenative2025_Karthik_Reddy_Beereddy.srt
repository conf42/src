1
00:00:00,840 --> 00:00:01,680
Good evening everyone.

2
00:00:01,770 --> 00:00:05,610
This is Karthik Red, a senior society
level team engineer specializing in

3
00:00:05,610 --> 00:00:08,700
cloud native platform modernization
and event driven architecture.

4
00:00:09,330 --> 00:00:12,390
Over the past decade, my work has
focused on building resilience,

5
00:00:12,390 --> 00:00:16,200
scalable, and self-healing system
on Azure Kubernetes services with an

6
00:00:16,200 --> 00:00:19,770
emphasis on automation, observability,
and zero downtime operations.

7
00:00:20,250 --> 00:00:23,160
My journey in SRE has been about
transforming large traditional

8
00:00:23,160 --> 00:00:26,990
systems into high availability
realtime platform using open source

9
00:00:26,990 --> 00:00:28,950
technologies and modern DevOps practices.

10
00:00:29,790 --> 00:00:32,970
Today I'll be sharing insights
from the recent work on Kubernetes

11
00:00:32,970 --> 00:00:35,320
native platform modernization.

12
00:00:35,770 --> 00:00:39,880
So before getting into the topic,
so I would like to share how the

13
00:00:39,880 --> 00:00:41,740
current industry has been evolving.

14
00:00:42,160 --> 00:00:46,000
So over the last decade, we have seen
cloud architecture evolve from the static

15
00:00:46,000 --> 00:00:50,200
VMs, two dynamic container workloads that
can respond to change in a real time.

16
00:00:50,620 --> 00:00:53,470
Kubernetes has become the
backbone of this transformation.

17
00:00:54,145 --> 00:00:57,895
Not just as a container orchestrator,
but as an enabler of declarative

18
00:00:58,165 --> 00:01:00,265
automation, resilience, and portability.

19
00:01:00,925 --> 00:01:05,065
What's exciting today is how teams are
rethinking reliability, shifting from

20
00:01:05,065 --> 00:01:09,715
reactive firefighting to proactive
engineering, where the platform itself

21
00:01:09,895 --> 00:01:11,725
anticipates and recovers from failure.

22
00:01:12,295 --> 00:01:15,485
This evaluation is not just
technical it's cultural.

23
00:01:15,545 --> 00:01:19,955
It's about empowering developers,
automating recovery, and bridging

24
00:01:20,225 --> 00:01:21,485
observability with operation.

25
00:01:21,985 --> 00:01:25,345
So what we are gonna learn today,
so one is the battle tested design

26
00:01:25,345 --> 00:01:29,395
patterns, real world deployment insights
and comprehensive pattern catalog.

27
00:01:30,145 --> 00:01:34,555
In the battle tested design patterns
the, we discover proven patterns for

28
00:01:34,555 --> 00:01:38,275
building self-healing, cloud native
streaming systems that ensure throughput

29
00:01:38,275 --> 00:01:40,075
consistency and availability at scale.

30
00:01:40,705 --> 00:01:44,785
In the real world deployment insights,
we gain critical insights from higher

31
00:01:44,785 --> 00:01:46,405
volume, even stream deployments.

32
00:01:46,945 --> 00:01:50,395
Processing mission critical
data in production environment.

33
00:01:50,965 --> 00:01:54,505
Whereas in comprehensive pattern
catalog, we acquire concrete tactics

34
00:01:54,625 --> 00:01:59,575
and the robot toolkit for designing
streamlining pipelines that withstand

35
00:01:59,665 --> 00:02:01,495
failure and recover gracefully.

36
00:02:01,995 --> 00:02:05,235
So let's get into the architecting
for resilience streaming, foundation,

37
00:02:05,715 --> 00:02:09,075
resilience, streaming systems, demands
and architecture built to withstand

38
00:02:09,075 --> 00:02:13,050
the unpredictable nature of distributed
environment such as architecture.

39
00:02:13,770 --> 00:02:17,370
Simultaneously uphold three
properties, which are critical.

40
00:02:17,820 --> 00:02:20,160
One is throughput,
consistence, and availability.

41
00:02:20,850 --> 00:02:24,230
So the throughput which sustain
high volume message crossing without

42
00:02:24,230 --> 00:02:28,580
performance degradation, whereas
consistency which preserve data integrity

43
00:02:28,670 --> 00:02:33,140
and message order despite failures and in
the availability, which ensure continuous

44
00:02:33,140 --> 00:02:37,760
service operation, even emits component
failures, these critical properties,

45
00:02:37,880 --> 00:02:39,905
empathy, the cap, the trade off.

46
00:02:40,820 --> 00:02:44,630
Inherent in designing any
cloud native streaming systems.

47
00:02:45,130 --> 00:02:48,530
So let's discuss about the ready
streams which, which is replayable

48
00:02:48,680 --> 00:02:53,000
event log and observe downstream
failures and coordinate consumer groups.

49
00:02:53,600 --> 00:02:57,320
So in the REPLAYABLE event, log the
ready stream maintenance and append

50
00:02:57,440 --> 00:03:01,910
only lock enabling deterministic
event play messages persist with

51
00:03:01,910 --> 00:03:06,195
unique it and timestamp precisely
reconstructing sequence after failure.

52
00:03:06,695 --> 00:03:10,985
In the absorb downstream failures,
this resilient buffer absorbs temporary

53
00:03:10,985 --> 00:03:14,405
downstream failures, preventing
data loss, automatic back pressure

54
00:03:14,405 --> 00:03:18,875
management, safeguards against memory
exhaust during extended outages.

55
00:03:19,625 --> 00:03:23,615
And the coordinate consumer groups,
which built in consumer group management

56
00:03:23,705 --> 00:03:28,590
ensures load distribution and automatic
failover failed consumers resume

57
00:03:28,650 --> 00:03:33,510
processing from their last acknowledged
positions, maintaining so on top of this.

58
00:03:34,350 --> 00:03:38,130
So we in current, the
platform has been modernized.

59
00:03:38,380 --> 00:03:42,610
In large enterprises, especially those
with decades of legacy systems, the

60
00:03:42,610 --> 00:03:47,080
modernization is not, it's not just about
containerization, it's about transforming

61
00:03:47,140 --> 00:03:52,060
anti delivery models from moving from
nightly batch jobs and static AP PS to

62
00:03:52,120 --> 00:03:56,355
event driven scalable services that can
handle millions of requests per day.

63
00:03:57,235 --> 00:04:01,090
So the challenge often lies in
maintaining service clusters while

64
00:04:01,090 --> 00:04:02,695
ensuring compliance and security.

65
00:04:03,195 --> 00:04:06,080
That's where reliability
engineering principle intersects

66
00:04:06,080 --> 00:04:08,085
with Kubernetes native design.

67
00:04:08,585 --> 00:04:11,374
So let's discuss about the
circuit breakers, which is

68
00:04:11,374 --> 00:04:13,054
mitigating cascading failures.

69
00:04:13,684 --> 00:04:17,674
So the close, there are three states,
close the state open state, half open

70
00:04:17,674 --> 00:04:23,225
state in the closed state request flow
normally while the system monitor error

71
00:04:23,465 --> 00:04:25,414
rates and response times for animal.

72
00:04:25,655 --> 00:04:30,635
In the open state, when failure
thresholds are exceeded, the circuit opens

73
00:04:30,845 --> 00:04:32,615
causing requests to fail immediately.

74
00:04:33,034 --> 00:04:35,975
This prevents resource exertion
and cascading failures.

75
00:04:36,784 --> 00:04:37,655
Half open state.

76
00:04:38,105 --> 00:04:41,975
So this periodic health checks are
conducted to assert service recovery.

77
00:04:42,095 --> 00:04:47,135
Traffic is gradually restored
as downstream service stabilize.

78
00:04:47,635 --> 00:04:49,695
So let's discuss about the reliable event.

79
00:04:49,695 --> 00:04:51,015
Replace for system consistent.

80
00:04:51,515 --> 00:04:55,065
So in the, in this event
replay, we have three aspects.

81
00:04:55,065 --> 00:04:58,875
One is preserve message, order,
restore system, state efficiently

82
00:04:59,235 --> 00:05:01,035
ensure item potent processing.

83
00:05:01,905 --> 00:05:05,565
So preserve message, order where
employee partition keys and sequence

84
00:05:05,565 --> 00:05:08,925
numbers to ensure deterministic
message ordering during replay.

85
00:05:09,435 --> 00:05:12,915
This is crucial for maintaining
state consistency across distributed

86
00:05:13,035 --> 00:05:15,525
systems in the restore system.

87
00:05:15,525 --> 00:05:19,125
State efficiency where harness
event sourcing patterns.

88
00:05:19,770 --> 00:05:23,910
Combined with snapshots and incremental
replay so quickly, restore system

89
00:05:24,000 --> 00:05:29,760
state after failures, minimize recovery
time while ensuring data item, potent

90
00:05:29,760 --> 00:05:34,440
processing design message handlers to
be item Putin, allowing safe replay of

91
00:05:34,440 --> 00:05:36,420
duplicate messages without side effects.

92
00:05:36,840 --> 00:05:41,340
This guarantees exactly once processing
semantics in distributor environment.

93
00:05:42,190 --> 00:05:45,795
So now let's discuss about the
Kubernetes microservices partition.

94
00:05:46,350 --> 00:05:49,230
And data scaling and service integration.

95
00:05:49,830 --> 00:05:56,380
So in the event of ing use this ing to
distribute load among consumers while

96
00:05:56,380 --> 00:06:00,659
preserving message order when we talk
about this event driven architectures

97
00:06:00,900 --> 00:06:05,240
one of the most overlooked critical
competencies like the event of, so the

98
00:06:05,240 --> 00:06:09,710
event of partitioning plays a major
role in parallelism and auditing each

99
00:06:09,710 --> 00:06:12,080
partition as an independent commit law.

100
00:06:12,484 --> 00:06:16,534
Allowing multiple consumers to
process events concurrently while

101
00:06:16,625 --> 00:06:18,455
preserving order with that partition.

102
00:06:19,205 --> 00:06:22,524
The key is finding the right
balance between the partition

103
00:06:22,524 --> 00:06:24,234
count and consumer scalability.

104
00:06:24,734 --> 00:06:29,154
In one of in one of our implementations,
which you partition comes dynamically

105
00:06:29,214 --> 00:06:31,044
based on throughput trends.

106
00:06:31,599 --> 00:06:35,664
For example, billing events, speaking
during business hours are synchronous

107
00:06:35,664 --> 00:06:37,314
reasons after nightly basis.

108
00:06:37,854 --> 00:06:38,784
This approach helped.

109
00:06:39,279 --> 00:06:43,119
Maintain high rates without
hitting throttling limits of

110
00:06:43,119 --> 00:06:44,959
our provisioning resources.

111
00:06:45,459 --> 00:06:49,369
From a site reliability engineering
perspective partitioning is not just

112
00:06:49,369 --> 00:06:51,349
about performance, it's about resilience.

113
00:06:51,799 --> 00:06:53,359
It isolates failure domains.

114
00:06:53,599 --> 00:06:57,709
If one consumer group are partition
experiences, select the other, con,

115
00:06:57,799 --> 00:06:59,839
the others continue processing.

116
00:06:59,839 --> 00:07:00,319
Seamless.

117
00:07:00,444 --> 00:07:04,039
In short if it well planned
partitioning gives you both

118
00:07:04,039 --> 00:07:05,959
scalability and fault isolation.

119
00:07:06,589 --> 00:07:09,649
Twin pillars of reliability
in any streaming system.

120
00:07:10,579 --> 00:07:14,419
The second one will be that clear scaling
is Kubernetes event driven auto scaling.

121
00:07:14,609 --> 00:07:18,239
Which is traditional auto scaling
in Kubernetes is often metric

122
00:07:18,239 --> 00:07:21,029
driven, like CPU memory orent.

123
00:07:21,509 --> 00:07:26,609
But in event driven systems, the real
indicator of Lois Q Depth or event

124
00:07:26,609 --> 00:07:28,734
plan, this is where cada shines.

125
00:07:29,074 --> 00:07:33,459
So CADA allows you to scale
workloads based on external metrics.

126
00:07:34,074 --> 00:07:36,279
In our case, the event of consumer lack.

127
00:07:36,849 --> 00:07:40,899
So when the queue backlog increases,
new pos are automatically spawned as the

128
00:07:40,899 --> 00:07:43,599
queue drain, they gracefully scale them.

129
00:07:43,749 --> 00:07:48,939
This dynamic responsiveness eliminates
ideal computer and reduce cost while

130
00:07:48,939 --> 00:07:51,009
maintaining near real time process.

131
00:07:51,609 --> 00:07:55,269
What I like most about Qda, it's
about bringing operational simplicity

132
00:07:55,359 --> 00:07:59,919
created with the HPA, which is
horizontal port autoscaler, and

133
00:07:59,919 --> 00:08:01,689
works natively with Prometheus metal.

134
00:08:02,574 --> 00:08:05,934
From an SRE perspective,
it's a perfect balance of

135
00:08:05,934 --> 00:08:07,854
performance and cost efficiency.

136
00:08:07,854 --> 00:08:11,634
Scaling, not by guesswork, but by data.

137
00:08:12,214 --> 00:08:15,004
It's also highly observable friendly.

138
00:08:15,064 --> 00:08:18,544
You can actually trace auto scaling
events alongside system metrics,

139
00:08:19,024 --> 00:08:23,734
which helps correlate scaling behavior
with real user or bachelor pack.

140
00:08:24,694 --> 00:08:26,489
Next comes the service mass integration.

141
00:08:27,304 --> 00:08:30,544
So this Service MES integration
is one of the most transformative

142
00:08:30,544 --> 00:08:32,104
ships in Kubernetes networking.

143
00:08:32,734 --> 00:08:37,534
As microservice grows, so does the
complexity of managing communication

144
00:08:38,034 --> 00:08:41,584
authentication which retries,
encryption and observability.

145
00:08:42,454 --> 00:08:47,224
So a service mesh like Linker Deep
are abstracts these concerns out

146
00:08:47,224 --> 00:08:51,124
of the application and handles
thems the infrastructure layer.

147
00:08:51,664 --> 00:08:55,654
For example, by enabling
MTLS, which is mutual TLS.

148
00:08:56,209 --> 00:08:59,719
We ensure secure service to
service communication without

149
00:08:59,719 --> 00:09:01,669
changing a single line of code.

150
00:09:02,669 --> 00:09:06,899
From a reliability standpoint, servicemen
gives you like powerful management

151
00:09:06,899 --> 00:09:12,119
circuit braking with backup and
canary routing are all policy driven.

152
00:09:12,989 --> 00:09:14,909
Observability is another major advantage.

153
00:09:15,059 --> 00:09:19,279
With built-in telemetry, we
can trace latency per identity

154
00:09:19,279 --> 00:09:23,809
bottlenecks, and detect cascading
failures before the impact end user.

155
00:09:24,309 --> 00:09:28,419
So now let's hear about the automated
failure detention, like as we discussed,

156
00:09:28,539 --> 00:09:33,219
which is liveliness, probs, readiness,
probs, container restart policies, and HPS

157
00:09:33,219 --> 00:09:38,079
scaling response in the liveliness pros,
which verifies the application help via

158
00:09:38,229 --> 00:09:44,424
HT TP endpoints, trigger container restart
for unresponsive services, set appropriate

159
00:09:44,424 --> 00:09:49,239
timeouts and failure threshold, whereas in
the readiness groups, use startup checks

160
00:09:49,509 --> 00:09:51,909
to remove unhealthy pots from service.

161
00:09:52,434 --> 00:09:55,814
Discovery until ready to
serve traffic prevent routing

162
00:09:55,814 --> 00:09:57,674
requests to degraded instances.

163
00:09:57,974 --> 00:10:02,294
Container restart policies utilize
Kubernetes built in restart mechanism

164
00:10:02,684 --> 00:10:07,804
with exponential backup of for automatic
recovery from transition failures.

165
00:10:07,864 --> 00:10:12,349
Eliminating manual intervention HPS
scaling response, configure horizontal

166
00:10:12,349 --> 00:10:16,864
port autoscaler policies to swiftly
scale replica based on CPU memory.

167
00:10:17,419 --> 00:10:22,399
Customer metrics, like as we discussed,
the event of messages and processing,

168
00:10:22,519 --> 00:10:25,359
like observability with Prometheus.

169
00:10:25,449 --> 00:10:30,639
And so establishing the baselines for
anomaly detention which is establishing

170
00:10:30,639 --> 00:10:35,739
robust baselines of normal performance
metrics is crucial for identifying

171
00:10:35,829 --> 00:10:38,499
deviations that signal potential issues.

172
00:10:38,679 --> 00:10:40,899
Pay close attention to
these critical indicators.

173
00:10:41,859 --> 00:10:43,329
One is message throughput.

174
00:10:43,509 --> 00:10:47,979
Second one will be the consumer lag, or
third one will be the error rate results.

175
00:10:47,984 --> 00:10:49,959
And the fourth one is
resource utilization.

176
00:10:50,739 --> 00:10:56,259
Message throughput, which monitor volume
and processing speed, consumer lack, which

177
00:10:56,259 --> 00:11:01,234
is tracks consumer delays and message two
error rate results which define acceptable

178
00:11:01,534 --> 00:11:06,184
error rates and alert on breaches resource
utilization, which observes CPU Memory

179
00:11:06,244 --> 00:11:08,369
network and this usage for animal.

180
00:11:08,869 --> 00:11:12,589
This regularly update baselines
for effective automated and

181
00:11:12,589 --> 00:11:15,739
scaling discussions when
it comes to observability.

182
00:11:16,099 --> 00:11:20,384
So one of the biggest breakthroughs for
me personally has been operationalizing

183
00:11:20,384 --> 00:11:25,850
observability, which is going beyond logs
and metrics to actual actionable insight.

184
00:11:26,030 --> 00:11:29,880
For example distributor tracing
combined with real time dashboards

185
00:11:29,970 --> 00:11:34,350
allows us to visualize transaction
journeys end to end instead of treating

186
00:11:34,350 --> 00:11:36,449
incidents as isolated problems.

187
00:11:36,885 --> 00:11:41,475
We now view them as signals to improve
system, device, and observability is

188
00:11:41,475 --> 00:11:45,375
embedded into the platform from day
one, meantime to detection and recovery

189
00:11:45,375 --> 00:11:49,875
drops dramatically and engineers gain
confidence in continuous delivery.

190
00:11:50,375 --> 00:11:53,795
Visual bottleneck identification,
which is here we are using

191
00:11:53,945 --> 00:11:55,415
heat map analysis techniques.

192
00:11:56,165 --> 00:11:59,225
The harness Grafana advanced
visualizations to create

193
00:11:59,345 --> 00:12:00,245
comprehensive heat maps.

194
00:12:01,235 --> 00:12:05,735
Pinpointing performance, bottlenecks and
capacity constraints across your stream.

195
00:12:05,855 --> 00:12:11,215
Streaming pipeline, the visually
identity which processing hot

196
00:12:11,215 --> 00:12:13,285
spots and uneven load distribution.

197
00:12:13,855 --> 00:12:18,555
Temporal patterns in CRO microservices
network and storage I your bottlenecks.

198
00:12:19,095 --> 00:12:24,075
The color coded intensity maps provide
intuitive understanding of system

199
00:12:24,075 --> 00:12:28,185
behavior, writing optimization efforts
based on actual performance impact.

200
00:12:28,685 --> 00:12:33,485
So architecting resilience in this
is the pattern catalog where we have

201
00:12:33,515 --> 00:12:38,945
modernized legacy ETL build NextGen event
platforms and ensure SLA compliance.

202
00:12:39,445 --> 00:12:43,015
So when it comes to the resilience, so
where the automation plays individual

203
00:12:43,020 --> 00:12:47,155
in, in achieving this zero downtime
operating pipelines and progressive

204
00:12:47,155 --> 00:12:51,915
delivery techniques like Bluegreen
and Canary Argo rolls we which can

205
00:12:51,915 --> 00:12:53,620
control risk while maintaining velocity.

206
00:12:54,120 --> 00:12:57,780
Automation is not only about
deployment, it's also about recovery.

207
00:12:58,380 --> 00:13:02,310
Self-healing systems that restart
parts, rotate secrets or failover

208
00:13:02,310 --> 00:13:06,450
seamlessly having have become
the foundation of reliability.

209
00:13:07,020 --> 00:13:10,530
The ultimate goal is to make
failure, protein predictable and

210
00:13:10,530 --> 00:13:14,225
recoverable so team can focus on
innovation instead of firefighting.

211
00:13:14,725 --> 00:13:19,350
So the in the modernized legacy ETL, the
transform traditional batch pipelines

212
00:13:19,350 --> 00:13:20,945
into resilient streaming architecture.

213
00:13:21,915 --> 00:13:25,125
Leveraging event driven patterns
and cloud native orchestration.

214
00:13:25,545 --> 00:13:28,955
Whereas in the Build NextGen event
platforms, we have architect autoscaling

215
00:13:29,735 --> 00:13:33,725
event driven systems with comprehensive
observability built in from day one.

216
00:13:34,445 --> 00:13:38,885
And in the Ensure SLA complex, we
implement proactive monitoring and

217
00:13:38,885 --> 00:13:43,865
automated recovery to meet strings and
availability and performance requirements

218
00:13:44,565 --> 00:13:46,035
for mission critical environments.

219
00:13:46,335 --> 00:13:50,145
So let's get into the architecting
resilient streaming system.

220
00:13:50,505 --> 00:13:53,750
Which is like production ready
patterns automate with Kubernetes

221
00:13:53,750 --> 00:13:55,110
comprehensive observability.

222
00:13:55,610 --> 00:13:59,270
The production ready patterns,
which leverage proven pattern for

223
00:13:59,270 --> 00:14:03,170
registry, circuit breaker, and
deterministic replace strategies to

224
00:14:03,170 --> 00:14:05,480
mitigate real world failure scenarios.

225
00:14:06,260 --> 00:14:10,160
And in the automate with Kubernetes,
which utilize liveliness props, readiness

226
00:14:10,160 --> 00:14:15,140
checks, and HPO policies to create
self feeling systems ensure graceful

227
00:14:15,140 --> 00:14:17,330
recovery with minimal intervention.

228
00:14:17,830 --> 00:14:21,700
In the comprehensive observability,
which implement Prometheus and Grafana

229
00:14:21,700 --> 00:14:26,620
monitoring with metric baseline lag
detection and visual heat maps to

230
00:14:26,770 --> 00:14:29,170
uphold SLA complaints in critical.

231
00:14:29,290 --> 00:14:35,250
And so as we look ahead, the future of
SRE and Kubernetes is heading toward

232
00:14:35,280 --> 00:14:40,840
even greater abstraction as and in the
serverless orchestration, policy driven

233
00:14:40,870 --> 00:14:43,090
automation and AI assisted operations.

234
00:14:43,930 --> 00:14:48,130
The combination of AI based
anomaly detection with even driven

235
00:14:48,130 --> 00:14:52,480
remediation will reshape how we
approach reliability at scale.

236
00:14:53,320 --> 00:14:57,790
The next phase is not just about,
it's about the system intelligence.

237
00:14:58,280 --> 00:15:01,860
Platforms that learn from
incidents predict, degradation,

238
00:15:01,920 --> 00:15:03,690
and adapt autonomously.

239
00:15:04,190 --> 00:15:08,240
This is where the boundary
between human insight and mission

240
00:15:08,240 --> 00:15:10,400
position to truly start to.

241
00:15:10,900 --> 00:15:13,350
Blood and so that's it.

242
00:15:13,950 --> 00:15:15,810
Thank you for, thank you
all for joining today.

243
00:15:16,140 --> 00:15:20,190
I hope the session gave you a
practical view of how Kubernetes

244
00:15:20,190 --> 00:15:24,290
native design, automation, and
observability can truly elevate

245
00:15:24,530 --> 00:15:26,390
reliability in modern cloud platforms.

246
00:15:26,870 --> 00:15:31,040
It's been great sharing my experience and
I'll love to continue the conversation.

247
00:15:31,340 --> 00:15:33,410
Feel free to contact or reach out.

248
00:15:33,920 --> 00:15:37,310
Would like to discuss resilience,
engineering, or cloud native

249
00:15:37,700 --> 00:15:38,750
modernization further.

250
00:15:39,155 --> 00:15:40,235
Thank you again for your time.

251
00:15:40,475 --> 00:15:40,745
Thank you.

