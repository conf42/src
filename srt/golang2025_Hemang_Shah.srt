1
00:00:00,500 --> 00:00:04,550
Hi, my name is Heman and I'm a
software developer@amazon.com.

2
00:00:05,360 --> 00:00:10,160
I specialize in building machine learning
solutions for identifying and detecting

3
00:00:10,550 --> 00:00:12,080
intellectual property infringement.

4
00:00:12,709 --> 00:00:15,784
So today I'm here to give a talk
on machine learning approaches.

5
00:00:16,314 --> 00:00:18,625
YP protections and cloud environments.

6
00:00:19,405 --> 00:00:21,205
Let's go over the agenda
for the presentation.

7
00:00:21,955 --> 00:00:25,854
We'll start off with discussing what
are the key challenges or problems

8
00:00:25,884 --> 00:00:27,625
that when it comes to IP protection.

9
00:00:28,225 --> 00:00:33,175
Then we'll go over some of the techniques
and methodologies to identify use of ip.

10
00:00:33,955 --> 00:00:38,095
Then we'll go over some of the ML models
and techniques that you can use to

11
00:00:38,095 --> 00:00:41,415
detect illegal or violation of IP use.

12
00:00:41,665 --> 00:00:45,025
then we'll talk about how to
make these systems more scalable

13
00:00:45,175 --> 00:00:46,555
by deploying them in cloud.

14
00:00:46,825 --> 00:00:51,655
And finally, we'll go over how we
can use Colan to improve performance

15
00:00:51,655 --> 00:00:52,825
of these similar pipelines.

16
00:00:53,325 --> 00:00:56,805
Intellectual property theft
is a huge issue to companies.

17
00:00:57,195 --> 00:01:02,235
it's estimated that globally companies
spend almost $600 billion every year.

18
00:01:02,505 --> 00:01:06,525
Fighting IP theft and 60% of
these cases are all digital.

19
00:01:07,025 --> 00:01:11,715
Predominantly this industry of,
intellectual property protections has

20
00:01:11,715 --> 00:01:18,175
been very manual, but the advent of
machine learning systems, we've been

21
00:01:18,175 --> 00:01:20,305
able to automate a lot of the processes.

22
00:01:20,655 --> 00:01:21,795
and improve detection.

23
00:01:22,395 --> 00:01:27,495
ML systems are just as good as humans
when it comes to identifying patterns

24
00:01:27,495 --> 00:01:30,975
or repetitive patterns, and that's
one of the major reasons why we've

25
00:01:30,975 --> 00:01:32,925
been able to automate a lot of these.

26
00:01:33,425 --> 00:01:37,505
So what are some of the critical
challenges in IP protection and how

27
00:01:37,505 --> 00:01:39,995
can ML help solve these challenges?

28
00:01:40,655 --> 00:01:44,435
As I just mentioned, automation
was one of the key cha, key issues.

29
00:01:44,635 --> 00:01:49,960
this industry was predominantly manual
and a human could take, only a hundred to

30
00:01:49,960 --> 00:01:55,755
200 cases in an R, but with ML systems,
we can process and identify almost.

31
00:01:55,945 --> 00:01:59,785
A hundred thousand occurrences of
IP infringements within an R. one of

32
00:01:59,785 --> 00:02:04,285
the key problems for this industry
is pattern recognition, because

33
00:02:04,825 --> 00:02:09,565
when it comes to IP protection, you
need to be able to identify use of,

34
00:02:09,595 --> 00:02:14,245
logos, copyrights, and such product
protected assets in different places.

35
00:02:14,755 --> 00:02:20,935
So machine learning is, this is a really
good use case for machine learning.

36
00:02:21,835 --> 00:02:26,215
with the advent of convolutional neural
networks, pattern recognition become,

37
00:02:26,365 --> 00:02:29,125
became very easy for, ML systems.

38
00:02:29,245 --> 00:02:34,515
And with the recent advancement of the
transformer architecture, of identifying

39
00:02:34,725 --> 00:02:39,195
nuances or identifying edge cases where
it comes to pattern recognition has just

40
00:02:39,195 --> 00:02:41,175
become even more easier for machines.

41
00:02:41,475 --> 00:02:44,355
The other problem is how to
make it more scalable, right?

42
00:02:44,725 --> 00:02:46,195
this is where cloud comes in.

43
00:02:46,315 --> 00:02:48,640
Cloud-based ML systems can handle almost.

44
00:02:49,435 --> 00:02:51,295
Like huge volume.

45
00:02:51,335 --> 00:02:54,150
it can easily handle
traffic spikes and whatnot.

46
00:02:54,900 --> 00:02:58,829
And with the recent advancements
that we are seeing, that it comes to

47
00:02:59,010 --> 00:03:04,250
training and inferences, inference of,
a lot of, GPU based chips being built,

48
00:03:04,340 --> 00:03:06,990
which are very efficient, it can help.

49
00:03:07,050 --> 00:03:11,070
Train ML models in shorter time,
it can handle very high throughput

50
00:03:11,070 --> 00:03:12,820
inference, inference volumes.

51
00:03:13,270 --> 00:03:18,395
so with that, we're seeing the costs
almost reduce 20 to 30% year over year,

52
00:03:18,905 --> 00:03:20,345
when it comes to deploying ML pipelines.

53
00:03:20,845 --> 00:03:23,515
So let's get into the
process of IP protection.

54
00:03:24,205 --> 00:03:26,845
I would say IP protection
is a two step process.

55
00:03:27,055 --> 00:03:30,895
Step one is discovery,
and step two is detection.

56
00:03:31,395 --> 00:03:36,030
Discovery is basically searching
or finding the use of your ip.

57
00:03:36,530 --> 00:03:41,040
Step two is detecting if this
use is indeed, fraudulent or not.

58
00:03:41,340 --> 00:03:44,040
let's focus on the search part first.

59
00:03:44,730 --> 00:03:47,610
There are two foundational
search methodologies.

60
00:03:47,670 --> 00:03:51,540
One is keyword based search, and
the other is embedding based search.

61
00:03:52,410 --> 00:03:56,880
Keyword based search is basically
searching for use of keywords.

62
00:03:57,030 --> 00:04:00,180
It's predominantly a text-based
search mechanism where you try

63
00:04:00,180 --> 00:04:02,010
to search if certain keywords.

64
00:04:02,055 --> 00:04:05,445
Are present in a subset
of your search base.

65
00:04:05,845 --> 00:04:09,655
so say you have a hundred million
products, or a hundred million web

66
00:04:09,835 --> 00:04:13,255
pages, or a hundred million books,
and you're trying to search for the

67
00:04:13,255 --> 00:04:20,485
trademark, Nike being used in, which
of these web pages, does mention Nike?

68
00:04:21,065 --> 00:04:25,900
you could employ keyword based
search mechanisms to identify the

69
00:04:25,900 --> 00:04:27,635
pages, which use the term Nike.

70
00:04:27,815 --> 00:04:33,425
F the plus point of keyword based, systems
is that, they offer high throughput.

71
00:04:33,665 --> 00:04:37,745
The compute that it uses is min,
minimal or lesser compared to embedding

72
00:04:37,745 --> 00:04:43,415
based searches, and that's why you can
search across hundreds of millions of,

73
00:04:43,515 --> 00:04:45,895
documents within a fraction of a second.

74
00:04:46,775 --> 00:04:51,215
on the contrary, the drawback is
that it lacks context awareness.

75
00:04:51,775 --> 00:04:56,295
It would, given that it is very
rudimentary and string based search

76
00:04:56,295 --> 00:05:00,495
operation, it does not actually
understand the context or the meaning

77
00:05:00,495 --> 00:05:04,755
behind the search query and the meaning
behind the use of the search term in

78
00:05:04,845 --> 00:05:06,255
the documents that it's searching.

79
00:05:07,015 --> 00:05:09,625
so that's where embedding
based search comes handy.

80
00:05:10,085 --> 00:05:14,525
embedding based search is a lot more
context aware search where you employ

81
00:05:14,525 --> 00:05:16,565
different machine learning models.

82
00:05:16,895 --> 00:05:22,505
To convert your search query or your
document into embeddings and these

83
00:05:22,595 --> 00:05:27,525
ML models to the job of extracting
context from your text, or image.

84
00:05:27,855 --> 00:05:33,015
And store these context in the
form of vector representations

85
00:05:33,105 --> 00:05:34,845
or numeric representations.

86
00:05:35,415 --> 00:05:41,855
So because of such, mechanisms, it is
able to gather context out of your query

87
00:05:41,855 --> 00:05:44,375
and do a much more targeted search.

88
00:05:45,020 --> 00:05:49,150
but the drawback of embedding based
search is that, it uses a lot of compute,

89
00:05:49,420 --> 00:05:52,350
so it's hard to, make it very scalable.

90
00:05:52,680 --> 00:05:57,510
If you want to use embedding based
search mechanisms on a billion scale,

91
00:05:57,840 --> 00:06:02,520
your queries tend to perform much more
slower than keyboard based search.

92
00:06:03,370 --> 00:06:07,390
The gold standard generally is
to use a hybrid solution so that

93
00:06:07,390 --> 00:06:09,100
you get the best of both worlds.

94
00:06:09,400 --> 00:06:14,140
You get the high throughput, advantages
from keyboard based search, and

95
00:06:14,140 --> 00:06:18,230
you get high precision, related
advantages from embedding based search.

96
00:06:18,650 --> 00:06:20,720
what we generally do is first employee.

97
00:06:21,360 --> 00:06:24,150
Keyword based search to
narrow down your search base.

98
00:06:24,420 --> 00:06:28,710
And then when you have a base, you
use embedding based search to just

99
00:06:28,710 --> 00:06:31,650
give you results that are highly
accurate to your search query.

100
00:06:32,349 --> 00:06:37,809
if I were to give an example, say you
have 500 million web pages and from these

101
00:06:37,809 --> 00:06:40,239
500 million web pages, you only want.

102
00:06:40,340 --> 00:06:47,090
Web pages that use, which have images
of Nike Air Jordans, the shoes,

103
00:06:47,539 --> 00:06:54,770
what you can first do is first use
keyword based search to find all the

104
00:06:54,770 --> 00:06:59,570
web pages that have mentioned of the
term shoes or any of its synonyms.

105
00:07:00,070 --> 00:07:05,195
And by doing this, you might come,
you might reduce your space from.

106
00:07:05,445 --> 00:07:10,545
500 million to just 10 million web pages,
which have mention of the term shoes.

107
00:07:10,965 --> 00:07:15,980
And then you do a embedding based
search, where you generate embeddings

108
00:07:15,980 --> 00:07:18,200
for the images of Nike Air Jordans.

109
00:07:18,200 --> 00:07:23,900
And you then, generate embeddings of,
these 10 million web pages and see which.

110
00:07:24,170 --> 00:07:26,480
do a sort of a similarity search.

111
00:07:26,630 --> 00:07:31,219
K and then similarity search to identify
which webpages from these 10 million

112
00:07:31,430 --> 00:07:36,830
actually have the use of a shoe, which is
very similar, looking to Nike Air Jordans.

113
00:07:37,280 --> 00:07:41,419
So with this, you might be just
left behind with two, 2000 or 3000,

114
00:07:41,719 --> 00:07:46,399
webpages that have use of images
of a shoe that are very similar.

115
00:07:46,399 --> 00:07:47,660
Looking to a Nike Air Journal.

116
00:07:48,160 --> 00:07:52,630
Now that we've seen some of the search
techniques on how to, shrink your

117
00:07:52,630 --> 00:07:57,300
search space or just find relevant
documents, from your search space,

118
00:07:57,570 --> 00:08:01,830
let's look at some of the techniques
on how to detect IP infringement.

119
00:08:02,669 --> 00:08:06,040
we'll look at two, two domains, broadly.

120
00:08:06,099 --> 00:08:06,910
One is.

121
00:08:07,595 --> 00:08:10,835
image-based IP detection, and
one is text-based IP detection.

122
00:08:11,185 --> 00:08:13,255
let's look at image-based
IP detection first.

123
00:08:13,765 --> 00:08:18,475
There are three main, use cases or
problem statements in this category.

124
00:08:18,795 --> 00:08:24,585
one is the identifying
use of logos in images.

125
00:08:24,645 --> 00:08:28,575
The other could be identifying use
of visual trademarks or copyrights.

126
00:08:29,085 --> 00:08:30,615
And the third is.

127
00:08:30,720 --> 00:08:35,520
detecting manipulation of a
copyright to a certain degree.

128
00:08:36,510 --> 00:08:42,520
so logo detection is basically identifying
use of brands, logos in, images.

129
00:08:43,120 --> 00:08:47,200
there are a few ML models that are
doing really good job in this space.

130
00:08:47,330 --> 00:08:52,900
one of the, one of the state of the art
models is the YOLO family of models.

131
00:08:53,110 --> 00:08:54,880
YOLO stands for you only look once.

132
00:08:55,515 --> 00:09:01,775
These are predominantly object detection
models, but it's really easy to pick a,

133
00:09:01,835 --> 00:09:07,655
one of the, pre-trained yellow model and
fine tune it for, for a data set that

134
00:09:07,655 --> 00:09:12,075
only contains, annotated logos, in images.

135
00:09:12,285 --> 00:09:15,945
And the output model that
you get after fine tuning is.

136
00:09:16,445 --> 00:09:22,705
A model that easily detects, a logo in
an image when you pass an image to it.

137
00:09:23,245 --> 00:09:28,265
And these are very lightweight models,
so you can blast it up at, 20, 30,000

138
00:09:28,265 --> 00:09:33,575
transactions per second and you can
really get good, good throughput out

139
00:09:33,575 --> 00:09:35,975
of these models for lesser costs.

140
00:09:36,725 --> 00:09:41,825
these models generally have a high
accuracy upwards of 96, 90 7%, and

141
00:09:41,825 --> 00:09:47,065
they allow for high throughput as well
when it comes to identifying use of,

142
00:09:47,315 --> 00:09:50,135
a brand's or an artist's copyright.

143
00:09:50,575 --> 00:09:51,025
the.

144
00:09:51,385 --> 00:09:56,635
VIT, which stands for Vision Transformer
Models or the VLM Visual Language Models

145
00:09:56,695 --> 00:10:00,185
has been really useful, in these domains.

146
00:10:00,545 --> 00:10:00,940
These are.

147
00:10:01,925 --> 00:10:07,985
Predominantly multimodal models where
they take an image, gather as much as

148
00:10:07,985 --> 00:10:10,775
information as possible from this image.

149
00:10:11,065 --> 00:10:15,565
they also convert this, the image
into textual format and then try to

150
00:10:16,065 --> 00:10:17,955
understand what is there in this image.

151
00:10:18,265 --> 00:10:22,975
so these models really help
identify, use of, copyrights such

152
00:10:22,975 --> 00:10:26,615
as, Fictional characters being
present, present in content.

153
00:10:27,005 --> 00:10:32,295
And they, these models are much
more larger than some of the logo

154
00:10:32,295 --> 00:10:33,975
detection models such as yellow models.

155
00:10:34,495 --> 00:10:40,225
but, and therefore they take much more
time for, inference than the logo models.

156
00:10:40,225 --> 00:10:44,425
But these, these have really
high precision and low FB rates.

157
00:10:45,350 --> 00:10:49,820
The third problem, image manipulation is
a much more simpler problem than the other

158
00:10:49,820 --> 00:10:56,580
two because this is, the, the solution
that you could employ is very simple.

159
00:10:56,850 --> 00:10:59,780
You just use, some
image-based embedding models.

160
00:10:59,815 --> 00:11:06,135
To generate embeddings of your artwork or
your asset, and then, generate embeddings

161
00:11:06,165 --> 00:11:11,085
of other images that are there on the
internet and see how similar are these

162
00:11:11,085 --> 00:11:16,405
two vectors, if the cosign similarity
between, these, these two vectors.

163
00:11:16,630 --> 00:11:22,010
your vector and the vector of, image that
you fetch from the internet are very high.

164
00:11:22,250 --> 00:11:26,480
Then it means that it's a very similar
looking image or very similar looking

165
00:11:26,510 --> 00:11:28,760
artwork with just a few nuances.

166
00:11:29,030 --> 00:11:32,840
You don't even need a multimodal
model for such use cases.

167
00:11:33,070 --> 00:11:36,080
just a pure image based
embedding model, does the job.

168
00:11:36,330 --> 00:11:38,670
next comes text-based IP protection.

169
00:11:39,200 --> 00:11:42,470
so let's talk about text
similarity detection first.

170
00:11:42,720 --> 00:11:43,440
just like.

171
00:11:43,845 --> 00:11:45,435
Image-based embedding models.

172
00:11:45,465 --> 00:11:50,565
There are a lot of text-based embedding
models, which, have, been j been trained

173
00:11:50,565 --> 00:11:52,275
on the entirety of the dictionary.

174
00:11:52,575 --> 00:11:56,675
And, the probability of occurrence
of every word in the dictionary

175
00:11:57,035 --> 00:12:00,945
to rank, different words in the
dictionary and generate embeddings

176
00:12:00,975 --> 00:12:03,125
for any given, text phrase.

177
00:12:03,635 --> 00:12:09,115
these models can be pretty useful
in, generating embeddings of.

178
00:12:09,520 --> 00:12:15,475
a text phrase and seeing,
if you if, another phrase.

179
00:12:16,005 --> 00:12:19,750
out there in the search space is
very similar to your trademark text.

180
00:12:20,120 --> 00:12:23,170
some of the use cases where this
comes in very handy, we see in

181
00:12:23,170 --> 00:12:25,750
the industry is for plagiarism.

182
00:12:25,750 --> 00:12:30,760
Check, for checking if research papers
are very similar to, very similar

183
00:12:30,760 --> 00:12:36,350
of a research paper in the past,
or, Or text-based original content.

184
00:12:36,560 --> 00:12:41,970
For example, books, books of authors,
if they're being infringed on, text

185
00:12:42,030 --> 00:12:46,190
of another book that has been, you
know, launched or released in the past.

186
00:12:46,490 --> 00:12:49,160
the other use case is
brand misrepresentation.

187
00:12:49,610 --> 00:12:56,070
So what in the, in the retail
industry, what we see is that.

188
00:12:56,580 --> 00:13:01,260
Once a brand becomes famous, there
are a lot of knock knockoffs that

189
00:13:01,260 --> 00:13:05,040
come, come into the market, which,
sound very similar to this brand.

190
00:13:05,400 --> 00:13:10,890
some of the famous examples are Adidas
and, ABAs, where the second D is

191
00:13:10,890 --> 00:13:16,990
converted to a B. just to, just to,
Have a difference in the name, but

192
00:13:16,990 --> 00:13:22,320
have the design language and everything
that's very similar to, Aidas and,

193
00:13:22,930 --> 00:13:29,950
some of these NLP techniques where
you can, normalize, text using a lot

194
00:13:29,950 --> 00:13:34,480
of different tokenizes and a lot of
different packages that are given out.

195
00:13:34,510 --> 00:13:37,900
A lot of Python packages that are
available to normalize your text

196
00:13:38,200 --> 00:13:42,760
and then perform a similarity search
is helpful for such use cases.

197
00:13:43,330 --> 00:13:47,890
And then comes, the l LMS that
we've seen in the recent, past

198
00:13:48,070 --> 00:13:50,050
where these are multilingual.

199
00:13:50,380 --> 00:13:53,680
it can perform similarity
search across languages.

200
00:13:54,005 --> 00:13:59,435
so it has a machine translation layer,
which normalizes, text before generating,

201
00:13:59,465 --> 00:14:01,295
embedding into a single language.

202
00:14:01,545 --> 00:14:07,765
u using, using the same ip, or the same
trademarks, but in a different language,

203
00:14:07,865 --> 00:14:14,275
can also be now identified using, such
models, deploying ML pipelines in cloud.

204
00:14:14,395 --> 00:14:21,555
Helps reduce costs by a large percentage
because, cloud is generally a multi-tenant

205
00:14:21,615 --> 00:14:25,605
environment where multiple organizations
or multiple clients deploy their

206
00:14:25,605 --> 00:14:28,875
workloads and you only pay as you need.

207
00:14:29,085 --> 00:14:34,065
You only pay for the services or for
the duration that you actually use.

208
00:14:34,175 --> 00:14:40,205
an instance or a cloud resource,
given that these are huge data

209
00:14:40,205 --> 00:14:44,615
centers, they high, they offer
really high availability, with.

210
00:14:45,150 --> 00:14:49,660
a lot of replication factor in place
such that, your instances are deployed

211
00:14:49,660 --> 00:14:55,080
and replicated in different, zones in
multiple, data centers across the world

212
00:14:55,080 --> 00:14:58,530
to offer high system availability.

213
00:14:58,830 --> 00:15:03,570
the use of GPUs is a lot more
efficient when it comes to cloud.

214
00:15:03,570 --> 00:15:11,360
Also, having our own GPUs, on an
on-premise system is a very expensive, is

215
00:15:11,360 --> 00:15:16,940
an, is a very expensive affair, especially
if you are underutilizing the GPU.

216
00:15:17,060 --> 00:15:22,150
If your workloads aren't, continuous,
continuously high or maxing out the

217
00:15:22,150 --> 00:15:29,020
GPU, then you're paying a lot of money
for the GPUs when utilizing very less,

218
00:15:29,650 --> 00:15:34,690
with cloud, this becomes cheaper because
there are multiple tenants and everyone

219
00:15:34,930 --> 00:15:37,360
reuses the same GPU when required.

220
00:15:37,420 --> 00:15:42,680
Let's look at some of the techniques
that, one can use to optimize or improve

221
00:15:42,680 --> 00:15:45,650
performance of ML systems or ML models.

222
00:15:46,415 --> 00:15:50,285
there are four main things, main
approaches that we look at, model

223
00:15:50,345 --> 00:15:55,335
parallelism, data parallelism,
model, shing, and quantization.

224
00:15:55,935 --> 00:15:59,265
So model parallelism is something
that you can use when your models

225
00:15:59,295 --> 00:16:04,315
are very large, where, model might
not fit on a single GPU instance.

226
00:16:04,585 --> 00:16:09,255
So what you do is you split the model,
where certain layers you, split the

227
00:16:09,255 --> 00:16:11,415
model horizontally across layers.

228
00:16:11,680 --> 00:16:18,180
And deploy the chunk of the model on
different, GPO instances and invoke

229
00:16:18,450 --> 00:16:23,460
each model, each instance one of the
other, where the output of the first

230
00:16:23,520 --> 00:16:27,120
instance goes as the input of the other
instance, and then you can reuse the

231
00:16:27,120 --> 00:16:31,320
first instance once the it has finished
processing for the next request.

232
00:16:31,620 --> 00:16:35,460
So this is where you can split
your model across multiple GPUs.

233
00:16:35,825 --> 00:16:39,335
and this is generally useful when your
models are very large and you want

234
00:16:39,335 --> 00:16:41,565
to use smaller GPO instance sizes.

235
00:16:41,775 --> 00:16:45,825
And of course, when the model is built
in such a way that it allows for,

236
00:16:46,195 --> 00:16:48,025
splitting up the layers of the model.

237
00:16:48,275 --> 00:16:49,805
then comes data parallelism.

238
00:16:49,955 --> 00:16:53,975
So this is useful when you
are underutilizing A GPU.

239
00:16:54,315 --> 00:16:58,285
it is pos you can deploy multiple
instances of a very small model

240
00:16:58,525 --> 00:17:03,295
on the same GPU and or across
different GPUs as well, and.

241
00:17:03,555 --> 00:17:08,380
and batch your data and invoke
it across all the different

242
00:17:08,380 --> 00:17:10,580
instances, for inferences.

243
00:17:10,980 --> 00:17:15,730
this helps, boost the throughput as
well because now you are utilizing your

244
00:17:15,730 --> 00:17:18,610
memory and compute more efficiently.

245
00:17:19,110 --> 00:17:20,580
Third comes model shouting.

246
00:17:20,880 --> 00:17:22,260
So model shouting is.

247
00:17:23,025 --> 00:17:27,005
A little similar to model parallelism,
but works a bit differently.

248
00:17:27,335 --> 00:17:32,885
In Model Sharding, you vertically split
the model into a number of pieces when

249
00:17:32,975 --> 00:17:36,775
it's hard to deploy, the model on a
single instance because it's too large.

250
00:17:37,075 --> 00:17:42,545
So you shard the model, in such a way
that your input data also can be shorted.

251
00:17:42,870 --> 00:17:47,580
when you want to invoke the model for
inference, so the model which is now

252
00:17:47,580 --> 00:17:52,250
charted into four or five different pieces
are, deployed on different instances.

253
00:17:52,400 --> 00:17:57,770
And when you invoke the model, your
input tensor also can be broken down into

254
00:17:58,070 --> 00:18:03,320
corresponding pieces and sent to the,
Respective shot and the output can then

255
00:18:03,320 --> 00:18:05,540
be concatenated to generate the response.

256
00:18:06,420 --> 00:18:12,150
the last one is quantization, so this
helps lower the size of the model by

257
00:18:12,150 --> 00:18:13,405
compressing the weights of the model.

258
00:18:13,875 --> 00:18:18,605
So say you, compress or convert
the weights of the model from

259
00:18:18,655 --> 00:18:22,735
a floating point, a 32 bit
to a 16 bit floating point.

260
00:18:22,945 --> 00:18:27,955
You reduce, the size of the model
and also the memory required

261
00:18:27,955 --> 00:18:29,275
for inference of the model.

262
00:18:29,775 --> 00:18:34,695
So let's talk about how Golan
can be helpful in, ML pipelines.

263
00:18:35,025 --> 00:18:38,115
So before we get into that,
some overview about Go.

264
00:18:38,435 --> 00:18:41,345
it's an open source programming
language developed by Google,

265
00:18:41,765 --> 00:18:44,605
and, the best part about it is.

266
00:18:44,885 --> 00:18:50,455
It's, speed, because it compiles
to, machine code, its performance,

267
00:18:50,505 --> 00:18:53,215
similar on the likes of, CCC plus.

268
00:18:53,455 --> 00:18:57,025
it's really fast, compared to
programming languages like Java,

269
00:18:57,025 --> 00:19:01,435
which have a lot of overhead because
of JVM or Java Virtual machines.

270
00:19:01,885 --> 00:19:02,095
it's.

271
00:19:02,605 --> 00:19:06,475
very cloud native and designed
because it seamlessly integrates

272
00:19:06,475 --> 00:19:07,945
with, Docker, Kubernetes.

273
00:19:08,395 --> 00:19:13,065
it's really useful to build
containerized, APIs, or services,

274
00:19:13,165 --> 00:19:14,455
basically microservices.

275
00:19:15,205 --> 00:19:19,845
And, it offers high concurrency with,
built in go routines and channels,

276
00:19:20,105 --> 00:19:23,735
which enable multitasking and
help build high through pipelines.

277
00:19:23,985 --> 00:19:28,485
so specifically when it comes to ML
pipelines, the biggest use case or biggest

278
00:19:28,485 --> 00:19:33,045
place where go could be useful is building
streaming, high throughput streaming

279
00:19:33,045 --> 00:19:38,375
pipeline, where, you have, incoming, data
in a streaming fashion, where millions of.

280
00:19:39,115 --> 00:19:41,455
records are coming in
every minute or every hour.

281
00:19:41,765 --> 00:19:47,835
you use, a sort of a Kafka plus co
consumer set up to react to these

282
00:19:47,895 --> 00:19:52,765
incoming reactions or incoming signals
and, invoke machine learning models.

283
00:19:53,320 --> 00:19:58,930
so you could use, A GRP, GRPC client,
to invoke remote procedure calls, and

284
00:19:58,930 --> 00:20:01,870
go to a PyTorch or a TensorFlow backend.

285
00:20:02,350 --> 00:20:06,990
it really helps, convert a lot
of asynchronous applications into

286
00:20:06,990 --> 00:20:10,970
synchronous or real time processing
applications, because of its throughput

287
00:20:11,090 --> 00:20:13,250
and low latency capabilities.

288
00:20:14,125 --> 00:20:20,275
And it's really easy to deploy these
services on cloud because of it's,

289
00:20:20,375 --> 00:20:24,925
it's, integration with Docker and
how we can containerize these web

290
00:20:24,925 --> 00:20:29,535
services and deploy, in containerize
web, services platforms such as.

291
00:20:29,905 --> 00:20:32,455
ECS provided by AWS and so on.

292
00:20:32,955 --> 00:20:37,125
Looking ahead, there's a lot more to be
achieved in this space of IP protection.

293
00:20:37,625 --> 00:20:42,005
there's a lot more automation that is
required and for which there is scope for.

294
00:20:42,325 --> 00:20:47,505
where, AI is, or machine learning is able
to identify most of the patterns, but

295
00:20:47,625 --> 00:20:52,485
the novel approaches taken by bad actors
are still being identified by humans.

296
00:20:53,085 --> 00:20:56,835
there's research being done on how
to use blockchain and blockchain

297
00:20:56,835 --> 00:21:02,095
based ledgers for verification and,
determining ownership of IP assets.

298
00:21:02,605 --> 00:21:03,205
And there's.

299
00:21:03,705 --> 00:21:08,505
With the time we are seeing adaptive
AI systems, where we use a lot of

300
00:21:08,505 --> 00:21:14,840
these lms, to make themself learn and
help them identify the new patterns,

301
00:21:14,940 --> 00:21:19,500
that bad actors can develop over time
without a human having to tell them

302
00:21:19,500 --> 00:21:21,840
what to do or pass training information.

303
00:21:22,340 --> 00:21:23,390
Thank you so much for your time.

304
00:21:23,480 --> 00:21:25,130
Hope this was a informative session.

