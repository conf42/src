1
00:00:00,190 --> 00:00:00,850
Hi, everyone.

2
00:00:01,470 --> 00:00:06,120
My name is Mohammad Ahmad Saeed, and
today I'm going to talk about chaos

3
00:00:06,150 --> 00:00:08,910
engineering for resilient microservices.

4
00:00:09,410 --> 00:00:12,870
First, let's look at a typical
microservices architecture.

5
00:00:13,380 --> 00:00:18,090
as we can see in the diagram, we
have multiple microservices, and

6
00:00:18,189 --> 00:00:20,390
each microservice has a dependency.

7
00:00:21,070 --> 00:00:24,690
The two main dependencies for our
microservices are the databases,

8
00:00:25,200 --> 00:00:27,300
and other microservices.

9
00:00:27,580 --> 00:00:31,820
So for instance, in this diagram,
we have a microservice one, which

10
00:00:31,820 --> 00:00:36,459
is dependent on the database one,
and it is also communicating with

11
00:00:36,460 --> 00:00:38,630
microservice two via a queue.

12
00:00:39,350 --> 00:00:43,740
But if we look at The communication
between microservice two and

13
00:00:43,740 --> 00:00:48,020
microservice three, it is
happening, via a direct HTTP calls.

14
00:00:48,620 --> 00:00:54,960
So the point to raise here is,
each microservice has dependency.

15
00:00:54,980 --> 00:00:59,350
It can either be database or it
can be either, other microservices.

16
00:00:59,920 --> 00:01:03,870
There are also other type of dependencies,
which will, we will look into it later.

17
00:01:04,370 --> 00:01:07,110
Now let's look at what are our goals.

18
00:01:07,130 --> 00:01:09,460
So we have three main goals here.

19
00:01:09,740 --> 00:01:14,110
The first is to uncover
weaknesses in our system.

20
00:01:14,800 --> 00:01:18,980
it can also include like identifying
single point of failures.

21
00:01:19,260 --> 00:01:24,720
So what happens if one of the
microservice goes down or if the

22
00:01:24,720 --> 00:01:28,870
dependency like a database, so if
the database, is overloaded, if the

23
00:01:28,870 --> 00:01:34,460
database is responding slowly, so What
is the behavior of our microservices?

24
00:01:34,460 --> 00:01:36,100
So that's the first goal.

25
00:01:36,140 --> 00:01:39,810
We have to identify the
weaknesses in our system.

26
00:01:40,720 --> 00:01:47,050
The next is to improve system robustness
and that actually means that we have

27
00:01:47,050 --> 00:01:52,090
to make our system more robust and
strong and By increasing the stability

28
00:01:52,150 --> 00:01:58,430
and keeping the performance within a
defined range So the performance of

29
00:01:58,460 --> 00:02:03,860
every microservice is doesn't gets
worse and worse with time If anything

30
00:02:03,870 --> 00:02:07,930
happens in the system the microservice
the performance of that microservice

31
00:02:07,950 --> 00:02:10,330
It should stay within a defined range.

32
00:02:11,110 --> 00:02:15,610
The third is to handle
unexpected failures gracefully.

33
00:02:16,060 --> 00:02:19,910
So there can be a lot of
unexpected failures in the

34
00:02:19,910 --> 00:02:21,720
microservices architecture.

35
00:02:21,760 --> 00:02:26,730
the most typical ones are the
network issues, the dependency,

36
00:02:26,780 --> 00:02:31,440
failures, the bad configurations,
the data consistency issues.

37
00:02:31,850 --> 00:02:35,830
So there can be a lot of, failures,
a lot of different type of failures,

38
00:02:35,880 --> 00:02:37,440
in the microservices architecture.

39
00:02:37,480 --> 00:02:43,885
and In case of any failure, we should
ensure that the, that failure is

40
00:02:43,915 --> 00:02:48,615
handled gracefully and one failure
doesn't take the whole service down.

41
00:02:49,115 --> 00:02:53,785
Let's look at why is chaos engineering
critical for microservices.

42
00:02:54,365 --> 00:02:57,275
So microservices are complex.

43
00:02:57,455 --> 00:02:59,905
They have a distributed complexity.

44
00:03:00,445 --> 00:03:03,605
Microservices, they have
like many moving parts.

45
00:03:04,180 --> 00:03:09,230
which can be like APIs, which can be
databases, which can be messaging queues,

46
00:03:09,240 --> 00:03:11,830
which can be a third party services.

47
00:03:12,140 --> 00:03:16,920
So failure in one service can
ripple through the system.

48
00:03:17,200 --> 00:03:23,030
That's one of the complexity that, that
is very necessary to actually, look into.

49
00:03:23,740 --> 00:03:26,570
The next is the network dependencies.

50
00:03:26,860 --> 00:03:30,985
So As we have seen in the previous
slide, microservices, they do

51
00:03:31,035 --> 00:03:36,875
communicate over networks and networks
are prone to latency, a packet

52
00:03:36,945 --> 00:03:42,465
loss, outages, there can be a lot of
stuff which go wrong in the network.

53
00:03:43,205 --> 00:03:45,565
The third thing is the dynamic scaling.

54
00:03:45,885 --> 00:03:50,715
So microservices, they often run in
the containerized environment like

55
00:03:51,015 --> 00:03:53,475
Kubernetes, that dynamically scale.

56
00:03:53,965 --> 00:03:58,485
And that actually makes it
harder to predict failure modes.

57
00:03:58,745 --> 00:04:04,065
one example is, did this service
scale properly when there was

58
00:04:04,105 --> 00:04:05,685
a lot of load on the system?

59
00:04:06,564 --> 00:04:08,404
the next is the frequent deployments.

60
00:04:09,184 --> 00:04:14,414
There is a continuous deployment
in microservices, like for

61
00:04:14,414 --> 00:04:17,644
instance, microservice one is
getting deployed every day.

62
00:04:17,874 --> 00:04:20,344
Microservice two is getting
deployed every week.

63
00:04:20,374 --> 00:04:24,604
And actually that increases the
risk of reducing, sorry, the risk

64
00:04:24,604 --> 00:04:27,304
of introducing, bugs or regressions.

65
00:04:27,334 --> 00:04:31,344
with proper cross engineering,
we can make sure that.

66
00:04:31,809 --> 00:04:35,329
If there is any failure in
the system, it doesn't take

67
00:04:35,339 --> 00:04:37,239
the whole system down with it.

68
00:04:37,739 --> 00:04:40,159
Let's look at what are the microservices.

69
00:04:41,099 --> 00:04:45,869
So the very first is the service
failures and the service failures like

70
00:04:45,889 --> 00:04:50,119
in, we have to basically simulate the
failure of individual microservice.

71
00:04:50,619 --> 00:04:54,409
The most common one in every
system, at least what I have seen

72
00:04:54,459 --> 00:04:56,029
is the authentication service.

73
00:04:56,039 --> 00:04:59,419
So as soon as the authentication
service is down, it takes.

74
00:05:00,054 --> 00:05:02,634
Most of the other services
down with it because they are

75
00:05:02,904 --> 00:05:04,374
dependent on that service.

76
00:05:04,614 --> 00:05:10,494
So here we have to test how the system
handles service availability or crashes.

77
00:05:10,504 --> 00:05:14,914
So if one service is crashing, then is
the other service crashing or is the

78
00:05:14,954 --> 00:05:17,374
other service like giving outage or not?

79
00:05:18,064 --> 00:05:19,574
Then the network issues.

80
00:05:19,904 --> 00:05:24,974
So in, in network issues, we can
introduce, latency, packet loss,

81
00:05:25,024 --> 00:05:30,614
network partitions, and then we have
to test how the system behaves under

82
00:05:30,614 --> 00:05:34,314
slow or unreliable network conditions.

83
00:05:35,294 --> 00:05:40,534
The third is the dependency failures,
independencies failures, we can, like

84
00:05:40,584 --> 00:05:45,204
similar dependence, dependency failure,
like databases, the message queues, the

85
00:05:45,204 --> 00:05:48,544
message brokers, or the third party APIs.

86
00:05:48,584 --> 00:05:52,514
And again, the goal is to make
sure that any dependency failure,

87
00:05:52,924 --> 00:05:54,584
it is handled gracefully.

88
00:05:55,324 --> 00:05:57,574
The next is the load and scalability.

89
00:05:57,584 --> 00:05:58,004
So.

90
00:05:58,824 --> 00:06:04,454
if one of the service is getting excessive
traffic, then, does it auto scale

91
00:06:04,494 --> 00:06:06,324
and handle the load properly or not?

92
00:06:06,404 --> 00:06:10,684
And what is the behavior of other services
which are depending on the service?

93
00:06:11,274 --> 00:06:13,304
the next is the data consistency.

94
00:06:13,484 --> 00:06:20,154
So if, in case of any like data corruption
or delays in the data replication, then

95
00:06:20,154 --> 00:06:24,614
we have to verify like how the system
maintain consistency during failures.

96
00:06:25,479 --> 00:06:27,459
The next is the configuration changes.

97
00:06:27,619 --> 00:06:32,289
we are here, we are testing how the
system reacts to misconfiguration

98
00:06:32,319 --> 00:06:34,519
or sudden changes in configurations.

99
00:06:34,889 --> 00:06:39,249
Does it propagate and take everything down
with it or does it, handle it gracefully?

100
00:06:40,109 --> 00:06:42,219
The next is the cascading failures.

101
00:06:42,309 --> 00:06:48,509
So we have to simulate the scenarios,
where like one failure, it triggers a

102
00:06:48,559 --> 00:06:51,669
chain reaction across multiple services.

103
00:06:52,169 --> 00:06:56,109
Now let's look at what are the
principles of chaos engineering.

104
00:06:56,854 --> 00:07:00,844
in Chaos Engineering, the very first
thing is to define a steady state.

105
00:07:01,394 --> 00:07:05,864
which actually means that we have
to identify the normal behavior

106
00:07:05,924 --> 00:07:08,154
of the system using metrics.

107
00:07:08,244 --> 00:07:13,504
And those metrics can be like,
latency, error rate, throughput.

108
00:07:13,654 --> 00:07:18,624
In normal circumstances where we are
with those metrics, of course, we cannot

109
00:07:18,634 --> 00:07:20,854
be at zero like errors cannot be zero.

110
00:07:20,854 --> 00:07:22,354
There will be some errors for sure.

111
00:07:22,874 --> 00:07:25,704
so we have to first identify the
baselines where we are, where

112
00:07:25,704 --> 00:07:26,924
the system is currently at.

113
00:07:27,594 --> 00:07:32,014
And then the next step is to, then we
have to hypothesize about failures.

114
00:07:32,244 --> 00:07:34,704
So it's more about like hypothesis.

115
00:07:34,704 --> 00:07:38,934
So what do we think that, that
should happen when a failure occurs?

116
00:07:39,284 --> 00:07:46,654
We can predict how our system will
behave And the third step is to actually

117
00:07:46,964 --> 00:07:52,354
Introduce the controlled kiosk which
means we are going to simulate failures

118
00:07:52,384 --> 00:07:57,214
like failures can be like network
delays server crashes then we have

119
00:07:57,214 --> 00:08:02,264
to simulate this but they should all
be in a controlled manner The fourth

120
00:08:02,274 --> 00:08:03,854
thing is to observe and measure.

121
00:08:04,254 --> 00:08:09,594
After we have introduced the kiosk, now
we are checking the response of the system

122
00:08:09,744 --> 00:08:15,234
and the most important thing is to ensure
that it can self recover because we are

123
00:08:15,234 --> 00:08:17,534
doing all of this exercise on production.

124
00:08:18,379 --> 00:08:22,069
The next thing is the automate
and integrate into CICD.

125
00:08:22,479 --> 00:08:23,899
this is a very important step.

126
00:08:24,279 --> 00:08:28,969
Once we have some experiments, chaos
experiments, we have to make sure

127
00:08:28,999 --> 00:08:31,119
that we are doing them continuously.

128
00:08:31,129 --> 00:08:32,949
They are happen, happening frequently.

129
00:08:33,289 --> 00:08:37,889
And, that's the only way that we can
ensure that our service, microservices

130
00:08:37,929 --> 00:08:42,759
are resilient because we are testing
them continuously in our pipelines.

131
00:08:43,259 --> 00:08:46,269
Let's look at the popular
chaos engineering tools.

132
00:08:46,869 --> 00:08:50,739
the very first and the very the
most famous is the chaos monkey.

133
00:08:50,859 --> 00:08:56,659
So chaos monkey is It was developed
by Netflix and what this tool

134
00:08:56,669 --> 00:09:01,809
does is it basically, randomly
terminate instances in production.

135
00:09:02,289 --> 00:09:07,209
to be like, just to make sure that if,
for example, one service is terminated,

136
00:09:07,219 --> 00:09:11,669
one service is killed, then what happens
to the other, other dependencies,

137
00:09:11,689 --> 00:09:13,589
how the system behaves overall.

138
00:09:14,089 --> 00:09:18,539
The next is the Kiosk Mesh, Kiosk
Mesh is the open source platform.

139
00:09:18,609 --> 00:09:23,179
It was built by, it was built for
Kubernetes, but it is like really

140
00:09:23,179 --> 00:09:28,139
powerful and flexible and it
supports diverse, fault injection.

141
00:09:28,289 --> 00:09:31,349
And it also has a web UI
for managing experiments.

142
00:09:31,359 --> 00:09:37,229
So we can manage and run and monitor
different experiments through a really UI.

143
00:09:37,729 --> 00:09:39,509
The next is the Gremlin.

144
00:09:40,019 --> 00:09:44,809
So Gremlin is, it's not an open source,
it's a commercial platform, opening

145
00:09:44,849 --> 00:09:49,579
a wide range, of attacks that can
run against various infrastructure

146
00:09:49,619 --> 00:09:55,639
components, including, Kubernetes,
cloud providers, and in, it also has

147
00:09:55,639 --> 00:10:00,569
a very, user friendly, interface,
and it also has like very extensive

148
00:10:00,609 --> 00:10:05,379
attack library, robust capabilities
and integration with monitoring tools.

149
00:10:05,429 --> 00:10:09,559
it's again like very powerful tool
and it's, used, across the industry.

150
00:10:10,059 --> 00:10:11,439
The next is the Toxy proxy.

151
00:10:11,439 --> 00:10:17,239
So Toxy proxy is, mostly limited
to the network conditions.

152
00:10:17,279 --> 00:10:22,539
it's an open source TCP, proxy to
simulate The network and system

153
00:10:22,539 --> 00:10:27,749
conditions for chaos and resiliency
testing, it, it allows for fine grain

154
00:10:27,749 --> 00:10:32,469
control over the network, impairments
and, network impairments and can

155
00:10:32,469 --> 00:10:34,309
be used like with various tools.

156
00:10:34,319 --> 00:10:38,409
So all of these, four tools,
these are like really powerful

157
00:10:38,449 --> 00:10:40,339
and these are quite famous.

158
00:10:40,839 --> 00:10:45,699
Let's look at the real world case
studies of chaos engineering and the

159
00:10:45,699 --> 00:10:50,989
first example here would be slack as you
all know that slack is a very popular

160
00:10:50,989 --> 00:10:55,549
messaging platform and it is used by
millions of users across the world.

161
00:10:56,359 --> 00:10:57,539
So slack.

162
00:10:57,929 --> 00:11:03,139
is basically conducting regular chaos
experiments to test the resilience of

163
00:11:03,159 --> 00:11:05,629
their services and infrastructure overall.

164
00:11:06,199 --> 00:11:12,189
And they also have like incident response
drills where Slack is using chaos

165
00:11:12,229 --> 00:11:19,329
engineering to simulate incidents and then
checking the responses to those incidents.

166
00:11:19,829 --> 00:11:24,329
By using this chaos engineering
to maintain their, and maintain

167
00:11:24,349 --> 00:11:25,829
and test their infrastructure.

168
00:11:26,199 --> 00:11:31,749
So slack was able to minimize, downtime
and, it made, they maintained a ready

169
00:11:31,749 --> 00:11:33,589
to rival platform for each user.

170
00:11:33,699 --> 00:11:37,139
I think their current reliability is
like 99 something, 99 point something.

171
00:11:37,639 --> 00:11:42,529
Let's look at another real world
case study of Kiosk Engineering and

172
00:11:42,839 --> 00:11:45,489
we can look at Azure Kiosk Studio.

173
00:11:45,829 --> 00:11:52,069
So Azure Kiosk Studio is built by
Microsoft and it is a managed service.

174
00:11:52,569 --> 00:11:57,024
It basically, it uses Kiosk Engineering
to help measure, understand,

175
00:11:57,054 --> 00:11:58,964
and improve service resilience.

176
00:11:59,344 --> 00:12:04,024
Here we can set up some experiments,
we can configure which faults to run,

177
00:12:04,104 --> 00:12:09,354
and we can also select resources,
resources to run our faults against.

178
00:12:09,714 --> 00:12:14,344
And it is used by, a lot of companies
to run experiments and improve

179
00:12:14,374 --> 00:12:17,504
the resilience of their services.

180
00:12:18,004 --> 00:12:22,994
Let's look at the benefits of adopting
kiosk engineering for microservices.

181
00:12:23,004 --> 00:12:27,204
So the very first for any
business is the financial benefit.

182
00:12:27,614 --> 00:12:33,569
in case of a down time, the businesses
suffer, the, if the platform is down,

183
00:12:33,589 --> 00:12:35,259
let's consider an example of bank.

184
00:12:35,279 --> 00:12:40,289
So if the banking system is down, it
will disrupt the whole, the, all the

185
00:12:40,739 --> 00:12:45,849
customers of that bank and it can
result in a significant financial loss.

186
00:12:46,119 --> 00:12:46,859
So here.

187
00:12:47,639 --> 00:12:52,479
Using kiosk engineering to ensure the
stability and resiliency of our services,

188
00:12:52,809 --> 00:12:55,669
we can, stop the, financial loss.

189
00:12:55,749 --> 00:13:00,319
and the main thing is because
we are preventing outages.

190
00:13:01,319 --> 00:13:03,549
The next is the technical benefits.

191
00:13:03,869 --> 00:13:07,779
So technical benefits, means for
example, we have, reduced incidents.

192
00:13:07,779 --> 00:13:09,349
We have less firefighting.

193
00:13:09,599 --> 00:13:11,879
We have better on call experience.

194
00:13:12,199 --> 00:13:16,189
and then, yeah, we have less
technical issues in our system.

195
00:13:16,919 --> 00:13:19,199
And the third is the customer benefit.

196
00:13:19,209 --> 00:13:25,349
So yeah, it's, pretty, obvious that if the
system is down, the customers will affect.

197
00:13:25,359 --> 00:13:29,059
And, If the customers, they are not
satisfied with the platform, they

198
00:13:29,059 --> 00:13:31,419
will just jump into another platform.

199
00:13:31,419 --> 00:13:37,579
So here we are providing, a
better experience, the app uptime,

200
00:13:37,719 --> 00:13:41,319
the, all of those, the system is
stable, the system is reliable.

201
00:13:41,489 --> 00:13:45,539
So if there is no downtime,
customer will be satisfied and

202
00:13:45,609 --> 00:13:47,069
their experience will be improved.

203
00:13:47,569 --> 00:13:53,299
Let's look at the best practices for,
doing kiosk engineering for microservices.

204
00:13:53,619 --> 00:13:57,209
So the very first is to start very small.

205
00:13:57,249 --> 00:14:03,559
So select some non critical services,
and then, start, like looking

206
00:14:04,109 --> 00:14:08,559
into running the kiosk engineering
experiments for that services.

207
00:14:08,939 --> 00:14:12,429
I think this is very important
because we don't want to take

208
00:14:12,439 --> 00:14:14,309
a very important service down.

209
00:14:15,189 --> 00:14:16,329
While doing this exercise.

210
00:14:16,339 --> 00:14:19,949
So start with very small start with
the service which has less usages,

211
00:14:19,969 --> 00:14:25,359
which is non critical So once we have
selected the service, then we can run

212
00:14:25,589 --> 00:14:30,259
the experiment in a controlled manner and
by controlled manner, I mean we have to

213
00:14:30,259 --> 00:14:35,834
use like feature flags and also We have
to ensure that we roll back into a very

214
00:14:35,834 --> 00:14:38,534
stable state if, if something goes bad.

215
00:14:38,544 --> 00:14:45,724
So maybe we were expecting that, just
one, like one dependency, like just in

216
00:14:45,724 --> 00:14:49,754
case of a failure of one dependency, it
will not have any severe consequences,

217
00:14:49,784 --> 00:14:51,534
but the reality is the opposite.

218
00:14:51,874 --> 00:14:55,024
So here we have to make sure that
we are using feature flags and we

219
00:14:55,024 --> 00:14:57,094
are able to roll back if needed.

220
00:14:57,594 --> 00:14:59,494
And then we have to.

221
00:14:59,719 --> 00:15:03,299
Monitor everything like there
are a lot of observability

222
00:15:03,299 --> 00:15:04,629
tools available in the market.

223
00:15:04,629 --> 00:15:10,409
Like some of them are like
prometheus Grafana and elk stack.

224
00:15:10,819 --> 00:15:15,689
So yeah, we have to make sure that
we are monitoring properly and

225
00:15:16,189 --> 00:15:17,949
The next step is the automation.

226
00:15:18,149 --> 00:15:22,569
the chaos engineering, should be a
regular part of the DevOps pipeline.

227
00:15:22,609 --> 00:15:24,459
It should be running continuously.

228
00:15:24,459 --> 00:15:25,979
It should be running on the pipelines.

229
00:15:26,259 --> 00:15:30,579
It should not be, one in a year
or one in a month exercise.

230
00:15:30,959 --> 00:15:33,429
It should happen regular, regularly.

231
00:15:33,759 --> 00:15:36,189
That's the only way to
make it more useful.

232
00:15:37,029 --> 00:15:39,319
And then we should have a recovery plan.

233
00:15:39,559 --> 00:15:45,139
So we have to make sure then, we
are able to roll back and there are

234
00:15:45,149 --> 00:15:48,069
some, self healing mechanism exists.

235
00:15:48,129 --> 00:15:52,639
And that is basically to, for
example, if we are facing a failure,

236
00:15:52,649 --> 00:15:55,849
then yeah, there should be a self
healing mechanism that exists.

237
00:15:56,424 --> 00:15:59,044
Is helping us to recover from that failure

238
00:15:59,544 --> 00:16:01,904
And yeah, that's it for today.

239
00:16:02,154 --> 00:16:03,034
Thank you everyone.

240
00:16:03,094 --> 00:16:05,724
I hope that this was useful Bye

