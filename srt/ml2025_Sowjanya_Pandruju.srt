1
00:00:02,460 --> 00:00:03,210
Hi everyone.

2
00:00:03,720 --> 00:00:05,100
I'm Nia Gio.

3
00:00:05,580 --> 00:00:10,500
I'm a cloud application architect at
Amazon Web Services, and today I'm

4
00:00:10,500 --> 00:00:16,230
here to talk about the topic machine
learning for advanced AI search.

5
00:00:17,040 --> 00:00:22,870
So let's quickly get into the topic and
before I dive deep into the advanced

6
00:00:22,870 --> 00:00:27,529
AI search, I wanna talk about, some
of the terms that we are going to

7
00:00:27,529 --> 00:00:29,210
talk about through the presentation.

8
00:00:29,599 --> 00:00:30,919
So let's get started.

9
00:00:32,330 --> 00:00:38,129
So what is generative Artificial
intelligence also known as gen ai.

10
00:00:39,029 --> 00:00:43,519
So it refers to a class of AI models.

11
00:00:43,800 --> 00:00:51,370
And it's designed to create new content
such as text, images audio, video or code.

12
00:00:51,449 --> 00:00:55,980
By learning patterns and
structures from existing data.

13
00:00:56,459 --> 00:01:02,919
So unlike traditional AI systems that
analyze in or classify data, gene AI

14
00:01:02,919 --> 00:01:08,690
can actually produce original outputs
and that mimics human creativity.

15
00:01:09,080 --> 00:01:11,450
So that's why it's the buzz right now.

16
00:01:11,809 --> 00:01:16,559
So these models often based on
deep learning architectures.

17
00:01:16,884 --> 00:01:22,615
Are trained on vast data sets and
can generate responses if they can

18
00:01:22,615 --> 00:01:29,555
complete tasks or they can even simulate
scenarios with remarkable realism.

19
00:01:29,855 --> 00:01:34,965
So that's why gen AI is widely used
right now in applications ranging

20
00:01:34,965 --> 00:01:41,255
from content creation or design
to personalized recommendations.

21
00:01:41,565 --> 00:01:43,215
Coding assistance is a big one.

22
00:01:43,515 --> 00:01:46,805
And also natural language interfacing.

23
00:01:47,075 --> 00:01:53,525
And now that we understand on a high
level, the gene ai generate gene ai, let's

24
00:01:53,525 --> 00:01:59,995
talk in, let's talk about gene ai how it
is found powered by foundation models.

25
00:02:00,234 --> 00:02:04,044
As I said, gene AI is powered by
foundation models and throughout this

26
00:02:04,044 --> 00:02:08,514
course we are gonna reference to them
as fms or foundation models, depending

27
00:02:08,514 --> 00:02:10,044
on how we get through the content.

28
00:02:10,044 --> 00:02:15,234
But they're also short formed
into fms with which are these

29
00:02:15,454 --> 00:02:17,704
pre-trained mass on, which are.

30
00:02:18,230 --> 00:02:21,720
These foundation models, they
are pre-trained on massive

31
00:02:21,780 --> 00:02:23,460
amounts of unstructured data.

32
00:02:23,460 --> 00:02:26,400
It could be text, images, audio, whatnot.

33
00:02:26,880 --> 00:02:33,180
And these models contain a very large
number of parameters and it enables them

34
00:02:33,240 --> 00:02:36,250
to learn complex patterns and concepts.

35
00:02:36,640 --> 00:02:41,680
Because of the broad knowledge
and flexibility, foundation models

36
00:02:41,680 --> 00:02:47,050
can be applied across a wide range
of tasks and enterprises even.

37
00:02:47,455 --> 00:02:51,445
When I say enterprises like industries,
enterprises, organizations, whatever

38
00:02:51,445 --> 00:02:55,955
you call it these organizations
can customize these models.

39
00:02:55,955 --> 00:03:01,135
Also, they can use their own data to
fine tune these models for specific

40
00:03:01,135 --> 00:03:07,595
domains and very specialized applications
that unlocks greater value for

41
00:03:07,595 --> 00:03:09,485
these organizations and enterprises.

42
00:03:09,965 --> 00:03:10,595
Really cool.

43
00:03:10,645 --> 00:03:13,045
And we talk about
foundation models, right?

44
00:03:13,095 --> 00:03:16,635
And the first thing comes to our
mind is what are the inputs and

45
00:03:16,635 --> 00:03:18,495
outputs of foundation models?

46
00:03:18,935 --> 00:03:19,295
Yeah.

47
00:03:20,105 --> 00:03:24,005
So we, when we talk about a foundation
model, and also when we talk about

48
00:03:24,005 --> 00:03:26,514
inputs inputs can be anything.

49
00:03:26,514 --> 00:03:29,884
And they're typically unstructured
data sets like we talked about,

50
00:03:29,884 --> 00:03:32,029
like text, images, audio, video.

51
00:03:32,819 --> 00:03:33,839
It can be code.

52
00:03:34,079 --> 00:03:36,239
It depends on the model type, in fact.

53
00:03:36,659 --> 00:03:42,774
And these inputs are used during the
pre the pre-training phase to build the

54
00:03:42,774 --> 00:03:48,514
general capabilities and during inference
as well when users provide prompts

55
00:03:48,514 --> 00:03:51,934
or questions or content to process.

56
00:03:52,264 --> 00:03:58,334
So these are, this is how the
inputs is defined and the outputs.

57
00:03:58,334 --> 00:04:00,164
What are the outputs of foundation models?

58
00:04:00,689 --> 00:04:05,399
So the outputs, they are typically
the newly generated content based on

59
00:04:05,399 --> 00:04:10,679
the input, such as, it could be a text
or it could be a translated language.

60
00:04:10,929 --> 00:04:15,239
It could be a generated image, it
could be a summarized document.

61
00:04:15,569 --> 00:04:19,799
It could be answering a question
that you have asked, or if you it

62
00:04:19,799 --> 00:04:24,579
could be a nap, it could predict the
next step in a sequence if you, if

63
00:04:24,579 --> 00:04:25,924
that's how you phrase your input.

64
00:04:26,394 --> 00:04:31,944
So the the important thing here is that
for the foundation model, the output

65
00:04:31,944 --> 00:04:37,374
of the foundation model is that these
outputs are designed to be coherent,

66
00:04:38,064 --> 00:04:41,394
contextually relevant, and often creative.

67
00:04:41,764 --> 00:04:47,284
That reflects the models understanding,
the learn understanding of how

68
00:04:47,284 --> 00:04:51,824
complex patterns, of how complex
patterns it can analyze and how

69
00:04:51,824 --> 00:04:54,264
well it is responding to your input.

70
00:04:54,634 --> 00:04:57,314
A quick recap on what an input is.

71
00:04:57,344 --> 00:05:02,124
It's typically a raw or unstructured
data or prompts like text or

72
00:05:02,124 --> 00:05:04,054
image or audio, et cetera.

73
00:05:04,474 --> 00:05:08,234
And the output would be the
generated content based on the input.

74
00:05:08,284 --> 00:05:12,604
It could also be text, image, audio
or any that answer to your question.

75
00:05:13,064 --> 00:05:15,224
Based on the learned
knowledge that it has.

76
00:05:15,734 --> 00:05:21,914
So now that we are covering the basics,
let's talk more into how we can customize

77
00:05:21,964 --> 00:05:23,554
these foundation models as well.

78
00:05:25,294 --> 00:05:26,974
So yeah, I was talking about that, right?

79
00:05:26,974 --> 00:05:31,744
Like foundation model, you give a
prompt and the prompt can be anything.

80
00:05:31,744 --> 00:05:34,894
And this is the example I'm
using here where it talks about

81
00:05:34,954 --> 00:05:38,914
the prompt could be as simple as
explain what thermodynamics is.

82
00:05:39,404 --> 00:05:41,594
To a middle school student, right?

83
00:05:42,314 --> 00:05:46,754
And the foundation model, based on its
initial pre-training or whatnot, it is

84
00:05:46,754 --> 00:05:50,984
supposed to be, it is going to use all
the data and generate a response, which

85
00:05:50,984 --> 00:05:52,634
is our answer, which is the output.

86
00:05:52,934 --> 00:05:57,864
And it'll explain in terms that a
middle school student can understand.

87
00:05:57,864 --> 00:06:02,294
So that's how smart an answer should
be from the foundational model.

88
00:06:03,569 --> 00:06:07,409
As I talked about, how do we
customize a foundation model?

89
00:06:07,859 --> 00:06:13,709
So foundation models can be customized
to better suit a specific task, or

90
00:06:13,809 --> 00:06:18,819
a company based on, we have several
approaches that we can do, and I'm gonna

91
00:06:18,819 --> 00:06:21,559
talk about three main approaches at hand.

92
00:06:21,929 --> 00:06:24,989
The first is the instruction
based fine tuning.

93
00:06:25,439 --> 00:06:31,459
So instruction based fine tuning is
adjusting the model by training it in

94
00:06:31,519 --> 00:06:38,919
on example prompts and also like desired
outputs like teaching it how to respond to

95
00:06:38,919 --> 00:06:41,799
a specific instruction more effectively.

96
00:06:42,219 --> 00:06:45,599
So that is the name suggests like
what it is going to do, right?

97
00:06:45,599 --> 00:06:47,249
The instruction based fine tuning.

98
00:06:48,149 --> 00:06:52,299
And the other one I have brought
here is the domain adaptation.

99
00:06:52,659 --> 00:06:57,139
And this makes a lot of sense
for domain specific needs.

100
00:06:57,139 --> 00:07:01,589
For example if you need to fine tune
the model, but domain specific data.

101
00:07:01,589 --> 00:07:04,789
And when I say it could be like
the domain could be medical

102
00:07:04,789 --> 00:07:07,429
or legal or financial content.

103
00:07:08,239 --> 00:07:09,589
So the model.

104
00:07:09,874 --> 00:07:14,014
We do this domain adaptation
so that it better understands

105
00:07:14,324 --> 00:07:17,204
specialized vocabulary and context.

106
00:07:17,304 --> 00:07:21,644
It can play a crucial part in,
when it has such use cases.

107
00:07:22,334 --> 00:07:26,354
And the next one I have is
the information retrieval.

108
00:07:26,834 --> 00:07:30,054
So this approach could be that.

109
00:07:31,099 --> 00:07:35,479
We are enhancing the model by
integrating it with external

110
00:07:35,479 --> 00:07:38,259
databases or search systems.

111
00:07:38,589 --> 00:07:43,789
So what it does is it allows it
to fetch and incorporate UpToDate

112
00:07:43,789 --> 00:07:45,769
information during responses.

113
00:07:46,089 --> 00:07:49,739
And for this process you don't
have to retrain the model fully.

114
00:07:49,769 --> 00:07:54,829
Like you could augment the data so
that your responses are accurate.

115
00:07:55,189 --> 00:07:59,209
These are some of the customizations
that we could do to a foundation model.

116
00:07:59,659 --> 00:08:04,189
And information retrieval is also
standing out prominently for a lot of

117
00:08:04,189 --> 00:08:10,249
organizations because not everybody
could effort a lot of time in retraining

118
00:08:10,249 --> 00:08:15,419
for specific use cases, but rather
this will help them augment the data

119
00:08:15,419 --> 00:08:19,049
as they go and not, don't have to
deal with retraining the model fully.

120
00:08:19,229 --> 00:08:20,369
So it works both ways.

121
00:08:20,819 --> 00:08:24,544
And this addresses the situation
that we have at hand, right?

122
00:08:24,544 --> 00:08:28,214
Like, why do we customize the
foundation model also, is that using

123
00:08:28,214 --> 00:08:32,984
all these methods, depending on the
use case, these allow the organizations

124
00:08:33,194 --> 00:08:38,864
to tailor foundation models for
relevance and higher accuracy and

125
00:08:39,324 --> 00:08:41,724
effectiveness for their use cases.

126
00:08:41,814 --> 00:08:43,464
So it works really well that way.

127
00:08:45,384 --> 00:08:50,164
And now that we talked about a
different all things about gen AI

128
00:08:50,164 --> 00:08:53,914
and the foundational models and the
inputs and outputs and customizations.

129
00:08:54,454 --> 00:08:56,254
Let's also slowly get into our topic.

130
00:08:57,064 --> 00:08:58,924
And here I'm bringing,
introducing the word vectors.

131
00:08:59,674 --> 00:09:04,394
So in machine learning a vector is
simply an ordered list of numbers.

132
00:09:04,959 --> 00:09:09,079
These numbers represent data
in a mathematical form that

133
00:09:09,079 --> 00:09:10,849
algorithms can easily process.

134
00:09:11,150 --> 00:09:15,960
Vectors each number in the vector could
be like a component or a dimension.

135
00:09:16,410 --> 00:09:22,700
So they allow the machines to perform
calculations like similarity measurement,

136
00:09:22,760 --> 00:09:27,349
clustering classification, and more
let's say you have a customer profile

137
00:09:27,660 --> 00:09:32,039
and it is being represented in a vector
like, age, income, number of purchases.

138
00:09:32,339 --> 00:09:35,839
They could all be grouped
together in a vector dimension.

139
00:09:36,739 --> 00:09:41,729
And so the key point here that is, has
been highlighted here as well, is that

140
00:09:41,729 --> 00:09:46,589
vectors are the ones that turn real
world data into a format, like a array

141
00:09:46,649 --> 00:09:49,499
of numbers or floats to be precise.

142
00:09:50,009 --> 00:09:51,599
That machine language.

143
00:09:51,879 --> 00:09:55,719
Machine learning models can understand
and they can compare and they can

144
00:09:56,059 --> 00:09:57,709
manipulate the data efficiently.

145
00:09:57,709 --> 00:10:01,459
So that's that's a huge role
that vectors play when it comes

146
00:10:01,459 --> 00:10:03,579
to the machine learning models.

147
00:10:04,420 --> 00:10:07,990
So now that we talked about
vectors what is a vector embedding?

148
00:10:08,050 --> 00:10:10,645
That's the next question
we have in our mind, right?

149
00:10:11,075 --> 00:10:14,765
As we talked about some of the
terms, now putting them into context.

150
00:10:15,325 --> 00:10:18,655
Vector embedding is machine
learning, especially is a way of

151
00:10:18,655 --> 00:10:23,305
representing complex data like
words, images, products, or users.

152
00:10:23,655 --> 00:10:28,575
Everything has different structure of
format of the data as numerical vectors.

153
00:10:28,905 --> 00:10:33,045
That is a list of numbers like in a
continuous high dimensional space.

154
00:10:33,355 --> 00:10:36,875
So each embedding captures
the meaning features or

155
00:10:36,875 --> 00:10:38,615
relationships of the original data.

156
00:10:39,395 --> 00:10:41,415
Like for example similar items.

157
00:10:41,940 --> 00:10:47,090
Are placed together in the space while
different items are placed further apart.

158
00:10:47,300 --> 00:10:49,790
Like for example, if you are
looking at the recommendation

159
00:10:49,790 --> 00:10:54,920
systems two similar movies will have
emits located next to each other.

160
00:10:55,820 --> 00:11:00,580
The key points that we have
to remember is that they are

161
00:11:00,820 --> 00:11:03,880
learned from data using models.

162
00:11:03,980 --> 00:11:08,630
They preserve important properties
like similarities, structure, and

163
00:11:08,630 --> 00:11:12,950
they allow machines to perform
operations, like I mentioned like

164
00:11:13,250 --> 00:11:18,710
similarity search or clustering or
classification in a very efficient way.

165
00:11:19,040 --> 00:11:22,095
That's a huge role that vector
embedding plays in here.

166
00:11:23,025 --> 00:11:26,185
And the next topic I have is vector store.

167
00:11:27,160 --> 00:11:30,890
And I want to talk about Vector
store using a setup that I have

168
00:11:31,010 --> 00:11:35,400
by like how the vectors, like how
the for whole flow looks like.

169
00:11:35,940 --> 00:11:42,690
And here I have in the setup, I have some
raw data such as audio, images documents

170
00:11:42,690 --> 00:11:47,860
and whatnot is first put into I'm using
Amazon Bedrock, the machine learning model

171
00:11:48,040 --> 00:11:51,310
for emitting here and bedrock processes.

172
00:11:51,860 --> 00:11:57,200
This data and then it generates
embeddings into our dance vector

173
00:11:57,540 --> 00:12:02,410
representations that capture the essential
meaning and features of the input.

174
00:12:03,250 --> 00:12:08,410
As you can see here, the representation
is how the vector embeddings look like.

175
00:12:08,740 --> 00:12:14,320
So these dense vectors are then
stored in a vector database and

176
00:12:14,320 --> 00:12:18,710
in this sample I'm using Amazon
open search as a vector database.

177
00:12:19,035 --> 00:12:21,795
Which supports vector
search capabilities also.

178
00:12:22,045 --> 00:12:26,155
Storing a embeddings en enables
efficient similarity search.

179
00:12:26,505 --> 00:12:31,775
It also supports semantic retrieval
and recommendation systems so that

180
00:12:31,775 --> 00:12:35,905
users can quickly find data points
that are meaningfully related

181
00:12:36,145 --> 00:12:38,255
based on their vector proximity.

182
00:12:39,185 --> 00:12:39,425
Yeah.

183
00:12:39,965 --> 00:12:45,965
So now that we understand about foundation
models and vectors, vector stores,

184
00:12:45,965 --> 00:12:51,455
vector embeddings, let's start into
look into some of the vector databases.

185
00:12:51,505 --> 00:12:55,265
And I want to quickly
highlight Postgres sql here.

186
00:12:55,745 --> 00:12:59,040
Before I go into that,
let me also talk about.

187
00:12:59,655 --> 00:13:02,885
The Vector data stores because in
this topic I'm covering most of the

188
00:13:02,885 --> 00:13:05,975
AWS services because that is what
I'm familiar, a lot, familiar with.

189
00:13:06,405 --> 00:13:10,595
I wanna touch, because in the previous
example I picked up an example to show

190
00:13:10,595 --> 00:13:14,105
Amazon Open Search, but that's not
the only data store that is available.

191
00:13:14,105 --> 00:13:17,915
There are multiple others that are
available the director stores as well.

192
00:13:18,705 --> 00:13:21,555
The first one here is the Amazon Kendra.

193
00:13:22,120 --> 00:13:25,265
There are multiple, as I mentioned, there
are multiple vector data stores that are

194
00:13:25,265 --> 00:13:27,695
available and they all work efficiently.

195
00:13:27,725 --> 00:13:30,845
But depending on the use case
that we have, you could be

196
00:13:30,845 --> 00:13:32,675
looking at one over the other.

197
00:13:32,945 --> 00:13:37,285
For example, Amazon Kendra, it
is a intelligent search service

198
00:13:37,595 --> 00:13:41,225
that now supports semantic
search using vector embeddings.

199
00:13:41,505 --> 00:13:44,415
It helps find users, find
relevant information based

200
00:13:44,415 --> 00:13:46,385
on meaning not just keywords.

201
00:13:46,715 --> 00:13:48,215
So it's very powerful tool.

202
00:13:48,500 --> 00:13:51,560
And you would be, this would be a
good fit for you if you're looking

203
00:13:51,560 --> 00:13:55,380
for something low-code, no-code
solution and rapid deployment.

204
00:13:55,440 --> 00:14:00,760
This is your go-to use case because it
supports a lot of ingestion data sources.

205
00:14:00,820 --> 00:14:03,530
It has 40 plus connectors
and stuff like that.

206
00:14:03,890 --> 00:14:07,495
So you do not want to deal with some
of the things like data chunking and

207
00:14:07,500 --> 00:14:09,980
embeddings and mixing algorithm choices.

208
00:14:10,010 --> 00:14:11,390
It does all of that for you.

209
00:14:11,450 --> 00:14:13,680
So really powerful service to go with.

210
00:14:14,055 --> 00:14:17,625
And the next one I have is the
Amazon Open search service that I

211
00:14:17,625 --> 00:14:19,395
just used in a previous example.

212
00:14:20,215 --> 00:14:23,835
This is an open source search
and analytics engine that

213
00:14:23,835 --> 00:14:27,345
supports dense vector fields
and K nearest neighbor search.

214
00:14:27,655 --> 00:14:32,205
It is useful for use cases like
recommendation systems, semantic

215
00:14:32,205 --> 00:14:34,255
search engineer retrieval.

216
00:14:34,255 --> 00:14:35,765
So really good as well.

217
00:14:35,780 --> 00:14:36,000
And.

218
00:14:36,635 --> 00:14:40,645
This will be a good use case for you
if you are already using Amazon open

219
00:14:40,645 --> 00:14:44,645
search service and you wanna go with a
solution where you are you don't really

220
00:14:44,645 --> 00:14:49,695
want a lot of deal with sql, like a no
SQL solution, and you also need like a

221
00:14:49,695 --> 00:14:54,975
low latency algorithm and you want higher
search accuracy and stuff like that.

222
00:14:55,035 --> 00:14:59,255
The other item that I have and more
focused on our use case for this

223
00:14:59,255 --> 00:15:04,135
presentation today is a, is about the
Amazon RDS and also Amazon Aurora.

224
00:15:04,445 --> 00:15:08,465
So this has the post se Postgre
SQL with the PG vector extension,

225
00:15:08,495 --> 00:15:12,145
and I'm gonna talk about those
things also in the upcoming slides.

226
00:15:12,415 --> 00:15:17,385
So this is a managed relational database
that supports that now supports the

227
00:15:17,385 --> 00:15:19,005
vector storage and querying as well.

228
00:15:19,380 --> 00:15:25,060
So this is great for applications needing
tight integration of structured data with

229
00:15:25,060 --> 00:15:30,190
vector search, such as, and this also
helps with, personalized recommendations.

230
00:15:30,190 --> 00:15:32,860
Gives you hybrid search
as I mentioned, right?

231
00:15:32,860 --> 00:15:36,140
It could have that integration
between your vector search as well

232
00:15:36,140 --> 00:15:37,550
as your structured data search.

233
00:15:38,030 --> 00:15:42,230
And this will be a good use case
for you when you already have.

234
00:15:42,465 --> 00:15:48,510
You prefer SQL and you are dealing
with RDS or Aurora Postgres sql and

235
00:15:48,510 --> 00:15:53,610
you want to keep the application and
the AI ML data vectors in the same DB

236
00:15:53,760 --> 00:15:55,470
for better governance and faster loans.

237
00:15:55,800 --> 00:15:59,880
So this is the, I think, the biggest
advantage that you get with this

238
00:15:59,880 --> 00:16:04,590
solution where you wanna keep them
together, the application and the AI

239
00:16:04,590 --> 00:16:06,690
ML data and vectors in the same db.

240
00:16:06,690 --> 00:16:09,720
So that's where this is coming from, and.

241
00:16:10,320 --> 00:16:14,000
Now that we discuss some of the data
store vector data stores here I want to

242
00:16:14,000 --> 00:16:20,470
dive deep into the poster SQL and the PG
vector and why and how they're beneficial

243
00:16:20,510 --> 00:16:21,920
to what we are trying to achieve.

244
00:16:23,100 --> 00:16:23,400
Yeah.

245
00:16:23,400 --> 00:16:26,100
Also, one more thing is that offer
all of these services, right?

246
00:16:26,130 --> 00:16:30,360
One of the advantages on using
them also is that they come with

247
00:16:30,390 --> 00:16:33,030
all the builts and whistles like.

248
00:16:33,855 --> 00:16:38,175
You have role based access controls, you
have authorization authentication, you

249
00:16:38,175 --> 00:16:41,555
have these are fully managed services
with serverless options available.

250
00:16:41,645 --> 00:16:45,605
So you don't have to work on managing
the infrastructure, setting these up,

251
00:16:45,695 --> 00:16:48,095
but rather work on the actual meet.

252
00:16:48,425 --> 00:16:51,715
And that's, that, that's one of
the big advantages coming out.

253
00:16:52,055 --> 00:16:54,905
For this use case, as I mentioned,
we are going to be focusing on

254
00:16:54,935 --> 00:16:58,105
Postgres SQL in Aurora or Amazon, RDS.

255
00:16:58,390 --> 00:17:00,740
And the PG vector connected into it.

256
00:17:01,010 --> 00:17:02,030
So let's dive into that.

257
00:17:02,840 --> 00:17:07,730
So why Postgres sql, like, why I
have chosen this for this use case or

258
00:17:07,730 --> 00:17:09,320
what I'm trying to talk about here?

259
00:17:09,660 --> 00:17:14,330
Postgres sql it's becoming a popular
choice as a vector database because it

260
00:17:14,330 --> 00:17:20,930
combines the reliability of traditional
relational database with new capabilities

261
00:17:21,230 --> 00:17:22,970
to store and query vector embeds.

262
00:17:24,200 --> 00:17:24,740
The.

263
00:17:25,590 --> 00:17:30,020
If you ask me like, why, what are the
reasons that we might wanna go for

264
00:17:30,020 --> 00:17:35,930
PostgreSQL, for a Vector database is, as I
mentioned, there is a PG vector extension.

265
00:17:36,410 --> 00:17:40,460
It adds native support for
vector data types and similarity

266
00:17:40,460 --> 00:17:42,590
search and the flexibility.

267
00:17:42,650 --> 00:17:46,400
It lets to store both structured
data like a customer data like

268
00:17:46,400 --> 00:17:52,130
product metadata alongside vectors
in one database that enables you.

269
00:17:52,445 --> 00:17:55,055
Powerful, really powerful hybrid search.

270
00:17:55,175 --> 00:18:00,305
Like you can get filters and similarity
and that is one of the examples towards

271
00:18:00,305 --> 00:18:04,555
the end where I'm gonna show you with
the PostgreSQL, PG vector, how that

272
00:18:04,555 --> 00:18:09,625
looks like, and also the scalability and
maturity that we are talking about here.

273
00:18:10,045 --> 00:18:12,385
So PostgreSQL is battle tested.

274
00:18:12,565 --> 00:18:14,365
It's highly scalable and.

275
00:18:14,755 --> 00:18:17,665
Trusted already for mission
critical applications.

276
00:18:17,665 --> 00:18:20,245
This is not something new
that has not been around.

277
00:18:20,275 --> 00:18:24,745
It's been around for a while and
does a great job and cost effective.

278
00:18:25,435 --> 00:18:27,145
Why am I saying cost effective?

279
00:18:27,145 --> 00:18:30,775
Because typically, RDS might not sound
like it is cost effective, but the

280
00:18:30,775 --> 00:18:35,815
thing is, it avoids the need for a
separate, specialized vector database.

281
00:18:36,085 --> 00:18:40,105
For many use cases, it simplifies the
architecture and reduces the cost.

282
00:18:40,105 --> 00:18:41,785
Be because you don't need that.

283
00:18:42,175 --> 00:18:44,365
Another separate
specialized vector database.

284
00:18:44,365 --> 00:18:45,685
It supports natively now.

285
00:18:46,615 --> 00:18:52,105
So PostgreSQL with PG Vector, it can
give you a very powerful, unified

286
00:18:52,165 --> 00:18:58,455
platform for building intelligent,
vector driven applications with very

287
00:18:58,455 --> 00:19:00,645
familiar tools and minimal overhead.

288
00:19:01,635 --> 00:19:05,205
So let's talk about a
little bit about PG Vector.

289
00:19:05,475 --> 00:19:09,105
It is an open source
library for vector search.

290
00:19:09,485 --> 00:19:13,035
A vector as a data type helps
in similarity search and it

291
00:19:13,035 --> 00:19:15,355
has the senior SQL integration.

292
00:19:15,745 --> 00:19:17,585
So I say these things, right?

293
00:19:17,615 --> 00:19:20,495
So what are the key features
that I'm highlighting here

294
00:19:20,495 --> 00:19:21,725
is also vector data type.

295
00:19:21,725 --> 00:19:23,765
So it enables storing vectors.

296
00:19:23,770 --> 00:19:28,470
It's as I mentioned it, you can think
of that as a list of a red area of

297
00:19:28,470 --> 00:19:30,300
like numbers, especially floats.

298
00:19:30,350 --> 00:19:31,610
Think of it as an area of float.

299
00:19:32,255 --> 00:19:34,905
And it supports it as a native data type.

300
00:19:35,475 --> 00:19:39,675
It provide, provides similarity search,
so supports finding vectors that are

301
00:19:39,675 --> 00:19:44,475
closest to a given vector based on
metrics like cosign similarity and to

302
00:19:44,475 --> 00:19:48,145
the EQU and in distance or inner product.

303
00:19:48,175 --> 00:19:50,485
I, it can index for fast search.

304
00:19:50,845 --> 00:19:53,305
It has seamless SQL integration.

305
00:19:54,145 --> 00:19:56,425
So for all these advantages.

306
00:19:56,735 --> 00:20:01,605
We can go this is a good good choice and
there are many use cases, as I mentioned,

307
00:20:01,605 --> 00:20:05,295
the semantic search or recommendation
engines or personalized delivery,

308
00:20:05,295 --> 00:20:09,755
content delivery, all of these will be
a very good use case for the PG vector.

309
00:20:10,775 --> 00:20:16,695
So I want to discuss the Fiji
Vector example with using the query

310
00:20:16,695 --> 00:20:18,195
in the nearest neighbor sample.

311
00:20:18,525 --> 00:20:22,155
So let's talk about some of
the algorithms we have at hand.

312
00:20:22,545 --> 00:20:23,415
So the first one.

313
00:20:24,855 --> 00:20:29,915
So it has three common similarity
metrics, like it enables the nearest

314
00:20:30,005 --> 00:20:36,405
neighbor searches in post SQL by using
three most common similarity metrics.

315
00:20:36,465 --> 00:20:39,185
The first one is the Euclidean distance.

316
00:20:39,335 --> 00:20:45,165
So it measures the straight line
distance between two vectors.

317
00:20:45,525 --> 00:20:50,530
So typically it is very useful
when the physical closeness.

318
00:20:51,325 --> 00:20:53,935
Or the raw differences really matter.

319
00:20:54,865 --> 00:20:58,755
And this is how you show
that it's a Euclidean.

320
00:20:58,805 --> 00:21:02,375
I will talk about this, the
thing, the hyphen that you're

321
00:21:02,375 --> 00:21:05,035
seeing within the brackets.

322
00:21:05,035 --> 00:21:07,255
We will see how that
plays into the picture.

323
00:21:07,525 --> 00:21:10,555
In the next slide where I'm giving,
going to give you the actual sample.

324
00:21:11,455 --> 00:21:15,155
And the next one the next metric
is the co-sign similarity.

325
00:21:15,515 --> 00:21:16,445
Cosign similarity.

326
00:21:16,445 --> 00:21:20,165
As you can see the picture, it
measures the angle between two

327
00:21:20,165 --> 00:21:25,135
vectors, like it focuses on their
orientation rather than the magnitude.

328
00:21:25,535 --> 00:21:30,415
For example, I think some of the most
common use case would be like in text

329
00:21:30,415 --> 00:21:35,885
embeddings, where the direction of the
meaning matters more than magnitude.

330
00:21:36,455 --> 00:21:39,915
And it also has, it is
depicted using a hash.

331
00:21:40,025 --> 00:21:41,015
And the next one is.

332
00:21:41,505 --> 00:21:42,645
The top product.

333
00:21:43,035 --> 00:21:46,785
Also, I think I also use
inner product to define this.

334
00:21:46,835 --> 00:21:53,135
So it calculates the product of two
vectors and then sums the result.

335
00:21:53,465 --> 00:21:58,815
So this is often used when
embeddings are normalized.

336
00:21:58,815 --> 00:22:01,625
And higher dot products
mean greater similarity.

337
00:22:02,595 --> 00:22:08,175
If you are looking at some of the examples
for query patterns in a PG vector.

338
00:22:08,565 --> 00:22:14,865
So it would be like finding a vector's
closest to a vector when using one of

339
00:22:14,865 --> 00:22:16,845
these distances or similarity measures.

340
00:22:16,845 --> 00:22:22,555
And that is a sample that I'm going to
show you right now and you can combine

341
00:22:22,555 --> 00:22:24,955
similarity search with SQL filters.

342
00:22:25,205 --> 00:22:28,265
Like product categories or user
segments or something like that.

343
00:22:28,325 --> 00:22:31,455
So that is a sample that we
are going to look at right now.

344
00:22:32,535 --> 00:22:34,245
So that is yep.

345
00:22:34,665 --> 00:22:38,645
So I'm going to give an example
for query nearest neighbor.

346
00:22:38,945 --> 00:22:44,165
And to do this, I'm going to create
a table called test Embeddings.

347
00:22:44,405 --> 00:22:48,255
And it has two columns
product ID and embedding.

348
00:22:49,320 --> 00:22:53,160
You see this embed column, it
is using a vector data type.

349
00:22:53,670 --> 00:22:56,880
And this is possible because
I have the PG vector plugin.

350
00:22:57,810 --> 00:23:03,620
So now the next thing I'm going to do
is I'm going to insert some data, like

351
00:23:03,650 --> 00:23:06,220
I think 1, 2, 3, 4, 4, 4 data points.

352
00:23:06,520 --> 00:23:10,340
So the first one for everything
I have product ID and embeddings.

353
00:23:10,400 --> 00:23:12,890
The product ID is a big in
inte, and the embeddings is a

354
00:23:12,890 --> 00:23:14,240
vector data type that I have.

355
00:23:14,615 --> 00:23:17,495
So in each of them, this is a
sample test data that I'm inserting.

356
00:23:17,555 --> 00:23:20,105
So the first item is one, and
then it's vector embedding.

357
00:23:20,465 --> 00:23:23,975
Second is two, and then it is,
it's vector embedding and so on.

358
00:23:24,395 --> 00:23:28,385
In real world, these embeddings that I
have, the 1, 2, 3, 2, 3, 4, the vector

359
00:23:28,415 --> 00:23:32,915
embeddings that you're seeing, these will
come from your machine learning model.

360
00:23:33,845 --> 00:23:39,720
Now, if you do the nearest neighbor search
what you would say could look like this.

361
00:23:39,990 --> 00:23:45,570
Like you would do a select query
select product ID and embeddings where

362
00:23:45,600 --> 00:23:48,360
embeddings is similar to this vector.

363
00:23:48,360 --> 00:23:51,090
So you see that hyphen right there, right?

364
00:23:51,340 --> 00:23:52,300
Bracket Hyen.

365
00:23:52,300 --> 00:23:54,010
Bracket beside embeddings.

366
00:23:54,250 --> 00:24:00,350
So that is what we saw in the previous
here, where you have shapes behind each

367
00:24:00,350 --> 00:24:02,390
work, one for hyphen hash and equal to.

368
00:24:02,825 --> 00:24:07,025
So here, which means that we are
using the similarity metric Euclidean

369
00:24:07,085 --> 00:24:09,275
in my sample that I'm trying to use.

370
00:24:09,695 --> 00:24:11,655
So it is using Euclidean metric.

371
00:24:11,985 --> 00:24:17,895
And what we are trying to say is that
we are selecting the product ID and

372
00:24:17,955 --> 00:24:22,125
and embeddings and we are seeing where
embeddings is similar to this vector.

373
00:24:22,545 --> 00:24:25,915
This is the, like I said,
symbol, fake cian distance.

374
00:24:26,240 --> 00:24:31,250
And from the stable test embeddings, and
we are saying that to order by embeddings.

375
00:24:31,640 --> 00:24:35,690
So you can see how easily we were
able to integrate this vector data

376
00:24:35,690 --> 00:24:39,160
type into our regular structured sql.

377
00:24:39,470 --> 00:24:42,230
And how powerful it is and easy it is.

378
00:24:42,230 --> 00:24:46,370
If you already have SQL knowledge,
then it's not big, a big learning curve

379
00:24:46,460 --> 00:24:48,740
or maintenance curve for you either.

380
00:24:48,970 --> 00:24:52,990
And I'm all and you, and when I run
this query, I'm getting this response.

381
00:24:53,570 --> 00:24:55,000
And I'll get an answer like this.

382
00:24:55,090 --> 00:24:58,000
I'm getting an answer like this,
and it has only two rows because

383
00:24:58,000 --> 00:25:01,690
if you see my query, I said I'll
limit it to only two results.

384
00:25:02,380 --> 00:25:07,030
So it is able to find that and give me
that data correctly, as you can see.

385
00:25:07,310 --> 00:25:10,230
In summary, I can say PG Vector.

386
00:25:10,350 --> 00:25:12,690
It helps you build a product.

387
00:25:13,030 --> 00:25:14,050
Catch log similarity.

388
00:25:14,050 --> 00:25:18,080
Search this way by allowing you
to store product embeddings.

389
00:25:18,420 --> 00:25:20,340
Which you get from your foundation models.

390
00:25:20,370 --> 00:25:23,940
These are the vector representations
of products directly into

391
00:25:23,940 --> 00:25:26,280
your Postgres SQL database.

392
00:25:26,550 --> 00:25:31,040
So you can generate embeddings
from product attributes like you

393
00:25:31,130 --> 00:25:36,090
titles or descriptions, images
using your machine learning models.

394
00:25:36,390 --> 00:25:41,240
And with PG Vector, you can efficiently
perform nearest neighbor searches

395
00:25:41,600 --> 00:25:43,460
to find products that are most.

396
00:25:44,090 --> 00:25:47,330
Similar based on semantic
meaning, not just keywords.

397
00:25:47,660 --> 00:25:51,080
So through this we are able
to see that you have a lot

398
00:25:51,080 --> 00:25:52,520
of key benefits here, right?

399
00:25:52,520 --> 00:25:55,550
As I mentioned as a recap of what
we have been discussing so far.

400
00:25:55,580 --> 00:25:58,160
Like with this approach,
you have unified storage.

401
00:25:58,550 --> 00:26:02,980
You get to keep product metadata and
embeddings together in one database.

402
00:26:03,340 --> 00:26:05,620
You are getting this
flexible similarity search.

403
00:26:05,620 --> 00:26:09,990
As we just saw in this example we
were able to use utility distance.

404
00:26:10,305 --> 00:26:14,545
And to find related ones and I'm
able to write hybrid queries.

405
00:26:14,545 --> 00:26:20,375
I'm able to combine my traditional SQL
filtering here I with the filtering

406
00:26:20,465 --> 00:26:23,105
with vector based similarity search.

407
00:26:23,285 --> 00:26:27,155
So not and it is giving me
very highly relevant results.

408
00:26:27,575 --> 00:26:31,325
And also, I didn't need any
separate vector database.

409
00:26:31,535 --> 00:26:35,125
I was able to extend my
existing Postgres SQL setup.

410
00:26:35,405 --> 00:26:39,535
In, in all together, like in
short this PG vector helps.

411
00:26:39,565 --> 00:26:44,085
It enables fast and intelligent
and it is very scalable.

412
00:26:44,425 --> 00:26:49,465
Similar search product search
by it's bringing powerful AI

413
00:26:49,615 --> 00:26:54,185
vector search capabilities into
already familiar workflows.

414
00:26:54,845 --> 00:26:59,115
I think I was able to explain,
the use case and also how to use

415
00:26:59,115 --> 00:27:03,885
this advanced data search using
Post sql, PG Vector, a data store.

416
00:27:04,015 --> 00:27:07,765
Hope you learn something new
through this presentation.

417
00:27:08,305 --> 00:27:10,075
And yeah, thank you.

418
00:27:10,535 --> 00:27:12,795
I hope you have you guys
have a good day now.

419
00:27:13,425 --> 00:27:13,995
Thanks a lot.

