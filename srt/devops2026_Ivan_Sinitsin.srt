1
00:00:00,600 --> 00:00:01,590
Speaker 15: Hello everyone.

2
00:00:01,680 --> 00:00:08,400
My name is even Sen and today I wanted
to talk to you about designing a r

3
00:00:08,580 --> 00:00:14,160
intensive architecture for systems
with high data throughout output.

4
00:00:14,660 --> 00:00:17,070
Let's think about terminology first.

5
00:00:17,340 --> 00:00:22,650
What you see here is a golden standard
for modern lead intensive architecture.

6
00:00:23,150 --> 00:00:29,549
It's what you'll find in almost any
big tech stacks Kubernetes, where ports

7
00:00:29,639 --> 00:00:36,480
are moving constantly and microservices
where logic is the coate into domains.

8
00:00:37,109 --> 00:00:42,720
It's great for developer but it
puts a heavy tax on the network and

9
00:00:42,929 --> 00:00:45,550
charted and replicated storages.

10
00:00:46,094 --> 00:00:48,194
Caches and persistent license.

11
00:00:48,194 --> 00:00:50,414
Design it for fault tolerance.

12
00:00:50,984 --> 00:00:53,714
In theory, this scales linearly.

13
00:00:54,314 --> 00:00:59,064
If you need more just throw more
pots or knots at the cluster

14
00:01:00,054 --> 00:01:02,304
and allow to medium load.

15
00:01:02,694 --> 00:01:04,524
This works like a charm.

16
00:01:05,024 --> 00:01:07,244
But the magic disappears.

17
00:01:07,335 --> 00:01:12,855
One, you cross the thousand
North, north threshold.

18
00:01:13,635 --> 00:01:18,345
That beautiful architecture
diagram turns into source of pain.

19
00:01:18,845 --> 00:01:21,625
We hit a wall of system issues like.

20
00:01:22,125 --> 00:01:27,895
Tail latencies or protocol entropy,
high contention and topology drifts.

21
00:01:28,495 --> 00:01:30,475
Let's discuss them.

22
00:01:30,975 --> 00:01:36,425
On large scale mean latency
does matter and fight a tail.

23
00:01:37,365 --> 00:01:40,375
Look at the fanau pattern on the ride.

24
00:01:40,780 --> 00:01:48,270
To solve single query search your
service might pull 50 database charts.

25
00:01:48,810 --> 00:01:52,110
This is where slowest link rule kicks in.

26
00:01:52,680 --> 00:01:59,260
Your total response time is dedicated by
the slowest chart in a bunch at scale.

27
00:01:59,560 --> 00:02:06,420
Those rare anomalies become
your daily statistical reality.

28
00:02:06,920 --> 00:02:08,690
Scalability isn't free.

29
00:02:09,110 --> 00:02:12,560
There is a tax you paid.

30
00:02:12,620 --> 00:02:14,820
Just keeps the light on.

31
00:02:15,320 --> 00:02:16,310
Orchestration.

32
00:02:16,610 --> 00:02:20,989
A counter plane and serial
discovery generate a massive event.

33
00:02:21,020 --> 00:02:21,410
Term.

34
00:02:22,040 --> 00:02:29,234
CPU Overhead of every note must track
in neighbors protocols like gossip.

35
00:02:30,209 --> 00:02:36,269
Starts eating significant apart
of CPU cycles, network noise

36
00:02:36,899 --> 00:02:41,510
and management traffic checks
lead directions gross non-line.

37
00:02:42,010 --> 00:02:46,359
It eventually start competing
with actual user traffic.

38
00:02:46,859 --> 00:02:53,149
You might add 20% more hardware
and get only five percent.

39
00:02:54,004 --> 00:02:58,214
Of performance boost
this attacks in action.

40
00:02:58,714 --> 00:03:02,195
Let's look at the math of resort death.

41
00:03:02,695 --> 00:03:08,595
Imagine an application pots and m
and cash pos in starting set type.

42
00:03:08,685 --> 00:03:12,565
Every port opens connection
pool to every cash.

43
00:03:12,565 --> 00:03:17,335
No, with 1000 application ports and 100.

44
00:03:17,835 --> 00:03:18,705
Cash notes.

45
00:03:19,155 --> 00:03:24,615
You suddenly have 100
thousands open connections.

46
00:03:25,155 --> 00:03:32,345
This exhausts a file descriptor and create
massive connection for top of rack switch.

47
00:03:32,845 --> 00:03:36,025
This system literally shocks itself.

48
00:03:36,525 --> 00:03:40,630
This is the most critical chart for SRE.

49
00:03:41,410 --> 00:03:44,530
The latency was throughout procure.

50
00:03:45,220 --> 00:03:47,590
There are three zone, A linear zone.

51
00:03:48,190 --> 00:03:49,860
When load is low.

52
00:03:50,579 --> 00:03:51,839
Latency is stable.

53
00:03:52,440 --> 00:03:58,780
A point of inflation where queue
queues start form latency shift

54
00:03:58,780 --> 00:04:00,645
from Lanier to exponential.

55
00:04:01,435 --> 00:04:03,385
And a saturation zone.

56
00:04:03,385 --> 00:04:08,525
This is a blessed zone where
we heat the throughout ceiling.

57
00:04:09,225 --> 00:04:11,835
Any tiny increase of flood

58
00:04:12,335 --> 00:04:15,695
Causes a catastrophical latency spikes.

59
00:04:16,445 --> 00:04:21,525
Our job is to keep the system in
a linear zone with enough safety

60
00:04:21,525 --> 00:04:24,655
margin to absorb traffic spikes.

61
00:04:25,155 --> 00:04:29,955
When throwing hardware to the
problem stops working, we use

62
00:04:30,114 --> 00:04:37,694
architectural painkillers like graceful
degradation circuit breakers, soft

63
00:04:37,874 --> 00:04:40,934
amounts, and reading stale data.

64
00:04:41,654 --> 00:04:46,215
Yes, we just don't wait for
our throttle resources forever.

65
00:04:46,604 --> 00:04:47,774
If the database.

66
00:04:48,149 --> 00:04:52,349
Doesn't respond within 20 milliseconds,
just kills the connection.

67
00:04:52,409 --> 00:04:57,239
It's better to serve a partial
response than a full response.

68
00:04:57,239 --> 00:04:57,569
Never.

69
00:04:58,069 --> 00:05:02,839
Once the painkillers are in the
place, we move to a physical

70
00:05:02,869 --> 00:05:07,639
segmentation multi AZ or Zal isation.

71
00:05:07,639 --> 00:05:09,349
The concept is simple.

72
00:05:09,589 --> 00:05:11,509
We duplicate the entire.

73
00:05:11,929 --> 00:05:15,949
Stack across individual
availability zones.

74
00:05:16,449 --> 00:05:23,819
Each zone gets its own microservices
load balance, cash layers, and databases.

75
00:05:24,819 --> 00:05:28,389
The goal is a total fault is a lesion.

76
00:05:28,449 --> 00:05:31,714
If one zone goes down, the others don't.

77
00:05:32,214 --> 00:05:35,844
And the price is infrastructure overhead.

78
00:05:36,294 --> 00:05:42,254
You're paying for triple set of resources
and a complex data synchronization.

79
00:05:42,754 --> 00:05:46,364
At this point, your
dashboard might look great.

80
00:05:46,464 --> 00:05:51,314
Multi disease rion, circuit
breakers are turned.

81
00:05:51,314 --> 00:05:52,759
The system seems stable.

82
00:05:53,534 --> 00:05:54,884
But don't default.

83
00:05:55,084 --> 00:05:57,334
This is illusion of stability.

84
00:05:57,814 --> 00:06:02,614
We have mask the symptoms, but
we haven't cured the disease.

85
00:06:03,094 --> 00:06:08,614
Our painkillers helped us survive
the day, but they didn't affect the

86
00:06:08,884 --> 00:06:11,974
underlying inefficient to truly scale.

87
00:06:12,664 --> 00:06:19,154
We need to stop fighting side effects
and start rethinking our services.

88
00:06:19,254 --> 00:06:23,704
Our how our service handle
data at the lowest level.

89
00:06:24,204 --> 00:06:29,814
To move the needle on performance, we
need to look inside the black boxes.

90
00:06:29,894 --> 00:06:34,824
In typical grid intensive
systems resources leak in

91
00:06:34,824 --> 00:06:38,124
a three zones protection.

92
00:06:38,214 --> 00:06:41,869
We burn CPU cycle processing in official.

93
00:06:42,439 --> 00:06:43,549
Format months.

94
00:06:43,549 --> 00:06:48,229
The runtime spent more time digitalizing
objects than executing a business.

95
00:06:48,229 --> 00:06:48,649
Logic.

96
00:06:49,399 --> 00:06:53,239
A transfer a network is
our toughest bottleneck.

97
00:06:53,779 --> 00:07:00,239
We, we waste bandwidth on redundant key
and metadata repeated in every single.

98
00:07:00,839 --> 00:07:06,379
Request and the storage how data
sits in, in, in memory dedicates

99
00:07:06,409 --> 00:07:08,509
its IO and RAM consumption.

100
00:07:09,109 --> 00:07:14,729
Reading an entire row just
to get one column is luxury.

101
00:07:15,089 --> 00:07:15,869
We can't afford.

102
00:07:16,369 --> 00:07:23,179
We need to make our data smarter and
more efficient in all three states.

103
00:07:23,679 --> 00:07:29,109
In highlights data formats aren't
a matter of developer convenience.

104
00:07:30,009 --> 00:07:37,659
They are resources entry ticket for
your CPU and ram, how you pick your

105
00:07:37,659 --> 00:07:43,709
bytes data minds, how much actual
work your processes can handle.

106
00:07:44,209 --> 00:07:44,309
Core.

107
00:07:45,134 --> 00:07:50,529
Our core optimizing
levels are bin over text.

108
00:07:51,279 --> 00:07:58,189
Moving your data from human readable
formats eliminate overhead of par systems

109
00:07:58,729 --> 00:08:02,409
syntax, and drastically shrinks payload.

110
00:08:02,909 --> 00:08:08,519
Zero copy access utilizing the structures
that doesn't require sterilization.

111
00:08:09,339 --> 00:08:16,689
By mapping data directly to memory,
we bypass the CPU tax of creation and

112
00:08:16,689 --> 00:08:21,404
eliminate a garbage collection pressure
physically skipping unnecessary bite.

113
00:08:21,664 --> 00:08:27,609
If you only need three fields
out of 100, the system should.

114
00:08:28,099 --> 00:08:29,319
Mon sorry.

115
00:08:30,169 --> 00:08:35,334
Saving this KO Network, bandwidth
and memory stripping s schemas

116
00:08:35,395 --> 00:08:37,995
and field needs from message body.

117
00:08:38,025 --> 00:08:44,335
By using Center Central Registry,
the Y occurs only the data,

118
00:08:44,835 --> 00:08:50,565
Optimizing data isn't just about saving
space, is about reclaiming CPU cycles

119
00:08:50,565 --> 00:08:55,615
from infrastructure to power your
logic, to power your business logic.

120
00:08:56,115 --> 00:08:57,765
Clean code and elegant.

121
00:08:57,955 --> 00:09:03,005
Abstractions are great but the hardware
doesn't see them at higher scale.

122
00:09:03,485 --> 00:09:07,765
Your CPU only cares about
data collect and cache.

123
00:09:08,440 --> 00:09:15,845
Efficiency grouping data into ETF
object is develop friendly, but it often

124
00:09:16,215 --> 00:09:23,285
result in polluted cache lines forcing
the CPU and load unnecessary bytes.

125
00:09:23,785 --> 00:09:24,505
Mal Lightning.

126
00:09:24,920 --> 00:09:30,810
That layout with the CPU physical
architecture can eliminate caius and

127
00:09:30,850 --> 00:09:34,630
unlock hardware through processing speed.

128
00:09:35,380 --> 00:09:37,510
There's not a silver bullet.

129
00:09:37,560 --> 00:09:40,780
One pattern might excel
and a random as well.

130
00:09:41,170 --> 00:09:46,440
Another dominates in a stream procession
don't default toward standard.

131
00:09:46,940 --> 00:09:53,060
Profile measure and choose layout
that fits your specific max pattern.

132
00:09:53,560 --> 00:09:58,540
We are shifting from bite level
optimization to high level architecture.

133
00:09:59,320 --> 00:10:03,130
When your rate load hits a
wall, stop using the same

134
00:10:03,130 --> 00:10:05,970
data models in every process.

135
00:10:06,660 --> 00:10:08,430
It's time for CQS.

136
00:10:09,420 --> 00:10:13,200
The principle simple prepare
data for task in advance.

137
00:10:13,560 --> 00:10:18,640
We take a single source of
truth and generate a projection

138
00:10:18,640 --> 00:10:22,080
tailored for specific processes.

139
00:10:22,410 --> 00:10:28,730
Each projection contains only the
field and format require by customer,

140
00:10:29,510 --> 00:10:33,430
no blood, no overhead, so the format.

141
00:10:34,340 --> 00:10:36,550
A set the structures ready.

142
00:10:36,910 --> 00:10:43,490
Now we must move the data as
close to the customer as possible.

143
00:10:44,120 --> 00:10:50,330
We are shifting from an external cache
to cache directly within the memory.

144
00:10:50,830 --> 00:10:56,290
So the benefits are zero network
latency in process memory.

145
00:10:56,340 --> 00:10:59,980
Access is measured in
nanoseconds, not in milliseconds.

146
00:11:00,820 --> 00:11:04,710
And resource efficiency, we
flowed a shattered cached cluster.

147
00:11:05,210 --> 00:11:12,170
The but the challenge is if your
cash everything, the port will

148
00:11:12,220 --> 00:11:14,360
run out of memory instantly.

149
00:11:14,860 --> 00:11:17,140
So that type of question is useless.

150
00:11:17,140 --> 00:11:24,890
Without Smart r we transform
our ports into from identical

151
00:11:24,890 --> 00:11:27,290
copies into specialized data.

152
00:11:27,570 --> 00:11:29,580
Shark, we achieve this.

153
00:11:29,790 --> 00:11:36,530
We are three mechanism, a service
discovery, a real time tracking of exact.

154
00:11:36,920 --> 00:11:38,150
Clustered topology.

155
00:11:38,840 --> 00:11:45,470
A clean side balancing is a clean
celiac is the target node eliminated.

156
00:11:45,530 --> 00:11:49,960
Unnecessary network hubs
and a consistent mechanism.

157
00:11:50,650 --> 00:11:56,410
A consistent hash to choose a right
po it guarantees that a request

158
00:11:56,440 --> 00:12:00,170
of specific my D always lands.

159
00:12:00,635 --> 00:12:04,415
On the same note, this
create data locality.

160
00:12:04,955 --> 00:12:09,905
Each pot stores only its
unique slice of the global data

161
00:12:10,405 --> 00:12:11,905
and a final touch.

162
00:12:12,405 --> 00:12:16,145
The different formats in
different storage tires.

163
00:12:16,695 --> 00:12:18,255
What work on a disc.

164
00:12:18,615 --> 00:12:27,285
Is often poison In A CPU we prioritize
storage based on the environment.

165
00:12:27,785 --> 00:12:31,715
External storage is like
noise scale or L 2K.

166
00:12:32,255 --> 00:12:35,795
The priority are compactness
and backward compatibility.

167
00:12:36,295 --> 00:12:40,255
We optimize for transfer
sites and storage volume.

168
00:12:40,755 --> 00:12:45,115
In process ces here we
fight for every C cycle.

169
00:12:45,115 --> 00:12:51,305
We use large, specific language formats
native to your machine and runtime.

170
00:12:52,175 --> 00:12:56,435
We use structures at all
that I'm allowed direct.

171
00:12:56,975 --> 00:12:58,835
Field access without passing.

172
00:12:59,135 --> 00:13:04,935
For example use flood buffers for
transport but transforms them into

173
00:13:04,935 --> 00:13:07,805
native object upon internal support.

174
00:13:07,805 --> 00:13:10,595
For instance, zero decoder access.

175
00:13:11,095 --> 00:13:16,970
It is the bottom line of this architecture
marathon economics versus risk.

176
00:13:17,470 --> 00:13:23,710
When classic set up, 100% of
request hit the external caches.

177
00:13:23,800 --> 00:13:31,230
In our target architecture, 85%
is settled in the local cache, and

178
00:13:31,260 --> 00:13:34,710
only 1% ever reaches the databases.

179
00:13:35,210 --> 00:13:38,100
This delivers massive resource efficiency.

180
00:13:38,100 --> 00:13:38,820
You need.

181
00:13:39,320 --> 00:13:45,630
A significantly fewer service and
drastically cut your AZ traffic costs.

182
00:13:46,560 --> 00:13:53,185
But on the other hand, there is
there is a com very complex system

183
00:13:53,185 --> 00:13:55,735
with trading simplicity for density.

184
00:13:56,305 --> 00:13:58,520
The system is harder
to maintain and impact.

185
00:13:59,420 --> 00:14:04,310
Unpacking the hardware and reduces
your safety margin and urine

186
00:14:04,340 --> 00:14:11,070
anomalies out scale and monitoring
now requires surgical precision.

187
00:14:11,570 --> 00:14:14,150
Yes, the system is complex.

188
00:14:14,270 --> 00:14:19,370
Every layer we discuss introduces
new protocols and design risks.

189
00:14:19,870 --> 00:14:25,270
But this complexity unlock the data
density and performance that are simply

190
00:14:25,360 --> 00:14:27,320
impossible in a standard hardware.

191
00:14:27,820 --> 00:14:33,560
So this architecture is a balance
between survival and complex.

192
00:14:33,690 --> 00:14:38,800
Before implementing this solution ensure
you're fighting the physical limits of

193
00:14:38,800 --> 00:14:42,160
hardware and not just suboptimal code.

194
00:14:43,145 --> 00:14:43,720
Thank you.

195
00:14:44,220 --> 00:14:44,340
I.

