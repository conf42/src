1
00:00:01,000 --> 00:00:04,490
Hi welcome to my presentation
called the Limits of Imagination,

2
00:00:04,910 --> 00:00:05,900
an open source journey.

3
00:00:06,200 --> 00:00:09,350
Basically, this is about like my open
source journey and projects in natural

4
00:00:09,350 --> 00:00:10,730
language processing over the years.

5
00:00:11,250 --> 00:00:13,809
Starting from 2018 till 2025.

6
00:00:14,079 --> 00:00:15,729
Basically like seven years of work.

7
00:00:16,700 --> 00:00:18,590
Of these projects are open source.

8
00:00:18,890 --> 00:00:20,120
My name is Aroma Rodricks.

9
00:00:20,120 --> 00:00:21,260
This is a little bit about me.

10
00:00:21,320 --> 00:00:22,100
This is my bio.

11
00:00:22,700 --> 00:00:23,840
I'm a Python enthusiast.

12
00:00:23,840 --> 00:00:25,280
I've been coding since 2014.

13
00:00:25,369 --> 00:00:26,660
It'll be a decade next year.

14
00:00:26,880 --> 00:00:32,550
I'm a multi speaker, so I've spoken at
Picon, US Estonia, Lithuania, Hong Kong,

15
00:00:32,550 --> 00:00:38,310
Sweden, Europe, Python, bio Hire, Picon,
South Africa, India, and also had a pro

16
00:00:38,360 --> 00:00:40,310
project being present at first Asia.

17
00:00:40,740 --> 00:00:43,800
I've been on the natural language
processing train since 2018.

18
00:00:44,040 --> 00:00:48,650
I do fun experiments sometimes through
data python and language models because

19
00:00:48,650 --> 00:00:53,150
I believe as do most multinational
companies now that the human condition is

20
00:00:53,150 --> 00:00:57,890
encoded in language just as science and
math, and it's inevitable that one day we

21
00:00:57,890 --> 00:01:02,360
will be able to use computers to help us
linguistically as they do mathematically.

22
00:01:02,780 --> 00:01:09,260
I've been using this bio since 2019
and it has come true basically.

23
00:01:11,650 --> 00:01:15,190
So these are like some of the stories
that I'm going to be sharing with you.

24
00:01:15,310 --> 00:01:17,290
I'm going to be sharing
like 10 stories with you.

25
00:01:17,800 --> 00:01:19,990
The first one is from Picon n India, 2018.

26
00:01:20,260 --> 00:01:22,030
It's a tho conditions summarizer.

27
00:01:22,630 --> 00:01:25,600
The second one is Picon
South Africa in 2019.

28
00:01:25,870 --> 00:01:27,610
It's an NLP fake News detector.

29
00:01:27,910 --> 00:01:30,130
The third one is Picon US 2024.

30
00:01:30,640 --> 00:01:32,710
It's called Only BMOs in the building.

31
00:01:33,395 --> 00:01:35,555
The first one is bike on NTR 2018.

32
00:01:35,675 --> 00:01:37,235
Terms and conditions, summarize it.

33
00:01:37,995 --> 00:01:40,845
So terms and conditions, everyone
knows terms and conditions

34
00:01:41,115 --> 00:01:42,705
and it's all very clunky.

35
00:01:42,905 --> 00:01:46,565
Not great to go through, but you
basically just have to do it.

36
00:01:48,035 --> 00:01:51,115
I personally have not gone through a
lot of terms and conditions except like

37
00:01:51,145 --> 00:01:52,825
a couple of like rental agreements.

38
00:01:53,125 --> 00:01:56,815
And for this project specifically,
I think I went through a terms

39
00:01:56,815 --> 00:01:58,495
and conditions document by PayPal.

40
00:01:59,365 --> 00:02:00,205
If I remember right.

41
00:02:01,185 --> 00:02:05,745
We are always drowning in
unreadable legal language across

42
00:02:05,775 --> 00:02:07,335
emails, apps, and websites.

43
00:02:08,025 --> 00:02:12,015
What I was trying to do is I was trying
to build an NLP pipeline to extract

44
00:02:12,015 --> 00:02:15,765
obligations, permissions, and risks
from terms and conditions, documents.

45
00:02:16,125 --> 00:02:17,210
Basically what I wanted to do.

46
00:02:18,405 --> 00:02:21,525
Was when I was going through
like a document that was full of

47
00:02:21,615 --> 00:02:26,955
terms and conditions, I wanted to
see the risks associated in that

48
00:02:26,955 --> 00:02:28,245
terms and conditions document.

49
00:02:28,515 --> 00:02:31,275
The risk had to be highlighted
in like red basically.

50
00:02:32,530 --> 00:02:36,610
It would have empowered users
to skim legal agreements with

51
00:02:36,610 --> 00:02:38,020
context and not confusion.

52
00:02:38,200 --> 00:02:41,870
And if the line would be like a risk,
it would like basically be highlighted.

53
00:02:42,980 --> 00:02:47,210
My problem with this particular project
was that there was no existing data asset.

54
00:02:47,780 --> 00:02:52,010
And I had to hand annotate
a bunch of sentences as

55
00:02:52,280 --> 00:02:53,900
obligation, permission, or risk.

56
00:02:54,510 --> 00:02:57,240
So I built a label data set
from scratch using first

57
00:02:57,240 --> 00:02:59,070
principles of contact structure.

58
00:02:59,530 --> 00:03:03,190
The outcome was an NLP pipeline
to extract obligations, risks, and

59
00:03:03,190 --> 00:03:07,540
user rights from legal documents
because there was no usable dataset.

60
00:03:08,620 --> 00:03:10,720
I could only answer it,
I want to say around two.

61
00:03:12,430 --> 00:03:15,719
Which was not a good amount to
set of train a good classifier.

62
00:03:16,935 --> 00:03:21,345
But I created an SNLI style dataset
from scratch using first principles.

63
00:03:22,215 --> 00:03:26,484
And what you could do today to improve
accuracy is basically to use some of

64
00:03:26,484 --> 00:03:32,665
those sentences to one shot train, an
LLM, which is a large language model, or

65
00:03:32,665 --> 00:03:36,355
you could use an LLM, which is a large
language model to create like synthetic

66
00:03:36,355 --> 00:03:41,695
data of around 10,000 sentences to train
like a more powerful classifier basically.

67
00:03:42,909 --> 00:03:46,659
This project basically came from
my work at Fidelity Investments.

68
00:03:47,489 --> 00:03:51,899
We were working on a bunch of 4 0 1
accounts and what happened at that time

69
00:03:51,899 --> 00:03:55,859
was there's supposed to be a lot of
rules that are supposed to be applied

70
00:03:56,249 --> 00:03:57,899
on of all of the accounts later.

71
00:03:58,319 --> 00:04:01,259
And what happens is you
have legal documents.

72
00:04:01,634 --> 00:04:07,194
That are these rules and then you
have aqua map in like a language that

73
00:04:07,194 --> 00:04:10,585
computers can understand, but there
is no mapping between these two.

74
00:04:11,304 --> 00:04:14,395
So because I was working with
natural language at that time, I

75
00:04:14,395 --> 00:04:18,304
was like, we could maybe create
our own rule processor of thought.

76
00:04:20,614 --> 00:04:23,960
The second one is from back on US 2024.

77
00:04:24,150 --> 00:04:25,879
It's called Only Bads in the Building.

78
00:04:27,179 --> 00:04:28,559
I was spending a lot of time at home.

79
00:04:29,499 --> 00:04:33,129
During this week called fix
hackler at work, where they just

80
00:04:33,129 --> 00:04:34,539
let you do whatever you want.

81
00:04:34,959 --> 00:04:38,979
And I had a couple of existential
questions that I wanted answers to.

82
00:04:39,039 --> 00:04:42,039
For example, will forensic
portrait artists keep their jobs?

83
00:04:42,129 --> 00:04:44,109
Can AI write my job block better than me?

84
00:04:44,859 --> 00:04:47,349
Can I build a clueless
trial fashion match?

85
00:04:47,619 --> 00:04:49,929
How do I under clean, non
duplicate synthetic data?

86
00:04:50,949 --> 00:04:55,479
So basically what happened was I was
waiting at home for my H one B Visa and

87
00:04:55,479 --> 00:04:57,219
I was not in contact with my colleagues.

88
00:04:57,309 --> 00:05:00,819
They were like 12 hours away from
me, so I could not collaborate

89
00:05:00,819 --> 00:05:01,959
with them for this week.

90
00:05:02,519 --> 00:05:04,229
I went through an existential crisis.

91
00:05:04,619 --> 00:05:09,359
I got into a little bit of stoicism,
looked up a lot of philosophy,

92
00:05:09,809 --> 00:05:12,539
and that's why I had all of these
philosophical questions basically.

93
00:05:13,369 --> 00:05:16,069
So who gets to keep their
jobs in this post AI world?

94
00:05:16,674 --> 00:05:18,804
Will forensic portrait
makers keep their jobs?

95
00:05:18,804 --> 00:05:20,454
Not so what happened was.

96
00:05:21,129 --> 00:05:25,419
What happened was we were going through
a lot of large language models being

97
00:05:25,419 --> 00:05:30,139
released in the world and everyone had
this exist existential crisis of whether

98
00:05:30,139 --> 00:05:31,579
they were going to keep their jobs or not.

99
00:05:32,329 --> 00:05:36,199
And because I'm also an artist on the
side, do I don't draw a lot of portraits.

100
00:05:36,799 --> 00:05:41,899
I was like what's a good job to
get into if they do take away

101
00:05:41,929 --> 00:05:43,399
my software engineering job?

102
00:05:44,809 --> 00:05:46,929
And, I was like what is AI not good at?

103
00:05:46,959 --> 00:05:48,069
What is AI not good at?

104
00:05:48,519 --> 00:05:51,189
And the answers were
like, oh, maybe images.

105
00:05:51,399 --> 00:05:53,349
But they have to be like
very specific images.

106
00:05:53,499 --> 00:05:55,299
And when do you need very specific images?

107
00:05:55,839 --> 00:05:59,499
So the answer that came to me was
forensic portrait makers, basically.

108
00:06:00,279 --> 00:06:02,229
And what I did was I went onto Kaggle.

109
00:06:02,559 --> 00:06:06,879
I found this data set of character
actors, what happens with character.

110
00:06:08,244 --> 00:06:11,004
They are like very they're
very descriptive, right?

111
00:06:11,334 --> 00:06:14,484
You're always introduced to a
character as, or he was like

112
00:06:14,994 --> 00:06:16,644
a particularly looking man.

113
00:06:16,704 --> 00:06:22,044
He had a certain kind of a nose very
bushy eyebrows, maybe like a spot on the

114
00:06:22,044 --> 00:06:23,664
top of his lip, et cetera, et cetera.

115
00:06:25,194 --> 00:06:30,034
So the first one is a picture of the
actor and all of the rest of those

116
00:06:30,034 --> 00:06:33,080
pictures are from they're created by ai.

117
00:06:34,005 --> 00:06:37,365
So it's a man with a curly
hair, dark eyes, small nose

118
00:06:37,365 --> 00:06:39,075
and thin lips, wearing glasses.

119
00:06:39,795 --> 00:06:42,165
And there is a couple of, like
other descriptors basically.

120
00:06:42,925 --> 00:06:44,695
And this is basically what AI created.

121
00:06:45,045 --> 00:06:50,175
I do think this is a particularly hard
example simply because the first one

122
00:06:50,575 --> 00:06:55,795
if you see the structure of his like
upper lip, it's like a certain structure

123
00:06:55,795 --> 00:06:58,015
that's like difficult to explain.

124
00:06:58,690 --> 00:07:01,990
Unless you like gesture, put
like your fingers or something.

125
00:07:03,670 --> 00:07:07,960
So the AI didn't get it right till,
I want to say the third picture.

126
00:07:08,450 --> 00:07:10,520
Here's a couple of like other examples.

127
00:07:11,000 --> 00:07:13,130
This wasn't a very good example either.

128
00:07:13,180 --> 00:07:17,170
It didn't get it right and it also put on
a lot of like facial hair or something.

129
00:07:17,920 --> 00:07:21,100
This is another example with a
blonde character with dark eyes

130
00:07:21,100 --> 00:07:23,660
and a side smile and fringe.

131
00:07:23,720 --> 00:07:28,490
His eyes are dark and small and his lips
are thin, his upper teeth are visible.

132
00:07:28,490 --> 00:07:30,080
He is smiling and he seems excited.

133
00:07:31,640 --> 00:07:34,460
None of them accurate
are the original actor.

134
00:07:35,330 --> 00:07:39,170
So basically my conclusion was
that, oh, maybe forensic portrait

135
00:07:39,270 --> 00:07:40,950
painters will keep their job.

136
00:07:41,610 --> 00:07:45,560
And then what happened is I put in
this example I really this example

137
00:07:45,560 --> 00:07:50,630
because this is the perfect example
of what how to say, a really nicely

138
00:07:50,630 --> 00:07:52,820
descriptive character actor looks like.

139
00:07:54,830 --> 00:07:59,050
And what happens because of this
specific sort of description

140
00:07:59,050 --> 00:08:00,820
is AI almost gets it right.

141
00:08:01,480 --> 00:08:08,240
You should see these examples,
especially the first generated image.

142
00:08:08,450 --> 00:08:09,440
It's like very similar.

143
00:08:09,950 --> 00:08:12,650
It's very similar to the
face of the actor itself.

144
00:08:13,430 --> 00:08:15,440
These are from, I want
to say a bank creator.

145
00:08:15,660 --> 00:08:17,055
These are from Dreamworks.

146
00:08:18,210 --> 00:08:18,810
And

147
00:08:21,240 --> 00:08:22,740
both of them did really well.

148
00:08:22,770 --> 00:08:30,180
I want to say even the bottom left
over here looks like the Charact actor.

149
00:08:31,150 --> 00:08:31,990
What did I do then?

150
00:08:32,120 --> 00:08:35,750
I thought about what would I do
if they took away like my job?

151
00:08:35,750 --> 00:08:36,380
What would I do?

152
00:08:36,770 --> 00:08:39,410
So I have a couple of dream projects
that I would spend a lot of time

153
00:08:39,410 --> 00:08:42,185
on, and one of my dream project
was to write my own travel blog.

154
00:08:43,895 --> 00:08:45,725
But I had a lot of logistical issues.

155
00:08:45,725 --> 00:08:49,845
Like for instance I took 300
pictures of a day out in China.

156
00:08:50,535 --> 00:08:54,855
So what I wanted to do was I
wanted to make AI do it basically.

157
00:08:56,385 --> 00:09:01,760
So I wrote so I wanted to write my
own, and basically what I try to

158
00:09:01,760 --> 00:09:06,170
do is I just try to use a lot of AI
models to write my trial log for me.

159
00:09:06,720 --> 00:09:10,410
I had a lot of photos that I
plugged into from my drive.

160
00:09:10,870 --> 00:09:15,580
Used a caption and I would just
say did pretty well actually

161
00:09:15,610 --> 00:09:20,060
some of the captions like what a
child would say about an image.

162
00:09:20,450 --> 00:09:24,740
For instance, this one says My prompt
was on her day out in China, a.

163
00:09:27,185 --> 00:09:28,325
And it's all very simplistic.

164
00:09:28,325 --> 00:09:30,035
It's oh, I saw through a window.

165
00:09:30,905 --> 00:09:33,565
It doesn't say a lot about the
image or anything descriptive,

166
00:09:33,655 --> 00:09:35,245
but it was like, it was factual.

167
00:09:35,305 --> 00:09:36,115
It was a caption.

168
00:09:36,935 --> 00:09:40,615
The second image, it's on her
day out in Chen Aroma saw I saw

169
00:09:40,615 --> 00:09:42,535
this beautiful floor on her.

170
00:09:42,535 --> 00:09:46,735
Day out in Chen Aroma saw she spotted
a car driving down the street.

171
00:09:47,455 --> 00:09:52,885
Now the issue with that is that if
you check out images on Instagram,

172
00:09:53,755 --> 00:09:55,735
you'll see an image posted.

173
00:09:56,470 --> 00:10:00,010
Like the one with the car and like all of
those like colorful buildings of sorts.

174
00:10:00,730 --> 00:10:04,390
And it wouldn't look like, oh, she
spotted a car driving down the street.

175
00:10:04,420 --> 00:10:07,300
It would be something like, oh, there
were like a lot of colorful buildings

176
00:10:07,840 --> 00:10:12,490
lining the street that would, that is
what like an artist would say, or someone

177
00:10:12,490 --> 00:10:13,930
who was using Instagram would say.

178
00:10:13,980 --> 00:10:17,910
But AI basically decided to
concentrate on cost simply because

179
00:10:17,910 --> 00:10:19,720
of like how it had been tried.

180
00:10:21,120 --> 00:10:28,600
On aroma saw the plums of the frania
trees were it basically gave me

181
00:10:28,600 --> 00:10:32,370
like the descriptors, but it didn't
do a really good job with adding

182
00:10:32,370 --> 00:10:34,320
poetry until the captions basically.

183
00:10:34,800 --> 00:10:37,780
I was like there's some work
to do, but it'll probably just

184
00:10:37,780 --> 00:10:38,740
write my child off for me.

185
00:10:39,340 --> 00:10:42,340
And then I just basically gave
up and watched a movie, and the

186
00:10:42,340 --> 00:10:43,720
movie that I watched was Clueless.

187
00:10:44,860 --> 00:10:49,390
And there's this scene where it
lets you like match dresses and

188
00:10:49,420 --> 00:10:53,740
lets the dress be fit onto your
body so that you can see what you

189
00:10:53,740 --> 00:10:56,420
look like on that Kevin side, and I.

190
00:10:58,190 --> 00:11:01,580
That sounds absurd, but can AI do
it because we can't do it anymore.

191
00:11:01,630 --> 00:11:06,960
So I went to Kaggle again, and I looked
up like a data set that had a lot of

192
00:11:06,990 --> 00:11:14,005
images of tops, bottoms dresses, et
cetera, et cetera, and I plugged it into.

193
00:11:14,765 --> 00:11:16,415
This collab notebook of thoughts.

194
00:11:16,865 --> 00:11:22,355
And what I tried to do is I tried to
superimpose it on the photograph of a

195
00:11:22,355 --> 00:11:27,245
model wearing completely, a completely
different dress of thoughts by finding

196
00:11:27,245 --> 00:11:31,895
out the boundaries of the dress that they
were wearing, like the top or the bottom.

197
00:11:32,315 --> 00:11:34,235
And I tried to repair it using AI.

198
00:11:34,685 --> 00:11:36,925
And let's see what AI basically did.

199
00:11:38,335 --> 00:11:40,885
So this is what the original
image sort looked like.

200
00:11:41,545 --> 00:11:43,725
And if you look at repair images.

201
00:11:43,785 --> 00:11:48,975
I would just say the third and the
fourth one look slightly normal.

202
00:11:49,485 --> 00:11:54,180
Everything else looks off, but the third
and the fourth ones look slightly normal.

203
00:11:55,455 --> 00:11:58,755
A long way to go, but we are
basically on the right path.

204
00:11:59,455 --> 00:12:03,715
The third one is from South Africa
in 2019, which was a natural language

205
00:12:03,715 --> 00:12:05,275
processing, fake news detector.

206
00:12:05,905 --> 00:12:08,665
So the goal was to trace
who was being blamed.

207
00:12:09,015 --> 00:12:10,695
Misinformation, not just flag.

208
00:12:10,755 --> 00:12:11,295
What's fake.

209
00:12:11,805 --> 00:12:15,915
The dataset was real WhatsApp forwards,
informal, noisy, multilingual.

210
00:12:16,485 --> 00:12:21,335
My method was combined SciPi and
NLT care based event models with POS

211
00:12:21,335 --> 00:12:26,765
tagging and dependency parsing with
a lot of like minimal training data

212
00:12:26,765 --> 00:12:30,965
required basically, because these
were parsers, these were not trainers.

213
00:12:30,965 --> 00:12:32,255
These were not classifiers.

214
00:12:32,890 --> 00:12:34,150
The improvement is today.

215
00:12:34,390 --> 00:12:37,750
LMS will offer richer context,
better understanding of blame

216
00:12:37,750 --> 00:12:39,700
dynamics and scalable solutions.

217
00:12:40,840 --> 00:12:42,430
So impact of fake news.

218
00:12:43,050 --> 00:12:49,410
Buzzfeed, they were like 22 million
interactions on the top 50 fake stories.

219
00:12:49,410 --> 00:12:52,170
This is from 2018 Knight Foundation.

220
00:12:52,590 --> 00:12:57,960
10 million plus tweets from 700 K plus
accounts linked to fight conspiracy

221
00:12:57,960 --> 00:13:00,930
news routers, which is in India.

222
00:13:01,785 --> 00:13:04,185
52% get news from WhatsApp.

223
00:13:04,335 --> 00:13:07,485
Rumor fuel violence had led to deaths.

224
00:13:07,825 --> 00:13:11,025
This was around the time when
WhatsApp forwards would basically

225
00:13:11,025 --> 00:13:12,675
get you Lynch back home in India.

226
00:13:13,385 --> 00:13:16,755
So techniques for detection keyboard
extraction and verification.

227
00:13:16,855 --> 00:13:20,665
So basically I used direct and
LCK for extracting key phrases.

228
00:13:21,025 --> 00:13:22,645
I used a news api.

229
00:13:22,705 --> 00:13:26,245
Google had a news API back then
to crosscheck real coverage

230
00:13:26,245 --> 00:13:27,415
of keyword based claims.

231
00:13:28,135 --> 00:13:32,515
I used reverse image search
for detecting Photoshop images.

232
00:13:32,735 --> 00:13:36,485
There was a lot of content verification,
so I compared articles from spoof

233
00:13:36,485 --> 00:13:38,465
sites versus mainstream media.

234
00:13:39,035 --> 00:13:44,465
I used a lot of fact checking platforms
such as all news or hooks, layers.

235
00:13:45,035 --> 00:13:49,325
And for textual cues, we use grammar
and spelling mistakes, overtly

236
00:13:49,325 --> 00:13:53,135
positive or negative sentiment,
no sources or suspicious sources

237
00:13:53,405 --> 00:13:55,085
and repetition of certain words.

238
00:13:55,805 --> 00:13:59,635
Some other techniques that were
used, and this is way before we

239
00:13:59,635 --> 00:14:03,325
had like really good classifiers
on sentences before bot basically.

240
00:14:04,585 --> 00:14:09,385
So all of these things are normal
parcels, and they use NLTK from Stanford.

241
00:14:09,905 --> 00:14:14,245
Syntactic patterns, POS tagging
statements to detect blame assignments,

242
00:14:14,575 --> 00:14:20,780
price even casually active passive
voice patterning using NLTK entity

243
00:14:20,780 --> 00:14:25,065
and emotion tracking tracking name
politicians, detecting associated

244
00:14:25,065 --> 00:14:27,675
emotions that are fear, hatred, sympathy.

245
00:14:29,130 --> 00:14:33,210
My basic goal over here was
to actually detect propaganda.

246
00:14:33,800 --> 00:14:38,270
The reason I wanted to detect
propaganda was because sometimes the

247
00:14:38,270 --> 00:14:40,710
news stories are like, oh, X happened.

248
00:14:41,190 --> 00:14:44,910
And sometimes it's X happened because
of Y. So they're basically trying

249
00:14:44,940 --> 00:14:47,760
to blame Y for whatever happened.

250
00:14:47,970 --> 00:14:48,840
Whatever happened.

251
00:14:49,500 --> 00:14:53,520
So there is, like in literature,
there is a thing called depart

252
00:14:53,520 --> 00:14:56,410
model of blame, coded what and why.

253
00:14:57,700 --> 00:15:03,460
In literature, like in languages,
they basically have a certain language

254
00:15:03,460 --> 00:15:09,340
model to identify if the structural
features of that particular sentence

255
00:15:09,730 --> 00:15:12,760
would actually lead to someone to blame.

256
00:15:13,495 --> 00:15:17,215
So there were like positive words,
basically ordered, cause claims.

257
00:15:18,445 --> 00:15:22,855
There was some sort of thresholding,
like the percentage of sentences that

258
00:15:22,855 --> 00:15:25,285
showed propaganda structure in that text.

259
00:15:26,045 --> 00:15:28,655
So this is an example of how
the parts are basically worked.

260
00:15:29,155 --> 00:15:32,535
This is a sentence and we are
just putting in like a pattern.

261
00:15:32,970 --> 00:15:35,730
Now these patterns at that time,
because this is a parcel, right?

262
00:15:35,730 --> 00:15:38,220
So you would have to write
your own pattern based on your

263
00:15:38,220 --> 00:15:41,670
understanding of language and
whatever is present in that text.

264
00:15:42,690 --> 00:15:46,540
And after parsing it, you would
find out if it's followed the

265
00:15:46,540 --> 00:15:48,340
path model of blame model or not.

266
00:15:49,910 --> 00:15:52,850
So these are some of the other stories
that I'm going to share with you.

267
00:15:53,240 --> 00:15:58,640
The one in Estonia in 2023 is called If
Your Friends Are Bullshitting, using,

268
00:15:59,870 --> 00:16:04,710
the other ones are were presented
at Europe Biden in 2022 by in Hong

269
00:16:04,710 --> 00:16:07,800
Kong in 2021 by on Sweden in 2021.

270
00:16:08,230 --> 00:16:12,070
Which was of how weird conditions
to believe the news is polarized.

271
00:16:12,620 --> 00:16:15,500
The other one was presented
at Bio Ohio in 2020.

272
00:16:15,800 --> 00:16:19,340
It was analyzing bias in
children's educational materials.

273
00:16:20,320 --> 00:16:23,890
The one in Picon, Estonia in
2023 is called If Your Friends

274
00:16:23,920 --> 00:16:25,720
Are Bullshitting Using SNLI.

275
00:16:26,240 --> 00:16:30,350
SNLI is one of my favorite data
sets specifically because of like

276
00:16:30,380 --> 00:16:35,630
iner contradiction and neutral
sentiment analysis basically because

277
00:16:35,640 --> 00:16:38,280
I think it models a lot of things.

278
00:16:39,410 --> 00:16:42,770
That we need, we will need to
use in the future, basically.

279
00:16:43,470 --> 00:16:49,590
For instance, even when we are giving
like an instruction to an LLM, you need to

280
00:16:50,190 --> 00:16:55,550
make sure that it's, understands if it's
a contradiction, if it's an entitlement

281
00:16:56,030 --> 00:17:00,680
or if it's like neutral in comparison to
like the instruction it was given before.

282
00:17:01,585 --> 00:17:05,425
So the goal was to use natural language
processing to spot contradictions

283
00:17:05,425 --> 00:17:09,165
in statements proving when your
friends are being inconsistent.

284
00:17:10,485 --> 00:17:15,405
This project actually came from, it did
actually come from a lot of bullshit that

285
00:17:15,405 --> 00:17:19,875
I heard from my friends, for instance
they were like, oh, we are like 15 minutes

286
00:17:19,875 --> 00:17:23,265
away, and then they wouldn't show up
for the next 30 minutes or something.

287
00:17:23,445 --> 00:17:25,035
That's where this
project really came from.

288
00:17:26,355 --> 00:17:27,765
So it was a real life scenario.

289
00:17:28,015 --> 00:17:32,155
The models that I used at that time
were to compare using both and GP

290
00:17:32,155 --> 00:17:35,905
two embeddings, GPD two embeddings
for detecting contradiction.

291
00:17:36,535 --> 00:17:39,535
My data set was to leverage
SNLI for building and

292
00:17:39,535 --> 00:17:40,915
training contradiction models.

293
00:17:41,305 --> 00:17:45,385
The impact was to improve L lamp
coherency, tackle, hallucinations, and

294
00:17:45,385 --> 00:17:47,515
filter, fake or contradictory news.

295
00:17:49,790 --> 00:17:53,830
So this is like some of the slides that
are presented in this presentation.

296
00:17:54,130 --> 00:17:54,580
It's like sex.

297
00:17:54,580 --> 00:17:56,260
Why is you hone your bullshit detector?

298
00:17:56,260 --> 00:17:59,350
Edward we're like surrounded
by bullshit all the time.

299
00:17:59,350 --> 00:18:04,350
It would be really nice to give someone
else the job to detect bullshit.

300
00:18:04,530 --> 00:18:06,690
It's like thinking through
bullshit all the time.

301
00:18:07,460 --> 00:18:10,130
There's a couple of articles that
go like how to direct bullshit

302
00:18:10,430 --> 00:18:11,840
even on sites like LinkedIn.

303
00:18:11,930 --> 00:18:13,670
So it is actually a real problem.

304
00:18:13,670 --> 00:18:15,260
It's not just a personal problem.

305
00:18:15,650 --> 00:18:21,240
It would also be I want to say
a professional problem or how

306
00:18:21,240 --> 00:18:22,650
to measure someone's bullshit.

307
00:18:23,370 --> 00:18:28,710
And it seems to be for someone on the
spectrum, for people on the spectrum.

308
00:18:28,710 --> 00:18:32,340
So I guess like people on the
spectrum needs a bullshit detector

309
00:18:32,400 --> 00:18:34,040
more than someone else needs it.

310
00:18:34,380 --> 00:18:38,250
So there's three classifications
for bullshiting using SNLI.

311
00:18:38,490 --> 00:18:39,660
The first one is ENT entailment.

312
00:18:39,810 --> 00:18:42,630
The second one is contradiction,
and the third one is neutrality.

313
00:18:43,260 --> 00:18:45,960
Now these things mean
exactly what they're saying.

314
00:18:46,320 --> 00:18:49,560
So entailment means that there
is two sentences that basically

315
00:18:49,610 --> 00:18:51,110
say almost the same thing.

316
00:18:51,110 --> 00:18:52,640
They like support each other.

317
00:18:53,120 --> 00:18:57,140
Contradiction is like when they contradict
each other and neutrality is like when

318
00:18:57,140 --> 00:19:00,950
they don't know anything or they don't
have anything to do with each other.

319
00:19:02,115 --> 00:19:06,255
For instance, Jim rides a bike
to school every morning, and the

320
00:19:06,255 --> 00:19:07,785
second one is Jim can ride a bike.

321
00:19:09,525 --> 00:19:14,175
I would say these are like lightly
entailed because they don't contradict

322
00:19:14,175 --> 00:19:17,355
each other and they're not too
neutral with respect to each other.

323
00:19:17,355 --> 00:19:22,095
They do actually have some source
of information with respect to both.

324
00:19:24,155 --> 00:19:27,335
So this is an example from the
paper that was published using SNLI.

325
00:19:27,545 --> 00:19:29,345
The premise is a dog jumping.

326
00:19:29,645 --> 00:19:31,085
For a Frisbee in the snow.

327
00:19:31,715 --> 00:19:35,495
The example one is an animal is outside in
the cold weather, playing with a plastic

328
00:19:35,495 --> 00:19:41,225
toy, which is an entailment, an animal,
a dog is outside in the cold weather,

329
00:19:41,345 --> 00:19:43,655
in the snow, playing with a plastic toy.

330
00:19:43,655 --> 00:19:46,595
A plastic toy is a Frisbee,
so it's an entailment.

331
00:19:46,955 --> 00:19:50,825
The example two is a cat washed his
face and whiskers with his front P.

332
00:19:51,695 --> 00:19:56,315
Now the animal is a dog, so
clearly it's not a cat, and

333
00:19:56,435 --> 00:19:58,085
they're like washing their face.

334
00:19:58,700 --> 00:20:00,080
Whiskers with their front paw.

335
00:20:00,080 --> 00:20:04,580
So it's like contradicting what is
being said in the first sentence,

336
00:20:05,630 --> 00:20:10,040
which says clearly that it's a dog
jumping for a Frisbee in this no.

337
00:20:10,970 --> 00:20:15,530
The third example is a pet is enjoying
a game of fetch with his owner.

338
00:20:15,990 --> 00:20:20,020
This is not neutral because it could
I want to say in sale or it could be a

339
00:20:20,020 --> 00:20:25,240
contradiction or it could be like not
related to the first sentence at all.

340
00:20:25,240 --> 00:20:31,780
I want to say precisely if you wanted to
use by precise language, I would say the

341
00:20:31,780 --> 00:20:38,890
actual sentence that would be a direct
contradiction of the premise is a dog is

342
00:20:39,100 --> 00:20:41,230
like a dog and you'd have to name the dog.

343
00:20:41,595 --> 00:20:46,935
A dog is eating a bone inside the house
and you would have to basically make

344
00:20:46,935 --> 00:20:50,625
sure that it's the same dog that we are
speaking about as in the first sentence.

345
00:20:51,205 --> 00:20:55,675
SNLI is a collection of 570 k,
human written English sentence

346
00:20:55,675 --> 00:20:59,785
pairs, manually labeled for balanced
classifications with labels,

347
00:20:59,785 --> 00:21:01,615
entailment, contradiction, and neutral.

348
00:21:02,005 --> 00:21:06,865
We aim for it to serve both as a benchmark
for evaluating representational systems

349
00:21:06,865 --> 00:21:11,605
for text, especially including those
induced by representation learning

350
00:21:11,605 --> 00:21:16,015
methods, as well as resource for
developing NLP models of any kind.

351
00:21:16,735 --> 00:21:19,015
This these are like examples from a line.

352
00:21:19,375 --> 00:21:23,995
There's text judgments and hypothesis
and everything in the judgment column.

353
00:21:24,625 --> 00:21:27,145
Everything in the judgment
column is basically the label.

354
00:21:29,065 --> 00:21:32,235
So this was basically my pipeline.

355
00:21:32,925 --> 00:21:36,495
What on the screen is basically the
pipeline that I use to find out if

356
00:21:36,495 --> 00:21:38,265
my friends were bullshit using Sana.

357
00:21:38,505 --> 00:21:39,405
That's a dataset.

358
00:21:39,495 --> 00:21:44,025
We use a tokenize, we convert like all
of our sentences into tokens and we

359
00:21:44,025 --> 00:21:48,505
put them in into a model and we get
like pre-trained everything sort of it.

360
00:21:49,135 --> 00:21:52,655
And then we put it into
a new model for training.

361
00:21:52,955 --> 00:21:54,965
Basically what we are doing
at like the second step.

362
00:21:55,925 --> 00:21:59,855
We are trying to use whatever
information has already been

363
00:21:59,885 --> 00:22:02,795
learned by a certain model.

364
00:22:02,795 --> 00:22:10,475
For instance, in this case it's B or
GT two, and we are extracting a form

365
00:22:10,745 --> 00:22:14,555
of that sentence that has all of that
information from like the contact

366
00:22:14,555 --> 00:22:17,975
space of b or two, which is what?

367
00:22:18,065 --> 00:22:21,695
Which is why what we get
out is a pre-trained ing.

368
00:22:22,250 --> 00:22:28,160
There's a sentence, and the end product
is basically a pre-reading, which has

369
00:22:28,160 --> 00:22:32,680
the semantic or like the meaningful
understanding of that particular sentence

370
00:22:33,370 --> 00:22:37,750
in the context of everything that has been
learned by a model such as, but or GT two.

371
00:22:39,535 --> 00:22:40,885
So I enter this new model.

372
00:22:40,885 --> 00:22:45,955
We're basically putting in all of our
old training as well, so it gets to use

373
00:22:46,165 --> 00:22:51,025
it's old training as well as gets to do
some sort of like new magic on top of it.

374
00:22:51,625 --> 00:22:54,405
And the last one was the
inference box basically.

375
00:22:54,795 --> 00:22:56,295
Some details about the birth model.

376
00:22:56,295 --> 00:23:00,235
The birth model was proposed in bird
pre-training of deep bidirectional

377
00:23:00,295 --> 00:23:05,215
transformers for language understanding
by Jacob Devlin Ming by Chime,

378
00:23:05,545 --> 00:23:08,305
Canton Lee and Christina Anova.

379
00:23:08,545 --> 00:23:13,345
It's a bidirectional transformer
pre-trained using a combination of

380
00:23:13,345 --> 00:23:16,345
mass language modeling objective,
and the next sentence production

381
00:23:16,645 --> 00:23:21,115
on a large corpus comprising the
turn to book corpus and Wikipedia.

382
00:23:22,000 --> 00:23:23,710
The abstract from the
paper is the following.

383
00:23:23,950 --> 00:23:27,040
We introduced a new language
representation model called Bird,

384
00:23:27,250 --> 00:23:31,220
which stands for bi-directional encoder
representations from transformers.

385
00:23:31,700 --> 00:23:36,110
Unlike recent language representation
models, bird is designed to pre-train

386
00:23:36,200 --> 00:23:41,540
deep bi-directional representations from
unlabeled text by jointly conditioning on

387
00:23:41,540 --> 00:23:43,850
both left and right context in all layers.

388
00:23:44,310 --> 00:23:48,000
As a result, the pre-trade birth
model can be fine tuned with just

389
00:23:48,000 --> 00:23:51,660
one additional output layer to create
the state-of-the-art models for a

390
00:23:51,660 --> 00:23:55,960
wide range of tasks, such as question
answering and language efforts without

391
00:23:55,960 --> 00:24:00,880
substantial tasks, specific architecture
modifications, but is conceptually

392
00:24:00,880 --> 00:24:02,770
simple and am empirically powerful.

393
00:24:03,130 --> 00:24:08,150
It obtains new state of the art results
on 11 natural language processing tasks,

394
00:24:08,270 --> 00:24:11,780
including pushing the glue score to 80.5%.

395
00:24:13,060 --> 00:24:14,050
This is all really old.

396
00:24:14,620 --> 00:24:18,980
Yeah, we don't really need bird
today, but this is an old project.

397
00:24:19,580 --> 00:24:20,780
I say it's a very old project.

398
00:24:20,780 --> 00:24:24,380
It's just, I want to say two
years old or three years old.

399
00:24:24,920 --> 00:24:29,480
But that's basically how massive
the jump has been from Bird

400
00:24:30,170 --> 00:24:32,150
to what we do today using GPT.

401
00:24:33,380 --> 00:24:37,890
So this is like from the open source
documentation associated with birth.

402
00:24:38,970 --> 00:24:42,480
The open a IG BT model was
proposed in language models.

403
00:24:42,720 --> 00:24:48,780
Our unsupervised multitask learners by
Radford, Jeffrey W River, Charles David

404
00:24:48,810 --> 00:24:53,190
Luan, Dar Moai, and Elia Soki from OpenAI.

405
00:24:54,030 --> 00:24:58,170
So this is a large transformer based
language model with 1.5 billion

406
00:24:58,170 --> 00:25:03,030
parameters too, and it's trained on
a data set of 8 million web pages.

407
00:25:03,720 --> 00:25:06,510
All right, so this is like a page from.

408
00:25:08,880 --> 00:25:11,490
It's open source sort of documentation.

409
00:25:11,970 --> 00:25:14,490
These are the two language
models that I use basically.

410
00:25:15,240 --> 00:25:22,170
And this is a diagram for you to
understand what happens when we get

411
00:25:22,290 --> 00:25:24,600
pre-trained evidence from a certain model.

412
00:25:25,020 --> 00:25:26,760
So this is the sematic space.

413
00:25:27,420 --> 00:25:29,010
This is like meaningful space.

414
00:25:30,120 --> 00:25:32,460
What it basically means
is this certain word.

415
00:25:33,460 --> 00:25:37,300
A certain distance from the
X axis, meaningful X axis.

416
00:25:37,540 --> 00:25:42,010
We don't know what the meaningful x
axis means, but the model nodes and the

417
00:25:42,010 --> 00:25:47,110
meaningful y axis and the meaningful C
axis, basically, and these are like all

418
00:25:47,110 --> 00:25:49,060
of the distances between those two words.

419
00:25:49,210 --> 00:25:52,390
And those are like the sematic
distances between those two words

420
00:25:52,510 --> 00:25:56,380
that basically says the meaning of
this particular word is different

421
00:25:56,380 --> 00:26:00,010
from this, the meaning of this
particular word by so and so distance.

422
00:26:00,910 --> 00:26:02,650
These are basically what I did.

423
00:26:02,770 --> 00:26:06,090
I just trains both and G two embed things.

424
00:26:06,640 --> 00:26:10,200
This is another example of
it's two headlines from two

425
00:26:10,200 --> 00:26:12,360
different editions of news.

426
00:26:12,420 --> 00:26:15,630
The first one says, Bernie
Sanders scored victories for

427
00:26:15,630 --> 00:26:17,610
years via legislative siders.

428
00:26:18,285 --> 00:26:20,865
The second one says we
are legislative siders.

429
00:26:20,955 --> 00:26:24,375
Bernie Sanders won
modest victories, right?

430
00:26:24,645 --> 00:26:27,545
The first one makes it seem like
the victories were really grand.

431
00:26:28,145 --> 00:26:31,565
And the second one makes it seem
like the victories were very modest.

432
00:26:32,225 --> 00:26:35,555
So someone who's reading the
first edition would think that

433
00:26:35,555 --> 00:26:37,175
it was like a great thing.

434
00:26:37,205 --> 00:26:40,205
It would be praise the second edition.

435
00:26:40,265 --> 00:26:43,595
It would make it seem like it was not
praise, it was not such a great thing.

436
00:26:44,465 --> 00:26:48,305
So this would basically slightly
contradict each other, right?

437
00:26:48,915 --> 00:26:50,535
So these are like some of the labels.

438
00:26:50,955 --> 00:26:51,765
I do not remember.

439
00:26:51,765 --> 00:26:53,535
I think label two is contradiction.

440
00:26:53,835 --> 00:26:55,935
And you can see like how.

441
00:26:58,275 --> 00:27:02,045
The scores are like higher for the
first and the last sentence and like

442
00:27:02,045 --> 00:27:04,745
slightly lower for the second sentence.

443
00:27:05,485 --> 00:27:10,925
Maybe it just feels like modest victories
and victories is not that that different.

444
00:27:10,925 --> 00:27:15,615
Maybe it's if we basically
said that Bernie Sanders lost.

445
00:27:17,310 --> 00:27:21,570
It would say that it's contradictory
on a large scale, but it does say that

446
00:27:21,660 --> 00:27:24,210
it is contradictory though lightly.

447
00:27:24,900 --> 00:27:30,100
So this is another project that I
presented at Europe by, in 2022 by, in

448
00:27:30,100 --> 00:27:33,280
Hong Kong 2021 at by on Sweden in 2021.

449
00:27:33,840 --> 00:27:37,920
It was to analyze sentiments and news
headlines to examine bias, comparing

450
00:27:37,920 --> 00:27:40,260
it to public perception from surveys.

451
00:27:41,000 --> 00:27:42,170
The initial approaches.

452
00:27:42,515 --> 00:27:45,965
Basically fail to capture the complexity
of these sentiments and the bias

453
00:27:45,965 --> 00:27:48,185
in headlines, the revised approach.

454
00:27:49,365 --> 00:27:52,545
It leveraged advanced NLP
techniques to metro line sentiment

455
00:27:52,545 --> 00:27:54,165
with real world reader biases.

456
00:27:54,675 --> 00:27:59,635
The outcome was the failure led to deeper
understanding of contextual sentiments and

457
00:27:59,635 --> 00:28:03,955
analysis and bias measurement essential
for interpreting media influence.

458
00:28:05,165 --> 00:28:07,985
So my next project was at Bio Ohio 2020.

459
00:28:08,165 --> 00:28:11,345
It was analyzing bias in
children's educational materials.

460
00:28:11,925 --> 00:28:15,085
This was specifically gender
biased such as portraying female

461
00:28:15,085 --> 00:28:18,715
characters predominantly as
mothers and housewives while male

462
00:28:18,775 --> 00:28:20,995
characters was seen as breadwinners.

463
00:28:21,575 --> 00:28:22,835
I use NFP techniques.

464
00:28:23,285 --> 00:28:24,905
This was way before birth.

465
00:28:26,495 --> 00:28:27,965
Or maybe both had just come out.

466
00:28:28,895 --> 00:28:34,655
So I was using a lot of parser structures
and rudimentary NLP techniques.

467
00:28:34,925 --> 00:28:36,425
Using mostly NLTK.

468
00:28:37,445 --> 00:28:41,165
We can analyze representation bias by
tracking the frequency of gender terms.

469
00:28:41,165 --> 00:28:45,385
I. Identify stereotypes through
objective associations and detect

470
00:28:45,385 --> 00:28:47,635
victim blaming language and text.

471
00:28:47,965 --> 00:28:51,055
For instance, in some countries,
the proportion of female characters

472
00:28:51,055 --> 00:28:54,925
in mathematics textbooks was
found to be as low as 30%.

473
00:28:55,195 --> 00:29:00,595
To combat these biases is crucial
to revise curriculum, to promote

474
00:29:00,895 --> 00:29:05,125
gender equality, update textbooks to
be more inclusive, and use NLP tools

475
00:29:05,515 --> 00:29:09,775
to continually monitor and correct
those biases in educational content.

476
00:29:10,435 --> 00:29:12,145
So some of these
techniques are really easy.

477
00:29:12,145 --> 00:29:15,775
It's just basically counting a lot of
pronouns, making sure your entities

478
00:29:15,775 --> 00:29:20,135
are mapped really well to their
genders and counting how many people

479
00:29:20,525 --> 00:29:22,235
of that particular gender are present.

480
00:29:22,835 --> 00:29:26,345
And because of like rudimentary
techniques, you were able to

481
00:29:26,345 --> 00:29:28,895
like PUS tag a lot of objectives.

482
00:29:29,075 --> 00:29:32,285
So you could associate objectives
with the entities in that particular

483
00:29:32,285 --> 00:29:36,475
sentence and be able to deduce like
the sentiment of that objective.

484
00:29:36,835 --> 00:29:42,805
You would not need a lot of I want
to say high tech NLP techniques.

485
00:29:42,805 --> 00:29:46,885
You would only need maybe a
dictionary and a score of sorts.

486
00:29:47,375 --> 00:29:51,365
So my research focus was examining how
gender bias in children's educational

487
00:29:51,365 --> 00:29:53,915
materials shapes, perceptions and roles.

488
00:29:54,415 --> 00:29:55,015
For example.

489
00:29:56,380 --> 00:30:00,880
Female characters predominantly
portrayed as housewives, while male

490
00:30:00,940 --> 00:30:02,860
characters are shown as breadwinners.

491
00:30:03,010 --> 00:30:06,490
My key findings were there was
a representation bias, so female

492
00:30:06,490 --> 00:30:10,030
characters were underrepresented
and fields like mathematics.

493
00:30:10,390 --> 00:30:14,190
So a lot of the people doing
the math used to be meant.

494
00:30:14,400 --> 00:30:21,270
So like it would be Tom bought 12
watermelons and never like daisy bots.

495
00:30:21,630 --> 00:30:22,650
12 What moment?

496
00:30:23,200 --> 00:30:24,670
There was also steward of bias.

497
00:30:24,670 --> 00:30:29,460
For instance a doctor was usually
male and a nurse was usually female.

498
00:30:29,610 --> 00:30:31,650
There was also sort of cultural blame.

499
00:30:32,070 --> 00:30:36,810
So a lot of like victim blaming
languages maybe using passive language

500
00:30:36,810 --> 00:30:38,460
as opposed to active language.

501
00:30:38,960 --> 00:30:43,100
Like she was raped instead
of someone raped her.

502
00:30:43,790 --> 00:30:47,780
So we did a lot of like analysis
on techniques for bias detection.

503
00:30:47,840 --> 00:30:50,990
Representation analyst is
using token frequency in text.

504
00:30:51,095 --> 00:30:54,365
Stereotype evaluation using
objective and role association

505
00:30:54,845 --> 00:31:00,005
using spacey and LTK, blame language
detection via path model of blame,

506
00:31:00,005 --> 00:31:03,635
identifying casual and labeling bias.

507
00:31:03,995 --> 00:31:10,155
The change that I came out from the
study was to revise the curriculum

508
00:31:10,275 --> 00:31:14,325
to reflect gender equality and
to remove these stereotypes.

509
00:31:14,925 --> 00:31:20,315
So some of the other stories that I'm
going to share with you are one of it

510
00:31:20,315 --> 00:31:22,985
is from by Lithuania again in 2023.

511
00:31:23,735 --> 00:31:26,480
It's about chatting with Chad about
everything and nothing at all.

512
00:31:27,050 --> 00:31:32,030
It was when Charge g BT first came out and
most of my talk is basically based out of

513
00:31:32,030 --> 00:31:35,480
my free time explorations with charge GBT.

514
00:31:36,935 --> 00:31:38,735
It started out as like a fun experiment.

515
00:31:38,735 --> 00:31:41,915
I used to even post on social media
about, oh, this is what I asked Chad,

516
00:31:42,275 --> 00:31:43,775
and this is what it said back to me.

517
00:31:44,165 --> 00:31:46,025
A lot of like jailbreaking
and all of that.

518
00:31:46,455 --> 00:31:49,305
And I decided to make a
whole study out of it.

519
00:31:49,305 --> 00:31:53,115
Basically, my next story is about
AI and software development.

520
00:31:53,235 --> 00:31:58,155
So it's about experimenting with SVMs
and why SVMs are really important.

521
00:31:58,425 --> 00:32:02,895
And my ninth story is about
choose for the visually impaired.

522
00:32:03,025 --> 00:32:06,175
This was a project that was
presented at First Asia in 2016.

523
00:32:06,875 --> 00:32:11,375
So Pi Lithuania 2023 chatting with Chad
Gpt about everything and nothing at all.

524
00:32:12,745 --> 00:32:17,275
So it was like an overview of
Chad gps rise and the question of

525
00:32:17,275 --> 00:32:19,135
whether it was through AGI I or not.

526
00:32:19,765 --> 00:32:23,004
There was an explanation of the
training process for including

527
00:32:23,004 --> 00:32:24,925
data usage and model architecture.

528
00:32:25,084 --> 00:32:29,735
I subjected it to the WIT test because
at that point Jiri was like trained on a

529
00:32:29,735 --> 00:32:32,195
lot of data that was present on Reddit.

530
00:32:32,644 --> 00:32:34,684
So it was not very good with wit.

531
00:32:35,404 --> 00:32:40,144
So I decided to test like what it
feels or like what it understands

532
00:32:40,234 --> 00:32:41,794
through like various riddles.

533
00:32:42,274 --> 00:32:46,624
And I also use like riddles from like
specific languages like Hindi or like

534
00:32:46,624 --> 00:32:51,694
some other sort of like cultural nuanced
riddles to see how it would perform.

535
00:32:52,174 --> 00:32:54,934
And like complex context and
nuance language basically.

536
00:32:55,414 --> 00:33:00,124
It had at that point, it had issues
with verbal math problems and concept

537
00:33:00,154 --> 00:33:02,314
concepts of understanding Venn diagrams.

538
00:33:02,314 --> 00:33:07,594
So if you would give a kid a problem
statement to solve using a Venn diagram,

539
00:33:07,594 --> 00:33:12,634
they would be able to solve it easily
if they were, I want to say in grade

540
00:33:12,664 --> 00:33:14,804
eight, to charge GPD at that time.

541
00:33:15,924 --> 00:33:17,544
Also sensory experience gaps.

542
00:33:17,634 --> 00:33:21,204
So highlighting the model's, inability
to process real world sensory

543
00:33:21,204 --> 00:33:23,934
experiences such as hearing an egg crack.

544
00:33:24,264 --> 00:33:28,974
'cause it, it didn't know what
happened after you heard an egg

545
00:33:28,974 --> 00:33:32,274
crack 'cause it didn't have the
sensors to sense any of those things.

546
00:33:32,914 --> 00:33:35,104
Also metaphorical
language on relationships.

547
00:33:35,164 --> 00:33:40,174
Oh, at that time in 2023, child jupi was
not really good at metaphorical language.

548
00:33:40,474 --> 00:33:42,094
And comparing it to human performance.

549
00:33:42,334 --> 00:33:46,084
Basically to answer the question
of whether charge GPD is a GI.

550
00:33:47,374 --> 00:33:52,324
My answer to that today actually
is that in certain ways charge

551
00:33:52,324 --> 00:33:54,364
g PT is better than humans.

552
00:33:55,204 --> 00:34:00,534
So it is I want to say a GI plus,
and at some things it's not, so it's.

553
00:34:01,434 --> 00:34:03,294
In those areas, it's not a GI.

554
00:34:03,654 --> 00:34:07,174
So some of these questions that
I basically asked Chad GTI was,

555
00:34:07,594 --> 00:34:09,334
when is a door no longer a door?

556
00:34:09,904 --> 00:34:13,204
So the answer you gave me was
very logical, but because this is

557
00:34:13,204 --> 00:34:15,214
a riddle, it's a riddle, right?

558
00:34:15,724 --> 00:34:19,324
So it's like a door is no longer a
door when it no longer serves the

559
00:34:19,324 --> 00:34:23,704
purpose of providing an entrance or
exit to a room or building, or when

560
00:34:23,704 --> 00:34:25,774
it is unable to be open or closed.

561
00:34:26,994 --> 00:34:27,864
Logical answer.

562
00:34:28,164 --> 00:34:29,064
Logical answer.

563
00:34:29,454 --> 00:34:33,774
But then I write it that we were
solving griddles, so it had to be like

564
00:34:33,774 --> 00:34:35,274
in a fun mood of thoughts, basically.

565
00:34:35,604 --> 00:34:36,924
So I was like this is a riddle.

566
00:34:36,924 --> 00:34:37,524
Answer it.

567
00:34:37,914 --> 00:34:39,594
When is a door no longer a door?

568
00:34:40,779 --> 00:34:44,039
And then it's grab the answer
from somewhere and it's like

569
00:34:44,039 --> 00:34:45,419
training data or something.

570
00:34:45,419 --> 00:34:48,959
It was like a door is no longer
a door when it's a jar, which is

571
00:34:48,959 --> 00:34:50,129
basically what the answer was.

572
00:34:50,129 --> 00:34:51,359
So it was like way witty.

573
00:34:52,059 --> 00:34:54,939
The second question was, what
tastes better than it smells?

574
00:34:55,479 --> 00:34:59,669
And BOUs, it had like previous context
of oh, we were in like riddle mode.

575
00:34:59,729 --> 00:35:03,719
It knew that something that sounded
absurd, like that was a riddle.

576
00:35:04,964 --> 00:35:06,494
And it told me back, this is a riddle.

577
00:35:06,674 --> 00:35:10,184
The answer is tongue because it
ties food but doesn't have a smell.

578
00:35:10,804 --> 00:35:12,484
What building has the most stories?

579
00:35:12,774 --> 00:35:15,684
It detected that this was a riddle.

580
00:35:15,684 --> 00:35:18,474
It said the answer is library
because it has very stories.

581
00:35:18,534 --> 00:35:19,134
It was right.

582
00:35:20,244 --> 00:35:24,174
It got a bunch of them rights after I
told it that we were like in riddle mode.

583
00:35:24,174 --> 00:35:28,314
So maybe it had to access like
some sort of like training memory

584
00:35:28,794 --> 00:35:34,154
that was associated with riddle and
like witty I want to say language

585
00:35:36,164 --> 00:35:40,874
and I guess it got like some sort
of an answer, which is probably

586
00:35:40,924 --> 00:35:42,154
I dunno, it could be right.

587
00:35:42,244 --> 00:35:43,984
Does the stackers have
a bottom at the top?

588
00:35:43,984 --> 00:35:44,524
I don't know.

589
00:35:45,574 --> 00:35:47,404
But the real answer was
supposed to be legs.

590
00:35:47,884 --> 00:35:52,734
And it did give an explanation that could
in some contexts be the right explanation.

591
00:35:53,184 --> 00:35:55,104
So I did really well on creative tasks.

592
00:35:55,544 --> 00:35:59,374
I asked it like a bunch of questions
again around sensory experiences as well.

593
00:35:59,584 --> 00:36:02,074
It didn't have really good
answers for it at that time.

594
00:36:02,764 --> 00:36:05,554
So my next presentation is
about experimenting with SVMs.

595
00:36:06,124 --> 00:36:10,594
This was presented at AI and
Software Development Summit in 2024.

596
00:36:11,054 --> 00:36:19,774
The reasons we like SVMs, I like SVMs
or like data scientists like SVMs.

597
00:36:20,254 --> 00:36:21,874
It's called Support Vector Clustering.

598
00:36:23,029 --> 00:36:26,779
It's a clustering technique using SPM
principles for unsupervised learning,

599
00:36:27,289 --> 00:36:31,129
it maps data to high dimensional hill
butt space versus a kernel trick.

600
00:36:31,789 --> 00:36:35,299
It finds minimum enclosing
sphere and then maps boundaries

601
00:36:35,299 --> 00:36:36,469
back to the original sphere.

602
00:36:37,129 --> 00:36:40,549
It uses gian kernel, which is
a non-linear transformation.

603
00:36:40,729 --> 00:36:42,649
It has two key hyper parameters.

604
00:36:42,979 --> 00:36:45,979
Q stands for kernel width
controls cluster granularity.

605
00:36:46,849 --> 00:36:50,599
P, which is a soft machine
controls stall, and star outliers.

606
00:36:51,259 --> 00:36:55,309
And it does not assume shape
or the number of clusters, it

607
00:36:55,309 --> 00:36:56,569
basically adapts naturally.

608
00:36:57,199 --> 00:37:03,559
So going over it is basically the size of
the circle inside, which you would want

609
00:37:03,559 --> 00:37:08,149
your points to be placed to say that,
oh, this is the space that is similar.

610
00:37:08,824 --> 00:37:10,454
Then all of the others.

611
00:37:11,024 --> 00:37:14,504
And p is it's like this soft
margin between two peaks.

612
00:37:14,804 --> 00:37:18,164
If there are two peaks, there
could be points that lie in both

613
00:37:18,164 --> 00:37:19,754
of, like those circles of sorts.

614
00:37:20,354 --> 00:37:26,044
So you want to make sure that
those between points lie in

615
00:37:26,044 --> 00:37:29,314
like circle one or circle two,
either of those circles, right?

616
00:37:29,794 --> 00:37:34,274
Which is a factor that we call p And these
are like the hyper parameters that are.

617
00:37:35,789 --> 00:37:36,659
To get that data.

618
00:37:37,249 --> 00:37:41,839
So SBMs basically handle non
convex, overlapping noisy data.

619
00:37:42,229 --> 00:37:46,549
They have better accuracy than traditional
clustering on complex data sets.

620
00:37:46,729 --> 00:37:52,359
For example Iris is like a very, I would
just say f dataset that data set like

621
00:37:52,389 --> 00:37:53,949
data scientists like to talk about.

622
00:37:54,389 --> 00:37:55,979
It's an elegant blend of theory.

623
00:37:56,504 --> 00:37:58,254
And practical clustering part.

624
00:37:59,424 --> 00:38:03,894
The reason I really like spms,
the reason I really like spms, and

625
00:38:03,894 --> 00:38:08,964
this is like an example that I give
everyone, is that say you have a

626
00:38:08,964 --> 00:38:11,064
line which is only an X axis, right?

627
00:38:12,744 --> 00:38:15,924
And it has, I want to say on the screen

628
00:38:18,834 --> 00:38:21,594
14, those have to be 14 dots.

629
00:38:22,164 --> 00:38:26,834
So you can see that clearly, there's
seven green bolts and then there's

630
00:38:26,834 --> 00:38:30,794
seven red bolts and you're able
to draw like a line in between.

631
00:38:30,794 --> 00:38:35,594
And you're able to say that these are like
two different clusters of data basically.

632
00:38:36,974 --> 00:38:39,974
But what happens if the data is mixed?

633
00:38:40,484 --> 00:38:43,004
Like on the line on the third line?

634
00:38:43,004 --> 00:38:47,204
Basically it's like mix of like
red and green balls, so you can't

635
00:38:47,204 --> 00:38:48,854
really go in and draw a line.

636
00:38:51,239 --> 00:38:52,619
The X to say that.

637
00:38:52,679 --> 00:38:55,019
This is the separation
between these two clusters.

638
00:38:55,739 --> 00:39:01,229
Now what happens is, and this is
basically the principle of svs, we

639
00:39:01,229 --> 00:39:03,329
introduce a different dimension.

640
00:39:04,679 --> 00:39:08,219
We introduce a different dimension,
but take all of our data into this

641
00:39:08,219 --> 00:39:12,119
different dimension, basically a
y axis, and we separate it out.

642
00:39:12,629 --> 00:39:18,619
For instance, if all of the red colored
balls go to the positive y-axis and

643
00:39:18,619 --> 00:39:22,909
all of the green colored balls come
to the negative y axis, we can just

644
00:39:22,909 --> 00:39:28,879
separate out the data by saying
that the separator is the X axis.

645
00:39:29,599 --> 00:39:31,579
So this is basically what s VNS do.

646
00:39:31,759 --> 00:39:37,429
And this was my presentation at AI and
Software Development Summits in 2024.

647
00:39:38,119 --> 00:39:41,509
The next project I want to talk
about is the one that I presented

648
00:39:41,509 --> 00:39:45,109
at First Asia in 2016, which was
choose for the visually impaired.

649
00:39:45,689 --> 00:39:50,159
My purpose was to improve daily mobility
and confidence for the visual impact.

650
00:39:50,729 --> 00:39:54,509
They were just shoes with
some basic ultrasound sensors.

651
00:39:54,989 --> 00:39:58,859
And what they wanted to do was they wanted
to sense an obstruction either in front

652
00:39:58,859 --> 00:40:01,219
of them, behind them, or at the sites.

653
00:40:01,739 --> 00:40:05,519
It was supposed to improve
mobility for visually impaired.

654
00:40:05,729 --> 00:40:09,829
So instead of using like an ultrasound
stick they would be able to get

655
00:40:09,829 --> 00:40:12,409
feedback using those ultrasound centers.

656
00:40:13,519 --> 00:40:18,499
The control unit was a. And it had a
Bluetooth module, which transmitted

657
00:40:18,499 --> 00:40:20,509
sensor data into an Android device.

658
00:40:20,959 --> 00:40:25,999
The Android app at that time was built
using MIT app, inventor and phone Gap.

659
00:40:26,359 --> 00:40:31,009
So our vision was to like maybe move to
a MATLAB based prototype at that time,

660
00:40:31,009 --> 00:40:36,399
because this was back in 2016 and, maybe
change like the feedback, because at the

661
00:40:36,399 --> 00:40:41,079
time it was like really clunky and you
would get feedback only through like voice

662
00:40:41,349 --> 00:40:43,149
into your Bluetooth headsets or something.

663
00:40:43,679 --> 00:40:47,939
A more graceful approach would be
to give like someone haptic feedback

664
00:40:48,089 --> 00:40:50,159
like in the clothes or something else.

665
00:40:50,969 --> 00:40:54,459
My last jury is create your
own data that are presented at.

666
00:40:56,769 --> 00:40:58,389
So when do you need your own data?

667
00:40:58,389 --> 00:41:04,029
We are in the mood of like
magical LLM thinking available

668
00:41:04,029 --> 00:41:06,039
to all of us at all times, right?

669
00:41:06,489 --> 00:41:08,589
So what do we do with lms?

670
00:41:09,309 --> 00:41:12,879
Generative models are useful for creating
synthetic data, even if they struggle

671
00:41:12,879 --> 00:41:14,769
with factual accuracy and reliability.

672
00:41:15,309 --> 00:41:17,739
Use case one is thought
lodge generator train.

673
00:41:17,739 --> 00:41:20,259
A model to generate Thought
Lodge is focusing on internal

674
00:41:20,259 --> 00:41:22,149
consistency rather than grounded two.

675
00:41:23,589 --> 00:41:27,759
Use case travel image tagging, generate
image captions from personal photos

676
00:41:27,759 --> 00:41:31,869
using LMS to create contextual and
creative tag for journaling or curation.

677
00:41:32,629 --> 00:41:34,429
Synthetic data creation through lms.

678
00:41:34,429 --> 00:41:38,179
Use LMS to bootstrap data sets
for tasks like totology generation

679
00:41:38,179 --> 00:41:40,729
or image captioning without
relying on factual correctness.

680
00:41:41,224 --> 00:41:44,914
I use techniques like distillation
and fine tuning to help adapt other

681
00:41:44,914 --> 00:41:48,964
elements to specialized tasks by
reusing generated data and tailoring

682
00:41:48,964 --> 00:41:50,614
models for specific use cases.

683
00:41:50,974 --> 00:41:54,274
The takeaway was synthetic data
generation can unlock meaningful

684
00:41:54,274 --> 00:41:58,714
creator workflows in areas where
factual accuracy is not a priority.

685
00:42:00,354 --> 00:42:03,354
So some examples from like the
experiments that I performed the

686
00:42:03,354 --> 00:42:07,634
original collab is like cocky is
hundred 10 examples of dot lodging.

687
00:42:07,634 --> 00:42:11,054
And do not give a reason why, because
usually what happens when you're like

688
00:42:11,054 --> 00:42:15,374
talking to something that was designed
to be a chat bot is they give you a lot

689
00:42:15,374 --> 00:42:21,074
of reasons back as to why the sentence
that they have is a chart logic.

690
00:42:21,344 --> 00:42:26,234
A bunch of these sentences are actually
not real ologies, but some of them are.

691
00:42:26,494 --> 00:42:31,084
And the trick that I found out to like
verify if the sentences are actually

692
00:42:31,084 --> 00:42:34,894
ologies, is to give them back to another
element to verify whether they are

693
00:42:34,894 --> 00:42:39,854
actually but some of the good examples
I really like are the second one.

694
00:42:39,884 --> 00:42:40,544
Water is wet.

695
00:42:40,844 --> 00:42:41,294
That's a tar.

696
00:42:42,344 --> 00:42:43,964
Fire is hot, that's a tar.

697
00:42:45,554 --> 00:42:49,374
And general 10 examples of metaphors
and do not give reasons why.

698
00:42:49,934 --> 00:42:55,844
Her voice was a CD that played hundred
p in my mind, and basically I gave

699
00:42:55,844 --> 00:42:57,734
her the prompt to use the word cd.

700
00:42:58,334 --> 00:42:59,474
That's straight up crazy, right?

701
00:42:59,554 --> 00:43:01,744
Can you think of a lot of
metaphors using the word cd?

702
00:43:02,194 --> 00:43:03,334
Her voice was a cd.

703
00:43:03,754 --> 00:43:07,384
His angers was a C that scratched and
sced the surface of our relationship.

704
00:43:07,744 --> 00:43:12,244
My job was a CD that's kept and glitched,
making it hardware enjoy the music.

705
00:43:13,414 --> 00:43:15,214
So there, this is synthetic data.

706
00:43:15,214 --> 00:43:16,564
It's really cool data.

707
00:43:16,564 --> 00:43:19,324
I just want to say it's really cool data.

708
00:43:19,774 --> 00:43:20,104
Thank you.

709
00:43:20,104 --> 00:43:25,764
This was all for an example of caption
generation using images that was

710
00:43:25,764 --> 00:43:27,294
something that I used in my travel.

711
00:43:27,984 --> 00:43:28,794
Thank you so much.

712
00:43:28,854 --> 00:43:30,894
The slides are of slides, carnival.

713
00:43:30,894 --> 00:43:31,674
Thank you so much.

714
00:43:31,714 --> 00:43:32,764
Have a good bye.

