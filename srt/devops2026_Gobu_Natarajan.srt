1
00:00:00,840 --> 00:00:01,620
Speaker 10: Hello everyone.

2
00:00:02,100 --> 00:00:07,620
I am Gober working for Amazon, and
today's topic is from demo to deployment

3
00:00:07,950 --> 00:00:12,300
building, evaluating and observing
multimodal conversational agents.

4
00:00:12,800 --> 00:00:16,129
And today I want to talk about
something many of us have seen, but

5
00:00:16,129 --> 00:00:18,230
far fewer have successfully deployed.

6
00:00:19,130 --> 00:00:22,820
We all have seen impressive
multimodal demos, right?

7
00:00:22,820 --> 00:00:26,780
It could be voice, it could be
vision, it could be touch all working

8
00:00:26,780 --> 00:00:28,640
beautifully in a controlled environment.

9
00:00:29,600 --> 00:00:33,710
But turning those demos into production
study systems that are relatable,

10
00:00:33,890 --> 00:00:37,190
observable, and cost effective
is a very different challenge.

11
00:00:37,940 --> 00:00:41,870
Let's take a real example of
how we interact with the voice

12
00:00:41,870 --> 00:00:43,670
enabled TV in our living room.

13
00:00:44,170 --> 00:00:49,520
Let's like like asking, it's find a
comedy movie for family night and turn

14
00:00:49,520 --> 00:00:53,960
this into a production system that
understand voice onscreen context,

15
00:00:54,320 --> 00:00:59,240
personalization signals and realtime
content availability behind the simple

16
00:00:59,240 --> 00:01:03,140
interaction with your multimodal
orchestration, you need to interact with.

17
00:01:03,830 --> 00:01:08,830
Speech ui ML models and cloud
services, along with continuous

18
00:01:08,830 --> 00:01:14,770
evaluation and deep observability to
ensure you are getting the accuracy,

19
00:01:15,100 --> 00:01:17,440
latency, and reliability at scale.

20
00:01:18,100 --> 00:01:23,620
This is a simple example of how your
multi conversational agents work in the

21
00:01:23,620 --> 00:01:27,790
background to enlight and delight the
customer experience in your living room.

22
00:01:28,645 --> 00:01:34,555
Today I walked you through how multimodal
conversational agents actually move

23
00:01:34,555 --> 00:01:40,045
from demo to deployment and what
it tends to break along the way.

24
00:01:40,545 --> 00:01:41,595
Let's go to the next slide.

25
00:01:42,215 --> 00:01:47,435
Let's get an introduction on what is this
multimodal shift is all about, right?

26
00:01:48,395 --> 00:01:51,365
Multimodal, conversational agents
are no longer experimental.

27
00:01:51,865 --> 00:01:56,065
They're quickly becoming the default
interaction model across smart homes,

28
00:01:56,125 --> 00:02:00,625
vehicles, variables, and any sort
of computing devices in your home.

29
00:02:01,125 --> 00:02:07,815
In field studies across 250 households, we
saw as 30 percentage improvement in task

30
00:02:07,815 --> 00:02:13,485
completion when users had access to multim
modalities instead of just voice or text.

31
00:02:14,325 --> 00:02:18,315
What's more interesting is
behavior within about three weeks.

32
00:02:18,675 --> 00:02:23,685
Users naturally shifted towards
multimodal commands without being taught.

33
00:02:23,685 --> 00:02:26,535
You don't need a special user training
to give them, Hey, this is how you

34
00:02:26,535 --> 00:02:31,665
should interact with, it just becomes
like a natural conversational activity.

35
00:02:31,725 --> 00:02:32,835
Any human will be doing it.

36
00:02:33,705 --> 00:02:35,925
That tells us this is inte.

37
00:02:36,735 --> 00:02:40,905
It's a fundamental shift in how
in humans interact with systems.

38
00:02:41,385 --> 00:02:44,835
So the question is, no
longer does multimodal work.

39
00:02:45,315 --> 00:02:49,995
The real question is how we operate
at scale, which we are going to cover

40
00:02:49,995 --> 00:02:51,315
as part of our today's discussion.

41
00:02:51,815 --> 00:02:54,655
Today's agenda, I will
start with the disclaimer.

42
00:02:54,925 --> 00:02:57,655
We are not going to get to the
low level design or low level

43
00:02:57,655 --> 00:02:59,875
architecture of hows get implemented.

44
00:03:00,430 --> 00:03:01,900
It's more of a framework.

45
00:03:02,350 --> 00:03:03,790
This goes like a checklist.

46
00:03:03,940 --> 00:03:08,740
Whenever you want to move your demo
to de demo, to production of any

47
00:03:08,740 --> 00:03:12,130
of your conversational agents, what
framework you should think about

48
00:03:12,130 --> 00:03:15,060
and what things you need to consider
before moving into production.

49
00:03:15,360 --> 00:03:17,670
That's what we are going to cover
as part of our today's session.

50
00:03:18,170 --> 00:03:20,750
It's my majorly classified
into five areas.

51
00:03:21,290 --> 00:03:25,280
First one is, what does a production
grade reference architecture looks like?

52
00:03:26,030 --> 00:03:30,170
Se second, we are going to talk
about the fusion strategies and

53
00:03:30,170 --> 00:03:31,730
their operational trade-offs.

54
00:03:32,030 --> 00:03:35,960
That might not be a situation where
one, one thing will fit for every single

55
00:03:35,960 --> 00:03:37,400
use case you're going to implement.

56
00:03:37,910 --> 00:03:40,910
So we are going to discuss about
the various fusion strategies

57
00:03:40,910 --> 00:03:44,180
available and their operational
trade-offs in the system.

58
00:03:45,110 --> 00:03:49,190
Third one, how evaluation changes
when text is no longer e enough.

59
00:03:50,120 --> 00:03:50,855
The fourth one is.

60
00:03:51,230 --> 00:03:54,260
Observability and incident readiness.

61
00:03:54,530 --> 00:03:58,340
How are you keeping your systems
being ready for incidents?

62
00:03:58,340 --> 00:04:00,560
It happens in the production systems.

63
00:04:01,040 --> 00:04:05,720
And finally, what are the real world
deployment patterns across industries?

64
00:04:06,560 --> 00:04:09,830
Okay, let's start with
our architecture today.

65
00:04:10,330 --> 00:04:10,750
Awesome.

66
00:04:10,840 --> 00:04:12,105
Let's dive into this architecture.

67
00:04:12,605 --> 00:04:16,055
A production multimodal, A
multimodal pipeline typically

68
00:04:16,055 --> 00:04:17,465
breaks into four stages.

69
00:04:17,495 --> 00:04:19,175
We are seeing over here in this slide.

70
00:04:19,865 --> 00:04:22,955
The first one is ingestion,
second one is fusion.

71
00:04:23,615 --> 00:04:25,265
Then reasoning and action.

72
00:04:25,805 --> 00:04:30,055
So these are the four different layers
are the four different stages of

73
00:04:30,105 --> 00:04:32,025
architecture of any multimodal pipeline.

74
00:04:32,525 --> 00:04:36,185
This, there is a reason for the
separation because it's very intentional.

75
00:04:36,995 --> 00:04:39,275
Each stage needs to scale independently.

76
00:04:39,695 --> 00:04:42,815
Fail independently and be
observable independently.

77
00:04:43,715 --> 00:04:48,365
When team blurs these boundaries,
Deva game becomes nearly impossible

78
00:04:48,395 --> 00:04:51,785
once you hit the real production
systems or production traffic.

79
00:04:52,265 --> 00:04:55,055
So let's talk a little more
about ingestion, right?

80
00:04:55,625 --> 00:04:58,475
Ingestion is where
things quietly go wrong.

81
00:04:58,835 --> 00:05:03,575
What it means is you have an
audio, video, touch, or sensor.

82
00:05:03,845 --> 00:05:07,040
That's where actually it gets injected
into your system, into our system.

83
00:05:07,540 --> 00:05:11,350
Suppose all the sensor data, all
this data arrive at different

84
00:05:11,350 --> 00:05:13,030
frequencies and latencies.

85
00:05:13,870 --> 00:05:17,590
If timestamps aren't aligned
properly here, the downstream

86
00:05:17,590 --> 00:05:19,030
reasoning will be incorrect.

87
00:05:19,630 --> 00:05:23,890
Even if your models are perfect
in production, engine ingestion

88
00:05:23,890 --> 00:05:27,670
needs strict synchronization
guarantees back pressure handling

89
00:05:28,125 --> 00:05:30,100
and observability hooks for day one.

90
00:05:30,600 --> 00:05:34,110
The reason is, for example, you
give a command and it takes long

91
00:05:34,170 --> 00:05:35,310
and you give another command.

92
00:05:35,370 --> 00:05:39,510
You get multiple results in a
different timeframe, that it entirely

93
00:05:39,510 --> 00:05:43,350
messes up your experience altogether
with your interaction, right?

94
00:05:43,620 --> 00:05:46,890
That's why ingestion is very
key, because every individual

95
00:05:46,890 --> 00:05:49,875
interaction will be received through
different frequencies and latencies.

96
00:05:50,375 --> 00:05:52,355
So that's the very beginning
of your architecture.

97
00:05:52,415 --> 00:05:55,415
What type of, how are you going
to make sure your tion is tight?

98
00:05:56,285 --> 00:05:58,145
The next thing we are going
to talk about is fusion.

99
00:05:58,925 --> 00:06:02,555
Fusion is where multimodal systems
either shine or fall apart.

100
00:06:03,055 --> 00:06:06,685
This is where signals are combined
and meaning starts to emerge.

101
00:06:07,285 --> 00:06:10,555
But fusion decisions have
direct operational consequences.

102
00:06:10,705 --> 00:06:15,685
For example, latency cost
resilience and failure isolation.

103
00:06:16,405 --> 00:06:18,715
This bring us to FU fusion strategies.

104
00:06:19,045 --> 00:06:22,825
Let's talk about how a fusion strategies
work and what are the different

105
00:06:22,825 --> 00:06:24,475
type of fusion strategies as there.

106
00:06:25,285 --> 00:06:28,405
So there are three major things we are
going to discuss on today's presentation.

107
00:06:28,615 --> 00:06:31,915
One is early fusion, late
fusion, and hybrid fusion.

108
00:06:32,095 --> 00:06:32,785
Let's talk through it.

109
00:06:32,785 --> 00:06:38,815
What does early fusion means by So early
fusion combines signals immediately.

110
00:06:39,355 --> 00:06:42,385
What it means is it fasts it.

111
00:06:42,595 --> 00:06:46,165
It's fast when inputs are
perfectly synchronized, but

112
00:06:46,195 --> 00:06:47,515
it's expensive and brittle.

113
00:06:48,015 --> 00:06:52,515
One missing modality can disrupt the
entire pipeline to get processed.

114
00:06:53,015 --> 00:06:54,365
The second one is late fusion.

115
00:06:55,355 --> 00:07:00,005
Late fusion process modalities
independently and merges decisions later.

116
00:07:00,505 --> 00:07:05,455
This improves fault isolation and
flexibility, but adds latency and

117
00:07:05,455 --> 00:07:07,525
often loses cross model nuances.

118
00:07:08,025 --> 00:07:09,435
The third option is third.

119
00:07:09,465 --> 00:07:15,405
Third strategy is about combination of
both hybrid fusion NOS production system

120
00:07:15,405 --> 00:07:20,145
end up with hybrid fusion, selectively
combining signals based on context.

121
00:07:20,595 --> 00:07:28,515
It's more complex to build, but it balance
costs latency and resilience, which

122
00:07:28,515 --> 00:07:30,795
makes it more practical choice at scale.

123
00:07:31,295 --> 00:07:33,660
So it's not like we will be able to.

124
00:07:34,535 --> 00:07:39,365
One thing will fit for every single
need of our business need based on what

125
00:07:39,365 --> 00:07:42,905
you are trying to implement and based
on your system you're trying to do,

126
00:07:43,235 --> 00:07:47,805
you can choose one among this fusion
strategies which can fit for it, right?

127
00:07:48,735 --> 00:07:51,415
So when you when you are choosing
the operational tradeoffs, what are

128
00:07:51,415 --> 00:07:53,605
the things which is considered for
operational trade off, which we'll

129
00:07:53,605 --> 00:07:54,715
talk about it in the next slide.

130
00:07:55,215 --> 00:07:58,075
So when you're selecting the operational
when you're selecting the fusion

131
00:07:58,075 --> 00:07:59,600
strategies, what you need to trade off is.

132
00:08:00,100 --> 00:08:02,200
Teams should focus on
five important factors.

133
00:08:02,410 --> 00:08:04,810
The first one is latency budgets.

134
00:08:05,410 --> 00:08:06,310
What is your budget?

135
00:08:06,580 --> 00:08:09,880
What is your real time requirements
for voice and video streams?

136
00:08:10,150 --> 00:08:13,560
Is it required that it has to be
high performing and you cannot

137
00:08:13,560 --> 00:08:14,760
tolerate on any latencies?

138
00:08:14,775 --> 00:08:18,930
Or is it okay to have little bit of
latency that decides your latency budget?

139
00:08:19,430 --> 00:08:24,415
Where GPUs and eh compute live, where
you want to make sure you are your

140
00:08:24,415 --> 00:08:28,735
compute, a compute distribution between
your cloud and devices, what you want

141
00:08:28,735 --> 00:08:32,545
to have it on device, what you want to
have it on cloud that has to be decided.

142
00:08:33,205 --> 00:08:38,715
Streams, streaming versus batch workflows,
whether you want to stream on live

143
00:08:39,135 --> 00:08:40,935
or it can be processed on a batch.

144
00:08:40,935 --> 00:08:41,565
Workflows.

145
00:08:42,380 --> 00:08:44,930
Failure models and de degradation paths.

146
00:08:45,200 --> 00:08:47,450
How do you want your failure
models to be handled?

147
00:08:47,510 --> 00:08:52,280
It should be a graceful degradation when
mo, when something fails or how you want

148
00:08:52,280 --> 00:08:54,440
to handle your failure modules, right?

149
00:08:54,440 --> 00:08:57,140
Model modes and cost controls.

150
00:08:57,320 --> 00:08:59,960
How are you going to utilize
your data transfer costs,

151
00:08:59,960 --> 00:09:01,490
cross region costs and whatnot.

152
00:09:01,970 --> 00:09:05,780
So these are the five trade offs you
need to decide when you are going to

153
00:09:05,780 --> 00:09:07,970
design your strategies for fusion.

154
00:09:08,805 --> 00:09:10,065
It could be everything.

155
00:09:10,205 --> 00:09:10,595
Everything.

156
00:09:10,595 --> 00:09:13,535
If you put into the metrics what
you can trade off accordingly,

157
00:09:13,535 --> 00:09:17,095
you can choose your strategy for
your fusion fused methodology.

158
00:09:17,365 --> 00:09:17,635
Okay.

159
00:09:18,175 --> 00:09:22,255
In production, these constraints matters
more than theoretical model accuracy.

160
00:09:22,255 --> 00:09:22,735
Yes.

161
00:09:23,305 --> 00:09:23,485
Okay.

162
00:09:23,985 --> 00:09:27,315
Let's dive further on
evaluation beyond text.

163
00:09:27,815 --> 00:09:29,255
Now, let's talk about evaluation.

164
00:09:29,405 --> 00:09:32,880
What does evaluation
means mean by text only.

165
00:09:33,605 --> 00:09:37,145
Metrics completely fail
for multimodal systems.

166
00:09:37,295 --> 00:09:39,005
You can't just rely only on text.

167
00:09:39,605 --> 00:09:43,385
You can't measure success by scores
where the system is looking at the room.

168
00:09:43,565 --> 00:09:48,215
Hearing a command and touching
a screen evaluation must

169
00:09:48,215 --> 00:09:50,285
reflect real user outcomes.

170
00:09:50,855 --> 00:09:56,515
That's why you have to maintain some sort
of a way to make sure, that way to ensure

171
00:09:56,515 --> 00:09:59,635
that your your system is beyond text.

172
00:10:00,145 --> 00:10:01,675
That's where task success rate.

173
00:10:02,155 --> 00:10:05,875
So end-to-end completion of
user intents across modalities.

174
00:10:05,875 --> 00:10:09,385
You make sure that you have
end-to-end completion of user

175
00:10:09,385 --> 00:10:13,245
intents to give a complete,
meaningful input and output as well.

176
00:10:13,905 --> 00:10:20,380
Grounding a accuracy correctness of object
recognition and spa reasoning, right?

177
00:10:21,130 --> 00:10:24,190
So we focus on those four key
metrics, multimodal metrics.

178
00:10:24,190 --> 00:10:25,840
Is that based on these four key areas.

179
00:10:26,815 --> 00:10:28,855
What is that task success rate?

180
00:10:29,275 --> 00:10:31,825
Did the user actually
get what they wanted?

181
00:10:31,895 --> 00:10:32,105
End?

182
00:10:32,250 --> 00:10:34,170
End completion of use
sentence across modalities.

183
00:10:34,170 --> 00:10:36,900
That's what it mean by, did the
user actually get what they wanted?

184
00:10:37,500 --> 00:10:41,190
Grounding accuracy, which means that
did the system correctly identifies

185
00:10:41,190 --> 00:10:43,020
objects and partial context.

186
00:10:43,995 --> 00:10:45,315
Does it got the right context?

187
00:10:45,345 --> 00:10:47,985
Is it really getting identifiable
to identify the objects?

188
00:10:48,255 --> 00:10:53,835
Next is modality agreement to voice,
vision and touch Align does all of this

189
00:10:53,835 --> 00:10:56,445
line up in the way it's expected to be?

190
00:10:57,045 --> 00:11:00,575
The last is the temporal
alignment or signal synchronized

191
00:11:00,580 --> 00:11:01,595
correctly over the time.

192
00:11:02,195 --> 00:11:03,395
That's what mean by temporal element.

193
00:11:03,845 --> 00:11:07,945
These are the key metrics which
will help us to evaluate evaluate.

194
00:11:08,270 --> 00:11:12,700
Evaluate whether the system which we are
designing are hardened enough and it's

195
00:11:12,700 --> 00:11:14,620
going to manage the production load.

196
00:11:15,310 --> 00:11:17,320
Okay, let's talk about observability.

197
00:11:17,820 --> 00:11:21,680
Without observability, multimodal
systems become unmanageable.

198
00:11:22,580 --> 00:11:24,740
Failures are rarely obvious.

199
00:11:25,505 --> 00:11:30,755
A system might work, but with de
degraded accuracy, pricing latency,

200
00:11:31,055 --> 00:11:33,575
or runaway GPU costs, it's very tough.

201
00:11:33,995 --> 00:11:37,185
Let's talk about individual
tracing individual way.

202
00:11:37,185 --> 00:11:40,215
How we can absorb the production
system is actually working in

203
00:11:40,215 --> 00:11:41,355
the way it is expected to work.

204
00:11:42,135 --> 00:11:43,995
The first one is cross model tracing.

205
00:11:44,745 --> 00:11:49,605
Cross model tracing lets us follow
a single user interaction across

206
00:11:49,605 --> 00:11:53,780
audio, video, and touch pipelines
using correlate correlation IDs.

207
00:11:54,280 --> 00:12:00,080
This is essential for diagnosing latency
spikes, dropped signals, and subtile

208
00:12:00,080 --> 00:12:01,970
failures that only occurred at the load.

209
00:12:02,450 --> 00:12:05,630
So it's very tough to replicate
in a development instance or in

210
00:12:05,630 --> 00:12:09,620
a non-production instance because
it, the load amount of load we are

211
00:12:09,620 --> 00:12:14,270
trying to test is mere impossible to
create it in that environment, right?

212
00:12:14,600 --> 00:12:18,080
That's why this is all this are
going to help us track it towards it.

213
00:12:18,710 --> 00:12:20,990
The next one is computer
compute attribution.

214
00:12:21,875 --> 00:12:22,505
Oh, sorry.

215
00:12:22,535 --> 00:12:22,805
Drift.

216
00:12:22,965 --> 00:12:24,105
Yeah, drift reduction.

217
00:12:24,615 --> 00:12:27,465
Second is compute attribution,
sorry, per modality.

218
00:12:27,915 --> 00:12:28,925
Per per modality.

219
00:12:28,925 --> 00:12:32,825
Compute attribution tells us where
GPUs are actually being used.

220
00:12:33,545 --> 00:12:38,345
Many teams discover that one modality
quietly consumes the majority of

221
00:12:38,345 --> 00:12:40,655
resources without this visibility.

222
00:12:40,680 --> 00:12:40,970
Cost.

223
00:12:41,360 --> 00:12:42,890
Cost optimization is guest work.

224
00:12:42,890 --> 00:12:43,170
Guest work.

225
00:12:43,670 --> 00:12:47,840
The amount of infrastructure costs
required for any agents is very high,

226
00:12:48,470 --> 00:12:52,670
so we have to be very frugal when we
are developing the systems at scale.

227
00:12:53,170 --> 00:12:55,180
Last one is drift detection.

228
00:12:55,870 --> 00:12:57,220
Drift happens everywhere.

229
00:12:58,060 --> 00:13:02,080
Input distributions, user
behavior and model performance.

230
00:13:02,620 --> 00:13:04,535
Task statistical monitoring helps.

231
00:13:05,035 --> 00:13:08,845
Catches the shifts early,
but users, before users, user

232
00:13:08,845 --> 00:13:10,405
experiences, any failures.

233
00:13:10,585 --> 00:13:12,175
That's the use of drift reduction.

234
00:13:12,595 --> 00:13:13,315
Let's get into safeguards.

235
00:13:13,815 --> 00:13:15,645
Production systems must assume failure.

236
00:13:15,705 --> 00:13:18,435
Like it's not possible that
it cannot go through failure.

237
00:13:18,435 --> 00:13:20,745
We can have a hundred percent
Accu a hundred percent run time

238
00:13:20,745 --> 00:13:21,915
on any of the production systems.

239
00:13:22,275 --> 00:13:26,955
So we must assume failure,
multi-model pipeline should degrade

240
00:13:27,015 --> 00:13:30,105
gracefully instead of collapsing
in case there is a failure happen.

241
00:13:30,585 --> 00:13:35,445
We should find ways to gracefully
gracefully degrade our pipelines

242
00:13:35,445 --> 00:13:38,025
instead of just collapsing
and then left with no choice.

243
00:13:38,295 --> 00:13:40,965
So what are the ways we can do this right?

244
00:13:41,265 --> 00:13:45,015
Fall back and degradation infusion fails.

245
00:13:45,515 --> 00:13:50,645
Systems should automatically fall back
to a single modality like voice only.

246
00:13:50,675 --> 00:13:52,655
So use still complete code tasks.

247
00:13:53,345 --> 00:13:57,605
Graceful degradation, keep
system us usable during partial

248
00:13:57,635 --> 00:14:00,305
outages or traffic spikes, right?

249
00:14:00,805 --> 00:14:02,065
Privacy boundaries.

250
00:14:02,275 --> 00:14:06,775
Audio and video data requires
strict privacy controls, encryptions

251
00:14:07,135 --> 00:14:10,945
access, boundaries, retention
limits, and user contact Consent.

252
00:14:11,725 --> 00:14:15,055
Privacy is not an option in the
production environment and requirement.

253
00:14:15,445 --> 00:14:19,405
The reason is in production you
will be capturing most of capturing

254
00:14:19,430 --> 00:14:21,860
the voice movements and whatnot.

255
00:14:22,310 --> 00:14:25,910
So privacy is very key in
ensuring that you are handling

256
00:14:25,910 --> 00:14:27,500
the customer data effectively.

257
00:14:28,325 --> 00:14:30,005
Let's get into the next slide.

258
00:14:30,505 --> 00:14:32,575
ML OPS and L-L-L-L-M ops.

259
00:14:33,145 --> 00:14:36,745
So these are some of the operational
patterns which can help us to

260
00:14:36,745 --> 00:14:40,345
see how we can able to operate
effectively in a production system.

261
00:14:40,845 --> 00:14:44,205
The multimodal systems multiply
operational complexity.

262
00:14:44,925 --> 00:14:46,640
Your version, your versioning.

263
00:14:47,230 --> 00:14:51,070
This can help us in you, you are
version your audio models, vision models

264
00:14:51,580 --> 00:14:53,500
and language models often together.

265
00:14:54,250 --> 00:14:54,910
So there are ways.

266
00:14:55,410 --> 00:14:58,440
You can maintain your models
effectively only among through

267
00:14:58,440 --> 00:15:00,120
the versioning of your models.

268
00:15:00,210 --> 00:15:02,250
It ensure that it's
always a version exists.

269
00:15:02,460 --> 00:15:06,020
If something doesn't work, you can
always fall back to the previous version.

270
00:15:06,560 --> 00:15:09,350
How do you test your ML models
through a b testing, right?

271
00:15:09,350 --> 00:15:13,760
That helps you to experiment your
data, collect enough data pointers

272
00:15:13,790 --> 00:15:15,050
before you roll out to production.

273
00:15:15,290 --> 00:15:19,220
So you do AB testing on your
ML models, model registries.

274
00:15:19,490 --> 00:15:24,120
This is, this is like a registry
with metadata for modality specific

275
00:15:24,120 --> 00:15:26,670
performance characteristics, how we
see your model actually performing.

276
00:15:26,670 --> 00:15:28,380
How is your characteristics of your model?

277
00:15:28,890 --> 00:15:32,240
So these are the rollout rollout
strategies must understand

278
00:15:32,690 --> 00:15:35,555
modality specific behavior before
you roll out to production.

279
00:15:36,055 --> 00:15:38,365
The next comes the data operations, right?

280
00:15:38,755 --> 00:15:40,345
How do you operate your data?

281
00:15:40,885 --> 00:15:44,155
The data operations, data pipelines
must support multimodal data

282
00:15:44,155 --> 00:15:48,325
sets with timestamp validation,
synthetic data generation for rare

283
00:15:48,325 --> 00:15:50,995
cases, and automatic PIE detection.

284
00:15:51,925 --> 00:15:54,685
Without this model
evaluation slows to a crawl.

285
00:15:55,075 --> 00:15:56,755
Let's move on to next slide.

286
00:15:57,255 --> 00:15:59,935
So let's talk about real
world applications, like

287
00:15:59,935 --> 00:16:01,615
how it works across systems.

288
00:16:02,245 --> 00:16:08,825
We see successful deployment across smart
homes with subset subsecond responses.

289
00:16:09,215 --> 00:16:12,725
It could be voice, gesture,
screen interactions for lighting,

290
00:16:12,725 --> 00:16:15,875
climate, and environment control
with subsecond responses, right?

291
00:16:16,865 --> 00:16:18,035
The second one is automotive.

292
00:16:18,185 --> 00:16:22,445
Automotive systems combining driver
mon monitoring and voice, right?

293
00:16:23,060 --> 00:16:28,850
The third one is wearables, using
voice and ha haptics and AR and vr.

294
00:16:28,940 --> 00:16:32,930
There are like AR and VR environments
built around spatial interactions.

295
00:16:33,500 --> 00:16:38,420
Each domain pushes different constraints,
but the core principles remains the same.

296
00:16:39,260 --> 00:16:39,530
Right.

297
00:16:40,030 --> 00:16:43,290
Let's talk about from
production from prototype, pro

298
00:16:43,290 --> 00:16:45,300
production, key success factors.

299
00:16:46,140 --> 00:16:48,780
Team that succeeded in
production do five things well.

300
00:16:49,280 --> 00:16:52,470
They define base metrics, early metrics.

301
00:16:52,470 --> 00:16:54,390
That's your key for your success.

302
00:16:54,390 --> 00:16:56,400
Success, what type of metrics you measure.

303
00:16:56,430 --> 00:17:00,270
How you measure it is very crucial in
any production system, especially on

304
00:17:00,270 --> 00:17:04,830
this multi, on this agent based model and
deployment and through pipelines, right?

305
00:17:05,190 --> 00:17:07,080
So they define baseline metrics early.

306
00:17:07,770 --> 00:17:10,740
They roll out progressively
with feature flags, right?

307
00:17:10,980 --> 00:17:11,760
They deploy.

308
00:17:12,070 --> 00:17:15,880
This is like differentiating between your
deployment and your release strategy.

309
00:17:15,910 --> 00:17:19,660
You deploy your software either to
the device or in your cloud or in

310
00:17:19,660 --> 00:17:23,410
your cloud system, but you're not
open up to all the customers, so you

311
00:17:23,410 --> 00:17:24,790
control it through feature flags.

312
00:17:25,060 --> 00:17:27,490
So they roll out progressively
through feature flags, and they

313
00:17:27,490 --> 00:17:29,320
have an option to turn it off too.

314
00:17:30,040 --> 00:17:33,160
They build feedback loops
from real usage, right?

315
00:17:33,640 --> 00:17:36,760
So capture those user corrections
and then identify anything you

316
00:17:36,760 --> 00:17:39,700
can able to capture from the real
usage and then course corrected.

317
00:17:40,415 --> 00:17:42,035
They actively optimize cost.

318
00:17:42,035 --> 00:17:44,745
Cost is very crucial in
this complete process.

319
00:17:45,045 --> 00:17:49,095
So they optimize cost and they maintain
operational ance with runbooks and

320
00:17:49,095 --> 00:17:53,955
on-call rotations so that anything go,
anything and everything can be supported

321
00:17:53,955 --> 00:17:57,465
at any time and point of any point
of time for the system in production.

322
00:17:57,495 --> 00:17:59,675
Okay, let's go to key takeaways.

323
00:18:00,175 --> 00:18:03,625
So one message, if there is one message
to take away from this presentation

324
00:18:04,075 --> 00:18:06,715
is the architecture is matters.

325
00:18:07,015 --> 00:18:11,725
Architecture matters most,
measure real tasks, success.

326
00:18:11,725 --> 00:18:12,955
Success, right?

327
00:18:12,985 --> 00:18:16,545
You observe everything and
plan for failure before it

328
00:18:16,545 --> 00:18:18,315
happens, not during an incident.

329
00:18:18,915 --> 00:18:22,305
So these are the four takeaways
I would leave you with on

330
00:18:22,305 --> 00:18:23,385
this particular presentation.

331
00:18:23,915 --> 00:18:24,875
I'll repeat one more time.

332
00:18:24,875 --> 00:18:26,285
It's architecture matters.

333
00:18:26,675 --> 00:18:31,055
Measure real tasks success,
observe everything, and plan

334
00:18:31,055 --> 00:18:32,645
for failure before it happens.

335
00:18:32,760 --> 00:18:33,050
Okay.

336
00:18:33,550 --> 00:18:36,610
And last but not least,
thank you for your time.

337
00:18:37,030 --> 00:18:41,350
I hope this gives you a practical
framework for building multimodal

338
00:18:41,470 --> 00:18:44,920
conversational agents that your
teams can deploy with confidence.

339
00:18:45,430 --> 00:18:49,120
And if you have any questions, feel
free to shoot it over my LinkedIn.

340
00:18:49,450 --> 00:18:51,430
I'm happy to answer it over there as well.

341
00:18:51,810 --> 00:18:54,450
Thank you so much for your
time and see you soon.

342
00:18:54,495 --> 00:18:54,715
Bye.

