1
00:00:00,930 --> 00:00:02,009
Good afternoon everyone.

2
00:00:02,310 --> 00:00:06,270
My name is She Gupta, and I'm thrilled
to be here today to talk about a

3
00:00:06,270 --> 00:00:09,870
truly transformative trend in the
world of artificial intelligence.

4
00:00:10,740 --> 00:00:15,630
Serverless AI rise revolutionizing
ML deployment with scalability

5
00:00:15,810 --> 00:00:16,950
and cost efficiency.

6
00:00:17,580 --> 00:00:22,680
Over the next 25 to 30 minutes, we'll
explore how serverless computing is

7
00:00:22,680 --> 00:00:24,540
not just an incremental improvement.

8
00:00:24,915 --> 00:00:30,195
But a paradigm shift in how we
manage and deploy AI solutions.

9
00:00:30,615 --> 00:00:36,165
We'll see how it is enabling organizations
to build and scale AI's application

10
00:00:36,165 --> 00:00:41,745
faster, more efficiently, and often
more cost efficiently than ever before.

11
00:00:42,495 --> 00:00:47,055
As you can see, serverless computing
has already made a significant

12
00:00:47,055 --> 00:00:50,025
impact offering automatic scaling.

13
00:00:50,280 --> 00:00:55,050
And paper use models without the headache
of managing underlying infrastructure.

14
00:00:55,440 --> 00:01:01,769
The market itself valued at USD
17.2 billion this year is projected

15
00:01:01,950 --> 00:01:07,950
to grow significantly hitting
A-C-H-E-R of 14.1% through 2030.

16
00:01:08,250 --> 00:01:08,280
Okay.

17
00:01:08,940 --> 00:01:14,940
This presentation will delve into how
serverless AI is streamlining deployment,

18
00:01:15,390 --> 00:01:20,200
boosting resource utilization, and
ultimately accelerating innovation.

19
00:01:20,740 --> 00:01:21,370
Let's begin.

20
00:01:21,870 --> 00:01:27,180
So what exactly is serverless
AI architecture at its score?

21
00:01:27,240 --> 00:01:29,370
It operates on an event driven model.

22
00:01:29,925 --> 00:01:35,295
This means computing resources are
dynamically allocated only when needed

23
00:01:35,565 --> 00:01:37,425
in response to specific triggers.

24
00:01:37,875 --> 00:01:39,885
Think of it like a light switch.

25
00:01:40,095 --> 00:01:42,975
The power is only on
when you flip the switch.

26
00:01:43,365 --> 00:01:47,565
The key components as illustrated
here include even sources,

27
00:01:47,775 --> 00:01:51,525
function, containers, supporting
services, and finally, resource

28
00:01:51,525 --> 00:01:53,835
management for even sources.

29
00:01:54,045 --> 00:01:57,765
These are the triggers I
mentioned, things like API request.

30
00:01:58,155 --> 00:02:02,925
Changes in data or schedule task,
they initiate the process function.

31
00:02:02,925 --> 00:02:03,615
Containers.

32
00:02:04,005 --> 00:02:08,955
These are stateless environments where
your AI model code actually executes.

33
00:02:08,985 --> 00:02:10,755
Stateless is crucial here.

34
00:02:11,325 --> 00:02:15,045
Each function in vocation is
independent, which is what allows

35
00:02:15,045 --> 00:02:18,075
for massive parallelization
and seamless automatic scaling.

36
00:02:18,575 --> 00:02:20,075
Third, supporting services.

37
00:02:20,464 --> 00:02:22,714
These are essential backend components.

38
00:02:22,834 --> 00:02:27,095
Authentication data, storage
monitoring, all managed for you.

39
00:02:27,605 --> 00:02:29,795
And finally, resource management.

40
00:02:30,185 --> 00:02:33,365
The platform handles the
dynamic allocation of resources

41
00:02:33,695 --> 00:02:35,255
based on the workload demands.

42
00:02:35,915 --> 00:02:40,325
This architecture significantly reduces
the burden of infrastructure management.

43
00:02:40,745 --> 00:02:45,065
In fact, organizations leveraging
this approach have reported up

44
00:02:45,065 --> 00:02:49,865
to a 68% in the reduction in
infrastructure management overhead.

45
00:02:50,345 --> 00:02:51,454
That's a huge win.

46
00:02:51,875 --> 00:02:56,704
Freeing up teams to focus on building
great ai, not managing servers.

47
00:02:57,204 --> 00:02:59,125
Now we understand the architecture.

48
00:02:59,545 --> 00:03:03,834
Let's look at how serverless AI
solutions are typically implemented.

49
00:03:04,345 --> 00:03:09,054
It generally follows a cyclical pattern
as you can see in the diagram model.

50
00:03:09,054 --> 00:03:13,524
Encapsulation first developer
package, their machine learning models

51
00:03:13,734 --> 00:03:15,385
within their stateless functions.

52
00:03:15,565 --> 00:03:19,910
These functions are often triggered
via STTP request for inference points.

53
00:03:20,019 --> 00:03:21,609
Or other events.

54
00:03:21,790 --> 00:03:27,159
The beauty here is the minimal IV
infrastructure code, second deployment.

55
00:03:27,519 --> 00:03:31,600
Next, this packaged function, which
was created in model encapsulation,

56
00:03:31,869 --> 00:03:36,129
is pushed to a serverless platform
along with its trigger configuration.

57
00:03:36,670 --> 00:03:38,079
Third execution.

58
00:03:38,410 --> 00:03:43,869
When an event occurs, say an API call with
new data, the function runs performing

59
00:03:43,869 --> 00:03:46,149
the AI task, like making a prediction.

60
00:03:46,959 --> 00:03:51,219
Scaling and critically, the
platform automatically allocates

61
00:03:51,219 --> 00:03:52,839
resource based on the demand.

62
00:03:53,439 --> 00:03:56,409
If you see a sudden spike
in request, it scales up.

63
00:03:56,649 --> 00:03:58,629
When demands drop, it scales down.

64
00:03:59,049 --> 00:04:01,269
You only pay for what you use.

65
00:04:01,929 --> 00:04:06,489
Modern serverless platforms are quite
versatile, supporting diverse simul

66
00:04:06,489 --> 00:04:08,410
frameworks and model architectures.

67
00:04:09,189 --> 00:04:14,230
We are seeing successful deployments of
complex deep learning models ranging from

68
00:04:14,350 --> 00:04:21,250
75 MB up to 1.8 gb, all while maintaining
impressive performance, like an average

69
00:04:21,250 --> 00:04:24,429
inference time of just 1 1 2 milliseconds.

70
00:04:24,579 --> 00:04:25,539
Can you believe that?

71
00:04:26,039 --> 00:04:30,179
Okay, so of course not all serverless
platforms are created equal.

72
00:04:30,659 --> 00:04:34,770
Choosing the right one is critical
for AI deployments, especially when

73
00:04:34,770 --> 00:04:39,539
considering performance, cost, and
specific functionalities like GPU support.

74
00:04:40,229 --> 00:04:44,549
This slide presents a snapshot from
standardized benchmarking studies that

75
00:04:44,549 --> 00:04:46,409
used identical deep learning models.

76
00:04:46,739 --> 00:04:51,630
RESNET 50 birth base, and YOLO V
four across major cloud providers.

77
00:04:52,275 --> 00:04:56,805
Let's talk about GCP Google Cloud
Functions, which is a product of GCP

78
00:04:57,135 --> 00:05:02,085
standouts for an image classification
workloads demonstrating the lowest average

79
00:05:02,115 --> 00:05:07,365
inference latency at 1 1 2 milliseconds,
and also the fastest cold start time.

80
00:05:07,815 --> 00:05:11,685
Their GPU support is also
quite comprehensive for Amazon.

81
00:05:12,015 --> 00:05:16,755
AWS Lambda follows with a strong
1 36 millisecond inference latency

82
00:05:16,965 --> 00:05:18,525
and their Snap Start feature.

83
00:05:18,705 --> 00:05:22,364
Significantly improved Cold start
time for Java functions with

84
00:05:22,364 --> 00:05:24,525
good but limited GPU support.

85
00:05:24,705 --> 00:05:25,905
Currently it's on T four.

86
00:05:26,565 --> 00:05:30,554
Azure functions for Microsoft
offers the most consistent

87
00:05:30,554 --> 00:05:34,304
cloud Cold Start performance and
container based GPU supports.

88
00:05:34,815 --> 00:05:39,015
Last but not the least, A BM
Cloud functions while cap.

89
00:05:39,195 --> 00:05:43,455
A capable platform currently shows
slower inference and cold start

90
00:05:43,455 --> 00:05:47,114
times in these benchmarks and lacking
direct GP support for function.

91
00:05:47,724 --> 00:05:49,584
Let's see, what's the key takeaway here?

92
00:05:49,884 --> 00:05:52,465
It is the cold start, the initial delay.

93
00:05:52,465 --> 00:05:57,744
When a function is invoked after a period
of inactivity, it is called a cold start.

94
00:05:58,074 --> 00:06:01,284
This could be a significant
factor, especially for A

95
00:06:01,284 --> 00:06:03,114
GPU accelerated functions.

96
00:06:03,594 --> 00:06:08,724
Google Cloud functions, for
instance, showed 58.7% faster

97
00:06:08,724 --> 00:06:10,344
initialization time for these.

98
00:06:10,704 --> 00:06:14,994
So depending upon your specific AI
workloads, sensitivity to latency.

99
00:06:15,340 --> 00:06:17,560
And the choice of
platform matters greatly.

100
00:06:18,060 --> 00:06:21,915
Okay, so now let's talk about the
key advantages of serverless ai.

101
00:06:22,680 --> 00:06:27,060
The benefit of adopting serverless
AI are compelling and transformative.

102
00:06:27,570 --> 00:06:28,950
Let's look at some numbers here.

103
00:06:29,450 --> 00:06:33,200
It has 64.8% deployment time production.

104
00:06:33,679 --> 00:06:37,369
Imagine cutting your deployment
model cycles from an 85 hertz

105
00:06:37,369 --> 00:06:39,559
down to approximately 29 hours.

106
00:06:39,919 --> 00:06:44,330
This is a reality for many teams adopting
serverless, a massive improvement

107
00:06:44,330 --> 00:06:46,219
compared to a traditional infrastructure.

108
00:06:46,879 --> 00:06:50,689
79.3% resource utilization improvement.

109
00:06:51,289 --> 00:06:55,849
Serverless ensures you are using
computing resources much more efficiently.

110
00:06:56,405 --> 00:06:59,974
No more over provisioning services
servers that said ile, most of

111
00:06:59,974 --> 00:07:03,694
the time, 38.4% cost reduction.

112
00:07:04,234 --> 00:07:07,174
This directly translates to
average savings compared to

113
00:07:07,174 --> 00:07:08,465
the traditional deployments.

114
00:07:08,914 --> 00:07:12,124
As you are only paying for the
compute time, you actually consume.

115
00:07:13,039 --> 00:07:16,309
99.95% deployment availability.

116
00:07:16,999 --> 00:07:21,739
This high reliability is inherent in the
managed nature of the serverless platform.

117
00:07:22,280 --> 00:07:27,409
So to conclude, as mentioned earlier,
automatic scaling capabilities are a

118
00:07:27,409 --> 00:07:29,599
game changer for various AI workloads.

119
00:07:30,484 --> 00:07:35,824
Platform can scale from handling 40
requests per second to over 4,200 requests

120
00:07:35,824 --> 00:07:39,874
per second in just over three seconds,
all while maintaining the response

121
00:07:39,874 --> 00:07:42,814
time of under 23, 2 35 milliseconds.

122
00:07:43,294 --> 00:07:45,724
This agility is incredibly valuable.

123
00:07:46,224 --> 00:07:51,295
Okay, so let's make this entire numbers
concrete with a real world example

124
00:07:51,295 --> 00:07:53,005
from the financial services industry.

125
00:07:53,710 --> 00:07:58,120
A leading global investment banking was
struggling with the scalability for their

126
00:07:58,120 --> 00:08:00,159
traditional risk analysis infrastructure.

127
00:08:00,729 --> 00:08:05,080
They had 38 dedicated high performance
servers, but they were operating at an

128
00:08:05,080 --> 00:08:09,340
average utilization of only 31.7% A.

129
00:08:09,840 --> 00:08:13,140
So it's a classic case of over
provisioning and inefficiency.

130
00:08:13,860 --> 00:08:16,710
They decided to migrate to
a serverless AI solution.

131
00:08:17,220 --> 00:08:19,380
The migration took about 4.5 months.

132
00:08:19,755 --> 00:08:23,414
And it wasn't without challenges
adapting to complex workloads and

133
00:08:23,414 --> 00:08:27,615
ensuring regulatory compliance
were key hurdles, but the post

134
00:08:27,615 --> 00:08:29,954
implementation results were substantial.

135
00:08:30,554 --> 00:08:35,804
68.2% reduction in infrastructure
cost a direct result of

136
00:08:35,804 --> 00:08:37,454
optimized resource utilization.

137
00:08:38,085 --> 00:08:41,520
Three one 5% increase in
the peak processing capacity

138
00:08:41,949 --> 00:08:43,199
they could now handle over.

139
00:08:43,845 --> 00:08:46,545
12,000 concurrent model execution.

140
00:08:46,725 --> 00:08:52,605
During the peak reporting periods,
something unimaginable before 62.2%

141
00:08:52,605 --> 00:08:54,285
improvement in the processing speed.

142
00:08:54,855 --> 00:09:00,045
Average latency for a risk analysis
dropped from 8.2 seconds to 3.1

143
00:09:00,045 --> 00:09:04,665
seconds, and crucially for finance
enhanced regulatory compliance

144
00:09:04,755 --> 00:09:08,055
through comprehensive audit trails
and improved security protocols

145
00:09:08,240 --> 00:09:09,120
inherent in the new system.

146
00:09:09,960 --> 00:09:14,940
This case study clearly demonstrates the
tangible business benefits of serverless

147
00:09:15,000 --> 00:09:17,970
AI in a demanding regulated environment.

148
00:09:18,470 --> 00:09:21,710
Now, let's take a real life
case study for another critical

149
00:09:21,710 --> 00:09:23,960
center, which is healthcare.

150
00:09:24,650 --> 00:09:28,670
A healthcare technology provider
specializing in medical diagnostic

151
00:09:28,670 --> 00:09:32,600
tool faced similar challenge their
traditional server based architecture

152
00:09:32,630 --> 00:09:34,699
for a medical image processing required.

153
00:09:34,970 --> 00:09:39,440
Significant upfront capital
investment, specialized personal, and

154
00:09:39,440 --> 00:09:43,670
suffered from a long infrastructure
provisioning timelines, averaging

155
00:09:44,180 --> 00:09:46,070
86 days to expand service.

156
00:09:46,640 --> 00:09:51,890
They adopted a serverless AI solution
using container I achieve learning models.

157
00:09:52,340 --> 00:09:56,300
The migration spanned over seven
months, over three phases with a

158
00:09:56,300 --> 00:10:01,415
strong focus on security, strict access
controls, and comprehensive encryption.

159
00:10:01,955 --> 00:10:03,615
The outcomes were impressive.

160
00:10:04,400 --> 00:10:10,550
First operational improvements, a 73.5%
reduction in the operational overhead.

161
00:10:11,030 --> 00:10:15,590
Their auto-scaling capabilities now
support variable workloads from 50 to

162
00:10:15,590 --> 00:10:20,750
5,000 images and R without any manual
intervention deployment efficiency.

163
00:10:21,290 --> 00:10:24,710
Their deployment cycle for a
new model updates plummeted

164
00:10:24,920 --> 00:10:27,620
from 36 days to just 4.8 days.

165
00:10:28,220 --> 00:10:31,670
This allowed them to rapidly
interior research advances

166
00:10:31,730 --> 00:10:32,810
into their production system.

167
00:10:33,604 --> 00:10:40,775
Financial impact, they achieved a 41.3%
reduction in total expenses despite a two

168
00:10:40,775 --> 00:10:43,504
one 2% increase in the processing volume.

169
00:10:43,714 --> 00:10:44,314
Imagine that.

170
00:10:44,314 --> 00:10:48,844
Number furthermore, the granular
price pricing models of serverless

171
00:10:48,944 --> 00:10:52,995
server allowed them to better
serve smaller healthcare providers.

172
00:10:53,535 --> 00:10:57,765
Again, we are seeing serverless
AI delivering significant

173
00:10:57,765 --> 00:10:59,505
improvement in efficiency speed.

174
00:10:59,955 --> 00:11:02,895
Cost even while handling
increased demands.

175
00:11:03,395 --> 00:11:06,755
Okay, so now let's talk about
innovation and rapid development

176
00:11:06,905 --> 00:11:07,955
and how it helps there.

177
00:11:08,455 --> 00:11:13,075
Beyond operational efficiencies, a
serverless paradigm fundamentally changes

178
00:11:13,255 --> 00:11:18,175
how organizations approach innovation
and rapid prototyping in AI development

179
00:11:18,715 --> 00:11:23,365
organizations leveraging serverless
architecture have on an average, reduced

180
00:11:23,365 --> 00:11:26,935
their development cycle time by 57.8%.

181
00:11:27,355 --> 00:11:33,085
High performance teams are even
seeing up to a 73.5 reduction in time

182
00:11:33,085 --> 00:11:34,915
to market for the new AI features.

183
00:11:35,215 --> 00:11:36,535
That's incredibly fast.

184
00:11:37,035 --> 00:11:41,985
Rapid prototyping in particular, sees and
impressive games development teams can now

185
00:11:41,985 --> 00:11:48,285
deploy and test a new AI model variance
in just 3.4 hours compared to nearly 23

186
00:11:48,285 --> 00:11:49,635
hours in the traditional environment.

187
00:11:50,175 --> 00:11:54,995
This agility, flu fuels increased
experimentation, reduced complexity,

188
00:11:55,205 --> 00:11:58,160
improved collaboration, and
accelerated time to market.

189
00:11:58,955 --> 00:12:02,135
You can see the numbers here
for increased experimentation.

190
00:12:02,195 --> 00:12:07,925
Hydration speed can be increased by 2.9 x,
allowing for much faster validation of AI

191
00:12:07,925 --> 00:12:13,355
models and hypothesis teams have reported
a 68.4% reduction in the implementation

192
00:12:13,355 --> 00:12:19,505
complexity, which also leads to decrease
in your bug density by 52.3% and

193
00:12:19,505 --> 00:12:21,815
few post-deployment issues improved.

194
00:12:21,815 --> 00:12:25,565
Collaboration code reusability
improves significantly with teams

195
00:12:25,565 --> 00:12:28,115
reporting a 53.2% improvement.

196
00:12:28,615 --> 00:12:32,305
Average feature deployment time
dropped from 15 days to just

197
00:12:32,305 --> 00:12:37,195
under five days with a high first
deployment success rate of nearly 79%.

198
00:12:37,645 --> 00:12:41,845
So it is safe to say that
serverless empowers AI teams to

199
00:12:41,845 --> 00:12:43,645
innovate faster and more freely.

200
00:12:44,145 --> 00:12:48,045
Okay, so now let's talk about
technical challenges and limitations.

201
00:12:48,765 --> 00:12:52,665
It's important to be balanced
despite all the advanced advantages

202
00:12:52,665 --> 00:12:53,985
which we have discussed so far.

203
00:12:54,315 --> 00:12:59,565
Serverless AI isn't what isn't without
its technical challenges and limitations.

204
00:13:00,165 --> 00:13:01,245
Cold start latency.

205
00:13:01,785 --> 00:13:03,915
This remains one of
the significant issues.

206
00:13:04,185 --> 00:13:08,085
Complex deep learning models can
experience significant delays up to

207
00:13:08,085 --> 00:13:11,055
6.2 seconds during initial initiation.

208
00:13:11,685 --> 00:13:14,685
While platforms are improving,
it is a factor to consider.

209
00:13:15,330 --> 00:13:16,290
Memory limitation.

210
00:13:16,830 --> 00:13:20,370
There are constraints on the model
size and the complexity that can

211
00:13:20,370 --> 00:13:21,990
be loaded into a function's memory.

212
00:13:22,830 --> 00:13:28,500
Execution time limits functions typically
have a maximum execution duration limits

213
00:13:28,890 --> 00:13:32,220
ranging from two 50 to eight 50 seconds.

214
00:13:32,520 --> 00:13:36,570
This affects nearly 39% of
the complex AI processing task

215
00:13:36,840 --> 00:13:37,980
that might be longer running.

216
00:13:38,605 --> 00:13:42,555
Vendor lockin dependency on
provider specific features and

217
00:13:42,555 --> 00:13:46,215
services can make it harder to
migrate between the cloud members.

218
00:13:46,455 --> 00:13:49,485
Imagine you started with GCP
and then you wanted to shift to

219
00:13:49,485 --> 00:13:51,495
Microsoft in the between determine.

220
00:13:51,585 --> 00:13:55,425
Imagine the number of times the number
of ads you have to spend a resource

221
00:13:55,425 --> 00:13:57,825
constraint while scaling is automatic.

222
00:13:57,945 --> 00:14:02,325
The available compute options, your
CPU types, memory configurations for

223
00:14:02,355 --> 00:14:06,405
individual functions might be more limited
compared to dedicated virtual machines.

224
00:14:06,905 --> 00:14:09,815
Especially for a very
large or specialized model.

225
00:14:10,445 --> 00:14:16,355
In fact, over 82% of the enterprise
AI deployments encounter at least

226
00:14:16,355 --> 00:14:18,155
one resource related constraint.

227
00:14:18,755 --> 00:14:22,025
These are real considerations
that organization needs to plan

228
00:14:22,175 --> 00:14:24,095
for when adopting serverless ai.

229
00:14:24,595 --> 00:14:26,605
Okay, now let's talk about security.

230
00:14:26,965 --> 00:14:30,445
Security is of course, paramount
in any AI implementation.

231
00:14:31,015 --> 00:14:34,225
So first less introduces its
own unique considerations

232
00:14:34,225 --> 00:14:38,035
compared for to the traditional
architectures isolation mechanism.

233
00:14:38,395 --> 00:14:42,835
Functional isolation is a primary defense
against the cross tenant vulnerabilities.

234
00:14:43,165 --> 00:14:46,525
Container-based isolations
used by many platforms provides

235
00:14:46,885 --> 00:14:48,505
effective security boundaries.

236
00:14:48,505 --> 00:14:53,005
For a vast majority, nearly 95%
of the common attack vendors.

237
00:14:53,605 --> 00:14:57,985
Access control, effective identity
and access management is fundamental.

238
00:14:58,485 --> 00:15:02,265
Around 69% of organizations are
implementing fine-grain access

239
00:15:02,265 --> 00:15:06,105
control at the functional level,
ensuring functions only have

240
00:15:06,105 --> 00:15:07,755
permissions they absolutely need.

241
00:15:08,475 --> 00:15:13,185
The use of short-lived credentials has
been particularly effective, reducing

242
00:15:14,055 --> 00:15:20,805
credential misuse incidents by over 72%
data protection, end-to-end encryption

243
00:15:20,805 --> 00:15:23,265
strategy for data addressed in transit.

244
00:15:23,610 --> 00:15:27,000
And during processing are
crucial and well supported by

245
00:15:27,000 --> 00:15:29,550
serverless platforms compliance.

246
00:15:29,790 --> 00:15:33,930
Comprehensive logging and monitoring
capabilities are available, which

247
00:15:33,990 --> 00:15:37,890
are essential for meeting regulatory
requirements and for auditability.

248
00:15:38,610 --> 00:15:42,480
While the attack surface ships
with serverless robust security

249
00:15:42,480 --> 00:15:46,320
practices and platform features
can effectively mitigate risks.

250
00:15:46,820 --> 00:15:49,220
Okay, so now let's talk
about the future outlook.

251
00:15:49,790 --> 00:15:54,050
Looking ahead, the serverless AI
landscape is set to evolve rapidly.

252
00:15:54,590 --> 00:15:57,980
We are expecting significant
transformations in the serverless

253
00:15:57,980 --> 00:16:03,650
platform with increased specialization and
optimization, especially for AI workloads.

254
00:16:03,980 --> 00:16:06,290
One exciting concept is nano functions.

255
00:16:06,740 --> 00:16:10,310
This represents a ship towards
even smaller execution units.

256
00:16:10,805 --> 00:16:14,495
Capable of executing specific
components of a computational

257
00:16:14,495 --> 00:16:16,535
graph rather than the entire model.

258
00:16:17,165 --> 00:16:21,695
This could lead to more precision,
precise resource allocation, improved

259
00:16:21,695 --> 00:16:27,815
parallelization and early results
already showing a potential 37.8%

260
00:16:27,815 --> 00:16:30,035
reduction in the overall execution time.

261
00:16:30,875 --> 00:16:33,095
Here's a potential timeline
of emerging trends.

262
00:16:33,095 --> 00:16:36,084
Let's quickly go over 25 to 26.

263
00:16:36,459 --> 00:16:39,910
We likely see standardized
function interfaces and deployment

264
00:16:39,910 --> 00:16:44,320
specification emerge, which will
help reduce vendor lockin service.

265
00:16:44,530 --> 00:16:50,170
Lockin concerns 26 to 27 expect an
ultra light weight container format

266
00:16:50,199 --> 00:16:53,500
optimized for AI workloads 27 to 28.

267
00:16:54,130 --> 00:16:57,040
Predictive scaling algorithms
could achieve a very high

268
00:16:57,040 --> 00:16:58,899
frequency in workload forecasting.

269
00:16:59,649 --> 00:17:04,569
28 to 30 platforms will very
likely incorporate more inbuilt

270
00:17:04,569 --> 00:17:08,199
capabilities for responsible ai,
such as bias detection, fairness

271
00:17:08,199 --> 00:17:10,179
evaluation, and explainability tools.

272
00:17:10,629 --> 00:17:15,310
Therefore, the future of serverless
AI is bright and full of innovation.

273
00:17:15,810 --> 00:17:22,115
Okay, so to recap, serverless AI
offers a powerful new way to deploy

274
00:17:22,175 --> 00:17:24,155
and scale machine learning models.

275
00:17:24,455 --> 00:17:28,625
Bringing significant benefits in
terms of cost, efficiency, speed,

276
00:17:28,865 --> 00:17:31,205
scalability, and fostering innovations.

277
00:17:31,445 --> 00:17:36,635
While challenges exist, the ongoing
advancements and a clear value proposition

278
00:17:36,965 --> 00:17:42,065
make it an interesting, compelling choice
for organizations across industries.

279
00:17:42,695 --> 00:17:44,675
Thank you very much for
your time and attention.

280
00:17:44,975 --> 00:17:47,525
I'd be happy to answer any
questions you may have.

281
00:17:47,765 --> 00:17:48,185
Thank you.

