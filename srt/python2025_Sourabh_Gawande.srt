1
00:00:00,010 --> 00:00:00,630
Hello, everyone.

2
00:00:00,780 --> 00:00:01,530
My name is Saurabh.

3
00:00:01,620 --> 00:00:04,010
I am the co founder and CTO of Kusho AI.

4
00:00:04,010 --> 00:00:07,560
And today I'm going to talk about
practical tips for building AI

5
00:00:07,840 --> 00:00:10,859
applications or AI agents using LLMs.

6
00:00:11,419 --> 00:00:15,479
At Kusho AI, we have been, working
on building AI applications.

7
00:00:15,899 --> 00:00:18,379
And, agents for the last, 18 months.

8
00:00:18,749 --> 00:00:23,539
And, during our journey, we have
identified a bunch of unique

9
00:00:23,919 --> 00:00:26,829
problems that people generally face.

10
00:00:26,899 --> 00:00:30,729
And, we have also faced, those
same problems specifically while

11
00:00:30,749 --> 00:00:32,829
building, applications using LLMs.

12
00:00:33,329 --> 00:00:39,954
And, the, agenda for today's talk
is that, we want to, educate devs

13
00:00:39,994 --> 00:00:42,174
about these problems, so that.

14
00:00:42,874 --> 00:00:46,684
when they are building apps on top
of LLMs, they are aware of these

15
00:00:46,694 --> 00:00:50,474
problems and, also discuss what
are, the solutions that work for us

16
00:00:50,884 --> 00:00:54,154
and, the dev tools or tooling that
we use to solve these problems.

17
00:00:54,254 --> 00:00:58,814
and, by, by sharing this information,
we, we want to save time, when

18
00:00:58,815 --> 00:01:02,794
devs are, building applications
on top of LLMs for the first time.

19
00:01:03,294 --> 00:01:06,104
The first thing that you will need
to solve when you start building

20
00:01:06,154 --> 00:01:10,629
apps on top of, LLMs is, how
to handle LLM inconsistencies.

21
00:01:11,259 --> 00:01:14,959
you, if you have some experience
building, applications using

22
00:01:15,019 --> 00:01:18,169
normal APIs, you would have seen
that they don't, fail that often.

23
00:01:18,219 --> 00:01:22,369
So, by building, general applications,
you don't really worry about,

24
00:01:22,429 --> 00:01:25,189
inconsistencies or failures, that much.

25
00:01:25,359 --> 00:01:27,889
if an API fails, you
just let the API fail.

26
00:01:27,929 --> 00:01:31,549
And, the user, when they refresh
their page, you make another API call

27
00:01:31,549 --> 00:01:32,689
and it will most probably succeed.

28
00:01:33,069 --> 00:01:34,329
but in case of LLMs.

29
00:01:34,849 --> 00:01:37,809
this is the first thing that you'll
probably need to solve, when you're

30
00:01:37,809 --> 00:01:41,689
actually building an application
because, LLMs have a much higher

31
00:01:41,749 --> 00:01:43,969
error rate, than your normal APIs.

32
00:01:44,424 --> 00:01:48,594
And unless you solve this particular
thing, your application will have

33
00:01:48,674 --> 00:01:51,464
a terrible UX, or user experience.

34
00:01:52,324 --> 00:01:55,254
Because, in your application,
you'll generally use the LLM

35
00:01:55,254 --> 00:01:56,674
response, somewhere else.

36
00:01:57,064 --> 00:02:02,224
And every time the LLM gives you a wrong
output, your application will also crash.

37
00:02:02,434 --> 00:02:07,814
So, this is the first, problem
that, you should solve, while

38
00:02:07,854 --> 00:02:09,704
building LLM, applications.

39
00:02:10,274 --> 00:02:16,984
Now, before we get into, like why, how
to solve this particular problem, let's

40
00:02:16,984 --> 00:02:22,104
just talk about why this even occurs, in,
in these, in these specific applications.

41
00:02:22,604 --> 00:02:25,634
so like I mentioned earlier, if
you are, if you are working with,

42
00:02:25,744 --> 00:02:31,024
normal APIs, you generally don't
worry, much about the error rate.

43
00:02:31,379 --> 00:02:36,170
in like fairly stable, APIs, but even
the most stable LLMs give you a much

44
00:02:36,170 --> 00:02:38,400
higher error rate, than your normal APIs.

45
00:02:38,710 --> 00:02:44,290
And the reason for this is LLMs
are inherently non deterministic.

46
00:02:44,930 --> 00:02:45,820
so what do you mean by that?

47
00:02:46,380 --> 00:02:50,260
so if you, if you, if you look at an
LLM under the hood, they are essentially

48
00:02:50,260 --> 00:02:56,180
statistical machines, that produce
token after token based on, the input

49
00:02:56,180 --> 00:02:59,840
prompt and whatever tokens have been
generated previously, Statistical

50
00:02:59,860 --> 00:03:02,590
machines are basically probabilistic.

51
00:03:02,650 --> 00:03:06,390
And as soon as you bring probability
into software, you're going to

52
00:03:06,390 --> 00:03:07,660
get something non deterministic.

53
00:03:08,240 --> 00:03:10,880
Now, what do we mean by non deterministic?

54
00:03:11,520 --> 00:03:15,370
you basically, we'll get a
different output for the same input.

55
00:03:15,400 --> 00:03:18,720
Every time, You, ask LLM for a response?

56
00:03:19,020 --> 00:03:23,450
you could, I, and I'm, I'm pretty
sure like you are, you would have

57
00:03:23,450 --> 00:03:26,820
seen this problem while using,
all the different, chat bots that

58
00:03:26,820 --> 00:03:28,670
are available, like chat, GPT or.

59
00:03:29,270 --> 00:03:33,600
the deep seek or cloud, chat, you
would have noticed that, every time

60
00:03:33,600 --> 00:03:37,460
you give, give, given input, for
the same input, every time you hit

61
00:03:37,460 --> 00:03:38,840
retry, you'll get a different output.

62
00:03:39,120 --> 00:03:41,370
that's the same thing that
will happen with, the LLM

63
00:03:41,370 --> 00:03:42,910
responses in your applications.

64
00:03:43,530 --> 00:03:47,360
you most of the time don't, have
a lot of control or, will you

65
00:03:47,360 --> 00:03:48,510
get the exact output or not?

66
00:03:49,100 --> 00:03:55,010
Now, because of this particular problem,
which is, LLMs being non deterministic,

67
00:03:55,680 --> 00:04:00,350
every time you give it an input, you'll
not always get the response that you want.

68
00:04:00,360 --> 00:04:06,040
Like, for example, for example,
if you ask an LLM API to generate,

69
00:04:06,150 --> 00:04:07,970
JSON, which is a structured output.

70
00:04:08,470 --> 00:04:11,840
You might get more fields
than, what you asked for.

71
00:04:11,870 --> 00:04:13,290
Sometimes you might get less fields.

72
00:04:13,670 --> 00:04:16,460
sometimes you might
have, a bracket missing.

73
00:04:16,960 --> 00:04:21,720
Based on what we have seen, if, if
you have like a normal stable API.

74
00:04:22,375 --> 00:04:26,505
you will see an error
rate of something like 0.

75
00:04:26,555 --> 00:04:31,825
1%, but if you are working with an
LLM, even the most stable, the LLMs,

76
00:04:31,835 --> 00:04:35,375
which have been, here for the longest
amount of time, they'll give you an

77
00:04:35,375 --> 00:04:38,665
error rate of, something like one
to 5 percent based on what kind of

78
00:04:38,695 --> 00:04:40,005
task you're asking it to perform.

79
00:04:40,415 --> 00:04:44,795
And if you're working with chained
LLM responses, like, basically you.

80
00:04:45,245 --> 00:04:49,685
provide, the LLM API with a prompt,
you take that response, and then

81
00:04:49,685 --> 00:04:54,175
you provide it with another prompt,
using the response, that you got

82
00:04:54,175 --> 00:04:59,315
earlier, this is basically a chained,
LLM responses, you will see that

83
00:04:59,435 --> 00:05:00,305
your error rate gets compounded.

84
00:05:00,885 --> 00:05:06,636
And, this particular thing, will
probably not have a solution, in the

85
00:05:06,636 --> 00:05:11,509
LLMs because of LLMs, like I mentioned,
are inherently non deterministic,

86
00:05:11,539 --> 00:05:14,449
like that is, how the architecture is.

87
00:05:14,749 --> 00:05:19,399
So this is something that needs
to be solved, in your application.

88
00:05:19,759 --> 00:05:23,829
you can't really wait for, like
LLMs to get better and, start

89
00:05:23,829 --> 00:05:24,789
providing better responses.

90
00:05:24,819 --> 00:05:28,949
Like, I mean, they will definitely,
get better and, reduce the error rate.

91
00:05:28,969 --> 00:05:32,039
But, I think as an application
developer, it's your responsibility

92
00:05:32,089 --> 00:05:35,199
to take care of this issue
within your application as well.

93
00:05:35,879 --> 00:05:37,569
So what are your options?

94
00:05:38,414 --> 00:05:42,914
The first thing that you should definitely
try out is, retries and timeouts.

95
00:05:43,584 --> 00:05:45,904
Now, these are not new concepts.

96
00:05:45,954 --> 00:05:49,304
if you have worked in software development
for a while now, you would know what

97
00:05:49,304 --> 00:05:54,134
a retry is basically, when an API,
gives you a wrong response, you try

98
00:05:54,134 --> 00:05:59,384
it again with some, cool down period,
or, maybe not depending on how the

99
00:05:59,394 --> 00:06:04,554
rate limits are, retry is basically,
you make an API call, the API fails,

100
00:06:04,804 --> 00:06:08,779
you wait for a while, and then you
retry it again, as simple as that.

101
00:06:09,269 --> 00:06:15,649
Now, when you are developing, general
applications, I think retries and

102
00:06:15,649 --> 00:06:20,559
timeouts are something that, are not
the first thing that you would implement

103
00:06:21,129 --> 00:06:25,859
because, you just assume you just go
with the assumption that the API response

104
00:06:25,889 --> 00:06:27,709
rate is going to be fairly reasonable.

105
00:06:27,749 --> 00:06:28,269
So.

106
00:06:28,779 --> 00:06:35,359
they will most of the time work and,
like not adding APIs will, not really

107
00:06:35,369 --> 00:06:36,779
degrade the application performance.

108
00:06:37,479 --> 00:06:41,429
unless like you are working with
very critical, applications like,

109
00:06:41,479 --> 00:06:45,429
something in finance or health
where, the operation has to finish,

110
00:06:45,429 --> 00:06:48,469
in which case you will, definitely
start with the retries and timeouts.

111
00:06:48,469 --> 00:06:51,739
But, in our general experience, if
you're working with normal APIs, you

112
00:06:51,739 --> 00:06:53,039
don't really worry about these things.

113
00:06:53,639 --> 00:06:58,249
but because LLM APIs specifically
have a higher error rate, retries

114
00:06:58,249 --> 00:07:03,059
and timeouts are something that,
need to be implemented from day one.

115
00:07:03,659 --> 00:07:06,399
So, timeouts again, I think,
I don't need to get into this.

116
00:07:06,399 --> 00:07:07,529
A timeout is basically.

117
00:07:08,029 --> 00:07:13,929
You make an API call and you wait
for X seconds for the API to return.

118
00:07:14,179 --> 00:07:18,309
If it doesn't return in X seconds for
whatever reason, you dominate that

119
00:07:18,629 --> 00:07:20,009
particular API and you try again.

120
00:07:20,479 --> 00:07:27,199
this basically is protection against, the
server, the API server being down and,

121
00:07:27,299 --> 00:07:31,219
So like, if you don't do this, and if
the API takes like a minute to respond,

122
00:07:31,279 --> 00:07:35,509
you, your application is also stuck
for a minute and, so are your users.

123
00:07:35,589 --> 00:07:38,889
So a timeout is basically protection.

124
00:07:39,354 --> 00:07:43,894
So that if an API doesn't return in like a
reasonable amount of time, you cancel that

125
00:07:44,004 --> 00:07:46,604
API call and you retry that API again.

126
00:07:46,694 --> 00:07:48,024
that's where timeout comes into picture.

127
00:07:48,744 --> 00:07:49,424
So, cool.

128
00:07:49,474 --> 00:07:52,464
So how do you implement
this into your application?

129
00:07:52,554 --> 00:07:56,004
I would suggest don't, write the
board for retries and timeouts from

130
00:07:56,014 --> 00:07:59,984
scratch, because there are a bunch
of, battle tested libraries available

131
00:08:00,034 --> 00:08:02,914
in every language that you can use.

132
00:08:02,964 --> 00:08:07,134
And, with a few lines of code, add
these, behaviors to your application.

133
00:08:07,194 --> 00:08:09,624
So let's look at a few examples.

134
00:08:10,284 --> 00:08:13,624
the one that we actually use in
production is, this one called

135
00:08:13,624 --> 00:08:15,314
Tenacity by, it's a Python library.

136
00:08:16,184 --> 00:08:21,254
And, it allows you to add retry to
your functions by simply doing this.

137
00:08:21,544 --> 00:08:25,614
you add a decorator, which is provided
by the, by this particular library,

138
00:08:25,664 --> 00:08:29,494
you add it to a function and this,
this function will be retried.

139
00:08:29,864 --> 00:08:33,804
whenever there is an exception or
error in this particular function,

140
00:08:34,364 --> 00:08:40,434
now, you'd ideally want more control
over, how many retries to do, how,

141
00:08:40,884 --> 00:08:44,264
like how long to wait after, every
retry and those kinds of things.

142
00:08:44,754 --> 00:08:48,214
So, those all options are
present in this library.

143
00:08:48,424 --> 00:08:52,844
You can, give it stopping conditions where
you want to stop after three retries.

144
00:08:52,844 --> 00:08:53,994
You want to stop, stop after like.

145
00:08:54,404 --> 00:08:59,564
retrying, you can add a wait
time before every retry.

146
00:08:59,854 --> 00:09:00,964
you can add a fixed wait time.

147
00:09:00,964 --> 00:09:05,274
You can add a random wait time, all
these, different kinds of, behaviors

148
00:09:05,324 --> 00:09:09,079
can We added using this library,
with like a few lines, of course.

149
00:09:09,089 --> 00:09:13,489
So, if you are working in Python,
this is our choice, has been working,

150
00:09:14,029 --> 00:09:15,629
very well for us in production.

151
00:09:15,969 --> 00:09:18,529
so, this is what we, have been
using for a very long time.

152
00:09:18,529 --> 00:09:22,969
So I would recommend this, if you're
working in JS, there is a similar library

153
00:09:23,009 --> 00:09:24,359
called retract, very conveniently.

154
00:09:25,039 --> 00:09:26,979
that you can, that is available on NPM.

155
00:09:27,279 --> 00:09:29,669
So, similar type of functionalities.

156
00:09:29,669 --> 00:09:32,079
It gives you, retries and timeouts.

157
00:09:32,349 --> 00:09:35,709
Oh, by the way, tenacity also
has, timeout related decorators.

158
00:09:36,149 --> 00:09:36,809
works the same way.

159
00:09:36,859 --> 00:09:40,014
if you want to add a timeout to
particular function, you just add

160
00:09:40,014 --> 00:09:41,649
that decorator, specify the timeout.

161
00:09:42,129 --> 00:09:43,059
yeah, back to.

162
00:09:43,599 --> 00:09:48,439
This library, if you are working with
the JS application, retry is, our choice.

163
00:09:49,049 --> 00:09:54,089
the third option is, basically, a
lot of people who are, developing

164
00:09:54,109 --> 00:09:57,979
LLM applications are using these
frameworks, LLM frameworks to handle, JS.

165
00:09:58,479 --> 00:10:01,649
API calls, retries, and like
a bunch of different things.

166
00:10:02,009 --> 00:10:05,569
So the most famous LLM framework,
framework, which a lot of

167
00:10:05,589 --> 00:10:07,109
people are using is Lanchain.

168
00:10:07,699 --> 00:10:10,239
And if you are working on top of Lanchain.

169
00:10:10,739 --> 00:10:16,479
Lanshan provides you, basically, some,
mechanism to retry, out of the box.

170
00:10:16,719 --> 00:10:19,589
So, it's called, the retry output parser.

171
00:10:20,504 --> 00:10:22,954
where, you can use this
to make the LLM calls.

172
00:10:22,954 --> 00:10:28,224
And whenever the LLM call fails, this
parser will basically, handle retry

173
00:10:28,224 --> 00:10:32,464
on your behalf, by passing, the prompt
again and, also the previous output,

174
00:10:32,534 --> 00:10:35,924
so that the, L-L-M-A-P has a better
idea that, okay, the last out failed.

175
00:10:35,984 --> 00:10:38,314
And, I'm not supposed to,
give this response again.

176
00:10:38,864 --> 00:10:41,584
So if you're on , then it's
already sorted out for you.

177
00:10:41,584 --> 00:10:42,904
You use the retry out pass.

178
00:10:43,404 --> 00:10:43,784
All right.

179
00:10:43,854 --> 00:10:47,514
So this sorts out how to
implement retries and timeouts.

180
00:10:47,894 --> 00:10:54,064
The next most common reason for a
failure or LLM inconsistency is when

181
00:10:54,064 --> 00:10:55,514
you're working with structured outputs.

182
00:10:56,094 --> 00:10:59,124
So when I say structured output,
I mean, something like you asked.

183
00:10:59,554 --> 00:11:04,974
The LLM to generate a JSON or XML CSV,
even list arrays, those kinds of things.

184
00:11:04,974 --> 00:11:08,994
So, whenever you're asking an LLM to
generate a structured output, there

185
00:11:08,994 --> 00:11:11,754
is a slight chance that, there'll be
something wrong with that structure.

186
00:11:12,184 --> 00:11:16,784
Maybe there are some fields missing, the
extra fields, in case of like JSONs, XMLs,

187
00:11:16,844 --> 00:11:18,794
there are brackets missing, might happen.

188
00:11:19,294 --> 00:11:21,524
So how do you handle that?

189
00:11:22,094 --> 00:11:28,024
The simplest way to do that is to, is
to integrate a schema library instead

190
00:11:28,024 --> 00:11:29,724
of like doing it on your own every time.

191
00:11:29,984 --> 00:11:33,074
So, a schema library could
be something like Pydantic.

192
00:11:33,414 --> 00:11:37,914
And, this is what we
use, in our production.

193
00:11:38,124 --> 00:11:40,474
So, Pydantic is basically, the most.

194
00:11:40,904 --> 00:11:43,384
commonly used data
validation library in Python.

195
00:11:43,904 --> 00:11:48,654
And what it does is it allows
you to, create classes.

196
00:11:49,369 --> 00:11:54,399
in which you describe the structure
of your response, and then you use

197
00:11:54,459 --> 00:11:58,839
this particular class to, check
whether the LRM response fits,

198
00:11:59,319 --> 00:12:00,839
this particular structure or not.

199
00:12:01,099 --> 00:12:06,499
it will check for, fields, extra fields
or less fields that will check for data

200
00:12:06,499 --> 00:12:08,589
types, and a bunch of other options.

201
00:12:09,129 --> 00:12:13,909
So on Python, just go for Pydantic,
it is a tried and tested library.

202
00:12:14,169 --> 00:12:17,899
and, it will make the data validation
part when you're working with

203
00:12:17,899 --> 00:12:19,119
structured outputs, hassle free.

204
00:12:19,619 --> 00:12:23,149
similarly, if you're working with NPM,
there's something called EOP, same

205
00:12:23,149 --> 00:12:25,399
stuff as Pydantic, data validation.

206
00:12:25,809 --> 00:12:28,979
you essentially, Define
the shape of your output.

207
00:12:29,389 --> 00:12:34,279
And, you basically use that shape,
which is essentially a class, JS class,

208
00:12:34,609 --> 00:12:39,649
or JS object to, check or enforce,
the structure of your L responses.

209
00:12:40,029 --> 00:12:44,309
and the idea is to use, these,
these, data validation libraries,

210
00:12:44,409 --> 00:12:46,199
along with, retries and timers.

211
00:12:46,199 --> 00:12:46,679
So.

212
00:12:47,049 --> 00:12:50,389
what you basically do is when you
make an LLM API call, and you get

213
00:12:50,389 --> 00:12:55,139
a response, you pass it through
Pydantic or YUP or, whatever data

214
00:12:55,139 --> 00:12:56,829
validation, library you are using.

215
00:12:57,319 --> 00:13:01,389
And if you get an error, you use the
retry, to like, like basically let the

216
00:13:01,389 --> 00:13:03,319
LLM generate that structured output again.

217
00:13:03,809 --> 00:13:09,279
most of the time, you will see that,
a couple of retries sorts it out.

218
00:13:09,289 --> 00:13:12,499
Like, it's not like every API
call will fail in the same way.

219
00:13:12,849 --> 00:13:17,224
So if let's say there are a few things
missing, you can in your structured output

220
00:13:17,224 --> 00:13:20,634
the first time when you do a retry, the
next time you'll get the correct output.

221
00:13:21,384 --> 00:13:27,184
but just, just as a, as a, general
advice, if you see that there are

222
00:13:27,604 --> 00:13:31,304
particular kind of issues happening
again and again, you should mention

223
00:13:31,304 --> 00:13:33,564
that, instruction in the prompt itself.

224
00:13:34,014 --> 00:13:38,824
Because what happens is that, when you
do retry, like, an API call, which.

225
00:13:39,314 --> 00:13:42,124
Was supposed to take five seconds
might end up taking 15 to 20

226
00:13:42,124 --> 00:13:46,074
seconds and, it will make your,
make your application feel laggy.

227
00:13:46,789 --> 00:13:50,219
because, at, at the end of that
API call, you're going to provide

228
00:13:50,229 --> 00:13:52,979
some output to your users and,
they're waiting for that output.

229
00:13:53,449 --> 00:13:57,849
so if, you see that there are particular
kind of, problems that are happening

230
00:13:57,869 --> 00:14:03,079
again and again, like for example, if,
if you are, generating JSON, using.

231
00:14:03,579 --> 00:14:05,319
JSON using an LLM API.

232
00:14:05,339 --> 00:14:08,099
And you'll see that, like the
LLM is always using single quotes

233
00:14:08,099 --> 00:14:10,559
instead of double quotes, which
will generally cause issues.

234
00:14:11,009 --> 00:14:14,089
you should specify that as an
important point in your prompt so

235
00:14:14,089 --> 00:14:17,559
that, you get the correct output
in the first attempt itself.

236
00:14:18,459 --> 00:14:21,979
this is just like, an additional
level of check, but the idea is

237
00:14:21,989 --> 00:14:25,859
that the first response should
itself give you the correct output.

238
00:14:25,859 --> 00:14:26,299
So.

239
00:14:26,764 --> 00:14:29,994
anything that is, that is known
should be mentioned in the prompt

240
00:14:30,014 --> 00:14:33,414
as a special instruction, so that
you don't keep retrying and you use

241
00:14:33,414 --> 00:14:34,484
this and not waiting for an output.

242
00:14:34,984 --> 00:14:40,504
one, one additional option worth, one
special mention here is, the structured

243
00:14:40,504 --> 00:14:42,684
output capabilities provided by OpenAI.

244
00:14:43,284 --> 00:14:49,269
So, if you're using, GPT models,
and OpenAI APIs, what you can do

245
00:14:49,309 --> 00:14:52,529
is there is a response format field
where you can specify a PyDynamic

246
00:14:52,529 --> 00:14:58,819
class and, the OpenAI APIs themselves
will, try to enforce the structure.

247
00:14:59,249 --> 00:15:02,749
but this one problem here, which
is if you want to switch out.

248
00:15:03,299 --> 00:15:07,989
The model and use something else
like Claude or Grok, then you have

249
00:15:07,989 --> 00:15:11,719
basically, lost the structured output
capabilities because those are not

250
00:15:12,369 --> 00:15:15,899
available right now in other LLM APIs.

251
00:15:15,899 --> 00:15:20,149
So, my suggestion is to just handle
the, schema enforcing and checking

252
00:15:20,149 --> 00:15:23,119
in your application itself so that
like it's easy for you to switch out

253
00:15:23,129 --> 00:15:25,259
the models and use different models.

254
00:15:25,759 --> 00:15:29,479
That's all for handling
LLM inconsistencies.

255
00:15:29,979 --> 00:15:32,279
Two main things, retries and timeouts.

256
00:15:32,509 --> 00:15:34,669
Use them from the start.

257
00:15:35,239 --> 00:15:39,309
If you are working with structured
outputs, use a data validation

258
00:15:39,319 --> 00:15:41,599
library to check the structure.

259
00:15:42,099 --> 00:15:43,099
You see the options here.

260
00:15:43,629 --> 00:15:44,379
Any of these are good.

261
00:15:44,879 --> 00:15:48,939
The next thing that you should, start
thinking about is how to implement

262
00:15:48,959 --> 00:15:51,319
streaming in your LLM application.

263
00:15:51,789 --> 00:15:56,849
generally when you develop APIs, you, you
implement, you implement normal request

264
00:15:56,849 --> 00:15:58,339
response, like, you get an API call.

265
00:15:58,744 --> 00:16:02,754
And, the server does some work
and then you, then you return

266
00:16:02,764 --> 00:16:04,134
the entire response in one go.

267
00:16:04,734 --> 00:16:10,654
in, in case of, LLMs, what happens is
sometimes it might take a long time

268
00:16:10,654 --> 00:16:12,194
for the LLM to generate a response.

269
00:16:12,694 --> 00:16:14,244
That's where streaming comes into picture.

270
00:16:14,254 --> 00:16:18,624
Streaming, your responses allow
you to start returning partial

271
00:16:18,624 --> 00:16:20,394
responses, to the client.

272
00:16:20,894 --> 00:16:23,444
Even when, the LLM is not done.

273
00:16:24,009 --> 00:16:25,109
done with the generation.

274
00:16:25,749 --> 00:16:32,199
So, let's look at why, streaming is so
important while building NLM applications.

275
00:16:32,729 --> 00:16:38,889
like I mentioned, NLMs might take long
time for, for generation, to complete.

276
00:16:39,209 --> 00:16:43,209
Now, when, when your user is,
using your application, Most

277
00:16:43,209 --> 00:16:44,249
users are very impatient.

278
00:16:44,319 --> 00:16:48,319
you, can't ask them to wait for, seconds.

279
00:16:48,649 --> 00:16:50,169
Like I'm not even talking about minutes.

280
00:16:50,489 --> 00:16:53,589
if you have like a 10 second
delay in showing the response,

281
00:16:53,619 --> 00:16:54,849
you might see a lot of drop off.

282
00:16:55,309 --> 00:16:58,079
so, and, and like most of the LLMs
that you would work with would

283
00:16:58,089 --> 00:17:03,024
take like 5 to 10 seconds for even,
you know, The simplest prompts.

284
00:17:03,484 --> 00:17:06,734
So how do you improve the UX?

285
00:17:07,474 --> 00:17:09,704
and make sure that your
users don't drop off.

286
00:17:10,184 --> 00:17:11,714
that's where streaming comes into picture.

287
00:17:12,204 --> 00:17:15,724
what streaming allows you
to do is, NLMs generate.

288
00:17:16,264 --> 00:17:19,674
The response is token by token, like
they'll generate it word by word.

289
00:17:20,124 --> 00:17:23,464
And what streaming allows you
to do is you don't need to wait

290
00:17:23,484 --> 00:17:26,704
for the LLM response or output.

291
00:17:27,154 --> 00:17:29,824
What you can do is as soon as it
is done generating a few words,

292
00:17:30,154 --> 00:17:33,234
you can send them to the client and
start displaying them on the UI.

293
00:17:33,839 --> 00:17:39,719
Or, whatever client you're using, in this
way, the, the user, doesn't really feel

294
00:17:39,759 --> 00:17:45,769
the lag, that, LLM generation results
in, what they see is that, as soon as

295
00:17:45,779 --> 00:17:48,919
they type out a prompt, immediately
they start seeing some response

296
00:17:49,369 --> 00:17:50,524
and they can start reading it out.

297
00:17:51,024 --> 00:17:56,174
This is a very common pattern
in any chat or LLM application

298
00:17:56,174 --> 00:17:57,144
that you would have used.

299
00:17:57,454 --> 00:18:00,044
As soon as you type something out
or you do an action, you start

300
00:18:00,134 --> 00:18:02,284
seeing partial results on your UI.

301
00:18:02,604 --> 00:18:03,974
That is implemented through streaming.

302
00:18:04,474 --> 00:18:07,534
the most common or the most, used way.

303
00:18:08,104 --> 00:18:10,664
The other way to, to implement
streaming is WebSockets.

304
00:18:11,284 --> 00:18:16,254
WebSockets allow you to send, generated
tokens or vaults in real time.

305
00:18:16,694 --> 00:18:20,274
the connection is established,
between client and server.

306
00:18:20,354 --> 00:18:25,214
And then, until the entire generation
is completed or, as long as, the user

307
00:18:25,244 --> 00:18:30,214
is, live on the UI, you can just like
reuse that connection to keep sending a

308
00:18:30,214 --> 00:18:33,494
response, as and when it gets generated.

309
00:18:34,034 --> 00:18:37,034
this is also a bidirectional,
communication method.

310
00:18:37,054 --> 00:18:42,124
So you can use the same method to
get some input from the client.

311
00:18:42,474 --> 00:18:46,674
Also, you know, one drawback
of, WebSockets is that they need

312
00:18:46,684 --> 00:18:48,424
some custom, Implementation.

313
00:18:48,714 --> 00:18:53,304
you can't just take like your
simple HTTP, REST server and,

314
00:18:53,354 --> 00:18:54,464
convert it into WebSockets.

315
00:18:54,474 --> 00:18:59,614
You'll need to redo your implementation,
use new libraries, probably

316
00:18:59,614 --> 00:19:01,034
even new, use a new language.

317
00:19:01,074 --> 00:19:06,434
Like, for example, if you are working on
Python, Python is not very, efficient,

318
00:19:07,064 --> 00:19:08,564
way for implementing WebSockets.

319
00:19:08,574 --> 00:19:14,034
You probably want to move to a different
language, which handles, threads or

320
00:19:14,034 --> 00:19:19,304
multiprocessing in a much better way than
Python, like Golang or Java or even C

321
00:19:20,074 --> 00:19:22,694
so generally WebSocket implementation.

322
00:19:23,359 --> 00:19:24,679
is a considerable effort.

323
00:19:25,129 --> 00:19:29,069
And, if all you want to do is
stream LLM responses, it probably

324
00:19:29,069 --> 00:19:30,419
is not the best way to do it.

325
00:19:30,999 --> 00:19:36,439
there is another, solution for
streaming, over HTTP, which

326
00:19:36,439 --> 00:19:37,799
is called server side events.

327
00:19:38,319 --> 00:19:41,139
which basically uses, your.

328
00:19:42,039 --> 00:19:46,089
your, server itself, like basically if you
are on Python and you're using flask or

329
00:19:46,109 --> 00:19:51,869
fast API, you won't need to do a lot of
changes to start streaming, using server

330
00:19:51,869 --> 00:19:54,919
sentiments, code wise or implementation
wise, this is a minimal effort.

331
00:19:55,439 --> 00:19:58,999
what this essentially does
is, it will use the same.

332
00:19:59,464 --> 00:20:05,714
HTTP, connection, which your STPA call
utilizes, but instead of, sending the

333
00:20:05,714 --> 00:20:11,154
entire response in one shot, you can
send the response in chunks and, on

334
00:20:11,154 --> 00:20:14,404
your client side, you can, receive
it in chunks and start displaying.

335
00:20:14,904 --> 00:20:17,724
Now, this is a unidirectional, flow.

336
00:20:18,444 --> 00:20:22,324
It works exactly as a REST API
call, but instead of, the client

337
00:20:22,334 --> 00:20:25,224
waiting for the entire response.

338
00:20:25,909 --> 00:20:30,469
to come, the client starts showing
chunks that have been sent from

339
00:20:30,469 --> 00:20:35,469
server, using server sent events,
implementation wise, it's very simple,

340
00:20:35,469 --> 00:20:40,079
like you, just need to maybe implement
a generator, if you're using Python

341
00:20:40,539 --> 00:20:42,639
and, maybe add a couple of headers.

342
00:20:43,299 --> 00:20:46,089
we won't get into specific details
because these are, things that you

343
00:20:46,089 --> 00:20:47,619
can easily Google and, find out.

344
00:20:47,999 --> 00:20:51,809
But, Our recommendation, if you
want to implement streaming in your

345
00:20:51,809 --> 00:20:55,659
application and you already have a
REST, setup ready on the backend, just

346
00:20:55,689 --> 00:21:00,359
go for server side events, much, faster
implementation, also much easier to

347
00:21:00,359 --> 00:21:05,019
implement, WebSockets is a bit heavy
and unless you have like a specific

348
00:21:05,029 --> 00:21:11,299
use case, For, WebSockets, I won't
recommend, going that, that, on that path.

349
00:21:11,799 --> 00:21:17,329
Streaming is a good solution if,
the particular task that an LLM

350
00:21:17,329 --> 00:21:19,739
is handling, gets over in a few
seconds, like 5 to 10 seconds.

351
00:21:20,279 --> 00:21:26,139
but if your task is going to take minutes,
streaming, Probably is not a good option.

352
00:21:26,629 --> 00:21:29,349
that's where background
jobs come into picture.

353
00:21:29,719 --> 00:21:33,899
So, if you have a task which can
be done in like five to 10 seconds,

354
00:21:34,089 --> 00:21:37,409
probably you streaming and, it's
a good way to start showing, an

355
00:21:37,449 --> 00:21:40,229
output, to the user on client side.

356
00:21:40,469 --> 00:21:43,019
but if you have a task, which
is going to take minutes.

357
00:21:43,519 --> 00:21:47,119
It is better to handle it
asynchronously instead of synchronously

358
00:21:47,119 --> 00:21:50,759
in your backend server and
background jobs help you do that.

359
00:21:51,259 --> 00:21:57,859
So what are these particular use
cases where you might want to use

360
00:21:57,869 --> 00:21:59,249
background jobs instead of streaming?

361
00:21:59,689 --> 00:22:00,469
think of it this way.

362
00:22:00,849 --> 00:22:06,899
Let's say if you, if you are
building something like an essay

363
00:22:06,899 --> 00:22:11,734
generator and you allow the user
to enter essay topics in bulk.

364
00:22:11,864 --> 00:22:15,824
So if someone, gives you a single
essay topic, probably, you'll finish

365
00:22:15,824 --> 00:22:18,734
the generation in a few seconds
and, streaming is the way to go.

366
00:22:19,204 --> 00:22:24,164
But let's say if someone, gives you a
hundred essay topics, for, for generation

367
00:22:24,624 --> 00:22:28,464
and that this particular task, doesn't
matter how fast the LLM is, is going to

368
00:22:28,464 --> 00:22:31,114
take minutes at, at least a few minutes.

369
00:22:31,524 --> 00:22:35,604
And, if you use streaming for
this, streaming will do all the

370
00:22:35,614 --> 00:22:37,164
work in your backend server.

371
00:22:37,714 --> 00:22:40,764
and, until this particular
task is completed, which is

372
00:22:40,764 --> 00:22:41,704
going to be a few minutes.

373
00:22:42,209 --> 00:22:48,959
your backend server resources are going
to get, hop or are going to be, tied up

374
00:22:49,019 --> 00:22:53,629
in this particular task, which is very
inefficient because, like your backend

375
00:22:53,629 --> 00:22:55,799
servers job is basically take a request.

376
00:22:56,649 --> 00:22:59,819
Process it in a few seconds
and send it back to the client.

377
00:23:00,639 --> 00:23:05,849
If you start doing things which take
minutes, you will see that if you have

378
00:23:05,849 --> 00:23:10,769
a lot of concurrent users, your backend
server will be busy and it will not

379
00:23:10,789 --> 00:23:16,179
be able to handle tasks which take a
few seconds and your APIs will start

380
00:23:16,199 --> 00:23:20,439
getting blocked and your application
performance will start to degrade.

381
00:23:20,979 --> 00:23:23,239
So what's the solution here?

382
00:23:23,499 --> 00:23:26,169
You, the solution is, you don't handle.

383
00:23:26,669 --> 00:23:30,329
Long running tasks in
backend server synchronously.

384
00:23:30,719 --> 00:23:33,559
You handle them in background
jobs asynchronously.

385
00:23:33,849 --> 00:23:36,869
Basically, when a user gives you
a task, which is going to take

386
00:23:36,869 --> 00:23:41,249
minutes, you log it in a database, a
background job will pick that task up.

387
00:23:41,689 --> 00:23:44,889
Till then, you tell the, you basically
communicate to the user that, okay,

388
00:23:44,889 --> 00:23:46,159
this is going to take a few minutes.

389
00:23:46,659 --> 00:23:47,159
once.

390
00:23:47,634 --> 00:23:49,184
The task is completed.

391
00:23:49,194 --> 00:23:53,114
You will get a notification,
probably as an email, or on slack.

392
00:23:53,554 --> 00:23:57,834
And, what do you do is you use a
background job to, pick up the task,

393
00:23:57,834 --> 00:24:01,999
process it, And once it's ready,
send out a notification, easiest

394
00:24:01,999 --> 00:24:04,299
way to implement this is cron jobs.

395
00:24:04,349 --> 00:24:08,049
cron jobs have been here for, I
don't know, for a very long time.

396
00:24:08,339 --> 00:24:13,529
very easy to implement, on any
Unix based, server, which is

397
00:24:13,529 --> 00:24:16,839
probably, what will be used in most
of, production backend servers.

398
00:24:17,299 --> 00:24:21,179
all you need to do is set up a cron
job, which does the processing.

399
00:24:21,689 --> 00:24:24,469
and the cron job runs every few
minutes, checks the database

400
00:24:24,469 --> 00:24:25,519
if there are any pending tasks.

401
00:24:26,149 --> 00:24:27,419
now when your user.

402
00:24:27,914 --> 00:24:34,804
comes to you with, with a task, you just
put it in a DB and, market as pending.

403
00:24:35,304 --> 00:24:38,594
when the cron job wakes up in a few
minutes, it will check for any pending

404
00:24:38,594 --> 00:24:39,984
tasks and start the processing.

405
00:24:40,814 --> 00:24:44,704
And, on the US side, you can probably,
implement some sort of polling.

406
00:24:45,214 --> 00:24:46,904
To check if the task is completed or not.

407
00:24:46,944 --> 00:24:49,704
And once it is completed, you
can display that on the UI.

408
00:24:50,154 --> 00:24:51,334
But, this is an optional thing.

409
00:24:51,334 --> 00:24:56,594
Ideally, if you're using background jobs,
you should also, sorry, you should also,

410
00:24:57,034 --> 00:25:01,044
separately communicate, that the task
is completed with the user, because,

411
00:25:01,294 --> 00:25:05,054
the general, idea is that, when you,
when, when a task is going to take a few

412
00:25:05,054 --> 00:25:09,804
minutes, your users will probably come
to your platform, submit that task and

413
00:25:09,804 --> 00:25:11,754
they will move away from your platform.

414
00:25:11,794 --> 00:25:15,014
So they're not looking at,
the UI of your application.

415
00:25:15,014 --> 00:25:19,154
So you should probably communicate
that the task is completed through

416
00:25:19,154 --> 00:25:21,054
an email or a Slack notification.

417
00:25:21,534 --> 00:25:25,304
so that the users who have moved away
from, the, you also know that, okay, that,

418
00:25:25,304 --> 00:25:27,944
that, that generation has been completed.

419
00:25:28,764 --> 00:25:31,894
this works very well, minimal
setup, nothing new that

420
00:25:31,894 --> 00:25:33,004
you'll probably need to learn.

421
00:25:33,014 --> 00:25:37,394
Nothing new that you need to install,
for the initial stages of your LLM

422
00:25:37,394 --> 00:25:39,134
application, just go for a cron job.

423
00:25:39,684 --> 00:25:46,624
what happens is that as your, application
grows, you'll probably need to scale this.

424
00:25:46,684 --> 00:25:48,404
Now, if you run multiple cron jobs.

425
00:25:48,824 --> 00:25:53,774
you need to handle which cron job, picks
up which task you need to implement some

426
00:25:53,784 --> 00:25:58,314
sort of, distributed locking and, all
those complexities come into picture.

427
00:25:58,414 --> 00:26:03,484
Basically, cron jobs are good
for the initial stages, but, like

428
00:26:03,544 --> 00:26:05,194
we also started with cron jobs.

429
00:26:05,244 --> 00:26:08,864
we still use cron jobs for some
simple tasks, but there will be

430
00:26:08,904 --> 00:26:12,954
a stage, when you'll need to move
away from cron jobs for scalability.

431
00:26:13,269 --> 00:26:17,789
and for, better retrying
mechanisms, that's where task

432
00:26:17,789 --> 00:26:18,759
queues come into picture.

433
00:26:19,269 --> 00:26:23,999
So basically think of task queues as cron
jobs with like more intelligence, where

434
00:26:24,639 --> 00:26:29,899
all the, task management that needs to be
done, is handled by the task queue itself.

435
00:26:30,369 --> 00:26:35,354
when I say task management, on a very
high level, what I It means is that,

436
00:26:35,454 --> 00:26:37,294
you submit a task to the task queue.

437
00:26:37,464 --> 00:26:42,544
generally a task queue is backed by some
storage like Redis or some other cache.

438
00:26:42,894 --> 00:26:47,544
the task is stored over there and
then the task queue handles, basically

439
00:26:47,544 --> 00:26:51,154
a task queue will have a bunch of
workers running and, the, the task

440
00:26:51,154 --> 00:26:56,804
queue will then handle, how to allocate
that work to which worker based on

441
00:26:56,814 --> 00:26:58,234
like a bunch of different mechanisms.

442
00:26:58,264 --> 00:26:59,074
Like you can have.

443
00:26:59,074 --> 00:26:59,214
Yeah.

444
00:26:59,514 --> 00:27:02,824
priority queues, you can have
a bunch of different retry

445
00:27:02,824 --> 00:27:04,754
mechanisms, and all those things.

446
00:27:05,124 --> 00:27:07,424
So, two good things
about using task queues.

447
00:27:07,934 --> 00:27:09,204
task queues are much easier to scale.

448
00:27:09,704 --> 00:27:13,086
in a cron job, if you go from one
to two to 10 cron jobs, you have to

449
00:27:13,086 --> 00:27:17,007
handle, A bunch of, locking related
stuff yourself, in task queues,

450
00:27:17,007 --> 00:27:19,027
it's already, implemented for you.

451
00:27:19,027 --> 00:27:22,227
So all you can do is increase the
number of workers in a task queue.

452
00:27:22,807 --> 00:27:28,677
And if you start getting more,
tasks or workload, the, you can

453
00:27:28,677 --> 00:27:31,545
just like, it's as easy as just
changing the number on a dashboard,

454
00:27:31,985 --> 00:27:33,155
to increase the number of workers.

455
00:27:33,775 --> 00:27:38,525
again, like all the, additional
handling of race conditions, retries,

456
00:27:38,525 --> 00:27:40,605
timeouts, it's already taken care of.

457
00:27:40,625 --> 00:27:43,175
All you need to do is,
provide some configuration.

458
00:27:43,605 --> 00:27:45,545
you also get better
monitoring with task queues.

459
00:27:45,825 --> 00:27:50,995
you, every task queue comes with
some sort of, monitoring mechanism

460
00:27:51,015 --> 00:27:55,315
or a dashboard where you can see
what are the tasks currently running,

461
00:27:55,325 --> 00:27:56,155
how much resources there are.

462
00:27:56,845 --> 00:28:00,715
Eating up, which tasks are
failing, start or restart tasks

463
00:28:00,765 --> 00:28:01,835
and all those kinds of things.

464
00:28:02,305 --> 00:28:05,765
So, once you start scaling your
application, go for task queues.

465
00:28:06,495 --> 00:28:11,265
The task queue that we use in
our production is called RQ,

466
00:28:11,995 --> 00:28:13,575
which stands for Redis Queue.

467
00:28:14,030 --> 00:28:19,650
And, as the name suggests, it's backed
by Redis and it's a very simple,

468
00:28:19,760 --> 00:28:23,590
library for queuing and processing
background jobs with workers.

469
00:28:24,160 --> 00:28:27,300
very easy setup, hardly takes
15 minutes to set it up.

470
00:28:27,320 --> 00:28:31,450
If you already have a Redis, you don't
even need to, set, set up a Redis.

471
00:28:32,355 --> 00:28:39,255
for RQ and, very simple, mechanism
for queuing and processing.

472
00:28:39,345 --> 00:28:42,945
All you need to do is create a queue,
provide a red connection so that,

473
00:28:43,005 --> 00:28:44,925
it has a place to store the tasks.

474
00:28:45,415 --> 00:28:49,605
when you get a task queue nq, you
can, and, this is basically a function

475
00:28:49,635 --> 00:28:53,125
which is going to get called in
the worker to process your tasks.

476
00:28:53,125 --> 00:28:56,830
So, it's this simple and you can also
provide some arguments for that function.

477
00:28:57,330 --> 00:28:58,200
And

478
00:28:58,700 --> 00:28:59,660
the worker.

479
00:29:00,320 --> 00:29:04,590
For the worker, you just need to start
it like this, on your command line.

480
00:29:04,770 --> 00:29:08,350
And, it consumes tasks from
Redis and, process them.

481
00:29:08,380 --> 00:29:12,990
If you, want to increase the number
of workers, you just like, start 10

482
00:29:12,990 --> 00:29:17,840
different workers, connect them to
the same Redis, and, RQ will itself

483
00:29:17,840 --> 00:29:20,690
handle all the, all the complexities.

484
00:29:21,080 --> 00:29:25,440
of, managing which worker gets what
task, and all those kinds of things.

485
00:29:25,590 --> 00:29:29,160
So, if you're on Python,
RQ is the way to go.

486
00:29:30,090 --> 00:29:35,430
Celery provides you with a similar,
functionality, but we just found

487
00:29:35,440 --> 00:29:39,100
that, there were a bunch of things in
celery, which we did not really need.

488
00:29:39,600 --> 00:29:40,900
and it seemed like an overkill.

489
00:29:41,405 --> 00:29:44,735
so we decided to go with RQ, which
was much simpler to set up on our end.

490
00:29:45,235 --> 00:29:48,105
Prompted at what inputs are not working,
what models are working, what models

491
00:29:48,105 --> 00:29:49,585
are not working and, things like that.

492
00:29:50,085 --> 00:29:56,505
So, if you, if you want an analogy,
you can think of evals as unit testing.

493
00:29:56,745 --> 00:29:59,545
So, think of it as unit
testing for your prompts.

494
00:29:59,725 --> 00:30:04,765
So this allows you to take a
prompt template and individually

495
00:30:04,795 --> 00:30:07,945
just test out that template with
a bunch of different, Values.

496
00:30:08,475 --> 00:30:14,375
and, you can, there are, there are a
bunch of, reasons why you should ideally,

497
00:30:14,475 --> 00:30:16,275
use evals with your prompt templates.

498
00:30:16,985 --> 00:30:20,235
one, it allows you to just test
out the prompts in isolation,

499
00:30:20,235 --> 00:30:21,205
which makes it very fast.

500
00:30:21,255 --> 00:30:24,495
the same way unit tests are fast, because
you are just checking one function

501
00:30:24,495 --> 00:30:25,745
against different types of inputs.

502
00:30:26,285 --> 00:30:28,345
using prompts, using, sorry, I'm sorry.

503
00:30:28,345 --> 00:30:33,905
using evals, You will be able to
figure out different things like which

504
00:30:33,905 --> 00:30:37,765
input works, which input doesn't work,
which model works for a particular

505
00:30:37,845 --> 00:30:39,575
task, which model does not work.

506
00:30:39,855 --> 00:30:43,455
you'll be able to compare, costs
of different models for different

507
00:30:43,465 --> 00:30:45,135
types of inputs and so on.

508
00:30:45,735 --> 00:30:49,545
an additional, benefit
of using evals is that.

509
00:30:50,045 --> 00:30:55,015
You can directly, integrate them
with your CI CD pipeline so that,

510
00:30:55,075 --> 00:30:57,785
you don't need to manually keep
checking before every release.

511
00:30:57,815 --> 00:31:01,865
If your prompts are still working the
way they're working just like unit test.

512
00:31:01,865 --> 00:31:03,945
You just, hook it up
to your CI CD pipeline.

513
00:31:04,395 --> 00:31:08,345
And, before every commit or, I'm
sorry, after every commit or, after

514
00:31:08,345 --> 00:31:13,955
every build, you, straight up run the
evals and, similar to, assertions in

515
00:31:13,955 --> 00:31:18,345
unit as evals also have assertions
or checks where you can check the

516
00:31:18,345 --> 00:31:23,375
response, and, specify whether it
is as expected or not as expected.

517
00:31:23,800 --> 00:31:25,120
And pass or fail an eval.

518
00:31:25,570 --> 00:31:29,870
So, that's how on a very
high level evals work.

519
00:31:30,510 --> 00:31:34,430
we have tried out a bunch of
different, eval libraries.

520
00:31:34,970 --> 00:31:37,220
the one we like the most is promptful.

521
00:31:37,720 --> 00:31:38,930
very easy to set up.

522
00:31:39,430 --> 00:31:46,000
simply works using YAML files,
basically you, you create a YAML

523
00:31:46,000 --> 00:31:50,150
file where you specify your prompt
template and you press specify a bunch

524
00:31:50,150 --> 00:31:52,410
of inputs, for that prompt template.

525
00:31:52,800 --> 00:31:58,320
And, using, so, Promfo is an open source,
tool, so you can just like, straight up

526
00:31:58,680 --> 00:32:01,270
install it from NPM, run it in your CLI.

527
00:32:01,850 --> 00:32:06,160
and at the end of the event,
you get a nice, graph like this.

528
00:32:06,705 --> 00:32:11,135
which will show you for different
types of inputs, whether the

529
00:32:11,175 --> 00:32:13,015
output has passed the condition.

530
00:32:13,415 --> 00:32:18,465
it will also allow you to compare
different, models and, there is

531
00:32:18,665 --> 00:32:20,445
some way to compare cost as well.

532
00:32:20,445 --> 00:32:23,095
I don't think they have displayed
it here, but yeah, cost comparison

533
00:32:23,095 --> 00:32:24,885
is also something that you
will get in the same dashboard.

534
00:32:25,755 --> 00:32:30,165
And, you can start off with the open
source version of promptful, but they

535
00:32:30,175 --> 00:32:31,945
also have a cloud hosted version.

536
00:32:31,945 --> 00:32:35,575
So if you want more reliability or
don't want to manage your own instance.

537
00:32:35,970 --> 00:32:37,470
that option is also available.

538
00:32:37,970 --> 00:32:41,840
before we end the stock, let's do a
quick walkthrough of all the different

539
00:32:41,840 --> 00:32:46,530
foundational models, or foundational model
APIs that are available for public use.

540
00:32:46,980 --> 00:32:51,450
the reason for doing this is basically,
this landscape is changing very fast.

541
00:32:51,860 --> 00:32:57,770
So the last time you had gone over all the
available models, so I'm pretty sure that.

542
00:32:58,475 --> 00:33:03,535
By now, the list of models and also
their comparisons have changed.

543
00:33:03,885 --> 00:33:07,805
Probably the models you
thought are not that great have

544
00:33:07,815 --> 00:33:09,495
become very good and so on.

545
00:33:09,705 --> 00:33:13,115
So let's do a quick run through
of all the available models.

546
00:33:13,115 --> 00:33:13,855
What are they good at?

547
00:33:13,855 --> 00:33:16,440
What are They're not good
at what kind of use cases?

548
00:33:16,490 --> 00:33:20,990
you, what case, what kind of use cases
work with a particular kind of model?

549
00:33:21,490 --> 00:33:23,470
let's start with the oldest player OpenAI.

550
00:33:23,865 --> 00:33:29,840
OpenAI has, three main families of
models, which is GPT 4 0 4 O Mini, and,

551
00:33:29,840 --> 00:33:32,090
Owen, which are available for public use.

552
00:33:32,400 --> 00:33:34,740
I think they've deprecated
their three and 3.5 models.

553
00:33:35,210 --> 00:33:37,910
so these are the models that
are available right now.

554
00:33:38,450 --> 00:33:42,210
If you don't know what to
use, just go with OpenAI.

555
00:33:42,860 --> 00:33:46,010
These are the most versatile models.

556
00:33:46,010 --> 00:33:54,200
They work very well with a wide variety
of tasks, within these models, between

557
00:33:54,210 --> 00:33:58,910
Foro and, Foro Mini, the difference
is mainly, the trade off between,

558
00:33:59,000 --> 00:34:01,470
cost and latency versus accuracy.

559
00:34:01,470 --> 00:34:05,530
So if you have a complex task
or something that requires a bit

560
00:34:05,730 --> 00:34:07,859
more of reasoning, go for Foro.

561
00:34:08,329 --> 00:34:11,209
if you are worried about cost,
or if you're worried about, how

562
00:34:11,209 --> 00:34:14,629
fast the response is going to
be, go for 4 O Mini, but, it will

563
00:34:14,689 --> 00:34:17,189
basically, give you lesser accuracy.

564
00:34:17,739 --> 00:34:19,469
O L is something that
I have not tried out.

565
00:34:19,529 --> 00:34:22,829
these are supposed to be,
open air flagship models.

566
00:34:23,249 --> 00:34:25,949
but from, What I've heard,
these are like fairly new.

567
00:34:25,999 --> 00:34:29,909
So, before you put it in
production, maybe, test them out

568
00:34:29,929 --> 00:34:33,719
thoroughly, Foro and Foro Mini
have been around for a while now.

569
00:34:33,719 --> 00:34:36,469
So I think, you should not see
a lot of problems, with them.

570
00:34:36,679 --> 00:34:41,859
Also, like reliability wise,
as, according to us, OpenAI APIs

571
00:34:41,889 --> 00:34:42,909
have been the most reliable.

572
00:34:43,429 --> 00:34:48,189
so you don't need to worry
about downtime or, having to.

573
00:34:48,864 --> 00:34:52,194
handle switching models because,
this provider is not working.

574
00:34:52,194 --> 00:34:56,214
The next provider is, and stopping.

575
00:34:56,814 --> 00:35:00,524
I think for a while, these
guys were working mostly on

576
00:35:00,524 --> 00:35:02,439
the, chat, the, the, the.

577
00:35:02,759 --> 00:35:07,329
APIs were not publicly available as
far as I know, but, I think in the

578
00:35:07,329 --> 00:35:10,239
last few months, I think that has
changed, the APIs are available.

579
00:35:10,239 --> 00:35:12,619
You can just directly, and
they're completely self serve.

580
00:35:12,619 --> 00:35:16,649
You can just directly go, on,
anthropics, to Anthropx console

581
00:35:16,649 --> 00:35:20,709
and create an API key, load up some
credit and get started with it.

582
00:35:21,209 --> 00:35:26,589
If you have any coding related use
case, cloud APIs are your best choice.

583
00:35:26,629 --> 00:35:31,449
I think, as far as coding is concerned,
coding as a particular task, cloud,

584
00:35:32,049 --> 00:35:36,119
works much better than, all the other
models, which is also why you would have

585
00:35:36,119 --> 00:35:40,829
seen that, everyone is using cloud with,
their, code editors as well, like cursor.

586
00:35:41,449 --> 00:35:44,439
so yeah, if code is what
you want, work with Claude.

587
00:35:44,939 --> 00:35:49,349
Next up is Grok, not to be
confused with XAI's Grok.

588
00:35:49,689 --> 00:35:56,639
So Grok is, essentially, a company that
is building, special purpose chips.

589
00:35:56,989 --> 00:36:00,729
They call them LPUs, for
running LLMs, which, makes,

590
00:36:00,779 --> 00:36:02,129
their inference time on LLMs.

591
00:36:02,129 --> 00:36:04,779
Very low, probably,
even the inference cost.

592
00:36:05,189 --> 00:36:11,849
so if latency is what you're trying to
optimize, tryout, grok, grok, cloud,

593
00:36:12,599 --> 00:36:16,829
which is their API, which are their,
LLM APIs, they generally host, most of.

594
00:36:17,429 --> 00:36:19,359
Commonly used open source models.

595
00:36:19,719 --> 00:36:24,459
So you have Lama, Mixtel, Gemma available,
apart from that, a bunch of other things.

596
00:36:24,459 --> 00:36:31,709
Latency wise, they are much faster
than all the other model providers.

597
00:36:31,709 --> 00:36:36,309
So if you are optimizing for
latency and these models work for

598
00:36:36,309 --> 00:36:38,109
your particular task, go for it.

599
00:36:38,609 --> 00:36:38,969
All right.

600
00:36:39,059 --> 00:36:43,989
So AWS, mainly works kind of like rock.

601
00:36:43,999 --> 00:36:48,369
They host a lot of open source models
on, and along with that, I think

602
00:36:48,369 --> 00:36:51,909
they also have, their own models,
which we have not tried out yet.

603
00:36:52,399 --> 00:36:58,759
but the biggest USP of using AWS,
bedrock would be if you're already

604
00:36:58,779 --> 00:37:04,209
in the AWS ecosystem and you are,
worried about, your sensitive data,

605
00:37:04,519 --> 00:37:08,449
getting out of your infra and you
don't want to like, send it to open AI

606
00:37:08,449 --> 00:37:10,089
or cloud or any other model provider.

607
00:37:10,559 --> 00:37:12,759
in that case, Bedrock
should be your choice.

608
00:37:13,259 --> 00:37:16,439
One good thing is Bedrock
also hosts cloud APIs.

609
00:37:16,819 --> 00:37:21,459
so, the limits are lower, as far
as I know, I think you'll need

610
00:37:21,499 --> 00:37:24,019
to talk to the support and, get
your service quotas increased.

611
00:37:24,029 --> 00:37:28,479
But, if you are worried about, sensitive
data and you're okay with cloud,

612
00:37:28,549 --> 00:37:30,321
Bedrock should work for you very well.

613
00:37:30,321 --> 00:37:32,742
and along with that, they
also host Lama and Mixtel.

614
00:37:32,822 --> 00:37:35,572
And a few other, APIs, multimodal APIs.

615
00:37:36,072 --> 00:37:41,152
Azure is, the last time I checked
Azure is hosting GPT models.

616
00:37:41,592 --> 00:37:45,372
separately, like, the hosting, which
open AI does is separate from Azure.

617
00:37:45,702 --> 00:37:51,212
And, the last time we checked, Azure GPT
APIs were a bit more faster than open air.

618
00:37:51,642 --> 00:37:57,962
So again, like, Oh, if you want to use
open API APIs and you, want, a slightly

619
00:37:57,972 --> 00:38:01,732
better latency, try out Azure, but
they'll make you fill a bunch of forms.

620
00:38:02,442 --> 00:38:06,762
I think these APIs or these models are not
publicly available on Azure for everyone.

621
00:38:07,262 --> 00:38:09,012
GCP, I've not tried out.

622
00:38:09,072 --> 00:38:12,022
so, again, like, I mean, I think
the setup was a bit complex.

623
00:38:12,022 --> 00:38:16,412
So, we didn't get a chance to give it
a try, but from what we've heard, the

624
00:38:16,492 --> 00:38:19,212
developer experience is much better now.

625
00:38:19,212 --> 00:38:21,682
So someday we'll give it a try again.

626
00:38:21,682 --> 00:38:24,882
But GCP has a Gemini and the.

627
00:38:25,382 --> 00:38:30,472
Latest, the newest kid on the block
is DeepSync, if you are active on

628
00:38:30,492 --> 00:38:35,432
Twitter, you would have already
heard, about DeepSync, APIs, from

629
00:38:35,442 --> 00:38:40,902
the chatter, it seems as if they
are at par with own APIs, again,

630
00:38:40,942 --> 00:38:42,782
haven't tried it out, give it a try.

631
00:38:43,077 --> 00:38:45,737
one concern could be, the
hosting, which is in China,

632
00:38:46,277 --> 00:38:47,967
but, definitely give it a try.

633
00:38:48,067 --> 00:38:51,937
probably you might find it, to
be a good fit for your use case.

634
00:38:51,997 --> 00:38:56,017
And, one more thing, deep seeks
models are also open source, so

635
00:38:56,047 --> 00:38:57,017
you can host them on your own.

636
00:38:57,517 --> 00:38:58,777
And that's all from me.

637
00:38:58,967 --> 00:39:05,157
I hope you find the information shared
in the stock useful, and it speeds up

638
00:39:05,157 --> 00:39:09,607
your development process when you're
building LLM applications and, AI agents.

639
00:39:10,067 --> 00:39:13,287
if you have any, queries or
if you want to, talk more

640
00:39:13,297 --> 00:39:14,527
about this, drop us an email.

641
00:39:14,707 --> 00:39:15,577
You can find our email.

642
00:39:16,287 --> 00:39:21,487
on Kusho AI's landing page, or
just, send me a message on LinkedIn.

643
00:39:21,587 --> 00:39:25,157
I'm happy to chat about this
and, go build something awesome.

644
00:39:25,737 --> 00:39:25,757
Bye.

