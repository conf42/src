1
00:00:00,500 --> 00:00:01,250
Hello everyone.

2
00:00:01,490 --> 00:00:02,599
I'm Akra.

3
00:00:02,780 --> 00:00:05,660
I have over 17 years experience
in performance engineering.

4
00:00:06,410 --> 00:00:10,010
Today I want to talk to you about
how we can build a relatable,

5
00:00:10,010 --> 00:00:14,060
high performing cloud applications
by applying SRE principles.

6
00:00:14,569 --> 00:00:15,080
These days.

7
00:00:15,080 --> 00:00:17,744
If you see cloud native
applications are everywhere.

8
00:00:18,165 --> 00:00:22,335
They're powerful, but also
complex and unpredictable, and

9
00:00:22,335 --> 00:00:23,894
that's where the challenge lies.

10
00:00:23,955 --> 00:00:28,424
How do we make these systems
faster, more reliable, and also

11
00:00:28,479 --> 00:00:30,009
ready to scale when required?

12
00:00:30,729 --> 00:00:35,974
So over the years, I've helped several
Fortune 500 companies tackle exactly that.

13
00:00:36,394 --> 00:00:38,885
I'll be sharing a
framework that goes beyond.

14
00:00:39,595 --> 00:00:42,385
Just tweaking code or infrastructure.

15
00:00:42,475 --> 00:00:47,425
It brings together architecture,
engineering practices and business

16
00:00:47,425 --> 00:00:50,845
goals to build truly resilient systems.

17
00:00:50,995 --> 00:00:51,700
Let's dive in.

18
00:00:52,200 --> 00:00:56,310
So now that we have seen big picture,
let's break down the framework.

19
00:00:56,340 --> 00:00:58,830
We used to build resilient systems.

20
00:00:59,190 --> 00:01:02,340
I like to think of it
as a layered pyramid.

21
00:01:02,520 --> 00:01:05,580
Each layer builds on the one below it.

22
00:01:06,570 --> 00:01:11,525
So if coming to implementation fund
fundamentals, so for example, in

23
00:01:11,525 --> 00:01:16,755
architecture, we ca we focus on clear
boundaries like separating, say order

24
00:01:16,755 --> 00:01:21,335
service from building service using
events instead of direct API calls.

25
00:01:21,945 --> 00:01:27,045
Basically this improves fault to
tolerance and reduces coupling in code.

26
00:01:27,195 --> 00:01:31,305
We prevented, say, n plus one
database queries use as in

27
00:01:31,305 --> 00:01:33,675
corporations where possible and.

28
00:01:33,990 --> 00:01:38,850
And also we made sure APIs can
handle retries without duplication.

29
00:01:39,450 --> 00:01:44,430
And on the infrastructure side, we use
auto-scaling in Kubernetes infrastructure

30
00:01:44,430 --> 00:01:49,649
as a code with Terraform and also
added reduplicate to ease database.

31
00:01:49,700 --> 00:01:50,870
Basically reload.

32
00:01:51,370 --> 00:01:55,940
So next layer, once the
foundation is strong we.

33
00:01:56,420 --> 00:02:00,410
Enabled performance thinking
into our delivery work workflows.

34
00:02:01,010 --> 00:02:07,280
So we integrated performance testing into
CICD and also automated observability and

35
00:02:07,399 --> 00:02:10,040
enforced SLS throughout the lifecycle.

36
00:02:10,519 --> 00:02:13,130
This makes performance
basically everyone's stress.

37
00:02:13,130 --> 00:02:16,090
Responsibility, not just
a pro, a post-production

38
00:02:16,090 --> 00:02:18,760
concern in rail layer three.

39
00:02:18,790 --> 00:02:22,140
After after say engineering
practices, what we did was like,

40
00:02:22,140 --> 00:02:26,880
when we implemented these practices
consistently, we start seeing the results

41
00:02:27,300 --> 00:02:29,500
reduced response times or latencies.

42
00:02:29,500 --> 00:02:32,410
Also improved throughput
and better scaling.

43
00:02:33,010 --> 00:02:37,105
So system become measurable,
predictable, and also tuneable.

44
00:02:37,605 --> 00:02:42,215
For example, in one case we brought
down response times from over

45
00:02:42,215 --> 00:02:46,505
400 milliseconds to just under a
hundred milliseconds by optimizing

46
00:02:46,505 --> 00:02:48,995
queries and also by using caching.

47
00:02:49,214 --> 00:02:52,185
This I'll be talking more
about in, future slides.

48
00:02:52,965 --> 00:02:57,795
So ultimately if I take business
value, ultimately this leads to real

49
00:02:57,795 --> 00:03:02,005
business outcomes with respect to
cost savings, happier users, better

50
00:03:02,005 --> 00:03:04,615
uptime, and also faster releases.

51
00:03:05,485 --> 00:03:10,705
In fact, in one of the project where
I have worked, we have saved almost

52
00:03:10,705 --> 00:03:16,045
40% on cloud cost while handling
five times more users simply by

53
00:03:16,045 --> 00:03:18,345
engineering smarter and better systems.

54
00:03:18,845 --> 00:03:22,074
So now that we have covered
foundational framework these

55
00:03:22,074 --> 00:03:25,254
are some of the outcomes we have
achieved in our production systems.

56
00:03:25,734 --> 00:03:30,544
So first one is we have reduced API
response time 200 milliseconds by

57
00:03:30,544 --> 00:03:35,134
applying query tuning, optimizing caching
layers, and also reducing synchronous.

58
00:03:35,139 --> 00:03:35,889
Dependencies.

59
00:03:36,369 --> 00:03:41,869
We brought down average response times
drastically from over 400 milliseconds,

60
00:03:41,869 --> 00:03:46,929
500 milliseconds, 200 milliseconds and
their boot, for example, we identified

61
00:03:46,929 --> 00:03:52,030
here, say unnecessary db giants in
giants in DB queries, and also added

62
00:03:52,090 --> 00:03:57,100
proper indexing and introduced in memory
caching for frequently access to data.

63
00:03:57,520 --> 00:04:00,670
So next outcome was
basically like we throughput.

64
00:04:01,609 --> 00:04:03,619
Increase under P load.

65
00:04:03,739 --> 00:04:06,979
So almost we achieved three x
improvement in throughput by

66
00:04:06,979 --> 00:04:08,509
removing processing bottlenecks.

67
00:04:08,869 --> 00:04:12,889
We used techniques like connection
pooling synchronous processing, and

68
00:04:12,889 --> 00:04:16,489
also refined the thread management
to handle more request per second.

69
00:04:17,119 --> 00:04:18,339
Even during the.

70
00:04:18,684 --> 00:04:22,874
Peak traffic we achieved this
300% throughput in improvement.

71
00:04:23,384 --> 00:04:30,054
And next achievement was basically 75%
reduction in database table storage.

72
00:04:30,324 --> 00:04:34,879
So we also addressed data database
storage efficiency, not just by tuning

73
00:04:34,879 --> 00:04:38,329
queries, but optimizing also schemas.

74
00:04:38,419 --> 00:04:43,429
So this included basically,
archiving, say old data, removing

75
00:04:43,429 --> 00:04:48,739
unused columns, and also normalizing
overly de-normalized tables.

76
00:04:48,770 --> 00:04:52,880
In one case, we cut down the storage
of footprint of a key table, as I was

77
00:04:52,880 --> 00:04:55,729
saying earlier, by almost 70, 75%.

78
00:04:56,150 --> 00:04:57,919
This also improved the performance.

79
00:04:58,219 --> 00:05:02,190
Another major win for us was
basically concurrent users.

80
00:05:02,290 --> 00:05:07,479
Finally we were able to scale almost
to IX before these improvements.

81
00:05:07,664 --> 00:05:11,684
Basically we were just
scaling 1000 concurrent users.

82
00:05:12,134 --> 00:05:16,224
But with the same infrastructure and
with the with the number of parts and

83
00:05:16,224 --> 00:05:21,174
everything, we were able to scale to
500 5,000 concurrent users without

84
00:05:21,174 --> 00:05:25,304
any degradation in, performance
and the best part these weren't

85
00:05:25,454 --> 00:05:27,364
basically theoretical wins for us.

86
00:05:27,364 --> 00:05:32,314
Basically, these we achieved in real
production systems with precision when

87
00:05:32,314 --> 00:05:37,204
we take a holistic approach, metric and
holistic and metric driven approach to

88
00:05:37,204 --> 00:05:41,344
performance and resilience improvement,
improvements like this basically become

89
00:05:41,614 --> 00:05:43,834
very predictable and also repeatable.

90
00:05:44,334 --> 00:05:45,504
So next slide.

91
00:05:46,224 --> 00:05:47,179
So next slide.

92
00:05:47,179 --> 00:05:51,299
I would like to talk about more about
query optimization techniques we used.

93
00:05:51,689 --> 00:05:57,299
So as systems scale, in my experience,
almost 60 to 70% of the performance

94
00:05:57,299 --> 00:06:01,319
bottlenecks I have seen in database layer.

95
00:06:01,859 --> 00:06:07,259
In this in this basically database part
of journey, we focused on optimizing the

96
00:06:07,259 --> 00:06:12,239
way our services basically interact with
the data, both at the application layer

97
00:06:12,389 --> 00:06:14,299
and layer and also a database layer.

98
00:06:14,779 --> 00:06:17,289
So one of the major issue was.

99
00:06:17,304 --> 00:06:19,734
Problem was excessive database load.

100
00:06:20,034 --> 00:06:23,934
So we were seeing spikes in
database CPU and IO during peak

101
00:06:23,934 --> 00:06:26,004
load slowing down critical APIs.

102
00:06:26,004 --> 00:06:31,184
So much of this basically came from poorly
written queries, things like fetching

103
00:06:31,184 --> 00:06:36,524
more data than needed, doing expensive
joins, also repeatedly querying in loops.

104
00:06:36,764 --> 00:06:41,264
So one in one classic issue we
tackled was n plus one query pattern,

105
00:06:41,264 --> 00:06:45,264
basically for one, one API call,
we were doing n plus one queries,

106
00:06:45,264 --> 00:06:47,334
so this was scaling performance.

107
00:06:47,334 --> 00:06:51,684
Basically what we did was like we
club this entire query into one.

108
00:06:51,744 --> 00:06:53,984
So this improved performance a lot.

109
00:06:54,525 --> 00:06:56,354
So here in database.

110
00:06:56,354 --> 00:06:58,875
So also we created indexing.

111
00:06:59,375 --> 00:07:01,535
And also did the query rewriting.

112
00:07:01,595 --> 00:07:05,860
We reviewed the most frequently executed
and slowest queries during using

113
00:07:05,860 --> 00:07:11,380
database logs and a PM tools like New
Relic and also Oracle a WR Reports.

114
00:07:11,620 --> 00:07:17,280
We added basically missing indexes,
rewrote queries to reduce joins, and

115
00:07:17,280 --> 00:07:19,930
also avoided unnecessary queries.

116
00:07:19,930 --> 00:07:21,599
We used DB query hints.

117
00:07:21,910 --> 00:07:26,980
By, and also in one scenario,
basically by just creating a composite

118
00:07:27,400 --> 00:07:31,400
indexes, basically we reduced the
query response time almost around

119
00:07:31,450 --> 00:07:33,810
two seconds to under 50 milliseconds.

120
00:07:34,320 --> 00:07:37,910
So next we concentrated on
data access pattern patterns.

121
00:07:38,500 --> 00:07:43,900
So we worked with the developer developers
to analyze how data was being accessed.

122
00:07:44,170 --> 00:07:48,010
So if at all, suppose in a UI
page, if you are seeing if you are

123
00:07:48,010 --> 00:07:52,240
showing only 10 records do we really
need to fetch a hundred records?

124
00:07:52,540 --> 00:07:55,830
So by understanding these kind
of users journeys, we tuned

125
00:07:55,830 --> 00:07:58,000
APIs to retrieve only what.

126
00:07:58,385 --> 00:08:04,305
Actually used and needed this basically
reduced payload size and also DB load.

127
00:08:04,995 --> 00:08:09,791
And another part we did was we
did strategic denormalization.

128
00:08:10,211 --> 00:08:14,131
So sometimes normalization
basically creates too many joints

129
00:08:14,431 --> 00:08:15,816
for performance critical parts.

130
00:08:16,021 --> 00:08:18,961
We de-normalized just
selectively, for example.

131
00:08:19,006 --> 00:08:22,906
We embedded commonly joined
fields directly into a reporting

132
00:08:22,906 --> 00:08:24,796
table used for dashboards.

133
00:08:24,826 --> 00:08:31,086
So this basically removed three joins
and also query time was 80, 80% faster.

134
00:08:31,586 --> 00:08:36,076
And also, as I was saying earlier,
we focused on database specific

135
00:08:36,076 --> 00:08:37,876
optimizations like for Oracle.

136
00:08:38,376 --> 00:08:44,026
So we used the optimizer hinges, SQL
plan baselines and also for my MySQL.

137
00:08:44,396 --> 00:08:49,136
Basically we used explains explain
plans to query to tune complex queries.

138
00:08:49,346 --> 00:08:53,516
So by combining application level
changes with the d. Per DB knowledge,

139
00:08:53,516 --> 00:08:58,166
we cut down query volume almost
by 85% in several user flows.

140
00:08:58,466 --> 00:09:03,206
So if you see like a query optimization
might not sound glamorous, but

141
00:09:03,236 --> 00:09:07,136
it's one of the highest ROI
efforts in performance engineering.

142
00:09:07,616 --> 00:09:12,176
It directly impacts speed, cost,
and also user satisfaction.

143
00:09:12,676 --> 00:09:17,086
So next slide I would like
to talk about concurrency.

144
00:09:17,586 --> 00:09:22,206
So now because handling more users
is not just about throwing, say,

145
00:09:22,206 --> 00:09:27,256
hardware at the problem it's about
using your resources very efficiently,

146
00:09:27,316 --> 00:09:29,126
especially under, peak load.

147
00:09:29,486 --> 00:09:33,536
So in this phase we focused on how
the system handled the concurrent

148
00:09:33,606 --> 00:09:37,926
load across threats, connection
pools, and also different services.

149
00:09:38,406 --> 00:09:42,596
So first one we did was like we did
the connection pool optimization.

150
00:09:42,936 --> 00:09:47,196
We noticed that under load connection
saturation was leading to request

151
00:09:47,196 --> 00:09:49,476
delays even dropped to transactions.

152
00:09:49,776 --> 00:09:52,526
So we implemented
dynamic connection pools.

153
00:09:52,706 --> 00:09:55,016
Tuned just not default settings.

154
00:09:55,066 --> 00:09:59,061
We tuned we adapted basically to
actual workload characteristics.

155
00:09:59,571 --> 00:10:03,531
So this alone reduced the
connection overhead by almost

156
00:10:03,531 --> 00:10:05,981
40% in our busiest services.

157
00:10:06,311 --> 00:10:11,111
For example, here, instead of having
one static max size, minimum size, we

158
00:10:11,111 --> 00:10:16,541
configured separate connection pools for
read heavy and write heavy services based

159
00:10:16,541 --> 00:10:18,256
on the traffic patterns we analyzed.

160
00:10:19,081 --> 00:10:22,921
And next one was we moved
away from blocking threads

161
00:10:22,971 --> 00:10:26,961
and shifted to non-blocking
asynchronous logic where possible.

162
00:10:27,351 --> 00:10:32,121
So one service that processed incoming
orders basically used to block on

163
00:10:32,121 --> 00:10:33,981
downstream, say, inventory calls.

164
00:10:34,371 --> 00:10:39,521
So we wrote we rewrote this using
an event driven architecture

165
00:10:39,891 --> 00:10:41,241
and decoupling the flow.

166
00:10:42,161 --> 00:10:45,911
And improving response time,
even under highest loads.

167
00:10:46,601 --> 00:10:51,911
So we used in the code, we used
basically promises, futures and we

168
00:10:51,911 --> 00:10:58,441
used the reactive like libraries like
Rx, Java or Spring web flux to handle

169
00:10:58,441 --> 00:11:00,506
this asynchronous flow flows cleanly.

170
00:11:01,431 --> 00:11:04,911
So next in thread management,
basically thread contention is one

171
00:11:04,911 --> 00:11:08,611
of the hidden killers for performance
for us in most of the applications.

172
00:11:08,611 --> 00:11:14,071
So what we did was like we implemented
custom thread pools, fine tuned workloads.

173
00:11:14,371 --> 00:11:18,831
And also we introduced work
scaling algorithms to redistribute

174
00:11:18,891 --> 00:11:20,361
ideal threats dynamically.

175
00:11:20,721 --> 00:11:24,731
So we also introduced in thread
management back pressure.

176
00:11:24,971 --> 00:11:30,301
So when the system was under extreme load,
instead of cascading failures, we shed,

177
00:11:30,841 --> 00:11:33,181
we could shed some excess load gracefully.

178
00:11:33,681 --> 00:11:37,341
And another one we concentrated
was timeout strategy.

179
00:11:37,551 --> 00:11:40,451
So timeouts, if you think are like breaks.

180
00:11:40,661 --> 00:11:43,691
So the prevent runaway resource usage.

181
00:11:44,051 --> 00:11:48,041
So we designed the cascading
timeout strategies at every level.

182
00:11:48,371 --> 00:11:53,021
So from database to HTP clients,
we also used circuit breakers.

183
00:11:53,041 --> 00:11:57,661
To trip and recover services
automatically if something is going wrong.

184
00:11:58,301 --> 00:12:03,081
This approach prevented threat pools
from being as exhausted when as a

185
00:12:03,081 --> 00:12:07,541
downstream service failed May this
maintained system stability as well.

186
00:12:07,841 --> 00:12:12,656
So by optimizing concurrency across
all these layers, we ensured the

187
00:12:12,661 --> 00:12:15,111
system remained responsive even.

188
00:12:15,396 --> 00:12:16,896
User load got spiked.

189
00:12:17,226 --> 00:12:19,356
This wasn't just a performance for us.

190
00:12:19,356 --> 00:12:22,596
Basically, it was a resilient
resiliency enabler as well.

191
00:12:23,096 --> 00:12:26,266
Next I would like to talk about
caching strategies we used.

192
00:12:26,416 --> 00:12:31,996
So when we when we think about scaling
systems and reducing latency caching is

193
00:12:31,996 --> 00:12:36,586
one of the powerful tools available if
we use that correctly in this approach.

194
00:12:36,616 --> 00:12:40,426
Basically we built a multi-layered
caching strategy targeting

195
00:12:40,426 --> 00:12:41,836
every layer of the stack.

196
00:12:42,256 --> 00:12:44,266
From client side to database.

197
00:12:44,626 --> 00:12:48,406
So if I talk about client side,
basically we began with the front

198
00:12:48,406 --> 00:12:53,836
end by adding cache control headers,
eTax, and also service workers, we

199
00:12:53,836 --> 00:12:58,606
allowed browsers to browsers and mo
mod mobile apps to reuse previously

200
00:12:58,636 --> 00:13:01,476
first static data, be it images.

201
00:13:01,516 --> 00:13:03,616
Say CS files, js files.

202
00:13:03,616 --> 00:13:07,856
Basically we version them and
used stored on the client devices.

203
00:13:08,216 --> 00:13:13,796
So this reduced the basically number of
kits to the server for static content.

204
00:13:14,126 --> 00:13:19,586
And this dropped network traffic
almost by 65% for returning users.

205
00:13:19,976 --> 00:13:23,686
For example here, basically
for user profile images.

206
00:13:23,716 --> 00:13:24,496
Settings.

207
00:13:24,496 --> 00:13:29,246
We cast them locally on the client
device, so only revalidated them

208
00:13:29,296 --> 00:13:33,326
very periodically so that like
we will not show any stale data.

209
00:13:33,326 --> 00:13:35,056
Also, we use the
worsening techniques here.

210
00:13:35,556 --> 00:13:40,576
So next at the API gateway level we added
the edge caching at the API gateway level.

211
00:13:40,816 --> 00:13:44,286
So especially for frequently
accessed endpoints like product

212
00:13:44,286 --> 00:13:48,756
listings, configurations, and
also pricing related things where

213
00:13:48,846 --> 00:13:50,616
they won't change very frequently.

214
00:13:50,946 --> 00:13:56,706
So we also, here, we also built a
smart invalidation mechanisms so that

215
00:13:56,706 --> 00:14:00,326
when the data changed in the backend
only the affected cache entries

216
00:14:00,326 --> 00:14:02,126
were evicted and rebuilt again.

217
00:14:03,046 --> 00:14:07,566
This offloaded significant traffic
from the app and db layer and also

218
00:14:07,566 --> 00:14:09,471
improved a a PA response times.

219
00:14:09,971 --> 00:14:12,461
Next one was application level caching.

220
00:14:12,821 --> 00:14:17,051
So inside the application we used
a mix of in memory cache and also

221
00:14:17,051 --> 00:14:22,146
distributed cache like red multi-node
and in red for multi-node environments.

222
00:14:22,146 --> 00:14:25,806
We applied the cache aside PA
pattern where the application

223
00:14:25,806 --> 00:14:27,126
basically fast checks.

224
00:14:27,541 --> 00:14:31,591
In the cache and then only
queries DB if basically if entry

225
00:14:31,591 --> 00:14:32,851
is not available in the cache.

226
00:14:33,151 --> 00:14:36,001
Also, we fine tuned the TTL values.

227
00:14:36,051 --> 00:14:38,581
Tt l is basically time
to leave cache values.

228
00:14:39,001 --> 00:14:43,801
So based on how volatile each dataset
was, so example, static config was, say

229
00:14:43,801 --> 00:14:49,801
cached for hours, and also user session
related info had basically shorter TTLs.

230
00:14:50,301 --> 00:14:53,931
So next we concentrated on
database result caching.

231
00:14:54,141 --> 00:14:59,841
For ex, for expensive DB operations large
reports and complex joints, we cached the

232
00:14:59,841 --> 00:15:04,211
result and invalidated it automatically
when that relevant data changed.

233
00:15:04,751 --> 00:15:09,071
We used ride through strategies
also to keep the cache and DB in

234
00:15:09,071 --> 00:15:13,721
sync, ensuring the consistency
without sacrificing the query speed.

235
00:15:14,291 --> 00:15:19,291
So this help us reduce query volume
by 30% during traffic spikes.

236
00:15:19,681 --> 00:15:25,351
So overall, if you see like a caching is
not just about speed, it's about control.

237
00:15:25,741 --> 00:15:30,361
By implementing the right strategy
at each layer we achieved be better

238
00:15:30,861 --> 00:15:34,546
respons, responsiveness, and also
scalability without compromis

239
00:15:34,601 --> 00:15:37,391
compromising on data say integrity.

240
00:15:37,891 --> 00:15:40,501
So next I would like to
talk about load balancing.

241
00:15:41,111 --> 00:15:44,831
So if you see traditional load
balancing, like road robin or

242
00:15:44,831 --> 00:15:49,851
sticky sessions we in this complex
environments just isn't cut anymore.

243
00:15:50,316 --> 00:15:53,346
In a dynamic and also
microservices architectures.

244
00:15:53,676 --> 00:15:57,786
So in this phase we implemented
intelligent load balancing, which

245
00:15:58,116 --> 00:16:02,526
basically adapts in real time to
system conditions and also traffic

246
00:16:02,526 --> 00:16:04,296
patterns and also service health.

247
00:16:04,866 --> 00:16:08,616
So first what we did was like,
we did request classification.

248
00:16:08,616 --> 00:16:11,196
So if you see not all
requests are equal, right?

249
00:16:11,466 --> 00:16:17,436
So we started by classifying request based
on type, priority, and also user type.

250
00:16:18,366 --> 00:16:23,956
If you, if you VIP customer is logging
in, then they will get highest priority.

251
00:16:24,196 --> 00:16:26,226
And also we used the resource.

252
00:16:26,256 --> 00:16:28,896
What kind of resource demand
that a PA call requires.

253
00:16:29,176 --> 00:16:33,126
For example if you see a get profile
call should not be treated the

254
00:16:33,126 --> 00:16:38,116
same as say, if you are doing some
reporting, say monthly report request.

255
00:16:38,116 --> 00:16:38,776
Both should not be.

256
00:16:38,966 --> 00:16:39,716
Treated as same.

257
00:16:39,766 --> 00:16:42,736
Here, get profile should get
highest priority, so that like it'll

258
00:16:42,736 --> 00:16:46,356
complete faster and also user will
not be waiting for get profile.

259
00:16:46,456 --> 00:16:49,666
A PA call most of the times
say generate monthly report.

260
00:16:49,806 --> 00:16:52,136
Most of the times user will
will be willing to wait.

261
00:16:53,006 --> 00:16:58,836
So we tag the a PA request at a PA request
at the a PA gateway level and allowing

262
00:16:58,836 --> 00:17:01,206
us to prioritize and route intelligently.

263
00:17:01,896 --> 00:17:04,686
So high priority transactions like.

264
00:17:04,961 --> 00:17:08,441
Checkout or login those kind
of transactions, basically

265
00:17:08,441 --> 00:17:09,821
were given fast claims.

266
00:17:10,321 --> 00:17:13,441
Next we concentrated
on routing strategies.

267
00:17:13,591 --> 00:17:16,561
So routing was basically made dynamic.

268
00:17:16,961 --> 00:17:21,911
So based on realtime health,
capacity signals traffic could be

269
00:17:21,911 --> 00:17:26,101
shifted away from overloaded or
degraded nodes to healthy nodes.

270
00:17:26,551 --> 00:17:32,401
So we integrated service discovery with
health checks so that if one node slowed

271
00:17:32,401 --> 00:17:36,876
down or failed traffic would automatically
reroute with the minimal impact.

272
00:17:37,616 --> 00:17:41,521
So this strategy also helped
during our rolling deployments

273
00:17:41,521 --> 00:17:43,501
and also blue-green releases.

274
00:17:44,401 --> 00:17:49,251
So next we used instead of equal
load distribution, we used weighted

275
00:17:49,251 --> 00:17:54,201
algorithms based on capacity
latency and even based on geography.

276
00:17:54,591 --> 00:17:59,391
For example, if one node had twice
the CPU capacity of another, so

277
00:17:59,391 --> 00:18:00,921
it got a higher traffic weight.

278
00:18:01,261 --> 00:18:05,711
So we even used geolocation
based routing to reduce the

279
00:18:05,711 --> 00:18:07,631
cross region network latency.

280
00:18:08,131 --> 00:18:12,081
So next finally we added continuous
health monitoring with the

281
00:18:12,081 --> 00:18:13,821
graceful degradation built in.

282
00:18:14,151 --> 00:18:19,641
So if a dependent service, say, search
or recommendation recommendations became

283
00:18:19,641 --> 00:18:24,741
unhealthy, we return fallbacks or say
cache data instead of failing outright.

284
00:18:24,981 --> 00:18:30,456
So this overall improved user
experience and also, also this helped

285
00:18:30,456 --> 00:18:32,286
immensely during partial outages.

286
00:18:33,006 --> 00:18:37,946
So together these techniques created
a smarter self-aware load balancing

287
00:18:37,946 --> 00:18:43,376
layer that maintained performance and
also uptime, especially during failure

288
00:18:43,376 --> 00:18:46,046
scenarios and also peak traffic events.

289
00:18:46,406 --> 00:18:49,886
And because it was all
observability driven, we could,

290
00:18:49,946 --> 00:18:54,146
we can improve routing strategies
based on real telemetry we used.

291
00:18:54,646 --> 00:18:57,351
So next I would like to talk
about the observability.

292
00:18:57,611 --> 00:19:01,861
So when we, when systems grow in
complexity, if you see like visibility

293
00:19:01,891 --> 00:19:06,111
becomes non-negotiable, without the
right visibility in place, even small

294
00:19:06,111 --> 00:19:08,151
issues can become major outages.

295
00:19:08,671 --> 00:19:12,731
So we built a comprehensive
observability platform one that

296
00:19:12,821 --> 00:19:15,401
basically combined metrics locks.

297
00:19:16,111 --> 00:19:20,401
Traces and alerts into a
single cohesive feedback loop.

298
00:19:20,821 --> 00:19:24,271
So if I talk about metrics metrics
gave us a high level view of

299
00:19:24,271 --> 00:19:26,591
how our system is be behaving.

300
00:19:27,011 --> 00:19:30,551
So we used two proven frameworks.

301
00:19:30,551 --> 00:19:34,421
One is red that is RED
rate reds and duration.

302
00:19:34,721 --> 00:19:38,191
This is basically great for,
services and a p monitoring.

303
00:19:38,491 --> 00:19:43,691
So another framework we used was
USE utilization, saturation errors.

304
00:19:44,291 --> 00:19:48,061
So this is, this framework is
basically ideal for infrastructure

305
00:19:48,061 --> 00:19:49,261
and resource monitoring.

306
00:19:49,681 --> 00:19:54,001
For example, by tracking request
rates and latency across services,

307
00:19:54,331 --> 00:19:58,811
we spotted bottlenecks long before
they even caused di downtime.

308
00:19:59,131 --> 00:19:59,611
Downtime.

309
00:20:00,391 --> 00:20:05,281
So we also tracked the business
KPIs like say card conversion rate

310
00:20:05,281 --> 00:20:07,291
or average transaction latency.

311
00:20:07,591 --> 00:20:12,001
So teams could correlate
technical changes to user impact.

312
00:20:12,051 --> 00:20:13,521
So next logs.

313
00:20:13,611 --> 00:20:17,121
So metrics, basically, if you see like
metrics tell us something is wrong,

314
00:20:17,181 --> 00:20:19,911
but logs tell us why that is wrong.

315
00:20:20,391 --> 00:20:25,081
So we standardized our structured
log logging, so including error

316
00:20:25,081 --> 00:20:30,296
codes, request IDs user IDs, and
also we log the relevant payloads.

317
00:20:31,066 --> 00:20:35,466
So with the correlation IDs, we
could trace a single user request

318
00:20:35,466 --> 00:20:37,866
across say, dozens of microservices.

319
00:20:38,226 --> 00:20:42,966
And also we enrich the logs with
the contextual data, like region

320
00:20:43,056 --> 00:20:47,046
environment, and also feature
flags to make debugging faster.

321
00:20:48,001 --> 00:20:49,591
So next traces.

322
00:20:49,951 --> 00:20:54,731
So if you see if we implemented dis
distributed tracing using tools like

323
00:20:54,731 --> 00:21:00,161
open Telemetry or agar this allowed
us to visualize how we request

324
00:21:00,161 --> 00:21:02,321
flowed across multiple services.

325
00:21:02,681 --> 00:21:06,701
From front end to backend and see
exactly where delays occurred.

326
00:21:07,031 --> 00:21:11,441
For example, we identified 60%
of latency in checkout flow came

327
00:21:11,441 --> 00:21:13,421
from a downstream payment system.

328
00:21:13,811 --> 00:21:18,671
So we could, we would not have
caught that with the logs alone.

329
00:21:18,701 --> 00:21:21,331
So here basically traces traces helped us.

330
00:21:21,831 --> 00:21:23,211
So next alerts.

331
00:21:23,261 --> 00:21:28,721
So instead of say nice threshold based
alerts we used SLA and SLO driven

332
00:21:28,721 --> 00:21:34,061
alerting, which only triggers when, say
actual reliability goals are at risk.

333
00:21:34,361 --> 00:21:37,126
We also implemented
Alert, alert, correlation.

334
00:21:37,421 --> 00:21:41,891
So that a cascade of downstream error
alerts would not overwhelm support

335
00:21:41,891 --> 00:21:44,561
engineers with redundant notifications.

336
00:21:44,921 --> 00:21:50,041
So this led to a few fewer false
alarms better on-call experience

337
00:21:50,041 --> 00:21:51,481
and faster resolution time.

338
00:21:52,051 --> 00:21:55,731
With this observability framework,
the end result is basically we give

339
00:21:55,731 --> 00:22:00,351
engineering teams near total vis
near realtime total visibility into

340
00:22:00,351 --> 00:22:02,511
their services, making it ease.

341
00:22:02,511 --> 00:22:07,671
It is easier to spot, investigate
and also fix issues before users.

342
00:22:07,686 --> 00:22:09,576
Even notice them.

343
00:22:10,426 --> 00:22:15,166
And just as importantly, we tied all this
observability back into business goals.

344
00:22:15,166 --> 00:22:19,536
So every alert or dashboard
was rooted in real impact.

345
00:22:20,036 --> 00:22:23,146
Next I would like to talk about
data-driven capacity planning.

346
00:22:23,446 --> 00:22:26,836
So one of the biggest challenges
in any large scale system is

347
00:22:26,886 --> 00:22:28,896
balancing cost with performance.

348
00:22:29,286 --> 00:22:33,776
So if you overprovision, basically
that leads to into higher higher

349
00:22:33,776 --> 00:22:35,066
or unnecessary cloud spend.

350
00:22:35,516 --> 00:22:39,906
But if you under provision basically
you may face outages as well.

351
00:22:40,236 --> 00:22:41,796
So our answer to this.

352
00:22:41,811 --> 00:22:43,771
Is data-driven capacity planning.

353
00:22:44,131 --> 00:22:47,721
So first what we did was like, we
started the deep analysis of historical

354
00:22:48,006 --> 00:22:54,456
utilization, looking at CPU, memory, iops,
and also network usage across services.

355
00:22:54,996 --> 00:22:59,286
We profiled each component and
identified the seasonal traffic patterns.

356
00:22:59,556 --> 00:23:04,516
For example, most of the times usage
basically suggests on Monday mornings.

357
00:23:05,331 --> 00:23:08,981
Or traffic spikes occur
during end of quarter events.

358
00:23:09,881 --> 00:23:15,661
We also built anomaly filters to
distinguish between real spike or

359
00:23:15,691 --> 00:23:17,831
one of the spikes a random spike.

360
00:23:18,071 --> 00:23:21,311
So we would not scale
infrastructure based on outliers.

361
00:23:21,581 --> 00:23:25,781
So this helped us establish accurate
baselines for each environment.

362
00:23:26,391 --> 00:23:27,651
And also each service.

363
00:23:28,151 --> 00:23:30,041
So next we did the growth modeling.

364
00:23:30,091 --> 00:23:36,101
So we applied statistical models and
machine learning to forecast how usage

365
00:23:36,101 --> 00:23:39,761
would evolve based on, say, product
growth and also business events.

366
00:23:40,271 --> 00:23:44,741
We factored in new feature rollouts,
marketing campaigns, and also

367
00:23:44,741 --> 00:23:46,271
customer onboarding timelines.

368
00:23:46,601 --> 00:23:48,906
For example, we modeled several scenarios.

369
00:23:48,906 --> 00:23:50,941
So first one is best case scenario.

370
00:23:50,941 --> 00:23:52,261
What is worst case scenario?

371
00:23:52,471 --> 00:23:55,101
And also what is say expected scenario?

372
00:23:55,491 --> 00:23:58,411
Each we associated with
the confidence levels.

373
00:23:58,711 --> 00:24:02,031
So this gave product and
also infrastructure teams a

374
00:24:02,031 --> 00:24:04,011
shared and also data back.

375
00:24:04,011 --> 00:24:05,481
The plan they could lean on.

376
00:24:05,981 --> 00:24:08,461
So next resource optimization.

377
00:24:08,531 --> 00:24:12,631
So based on those projections, we
implemented dynamic scaling strategies

378
00:24:12,661 --> 00:24:14,731
across services and also infrastructure.

379
00:24:15,271 --> 00:24:19,691
So we used Kubernetes horizontal
pod, auto scalers, and also.

380
00:24:20,261 --> 00:24:24,371
Cloud natives, scaling policies for
database and also messaging systems.

381
00:24:24,911 --> 00:24:27,851
We also defined resource
utilization targets.

382
00:24:27,851 --> 00:24:33,251
So services would scale only when truly
needed and scale back when they are idle.

383
00:24:33,731 --> 00:24:37,261
So this approach allowed us
to strike the right balance.

384
00:24:37,311 --> 00:24:40,481
Between performance headroom
and also cost efficiency.

385
00:24:40,781 --> 00:24:45,221
In one case, we reduced monthly
cloud spend by almost 30% while

386
00:24:45,221 --> 00:24:49,221
maintaining 99.9% 9 99 0.99% uptime.

387
00:24:49,221 --> 00:24:51,631
Just by optimizing our,
provisioning strategy.

388
00:24:52,021 --> 00:24:57,091
So key takeaway from this is the capacity
planning should not be a guessing game.

389
00:24:57,451 --> 00:25:01,861
So when it's a data-driven, predictive
and aligned with business growth, it

390
00:25:01,861 --> 00:25:04,581
becomes a strategic strategic advantage.

391
00:25:05,081 --> 00:25:08,591
So next I would like to talk about
performance testing, which is crucial.

392
00:25:08,951 --> 00:25:13,991
So in traditionally if you see like
performance testing has been seen as one

393
00:25:13,991 --> 00:25:15,337
time phase at the end of the product.

394
00:25:15,726 --> 00:25:19,446
Development life cycle, but in
modern delivery pipelines that the

395
00:25:19,446 --> 00:25:21,006
approach does not work anymore.

396
00:25:21,276 --> 00:25:26,156
So we treated performance testing as a
first class citizen with CICD from the

397
00:25:26,156 --> 00:25:28,316
smallest function to full scale low test.

398
00:25:28,676 --> 00:25:30,456
So first unit test.

399
00:25:30,456 --> 00:25:33,226
So we started with unit
level performance test.

400
00:25:33,406 --> 00:25:37,246
These are basically fast,
lightweight benchmarks embedded

401
00:25:37,246 --> 00:25:38,716
in directly into build.

402
00:25:39,076 --> 00:25:44,866
If your core function, say a pricing
calculation, suddenly slowed say slowed

403
00:25:44,866 --> 00:25:46,246
by, slowed down by two x, we could.

404
00:25:46,696 --> 00:25:48,826
Catch it immediately
during a pull request.

405
00:25:49,276 --> 00:25:52,816
This help us developers fix
regressions before they reached

406
00:25:52,816 --> 00:25:54,436
integration or staging environments.

407
00:25:54,886 --> 00:25:58,136
So next we did service
level isolated testing.

408
00:25:58,526 --> 00:26:03,716
So each microservices service basically
had its own performance benchmark suit,

409
00:26:04,166 --> 00:26:06,236
so running in isolated environments.

410
00:26:06,296 --> 00:26:10,946
This, we ran in isolated environments,
so we measured the service level latency.

411
00:26:11,201 --> 00:26:16,091
Throughput memory usage, and also error
rates in controlled con conditions.

412
00:26:16,481 --> 00:26:20,281
So we used the marks here to
simulate, say, downstream systems.

413
00:26:20,761 --> 00:26:23,671
So this allowed teams to
independently tune their

414
00:26:23,671 --> 00:26:25,801
services without needing to test.

415
00:26:26,301 --> 00:26:28,341
The entire system every time.

416
00:26:28,731 --> 00:26:32,601
For example, a Cadillac service
could be validated against a

417
00:26:32,601 --> 00:26:37,871
known data set also, or known mock
endpoint for consistent response

418
00:26:37,871 --> 00:26:40,091
times for different load scenarios.

419
00:26:40,841 --> 00:26:43,671
So next we did integration
performance testing.

420
00:26:43,801 --> 00:26:49,541
So when we then tested across
basically multiple workflows, how

421
00:26:49,601 --> 00:26:54,181
this basically tell us how service
services behaved when changed together.

422
00:26:54,601 --> 00:26:57,851
This revealed issues like
cumulative latency, serial

423
00:26:58,061 --> 00:27:01,991
serialization, bottlenecks, and
also inefficient retrial logics.

424
00:27:02,441 --> 00:27:04,721
So in one case, a checkout flow that.

425
00:27:05,126 --> 00:27:06,446
Fine in isolation.

426
00:27:06,746 --> 00:27:08,876
Almost showed a two second delay.

427
00:27:08,926 --> 00:27:10,456
When we tested it end to end.

428
00:27:10,786 --> 00:27:14,186
The culprit here basically was
a synchronous email service

429
00:27:14,286 --> 00:27:15,846
holding up the complete flow.

430
00:27:16,346 --> 00:27:20,306
So next we did full scale testing
in pre-production environment.

431
00:27:20,386 --> 00:27:22,676
The environment was
production like environment.

432
00:27:23,156 --> 00:27:27,446
So we finally, before
if a, any major release.

433
00:27:27,476 --> 00:27:32,366
What we did was like we ran production,
like low test in pre-prod environment,

434
00:27:32,426 --> 00:27:34,626
assimilating real traffic patterns.

435
00:27:34,986 --> 00:27:38,306
So with realistic data,
and also concurrency.

436
00:27:38,666 --> 00:27:44,096
We used the tools like Geometer Gatling
to mimic user load across geographies.

437
00:27:44,486 --> 00:27:49,046
So those results directly
mapped into to SL os.

438
00:27:49,316 --> 00:27:53,346
So if any service missed its
latency or error target the

439
00:27:53,346 --> 00:27:55,216
deployment was paused automatically.

440
00:27:55,716 --> 00:28:00,576
By integrating performance testing
throughout the pipeline, we sh shifted the

441
00:28:00,576 --> 00:28:04,236
left and caught performance regressions
before they reached production.

442
00:28:04,596 --> 00:28:08,516
This not only improved reliability,
but also built the confidence

443
00:28:08,516 --> 00:28:12,336
across teams that every release
was truly ready for scale.

444
00:28:12,836 --> 00:28:18,146
So up until now we talked about
optimization, scaling, and efficiency,

445
00:28:18,446 --> 00:28:20,566
but what happens when things go wrong?

446
00:28:20,756 --> 00:28:24,876
Because in real world systems
failure is basically unavoidable.

447
00:28:25,226 --> 00:28:28,946
So that's why we introduced
kiosk engineering a discipline

448
00:28:28,976 --> 00:28:33,706
that lets us prepare for failure
before it actually happens.

449
00:28:33,976 --> 00:28:37,576
So in for kiosk engineering
first what we did was like, we

450
00:28:37,576 --> 00:28:39,306
did the hypothesis formation.

451
00:28:39,576 --> 00:28:41,216
We started basically by forming.

452
00:28:41,371 --> 00:28:45,371
Clear, testable hypothesis
about how the system might fail.

453
00:28:45,611 --> 00:28:49,811
For example, if the say, inventory
service goes down, can checkout still

454
00:28:49,811 --> 00:28:51,941
succeed using cash inventory data?

455
00:28:52,241 --> 00:28:58,031
Or say if a database latency spikes for
30 seconds, will the user session expire

456
00:28:58,031 --> 00:29:00,041
gracefully or degrade the performance?

457
00:29:00,341 --> 00:29:04,421
We focused on these tests on
critical business flows sign up.

458
00:29:04,951 --> 00:29:08,191
Login, checkout, and
also on data ingestion.

459
00:29:08,711 --> 00:29:12,751
Once hypothesis has been formed,
we experimented the design.

460
00:29:12,971 --> 00:29:13,391
Experiment.

461
00:29:13,501 --> 00:29:16,321
We designed experiments to
simulate real failure modes.

462
00:29:16,681 --> 00:29:21,321
That could mean as if sometimes
killing a pod or injecting a latency

463
00:29:21,621 --> 00:29:23,751
or simulating a dependency failure.

464
00:29:24,351 --> 00:29:29,591
The key was here control, we scoped
the blast radius and applied the

465
00:29:29,591 --> 00:29:34,631
in, in it in isolated environments
first, and then monitored every step.

466
00:29:35,021 --> 00:29:39,481
So in one case basically we introduced
30 seconds latency to our payment

467
00:29:39,481 --> 00:29:44,811
provider integration and watched how
our say retry policies handle this.

468
00:29:45,311 --> 00:29:50,811
We executed this test progressively in
controlled manner starting in dev, moving

469
00:29:50,811 --> 00:29:54,711
to stage, and then selectively into
production using cannery deployments.

470
00:29:55,101 --> 00:29:58,731
Each ex experiment had
auto termination criteria.

471
00:29:58,911 --> 00:30:03,651
If key metrics like error rate or
latency crossed safe thresholds,

472
00:30:03,936 --> 00:30:05,681
the test stopped immediately.

473
00:30:06,031 --> 00:30:10,501
This ensured we did not cause actual
harm, especially in production

474
00:30:10,501 --> 00:30:12,091
while uncovering the real risks.

475
00:30:12,591 --> 00:30:14,391
Next one is most important thing.

476
00:30:14,461 --> 00:30:16,051
We did not stop at discovery.

477
00:30:16,111 --> 00:30:18,661
We converted findings into actions.

478
00:30:19,361 --> 00:30:23,971
We added circuit breakers where
needed improved RY logic and also

479
00:30:23,971 --> 00:30:25,911
made our fallbacks are smarter.

480
00:30:26,411 --> 00:30:31,040
For example, after one test it
revealed a slow down downstream.

481
00:30:31,040 --> 00:30:33,531
Such API stalling the homepage.

482
00:30:33,950 --> 00:30:38,210
We implemented a timeout plus a
cached fallback cutting failover

483
00:30:38,210 --> 00:30:39,880
time from minutes to seconds.

484
00:30:40,150 --> 00:30:42,581
We also automated the many of.

485
00:30:42,856 --> 00:30:46,906
These recovery patterns, so
systems could self-heal instead of

486
00:30:46,936 --> 00:30:48,586
waiting for a human intervention.

487
00:30:49,306 --> 00:30:55,066
So if you see overall kiosk engineering
gave us a proactive resiliency mindset.

488
00:30:55,366 --> 00:30:59,976
We stopped the fearing of fearing for
failure and started designing system

489
00:30:59,976 --> 00:31:02,015
that could withstand and adapt to it.

490
00:31:02,515 --> 00:31:02,995
Thank you.

491
00:31:03,095 --> 00:31:04,355
This is about my session.

492
00:31:04,355 --> 00:31:08,915
Thank you all for being here and taking
time to explore this journey with me.

493
00:31:09,305 --> 00:31:14,646
We have gone far beyond just tuning
response times or adding few dashboards.

494
00:31:15,495 --> 00:31:17,775
What we have seen here is a complete.

495
00:31:17,775 --> 00:31:21,880
The layered approach, one that
brings together architecture, code,

496
00:31:21,940 --> 00:31:24,250
infrastructure, and also automation.

497
00:31:24,650 --> 00:31:28,560
By engineering with reliability
in mind right from the ground

498
00:31:28,560 --> 00:31:33,520
up, we can build a cloud native
systems that are not only fast and

499
00:31:33,520 --> 00:31:36,370
scalable, but resilient by design.

500
00:31:36,870 --> 00:31:42,050
My hope is that today stock sparked
a new ideas, for your teams can shift

501
00:31:42,050 --> 00:31:46,910
left on performance and also break silos
and develop systems that perform well.

502
00:31:47,270 --> 00:31:48,940
Thank you again for joining with me.

