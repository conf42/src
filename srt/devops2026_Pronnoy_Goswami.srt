1
00:00:00,500 --> 00:00:01,280
Speaker 32: Hello everyone.

2
00:00:01,790 --> 00:00:03,110
I'm Pran ami.

3
00:00:03,170 --> 00:00:08,630
I am excited to be presenting at
DevOps 2026 organized by Con 42.

4
00:00:08,870 --> 00:00:13,130
I'm thankful for the opportunity,
and today I will be speaking about

5
00:00:13,250 --> 00:00:18,619
from Signal to Insight, how I've
built a Queryable observability

6
00:00:18,920 --> 00:00:23,659
dashboard and a tool chain using MCP,
which is the model context protocol.

7
00:00:24,159 --> 00:00:25,359
So let's get into it.

8
00:00:25,859 --> 00:00:26,489
About me.

9
00:00:26,579 --> 00:00:30,779
So I am currently a software engineer
at Workday, and previously I've had

10
00:00:30,779 --> 00:00:33,569
stints at McKinsey, Microsoft, PayPal.

11
00:00:34,069 --> 00:00:39,289
I am experienced with distributed
systems, AI and cloud infrastructure,

12
00:00:39,949 --> 00:00:44,614
and my work focuses primarily on ai
ML infrastructure and observability.

13
00:00:45,349 --> 00:00:48,300
I often write about my findings and.

14
00:00:48,610 --> 00:00:51,850
My learnings in my blog on LinkedIn
called Distributed Bys with

15
00:00:51,850 --> 00:00:54,430
Prone, and these are my socials.

16
00:00:55,059 --> 00:00:59,980
So I'm excited to be sharing this
con this presentation with you.

17
00:01:00,460 --> 00:01:02,350
So now let's get into the weed of things.

18
00:01:02,480 --> 00:01:03,980
Why should you actually care?

19
00:01:04,160 --> 00:01:07,610
And what's in it for you guys
who's attending the conferences?

20
00:01:08,110 --> 00:01:13,630
So let's imagine a scenario like we have
all been there as on-call engineers,

21
00:01:13,690 --> 00:01:19,840
that we get a 2:00 AM page and whenever
we get on that, we start looking into.

22
00:01:20,340 --> 00:01:24,960
The data avalanche, like tens of terabytes
of blogs daily are being ingested in

23
00:01:24,960 --> 00:01:29,700
that pla in a platform or services,
millions of metrics and data points,

24
00:01:29,700 --> 00:01:33,840
millions of distributed traces and
thousands of correlation ID per minute.

25
00:01:34,590 --> 00:01:40,740
And what we struggle with most is manual
correlation across systems, tribal

26
00:01:40,740 --> 00:01:46,080
knowledge, dependency, alert fatigue,
and delayed root cause analysis.

27
00:01:46,289 --> 00:01:48,090
So from what we have seen.

28
00:01:48,460 --> 00:01:53,080
So far in our careers is that we don't
have a data shortage problem because data

29
00:01:53,080 --> 00:01:58,240
today is not the scarce commodity, but we
have a data context or meaning problem,

30
00:01:58,750 --> 00:02:04,990
and that's what I'm going to solve with my
proposed new architecture from converting

31
00:02:05,199 --> 00:02:12,320
observability from a reactive standpoint
to a more proactive and measured approach.

32
00:02:12,820 --> 00:02:15,920
So let's look at why
modern observability fails.

33
00:02:16,029 --> 00:02:20,329
So we all know that observability,
there are three key pillars in

34
00:02:20,329 --> 00:02:24,589
observability, which is metrics, which
tells us essentially what is happening.

35
00:02:24,619 --> 00:02:28,640
Logs tell us why it happened,
and traces where it happened.

36
00:02:29,450 --> 00:02:33,439
And the core problem is that
without the consistent thread

37
00:02:33,439 --> 00:02:38,169
of context, debugging and manual
correlation it becomes really hard.

38
00:02:39,069 --> 00:02:41,439
And this has been actually backed by data.

39
00:02:42,369 --> 00:02:48,999
So 50% of the organizations report siloed
telemetry, and only 33% achieved unified

40
00:02:48,999 --> 00:02:50,829
views across metrics, logs, and traces.

41
00:02:50,829 --> 00:02:55,089
According to the 2023 Observability
for Forecast report by New Relic,

42
00:02:55,719 --> 00:03:00,849
which has been cited let's look at the
paradigm shift that we are observing

43
00:03:00,909 --> 00:03:02,979
and that we have leaned into it.

44
00:03:03,399 --> 00:03:05,289
Leaned into to solve the problem.

45
00:03:06,099 --> 00:03:09,519
So traditional observability
correlation at incident time.

46
00:03:09,579 --> 00:03:13,539
So whenever we get an incident, we
correlate and draw the path of the

47
00:03:13,539 --> 00:03:18,069
whole incident, how it happened, why it
happened, and understand the context.

48
00:03:18,339 --> 00:03:23,379
The minimum time to detect
resolve becomes slower.

49
00:03:23,499 --> 00:03:28,969
However, in the new approach, the minimum
time to detect resolve becomes faster.

50
00:03:29,509 --> 00:03:30,979
There is higher alert fatigue.

51
00:03:31,309 --> 00:03:35,809
On the on-call engineers or an SRE
and reduce developer productivity.

52
00:03:35,809 --> 00:03:39,859
However, on crown contrast this, by
this approach, we feel that there

53
00:03:39,859 --> 00:03:44,689
would be lower alert fatigue, increased
developer productivity, and so on.

54
00:03:45,189 --> 00:03:50,649
So let's look at model context protocol,
which lies at the heart of this whole data

55
00:03:51,099 --> 00:03:54,309
pipeline for the AI enhanced telemetry.

56
00:03:54,699 --> 00:03:57,639
So MCP is an is I think we all have.

57
00:03:58,129 --> 00:04:03,049
Might have worked with it or
heard about it, but MCP is an open

58
00:04:03,049 --> 00:04:06,199
standard that allows developer to
create a secure two-way connection

59
00:04:06,199 --> 00:04:07,969
between data sources and AI tools.

60
00:04:08,239 --> 00:04:13,199
So if we look at the image here, MCP,
there is an MCP server that connects

61
00:04:13,199 --> 00:04:17,374
with different large language models
like Anthropics Cloud, Opus Open, ai,

62
00:04:17,574 --> 00:04:21,879
OSS and then there are different tools
or services that get integrated and.

63
00:04:22,379 --> 00:04:24,599
There are three key components here.

64
00:04:24,749 --> 00:04:28,284
So one is the contextual ETL for ai.

65
00:04:28,784 --> 00:04:33,299
So for any AI model, I think the
most important part is like how do

66
00:04:33,299 --> 00:04:38,609
we provide them the right context and
data so it standardizes the context.

67
00:04:38,609 --> 00:04:42,989
Extraction for multiple data resources
basically defines a schema of what

68
00:04:42,989 --> 00:04:46,380
our data should look like across
different services and resources.

69
00:04:46,770 --> 00:04:48,539
A structured query interface, so that.

70
00:04:49,200 --> 00:04:54,150
The AI can interpret, the AI models
can interpret and trans get transparent

71
00:04:54,150 --> 00:04:55,890
access to these enriched data layers.

72
00:04:56,610 --> 00:05:00,939
The semantic data enrichment embeds
meaningful context directly into the

73
00:05:00,939 --> 00:05:05,289
telemetry signal that we have received for
whether it be metrics, logs, or traces.

74
00:05:05,789 --> 00:05:08,789
So let's deep dive into
the proposed architecture.

75
00:05:09,280 --> 00:05:12,490
This essentially can be called
as a three layer architecture.

76
00:05:12,490 --> 00:05:15,820
Here I have shown a payments
processing microservice.

77
00:05:16,240 --> 00:05:22,109
It could be any microservice or also a
monolith, but essentially it's just here.

78
00:05:22,169 --> 00:05:25,349
It's here for more
easiness of understanding.

79
00:05:25,739 --> 00:05:30,419
So we have an open telemetry exporter
that emits raw metrics, logs, and traces.

80
00:05:31,379 --> 00:05:36,960
Then it goes into the service that does
the hotel instrumentation and the context

81
00:05:36,960 --> 00:05:38,579
enrichment, which is the second box.

82
00:05:38,699 --> 00:05:43,169
And then the enrich telemetry is stored
into, say, a telemetry storage like

83
00:05:43,169 --> 00:05:45,869
influx TB or Prometheus or something.

84
00:05:46,679 --> 00:05:50,519
Then there is an MCP server
and an MCB query layer.

85
00:05:50,819 --> 00:05:56,579
The MCP server acts as an interface
between the influx tvb and the actual AI

86
00:05:56,969 --> 00:05:59,394
analysis engine, which is essentially the.

87
00:06:00,174 --> 00:06:04,974
Large language models, fine tune
for specific observability, and

88
00:06:04,974 --> 00:06:08,394
then there are dashboards on top
of that based on, again, APIs.

89
00:06:08,444 --> 00:06:13,294
I think today Grafana we'll see for
in the coming slides that Grafana

90
00:06:13,294 --> 00:06:17,794
and other have good integration
with any of the large language

91
00:06:17,794 --> 00:06:19,984
models and the API interface layers.

92
00:06:20,404 --> 00:06:25,624
So the goal here, and the key insight here
is to shift observability from a reactive

93
00:06:25,624 --> 00:06:27,694
problem solving to proactive insights.

94
00:06:28,194 --> 00:06:32,564
Let's do an implementation deep type
without further a so the layer one as

95
00:06:32,564 --> 00:06:34,514
we spoke about is context enrichment.

96
00:06:34,784 --> 00:06:39,704
So I have here shown a code example
where again, this has been simplifying

97
00:06:39,704 --> 00:06:42,014
and obfuscated for understanding.

98
00:06:42,014 --> 00:06:45,734
So for example, at step one
we generate the correlation

99
00:06:45,999 --> 00:06:47,474
IDs for context propagation.

100
00:06:47,715 --> 00:06:51,044
So here we look at the process
checkout for a payment.

101
00:06:51,354 --> 00:06:53,634
Gateway, like PayPal or Stripe.

102
00:06:54,264 --> 00:06:59,904
And then we generate here the order ID and
the request id, and then we initialize a

103
00:06:59,904 --> 00:07:02,484
context dictionary to enrich the spans.

104
00:07:03,384 --> 00:07:08,604
Another interesting thing here is that I
am going to give you a walkthrough on how.

105
00:07:09,084 --> 00:07:15,224
Traces for distributed tracing are being
enriched, and how we have used this to

106
00:07:15,974 --> 00:07:20,714
reduce MTTR and con and basically adopt
this and lean into this AI enriched

107
00:07:20,954 --> 00:07:22,694
data pipeline for observability.

108
00:07:22,964 --> 00:07:27,464
So the key idea here is that the ev
every telemetry signals contains the same

109
00:07:27,464 --> 00:07:31,304
core contextual data, which we can see
here, and it can depend, vary from you.

110
00:07:31,394 --> 00:07:32,814
Case case by case.

111
00:07:33,084 --> 00:07:36,834
So user id order, id request id,
card item count, payment method,

112
00:07:36,834 --> 00:07:38,424
service name, and the service version.

113
00:07:38,724 --> 00:07:41,964
So tomorrow if something breaks
or some there, we see some high

114
00:07:41,964 --> 00:07:44,994
latency in the fulfillment service.

115
00:07:44,994 --> 00:07:48,234
Like after or after the payment is
done, there is a fulfillment service.

116
00:07:48,534 --> 00:07:51,324
Then there's a W warehouse
service that actually dispatches.

117
00:07:51,534 --> 00:07:55,234
So we know where actually the
delay is happening or where

118
00:07:55,234 --> 00:07:56,315
the incident has happened.

119
00:07:57,254 --> 00:08:00,434
Now let's see a very simple
example of context enrichment.

120
00:08:00,645 --> 00:08:05,515
So this is a simple, very simple
debugging scenario and we, I've done

121
00:08:05,515 --> 00:08:09,995
a comparative analysis on how traces
how we see error messages without

122
00:08:09,995 --> 00:08:11,945
trace context and vic trace context.

123
00:08:12,215 --> 00:08:15,965
So without trace context, we see something
like payment failed and we get some

124
00:08:15,965 --> 00:08:20,685
error, and then the and it fires an
alert, and then the on-call engineer or

125
00:08:20,685 --> 00:08:24,660
the subject matter experts goes into and
starts looking into logs for that event.

126
00:08:25,310 --> 00:08:26,719
And it takes a lot of time.

127
00:08:26,719 --> 00:08:28,670
It requires background knowledge.

128
00:08:29,010 --> 00:08:34,380
However, in the enriched case, we see
that okay, there is an error at the same

129
00:08:34,380 --> 00:08:38,760
time, but we see that the request IDs,
their order IDs, their user ID service.

130
00:08:39,030 --> 00:08:43,840
So even in the first layer as
different dimensions, we see that

131
00:08:43,930 --> 00:08:46,090
these traces have been enriched.

132
00:08:46,910 --> 00:08:47,635
So what?

133
00:08:47,900 --> 00:08:52,400
What it does immediately is that
instant correlation via the request id.

134
00:08:52,400 --> 00:08:56,960
We can backtrack this request ID
and figure out where we went wrong.

135
00:08:57,290 --> 00:09:00,200
Service identification, the
user assessment, and automatic

136
00:09:00,200 --> 00:09:01,820
trace assembly, essentially.

137
00:09:02,180 --> 00:09:07,155
So we can see here how context enrichment
helps with debugging scenarios with a

138
00:09:07,160 --> 00:09:11,450
very simple example, and this is very
simple and you can adopt it and make it

139
00:09:11,450 --> 00:09:13,880
more versatile for different use cases.

140
00:09:14,780 --> 00:09:18,260
Layer two is the MCP server, which
is the structured query interface,

141
00:09:18,290 --> 00:09:23,390
which acts like an interface between
the, as I said in the diagram,

142
00:09:23,390 --> 00:09:28,430
the influx TB or the telemetry
storage database and the AI models.

143
00:09:28,790 --> 00:09:34,555
So here we do three things essentially
and step by step, we index we index

144
00:09:34,575 --> 00:09:39,090
the telemetry spans based off our
efficient lookups via context field.

145
00:09:39,480 --> 00:09:43,720
We do filtering, based on precise
data segregation so that like we

146
00:09:43,720 --> 00:09:45,100
apply different time-based filters.

147
00:09:45,600 --> 00:09:49,045
There could be different other
filters based on different dimensions.

148
00:09:49,375 --> 00:09:53,465
Like for example, we know that during
Black Friday there are some deals

149
00:09:53,465 --> 00:09:55,385
that are happening on amazon.com.

150
00:09:55,625 --> 00:09:59,045
So we might know that, okay, these
are Black Friday lightning deals and

151
00:09:59,045 --> 00:10:02,405
we want to track what's if something
goes wrong, how do we solve it?

152
00:10:02,705 --> 00:10:04,805
So there is filter filtering.

153
00:10:05,075 --> 00:10:07,505
And then at the last step
there is aggregation.

154
00:10:07,895 --> 00:10:11,075
So this is very simple the
results for login results.

155
00:10:11,540 --> 00:10:16,510
If log context and this and request
ID is equal to the given request id.

156
00:10:16,840 --> 00:10:20,710
We do a bunch of things and then
we apply a time-based filter.

157
00:10:21,210 --> 00:10:25,525
Then this is the heart of the
whole architecture here is

158
00:10:25,525 --> 00:10:27,685
the AI driven analysis engine.

159
00:10:28,075 --> 00:10:31,735
So it defines the analysis
time window in step one.

160
00:10:32,060 --> 00:10:36,510
Basically when we want to analyze,
and this is exposed via an API on

161
00:10:36,510 --> 00:10:40,680
the Grafana dashboard or wherever
you are viewing your telemetry.

162
00:10:40,860 --> 00:10:45,600
Like it could be for tracers, it could be
Zipkin or Yeager, but again, this is just

163
00:10:45,600 --> 00:10:49,290
the internal heart of what the API does.

164
00:10:49,950 --> 00:10:52,950
So we define an analysis
window which the user provides.

165
00:10:53,190 --> 00:10:57,330
We extract the relevant telemetry based
on the context current one context.

166
00:10:57,540 --> 00:11:00,450
So we extract here metric names by values.

167
00:11:00,450 --> 00:11:04,200
We extract here traces and
whatnot, and logs as well.

168
00:11:04,770 --> 00:11:09,210
And then we calculate statistical pro
properties to capture anomalies and AI

169
00:11:09,210 --> 00:11:13,110
recommendations, which is this part,
which is essentially we are using Z-score

170
00:11:13,480 --> 00:11:15,580
and checking the standard deviation.

171
00:11:16,050 --> 00:11:20,155
To calculate and see if there is any
anomaly in the current traces and

172
00:11:20,155 --> 00:11:21,630
the telemetry that we are seeing.

173
00:11:22,230 --> 00:11:25,950
So here, context enables
automatic cross signal analysis

174
00:11:25,950 --> 00:11:27,240
without manual intervention.

175
00:11:27,240 --> 00:11:31,380
So the from, so for from the user
perspective, it would just be that you

176
00:11:31,380 --> 00:11:36,180
enter the timestamp just like you do
for looking into traces or logs and we,

177
00:11:36,180 --> 00:11:40,470
and basically the whole model and this
whole infrastructure that is there,

178
00:11:40,500 --> 00:11:41,670
pipeline is there, would give you.

179
00:11:42,225 --> 00:11:45,465
Insights that, okay, this is what
has happened in the past seven days.

180
00:11:45,735 --> 00:11:51,045
Rani, right now we are seeing it as
an anomaly or not and what, or whether

181
00:11:51,045 --> 00:11:55,315
it's the right whether it's right
and everything looks good essentially

182
00:11:55,315 --> 00:11:57,205
from latency wise or error wise.

183
00:11:58,075 --> 00:12:00,390
So what is the impact
that we have observed?

184
00:12:00,915 --> 00:12:05,715
So a real world impact from our,
we've seen like the time that has

185
00:12:05,715 --> 00:12:07,605
been reduced from ours, two minutes.

186
00:12:07,935 --> 00:12:11,775
So before adopting this and leaning
into this pipeline, we saw that

187
00:12:11,835 --> 00:12:15,370
approximately 45 minutes times were
spent in average and resolution.

188
00:12:16,345 --> 00:12:21,565
We had to do manual law correlation,
multiple context switches between tools,

189
00:12:21,655 --> 00:12:25,165
tribal knowledge required for developers,
and a lot of high cognitive load.

190
00:12:25,465 --> 00:12:29,155
However, after this, we've seen
approximately average our MTTR

191
00:12:29,215 --> 00:12:30,715
being reduced to eight minutes.

192
00:12:31,365 --> 00:12:34,485
Automatic and because, and it's
been reduced because of automatic

193
00:12:34,485 --> 00:12:38,565
correlation, a single pane of glass
unified view in observability.

194
00:12:38,565 --> 00:12:42,015
There is a silver bullet term called
single pane of glass view or unified

195
00:12:42,465 --> 00:12:45,560
observability, and this allows
us to do unified observability.

196
00:12:46,395 --> 00:12:50,880
We also get context of insights and
AI suggested and enhanced rc, so

197
00:12:51,045 --> 00:12:54,915
you just get a boilerplate RC and
you can work on top of that based

198
00:12:54,915 --> 00:12:58,615
on the AI model's understanding of
your context and how well you have.

199
00:12:59,120 --> 00:13:02,510
Propagated your context and
given basically populated

200
00:13:02,510 --> 00:13:04,360
the enrichment dictionary.

201
00:13:04,840 --> 00:13:10,360
So we saw how we have seen 82% reduction
in the MTTR 65% reduction in alert

202
00:13:10,360 --> 00:13:13,660
fatigue and 3.2 x faster duction.

203
00:13:13,660 --> 00:13:18,190
Again, this has been right
now in more of a data phase

204
00:13:18,550 --> 00:13:20,765
within for the implementation.

205
00:13:21,035 --> 00:13:23,555
But again, we see massive
potential here for.

206
00:13:23,930 --> 00:13:24,680
Adopting it.

207
00:13:24,950 --> 00:13:29,100
And recently a lot of companies like open
source companies like Elastic Search,

208
00:13:29,100 --> 00:13:33,960
have been using Elastic Stream that allows
the same thing to happen, like for logs.

209
00:13:33,960 --> 00:13:38,850
They can do context propagation and
context enrichment in the L Clear.

210
00:13:39,200 --> 00:13:42,830
But it is again, a vendor
specific, however, by this

211
00:13:42,830 --> 00:13:44,300
way, which is open source.

212
00:13:44,300 --> 00:13:48,890
You can be vendor agnostic and
do a lot of enrichment and AI

213
00:13:48,890 --> 00:13:50,420
driven analysis on your end.

214
00:13:50,480 --> 00:13:53,960
And of course, use Elasticsearch
for specific problem solving

215
00:13:54,500 --> 00:13:56,870
because again, that's also paper.

216
00:13:56,920 --> 00:13:58,435
API pay pay as you go.

217
00:13:58,645 --> 00:14:01,885
So the cost also adds
up given observability.

218
00:14:01,915 --> 00:14:03,805
Anyways, a very costly business.

219
00:14:04,375 --> 00:14:09,115
So I think the question that was raised
was, this seems like a lot of overhead.

220
00:14:09,115 --> 00:14:13,495
I think that would come through your mind
as well, but it came to our mind as well

221
00:14:13,495 --> 00:14:15,175
when we were doing the decision making.

222
00:14:15,585 --> 00:14:19,195
Is the complexity worth the
effort that we are spending?

223
00:14:19,495 --> 00:14:22,375
So what's, what are, so
let's compare to scenarios.

224
00:14:22,405 --> 00:14:24,745
What are the hidden costs
that you're already paying?

225
00:14:25,105 --> 00:14:29,095
So usually we, like, whenever we do
capacity planning, we do put like around

226
00:14:29,185 --> 00:14:34,515
20% of an engineer's time on, on-call
workload or like engineering efficiencies,

227
00:14:35,085 --> 00:14:41,055
so which is around 15 to 20 hours per
week, 60 to 70% of alerts are unactionable

228
00:14:41,295 --> 00:14:43,485
or get auto resolved context switching.

229
00:14:43,485 --> 00:14:48,495
So there is a massive amount of
study done here that 23 minutes, it

230
00:14:48,495 --> 00:14:51,780
takes 23 minutes to refocus after
an interruption, whether it be just

231
00:14:51,780 --> 00:14:53,865
be chatting with a colleague or.

232
00:14:54,255 --> 00:14:56,025
Something else for you to get in the zone.

233
00:14:56,025 --> 00:14:58,835
And we all have been heard
by this multiple times.

234
00:14:58,835 --> 00:15:01,205
I have been over my span
of career of 10 years.

235
00:15:01,205 --> 00:15:02,945
I've been heard by it multiple times.

236
00:15:03,305 --> 00:15:06,545
And then tribal knowledge, like
critical dependencies on certain

237
00:15:06,545 --> 00:15:09,975
individuals, and you become the
bottleneck in the whole process.

238
00:15:10,305 --> 00:15:15,015
However, if we invest in this, so the,
if this is again, a one time setup and

239
00:15:15,015 --> 00:15:17,505
it could just be, iterative increments.

240
00:15:17,505 --> 00:15:19,665
You don't have to boil the ocean at once.

241
00:15:19,725 --> 00:15:24,765
You can just implement one service and
focus on that, which is your most critical

242
00:15:24,765 --> 00:15:28,545
service, and then later evolve your
learning from it and implement it across.

243
00:15:28,545 --> 00:15:31,415
So initially you might have to spend
two to three weeks, I would say

244
00:15:31,415 --> 00:15:33,335
even four weeks if starting out new.

245
00:15:33,385 --> 00:15:34,495
Incremental adoption.

246
00:15:34,495 --> 00:15:38,005
Start with critical services, permanent
against, replicate to every service.

247
00:15:38,005 --> 00:15:42,955
Once you implement it, it's easily
replica, replicable to any other service.

248
00:15:42,985 --> 00:15:47,395
And the break even time is around six to
eight weeks because given like everybody's

249
00:15:47,395 --> 00:15:51,505
spending 15 to 20 hours, that time would
drastically reduced to five, four to

250
00:15:51,505 --> 00:15:53,845
five hours, right around 80% reduction.

251
00:15:54,295 --> 00:15:56,000
So in six to eight weeks or maybe.

252
00:15:56,500 --> 00:16:00,600
10 weeks you get your ROI or the
time spent in resource allocation

253
00:16:00,990 --> 00:16:04,950
and engineer time spent in
designing this whole infrastructure.

254
00:16:05,940 --> 00:16:09,380
So the key insight here is
that the complexity you add

255
00:16:09,380 --> 00:16:10,735
at generation time eliminates.

256
00:16:11,630 --> 00:16:14,300
Exponentially more
complexity at analysis time.

257
00:16:14,910 --> 00:16:18,840
So yeah, you are doing laying up
the foundation, but later on when

258
00:16:19,020 --> 00:16:23,140
the incidents really happen the
complexity and the time spent and

259
00:16:23,170 --> 00:16:25,420
alert fatigue gets reduced drastically.

260
00:16:26,350 --> 00:16:27,730
So what are the key takeaways?

261
00:16:27,730 --> 00:16:32,320
So we now get into what have been
the key takeaways for us when we have

262
00:16:32,320 --> 00:16:35,050
implemented it and others have adopted it.

263
00:16:35,410 --> 00:16:39,080
So first insight is that
start with context enrichment.

264
00:16:39,950 --> 00:16:43,010
Again, as I said previously,
big begin small and think big.

265
00:16:43,010 --> 00:16:45,710
Don't implement the full three
layer architecture at once.

266
00:16:46,100 --> 00:16:48,260
Just start by enriching
one critical service.

267
00:16:48,845 --> 00:16:52,355
So minimum viable context,
like essentially says,

268
00:16:52,355 --> 00:16:54,575
what are your crucial tags?

269
00:16:54,605 --> 00:16:59,105
It could just be for cost tracking
that you are track tracking a team's

270
00:16:59,165 --> 00:17:04,835
id, for example, an owner, team
owner tag, or like a service id,

271
00:17:04,835 --> 00:17:05,835
and you know that this service.

272
00:17:06,335 --> 00:17:07,595
Really heavy load.

273
00:17:07,985 --> 00:17:11,285
So just start with the minimum
viable context and also the

274
00:17:11,285 --> 00:17:13,205
minimum viable implementation.

275
00:17:13,685 --> 00:17:17,685
So for that, we do we add dictionary
context dictionary to your logging

276
00:17:17,685 --> 00:17:22,065
setup, propagate context through
service calls, include context in oal

277
00:17:22,065 --> 00:17:26,835
spans, validate context appears in all
signals, and expand to other server.

278
00:17:27,660 --> 00:17:33,300
So the first insight is that for
one, your week one goal is that pick

279
00:17:33,300 --> 00:17:36,780
one high traffic service and add
basic context enrichment on that.

280
00:17:36,810 --> 00:17:40,960
You don't even have to add the, the
analysis engine layer, the AI layer.

281
00:17:41,590 --> 00:17:46,120
The second insight is that
it's very essential for us to.

282
00:17:46,570 --> 00:17:50,740
Make sure that our data is variable and
it can, we can draw insights from that.

283
00:17:51,290 --> 00:17:55,070
So make the data AI ready even without
the full implementation, create

284
00:17:55,070 --> 00:17:57,320
structured APIs over all of a telemetry.

285
00:17:57,320 --> 00:18:01,090
The thing that we saw previously,
based on request ID, is based

286
00:18:01,090 --> 00:18:05,010
on timestamp filtering create
the APIs and integrated with.

287
00:18:05,510 --> 00:18:09,470
Dashboards, wherever you view your
telemetry, whether it be Grafana,

288
00:18:09,470 --> 00:18:11,180
zip gain, Kibana, anything.

289
00:18:12,050 --> 00:18:13,580
So the what are the benefits?

290
00:18:13,580 --> 00:18:17,480
So programmatic access, consistent
interfaces and AI friendly formats.

291
00:18:18,230 --> 00:18:22,160
And here I've provided some simple
rest API patterns for the payment

292
00:18:22,160 --> 00:18:23,450
service, which we used as an.

293
00:18:23,950 --> 00:18:27,160
So the quick, the second insight
is, and the quick win from here is

294
00:18:27,160 --> 00:18:31,470
that wrap existing observability
tools with simple query APIs as we

295
00:18:31,470 --> 00:18:35,520
spoke about, so that filtering and
querying that data becomes easier.

296
00:18:36,300 --> 00:18:40,690
So insight three, I think this is a
very, very well known fact and software

297
00:18:40,720 --> 00:18:45,550
engineering and also product development
is that we always iterate with feedback

298
00:18:45,550 --> 00:18:47,440
from either customers or from within.

299
00:18:47,920 --> 00:18:50,170
So from within we say
like operational feedback.

300
00:18:50,560 --> 00:18:53,970
So context is a living system and
because context changes is every

301
00:18:53,970 --> 00:18:57,155
day the best schema emerges from
real world incident patterns.

302
00:18:57,160 --> 00:19:02,070
So I would say like what we have
basically lean into is that we measure.

303
00:19:03,060 --> 00:19:04,350
Refine and expand.

304
00:19:04,410 --> 00:19:09,000
And what I mean by that is that we
track which context field engineers

305
00:19:09,090 --> 00:19:10,680
actually use during incidents.

306
00:19:11,250 --> 00:19:15,120
And if those fields are missing, we add
those and remove the unused fields so

307
00:19:15,120 --> 00:19:21,290
that we are not adding essentially like
raw data, which is not very actionable.

308
00:19:21,410 --> 00:19:23,240
Fraud, fraud, even the AI models.

309
00:19:23,240 --> 00:19:26,900
And then expand that learning to
different services incrementally.

310
00:19:27,530 --> 00:19:31,100
So the anti-patent to avoid here
is that don't try to design the

311
00:19:31,100 --> 00:19:32,570
perfect contact schema upfront.

312
00:19:32,720 --> 00:19:37,790
Let on-call incident and operational
load guide really guide you into

313
00:19:37,790 --> 00:19:40,310
designing the contact schema.

314
00:19:40,810 --> 00:19:41,680
What's next?

315
00:19:41,980 --> 00:19:47,080
So let's see, like what are we planning
for the future and then what are

316
00:19:47,080 --> 00:19:48,670
the enhancements that we want to do?

317
00:19:49,240 --> 00:19:52,510
So industry adoption and scalability.

318
00:19:52,760 --> 00:19:58,070
Here we talk about like the different
phases in which like somebody wants

319
00:19:58,070 --> 00:20:01,940
to implement this, how would they
implement it and how have we thought

320
00:20:01,940 --> 00:20:06,480
about implementing it and also
getting returns from our investment.

321
00:20:06,900 --> 00:20:11,130
So firstly, the solution scales
from startups to enterprises.

322
00:20:11,250 --> 00:20:17,600
There is no one size fits all but
essentially here, like anybody

323
00:20:17,600 --> 00:20:19,325
who uses this solution can.

324
00:20:19,825 --> 00:20:22,225
More fit to their specific needs.

325
00:20:22,705 --> 00:20:26,155
So it's ideal for organizations
with microservice architectures,

326
00:20:26,155 --> 00:20:30,625
cloud native deployments, systems
at scale, having a very cluttered

327
00:20:30,625 --> 00:20:35,085
non-uniform observability stack, alert
fatigue problems and long mean time

328
00:20:35,085 --> 00:20:37,875
to detect and resolve essentially.

329
00:20:38,355 --> 00:20:42,885
So phase one, as I said, is like context
enrichment, which is the key part Then.

330
00:20:43,785 --> 00:20:49,335
Query layer, which is phase two, the basic
APIs that you have, and then AI analysis,

331
00:20:49,635 --> 00:20:54,145
anomaly direction layer basically, which
and then scale for scale across services,

332
00:20:54,145 --> 00:20:56,275
and then do a full MCP implementation.

333
00:20:56,635 --> 00:21:02,365
So till here, I think a lot of folks like
till phase four, a lot of folks and we

334
00:21:02,365 --> 00:21:07,465
all have understanding of how things work
and this we can move fast even before

335
00:21:07,465 --> 00:21:10,765
into getting, before getting into the
complexity of the full MCP implementation.

336
00:21:11,265 --> 00:21:13,455
So what's the future of observability?

337
00:21:13,455 --> 00:21:18,945
And what we are seeing today is that
observability is shifting massively from,

338
00:21:19,045 --> 00:21:25,145
error prone like fatigued, reactive to
a more optimized, proactive approach.

339
00:21:25,595 --> 00:21:29,525
So traditional observability, we wait
for alerts to happen, hunt root for

340
00:21:29,525 --> 00:21:31,925
root causes, and do manual correlations.

341
00:21:32,195 --> 00:21:33,725
We do reactive firefighting and.

342
00:21:33,905 --> 00:21:37,595
Fatigued on-call engineers, like
on-call becomes usually a nightmare.

343
00:21:38,135 --> 00:21:42,975
However, in this new paradigm where we
have AI powered observability which is

344
00:21:42,975 --> 00:21:48,445
full of context enriched context, so we
predict issues before impact, even if we

345
00:21:48,445 --> 00:21:52,865
could solve like around 20% of the issues
and that is happening again and again,

346
00:21:52,895 --> 00:21:57,965
and we can predict them, it becomes very
easy for us for engineers to scale it.

347
00:21:58,185 --> 00:22:00,195
If the load is predicted
to increase, like you can.

348
00:22:00,695 --> 00:22:01,775
Hyperactively scale it.

349
00:22:02,345 --> 00:22:06,095
You can also automate it, if you
have an MCP server, like it can scale

350
00:22:06,095 --> 00:22:09,995
on your behalf but again, it can
be manual or it can be automated.

351
00:22:09,995 --> 00:22:11,525
So AI suggested root causes.

352
00:22:11,525 --> 00:22:14,765
So if you get a boilerplate of the
root cause, you at least get what

353
00:22:14,765 --> 00:22:18,635
is the hypothesis and deep dive into
that For creating the public, facing

354
00:22:18,635 --> 00:22:21,755
RCS usually automatic correlation.

355
00:22:21,915 --> 00:22:24,255
Of course proactive optimization.

356
00:22:24,435 --> 00:22:27,055
You get and the on-call engineers are.

357
00:22:27,430 --> 00:22:32,075
Empower and they can focus on
better things rather than solving

358
00:22:32,135 --> 00:22:33,575
the same problem repetitively.

359
00:22:34,295 --> 00:22:40,085
So from what we've see, from what we
are seeing is that observability problem

360
00:22:40,085 --> 00:22:44,465
is fundamentally a data problem, and it
should be solved at the data layer, not at

361
00:22:44,465 --> 00:22:46,565
the insights layer or the inference layer.

362
00:22:46,955 --> 00:22:49,360
So we should try to solve
that at the data layer itself.

363
00:22:49,860 --> 00:22:50,220
Yeah.

364
00:22:50,400 --> 00:22:50,790
Thank you.

365
00:22:50,790 --> 00:22:54,500
Hope, hope you find this
presentation insightful.

366
00:22:54,980 --> 00:22:57,560
Feel free to follow me
on LinkedIn and Twitter.

367
00:22:57,560 --> 00:22:59,450
And these are my socials.

368
00:22:59,880 --> 00:23:00,170
I have just.

369
00:23:00,550 --> 00:23:04,780
Put the sample code and references,
and I've made it open source on

370
00:23:04,780 --> 00:23:09,610
my GitHub, so feel free to start
it or use it and raise issues.

371
00:23:10,040 --> 00:23:12,710
If you want to reach out to
me there, here's my email.

372
00:23:12,800 --> 00:23:17,670
But yeah, it was really glad presenting
this in front of you folks and I hope.

373
00:23:18,100 --> 00:23:23,590
You all have a great DevOps 2026,
and I thank Con 42 and Mark for the

374
00:23:23,590 --> 00:23:27,670
opportunity to present, and I hope
you guys liked it, so signing off.

375
00:23:27,760 --> 00:23:28,240
Thank you everyone.

376
00:23:28,740 --> 00:23:28,860
I.

