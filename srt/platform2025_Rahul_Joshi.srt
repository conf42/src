1
00:00:00,500 --> 00:00:01,130
Hello everyone.

2
00:00:01,250 --> 00:00:02,240
I'm Rahul Joshi.

3
00:00:02,809 --> 00:00:04,220
Thank you for attending my session.

4
00:00:04,530 --> 00:00:06,420
I'm going to talk about
platform engineering for

5
00:00:06,420 --> 00:00:07,710
modern data infrastructure.

6
00:00:08,445 --> 00:00:13,615
Where we are gonna see how the data
infrastructure has evolved over last 15,

7
00:00:13,615 --> 00:00:18,955
20 years, what all things have emerged
as a new technologies and capabilities

8
00:00:18,955 --> 00:00:23,725
and how the organizations are generating
and consuming data at large scale.

9
00:00:23,725 --> 00:00:26,080
And that has created a need.

10
00:00:26,625 --> 00:00:31,395
For platform engineering, we look
at these systems as not as isolated

11
00:00:31,395 --> 00:00:37,365
platforms or components in the overall
enterprise, but look at it as integrated

12
00:00:37,365 --> 00:00:43,075
platforms, self-service, large scale,
multi-tenant, right access controls to

13
00:00:43,075 --> 00:00:47,545
enable all the business capabilities of
the latest and greatest capabilities.

14
00:00:48,045 --> 00:00:49,745
That these platforms, enable.

15
00:00:49,745 --> 00:00:54,285
So let's take a deep dive and first
we are gonna start with the evolution

16
00:00:54,285 --> 00:00:55,875
of data platform architecture itself.

17
00:00:56,505 --> 00:01:00,945
So as you can see, I'm showing
three key here on timeline

18
00:01:01,145 --> 00:01:03,125
only big data era there around.

19
00:01:03,710 --> 00:01:09,510
2006, 2007 Loop was, emerged
as a, as an outcome of the

20
00:01:09,930 --> 00:01:12,420
Google's HDFS and MAP newspaper.

21
00:01:12,510 --> 00:01:17,070
And a lot of that has triggered, that, had
triggered a lot of new use cases where,

22
00:01:17,070 --> 00:01:22,200
companies started to build big data lakes,
ingest a lot of data into the lakes and

23
00:01:22,200 --> 00:01:26,330
process all that data by maintaining
their own complex clusters, right?

24
00:01:26,360 --> 00:01:27,860
Clusters of commodity hardware.

25
00:01:28,010 --> 00:01:29,570
The principle for Hadoop was.

26
00:01:29,955 --> 00:01:36,045
It used to run commodity hardware and the
platform engineers used to focus primarily

27
00:01:36,045 --> 00:01:38,025
on keeping these systems operational.

28
00:01:38,245 --> 00:01:43,225
So a lot of heavy lifting used to be done
by these enterprise, and ultimately these

29
00:01:43,225 --> 00:01:44,785
teams that are owned by these enterprises.

30
00:01:45,090 --> 00:01:48,920
For platform engineering
later, the cloud has changed.

31
00:01:49,020 --> 00:01:52,825
The infrastructure management part
of it and data platform shifted

32
00:01:52,825 --> 00:01:58,555
towards managed services like WS,
Google and Microsoft Azure Cloud.

33
00:01:58,615 --> 00:02:02,815
There are a lot of software as a service,
conce where emerged Adobe managed services

34
00:02:02,815 --> 00:02:04,585
like Redshift to BigQuery and Snowflake.

35
00:02:05,065 --> 00:02:08,335
That abstracted a lot of
operational complexity, but

36
00:02:08,335 --> 00:02:09,565
it introduced new challenges.

37
00:02:10,280 --> 00:02:13,340
As it comes with any new
technology on cost optimization

38
00:02:13,790 --> 00:02:14,825
and vendor management because.

39
00:02:15,700 --> 00:02:20,420
Once you go on cloud, you have infinite,
compute level access, so you can

40
00:02:20,420 --> 00:02:25,190
use, you have to use it wisely and
cost optimization, that is key role

41
00:02:25,280 --> 00:02:29,960
on how do you be up to speed on all
the computes and activities you need.

42
00:02:29,960 --> 00:02:32,960
But at the same time, how do you
manage your cost efficiently?

43
00:02:33,115 --> 00:02:37,795
Then third is the Lakehouse architectures
where technologies like Delta Lake,

44
00:02:38,095 --> 00:02:42,835
iceberg and Hoodie are enabling
platform engineers to build systems that

45
00:02:42,835 --> 00:02:47,095
support both analytical and operational
workloads on the same platforms.

46
00:02:47,285 --> 00:02:51,495
Unifying storage with strong
consistency and the right governance.

47
00:02:51,995 --> 00:02:55,535
Each architectural phase here has
contributed important lessons and

48
00:02:55,535 --> 00:02:59,135
has influenced how do we design
and operate data platforms today.

49
00:02:59,615 --> 00:03:03,395
So we are gonna focus on the modern
architecture and look at this thing

50
00:03:03,455 --> 00:03:07,234
on that lens of what are some of
the things platform engineers have

51
00:03:07,285 --> 00:03:11,185
to be aware of or consider while
designing these new platforms based

52
00:03:11,185 --> 00:03:12,535
on the new architecture patterns.

53
00:03:13,045 --> 00:03:16,075
So let's look at some of the core
platform engineering principles here.

54
00:03:16,135 --> 00:03:17,725
If you're building a data platform set.

55
00:03:18,549 --> 00:03:19,510
Enterprise scale.

56
00:03:19,780 --> 00:03:22,630
You need abstraction and interface design.

57
00:03:22,810 --> 00:03:24,340
You have different types of personnel.

58
00:03:25,030 --> 00:03:25,990
There are business users.

59
00:03:25,990 --> 00:03:27,460
There are technical users, right?

60
00:03:27,460 --> 00:03:31,840
They're engineers and these platforms,
data platforms today, which at the heart,

61
00:03:32,060 --> 00:03:36,629
heart of the company data is, powering
so many business capability for, so you

62
00:03:36,629 --> 00:03:41,189
need abstract all the complexities that
are, that goes into the implementation

63
00:03:41,189 --> 00:03:43,039
of the platform and make the interface.

64
00:03:43,189 --> 00:03:47,920
And they know easy for all these personas
so that they can come and use this data

65
00:03:47,920 --> 00:03:52,630
platform capabilities, be it engineer or
data scientist or analyst, business user.

66
00:03:52,749 --> 00:03:57,500
We need to balance what capabilities
we enable to these end users as

67
00:03:57,500 --> 00:04:00,950
part of user experience and what
capabilities we, we abstract as

68
00:04:00,950 --> 00:04:02,269
part of complex implementation.

69
00:04:02,450 --> 00:04:03,529
Second is self-service.

70
00:04:03,724 --> 00:04:09,085
So all these data platforms must provision
resources, deploy, pipelines, right?

71
00:04:09,085 --> 00:04:11,665
Ability to create
resources, use them, right?

72
00:04:11,785 --> 00:04:14,725
Exposition them again, and access data.

73
00:04:14,905 --> 00:04:19,615
Use data, access data as it requires
in a self-service fashion because you

74
00:04:19,615 --> 00:04:22,165
are, if you're talking at enterprise
scale, you're talking about thousands

75
00:04:22,165 --> 00:04:26,335
of users and applications, human
users and applications using these

76
00:04:26,725 --> 00:04:28,615
shared multi-tenant data platform.

77
00:04:28,615 --> 00:04:31,105
So service capabilities
plays critical role there.

78
00:04:31,110 --> 00:04:32,270
Where is infrastructure as code?

79
00:04:32,735 --> 00:04:37,595
Because we are in cloud era now, all
these, a lot of different interconnected

80
00:04:37,595 --> 00:04:41,675
components that must work together
seamlessly as part of cohesive solution.

81
00:04:42,215 --> 00:04:47,225
So platform engineers must they, a lot
of people use infrastructure as a code

82
00:04:47,315 --> 00:04:50,165
to deploy and create resources, right?

83
00:04:50,165 --> 00:04:52,595
In instant deploy your applications.

84
00:04:52,685 --> 00:04:56,284
I update it, but this, these
capabilities need to be.

85
00:04:56,675 --> 00:04:57,665
Used at scale.

86
00:04:57,815 --> 00:05:01,355
And you also need observability and
monitoring sophisticated capabilities

87
00:05:01,355 --> 00:05:05,375
there so that you keep your data
platforms healthy and you maintain

88
00:05:05,375 --> 00:05:07,440
the quality of data these platforms.

89
00:05:07,830 --> 00:05:10,980
And at the same time, look at
how the data is flowing through.

90
00:05:11,490 --> 00:05:15,420
If there are any concerns like latency,
freshness, and schema evolution, you are

91
00:05:15,420 --> 00:05:19,730
able to, look at these things and the
data is flowing through at runtime and

92
00:05:19,730 --> 00:05:22,070
detect some of the problems proactively.

93
00:05:22,270 --> 00:05:26,440
Then, and, reacting to it later when
some application fails in production.

94
00:05:26,469 --> 00:05:30,660
So these are some of the core principles
or modern platform engineering actually

95
00:05:30,660 --> 00:05:36,015
look at if you're gonna do build the data
platform or developer self-service, right?

96
00:05:36,015 --> 00:05:38,114
How do you create truly
self-service data platform?

97
00:05:38,724 --> 00:05:42,445
That needs a fundamental shift in how
platform engineers think about user

98
00:05:42,445 --> 00:05:44,484
interface and developer experience.

99
00:05:44,905 --> 00:05:49,614
The goal is to enable data teams work
independently while ensuring their actions

100
00:05:49,614 --> 00:05:51,715
aligned with organization standards.

101
00:05:51,715 --> 00:05:55,375
So first, platform APIs
and developer interfaces.

102
00:05:55,614 --> 00:05:58,284
You need well-defined,
well-defined platform api.

103
00:05:58,724 --> 00:06:03,304
To provide programmatic access
to the platform capabilities.

104
00:06:03,484 --> 00:06:06,914
These APIs could be rest, or
similar standards, which are well

105
00:06:06,914 --> 00:06:08,505
established industry standards.

106
00:06:08,505 --> 00:06:12,415
So you are keeping your applications
later aligned with latest and greatest.

107
00:06:12,985 --> 00:06:16,445
And you, you should have clear
documentation and versioning strategies

108
00:06:16,445 --> 00:06:18,745
of when the schema changes for all these.

109
00:06:18,965 --> 00:06:23,805
APIs or contracts between the
data producers and data consumers

110
00:06:23,895 --> 00:06:26,175
and the platform capabilities,
it shouldn't be an issue.

111
00:06:26,325 --> 00:06:28,065
Second is standardized data processing.

112
00:06:28,215 --> 00:06:31,245
So there are, if you think about
enterprise, there are so many

113
00:06:31,245 --> 00:06:35,505
different applications and personas
and if they, if it's a shared, their

114
00:06:35,505 --> 00:06:37,755
tenant, multi-tenant data platform.

115
00:06:38,020 --> 00:06:43,590
Then, every use case needs to read
data, get data out, all the data that's

116
00:06:43,590 --> 00:06:47,110
required, do some standardization,
do some processing on that data,

117
00:06:47,530 --> 00:06:49,240
and then, create some business.

118
00:06:49,890 --> 00:06:52,450
Related, meaningful insight
out of that data, right?

119
00:06:52,450 --> 00:06:56,880
So there is, we need some if we do
not have standardization, standardized

120
00:06:56,880 --> 00:07:00,780
data processing patterns across
these different use cases, every

121
00:07:00,780 --> 00:07:04,380
use case will do some redundant data
processing in their own, application.

122
00:07:04,480 --> 00:07:10,450
So standardizing these data processing
patterns is the key to avoid.

123
00:07:10,690 --> 00:07:14,730
All the redundant data processing
and joints and, can be joints as

124
00:07:14,730 --> 00:07:18,560
an example, but it can be, reading
some data, maybe flattening it

125
00:07:18,560 --> 00:07:19,820
and things of that nature, right?

126
00:07:19,820 --> 00:07:23,505
So you, the, you don't want to do
all you, you don't want all your.

127
00:07:24,005 --> 00:07:26,615
All your applications to do
some of these redundant things.

128
00:07:27,275 --> 00:07:31,595
So you have to centralize those and
create consistent data processing

129
00:07:31,595 --> 00:07:36,665
patterns, standardize it so everybody
can use, reuse whatever is available

130
00:07:36,665 --> 00:07:41,015
out there, and then build their
own use case specific applications

131
00:07:41,135 --> 00:07:42,845
or implementations as required.

132
00:07:43,445 --> 00:07:45,875
Third is data discovery
and catalog services.

133
00:07:46,265 --> 00:07:50,305
So how are you going to find what
data is available out there for you?

134
00:07:51,085 --> 00:07:55,675
If you're working at enterprise scale,
you might have thousands of applications

135
00:07:56,455 --> 00:08:01,305
and hundreds of systems producing data
every day, and that's a shared tenant.

136
00:08:01,305 --> 00:08:04,815
So you have different lines of
businesses using same data platforms.

137
00:08:05,295 --> 00:08:10,455
So identifying data, it, having a shared
catalog of all these data sets, making

138
00:08:10,455 --> 00:08:15,465
data easy to find for all these different
applications and humans is key for

139
00:08:15,525 --> 00:08:17,570
developing the self-service data platform.

140
00:08:18,070 --> 00:08:21,460
Next key thing is, big thing is
resource management and cost control.

141
00:08:21,550 --> 00:08:26,885
As we discussed earlier, if your platform
is self-service, then users can do.

142
00:08:27,385 --> 00:08:30,715
Whatever they have access
to, they'll create resources.

143
00:08:30,715 --> 00:08:36,295
They'll run queries viewed as they want to
get their results quickly as, as quickly

144
00:08:36,295 --> 00:08:38,035
as possible if they have right access.

145
00:08:38,035 --> 00:08:42,835
So how are we So resource management
becomes key because you, you can

146
00:08:42,835 --> 00:08:44,845
have practically have infinite skill.

147
00:08:45,410 --> 00:08:47,750
Compute and storage when you're on cloud.

148
00:08:48,200 --> 00:08:53,460
So this is, managing your costs and
processing workloads where for processing

149
00:08:53,460 --> 00:08:57,300
workloads, where they consume a lot
of significant resources and compute

150
00:08:57,390 --> 00:09:00,660
our resource management and cost
controls becomes another key dimension

151
00:09:01,160 --> 00:09:03,170
for designing modern data platforms.

152
00:09:03,320 --> 00:09:06,320
It's looking at multi-tenant
architecture and resource isolation.

153
00:09:06,470 --> 00:09:09,170
So multi-tenancy or enterprises
at least, where you have

154
00:09:09,170 --> 00:09:10,430
multiple lines of businesses.

155
00:09:10,700 --> 00:09:12,830
Each lines of business own their own.

156
00:09:13,475 --> 00:09:17,645
Source of system of records have
their own business lifecycle for

157
00:09:17,645 --> 00:09:19,415
their customers or the end customers.

158
00:09:19,415 --> 00:09:20,555
So it generates data.

159
00:09:21,045 --> 00:09:22,305
Produces data, right?

160
00:09:22,480 --> 00:09:26,560
It needs to store data, needs to
ingest data, it needs to process data.

161
00:09:26,560 --> 00:09:27,925
And if you, are they.

162
00:09:28,425 --> 00:09:30,645
They don't want to build, you
don't want to build different data

163
00:09:30,645 --> 00:09:32,895
platforms for each line of businesses.

164
00:09:32,895 --> 00:09:34,355
So for doing the same thing.

165
00:09:34,475 --> 00:09:37,715
So you need, and that's where the
multi-tenancy comes into picture.

166
00:09:37,715 --> 00:09:42,605
So you have to build your data platform
in a way that the capabilities can be

167
00:09:42,605 --> 00:09:44,825
used across different lines of businesses.

168
00:09:45,395 --> 00:09:47,465
IE in tenants in this case.

169
00:09:47,735 --> 00:09:51,594
So each tenant have their own workspace,
have their own right access controls.

170
00:09:51,999 --> 00:09:56,164
Have their own isolation in terms of how
compute resources are allocated to it.

171
00:09:56,224 --> 00:09:59,824
Have the right access control, or
sharing or not sharing data with other

172
00:10:00,394 --> 00:10:02,944
lines of businesses or subsidiaries or.

173
00:10:03,444 --> 00:10:06,654
Country specific geopolitical
restrictions, geographical

174
00:10:06,654 --> 00:10:08,064
restrictions, and so on, so forth.

175
00:10:08,214 --> 00:10:11,904
And at the same time, you need a
storage and performance isolation.

176
00:10:11,904 --> 00:10:16,524
So you need both logical separation
and physical separation, isolation of

177
00:10:16,524 --> 00:10:18,714
dedicated storage systems and compute.

178
00:10:18,954 --> 00:10:23,229
So that had becomes well optimized
and well managed data platform for

179
00:10:23,229 --> 00:10:28,764
a large enterprise where you can use
multi-tenancy to to reuse as much of.

180
00:10:29,394 --> 00:10:33,264
Platform capabilities as spot to
power all these different businesses.

181
00:10:33,764 --> 00:10:38,034
So what are some of the key
cost optimization strategies?

182
00:10:38,994 --> 00:10:41,274
Let's understand some of the cost drivers.

183
00:10:41,274 --> 00:10:45,535
So primary cost usually includes
computer resources for data

184
00:10:45,535 --> 00:10:48,925
processing because when data is at
rest, it is stored on a platform.

185
00:10:49,045 --> 00:10:55,635
So when you run some, query or request
to get it out, the compute is used,

186
00:10:56,145 --> 00:10:59,455
which basically goes and try to read
data out and give it back to you.

187
00:10:59,605 --> 00:11:02,665
So computer resources for data
processing is one of the key factor.

188
00:11:03,115 --> 00:11:04,195
The data is at rest.

189
00:11:04,345 --> 00:11:05,665
Your storage plays another.

190
00:11:06,145 --> 00:11:08,935
Key factor for your raw data
and then the process data.

191
00:11:08,935 --> 00:11:13,255
If you're working on like data products
or in different layers of data, as part

192
00:11:13,255 --> 00:11:16,645
of Alion architecture, you have raw
data, you have refined data, you have,

193
00:11:16,645 --> 00:11:20,455
purpose built use case driven data,
and all this data needs to be stored.

194
00:11:20,605 --> 00:11:24,655
So at rest you're gonna pay a lot of
storage cost and then network cost

195
00:11:24,805 --> 00:11:28,285
at the time of compute, as well as
at the time of data transfer between

196
00:11:28,465 --> 00:11:30,175
these different systems at runtime.

197
00:11:30,685 --> 00:11:32,815
So platform engineers must implement.

198
00:11:32,875 --> 00:11:35,965
Comprehensive tracking,
monitoring, and, attribute these

199
00:11:35,965 --> 00:11:37,855
cost to specific teams, right?

200
00:11:37,855 --> 00:11:40,155
So we need a good
chargeback models, right?

201
00:11:40,185 --> 00:11:43,095
Basically where who is
using this platform, right?

202
00:11:43,125 --> 00:11:44,265
What are they using it for?

203
00:11:44,325 --> 00:11:45,675
What is the cost of their compute?

204
00:11:46,590 --> 00:11:47,400
So chargeback.

205
00:11:47,520 --> 00:11:51,949
Chargeback is usually well known and well
followed model across large enterprises

206
00:11:51,949 --> 00:11:55,640
where if there is a centralized theme,
let's take an example there, there is

207
00:11:55,640 --> 00:11:59,599
a centralized data lake, which is on
cloud, is managed by enterprise data

208
00:11:59,930 --> 00:12:03,530
team, and then different lines of
businesses can use data lake not to ingest

209
00:12:03,530 --> 00:12:07,910
their data, store it there to consume
it, to process it for any use case.

210
00:12:07,910 --> 00:12:12,739
So chargeback model can be actually
used to track all these expenses and

211
00:12:12,739 --> 00:12:13,935
I'll assign it back to the right.

212
00:12:14,529 --> 00:12:19,300
Line of business where they can get
visibility on how much they're spending.

213
00:12:19,389 --> 00:12:20,529
Are they overspending?

214
00:12:20,529 --> 00:12:23,100
Are they, using these
platforms efficiently or not?

215
00:12:23,310 --> 00:12:24,780
What are things they can optimize?

216
00:12:24,840 --> 00:12:29,600
All the thing things teams can do is
they optimize the jobs by looking at

217
00:12:29,600 --> 00:12:34,010
the scheduling on when this data needs
to be processed and ing and scheduling

218
00:12:34,010 --> 00:12:36,080
rightly, all the workloads could optimize.

219
00:12:36,490 --> 00:12:41,170
The usage of the platform can use
autoscaling and descaling as your,

220
00:12:41,260 --> 00:12:44,740
if you are aware of workloads
and if your workloads and the

221
00:12:44,740 --> 00:12:47,020
demand of compute is predictable.

222
00:12:47,410 --> 00:12:49,330
And third, you can use tiered storage.

223
00:12:49,360 --> 00:12:52,390
For example, on S3, you can
use intelligent tiered data

224
00:12:52,390 --> 00:12:53,315
gets moved to the lower.

225
00:12:53,815 --> 00:12:58,675
Lower, frequent, frequently accessed tier
to lower, frequently accessed tiers, low

226
00:12:58,675 --> 00:13:00,535
usage tier, and things of that nature.

227
00:13:00,535 --> 00:13:04,525
So that's gonna save a lot of money
for you that all these cloud vendors

228
00:13:04,915 --> 00:13:06,355
are coming up with capabilities.

229
00:13:06,385 --> 00:13:10,015
S3, for example, has intelligent
tiering, but even if not you, if you

230
00:13:10,015 --> 00:13:11,635
are aware of your data lifecycle.

231
00:13:12,340 --> 00:13:16,630
Data retention policies, you can apply
the right data retention policies and

232
00:13:16,630 --> 00:13:22,600
move your data sets from a tier to another
tier as part of the lifecycle management.

233
00:13:23,100 --> 00:13:25,050
Alright, let's take a look at next slide.

234
00:13:25,050 --> 00:13:26,850
So automation is key.

235
00:13:27,350 --> 00:13:30,810
We talked about infrastructure as
called looking at, again, thinking about

236
00:13:30,810 --> 00:13:34,140
enterprise scale data platforms here,
if you are talking about thousands of

237
00:13:34,140 --> 00:13:38,340
applications where you need to create
the, provisioning these resources, right?

238
00:13:38,430 --> 00:13:42,030
Compute, then infrastructure
automation is required where you

239
00:13:42,030 --> 00:13:45,555
can create these resources, storage
resources, compute resources in a, in

240
00:13:45,555 --> 00:13:49,660
an automated way using configuration
frameworks, security policies,

241
00:13:49,660 --> 00:13:51,400
network rules, and monitoring systems.

242
00:13:52,130 --> 00:13:55,400
Pipeline automation is required for
all these different applications that

243
00:13:55,400 --> 00:14:00,200
are gonna use your platform to deploy
these pipelines on your platform.

244
00:14:00,210 --> 00:14:00,870
Test it out.

245
00:14:00,990 --> 00:14:01,230
No.

246
00:14:01,230 --> 00:14:04,970
Move it all from dev to QA to
UT Uua to production and monitor

247
00:14:05,390 --> 00:14:08,330
the health of these pipeline in
terms of performance and failures.

248
00:14:08,980 --> 00:14:10,690
Third is continuous improvement.

249
00:14:10,840 --> 00:14:14,740
So att refinement of these automation
processes based on operational

250
00:14:14,740 --> 00:14:18,490
feedback and best practices from
the industries could be taken and

251
00:14:18,490 --> 00:14:21,040
implemented back into these automation.

252
00:14:21,100 --> 00:14:22,600
Fourth is configuration management.

253
00:14:22,720 --> 00:14:26,890
So you have to ensure the, there
is a consistency across all these

254
00:14:26,890 --> 00:14:29,620
different environments because then
you have multiple environments,

255
00:14:29,710 --> 00:14:33,240
f, qa, prod, UAT, all the, all
of them should have exactly same.

256
00:14:33,605 --> 00:14:37,855
Consistent configuration so your,
all your users and applications

257
00:14:37,915 --> 00:14:41,945
get the same experience and early
detect all the issues, if any, and

258
00:14:41,945 --> 00:14:46,505
they don't face any discrepancies in
terms of experience of the behavior

259
00:14:46,505 --> 00:14:47,825
of the plaque across different ones.

260
00:14:48,325 --> 00:14:51,025
Alright, so monitoring,
observability and data quality.

261
00:14:51,115 --> 00:14:54,235
Another key dimension, deep
dive on, so observability in

262
00:14:54,235 --> 00:14:56,425
data platforms is monitoring.

263
00:14:57,195 --> 00:15:00,855
Traditional infrastructure monitoring
along with data specific concepts.

264
00:15:00,915 --> 00:15:02,595
So we are talking about
data platform share.

265
00:15:02,595 --> 00:15:05,985
So there is an infrastructure level
monitoring platform level monitoring

266
00:15:06,015 --> 00:15:09,915
whether the platform is an up and running,
it's healthy, how is it performing, how

267
00:15:09,915 --> 00:15:12,975
is the scalability, how is the usage?

268
00:15:13,005 --> 00:15:16,215
How is the compute usage, what's the
throughput and things of that nature.

269
00:15:16,275 --> 00:15:18,585
And then we, because it's a data
platform, there are some data

270
00:15:18,585 --> 00:15:22,005
specific concerns, like data
quality, pipeline performance, right?

271
00:15:22,095 --> 00:15:23,925
Data schema, evolution.

272
00:15:24,190 --> 00:15:24,430
It.

273
00:15:24,430 --> 00:15:28,120
Errors, failures, e rejection
of data processing rates, right?

274
00:15:28,180 --> 00:15:30,100
Thresholds and things of that nature.

275
00:15:30,100 --> 00:15:35,910
So infrastructure and performance
monitoring covers tracking, resource

276
00:15:35,910 --> 00:15:39,750
utilization, system performance
and service availability across

277
00:15:39,750 --> 00:15:41,010
all the platform components.

278
00:15:41,010 --> 00:15:44,820
Cloud will naturally give you
a lot of monitoring out of the

279
00:15:44,820 --> 00:15:47,610
box, but if there are, if you are
building your own capabilities.

280
00:15:48,395 --> 00:15:52,145
For your data platform, you need
to build your own monitoring and

281
00:15:52,145 --> 00:15:54,005
alerting for your applications.

282
00:15:54,345 --> 00:15:57,495
For data quality, you have
to, and pipeline monitoring.

283
00:15:57,495 --> 00:16:01,425
Some of the key things you need to
look at is there any data missing?

284
00:16:01,815 --> 00:16:04,035
Is there any data leak
between your pipeline?

285
00:16:04,035 --> 00:16:06,585
If it's a data pipeline, you
getting duplicate records?

286
00:16:06,705 --> 00:16:07,905
Are you missing any records?

287
00:16:07,995 --> 00:16:11,565
Are there any schema
validation violations?

288
00:16:11,595 --> 00:16:13,605
Are there any statistical anomalies?

289
00:16:13,665 --> 00:16:18,075
You can have your own data quality rules
and have your pipeline and platform

290
00:16:18,075 --> 00:16:21,915
actually run those data quality rules
when you're trying to process data or

291
00:16:21,915 --> 00:16:22,955
when you're trying to, ingest data.

292
00:16:23,700 --> 00:16:28,260
So tiered alerting systems with different
escalation procedures from different

293
00:16:28,260 --> 00:16:32,770
issue types usually is the right way to,
to implement monitoring observability

294
00:16:32,770 --> 00:16:34,480
and data quality at a large scaling.

295
00:16:34,980 --> 00:16:36,880
Let's look at security and compliance.

296
00:16:36,880 --> 00:16:39,610
Many, not just regulatory
industries like finance, right?

297
00:16:39,610 --> 00:16:45,550
Almost all industries and all the
enterprises in today's cloud world facing

298
00:16:46,090 --> 00:16:48,250
a lot of security, concerns, right?

299
00:16:48,250 --> 00:16:51,580
Cybersecurity and compliance, and
really for regulated industries.

300
00:16:51,580 --> 00:16:54,930
So security and compliance
is not is very critical.

301
00:16:55,579 --> 00:16:58,280
For all the enterprises, no
matter if you're regulated or not.

302
00:16:58,400 --> 00:17:02,660
So that becomes a key requirement or
dimension while designing your modern data

303
00:17:02,660 --> 00:17:04,640
platforms data encryption and protection.

304
00:17:05,180 --> 00:17:07,880
So because we are talking about
data platforms here, how are

305
00:17:07,880 --> 00:17:08,714
you going to keep your data?

306
00:17:09,635 --> 00:17:11,735
State, how are you going
to keep data secure?

307
00:17:11,785 --> 00:17:16,885
You can actually implement multi-level
encryption for data at rest as well

308
00:17:16,885 --> 00:17:20,605
as data in transit and in memory
data masking tokenization, right?

309
00:17:20,695 --> 00:17:22,705
How do you protect your
sensitive information?

310
00:17:22,855 --> 00:17:24,325
That becomes a key capability.

311
00:17:24,415 --> 00:17:27,194
You can have different
data platforms, types.

312
00:17:27,194 --> 00:17:32,395
There could be data platform types which
are allowed to save customer, customer

313
00:17:32,395 --> 00:17:36,355
information, sensitive information
with the title access controls, there

314
00:17:36,355 --> 00:17:40,425
are, there could be data platforms
where you tokenize or, anonymize all

315
00:17:40,425 --> 00:17:42,875
your data and then store data there.

316
00:17:42,965 --> 00:17:47,315
So those are not, those platforms do
not have customer sensitive information.

317
00:17:47,435 --> 00:17:49,775
Access controls and
authentication becomes key.

318
00:17:49,865 --> 00:17:54,035
You have to make sure you have
sophisticated systems and access controls.

319
00:17:54,535 --> 00:17:58,255
At multiple granularity level, you
can have role-based access controls.

320
00:17:58,255 --> 00:17:59,870
You can have attribute
based access control.

321
00:18:00,700 --> 00:18:05,830
You can have or screen green and fine
grain access control at a file level or

322
00:18:05,980 --> 00:18:09,730
a table level or at a roll level, but at
the record level and at the column level.

323
00:18:09,730 --> 00:18:14,320
So all these different capabilities
become key or key implementation

324
00:18:14,320 --> 00:18:18,400
techniques to enable right access
controls and authentication mechanism

325
00:18:18,670 --> 00:18:22,270
for your sharing data platforms and
compliance and audit capabilities.

326
00:18:22,320 --> 00:18:26,070
Comprehensive audit, logging, data
governance, automated compliance.

327
00:18:26,130 --> 00:18:28,380
These are the things which are
covered on in this section.

328
00:18:28,430 --> 00:18:29,310
Efficient system capture.

329
00:18:30,100 --> 00:18:33,820
Detailed access and modification logs
while providing query capabilities

330
00:18:33,820 --> 00:18:35,560
for reporting and investigation.

331
00:18:35,560 --> 00:18:39,919
If you're using things like Delta Lake
or Snowflake for example, you have

332
00:18:39,919 --> 00:18:45,229
a table format and you are allowing
your consumers to use SQL based

333
00:18:45,229 --> 00:18:49,464
connection, then you are gonna be fine
because SQL based consumption tracks

334
00:18:49,464 --> 00:18:52,594
what data is being used, who is using
it, what attributes are being used.

335
00:18:52,594 --> 00:18:56,344
But on non SQL patterns, you may
want to build extra capabilities.

336
00:18:56,844 --> 00:19:00,774
File consumption, for example, identify
and track on who's using which data,

337
00:19:00,834 --> 00:19:03,834
which file, which attribute what
they're doing and things of that nature.

338
00:19:03,934 --> 00:19:07,834
So platform engineers must design
security systems and capabilities that

339
00:19:07,834 --> 00:19:12,094
provide comprehensive protection while
maintaining the performance and usability.

340
00:19:12,094 --> 00:19:15,754
You don't want to, you don't want to
have a security capabilities, which

341
00:19:15,754 --> 00:19:19,894
are impacting your performance and
usability of the platform, or limiting

342
00:19:19,894 --> 00:19:21,334
users to actually use the data.

343
00:19:21,334 --> 00:19:22,774
So that becomes a key.

344
00:19:23,274 --> 00:19:25,524
Consideration for platform engineers.

345
00:19:26,024 --> 00:19:28,484
How do you future proof your
data platform architecture?

346
00:19:29,084 --> 00:19:34,154
Because the, as we saw, the data
technologies and capabilities

347
00:19:34,154 --> 00:19:35,744
are emerging so rapidly.

348
00:19:35,864 --> 00:19:39,524
You have to think about how are you gonna,
while designing new platform, you're

349
00:19:39,524 --> 00:19:43,395
gonna think about how you, how do you
want to future proof your data platform?

350
00:19:43,395 --> 00:19:43,620
So one.

351
00:19:44,445 --> 00:19:48,975
You have to design architectures that
can accommodate new tech technologies

352
00:19:49,334 --> 00:19:51,254
without vendor locking, right?

353
00:19:51,254 --> 00:19:51,919
A lot of this cloud.

354
00:19:52,679 --> 00:19:55,409
Services, managed services
Q into vendor locking.

355
00:19:55,830 --> 00:19:58,680
So you have to load your
data into their systems.

356
00:19:58,680 --> 00:20:00,210
For example, Redshift, right?

357
00:20:00,390 --> 00:20:01,050
Snowflake.

358
00:20:01,050 --> 00:20:04,560
Historically, you have to load
data into Snowflake, so you want

359
00:20:04,560 --> 00:20:06,000
to implement abstract layers.

360
00:20:06,000 --> 00:20:09,865
You want to keep your data platform
as open standard as you can.

361
00:20:10,350 --> 00:20:13,290
So you can, and have an
abstraction there on top of it.

362
00:20:13,290 --> 00:20:18,180
So your users and consumers would not
be impacted even if you change your

363
00:20:18,180 --> 00:20:22,200
underlying storage or compute, layering
future from one thing to another.

364
00:20:22,380 --> 00:20:26,080
So managing your vendors correctly
and managing your, architectures

365
00:20:26,080 --> 00:20:30,100
to keep it go with, as much open
formats as you can, becomes the key.

366
00:20:30,220 --> 00:20:34,980
Keep it future for second is scalability
and performance planning as you.

367
00:20:35,910 --> 00:20:37,890
Create multi-tenant shared platforms.

368
00:20:38,000 --> 00:20:42,650
You need an architecture that can scale
for growing data volumes from terabytes,

369
00:20:42,700 --> 00:20:45,160
to petabytes and so on, so forth.

370
00:20:45,160 --> 00:20:49,570
Because the data is keeps growing
only as a lot of businesses

371
00:20:49,570 --> 00:20:55,010
are, enabling data powered AI
capabilities and ai, platforms and,

372
00:20:55,140 --> 00:20:56,970
understanding their customer behavior.

373
00:20:57,540 --> 00:20:58,800
The data is gonna grow.

374
00:20:58,830 --> 00:21:00,810
It's not gonna spend in future.

375
00:21:00,810 --> 00:21:03,990
So you need capabilities that
and sustain these growing data.

376
00:21:04,290 --> 00:21:07,795
Data volumes, increasing usage,
growing data ingestion, data

377
00:21:07,795 --> 00:21:09,235
processing, data transformation.

378
00:21:09,285 --> 00:21:13,725
You need caching strategies, multi-tiered
storage strategies, all inbuilt in your.

379
00:21:14,280 --> 00:21:19,380
Platform architecture so that in future,
when your data grows, your usage grows,

380
00:21:19,470 --> 00:21:22,410
your platform will scale as required.

381
00:21:22,830 --> 00:21:27,550
And third, you need flexible architectures
that can accommodate business growth.

382
00:21:28,165 --> 00:21:31,285
Changing analytical environments
and evolving governance needs.

383
00:21:31,705 --> 00:21:36,495
So organizational, evolution, if you
are, if become, if you add, if your

384
00:21:36,495 --> 00:21:41,265
business adds more capabilities in future,
which might need in governance or some

385
00:21:41,265 --> 00:21:46,735
compliance, your platform should be able
to easily enable it as required in future.

386
00:21:47,605 --> 00:21:49,225
So successful data platform.

387
00:21:49,870 --> 00:21:54,370
Engineering requires balancing multiple
competing priorities that cannot be

388
00:21:54,370 --> 00:21:58,510
achieved through technology alone,
but require helpful consideration.

389
00:21:58,600 --> 00:22:03,280
So some of that is flexibility,
consistency, self-service, governance,

390
00:22:03,310 --> 00:22:04,900
cost optimization performance.

391
00:22:04,900 --> 00:22:07,720
We covered all of these
in our previous slides.

392
00:22:07,750 --> 00:22:11,775
And these are, these become key dimensions
for, platform engineers and architects

393
00:22:11,995 --> 00:22:15,975
to, to basically design the data
platforms in modern data infrastructure.

394
00:22:16,475 --> 00:22:19,265
So the future of data platform
engineering, as the data

395
00:22:19,265 --> 00:22:21,085
technology continues to evolve.

396
00:22:21,585 --> 00:22:24,675
Platform engineer must remain
adaptable while maintaining

397
00:22:24,675 --> 00:22:26,264
focus on core principles.

398
00:22:26,625 --> 00:22:29,745
So four key principles, core
principles that we covered.

399
00:22:29,745 --> 00:22:30,764
One abstraction.

400
00:22:31,305 --> 00:22:35,435
How do you abstract all your capabilities
that how do you abstract the complex

401
00:22:35,435 --> 00:22:40,805
implementations and enable multi-tenant
platforms for your users and applications?

402
00:22:40,805 --> 00:22:42,004
Second, how do you automate?

403
00:22:42,575 --> 00:22:43,625
And second is automation.

404
00:22:43,805 --> 00:22:44,705
First is abstraction.

405
00:22:44,705 --> 00:22:45,935
So automation, how do you.

406
00:22:46,629 --> 00:22:48,580
How do automate your
configuration management?

407
00:22:48,580 --> 00:22:49,990
How do you automate your deployment?

408
00:22:49,990 --> 00:22:54,370
How do you automate your monitoring
and, predictive analysis, all the

409
00:22:54,370 --> 00:22:56,139
issues in there in the pipelines.

410
00:22:56,650 --> 00:23:00,340
Third is observability, which
includes monitoring, alerting, data

411
00:23:00,340 --> 00:23:03,879
quality, data level observability,
and infrastructure observability.

412
00:23:03,879 --> 00:23:07,120
And fourth, last but not the
least, is user experience.

413
00:23:07,210 --> 00:23:08,379
How do you make your data.

414
00:23:08,835 --> 00:23:11,830
Easy to find for your consumers
through the right governments and

415
00:23:11,830 --> 00:23:16,330
right user experience and how it is
made, available for our use for both

416
00:23:16,330 --> 00:23:19,840
human users and application users
across different lines of businesses.

417
00:23:20,439 --> 00:23:24,189
These principles provide a stable
foundation for navigating technologies

418
00:23:24,189 --> 00:23:27,730
change while building platforms
that can grow with the organization

419
00:23:27,730 --> 00:23:30,370
needs some of the emerging areas.

420
00:23:30,435 --> 00:23:34,664
Where a lot of companies are
exploring to have their data

421
00:23:34,664 --> 00:23:39,444
platforms support key, some of the new
capabilities is real time processing.

422
00:23:40,034 --> 00:23:43,405
How do you enable stream
processing in real analytics?

423
00:23:43,405 --> 00:23:44,754
Second is ML integration.

424
00:23:44,995 --> 00:23:48,864
How do how do you avoid seamless
integration of your machine learning and

425
00:23:48,864 --> 00:23:50,904
analytics models with your data platforms?

426
00:23:50,995 --> 00:23:55,894
Or you have your data platforms isolated,
then all your ml. Workloads cannot use

427
00:23:55,894 --> 00:23:58,095
your data as seamlessly as they can.

428
00:23:58,215 --> 00:24:00,885
And how do you, how could
you build cloud native, fully

429
00:24:00,885 --> 00:24:05,115
distributed and containerized
architectures to gain maximum

430
00:24:05,115 --> 00:24:06,975
benefit of your cloud architecture?

431
00:24:07,065 --> 00:24:11,195
These are some of the key areas
that platform engineers need to

432
00:24:11,614 --> 00:24:15,495
consider while defining while
designing new data platforms.

433
00:24:15,995 --> 00:24:20,975
So the platform engineering approach
to data infrastructure offers a path to

434
00:24:20,975 --> 00:24:25,415
manage complexity of these data platforms
while enabling innovation and agility.

435
00:24:26,075 --> 00:24:29,205
But what we looked at there
are three core pillars, right?

436
00:24:29,205 --> 00:24:33,395
Abstraction, automation, and user
experience looked at two key balances,

437
00:24:33,395 --> 00:24:36,200
which is flexibility with governance
and self-service with control.

438
00:24:36,700 --> 00:24:39,730
Ultimate with the ultimate
goal of enabling transformative

439
00:24:40,360 --> 00:24:41,440
business capabilities.

440
00:24:41,650 --> 00:24:45,850
So by applying these principles
thoughtfully and consistently,

441
00:24:46,300 --> 00:24:50,410
organizations can build infrastructure
that serves as a foundation for

442
00:24:50,410 --> 00:24:55,840
data-driven decision making and
business growth powered by AI and ml.

443
00:24:56,340 --> 00:24:58,500
Thank you so much for watching my session.

444
00:24:59,100 --> 00:25:00,090
I hope this was helpful.

445
00:25:00,590 --> 00:25:00,810
Bye.

