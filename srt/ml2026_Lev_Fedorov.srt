1
00:00:00,660 --> 00:00:01,620
Speaker 12: Hello everyone.

2
00:00:01,620 --> 00:00:03,630
Thank you so much for joining conference.

3
00:00:03,900 --> 00:00:07,440
My name is Lev Federov and today
I will tell you how to make

4
00:00:07,440 --> 00:00:09,090
recommendations explainable.

5
00:00:09,540 --> 00:00:13,740
My presentation will be very practical
and it'll consist of two parts.

6
00:00:14,010 --> 00:00:18,120
In the first part, I will guide you
through the re existing recommendation

7
00:00:18,120 --> 00:00:23,070
system pipeline, and in the second
part I will tell you which changes were

8
00:00:23,070 --> 00:00:28,960
implemented to make recommendations
explainable for both users and developers.

9
00:00:29,460 --> 00:00:35,360
To be on the same context let's understand
what recommendation systems are when you

10
00:00:35,360 --> 00:00:41,090
watch the YouTube video during a launch
break or when you are listening to Spotify

11
00:00:41,090 --> 00:00:45,170
music in the background while you are
coding, or where you're scroll when you're

12
00:00:45,170 --> 00:00:51,330
scrolling at wis. In the tube you are
actually using a recommended services and

13
00:00:51,330 --> 00:00:56,550
usually for this services, their homepage
will be recommendation feed, and that feed

14
00:00:56,550 --> 00:01:03,180
will be highly personal and unique for
each user based on the user's interests.

15
00:01:03,680 --> 00:01:08,320
And when I said that we would like to
be able to explain recommendations,

16
00:01:08,320 --> 00:01:09,640
what did it actually mean?

17
00:01:10,180 --> 00:01:15,440
So from user perspective I meant that
we would like to show the reason why

18
00:01:15,440 --> 00:01:18,040
particular card appeared in their feed.

19
00:01:18,760 --> 00:01:23,200
In this example, we have a video
about 10 tips for better productivity.

20
00:01:23,710 --> 00:01:27,550
And it was recommended because
user in the past liked the

21
00:01:27,550 --> 00:01:29,290
video about effective planning.

22
00:01:29,790 --> 00:01:33,420
And what is actually motivation
for adding these explanations

23
00:01:33,450 --> 00:01:34,770
and changing the system?

24
00:01:35,270 --> 00:01:37,610
So we faced couple of problems.

25
00:01:37,660 --> 00:01:40,480
First problem is user reports.

26
00:01:40,720 --> 00:01:45,950
We have questionnaires in our service
and users start, sending that as

27
00:01:45,950 --> 00:01:49,040
they see some relevant contents
that they're not interested in.

28
00:01:49,670 --> 00:01:55,560
And the second problem, as the developers
with the current architecture, we had a

29
00:01:55,560 --> 00:02:01,020
lack of traceability tools to understand
which interaction in the past all led

30
00:02:01,140 --> 00:02:03,750
to particular recommendation for a user.

31
00:02:04,249 --> 00:02:07,430
So let's go through the
recommendations pipeline.

32
00:02:07,430 --> 00:02:11,940
In our system, we have a user and
when they open our surveys, we

33
00:02:11,940 --> 00:02:13,980
associate user profile with them.

34
00:02:14,580 --> 00:02:19,910
After that out of 100 million items
on candidate selection stage, we

35
00:02:19,910 --> 00:02:23,870
select just couple of thousands
that will be relevant for users.

36
00:02:24,595 --> 00:02:30,335
After that we compute features for each
candidate and pass the ranking formula.

37
00:02:30,665 --> 00:02:36,905
So after that, we will have on the top
the most relevant items for this user.

38
00:02:37,545 --> 00:02:42,115
And we are gonna apply some
business logic and receive just 20

39
00:02:42,115 --> 00:02:45,125
items and return them to the user.

40
00:02:45,175 --> 00:02:45,895
Recommendations feed.

41
00:02:46,395 --> 00:02:50,445
Our recommendation system use
content-based filtering and

42
00:02:50,445 --> 00:02:52,545
collaborating, filtering approaches.

43
00:02:52,965 --> 00:02:56,955
And I'll not dive into details
how this approach works.

44
00:02:57,355 --> 00:03:02,075
But the main fact that you should
know that after applying this approach

45
00:03:02,075 --> 00:03:08,085
is we get embeddings as associated
with both users and publications.

46
00:03:08,585 --> 00:03:13,775
And regarding embedding, it's some
vector in end dimensional space, or it

47
00:03:13,775 --> 00:03:15,965
can be represented as a flowed array.

48
00:03:16,145 --> 00:03:22,085
And you can see cat is being equal
to some flowed array in this example.

49
00:03:22,555 --> 00:03:28,125
And regarding the embedding, since
the space for similar publications,

50
00:03:28,365 --> 00:03:30,225
embedding will be close enough.

51
00:03:30,540 --> 00:03:36,170
As you can see for Cat one and CAT two
and for different publications embeddings

52
00:03:36,200 --> 00:03:41,940
in the space will be far away as you see
for cats and politics or cats and cars.

53
00:03:42,210 --> 00:03:46,680
And when I say close, I mean
that their angle will tend to

54
00:03:46,680 --> 00:03:49,410
zero and cassan will tend to.

55
00:03:49,910 --> 00:03:53,030
So let let's go to the
first stage user profile.

56
00:03:53,250 --> 00:03:56,110
Let's say we got our
embedding for example, this

57
00:03:56,110 --> 00:03:57,970
collaborating filtering approach.

58
00:03:58,430 --> 00:04:02,720
And our expectation is that this embedding
will be close to all the interest.

59
00:04:03,220 --> 00:04:05,860
All the publications that
user is interested in.

60
00:04:06,200 --> 00:04:11,600
But in the reality it's not the true
and we can have some other publications

61
00:04:12,020 --> 00:04:17,665
and it's hard to interpret or explain
why this happened and also it.

62
00:04:18,425 --> 00:04:22,835
This embedding With this embedding,
it's hard to explain in as a feed

63
00:04:23,075 --> 00:04:29,135
of why certain publication was shown
this downsides and another approach.

64
00:04:29,185 --> 00:04:36,475
For content based models we can use a K
means algorithm when we clusterize our po.

65
00:04:36,555 --> 00:04:38,985
We select clusters with user interests.

66
00:04:39,235 --> 00:04:41,755
We find the OID of this cluster.

67
00:04:42,215 --> 00:04:47,095
And after that user profile is
represented with not only one

68
00:04:47,095 --> 00:04:49,015
embedding, but with the multiple ones.

69
00:04:49,645 --> 00:04:53,845
But in the reality, it's also not a
good approach because some clusters.

70
00:04:54,065 --> 00:04:58,695
Can overlap with each other and user
and Centro for this cluster will

71
00:04:58,695 --> 00:05:01,995
be in completely different cluster
that user is not interested in.

72
00:05:02,415 --> 00:05:09,005
Or after finding the Centro it's closer
to C sharp and c plus that user don't

73
00:05:09,005 --> 00:05:11,910
like while he liked J and Python.

74
00:05:12,410 --> 00:05:16,820
So I'm gonna address these issues in
the second part of my presentation, and

75
00:05:16,820 --> 00:05:19,040
let's move on to the candidate selection.

76
00:05:19,310 --> 00:05:25,880
As I mentioned this stage is responsible
for quickly selecting k or thousands

77
00:05:25,885 --> 00:05:29,265
of publications out of a hundred
of millions in our content base.

78
00:05:29,415 --> 00:05:33,945
And for that we use methods that
is called KNNK, nearest Neighbors.

79
00:05:34,375 --> 00:05:38,215
It can help us to return K nearest
neighbors to a given embedding.

80
00:05:38,575 --> 00:05:41,335
And in this example, we
have an elephant embedding.

81
00:05:41,695 --> 00:05:47,035
And when we select four nearest neighbors,
we got animals because they're embedding

82
00:05:47,365 --> 00:05:50,195
as the closest one to the elephants.

83
00:05:50,695 --> 00:05:53,935
And as an implementation, we use HNSW.

84
00:05:53,985 --> 00:05:59,515
You can read if you're interested
about this structure in Eric in the

85
00:05:59,605 --> 00:06:01,555
links that I attached to the slide.

86
00:06:02,045 --> 00:06:04,865
But basically we gonna put in this.

87
00:06:04,960 --> 00:06:10,400
Index all publications that we have
in our content base as an input.

88
00:06:10,400 --> 00:06:15,140
We will pass the user profile and amount
of items that we would like to select.

89
00:06:15,680 --> 00:06:22,190
And as an output, we will get K items
that we requested ations in our case,

90
00:06:22,580 --> 00:06:25,190
and it'll be done very effectively.

91
00:06:25,690 --> 00:06:31,370
The issue with this can and indices,
we, first of all when we ask

92
00:06:31,370 --> 00:06:35,260
nearest neighbors, it doesn't mean
that they're really close to the

93
00:06:35,260 --> 00:06:37,090
original embeddings that we passed.

94
00:06:37,540 --> 00:06:43,360
So in, you can see that we got some
items with low design and they're

95
00:06:43,360 --> 00:06:45,250
probably about different topics.

96
00:06:45,910 --> 00:06:49,350
And also another disadvantage
of this index that.

97
00:06:49,690 --> 00:06:54,995
We know our publications, that they're
similar, but we doesn't know anything

98
00:06:54,995 --> 00:06:58,565
about their quality or their popularity.

99
00:06:59,135 --> 00:07:03,485
And this issue I'm gonna address in the
second part of presentation as well.

100
00:07:03,815 --> 00:07:08,275
So now we have our candidates
and for each candidates we go.

101
00:07:08,775 --> 00:07:10,155
Compute some features.

102
00:07:10,415 --> 00:07:15,625
These features can be as simple as amount
of likes or if the publication is a video

103
00:07:15,655 --> 00:07:20,615
duration of this video or a bit more
complex as a sign between embeddings.

104
00:07:21,615 --> 00:07:26,085
For collaborative filtering models
and content models, and we're

105
00:07:26,085 --> 00:07:28,455
gonna pass it to ranking formula.

106
00:07:28,455 --> 00:07:32,355
And ranking formula will predict
the score, the relevance for user.

107
00:07:32,655 --> 00:07:36,975
So now we can sort all the
candidates by this score.

108
00:07:37,245 --> 00:07:41,625
And in the top we are gonna get the
most relevant publications for the user.

109
00:07:42,125 --> 00:07:44,495
And last part, business logic.

110
00:07:44,645 --> 00:07:48,665
It's actually quite interesting that
after applying ranking formula, we

111
00:07:48,665 --> 00:07:54,385
can get publications from one only
after in the top, or we can get

112
00:07:54,415 --> 00:07:57,205
only publications about one topic.

113
00:07:57,655 --> 00:08:03,895
So to avoid this situations, we gonna
reward the top in that way that it has

114
00:08:03,895 --> 00:08:07,205
multiple outs and it has multiple topics.

115
00:08:07,205 --> 00:08:08,705
So it's being diverse.

116
00:08:09,205 --> 00:08:13,155
And let's talk about approach
that we implemented to make

117
00:08:13,155 --> 00:08:14,685
recommendations explainable.

118
00:08:15,055 --> 00:08:19,195
I highlighted in the green box user
profile and candidate selection.

119
00:08:19,585 --> 00:08:22,070
These are the stages that
we would like to change.

120
00:08:22,570 --> 00:08:28,170
Regarding user profile, if you remember
we used embeddings and the issue

121
00:08:28,170 --> 00:08:30,710
that we can't easily interpret them.

122
00:08:31,130 --> 00:08:35,820
But what if instead of embedding, we
store publication Id that user positively

123
00:08:35,820 --> 00:08:41,830
interacted with so we wouldn't have any
approximations and user profile will

124
00:08:41,830 --> 00:08:44,380
consist of the list of publication id.

125
00:08:44,660 --> 00:08:50,240
Then in this case we need to rework
our candidate selection and, we

126
00:08:50,240 --> 00:08:52,090
implemented the following structure.

127
00:08:52,090 --> 00:08:56,680
So you can think of it as a simple
map, wherein keys, you will have

128
00:08:56,710 --> 00:09:01,500
positives or publications and
keys are associated with values.

129
00:09:01,530 --> 00:09:06,430
And each value contains a list of
candidates that is relevant, that

130
00:09:06,435 --> 00:09:09,210
are relevant to the original key.

131
00:09:09,990 --> 00:09:12,270
So let's talk about relevancy.

132
00:09:12,300 --> 00:09:15,350
How we gonna, keep our list relevant.

133
00:09:15,800 --> 00:09:20,930
So at the beginning, let's assume
that we have all the publications in

134
00:09:20,930 --> 00:09:25,830
the list and we would like to filter
non similar publications to the key.

135
00:09:26,370 --> 00:09:29,580
So to do we can annotate
pairs of publications as

136
00:09:29,580 --> 00:09:31,620
relevant and irrelevant first.

137
00:09:32,010 --> 00:09:37,560
And, for doing so, we use some
crowdsourcing platform and send some tasks

138
00:09:37,680 --> 00:09:42,750
with these annotations so assessors can
mark our publication, similar or not.

139
00:09:43,050 --> 00:09:48,300
And once we get this data
set, f for version zero.

140
00:09:48,520 --> 00:09:52,040
We can define a assigned
threshold for content-based

141
00:09:52,040 --> 00:09:54,330
model based on the sanitation.

142
00:09:54,420 --> 00:09:58,500
After everything above this
threshold will be relevant and.

143
00:09:59,420 --> 00:10:01,130
This will solve our problem.

144
00:10:01,410 --> 00:10:05,160
But we can have some errors or mistakes.

145
00:10:05,470 --> 00:10:10,760
And instead of that, let's train
some relevance formula that we'll be

146
00:10:10,760 --> 00:10:15,770
using as a features multiple cosign
for multiple content-based models.

147
00:10:16,560 --> 00:10:20,650
And it'll predict if, publications
are relevant to each other or not.

148
00:10:21,460 --> 00:10:25,520
So now we have only relevant
publications in the list.

149
00:10:25,910 --> 00:10:27,730
But what about order?

150
00:10:27,730 --> 00:10:31,670
Can we order then somehow
how that the most attractive

151
00:10:31,670 --> 00:10:33,230
publications are in the top?

152
00:10:33,290 --> 00:10:35,330
And yes, we can for that.

153
00:10:35,330 --> 00:10:38,420
We have another stage that
we called attractiveness.

154
00:10:38,890 --> 00:10:42,310
For first version, we can
use a conditional CTR.

155
00:10:42,810 --> 00:10:48,570
And conditional CTR is CTR of publication
A, given that the user had a positive

156
00:10:48,570 --> 00:10:53,270
interaction with publication B. So
you can think that publication B is a

157
00:10:53,270 --> 00:11:00,070
key of our index and publication A is
one of many publications in the list.

158
00:11:00,790 --> 00:11:06,460
So with that, we can have a CTR for
all the candidates inside of the list

159
00:11:06,760 --> 00:11:09,700
and we can order them by this value.

160
00:11:10,045 --> 00:11:13,015
So we can have the most
attractive one in the top.

161
00:11:13,515 --> 00:11:15,855
The issue here, again, the same.

162
00:11:16,125 --> 00:11:20,605
We rely only just on one metric
while we can rely on much more.

163
00:11:20,855 --> 00:11:24,605
So instead of that, let's use
attractiveness formula that will

164
00:11:24,605 --> 00:11:27,175
be predicting positive interaction.

165
00:11:27,265 --> 00:11:30,475
This publication a, given that the
user had a positive interaction

166
00:11:30,480 --> 00:11:33,895
with publication B, it's very
similar to ranking formula.

167
00:11:34,115 --> 00:11:37,655
And as a features, we can pass
their information met information

168
00:11:37,655 --> 00:11:42,345
about publication in the key,
and we can pass met information

169
00:11:42,375 --> 00:11:44,445
about the publication in the list.

170
00:11:44,655 --> 00:11:48,645
And also we can pass conditional
CTR as another feature.

171
00:11:49,125 --> 00:11:54,165
So now our recommendation pipeline will
looks like that we have a positive that

172
00:11:54,165 --> 00:11:56,205
is associated with some candidates.

173
00:11:56,385 --> 00:12:00,525
And let's say some candidates were
promoted and then they became a

174
00:12:00,525 --> 00:12:02,925
publication in the feed of the user.

175
00:12:03,315 --> 00:12:04,785
So how are we going.

176
00:12:05,100 --> 00:12:08,100
Play now, why this
publication is being shown?

177
00:12:08,620 --> 00:12:12,310
If we go back to positive, we know
that it is publication id and we

178
00:12:12,310 --> 00:12:14,320
have some meta information about it.

179
00:12:14,560 --> 00:12:16,540
We have a title, we have an author.

180
00:12:16,870 --> 00:12:23,240
So we can say that you liked some title
or you interacted with author, and that's

181
00:12:23,245 --> 00:12:25,550
why you see this publication in your feed.

182
00:12:26,050 --> 00:12:31,520
And let's talk about one more issue
with our system over the time.

183
00:12:31,575 --> 00:12:36,945
User user positives list will, growing
up and growing up, and obviously

184
00:12:36,945 --> 00:12:41,805
we would like to limit this list
and select just the best items.

185
00:12:42,225 --> 00:12:45,655
One of the approaches
determinational point process.

186
00:12:46,055 --> 00:12:47,975
And this algorithm works.

187
00:12:48,005 --> 00:12:54,895
As following, we select K points and we
try to satisfy both of the conditions.

188
00:12:54,925 --> 00:12:57,895
We prefer to select
points with high weights.

189
00:12:58,280 --> 00:13:01,970
And we would like to keep
diversity in the selected set.

190
00:13:02,470 --> 00:13:07,340
And as a wait you can think of
for example, a rate between how

191
00:13:07,340 --> 00:13:11,240
long user watched video and what
was the duration of the video.

192
00:13:11,740 --> 00:13:17,660
We can increase the weight in case a
user liked this video or they share it

193
00:13:17,660 --> 00:13:19,820
with their friends or they comment it.

194
00:13:20,360 --> 00:13:25,530
So I just showed a simple example
when we have six positives and we

195
00:13:25,530 --> 00:13:26,940
would like to select just three.

196
00:13:27,150 --> 00:13:31,420
So on the right part, we
selected, this positive that meets

197
00:13:31,420 --> 00:13:33,400
their original two conditions.

198
00:13:33,900 --> 00:13:37,260
And also you can think
about positives as a three.

199
00:13:37,470 --> 00:13:42,840
So each positive leads us to another
positive or leads us to nothing.

200
00:13:42,990 --> 00:13:46,860
And if they leads us to nothing, then
probably we would like to eliminate

201
00:13:46,860 --> 00:13:48,720
this positives from the user.

202
00:13:49,475 --> 00:13:54,635
For doing so in the history event, now we
have positive id, publication ID and event

203
00:13:54,635 --> 00:13:57,985
id like show click and anything like that.

204
00:13:58,345 --> 00:14:02,605
So we can compute starts for
each positive, like CTR, like

205
00:14:02,665 --> 00:14:07,245
dislikes, rate, watch time, and
we can train one more formula.

206
00:14:07,455 --> 00:14:09,395
For ranking positives.

207
00:14:09,765 --> 00:14:14,205
So we have the most impactful
positives in the top of the list,

208
00:14:14,775 --> 00:14:19,575
and we can remove bad positives as
I showed in the previous example.

209
00:14:19,815 --> 00:14:23,895
And we also can pass the statistics
to recommender ranking formula

210
00:14:24,135 --> 00:14:28,445
to make our recommendation system
better and improve and improve it.

211
00:14:28,945 --> 00:14:33,595
And now about candidate selection,
we can use some bandits ideology.

212
00:14:33,985 --> 00:14:38,215
But let's firstly think of we have
three positives for some user.

213
00:14:38,585 --> 00:14:39,830
So what we want to do.

214
00:14:40,550 --> 00:14:44,360
We want to select 300 candidates
out of three positives.

215
00:14:44,640 --> 00:14:49,310
Instead of equally sharing and
selecting 100 candidates, let's

216
00:14:49,310 --> 00:14:54,220
use the CTR for the positives that
we know from the previous stage.

217
00:14:54,700 --> 00:15:00,070
And we see that positive one has the
most impactful CTR, so that's why we.

218
00:15:00,405 --> 00:15:02,205
Proportion in the proportion.

219
00:15:02,205 --> 00:15:07,725
We proportionally shared the quota
and we selected 150 candidates from

220
00:15:07,725 --> 00:15:13,085
it, and we selected 60 candidates from
positive three as a less impactful one.

221
00:15:13,210 --> 00:15:14,260
There's a conclusion.

222
00:15:14,260 --> 00:15:17,920
So we made our system transparent.

223
00:15:17,970 --> 00:15:23,640
So our recommendations pipeline is very
explainable now, and we can show the

224
00:15:23,640 --> 00:15:26,040
recommendation, explanation to the user.

225
00:15:26,490 --> 00:15:30,910
We also addressed couple of
issues in, in previous pipeline.

226
00:15:30,910 --> 00:15:36,960
So we managed to get feed relevance
increase and least but not last.

227
00:15:36,960 --> 00:15:42,810
Developers can trace the backend, improve
system and investigate if user complaints

228
00:15:42,860 --> 00:15:45,140
about the relevant cards in their feed.

229
00:15:45,640 --> 00:15:49,250
And thank you so much for,
listening my presentation.

230
00:15:49,250 --> 00:15:50,330
I hope you enjoyed.

231
00:15:50,690 --> 00:15:53,840
If you have any questions, you
can contact me and link it in.

232
00:15:53,870 --> 00:15:57,480
My link it in is attached
to the conference page.

233
00:15:58,380 --> 00:15:59,040
Thank you.

