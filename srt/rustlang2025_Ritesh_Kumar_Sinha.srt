1
00:00:00,000 --> 00:00:00,720
Hello everyone.

2
00:00:00,720 --> 00:00:01,440
Good morning.

3
00:00:01,500 --> 00:00:02,370
Good afternoon.

4
00:00:02,420 --> 00:00:03,650
I'm from Jane to India.

5
00:00:03,710 --> 00:00:08,750
Today I'm excited to present rest power
data engineering, building performance

6
00:00:08,750 --> 00:00:14,375
critical systems for global impact
in a world drowning in data for 1

7
00:00:14,375 --> 00:00:17,330
75 zetabytes process to annually.

8
00:00:17,660 --> 00:00:23,030
How do we build systems that are fast,
secure, and ready for global challenges?

9
00:00:23,810 --> 00:00:24,995
That's what we will explore.

10
00:00:25,495 --> 00:00:29,575
Imagine processing petabytes of
data without crashes or breaches.

11
00:00:30,295 --> 00:00:35,875
Traditional tools like Java or Python
of in Fall Shock garbage collection

12
00:00:36,025 --> 00:00:41,785
causes cause unpredictability,
memory leaks, risk, corruption.

13
00:00:42,475 --> 00:00:44,845
We need to rethink for safety and scale.

14
00:00:45,685 --> 00:00:51,595
Rest offers that performance of low
level language with high level safety.

15
00:00:52,360 --> 00:00:53,139
So why dust?

16
00:00:53,290 --> 00:00:54,339
Let's break it down

17
00:00:54,839 --> 00:00:58,529
First, memory, safety, rest Ownership.

18
00:00:58,529 --> 00:01:03,989
Model tracks who owns data preventing
books like null pointer at compiled time.

19
00:01:04,769 --> 00:01:05,940
No runtime overhead.

20
00:01:06,899 --> 00:01:07,649
Zero cost abstractions.

21
00:01:08,640 --> 00:01:13,259
Let lets you write clean code without
slowing down Petabytes processing.

22
00:01:13,759 --> 00:01:15,379
Concurrency in the list.

23
00:01:15,619 --> 00:01:20,029
The compiler catches risk conditions,
so no more debugging, deadlocks,

24
00:01:20,529 --> 00:01:22,210
and also on the predictability.

25
00:01:22,630 --> 00:01:26,890
New garbage collection means consistent
performance like c plus, but C,

26
00:01:27,390 --> 00:01:33,690
in my experience, I've seen teams
spending weeks on memory issues on C

27
00:01:33,750 --> 00:01:36,960
plus birth and rest eliminates that.

28
00:01:37,460 --> 00:01:39,320
These aren't hypotheticals.

29
00:01:39,500 --> 00:01:45,920
Thus delivers 10 times throughput
or JBM tools in streaming, 99.99%

30
00:01:46,430 --> 00:01:53,120
uptime for massive workloads, 80%
lower latency at the edge, and 60%

31
00:01:53,120 --> 00:01:55,689
saving while zero copy in production.

32
00:01:55,779 --> 00:01:59,020
This means handling billions
of records without hiccups.

33
00:01:59,520 --> 00:02:00,990
Think about your own pipeline.

34
00:02:01,080 --> 00:02:03,510
Could you use an 80% latency kit?

35
00:02:04,010 --> 00:02:05,000
Rust isn't alone.

36
00:02:05,540 --> 00:02:08,360
Two Q Powers a sync IO
for high throughput.

37
00:02:09,110 --> 00:02:11,750
Apache Arrow enables fast columnar data.

38
00:02:11,959 --> 00:02:13,100
Great for analytics.

39
00:02:13,820 --> 00:02:16,850
Data fusion is like a rest
waste spark for queries.

40
00:02:17,600 --> 00:02:20,780
The ecosystem is booming,
making rust practical today, but

41
00:02:21,280 --> 00:02:22,815
does this work in real world?

42
00:02:22,995 --> 00:02:24,975
Let's take a look at the case studies.

43
00:02:25,475 --> 00:02:28,265
The first case study is on
the client monitoring system.

44
00:02:28,765 --> 00:02:34,525
Terabytes of satellite data is
stored in remote spots, needing real

45
00:02:34,525 --> 00:02:36,595
time analysis on limited hardware.

46
00:02:37,405 --> 00:02:42,355
The rest solution would be to
efficient pipe to develop efficient

47
00:02:42,355 --> 00:02:47,234
pipelines that run flawlessly on
the edge, which pro which help us in

48
00:02:47,234 --> 00:02:52,575
growing 65% faster processing, 40%
cheaper infra, and reach to remote

49
00:02:52,575 --> 00:02:55,275
areas using this edge technology.

50
00:02:56,084 --> 00:03:00,015
This powers global climate
impact and think monitoring

51
00:03:00,515 --> 00:03:03,365
deforestation in real time.

52
00:03:04,325 --> 00:03:08,075
For environmental engineers
here, this means deploying

53
00:03:08,075 --> 00:03:11,435
AI where it's needed the most

54
00:03:11,935 --> 00:03:18,015
in health systems, public health systems,
national scale data for epidemic tracking.

55
00:03:18,105 --> 00:03:18,855
No room for error.

56
00:03:19,355 --> 00:03:23,045
Ensure memory, safety, and
privacy at compiled time.

57
00:03:23,674 --> 00:03:28,954
With quarries hitting eight millisecond
on billions of records, zero breaches

58
00:03:28,984 --> 00:03:31,144
near perfect uptime during crisis.

59
00:03:31,894 --> 00:03:35,404
This even sees slides by
enabling instant insights.

60
00:03:36,274 --> 00:03:39,954
These wins comes from first core
features and let's deep dive on those.

61
00:03:40,454 --> 00:03:40,754
Rest.

62
00:03:40,754 --> 00:03:41,864
Ownership is key.

63
00:03:42,044 --> 00:03:45,794
It prevents corruption by enforcing
single ownership or borrowing.

64
00:03:46,294 --> 00:03:48,544
Buffer flows all at compile time.

65
00:03:49,084 --> 00:03:52,644
This makes data flows clear
and maintainable perfect

66
00:03:52,644 --> 00:03:53,844
for complex pipelines.

67
00:03:54,344 --> 00:03:59,024
It's like a strict librarian where you
borrow a book, but obviously you can't

68
00:03:59,024 --> 00:04:02,024
keep it forever or lend it out twice.

69
00:04:02,524 --> 00:04:07,164
Here is a sync processing with Tokyo,
which read lines with zero copy.

70
00:04:08,004 --> 00:04:08,959
It processes data.

71
00:04:09,459 --> 00:04:14,699
Processes without copying data, and
also at the source, it parts passes

72
00:04:14,699 --> 00:04:20,819
the streams efficiently, and once it
has passes the streamed stream data, it

73
00:04:20,819 --> 00:04:27,729
can also transform the data in parallel
without logs, which in the next step

74
00:04:27,969 --> 00:04:31,509
can serialize safely to the data sync.

75
00:04:31,869 --> 00:04:34,090
This avoids overhead in high volume data.

76
00:04:34,590 --> 00:04:38,550
Loop also reads without
blocking clear buffers to reuse.

77
00:04:39,050 --> 00:04:40,820
Code creates array and batches.

78
00:04:40,820 --> 00:04:41,570
Zero copy.

79
00:04:42,350 --> 00:04:44,750
We share data without duplication.

80
00:04:44,810 --> 00:04:47,390
Column data is used for first queries.

81
00:04:48,200 --> 00:04:52,310
It also integrates with Python, which
is ideal for mixed language teams.

82
00:04:52,790 --> 00:04:56,210
And if you're in analytics,
a arrows columnar format.

83
00:04:56,750 --> 00:04:58,400
Speeds of aggregation, massively

84
00:04:58,900 --> 00:05:04,700
building ethical data systems with rust,
where also we need to take a look at the

85
00:05:04,909 --> 00:05:10,250
efficient query execution, and this is
also SQL is to optimize execution where

86
00:05:10,250 --> 00:05:14,210
you can register your CSV, for example.

87
00:05:14,359 --> 00:05:14,870
Run.

88
00:05:15,370 --> 00:05:20,130
Like a VG temperature with average
temperature, with filters, and it

89
00:05:20,130 --> 00:05:21,870
can provide you a zero copy results.

90
00:05:22,170 --> 00:05:26,630
And this itself is a example
of data freedom in action,

91
00:05:27,080 --> 00:05:29,090
which is vectorized for speed.

92
00:05:29,590 --> 00:05:31,810
Rest is in this fast, it is ethical.

93
00:05:31,810 --> 00:05:35,969
Tool ownership enforces privacy
boundaries, efficient bias

94
00:05:36,030 --> 00:05:38,610
detection in ML without slowdowns.

95
00:05:39,450 --> 00:05:45,150
It is also sustainable because it
takes low energy to process these

96
00:05:45,150 --> 00:05:47,700
data, which in turn cuts carbon.

97
00:05:48,270 --> 00:05:52,560
Dust lets us bend responsibility
responsibly at scale.

98
00:05:53,060 --> 00:05:56,810
In this slide, it talks about
the lock free with automate and

99
00:05:56,810 --> 00:05:58,975
the code shows a safer counter.

100
00:05:59,740 --> 00:06:03,450
Where streaming with Tokyo Arrow
provides higher throughput.

101
00:06:04,410 --> 00:06:06,780
It also optimizes for energy savings.

102
00:06:07,470 --> 00:06:08,610
So start small.

103
00:06:08,700 --> 00:06:10,710
Try risk pipeline in your next project.

104
00:06:11,010 --> 00:06:16,760
These are just plug and play
experiment to see this in action

105
00:06:17,260 --> 00:06:17,680
trends.

106
00:06:18,180 --> 00:06:21,810
The future trends of first data
engineering, Western data engineering.

107
00:06:22,410 --> 00:06:25,770
Obviously Rust is Cloud
native ML is integrated.

108
00:06:25,920 --> 00:06:30,030
This is also specialized tool which
provides us the performance and safety

109
00:06:30,420 --> 00:06:36,955
at lower cost, fewer vulnerabilities,
global scalability and this is why

110
00:06:37,015 --> 00:06:39,205
Rusty is the future of data engineering.

111
00:06:39,715 --> 00:06:42,925
In summary, rusty transforms
data engineering for impact.

112
00:06:43,405 --> 00:06:46,375
Fast, safe and ethical programming.

113
00:06:46,795 --> 00:06:47,140
Thank you.

