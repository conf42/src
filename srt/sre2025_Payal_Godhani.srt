1
00:00:01,350 --> 00:00:02,220
Hello everyone.

2
00:00:02,700 --> 00:00:06,990
Welcome to Site Reliability
Engineering Conference of 2025.

3
00:00:07,470 --> 00:00:08,910
My name is Ani.

4
00:00:09,360 --> 00:00:14,520
I am from Rapid Cloud Infrastructure
and I'm very excited to talk about

5
00:00:14,520 --> 00:00:17,070
zero downtime ML observability Today.

6
00:00:18,150 --> 00:00:21,930
Today we will explore how to
implement observability into

7
00:00:21,930 --> 00:00:27,060
machine learning systems without
causing any disruption or downtime.

8
00:00:27,495 --> 00:00:30,105
To the services that rely on them.

9
00:00:31,365 --> 00:00:36,255
Machine learning is no longer
on experimental playground.

10
00:00:36,435 --> 00:00:41,655
It has become a reality and it has
been used pretty much in all production

11
00:00:41,655 --> 00:00:47,085
systems, but it brings with itself
unique set of operational challenges.

12
00:00:47,805 --> 00:00:51,585
Today we'll explore some of
those SRE challenges that are

13
00:00:51,585 --> 00:00:55,005
being faced and how we can make.

14
00:00:55,065 --> 00:01:00,614
Okay, the ML systems more
observable, scalable, and reliable.

15
00:01:01,875 --> 00:01:03,074
So let's dive right in.

16
00:01:07,395 --> 00:01:10,395
As you can see, there are
two different worlds today.

17
00:01:10,664 --> 00:01:14,295
One is of ML engineers and then of sre.

18
00:01:14,295 --> 00:01:18,225
These two work on very
different philosophies.

19
00:01:18,824 --> 00:01:22,545
ML engineers are experts
at building models, whereas

20
00:01:22,634 --> 00:01:24,765
SREs are experts at running.

21
00:01:24,765 --> 00:01:24,914
So.

22
00:01:24,960 --> 00:01:25,199
Systems.

23
00:01:26,070 --> 00:01:31,169
ML engineers tend to be focused
more on experiments and they

24
00:01:31,410 --> 00:01:33,365
work in an ator process.

25
00:01:34,655 --> 00:01:38,610
SREs are more focused on the service
level agreements and the service

26
00:01:38,610 --> 00:01:41,220
level objectives of their services.

27
00:01:42,359 --> 00:01:47,550
ML engineers tend to focus more
on the accuracy of the models and

28
00:01:47,550 --> 00:01:53,294
the predictions by the models,
whereas SREs are focused more on.

29
00:01:53,970 --> 00:01:58,530
The uptime and the latency of
their distributed system services.

30
00:01:59,880 --> 00:02:05,100
ML engineers work more on an offline
setting where they evaluate their

31
00:02:05,100 --> 00:02:11,790
models, train their models, work in the
inferencing of it, whereas the SRE are

32
00:02:11,850 --> 00:02:14,940
working on real time production matrix.

33
00:02:15,870 --> 00:02:19,740
As you can see, there is a gap
between both of these worlds.

34
00:02:20,220 --> 00:02:23,490
So now when SREs are tasked.

35
00:02:23,865 --> 00:02:30,674
With running machine learning services
in production, they would likely face a

36
00:02:30,674 --> 00:02:36,195
lot of challenges because they have not
designed these models and they don't know

37
00:02:36,195 --> 00:02:39,550
how the these models are supposed to run.

38
00:02:39,810 --> 00:02:42,285
And what are the
predictions along with it?

39
00:02:42,435 --> 00:02:42,584
Right?

40
00:02:43,275 --> 00:02:47,445
So let's try and understand what
are, what are the traditional

41
00:02:47,445 --> 00:02:50,204
observatory matrix that looks like?

42
00:02:53,735 --> 00:02:58,560
Today's traditional tools for
observability that are equipped

43
00:02:58,560 --> 00:03:04,110
for distributed systems are more
along logs, matrix, and traces.

44
00:03:04,530 --> 00:03:09,455
That tells us if the distributed
service is running up or if it is

45
00:03:09,455 --> 00:03:13,650
running into error, if it's hundred
percent available, and so on.

46
00:03:14,760 --> 00:03:16,800
But it doesn't necessarily tell.

47
00:03:17,910 --> 00:03:25,530
If the model that is being run by
the service is working correct and

48
00:03:25,739 --> 00:03:30,810
in an accurate fashion, and if it is
predicting the correct output for which

49
00:03:30,810 --> 00:03:36,540
it was designed, so the errors tend
to be pretty silent when they occur

50
00:03:36,989 --> 00:03:38,820
in this ML systems in production.

51
00:03:39,929 --> 00:03:46,859
The predictions returned by these models
could be very much or super biased.

52
00:03:47,579 --> 00:03:53,970
But it is more than likely that an
SRE engineer won't get to notice this.

53
00:03:55,140 --> 00:04:01,589
There could be drift that is arising
in the data and it is degrading

54
00:04:01,589 --> 00:04:03,179
the performance of the model.

55
00:04:04,260 --> 00:04:10,380
But when an SRE looks at all the
matrix, but it, it shows that all

56
00:04:10,380 --> 00:04:13,019
the services are running up and fine.

57
00:04:13,019 --> 00:04:15,744
They're meeting their SLAs and the sles.

58
00:04:17,279 --> 00:04:22,500
So as we see, there are no standard
SLOs that are defined for ML quality

59
00:04:22,500 --> 00:04:30,300
today that an SRE can track their
accuracy and the fairness and the

60
00:04:30,300 --> 00:04:32,520
correction ratio of this models.

61
00:04:34,830 --> 00:04:39,570
So what are the other challenges
that ML engineers are face facing and

62
00:04:39,570 --> 00:04:44,640
the SREs are facing, who are running
these systems as we spoke about it?

63
00:04:45,060 --> 00:04:47,969
There are these silent
failures, which often grow.

64
00:04:48,150 --> 00:04:56,039
Undetected system is producing incorrect
output, but SRE has no way to notice this.

65
00:04:56,670 --> 00:05:00,780
There are data drips happening, and
we'll talk about the different kind of

66
00:05:00,780 --> 00:05:06,210
data drips in the next section, but the
data distribution has changed over time.

67
00:05:06,210 --> 00:05:08,550
The data drifts are happening.

68
00:05:08,550 --> 00:05:12,900
There could be prediction drift, but there
are not enough tooling around it today.

69
00:05:13,950 --> 00:05:17,609
That an SRE could use,
it is slowly changing.

70
00:05:19,950 --> 00:05:24,750
The model could very well get
stale if it is not retrained.

71
00:05:26,010 --> 00:05:32,825
So the model is losing its effectiveness,
and an SRE should be equipped to deal

72
00:05:32,825 --> 00:05:35,130
with it and to detect it early on.

73
00:05:36,075 --> 00:05:41,025
There could be very much a lot of
debugging issues that could arise, and

74
00:05:41,025 --> 00:05:45,075
the features that are being tracked
by the models often lack visibility.

75
00:05:46,305 --> 00:05:48,975
There could be more versioning problem.

76
00:05:49,035 --> 00:05:53,625
The code version may not match
with the model version and the

77
00:05:53,625 --> 00:05:55,665
latency issues could also arise.

78
00:05:55,665 --> 00:06:00,345
The inferencing latency could be
more, the loading of the model could

79
00:06:00,345 --> 00:06:01,815
be more, which will be resulting.

80
00:06:02,835 --> 00:06:10,995
In some delay at the customer end for a
good ML monitoring service work, all of

81
00:06:10,995 --> 00:06:13,935
these challenges needs to be addressed.

82
00:06:17,535 --> 00:06:23,865
So an ML observatory model should
be able to monitor, debug, and be

83
00:06:23,865 --> 00:06:29,175
able to understand these complex and
complicated systems that are in product.

84
00:06:30,960 --> 00:06:37,500
The data that is ingested by these
models need to be very accurate,

85
00:06:38,280 --> 00:06:42,690
should mimic real time traffic
and not any synthetic traffic.

86
00:06:43,800 --> 00:06:48,870
So the data quality needs to be high
and should be consistent over time.

87
00:06:50,820 --> 00:06:54,390
There should be way to track
the distribution of the

88
00:06:54,390 --> 00:06:57,840
features over time, and so that.

89
00:06:58,305 --> 00:07:03,405
SREs are able to see if there is any
change over time in this distribution

90
00:07:03,405 --> 00:07:05,415
and be able to catch it early on.

91
00:07:06,825 --> 00:07:12,075
The model performance should be
detected using Matrix over time

92
00:07:12,675 --> 00:07:17,445
so that we can detect if model is
performing accurately and correct over

93
00:07:17,445 --> 00:07:19,965
time and hasn't changed its trend.

94
00:07:22,065 --> 00:07:22,695
That should be.

95
00:07:23,610 --> 00:07:29,280
For SREs to identify changes in the
input and output data distributions

96
00:07:29,400 --> 00:07:30,900
that is known as drift detection,

97
00:07:33,060 --> 00:07:40,650
a way to track different versions of
the model across retraining, and then

98
00:07:40,650 --> 00:07:46,560
last, the model should be compared
with the live traffic and not with the

99
00:07:46,770 --> 00:07:48,990
three traffic on which it was trained.

100
00:07:50,700 --> 00:07:55,140
Think of observability here, not just
as monitoring the pipeline, but as

101
00:07:55,140 --> 00:07:59,940
observing the health and reliability
of the data logic and the outcomes.

102
00:08:00,390 --> 00:08:03,780
It's like adding an x-ray
vision to the black box.

103
00:08:05,970 --> 00:08:11,040
Let's look at the simple ML
monitoring system in production.

104
00:08:12,090 --> 00:08:16,740
An ideal model monitoring
service should be sitting right

105
00:08:16,740 --> 00:08:18,240
next to the prediction service.

106
00:08:18,240 --> 00:08:24,960
With it should not just ingest the
data input that is fed to the pipeline,

107
00:08:25,470 --> 00:08:30,960
but it should also be ingesting the
predictions that are emitted by the model.

108
00:08:32,730 --> 00:08:40,560
With this, it'll be able to make some
complex calculations and feed it to

109
00:08:40,560 --> 00:08:42,745
the monitoring and visualization tools.

110
00:08:43,815 --> 00:08:46,740
Now, these tools could help SRE.

111
00:08:47,010 --> 00:08:52,230
To look at any alarms or matrix
that could be generated if the

112
00:08:52,230 --> 00:08:54,780
model is not behaving at par.

113
00:08:56,280 --> 00:09:02,100
And whenever possible, there should
be B matrix calculation that looks

114
00:09:02,100 --> 00:09:07,170
at the historical data and compares
it with the ground truth, and see if

115
00:09:07,170 --> 00:09:09,180
there are any changes along the way.

116
00:09:11,280 --> 00:09:15,005
From an SRE perspective, the system
looks very much like a service.

117
00:09:15,005 --> 00:09:19,380
This, but this service correctness
depends not just on the binary,

118
00:09:20,010 --> 00:09:25,560
but on dynamic external input data
and its own historic behavior.

119
00:09:27,570 --> 00:09:33,360
Let's look at some simple matrix, which
are out there for a shopping cart.

120
00:09:34,500 --> 00:09:40,560
As you can see about, there is an average
latency for every region, and there is

121
00:09:40,560 --> 00:09:43,225
something known as R, which is basically.

122
00:09:43,665 --> 00:09:45,255
Root means squared error.

123
00:09:45,795 --> 00:09:51,375
A ma a metric, often tracked in machine
learning world to see how far the

124
00:09:51,375 --> 00:09:53,475
prediction is from the ground root.

125
00:09:54,435 --> 00:09:56,385
It should be as low as possible.

126
00:09:58,185 --> 00:10:01,125
Then there are real user metric data.

127
00:10:01,785 --> 00:10:06,045
For example, there are page views
that are being track and the number

128
00:10:06,045 --> 00:10:12,290
of checkouts, and towards the end we
can see an end-to-end conversion rate.

129
00:10:12,825 --> 00:10:19,245
Which tells us how many users visited
the, uh, the shopping page for browsing

130
00:10:19,245 --> 00:10:26,445
sofas, how many of them added those
sofas into the cart, and how many of

131
00:10:26,445 --> 00:10:28,755
these sofas were actually checked out.

132
00:10:29,445 --> 00:10:35,230
This is some of the good matrix that
SREs could track, especially the RMSE.

133
00:10:37,065 --> 00:10:42,255
Now let's go and take a look
at the different types of drift

134
00:10:42,255 --> 00:10:43,725
that we spoke about earlier.

135
00:10:45,975 --> 00:10:48,945
There is prediction drift,
data drift, feature drift,

136
00:10:49,070 --> 00:10:50,730
and feature attribution drift.

137
00:10:51,470 --> 00:10:57,705
Today there are tools available that
would be able to track these drips

138
00:10:58,005 --> 00:11:00,410
and an SRE can detect this early on.

139
00:11:01,995 --> 00:11:07,334
As SRE supporting ML workload, our job
doesn't stop once a model is deployed.

140
00:11:07,875 --> 00:11:09,944
In fact, that's where it really starts.

141
00:11:10,875 --> 00:11:16,995
One of the key challenges is understanding
if and when the model starts behaving

142
00:11:16,995 --> 00:11:21,824
differently, and this is where the
drift modeling becomes very crucial

143
00:11:21,824 --> 00:11:28,920
and critical, even when ground truth is
delayed or unavailable monitoring drip.

144
00:11:28,965 --> 00:11:36,015
It helps us maintain the model quality,
reduce the customer impact, and triggering

145
00:11:36,015 --> 00:11:39,555
retraining workflows at the right time.

146
00:11:41,295 --> 00:11:42,885
Let's talk about prediction drought.

147
00:11:44,205 --> 00:11:48,615
This is when the distribution of model
prediction starts changing over time.

148
00:11:49,545 --> 00:11:50,565
Let's take an example.

149
00:11:51,525 --> 00:11:55,185
Let's say we have a recommendation
model that suggests product.

150
00:11:56,069 --> 00:12:02,694
Normally it suggests a mix of budget
and premium items, but over time we

151
00:12:02,694 --> 00:12:07,589
have noticed the model is heavily
skewed towards premium products.

152
00:12:07,920 --> 00:12:09,510
So now that's a prediction shift.

153
00:12:10,229 --> 00:12:11,520
Why could it be happening?

154
00:12:11,640 --> 00:12:16,860
It could be because of the shifts in the
user behavior, seasonal shopping trends

155
00:12:16,860 --> 00:12:19,079
like Black Friday or some festivities

156
00:12:21,120 --> 00:12:22,170
or even boxing.

157
00:12:22,170 --> 00:12:24,750
The upstream service, feeding the model.

158
00:12:26,415 --> 00:12:31,245
Importantly, we don't need
ground truth to detect this.

159
00:12:31,635 --> 00:12:34,425
Just monitoring the shape of
the predictions is enough.

160
00:12:35,715 --> 00:12:41,160
Predicting drift can be the very
first alarm bell that an SRE

161
00:12:41,160 --> 00:12:47,055
can see to detect if there is a
prediction drift happening or not.

162
00:12:48,705 --> 00:12:51,885
Now let's go on to the next drift.

163
00:12:52,335 --> 00:12:52,740
The data.

164
00:12:53,864 --> 00:12:59,265
Data drift is a typical example
of a wide gap between the training

165
00:12:59,265 --> 00:13:04,724
work set versus the real work
set on which it has been trained.

166
00:13:05,415 --> 00:13:10,905
For example, if a model hasn't
seen any data coming out from a new

167
00:13:10,905 --> 00:13:15,555
geography, it is unlikely that it's
going to predict correct output.

168
00:13:16,964 --> 00:13:22,520
So this data drip happens typically when
the model hasn't seen this type of data.

169
00:13:23,160 --> 00:13:26,849
And it could be for reasons like the
business has expanded in new geographies.

170
00:13:26,849 --> 00:13:30,900
A new product has been rolled out, but
the model hasn't been trained on it.

171
00:13:31,290 --> 00:13:34,979
Or there could be new marketing
campaigns due to which this new

172
00:13:34,979 --> 00:13:37,739
data has been, uh, rolled out.

173
00:13:39,959 --> 00:13:44,939
Now let's look at, take a look at one more
granular drip, which is a feature drip.

174
00:13:44,939 --> 00:13:50,910
It is much granular as compared to data
drip, and this also happens because.

175
00:13:50,925 --> 00:13:53,714
Of a change in data, but
for a particular feature.

176
00:13:54,734 --> 00:14:00,704
For example, if there is a shift in the
values of input for an individual feature.

177
00:14:01,635 --> 00:14:07,094
For example, let's say we have
a user location data, and most

178
00:14:07,094 --> 00:14:12,040
of the time it used to capture
70% urban and 30% rural data.

179
00:14:12,740 --> 00:14:14,745
But now over time it has flipped.

180
00:14:15,045 --> 00:14:18,314
Maybe due to a regional ad
campaign that must have happened.

181
00:14:19,035 --> 00:14:23,115
Now that change in the future feature
distribution is known as feature drift.

182
00:14:24,135 --> 00:14:29,025
This often results from changes in
how or where the data is collected,

183
00:14:29,565 --> 00:14:31,875
all shipped in the user population.

184
00:14:32,535 --> 00:14:35,685
It is especially important to
drag these features that are

185
00:14:35,685 --> 00:14:37,635
heavily weighted in your model.

186
00:14:38,745 --> 00:14:42,525
So you would want to track the most
important features of your model

187
00:14:42,555 --> 00:14:44,925
and take a careful look at these.

188
00:14:45,000 --> 00:14:50,670
Feature drips specifically that are
affecting the model in a higher ratio.

189
00:14:53,340 --> 00:14:55,890
The last trip is the
feature attribution route.

190
00:14:56,970 --> 00:15:01,560
This is a shift in how important
feature is to model prediction,

191
00:15:01,710 --> 00:15:04,260
especially across retraining.

192
00:15:05,130 --> 00:15:06,330
Now, let's take an example.

193
00:15:07,440 --> 00:15:12,720
Initially, our model rank prizes the most
important features of predicting code.

194
00:15:14,700 --> 00:15:20,070
But after several regions and
over time, suddenly brand emerges

195
00:15:20,070 --> 00:15:21,810
to become more influential.

196
00:15:21,810 --> 00:15:24,210
Now, that's an attribution drift.

197
00:15:24,930 --> 00:15:26,130
Why could it be happening?

198
00:15:26,700 --> 00:15:29,700
Usually caused by retraining on new data.

199
00:15:30,180 --> 00:15:35,610
Sometimes that could be subtle label
noise that was introduced, or there were

200
00:15:35,610 --> 00:15:41,775
feature core relations that have happened,
or it could be due to an over of a model.

201
00:15:41,850 --> 00:15:49,350
Do that the model has learned over time
to make decisions differently than what it

202
00:15:49,560 --> 00:15:56,085
used to previously attribute distribution
doesn't always show up in the raw data,

203
00:15:56,985 --> 00:15:59,325
but it changes how the model thinks.

204
00:16:00,480 --> 00:16:06,420
We can have a very stable input data
and still see a noticeable performance

205
00:16:06,420 --> 00:16:09,120
drop because of the attribution shift.

206
00:16:09,720 --> 00:16:11,585
These are typically little hard.

207
00:16:11,685 --> 00:16:12,405
To catch.

208
00:16:13,125 --> 00:16:16,665
But as SREs or platform
engineers, we are not just

209
00:16:16,665 --> 00:16:19,244
monitoring CPU and memory anymore.

210
00:16:20,175 --> 00:16:25,484
We are monitoring the health of our
algorithms and adding drift matrix

211
00:16:25,545 --> 00:16:30,944
alongside traditional observability matrix
have become all the more crucial ship,

212
00:16:31,574 --> 00:16:38,324
and it'll bring the right attention to
all the silent failures that is happening

213
00:16:38,324 --> 00:16:40,339
in ML systems that we could catch.

214
00:16:40,365 --> 00:16:45,765
Very early on and hopefully
mitigate any large scale events

215
00:16:45,765 --> 00:16:48,945
or any ML related incidents.

216
00:16:50,385 --> 00:16:53,955
Now let's spend a little bit of
time to look at what a good ML

217
00:16:53,955 --> 00:16:55,755
observability model looks like.

218
00:16:56,265 --> 00:17:02,085
Starting with the data logging, as we
spoke about earlier, the model or the

219
00:17:02,085 --> 00:17:06,150
monitoring service should not only
capture the raw inputs, but also the

220
00:17:06,155 --> 00:17:08,835
prediction to make the right set of.

221
00:17:10,730 --> 00:17:12,944
And next is the feature monitoring.

222
00:17:13,485 --> 00:17:18,915
Each of the important features that
track by the model should be captured

223
00:17:19,155 --> 00:17:21,735
and the drift should be populated.

224
00:17:23,655 --> 00:17:25,214
Then comes the prediction Monitoring.

225
00:17:26,835 --> 00:17:30,465
The monitoring service should
have the ability to compare the

226
00:17:30,465 --> 00:17:34,695
predicted outcomes versus the actual
outcomes that are based on ground.

227
00:17:34,905 --> 00:17:38,145
Truth then comes the version.

228
00:17:39,780 --> 00:17:45,899
There should be a way for the service
to model predictions and the versions

229
00:17:46,560 --> 00:17:49,860
and dust and the most popular one.

230
00:17:50,760 --> 00:17:57,540
An SRE should have dashboards through
which they can visualize latency, the

231
00:17:57,600 --> 00:18:02,160
throughput, and most importantly, the
drips that are happening over time.

232
00:18:05,100 --> 00:18:06,240
So how has.

233
00:18:06,570 --> 00:18:08,610
The SRE practices changed.

234
00:18:09,150 --> 00:18:13,920
We have already started seeing that
the SRE practices have emerged.

235
00:18:15,510 --> 00:18:20,940
In our world, the traditional SRE
practices were more around the latency

236
00:18:20,940 --> 00:18:25,950
monitoring and the health checks, whether
the candidates are running fine and the

237
00:18:25,950 --> 00:18:28,195
incident response was catered around it.

238
00:18:29,055 --> 00:18:33,780
But now the ML aware
SRE practices lets us.

239
00:18:34,350 --> 00:18:37,320
Do monitoring of the prediction
drift and all of the drifts

240
00:18:37,320 --> 00:18:38,610
that we spoke about earlier.

241
00:18:39,330 --> 00:18:43,919
There are enough data pipeline
checks, shadow deployments are also

242
00:18:43,919 --> 00:18:49,110
happening, and the incident response
is cd from an ML perspective.

243
00:18:52,590 --> 00:18:55,860
Now, what are the tools that
one can use for detecting these?

244
00:18:56,550 --> 00:19:00,655
There are enough open source tools that
could be used like evidently while.

245
00:19:03,345 --> 00:19:08,865
The major cloud providers has also
rolled out, uh, tools like Vertex AI is

246
00:19:08,865 --> 00:19:14,505
a popular one from Google, and Amazon
SageMaker is one other one that is

247
00:19:14,505 --> 00:19:17,085
quite popular amongst AWS customers.

248
00:19:21,675 --> 00:19:23,590
Let's look at why zero downtime matters.

249
00:19:24,220 --> 00:19:28,910
Zero downturn matters per any distributed
system service, but especially for

250
00:19:28,910 --> 00:19:31,150
real time or customer facing systems.

251
00:19:33,510 --> 00:19:38,550
Any monitoring or analysis steps
that disrupts availability can

252
00:19:38,550 --> 00:19:41,639
lead to significant business
impacts and revenue loss.

253
00:19:42,149 --> 00:19:44,580
So zero downtime is non-negotiable.

254
00:19:48,000 --> 00:19:52,290
Now, as we spoke about earlier, when we
are deploying a new model, we need to

255
00:19:52,290 --> 00:19:57,510
send light traffic to a shadow wood and
compare decisions with production and

256
00:19:57,510 --> 00:20:00,600
flip it only when we are fairly confident.

257
00:20:01,710 --> 00:20:06,510
Having said that, all the observability,
uh, monitoring should happen

258
00:20:06,780 --> 00:20:08,790
asynchronous to the inferencing path.

259
00:20:08,880 --> 00:20:11,130
It should not come in the
way of the critical path.

260
00:20:11,190 --> 00:20:13,200
Otherwise, it'll impact the latency.

261
00:20:15,060 --> 00:20:21,420
And if at all, the model is not
predicting or is giving wrong prediction,

262
00:20:21,420 --> 00:20:23,130
there should be graceful degradation.

263
00:20:23,610 --> 00:20:27,450
Last but not least, the
observability has to be part of

264
00:20:27,485 --> 00:20:29,565
every CICD pipeline in production.

265
00:20:29,565 --> 00:20:29,700
Action.

266
00:20:31,770 --> 00:20:36,255
Wrapping up with my very last
thoughts, ML systems fail differently.

267
00:20:36,565 --> 00:20:42,015
They brought silently, they don't
always crash, but they decay as SREs.

268
00:20:42,015 --> 00:20:47,550
We must evolve our observability mindset
to include these nuances and build systems

269
00:20:47,550 --> 00:20:50,070
that don't just deal up, but stay smart.

270
00:20:50,730 --> 00:20:51,540
Thank you for your time.

