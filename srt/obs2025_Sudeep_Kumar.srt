1
00:00:01,080 --> 00:00:01,920
Hello everyone.

2
00:00:03,480 --> 00:00:08,969
Topic of this session is turning
synthetic traces into gold by scaling

3
00:00:09,090 --> 00:00:10,919
monitoring for critical user journeys.

4
00:00:10,980 --> 00:00:12,239
I am EP Kumar.

5
00:00:12,600 --> 00:00:17,190
I'm a principal engineer in
Salesforce, and I belong to the

6
00:00:17,190 --> 00:00:18,780
Monitoring Cloud organization.

7
00:00:18,900 --> 00:00:23,940
Our organization provides a
single stop solution catering to

8
00:00:23,940 --> 00:00:27,840
all the observability needs of
different Salesforce applications.

9
00:00:29,460 --> 00:00:34,200
So we have many stacks for
metrics, events, locks, and traces.

10
00:00:35,160 --> 00:00:38,680
To give a little bit context about
the scale at which we operate we are

11
00:00:38,680 --> 00:00:43,630
responsible for providing observability
for over a million hosting containers.

12
00:00:43,660 --> 00:00:48,220
More than 2000 teams and 13,000 developers
are actively engaged on our platform.

13
00:00:48,990 --> 00:00:50,970
To give a little bit idea about the.

14
00:00:51,480 --> 00:00:55,280
Telemetry volume that we handle
around one 40 billion times series.

15
00:00:55,280 --> 00:00:59,480
Every hour, more than 300 billion
spans a day and more than one

16
00:00:59,480 --> 00:01:02,440
petabytes of log volume every day

17
00:01:04,780 --> 00:01:05,860
critical user journeys.

18
00:01:05,860 --> 00:01:07,560
Now what are what are there really, right?

19
00:01:07,575 --> 00:01:11,575
I like to define them as
hunting down hero flows.

20
00:01:12,255 --> 00:01:15,405
Basically they are a way to be able to.

21
00:01:16,095 --> 00:01:21,875
Identify end user's journey, emulate it.

22
00:01:22,685 --> 00:01:27,115
And often these end user journeys
are high value request flows.

23
00:01:27,294 --> 00:01:32,815
They are customer facing and
business critical experiences, so

24
00:01:32,815 --> 00:01:34,585
potentially revenue impacting as well.

25
00:01:35,304 --> 00:01:38,369
Now, typically such
external user action flows.

26
00:01:38,469 --> 00:01:42,339
Often traverses through multiple
service in your infrastructure.

27
00:01:42,339 --> 00:01:47,529
And this is especially true in today's
world of microservices, where you have

28
00:01:47,529 --> 00:01:53,410
a plethora of microservices and each
microservice enables a specific function

29
00:01:53,860 --> 00:01:59,530
or a feature, and therefore it becomes
so much harder to actually pinpoint where

30
00:01:59,530 --> 00:02:01,750
an issue lies when an incident occurs.

31
00:02:03,400 --> 00:02:07,269
It also emphasizes the need
to have robust monitoring.

32
00:02:07,869 --> 00:02:12,410
To ensure availability and performance
of such key critical transaction

33
00:02:12,410 --> 00:02:16,850
flows, and this is where distributed
tracing can come in and really shine.

34
00:02:16,880 --> 00:02:20,549
For folks who are not aware of
distributed tracing distributed

35
00:02:20,549 --> 00:02:24,779
tracing is a way by which you can
track individual request flows as they

36
00:02:25,109 --> 00:02:27,099
travels through different services.

37
00:02:27,789 --> 00:02:32,889
Typically, each of these requests are
tracked by a unique grid or a trace id.

38
00:02:34,239 --> 00:02:37,939
We will delve a little deep into
semantics of distributed tracing

39
00:02:37,969 --> 00:02:39,260
to understand this topic better.

40
00:02:40,820 --> 00:02:44,089
Now, how can distributed
tracing help with cgs?

41
00:02:44,170 --> 00:02:47,619
It can help you answer very key
questions, for example, which are

42
00:02:47,619 --> 00:02:52,605
the services that I involve in my
critical path, and what are the health.

43
00:02:53,569 --> 00:02:56,899
Of these critical CUJ transactions.

44
00:02:57,920 --> 00:03:04,919
Now, these user journey flows often
evolve by because they new features

45
00:03:04,919 --> 00:03:09,399
get added and therefore it becomes
important to be able to keep track of it.

46
00:03:10,149 --> 00:03:14,829
And tracing can help identify these
unwanted, unsafe access patterns as well.

47
00:03:14,924 --> 00:03:15,144
And.

48
00:03:15,859 --> 00:03:19,489
Since you're track like tracking
individual requests, it can

49
00:03:19,489 --> 00:03:25,159
help you identify where exactly
the performance bottleneck is.

50
00:03:25,489 --> 00:03:30,999
And by doing so, it can reduce the time
to remediate, time to detect issues.

51
00:03:31,049 --> 00:03:34,619
And I cannot emphasize this enough,
but instrumenting applications

52
00:03:34,619 --> 00:03:38,009
carefully on your CUJ park becomes
so much more important because.

53
00:03:38,414 --> 00:03:44,234
Those give you golden signals on
where exactly an issue might lie.

54
00:03:44,414 --> 00:03:51,734
Often this means that you go in manually
instrument one span at a time and add

55
00:03:51,734 --> 00:03:56,635
value to your to the traces themselves,
right on that particular request flow.

56
00:03:59,079 --> 00:04:00,940
A little bit on our tracer platform.

57
00:04:00,940 --> 00:04:04,300
Our platform provides disable tracing
for all the Salesforce services.

58
00:04:04,330 --> 00:04:08,230
We are a centralized collection of
tracers, and we understand that these

59
00:04:08,230 --> 00:04:12,670
user journeys, the source of it can
come from different things, right?

60
00:04:12,670 --> 00:04:16,600
Different regions, different
application stacks and so forth.

61
00:04:16,600 --> 00:04:21,530
So we have the ability to collect
spans from different a PM agents, from

62
00:04:21,530 --> 00:04:24,800
managed frameworks, from infrastructure
layers, such as service mesh.

63
00:04:25,230 --> 00:04:28,100
Which is especially true for
our Kubernetes workloads.

64
00:04:28,490 --> 00:04:32,070
And we have tracing enable
on our integration test a

65
00:04:32,100 --> 00:04:33,260
little bit on the scale.

66
00:04:33,260 --> 00:04:37,765
We handle per, on the per minute basis,
we handle around 300 million spans.

67
00:04:38,215 --> 00:04:42,090
And we have around 10 million unique
traces that gets reported every minute.

68
00:04:43,675 --> 00:04:47,095
Let's briefly try to understand
the semantics of disability

69
00:04:47,095 --> 00:04:49,585
tracing because that will help
us understand this topic better.

70
00:04:49,645 --> 00:04:52,975
So consider a individual request
flow that goes across five different

71
00:04:52,975 --> 00:04:55,045
applications, S one to S five.

72
00:04:55,465 --> 00:04:59,695
Each of these services would emit
a certain record or an event, which

73
00:04:59,695 --> 00:05:01,465
is often referred to as a span.

74
00:05:01,915 --> 00:05:05,575
And a span record has few
characteristics, for example.

75
00:05:05,980 --> 00:05:07,460
Take the take S one.

76
00:05:07,460 --> 00:05:13,710
It has three spans at Ms. Ad and
h. Now each span has a start time a

77
00:05:13,710 --> 00:05:18,030
duration of the span and which operation
within the service does it represent.

78
00:05:18,420 --> 00:05:21,090
It can also have various other
metadata associated with it.

79
00:05:22,470 --> 00:05:26,970
And each of these services would
independently send their SPAN records.

80
00:05:26,970 --> 00:05:31,830
And all of these SPAN records remember,
are tied to a single Geo ID or a trace

81
00:05:31,830 --> 00:05:37,340
id, and they all are, say, collected
to some kind of collection framework

82
00:05:37,340 --> 00:05:39,140
and end up on the tracing backend.

83
00:05:39,200 --> 00:05:39,230
Okay.

84
00:05:40,160 --> 00:05:43,520
Once they arrive at the tracing
backend, you can actually now show a

85
00:05:43,520 --> 00:05:47,090
nice waterfall representation, which
is a hierarchy representation of the

86
00:05:47,090 --> 00:05:52,570
request flow by first overlaying or
placing the root span under which

87
00:05:52,570 --> 00:05:54,130
you start displaying the child span.

88
00:05:54,970 --> 00:05:58,180
The placement of the child span
is based on the start time of that

89
00:05:58,180 --> 00:06:02,290
particular span, and the width of
the span is generally indicative of

90
00:06:02,290 --> 00:06:03,760
the duration of this particular span.

91
00:06:05,395 --> 00:06:09,775
Now with this, you can track
each and every request that is

92
00:06:09,775 --> 00:06:11,705
happening on your infrastructure.

93
00:06:14,195 --> 00:06:16,145
Now, how does context
propagation happens now?

94
00:06:16,185 --> 00:06:20,625
Each of the spans that the services Emett
needs to be tied up to the same trace id.

95
00:06:21,000 --> 00:06:25,140
Now that happens through context
propagation, through transport headers.

96
00:06:25,470 --> 00:06:29,130
Consider this example where application
one is talking to application two.

97
00:06:29,200 --> 00:06:34,300
Say it's an HCTP post call as part of the
post call, it'll send in some headers.

98
00:06:34,630 --> 00:06:39,660
We at Salesforce today use B three
headers for legacy reasons, but you can

99
00:06:39,660 --> 00:06:41,620
use ot open elementary headers as well.

100
00:06:42,385 --> 00:06:47,275
As the trace contact is propagated to the
next hop, say in this case, application

101
00:06:47,275 --> 00:06:52,975
two, when it generates a new span, it'll
use this context propagated to it to start

102
00:06:52,975 --> 00:06:57,280
a span with the same request trace ID as
the caller which is the application one.

103
00:06:57,700 --> 00:07:02,180
And that's how every record in
the request path gets stitched

104
00:07:02,180 --> 00:07:03,350
up to the same presiding.

105
00:07:06,480 --> 00:07:10,500
The sampling flag is quite important
because the sampling flag basically

106
00:07:10,500 --> 00:07:14,310
would mean whether you are going
to collect that per trace or not.

107
00:07:14,310 --> 00:07:18,910
If it is set to zero, even though the
context fabrication happens generally,

108
00:07:18,990 --> 00:07:24,180
this application two would not emit
that span to the collection endpoint.

109
00:07:25,510 --> 00:07:31,110
This is often done to reduce the overall
volume of the trace data that you collect.

110
00:07:34,920 --> 00:07:38,040
How can you enable now CS
with distributed tracing?

111
00:07:38,820 --> 00:07:40,980
You can do that with synthetic test.

112
00:07:40,980 --> 00:07:45,690
So when you are enabling, synthetic
test, you always are enabling it

113
00:07:45,690 --> 00:07:47,010
with a hundred percent sampling.

114
00:07:47,310 --> 00:07:53,310
That means you, you firstly,
enable tracing on these synthetic

115
00:07:53,310 --> 00:07:58,605
tests and ensure that all of these
synthetic tests are sample with 100%.

116
00:07:59,025 --> 00:08:01,395
That means you don't drop
any kind of trace, right?

117
00:08:02,010 --> 00:08:06,240
So each of your synthetic tests
that is emulating the request flow

118
00:08:06,240 --> 00:08:07,770
is completely captured, right?

119
00:08:08,220 --> 00:08:13,010
And with that, you can actually now do
things like real time browser monitoring,

120
00:08:13,010 --> 00:08:16,490
a PM monitoring, DNS monitoring,
and so many other things, right?

121
00:08:16,940 --> 00:08:20,090
And overall, it gives you a
performance and availability

122
00:08:20,090 --> 00:08:23,690
from an outside in perspective,
which is what you really want.

123
00:08:26,295 --> 00:08:27,375
How would you do that?

124
00:08:27,375 --> 00:08:30,225
Firstly, you need to have a framework
to be able to enable you to do

125
00:08:30,225 --> 00:08:31,725
this, so you need to build one.

126
00:08:31,785 --> 00:08:37,315
But consider this case wherein you
are having a test a synthetic test.

127
00:08:37,615 --> 00:08:42,205
And each of the synthetic test
would have a set of steps and each

128
00:08:42,205 --> 00:08:45,175
step would emit its own trace.

129
00:08:46,265 --> 00:08:50,825
So remember all these steps would be
emitting their own trace and these traces

130
00:08:50,825 --> 00:08:54,035
are TA getting tied to the same test id.

131
00:08:54,365 --> 00:08:58,715
Also, these steps are
generally, are like templates.

132
00:08:58,745 --> 00:08:59,915
On these templates.

133
00:08:59,915 --> 00:09:04,475
You can apply different variables
for example, you want a test to

134
00:09:04,475 --> 00:09:10,235
be run from Europe, from Japan,
some US, so you can have the same.

135
00:09:10,775 --> 00:09:15,125
Test definition, but you can apply, you
can have different kind of variables that

136
00:09:15,125 --> 00:09:20,705
gets supplied onto this the step and the
emulation of that particular user journey

137
00:09:20,705 --> 00:09:22,115
happens from that particular region.

138
00:09:25,085 --> 00:09:27,155
So that's how you can enable it.

139
00:09:27,155 --> 00:09:31,595
But once you enable your synthetic, you
can give this particular kind of view

140
00:09:31,625 --> 00:09:34,805
where you show individual execution runs.

141
00:09:35,480 --> 00:09:38,630
And when you try to drill down on
a particular execution run, it will

142
00:09:38,630 --> 00:09:40,610
show associated trace IDs with it.

143
00:09:41,090 --> 00:09:44,600
You can click on the trace IDs
and that will give it to the, take

144
00:09:44,600 --> 00:09:45,890
you to the same waterfall view.

145
00:09:46,500 --> 00:09:50,720
The graph here, if it's one,
it shows a successful run.

146
00:09:50,780 --> 00:09:53,150
And if it's a minus one,
it shows a failure, right?

147
00:09:53,540 --> 00:09:56,030
And you can drill down to that
particular taste to see where

148
00:09:56,030 --> 00:09:57,020
the failure had happened.

149
00:09:57,380 --> 00:09:58,520
So at a very.

150
00:09:59,600 --> 00:10:05,100
Raw or a level, you can actually
track the execution at a trace level.

151
00:10:05,200 --> 00:10:08,340
But is it useful to actually track it?

152
00:10:08,760 --> 00:10:09,600
Yeah, it is.

153
00:10:09,630 --> 00:10:11,400
But you can probably do more, right?

154
00:10:11,480 --> 00:10:16,610
You can do things like aggregating these
traces to get more meaningful insights.

155
00:10:17,060 --> 00:10:19,580
For example, consider this
case wherein you're doing a.

156
00:10:20,255 --> 00:10:21,635
Create account transaction.

157
00:10:21,635 --> 00:10:23,285
You can see that with a metric.

158
00:10:23,285 --> 00:10:26,405
Generally it takes around 85 milliseconds
for this operation to complete,

159
00:10:26,885 --> 00:10:30,575
but you don't know exactly where it
is spending most of your time in.

160
00:10:30,675 --> 00:10:34,695
Through tracing that particular flow
has different services involved,

161
00:10:34,695 --> 00:10:36,615
like app or storage or attend db.

162
00:10:37,195 --> 00:10:42,045
But you don't know exactly where
the time is being well spent, so

163
00:10:42,045 --> 00:10:44,745
you look at similar looking traces.

164
00:10:45,060 --> 00:10:49,570
Aggregate them, and you can give a time
series view like this which will say that,

165
00:10:49,570 --> 00:10:53,980
okay, authentication operation is the
one which is taking the most time, right?

166
00:10:54,730 --> 00:10:57,460
This is a very powerful view
by which you can quickly drill

167
00:10:57,460 --> 00:10:59,110
down towards a problematic.

168
00:10:59,795 --> 00:11:02,245
Or where the problem is
actually having, right?

169
00:11:02,755 --> 00:11:04,225
This same view.

170
00:11:04,315 --> 00:11:09,235
Now, you can have not just for latency
degradation, but even for errors.

171
00:11:09,295 --> 00:11:12,775
So also you can slice and
d this data as you choose.

172
00:11:12,775 --> 00:11:16,525
For example, you can have this
contributors for a particular

173
00:11:16,525 --> 00:11:18,655
step across different runs.

174
00:11:19,480 --> 00:11:24,820
You can have this particular graph
or different variables for say only

175
00:11:24,820 --> 00:11:27,850
for Japan location or a US location.

176
00:11:27,850 --> 00:11:30,730
You want to see how the
contribution looks like and where

177
00:11:30,730 --> 00:11:31,870
exactly the issue is happening.

178
00:11:31,870 --> 00:11:32,825
That can be done as well.

179
00:11:34,300 --> 00:11:37,510
You can overlay this with some kind
of baseline performance to see.

180
00:11:37,900 --> 00:11:42,880
Changes or trends that has happened
week or month over month Overall,

181
00:11:42,880 --> 00:11:48,580
you can quickly find out where the
possibility of optimization lies by

182
00:11:48,580 --> 00:11:52,710
identifying the faulty service which
is causing the performance degradation.

183
00:11:54,675 --> 00:11:58,785
You can also do more
with trace segregation.

184
00:11:58,785 --> 00:12:04,755
You can create a directed graph
representation of the flow of the request.

185
00:12:05,245 --> 00:12:06,055
Like this, right?

186
00:12:06,055 --> 00:12:11,245
You basically are giving a directed
graph representation of how the

187
00:12:11,245 --> 00:12:16,145
request was flowing and what the health
of those individual requests were

188
00:12:16,145 --> 00:12:18,145
looking at an aggregated level, right?

189
00:12:18,175 --> 00:12:19,615
For example, you had 72.

190
00:12:20,065 --> 00:12:24,595
Runs of a particular test, out of
which as they were going across

191
00:12:24,595 --> 00:12:26,395
different services started failing.

192
00:12:26,445 --> 00:12:30,355
When the core was calling Salesforce
app 12 of them failed, right?

193
00:12:30,715 --> 00:12:33,925
Remember, all these traces
are already sampled, so you're

194
00:12:33,925 --> 00:12:34,885
already collecting them.

195
00:12:34,885 --> 00:12:36,205
You're not dropping anything.

196
00:12:36,265 --> 00:12:40,865
So you can get a, you can
enable a view like this to be

197
00:12:40,865 --> 00:12:43,475
able to get nice quick view of.

198
00:12:44,010 --> 00:12:47,880
How the flow of the request is happening
and where exactly an issue might lie.

199
00:12:50,650 --> 00:12:53,770
I would also like to emphasize
the need for on demand tracing.

200
00:12:53,770 --> 00:12:57,580
Sometimes when in user flows, there
may be one user which has some

201
00:12:57,580 --> 00:13:02,530
issue, and you may want to actually
enable tracing for a specific user,

202
00:13:02,950 --> 00:13:05,140
so have mechanisms to enable that.

203
00:13:05,140 --> 00:13:09,745
We have created our phone framework
to actually enable it and sometimes.

204
00:13:10,090 --> 00:13:14,560
Issues are sparse and sporadic, and
they happen maybe only at certain time

205
00:13:14,560 --> 00:13:16,000
of the day that you're not aware of.

206
00:13:16,220 --> 00:13:21,210
Long-term tracing is also sometimes
important and have the ability to do that.

207
00:13:21,700 --> 00:13:26,160
We also have a, have the ability to
do instance waste tracing because

208
00:13:26,550 --> 00:13:28,770
you deploy multiple instances.

209
00:13:28,885 --> 00:13:32,220
Each instance may be tied to a
tenant and one instance might be

210
00:13:32,220 --> 00:13:33,420
the one which is having issue.

211
00:13:33,750 --> 00:13:38,250
So you can enable tracing on a particular
instance and then the critical user

212
00:13:38,250 --> 00:13:42,210
journey flow through that instance can
capture what exactly might be happening.

213
00:13:43,200 --> 00:13:43,410
Yeah.

214
00:13:43,830 --> 00:13:46,260
And with that, I would like
to conclude my talk here.

215
00:13:46,300 --> 00:13:47,200
Thank you everyone.

216
00:13:47,200 --> 00:13:47,260
I.

