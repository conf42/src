1
00:00:00,500 --> 00:00:02,360
Hi, this Ishi.

2
00:00:02,735 --> 00:00:07,085
I am going to talk about how Rust
is powering Modern Data Lakes.

3
00:00:07,604 --> 00:00:09,264
I'll start with my introduction quickly.

4
00:00:09,414 --> 00:00:13,234
I work in data architecture,
data platform space.

5
00:00:13,264 --> 00:00:15,664
I've been in the industry
for around 20 years.

6
00:00:16,214 --> 00:00:19,095
Been working with large scale
financial services companies

7
00:00:19,544 --> 00:00:21,599
to build modern data platforms.

8
00:00:21,860 --> 00:00:26,229
Near real time analytical
and data based applications.

9
00:00:26,729 --> 00:00:28,949
Yeah with that, let's
start the conversation.

10
00:00:29,450 --> 00:00:33,070
So first we are gonna talk about, we
are gonna look at the the landscape

11
00:00:33,070 --> 00:00:36,460
of enterprise data architecture,
which has actually undergone

12
00:00:36,490 --> 00:00:39,670
a fundamental transformation
over the last 15 years or so.

13
00:00:39,920 --> 00:00:41,900
I have laid out three
different phases here.

14
00:00:41,900 --> 00:00:42,200
Phase one.

15
00:00:43,025 --> 00:00:44,915
Which is on-premise Hadoop era.

16
00:00:45,225 --> 00:00:51,285
In around early 2000 where Hadoop was
launched it had two primary components

17
00:00:51,285 --> 00:00:56,315
such DFS and map produce which dominated
big data processing, large scale data

18
00:00:56,315 --> 00:01:01,195
processing with JV based technologies
primarily to handle massive data sets.

19
00:01:01,675 --> 00:01:04,325
It was a big hit, but it had
some limitations, to scale.

20
00:01:04,765 --> 00:01:07,015
And to meet the performance needs.

21
00:01:07,135 --> 00:01:10,845
Some of the limitations were delivered
to garbage collection, overheads,

22
00:01:11,265 --> 00:01:12,735
and memory management issues.

23
00:01:13,245 --> 00:01:18,785
Lately, after a few few years in
mid 2010s cloud data warehouses

24
00:01:18,785 --> 00:01:24,635
were launched which were primarily
targeted to to to all limitations

25
00:01:24,635 --> 00:01:26,315
of kadu and to meet the performance.

26
00:01:27,005 --> 00:01:32,705
Criteria and solve more usability
problems support analytical applications

27
00:01:32,765 --> 00:01:34,685
and SQL friendly applications.

28
00:01:34,895 --> 00:01:37,775
So Snowflake and Redshift are
some of the examples there.

29
00:01:38,195 --> 00:01:42,915
And then more recently, Lakehouse
is trying to converge the data

30
00:01:42,915 --> 00:01:47,405
lakes and and cloud data warehouse
to give you the best of both.

31
00:01:47,655 --> 00:01:49,665
And that is achieved using open formats.

32
00:01:50,370 --> 00:01:52,950
Unified processing and
separation of story and compute.

33
00:01:53,050 --> 00:01:56,930
And also Rust has emerged as a critical
technology for modern lake houses.

34
00:01:56,930 --> 00:02:00,100
So we are going to deep dive
into these three phases in the

35
00:02:00,100 --> 00:02:01,480
rest of the conversation here.

36
00:02:01,980 --> 00:02:05,760
So why rest for data infrastructure and
Rust has a unique value proposition.

37
00:02:06,160 --> 00:02:11,080
Memory safety with without garbage
collection no garbage collection

38
00:02:11,080 --> 00:02:14,380
process are required during critical
large scale data processing,

39
00:02:14,500 --> 00:02:16,240
especially those jobs which runs.

40
00:02:17,005 --> 00:02:21,280
Long time type system
data racists at compiling.

41
00:02:21,815 --> 00:02:26,915
And it also allows parallelism for
multicore effective utilization, so

42
00:02:26,915 --> 00:02:29,415
of multicore compute infrastructure.

43
00:02:29,655 --> 00:02:32,365
And it gives you predictable
resource usage with

44
00:02:32,365 --> 00:02:33,865
deterministic primary management.

45
00:02:34,484 --> 00:02:41,145
So let's see how phase one limitations
were, what phase one limitations were.

46
00:02:41,910 --> 00:02:45,240
So first is Jian based, tasks, right?

47
00:02:45,330 --> 00:02:50,900
Hadoop was primarily used to have tasks,
lab tasks, and reduced tasks and and

48
00:02:50,900 --> 00:02:53,670
Hadoop was basically jian based execution.

49
00:02:54,030 --> 00:02:58,530
So each task used to meet its own
heap space and the object wave

50
00:02:58,560 --> 00:03:01,620
presentations added 12 to 16 bytes.

51
00:03:01,965 --> 00:03:07,155
For instance, and as you scale, it used to
create many maps and many reduced tasks.

52
00:03:07,515 --> 00:03:12,454
They used to they used to exchange
data which which needed to serialize de

53
00:03:12,480 --> 00:03:17,980
serialize the data right before and after
the exchange which added the serialization

54
00:03:17,980 --> 00:03:22,800
de serialization cost to move the data
movement between JVMs which was very

55
00:03:22,860 --> 00:03:25,140
significant in terms of CPU and memory.

56
00:03:25,935 --> 00:03:26,205
Neat.

57
00:03:26,685 --> 00:03:30,915
And the third is the garbage collection
Pauses, because it's a jian based, it used

58
00:03:30,915 --> 00:03:36,405
to do unpredictable GC pauses, sometimes
few seconds causing a lot of inconsistent

59
00:03:36,405 --> 00:03:38,565
processing times and resource utilization.

60
00:03:39,135 --> 00:03:44,160
How rust is how Rust has created an
alternative to map these frameworks.

61
00:03:44,485 --> 00:03:46,315
So Rust has created an alternative.

62
00:03:46,815 --> 00:03:50,245
As you can see here this chart shows
the performance comparison of the

63
00:03:50,245 --> 00:03:55,344
word count which is one of the popular
benchmark in the big data large scale.

64
00:03:55,644 --> 00:03:57,014
Data processing world.

65
00:03:57,264 --> 00:04:02,704
And as you can see, rust has actually
rust is actually finished the latest

66
00:04:03,164 --> 00:04:09,394
the data fusion, which is based on rust,
has finished the the work count on the

67
00:04:09,524 --> 00:04:11,534
large skin data in just in eight seconds.

68
00:04:11,564 --> 00:04:12,074
So it has.

69
00:04:12,639 --> 00:04:15,639
Work on all the records and
it's super fast compared to

70
00:04:16,089 --> 00:04:17,899
how do spark or even Ruston.

71
00:04:18,709 --> 00:04:22,999
So risk-based imp implementations
of all the frameworks basically

72
00:04:22,999 --> 00:04:27,229
eliminated GC process while providing
more predictive resource utilization.

73
00:04:27,379 --> 00:04:30,249
And we are gonna see more
on this in the follow slide.

74
00:04:30,279 --> 00:04:33,159
So phase two was the cloud data warehouse.

75
00:04:33,259 --> 00:04:34,069
We talked about it.

76
00:04:34,444 --> 00:04:37,434
The primary some of the examples
of these are snowflake and

77
00:04:37,974 --> 00:04:39,084
and Redshift of the world.

78
00:04:39,474 --> 00:04:43,794
And the primary limitation there
is to, in order to use this cloud

79
00:04:43,794 --> 00:04:47,214
data warehouses historically
you needed to load data there.

80
00:04:47,634 --> 00:04:51,504
So they used to store data in
proprietary formats like Snowflake

81
00:04:51,504 --> 00:04:53,119
stores data in in the packs.

82
00:04:53,839 --> 00:04:56,689
Which is their hybrid column
or storage format, right?

83
00:04:56,719 --> 00:05:01,079
Pql and Redshift store data into their
proprietary formats which created a vendor

84
00:05:01,079 --> 00:05:03,029
log vendor locking for the enterprises.

85
00:05:03,029 --> 00:05:06,779
So if you wanna load data into these
platforms, it's gonna create a lock-in

86
00:05:07,209 --> 00:05:12,279
for you because the, it's gonna have
a egress cost to take data out later.

87
00:05:12,399 --> 00:05:17,209
And you're gonna use compute to load
data and compute to query the data.

88
00:05:17,584 --> 00:05:20,104
So that was a big limitation there.

89
00:05:20,204 --> 00:05:23,224
And the model of the
compute was paper pay.

90
00:05:23,584 --> 00:05:28,184
The model for the compute price was
paper compute which was incentivized

91
00:05:28,184 --> 00:05:32,594
for optimization, but it limited the
control over execution if you wanna scale

92
00:05:32,594 --> 00:05:34,634
just storage or compute or vice versa.

93
00:05:35,024 --> 00:05:37,334
Storage and compute used
to be tightly coupled.

94
00:05:37,764 --> 00:05:40,434
And additionally there was
very limited ML support.

95
00:05:40,809 --> 00:05:44,529
Primarily these applications, these
platforms were built for OLA use

96
00:05:44,529 --> 00:05:46,799
cases not machine learning use cases.

97
00:05:46,799 --> 00:05:52,149
If you want to use data for ML use cases,
you gotta to take data out and then use it

98
00:05:52,149 --> 00:05:56,199
in some other platforms, which has created
some silos and duplicate data storage.

99
00:05:56,699 --> 00:05:58,264
So with that, let's look at.

100
00:05:58,764 --> 00:06:03,254
All frameworks have recently emerged
and components which have recently

101
00:06:03,254 --> 00:06:05,024
emerged, which are powered by rest.

102
00:06:05,354 --> 00:06:10,504
So Apache you might have been found there
with which is a column of memory standard.

103
00:06:10,904 --> 00:06:14,774
It's a rest based implementation
providing zero copy access to column

104
00:06:14,894 --> 00:06:17,144
data with predictable memory layout.

105
00:06:17,334 --> 00:06:20,514
It enables efficient processing
across different long languages.

106
00:06:21,309 --> 00:06:22,869
And no civilization costs, right?

107
00:06:22,999 --> 00:06:27,329
Data fusion is another project
which is followed by rust to to

108
00:06:27,329 --> 00:06:29,999
enable performance SQL capabilities.

109
00:06:30,049 --> 00:06:35,069
It's it's native and it provides
both SQL and data frame APIs.

110
00:06:35,839 --> 00:06:37,129
Physical query optimization.

111
00:06:37,579 --> 00:06:39,319
And the third one is efficient storage.

112
00:06:39,559 --> 00:06:40,969
Lance and Par k Rs.

113
00:06:41,479 --> 00:06:44,359
These are also resonating
implementations of column or storage

114
00:06:44,359 --> 00:06:48,349
formats, delivering performance
improvement of three to 10 x. Right?

115
00:06:48,349 --> 00:06:52,789
Based on some of the recent benchmarks
here and in general, as you can see

116
00:06:52,789 --> 00:06:58,499
here, memory usage comparison for nurse
base query engines is typically 32 70%

117
00:06:58,499 --> 00:07:00,869
less based on some of the recent tests.

118
00:07:01,289 --> 00:07:06,494
Then j counterpart counterparts when the,
when while processing the same dataset.

119
00:07:07,064 --> 00:07:10,604
So that's pretty significant
and powerful and game changing

120
00:07:10,604 --> 00:07:14,704
in terms of infrastructure and,
efficient infrastructure and

121
00:07:14,704 --> 00:07:16,654
data processing applications.

122
00:07:17,194 --> 00:07:20,464
So let's look at phase three
and which is where we are today.

123
00:07:20,824 --> 00:07:23,604
The modern lakehouse architecture
the Lakehouse paradigm.

124
00:07:23,604 --> 00:07:25,614
As I said, it gives you the best of both.

125
00:07:26,154 --> 00:07:31,804
Warehouse and and data lakes and which
addresses the limitations of both OOP

126
00:07:31,804 --> 00:07:33,814
era and the cloud data warehouse era.

127
00:07:33,874 --> 00:07:36,844
And it does that by using
open storage formats.

128
00:07:37,084 --> 00:07:40,894
Some of those are data lake is
for hoodie which provides stable

129
00:07:40,894 --> 00:07:42,724
formats, which is asset compliant.

130
00:07:42,814 --> 00:07:45,604
You can do asset transactions,
you can change schema.

131
00:07:46,084 --> 00:07:47,644
Schema can evolve over time.

132
00:07:48,029 --> 00:07:51,209
And you can also do time travel
on the scalable object storage.

133
00:07:51,539 --> 00:07:53,909
It basically decouples
compute and storage.

134
00:07:53,934 --> 00:07:57,894
It it gives you ability to do
single processing layer to sql,

135
00:07:57,894 --> 00:08:01,924
both to serve both SQL users
and machine learning users which

136
00:08:01,924 --> 00:08:04,324
eliminates the need for copy of data.

137
00:08:04,794 --> 00:08:09,294
The architecture is bonds scalable
because you're decoupling your

138
00:08:09,294 --> 00:08:10,434
storage and compute layer.

139
00:08:10,824 --> 00:08:15,024
So separating them allows independent
scaling of resources with the ones.

140
00:08:15,444 --> 00:08:17,294
Met management, manage.

141
00:08:17,349 --> 00:08:22,919
So what is the risk role in RU'S
role in Lakehouse architecture

142
00:08:23,039 --> 00:08:24,479
or Lakehouse, infrastructure?

143
00:08:24,479 --> 00:08:28,899
So the key components that are
powered by RU today are table formats.

144
00:08:28,899 --> 00:08:32,849
There are table formats, which are, which
allows you to interact with with the

145
00:08:32,849 --> 00:08:37,589
deltaic cables or iceberg tables using
Delta and iceberg various libraries.

146
00:08:37,649 --> 00:08:39,389
It supports without going through Spark.

147
00:08:39,734 --> 00:08:43,004
So you can interact with these stable
formats without going through spark

148
00:08:43,464 --> 00:08:47,664
compute, which is J based compute
and native asset transactions

149
00:08:48,114 --> 00:08:49,884
support with minimal overhead.

150
00:08:50,424 --> 00:08:50,544
Right?

151
00:08:50,574 --> 00:08:54,764
There is another there are column of
storage format implementations, which

152
00:08:54,764 --> 00:08:56,954
are host based, which we saw earlier.

153
00:08:56,954 --> 00:08:59,754
Arrow Parque, ris and Lance.

154
00:09:00,279 --> 00:09:03,689
Which allows predictive memory
usage and serial copy data access.

155
00:09:04,019 --> 00:09:07,439
In terms of query processing, there
is data fusion, which has emerged as

156
00:09:07,439 --> 00:09:12,269
a very promising project recently,
and it allows parallel execution

157
00:09:12,269 --> 00:09:13,589
with minimal resource footprint.

158
00:09:14,109 --> 00:09:18,339
And last is data frame,
bilities, polars, and arrow.

159
00:09:18,339 --> 00:09:21,759
Data fusion is 10 to hundred
x faster based on some of the

160
00:09:21,759 --> 00:09:23,709
recent benchmarks that Python has.

161
00:09:23,809 --> 00:09:24,559
With less memory.

162
00:09:24,919 --> 00:09:29,309
So these are some of the powerful
results that, that rust has shown in

163
00:09:29,309 --> 00:09:34,049
benchmarking which is gonna radically
change how the lakehouse, modern lakehouse

164
00:09:34,049 --> 00:09:35,699
architecture and infrastructure are built.

165
00:09:36,539 --> 00:09:41,039
And as you can see, some of the
panel performance be marks was for

166
00:09:41,039 --> 00:09:44,999
us versus traditional technologies,
is 10 x more query performance.

167
00:09:45,344 --> 00:09:48,794
Using data fusion compared to
opposite spark on the same dataset,

168
00:09:48,794 --> 00:09:50,684
configuration and hardware configuration.

169
00:09:51,114 --> 00:09:56,904
85% memory reduction required for
polars data frame compared to pandas

170
00:09:57,204 --> 00:09:58,794
while processing a hundred GB dataset.

171
00:09:58,974 --> 00:10:00,764
30 x data.

172
00:10:00,974 --> 00:10:01,934
Faster data loading.

173
00:10:02,084 --> 00:10:06,674
If you're loading data in an application
from par K, you can load 30 x more data.

174
00:10:07,164 --> 00:10:13,154
As by using by using Arrow Rs compared
to pyro for terabyte scale data sets.

175
00:10:13,244 --> 00:10:17,814
So truly truly for large scale and
enterprise scale analytical processing

176
00:10:17,814 --> 00:10:25,584
here, and 99.9% reliability because you
are not going to get outta memory errors

177
00:10:26,124 --> 00:10:27,954
and there is no garbage collection needed.

178
00:10:28,334 --> 00:10:31,994
For long running data processing
jobs, which is one of the limitations

179
00:10:31,994 --> 00:10:34,304
for dealing based executions.

180
00:10:34,814 --> 00:10:36,434
So with that, I'm gonna
move to next slide.

181
00:10:36,624 --> 00:10:38,934
What are some of the practical
implementation patterns?

182
00:10:38,934 --> 00:10:42,994
When I should use, how should I, if
I'm using it, how should I optimize?

183
00:10:43,859 --> 00:10:46,209
If I'm not using rest
where should I use it?

184
00:10:46,509 --> 00:10:48,489
So there are three
integration approaches, right?

185
00:10:48,489 --> 00:10:50,979
Some one is microservices components.

186
00:10:51,029 --> 00:10:53,999
You can identify critical data
processing applications in.

187
00:10:54,479 --> 00:10:59,079
In your overall architecture and do
risk-based implementation, risk-based

188
00:10:59,079 --> 00:11:03,609
implementation and services and
build the interfaces around it so

189
00:11:03,609 --> 00:11:08,369
you can have highly performant and,
efficient applications as services,

190
00:11:08,669 --> 00:11:11,459
which could be integrated with other
components in the architecture.

191
00:11:11,489 --> 00:11:14,309
Secondly, the extension
libraries can create native

192
00:11:14,309 --> 00:11:16,739
extensions for Python on JVM.

193
00:11:17,239 --> 00:11:18,319
As a core processing.

194
00:11:18,799 --> 00:11:22,939
And if you want, you can even replace your
entire system or architecture for data

195
00:11:22,939 --> 00:11:26,299
pipelines and data processing with thrust
alternative for the maximum benefit.

196
00:11:26,779 --> 00:11:30,949
Some of the common use cases
where rust really shines is high

197
00:11:30,949 --> 00:11:34,579
throughput data emission pipelines,
which is processing millions or.

198
00:11:35,079 --> 00:11:37,539
Millions of records or events per second.

199
00:11:38,029 --> 00:11:42,929
Last key data processing of, feature
engineering for ML training right to 10

200
00:11:42,929 --> 00:11:46,749
your models and to identify the feature,
which needs a lot of data processing.

201
00:11:46,749 --> 00:11:51,539
Of course, interactive SQL query
engines requiring sub response time

202
00:11:51,959 --> 00:11:56,609
is another use case where we, rust
based application would really shine.

203
00:11:56,909 --> 00:11:59,849
So most organizations dig in
with targeted replacement.

204
00:12:00,249 --> 00:12:04,529
Of bottleneck components which are
already identified as a bottleneck.

205
00:12:04,529 --> 00:12:08,849
And then they start rewriting
or replacing those components or

206
00:12:08,849 --> 00:12:11,009
services with rust based applications.

207
00:12:11,319 --> 00:12:15,319
Then do doing a completely rewrite,
and then some at some point you,

208
00:12:15,769 --> 00:12:19,019
you might want to replace the entire
application depends on what you're

209
00:12:19,019 --> 00:12:22,789
doing in your data processing and
how how much bottlenecks you have.

210
00:12:23,419 --> 00:12:25,639
So with that, and we are gonna
talk about some of the key

211
00:12:25,639 --> 00:12:27,529
takeaways and the next steps.

212
00:12:27,579 --> 00:12:30,759
Rust addresses critical
performance limitations.

213
00:12:30,910 --> 00:12:34,579
Just to recap, a does not
need garbage collection.

214
00:12:34,640 --> 00:12:38,950
Predictable performance and efficient
resource utilization makes rust

215
00:12:39,010 --> 00:12:40,720
ideal for data infrastructure.

216
00:12:40,990 --> 00:12:45,020
The ecosystem, rust ecosystem is
maturing rapidly with arrow data,

217
00:12:45,020 --> 00:12:46,760
fusion delta leak, and colors.

218
00:12:47,135 --> 00:12:50,435
Libraries libraries and
trust based implementation.

219
00:12:50,885 --> 00:12:51,665
There are many.

220
00:12:51,785 --> 00:12:54,835
The whole whole landscape is
production ready and evolving

221
00:12:54,835 --> 00:12:59,045
rapidly for high performing data
platforms for organizations.

222
00:12:59,645 --> 00:13:03,975
I recommend you begin with isolated
components analysis and see which

223
00:13:04,035 --> 00:13:08,085
interfaces can be replaced or
enhanced or optimized using rust.

224
00:13:09,045 --> 00:13:14,155
Focus on the bottlenecks first and then
scale to the other components as well.

225
00:13:14,495 --> 00:13:17,885
So you can really use risk performance
characteristics for building

226
00:13:17,885 --> 00:13:21,425
modern lake data platforms and
future proof of your architecture.

227
00:13:22,055 --> 00:13:26,255
So with that, I would say I recommend
you evaluate your current data platform

228
00:13:26,255 --> 00:13:30,955
architecture and identify components
that would benefit most from gross

229
00:13:30,955 --> 00:13:32,965
performance and reliability advantages.

230
00:13:33,610 --> 00:13:34,890
And that's it from me.

231
00:13:34,980 --> 00:13:39,810
I I I hope this was helpful and
this information is helpful for

232
00:13:39,810 --> 00:13:43,120
you and your your data platform
modernization initiatives.

233
00:13:43,670 --> 00:13:46,100
Thank you for giving me
opportunity and thank you for

234
00:13:46,100 --> 00:13:47,870
listening to my presentation.

235
00:13:48,370 --> 00:13:48,590
Bye.

