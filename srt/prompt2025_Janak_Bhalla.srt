1
00:00:00,500 --> 00:00:01,429
Good morning everyone.

2
00:00:01,699 --> 00:00:05,359
Today I'm gonna talk about the most
exciting developments in software

3
00:00:05,359 --> 00:00:10,280
engineering, how we are using artificial
intelligence to transform code at massive

4
00:00:10,280 --> 00:00:15,415
scale, far beyond simple auto complete
suggestions to intelligent repository.

5
00:00:15,415 --> 00:00:18,465
By transformation, we are
living in a pivotal moment.

6
00:00:19,245 --> 00:00:22,940
Developers have traditionally relied
on like line by line tools and

7
00:00:22,940 --> 00:00:24,740
manual refactoring for large changes.

8
00:00:25,170 --> 00:00:29,570
But what if we could use AI for
doing entire package migrations,

9
00:00:30,080 --> 00:00:35,300
build and buck fixing, which
involves automated error resolutions?

10
00:00:35,600 --> 00:00:41,030
COVID refactoring structural improvements,
maintaining a specific behavior, fixing

11
00:00:41,030 --> 00:00:46,210
code style issues, enforcing a standards
automatically static analysis based

12
00:00:46,210 --> 00:00:49,215
repair, fixing link, or analyzer phonic.

13
00:00:49,870 --> 00:00:53,710
Static analysis annotations, such as
adding type hints or documentations.

14
00:00:54,210 --> 00:00:57,720
Let me show you how we are
actually using AI powered large

15
00:00:57,720 --> 00:00:59,370
scale code transformations.

16
00:00:59,870 --> 00:01:03,109
Before we talk about solution,
let's understand the core

17
00:01:03,109 --> 00:01:04,579
problems we are trying to solve.

18
00:01:05,150 --> 00:01:09,440
There are three fundamental challenges
that prevent Standard L LMS from handling

19
00:01:09,440 --> 00:01:11,345
large scale transformations effectively.

20
00:01:11,844 --> 00:01:14,424
First context window limitations.

21
00:01:14,574 --> 00:01:15,564
Here's the variety.

22
00:01:15,864 --> 00:01:17,664
Most code bases are massive.

23
00:01:18,054 --> 00:01:21,504
You can't fit an entire repository
in a single L-L-M-A-P-I call.

24
00:01:22,134 --> 00:01:25,644
We need to break the code base
into intelligent chunks and

25
00:01:25,644 --> 00:01:29,514
orchestrate multiple LLM calls
each with the right context.

26
00:01:29,874 --> 00:01:33,564
This requires sophisticated
mechanism that understand software

27
00:01:33,564 --> 00:01:37,584
structure, not just splitting code
randomly at character boundaries.

28
00:01:37,914 --> 00:01:40,104
Problem two, knowledge cutoff.

29
00:01:40,789 --> 00:01:43,164
LLM are trained up to a certain date.

30
00:01:43,344 --> 00:01:47,874
If a code basis use a library
that was updated yesterday, the

31
00:01:47,874 --> 00:01:49,464
model wouldn't know about it.

32
00:01:49,794 --> 00:01:53,544
This is especially critical in
programming where API changes happen

33
00:01:53,544 --> 00:01:58,074
frequently and outdated knowledge
can lead to incorrect solutions.

34
00:01:58,464 --> 00:02:03,509
Problem three, hallucinations ELs are
fantastic at generating code that looks.

35
00:02:04,120 --> 00:02:08,560
Good syntactically correct, but they're
also prone to making things up using

36
00:02:08,590 --> 00:02:13,690
APIs that don't exist and ignoring
edge cases or implementing logic that

37
00:02:13,690 --> 00:02:18,130
reasonable, that seems reasonable,
but it is fundamentally flawed.

38
00:02:18,130 --> 00:02:20,560
For critical transformations,
this is unacceptable.

39
00:02:21,310 --> 00:02:25,870
These three challenges mean that LMS
alone, even powerful ones, simply

40
00:02:25,870 --> 00:02:31,359
cannot reliably handle repository
scale core transmissions without

41
00:02:31,359 --> 00:02:33,490
human interventions at every step.

42
00:02:33,990 --> 00:02:35,435
Your Enters code plan.

43
00:02:36,065 --> 00:02:41,585
Code Plan is basically a symbolic
framework that combines the creative

44
00:02:41,585 --> 00:02:46,325
power of large language models with
the rigor and precision of traditional

45
00:02:46,325 --> 00:02:48,425
symbolic reasoning and static analysis.

46
00:02:49,175 --> 00:02:50,855
What does code plan do?

47
00:02:51,395 --> 00:02:54,360
It automates large
independent code changes.

48
00:02:55,220 --> 00:02:59,690
Think package migrations across your
code base, multiple file refactorings

49
00:02:59,720 --> 00:03:01,460
of modernizing legacy code.

50
00:03:01,880 --> 00:03:06,410
The key innovation here is treating
repository level coding as a planning

51
00:03:06,410 --> 00:03:08,360
problem, not just a generation problem.

52
00:03:09,350 --> 00:03:12,020
Code plan integrates LLM
with intelligent planning.

53
00:03:12,050 --> 00:03:15,820
It doesn't just generate code, it
reasons about what needs to happen,

54
00:03:15,880 --> 00:03:21,015
breaks it into steps, validates those
steps, and orchestrates the execution.

55
00:03:21,850 --> 00:03:26,230
Some real use cases are C
sharp package migrations across

56
00:03:26,320 --> 00:03:28,270
your entire code repository.

57
00:03:28,720 --> 00:03:33,340
Python dependency updates that touch
multiple services, static analysis

58
00:03:33,340 --> 00:03:37,360
driven repairs when you're fixing
violations, or compliance issues.

59
00:03:37,810 --> 00:03:40,715
Adding type annotations
to UN type code at scale.

60
00:03:41,215 --> 00:03:42,595
Code Plan is extensible.

61
00:03:42,685 --> 00:03:47,345
It's built on modules and
transformations, actions and prompts

62
00:03:47,345 --> 00:03:51,365
that you can customize for your
specific in software engineering tasks.

63
00:03:51,865 --> 00:03:56,605
Code plan at a high level, is a symbolic
framework for building AI powered software

64
00:03:56,605 --> 00:04:00,085
development workflows for inner and
outer loops for software engineering.

65
00:04:00,535 --> 00:04:07,135
It in the power of LMS with intelligent
planning for tackling complex coding tasks

66
00:04:07,165 --> 00:04:12,155
that cannot be solved using LLM alones
it basically combines the power of LLM

67
00:04:12,215 --> 00:04:15,395
and symbolic static analysis approaches.

68
00:04:15,895 --> 00:04:16,185
Okay.

69
00:04:16,250 --> 00:04:19,850
Let's, lemme walk you through
what, how code plan actually works.

70
00:04:20,210 --> 00:04:24,350
The architecture has majorly
three competence, input, code,

71
00:04:24,350 --> 00:04:25,940
plan, engine, and output.

72
00:04:26,375 --> 00:04:31,334
The input basically consists of your code
repository or directory knowledge based

73
00:04:31,334 --> 00:04:36,854
repositories with precise transformation
instructions and examples, static analysis

74
00:04:36,854 --> 00:04:39,404
reports and logs that identify issues.

75
00:04:39,974 --> 00:04:44,924
The code plan engine consists of four
major aspects, prompt contests, the first

76
00:04:45,194 --> 00:04:48,464
prompts contextualization before we pass.

77
00:04:48,964 --> 00:04:52,924
Before we ask anything to the
lm, we carefully prepare prompts

78
00:04:52,984 --> 00:04:54,784
with most relevant context.

79
00:04:54,934 --> 00:04:55,804
This is crucial.

80
00:04:56,104 --> 00:04:58,234
Bad context can lead to bad outputs.

81
00:04:58,534 --> 00:05:00,269
Second planning and orchestration.

82
00:05:00,874 --> 00:05:05,494
Complex transformations are in one
short operations code plan, breaks them

83
00:05:05,494 --> 00:05:09,394
into structured steps and coordinates
the execution, making decisions about

84
00:05:09,394 --> 00:05:11,134
which tools to use and in what order.

85
00:05:11,794 --> 00:05:13,024
Dependency analysis.

86
00:05:13,024 --> 00:05:18,544
Using a ST, we pass your code using
abstracts and text trees, a representation

87
00:05:18,544 --> 00:05:23,104
that captures the actual structure
of your code, not just a text.

88
00:05:23,344 --> 00:05:26,289
This lets us understand the relationship
between the different components.

89
00:05:27,124 --> 00:05:28,204
Static analysis.

90
00:05:28,474 --> 00:05:32,854
We validate the transformations and
use static analysis to guide them.

91
00:05:33,724 --> 00:05:36,064
Is this the, is this
transformation correct?

92
00:05:36,214 --> 00:05:39,364
Did we break any builds or did you test?

93
00:05:39,724 --> 00:05:44,024
Will it break any downstream
downstream dependencies stack

94
00:05:44,085 --> 00:05:46,004
analysis gives us the confidence.

95
00:05:46,304 --> 00:05:49,244
And finally the output is
basically the transfer generated

96
00:05:49,244 --> 00:05:50,804
code ready to be integrated.

97
00:05:51,304 --> 00:05:55,025
Here's a walkthrough of the
architecture diagram on the left side.

98
00:05:55,025 --> 00:06:00,034
As you can see, the inputs consist
of the code repository, the knowledge

99
00:06:00,034 --> 00:06:01,864
based repo, or the instructions.

100
00:06:02,194 --> 00:06:07,414
The static analysis allows for
verifying or for performing static

101
00:06:07,414 --> 00:06:09,724
analytics based transformations.

102
00:06:10,445 --> 00:06:16,174
The main engine consists of from
contextualization planning, orchestration.

103
00:06:16,685 --> 00:06:21,305
Dependency analysis, which uses
abstract syn tax trees, static

104
00:06:21,305 --> 00:06:23,795
analysis, validation, and the LLM.

105
00:06:24,605 --> 00:06:28,835
And on the right side, the generated code
is the output code which gets generated

106
00:06:29,255 --> 00:06:34,315
transforming your repository at a high
level the technical magic which happens.

107
00:06:34,315 --> 00:06:39,685
And code plan majorly consists of three
techniques, LLM planning instead of asking

108
00:06:39,685 --> 00:06:42,445
an LLM to solve a problem in one shot.

109
00:06:42,850 --> 00:06:44,380
We ask it to plan.

110
00:06:44,980 --> 00:06:48,310
Break the complex task
into structured steps.

111
00:06:48,490 --> 00:06:52,390
Choose which tools and techniques to
use decide the order of oxy execution.

112
00:06:52,840 --> 00:06:57,220
This enables the goal directed
execution rather than hoping on a

113
00:06:57,220 --> 00:06:59,080
general purpose model gets it right.

114
00:06:59,580 --> 00:07:00,270
Ad based chunking.

115
00:07:00,660 --> 00:07:04,020
We don't split the code at
arbitrary character positions.

116
00:07:04,110 --> 00:07:10,320
We use a ST to, which is abstract syn
to identify meaningful boundaries.

117
00:07:10,350 --> 00:07:11,730
Function classes, loops.

118
00:07:12,240 --> 00:07:15,450
Each chunk remains
syntactically valid on its own.

119
00:07:15,750 --> 00:07:19,320
This produces better embeddings,
better retrieval, and more accurate

120
00:07:19,405 --> 00:07:23,370
LLM responses RAG the retrieval.

121
00:07:23,460 --> 00:07:28,800
Augmented generation and RAG basically
consists of a retrieval and a generator.

122
00:07:29,310 --> 00:07:33,270
The retrieval finds relevant
chunks from your knowledge base

123
00:07:33,360 --> 00:07:35,430
and instructions and the code base.

124
00:07:35,930 --> 00:07:40,460
The LLM generator basically integrates
the context into its responses.

125
00:07:40,610 --> 00:07:44,780
This combination basically reduces
the hallucinations, overcomes the

126
00:07:44,780 --> 00:07:48,170
context length eliminations, and
incorporates, incorporates fresh

127
00:07:48,170 --> 00:07:49,670
knowledge into the training data.

128
00:07:50,360 --> 00:07:53,900
Together, these techniques create
a powerful system for handling

129
00:07:54,020 --> 00:07:55,940
repository scale transformations.

130
00:07:56,440 --> 00:08:00,250
Okay, now let's go
deeper into the abstract.

131
00:08:00,624 --> 00:08:03,265
Synex trees as well as the rank.

132
00:08:03,775 --> 00:08:04,015
Okay.

133
00:08:04,415 --> 00:08:08,795
Let's walk through an a SD
con concrete with an example.

134
00:08:08,915 --> 00:08:12,695
So here's a simple python code,
a function that greets someone.

135
00:08:13,475 --> 00:08:18,715
When we pass this a SD with our a SD
pass, we get a hierarchical restructure.

136
00:08:19,495 --> 00:08:21,670
At the top is a function deaf node.

137
00:08:22,180 --> 00:08:24,310
Under that, we have the
function name, greet.

138
00:08:24,670 --> 00:08:27,540
The argument's name and
the return statement.

139
00:08:27,990 --> 00:08:31,230
The return statement contains a
binary operation string court,

140
00:08:31,320 --> 00:08:32,730
which is string contact conation.

141
00:08:33,230 --> 00:08:35,510
Combining Hello with the variable name.

142
00:08:35,630 --> 00:08:35,990
Name.

143
00:08:36,799 --> 00:08:40,429
Now this might seem like a minor
detail, but it's transformative

144
00:08:41,240 --> 00:08:45,230
when an LLM sees the structure
representation instead of raw text.

145
00:08:45,319 --> 00:08:47,930
It understands the code
at a much deeper level.

146
00:08:48,470 --> 00:08:51,350
It knows these are distinct
Symantec components.

147
00:08:51,470 --> 00:08:55,189
It can reason about how changing
one part affects the others.

148
00:08:55,810 --> 00:08:59,680
This structure enables meaningful
code chunking file, LM, processing,

149
00:08:59,740 --> 00:09:03,100
and accurate relationship
between the different components,

150
00:09:03,600 --> 00:09:05,400
LLMC code as plain text.

151
00:09:05,900 --> 00:09:09,980
It's just a sequence of characters
they don't inherently understand.

152
00:09:09,980 --> 00:09:13,520
The function is different from a
loop that a variable declaration

153
00:09:13,520 --> 00:09:14,990
is different from a function call.

154
00:09:15,290 --> 00:09:18,770
They make educated guesses
based on statistical patterns.

155
00:09:19,190 --> 00:09:24,500
This leads to generate generic, inaccurate
responses and potential hallucinations.

156
00:09:25,000 --> 00:09:29,530
The A ST solution breaks the code into
organized hierarchical representation.

157
00:09:29,979 --> 00:09:33,219
Now, the LLM can understand
methods, properties, arguments,

158
00:09:33,280 --> 00:09:34,719
data types, and logic flow.

159
00:09:35,020 --> 00:09:37,000
It sees the skeleton of your program.

160
00:09:37,449 --> 00:09:40,930
This makes the AI responses far
more accurate and insightful.

161
00:09:41,410 --> 00:09:44,949
When you feed an LLM and a
ST in text, you're given a

162
00:09:44,949 --> 00:09:46,449
semantic understanding layer.

163
00:09:47,140 --> 00:09:50,920
That's the major difference between a
generic tool and a specialized tool.

164
00:09:51,420 --> 00:09:51,689
Okay.

165
00:09:52,160 --> 00:09:53,390
Next chunking.

166
00:09:54,020 --> 00:09:56,810
Now we are gonna look at the
two approaches for breaking

167
00:09:56,810 --> 00:09:57,685
the code into chunks.

168
00:09:58,380 --> 00:10:01,740
The first one is traditional and
the next one is ESD based chunking.

169
00:10:02,280 --> 00:10:05,430
Traditional character based
trunking treats code like any

170
00:10:05,430 --> 00:10:09,479
other text you might split at 500
characters or a thousand characters.

171
00:10:09,750 --> 00:10:10,680
Here's what happens.

172
00:10:10,680 --> 00:10:13,020
You end up cutting through
middle of functions.

173
00:10:13,290 --> 00:10:16,620
A function definition
gets split across chunks.

174
00:10:16,620 --> 00:10:19,680
An inclusive statement sits alone.

175
00:10:19,890 --> 00:10:24,115
The semicolon statement that closes a
statement might be in the next chunk.

176
00:10:24,834 --> 00:10:27,415
The core structure is disregarded.

177
00:10:27,594 --> 00:10:32,545
You get malformed, syntactically
invalid fragments, statements

178
00:10:32,545 --> 00:10:34,224
lack proper syntax closure.

179
00:10:34,435 --> 00:10:36,594
The semantic meaning is
completely destroyed.

180
00:10:37,165 --> 00:10:41,004
And LLM asked to process this fragments
would get completely confused.

181
00:10:41,035 --> 00:10:44,754
It's like asking a, someone, asking
someone to understand a sequence

182
00:10:44,754 --> 00:10:46,675
broken randomly across lines.

183
00:10:47,175 --> 00:10:47,624
Next.

184
00:10:48,045 --> 00:10:49,959
Now let's see how ad-based chunking.

185
00:10:50,459 --> 00:10:53,669
Significantly improves our use case.

186
00:10:54,149 --> 00:10:57,089
Each chunk represents a code boundary.

187
00:10:57,420 --> 00:11:00,680
The pre-processor
include is its own chunk.

188
00:11:00,920 --> 00:11:03,140
The using declaration is its own chunk.

189
00:11:03,589 --> 00:11:06,469
The complete functions are
preserved as a single chunk.

190
00:11:07,020 --> 00:11:07,499
This.

191
00:11:08,084 --> 00:11:11,954
Which makes more sense and provides
better understanding to the LLM.

192
00:11:12,374 --> 00:11:13,964
The code is split and meaningful.

193
00:11:13,964 --> 00:11:18,254
Boundaries, functions, control structures,
usings, declarations, et cetera.

194
00:11:18,645 --> 00:11:21,704
Each chunk is syntactically
valid and complete.

195
00:11:22,140 --> 00:11:24,630
Semantic integration is
completely preserved.

196
00:11:24,959 --> 00:11:29,430
When an LLM receives syntactically valid
code that represents semantic boundaries,

197
00:11:29,430 --> 00:11:31,229
it can generate better confirmation.

198
00:11:31,680 --> 00:11:34,290
There's no guessing about
context that was cut off.

199
00:11:34,589 --> 00:11:37,390
The model understands the
complete picture of that chunk.

200
00:11:37,920 --> 00:11:41,130
This is a foundation technique
which we use for enabling

201
00:11:41,130 --> 00:11:42,810
repository scale transformation.

202
00:11:43,229 --> 00:11:48,750
The planning, along with the A ST
analysis, helps us split the task into

203
00:11:49,079 --> 00:11:55,650
meaningful sections and loop our irate
over the LLM calls providing meaningful

204
00:11:55,710 --> 00:11:58,660
chunks of the A SDU back again and again.

205
00:11:59,160 --> 00:12:03,105
Next, we are gonna look at rack
retrieval, augmented generation.

206
00:12:03,605 --> 00:12:07,880
We use RAG majorly for two
major aspects, knowledge cutoff

207
00:12:07,970 --> 00:12:09,470
and high restoration problems.

208
00:12:10,100 --> 00:12:11,780
RAG has two simple components.

209
00:12:12,050 --> 00:12:16,910
The retriever searches through your
knowledge space, documentation examples,

210
00:12:16,970 --> 00:12:21,350
existing core best practices majorly
trying to understand what is the

211
00:12:21,350 --> 00:12:22,970
transformation you're trying to achieve.

212
00:12:23,470 --> 00:12:24,700
It uses the retrieval.

213
00:12:25,200 --> 00:12:30,840
It uses a retrieved documentation,
combines it with the a SD based chunking

214
00:12:30,900 --> 00:12:32,610
and provides it to the generator.

215
00:12:33,000 --> 00:12:37,650
The generator takes whatever the retrieval
found and process it through an LLM.

216
00:12:37,860 --> 00:12:41,040
The model integrates this fresh
up-to-date context into its.

217
00:12:42,015 --> 00:12:45,825
The beauty of this approach is that the
LLM has current information about your

218
00:12:45,825 --> 00:12:47,805
specific APIs, frameworks and patterns.

219
00:12:48,105 --> 00:12:51,315
It's not operating on the general
knowledge from its training data.

220
00:12:51,315 --> 00:12:54,495
It's operating on the actual code
base and the knowledge base of

221
00:12:54,495 --> 00:12:56,445
instructions, which we provided to it.

222
00:12:57,135 --> 00:13:01,305
This combination dramatically reduces
hallucinations, overcomes context,

223
00:13:01,305 --> 00:13:04,845
length limitations, and ensures your
transforms use latest frameworks and the

224
00:13:04,845 --> 00:13:06,825
best practice from your organization.

225
00:13:07,485 --> 00:13:07,785
Okay.

226
00:13:08,285 --> 00:13:14,715
Let's talk about real world impact
First time savings organization doing

227
00:13:14,715 --> 00:13:19,425
mainframe migrations have seen 70%
reduction in modernization timelines.

228
00:13:19,905 --> 00:13:23,685
Tasks that used to take three to
five years are completely automated

229
00:13:24,525 --> 00:13:29,295
quality improvements code plan
because it integrates with a

230
00:13:29,300 --> 00:13:31,605
feedback loop with static analysis.

231
00:13:31,875 --> 00:13:33,380
The transformations are.

232
00:13:34,260 --> 00:13:38,010
Completely valid and preserves
the original behavior.

233
00:13:38,190 --> 00:13:41,460
There are fewer manual errors
in large scale transformations,

234
00:13:41,700 --> 00:13:43,410
higher confidence in the results.

235
00:13:43,910 --> 00:13:48,080
Developer productivity developers
are free from repetitive error

236
00:13:48,290 --> 00:13:53,090
prone task allows focus on high
value architecture and design work

237
00:13:53,540 --> 00:13:55,640
enables non-experts to perform.

238
00:13:56,000 --> 00:13:57,590
Code transformation as well.

239
00:13:58,550 --> 00:14:02,660
Enterprise applications, packet
transformations across hundreds of

240
00:14:02,660 --> 00:14:08,090
files, hundreds of repositories,
legacy code modernization, compliance,

241
00:14:08,090 --> 00:14:09,680
and security updates at scale.

242
00:14:10,190 --> 00:14:13,730
These are real use cases delivering
real business value today.

243
00:14:14,230 --> 00:14:18,730
Future and challenges we need to be
honest about the current limitations.

244
00:14:19,180 --> 00:14:23,080
Context, length still limits
extremely large code bases.

245
00:14:23,170 --> 00:14:25,390
Some code bases are simply anonymous.

246
00:14:25,990 --> 00:14:29,800
Model Ations is not
completely disappeared.

247
00:14:30,010 --> 00:14:32,350
Their reduce, but validation
is still essential.

248
00:14:33,310 --> 00:14:36,430
Repository scale analysis
is computationally heavy.

249
00:14:36,520 --> 00:14:37,355
It's not free.

250
00:14:37,855 --> 00:14:41,755
Complexity in handling dynamic
languages such as Python and JavaScript

251
00:14:41,755 --> 00:14:46,945
is higher as we struggle with this
traditional tactic analysis with them.

252
00:14:47,445 --> 00:14:48,525
Emerging trends.

253
00:14:48,845 --> 00:14:52,295
One of the biggest emerging trends
is multi-agent systems where

254
00:14:52,295 --> 00:14:56,585
multiple specialized agents have the
different parts of transformation.

255
00:14:57,320 --> 00:15:00,635
Continuously learning from
developer feedback as to how

256
00:15:00,695 --> 00:15:02,885
developers modify its output.

257
00:15:03,385 --> 00:15:07,885
Integration with actual development
tools such as your id, CICD pipelines,

258
00:15:07,885 --> 00:15:09,475
not just standalone processing.

259
00:15:09,975 --> 00:15:12,495
I would like to conclude
with some key takeaways.

260
00:15:12,995 --> 00:15:17,645
First, we are all, we are moving from a
line by line auto complete to a repository

261
00:15:18,635 --> 00:15:23,825
wide reasoning from simple text generation
to structured planning with validation.

262
00:15:24,325 --> 00:15:30,655
Neural approaches are pure LLM based on
neural approaches are being integrated

263
00:15:30,655 --> 00:15:35,275
and made neuros symbolic hybrid systems
that combine the best of both words.

264
00:15:35,775 --> 00:15:41,355
Planning frameworks that decompose complex
tasks, ad based understanding MULTICONTEXT

265
00:15:41,385 --> 00:15:46,965
integration are some of the main aspects
which are enabling this transformation.

266
00:15:47,465 --> 00:15:51,635
Line, AI powered large core
transformations are no longer a

267
00:15:51,635 --> 00:15:53,585
theoretical or distant future.

268
00:15:54,185 --> 00:15:58,445
They are achieving production ready
results on real world repositories.

269
00:15:58,565 --> 00:16:02,375
Right now, this technology is transforming
what's possible in software engineering.

270
00:16:02,885 --> 00:16:04,685
We are not replacing software developers.

271
00:16:04,685 --> 00:16:07,790
We're amplifying or enabling
them to achieve more.

272
00:16:08,330 --> 00:16:09,230
We are automatically.

273
00:16:09,730 --> 00:16:13,420
We are automating what's repetitive
and error prone so that they can

274
00:16:13,420 --> 00:16:15,340
focus on what's creative and strategy.

275
00:16:16,000 --> 00:16:20,140
The future of software engineering
is at repository scale, intelligent

276
00:16:20,140 --> 00:16:21,730
planning and powered by ar.

277
00:16:22,630 --> 00:16:23,140
Thank you.

278
00:16:23,335 --> 00:16:23,455
I.

