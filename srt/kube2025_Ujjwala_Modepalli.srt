1
00:00:00,500 --> 00:00:01,250
Hi everyone.

2
00:00:01,549 --> 00:00:05,720
My name is Ola Modi and I'm
happy to be presenting here at

3
00:00:05,720 --> 00:00:07,465
the Cube Native Conference 2025.

4
00:00:08,299 --> 00:00:11,180
I wanted to start with a quick
introduction about myself.

5
00:00:11,240 --> 00:00:15,920
I currently work for Wells Fargo Bank
and I have around 14 years of hands-on

6
00:00:15,920 --> 00:00:20,630
experience in advanced data analytics,
credit risk strategy, and predictive

7
00:00:20,630 --> 00:00:24,860
modeling across financial services,
marketing, and customer intelligence.

8
00:00:25,520 --> 00:00:29,690
I also have extensive experience in
developing machine learning driven fraud

9
00:00:29,690 --> 00:00:34,910
detection strategies, segmentation models,
and campaign optimization frameworks that

10
00:00:34,910 --> 00:00:40,850
help improve ROI reduce risk exposure
and enhance operational efficiency.

11
00:00:41,330 --> 00:00:47,065
The topic I'm planning to present today
is container native ml scaling predictive

12
00:00:47,305 --> 00:00:49,065
customer segmentation on Kubernetes.

13
00:00:50,030 --> 00:00:55,250
Today I'll be sharing how we re-engineered
a traditional monolithic analytics

14
00:00:55,250 --> 00:01:00,470
system into a cloud native machine
learning ecosystem built on Kubernetes.

15
00:01:00,770 --> 00:01:04,730
Our focus was on predictive
customer segmentation, identifying.

16
00:01:05,575 --> 00:01:08,575
Behavioral patterns in real
time at Enterprise Scale.

17
00:01:08,935 --> 00:01:11,604
This session is about
more than just technology.

18
00:01:11,845 --> 00:01:16,075
It's about how containerization
and orchestration can transform the

19
00:01:16,075 --> 00:01:20,934
way organizations operationalize
ml, making it faster, more

20
00:01:20,934 --> 00:01:23,125
efficient, and more cost effective.

21
00:01:23,625 --> 00:01:25,485
Moving on to slide two.

22
00:01:25,985 --> 00:01:27,605
Let's start with the challenges.

23
00:01:27,605 --> 00:01:32,225
Most enterprises face traditional
analytics platforms were not designed

24
00:01:32,225 --> 00:01:34,295
for real-time large scale prediction.

25
00:01:34,565 --> 00:01:38,435
They rely on monolithic pipelines
that struggle under heavy

26
00:01:38,435 --> 00:01:40,835
workloads as data volumes grow.

27
00:01:41,375 --> 00:01:45,580
Performance degrades, leading to longer
processing times and slower insights.

28
00:01:46,315 --> 00:01:48,354
Scalability becomes a bottleneck.

29
00:01:48,595 --> 00:01:50,574
You can't just add more servers.

30
00:01:50,604 --> 00:01:54,655
You need horizontal scaling
elasticity and automation.

31
00:01:55,134 --> 00:02:00,175
Operationally, managing diverse ML
workloads on rigid infrastructure is

32
00:02:00,175 --> 00:02:05,815
complex data scientists, ML engineers
and dev DevOp teams often work in silos.

33
00:02:06,085 --> 00:02:08,335
We need an architecture that could adopt.

34
00:02:08,835 --> 00:02:13,484
Adapt dynamically to business
load, self-heal, and scale.

35
00:02:13,489 --> 00:02:15,400
Predictively, not reactively.

36
00:02:15,900 --> 00:02:18,810
Moving on to the next slide.

37
00:02:19,560 --> 00:02:23,040
This slide talks about container
orchestration solution.

38
00:02:23,429 --> 00:02:26,670
So this is where Kubernetes
changed the game for us.

39
00:02:27,090 --> 00:02:30,510
Kubernetes provides the
flexibility and reliability that

40
00:02:30,540 --> 00:02:32,310
monolithic architectures lack.

41
00:02:32,745 --> 00:02:37,905
It allows you to deploy ML workloads
as microservices, which are loosely

42
00:02:37,905 --> 00:02:39,734
coupled and highly scalable.

43
00:02:40,274 --> 00:02:45,135
We implemented horizontal pod auto
scaling, so during peak customer

44
00:02:45,135 --> 00:02:49,005
activity, pods automatically
spin up to handle the load.

45
00:02:49,514 --> 00:02:53,804
Intelligent resource orchestration
distributes workloads, efficiency

46
00:02:53,804 --> 00:02:59,804
efficiently across clusters, and with
built-in fall tolerance, even if one

47
00:02:59,804 --> 00:03:02,734
node fails, another sly takes over.

48
00:03:03,094 --> 00:03:08,135
The result is a system that's
always on, always fast and always

49
00:03:08,195 --> 00:03:12,334
efficient, the backbone of a
real time predictive analytics.

50
00:03:12,834 --> 00:03:14,429
Moving on to the next slide.

51
00:03:14,929 --> 00:03:18,539
This slide talks about native
ML architecture overview.

52
00:03:18,809 --> 00:03:23,549
This slide basically gives you a bird's
eye view of the full architecture.

53
00:03:23,939 --> 00:03:29,519
We structured it into four layers, data
ingestion services, feature engineering,

54
00:03:29,579 --> 00:03:32,189
model training, and interference services.

55
00:03:32,579 --> 00:03:37,399
So the first one is data ingestion
services, containerized pipelines.

56
00:03:37,449 --> 00:03:43,149
Pull customer data from multiple sources
like CRM, digital interactions and

57
00:03:43,149 --> 00:03:46,449
transactions using event driven ingestion.

58
00:03:46,989 --> 00:03:50,919
Next one is feature engineering,
independent microservices,

59
00:03:50,979 --> 00:03:55,869
clean, validate and convert raw
data into ML ready features.

60
00:03:55,899 --> 00:03:59,534
This ensures every model
uses standardized inputs.

61
00:04:00,260 --> 00:04:02,209
Model training is the next one.

62
00:04:02,469 --> 00:04:07,089
Distributed training runs across
Kubernetes clusters enabling

63
00:04:07,089 --> 00:04:11,979
parallelization for algorithms
like gradient boosting neural

64
00:04:11,979 --> 00:04:13,570
networks and clustering.

65
00:04:13,929 --> 00:04:16,539
And the last one is interference services.

66
00:04:17,299 --> 00:04:20,689
This layer basically provides
real time predictions.

67
00:04:20,899 --> 00:04:26,839
With Redis caching, we achieve millisecond
response times for customer segmentation.

68
00:04:27,169 --> 00:04:31,759
In short, this architecture
makes the ML process continuous,

69
00:04:31,759 --> 00:04:33,949
modular, and highly responsive.

70
00:04:34,449 --> 00:04:36,604
Moving on to next slide.

71
00:04:37,104 --> 00:04:42,324
Breaking the pipe ML pipeline into
microservices was a turning point.

72
00:04:42,624 --> 00:04:47,034
Each stage ingestion, feature
engineering training, and

73
00:04:47,034 --> 00:04:49,344
inference runs independently.

74
00:04:49,614 --> 00:04:51,204
This means if you're training jobs.

75
00:04:51,619 --> 00:04:53,929
Spike, you scale only that part.

76
00:04:54,229 --> 00:04:59,089
If your inference layer needs more
compute, you add pods just for that.

77
00:04:59,359 --> 00:05:04,729
It reduces resource waste stage
accelerates deployments and simplifies

78
00:05:04,729 --> 00:05:09,709
debugging, and most importantly, it
allows different teams to iterate

79
00:05:09,769 --> 00:05:11,779
without breaking the entire pipeline.

80
00:05:12,079 --> 00:05:16,639
So data engineers, ML developers,
and DevOps can all work

81
00:05:16,639 --> 00:05:19,039
concurrently a major boost.

82
00:05:19,159 --> 00:05:21,264
This is a major boost to HGDT.

83
00:05:21,764 --> 00:05:24,314
We on to the next slide.

84
00:05:25,224 --> 00:05:28,224
This slide talks about
deployment management with Helm.

85
00:05:28,734 --> 00:05:32,784
When you have dozens of microservices
deployment, consistency becomes

86
00:05:32,784 --> 00:05:35,964
crucial, and that's where Helm comes in.

87
00:05:36,324 --> 00:05:40,464
We use Helm charts to define
and version our deployments.

88
00:05:41,254 --> 00:05:46,474
This ensures every ML service, data
processing, training, or inference follows

89
00:05:46,474 --> 00:05:49,324
the same standard configuration pattern.

90
00:05:50,044 --> 00:05:54,374
Helm also supports environment
specific overrides.

91
00:05:54,554 --> 00:05:58,784
So the same chart can be
deployed in development, staging,

92
00:05:59,024 --> 00:06:00,404
or production seamlessly.

93
00:06:00,404 --> 00:06:01,064
Seamlessly.

94
00:06:01,544 --> 00:06:02,534
If something breaks.

95
00:06:03,014 --> 00:06:04,364
Rollback is instant.

96
00:06:04,694 --> 00:06:10,304
We extended Kubernetes with customer
resource definition, CRDs for ML

97
00:06:10,304 --> 00:06:16,154
specific needs, like scheduling, model
training jobs, managing GPU resources

98
00:06:16,154 --> 00:06:19,594
and automating, retraining lifecycles.

99
00:06:20,094 --> 00:06:22,524
Moving on to the next slide.

100
00:06:23,024 --> 00:06:26,264
This slide talks about event
driven processing, architecture.

101
00:06:26,804 --> 00:06:31,454
Customer behavior changes constantly
promotions, seasonal campaigns or new

102
00:06:31,454 --> 00:06:34,064
product launches can all shift patterns.

103
00:06:34,334 --> 00:06:38,744
We implemented event driven retraining
to keep our segmentation models are.

104
00:06:39,159 --> 00:06:43,899
To date when our monitoring system
detects data drift, meaning customer

105
00:06:43,929 --> 00:06:48,759
behavior is no longer aligned with
the model, Kubernetes automatically

106
00:06:48,759 --> 00:06:51,339
triggers a model retraining pipeline.

107
00:06:51,819 --> 00:06:56,079
So the new model is validated and
deployed automatically without

108
00:06:56,079 --> 00:06:57,939
manual intervention or downtime.

109
00:06:58,299 --> 00:07:02,669
This approach ensures that ACA
segmentation remains accurate

110
00:07:02,729 --> 00:07:06,544
and responsive even as customer
behavior evolves in real time.

111
00:07:07,044 --> 00:07:09,234
Moving on to the next slide.

112
00:07:09,814 --> 00:07:14,014
This slide talks about container
based feature store implementation.

113
00:07:14,494 --> 00:07:19,024
A consistent feature layer is
critical for reproducibility.

114
00:07:19,324 --> 00:07:24,664
We created a container based feature
store that centralizes all engineered

115
00:07:25,054 --> 00:07:26,164
features used by our ML models.

116
00:07:26,664 --> 00:07:31,404
Every service, whether it's training
or inference accesses features through

117
00:07:31,404 --> 00:07:34,074
this store guaranteeing consistency.

118
00:07:34,374 --> 00:07:40,314
We use red caching for sub millisecond
feature retrieval, and with feature

119
00:07:40,524 --> 00:07:46,284
versioning, we can run AB tests on
new features or rollback to previous

120
00:07:46,284 --> 00:07:48,624
versions efficiently, effortlessly.

121
00:07:49,104 --> 00:07:53,994
This ensures full traceability and
confidence in our model outputs.

122
00:07:54,494 --> 00:07:56,264
Moving on to the next slide.

123
00:07:56,704 --> 00:08:00,004
This slide talks about
advanced Kubernetes patterns.

124
00:08:00,334 --> 00:08:04,564
So we leverage several Kubernetes
native design patterns to

125
00:08:04,564 --> 00:08:06,274
make the system more robust.

126
00:08:06,754 --> 00:08:11,434
It's, some of them are in IT containers,
sidecar containers, and multi

127
00:08:11,434 --> 00:08:16,694
container pods for the in IT containers
before a main container starts.

128
00:08:16,904 --> 00:08:23,924
These run checks verifying data integrity,
dependencies, and environment readiness.

129
00:08:24,254 --> 00:08:26,174
The next one is sidecar containers.

130
00:08:26,384 --> 00:08:32,324
These monitor performance collect metrics
like latency or drift and push them to.

131
00:08:32,969 --> 00:08:38,669
PROEs or Grafana multi container
pods for components that need

132
00:08:38,669 --> 00:08:43,229
to share resources closely, like
pre-processing and model serving.

133
00:08:43,469 --> 00:08:46,679
These pods offer efficiency and isolation.

134
00:08:47,039 --> 00:08:51,839
These patterns improve observability,
ensure reliability, and made

135
00:08:51,839 --> 00:08:53,939
troubleshooting much easier.

136
00:08:54,439 --> 00:08:56,179
Moving on to the next slide.

137
00:08:56,809 --> 00:09:02,479
This slide talks about resource management
strategies, efficiencies, efficiency.

138
00:09:02,479 --> 00:09:07,579
At the heart of container native
ml, we introduce GPU scheduling

139
00:09:07,579 --> 00:09:12,379
policies to ensure deep learning
workloads get prioritized GPU access.

140
00:09:12,964 --> 00:09:18,604
Without idle cycles, we also optimized
memory allocation for large scale

141
00:09:18,604 --> 00:09:23,314
clustering, preventing out of memory
crashes, and improving throughput.

142
00:09:23,824 --> 00:09:29,284
The results that we saw were
remarkable, 85% GPU utilization.

143
00:09:29,534 --> 00:09:29,604
And.

144
00:09:30,249 --> 00:09:34,304
Three, three times more or
better, more memory efficiency.

145
00:09:34,555 --> 00:09:38,215
These are the two remarkable
results that we achieved.

146
00:09:38,465 --> 00:09:40,475
These aren't just performance stats.

147
00:09:40,505 --> 00:09:43,865
They represent cost savings,
unsustainable scalability.

148
00:09:44,365 --> 00:09:46,345
Moving on to the next slide.

149
00:09:46,795 --> 00:09:50,425
This slide talks about product
production performance improvements.

150
00:09:50,725 --> 00:09:53,815
After a full deployment,
we saw measurable impact.

151
00:09:54,215 --> 00:09:58,925
There was a 60% cost reduction,
containerization and auto-scaling,

152
00:09:58,925 --> 00:10:01,925
drastically reduced idle infrastructure.

153
00:10:02,265 --> 00:10:04,875
The next one is 99.9% uptime.

154
00:10:05,540 --> 00:10:11,180
The automated failover, redundancy kept
services continuously available, and

155
00:10:11,180 --> 00:10:14,330
it also increased the processing speed.

156
00:10:14,330 --> 00:10:19,520
It is 40% faster, so it distributed
computing, cut training and

157
00:10:19,520 --> 00:10:21,650
interference timing significantly.

158
00:10:22,180 --> 00:10:27,190
These improvements directly translated
to faster campaign turnarounds,

159
00:10:27,460 --> 00:10:32,590
better personalization, and ultimately
higher ROI for our marketing teams.

160
00:10:33,090 --> 00:10:33,380
Okay.

161
00:10:33,385 --> 00:10:35,375
Moving on to the next slide.

162
00:10:35,675 --> 00:10:38,945
This slide talks about
implementation strategy, roadmap.

163
00:10:39,245 --> 00:10:40,145
Here's the roadmap.

164
00:10:40,145 --> 00:10:43,715
We followed a structured
four phase approach.

165
00:10:43,995 --> 00:10:46,275
Phase one is containerization.

166
00:10:46,605 --> 00:10:48,225
We converted, monolithic.

167
00:10:48,485 --> 00:10:53,105
ML workflows into
dockerized microservices.

168
00:10:53,315 --> 00:10:55,475
Phase two orchestration.

169
00:10:55,685 --> 00:10:59,765
Kubernetes was implemented for
automated scaling, resource

170
00:10:59,765 --> 00:11:02,405
optimization and workload management.

171
00:11:02,675 --> 00:11:04,745
Phase three was optimization.

172
00:11:04,985 --> 00:11:11,390
We layered on advanced techniques, feature
stores, event driven retraining, and CRDs.

173
00:11:12,045 --> 00:11:13,815
Phase four was production.

174
00:11:14,295 --> 00:11:18,975
This was a full deployment with
monitoring, alerting and CI or CD

175
00:11:19,305 --> 00:11:21,915
integration for continuous improvements.

176
00:11:22,065 --> 00:11:26,325
Each phase was iterative,
allowing us to learn and refine

177
00:11:26,355 --> 00:11:28,185
before scaling enterprise wide.

178
00:11:28,685 --> 00:11:30,785
Moving on to the next slide.

179
00:11:31,155 --> 00:11:35,115
This slide talks about the key
takeaways for platform engineers

180
00:11:35,355 --> 00:11:37,095
for platform and ML engineers.

181
00:11:37,095 --> 00:11:39,705
Here are three lessons
that really stand out.

182
00:11:40,155 --> 00:11:44,925
Adopt microservice architecture,
automate with event driven processing,

183
00:11:45,165 --> 00:11:47,835
and focus on resource optimization.

184
00:11:48,155 --> 00:11:50,165
We can talk about adopt microservice.

185
00:11:50,525 --> 00:11:51,605
Architecture first.

186
00:11:51,815 --> 00:11:56,555
So to break your ML pipeline into
modular independently scalable

187
00:11:56,555 --> 00:12:02,055
units, it accelerates development
and improves reliability to automate

188
00:12:02,125 --> 00:12:04,285
with event driven processing.

189
00:12:04,505 --> 00:12:09,005
Let Kubernetes manage lifestyle
events, lifecycle events like

190
00:12:09,005 --> 00:12:10,625
restraining or redeployment.

191
00:12:11,205 --> 00:12:13,155
Focus on resource optimization.

192
00:12:13,395 --> 00:12:17,505
Use Kubernetes native tools
for GPU, scheduling, memory

193
00:12:17,505 --> 00:12:19,755
optimization, and auto-scaling.

194
00:12:20,355 --> 00:12:25,695
These principles turn ML systems from
brittle and reactive into adaptive,

195
00:12:25,725 --> 00:12:28,245
intelligent and cost efficient ecosystems.

196
00:12:29,115 --> 00:12:29,145
Okay.

197
00:12:29,645 --> 00:12:31,895
Moving on to the next slide.

198
00:12:31,995 --> 00:12:36,565
This slide talks about how cloud
native ML is the future now.

199
00:12:37,145 --> 00:12:42,315
The future is now container native ML
is not just a trend, it's the foundation

200
00:12:42,345 --> 00:12:44,655
for scalable enterprise analytics.

201
00:12:45,135 --> 00:12:50,140
We have moved from static centralized
systems to dynamic modular architectures.

202
00:12:50,790 --> 00:12:52,740
That evolve with business needs.

203
00:12:53,225 --> 00:12:58,080
Kubernetes and microservices are
enabling ML teams to deploy models

204
00:12:58,080 --> 00:13:03,810
faster, adapt to data changes instantly,
and deliver real business impact.

205
00:13:04,770 --> 00:13:08,760
The future of ML infrastructure
is cloud native, even driven

206
00:13:08,820 --> 00:13:13,260
and continuously optimized, and
that future is already here.

207
00:13:13,760 --> 00:13:16,400
I think that concludes our presentation.

208
00:13:16,590 --> 00:13:18,210
Thank you so much for your attention.

209
00:13:18,210 --> 00:13:22,770
I hope this session gave you practical
insights into how Kubernetes can

210
00:13:22,770 --> 00:13:26,100
be leveraged for scaling machine
learning in real world enterprise.

211
00:13:26,445 --> 00:13:30,135
Settings, I'd be happy to take
any questions, whether it's

212
00:13:30,135 --> 00:13:34,215
about architecture, automation,
or implementation specifics.

213
00:13:34,485 --> 00:13:39,975
You can also connect with me afterwards to
discuss use cases or share any experiences

214
00:13:39,975 --> 00:13:42,715
with your own ML infrastructure.

215
00:13:43,255 --> 00:13:46,135
Thank you everyone for giving
me this opportunity and I

216
00:13:46,135 --> 00:13:47,245
hope you have a good day.

