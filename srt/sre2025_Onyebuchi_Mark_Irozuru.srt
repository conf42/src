1
00:00:00,500 --> 00:00:01,730
Good morning everyone.

2
00:00:02,230 --> 00:00:03,730
My name is Mark Izu.

3
00:00:04,230 --> 00:00:07,470
I'm the lead DevOps
engineer at Botanics Labs.

4
00:00:07,970 --> 00:00:09,080
I wanna start with a confession.

5
00:00:09,580 --> 00:00:13,955
I love failures and not just because I
enjoy watching systems crash and burn.

6
00:00:14,455 --> 00:00:18,955
But because each failure teaches us
something profound about our system

7
00:00:19,855 --> 00:00:22,555
that success could never teach us.

8
00:00:23,245 --> 00:00:26,305
And that's just a personal opinion.

9
00:00:26,665 --> 00:00:31,705
But think about it, when was the last
time we learned something deep or a very

10
00:00:31,705 --> 00:00:36,085
deep engineering lesson from something
that's working perfectly or a system

11
00:00:36,085 --> 00:00:37,555
of use that is working perfectly?

12
00:00:38,055 --> 00:00:38,715
That's right.

13
00:00:39,015 --> 00:00:39,645
That's right.

14
00:00:39,975 --> 00:00:41,625
It's the failures that shape us.

15
00:00:41,655 --> 00:00:47,295
It's the failures that force us to
grow the reveal, the hidden weaknesses

16
00:00:47,895 --> 00:00:53,115
in our most carefully crafted
systems, in our most stable system.

17
00:00:53,115 --> 00:00:58,185
It's the failures that teaches
us the flaws in our designs.

18
00:00:58,365 --> 00:01:02,445
It's not the social stories,
but the failures and the fixing.

19
00:01:02,625 --> 00:01:04,185
The fixing of the failures.

20
00:01:04,485 --> 00:01:06,435
That is what Teach forces us to grow.

21
00:01:06,935 --> 00:01:11,495
I wanna thank you for joining me in
my talk today on engineering failure

22
00:01:11,495 --> 00:01:17,675
resilient systems and proactive strategies
for distributed network reliability.

23
00:01:18,175 --> 00:01:23,935
I would say I've spent the last couple
of years in the trenches building and

24
00:01:23,935 --> 00:01:29,815
sometimes breaking distributed systems,
but yeah, that's just for the fun of it.

25
00:01:30,790 --> 00:01:35,770
Today we're gonna cover why failure
is innovative in distributed systems,

26
00:01:36,100 --> 00:01:41,860
common failure patterns, proactive
resilience strategies, and building

27
00:01:41,860 --> 00:01:43,630
resilient teams and culture.

28
00:01:44,130 --> 00:01:46,910
I would say it's a
meaningful presentation.

29
00:01:46,960 --> 00:01:53,494
Just I try not to be too technical and
just go over the, just go high surface.

30
00:01:53,994 --> 00:01:54,834
It is three.

31
00:01:55,104 --> 00:01:58,314
The systems are down, services are halted.

32
00:01:58,974 --> 00:02:00,474
A lots are blowing up.

33
00:02:00,504 --> 00:02:01,404
What now?

34
00:02:01,464 --> 00:02:03,204
Your boss is on your neck.

35
00:02:03,234 --> 00:02:05,004
The whole team is gathering up.

36
00:02:05,514 --> 00:02:09,524
Need the dashboard is all
painted red services are down.

37
00:02:09,554 --> 00:02:10,664
What do you do?

38
00:02:10,965 --> 00:02:13,005
MI microservices can't connect.

39
00:02:13,234 --> 00:02:14,764
No external assets.

40
00:02:15,264 --> 00:02:19,884
We've all had that point in time
in our career where we've had to

41
00:02:19,914 --> 00:02:22,794
wake up to those annoying alerts.

42
00:02:23,724 --> 00:02:24,804
It's unavoidable.

43
00:02:25,179 --> 00:02:26,739
It's really unavoidable.

44
00:02:26,819 --> 00:02:27,359
It's what?

45
00:02:27,359 --> 00:02:31,229
It's what makes us engineers essentially.

46
00:02:31,729 --> 00:02:33,079
Failure is not the exception.

47
00:02:33,079 --> 00:02:37,459
It's to brew every time something,
everything fills all the time.

48
00:02:37,754 --> 00:02:42,464
Where NoGo is the Amazon CT o set in
distributed systems, failure isn't

49
00:02:42,464 --> 00:02:44,174
just possible, it's inevitable.

50
00:02:44,534 --> 00:02:48,884
The question is since if your
systems fail, but when, how, and

51
00:02:48,884 --> 00:02:52,784
at what cost, what would be the
cost of your system failure, what

52
00:02:52,784 --> 00:02:54,614
is in that service level agreement?

53
00:02:55,064 --> 00:02:57,334
What is in what are those indicators?

54
00:02:58,024 --> 00:03:00,574
What is the cost for a system for you?

55
00:03:00,574 --> 00:03:02,234
What is the cost for that?

56
00:03:02,734 --> 00:03:06,424
One hour of no assets to the systems.

57
00:03:06,679 --> 00:03:08,389
What does that cause The system.

58
00:03:08,889 --> 00:03:12,129
The key point here is in systems
with thousands of components,

59
00:03:12,129 --> 00:03:14,949
something is always filling something.

60
00:03:15,129 --> 00:03:19,749
One system is receiving
more traffic than the rest.

61
00:03:19,809 --> 00:03:23,609
The something is always
happening here, essentially.

62
00:03:24,109 --> 00:03:27,439
I had ask this question, how
many of us here have experienced

63
00:03:27,439 --> 00:03:28,669
the failure in production?

64
00:03:28,669 --> 00:03:31,459
Again, you can see all hands are up.

65
00:03:31,999 --> 00:03:38,619
Essentially that's just to tell us
the output and what it feels like.

66
00:03:38,950 --> 00:03:41,589
Essentially having a system failure.

67
00:03:41,799 --> 00:03:44,260
We've all experienced
that outage in production.

68
00:03:44,469 --> 00:03:50,260
We've all experienced that quick
fix, that deployment gone wrong.

69
00:03:50,760 --> 00:03:55,799
That no running out, running
really high on temperature.

70
00:03:56,010 --> 00:03:57,419
We've all experienced that.

71
00:03:57,919 --> 00:04:00,679
Every hand here represents a
lesson learned about resilience.

72
00:04:01,179 --> 00:04:05,609
I. That's, we can, as a Spiderman
meme indicates we're all pointing

73
00:04:05,609 --> 00:04:10,319
fingers and pointing hands, but no,
we shouldn't point hands at ourselves.

74
00:04:10,709 --> 00:04:13,049
These systems are bound to fail.

75
00:04:13,289 --> 00:04:17,369
It's all about engineering this
system to be failure resilient and

76
00:04:18,119 --> 00:04:23,639
knowing what to do and what next on
the next line of action essentially.

77
00:04:24,139 --> 00:04:25,999
Then we look at the cost of downtime.

78
00:04:26,329 --> 00:04:27,829
What is the real cost of downtime?

79
00:04:27,829 --> 00:04:32,439
As asked earlier, when Amazon's S3
experienced the four R outage in

80
00:04:32,439 --> 00:04:34,809
2017, it cost companies over the.

81
00:04:35,564 --> 00:04:37,514
Over 150 million.

82
00:04:37,965 --> 00:04:43,394
These failures creates ripples around in
industries, reliant on cloud services.

83
00:04:43,664 --> 00:04:47,534
Look, think about the customers,
how they felt regarding this fall

84
00:04:47,534 --> 00:04:49,304
out during this fall out outage.

85
00:04:49,514 --> 00:04:51,194
This shows how.

86
00:04:51,400 --> 00:04:52,299
Interconnected.

87
00:04:52,299 --> 00:04:53,979
We are in the or hyperconnected.

88
00:04:53,979 --> 00:04:56,619
We are and reliant on certain services.

89
00:04:56,889 --> 00:05:00,959
And this is, this asks the question,
what is the real cost of downtime?

90
00:05:00,959 --> 00:05:01,229
What?

91
00:05:01,619 --> 00:05:06,539
What is the real cost of downtime in
your system, in your service beyond

92
00:05:06,539 --> 00:05:11,519
traditional reliability engineering,
which focuses on maximum uptime

93
00:05:11,519 --> 00:05:13,649
through redundancy at four tolerance.

94
00:05:13,899 --> 00:05:16,809
These approaches are necessary,
but are they sufficient for today's

95
00:05:16,809 --> 00:05:18,849
complex distributed environments?

96
00:05:18,849 --> 00:05:24,569
We ask ourself due to the limits
of uptime metrics, resilience of

97
00:05:24,569 --> 00:05:28,049
our redundancy, and preparing for
the unknown Metrics like uptime and

98
00:05:28,049 --> 00:05:33,119
availability are no longer sufficient
for just to ensure system reliability.

99
00:05:33,419 --> 00:05:35,279
Modern infrastructures must plan to.

100
00:05:35,924 --> 00:05:38,024
Must plant for failure proactively.

101
00:05:38,294 --> 00:05:38,714
You must.

102
00:05:38,894 --> 00:05:42,214
These systems must be designed
and architected for resilience.

103
00:05:42,544 --> 00:05:48,024
They must be architected for to plant
for failure within them, essentially,

104
00:05:48,024 --> 00:05:49,914
because failure is not the exception.

105
00:05:50,154 --> 00:05:51,114
It's the rule.

106
00:05:51,614 --> 00:05:55,334
Here we talk about the pillars
of resilience, engineering, and

107
00:05:55,334 --> 00:05:58,694
the five pillars of resilience
engineering as outlined here.

108
00:05:58,944 --> 00:06:04,389
They are antifragile architecture,
chaos, engineering processes, system

109
00:06:04,389 --> 00:06:08,289
breaker design, circuit breaker
design, architecture patterns,

110
00:06:08,789 --> 00:06:12,930
dynamic resource allocation, and of
course monitoring and observability.

111
00:06:12,930 --> 00:06:16,349
Because you cannot fix
what you do not know.

112
00:06:16,349 --> 00:06:20,789
You cannot fix what your, you
can't monitor, essentially.

113
00:06:21,289 --> 00:06:25,009
So we look at to antifragility,
what Antifragility looks like.

114
00:06:25,399 --> 00:06:26,029
Anti.

115
00:06:26,209 --> 00:06:27,649
What is anti fragility?

116
00:06:27,649 --> 00:06:33,289
The question is, unlike resilient systems
that re resist failure, anti-fragile

117
00:06:33,289 --> 00:06:35,689
systems grow stronger through disruptions.

118
00:06:36,109 --> 00:06:39,189
They use adversity to evolve and adapt.

119
00:06:39,429 --> 00:06:42,789
This is incorporating chaos
input, real time feedback, and

120
00:06:42,789 --> 00:06:47,919
diversification to create systems
that optimize on that stress.

121
00:06:48,159 --> 00:06:51,399
The key word, optimize
on that stress here.

122
00:06:51,899 --> 00:06:54,494
And then we look at chaos
engineering practices.

123
00:06:55,274 --> 00:06:59,564
The case study here would be
Netflix's Chaos Monkey two.

124
00:06:59,564 --> 00:07:02,764
Which one do terminates
production instances?

125
00:07:03,034 --> 00:07:05,104
This wasn't madness, it was survivor.

126
00:07:05,734 --> 00:07:06,634
Think about this.

127
00:07:06,634 --> 00:07:07,744
Look at it in this way.

128
00:07:08,284 --> 00:07:10,684
Your systems are gonna fail either ways.

129
00:07:10,684 --> 00:07:15,344
There's always gonna be that increase in
traffic or in increase in system load.

130
00:07:15,524 --> 00:07:18,464
So why not recreate it yourself?

131
00:07:18,494 --> 00:07:20,834
Why not plan for it
yourself where you have.

132
00:07:21,359 --> 00:07:25,469
Total visibility on the system where
you have, when you know and you

133
00:07:25,469 --> 00:07:30,629
can monitor what is going on and
how, what failure looks like when

134
00:07:30,629 --> 00:07:33,809
you are in charge of the failure,
you are orchestrating the failure.

135
00:07:34,079 --> 00:07:34,709
Exactly.

136
00:07:34,889 --> 00:07:36,809
So that's what Chaos Monkey is about.

137
00:07:37,019 --> 00:07:38,939
I'd advice you looking
to the Chaos Monkey.

138
00:07:38,939 --> 00:07:41,189
So because that's
learning true, deliberate.

139
00:07:41,419 --> 00:07:41,989
Failure.

140
00:07:42,259 --> 00:07:46,699
The key points there, determinates,
random instances in production.

141
00:07:46,969 --> 00:07:52,849
Ensure systems can handle failure
component and just transformed into

142
00:07:53,449 --> 00:07:57,649
this, an entire discipline of chaos,
engineering, and multiple companies

143
00:07:57,649 --> 00:08:02,689
all around the world incorporate this
type of care, this type of discipline

144
00:08:02,719 --> 00:08:04,429
in their services, basically.

145
00:08:04,909 --> 00:08:06,379
Basically, because you have to learn.

146
00:08:06,974 --> 00:08:08,264
True deliberate failure.

147
00:08:08,264 --> 00:08:09,824
You have to plan for the failure.

148
00:08:09,824 --> 00:08:13,664
You need to take control
of what failure looks like.

149
00:08:14,114 --> 00:08:17,264
Then we look at circuit breaker
architecture part design patterns.

150
00:08:17,774 --> 00:08:21,614
A circuit breaker is a pro is a
protective and safety mechanism

151
00:08:21,614 --> 00:08:25,184
that prevents the application from
continuously making requests to a

152
00:08:25,184 --> 00:08:27,524
service that has problems or is done.

153
00:08:27,734 --> 00:08:30,494
This is basically isolation as a strategy.

154
00:08:30,994 --> 00:08:35,514
Think about, a distributed system
where of 10 nodes, where two nodes are

155
00:08:35,514 --> 00:08:41,240
down and you have a circuit breaker
in in, in front of the systems.

156
00:08:41,810 --> 00:08:45,860
When you implement a circuit breaker
design part down the traffic and

157
00:08:45,860 --> 00:08:49,940
traffic would never be directed to
the 14 nodes because the circuit

158
00:08:49,940 --> 00:08:54,110
breaker is, are aware that those nodes
are 40 and therefore traffic would

159
00:08:54,110 --> 00:08:56,150
be directed to the held in nodes.

160
00:08:56,420 --> 00:08:59,120
Circuit breakers prevent
cascading failures by failing.

161
00:08:59,465 --> 00:09:03,775
First when downstream services
degrade, essentially, then we look

162
00:09:03,775 --> 00:09:05,365
at dynamic resource allocation.

163
00:09:05,575 --> 00:09:06,145
Here.

164
00:09:06,715 --> 00:09:08,755
We encourage or we.

165
00:09:09,310 --> 00:09:13,300
We come coincidence with self
healing system that automatically

166
00:09:13,300 --> 00:09:15,160
respond to changing conditions.

167
00:09:15,430 --> 00:09:18,790
We look at Kubernetes portal
auto scaling as a case study that

168
00:09:18,790 --> 00:09:20,590
scales based on cost metrics.

169
00:09:20,890 --> 00:09:24,310
We look at predictive scaling systems
that analyze historical patterns and

170
00:09:24,460 --> 00:09:28,595
scale preemptively resource portals that
automatically redistribute adequate.

171
00:09:28,595 --> 00:09:31,180
We distribute capacity during
degraded performance and intelligent

172
00:09:31,180 --> 00:09:37,620
load that prioritize traffic base
on business impact Then of all.

173
00:09:37,890 --> 00:09:40,770
Monitoring of and observability,
like I earlier said, you

174
00:09:40,770 --> 00:09:42,630
can't fix what you can't see.

175
00:09:42,900 --> 00:09:47,790
Logs, traces, and metrics together from
the backbone of modern observability stack

176
00:09:48,030 --> 00:09:50,250
that gives engineers actionable insight.

177
00:09:50,610 --> 00:09:55,050
Monitoring must always establish
baseline behavior and detect anomaly

178
00:09:55,260 --> 00:10:00,060
automatically is your monitoring and
observability stack should always

179
00:10:00,060 --> 00:10:04,530
be able to distinguish between noise
and actionable signals because you

180
00:10:04,530 --> 00:10:06,960
do not want your engineers waking up.

181
00:10:07,245 --> 00:10:12,525
In the middle of the night for noisy
signals for or un actionable alerts.

182
00:10:12,705 --> 00:10:18,395
Basically, that's why you have to build
at a stable and proactive monitoring

183
00:10:18,935 --> 00:10:21,005
observability stock for your service.

184
00:10:21,505 --> 00:10:23,575
Now we look at common failure patterns.

185
00:10:23,845 --> 00:10:24,775
This is fine.

186
00:10:25,765 --> 00:10:28,885
So failure pattern number one,
we look at cascading failures.

187
00:10:29,295 --> 00:10:33,385
The look at multiple case study
here, which will be, provided

188
00:10:33,540 --> 00:10:38,750
in the documentation we look
at Slack's 2021 global outage.

189
00:10:38,750 --> 00:10:44,030
We look at the Netflix Eve, Netflix,
Christmas Eve outage in 2012.

190
00:10:44,080 --> 00:10:46,030
These are example of retry stops.

191
00:10:46,360 --> 00:10:49,040
We look at failures due to
resource contention, the

192
00:10:49,130 --> 00:10:51,440
Robinhood trading outage in 2020.

193
00:10:51,710 --> 00:10:57,110
These are example, these are public
examples of what failure or what

194
00:10:57,170 --> 00:10:58,850
common failure patterns look like.

195
00:10:59,240 --> 00:11:00,710
E essentially.

196
00:11:01,580 --> 00:11:03,860
Secondly, we look at operational failures.

197
00:11:03,860 --> 00:11:08,550
We look at when there is a configuration
drift, a deployment problem, or

198
00:11:08,550 --> 00:11:10,350
human error configuration drift.

199
00:11:10,890 --> 00:11:15,670
Firstly, when there is a slight change
in configuration, maybe from a a

200
00:11:15,670 --> 00:11:19,510
third party provider or even from a
user, we, the case study here is the

201
00:11:19,510 --> 00:11:22,480
Salesforce database outage in 2019.

202
00:11:22,510 --> 00:11:24,640
Look at data deployment problems.

203
00:11:24,640 --> 00:11:25,585
Look at TSB Bank.

204
00:11:26,470 --> 00:11:31,380
Migration failure in 2018, which
occurred when they tried to migrate

205
00:11:31,560 --> 00:11:34,980
their services from one provider to
another provider, and they run into

206
00:11:34,980 --> 00:11:39,140
deployment issues because this were not
planned for this deal, not preemptive.

207
00:11:39,500 --> 00:11:43,490
Look at the night capital trading loss
in 2012 and the Amazon SD outage in

208
00:11:43,490 --> 00:11:50,360
2017, which a whole lot of services
globally were heavily relied upon.

209
00:11:50,690 --> 00:11:51,170
We look at.

210
00:11:51,575 --> 00:11:54,455
Failures due, operational
failures due to human error.

211
00:11:54,815 --> 00:12:00,955
GitLab data loss in 2017 were
where a GitLab engineer mistakenly

212
00:12:00,955 --> 00:12:06,735
deleted production db and the
outage was due, was out for, and the

213
00:12:06,735 --> 00:12:08,415
services were out for four hours.

214
00:12:08,760 --> 00:12:11,340
These are things we do not want to happen.

215
00:12:11,620 --> 00:12:13,150
But they are common failure patterns.

216
00:12:13,300 --> 00:12:14,680
Failure due to human error.

217
00:12:15,040 --> 00:12:17,955
Look at failure pattern number
three, which is software failures.

218
00:12:18,455 --> 00:12:21,965
Failures due to resource
exhaustion, dependency failure.

219
00:12:21,965 --> 00:12:23,825
We look at the GitHub details.

220
00:12:23,825 --> 00:12:26,945
I would encourage you to look at
the GitHub leaders incident in

221
00:12:26,945 --> 00:12:29,675
2018 and the Reddit outage in 2016.

222
00:12:29,885 --> 00:12:32,915
These are failures due to resource
exhaustion when systems aren't

223
00:12:32,945 --> 00:12:39,665
architected or built in resilience
or to act to scale for when there's

224
00:12:39,715 --> 00:12:41,575
when there is an increase in traffic.

225
00:12:41,605 --> 00:12:43,615
This is what happens, dependency failures.

226
00:12:43,665 --> 00:12:44,145
When the.

227
00:12:45,070 --> 00:12:48,580
System is updated or there's a system
upgrade and there is no accountability

228
00:12:48,670 --> 00:12:50,710
on what dependency looks like.

229
00:12:50,710 --> 00:12:51,700
Look at that Stripe.

230
00:12:51,910 --> 00:12:56,470
A PIA case study will be the
Stripe API Outage in 28 in 2019,

231
00:12:56,740 --> 00:13:00,490
and fastly CD CDN outage in 2021.

232
00:13:00,990 --> 00:13:03,425
What was achieving 99.999?

233
00:13:04,120 --> 00:13:06,220
Reliability look like the five nine.

234
00:13:06,220 --> 00:13:09,190
Reliability just means five
minutes of downtime per year.

235
00:13:09,520 --> 00:13:10,930
Is that possible?

236
00:13:11,170 --> 00:13:14,580
When broad says downtime,
it's can we do that?

237
00:13:14,580 --> 00:13:15,990
Is that achievable?

238
00:13:16,200 --> 00:13:17,280
What does achieving down?

239
00:13:17,360 --> 00:13:18,590
What does this look like?

240
00:13:18,590 --> 00:13:21,550
What does achieving five
minutes of downtime look like?

241
00:13:21,820 --> 00:13:24,520
It looks like eliminating
all single points of failure,

242
00:13:24,520 --> 00:13:26,020
implementing zero downtime.

243
00:13:26,320 --> 00:13:27,760
Deployments and rollback.

244
00:13:28,030 --> 00:13:32,560
Designing for partial availability during
degradation, redundant infrastructure,

245
00:13:32,770 --> 00:13:37,090
resilient network architecture,
comprehensive monitoring, and also

246
00:13:37,090 --> 00:13:42,860
on the code level microservice design
pattern with resilience patterns.

247
00:13:43,730 --> 00:13:48,950
Essentially, that is what achieving
five nights means, essentially

248
00:13:49,160 --> 00:13:50,900
five minutes of downtime in a year.

249
00:13:50,930 --> 00:13:51,890
It's achievable.

250
00:13:51,890 --> 00:13:54,440
It is, it's possible when we plan for.

251
00:13:55,280 --> 00:13:56,510
A reliable system.

252
00:13:56,510 --> 00:13:59,120
That is what we achieve basically.

253
00:13:59,450 --> 00:14:03,890
So we look at the resilience
engineering toolkits, what's how

254
00:14:03,950 --> 00:14:08,570
we can identify fragility code, PA
code and patterns, and monitor and

255
00:14:08,570 --> 00:14:10,880
measure for identify fragility.

256
00:14:10,880 --> 00:14:14,960
We use architecture, architectural reviews
and failure modeling to look at single

257
00:14:15,050 --> 00:14:17,780
pain of failure and fragile dependencies.

258
00:14:17,990 --> 00:14:20,870
By fragile dependencies, we
look at mean dependencies that.

259
00:14:21,385 --> 00:14:24,565
That are actively updated or
actively wrote that we don't want

260
00:14:24,565 --> 00:14:28,505
to design our systems on those
kind of dependencies because we.

261
00:14:29,005 --> 00:14:31,255
We need to be able to
account for those changes.

262
00:14:31,525 --> 00:14:36,775
Essentially, we look at how we architect
our code and the kind of part, the kind

263
00:14:36,775 --> 00:14:41,630
of software partners we used in designing
our applications, because basically that's

264
00:14:41,780 --> 00:14:44,690
where the that always is the bottleneck.

265
00:14:44,930 --> 00:14:49,460
If we don't have accurate foundational
patterns or proper foundational patterns,

266
00:14:49,820 --> 00:14:51,495
then we also need to monitor and measure.

267
00:14:52,325 --> 00:14:56,165
Monitoring our measurement in our
resilience code toolkit because we

268
00:14:56,165 --> 00:15:00,455
need to define our SLIs and slus the
indicators and the objectives and the

269
00:15:00,455 --> 00:15:05,315
error budgets, and we also need to
have proactive dashboards and treat to

270
00:15:05,315 --> 00:15:07,775
observe this system health in real time.

271
00:15:08,275 --> 00:15:10,905
In conclusion, I feel.

272
00:15:11,355 --> 00:15:14,385
Can only be embraced essentially.

273
00:15:14,385 --> 00:15:18,195
That's why we need to embrace the
inevitable is your system built

274
00:15:18,195 --> 00:15:20,745
to break and bounce back stronger.

275
00:15:21,015 --> 00:15:23,265
Failure is not a risk, it's a certainty.

276
00:15:23,415 --> 00:15:27,795
Building failure, resilient distributed
system requires bold design and continuous

277
00:15:27,795 --> 00:15:30,675
experimentation and culture of resilience.

278
00:15:30,885 --> 00:15:35,685
With the right tools and mindsets,
we can build systems that don't just

279
00:15:35,685 --> 00:15:37,755
survive chaos, but thrive in it.

280
00:15:38,325 --> 00:15:40,215
Thank you very much and.

281
00:15:40,600 --> 00:15:44,980
I will be willing to answer any
questions about system resilience.

282
00:15:45,340 --> 00:15:46,615
Thank you very much once again.

