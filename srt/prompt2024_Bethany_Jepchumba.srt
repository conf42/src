1
00:00:00,210 --> 00:00:04,309
Hello and a warm welcome to PROMPT
Engineering Conference 2024.

2
00:00:04,790 --> 00:00:10,070
I am Bethany Jebchumba and I'll be taking
you through inside multimodal models.

3
00:00:10,600 --> 00:00:12,919
A little bit about myself
before we get started.

4
00:00:13,309 --> 00:00:17,840
I am a cloud advocate at Microsoft
and in case you need to reach out

5
00:00:17,889 --> 00:00:22,239
to me after the session you can find
me on the internet at Bethany Jeb.

6
00:00:22,739 --> 00:00:28,119
In this session, we'll be covering a lot
around multimodal models, understanding

7
00:00:28,119 --> 00:00:33,469
the inner workings of these models,
understanding what modality is, And

8
00:00:33,869 --> 00:00:38,659
following up with a couple of demos
to showcase how you can apply multi

9
00:00:38,959 --> 00:00:40,779
modal models in your day to day life.

10
00:00:41,279 --> 00:00:45,939
We are currently living in the era
of AI, and over the last few years,

11
00:00:46,009 --> 00:00:51,439
we have seen the speed of innovation
in the AI space be truly incredible.

12
00:00:51,969 --> 00:00:56,159
The breakthroughs, there are a couple
of breakthroughs happening many times

13
00:00:56,159 --> 00:00:58,059
a week across the entire industry.

14
00:00:58,679 --> 00:01:04,629
And About more than 50 percent of the
organizations have adopted AI in at

15
00:01:04,629 --> 00:01:11,189
least one business area And 70 percent
of employees are ready to test out AI and

16
00:01:11,189 --> 00:01:17,720
see what it is about The idea behind AI
and generative AI has been Language models

17
00:01:17,720 --> 00:01:23,645
can be able to generate new contexts and
in Majority of the times we've seen the

18
00:01:23,645 --> 00:01:27,085
content is particularly of one modality.

19
00:01:27,085 --> 00:01:33,114
For example, having content in natural
language, interacting with chat or

20
00:01:33,164 --> 00:01:38,125
having models that interact with the
modality of vision or speech only.

21
00:01:38,625 --> 00:01:41,514
But in real, models are amazing.

22
00:01:41,914 --> 00:01:46,409
But in reality, we as humans interact
with the real world using multiple senses.

23
00:01:46,850 --> 00:01:51,100
Since computers have been unable to
interact across multiple modalities,

24
00:01:51,110 --> 00:01:54,710
it has meant that us human beings
have had to learn how to interact with

25
00:01:54,719 --> 00:01:56,930
computers versus the other way around.

26
00:01:57,299 --> 00:01:59,080
But now everything is changing.

27
00:01:59,580 --> 00:02:05,089
The question around multimodality
is we as humans can be able to grasp

28
00:02:05,570 --> 00:02:06,749
all these different modalities.

29
00:02:06,749 --> 00:02:08,050
We can be able to see.

30
00:02:08,449 --> 00:02:11,820
It's a pretty different languages
and speak out loud, almost at the

31
00:02:11,820 --> 00:02:13,880
same time and almost simultaneously.

32
00:02:14,460 --> 00:02:17,460
But why can't large
language models do the same?

33
00:02:17,960 --> 00:02:22,819
The modalities available currently
include language and have models

34
00:02:22,829 --> 00:02:26,249
such as GPTs, which understand
and generate natural language.

35
00:02:26,870 --> 00:02:30,679
And then we have Whisper where models
such as, we have speech where models

36
00:02:30,690 --> 00:02:34,350
such as Whisper are able to understand
and generate spoken language.

37
00:02:34,844 --> 00:02:38,314
And finally, we have vision where
models such as Dali generate

38
00:02:38,364 --> 00:02:40,874
new images from textual prompts.

39
00:02:41,724 --> 00:02:45,874
But now when large language models
come into play, models can be able

40
00:02:45,924 --> 00:02:51,364
to process multiple data types, for
example, text, images, audio, and vision,

41
00:02:51,844 --> 00:02:54,284
and rezone across various scenarios.

42
00:02:54,784 --> 00:02:59,874
Before we go further into the inner
workings of multimodal models, the first

43
00:02:59,874 --> 00:03:07,619
thing we'll do is define exactly what The
technology, the architecture behind it is.

44
00:03:08,109 --> 00:03:10,159
And the first one is
the attention mechanism.

45
00:03:10,724 --> 00:03:13,874
Whereby models are able to weigh
the importance of different parts

46
00:03:13,884 --> 00:03:17,714
of the input data and provide
better context understanding.

47
00:03:18,524 --> 00:03:22,734
The attention mechanism demands a
focus on what is relevant at hand.

48
00:03:23,404 --> 00:03:28,825
And you can be able to build
relationships within a single modality

49
00:03:28,875 --> 00:03:35,385
or across modalities to be able to
now give contextual understanding

50
00:03:35,845 --> 00:03:38,284
and provide insights on which.

51
00:03:38,990 --> 00:03:42,660
Tax is more valuable than the
other based on the question asked.

52
00:03:43,280 --> 00:03:46,820
And then the transformer architecture,
which relies on these attention

53
00:03:46,830 --> 00:03:51,200
mechanisms to be able to transform
data and ensures you can be able

54
00:03:51,200 --> 00:03:53,170
to handle multiple modalities.

55
00:03:53,749 --> 00:03:58,570
For example, you can define
specific encoders that feed

56
00:03:58,630 --> 00:04:00,040
to a shared transformer layer.

57
00:04:00,640 --> 00:04:05,810
And these encoders, once in the same
layer, can be able to come in and

58
00:04:05,910 --> 00:04:07,504
perform multimodal transformation.

59
00:04:08,004 --> 00:04:11,824
Examples, then onto
the multi modal models.

60
00:04:12,284 --> 00:04:14,004
I'll talk about two today.

61
00:04:14,004 --> 00:04:17,804
The first one is CLIP, which
is Contrastive Language

62
00:04:17,814 --> 00:04:20,324
Image Pre training and DALI.

63
00:04:20,824 --> 00:04:27,974
How CLIP works is you're able to get
a lot of image and you're able to get

64
00:04:27,974 --> 00:04:33,824
a lot of image and Caption pairs and
code both of them in different encoders

65
00:04:34,324 --> 00:04:38,074
and have the embeddings in the same
vector space, whereby now you're able

66
00:04:38,114 --> 00:04:40,264
to match those with a similar pair.

67
00:04:40,944 --> 00:04:46,924
And those that are not similar, the pair
that are not matching are able to be.

68
00:04:47,424 --> 00:04:51,094
Those pairs that are not matching
are able to be minimized,

69
00:04:51,504 --> 00:04:53,754
ensuring you're able to now.

70
00:04:54,514 --> 00:04:58,824
Get a better response based
on what your input will be.

71
00:04:59,324 --> 00:05:03,524
For example, if you want to classify
images or generate a new image, it

72
00:05:03,524 --> 00:05:08,594
will be simpler to do so because
you can be able to have the caption

73
00:05:08,664 --> 00:05:14,144
and have the caption there and get
an image encoded, decoded for you.

74
00:05:14,924 --> 00:05:16,294
And then the next one is DALY.

75
00:05:17,064 --> 00:05:20,804
How DALY works is you're
able to give a prompt.

76
00:05:21,309 --> 00:05:24,569
And get a response based
on the prompt we've given.

77
00:05:25,229 --> 00:05:30,129
The mechanisms behind this is the
training data, whereby we have billions

78
00:05:30,129 --> 00:05:33,649
of parameters used to train the model.

79
00:05:33,699 --> 00:05:39,089
These are vast parameters that contain
image and text pairs, which are now

80
00:05:39,099 --> 00:05:44,069
able to give the association between
textual and visual information.

81
00:05:44,779 --> 00:05:47,269
Therefore, allowing you to
be as creative as possible.

82
00:05:47,949 --> 00:05:53,289
And the more the data, the better
the performance of the model.

83
00:05:53,289 --> 00:05:55,499
Up to, of course, a certain level.

84
00:05:56,099 --> 00:06:00,339
And at the base of DALI is transform
architecture, which, of course,

85
00:06:00,369 --> 00:06:06,589
ensures you're able to have the
attention mechanism to decode text

86
00:06:06,589 --> 00:06:09,609
prompts and transform it into images.

87
00:06:10,109 --> 00:06:11,599
We've talked a bit about the theory.

88
00:06:11,839 --> 00:06:14,199
So let's see multimodality in action.

89
00:06:14,559 --> 00:06:19,659
Before we go into it, the first thing
I'll mention is you can be able to

90
00:06:20,119 --> 00:06:22,789
access all these models on Azure OpenAI.

91
00:06:23,319 --> 00:06:26,604
And some of the models you
can access on Azure OpenAI.

92
00:06:26,754 --> 00:06:30,164
For specific modalities include
something like GPT 4 or 3.

93
00:06:30,164 --> 00:06:36,924
5 Turbo for your text generation,
Whisper for your audio generation,

94
00:06:36,954 --> 00:06:39,374
and Dally for image generation.

95
00:06:39,889 --> 00:06:45,319
But today we'll be exploring GPT-4 O for
our multi mod, our multimodal examples.

96
00:06:46,159 --> 00:06:47,299
So onto the demo,

97
00:06:47,799 --> 00:06:51,579
a way you can be able to apply
GPT-4 O, that is a multimodal

98
00:06:51,579 --> 00:06:56,619
model, is, for example, getting
detailed descriptions of your image.

99
00:06:56,619 --> 00:07:01,355
So I will upload an image of my
cookies that are recently backed

100
00:07:01,775 --> 00:07:05,495
and ask what is in the image.

101
00:07:05,995 --> 00:07:10,175
The response should be a description
of what exactly the image is

102
00:07:10,205 --> 00:07:14,355
about and it shows a baking
tray with several cookies on it.

103
00:07:14,725 --> 00:07:18,665
The cookies appear to be homemade,
slightly golden brown color.

104
00:07:18,995 --> 00:07:23,225
They're uneven and it's very
characteristic of many homemade cookies.

105
00:07:23,226 --> 00:07:32,925
So you can ask, okay, how can I
improve my baking based on now the

106
00:07:32,925 --> 00:07:39,025
feedback I got from the model and
it will give me ideas of probably

107
00:07:39,035 --> 00:07:41,635
should be able to do this and this.

108
00:07:41,835 --> 00:07:43,825
let's give it a second as it loads.

109
00:07:44,325 --> 00:07:46,165
It's still generating the response.

110
00:07:46,175 --> 00:07:48,555
So let's give it a second
and hear the response.

111
00:07:49,085 --> 00:07:51,615
So first, I ensure you read the recipe.

112
00:07:52,025 --> 00:07:57,375
Quality ingredients, the accuracy in
measurement, and so much more, which

113
00:07:57,395 --> 00:07:59,955
ensures your baking is way better.

114
00:08:00,615 --> 00:08:05,615
The next way is you can be able
to use your model to be able to

115
00:08:05,615 --> 00:08:07,795
detect if there's something wrong.

116
00:08:08,235 --> 00:08:14,645
With a particular image, so I can be
able to upload any of these images.

117
00:08:15,145 --> 00:08:18,575
Let's upload this cake that
was generated by Dal E.

118
00:08:18,925 --> 00:08:20,745
And I can ask, is there

119
00:08:21,245 --> 00:08:25,005
an anomaly in this image?

120
00:08:25,825 --> 00:08:26,865
I, good news comes.

121
00:08:27,305 --> 00:08:34,095
case for this is, especially in areas that
experience things like droughts or floods,

122
00:08:34,485 --> 00:08:39,935
you can be able to upload different images
and understand what is happening there.

123
00:08:40,505 --> 00:08:43,195
Also, this cake does
not have any anomalies.

124
00:08:43,585 --> 00:08:47,055
It's aesthetically pleasing
and everything is well done.

125
00:08:47,685 --> 00:08:49,915
So yay, no anomalies in the cake.

126
00:08:50,605 --> 00:08:54,205
And then the other bit you
can be able to do is help you

127
00:08:54,235 --> 00:08:55,975
understand and generate graphs.

128
00:08:56,325 --> 00:08:58,655
So I have a Power BI graph here.

129
00:08:59,575 --> 00:09:05,085
And I can be able to ask Dali
to explain the dashboard.

130
00:09:05,585 --> 00:09:14,515
What else I can be able
to add into the dashboard.

131
00:09:15,015 --> 00:09:17,475
Once that's done, I can
be able to upload it.

132
00:09:18,115 --> 00:09:23,025
And it will give me a description of
this dashboard, which shows a retail

133
00:09:23,075 --> 00:09:28,720
analysis of sales, the sales based
on the number of stores, the sales

134
00:09:28,720 --> 00:09:34,360
that happened that year, where did
the most sales comes from, and more

135
00:09:34,360 --> 00:09:36,760
details around the sales in the store.

136
00:09:37,360 --> 00:09:43,280
So let's give, opening our model a
few seconds for it to finish replying.

137
00:09:43,280 --> 00:09:46,220
Then you can be able
to check up the answer.

138
00:09:46,720 --> 00:09:47,110
Okay.

139
00:09:47,110 --> 00:09:49,240
Seems like there's a problem with my,

140
00:09:49,740 --> 00:09:49,950
yep.

141
00:09:50,190 --> 00:09:51,180
Here comes the reply.

142
00:09:51,700 --> 00:09:55,500
So we can be able to see these
are the sections and what

143
00:09:55,500 --> 00:09:56,820
are the sections indicate.

144
00:09:57,560 --> 00:09:59,650
These are other sections,
what it indicates.

145
00:09:59,660 --> 00:10:02,760
So the top section, the middle
and then the bottom section.

146
00:10:03,260 --> 00:10:06,970
And it's also telling us what additional
metrics we can be able to create.

147
00:10:07,490 --> 00:10:17,950
So probably I can ask, how can I
increment the sales trend over.

148
00:10:18,450 --> 00:10:26,300
multiple years and it's able to give
me step by step instructions to be

149
00:10:26,330 --> 00:10:31,730
able to now be able to create with
a multi year trend analysis to see

150
00:10:32,070 --> 00:10:37,130
the long term growth and decline of
the sales over the period of time.

151
00:10:37,630 --> 00:10:44,050
Then once you're done with DALI for
ensuring you can be able to create

152
00:10:44,050 --> 00:10:47,360
and generate your different graphs,
create more graphs, you can also

153
00:10:47,370 --> 00:10:49,180
be able to convert this into code.

154
00:10:49,470 --> 00:10:53,210
For example, if you have one of
the graphs that you really and you

155
00:10:53,210 --> 00:10:56,500
want to convert to code, that's
something you can be able to do.

156
00:10:57,200 --> 00:11:01,910
There are ways you can be able to
use multimodality is translating

157
00:11:01,910 --> 00:11:02,860
between different languages.

158
00:11:03,615 --> 00:11:08,135
I don't know, but, there's sometimes
you find a sticker and it's in a very

159
00:11:08,135 --> 00:11:11,475
different language, maybe on your
clothes, or maybe even a poster on the

160
00:11:11,475 --> 00:11:15,645
wall, or you're visiting a new country
and you're seeing all these posters and

161
00:11:15,645 --> 00:11:17,815
you want to get a translation of them.

162
00:11:18,165 --> 00:11:22,825
You can just take the picture of
the image and upload to GPT 40

163
00:11:22,825 --> 00:11:24,845
and get a response based on that.

164
00:11:25,345 --> 00:11:30,895
And then the last bit of how I use
this in a personal level is I use it

165
00:11:31,135 --> 00:11:36,525
for my designs and for my drawings
to be able to now get feedback of,

166
00:11:36,935 --> 00:11:38,605
okay, this is a drawing I made.

167
00:11:38,905 --> 00:11:44,485
Can you be able to convert this for me
or using it on my handwriting to be able

168
00:11:44,485 --> 00:11:47,245
to tell, okay, are you able to give me.

169
00:11:47,540 --> 00:11:50,720
feedback based on this, if
there's anything I need to

170
00:11:50,720 --> 00:11:52,930
change about my handwriting.

171
00:11:53,270 --> 00:11:57,270
So those are the different
use cases for GPT 4.

172
00:11:57,270 --> 00:12:04,230
And as we wrap up, we've been able
to cover what exactly GPT 4 is about,

173
00:12:04,650 --> 00:12:10,100
what exactly you can be able to do
with the different multi modal models,

174
00:12:10,400 --> 00:12:12,745
how you can be Able to implement them.

175
00:12:13,525 --> 00:12:18,295
So my final parting search is you can
be able to join the A JA community

176
00:12:18,295 --> 00:12:21,835
on Discord to learn more about these
multi model models, learn how you can

177
00:12:21,835 --> 00:12:26,845
be able to build them, and also get
started with the generative AI course

178
00:12:27,355 --> 00:12:29,935
for beginners, which gets you into now.

179
00:12:30,025 --> 00:12:31,615
Yes, learn the basics.

180
00:12:31,615 --> 00:12:37,225
So how can you continue forward and
create more robust applications?

181
00:12:37,675 --> 00:12:39,505
Thank you so much, and have a great day.

