1
00:00:00,500 --> 00:00:01,670
Speaker 24: Good morning everyone.

2
00:00:01,790 --> 00:00:05,480
It's an honor to be here at
the DevOps Conference 2026.

3
00:00:06,200 --> 00:00:07,520
My name is Hil Coker.

4
00:00:08,420 --> 00:00:11,210
Throughout my career at
Google, Salesforce, and Disney.

5
00:00:11,710 --> 00:00:16,390
I've had unique opportunity to build
and manage systems that don't just

6
00:00:16,390 --> 00:00:19,090
serve millions but billions of users.

7
00:00:19,659 --> 00:00:24,430
At Disney, I've seen our ad
platforms handle over 200 billion

8
00:00:24,430 --> 00:00:28,089
annual impressions, maintaining
low latencies, even during

9
00:00:28,089 --> 00:00:29,830
massive spikes like sport events.

10
00:00:30,754 --> 00:00:34,185
Today our wanna move beyond the
theory of high availability.

11
00:00:35,124 --> 00:00:38,065
We are gonna talk about
extreme scaler reliability.

12
00:00:38,995 --> 00:00:43,315
We will discuss what happens when
traditional scaling hits a wall and

13
00:00:43,315 --> 00:00:45,535
specific architectures and algorithms.

14
00:00:45,535 --> 00:00:48,894
You need to stay online when the
world is knocking at your door.

15
00:00:49,394 --> 00:00:53,745
We are currently in a transition
from large scale to extreme scale.

16
00:00:54,245 --> 00:00:57,994
APIs are no longer just
external interfaces.

17
00:00:58,625 --> 00:01:04,474
They are the nervous system of a
global infrastructure, whether it's IOT

18
00:01:04,474 --> 00:01:09,670
sensors, streaming telemetry, social
media platforms, managing viral burst.

19
00:01:10,645 --> 00:01:13,525
Or even ad tech processing
real time auctions.

20
00:01:13,885 --> 00:01:17,245
The demand for a fine nines
availability is no longer a

21
00:01:17,245 --> 00:01:19,135
luxury, but it's a requirement.

22
00:01:19,635 --> 00:01:23,325
The engineering reality we face is
that traditional scaling hits a wall.

23
00:01:23,955 --> 00:01:25,360
You cannot simply buy a bigger server.

24
00:01:25,860 --> 00:01:29,250
Vertical scaling has physical
limits and diminishing returns.

25
00:01:29,610 --> 00:01:34,890
Even horizontal scaling if done,
enabling introduces coordinated overhead.

26
00:01:35,789 --> 00:01:41,399
As you add more nodes, the time
system spends for self checks, service

27
00:01:41,399 --> 00:01:46,710
discovery, state synchronization,
all these things starts to exceed the

28
00:01:46,710 --> 00:01:48,899
time spent to doing the actual work.

29
00:01:49,410 --> 00:01:52,020
In this doc, we aren't looking for hacks.

30
00:01:52,520 --> 00:01:58,730
We are looking for a systematic framework
that aligns your architecture with

31
00:01:58,940 --> 00:02:06,050
algorithmic efficiency to ensure your
API remains fast and cost effective as

32
00:02:06,050 --> 00:02:08,540
you cross that billion request threshold.

33
00:02:09,040 --> 00:02:13,300
Before we look at the how, let's look
at the who we are standing on the

34
00:02:13,300 --> 00:02:17,065
shoulders of giants who have solved
these complex problems at scale.

35
00:02:17,565 --> 00:02:19,755
Netflix is a good example for this.

36
00:02:20,205 --> 00:02:24,075
With over two 50 million subscribers,
they don't just serve video.

37
00:02:24,765 --> 00:02:27,345
They manage a complex
web of microservices.

38
00:02:27,845 --> 00:02:32,825
They have pioneered the use of edge
caching and adaptive streaming algorithms

39
00:02:33,125 --> 00:02:40,115
that adjust the quality in real time
based on network congestion, ensuring

40
00:02:40,115 --> 00:02:41,915
the user experience never status.

41
00:02:42,415 --> 00:02:46,555
Uber handles billion of
real time location updates.

42
00:02:46,855 --> 00:02:50,575
To manage this, they utilize
something called S Geo Sharding.

43
00:02:51,355 --> 00:02:57,175
By partitioning the data and processing
based on geographic cells, they ensure

44
00:02:57,175 --> 00:03:03,415
that traffic surge in a one city does
not create a noisy neighbor effect on

45
00:03:03,415 --> 00:03:05,635
any other city and any other service.

46
00:03:06,535 --> 00:03:08,035
And then look at revenue Cat.

47
00:03:08,535 --> 00:03:13,545
They provide a masterclass in
request coalescing by identifying

48
00:03:13,545 --> 00:03:18,015
when multiple clients are asking
for the exact same piece of data at

49
00:03:18,015 --> 00:03:23,025
the same time, they collapse those
requests into a single backend call.

50
00:03:23,525 --> 00:03:29,915
The simple algorithm change can reduce
backend load by the order of magnitudes.

51
00:03:30,415 --> 00:03:32,810
Let's look at the four
layer scaling framework now.

52
00:03:33,310 --> 00:03:38,410
To manage extreme scale without
getting lost in the weeds, I recommend

53
00:03:38,500 --> 00:03:41,140
this four layer scaling framework.

54
00:03:41,800 --> 00:03:45,640
You should view this as series of
defendants defensive parameters.

55
00:03:46,140 --> 00:03:47,400
First is the edge layer.

56
00:03:48,000 --> 00:03:49,375
This is your first line of defense.

57
00:03:49,875 --> 00:03:56,385
It is about being close to the user
using CDN caching, geo routing.

58
00:03:56,775 --> 00:04:00,915
To reduce the physical distance, your
data has to travel to reach the user.

59
00:04:01,415 --> 00:04:02,705
Second is a gateway layer.

60
00:04:03,185 --> 00:04:04,715
This is your traffic cop.

61
00:04:05,285 --> 00:04:11,105
It handles authentication and crucially
distributed rate limiting to provide

62
00:04:11,405 --> 00:04:13,355
to protect your internal services.

63
00:04:13,855 --> 00:04:15,685
Next is the microservices layer.

64
00:04:16,315 --> 00:04:18,235
Here we focus on the resilience.

65
00:04:18,265 --> 00:04:23,155
We ensure that service meshes that we
create and circuit breakers to ensure

66
00:04:23,155 --> 00:04:27,475
that if one service fails, it does
not take down an entire ecosystem.

67
00:04:27,975 --> 00:04:30,345
Last, but not the least, the data layer.

68
00:04:31,215 --> 00:04:34,275
Here we address the concern
for ultimate bottleneck.

69
00:04:35,055 --> 00:04:39,555
We use sharding and distributed
caching to ensure that databases

70
00:04:39,615 --> 00:04:41,835
can scale as fast as the compute.

71
00:04:42,335 --> 00:04:46,955
By looking at your system through these
four lenses, you can identify exactly

72
00:04:46,955 --> 00:04:52,805
where your current bottleneck six, and
which tool is best suited to fix it.

73
00:04:53,305 --> 00:04:57,115
The first layer is the edge in 2026.

74
00:04:57,175 --> 00:05:02,155
A CDN isn't just for static images,
it's for dynamic API responses.

75
00:05:02,965 --> 00:05:08,275
The key to extreme scale caching
is strategic cache key design.

76
00:05:08,995 --> 00:05:14,125
If your cash key is too specific,
your head rate drops and traffic

77
00:05:14,125 --> 00:05:15,415
flows through the origin.

78
00:05:16,150 --> 00:05:18,820
And if it's too broad,
you serve stale data.

79
00:05:19,660 --> 00:05:25,240
You can solve this by segmenting
keys, by user class, geographic

80
00:05:25,240 --> 00:05:27,160
location, or even device type.

81
00:05:27,660 --> 00:05:31,530
You can use the stale
white revalidate pattern.

82
00:05:32,520 --> 00:05:37,080
Imagine a entry expires
in traditional system.

83
00:05:37,350 --> 00:05:41,219
The next request missions the
cash and goes to the backend.

84
00:05:42,120 --> 00:05:48,660
Causing a latency spike for that user
In this architecture, the ad service

85
00:05:49,200 --> 00:05:55,320
serves the stale data immediately, but in
turn triggers an asynchronous background

86
00:05:55,320 --> 00:05:57,780
process to fetch the fresh data.

87
00:05:58,470 --> 00:06:02,790
This guarantees that your user
always sees sub 50 milliseconds

88
00:06:02,790 --> 00:06:05,250
responses even during a cache refresh.

89
00:06:05,750 --> 00:06:09,050
Furthermore, you can
also use adaptive DTLs.

90
00:06:09,755 --> 00:06:14,345
If your backend is under a heavy
load, your edge should automatically

91
00:06:14,345 --> 00:06:17,915
extends at its TTM and practice.

92
00:06:18,065 --> 00:06:23,255
A combination of these can
reduce origin traffic by 95%.

93
00:06:23,755 --> 00:06:26,275
Now, let's look at the
request coalescing patterns.

94
00:06:26,775 --> 00:06:30,870
What happens when your cache
is empty and let's say about.

95
00:06:31,770 --> 00:06:37,050
10,000 requests for the same
item hits you all at once.

96
00:06:37,980 --> 00:06:39,660
This is known as thundering herd.

97
00:06:40,160 --> 00:06:42,380
We saw this with request coalescing.

98
00:06:42,880 --> 00:06:45,850
This is often implemented using
the single flight algorithm.

99
00:06:46,510 --> 00:06:49,930
When the first request
arrives, it misses the cash.

100
00:06:50,920 --> 00:06:54,580
The subsystem marks that
request as in flight.

101
00:06:55,080 --> 00:06:58,170
Any subsequent request for the same id.

102
00:06:58,500 --> 00:07:00,000
I put in the wait queue.

103
00:07:00,500 --> 00:07:05,720
Once the first request returns from
the database, the result is broadcasted

104
00:07:05,780 --> 00:07:07,460
to everyone waiting in the client.

105
00:07:08,420 --> 00:07:09,320
Think about the math.

106
00:07:09,820 --> 00:07:14,440
Instead of sending out 10,000
database queries, you perform one.

107
00:07:15,250 --> 00:07:18,220
This isn't just performance optimization.

108
00:07:18,340 --> 00:07:20,530
It's a reliability insurance policy.

109
00:07:21,220 --> 00:07:23,410
It prevents your database
from melting down.

110
00:07:23,830 --> 00:07:26,020
The moment a piece of content goes viral.

111
00:07:26,520 --> 00:07:30,180
You cannot build a reliable
system if you don't understand

112
00:07:30,180 --> 00:07:31,800
the shape of your traffic.

113
00:07:32,700 --> 00:07:35,245
At scale, we see three primary patterns.

114
00:07:35,745 --> 00:07:35,775
Okay.

115
00:07:36,705 --> 00:07:39,225
Diurnal searches are predictable.

116
00:07:39,225 --> 00:07:41,835
Waves of the sun moves across the globe.

117
00:07:42,335 --> 00:07:48,545
You can use historical modeling to prewarm
your clusters before these peaks arrive.

118
00:07:49,045 --> 00:07:53,335
Burst traffic is the new breaking
news or live event scenario.

119
00:07:54,055 --> 00:07:58,375
Traffic might jump 10 times or a
hundred times in under a minute.

120
00:07:59,275 --> 00:07:59,575
Here.

121
00:07:59,575 --> 00:08:01,495
The goal isn't to scale.

122
00:08:02,035 --> 00:08:08,995
It is to prevent cascading failures
where one overwhelm system service starts

123
00:08:08,995 --> 00:08:14,485
timing out, causing its callers to retry
with further barriers, the service.

124
00:08:15,085 --> 00:08:17,995
So we need to handle,
how do we prevent that?

125
00:08:18,495 --> 00:08:23,505
The adversarial loads are the bottlenecks
or the DDoS attacks on your systems.

126
00:08:24,005 --> 00:08:28,115
At extreme Scale, your system must
be able to distinguish between a

127
00:08:28,115 --> 00:08:34,115
loyal user and a malicious script in
milliseconds understanding which of

128
00:08:34,115 --> 00:08:39,875
these are your design for changes, how
you need to configure your auto-scaling

129
00:08:39,905 --> 00:08:43,265
and protect your layers of your system.

130
00:08:43,765 --> 00:08:49,305
The distributed LA rate limiting
is your API's immune system, but

131
00:08:49,305 --> 00:08:52,545
in a distributed environment,
it's actually a difficult problem.

132
00:08:53,045 --> 00:08:56,705
Here we primarily use to two algorithms.

133
00:08:57,215 --> 00:08:59,015
The first one is called token bucket.

134
00:08:59,765 --> 00:09:04,235
It allows for the burst
bursts nature of the traffic.

135
00:09:04,940 --> 00:09:09,200
A user can send flurry of requests as
long as they have tokens in their bucket.

136
00:09:09,740 --> 00:09:15,170
This is great for modern web apps
that might load 20 resources at once.

137
00:09:15,670 --> 00:09:20,890
The leaky bucket and algorithm enforces
a strict smooth flow of traffic.

138
00:09:21,700 --> 00:09:25,960
It's what you can use to protect
fragile downstreams legacy systems

139
00:09:25,960 --> 00:09:30,400
that can only handle specific
requests per second exit rate.

140
00:09:30,900 --> 00:09:34,680
But the real engineering
challenge is global coordination.

141
00:09:35,180 --> 00:09:40,070
Let's say for example, if a user
has a limit of thousand requests

142
00:09:40,070 --> 00:09:45,050
per minute and you have 50 data
centers, how do you keep track?

143
00:09:45,550 --> 00:09:50,410
You can use a local first
approach, nodes track.

144
00:09:50,485 --> 00:09:56,685
It limits locally and sync with
a central Redis or a similar

145
00:09:56,685 --> 00:09:58,995
cache clustered up periodically.

146
00:09:59,805 --> 00:10:04,685
This gives you the speed of local checks
with the accuracy of global enforcements.

147
00:10:05,185 --> 00:10:07,405
Next is multi-region active architecture.

148
00:10:08,275 --> 00:10:12,415
So when your request reads,
reaches a billion requests.

149
00:10:12,915 --> 00:10:18,755
High availability means multi-region,
but usually active passive,

150
00:10:19,255 --> 00:10:23,485
where one region sits idle is
wasteful and slow to fail over.

151
00:10:24,265 --> 00:10:27,205
Hence, we should always
try for an active solution.

152
00:10:27,895 --> 00:10:33,745
And this model, the traffic is routed
via a geo DNS to nearest healthy region.

153
00:10:34,615 --> 00:10:37,945
If US East one experiences a power outage.

154
00:10:38,680 --> 00:10:41,200
Traffic naturally flows to us to West two.

155
00:10:41,700 --> 00:10:44,280
The complexity here is data consistency.

156
00:10:44,780 --> 00:10:46,939
You can utilize regional autonomy.

157
00:10:47,930 --> 00:10:53,449
Each region should have enough
data locally to serve a request,

158
00:10:53,839 --> 00:10:56,964
even if cross region fiber is cut.

159
00:10:57,464 --> 00:11:00,314
You can use asynchronous
replication for noncritical data.

160
00:11:00,814 --> 00:11:08,944
And conflict free replicated data
types CDTs to ensure that if two users

161
00:11:08,974 --> 00:11:13,474
update their profile in different
regions simultaneously, the data merges

162
00:11:13,474 --> 00:11:16,145
correctly without any human intervention.

163
00:11:16,645 --> 00:11:19,045
Next, look, let's look at
the sharding techniques.

164
00:11:19,545 --> 00:11:22,334
Eventually your database
becomes a bottleneck.

165
00:11:22,834 --> 00:11:28,375
When a single cluster can't hold
data, we shard, you can use something

166
00:11:28,375 --> 00:11:29,995
called a consistent hashing.

167
00:11:30,715 --> 00:11:36,975
So unlike traditional hashing, the mod
N hashing that is consistent hashing

168
00:11:36,975 --> 00:11:42,694
allows you to add or remove database
nodes without remapping the entire

169
00:11:42,694 --> 00:11:45,160
data set by using virtual nodes.

170
00:11:45,814 --> 00:11:53,134
We ensure that data is distributed evenly
across the cluster, preventing hot shards.

171
00:11:53,634 --> 00:11:58,555
You can also use geographic sharding
to keep user data in their home region,

172
00:11:59,364 --> 00:12:01,194
which isn't just a performance win.

173
00:12:01,224 --> 00:12:04,104
It's often a legal requirement
for data residency.

174
00:12:04,604 --> 00:12:10,514
A hybrid approach combines the user
ID and geographic usually provides the

175
00:12:10,514 --> 00:12:13,364
best balance of performance and scale.

176
00:12:13,864 --> 00:12:19,984
As we move into the microservice layer,
managing communication between 500 plus

177
00:12:19,984 --> 00:12:22,984
services becomes impossible manually.

178
00:12:23,484 --> 00:12:27,175
Here we use a service
mesh like NY or Istio.

179
00:12:27,675 --> 00:12:32,925
The mesh acts as a transparent
sidecar to every service.

180
00:12:33,495 --> 00:12:34,845
It handles heavy lifting.

181
00:12:35,385 --> 00:12:41,045
Of DevOps mutual TLS for security,
automatic retries with exponential

182
00:12:41,045 --> 00:12:43,415
back off and even circuit breaking.

183
00:12:44,285 --> 00:12:45,995
A circuit breaker is actually critical.

184
00:12:46,085 --> 00:12:52,085
If service A sees that service B is
failing, it trips the circuit and stops

185
00:12:52,085 --> 00:12:54,485
sending request for a cooling off period.

186
00:12:55,325 --> 00:12:59,075
This prevents the death spiral
situation where a failing service

187
00:12:59,105 --> 00:13:03,365
is hit with more and more retries
until it can never recover.

188
00:13:04,220 --> 00:13:08,900
The separation of business logic and
network logic is what allows developers to

189
00:13:08,900 --> 00:13:12,140
move fast without breaking the platform.

190
00:13:12,640 --> 00:13:15,850
Let's talk about observative
observability a bit.

191
00:13:16,350 --> 00:13:21,360
So at billions of requests, you
cannot literally log everything.

192
00:13:21,870 --> 00:13:22,290
If you did.

193
00:13:22,980 --> 00:13:26,550
Your logging bill would be
higher than your compute bill.

194
00:13:27,050 --> 00:13:30,200
You can actually use something
called a tail based tracing.

195
00:13:30,700 --> 00:13:35,050
In traditional ting, you might sample
1% of the request, but if your error

196
00:13:35,050 --> 00:13:40,360
rate is 0.1%, you might miss the
very errors that you're looking for.

197
00:13:40,860 --> 00:13:46,205
With tail based tracing, what you do is
you keep the trace in a temporary buffer

198
00:13:46,985 --> 00:13:49,865
if the request is successful and fast.

199
00:13:50,180 --> 00:13:51,200
You can discard it.

200
00:13:51,800 --> 00:13:56,180
If it returns 500 error or
takes longer than, let's say 200

201
00:13:56,180 --> 00:13:58,850
milliseconds, you save the full trace.

202
00:13:59,570 --> 00:14:04,090
This gives you a hundred percent
visibility into failures, while only

203
00:14:04,090 --> 00:14:06,970
storing a tiny fraction of total data.

204
00:14:07,470 --> 00:14:12,415
High cardinality metrics and cost aware
logging samples are some of the other ways

205
00:14:12,535 --> 00:14:15,355
also to achieve observability at scale.

206
00:14:15,855 --> 00:14:18,360
Predictive autoscaling
is another technique.

207
00:14:19,170 --> 00:14:24,120
Reactive autoscaling, for
example, scales when CPU hits 70%.

208
00:14:24,870 --> 00:14:26,760
It's too slow for extreme scale.

209
00:14:27,090 --> 00:14:30,710
By the time a new Kubernetes cluster
node is ready, the traffic has

210
00:14:30,710 --> 00:14:32,540
already spiked and cost time out.

211
00:14:33,350 --> 00:14:38,895
You can use predict model pre predictive
models here by feeding historical

212
00:14:38,895 --> 00:14:43,694
data of how traffic works in your
system into machine learning models.

213
00:14:44,204 --> 00:14:47,535
You can predict with the high
accuracy, what the traffic will

214
00:14:47,564 --> 00:14:49,574
look like in the next 30 minutes.

215
00:14:50,355 --> 00:14:51,939
You can use this pre.

216
00:14:52,620 --> 00:14:55,890
To pre-provision the capacity
for major events where you

217
00:14:55,890 --> 00:14:57,360
expect traffic to be higher.

218
00:14:57,860 --> 00:15:01,730
This allows you to maintain a trite
headroom, which saves millions in

219
00:15:01,730 --> 00:15:08,480
infrastructure costs while ensuring your
service is scaled up enough to handle the

220
00:15:08,599 --> 00:15:14,329
uptick in the traffic, and you're never
caught off guard by predictable surges.

221
00:15:14,829 --> 00:15:21,099
As we look forward to 2027, the challenges
are evolving In this space, one major

222
00:15:21,099 --> 00:15:24,069
focus is energy aware elasticity.

223
00:15:24,569 --> 00:15:31,020
We are be beginning to see the green
DevOps, where we shift heavy background

224
00:15:31,020 --> 00:15:36,929
processing workloads like data indexing or
ML training, two regions where renewable

225
00:15:36,929 --> 00:15:39,689
energy is currently at a peak production.

226
00:15:40,189 --> 00:15:45,859
We are also attacking extreme tail
latency where we are moving past the P

227
00:15:45,859 --> 00:15:49,849
99 metric and focusing on the P 99.99.

228
00:15:50,269 --> 00:15:55,459
Eliminating those microsecond level
jitters caused by garbage collection

229
00:15:55,489 --> 00:15:59,959
or even kernel interrupts requiring
a level of full stack observability

230
00:16:00,589 --> 00:16:02,934
that reaches down to hardware itself.

231
00:16:03,434 --> 00:16:04,064
Finally.

232
00:16:04,564 --> 00:16:10,924
Cross region rate coordination remains an
active area of research, finding ways to

233
00:16:10,924 --> 00:16:17,974
enforce global limits with zero latency
penalty using edge computing platforms.

234
00:16:18,474 --> 00:16:21,354
So your billion dollar
request bill of rent.

235
00:16:21,939 --> 00:16:27,569
To summarize, scaling at Billions
is a marathon, not a sprint Layer.

236
00:16:27,569 --> 00:16:31,529
Your defenses don't expect
One tool to solve everything.

237
00:16:32,159 --> 00:16:33,959
Optimize this edge.

238
00:16:34,409 --> 00:16:37,499
The fastest request is the one
that never hits your backend.

239
00:16:37,499 --> 00:16:37,559
It.

240
00:16:38,059 --> 00:16:44,719
B algorithmic request coalescing and
smart sharding to reduce work per request.

241
00:16:45,529 --> 00:16:47,539
Bpo, be also proactive.

242
00:16:48,289 --> 00:16:52,099
Move from reactive monitoring
to predictive operations.

243
00:16:52,599 --> 00:16:55,749
These are the patterns that
you can use every day to keep

244
00:16:55,749 --> 00:16:57,489
your platform running smoothly.

245
00:16:58,179 --> 00:17:00,159
Start with your most critical bottleneck.

246
00:17:00,789 --> 00:17:06,879
Apply these patterns systematically, and
you'll find that extreme scale is not

247
00:17:06,879 --> 00:17:09,099
only achievable, but also manageable.

248
00:17:09,599 --> 00:17:11,099
Thank you all for joining me today.

249
00:17:11,459 --> 00:17:15,359
Building at this scale is a
collaborative effort, and I'm

250
00:17:15,359 --> 00:17:19,589
excited to see how you apply these
patterns into your own systems.

251
00:17:20,089 --> 00:17:21,709
Again, I am Hil Koko.

252
00:17:22,159 --> 00:17:26,689
You can find me on LinkedIn or reach out
to me if you have any questions regarding

253
00:17:26,689 --> 00:17:28,489
the architectures we discussed today.

254
00:17:29,359 --> 00:17:29,749
Thank you.

255
00:17:30,249 --> 00:17:30,369
I.

