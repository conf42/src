1
00:00:00,080 --> 00:00:03,090
Hi, Tim Spann here,
senior solutions engineer.

2
00:00:03,780 --> 00:00:07,100
my talk today is on
smart cities unleashed.

3
00:00:07,600 --> 00:00:11,360
So I've worked with a lot of data
companies, worked with lots of

4
00:00:11,360 --> 00:00:15,480
different types of data in motion,
sensors, all types of stuff.

5
00:00:15,529 --> 00:00:19,170
Now I'm working with Snowflake,
NiFi, Iceberg, Polaris, Streamlit,

6
00:00:19,759 --> 00:00:23,360
Casey, some Flink and Kafka,
some other cool stuff in there.

7
00:00:23,860 --> 00:00:27,839
Every week I put out a free newsletter
in a lot of different formats.

8
00:00:28,500 --> 00:00:31,879
Click or scan, get there,
get lots of free content.

9
00:00:32,029 --> 00:00:32,989
Never a charge.

10
00:00:33,869 --> 00:00:35,479
You can go on GitHub and find it.

11
00:00:35,979 --> 00:00:39,719
Pretty much every week unless there's
something going on and they interrupt it.

12
00:00:39,739 --> 00:00:40,989
But, we're pretty consistent.

13
00:00:40,999 --> 00:00:42,379
You gotta look at all the back stuff.

14
00:00:42,879 --> 00:00:45,049
Today we're gonna cover
a couple of things.

15
00:00:45,049 --> 00:00:48,499
We'll do an intro, some overview,
look at different types of data.

16
00:00:48,809 --> 00:00:50,050
Show you a lot of NiFi.

17
00:00:50,754 --> 00:00:55,304
If everything's running, I'm recording
this on a Sunday, we'll see if demo

18
00:00:55,304 --> 00:00:59,414
works, give you some resources so
you can continue on your path to

19
00:00:59,914 --> 00:01:02,064
unlock your data for smart cities.

20
00:01:02,464 --> 00:01:09,094
And you require a team of products
built around a solid data platform.

21
00:01:09,734 --> 00:01:13,964
Things like Python, NiFi,
Iceberg, these things.

22
00:01:14,499 --> 00:01:17,869
Help you solve these complex problems,
whether the data is structured,

23
00:01:17,869 --> 00:01:19,619
semi structured, or unstructured.

24
00:01:20,149 --> 00:01:23,649
We'll find all these formats
when we're working with cities.

25
00:01:24,439 --> 00:01:28,979
Because you've got things like cameras,
you've got documents, you've got,

26
00:01:29,039 --> 00:01:34,439
standard static data, you've got things
like transit data, which is often in

27
00:01:34,439 --> 00:01:41,899
the, GTFS, format, which is a binary
format, Based on Google protocol buffers.

28
00:01:42,139 --> 00:01:46,989
This is used by most, transit
systems out there around the world.

29
00:01:47,679 --> 00:01:52,179
And fortunately they're very consistent
and the mobility data people.

30
00:01:52,699 --> 00:01:54,449
Keep a good track and list of them.

31
00:01:54,529 --> 00:01:58,319
We can actually automate grabbing
all the transit systems in the world.

32
00:01:58,609 --> 00:02:02,149
But people tend to work on
one city for their smart city.

33
00:02:02,679 --> 00:02:06,819
Put together an example architecture
and did a little of the work on this.

34
00:02:06,859 --> 00:02:09,719
We'll see if people are interested
if we want to go further on this.

35
00:02:10,679 --> 00:02:12,269
So we got data from the real world.

36
00:02:12,269 --> 00:02:14,899
I can have NiFi running on edge devices.

37
00:02:15,349 --> 00:02:17,809
Kafka feeds coming in from all over.

38
00:02:18,569 --> 00:02:20,849
you're going to be working
with different providers.

39
00:02:21,629 --> 00:02:25,859
And open source groups and communities,
when you're getting this data, it's going

40
00:02:25,859 --> 00:02:32,879
to be living on, real machines, could be
on moving bicycles, trucks, lots of stuff.

41
00:02:33,209 --> 00:02:38,339
data sources we've looked at recently,
sensors in JSON format, transit data

42
00:02:38,339 --> 00:02:44,189
in GTFS, traffic data in JSON, some
raw data we might have to clean up.

43
00:02:44,654 --> 00:02:47,504
this could be documents
and other things, images.

44
00:02:48,034 --> 00:02:53,504
And fortunately, once we get this into
Snowflake with Cortex AI, we can do a

45
00:02:53,504 --> 00:02:58,494
lot of the, complex processing regardless
if that's documents or whatever.

46
00:02:59,074 --> 00:03:01,744
We've got Kafka dropping into, Snowpipe.

47
00:03:02,244 --> 00:03:03,414
We've got raw data.

48
00:03:03,464 --> 00:03:06,714
We're getting data from our
marketplace, including free weather

49
00:03:06,714 --> 00:03:08,314
data we could use for stuff.

50
00:03:08,634 --> 00:03:13,024
Got a couple quick SnowSite charts
and, Queries there just to show

51
00:03:13,024 --> 00:03:15,624
stuff, but we could do it in
Jupyter Notebooks or whatever.

52
00:03:16,104 --> 00:03:20,744
If we want to, we could store everything
in just an S3 data lake with Iceberg.

53
00:03:21,584 --> 00:03:23,794
Still have it part of this big puzzle.

54
00:03:24,564 --> 00:03:27,304
Again, we're talking about
semi structured data.

55
00:03:27,804 --> 00:03:31,575
It could be things like, from OpenAQ,
they've got air quality data, which

56
00:03:31,575 --> 00:03:33,425
is important, depending on your city.

57
00:03:33,925 --> 00:03:37,595
we've got data about location,
time, different sensors.

58
00:03:37,635 --> 00:03:39,775
Sometimes this data is
coming from other systems.

59
00:03:39,775 --> 00:03:43,525
Could be Avro, Parquet,
ORC, all Apache formats.

60
00:03:44,085 --> 00:03:46,184
JSON or XML, very common.

61
00:03:46,784 --> 00:03:49,694
I'm getting some RSS feeds,
which is a type of XML.

62
00:03:50,194 --> 00:03:50,964
Hierarchical data.

63
00:03:51,074 --> 00:03:52,444
So this is complex.

64
00:03:52,444 --> 00:03:53,964
We might drop it in RAW.

65
00:03:54,484 --> 00:03:58,454
But to query it, it's much better
in a more traditional table format.

66
00:03:58,494 --> 00:03:59,884
We could do some transformation.

67
00:04:00,304 --> 00:04:03,194
Maybe getting logs, see what's
going on in other systems.

68
00:04:03,194 --> 00:04:04,994
Key value data is everywhere.

69
00:04:05,494 --> 00:04:09,634
Now in the new world of AI,
we've got also unstructured data.

70
00:04:10,154 --> 00:04:11,864
And this is a ton of different formats.

71
00:04:11,894 --> 00:04:16,494
Lots of different text,
documents, PDF, spreadsheets.

72
00:04:17,184 --> 00:04:18,354
there's just thousands.

73
00:04:18,854 --> 00:04:19,994
Some more fun ones.

74
00:04:20,224 --> 00:04:25,714
Images, we grab these off camera, same
with video, audio, again, a lot of

75
00:04:25,714 --> 00:04:29,234
things happen for smart cities where
you may be tracking what's going on.

76
00:04:29,634 --> 00:04:31,464
Seeing if there's disturbances.

77
00:04:31,994 --> 00:04:36,184
Could also be having to take email
from different sources and process

78
00:04:36,184 --> 00:04:39,304
that, so that's available so I
know what's going on in the city

79
00:04:39,304 --> 00:04:41,074
with regulations and other stuff.

80
00:04:41,454 --> 00:04:44,854
Fortunately with Snowflake, we
put all these weird formats and,

81
00:04:44,864 --> 00:04:47,864
variant type, and be able to use it.

82
00:04:48,364 --> 00:04:50,544
Again, grabbing city cameras.

83
00:04:51,044 --> 00:04:52,494
huge source of data.

84
00:04:53,019 --> 00:04:56,169
And a lot of value in there, especially
when we start doing, analytics

85
00:04:56,169 --> 00:04:57,469
on it and see what's in there.

86
00:04:57,859 --> 00:05:00,259
It could help us know,
are the buses on time?

87
00:05:00,599 --> 00:05:01,839
what's slowing them down?

88
00:05:01,839 --> 00:05:03,189
Is it people double parking?

89
00:05:03,479 --> 00:05:05,669
Those sort of analytics
we could start doing.

90
00:05:06,429 --> 00:05:10,779
Now, the most fun and the most
valuable for the future is once we

91
00:05:10,789 --> 00:05:12,719
get this into a structured format.

92
00:05:13,089 --> 00:05:17,079
Whether that's a snowflake
table, or snowflake hybrid

93
00:05:17,079 --> 00:05:18,469
tables, which are pretty cool.

94
00:05:18,969 --> 00:05:21,839
Let's just do, transactional
kind of stuff on there too.

95
00:05:22,269 --> 00:05:25,569
Iceberg table is a great way to
share with different, systems,

96
00:05:25,609 --> 00:05:27,029
companies, what have you.

97
00:05:27,689 --> 00:05:31,309
The old style relational tables
like, all those old vendors.

98
00:05:31,829 --> 00:05:35,969
Of course, new relational stuff like
the, newest versions of PostgreSQL

99
00:05:35,969 --> 00:05:38,019
and all their 10, 000 variants.

100
00:05:38,339 --> 00:05:43,349
And of course, structured files like
comma separated values or tab separated.

101
00:05:43,699 --> 00:05:45,079
There's a lot of variations there.

102
00:05:45,089 --> 00:05:48,509
Some are Closer to semi structured
than fully structured, but

103
00:05:48,509 --> 00:05:49,979
it depends on what you have.

104
00:05:50,769 --> 00:05:55,009
with Iceberg, we can start appending
data as it comes in, whether that's

105
00:05:55,009 --> 00:06:00,839
through Snowpark with Python or
Java or Scala, or through NiFi with

106
00:06:00,839 --> 00:06:05,199
the Puticeberg table, depending
on which version of NiFi you have.

107
00:06:05,719 --> 00:06:07,119
Get that data in there.

108
00:06:08,069 --> 00:06:11,059
Just talk real quick about
the Snowflake AI Data Cloud.

109
00:06:11,739 --> 00:06:14,989
in the middle, so all these different
workloads, it's not just the

110
00:06:14,989 --> 00:06:17,239
data warehouse, data lake stuff.

111
00:06:17,329 --> 00:06:24,019
Unistore is cool, we can do, like I said,
that hybrid approach of analytical and

112
00:06:24,139 --> 00:06:26,319
relational style data in the same table.

113
00:06:26,559 --> 00:06:30,809
So we can do really fast stuff, get that
data available for reports right away.

114
00:06:31,599 --> 00:06:35,139
And, like always, data
engineering right in the center.

115
00:06:35,584 --> 00:06:38,074
Python, Java, busting out that code.

116
00:06:38,354 --> 00:06:41,114
Maybe AI helped you write
it or made your code better.

117
00:06:41,444 --> 00:06:43,054
Still need that data engineering.

118
00:06:43,194 --> 00:06:45,274
Adding, NiFi, OpenFlow there.

119
00:06:45,934 --> 00:06:47,584
Apps, this is really cool.

120
00:06:48,024 --> 00:06:52,714
With, Snowflake, I could write some
apps in Python and be really solid

121
00:06:52,714 --> 00:06:54,224
and share them with other companies.

122
00:06:54,724 --> 00:06:59,184
And of course, AI and ML are
just solving a lot of problems.

123
00:07:00,004 --> 00:07:05,074
And what's cool is I build these apps,
if they become useful, I could just put

124
00:07:05,074 --> 00:07:09,304
them on the marketplace and either give
them away, as a sense of community,

125
00:07:09,404 --> 00:07:11,414
or maybe sell it, make some money.

126
00:07:11,964 --> 00:07:17,864
And everything is, governed, so even when
I'm collaborating, I don't have to worry

127
00:07:17,864 --> 00:07:19,744
about losing data or something happen.

128
00:07:20,124 --> 00:07:22,004
You control all that, very cool.

129
00:07:22,974 --> 00:07:25,344
Architecture itself is pretty amazing.

130
00:07:25,784 --> 00:07:28,094
with Snow Grid, could share this too.

131
00:07:28,594 --> 00:07:31,504
know, different clouds and different
cloud regions around the world.

132
00:07:32,079 --> 00:07:35,639
Great way to be able to collaborate
and still lock down data if

133
00:07:35,639 --> 00:07:37,069
it's required by governments.

134
00:07:37,489 --> 00:07:40,609
All the cloud services to make
sure our metadata is managed.

135
00:07:40,779 --> 00:07:46,949
We have full administration, optimization,
security, data catalog, all that there.

136
00:07:47,269 --> 00:07:51,869
And as much of it that can be automated
is, so you don't have to worry about that.

137
00:07:52,499 --> 00:07:56,699
And then we've got AI from
Cortex as the next layer.

138
00:07:57,039 --> 00:08:01,839
Where you could do work with chat APIs or
easily got model registries in their ton

139
00:08:01,839 --> 00:08:08,069
of models, but they're Validated so, this
is good and we got security everywhere So

140
00:08:08,239 --> 00:08:12,169
none of these models are messing up your
stuff and the studio to make it really

141
00:08:12,169 --> 00:08:15,569
easy You don't have to be a data scientist
to be able to work with these models

142
00:08:16,399 --> 00:08:20,919
And you need to be able to run apps, so
we've got Elastic Compute, you can do

143
00:08:20,929 --> 00:08:24,779
serverless things, you can do things on
demand or wherever you need it, whether

144
00:08:24,779 --> 00:08:30,599
it's in a CPU or GPU, or if it's a more
complex app, we support running containers

145
00:08:30,879 --> 00:08:34,759
in a controlled secure environment, so
you can deploy your apps there, again

146
00:08:34,759 --> 00:08:39,429
support for the major languages, you can
do almost everything in SQL, but of course

147
00:08:39,459 --> 00:08:41,359
Python and Java and Scala are there.

148
00:08:41,854 --> 00:08:46,144
And as much storage as you need
for all these types of data that

149
00:08:46,154 --> 00:08:52,354
interrupts, whether that's in the
cloud, managed, unmanaged, or even

150
00:08:52,354 --> 00:08:55,774
touching your on premise if you
want to set up those connections.

151
00:08:56,274 --> 00:08:59,551
And next up we talk about patching NiFi.

152
00:08:59,551 --> 00:09:06,564
NiFi is an open source project that's been
taken in house by Snowflake to improve

153
00:09:06,854 --> 00:09:08,764
and make enterprise secure and ready.

154
00:09:09,649 --> 00:09:12,749
If you haven't seen NiFi before, you
might have seen some of my other talks.

155
00:09:12,749 --> 00:09:13,669
I really NiFi.

156
00:09:13,669 --> 00:09:19,439
It makes things a lot easier for doing
some of the mundane data engineering

157
00:09:19,439 --> 00:09:23,409
and data tasks that, you might
not want to have to do every time.

158
00:09:23,909 --> 00:09:27,739
But it's got all the things you
need for enterprise wide systems.

159
00:09:28,039 --> 00:09:29,599
Guarantee you get your data.

160
00:09:30,229 --> 00:09:33,449
There's buffering with MAC
pressure and you can control that

161
00:09:33,529 --> 00:09:35,059
so things don't get shut down.

162
00:09:35,059 --> 00:09:39,539
If you need to slow things down to make
it more controlled for downstream systems

163
00:09:39,539 --> 00:09:41,399
or to just save money, you can do that.

164
00:09:41,719 --> 00:09:44,919
You can prioritize queuing
of data through the system.

165
00:09:45,349 --> 00:09:53,369
You can control per data flow in a data
engineering pipeline all the quality

166
00:09:53,369 --> 00:09:55,119
of service things you need to control.

167
00:09:55,119 --> 00:09:55,659
There's latency.

168
00:09:56,159 --> 00:09:59,589
how much loss you want, throughput,
error handling, those kind of things.

169
00:10:00,039 --> 00:10:03,149
But we let you know everything
that happened along the way

170
00:10:03,179 --> 00:10:04,599
with full data provenance.

171
00:10:04,969 --> 00:10:09,209
This lineage of your pipeline
lets you know, each individual

172
00:10:09,219 --> 00:10:11,289
row of data or record.

173
00:10:11,769 --> 00:10:14,479
Or a flow file, we call it,
that comes through the system,

174
00:10:14,629 --> 00:10:18,599
everything that happened to
it, size, all kind of metadata.

175
00:10:18,739 --> 00:10:19,539
Very cool.

176
00:10:20,189 --> 00:10:22,869
And we can push data,
pull data, schedule data.

177
00:10:23,479 --> 00:10:25,979
Any kind of process you
need to do is in here.

178
00:10:26,439 --> 00:10:28,899
Including, if it's not there,
you could write your own in

179
00:10:28,899 --> 00:10:30,529
Java or Python and deploy it.

180
00:10:31,484 --> 00:10:35,164
There's hundreds of different processing
abilities there, whether that's

181
00:10:35,644 --> 00:10:40,074
opening things, closing, storing,
conversion, transformation, enrichment.

182
00:10:40,494 --> 00:10:42,154
You'll see it's all visual.

183
00:10:42,274 --> 00:10:43,094
Very nice.

184
00:10:43,554 --> 00:10:47,989
very easy to, start off with, templates
that are available on the internet.

185
00:10:48,369 --> 00:10:51,099
Or through your own
company and then just go.

186
00:10:51,779 --> 00:10:55,609
Lots of different security, whatever
one makes sense for your organization.

187
00:10:56,039 --> 00:10:57,469
Designed to be extended.

188
00:10:57,949 --> 00:10:59,459
Clusters very easily.

189
00:10:59,459 --> 00:11:00,859
Scales very linearly.

190
00:11:01,239 --> 00:11:03,249
Supports full version controls.

191
00:11:03,249 --> 00:11:05,379
You're doing real development here.

192
00:11:05,439 --> 00:11:08,349
It's not some throwaway
drag and drop tool.

193
00:11:08,849 --> 00:11:12,879
What's even better in today's
world, I can move all that binary

194
00:11:12,899 --> 00:11:17,659
unstructured image data plus tabular
data and understand it natively.

195
00:11:18,419 --> 00:11:22,189
This tool doesn't just
work with, JSON file.

196
00:11:22,199 --> 00:11:24,299
It's not a stretch to
push images through here.

197
00:11:24,299 --> 00:11:25,729
We've been doing it forever.

198
00:11:26,299 --> 00:11:27,989
Zip files of images.

199
00:11:28,559 --> 00:11:29,519
Zip files of data.

200
00:11:29,519 --> 00:11:30,889
Whatever you want to put through there.

201
00:11:30,919 --> 00:11:31,529
Enrich it.

202
00:11:32,394 --> 00:11:34,754
You do very cool, simple event processing.

203
00:11:34,754 --> 00:11:37,454
This isn't Flink, but
I could do a lot here.

204
00:11:37,664 --> 00:11:41,064
Very great for routing to multiple
places, depending on what it is.

205
00:11:41,344 --> 00:11:46,804
I need to send data to seven different
places, depending on certain scenarios.

206
00:11:47,024 --> 00:11:48,334
No problem, no code.

207
00:11:49,094 --> 00:11:52,774
feed this right into some kind of
central messaging system or just

208
00:11:52,774 --> 00:11:54,974
land it in your AI data cloud.

209
00:11:55,494 --> 00:11:56,984
All the modern protocols are there.

210
00:11:57,014 --> 00:11:59,584
Kafka, Pulsar, all those fun things.

211
00:12:00,264 --> 00:12:00,824
With the new 2.

212
00:12:00,824 --> 00:12:06,174
0 that's recent, it is designed
for real time integration and AI.

213
00:12:06,814 --> 00:12:12,864
Python is been improved, so you can
very easily add Python components.

214
00:12:12,864 --> 00:12:14,074
I'll show you a couple I built.

215
00:12:14,824 --> 00:12:16,284
Everything's parameterized.

216
00:12:16,444 --> 00:12:21,404
We work with the latest JDK, so
it's faster, more durable, all

217
00:12:21,404 --> 00:12:22,864
those new features in there.

218
00:12:23,264 --> 00:12:24,984
Got a rules engine to help you.

219
00:12:25,574 --> 00:12:28,494
Things for all the different
clouds, some new Azure ones.

220
00:12:28,934 --> 00:12:33,164
And work with a lot of those SAS systems
you might like to work with, like Zendesk

221
00:12:33,164 --> 00:12:35,934
and Slack and Salesforce and you name it.

222
00:12:36,234 --> 00:12:40,834
I can use existing data table as
my schema to control data loading.

223
00:12:41,164 --> 00:12:45,714
Or I can work with Amazon Glue or
Confluence, ton of different registry.

224
00:12:46,214 --> 00:12:48,754
Support for open telemetry
if you're using that.

225
00:12:49,644 --> 00:12:53,824
Architecture is pretty standard
for How these sort of, clustered

226
00:12:53,824 --> 00:12:58,714
systems work, but very scalable,
very survivable, very solid.

227
00:12:59,274 --> 00:13:04,784
I've done, real time demos with this
for eight years and have people in

228
00:13:04,784 --> 00:13:07,574
production that are mission critical.

229
00:13:08,254 --> 00:13:13,094
If you haven't heard, the original NiFi
came out of the NSA and they use this

230
00:13:13,094 --> 00:13:19,044
for extremely mission critical systems
in live environments out in the field.

231
00:13:19,714 --> 00:13:22,784
Providence, we mentioned that
Lineage, we track everything.

232
00:13:22,844 --> 00:13:26,204
You can visually look at it,
you can see all the metadata.

233
00:13:26,404 --> 00:13:30,834
This is really helpful, not just for
debugging, but to see if data got lost,

234
00:13:30,844 --> 00:13:32,424
what's going on in the real world.

235
00:13:33,164 --> 00:13:35,004
Lots of reasons why you want to know.

236
00:13:35,344 --> 00:13:38,794
Again, we mentioned all that unstructured
data, and the ability to do a lot

237
00:13:38,794 --> 00:13:44,104
of things with that, identify it,
chunk it, store it places, send it.

238
00:13:44,684 --> 00:13:50,864
Dissect it, convert from HTML to
text, extract pieces, uncompress,

239
00:13:50,894 --> 00:13:53,564
compress, all that kind of fun stuff.

240
00:13:53,564 --> 00:13:59,264
Coal, different ML, whether that's
through REST or through a native API

241
00:13:59,264 --> 00:14:01,014
or through Python, whatever it is.

242
00:14:01,514 --> 00:14:06,544
For all that more structured data out
there, the ability to work with all

243
00:14:06,544 --> 00:14:11,094
kinds of record oriented data very
easily, including pulling apart logs,

244
00:14:11,094 --> 00:14:16,394
syslogs, working with all kinds of
standard data out there, being able to

245
00:14:16,394 --> 00:14:19,114
do that without knowing all the fields.

246
00:14:19,474 --> 00:14:23,184
So fields change, versions
of things change, you don't

247
00:14:23,184 --> 00:14:24,444
have to change your code.

248
00:14:24,544 --> 00:14:25,524
Pretty amazing.

249
00:14:26,519 --> 00:14:27,769
One record at a time.

250
00:14:27,819 --> 00:14:29,039
Batches of records.

251
00:14:29,779 --> 00:14:30,669
Micro batches.

252
00:14:30,669 --> 00:14:32,899
Whatever makes sense for your use case.

253
00:14:33,559 --> 00:14:39,049
Like I mentioned, I've extended it with
some, Python, processors of my own.

254
00:14:39,569 --> 00:14:44,459
This one pulls out your company list for
any kind of data you push through it.

255
00:14:44,689 --> 00:14:46,179
So say you've got a document.

256
00:14:46,299 --> 00:14:48,639
You want to see what kind
of companies are referenced.

257
00:14:49,429 --> 00:14:50,619
This one does it for you.

258
00:14:50,789 --> 00:14:52,179
Certainly you can write your own.

259
00:14:52,579 --> 00:14:56,519
This is just using some standard,
spacey PyTorch and NLP to do it.

260
00:14:57,169 --> 00:14:59,249
You could use other AI models if you want.

261
00:14:59,719 --> 00:15:03,199
Got one that'll caption your images
that come through the pipeline.

262
00:15:03,709 --> 00:15:07,139
Using Salesforce's pretty
awesome Blip image captioning.

263
00:15:07,719 --> 00:15:11,469
They've been improving this one
recently so I probably gotta take

264
00:15:11,469 --> 00:15:14,369
a look at this one again, see if
there's a better model to pick.

265
00:15:14,869 --> 00:15:17,799
Standard image classification
with ResNet 50.

266
00:15:18,299 --> 00:15:19,059
Pretty easy one.

267
00:15:19,089 --> 00:15:20,779
This one comes in handy a lot.

268
00:15:21,189 --> 00:15:26,099
Using the, Nominitum library to
get your addresses into LatLng.

269
00:15:26,199 --> 00:15:30,839
This is helpful with all the GeoAbilities
and Snowflake and other systems.

270
00:15:31,719 --> 00:15:35,899
when I'm working with cities, you need
to know where you are in the city.

271
00:15:36,349 --> 00:15:39,359
So sometimes you have a LatLng
in data, sometimes you don't.

272
00:15:40,059 --> 00:15:42,369
This is one of the libraries
to help you with that.

273
00:15:42,489 --> 00:15:43,739
You could also go the other way.

274
00:15:43,739 --> 00:15:45,289
We could also map IPs.

275
00:15:45,299 --> 00:15:49,859
Again, you got Smart City, you
often have different Wi Fi, or

276
00:15:49,879 --> 00:15:53,869
different networking protocols that
we can map to where things are.

277
00:15:54,509 --> 00:15:57,699
That is an important type
of data we need with a city.

278
00:15:58,199 --> 00:15:59,299
is this enough data?

279
00:15:59,819 --> 00:16:01,869
Let's see if we can get into the demo.

280
00:16:02,029 --> 00:16:05,919
I'm hoping, I didn't talk so long
that I've timed everything out.

281
00:16:06,474 --> 00:16:11,264
that has happened before, maybe a little
long winded here, sorry about that.

282
00:16:11,974 --> 00:16:16,564
I've been able to load some of my
data into a snowflake table, and

283
00:16:16,564 --> 00:16:20,044
there's a lot of different ways to
do that, however you like to do it,

284
00:16:20,484 --> 00:16:22,064
there is a connector there for you.

285
00:16:22,374 --> 00:16:29,384
From ODBC, JDBC, Python, JARP, everything,
SPRING, so we got some data in, this

286
00:16:29,384 --> 00:16:35,864
is from the transcom system, that is
a Regulatory body that handles New

287
00:16:35,864 --> 00:16:37,894
Jersey, New York, and Connecticut.

288
00:16:38,294 --> 00:16:42,454
Three big metropolitan areas
over here and a lot of data.

289
00:16:42,754 --> 00:16:47,064
This says things like what's going on
with utility work, at what location.

290
00:16:47,424 --> 00:16:51,084
Again, important with your smart
city to know when, if you want to

291
00:16:51,144 --> 00:16:54,724
send a robot somewhere, don't send
it where there's already traffic

292
00:16:54,724 --> 00:16:59,524
or there's you know, a road down
and what's nice with a snowflake.

293
00:16:59,614 --> 00:17:02,534
I can use any of the Python
libraries, but we got one with

294
00:17:02,914 --> 00:17:07,264
Streamlit here that could show me
where there's some problematic spots.

295
00:17:07,584 --> 00:17:12,354
And as you can see, there's a lot
going on in Manhattan, which is not

296
00:17:12,354 --> 00:17:17,174
surprising, but just to give you a
quick look at that, let's see if we've

297
00:17:17,174 --> 00:17:19,734
got our, NiFi is still running here.

298
00:17:20,214 --> 00:17:25,104
we are very security conscious so
things will time out to keep us secure.

299
00:17:25,944 --> 00:17:29,944
so I've got a transit feed from
that transcom system I mentioned.

300
00:17:30,594 --> 00:17:33,924
Now I don't have this running
continuously because this is just a demo.

301
00:17:34,294 --> 00:17:37,834
In your smart city, you may want
this running pretty frequently.

302
00:17:38,404 --> 00:17:42,024
down to the minute, depending
again, we mentioned that trans

303
00:17:42,034 --> 00:17:43,634
comp, so we're going to run it once.

304
00:17:44,154 --> 00:17:46,014
This is the NiFi UI.

305
00:17:46,604 --> 00:17:48,714
This shows me live things running.

306
00:17:49,094 --> 00:17:54,714
Here is how I get, an HTTP feed,
which is the transcom data here.

307
00:17:54,714 --> 00:18:00,094
I'm going to take that XML data, that RSS
feed and convert it into Jason format.

308
00:18:00,094 --> 00:18:00,974
I like better.

309
00:18:01,494 --> 00:18:04,604
We can look at that lineage
and see what just came by.

310
00:18:05,369 --> 00:18:08,559
And we can look at the details
of that and see all the metadata.

311
00:18:09,129 --> 00:18:11,949
You can see what we called
and how we called it.

312
00:18:12,869 --> 00:18:16,559
And then we can actually look at the
content that came through as well.

313
00:18:16,869 --> 00:18:19,959
And as you can see we
converted it into JSON.

314
00:18:20,379 --> 00:18:23,279
And it's got all this data
about what's going on.

315
00:18:23,889 --> 00:18:27,169
There, just to give you an idea
when we go through this system,

316
00:18:27,859 --> 00:18:28,929
we could see what's happening.

317
00:18:28,929 --> 00:18:33,729
I'm splitting out the individual
records and here, I'm just making

318
00:18:33,729 --> 00:18:38,939
sure it's a clean format and I'm going
to add a timestamp and a unique ID.

319
00:18:39,589 --> 00:18:45,179
If I generate unique IDs for me, and
then I'm also going to add the lat long.

320
00:18:45,429 --> 00:18:47,599
There's a point in there.

321
00:18:47,599 --> 00:18:51,629
If you looked at the, Those records
closely when they were coming through

322
00:18:52,229 --> 00:18:58,789
and it's in a nice format But,
really, everybody else uses lat long.

323
00:18:59,389 --> 00:19:04,769
As you saw in that snowflake,
chart here, I wanted lat and long.

324
00:19:04,949 --> 00:19:11,349
I just parse that out here and do that
quick, very simple, fast enrichment as

325
00:19:11,349 --> 00:19:13,119
the data is traveling through the system.

326
00:19:13,949 --> 00:19:19,539
And then I'm sending it into another
component to, break it down further.

327
00:19:19,979 --> 00:19:22,189
And send that into Snowflake.

328
00:19:22,599 --> 00:19:26,189
Now I don't know if I still
have my VPN connection open.

329
00:19:26,509 --> 00:19:30,829
Snowflake is perhaps the most
secure data platform I've ever seen.

330
00:19:31,089 --> 00:19:32,259
And I've worked with a lot.

331
00:19:32,689 --> 00:19:35,489
So it does have a lot of
layers of security there.

332
00:19:35,899 --> 00:19:38,719
So sometimes that
requires some extra work.

333
00:19:39,249 --> 00:19:41,339
But, as we see here, it comes in.

334
00:19:41,439 --> 00:19:42,669
We keep getting more.

335
00:19:43,079 --> 00:19:45,579
I can bring in another window over here.

336
00:19:45,579 --> 00:19:50,579
We can do a query, see what's going
on with our Data platform here

337
00:19:51,469 --> 00:19:52,989
and we got a bunch of data here.

338
00:19:52,989 --> 00:19:59,029
So I'm just putting this in a database
one of the ones I have here for Just a

339
00:19:59,029 --> 00:20:04,589
demo one because this is just a demo you
could see the how we created the table

340
00:20:04,689 --> 00:20:07,919
pretty simple we can see what's going on.

341
00:20:08,429 --> 00:20:13,189
Nothing, too complex, but we can see,
we're getting, more records are coming in.

342
00:20:13,809 --> 00:20:16,809
And it's pretty
straightforward system here.

343
00:20:17,099 --> 00:20:19,359
one thing I didn't show
you is the, descriptions.

344
00:20:19,359 --> 00:20:20,459
You get a longer one.

345
00:20:20,859 --> 00:20:25,079
Again, I can apply, use
Cortex AI in a query.

346
00:20:25,519 --> 00:20:29,039
And, build this up for RAG,
or ask questions on this.

347
00:20:29,714 --> 00:20:33,364
what's going on with, the
Performing Arts Center and be able

348
00:20:33,364 --> 00:20:35,284
to get those questions answered.

349
00:20:35,764 --> 00:20:36,804
it's pretty nice here.

350
00:20:36,804 --> 00:20:41,714
I can create a bunch of stuff just right
from here and just query it if we need to.

351
00:20:42,094 --> 00:20:46,074
But let's see what's going on with my
run, trying to see if this timed out.

352
00:20:46,204 --> 00:20:47,394
It did not time out.

353
00:20:47,404 --> 00:20:52,134
So we're loading more data in there,
but that's just one type of system

354
00:20:52,174 --> 00:20:54,514
that makes sense to be reading.

355
00:20:54,784 --> 00:20:56,884
We also grab things from.

356
00:20:57,699 --> 00:21:01,399
Other sources, transit
data, this one's Ireland.

357
00:21:01,839 --> 00:21:04,059
I did some work, for the Irish Rail there.

358
00:21:04,529 --> 00:21:09,119
And this is pulling in a 80.

359
00:21:09,189 --> 00:21:13,359
If we look here, I kept a copy
just in case I needed to reload

360
00:21:13,359 --> 00:21:14,989
it and their system was down.

361
00:21:15,499 --> 00:21:17,319
And I did this a couple days ago.

362
00:21:17,329 --> 00:21:20,419
If we look at this is a very large file.

363
00:21:20,754 --> 00:21:26,414
That contains a bunch of zip files,
that we uncompress and unpack.

364
00:21:27,224 --> 00:21:29,554
See if we still have our, things here.

365
00:21:29,924 --> 00:21:32,654
And you can see all
those files we pull out.

366
00:21:33,464 --> 00:21:35,404
And then I split them up.

367
00:21:35,954 --> 00:21:37,674
assign a primary key.

368
00:21:38,074 --> 00:21:40,624
Then I can update a lookup table here.

369
00:21:41,114 --> 00:21:44,134
So the first time I
load it, it's automated.

370
00:21:44,214 --> 00:21:48,624
There's no hard code other than,
they all have different primary keys.

371
00:21:48,644 --> 00:21:52,204
So I got to set those, some
places you don't need to.

372
00:21:53,109 --> 00:21:54,069
pretty straightforward.

373
00:21:54,069 --> 00:21:58,719
Again, I could pull things out of Kafka,
don't have to know anything specific.

374
00:21:59,519 --> 00:22:04,139
And right here, I'll just say, oh,
it's a JSON file, and I'm going

375
00:22:04,139 --> 00:22:06,829
to insert it into, say, Oracle.

376
00:22:07,679 --> 00:22:09,789
And it's going to be an insert.

377
00:22:10,589 --> 00:22:11,569
very easy to do that.

378
00:22:11,569 --> 00:22:13,759
I don't have to know all
the fields or what have you.

379
00:22:14,199 --> 00:22:15,339
Pretty straightforward.

380
00:22:15,799 --> 00:22:17,839
not going to run through
a million things here.

381
00:22:18,119 --> 00:22:20,439
But let's, Get back to the slides.

382
00:22:21,129 --> 00:22:22,569
We don't need all these running.

383
00:22:23,319 --> 00:22:24,809
Let's get to our slides.

384
00:22:25,309 --> 00:22:26,399
And we'll get back to it.

385
00:22:26,409 --> 00:22:30,429
Hopefully that was enough of a demo
there to give you a little feeling

386
00:22:30,429 --> 00:22:35,449
of what we can do with the different
data around smart cities and do

387
00:22:35,449 --> 00:22:37,649
that very quickly and agilely.

388
00:22:37,709 --> 00:22:43,109
A tool like NiFi lets me build all these
data pipelines without having to test.

389
00:22:43,529 --> 00:22:47,669
and manually run and deploy things,
check the code, very easy in

390
00:22:47,669 --> 00:22:50,389
that UI, very easy to run this.

391
00:22:50,389 --> 00:22:55,059
I've got links to a number of articles
I've written around different data

392
00:22:55,069 --> 00:23:00,479
related to, Smart Cities data,
whether that's things that, real time

393
00:23:00,499 --> 00:23:08,039
buses, trains, planes, air quality,
street cameras, what have you.

394
00:23:08,619 --> 00:23:12,899
And what's cool is, from Brazil,
from Boston, from Halifax, all

395
00:23:12,899 --> 00:23:14,449
over the world, same stuff.

396
00:23:14,719 --> 00:23:19,679
You want to try NiFi on your own, you
can just download it, it is a Java app.

397
00:23:19,699 --> 00:23:21,909
Or you can run it easier in Docker.

398
00:23:22,439 --> 00:23:25,339
You have to have a login
and it always uses SSL.

399
00:23:25,729 --> 00:23:28,229
We care about security, very easy.

400
00:23:28,729 --> 00:23:31,639
Let's automate everything,
data everywhere.

401
00:23:32,269 --> 00:23:35,899
hope you enjoyed, I'll be
doing some more talks with Comp

402
00:23:35,899 --> 00:23:37,729
42, these guys are awesome.

403
00:23:38,609 --> 00:23:40,159
Enjoy the rest of the talks, thank you.

