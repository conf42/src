1
00:00:00,069 --> 00:00:00,699
Good morning, everyone.

2
00:00:01,199 --> 00:00:04,139
it's a real pleasure today to be
here today, with over 18 years of

3
00:00:04,139 --> 00:00:05,739
experience in data engineering field.

4
00:00:05,739 --> 00:00:07,970
I have seen the evolution
of data systems in action.

5
00:00:08,399 --> 00:00:12,839
And today I'm excited to share how AI
is revolutionizing the way we build

6
00:00:12,859 --> 00:00:16,259
the systems, the way we build the data
pipelines, and the way we move the

7
00:00:17,229 --> 00:00:19,129
data from one place to another place.

8
00:00:19,949 --> 00:00:21,099
Think of it as a journey.

9
00:00:21,339 --> 00:00:21,669
Okay.

10
00:00:21,739 --> 00:00:25,849
I'll take you through the evolution
of the data engineering from, early

11
00:00:25,850 --> 00:00:27,710
days of my, moving, like, simple job.

12
00:00:27,730 --> 00:00:31,820
data with basic data cleansing with
basic workflows, to today's intelligent

13
00:00:31,840 --> 00:00:33,550
automated way of moving the data.

14
00:00:34,270 --> 00:00:38,220
along the way, I'll share some stories
of both challenges and breakthroughs and

15
00:00:38,230 --> 00:00:42,950
how we mitigate, we can mitigate those
challenges, with, today's AI world, see

16
00:00:43,150 --> 00:00:44,460
the current state of data engineering.

17
00:00:44,490 --> 00:00:44,740
Okay.

18
00:00:44,860 --> 00:00:47,460
Let me start by sharing a
little bit of my own journey.

19
00:00:47,540 --> 00:00:52,060
when I began my career, a data pipeline
often means moving a few data files

20
00:00:52,080 --> 00:00:53,670
from one system to another system.

21
00:00:54,150 --> 00:00:58,250
maybe cleaning them, and then running
a basic report, for, for leaders or

22
00:00:58,280 --> 00:01:02,170
product managers and various users.

23
00:01:02,390 --> 00:01:05,930
if it is related to operational
reports, fast forward to today, the

24
00:01:05,930 --> 00:01:07,550
landscape has completely changed.

25
00:01:07,920 --> 00:01:14,020
The data we deal with is like massive,
dynamic, like frequent, increasingly

26
00:01:14,340 --> 00:01:17,680
unstructured, because of the way it
evolved and from where it evolved.

27
00:01:18,320 --> 00:01:20,670
it's not enough to just move
the data around anymore.

28
00:01:20,740 --> 00:01:25,310
We need a system that can not only handle
this complexity, but evolve with it.

29
00:01:25,950 --> 00:01:29,800
Today's discussion will revolve
around how AI is playing a pivotal

30
00:01:29,810 --> 00:01:32,070
role in solving these problems.

31
00:01:32,160 --> 00:01:35,730
I'll walk you through how AI is
reshaping the data pipelines, making them

32
00:01:35,730 --> 00:01:38,720
smarter, adaptable, and more efficient.

33
00:01:39,220 --> 00:01:40,420
What is a data pipeline?

34
00:01:40,920 --> 00:01:44,890
Before we dive into AI, let's take a
step back and make sure we are all on

35
00:01:44,890 --> 00:01:48,830
the same page about what we're talking
and what's basic data pipelines.

36
00:01:49,360 --> 00:01:50,520
Picture a traditional data pipeline.

37
00:01:50,820 --> 00:01:55,280
like a factory line, okay, or a pipeline
where data coming from various sources,

38
00:01:55,650 --> 00:02:00,160
it's getting processed and then ending up
in a warehouse or a dashboard for analysis

39
00:02:00,570 --> 00:02:05,780
or for a basic data set that can be done
like deeper insights, by data scientists

40
00:02:05,820 --> 00:02:07,310
or business intelligence engineers.

41
00:02:07,810 --> 00:02:08,040
users.

42
00:02:08,750 --> 00:02:14,540
But as data grows larger and more complex
and more varied, this factory line needs

43
00:02:14,540 --> 00:02:16,800
to become like more sophisticated, okay?

44
00:02:16,850 --> 00:02:20,350
The old systems are not like designed
to handle this complexity, and

45
00:02:20,350 --> 00:02:21,640
that's where the challenge lies.

46
00:02:22,140 --> 00:02:25,850
The struggles, What we see in basically
in these traditional pipelines are like,

47
00:02:26,160 --> 00:02:28,180
let me paint a picture of like early days.

48
00:02:28,240 --> 00:02:31,600
I remember like late night was
spending chasing down issues in the

49
00:02:31,600 --> 00:02:35,800
pipeline and debugging and finding,
the basic issues like, okay, data

50
00:02:35,840 --> 00:02:40,100
type, not mismatching, like one, one
field having a value, which is not

51
00:02:40,170 --> 00:02:43,740
in line with like the way we designed
the schema and everything goes wrong.

52
00:02:44,130 --> 00:02:47,550
And it would take sometimes hours
to debug and find and fix it.

53
00:02:47,580 --> 00:02:50,940
maybe the data source might go
offline or transformation step would

54
00:02:50,940 --> 00:02:51,950
fail just because of debugging.

55
00:02:51,970 --> 00:02:53,380
Basic data type mismatch.

56
00:02:53,800 --> 00:02:57,630
And we, we like literally
scrambling to fix it.

57
00:02:57,960 --> 00:02:59,910
those days were like full of firefighting.

58
00:03:00,000 --> 00:03:00,250
Okay.

59
00:03:00,250 --> 00:03:03,260
We're constantly pitching
things up and then only to

60
00:03:03,260 --> 00:03:04,970
find new problems the next day.

61
00:03:05,140 --> 00:03:05,550
Okay.

62
00:03:05,700 --> 00:03:09,200
and, and scaling the system, updating
the system to handle those problems.

63
00:03:09,520 --> 00:03:11,120
And that was kind of like headache.

64
00:03:11,330 --> 00:03:13,660
We would throw more
hardware with the problem.

65
00:03:13,700 --> 00:03:17,570
If it is like bottleneck from like,
efficiency, maybe we build new

66
00:03:17,590 --> 00:03:20,899
custom scripts to try to make things
faster or handle the bigger problems.

67
00:03:21,040 --> 00:03:24,310
mismatches in the data types,
but even with all that effort,

68
00:03:24,680 --> 00:03:25,830
it was not sustainable.

69
00:03:26,770 --> 00:03:27,760
We need something better.

70
00:03:28,260 --> 00:03:30,690
AI role in transforming, data pipeline.

71
00:03:30,790 --> 00:03:34,940
Let's see the turning point, for me, like,
okay, came when we started integrating,

72
00:03:35,020 --> 00:03:39,340
like looking into the AI, how we can
liberate the AI, suddenly the task that

73
00:03:39,350 --> 00:03:42,000
had like once taken hours, even days.

74
00:03:42,015 --> 00:03:43,455
Were happening like automatically.

75
00:03:44,005 --> 00:03:46,675
for example, okay, I'll take
a hypothetical example here.

76
00:03:46,825 --> 00:03:50,745
Manually checking your data quality, it
would flag anomalies in the real time.

77
00:03:51,245 --> 00:03:55,295
And, and at the best party, like
system was not just reacting, it's even

78
00:03:55,295 --> 00:03:57,445
learning from the, from the pipelines.

79
00:03:57,445 --> 00:03:58,495
Like why failing?

80
00:03:58,495 --> 00:03:59,365
How it's failing.

81
00:03:59,815 --> 00:04:00,175
Okay.

82
00:04:00,175 --> 00:04:04,045
And what could the potential issue, like
when, when we do some manual, correction

83
00:04:04,045 --> 00:04:08,485
and then process and then give back that
information to, ML algorithms handing the

84
00:04:08,485 --> 00:04:09,980
pipelines, it's like a new learning for.

85
00:04:10,415 --> 00:04:10,505
it.

86
00:04:10,905 --> 00:04:14,545
imagine a system that learning
on its own from the data and then

87
00:04:14,545 --> 00:04:15,965
getting smarter over the time.

88
00:04:16,345 --> 00:04:19,935
obviously this ML powered or AI powered
data pipelines can automatically

89
00:04:19,955 --> 00:04:23,535
adapt to these fluctuations, adjusting
the resources on the hardware side

90
00:04:24,135 --> 00:04:27,145
and improve their performance,
with, with, with all without like

91
00:04:27,565 --> 00:04:29,579
a very minimal human intervention.

92
00:04:30,079 --> 00:04:33,929
Key benefits, when we include this ML
and AI in the data engineering, some

93
00:04:33,929 --> 00:04:35,299
potential key benefits would be there.

94
00:04:35,329 --> 00:04:38,499
Like impact of ML and AI is profound.

95
00:04:38,579 --> 00:04:41,629
We, we are not just improving
the speed or accuracy.

96
00:04:41,629 --> 00:04:46,429
We're rethinking like how we can manage
these huge data sets, huge pipelines

97
00:04:46,459 --> 00:04:47,963
with complex transformation rules.

98
00:04:47,963 --> 00:04:52,604
for example, instead of worrying about
like scaling a pipeline, with like based

99
00:04:52,604 --> 00:04:56,745
upon the size and corresponding hardware
requirements or resources requirements.

100
00:04:57,175 --> 00:04:57,575
Okay.

101
00:04:57,675 --> 00:05:01,265
Just, this, this algorithms like
AI models will automatically adjust

102
00:05:01,295 --> 00:05:05,265
adding like more resources as needed
and, and, and the best part is this

103
00:05:05,285 --> 00:05:07,305
happens like without human intervention.

104
00:05:08,015 --> 00:05:12,105
This addresses the need of like cost of
hardware upgrades and manual interventions

105
00:05:12,145 --> 00:05:15,835
and with cloud coming into the picture,
this dynamicness like literally helps

106
00:05:16,495 --> 00:05:20,345
and allow us to focus on, on better
tasks like high level tasks and the

107
00:05:20,345 --> 00:05:24,635
business oriented things rather than like
spending a lot of time on the, on the

108
00:05:24,695 --> 00:05:26,545
technical challenges on day to day basis.

109
00:05:26,975 --> 00:05:27,515
real time.

110
00:05:27,695 --> 00:05:28,055
adoption.

111
00:05:28,155 --> 00:05:31,315
like obviously one of the
most exciting aspects of AI is

112
00:05:31,355 --> 00:05:32,505
like real time adaptability.

113
00:05:33,085 --> 00:05:38,195
Imagine during a flash sale, okay, all
of a sudden like, like, like, Black

114
00:05:38,195 --> 00:05:43,045
Friday or walking day in UK, okay,
there is a sale going and then the

115
00:05:43,135 --> 00:05:45,195
traffic spikes like unpredictably.

116
00:05:45,865 --> 00:05:49,725
In the past we would worry like, okay,
we can't predict like how much volume it

117
00:05:49,725 --> 00:05:54,085
could be like, okay, how much hardware
or resources we need to handle this.

118
00:05:54,175 --> 00:05:56,385
We would worry about like how
systems would handle this.

119
00:05:56,605 --> 00:06:00,875
new loads, all of a sudden coming
in, but with, with AI, the system can

120
00:06:00,875 --> 00:06:02,795
automatically scale and adjust itself.

121
00:06:03,245 --> 00:06:06,605
And it makes like decisions in
real time, giving like business

122
00:06:06,865 --> 00:06:08,875
significant competitive advantage.

123
00:06:09,765 --> 00:06:14,585
It's like allowing like lightning fast
adjustments to the ingestion, ensuring

124
00:06:14,585 --> 00:06:16,785
that there is no opportunity is missed.

125
00:06:16,945 --> 00:06:17,245
Okay.

126
00:06:17,355 --> 00:06:20,965
There is no failure in
the end to end execution.

127
00:06:21,565 --> 00:06:25,965
Even in with like huge workloads
and, and, and certain spikes.

128
00:06:26,465 --> 00:06:30,155
Another beauty, like, like, let
me share, one real world scenario.

129
00:06:30,525 --> 00:06:31,065
let's see.

130
00:06:31,245 --> 00:06:35,945
There is like large financial
institution, had been struggling to

131
00:06:35,945 --> 00:06:37,625
detect the fraud in the real time.

132
00:06:38,065 --> 00:06:40,585
And, and with, with, with
traditional systems, it's

133
00:06:40,585 --> 00:06:42,215
really, difficult to handle it.

134
00:06:42,725 --> 00:06:46,815
The, the one reason is because it's not
like real time with the ai, that could

135
00:06:46,815 --> 00:06:48,406
analyze the transaction patterns in real.

136
00:06:48,645 --> 00:06:52,355
time, with the, with the models that
already been trained with enough

137
00:06:52,355 --> 00:06:56,635
data and then detect these fraudulent
behaviors as it is happening.

138
00:06:56,635 --> 00:07:00,875
And before it could cause like much,
much damage for the end user, it's

139
00:07:00,885 --> 00:07:04,385
like having a real time security guard
for your data, it's like having a real

140
00:07:04,435 --> 00:07:09,585
time, person there sitting for you
and watching for you in terms of like

141
00:07:09,585 --> 00:07:12,855
what's happening around you in your,
around your financial transactions.

142
00:07:13,265 --> 00:07:17,845
it's able to spot the anomalies and
take action immediately, Using the

143
00:07:17,845 --> 00:07:23,075
impacts and further any impacts,
how we can efficiency, optimize the

144
00:07:23,085 --> 00:07:25,245
efficiency in this AI driven world.

145
00:07:25,595 --> 00:07:28,655
One of the most powerful features
is, AI powered data pipelines

146
00:07:28,695 --> 00:07:30,735
in that are like not static.

147
00:07:30,765 --> 00:07:35,135
As I mentioned earlier, by analyzing the
past performance data, that these machine

148
00:07:35,135 --> 00:07:40,005
learning algorithms learn more, identify
the potential bottlenecks, resources,

149
00:07:40,005 --> 00:07:41,985
underutilizations and inefficiencies.

150
00:07:42,620 --> 00:07:46,130
Inefficiencies making proactive
adjustments to the improved performance.

151
00:07:46,960 --> 00:07:50,770
For example, if a pipeline running
slower than usual Okay, the AI system

152
00:07:50,770 --> 00:07:55,050
can predict like what could be the future
bottlenecks based upon What's happened

153
00:07:55,050 --> 00:07:58,180
now what's happening now or what's
happened in the past and suggest or

154
00:07:58,180 --> 00:08:03,090
implement resources adjustments before the
impact Performance are breaking down the

155
00:08:03,100 --> 00:08:06,840
things end to end How it works is okay.

156
00:08:06,870 --> 00:08:11,490
Imagine a end to end system where
okay, we have like data ingestion data

157
00:08:11,490 --> 00:08:17,300
processing Storage And final analysis
are usage of this process information.

158
00:08:17,830 --> 00:08:21,770
This A dynamically selects the most
relevant sources and formats only.

159
00:08:22,180 --> 00:08:22,520
Okay.

160
00:08:22,570 --> 00:08:26,140
And it's advanced algorithms
automatically clean, transform, and

161
00:08:26,140 --> 00:08:30,540
even enrich the data for like missing
values or missing upstream, I mean,

162
00:08:30,580 --> 00:08:33,810
missing details in the upstream
systems are not entered by the users.

163
00:08:34,165 --> 00:08:36,895
Or, or like issues in
their legacy systems.

164
00:08:36,995 --> 00:08:40,005
Okay, all these will be taken
care, through, I mean, while

165
00:08:40,035 --> 00:08:42,655
we're doing the processing of the
data or transforming the data.

166
00:08:43,025 --> 00:08:46,455
And finally, when storage also is
automatically optimized the storage,

167
00:08:46,455 --> 00:08:51,135
okay, the best way of storage in terms
of like the level of degradation we need,

168
00:08:51,455 --> 00:08:55,175
it, it will recommend like, okay, how to
store, where to store and level of the

169
00:08:55,175 --> 00:08:59,535
grain we need to maintain, and, and based
upon the future access requirements.

170
00:08:59,535 --> 00:08:59,720
Thanks.

171
00:08:59,965 --> 00:09:03,315
and, and in the end, like, okay, once
we do all this, obviously we need

172
00:09:03,315 --> 00:09:08,365
this data for analytics and insights,
and, and AI driven analytics helps

173
00:09:08,365 --> 00:09:13,765
surface the most important insights,
flagging, like, any issues or anomalies

174
00:09:14,155 --> 00:09:18,485
and key patterns and deviations from,
like, expected, patterns of progress.

175
00:09:18,800 --> 00:09:21,940
Before, anyone, like any human
intervention comes into picture

176
00:09:22,140 --> 00:09:25,170
or any analyst or business
user look into the data itself.

177
00:09:25,520 --> 00:09:27,620
this is the future of data engineering.

178
00:09:27,740 --> 00:09:30,610
it's seamless, intelligent,
and fully automated.

179
00:09:31,220 --> 00:09:31,270
Okay.

180
00:09:31,480 --> 00:09:33,080
Let's see a case study.

181
00:09:33,510 --> 00:09:35,210
let's look at like a real world example.

182
00:09:35,800 --> 00:09:40,360
a, a, a real client, face delays in
generating daily sales forecasts.

183
00:09:40,520 --> 00:09:40,810
Okay.

184
00:09:41,150 --> 00:09:42,440
because of the slow data processing.

185
00:09:42,790 --> 00:09:46,030
Obviously if someone take me to
take a decision before like 8 a.

186
00:09:46,030 --> 00:09:46,260
m.

187
00:09:47,100 --> 00:09:50,020
And the pipeline is taking like
much larger time and we see a

188
00:09:50,020 --> 00:09:51,500
surprise in terms of failures.

189
00:09:52,020 --> 00:09:54,750
It would be like massive
disaster, obviously, because

190
00:09:54,750 --> 00:09:56,840
they can't take decision on time.

191
00:09:57,580 --> 00:10:00,760
And it's giving an edge to the
competition if we don't have like

192
00:10:01,020 --> 00:10:02,550
information available on time.

193
00:10:03,420 --> 00:10:07,880
And by switching to this AI driven
like pipeline management, that

194
00:10:07,880 --> 00:10:11,020
could process data in real time,
allowing them to generate daily

195
00:10:11,020 --> 00:10:13,020
forecasts in hours instead of days.

196
00:10:13,820 --> 00:10:16,910
Okay, this will allow them to adjust
the price strategies, managing the

197
00:10:16,910 --> 00:10:21,690
inventory requirements effectively, and
even personalize the marketing efforts,

198
00:10:21,790 --> 00:10:24,160
all based upon like up to minute data.

199
00:10:24,860 --> 00:10:27,790
Like, okay, what's happened last
minute would drive like what

200
00:10:27,810 --> 00:10:29,390
could happen in next minute.

201
00:10:29,845 --> 00:10:33,245
That kind of scenarios, this is the
kind of transformation we are seeing

202
00:10:33,245 --> 00:10:38,325
in the industry and AI is playing
a real major role here and driving

203
00:10:38,325 --> 00:10:41,655
meaningful change in how companies
should operate and will operate.

204
00:10:42,155 --> 00:10:44,385
what are the some key
technologies driving these?

205
00:10:44,615 --> 00:10:44,835
Okay.

206
00:10:44,885 --> 00:10:49,975
Now let's take a look at the
technologies that behind this AI powered

207
00:10:49,975 --> 00:10:52,125
pipelines and ML powered pipelines.

208
00:10:52,535 --> 00:10:55,445
one basic thing obviously is machine
learning and the deep learning.

209
00:10:55,475 --> 00:10:58,085
These models, I mean, when
learned from like past issues.

210
00:10:58,965 --> 00:11:02,425
Adopt and improve based on the
historical data, making the

211
00:11:02,425 --> 00:11:04,495
pipeline more smarter over the time.

212
00:11:04,505 --> 00:11:08,165
Making this end to end movement of
data more smarter over the time.

213
00:11:08,665 --> 00:11:15,145
Say NLP is another thing like
natural language processing.

214
00:11:15,145 --> 00:11:17,615
This allows pipeline to
process unstructured data

215
00:11:17,665 --> 00:11:19,435
more effectively, like text.

216
00:11:19,865 --> 00:11:24,515
logs or even converting the audio
to text and then process and then

217
00:11:24,595 --> 00:11:27,215
and then massively unstructured
data will be taken care of here.

218
00:11:27,265 --> 00:11:31,435
Another advantage here we can see is
in terms of NLP, because it's like

219
00:11:31,485 --> 00:11:34,535
behaves like a human understanding
the information and then processing.

220
00:11:34,845 --> 00:11:35,155
Okay.

221
00:11:35,395 --> 00:11:37,875
Take some example like,
okay, missing city.

222
00:11:38,175 --> 00:11:40,905
Based upon the postcode, okay,
anyone can say city name.

223
00:11:41,105 --> 00:11:41,965
It could be taken care.

224
00:11:42,015 --> 00:11:44,315
Similarly, name misspelled, okay?

225
00:11:44,685 --> 00:11:49,045
Say, instead of, my name's Rinoas, and
instead of S R I N I V E S, someone

226
00:11:49,045 --> 00:11:54,955
spelled it A is missing, and MLP have that
ability to repopulate the correct value

227
00:11:54,985 --> 00:11:57,435
based upon what it learned in the past.

228
00:11:57,935 --> 00:12:01,635
This automates the selection
and tuning of the machine models

229
00:12:01,685 --> 00:12:05,665
like machine learning models for
more optimal data transformation.

230
00:12:06,475 --> 00:12:08,645
And another one is like
cloud infrastructure.

231
00:12:08,985 --> 00:12:12,885
Obviously, to have this dynamicness
and necessary scalability in real

232
00:12:12,915 --> 00:12:17,085
time and flexibility in real time,
okay, the infrastructure should

233
00:12:17,125 --> 00:12:20,555
support the way we leveraging the
infrastructure to support all of this.

234
00:12:20,605 --> 00:12:22,565
And cloud is the main, main, main thing.

235
00:12:23,155 --> 00:12:25,935
that's required, which is the
foundation for all these things.

236
00:12:26,385 --> 00:12:30,035
Together, like, all these
technologies will enable that next

237
00:12:30,035 --> 00:12:31,115
generation of data engineering.

238
00:12:31,615 --> 00:12:35,665
how to get, started with AI
in data engineering, okay?

239
00:12:36,095 --> 00:12:39,365
step one, obviously, okay, we need
to assess the current infrastructure,

240
00:12:39,405 --> 00:12:44,015
current pipeline, identify all the pain
points, list down everything in the

241
00:12:44,015 --> 00:12:48,805
backlog, and, and we can, we can just
check, like, okay, where we can, we

242
00:12:48,805 --> 00:12:51,455
could apply these AI, ML models, to.

243
00:12:51,455 --> 00:12:55,915
To have like more impact, obviously we
need to bring down and bring the return

244
00:12:55,915 --> 00:13:00,305
on investment into the picture, before
moving forward, but, but that's, that's

245
00:13:00,305 --> 00:13:04,745
the step one, and then selecting the
smart tools, right tools and the framework

246
00:13:04,965 --> 00:13:07,325
that best suit to have like these.

247
00:13:07,730 --> 00:13:10,420
new effective and efficient
data architecture and goals.

248
00:13:11,200 --> 00:13:15,950
And start small, better to start, like,
okay, implement a small solution, okay,

249
00:13:15,950 --> 00:13:20,330
that driving through ML models on a
limited scale and iterate, like, see,

250
00:13:20,330 --> 00:13:24,270
like, okay, is there any benefit happening
and how much benefit is happening and then

251
00:13:24,450 --> 00:13:29,130
iterate and apply more models, like, on
the pain points to improve the things.

252
00:13:29,545 --> 00:13:33,315
Once we scale up like doing the scale
up, okay, we'll get like more confidence

253
00:13:33,315 --> 00:13:37,455
in the system, okay, and we'll see the
more benefit like how these are helping

254
00:13:37,815 --> 00:13:42,075
in terms of like both business and also
like technical team, and then expanding

255
00:13:42,075 --> 00:13:43,935
the scope across the data architecture.

256
00:13:44,405 --> 00:13:49,465
The key is here is to start small,
okay, manageable project, like the

257
00:13:49,515 --> 00:13:52,115
team can handle and build from there.

258
00:13:52,355 --> 00:13:53,715
what could be the future trends?

259
00:13:53,885 --> 00:13:57,695
Okay, one is like self handling, self
healing or self handling pipelines.

260
00:13:57,725 --> 00:14:01,645
we can say, imagine a system that
automatically detects and resolves its own

261
00:14:01,645 --> 00:14:03,745
problems and then without any human input.

262
00:14:04,155 --> 00:14:05,705
that's like an arch star, okay?

263
00:14:05,755 --> 00:14:08,215
it, it's not like a day two thing.

264
00:14:08,285 --> 00:14:11,815
It could be like long term with an,
I mean, long term effort going to be

265
00:14:11,815 --> 00:14:13,965
there and, and still we're evolving.

266
00:14:14,545 --> 00:14:16,525
and, and, real time data processing.

267
00:14:16,615 --> 00:14:20,505
As of now, we have time processing
working in a conventional way.

268
00:14:20,895 --> 00:14:25,165
But when we have like more advanced
ML models, we, we will see like even

269
00:14:25,165 --> 00:14:29,635
faster processing, in more use cases,
enabling business to make instant

270
00:14:29,635 --> 00:14:35,045
decisions, based upon that real time,
data processing across systems, across

271
00:14:35,105 --> 00:14:37,605
areas, and then augmented engineering.

272
00:14:38,110 --> 00:14:42,490
Data engineers will eventually, will
work alongside with the AI systems

273
00:14:42,490 --> 00:14:46,650
and ML models, using them like a
supporting tools to optimize the

274
00:14:46,850 --> 00:14:51,060
workflows and integrate them in the
business processes for macro impact.

275
00:14:51,560 --> 00:14:54,650
we just started scratching the
surface of what AI can do in

276
00:14:54,650 --> 00:14:55,740
the data engineering world.

277
00:14:56,160 --> 00:14:58,990
but I mean, it's not,
it's not, the final thing.

278
00:14:59,030 --> 00:15:01,090
Like we, we could see more advancements.

279
00:15:01,110 --> 00:15:02,950
We could see more surprises in the future.

280
00:15:03,450 --> 00:15:08,170
And final conclusion and what could be
the next steps, to wrap up, okay, in terms

281
00:15:08,170 --> 00:15:12,150
of like AI is fundamentally transforming
the way we're processing the data or

282
00:15:12,150 --> 00:15:17,160
data engineering by making pipelines more
smarter, more efficient, more adaptable.

283
00:15:17,500 --> 00:15:20,170
And we're seeing a shift
from manual process to self

284
00:15:20,220 --> 00:15:21,820
optimizing intelligent systems.

285
00:15:22,150 --> 00:15:26,520
my call here for action for, for all
of us today is like, just embrace it.

286
00:15:26,955 --> 00:15:27,725
Embrace AI.

287
00:15:27,735 --> 00:15:28,265
Use it.

288
00:15:28,765 --> 00:15:31,345
Whether you're just starting
already, experimenting with AI

289
00:15:31,345 --> 00:15:34,575
in your systems, take the next
step and just build, integrating

290
00:15:34,575 --> 00:15:36,105
it with your data architecture.

291
00:15:36,725 --> 00:15:41,005
that, that, that is like a starting point,
both in terms of learning and not both

292
00:15:41,055 --> 00:15:42,775
in terms of like seeing the benefits.

293
00:15:43,225 --> 00:15:45,045
Uh, thanks for all.

294
00:15:45,195 --> 00:15:47,365
I mean, thank you very
much, for your time today.

295
00:15:47,720 --> 00:15:51,140
I'd be happy to have, I mean,
answer the questions if you have

296
00:15:51,140 --> 00:15:53,670
any, or the comments, or offline.

297
00:15:54,250 --> 00:15:54,760
thanks a lot.

298
00:15:54,780 --> 00:15:55,560
Thank you very much.

