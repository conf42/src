1
00:00:00,500 --> 00:00:00,560
Hi.

2
00:00:01,060 --> 00:00:06,460
Today we are discussing about
resilience for secure and fault

3
00:00:06,490 --> 00:00:08,110
tolerance enterprise systems.

4
00:00:08,610 --> 00:00:13,200
First, welcome to this deep dive
into resilience engineer A three

5
00:00:13,200 --> 00:00:14,810
plan for modern enterprises.

6
00:00:15,770 --> 00:00:20,510
Today we are exploring how to build
systems that not only survive failures,

7
00:00:20,900 --> 00:00:22,460
but actually thrive under pressure.

8
00:00:22,960 --> 00:00:27,640
This is Practical Session grounded
in real world distributed systems

9
00:00:27,880 --> 00:00:29,590
challenges to at enterprise scale.

10
00:00:30,090 --> 00:00:34,200
By the end, you will have
actionable strategies to implement

11
00:00:34,289 --> 00:00:35,730
in your own architectures.

12
00:00:36,230 --> 00:00:37,370
Let me introduce myself.

13
00:00:38,180 --> 00:00:43,589
My name is, I am working as quality
performance engineer at Inc. I

14
00:00:43,589 --> 00:00:49,049
bring Tenness years of experience in
enterprise scale distributed systems.

15
00:00:49,559 --> 00:00:54,449
My focus is mainly on performance
optimization, reliability

16
00:00:54,449 --> 00:00:57,359
engineering, and DevSecOps practices.

17
00:00:58,290 --> 00:01:01,949
I specialize in building
resilient architectures that

18
00:01:01,949 --> 00:01:04,169
balance three competing demands.

19
00:01:04,590 --> 00:01:08,460
Security fault tolerance
and operational efficiency.

20
00:01:09,359 --> 00:01:13,919
I work extensively with complex
microservices environments

21
00:01:14,220 --> 00:01:18,299
where one failure can castrate
across your entire platform.

22
00:01:18,799 --> 00:01:25,014
I have debug systems serving 30 K request
per minute, so the challenge, modern

23
00:01:25,014 --> 00:01:27,054
distributed systems under pressure.

24
00:01:27,865 --> 00:01:29,979
There are three typical pressures.

25
00:01:30,479 --> 00:01:31,830
One is security threats.

26
00:01:32,009 --> 00:01:33,600
Second is cascading failures.

27
00:01:33,990 --> 00:01:35,460
Third is performance demands.

28
00:01:35,960 --> 00:01:41,269
The first related to security
threats, we are seeing evolving

29
00:01:41,329 --> 00:01:45,424
attack vectors, specifically
targeting distributed architectures.

30
00:01:45,924 --> 00:01:50,544
Example, attackers exploit
microservice boundaries where

31
00:01:50,574 --> 00:01:52,854
security controls are inconsistent.

32
00:01:53,354 --> 00:01:53,864
It's a vulnera.

33
00:01:54,364 --> 00:01:58,714
Vulnerability in one service
can propagate across your entire

34
00:01:58,714 --> 00:02:00,634
mesh, if not properly isolated.

35
00:02:01,624 --> 00:02:06,319
Cascading failures, single point
failures, don't stay, isolate it anymore.

36
00:02:06,549 --> 00:02:11,190
They cooperate across microservices,
database connection, pool exertion.

37
00:02:11,625 --> 00:02:16,335
Memory leaks, independent services,
and ultimately your entire platform

38
00:02:16,335 --> 00:02:22,355
cascades the do like saying service
safe rails, service speed, timeouts,

39
00:02:22,475 --> 00:02:26,885
and due to this service, C tries
aggressively, which causes resource,

40
00:02:26,885 --> 00:02:29,360
star, and entire system will collapse.

41
00:02:29,860 --> 00:02:33,520
Third threat performance demands,
you must maintain reliability

42
00:02:33,520 --> 00:02:35,320
under variable workloads.

43
00:02:36,280 --> 00:02:40,800
Example, black Friday traffic
is 10 x. Your baseline security

44
00:02:40,800 --> 00:02:42,690
instance create traffic spikes.

45
00:02:42,690 --> 00:02:44,940
Your system must handle report gracefully.

46
00:02:45,885 --> 00:02:50,505
You want to slow down to say cars,
but can compromise reliability.

47
00:02:51,005 --> 00:02:52,445
What is resilience engineering?

48
00:02:53,405 --> 00:02:58,545
So the core definition of resilience
engineering is about engineering

49
00:02:58,545 --> 00:03:03,480
systems to maintain secure and reliable
operation under adverse conditions.

50
00:03:04,110 --> 00:03:06,160
This goes beyond
traditional fault elements.

51
00:03:06,660 --> 00:03:09,810
The key insight is it's not
just about surveying failures.

52
00:03:09,870 --> 00:03:16,050
It's about adapting to threats, recovering
gracefully in a three part foundation.

53
00:03:16,650 --> 00:03:18,720
First is proactive design.

54
00:03:18,900 --> 00:03:20,670
Don't wait for failures to happen.

55
00:03:21,300 --> 00:03:24,090
Designwise system as human
failures will happen.

56
00:03:24,590 --> 00:03:26,405
Step two, security integration.

57
00:03:26,915 --> 00:03:31,130
Combine security controls with
reliability mechanisms not after

58
00:03:31,130 --> 00:03:33,920
tops, robust distribution systems.

59
00:03:34,420 --> 00:03:39,189
Systems that work reliably across
complex microservices environments.

60
00:03:39,689 --> 00:03:42,509
It's example for this is
for traditional approach.

61
00:03:42,989 --> 00:03:46,049
It's like building a service,
moderate and react others.

62
00:03:46,619 --> 00:03:51,540
But as part of resilience, engineering
approach, assume the service will fail.

63
00:03:51,989 --> 00:03:53,699
How does the system handle it?

64
00:03:54,119 --> 00:03:55,889
What mechanisms prevent gas?

65
00:03:56,549 --> 00:03:58,049
Can we recover automatically?

66
00:03:58,549 --> 00:03:59,539
So there are.

67
00:04:00,039 --> 00:04:04,369
Score four tolerance mechanisms
and the three, mechanisms.

68
00:04:04,699 --> 00:04:12,349
One is circuit breaker patterns, second is
logic, third is redundancy calculations.

69
00:04:13,129 --> 00:04:17,209
Let's discuss one problem,
so circuit breaker patterns.

70
00:04:17,709 --> 00:04:21,279
So the problem we are solving
here is when a downstream service

71
00:04:21,279 --> 00:04:26,139
fails, clients keep bonding it with
requests, making recovery harder.

72
00:04:26,639 --> 00:04:30,689
Resource based, slow recovery
and customer facing types.

73
00:04:31,109 --> 00:04:37,259
The solutions a state-based model
that detects failures and prevents

74
00:04:37,289 --> 00:04:42,449
cascading errors by temporarily
halting request to failing surfaces.

75
00:04:43,409 --> 00:04:46,349
Think of it like an
electrical circuit breaker.

76
00:04:47,279 --> 00:04:51,269
When current gets to
pipe, it trips and stops.

77
00:04:51,769 --> 00:04:52,819
There are three states.

78
00:04:53,299 --> 00:04:58,219
Closed state, open state, half
open state, enclosed state normal

79
00:04:58,219 --> 00:05:03,010
operations request flow through in
open state failure threshold exceeded

80
00:05:03,070 --> 00:05:07,030
then circuit clicks has stopped
sending the request half open state.

81
00:05:08,020 --> 00:05:11,739
In this, we test recovery with
the limited requests, like sending

82
00:05:11,739 --> 00:05:15,789
one or two requests and checking
whether the service is up or not.

83
00:05:16,690 --> 00:05:17,650
Let's take an example.

84
00:05:18,010 --> 00:05:21,195
Your microservice handles thousand
requests per second normally.

85
00:05:21,695 --> 00:05:22,985
Database crashed.

86
00:05:23,284 --> 00:05:29,395
So response times spike two, 300 seconds
or 30 seconds After five consecutive

87
00:05:29,395 --> 00:05:34,315
failures, circuit opens, so normal
requests sent to failing service.

88
00:05:35,034 --> 00:05:41,335
After 30 seconds of open, we'll send one
or two requests to DB to check if the full

89
00:05:41,395 --> 00:05:43,615
circuit closes and traffic will ratio.

90
00:05:43,705 --> 00:05:45,325
They open and they'll try again.

91
00:05:45,325 --> 00:05:48,504
Up 30 seconds coming to, we try logically.

92
00:05:49,004 --> 00:05:53,864
So the problem here is transit
failures are common in distributed

93
00:05:53,864 --> 00:05:59,979
systems like network, temporary
resource ET can create thundering hub.

94
00:06:00,219 --> 00:06:04,059
All clients retrain simultaneously
or wellbeing the system.

95
00:06:04,929 --> 00:06:09,340
So the solution is exponential
back of strategies that

96
00:06:09,399 --> 00:06:14,014
intelligently fe the operations
without overwhelming the systems.

97
00:06:14,514 --> 00:06:14,874
Each.

98
00:06:14,874 --> 00:06:18,114
We 12 wait longer than the
previous one in exponential.

99
00:06:18,474 --> 00:06:22,284
Like one second, two seconds, four
seconds, eight seconds, 16 seconds In

100
00:06:22,284 --> 00:06:25,844
that time, so jitter will add randomness.

101
00:06:25,844 --> 00:06:28,189
So all the clients don't replay the exer.

102
00:06:28,634 --> 00:06:32,134
Same moment example form our system.

103
00:06:32,134 --> 00:06:36,484
Six a p. A call failed
trans due to network error.

104
00:06:37,234 --> 00:06:39,964
So the first feature happened
at a hundred milliseconds.

105
00:06:40,804 --> 00:06:41,134
It'll wake.

106
00:06:41,960 --> 00:06:47,059
Then next will happen at 200
milliseconds, and then third

107
00:06:47,569 --> 00:06:49,309
will happen at 400 milliseconds.

108
00:06:50,119 --> 00:06:54,349
So by the time you get to the
ry, you are waiting 1.6 seconds,

109
00:06:54,769 --> 00:06:57,019
giving the system time to recover.

110
00:06:57,519 --> 00:07:00,039
Progressive delay interval,
prevent cascading tress.

111
00:07:00,539 --> 00:07:04,109
The key benefit of this
is transient failure.

112
00:07:04,109 --> 00:07:06,539
70% succeed on first retirement.

113
00:07:06,899 --> 00:07:07,559
Exponential.

114
00:07:07,559 --> 00:07:07,949
Back off.

115
00:07:08,449 --> 00:07:12,949
Recover from temperature failures without
adding already struggling systems.

116
00:07:13,449 --> 00:07:15,579
Next is redundancy calculations.

117
00:07:16,239 --> 00:07:21,859
So the concept of this is active,
passive and active deployments ensuring

118
00:07:21,859 --> 00:07:27,919
continuous available, no single mission,
pulse, the entire load, spread the risk.

119
00:07:28,419 --> 00:07:29,964
So there are two deployment patterns.

120
00:07:30,324 --> 00:07:30,884
I two passive.

121
00:07:31,854 --> 00:07:35,514
In Active U Primary server
handles all traffic.

122
00:07:36,234 --> 00:07:41,554
Secondary server status one standard
example, master slave database setup.

123
00:07:42,154 --> 00:07:48,174
No primary failure or on primary
failure to secondary intent to 32nd.

124
00:07:48,239 --> 00:07:48,399
Recovery.

125
00:07:48,900 --> 00:07:52,770
Active multiple service, actively
handled traffic submitted easily.

126
00:07:53,400 --> 00:07:55,290
Load balancer distributor requests.

127
00:07:55,830 --> 00:07:58,140
So example, three
identical payment process.

128
00:07:58,140 --> 00:08:01,860
Each handling one third of a
traffic one, fail the remaining

129
00:08:01,860 --> 00:08:03,720
to observe the traffic with grace.

130
00:08:04,110 --> 00:08:09,919
Perhaps with 50% slower, not complete
outage, geographic distribution.

131
00:08:10,640 --> 00:08:11,960
This is a real example.

132
00:08:12,080 --> 00:08:18,520
So recently, or if we take an AWS, we
can see that there are data centers

133
00:08:18,609 --> 00:08:19,884
that are present in different regions.

134
00:08:20,589 --> 00:08:23,839
Like USE is west eu China everywhere.

135
00:08:24,559 --> 00:08:31,369
So if a Ws USC has an auto US west
and EU keeps serving customers.

136
00:08:32,209 --> 00:08:36,919
So rt, recovery time
objective, it's near zero.

137
00:08:36,919 --> 00:08:38,209
Users don't notice.

138
00:08:38,839 --> 00:08:41,030
RP recovery point of ticking.

139
00:08:41,389 --> 00:08:45,909
Minimal data loss volume, replication,
load balancing across replicas.

140
00:08:46,409 --> 00:08:50,819
Three identical services just
to traffic will be 33% to each.

141
00:08:51,479 --> 00:08:55,619
Healthy checks ensures failed instances
get removed from road balance.

142
00:08:56,119 --> 00:09:00,619
Automated scaling as load increases,
it'll speed up more instances.

143
00:09:01,119 --> 00:09:04,664
Let's go in depth between circle
breaker pattern and action.

144
00:09:05,665 --> 00:09:09,194
Okay, so first State, ERBC.

145
00:09:09,614 --> 00:09:12,104
Normal operations requests
are flowing through.

146
00:09:12,104 --> 00:09:14,265
Normal system is healthy.

147
00:09:14,265 --> 00:09:15,525
Metrics are green.

148
00:09:15,824 --> 00:09:16,724
Customers are ahead.

149
00:09:16,905 --> 00:09:23,134
This is 99% of the terminal healthy system
stays to open state failure detected.

150
00:09:23,644 --> 00:09:28,284
So failure threshold, exited circuit
tricks, five consecutive failures

151
00:09:28,284 --> 00:09:29,975
happened and circuit got okay.

152
00:09:30,710 --> 00:09:34,370
No request will be sent to the
failing service written return,

153
00:09:34,400 --> 00:09:38,300
faster response to class saying
service temporarily and available.

154
00:09:38,750 --> 00:09:39,470
This is crucial.

155
00:09:39,590 --> 00:09:44,480
Stop wasting time from stage
three half ated testing recovery.

156
00:09:44,980 --> 00:09:48,940
We are testing recovery with limited
requests like sending one or two request.

157
00:09:48,940 --> 00:09:51,700
Proceed with the service if both succeed.

158
00:09:52,210 --> 00:09:53,830
Recess circuit two, closed.

159
00:09:54,100 --> 00:09:54,850
Full recovery.

160
00:09:55,420 --> 00:09:58,930
If circuit stays open,
we'll try again later.

161
00:09:59,430 --> 00:10:05,040
Stage four, recovery return to
normal so the now the service has

162
00:10:05,220 --> 00:10:07,680
covered and is successfully restored.

163
00:10:07,680 --> 00:10:08,370
Normal flow.

164
00:10:09,000 --> 00:10:13,800
Once service healthy, circuit complete,
full traffic will be resumed and

165
00:10:13,800 --> 00:10:15,750
customer see normal service levels.

166
00:10:16,710 --> 00:10:19,765
Business impact due to thesis
without circuit breaker.

167
00:10:20,265 --> 00:10:26,025
Outage last five minutes plus 15
minutes recovery, it'll be totally

168
00:10:26,205 --> 00:10:32,325
20 minutes Pain with circuit breaker
outage recognized in 10 seconds.

169
00:10:32,775 --> 00:10:35,025
Fast fail responses.

170
00:10:35,655 --> 00:10:37,335
Recovery happens automatically.

171
00:10:37,995 --> 00:10:40,065
User experience impact is minimum.

172
00:10:40,565 --> 00:10:45,020
Next is chaos engineering testing,
resilience through control feature.

173
00:10:45,520 --> 00:10:50,860
So the philosophy is chaos
engineering is disciplined approach

174
00:10:51,160 --> 00:10:52,870
to discovering system weakness.

175
00:10:53,710 --> 00:11:00,040
Con counter idea intentionally break
your system before real failures happen.

176
00:11:00,670 --> 00:11:03,520
This is not randomly breaking
points, it's hypothesis.

177
00:11:04,020 --> 00:11:05,910
The definition is chaos.

178
00:11:05,910 --> 00:11:09,930
Engineering proactively injects
failures into production like anyons.

179
00:11:10,335 --> 00:11:14,475
To validate resilience
mechanisms and security posture

180
00:11:15,435 --> 00:11:16,935
before real insurance occur.

181
00:11:17,435 --> 00:11:20,735
Example, instead of discovering
your disaster recovery doesn't

182
00:11:20,735 --> 00:11:22,895
work during an actual disaster.

183
00:11:22,985 --> 00:11:23,435
Test it.

184
00:11:23,435 --> 00:11:29,695
Intentional key practices control
failure induction example.

185
00:11:29,935 --> 00:11:31,405
Turn off one database replica.

186
00:11:32,035 --> 00:11:32,875
You are simulating here.

187
00:11:33,145 --> 00:11:33,925
Hardware failure.

188
00:11:34,615 --> 00:11:36,020
Run the experiment for finance.

189
00:11:36,490 --> 00:11:37,780
Observe system behavior.

190
00:11:38,280 --> 00:11:41,394
The system should automatically
fail our two remaining requests.

191
00:11:41,894 --> 00:11:44,209
Two hypothesis driven experiments.

192
00:11:44,539 --> 00:11:45,979
Start with the hypothesis.

193
00:11:46,939 --> 00:11:52,759
If the cash cluster goes down, our a
PA should still serve requests from

194
00:11:52,759 --> 00:11:54,709
the database slower, but flex down.

195
00:11:55,639 --> 00:11:56,719
Kill the cache to install.

196
00:11:57,019 --> 00:11:57,499
Observe.

197
00:11:57,799 --> 00:11:59,659
Does the hypothesis hold?

198
00:12:00,109 --> 00:12:02,059
Can we handle it for how long?

199
00:12:02,559 --> 00:12:04,719
Gradual blast radius expansion.

200
00:12:05,219 --> 00:12:10,400
So we test in terms like in week
one, test a single no failure.

201
00:12:10,790 --> 00:12:13,910
Week two, test network
partition between two regions.

202
00:12:14,900 --> 00:12:19,280
Failure of multiple critical
services, week four, similar security.

203
00:12:19,280 --> 00:12:22,010
Instead detect and
block malicious traffic.

204
00:12:22,510 --> 00:12:23,980
Continuous learning and improvement.

205
00:12:24,430 --> 00:12:26,950
Every chaos experiment reveals gaps.

206
00:12:27,310 --> 00:12:31,819
Example, when cash failed database
was, we need better fix the

207
00:12:31,819 --> 00:12:34,025
gap, then rerun the experiment.

208
00:12:34,525 --> 00:12:39,329
Netflix runs chaos and they didn't
constantly, they pioneered this.

209
00:12:39,930 --> 00:12:42,930
They intentionally kill services in
production using chaos and chaos.

210
00:12:43,180 --> 00:12:44,649
Experiments and tools.

211
00:12:45,149 --> 00:12:51,199
Because of this, they're ultra resilient
and competitors who let discuss a four

212
00:12:51,799 --> 00:12:58,239
CAP Thera CP is means consistency,
availability, addition, tolerance.

213
00:12:58,929 --> 00:13:04,404
So the fundamental transcr
distributed systems must

214
00:13:04,404 --> 00:13:06,534
choose two or three properties.

215
00:13:07,034 --> 00:13:09,044
This is a hard law of distributor systems.

216
00:13:09,104 --> 00:13:10,274
You cannot have all three.

217
00:13:10,774 --> 00:13:13,534
Let's explain each product consistence.

218
00:13:14,034 --> 00:13:16,074
All nodes see the same data.

219
00:13:16,074 --> 00:13:21,834
Some example, you transfer
hundred from account to company.

220
00:13:22,554 --> 00:13:31,434
Consistency means before the transfer
A has thousand dollars and B has $400.

221
00:13:31,934 --> 00:13:32,894
After that.

222
00:13:33,394 --> 00:13:38,494
A has $900 and B has
$600 never in between.

223
00:13:38,994 --> 00:13:42,834
Every query returns the
most recent point available.

224
00:13:43,334 --> 00:13:45,554
Every request receives a response.

225
00:13:46,244 --> 00:13:49,169
Your system always
response, never returns.

226
00:13:49,609 --> 00:13:50,049
I don't know.

227
00:13:50,549 --> 00:13:51,989
Availability goes to.

228
00:13:52,489 --> 00:13:57,739
Example, 99.99% availability
is equal to max.

229
00:13:57,769 --> 00:14:00,199
43 seconds down one year.

230
00:14:00,699 --> 00:14:03,609
Partition tolerance system continues.

231
00:14:03,909 --> 00:14:05,379
Disparate network failures.

232
00:14:05,949 --> 00:14:10,119
Example, your data center
are split across continents.

233
00:14:10,509 --> 00:14:16,059
A fiber cuts, some operating, even
though the two hos contacting each other.

234
00:14:16,559 --> 00:14:18,239
The trade off in this practice.

235
00:14:18,959 --> 00:14:21,149
CA systems, it's very rare.

236
00:14:21,569 --> 00:14:26,549
It's like consistent place available,
but failure if there is a portion

237
00:14:27,119 --> 00:14:32,639
example, traditional single data center,
relational database, not fault to.

238
00:14:32,999 --> 00:14:35,794
So one failure is complete
locked each CP systems.

239
00:14:36,544 --> 00:14:40,924
Consistent place, partition
tolerance, but may not be available.

240
00:14:41,164 --> 00:14:44,914
Example, some financial systems
priorit test consistency.

241
00:14:45,094 --> 00:14:50,534
Money must never be wrong, so the
trade off is if a partition happens,

242
00:14:51,014 --> 00:14:55,959
stop serving request are rather than
serve inconsistent data, AP systems.

243
00:14:56,459 --> 00:14:59,999
Available plus partition tolerant,
but may not be consistent.

244
00:15:00,299 --> 00:15:06,559
Example, most modern web services like
Twitter straight up, is on partition.

245
00:15:06,679 --> 00:15:10,399
You might solve slightly scale
data, but service keeps funnel.

246
00:15:10,899 --> 00:15:14,859
So the enterprise guidances,
enterprise architecture typically

247
00:15:14,859 --> 00:15:16,539
prioritize partition orders.

248
00:15:16,749 --> 00:15:18,009
It's non negotiable.

249
00:15:18,819 --> 00:15:24,219
They balance consistency and availability
based on businesses for money.

250
00:15:24,279 --> 00:15:28,179
Choose CP Sacrifice, availability
for consistency for social.

251
00:15:28,179 --> 00:15:30,039
First choose AP availability.

252
00:15:30,069 --> 00:15:31,779
More important than consistency.

253
00:15:32,279 --> 00:15:36,449
So next is Observability, unified
Security and Performance Manual.

254
00:15:36,949 --> 00:15:40,999
So the core concept is you can't
manage what you can in your

255
00:15:40,999 --> 00:15:43,249
systems behavior through data.

256
00:15:44,134 --> 00:15:47,249
Four pillars of observability
methods S collection.

257
00:15:47,749 --> 00:15:51,199
So performance counts,
latency measurements.

258
00:15:51,699 --> 00:15:55,374
So example of these metrics
is CPU is costed 45%.

259
00:15:55,874 --> 00:15:57,119
Memory is 72%.

260
00:15:57,309 --> 00:16:04,214
Request C, like P 99 is goes to 200
milliseconds, which is 99% of requests

261
00:16:04,514 --> 00:16:06,284
completing to 50 milliseconds.

262
00:16:06,884 --> 00:16:10,094
Error rate is 0.02 percentage.

263
00:16:10,214 --> 00:16:12,734
That is one error top 5,000 request.

264
00:16:13,234 --> 00:16:18,089
This metrics are collect two seconds
using data tools like Prometheus,

265
00:16:18,094 --> 00:16:20,434
Datadog, new Relic Plot, et cetera.

266
00:16:20,934 --> 00:16:22,584
Second is centralized logging.

267
00:16:23,439 --> 00:16:26,829
Aggregated logs with
security event correlation.

268
00:16:27,329 --> 00:16:29,249
Every service generates logs.

269
00:16:29,849 --> 00:16:31,379
You collect them in one place.

270
00:16:31,879 --> 00:16:33,139
Security correlation.

271
00:16:33,229 --> 00:16:35,179
Did we see suspicious patterns?

272
00:16:35,299 --> 00:16:38,869
Multiple failed logins from
CNIP, et cetera, like that.

273
00:16:39,859 --> 00:16:43,459
Distributed across
microservices parameters.

274
00:16:44,329 --> 00:16:48,894
One customer, click Buy now request
goes through seven microservices.

275
00:16:49,004 --> 00:16:49,294
Okay.

276
00:16:49,794 --> 00:16:53,409
Shows order service to payment,
service to inventory service to

277
00:16:53,425 --> 00:16:59,415
fulfillment service, and each lecture
timing like order is 50 milliseconds.

278
00:16:59,444 --> 00:17:01,409
Payment is to 200 milliseconds.

279
00:17:01,639 --> 00:17:03,490
Inventory is goes two 30 milliseconds.

280
00:17:04,365 --> 00:17:08,744
Find bottlenecks while payment,
taking 200 milliseconds when

281
00:17:08,744 --> 00:17:10,574
it should be 50 milliseconds.

282
00:17:11,074 --> 00:17:15,064
Intelligent alert, anomaly
detection, combining security

283
00:17:15,064 --> 00:17:16,474
and reliability signals.

284
00:17:16,879 --> 00:17:19,939
Not just threshold, almost
like CPU greater than 80%.

285
00:17:20,299 --> 00:17:21,489
I it's not that like that.

286
00:17:21,849 --> 00:17:28,739
So detecting anomaly error rate should
only jump from 0.02% to file loss.

287
00:17:29,099 --> 00:17:30,449
So it can be an attack or attack.

288
00:17:31,409 --> 00:17:35,579
Then latency increase gradually
unusual geographic pattern.

289
00:17:36,079 --> 00:17:39,199
So it can be DDS from new country.

290
00:17:40,039 --> 00:17:42,049
So why this matters for resilience.

291
00:17:42,469 --> 00:17:46,639
When failure happens, observability,
let us detect 18 seconds.

292
00:17:46,639 --> 00:17:47,359
Not ours.

293
00:17:47,829 --> 00:17:49,059
Not just what happened.

294
00:17:49,299 --> 00:17:55,129
Volume first of maintain less customer
impact, data driven resilience analysis.

295
00:17:56,119 --> 00:17:59,539
So we can use mathematics to
predict and proven failures.

296
00:18:00,199 --> 00:18:02,754
Move from to data foundation model.

297
00:18:03,614 --> 00:18:05,564
There are two mathematical models.

298
00:18:06,134 --> 00:18:08,244
First one is mark change.

299
00:18:08,744 --> 00:18:14,444
Model system states and transition
positive bilities to predict failure

300
00:18:14,444 --> 00:18:16,244
scenarios and recovery parts.

301
00:18:16,744 --> 00:18:20,114
Example, the service status
will be status is healthy, where

302
00:18:20,114 --> 00:18:24,119
94% chance is L 5% chance face.

303
00:18:24,839 --> 00:18:30,514
State B, it's degraded here,
60% chance recovers, 40% chance

304
00:18:30,514 --> 00:18:31,714
where it fails comfortable.

305
00:18:32,584 --> 00:18:36,664
And stage C, it's a first level, a
hundred percent chance must recover.

306
00:18:37,594 --> 00:18:41,404
Use probable math to predict
if one service has 5% failure

307
00:18:41,644 --> 00:18:44,164
probability, that's the probability.

308
00:18:44,164 --> 00:18:44,404
Three.

309
00:18:44,404 --> 00:18:50,419
Services all fail simul, the
answer will be 0.01 to 5%.

310
00:18:50,569 --> 00:18:51,979
It's very extremely clear.

311
00:18:52,479 --> 00:18:55,109
Helps you understand where
whether non helps must.

312
00:18:55,609 --> 00:18:58,429
The second mathematical
model is ING theory.

313
00:18:58,929 --> 00:19:02,489
Request patterns, service
rates, and resource to optimize.

314
00:19:03,084 --> 00:19:08,639
Example, we have a current request
thousand per second, and the service

315
00:19:08,639 --> 00:19:15,194
capacity is 400 per second average rate
than is four millisecond calculated duty.

316
00:19:15,834 --> 00:19:20,799
Q not, but if it goes where it's,
so then Q will be built up and the.

317
00:19:21,749 --> 00:19:23,479
Hundred Medicinals and customer.

318
00:19:23,979 --> 00:19:29,654
So human theory per cells, you do at
least 1400 capacity to stay below 50

319
00:19:29,654 --> 00:19:35,909
millisecond Ancy, real example from
the metrics is, let's say system can

320
00:19:35,909 --> 00:19:39,359
handle 25,000 request for minimum.

321
00:19:40,139 --> 00:19:41,919
That's four because possible.

322
00:19:42,689 --> 00:19:43,379
Approximately.

323
00:19:44,339 --> 00:19:49,409
If traffic spikes at thousand requests
per second, you need to scale up

324
00:19:49,709 --> 00:19:55,449
queuing three models, request rate,
service media business model is

325
00:19:56,199 --> 00:20:00,939
these quality operations enable
objective co of resilience strategies.

326
00:20:00,969 --> 00:20:07,094
Example, she will spend 50 K dollars on
active readiness, or 30 K dollars on.

327
00:20:07,594 --> 00:20:14,615
Map will show like active, reduce downtime
by eight 15 plus hours per year and

328
00:20:14,645 --> 00:20:17,655
care delivery 30 hours per year there.

329
00:20:17,794 --> 00:20:23,585
So we reduce chaos, delivery, integration,
challenges in network elements.

330
00:20:24,085 --> 00:20:27,505
So the first challenge is
technology stack diversity.

331
00:20:28,005 --> 00:20:31,185
We have multiple languages
about some platforms requiring

332
00:20:31,185 --> 00:20:32,805
unified resilience supports.

333
00:20:33,305 --> 00:20:38,465
In the real scenario, the new application
will help different frameworks, like

334
00:20:38,525 --> 00:20:43,105
a payment service can be a Java Spring
Boot framework service will have Python

335
00:20:43,105 --> 00:20:46,199
or first A p. A mostly will be or not.

336
00:20:46,699 --> 00:20:49,229
Database will be posted to non et cetera.

337
00:20:49,499 --> 00:20:54,245
Message will be each has different
failure modes, modern tools,

338
00:20:54,575 --> 00:20:56,585
recovery problems, the trial.

339
00:20:56,585 --> 00:21:00,685
How do you implement unified
pattern opposed to all this.

340
00:21:01,435 --> 00:21:05,324
And the solution will be like
used boxes slash pattern.

341
00:21:05,824 --> 00:21:07,834
Example is service me.

342
00:21:08,014 --> 00:21:09,834
It had a circuit in the past.

343
00:21:10,334 --> 00:21:12,744
Challenge, legacy system constraints.

344
00:21:13,604 --> 00:21:17,304
All the components we
limited for capabilities.

345
00:21:17,794 --> 00:21:23,110
Ready, protective block example, you
have 20-year-old main front thermal

346
00:21:23,179 --> 00:21:26,070
pass contacts are rator logic.

347
00:21:26,570 --> 00:21:28,815
Contact distributed person.

348
00:21:29,055 --> 00:21:30,620
We can't do not auto.

349
00:21:31,120 --> 00:21:36,595
The solution will be wrapping gateway with
modern resilience layer like a PA gateway,

350
00:21:37,095 --> 00:21:40,895
replace sets of problem, challenge
three, security policy and forecast.

351
00:21:41,285 --> 00:21:45,685
And first, all consistent security
controls across desk, correct

352
00:21:45,925 --> 00:21:47,665
systems, boundaries, and trust.

353
00:21:48,165 --> 00:21:51,100
Challenge is how do we ensure
all services enforce encryption,

354
00:21:51,100 --> 00:21:52,659
authentication, audit log.

355
00:21:53,159 --> 00:21:55,425
So parent service might be work.

356
00:21:56,024 --> 00:21:57,705
I take service, might be API.

357
00:21:57,705 --> 00:21:59,939
Keys legacy system might have no help.

358
00:22:00,749 --> 00:22:02,969
The solution is implement security pulse.

359
00:22:03,469 --> 00:22:06,939
A P Gateway service Challenge
will be operational contracts,

360
00:22:07,479 --> 00:22:09,249
managing resilience mechanisms.

361
00:22:09,249 --> 00:22:13,999
We thought engineering teams,
when we implement secure circuit

362
00:22:13,999 --> 00:22:16,360
breakers, timeouts, redundancy.

363
00:22:16,719 --> 00:22:18,399
And chaos testing.

364
00:22:18,669 --> 00:22:20,544
Each team needs to
understand these concept.

365
00:22:21,414 --> 00:22:25,719
Each service needs configuration
treatment, and the problem is

366
00:22:25,749 --> 00:22:29,889
configuration will be sprawled,
inconsistency, and team will partner.

367
00:22:30,789 --> 00:22:35,589
The solution will be its standardized
platform service mesh managed government

368
00:22:35,589 --> 00:22:37,544
is will handle common credits.

369
00:22:38,044 --> 00:22:40,534
Resource optimization
without compromising related.

370
00:22:41,034 --> 00:22:46,619
So the, you need to scale for peak
profit example, 50,000 request per

371
00:22:47,079 --> 00:22:52,284
on platform, but you can't afford to
run a peak team 365 days per year.

372
00:22:52,824 --> 00:22:54,379
So solution will be optimized.

373
00:22:54,579 --> 00:22:56,454
Resource without optimizing related.

374
00:22:56,954 --> 00:22:58,514
First it's baseline measurement.

375
00:22:58,994 --> 00:23:01,664
Establish performance and cost benchmarks.

376
00:23:02,444 --> 00:23:08,444
So run for a non from collect below, like
average traffic, 2000 equals per second.

377
00:23:09,044 --> 00:23:13,874
Peak traffic, 8,000 equal per second, and
the cost will be 50,000 bucks per month.

378
00:23:14,414 --> 00:23:19,769
Available is 99.94%, and
that run rate is 0.0 people.

379
00:23:20,189 --> 00:23:22,049
This will be your best address.

380
00:23:22,549 --> 00:23:24,559
Second, identify inefficiencies.

381
00:23:24,739 --> 00:23:27,589
Look at all profit from or
under replace resources.

382
00:23:28,129 --> 00:23:32,479
Example in efficiencies will
be database has 16 CPU course,

383
00:23:32,479 --> 00:23:35,719
but it feeds a 30% utilization.

384
00:23:36,084 --> 00:23:41,074
This work provision, KPI, gateway,
has expensive certificates, but

385
00:23:41,424 --> 00:23:43,274
traffic only needs 20% records.

386
00:23:43,634 --> 00:23:47,859
Capacity caching layer of
its data too aggressive.

387
00:23:48,279 --> 00:23:51,069
Should use bigger cash opportunity.

388
00:23:51,129 --> 00:23:54,424
Reduce database to eight
course still handle three.

389
00:23:55,164 --> 00:24:02,519
Circuit data rightsizing possible
applying out safety at 60% CP utilization.

390
00:24:02,759 --> 00:24:04,499
Add one, use user server utilization.

391
00:24:04,999 --> 00:24:05,819
Remove one server.

392
00:24:06,319 --> 00:24:08,269
Example, three servers.

393
00:24:08,799 --> 00:24:12,154
Black Friday can scale up
to eight server on January.

394
00:24:12,214 --> 00:24:13,789
Again, you can scale it
back to three server.

395
00:24:14,289 --> 00:24:17,469
Safety scale to exact amount, right?

396
00:24:17,889 --> 00:24:19,629
Keep 20% metro four.

397
00:24:20,169 --> 00:24:22,029
Step four, continuous violation.

398
00:24:22,419 --> 00:24:27,159
Ensure optimizations maintain
s. S after optimizing measures

399
00:24:27,159 --> 00:24:32,279
is available still 99.94% if you
drop to maintenance for eight.

400
00:24:32,279 --> 00:24:38,139
Four of the optimization went too far to
reduce the optimization of is latency.

401
00:24:38,259 --> 00:24:40,509
P 99 still below two 50 Millis.

402
00:24:41,009 --> 00:24:47,570
Increase we find NCE is no capacity,
is error rate still 0.0 0.03%.

403
00:24:48,429 --> 00:24:50,129
IT 50 increase time.

404
00:24:50,729 --> 00:24:56,975
Progressive example, new conflict and
they maintains SLS and cut cost from

405
00:24:56,975 --> 00:24:59,164
50 gig dollars, 38 K dollars per month.

406
00:24:59,634 --> 00:25:03,379
This will be like 1 44, $3
per year will be served.

407
00:25:03,879 --> 00:25:05,560
So actionable resilience strategies.

408
00:25:06,399 --> 00:25:10,179
So we have five strategies and
we can try in depth strategy.

409
00:25:11,075 --> 00:25:15,820
So first strategies defense, and
layer multiple security and form.

410
00:25:15,820 --> 00:25:18,260
Others don't rely on single control.

411
00:25:18,470 --> 00:25:21,740
Example, layer one, network failure.

412
00:25:22,010 --> 00:25:26,530
It'll blocks weight limiting and
a layer three circuit breaker

413
00:25:27,040 --> 00:25:28,450
blocks, cascading failures.

414
00:25:29,005 --> 00:25:30,835
Layer four, re try with backup.

415
00:25:30,835 --> 00:25:37,215
So here we handle transient
failure, ance layer six, automatic

416
00:25:37,215 --> 00:25:42,855
filler so it can check our health
checks layer seven kio system.

417
00:25:43,455 --> 00:25:47,405
So this will validate all the
layers one to six in scenario

418
00:25:47,405 --> 00:25:49,864
if one other still theia.

419
00:25:50,600 --> 00:25:52,870
So start regular chaos.

420
00:25:53,370 --> 00:25:56,465
Example, we Q1 data system.

421
00:25:56,465 --> 00:26:01,465
Should we two similar network partition
regions system should operate and

422
00:26:01,465 --> 00:26:06,869
take to tenant now system should
and then similar security breach.

423
00:26:07,369 --> 00:26:11,859
Monthly incident postmortem
to learn from the RC strategy.

424
00:26:11,859 --> 00:26:12,069
Three.

425
00:26:12,519 --> 00:26:15,934
Automated recovery implement
self-healing patterns and balance.

426
00:26:16,719 --> 00:26:18,159
Self healing patterns.

427
00:26:18,399 --> 00:26:22,079
Examples are hard pressure
ES automatically restarted.

428
00:26:22,589 --> 00:26:25,729
Database slip, memory leak causes service.

429
00:26:25,729 --> 00:26:29,289
Restart automatically drain
all the connections around.

430
00:26:29,289 --> 00:26:31,689
Book examples if latency exists.

431
00:26:31,749 --> 00:26:32,469
One second.

432
00:26:33,069 --> 00:26:33,909
First, check.

433
00:26:33,909 --> 00:26:34,359
Cing.

434
00:26:34,779 --> 00:26:37,659
Second, scale up, part clear cash.

435
00:26:37,929 --> 00:26:41,694
Four trigger logs if error rate exceed 1%.

436
00:26:42,294 --> 00:26:44,784
First rollback, deploy, check locks.

437
00:26:45,084 --> 00:26:50,324
Then incident deploy strategy
for unified observability, common

438
00:26:50,324 --> 00:26:51,974
security and performance development.

439
00:26:52,754 --> 00:26:58,424
Example, we'll have dashboards where
for performance latency error, red

440
00:26:58,484 --> 00:27:02,284
through and for security log failures
are not raised by accessories.

441
00:27:02,914 --> 00:27:07,434
Malicious traffic s. For correlation
will be like error rates.

442
00:27:07,434 --> 00:27:09,264
Spike correlates with traffic.

443
00:27:09,264 --> 00:27:13,024
Spike for it can be videos unified.

444
00:27:13,024 --> 00:27:16,264
Alert, trigger alert or
combine signal security.

445
00:27:16,264 --> 00:27:17,484
Press related strategy.

446
00:27:17,594 --> 00:27:17,884
Five.

447
00:27:18,384 --> 00:27:19,534
Continuous improvement.

448
00:27:19,784 --> 00:27:23,534
Learn from incidents and media
basis after everyone's dead.

449
00:27:24,194 --> 00:27:26,264
Blameless post product
should be done like that.

450
00:27:26,774 --> 00:27:30,344
Not who failed, but why
failed and what failed.

451
00:27:30,404 --> 00:27:33,944
We need to check Example breaker.

452
00:27:33,944 --> 00:27:34,994
Didn't open fast enough.

453
00:27:35,144 --> 00:27:39,424
Let's registration and chaos
test will build security.

454
00:27:39,424 --> 00:27:40,924
Incident show we are missing buffer.

455
00:27:41,464 --> 00:27:44,569
Let's add it so that time to detect.

456
00:27:44,759 --> 00:27:46,529
Time to respond, time to recover.

457
00:27:47,499 --> 00:27:48,849
Improve each quarter.

458
00:27:49,349 --> 00:27:53,964
The key takeaway is take
resilience, engineering, integrate

459
00:27:53,964 --> 00:27:55,284
security and reliability.

460
00:27:55,784 --> 00:28:00,434
Fault tolerance mechanisms must work
in harmony with security controls.

461
00:28:01,094 --> 00:28:02,924
Don't build resilience and security.

462
00:28:03,229 --> 00:28:04,469
Security must work with

463
00:28:04,969 --> 00:28:07,439
issue, can be exploited as DDO.

464
00:28:08,219 --> 00:28:11,654
So keep opening and it'll keep
on opting since the survey.

465
00:28:12,154 --> 00:28:15,094
Monitor if circuit optic
pattern was suspicious.

466
00:28:15,454 --> 00:28:21,384
This bottom take observable enables
faster detection and response.

467
00:28:22,014 --> 00:28:22,254
Front.

468
00:28:22,254 --> 00:28:26,244
Monitoring of security and
performance signals accelerates

469
00:28:26,364 --> 00:28:31,919
incident solution with observable
data proper and 10 seconds 1230.

470
00:28:32,814 --> 00:28:38,715
Faster production is equal to faster
response equals to plus customer impact.

471
00:28:39,449 --> 00:28:41,699
Example without observability it.

472
00:28:41,699 --> 00:28:42,659
Lets start minutes.

473
00:28:42,659 --> 00:28:48,544
Exclude S renal surface down, and
with the 30 seconds, take three.

474
00:28:48,554 --> 00:28:52,854
Chaos ing, while its assumptions
before production and failure,

475
00:28:52,854 --> 00:28:55,134
injection levels, weaknesses, system.

476
00:28:56,134 --> 00:28:57,604
Then discover by months real.

477
00:28:58,104 --> 00:28:58,584
Discover them.

478
00:28:58,820 --> 00:28:59,270
Us.

479
00:28:59,715 --> 00:29:01,145
Every chaos experiment.

480
00:29:01,784 --> 00:29:03,889
A real incident example.

481
00:29:04,850 --> 00:29:08,510
Will data backup start
fix before it is matter?

482
00:29:09,080 --> 00:29:11,304
Don't wait till it becomes a real issue.

483
00:29:11,804 --> 00:29:16,695
Data driven models provide
objective, benchmark mathematical

484
00:29:16,695 --> 00:29:20,774
approaches enabled quantitative
comparison of resilience strategies.

485
00:29:21,524 --> 00:29:27,554
Use smart chains and QIC data to make
objective decisions instead of, I

486
00:29:27,554 --> 00:29:29,534
think we should scale to 10 servers.

487
00:29:29,864 --> 00:29:32,384
You can say MAP shows.

488
00:29:32,384 --> 00:29:35,519
We need eight to 10 servers
for this traffic pattern.

489
00:29:36,279 --> 00:29:38,979
Enable fact-based conversation
Will stakeholders.

