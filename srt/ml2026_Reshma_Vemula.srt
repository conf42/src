1
00:00:00,500 --> 00:00:05,180
Speaker 21: Hi, this is Ishma Mila,
particularly specialized in healthcare,

2
00:00:05,180 --> 00:00:09,230
data security, machine learning,
integration and regulatory compliance

3
00:00:09,230 --> 00:00:11,300
for electronic health record systems.

4
00:00:11,899 --> 00:00:16,129
Today I'll be talking about how PHI
is exposed during machine learning

5
00:00:16,129 --> 00:00:19,910
development and why development
environments are now one of the

6
00:00:19,910 --> 00:00:25,090
biggest security risks in healthcare
as machine learning adoption grows.

7
00:00:25,514 --> 00:00:29,595
Securing data across the entire
life cycles become critical,

8
00:00:29,654 --> 00:00:33,705
not just in production, but
everywhere where the data touches.

9
00:00:34,205 --> 00:00:38,855
Healthcare data breaches have increased
dramatically over the last few years.

10
00:00:39,215 --> 00:00:43,745
What's especially concerning is that
nearly half of these breaches happen

11
00:00:43,775 --> 00:00:45,665
outside of the production systems.

12
00:00:45,995 --> 00:00:50,825
At the same time, healthcare organization
process hundreds of thousands of

13
00:00:50,825 --> 00:00:56,735
patient records every day, which means
even a small vulnerability can expose

14
00:00:56,795 --> 00:00:58,985
massive amounts of sensitive data.

15
00:00:59,484 --> 00:01:04,704
When we think about PHI exposure, we
often focus on production systems, but

16
00:01:04,764 --> 00:01:10,074
in reality, production data is copied
into multiple environments, development,

17
00:01:10,104 --> 00:01:12,714
testing, training, validation, and UAT.

18
00:01:13,375 --> 00:01:17,185
Each of these environments
contains sensitive patient data and

19
00:01:17,185 --> 00:01:19,375
creates additional points of risk.

20
00:01:19,945 --> 00:01:24,675
Most healthcare organizations maintain, at
least three to five copies of production

21
00:01:24,675 --> 00:01:27,795
data across non-production workflows.

22
00:01:28,185 --> 00:01:33,195
Each copy contains dozens of
PHI elements per patient record.

23
00:01:33,735 --> 00:01:39,075
This data multiplication dramatically
increases the attack and makes

24
00:01:39,075 --> 00:01:42,765
machine learning pipelines much
harder to secure consistently.

25
00:01:43,265 --> 00:01:48,965
Data shows that development environments
are a primary target for attackers.

26
00:01:49,385 --> 00:01:52,985
More than half of reported
healthcare breaches occur outside

27
00:01:52,985 --> 00:01:57,305
of healthcare production systems
with an average of tens of thousands

28
00:01:57,305 --> 00:01:59,705
of records exposed per incident.

29
00:02:00,245 --> 00:02:04,445
These environments often lack
the same level of monitoring and

30
00:02:04,445 --> 00:02:06,245
controls as that of the production.

31
00:02:06,745 --> 00:02:13,435
So if you look at this slide, 51.3%
breaches occur in dev environments,

32
00:02:13,465 --> 00:02:15,595
which is like more than half.

33
00:02:16,095 --> 00:02:19,935
Using real patient data in
development and testing has

34
00:02:19,935 --> 00:02:22,275
serious financial consequences.

35
00:02:22,605 --> 00:02:28,185
Organizations that rely on real PHI
phase remediation costs that are

36
00:02:28,215 --> 00:02:32,505
roughly three times higher than those
that are using the synthetic data.

37
00:02:33,105 --> 00:02:36,375
These cost include
regulatory, penalities legal.

38
00:02:37,095 --> 00:02:41,084
Operational disruption and a
long time reputational damage.

39
00:02:41,625 --> 00:02:46,135
And you can see a pictorial
graph here in the slide.

40
00:02:46,635 --> 00:02:50,715
Moving on talking about the
advanced data masking solutions.

41
00:02:51,015 --> 00:02:54,945
Advanced data masking and
encryption significantly reduces

42
00:02:55,005 --> 00:02:59,265
unauthorized access while still
preserving the testing accuracy.

43
00:02:59,745 --> 00:03:04,965
Deterministic masking ensures
consistent results for validation and

44
00:03:04,965 --> 00:03:09,075
model performance testing, allowing
teams to maintain productivity

45
00:03:09,075 --> 00:03:11,190
without exposing real PHI.

46
00:03:11,690 --> 00:03:17,850
So talking about the modern healthcare
PHI 0.0 security frameworks they

47
00:03:17,850 --> 00:03:22,140
can integrate production across the
entire machine learning life cycle.

48
00:03:22,590 --> 00:03:26,670
Organizations adopting these
approaches see dramatic reduction

49
00:03:26,670 --> 00:03:32,060
in reductions in cyber attacks, data
exposure, and reportable complaints

50
00:03:32,060 --> 00:03:36,620
events by embedding security into
every environment rather than.

51
00:03:36,970 --> 00:03:38,800
Treating it as an afterthought.

52
00:03:39,300 --> 00:03:44,790
Dev security operation shifts
security left by embedding it

53
00:03:44,790 --> 00:03:46,770
directly into development workflows.

54
00:03:47,100 --> 00:03:51,510
Security scanning happens during
code writing, testing, and

55
00:03:51,510 --> 00:03:53,700
deployment rather than after release.

56
00:03:54,090 --> 00:03:54,480
Teams.

57
00:03:54,480 --> 00:03:59,280
Using this approach remediate
over 95% of vulnerabilities

58
00:03:59,880 --> 00:04:04,600
during development, dramatically
reducing the risk of PHI exposure.

59
00:04:05,100 --> 00:04:10,230
AI driven security monitoring
continuously analyzes activity across

60
00:04:10,230 --> 00:04:14,519
machine learning pipelines to detect
anomaly anomalies in real time.

61
00:04:15,449 --> 00:04:19,980
These systems significantly reduce
incident response times, allowing

62
00:04:20,010 --> 00:04:25,079
security teams to contain threats
before large scale PHI exposure occurs.

63
00:04:25,579 --> 00:04:29,359
This roadmap, whatever you see
here, the five step practical

64
00:04:29,359 --> 00:04:34,329
implementation roadmap, often
outlines a, realistic path forward.

65
00:04:34,629 --> 00:04:39,369
Start with synthetic data generation,
then implement encryption and access

66
00:04:39,369 --> 00:04:43,779
controls, secure machine learning
training pipelines, integrate

67
00:04:43,779 --> 00:04:48,729
dev security operation practices,
and finally automate compliance.

68
00:04:49,089 --> 00:04:54,164
These steps can be adapted incrementally
without disrupting innovation.

69
00:04:54,664 --> 00:05:00,215
Synthetic data generation strategy
plays a foundational role in secure

70
00:05:00,215 --> 00:05:01,504
machine learning development.

71
00:05:01,864 --> 00:05:06,664
It preserves patient privacy, maintains
the statistical properties needed for

72
00:05:06,664 --> 00:05:11,284
accurate models, and reduces breach
remediation costs significantly.

73
00:05:11,705 --> 00:05:15,694
This allows teams to innovate
safely and cost effectively.

74
00:05:16,194 --> 00:05:19,884
Security must be built into every
stage of the machine learning pipeline.

75
00:05:20,244 --> 00:05:25,674
From automated PHI detection at
ingestion to isolated and encrypted

76
00:05:25,674 --> 00:05:30,414
training environments to secure
validation and control deployment.

77
00:05:30,774 --> 00:05:34,129
Each phase includes continuous
monitoring and logging.

78
00:05:35,124 --> 00:05:39,714
End-to-end production ensures PHI
Remo remains secure through the

79
00:05:39,714 --> 00:05:41,874
entire machine learning lifecycle.

80
00:05:42,294 --> 00:05:47,484
Automated controls access, logging
and compliance checks work together to

81
00:05:47,484 --> 00:05:52,434
provide continuous assurance that data
remains protected as models evolve.

82
00:05:52,934 --> 00:05:55,694
Development environments are
one of the highest risk areas

83
00:05:55,694 --> 00:05:57,254
in healthcare security today.

84
00:05:57,674 --> 00:06:01,335
Synthetic data significantly
reduces cost and exposure.

85
00:06:01,934 --> 00:06:06,794
Modern security frameworks deliver
measurable results and dev security

86
00:06:06,794 --> 00:06:10,825
operations integration is essential
for building secure compliant machine

87
00:06:11,065 --> 00:06:12,424
learning systems in healthcare.

88
00:06:12,924 --> 00:06:15,804
These are all the key
takeaways from my presentation.

89
00:06:15,834 --> 00:06:18,894
Hope you all found this content
useful and informational.

90
00:06:18,954 --> 00:06:20,754
Thank you for giving me this opportunity.

