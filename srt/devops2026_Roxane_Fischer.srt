1
00:00:00,500 --> 00:00:01,310
Speaker 35: Hi everyone.

2
00:00:01,970 --> 00:00:06,690
I'm Roxanne Fisher CO and co-founder
of In Shift, and today I'm very excited

3
00:00:06,690 --> 00:00:12,510
about speaking about the importance of
context when it comes to your production

4
00:00:12,510 --> 00:00:15,899
systems and especially context.

5
00:00:15,989 --> 00:00:20,920
We, a graph knowledge graph when
you speak about infrastructure

6
00:00:20,920 --> 00:00:22,000
and production system.

7
00:00:22,660 --> 00:00:25,630
And happy to deep dive into it.

8
00:00:26,130 --> 00:00:30,920
First of all, I just want to put
on myself, I am one of the of any

9
00:00:30,920 --> 00:00:34,400
shift we build AI on call engineers.

10
00:00:35,000 --> 00:00:40,550
So we are going to streamline debugging
and find root cause analysis when you

11
00:00:40,550 --> 00:00:44,240
have like complex systems, production
systems between different sources of the

12
00:00:44,240 --> 00:00:48,740
data and we actually build this graph.

13
00:00:49,130 --> 00:00:52,520
It's context engineering for
your infrastructure and to them.

14
00:00:52,520 --> 00:00:58,010
I'm not exactly going to speak about
our product layer at any shift, but

15
00:00:58,010 --> 00:01:02,630
more about the tech that we create
underneath, which is this graph

16
00:01:03,130 --> 00:01:08,179
to illustrate the point and the
important of context and graph databases.

17
00:01:08,720 --> 00:01:13,399
I'm going to use a first example on how.

18
00:01:14,210 --> 00:01:18,020
The context and why it's so
important to have one when it

19
00:01:18,020 --> 00:01:19,039
comes to your infrastructure.

20
00:01:19,610 --> 00:01:24,630
The first one I'm going to take is about,
it's going to be about infrastructure as

21
00:01:24,630 --> 00:01:31,159
code generation and why it fail short when
you don't have the right context of all

22
00:01:31,279 --> 00:01:37,579
the last past years, models have become
extremely good at generic team code, but

23
00:01:37,579 --> 00:01:38,899
when it comes to infrastructure code.

24
00:01:39,399 --> 00:01:43,419
It often falls short or there
some limitation, and I'm

25
00:01:43,419 --> 00:01:45,249
going to try to explain why.

26
00:01:45,749 --> 00:01:51,299
The example I'm going to take here is
going to be very simple example for a new

27
00:01:51,299 --> 00:01:56,550
one with familiar with infrastructure and
for the one who are not, let me explain.

28
00:01:57,050 --> 00:01:59,360
When you deal with infrastructure
score, you're going to

29
00:01:59,360 --> 00:02:00,355
deploy infrastructure with.

30
00:02:01,249 --> 00:02:07,969
Code files and one of the framework
that is one of the most wellknown is

31
00:02:07,969 --> 00:02:13,625
star from a easy thing that you could
ask a general developer or like at

32
00:02:13,625 --> 00:02:18,905
least depending of like how sensitive
infrastructure is to create a VPC

33
00:02:19,055 --> 00:02:21,245
peering connection in Terraform.

34
00:02:21,745 --> 00:02:28,065
To actually create a peering between two
VPCs your own, your company one, and an

35
00:02:28,215 --> 00:02:32,055
external one, which could be something
that is done as code for Terraform.

36
00:02:32,555 --> 00:02:37,595
So let's see how it goes, depending
of how you try to complete this task.

37
00:02:38,095 --> 00:02:41,725
What I'm showing here would be a
piece of code, which could be written

38
00:02:41,725 --> 00:02:44,695
by a general developer, no ai.

39
00:02:45,625 --> 00:02:49,645
Simple kind of very use
case with bad practices.

40
00:02:50,215 --> 00:02:54,865
Where you can see here that the VP
C Ping between two VPCs is going

41
00:02:54,865 --> 00:02:56,815
to be out coded by the developer.

42
00:02:57,505 --> 00:03:02,065
It got through the AWS console or any
kind of console to different ideas

43
00:03:02,065 --> 00:03:03,445
and going to create the connection.

44
00:03:03,945 --> 00:03:08,415
This is very bad practice because
you are going to create values

45
00:03:08,745 --> 00:03:10,305
which can break dependencies.

46
00:03:11,265 --> 00:03:13,935
When your infrastructure
gets more and more complex.

47
00:03:14,435 --> 00:03:19,834
So now let's see how the same
task would've been done by an

48
00:03:19,834 --> 00:03:22,234
LLM, let's say GitHub copilot.

49
00:03:22,734 --> 00:03:27,910
What you can see here is that with
the latest changes and updates of lms.

50
00:03:28,410 --> 00:03:33,239
The quality of the outcome would be
pretty good, and GitHub copilot, for

51
00:03:33,239 --> 00:03:38,399
instance, will know that it doesn't
have, it cannot do accurate values,

52
00:03:38,399 --> 00:03:40,630
which should use dependencies.

53
00:03:41,130 --> 00:03:45,060
In this case, it would be the
best practice, but it misses the

54
00:03:45,510 --> 00:03:47,100
real value to make the connection.

55
00:03:47,600 --> 00:03:54,100
What GitHub copilot will try to do next
is to then take into account like those

56
00:03:54,100 --> 00:03:58,600
missing values and try to create them.

57
00:03:59,560 --> 00:04:05,170
Because it doesn't have the context of
what the idea of my VPC is or where it

58
00:04:05,260 --> 00:04:10,200
is defined in another repository, it is
going to try to create a new one, which is

59
00:04:10,200 --> 00:04:12,355
realistic in term of code, but completely.

60
00:04:12,855 --> 00:04:16,454
Not which is complete in term of
code, but completely not realistic.

61
00:04:16,954 --> 00:04:22,564
And in this case, because we didn't have
the context of my entire infrastructure,

62
00:04:22,564 --> 00:04:25,564
which can be huge, the LLM falls short.

63
00:04:26,064 --> 00:04:32,304
Now the question is, can cursor
or any ai, IDE can do it normally?

64
00:04:32,304 --> 00:04:32,934
Such Id.

65
00:04:33,434 --> 00:04:37,934
Take into account multiple repositories
and able to make those connections.

66
00:04:38,434 --> 00:04:43,024
In my case, this is what should
have been done by Cursor.

67
00:04:43,864 --> 00:04:49,504
I have created this new VPC Ping
connection and what I want is that

68
00:04:49,894 --> 00:04:56,314
I'm referencing for it viable to
another repository with my own VPC,

69
00:04:56,374 --> 00:04:58,224
with my and with the remote states.

70
00:04:58,724 --> 00:05:05,464
The thing is, if I come back to my earlier
example where my junior developer has

71
00:05:05,464 --> 00:05:10,384
created in the past, at the beginning
of my company some accurate values

72
00:05:10,884 --> 00:05:16,739
in this case, this value is only a
value that refers to a cloud resource.

73
00:05:17,369 --> 00:05:19,199
It doesn't refer to anything else.

74
00:05:19,699 --> 00:05:27,229
In that case, cursor cannot, in any case,
make the connection, the dependency link

75
00:05:27,559 --> 00:05:32,419
between this article value, which is
part of my code very often, or when I

76
00:05:32,419 --> 00:05:38,464
have like multiple integrated modules,
two, my remote PC in another repository.

77
00:05:38,964 --> 00:05:41,754
This is the case because cursor
cannot do the reconciliation

78
00:05:42,054 --> 00:05:43,584
between cloud data and code.

79
00:05:43,734 --> 00:05:44,934
It doesn't have the context.

80
00:05:45,434 --> 00:05:49,509
I am actually going to skip the
live demo and speak straight away

81
00:05:49,539 --> 00:05:54,429
about how we could get this context
through a live knowledge graph.

82
00:05:54,929 --> 00:05:58,899
First of all, just to give some
context a graph database is another

83
00:05:58,899 --> 00:06:02,534
structure database structure,
which is not the control.

84
00:06:03,384 --> 00:06:07,414
Which under a graph actually
can represent different identity

85
00:06:07,914 --> 00:06:09,894
entities for nodes and edges.

86
00:06:10,794 --> 00:06:14,424
In the case of your infrastructure,
it makes total sense because your

87
00:06:14,424 --> 00:06:19,554
infrastructure is a graph and your
nodes represents different entities.

88
00:06:20,184 --> 00:06:25,544
VPC, ims, per nodes, containers that
actually can be linked by different

89
00:06:25,544 --> 00:06:27,884
type of connection, for instance.

90
00:06:28,384 --> 00:06:32,354
An IM can be linked to an IS
two instance and an ECI two

91
00:06:32,354 --> 00:06:33,944
instance can belong to A VPC.

92
00:06:34,904 --> 00:06:44,204
So that would be a very easy and
small represe of your infrastructure

93
00:06:44,204 --> 00:06:49,814
as a graph that you can see here that
represent versus instance service

94
00:06:49,814 --> 00:06:52,394
type accounts, providers, et cetera.

95
00:06:53,024 --> 00:06:54,284
At the end, your infrastructure.

96
00:06:55,244 --> 00:06:59,054
Production systems is a huge monster,
which is actually a connection

97
00:06:59,054 --> 00:07:03,794
between your cloud providers,
Kubernetes containers, code bases,

98
00:07:04,124 --> 00:07:05,954
vis data, et cetera, et cetera.

99
00:07:06,454 --> 00:07:11,644
The more deep and rich this
knowledge graph is, the better.

100
00:07:12,529 --> 00:07:15,649
Imagine you have all the
metadata on top of it.

101
00:07:16,369 --> 00:07:20,839
So how is my life data
defined in my code base?

102
00:07:21,439 --> 00:07:21,979
Who owns it?

103
00:07:22,549 --> 00:07:23,689
Who did a commit on it?

104
00:07:24,559 --> 00:07:27,289
What is my metadata in
term of like timeline?

105
00:07:28,279 --> 00:07:35,839
Then I can ask with ai very deep question
such as which instances like EC2 instances

106
00:07:35,839 --> 00:07:41,989
or pods are experiencing high CPU usage
over the last 24 hours, and what are the

107
00:07:41,989 --> 00:07:44,029
dependencies or service dependencies?

108
00:07:44,529 --> 00:07:48,219
And you can go even more deep because
you have access to all this information

109
00:07:48,719 --> 00:07:49,594
in reality.

110
00:07:50,094 --> 00:07:53,994
Your graph, your infrastructure
as a graph is much more complex.

111
00:07:54,114 --> 00:07:59,154
So it's even like still a small snapshot
of what we do at any shift, which is

112
00:07:59,154 --> 00:08:03,384
like this representation of a graph of
infrastructure as a graph, and at the

113
00:08:03,384 --> 00:08:08,374
end it can be 10 of million of nodes when
you also take versioning into account to

114
00:08:08,374 --> 00:08:11,425
create this graph is complex how to do it.

115
00:08:11,925 --> 00:08:17,905
The first thing to create such context
would be to recreate this entire

116
00:08:17,905 --> 00:08:20,965
pipeline by using a graph database.

117
00:08:21,865 --> 00:08:24,505
We use Norfor internally at any shift.

118
00:08:24,865 --> 00:08:29,185
You can use, for instance, Neptune
on AWS or Ola kind of providers.

119
00:08:30,085 --> 00:08:33,085
Once you have decided on your
graph database as a technology,

120
00:08:33,505 --> 00:08:35,395
you need to define your ontology.

121
00:08:36,235 --> 00:08:40,885
Your ontology actually correspond to
how you're going to build this graph.

122
00:08:40,885 --> 00:08:44,785
In term of structure, what are going to be
the nodes, what are going to be the edges?

123
00:08:45,385 --> 00:08:49,105
It's a very complex work because
imagine you need to create a common

124
00:08:49,105 --> 00:08:51,235
ontology for different cloud providers.

125
00:08:51,775 --> 00:08:56,815
You need to have a similar representation
for A-W-S-G-C-P and Azure, so your AI

126
00:08:56,815 --> 00:09:01,765
will be able to treat them as equivalent,
even if the definition are not the same.

127
00:09:02,265 --> 00:09:03,855
Once you have created your ontology.

128
00:09:04,620 --> 00:09:06,120
We need to populate this graph.

129
00:09:06,900 --> 00:09:11,120
So to do each integration is
going to be slightly different.

130
00:09:11,660 --> 00:09:12,949
How do you scrub the data?

131
00:09:12,949 --> 00:09:15,199
How do you update the graph?

132
00:09:15,860 --> 00:09:18,410
And each of them can be taken separately.

133
00:09:18,829 --> 00:09:23,180
So for instance, for AWS, you can
connect to different APIs, do some scan

134
00:09:23,180 --> 00:09:27,710
of the resources, but also like use
cloud trail to have live updated data

135
00:09:27,800 --> 00:09:30,680
for like graph, which is super fresh.

136
00:09:31,180 --> 00:09:33,070
This is where the artwork comes from.

137
00:09:33,910 --> 00:09:36,910
Having the clean and structured
data is something that takes a lot

138
00:09:36,910 --> 00:09:41,350
of time, especially when you have
like multiple integration and source

139
00:09:41,350 --> 00:09:43,530
of information as written here.

140
00:09:43,619 --> 00:09:47,910
It's time heavy on how to maintain,
but gives particularly good context.

141
00:09:48,410 --> 00:09:54,170
The other solution, which is
also pretty good at some lower

142
00:09:54,170 --> 00:09:56,540
scales for startups and scales.

143
00:09:57,040 --> 00:10:03,130
To use Mt. P servers, MCP servers model
context protocols, actually very easy

144
00:10:03,130 --> 00:10:07,990
way to connect your LLM to data sources.

145
00:10:08,490 --> 00:10:16,210
S for instance, have amazing MCP servers
and by connecting your credentials.

146
00:10:16,780 --> 00:10:22,420
To those cps and using them within
your AI tools, AI agents, you can

147
00:10:22,570 --> 00:10:27,130
easily get some very useful context
to answer to the question that

148
00:10:27,130 --> 00:10:29,590
you want to have an answer to.

149
00:10:29,750 --> 00:10:31,760
Such as that, what is my life data?

150
00:10:32,180 --> 00:10:36,470
What are the resources that ai,
what are the resources that are like

151
00:10:36,470 --> 00:10:38,540
under like heavy load or usages?

152
00:10:39,040 --> 00:10:43,030
But at the end, this solution, which
is really straightforward, would be

153
00:10:43,030 --> 00:10:50,830
limited if infrastructure is too big
because an SP server is just an API and

154
00:10:51,070 --> 00:10:55,090
each time you make an API call, you can
just archive a limited amount of data.

155
00:10:55,660 --> 00:10:59,800
If you really want to understand in
one call, all the dependencies, all the

156
00:10:59,800 --> 00:11:04,900
relationship, all the history on one
specific request, you need first to have

157
00:11:04,950 --> 00:11:10,500
this huge cache, this huge database of
how infrastructure actually looks like.

158
00:11:11,000 --> 00:11:16,840
In this example, in this slide,
I've actually tried to show

159
00:11:16,870 --> 00:11:21,160
how you can build this graph on
a subset, on a small example.

160
00:11:21,660 --> 00:11:27,330
The example here is how to create a
mapping between your cloud data, let's

161
00:11:27,330 --> 00:11:32,490
say A-W-A-W-S, still, in this case,
your states and new Terraform code.

162
00:11:33,435 --> 00:11:37,995
The case here is to be able to understand
what has drifted in your infrastructure.

163
00:11:38,415 --> 00:11:43,055
So what has been correctly defined in
your code and what has not been, and

164
00:11:43,205 --> 00:11:47,755
which resources in your cloud can be
considered as shadow it or drifted.

165
00:11:48,255 --> 00:11:52,785
To do you need to create reconciliation
between the graph in your cloud.

166
00:11:52,845 --> 00:11:55,545
So what actually really
exists in your cloud?

167
00:11:56,265 --> 00:11:59,925
What has been defined in your
code and what is actually like

168
00:11:59,925 --> 00:12:03,555
this inter intermediate cache
layer in your Terraform state?

169
00:12:04,055 --> 00:12:06,845
To create this graph, you have two steps.

170
00:12:07,505 --> 00:12:10,415
First, you need to pass the data, and
then you need to populate the graph.

171
00:12:11,360 --> 00:12:17,720
So in the first case, you need to
pass the data for AWS for Terraform

172
00:12:17,720 --> 00:12:20,180
States and for your Terraform file.

173
00:12:20,790 --> 00:12:24,780
Using the different ideas to make
some reconciliation and to create

174
00:12:24,780 --> 00:12:26,310
some links between the entities.

175
00:12:26,850 --> 00:12:32,400
So you're going to use the same idea for
your cloud of sources in the Midwest.

176
00:12:32,735 --> 00:12:38,015
To the Terraform entity in your
Terraform States and then to the

177
00:12:38,465 --> 00:12:41,315
Terraform entity with its code
definition in the Terraform file.

178
00:12:41,815 --> 00:12:44,505
Once you have been able to
make this mapping, you need

179
00:12:44,505 --> 00:12:46,625
to to put it in the graph.

180
00:12:46,685 --> 00:12:52,214
You need to build the graph and be
able to update it live, but actually

181
00:12:52,334 --> 00:12:55,964
once the first step has been done,
the second one is much easier if you

182
00:12:55,969 --> 00:12:59,939
don't have to if the scale of your
infrastructure is not too big and if your

183
00:12:59,939 --> 00:13:02,819
amount of change is like quite limited.

184
00:13:03,319 --> 00:13:08,860
And actually that's the end in 15
minutes, which was the maximum.

185
00:13:09,360 --> 00:13:13,980
But, so in conclusion, what I want
to emphasize here is that when it

186
00:13:13,980 --> 00:13:17,490
comes to your production system, and I
wanna speak about context engineering.

187
00:13:18,195 --> 00:13:21,135
Context is key and it should
be represented as a graph.

188
00:13:21,885 --> 00:13:27,255
Your different services, if that
forms within your infrastructure

189
00:13:27,615 --> 00:13:32,295
and your application can and should
be represented as a graph which

190
00:13:32,295 --> 00:13:36,675
is way more dense in information
than just a relational database.

191
00:13:37,175 --> 00:13:43,060
The AI can be direct because we
really need, when we speak of AI

192
00:13:43,330 --> 00:13:47,290
to have this context to create some
quality code and quality debugging.

193
00:13:47,790 --> 00:13:48,780
And that's it for me.

194
00:13:49,050 --> 00:13:51,150
Thank you so much and have a great day.

