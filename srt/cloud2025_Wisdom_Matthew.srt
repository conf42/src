1
00:00:00,140 --> 00:00:04,680
Hey there, I'm Rizomatu, a backend
engineer, and today I'll be

2
00:00:04,680 --> 00:00:08,350
walking us through distributed
caching and queuing in the cloud.

3
00:00:08,780 --> 00:00:09,919
Let's get started.

4
00:00:10,419 --> 00:00:14,659
So for folks who are new to all
of this, a cache is basically

5
00:00:14,659 --> 00:00:16,219
a high speed storage layer.

6
00:00:16,700 --> 00:00:20,390
The, temporarily who frequently
assess data to improve

7
00:00:20,870 --> 00:00:22,790
performance and reduce latency.

8
00:00:23,200 --> 00:00:27,820
So instead of going to a slow source,
let's say for instance, a database

9
00:00:27,820 --> 00:00:32,110
file system or a standard API, every
time you request data that you know,

10
00:00:32,110 --> 00:00:36,310
it's frequently assessed, you can
basically store a copy of that data

11
00:00:37,240 --> 00:00:40,449
on a cache so that future requests
can be served and much faster.

12
00:00:40,950 --> 00:00:43,530
This is the, typical setup
for most applications, right?

13
00:00:44,305 --> 00:00:48,085
You have a client that makes a request
to the server, which then makes a

14
00:00:48,085 --> 00:00:53,745
request to a database to fetch data,
which is then sent back to the client.

15
00:00:54,184 --> 00:00:58,565
So this works just fine for most
applications, but on high traffic

16
00:00:58,565 --> 00:01:00,694
systems, this doesn't scale well.

17
00:01:01,194 --> 00:01:06,019
If it's not obvious already, the issue
with this kind of setup is that Over time,

18
00:01:06,019 --> 00:01:12,019
the database becomes a bottleneck as all
clients requests get to hit the database.

19
00:01:12,520 --> 00:01:16,029
This is, where we can leverage a
cache to scale this solution, right?

20
00:01:16,399 --> 00:01:20,429
So basically, instead of the server
reaching out to the database each time

21
00:01:20,449 --> 00:01:24,890
for data that we know it's, frequently
accessed, the server can basically

22
00:01:24,890 --> 00:01:28,300
just reach out to the cache to say,
hey, is this record in the cache?

23
00:01:28,340 --> 00:01:32,250
If the record is available in the
cache, The server just returns this

24
00:01:32,280 --> 00:01:33,900
back to the client from the cache.

25
00:01:34,220 --> 00:01:37,990
But in the case where the record isn't
available in the cache, the server makes

26
00:01:37,990 --> 00:01:43,200
a request to the database to fetch this
data, writes back to the cache, then

27
00:01:43,200 --> 00:01:47,069
return to the client so that future
requests can be retrieved from the cache.

28
00:01:47,290 --> 00:01:51,280
This way we can improve the,
Read performance for clients

29
00:01:51,380 --> 00:01:52,510
request to the server.

30
00:01:52,970 --> 00:01:57,250
So if you're wondering already why,
retrieving data from the cache is faster

31
00:01:57,250 --> 00:02:00,900
than retrieving from the database,
it's simply because, data on the

32
00:02:00,950 --> 00:02:06,399
cache is usually written to a faster
storage, like memory, unlike data on

33
00:02:06,399 --> 00:02:08,239
a database, which is written to disk.

34
00:02:08,980 --> 00:02:14,530
So most times, as the application scales
and grows with time, a single caching

35
00:02:14,540 --> 00:02:19,290
stance isn't enough to, manage the
load you have or your caching demand.

36
00:02:19,660 --> 00:02:21,690
This is where distributed
caching comes in.

37
00:02:21,700 --> 00:02:26,840
Basically, distributed caching spreads
cache data across multiple nodes, instead

38
00:02:26,840 --> 00:02:29,560
of storing it on a single node or machine.

39
00:02:29,920 --> 00:02:33,900
This ensures scalability, fault
tolerance, and prevents cache overload.

40
00:02:34,400 --> 00:02:37,000
There are different strategies
out there for distributed caching.

41
00:02:37,825 --> 00:02:41,225
We have sharding, replication,
load balancing, consistent

42
00:02:41,235 --> 00:02:43,275
hashing, and some others out there.

43
00:02:43,275 --> 00:02:47,855
But for now, I'll just focus on the
first three on this list because of time.

44
00:02:48,275 --> 00:02:52,235
consistent hashing is a bit complicated
and, might take us a lot of time.

45
00:02:52,735 --> 00:02:54,284
So what's sharding?

46
00:02:54,284 --> 00:02:56,375
Sharding is a data replication technique.

47
00:02:56,890 --> 00:03:00,740
That distributes cache data across
multiple nodes instead of storing

48
00:03:00,810 --> 00:03:04,940
everything on a single, machine in
a distributed caching environment,

49
00:03:05,520 --> 00:03:09,930
sharding ensures scalability and
efficient memory usage by dividing on

50
00:03:09,930 --> 00:03:12,360
the cache data across multiple instances.

51
00:03:12,790 --> 00:03:17,690
So let's say, on this, diagram, as you
can see, we have a cache router, which

52
00:03:17,690 --> 00:03:22,269
is basically responsible for assigning
a cache request to the right nodes.

53
00:03:22,320 --> 00:03:27,330
In this case, we have a range based on
sharding setup, which means that we assign

54
00:03:27,980 --> 00:03:30,350
requests to nodes based on the key range.

55
00:03:30,650 --> 00:03:34,070
For keys in range 1 to 1,
000, it's assigned to node A.

56
00:03:34,870 --> 00:03:38,320
1, 000 to 2, 000, it's
assigned to node B and so on.

57
00:03:38,769 --> 00:03:43,180
So basically what this does is that
the cache data is split across each

58
00:03:43,180 --> 00:03:47,140
of these nodes, saving only a range
of these values or a range of this

59
00:03:47,160 --> 00:03:50,450
key value pairs on a single node.

60
00:03:50,590 --> 00:03:54,880
This way we can scale out our caching
system to support, more memory

61
00:03:54,880 --> 00:03:56,940
space or storage space, basically.

62
00:03:57,269 --> 00:04:00,050
So let's say we have a constraint
of five gigabytes per node.

63
00:04:00,645 --> 00:04:06,265
By adding like two extra nodes, we're able
to, increase that limits to 15 gigabytes,

64
00:04:06,345 --> 00:04:08,765
which comes in very handy for scaling.

65
00:04:09,265 --> 00:04:10,845
Up next, we have replication.

66
00:04:11,205 --> 00:04:14,525
Replication is also a data
partitioning technique.

67
00:04:14,875 --> 00:04:18,495
Yes, that this duplicates cache
data across multiple nodes to ensure

68
00:04:18,495 --> 00:04:22,305
high availability, fault tolerance,
and improve read performance.

69
00:04:22,740 --> 00:04:26,510
It allows on cache systems
to continue functioning even

70
00:04:26,510 --> 00:04:27,740
if some of the nodes fail.

71
00:04:27,780 --> 00:04:32,240
So basically no, no node is
like a bottleneck in MOS.

72
00:04:32,700 --> 00:04:33,800
replication setup.

73
00:04:33,920 --> 00:04:39,290
So in this case, what we have on in this
diagram is a geo replication setup, right?

74
00:04:39,290 --> 00:04:43,750
So we have a cache router that
is responsible for assigning the

75
00:04:43,750 --> 00:04:47,510
right node to like assigning a
cache request to the right node.

76
00:04:47,960 --> 00:04:52,410
So here we have primary node
A, which is basically the node

77
00:04:52,410 --> 00:04:53,910
responsible for writes requests.

78
00:04:53,950 --> 00:04:56,760
This is a master, secondary setup.

79
00:04:56,760 --> 00:05:00,970
So basically the primary node is
responsible for writes requests and

80
00:05:01,470 --> 00:05:05,220
I'm synchronizing the data across the
secondary nodes while the secondary

81
00:05:05,220 --> 00:05:09,199
nodes are just there for read
request and optimizing read request.

82
00:05:09,200 --> 00:05:12,110
So in this case, we're trying
to optimize for latency, right?

83
00:05:12,110 --> 00:05:15,840
So what happens is that the
cache router assigns them a node

84
00:05:15,850 --> 00:05:17,540
to a cache request based on.

85
00:05:18,120 --> 00:05:22,180
A node that is closer to the, back
end or the user making the request.

86
00:05:22,180 --> 00:05:27,100
So if a user is making a request from
Africa, we just simply route the, cache

87
00:05:27,100 --> 00:05:29,180
request to the cache node in Africa.

88
00:05:29,250 --> 00:05:31,770
If they're making a request from
Europe, we do the same thing.

89
00:05:32,090 --> 00:05:36,970
So basically it is a way to improve
or reduce latency using, replication.

90
00:05:37,470 --> 00:05:39,150
Up next, we have load balancing.

91
00:05:39,550 --> 00:05:44,320
Load balancing evenly distributes cache
requests across multiple Cache nodes

92
00:05:44,320 --> 00:05:46,430
to prevent overloading a single node.

93
00:05:47,090 --> 00:05:50,830
This improves response time,
ensures high availability.

94
00:05:51,240 --> 00:05:56,030
It also ensures that no single cache
node becomes a bottleneck in the system.

95
00:05:56,430 --> 00:05:59,690
in this diagram we can see here
that we have a cache load balancer,

96
00:06:00,200 --> 00:06:03,340
which is responsible for routing
requests to the correct node.

97
00:06:03,845 --> 00:06:07,395
The data is then synchronized
across the other nodes, right?

98
00:06:07,425 --> 00:06:14,085
Let's say we write a request, we, add
a cache value to node A, the data gets

99
00:06:14,215 --> 00:06:18,405
saved to node A, then synchronized
to B and C just to ensure that if

100
00:06:18,485 --> 00:06:23,400
a read request is routed to Any of
the other nodes, the cache data will

101
00:06:23,400 --> 00:06:25,120
be available there for retrieval.

102
00:06:25,260 --> 00:06:29,990
So basically the way most, load
balancing systems work is that most

103
00:06:29,990 --> 00:06:32,940
of them use, there are different
algorithms out there, but a very

104
00:06:32,940 --> 00:06:34,530
popular one is round robin, right?

105
00:06:34,920 --> 00:06:37,650
Basically what the load balancer
does is that for each request,

106
00:06:37,650 --> 00:06:41,310
it just sends it from one node to
another and that kind of stuff.

107
00:06:41,340 --> 00:06:45,170
Let's say we have five requests to
our caching load balancer, right?

108
00:06:45,550 --> 00:06:49,710
The first request goes to node A,
the second one goes to node B, the

109
00:06:49,730 --> 00:06:54,760
third one node C, the fourth one goes
back again to node A, then, so forth.

110
00:06:54,760 --> 00:06:58,040
So that's like how round
robin works in a nutshell.

111
00:06:58,390 --> 00:07:02,640
Before I proceed, I would like
to mention that, the caching

112
00:07:02,650 --> 00:07:03,900
strategies I've mentioned.

113
00:07:04,535 --> 00:07:08,375
can either be implemented on the
application layer or they can be

114
00:07:08,375 --> 00:07:12,205
implemented on the caching system
or comes with the caching system.

115
00:07:12,675 --> 00:07:17,114
It's highly recommended to go with, a
caching strategy implemented on with

116
00:07:17,114 --> 00:07:18,684
the caching system we're using, right?

117
00:07:19,134 --> 00:07:23,984
If you're using Redis for instance, Redis
cluster has, sharding and replication out

118
00:07:24,034 --> 00:07:26,374
of the box, which means you don't have to.

119
00:07:26,809 --> 00:07:29,489
I'm going implement this by yourself,
you just have to configure your Redis

120
00:07:29,509 --> 00:07:34,219
cluster to replicate or share your
data based on your requirements.

121
00:07:34,719 --> 00:07:37,439
So these are some of the popular
caching systems out there.

122
00:07:37,999 --> 00:07:41,189
The most popular ones are
Redis and Memcached, right?

123
00:07:41,189 --> 00:07:45,459
So Redis is an open source,
in memory data store.

124
00:07:45,949 --> 00:07:51,539
It also offers, advanced data structures,
replication, and sharding, and is widely

125
00:07:51,539 --> 00:07:53,429
adopted for high performance caching.

126
00:07:53,459 --> 00:07:58,489
So Redis is quite popular when you, when
a lot of folks hear caching, the first

127
00:07:58,489 --> 00:07:59,959
thing that comes to their mind is Redis.

128
00:08:00,279 --> 00:08:03,979
Then we have Memcached, which is
basically a simple key value pair

129
00:08:03,979 --> 00:08:06,869
store that is, very fast for, lookups.

130
00:08:07,369 --> 00:08:12,989
Some cloud providers out there do offer,
elastic, do offer custom solutions, right?

131
00:08:12,989 --> 00:08:17,979
For instance, AWS has Amazon
Elastic Catch, Google Cloud has

132
00:08:17,979 --> 00:08:20,269
Memorystore, Azure has Catch for Redis.

133
00:08:20,539 --> 00:08:23,199
So basically these are just
similar with Redis Cloud.

134
00:08:23,579 --> 00:08:26,139
These are just fully managed solutions.

135
00:08:26,439 --> 00:08:30,609
So basically let's say you
use Amazon Elastic Catch.

136
00:08:30,639 --> 00:08:34,509
This means that you don't have to bother
about the little details of managing your

137
00:08:34,509 --> 00:08:38,049
Redis cluster or instance or whatever.

138
00:08:38,049 --> 00:08:40,929
So same thing with
Google Cloud Memorystore.

139
00:08:41,089 --> 00:08:43,059
This is managed like for you.

140
00:08:43,089 --> 00:08:47,099
You just have to configure how
you want to scale your caching

141
00:08:47,099 --> 00:08:48,979
system or your caching components.

142
00:08:49,549 --> 00:08:53,289
Also, alternatively, you can go with
the self hosted approach where you

143
00:08:53,919 --> 00:08:55,599
purchase a server online and run.

144
00:08:56,274 --> 00:08:56,654
Ready.

145
00:08:56,654 --> 00:09:00,214
So memcache DNAT and, connect
it to your backend system.

146
00:09:00,654 --> 00:09:04,254
Although this can be a bit painful
to manage, but, if this meets

147
00:09:04,254 --> 00:09:05,794
your need is also an option.

148
00:09:06,294 --> 00:09:07,204
So that is it for caching.

149
00:09:08,134 --> 00:09:08,934
what is a queue?

150
00:09:09,424 --> 00:09:13,194
A queue is a messaging system that
enables asynchronous communication

151
00:09:13,544 --> 00:09:18,174
between components by temporarily storing
messages until they are processed.

152
00:09:18,554 --> 00:09:22,444
most times in your application
or on your backend system, right?

153
00:09:22,914 --> 00:09:27,884
You have, different services that
needs to communicate asynchronously

154
00:09:27,884 --> 00:09:30,154
to maybe carry on different tasks.

155
00:09:30,164 --> 00:09:33,444
For instance, an example would
be, let's say you have, an

156
00:09:33,444 --> 00:09:35,444
e commerce website, right?

157
00:09:35,864 --> 00:09:39,214
Where you let users place an
order online and you send them.

158
00:09:40,099 --> 00:09:42,559
a receipt to their email
after placing an order.

159
00:09:42,899 --> 00:09:47,469
So basically, when a user sends a
request to place an order, you don't

160
00:09:47,589 --> 00:09:51,579
really want to send them a receipt
immediately as that can be computationally

161
00:09:51,579 --> 00:09:57,379
expensive to compile the receipts to
PDF and also send them a mail for this.

162
00:09:57,419 --> 00:10:02,099
So one thing you could do is, have
a separate component or separate

163
00:10:02,099 --> 00:10:06,069
service in charge of sending
mails or sending, PDFs or stuff.

164
00:10:06,464 --> 00:10:13,404
to users, then the, your main
backend system sends a message to

165
00:10:13,444 --> 00:10:18,554
the email components to compile
the mail and send to them.

166
00:10:18,554 --> 00:10:23,334
So this way you separate like
the processing, keeping things

167
00:10:23,334 --> 00:10:26,914
asynchronous and ensuring that
user requests are not delayed.

168
00:10:27,564 --> 00:10:31,494
Having them wait for you to compile
PDFs and emails before showing them a

169
00:10:31,494 --> 00:10:35,079
social message on your A PIO on a ui.

170
00:10:35,579 --> 00:10:39,194
in a typical event driven
setup, we have a producer.

171
00:10:39,734 --> 00:10:44,734
Which is responsible for publishing
events or text through a message queue.

172
00:10:45,174 --> 00:10:46,494
Then you have a message queue.

173
00:10:46,554 --> 00:10:50,744
Basically what a message queue does is
that it holds the tax on until there's

174
00:10:50,744 --> 00:10:53,054
an available consumer to dispatch it to.

175
00:10:53,594 --> 00:10:57,524
It's, most of them usually have a
first in first out approach, although

176
00:10:57,524 --> 00:11:00,164
this othering isn't always guaranteed.

177
00:11:00,164 --> 00:11:02,094
So just keep that in mind.

178
00:11:02,094 --> 00:11:03,564
And you have a consumer, which is.

179
00:11:04,064 --> 00:11:08,614
Basically a component or a system
that just picks up events from the

180
00:11:08,634 --> 00:11:12,624
message queue or receives events from
the message queue and works on them.

181
00:11:13,254 --> 00:11:17,144
And the example of the, e
commerce, receipts, example, right?

182
00:11:17,144 --> 00:11:22,194
So you have a producer that is the
maybe order service sends a message

183
00:11:22,234 --> 00:11:26,214
to the queue, then it gets consumed
by, your email or messaging service.

184
00:11:26,454 --> 00:11:28,664
Basically to send the user the receipts.

185
00:11:28,974 --> 00:11:33,134
the way most consumers work is that
they do, they subscribe to topics or

186
00:11:33,754 --> 00:11:37,554
events on the message queue saying,
okay, I'm interested in this topic.

187
00:11:37,554 --> 00:11:43,194
So when this kind of topics do come
across, it sends to the right consumer.

188
00:11:43,694 --> 00:11:50,414
As your, demand for your queuing
system grows, bottleneck, right?

189
00:11:50,474 --> 00:11:51,934
If all messages are routed.

190
00:11:52,444 --> 00:11:53,714
Through that, single queue.

191
00:11:54,104 --> 00:11:57,934
distributed queuing addresses these
challenges by spreading traffic

192
00:11:58,004 --> 00:11:59,854
across multiple queue partitions.

193
00:12:00,424 --> 00:12:04,244
This, allows for, high volume
asynchronous processing, and also

194
00:12:04,244 --> 00:12:07,674
enables multiple producers and
consumers to operate concurrently,

195
00:12:08,104 --> 00:12:12,074
ensuring scalability, resilience,
and, scale under high load basically.

196
00:12:12,574 --> 00:12:16,724
So this is, a quick diagram just
explaining, what you could expect or

197
00:12:16,724 --> 00:12:18,834
how distributed queuing works, right?

198
00:12:19,254 --> 00:12:21,734
So in this case, you
have multiple producers.

199
00:12:22,334 --> 00:12:26,274
Mind you, even in the previous
case of a single message queue,

200
00:12:26,294 --> 00:12:29,954
you can have multiple producers
and multiple consumers as well.

201
00:12:29,954 --> 00:12:30,904
This doesn't stop them.

202
00:12:31,224 --> 00:12:35,624
So the main important thing here, is
the broker or load balancer, right?

203
00:12:35,754 --> 00:12:37,764
This part of the system is.

204
00:12:38,279 --> 00:12:43,519
It's basically responsible for taking
events from the producers and routing

205
00:12:43,519 --> 00:12:45,559
it to the right rights and partition.

206
00:12:46,049 --> 00:12:47,969
In this case, we have
six partitions, right?

207
00:12:47,999 --> 00:12:53,299
Basically, this is a, skilled solution
saying, okay, we have this number of

208
00:12:53,319 --> 00:12:58,159
partitions to accept as much messages
from the providers as we need, right?

209
00:12:58,479 --> 00:13:01,439
So if the demand grows,
you simply just add.

210
00:13:01,844 --> 00:13:07,614
More partitions to the broker and
the broker is in charge of allocating

211
00:13:07,614 --> 00:13:12,404
which events get sent to a particular
partition and, determining how it

212
00:13:12,414 --> 00:13:14,844
gets sent to a consumer as well.

213
00:13:15,264 --> 00:13:19,594
So this way we can, scale our, queuing
components as much as we want by

214
00:13:19,594 --> 00:13:23,844
simply just adding more partition or
removing partition if the load drops.

215
00:13:24,344 --> 00:13:27,084
These are some of the popular
queuing systems out there.

216
00:13:27,220 --> 00:13:32,269
We have Apache Kafka, which is a
high throughput distributed streaming

217
00:13:32,319 --> 00:13:37,259
platform designed for real time event
streaming, and it's widely adopted for

218
00:13:37,259 --> 00:13:39,479
its scalability and fault tolerance.

219
00:13:39,779 --> 00:13:43,909
Then you also have RabbitMQ, which
is quite mature as well and, supports

220
00:13:43,909 --> 00:13:46,744
complex routing and different protocols.

221
00:13:47,284 --> 00:13:50,294
It's it's a very popular
queuing system as well.

222
00:13:50,594 --> 00:13:53,644
Then you have NATS, which is a
Lightweight High Performance Messaging

223
00:13:53,644 --> 00:13:58,944
System that, it's quite popular in
cloud native applications as it offers

224
00:13:59,004 --> 00:14:00,874
some simplicity and low latency.

225
00:14:01,304 --> 00:14:05,694
I would like to mention that The
distributed, queuing strategy or,

226
00:14:05,744 --> 00:14:09,194
managing partitions is usually
done by the broker, which in this

227
00:14:09,194 --> 00:14:11,694
case is RabbitMQ or Apache Kafka.

228
00:14:11,694 --> 00:14:15,204
So most times you don't have
to go into the little details

229
00:14:15,204 --> 00:14:17,154
of managing this by yourself.

230
00:14:17,204 --> 00:14:20,174
But in a worst case scenario where
this don't meet your need, you

231
00:14:20,174 --> 00:14:23,334
can simply, implement this on the
application layer or create a service.

232
00:14:24,179 --> 00:14:27,189
for managing like your
distributed, queuing strategy.

233
00:14:27,189 --> 00:14:31,509
So it's up to you, but it's best
to go with the strategy or the

234
00:14:31,519 --> 00:14:36,159
implementation that comes with
your broker or queue load balancer.

235
00:14:36,659 --> 00:14:40,259
Different cloud providers do
offer a queuing service as well.

236
00:14:40,269 --> 00:14:41,229
AWS has.

237
00:14:41,729 --> 00:14:44,429
Amazon SQS, Google Cloud has pops up.

238
00:14:44,579 --> 00:14:46,409
Then Azure has service bot.

239
00:14:46,859 --> 00:14:53,989
Alternatively too, you could run,
Kafka or RabbitMQ or nuts on a

240
00:14:53,989 --> 00:14:57,499
server you purchased online and
connect your application through it.

241
00:14:57,889 --> 00:15:01,029
And yeah, you could have that
Azure, although like similarly

242
00:15:01,029 --> 00:15:02,664
with the cashing, I'm self hosting.

243
00:15:03,459 --> 00:15:07,339
This could be a pain to manage and
scale, but this is also an option

244
00:15:07,819 --> 00:15:09,639
in case Army meets your needs.

245
00:15:10,619 --> 00:15:12,199
Thank you for sticking around.

246
00:15:12,309 --> 00:15:16,199
If you enjoyed this talk, feel
free to reach out to me on

247
00:15:16,209 --> 00:15:17,239
either LinkedIn or Twitter.

