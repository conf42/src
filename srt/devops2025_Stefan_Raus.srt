1
00:00:00,300 --> 00:00:05,010
Hello, everyone, and thank you for joining
me today at COM 42 DevOps conference.

2
00:00:05,580 --> 00:00:10,369
My name is Stefan and I work as
DevOps engineer at Analog Devices.

3
00:00:11,300 --> 00:00:14,970
I'm excited to present you today an
automated way of testing in hardware

4
00:00:15,100 --> 00:00:19,310
across a wide range of platforms that
run different Linux distributions.

5
00:00:19,810 --> 00:00:23,660
System level testing plays an
essential role for quality assurance

6
00:00:23,710 --> 00:00:27,670
in the fast evolving landscape of
software and hardware integration.

7
00:00:27,870 --> 00:00:33,170
A robust testing infrastructure is
required to ensure that individual

8
00:00:33,180 --> 00:00:35,329
software components work together.

9
00:00:35,829 --> 00:00:39,250
Key attributes like automation,
rigorous testing, and coverage

10
00:00:39,550 --> 00:00:43,339
are some of the must haves for
achieving a reliable integration.

11
00:00:43,589 --> 00:00:48,665
By implementing these best practices,
Organizations can deliver high quality

12
00:00:48,665 --> 00:00:51,004
systems that meet user expectations.

13
00:00:51,264 --> 00:00:55,604
A reliable test infrastructure
accelerates also test executions, but

14
00:00:55,604 --> 00:00:59,975
most important, reduce the probability
of finding bugs in production phase.

15
00:01:00,475 --> 00:01:03,964
The Linux distribution I talked about
is called Analog Devices Kuiper and it

16
00:01:03,964 --> 00:01:08,104
is a free open source distro customized
for analog devices signal chain.

17
00:01:08,604 --> 00:01:12,595
It comes pre equipped with essential
components like device drivers, pre

18
00:01:12,595 --> 00:01:17,634
built boot files or FPGA, and Raspberry
Pi based solutions, as well as a

19
00:01:17,634 --> 00:01:21,365
wide range of development utilities,
libraries, and project examples.

20
00:01:22,255 --> 00:01:27,654
The system supports multiple hardware
platforms, AMD and Intel FPGA based

21
00:01:27,654 --> 00:01:30,445
platforms, Raspberry Pi, and also NXP.

22
00:01:31,235 --> 00:01:34,124
To streamline software management,
KyperLinux incorporates a

23
00:01:34,124 --> 00:01:35,645
custom Linux package repository.

24
00:01:36,145 --> 00:01:39,045
Simplifying software components
installation or update.

25
00:01:39,595 --> 00:01:44,425
This is included by default in the
image ensuring easy of use for both

26
00:01:44,465 --> 00:01:46,165
testing and production environments.

27
00:01:46,665 --> 00:01:51,305
For more details regarding how Kyper
Linux releases got optimized, please

28
00:01:51,305 --> 00:01:56,205
check Refining the release strategy
of a custom Linux distro that is also

29
00:01:56,205 --> 00:02:00,535
presented at this edition of Comfortitude
DevOps by one of my colleagues, Andrea.

30
00:02:01,035 --> 00:02:05,375
The continuous integration flow for
individual software components is handled

31
00:02:05,375 --> 00:02:10,225
using classic CI tools, such as Jenkins,
ErrorPipeline, or GitHub Actions.

32
00:02:10,495 --> 00:02:13,815
The CI builds software components
across various operating systems

33
00:02:13,844 --> 00:02:17,555
and the resulting output binaries,
which might be Linux packages,

34
00:02:17,555 --> 00:02:21,965
Windows installers, or just archives,
are stored in GitHub releases,

35
00:02:22,035 --> 00:02:24,025
package manager, or internal server.

36
00:02:24,935 --> 00:02:27,785
This streamlined approach not
only improves efficiency, but

37
00:02:27,795 --> 00:02:32,775
also ensures traceability, version
control, and artifacts accessibility,

38
00:02:33,195 --> 00:02:37,115
making it easier to integrate and
deploy them into larger systems.

39
00:02:37,615 --> 00:02:41,255
The testing process in hardware
closely mirrors the one in software.

40
00:02:42,115 --> 00:02:45,295
The workflow typically begins
with specific boot files being

41
00:02:45,315 --> 00:02:48,575
written onto the hardware boards
to configure them for testing.

42
00:02:49,515 --> 00:02:52,265
Tests are executed in parallel
across multiple hardware setups.

43
00:02:52,765 --> 00:02:55,545
Reducing overall testing time
and increasing efficiency.

44
00:02:56,035 --> 00:03:00,335
Because popular testing framework
available in the market even are designed

45
00:03:00,345 --> 00:03:04,765
for application level testing or are
specific to a single hardware platform

46
00:03:05,015 --> 00:03:09,525
we worked on creating our own testing
framework called Hardware Test Harness.

47
00:03:09,755 --> 00:03:13,485
It is designed to unify testing across
a wide range of hardware platforms

48
00:03:13,895 --> 00:03:16,515
enabling consistent execution
regardless of the hardware type.

49
00:03:17,015 --> 00:03:19,925
Builds are triggered by pushes
or pull requests in GitHub.

50
00:03:20,205 --> 00:03:25,505
Multiple repositories are monitored like
HDL, Linux, libraries or applications.

51
00:03:25,905 --> 00:03:29,895
And changes from those will
trigger the entire testing process.

52
00:03:30,015 --> 00:03:33,555
Let's see what happens
after the build passes.

53
00:03:33,995 --> 00:03:38,985
The results binaries are saved on internal
servers, in this case JFrog Artifactory.

54
00:03:39,515 --> 00:03:41,235
And then the main
Jenkins job is triggered.

55
00:03:42,165 --> 00:03:45,815
The Jenkins job begins by downloading
the binaries from Artifactory and

56
00:03:45,815 --> 00:03:49,005
writing them onto the hardware
platforms connected to each agent.

57
00:03:49,745 --> 00:03:53,445
After this, the automated tests
are distributed and executed.

58
00:03:53,845 --> 00:03:58,055
The tests are mostly written in
Python and have specific tags that

59
00:03:58,075 --> 00:03:59,595
indicate hardware compatibility.

60
00:04:00,095 --> 00:04:04,285
Let's see now why do you think we need
an intermediate Artifactory server.

61
00:04:05,055 --> 00:04:07,845
this server is required
for several reasons.

62
00:04:08,345 --> 00:04:12,495
The main one is that the hardware
test harness is distributed across

63
00:04:12,515 --> 00:04:16,165
multiple physical locations, requiring
a centralized system to share binaries.

64
00:04:16,665 --> 00:04:18,985
Another reason is to handle concurrency.

65
00:04:19,525 --> 00:04:23,435
Since multiple repositories are
implied, this will act as a bumper.

66
00:04:23,935 --> 00:04:27,725
And it also ensures better organization
and versioning of binaries,

67
00:04:27,755 --> 00:04:29,415
making the process more scalable.

68
00:04:29,915 --> 00:04:33,725
Another question may be why the Jenkins
agents need to be physical machines.

69
00:04:34,225 --> 00:04:38,175
The primary reason here is that
these agents directly control the

70
00:04:38,175 --> 00:04:43,155
hardware setups, including power
management, WART connections, Ethernet,

71
00:04:43,215 --> 00:04:45,435
and in some cases JTAG interface.

72
00:04:45,935 --> 00:04:51,085
Some of the build servers are also
physical machines, because builds require

73
00:04:51,085 --> 00:04:57,525
significant computational resources or
paid licenses, as Xilinx or Matlab cases.

74
00:04:58,025 --> 00:05:00,345
A good framework should be
adaptable to different types of

75
00:05:00,345 --> 00:05:02,065
hardware and testing scenarios.

76
00:05:02,295 --> 00:05:05,835
It should be modular enough to
accommodate changes, such as adding

77
00:05:05,855 --> 00:05:08,175
new DUTs or modifying test cases.

78
00:05:08,765 --> 00:05:13,255
Because there are implied multiple
repositories and multiple Jenkins agents,

79
00:05:14,175 --> 00:05:17,965
there is necessary a test manager to
queue, distribute, and execute tests.

80
00:05:18,395 --> 00:05:21,325
But let's go first through
some implementation details.

81
00:05:21,825 --> 00:05:24,980
As main tool, there was used Jenkins.

82
00:05:25,890 --> 00:05:30,420
It can be hosted independently without
relying on external servers, integrates

83
00:05:30,420 --> 00:05:36,070
easily with tools like GitHub or
Artifactory, and benefits from a

84
00:05:36,070 --> 00:05:37,990
large online community for support.

85
00:05:38,500 --> 00:05:43,820
Additionally, features as Jenkins share
libraries, dynamic script language, and

86
00:05:43,830 --> 00:05:47,900
resource logging mechanism proved to
be extremely useful in this context.

87
00:05:48,400 --> 00:05:50,300
Another tool used is Nebula.

88
00:05:50,865 --> 00:05:53,925
This tool was developed by us
and consists in a collection of

89
00:05:54,005 --> 00:05:57,825
Python scripts that manage hardware
connections, such as sending word

90
00:05:57,885 --> 00:06:02,805
commands, configuring Ethernet IPs,
sending files through SSH and so on.

91
00:06:03,545 --> 00:06:07,485
In the event of a hardware setup failure,
we can physically reboot the system

92
00:06:07,725 --> 00:06:13,095
and bring it back online using Power
Distribution Unit and USB SD card mobs.

93
00:06:13,685 --> 00:06:16,815
Both of them are also controlled
through Python by Nebula.

94
00:06:17,315 --> 00:06:21,065
As we added more harder setups,
we needed a tool to keep tracking

95
00:06:21,555 --> 00:06:23,565
of all of the devices under test.

96
00:06:24,275 --> 00:06:25,935
And that's where Netbox comes in.

97
00:06:26,435 --> 00:06:29,615
It is a free open source tool,
originally designed for modeling

98
00:06:29,615 --> 00:06:33,745
and documenting network racks,
but it also fits in our use case.

99
00:06:34,475 --> 00:06:39,235
We use it to generate Nebola configuration
files, YAMLs that contain information

100
00:06:39,235 --> 00:06:43,735
about each DUT, such as platform,
the board that is plugged into it.

101
00:06:44,235 --> 00:06:48,025
Ethernet and serial addresses,
PDU outlet and USB connections.

102
00:06:48,575 --> 00:06:52,655
The NetBox configuration needs to
be updated only when new duties

103
00:06:52,655 --> 00:06:55,445
are added, removed or rearranged.

104
00:06:56,405 --> 00:07:01,595
All data stored in NetBox is backed up
automatically in Artifactory and it can

105
00:07:01,595 --> 00:07:03,695
be restored if something goes wrong.

106
00:07:04,195 --> 00:07:04,565
Jenkins.

107
00:07:04,565 --> 00:07:07,535
JAR library is a very good
way to centralize groupy code.

108
00:07:07,535 --> 00:07:10,200
https: and reuse it in
multiple Jenkins pipelines.

109
00:07:10,200 --> 00:07:14,890
It contains definition for common
functions and pipeline steps that can be

110
00:07:14,900 --> 00:07:17,010
shared across different Jenkins files.

111
00:07:17,710 --> 00:07:21,220
Improving in this way reusability,
modularity, and consistency.

112
00:07:21,900 --> 00:07:28,400
We use it in multiple pipeline stages
to update agent tools as Nebula, to

113
00:07:28,400 --> 00:07:32,380
send files to hardware setups, or
to run tests and collect results.

114
00:07:33,040 --> 00:07:37,050
this structured approach ensures
an efficient process of updating

115
00:07:37,230 --> 00:07:40,110
and maintaining the same
pipeline functionality across

116
00:07:40,140 --> 00:07:42,380
all the test harness instances.

117
00:07:42,880 --> 00:07:45,880
By combining continuous integration
with continuous testing, the

118
00:07:45,880 --> 00:07:48,090
resulting diagram will look like this.

119
00:07:48,590 --> 00:07:53,909
Behind it are over 100 CI pipelines
implied, implemented in Azure,

120
00:07:53,910 --> 00:07:58,970
GitHub Actions, or Jenkins, and
about 15 physical build servers.

121
00:07:59,595 --> 00:08:03,865
For most of them, besides build status,
the test result from hardware testing

122
00:08:03,865 --> 00:08:05,775
is returned back to github pull request.

123
00:08:06,275 --> 00:08:09,045
Some software components,
such as libraries, are tested

124
00:08:09,155 --> 00:08:10,455
individually on hardware.

125
00:08:11,025 --> 00:08:14,475
If all of the test passes, the
corresponding binaries are stored in

126
00:08:14,475 --> 00:08:18,084
artifactory or Linux package repository.

127
00:08:18,085 --> 00:08:21,985
For other components, the build
artifacts are first stored on internal

128
00:08:22,295 --> 00:08:24,265
servers and tested afterwards.

129
00:08:24,765 --> 00:08:28,855
In some cases, Linux packages are
created automatically at each push

130
00:08:29,015 --> 00:08:31,075
and saved into the package repository.

131
00:08:31,725 --> 00:08:35,655
Ideally, once all the software
components are packed as Linux

132
00:08:35,655 --> 00:08:40,685
packages, those packages generated
by each CI run will be uploaded

133
00:08:40,685 --> 00:08:42,585
automatically under testing environment.

134
00:08:42,945 --> 00:08:46,735
So they can be installed on Kyper
for further testing or just used

135
00:08:46,735 --> 00:08:48,735
internally as pre released versions.

136
00:08:49,235 --> 00:08:51,655
On the other side, whenever
there are changes in the Kyport

137
00:08:51,655 --> 00:08:55,975
sources, new Docker containers are
created, being used by other CIs.

138
00:08:56,275 --> 00:09:00,565
This ensures that everything is
consistently built and validated

139
00:09:00,685 --> 00:09:02,235
across multiple environments.

140
00:09:02,735 --> 00:09:06,225
Let's see how results
visualization was handled.

141
00:09:06,945 --> 00:09:10,785
The easiest way was to manually
verify status of Jenkins pipelines.

142
00:09:10,855 --> 00:09:14,665
Of course, this method didn't give
us any details about what stages

143
00:09:15,310 --> 00:09:17,700
Failed, and on which hardware setup.

144
00:09:18,200 --> 00:09:20,270
So we switch to BlueOceanView.

145
00:09:21,140 --> 00:09:22,120
It looks a bit better.

146
00:09:22,310 --> 00:09:27,070
For those of you who don't know, the
BlueOcean is a Jenkins plugin that offers

147
00:09:27,160 --> 00:09:29,360
a good visualization of parallel stages.

148
00:09:29,360 --> 00:09:33,890
In this case, we could see the
status of all the stages on all

149
00:09:33,890 --> 00:09:37,700
the hardware setups, becoming a bit
easier to know exactly what failed.

150
00:09:38,200 --> 00:09:39,910
I still couldn't see all the details.

151
00:09:39,910 --> 00:09:45,320
So next step was to convert results
into XML format and use JUnit

152
00:09:45,330 --> 00:09:47,360
Jenkins plugin for visualization.

153
00:09:47,860 --> 00:09:50,700
Even this method requires
logging into Jenkins and visually

154
00:09:50,710 --> 00:09:52,350
inspect the results at every run.

155
00:09:52,850 --> 00:09:55,820
So we have started to use
even more powerful tools.

156
00:09:56,220 --> 00:10:00,280
Logstash for processing results,
Elasticsearch for storing them into

157
00:10:00,280 --> 00:10:02,550
database and Kibana for generating graphs.

158
00:10:03,060 --> 00:10:07,760
At this point, we also created a web
page with multiple dashboards and added

159
00:10:07,760 --> 00:10:10,060
the ability to create and apply filters.

160
00:10:10,400 --> 00:10:13,680
The new implementation eliminates
the need of going through individual

161
00:10:13,940 --> 00:10:15,880
artifacts to check results.

162
00:10:16,870 --> 00:10:20,990
But developers were still needed to
look over the graphs to check the

163
00:10:21,000 --> 00:10:22,750
status before pushing their changes.

164
00:10:23,250 --> 00:10:26,450
Actually in all of the above
cases, developers were still needed

165
00:10:26,470 --> 00:10:28,779
to manually check the results.

166
00:10:29,210 --> 00:10:34,380
Even if the results were shown in tables,
graphs, or dashboards, it was not feasible

167
00:10:34,400 --> 00:10:38,410
to handle a big number of repositories
and pull requests in this way, so we

168
00:10:38,410 --> 00:10:41,030
needed to close the loop completely.

169
00:10:41,530 --> 00:10:45,030
An important step in the implementation
was to bind hardware test results

170
00:10:45,030 --> 00:10:46,190
back to GitHub pull requests.

171
00:10:46,190 --> 00:10:50,090
The main challenge here was to ensure
that private data from our internal

172
00:10:50,350 --> 00:10:55,700
build and test environment, such as
internal IPs, Jenkins links, or any

173
00:10:55,700 --> 00:11:00,210
other sensitive information, to not
be exposed in public repositories.

174
00:11:00,710 --> 00:11:03,830
At the same time, it was very
important to provide sufficient

175
00:11:03,850 --> 00:11:08,330
information about the build status
and testing results to aid developers.

176
00:11:08,830 --> 00:11:13,670
To achieve this, multiple tools were
used to parse results, merging them into

177
00:11:13,670 --> 00:11:18,710
the same tables with build statuses,
securely tunneling SSH connections,

178
00:11:19,260 --> 00:11:20,590
and posting summaries via GIST.

179
00:11:21,090 --> 00:11:26,490
With this system in place, we were finally
able to enable, require CI status to

180
00:11:26,490 --> 00:11:29,580
pass setting from GitHub repository.

181
00:11:30,050 --> 00:11:33,530
This ensures that only changes
that pass builds and don't broke

182
00:11:33,530 --> 00:11:37,650
any test are allowed to be merged,
increasing the overall stability

183
00:11:37,660 --> 00:11:39,540
and reliability of the code base.

184
00:11:40,400 --> 00:11:45,380
We can dive deeper into this topic by
accessing secure integration of private

185
00:11:45,420 --> 00:11:46,939
testing infrastructure with GitHub.

186
00:11:47,280 --> 00:11:51,340
public github repositories presented
by my colleague Bianca at the

187
00:11:51,340 --> 00:11:52,950
same edition of home 42 devops.

188
00:11:52,950 --> 00:11:58,620
The final step needed to achieve a
fully automated testing framework was

189
00:11:58,640 --> 00:12:03,570
to implement a mechanism for recovering
harder setups from bad states.

190
00:12:04,070 --> 00:12:08,440
One common issue arises when boot
files produced by CIs are faulty.

191
00:12:08,610 --> 00:12:12,620
In this case, harder setups can
hang during the boot process and

192
00:12:12,630 --> 00:12:15,170
remain stuck in the unstable state.

193
00:12:15,670 --> 00:12:19,910
The framework detects these failures
and attempts to recover the affected

194
00:12:19,910 --> 00:12:21,610
boards through various methods.

195
00:12:22,260 --> 00:12:27,750
As part of this process, we maintain
a set of golden files, a reliable

196
00:12:27,790 --> 00:12:31,030
baseline of boot files that are
overwritten with the latest set

197
00:12:31,100 --> 00:12:33,780
of files that passes successfully.

198
00:12:34,360 --> 00:12:38,260
They serve as a fallback option,
allowing to set the hardware

199
00:12:38,270 --> 00:12:40,490
systems back up and running.

200
00:12:40,985 --> 00:12:44,605
And of course, to be prepared for
next set of files to be tested.

201
00:12:45,385 --> 00:12:49,605
However, the rare scenario when
the hardware setups are physically

202
00:12:49,645 --> 00:12:53,195
damaged remains the single situation
that requires manual intervention.

203
00:12:53,695 --> 00:12:57,745
This recovery mechanism ensures that
the testing framework remains resilient,

204
00:12:58,175 --> 00:13:02,305
minimize the downtime, and increase
the efficiency of testing framework.

205
00:13:02,805 --> 00:13:05,755
Now that I have gone through
all the details, let's see how

206
00:13:05,755 --> 00:13:07,775
the overview design looks like.

207
00:13:08,720 --> 00:13:12,250
On the left side, you can see the
triggering mechanism, multiple

208
00:13:12,250 --> 00:13:14,240
Jenkins files and Jenkins server.

209
00:13:14,760 --> 00:13:19,310
The server manages the testing requests
queue, ensuring efficient resource

210
00:13:19,310 --> 00:13:23,060
allocation and also merges results
from all test harness instances

211
00:13:23,130 --> 00:13:24,680
and prepare them to be published.

212
00:13:25,180 --> 00:13:26,810
Then are the Jenkins agents.

213
00:13:27,270 --> 00:13:30,780
By deploying agents inside Docker
containers, we have successfully

214
00:13:30,790 --> 00:13:35,470
connected multiple hardware setups
into a single physical machine,

215
00:13:35,690 --> 00:13:37,060
optimizing resource usage.

216
00:13:37,560 --> 00:13:40,320
Test hardness supports tests
written in different programming

217
00:13:40,340 --> 00:13:43,999
languages as Python, C or Matlab.

218
00:13:44,000 --> 00:13:48,200
Hardware boards are locked only when
the tests are running on them, otherwise

219
00:13:48,200 --> 00:13:51,910
they remain accessible for remote
connections, allowing team members to

220
00:13:51,910 --> 00:13:54,090
perform debugging and development work.

221
00:13:54,690 --> 00:13:59,180
Test results are very well structured
and presented clearly, ensuring

222
00:13:59,180 --> 00:14:02,780
that any defects are identified
and addressed in early stages.

223
00:14:03,280 --> 00:14:06,460
This system increases efficiency,
maintains stability, and

224
00:14:06,510 --> 00:14:09,590
enhances the overall reliability
of the testing process.

225
00:14:10,090 --> 00:14:14,080
But let's see how the hardware
setup looks in real life.

226
00:14:14,580 --> 00:14:17,580
This is how the prototype looked
in early beginning stages.

227
00:14:18,110 --> 00:14:21,530
There were just a few hardware
boards connected between them

228
00:14:21,570 --> 00:14:22,870
and lying around on a desk.

229
00:14:23,400 --> 00:14:27,870
We were experimenting at that time
using Raspberry Pi as Jenkins agent.

230
00:14:28,420 --> 00:14:31,000
and adding support for multiple platforms.

231
00:14:31,500 --> 00:14:34,330
And this is how Test Harness looks now.

232
00:14:34,830 --> 00:14:37,860
In conclusion, we have managed to
implement a very complex testing

233
00:14:37,860 --> 00:14:41,550
framework that can be triggered from
multiple GitHub repositories, as well

234
00:14:41,550 --> 00:14:43,950
as Jenkins, Cron, or even manually.

235
00:14:44,440 --> 00:14:47,537
Hardware setups remain accessible for
remote connection, allowing team members

236
00:14:47,537 --> 00:14:50,770
to perform debugging and development.

237
00:14:51,610 --> 00:14:56,080
It supports multiple platforms and can
run tests written in different languages.

238
00:14:56,530 --> 00:15:01,930
Resources got optimized by using Jenkins
agents inside Docker containers, and there

239
00:15:01,930 --> 00:15:04,220
is a robust recovery mechanism in place.

240
00:15:05,190 --> 00:15:09,260
Test results are well structured and
bind back to GitHub, ensuring that

241
00:15:09,290 --> 00:15:11,400
bugs are found as early as possible.

242
00:15:11,900 --> 00:15:15,300
The presented testing framework
is highly efficient, flexible, and

243
00:15:15,300 --> 00:15:17,270
robust, designed for complex workflows.

244
00:15:18,025 --> 00:15:21,575
It ensures software and hardware
integration, streamlining the testing

245
00:15:21,575 --> 00:15:24,255
process, and enhancing the stability.

246
00:15:24,755 --> 00:15:28,725
Thank you all for listening me, if you
have any related questions or need more

247
00:15:28,735 --> 00:15:30,595
details don't hesitate to contact me.

248
00:15:30,945 --> 00:15:32,585
Have a nice day, bye.

