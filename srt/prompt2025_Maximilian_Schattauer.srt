1
00:00:00,500 --> 00:00:01,279
Hello everyone.

2
00:00:01,400 --> 00:00:02,720
Thank you for joining my talk.

3
00:00:02,759 --> 00:00:04,319
My name is Maximi Chatow.

4
00:00:04,319 --> 00:00:06,959
I work at Perlin in Munich
as a technical consultant.

5
00:00:07,379 --> 00:00:12,729
And today I would like to present to
you a short result of investigations

6
00:00:12,729 --> 00:00:17,314
that we did at our clients with
regards to semantic retrievers or

7
00:00:17,314 --> 00:00:18,934
basically what is coming beyond it.

8
00:00:19,444 --> 00:00:23,694
Namely how we can not only
leverage embedding models.

9
00:00:24,429 --> 00:00:28,569
Keyword based methods, but also
LLMs that we prompt in order

10
00:00:28,569 --> 00:00:30,540
to get more satisfying results.

11
00:00:30,540 --> 00:00:35,450
With regards to retrievers, let's
first look at the challenges of

12
00:00:35,450 --> 00:00:37,280
conventional retrieval setups.

13
00:00:37,670 --> 00:00:42,410
So what usually happens is that
a user of a retrieval system, in

14
00:00:42,410 --> 00:00:47,120
order to get a set of documents
out of a corpus forward, a query.

15
00:00:47,495 --> 00:00:51,005
That they want to query
documents about to a retriever.

16
00:00:51,365 --> 00:00:58,025
This retriever then goes through a
corpus with the help of a, some kind of

17
00:00:58,025 --> 00:01:05,135
search or embedding model and retrieves
documents that fit to this query best.

18
00:01:05,435 --> 00:01:08,435
So in case of an embedding
model, that would be with regards

19
00:01:08,435 --> 00:01:14,515
to semantic relatedness with
keyword based models like BM 25.

20
00:01:14,845 --> 00:01:15,445
That would be.

21
00:01:15,895 --> 00:01:20,455
Kind of the congruency of of keywords
between the query and documents.

22
00:01:20,455 --> 00:01:25,075
And then people end up with retrieve,
with a set of retrieve documents

23
00:01:25,615 --> 00:01:26,905
with the amount that they like.

24
00:01:27,265 --> 00:01:30,835
And either they can use it with LLMs, but
they can also, and this is the important

25
00:01:30,835 --> 00:01:35,685
case here for us not use it with LLMs,
but for example, present it to the user.

26
00:01:35,945 --> 00:01:39,145
Use it with some downstream
data application.

27
00:01:39,645 --> 00:01:44,955
The challenges that come with this
setup are that you always need to

28
00:01:45,435 --> 00:01:51,015
take into account the entire query
when you do the retrieval process.

29
00:01:51,255 --> 00:01:55,805
So you cannot focus on certain
aspects or add in certain, like

30
00:01:55,805 --> 00:01:57,425
a certain focus on some part.

31
00:01:57,675 --> 00:02:00,675
It's always a black box that
you cannot really control.

32
00:02:00,865 --> 00:02:02,935
With this conventional retriever setup.

33
00:02:03,655 --> 00:02:07,205
And also there's no reasoning
process behind this.

34
00:02:07,445 --> 00:02:12,545
So that means that the order of the
documents, or the one that is favored

35
00:02:12,545 --> 00:02:17,305
the most by the whole setup cannot
be influenced by a reasoning process.

36
00:02:17,515 --> 00:02:21,385
And this might be of interest if you
have some very specialized domain.

37
00:02:21,845 --> 00:02:26,704
Where either semantic models or keyword
based models break down because stuff

38
00:02:26,704 --> 00:02:28,804
is too similar for these models.

39
00:02:29,154 --> 00:02:34,575
And then you would need a reasoning
process to yeah, to find minute details in

40
00:02:34,575 --> 00:02:39,165
the differences between the documents and
then use that as a basis for reordering.

41
00:02:39,554 --> 00:02:42,494
And we, at Perlin, were
confronted with such a case that

42
00:02:42,494 --> 00:02:43,845
one of our clients recently.

43
00:02:44,355 --> 00:02:48,494
And we saw in the quality, the
quantitative quality of the

44
00:02:48,494 --> 00:02:53,744
retrieval results that we cannot
just make do with this kind of setup.

45
00:02:54,054 --> 00:02:58,014
We would need to use an LLM in the
retrieval process to make it more fine

46
00:02:58,014 --> 00:03:00,534
grained and have more fine grain control.

47
00:03:01,314 --> 00:03:06,935
So what we did is that we used the
setup that I just presented, so

48
00:03:06,935 --> 00:03:08,345
feeding a query to a retriever.

49
00:03:08,960 --> 00:03:13,280
Equipped with corpus and embedding
model to only get preliminary documents.

50
00:03:13,519 --> 00:03:18,170
So with a classic retrieval process, you
would get a preliminary set of documents,

51
00:03:18,170 --> 00:03:25,310
but then you would leverage an LLM to
refine the set and refine the order and

52
00:03:25,310 --> 00:03:28,640
then end up with the retrieve documents
that you're actually looking for.

53
00:03:29,420 --> 00:03:33,770
The nice thing about this is that
it offers an additional text input

54
00:03:33,950 --> 00:03:40,010
dimension where you can refine aspects,
you can add metadata to your documents

55
00:03:40,490 --> 00:03:43,430
at this stage that you want to be
leveraged or that you want to be

56
00:03:43,430 --> 00:03:47,060
considered during the re-ranking process.

57
00:03:47,560 --> 00:03:51,305
So this enables us then
to now use reasoning.

58
00:03:51,365 --> 00:03:54,339
So you can either use a normal
LM or even a reasoning LLM.

59
00:03:55,084 --> 00:04:00,375
To reorder your documents and
you can inject other aspects

60
00:04:00,375 --> 00:04:02,715
like metadata rules, for example.

61
00:04:02,715 --> 00:04:08,859
Also, that could not be captured by
a classic non LM retrieval model.

62
00:04:09,359 --> 00:04:16,240
And now looking at, into the detail of how
we actually did this we actually used the

63
00:04:16,240 --> 00:04:22,029
LLM not just as a one off re ranker, so
putting all the documents into the prompt

64
00:04:22,029 --> 00:04:23,890
and then having it select from them.

65
00:04:24,460 --> 00:04:28,479
But we used the LLM as a
binary document operator.

66
00:04:28,960 --> 00:04:36,099
So what we did is we put two documents
into a prompt and a query, and then.

67
00:04:36,625 --> 00:04:37,555
Ask the LLM.

68
00:04:38,215 --> 00:04:39,235
This is the domain.

69
00:04:40,075 --> 00:04:41,845
Look at the following two documents.

70
00:04:41,935 --> 00:04:42,955
Look at the query.

71
00:04:43,345 --> 00:04:47,635
Which one did do you find more
suitable to answering this query?

72
00:04:48,205 --> 00:04:52,855
Please give me an answer, either A or
B, or left or right, or up or down.

73
00:04:53,605 --> 00:04:58,075
And what we end up with by doing this
for all combinations of documents is a

74
00:04:58,075 --> 00:05:00,745
matrix that compares all the documents.

75
00:05:01,090 --> 00:05:02,500
By verdicts of the LLM.

76
00:05:02,560 --> 00:05:05,620
And then you can see, for
example, that document two is

77
00:05:06,250 --> 00:05:07,900
more suited than document one.

78
00:05:08,260 --> 00:05:10,660
And document three is then
less suited than document one.

79
00:05:10,930 --> 00:05:14,790
And with this you can establish
an order of documents in the end.

80
00:05:15,190 --> 00:05:18,760
Especially if you use the LLM multiple
times at a non-zero temperature,

81
00:05:18,760 --> 00:05:22,349
you can get some statistics to
ground this ordering on, and then

82
00:05:22,349 --> 00:05:25,869
you can basically count the verdicts
and order the documents by that.

83
00:05:26,649 --> 00:05:31,059
And we found in our project research
that this is more reliable than a one

84
00:05:31,059 --> 00:05:34,719
of invocation, especially if you're
dealing with many preliminary documents

85
00:05:35,059 --> 00:05:38,419
because you would end up potentially
with a needle in the haystack problem.

86
00:05:38,749 --> 00:05:42,099
And documents would get
lost in the LLM processing.

87
00:05:42,249 --> 00:05:46,299
And this way you have more control
of this re-ranking process.

88
00:05:46,749 --> 00:05:49,149
But, and that is to be considered as well.

89
00:05:49,389 --> 00:05:51,579
Runtime and costs are of
course, an issue here.

90
00:05:51,729 --> 00:05:55,869
So this takes way longer than
just a one-off LLM comparison or

91
00:05:56,109 --> 00:05:58,539
even a classic retriever set up.

92
00:05:58,869 --> 00:05:59,349
Alone.

93
00:06:00,069 --> 00:06:04,219
And yeah, the incurring costs
are of course also higher than if

94
00:06:04,219 --> 00:06:06,889
you just use an embedding model
or even a keyword based model.

95
00:06:07,259 --> 00:06:12,359
Because you have to do many basically
on the order of the square of the

96
00:06:12,359 --> 00:06:14,309
number of documents, LLM calls.

97
00:06:14,809 --> 00:06:19,969
So what is our take home message
from this anecdotal anecdotal talk

98
00:06:19,999 --> 00:06:22,249
about one of our projects is that.

99
00:06:22,969 --> 00:06:26,979
We have seen that conventional
retriever setups do have difficulties,

100
00:06:26,979 --> 00:06:31,989
especially with specialized corpora
and retrieval tasks with special

101
00:06:31,989 --> 00:06:36,829
needs like certain aspects, certain
focuses to to pay attention to.

102
00:06:37,429 --> 00:06:43,649
And we recommend LLMs as re-ran in this
case in a binary comparison setup as

103
00:06:43,649 --> 00:06:48,119
I've just explained, because we have seen
that with just comparing two documents.

104
00:06:48,119 --> 00:06:49,769
The results are quantitatively.

105
00:06:50,459 --> 00:06:54,839
Better than with one
off LLM ranking setups.

106
00:06:55,339 --> 00:07:00,649
So thank you for listening and
looking forward to the other talks.

107
00:07:01,149 --> 00:07:01,269
I.

