1
00:00:00,300 --> 00:00:02,430
Hello, I'm Parvin Ghasemzadeh.

2
00:00:02,550 --> 00:00:05,129
I'm a senior software engineer at Amazon.

3
00:00:05,640 --> 00:00:09,580
And in this talk, I'll be talking about
the role of prompt engineering in ethical

4
00:00:09,580 --> 00:00:14,730
AI and how we can mitigate bias and
reduce hallucinations in AI systems.

5
00:00:15,230 --> 00:00:17,650
So here is the agenda for today's talk.

6
00:00:18,030 --> 00:00:24,040
We'll start with a brief introduction
to LLMs Then I will explain two main

7
00:00:24,450 --> 00:00:30,180
Challenges we face in this area by some
house nation in AI systems And we'll

8
00:00:30,180 --> 00:00:34,000
then discuss about methods to address
these challenges and I will wrap up

9
00:00:34,000 --> 00:00:37,719
with some key takeaways Let's start
with the definition of large language

10
00:00:37,759 --> 00:00:43,940
models And I thought it's better to ask
the definition of one of the LLM to see

11
00:00:43,940 --> 00:00:46,740
how they define itself So here I used

12
00:00:47,240 --> 00:00:51,790
So basically it says LLM is a type
of AI model designed to understand

13
00:00:51,790 --> 00:00:53,320
and generate human like text.

14
00:00:54,069 --> 00:00:57,999
And these models are trained on
vast amount of data which enables

15
00:00:58,000 --> 00:01:01,980
them to predict and produce response
based on the inputs they receive.

16
00:01:02,610 --> 00:01:06,380
So a few keywords here are
important to note and remember.

17
00:01:06,880 --> 00:01:10,750
Generate human like text and
trained on vast amount of data.

18
00:01:11,500 --> 00:01:15,370
So just let's keep these two
phrases in mind, and we'll come

19
00:01:15,380 --> 00:01:19,490
back to this to understand how it
impacts the outcome of the LLAMs.

20
00:01:20,110 --> 00:01:25,679
In general, LLAMs are powerful tools,
but their response depends on the

21
00:01:25,679 --> 00:01:29,549
data they have trained on, which can
lead to issues we'll discuss shortly.

22
00:01:30,049 --> 00:01:32,249
So let's discuss a bit
about the background.

23
00:01:32,509 --> 00:01:36,899
So the transformer architecture is
the background of the large language

24
00:01:36,929 --> 00:01:41,969
models, which allows AI to learn
language patterns and generate text.

25
00:01:42,699 --> 00:01:48,189
So in 2017, Google researchers introduced
the first transformer model, which is

26
00:01:48,809 --> 00:01:56,729
described in the paper, attention is all
you need paper, which is added here, which

27
00:01:56,729 --> 00:01:59,079
light the groundwork for the modern LLMs.

28
00:01:59,629 --> 00:02:03,539
So before transformers, all those
who relied on the sequence, like

29
00:02:03,539 --> 00:02:08,999
a processing, takes word by word,
however with transformers, it scans,

30
00:02:09,359 --> 00:02:14,869
transformers look at the entire sentence,
and, To understand the context better.

31
00:02:15,369 --> 00:02:20,269
here the model uses a special technique
called self attention, which allows to

32
00:02:20,269 --> 00:02:24,839
look at the, all the words in the sentence
at once, to understand the context and

33
00:02:24,839 --> 00:02:28,999
understand the relationship between
the words, to predict the next word.

34
00:02:29,769 --> 00:02:33,954
after this transformer model,
OpenAI created its own model.

35
00:02:33,954 --> 00:02:38,774
First, generative pre trained
transform model in 2018 and 2020.

36
00:02:38,774 --> 00:02:44,934
They created the GPT 3 model, which is one
of the largest language model created for

37
00:02:44,934 --> 00:02:48,284
now, and it has 175 billion parameters.

38
00:02:48,834 --> 00:02:53,304
And after that, so like many other
models with billions, even trillions of

39
00:02:53,374 --> 00:02:55,264
parameters created by different companies.

40
00:02:56,199 --> 00:03:01,219
And the main thing to remember here is
that all these models are trained using

41
00:03:01,219 --> 00:03:07,820
the data from like the public source
from books, from websites, wikipedia,

42
00:03:07,820 --> 00:03:09,559
and like public forms like reddit.

43
00:03:10,409 --> 00:03:15,384
And let's keep in mind that like These
are the, not the accurate source of

44
00:03:15,384 --> 00:03:20,134
the information, so it might have,
some misinformation, some, bias,

45
00:03:20,184 --> 00:03:24,484
within the data, which could, impact
the outcome of the LLMs, which we will

46
00:03:24,504 --> 00:03:26,744
discuss shortly in the upcoming slides.

47
00:03:27,244 --> 00:03:32,204
So now let's talk about a bit
on the ethical part of the AI.

48
00:03:32,204 --> 00:03:34,358
So what do we mean by the ethical AI?

49
00:03:34,358 --> 00:03:39,644
It's mainly about like developing
an AI stems that align with the

50
00:03:39,664 --> 00:03:41,874
core ethical principles and values.

51
00:03:42,374 --> 00:03:47,554
fairness, transparency, accountability,
privacy, safety and inclusivity.

52
00:03:48,004 --> 00:03:54,764
in short, ethical AI systems should treat
everyone fairly, regardless of their race,

53
00:03:54,864 --> 00:03:58,014
age, gender or any other characteristics.

54
00:03:58,514 --> 00:04:04,554
They should also make their process
clear, keep personal data secure and safe,

55
00:04:04,584 --> 00:04:07,144
and respect the diverse perspectives.

56
00:04:07,664 --> 00:04:14,254
So the two key issues we are
facing with ethical AI are bias

57
00:04:14,284 --> 00:04:18,114
and hallucination, which I will
go into details in the next slide.

58
00:04:18,614 --> 00:04:20,214
So let's start with the bias.

59
00:04:20,224 --> 00:04:24,974
So bias in AI mainly refers to
the systematic errors that cause

60
00:04:25,014 --> 00:04:29,404
certain groups or individuals to
be treated unfairly by the model.

61
00:04:29,984 --> 00:04:35,949
So for example, if let's think about
the example for if the face recognition

62
00:04:35,949 --> 00:04:41,499
application has been trained on, a
lighter skinned individual, it may

63
00:04:41,499 --> 00:04:46,789
not accurately identify the darker
skin tones, which can lead to the

64
00:04:47,019 --> 00:04:49,689
error rates for certain groups.

65
00:04:50,509 --> 00:04:55,099
So this, like a bias, can
come from multiple sources.

66
00:04:55,599 --> 00:05:01,569
And the training data, is, this is the
main source as we discussed before.

67
00:05:01,899 --> 00:05:05,539
So the, all these metals are
trained using the, the information

68
00:05:05,539 --> 00:05:06,499
from the public source.

69
00:05:06,979 --> 00:05:11,239
And this could impact the
bias on the output of the as.

70
00:05:11,569 --> 00:05:17,479
So this, like the data might be
imbalanced, so the model design

71
00:05:17,509 --> 00:05:18,919
could have some limitations.

72
00:05:18,919 --> 00:05:21,439
So just feature selection
is also important.

73
00:05:21,439 --> 00:05:22,189
Explaining some.

74
00:05:22,824 --> 00:05:25,494
Features could impact the AI's decision.

75
00:05:25,994 --> 00:05:28,964
Labeling might also be subjective here.

76
00:05:28,964 --> 00:05:34,594
So if the annotators, who, manually
label some training data, have some

77
00:05:34,594 --> 00:05:39,464
bias on specific topics, might also
impact the, decision of the AI.

78
00:05:40,294 --> 00:05:44,394
And also like even the biased
input from the users could

79
00:05:44,394 --> 00:05:46,404
also impact the final output.

80
00:05:46,414 --> 00:05:50,324
So if this data is used
for retraining the models.

81
00:05:50,824 --> 00:05:54,124
And also the ethical impact
of bias is significant.

82
00:05:54,504 --> 00:05:58,944
it can lead to the unfair treatment
of certain groups and ultimately

83
00:05:58,944 --> 00:06:01,144
people lose trust in AI systems.

84
00:06:01,644 --> 00:06:05,954
And for the hallucination, so
this is the mainly commonly

85
00:06:06,314 --> 00:06:08,284
common term used in AI world.

86
00:06:08,584 --> 00:06:11,724
when the model generates incorrect
or fabricated information.

87
00:06:12,544 --> 00:06:15,724
So this can also, happen
for several reasons.

88
00:06:16,194 --> 00:06:18,044
So similar to the bias.

89
00:06:18,439 --> 00:06:24,139
So training data is also plays a huge
role here, so low quality data also,

90
00:06:24,889 --> 00:06:30,619
cause some, AI models to hallucinate, to
provide a fabricated and misinformation.

91
00:06:31,229 --> 00:06:35,386
So lack of sufficient context
also cause the, issues here.

92
00:06:35,956 --> 00:06:42,886
Or, a complex language, inputs, Sarcasm,
like cultural references, could be

93
00:06:42,936 --> 00:06:47,756
difficult for AI models to understand,
and in that case AI models would just fill

94
00:06:47,756 --> 00:06:50,066
the gaps with some fabricated information.

95
00:06:50,566 --> 00:06:55,966
And also, this can also lead to the
misinformation and reduce reliability

96
00:06:55,966 --> 00:07:02,006
in AI systems, which is why it's
important to address all these issues.

97
00:07:02,506 --> 00:07:06,226
let's see what methods can
be used to mitigate the bias.

98
00:07:06,246 --> 00:07:10,476
here are the few strategies listed below.

99
00:07:11,166 --> 00:07:16,426
as we discussed, the source of the
issues is coming from the training data.

100
00:07:16,766 --> 00:07:20,726
using diverse training data
sets that cover the range of

101
00:07:20,806 --> 00:07:21,901
demographics is a good idea.

102
00:07:22,841 --> 00:07:24,151
key starting point here.

103
00:07:24,391 --> 00:07:30,141
So for example, so if you are building
a assistant like AI assistant to answer

104
00:07:30,151 --> 00:07:36,271
medical questions, we should use a data
set that reflects diverse patients like

105
00:07:36,591 --> 00:07:41,831
demographics and conditions to avoid like
a bias recommendation from the other side.

106
00:07:42,581 --> 00:07:47,241
So another method is fine tuning and
debiasing where we retrain model.

107
00:07:47,996 --> 00:07:50,956
with specific data that
helps to reduce the bias.

108
00:07:51,786 --> 00:07:56,426
So doing some bias audits and
transparency about how these

109
00:07:56,456 --> 00:07:58,466
algorithms function are also important.

110
00:07:58,776 --> 00:08:02,926
And finally, updating the
models regularly, keeping them

111
00:08:02,956 --> 00:08:06,226
aligned with social norms would
help to minimize the bias.

112
00:08:06,726 --> 00:08:11,616
And similar to the bias, in order
to reduce hallucinations, using high

113
00:08:11,616 --> 00:08:13,766
quality training data is important.

114
00:08:14,136 --> 00:08:18,456
So the more accurate the data, the
better the model's output it is.

115
00:08:18,456 --> 00:08:23,036
So contextual training is another
approach that we can use to reduce the

116
00:08:23,136 --> 00:08:28,456
hallucination here to where we provide
the model with additional background

117
00:08:28,456 --> 00:08:30,246
information for specific tasks.

118
00:08:30,626 --> 00:08:34,286
For example, if we train the
customer service chatbot on

119
00:08:34,666 --> 00:08:38,226
domain specific language, it
will give more relevant answers.

120
00:08:38,726 --> 00:08:45,296
So fine tuning the pre trained models
on this focus dataset also increase

121
00:08:45,346 --> 00:08:47,726
the reliability of the system.

122
00:08:48,256 --> 00:08:53,261
for example, I guess AI system design
for the medical advice should The

123
00:08:53,371 --> 00:08:57,761
fine tuned on the verified health
care data have an irrelevant output.

124
00:08:58,261 --> 00:09:02,261
So integrating with the external
knowledge sources like a database

125
00:09:02,321 --> 00:09:06,781
is going to help verify the facts
and provide additional context and

126
00:09:06,811 --> 00:09:08,491
help to reduce the hallucination.

127
00:09:09,081 --> 00:09:10,901
And also continuous learning is important.

128
00:09:11,186 --> 00:09:15,366
Which allows the model to keep improving
based on the user interaction here.

129
00:09:15,866 --> 00:09:20,736
So all the previous methods we discussed
for both minimizing the bias and reducing

130
00:09:20,756 --> 00:09:26,616
hallucinations is not scalable and cost
efficient as they require retraining, fine

131
00:09:26,616 --> 00:09:28,546
tuning, which are any costly operation.

132
00:09:29,146 --> 00:09:33,146
So now we will discuss a few, prompt
engineering methods, which will be

133
00:09:33,146 --> 00:09:35,006
easy and quick to implement and try.

134
00:09:35,586 --> 00:09:40,666
we'll discuss zero shot and few
shot learning, chain of thought, and

135
00:09:40,946 --> 00:09:44,566
modular reasoning, knowledge, and
language, in short, Miracle Framework.

136
00:09:45,066 --> 00:09:48,466
So let's start with the zero
shot and few shot learning.

137
00:09:49,066 --> 00:09:53,966
So mainly zero shot and few shot learning
methods are useful in areas where we

138
00:09:53,966 --> 00:09:56,376
don't have a lot of, labeled data.

139
00:09:57,116 --> 00:10:03,346
And zero shot means, allows the model
to apply its existing knowledge to, new

140
00:10:03,346 --> 00:10:05,136
tasks without any specific examples.

141
00:10:05,156 --> 00:10:07,156
So it relies on the training data.

142
00:10:07,546 --> 00:10:11,856
Where in the few shot learning, we
give model a few additional examples.

143
00:10:12,411 --> 00:10:15,281
Helping it to understand
what we expect, for example.

144
00:10:15,781 --> 00:10:21,361
if we are building an AI system
to, let's say, classify emails, we

145
00:10:21,361 --> 00:10:26,774
can show, show it a few examples,
which will adapt, more effectively.

146
00:10:27,444 --> 00:10:32,654
here, in the, actually, in this screenshot
shown here, shows the three examples,

147
00:10:32,824 --> 00:10:34,559
zero shot, like a glitched email.

148
00:10:34,559 --> 00:10:38,789
It doesn't provide any examples of one
shot which provides a single additional

149
00:10:38,789 --> 00:10:42,379
example and a few shot which provides
multiple examples that shows the

150
00:10:42,739 --> 00:10:48,069
translation of word from English to French
and in the one shot it provides a single

151
00:10:48,599 --> 00:10:52,779
example to show what the translation
of word from English to French and the

152
00:10:52,779 --> 00:10:54,469
few shot provides multiple examples.

153
00:10:54,809 --> 00:10:58,559
Which increases the accuracy
of the result of the AI system.

154
00:10:58,669 --> 00:11:01,859
another method is the
chain of thought approach.

155
00:11:02,509 --> 00:11:08,039
So this is the one, from the paper
created by the Google resource team.

156
00:11:08,309 --> 00:11:10,539
and this, I'll add the link here.

157
00:11:10,979 --> 00:11:15,289
this helps the method to think step
by step and break down the complex

158
00:11:15,299 --> 00:11:17,309
problems by reasoning through.

159
00:11:17,679 --> 00:11:22,249
like at each step, so instead of jumping
directly to the answer, so it just

160
00:11:22,749 --> 00:11:24,639
asks the model to think step by step.

161
00:11:25,639 --> 00:11:29,409
this is the screenshot, this is
the example from the paper itself.

162
00:11:29,719 --> 00:11:33,879
Make sure this is a good example that
shows the combination of one shot

163
00:11:33,919 --> 00:11:35,539
example and the chain of the tiles.

164
00:11:35,549 --> 00:11:40,329
So in each, like in the standard prompting
and the chain of the tile prompting, so

165
00:11:40,489 --> 00:11:45,729
it provides an additional example before
asking the, example to the model itself.

166
00:11:45,979 --> 00:11:50,699
So in the first one, It provides
the one example with math problem

167
00:11:51,059 --> 00:11:53,989
and in the answer part it just
directly provides the answer.

168
00:11:54,589 --> 00:11:58,609
However, when it asks the new
math problem it just couldn't

169
00:11:59,589 --> 00:12:00,969
find the answer correctly.

170
00:12:01,609 --> 00:12:06,579
In the second one, the chain of 10
problem, prompting part, so it provides

171
00:12:06,579 --> 00:12:11,809
a math, same math problem, but in the
answer, so it just gives a, like a step

172
00:12:11,809 --> 00:12:16,689
by step explanation more, explain the
reasoning of the answer here, which

173
00:12:16,689 --> 00:12:19,979
helps the, model to respond correctly.

174
00:12:20,019 --> 00:12:24,449
So in the, in that case, so it's
able to correctly find the math,

175
00:12:24,719 --> 00:12:25,899
answer of the math problem.

176
00:12:26,399 --> 00:12:32,189
And the last method is the Miracle
Framework created by the AI21 Labs.

177
00:12:32,689 --> 00:12:36,399
basically it suggests to enhance
the capabilities of language

178
00:12:36,429 --> 00:12:40,329
models by integrating with external
tools and knowledge sources.

179
00:12:40,829 --> 00:12:47,909
Some examples are like, using it for,
integrating with the additional source to,

180
00:12:47,909 --> 00:12:54,219
to, check the weather, real time weather,
or for the financial applications, maybe

181
00:12:54,509 --> 00:12:58,829
integrating with the financial source
to check the real time, stock prices.

182
00:12:59,149 --> 00:13:03,689
Could be a good example here and in this
screenshot below actually shows also like

183
00:13:03,689 --> 00:13:08,129
another use case for the math problems,
so which is able to find the answers for

184
00:13:08,129 --> 00:13:13,269
the simple math problems but struggles
to, calculate the complex math problems.

185
00:13:13,599 --> 00:13:18,349
So the calculator tool or calculator
application is another good use case here.

186
00:13:18,389 --> 00:13:20,379
So basically it just.

187
00:13:20,999 --> 00:13:24,749
integrates with the calculator
application and parse the user inputs

188
00:13:25,029 --> 00:13:30,359
and pass the parameters to the Miracle
framework like through the APS which

189
00:13:31,039 --> 00:13:37,799
calculates and responds back to the
correct result for the math problems.

190
00:13:38,299 --> 00:13:43,999
So just to wrap up, here are
the key points to remember

191
00:13:44,309 --> 00:13:45,989
as part of this presentation.

192
00:13:46,599 --> 00:13:52,039
So ethical AI is important for
creating systems that are fair,

193
00:13:52,179 --> 00:13:53,719
transparent, and accountable.

194
00:13:54,219 --> 00:13:56,729
We should also remember that
prompt engineering is a powerful

195
00:13:56,729 --> 00:14:01,479
tool to address issues of bias and
hallucinations, which makes the AI

196
00:14:01,489 --> 00:14:02,559
system more reliable and transportive.

197
00:14:03,059 --> 00:14:06,729
It's also important to remember
that the ethical is a continuous

198
00:14:06,729 --> 00:14:13,409
journey, so we need to continuously,
evaluate, update and take the diverse

199
00:14:13,439 --> 00:14:18,439
perspectives into account in order to
tackle new ethical challenges here.

200
00:14:19,289 --> 00:14:24,074
And finally, Organizations must commit
to ethical practices in AI development

201
00:14:24,494 --> 00:14:27,184
to ensure technology benefits society.

202
00:14:27,684 --> 00:14:32,304
So here are the list of references
I used throughout the slides

203
00:14:32,804 --> 00:14:34,744
and thank you for listening.

204
00:14:34,964 --> 00:14:40,494
Please feel free to connect me on LinkedIn
for further discussion and questions.

