1
00:00:00,780 --> 00:00:01,110
All right.

2
00:00:01,800 --> 00:00:04,530
Welcome to instrumenting
at 10 years per second.

3
00:00:04,950 --> 00:00:07,500
Today I'll talk about a new
approach that we've developed to

4
00:00:07,500 --> 00:00:09,780
accelerate manual instrumentation.

5
00:00:10,110 --> 00:00:13,470
Now, auto instrumentation will
get you 80% there, but that

6
00:00:13,470 --> 00:00:17,010
last 20% manual instrumentation.

7
00:00:17,075 --> 00:00:22,295
Takes a lot longer, 10 years in
our case, roughly, but manual

8
00:00:22,295 --> 00:00:26,855
instrumentation really increases
your ability to deeply understand

9
00:00:26,945 --> 00:00:29,435
what your system is doing and why.

10
00:00:30,095 --> 00:00:33,275
Now we piloted this approach
successfully with one of our neuro

11
00:00:33,275 --> 00:00:36,965
microservices, and we're looking
forward to spreading this approach

12
00:00:37,055 --> 00:00:40,625
across the rest of the microservices
in our fleet, and I'm really excited

13
00:00:40,625 --> 00:00:42,485
to share this approach with you today.

14
00:00:42,985 --> 00:00:44,005
A little about me.

15
00:00:44,065 --> 00:00:45,865
My name, I'm Jean Mark Wright.

16
00:00:46,255 --> 00:00:47,875
I'm a staff engineer at Wave.

17
00:00:47,905 --> 00:00:48,145
We're.

18
00:00:48,145 --> 00:00:50,305
I Lead Observability at Wave.

19
00:00:50,305 --> 00:00:53,275
We're building an operating
system for small business owners.

20
00:00:53,305 --> 00:00:56,455
We really want to take the tedium
and the complexity out of running

21
00:00:56,455 --> 00:01:01,435
the business so that small business
owners can focus on what they do best.

22
00:01:01,435 --> 00:01:05,655
So we provide a suite of products to
ensure that small business owners, can

23
00:01:05,655 --> 00:01:07,630
really thrive at what they are doing.

24
00:01:08,130 --> 00:01:12,780
Now, today I'll talk about the problem
of accelerating manual instrumentation.

25
00:01:13,110 --> 00:01:17,790
What we're trying to do is we're really
trying to dramatically increase the pace

26
00:01:18,300 --> 00:01:22,650
at which you can manually instrument
your service and which in turn is gonna

27
00:01:22,650 --> 00:01:26,910
really improve the data that's available
for doing investigations and really

28
00:01:26,910 --> 00:01:29,310
understanding what your system is doing.

29
00:01:29,670 --> 00:01:30,360
So I'll talk about.

30
00:01:30,730 --> 00:01:31,930
Our solution.

31
00:01:31,990 --> 00:01:34,810
I'll talk about how it works and
then I'll close off with some next

32
00:01:34,810 --> 00:01:37,130
steps in terms of what we thought of.

33
00:01:37,630 --> 00:01:39,760
But first, lemme tell you a story.

34
00:01:40,630 --> 00:01:43,720
I tell you this story
because for two reasons.

35
00:01:44,230 --> 00:01:48,730
First, it demonstrates the
power and value of high quality.

36
00:01:49,230 --> 00:01:50,670
Manual instrumentation.

37
00:01:51,090 --> 00:01:55,980
And then secondly, it really motivates
why we want to spend and invest a

38
00:01:55,980 --> 00:02:00,690
lot of engineering time and effort to
ensure that all engineers have access

39
00:02:00,690 --> 00:02:03,130
to this high quality instrumentation.

40
00:02:03,630 --> 00:02:08,549
Once upon a time our on-call engineer
was minding his own business,

41
00:02:09,449 --> 00:02:13,470
but then he realized something
that was particularly disturbing.

42
00:02:14,100 --> 00:02:21,619
He realized that our system was given
access to a premium feature for free.

43
00:02:22,399 --> 00:02:27,530
This feature allowed users to connect
their bank accounts and import their

44
00:02:27,530 --> 00:02:29,899
transactions into our accounting product.

45
00:02:30,320 --> 00:02:33,530
A costly feature for our
business, but very convenient

46
00:02:33,530 --> 00:02:35,269
and critical for business owners.

47
00:02:35,720 --> 00:02:42,084
This feature is called Transaction Import
Now, transaction import is one of several

48
00:02:42,084 --> 00:02:43,885
features that users have access to.

49
00:02:44,545 --> 00:02:49,285
The problem is that transaction
import is a premium feature.

50
00:02:49,785 --> 00:02:54,125
Typically folks would purchase
our pro plan, which gives access

51
00:02:54,125 --> 00:02:57,245
to transaction import, along
with some other premium features.

52
00:02:57,935 --> 00:03:03,395
And we also had this special legacy plan
that would also give access to transaction

53
00:03:03,395 --> 00:03:05,915
import for some of our existing users.

54
00:03:06,195 --> 00:03:09,285
But it wasn't live as yet.

55
00:03:09,885 --> 00:03:14,745
And then finally, we also had this startup
plan, which all customers were initialized

56
00:03:14,745 --> 00:03:17,055
on once they had completed onboarding.

57
00:03:17,475 --> 00:03:21,555
This startup plan gave
access to free features.

58
00:03:21,795 --> 00:03:26,385
It didn't give access to transaction
import, which is a premium feature, but

59
00:03:26,385 --> 00:03:31,455
whatever we needed to dig in, understand
what was going on quickly so we could

60
00:03:31,455 --> 00:03:33,915
stop the leak of this premium feature.

61
00:03:34,415 --> 00:03:39,305
Now, fortunately for our on-call engineer,
this application was really special.

62
00:03:39,905 --> 00:03:43,385
It had auto instrumentation, so all
our services, our databases, our

63
00:03:43,385 --> 00:03:45,305
caches, they were all instrumented.

64
00:03:45,575 --> 00:03:48,665
We got that for free using Datadog, right?

65
00:03:49,055 --> 00:03:51,275
But, and so we also had metrics, right?

66
00:03:51,275 --> 00:03:55,085
We have a PM metrics, we're getting
latency, throughput, error metrics.

67
00:03:55,390 --> 00:04:00,610
We also had distributed tracing, so
you could actually trace a request

68
00:04:00,610 --> 00:04:04,030
all the way from our API down to the
service that fulfilled that request.

69
00:04:04,330 --> 00:04:06,190
And we also had logs, meh.

70
00:04:06,689 --> 00:04:12,539
One thing that really set this system
apart was the fact that it had really

71
00:04:12,659 --> 00:04:16,799
high quality manual instrumentation.

72
00:04:17,219 --> 00:04:21,269
So auto instrumentation is the stuff
that you get out of the box, right?

73
00:04:21,479 --> 00:04:24,689
So from your vendor, like
Datadog or whatever your

74
00:04:24,689 --> 00:04:26,339
vendor is, are open telemetry.

75
00:04:26,704 --> 00:04:30,334
You'll get instrumentation for your
databases, for your caches, for the

76
00:04:30,334 --> 00:04:31,984
different transports that you use.

77
00:04:31,984 --> 00:04:34,054
But manual instrumentation.

78
00:04:34,414 --> 00:04:38,884
Manual instrumentation is what you
are adding from your application.

79
00:04:39,154 --> 00:04:41,044
But it's this instrumentation.

80
00:04:41,619 --> 00:04:44,590
That made this app particularly special.

81
00:04:45,039 --> 00:04:47,949
What it meant is that we could
actually peel back the layers.

82
00:04:48,099 --> 00:04:50,559
We could really inspect the system state.

83
00:04:50,829 --> 00:04:55,059
We could ask thoughtful questions, and
we could really interrogate the system's

84
00:04:55,059 --> 00:04:59,799
behavior to really understand what the
system was doing, why it was doing it,

85
00:04:59,799 --> 00:05:01,359
why it made the choices that it made.

86
00:05:01,859 --> 00:05:03,059
But let's get back to the story.

87
00:05:03,450 --> 00:05:07,080
So our on-call engineer, he noticed
that we're giving a premium feature

88
00:05:07,080 --> 00:05:10,229
away for free, and he's trying to figure
out, we're all trying to figure out

89
00:05:10,499 --> 00:05:13,049
why is the system doing this right?

90
00:05:13,289 --> 00:05:17,284
So our investigation took us to
an endpoint called initialized.

91
00:05:17,709 --> 00:05:18,429
Business.

92
00:05:18,879 --> 00:05:23,889
Now, initialized business is responsible
for putting customers on a plan after

93
00:05:23,889 --> 00:05:25,689
they've successfully onboarded with us.

94
00:05:26,139 --> 00:05:29,319
Now the plan is what gives you
access to the features, right?

95
00:05:29,709 --> 00:05:34,269
And after onboarding, the initialized
endpoint would put a customer,

96
00:05:34,659 --> 00:05:36,339
would put the customer on a plan.

97
00:05:36,669 --> 00:05:39,759
So by default, customers
are put on our start plan.

98
00:05:40,259 --> 00:05:44,309
So eventually I talked about this
legacy plan that we had as well.

99
00:05:44,369 --> 00:05:48,179
Eventually, we'd support putting
users on a legacy plan, which

100
00:05:48,179 --> 00:05:52,919
would give access to transaction
import, but we weren't there yet.

101
00:05:53,419 --> 00:05:56,719
But with the manual instrumentation
that we added, it meant that we could

102
00:05:56,719 --> 00:06:00,004
really dive deep to understand exactly.

103
00:06:00,414 --> 00:06:05,244
What this endpoint was doing, we could
really examine the data that was processed

104
00:06:05,244 --> 00:06:07,135
to better understand the system state.

105
00:06:07,614 --> 00:06:13,344
So not only could we look at common
attributes like user, we could ask

106
00:06:13,405 --> 00:06:15,534
bigger questions so we could ask, Hey.

107
00:06:16,034 --> 00:06:19,005
What plan was initialized for this user?

108
00:06:19,425 --> 00:06:23,265
Or we could look at, okay, once the
plan has initialized for, has been

109
00:06:23,265 --> 00:06:28,575
in, has been initialized for the user,
what features did we give to that user?

110
00:06:28,875 --> 00:06:32,735
And we could look at the country that
the user was being initialized from.

111
00:06:33,005 --> 00:06:36,665
There were bevy of different attributes
that we could look at, and all of

112
00:06:36,665 --> 00:06:41,345
this data we had access to in our
traces, in our errors, and in our logs.

113
00:06:41,845 --> 00:06:43,885
So a key question that came up.

114
00:06:44,414 --> 00:06:47,594
Was, did we initialize the right plan?

115
00:06:47,984 --> 00:06:51,374
We know that when users onboard,
we're supposed to be putting

116
00:06:51,374 --> 00:06:53,534
them on the start plan, right?

117
00:06:53,955 --> 00:06:58,364
So one key question that we had to ask,
and we probably started with first, was,

118
00:06:58,755 --> 00:07:01,859
did we put the users on the start plan?

119
00:07:02,755 --> 00:07:06,534
The good thing is that because we
had manually instrumented our system

120
00:07:06,534 --> 00:07:10,255
and we have all of this high quality
data available to us, we could

121
00:07:10,255 --> 00:07:12,115
actually interrogate our quest in.

122
00:07:12,145 --> 00:07:14,065
We could actually interrogate our traces.

123
00:07:14,065 --> 00:07:16,224
We could ask our system this question.

124
00:07:16,555 --> 00:07:21,104
So we could say, looking
at recent traces, which.

125
00:07:21,554 --> 00:07:24,314
Plan, have we been assigning to our users?

126
00:07:24,704 --> 00:07:27,224
And see here, we've actually
graphed that with our traces.

127
00:07:27,224 --> 00:07:30,344
We've broken down our
traces and essentially asked

128
00:07:30,344 --> 00:07:31,514
our system that question.

129
00:07:31,784 --> 00:07:34,934
And when you look at the result here,
you can see in the bottom left corner

130
00:07:34,934 --> 00:07:38,744
of the graph that we're actually
correctly assigning the starter plan.

131
00:07:38,984 --> 00:07:40,364
So that's actually expected.

132
00:07:40,904 --> 00:07:45,194
We know customers are supposed to be put
on a starter plan by default, so it looks

133
00:07:45,194 --> 00:07:47,384
like the system is doing that correctly.

134
00:07:47,699 --> 00:07:51,959
But somehow users are still getting
access to this premium feature.

135
00:07:52,459 --> 00:07:55,609
Maybe let's open a specific trace.

136
00:07:56,059 --> 00:08:00,559
Let's look at the span tags and then let's
look at each of these attributes in turn.

137
00:08:01,369 --> 00:08:03,469
So we know the user ID is fine.

138
00:08:03,889 --> 00:08:06,709
That's not the most
important thing for us.

139
00:08:06,709 --> 00:08:08,899
Now, the country's okay.

140
00:08:09,710 --> 00:08:10,669
The plan.

141
00:08:11,464 --> 00:08:15,034
We saw that yes, we're actually putting
the customer on the starter plan.

142
00:08:15,304 --> 00:08:19,594
We're a little concerned about features
because we know transaction import is in

143
00:08:19,594 --> 00:08:21,364
that list of features that the user got.

144
00:08:21,824 --> 00:08:27,524
And that's not expected, but,
wait, what's this feature group

145
00:08:27,524 --> 00:08:29,594
thing and why does it's a legacy.

146
00:08:30,094 --> 00:08:33,454
So it looks like we're initializing
the startup plan, but we're

147
00:08:33,454 --> 00:08:35,494
using a legacy feature group.

148
00:08:35,885 --> 00:08:36,574
That's weird.

149
00:08:36,905 --> 00:08:38,044
What's a feature group?

150
00:08:38,544 --> 00:08:43,014
So it turns out that we actually don't
assign plans directly to features.

151
00:08:43,764 --> 00:08:50,154
What we do is that we actually put
features into a feature group, and then

152
00:08:50,334 --> 00:08:53,364
we point the plan to the feature group.

153
00:08:53,864 --> 00:08:59,984
So what our instrumentation was telling
us is that we were initializing the right

154
00:09:00,134 --> 00:09:02,974
plan, but the incorrect feature group.

155
00:09:03,739 --> 00:09:08,839
So the feature group correctly defined
the set of features, so the legacy feature

156
00:09:08,839 --> 00:09:13,759
group, because it represents the legacy
plan, essentially it has transaction

157
00:09:13,759 --> 00:09:15,619
import in that list of features.

158
00:09:15,859 --> 00:09:20,659
But what we were doing wrong is
that the start plan was somehow

159
00:09:20,659 --> 00:09:23,809
incorrectly pointing to the legacy plan.

160
00:09:23,859 --> 00:09:28,599
What should have been happening was
the startup plan should be pointing to

161
00:09:28,659 --> 00:09:34,209
the start feature group, which would've
correctly pointed to the list of features.

162
00:09:34,569 --> 00:09:35,769
That's really interesting.

163
00:09:36,609 --> 00:09:41,469
There's so much information we're able
to glean just looking at our traces

164
00:09:41,469 --> 00:09:43,059
and looking at the data that's there.

165
00:09:43,659 --> 00:09:49,239
So the theory we have now is that the
start plan is incorrectly pointing.

166
00:09:49,884 --> 00:09:52,134
To the legacy feature
group, and that's incorrect.

167
00:09:52,704 --> 00:09:57,024
But can we ask our system, what
feature group have you been assigning?

168
00:09:57,354 --> 00:10:00,894
Turns out we actually can, we
can confirm that with our traces.

169
00:10:00,894 --> 00:10:03,894
We can ask our system, what feature
group have you been assigning?

170
00:10:04,254 --> 00:10:10,194
And if we graph that, you can actually see
from the graph that run about the 23rd.

171
00:10:10,694 --> 00:10:15,284
We started incorrectly assigning
the legacy plan to users.

172
00:10:15,314 --> 00:10:18,254
So all along you can see that
we're assigning the start of

173
00:10:18,864 --> 00:10:20,484
sorry, the start of feature group.

174
00:10:20,664 --> 00:10:23,724
All along you can see we're assigning
the start of feature group, but

175
00:10:23,724 --> 00:10:28,884
somewhere around the 23rd you can
see that we've started adding, we've

176
00:10:28,884 --> 00:10:35,214
started initializing the legacy
feature group and just like that.

177
00:10:35,574 --> 00:10:39,594
A little quality manual
instrumentation, saves the day.

178
00:10:40,094 --> 00:10:44,114
So our on-call engineer discovered we
were giving features away for free.

179
00:10:44,774 --> 00:10:48,404
We jumped into Datadog, put our
manual instrumentation to work.

180
00:10:48,554 --> 00:10:52,694
We interrogated the system
using traces, and within 12

181
00:10:52,694 --> 00:10:54,524
minutes we found the problem.

182
00:10:55,069 --> 00:10:56,719
It was a data corruption issue.

183
00:10:56,869 --> 00:10:59,569
There was a problem with
the data in our database.

184
00:11:00,019 --> 00:11:03,979
We weren't pointing to the right
pieces of data properly, right?

185
00:11:03,979 --> 00:11:07,819
Our starter plan was pointing to
this incorrect legacy feature group.

186
00:11:08,389 --> 00:11:12,429
Now, it only took us a few minutes
to fix the data in the database,

187
00:11:12,929 --> 00:11:20,039
but after 15 minutes, we had
debugged and fixed the premium leak.

188
00:11:20,724 --> 00:11:25,134
That is the power of
manual instrumentation.

189
00:11:25,634 --> 00:11:27,814
So you might say that's a great outcome.

190
00:11:28,264 --> 00:11:30,934
Lots of great manual
instrumentation, debugging and

191
00:11:30,934 --> 00:11:32,884
fixing a problem in 15 minutes.

192
00:11:33,214 --> 00:11:34,414
So what's the problem then?

193
00:11:34,914 --> 00:11:38,994
The good news is that we had
one really well instrumented,

194
00:11:38,994 --> 00:11:41,754
microservice, the bad news.

195
00:11:41,904 --> 00:11:43,164
We had one.

196
00:11:44,124 --> 00:11:46,254
Really well instrumented microservice.

197
00:11:46,854 --> 00:11:50,474
There are about 40 others,
so during the year.

198
00:11:50,639 --> 00:11:54,289
A year and a half that the
service was being developed, the

199
00:11:54,289 --> 00:11:56,029
engineers manually instrumented it.

200
00:11:56,419 --> 00:12:01,369
So if we say that it took three
months to completely add that manual

201
00:12:01,489 --> 00:12:05,659
instrumentation, conservative estimate,
if we assume it takes three months

202
00:12:05,659 --> 00:12:11,299
to instrument each existing service,
three months times 40 services gives

203
00:12:11,299 --> 00:12:15,049
us 120 months, which is 10 years.

204
00:12:15,469 --> 00:12:19,519
And yes, we could paralyze the work,
but it would still take us years.

205
00:12:20,019 --> 00:12:24,819
So what we decided to do was to do a pilot
with one of our new American services.

206
00:12:25,179 --> 00:12:29,049
We thought let's try and accelerate
manual instrumentation for them.

207
00:12:29,549 --> 00:12:32,279
Now in order to do that,
we looked at two things.

208
00:12:32,669 --> 00:12:36,719
The first thing we looked at
was the component interaction.

209
00:12:37,409 --> 00:12:42,029
So when a request comes into the
application, several components

210
00:12:42,179 --> 00:12:44,939
collaborate to fulfill that request.

211
00:12:45,019 --> 00:12:48,199
So in our application, we
typically have API components.

212
00:12:48,199 --> 00:12:53,249
We have services that capture our business
logic, and we also have repositories

213
00:12:53,429 --> 00:12:55,949
that handle an abstract persistence.

214
00:12:56,464 --> 00:13:00,829
So Initialized business endpoint, for
instance, there was an API component.

215
00:13:01,329 --> 00:13:04,769
Which would call a service method
on, or a subscription service.

216
00:13:05,009 --> 00:13:08,399
And then that subscription service
would call one or more repositories.

217
00:13:08,399 --> 00:13:12,689
So in this example, you can see the
subscription service initialized

218
00:13:12,689 --> 00:13:17,579
business method is calling the
plan repository dot get method, and

219
00:13:17,579 --> 00:13:21,359
it's also calling the subscription
repository that create method.

220
00:13:22,229 --> 00:13:23,969
So that's the first
thing that we looked at.

221
00:13:23,969 --> 00:13:26,609
We thought about the
component interaction and.

222
00:13:27,084 --> 00:13:30,025
It seemed like this was a piece
of the puzzle because what we're

223
00:13:30,025 --> 00:13:33,444
trying to do is we're trying to
figure out how can we accelerate

224
00:13:33,444 --> 00:13:35,484
the pace of manual instrumentation?

225
00:13:35,515 --> 00:13:40,944
How can we really help engineers to get
to a point where there's little or no

226
00:13:40,944 --> 00:13:45,655
manual instrumentation, but we've got
to a point where there's a lot more?

227
00:13:45,685 --> 00:13:47,634
How can we automate this process?

228
00:13:47,634 --> 00:13:49,314
So we looked at the component interaction.

229
00:13:49,344 --> 00:13:50,785
That was one piece of the puzzle.

230
00:13:51,384 --> 00:13:54,655
The other piece of the puzzle that
we thought about was the data.

231
00:13:54,709 --> 00:13:55,729
That's exchanged.

232
00:13:56,209 --> 00:13:58,579
So there's a lot of data
exchange in the process.

233
00:13:58,579 --> 00:14:03,800
So for example, when the subscription
API Initialized Business Method calls the

234
00:14:03,800 --> 00:14:08,329
subscription service initialized business
method, there's a lot of data that's

235
00:14:08,329 --> 00:14:10,160
passed between those two components.

236
00:14:10,160 --> 00:14:14,869
So the subscription API passes
a business ID, passes a user ID

237
00:14:14,869 --> 00:14:16,729
country, and a business name.

238
00:14:17,229 --> 00:14:21,259
And similarly, when the service
calls or repositories data is also

239
00:14:21,259 --> 00:14:23,310
passed between those components.

240
00:14:23,810 --> 00:14:29,959
Now, it would be great if we could
let engineers specify the data that

241
00:14:29,959 --> 00:14:31,399
they're interested in capturing.

242
00:14:31,790 --> 00:14:36,739
So if they could say, I wanna capture a
business id, user ID the business name.

243
00:14:37,009 --> 00:14:42,790
So if they could tell us the data that
they wanted to capture, then what we could

244
00:14:42,790 --> 00:14:48,249
do is we could look at these components,
because now what these components

245
00:14:48,249 --> 00:14:53,140
represent is a great opportunity
to insert some capturing logic.

246
00:14:53,469 --> 00:14:57,849
So if we're able to actually wrap each
of these components, then we could

247
00:14:57,849 --> 00:15:01,094
capture the data that the teams need.

248
00:15:01,594 --> 00:15:05,165
So what we're saying here is that
we're looking at the application stack.

249
00:15:05,435 --> 00:15:08,104
We're looking at the different
components that are used to

250
00:15:08,104 --> 00:15:09,994
fulfill our request, right?

251
00:15:10,264 --> 00:15:13,474
And we know that data gets passed
in between those components.

252
00:15:13,744 --> 00:15:18,514
So what we're thinking is if we
let engineers specify and say, Hey.

253
00:15:18,889 --> 00:15:21,409
This is the data that I want to capture.

254
00:15:21,799 --> 00:15:25,869
Then we could create hooks or
we could wrap those components

255
00:15:25,929 --> 00:15:30,099
and just capture the data that
engineers have asked us to collect.

256
00:15:30,599 --> 00:15:34,050
So what we did was we started
with this configuration.

257
00:15:34,859 --> 00:15:34,890
I.

258
00:15:34,890 --> 00:15:36,074
Tell us what data you want to capture.

259
00:15:36,135 --> 00:15:41,415
So maybe they wanna capture the
business id, user ID and the country.

260
00:15:42,074 --> 00:15:47,474
And then what we did is that we took
this notion of the configuration and

261
00:15:47,474 --> 00:15:49,244
then we converted that into code.

262
00:15:49,744 --> 00:15:51,634
So here's a sample configuration.

263
00:15:51,934 --> 00:15:55,594
We call this a telemetric
capture configuration.

264
00:15:56,104 --> 00:16:01,024
Now, with this configuration, they can
specify a load or disallowed field names,

265
00:16:01,024 --> 00:16:06,394
field types, and since it's written
in Python, we can also add support

266
00:16:06,394 --> 00:16:08,584
for data classes for dictionaries.

267
00:16:08,944 --> 00:16:11,704
And for each of these properties
that you're seeing here, there's

268
00:16:11,704 --> 00:16:13,174
an equivalent disallowed one.

269
00:16:13,234 --> 00:16:16,324
So for example, looking at allowed
field names, there's a disallowed

270
00:16:16,324 --> 00:16:18,334
field names or disallowed field types.

271
00:16:18,834 --> 00:16:25,435
So again, using this, engineers could
know very succinctly specify, this is the

272
00:16:25,435 --> 00:16:28,525
data that we want you to capture, right?

273
00:16:28,824 --> 00:16:34,015
And they don't have to worry about where
and which component is handling that data.

274
00:16:34,314 --> 00:16:38,665
They could focus on the data that's
exchanged, and then we would provide

275
00:16:38,665 --> 00:16:42,955
the hooks and wrap each of those
components that they have, and then

276
00:16:42,955 --> 00:16:47,094
capture all the data that they've
access to capture in this configuration.

277
00:16:47,594 --> 00:16:51,525
So in terms of wrapping the component
methods, we know that we needed to

278
00:16:51,525 --> 00:16:58,005
inject some logic to capture the, to
capture to we need, we know we needed

279
00:16:58,005 --> 00:17:02,985
to inject some logic before their
function was called to capture inputs.

280
00:17:03,225 --> 00:17:08,594
So in this example, when the subscription
API is calling the subscription services

281
00:17:08,685 --> 00:17:12,645
Initialized Business method, we know,
as you've seen that comment there,

282
00:17:12,645 --> 00:17:15,045
we know we need to insert some logic.

283
00:17:15,540 --> 00:17:20,429
Before that to capture that business
ID and capture that user id.

284
00:17:20,940 --> 00:17:25,050
But then the thing is, we can't
go modifying all of their code

285
00:17:25,110 --> 00:17:26,760
because that's not sustainable.

286
00:17:27,300 --> 00:17:32,100
So we needed to turn to an advanced
language feature of Python to help

287
00:17:32,100 --> 00:17:33,900
us do this, to help us do this.

288
00:17:33,900 --> 00:17:39,210
Capturing logic behind the scenes without
having to go and modify everybody's code.

289
00:17:39,710 --> 00:17:44,120
So what we wanted to do is we really
wanted to look at the data that was being

290
00:17:44,120 --> 00:17:45,950
passed between the components, right?

291
00:17:46,340 --> 00:17:51,530
So here, the subscription API is
passing a business ID and a user

292
00:17:51,530 --> 00:17:53,360
ID to the subscription service.

293
00:17:53,660 --> 00:17:57,950
And what we wanted to do is look
at the data and once it's, once

294
00:17:57,950 --> 00:18:02,510
that data is specified in the
configuration, then we will capture it.

295
00:18:03,010 --> 00:18:05,470
So in terms of the actual
component wrapping.

296
00:18:05,980 --> 00:18:09,850
Python has a, Python, has a
wonderful notion for doing this.

297
00:18:09,850 --> 00:18:11,020
It's called meta classes.

298
00:18:11,380 --> 00:18:15,190
So a meta class is a class that is
responsible for creating a class,

299
00:18:15,220 --> 00:18:16,960
which is a mouthful of a definition.

300
00:18:17,320 --> 00:18:23,350
But essentially what that means is that we
could create a class that is responsible

301
00:18:23,500 --> 00:18:25,960
for, that could be used to make.

302
00:18:26,460 --> 00:18:29,910
Any class or component that
an engineer wanted to make.

303
00:18:30,150 --> 00:18:35,880
So the subscription API, we could use
our meta class and our meta class could

304
00:18:35,880 --> 00:18:40,350
modify the creation of the subscription
API or the subscription repository

305
00:18:40,350 --> 00:18:44,430
or the subscription service, and then
we could insert our logic, right?

306
00:18:44,850 --> 00:18:49,320
So the meta class gives us this
perfect opportunity to wrap their

307
00:18:49,320 --> 00:18:53,460
functions, and then that allows
us to do capturing logic, right?

308
00:18:53,580 --> 00:18:54,360
And then.

309
00:18:54,795 --> 00:18:56,655
We can call their original function.

310
00:18:56,895 --> 00:18:59,925
So if we look at this simple example
here, what we're doing is that

311
00:18:59,925 --> 00:19:01,425
we're looping over each function.

312
00:19:01,925 --> 00:19:06,125
And then we're defining a new
function that captures telemetry.

313
00:19:06,665 --> 00:19:10,565
And then what we do is that we
call their original function.

314
00:19:11,105 --> 00:19:16,505
And then finally, we replace the original
function with our modified function.

315
00:19:16,955 --> 00:19:21,515
So now what is gonna happen is
that every time someone calls their

316
00:19:21,515 --> 00:19:23,765
function, it will call our logic first.

317
00:19:24,155 --> 00:19:25,865
So in that example, we're looking at.

318
00:19:26,045 --> 00:19:29,520
The initialized business method
on the subscription service.

319
00:19:29,820 --> 00:19:33,930
So what we would've done here is that
we would've wrapped that initialized

320
00:19:33,930 --> 00:19:39,570
business method on the subscription
service, so that what we do is that first

321
00:19:39,570 --> 00:19:43,230
we look at the inputs that are coming
into that initialized business method.

322
00:19:43,620 --> 00:19:46,740
We check if those inputs
are in the configuration.

323
00:19:47,015 --> 00:19:50,525
And if they're in the configuration,
then we'll just capture those

324
00:19:50,525 --> 00:19:52,055
inputs for the engineer.

325
00:19:52,355 --> 00:19:55,865
And then after we've captured those
inputs, we can go ahead and let the

326
00:19:55,865 --> 00:19:58,175
application resume as it would normally.

327
00:19:58,325 --> 00:20:01,745
So then we will call their
logic so that the rest of the

328
00:20:01,745 --> 00:20:03,455
application functions normally.

329
00:20:03,955 --> 00:20:07,225
Here we have a quick example
of how the meta class is used.

330
00:20:07,525 --> 00:20:10,525
So here you can see the
subscription service specifies

331
00:20:10,525 --> 00:20:12,325
that it wants to use the service.

332
00:20:12,325 --> 00:20:13,135
Meta class.

333
00:20:13,195 --> 00:20:14,845
Meta class, right?

334
00:20:14,845 --> 00:20:16,405
And that's how it gets used.

335
00:20:16,615 --> 00:20:21,115
And once an engineer has added
this meta class to their component.

336
00:20:21,895 --> 00:20:25,555
We are now automatically
wrapping their component, right?

337
00:20:25,825 --> 00:20:30,445
And once they have defined the telemetric
capture configuration, which specifies

338
00:20:30,445 --> 00:20:35,575
the data that they want us to capture,
then our meta class will get to work.

339
00:20:35,635 --> 00:20:40,945
Once this initialized business method gets
called, we consult our configuration that

340
00:20:40,945 --> 00:20:43,675
they've provided and look at the inputs.

341
00:20:43,915 --> 00:20:47,575
Okay, you've got a business ID coming in,
let's check if it's in the configuration.

342
00:20:47,575 --> 00:20:50,845
And if it's in the configuration, then
we'll just go ahead and capture it.

343
00:20:51,345 --> 00:20:54,795
And then because this manual
instrumentation process is now becoming

344
00:20:54,795 --> 00:20:58,455
a little bit more magical, one other
thing that we did was also to add

345
00:20:58,455 --> 00:21:02,755
a coverage report that provides,
is provides insights on what gets

346
00:21:02,755 --> 00:21:04,465
captured and what doesn't get captured.

347
00:21:04,765 --> 00:21:08,965
So what you're seeing here is it
will go through each component,

348
00:21:09,295 --> 00:21:10,765
it will go through each method.

349
00:21:10,915 --> 00:21:14,245
It will list each input for each
method, and it will tell you check

350
00:21:14,245 --> 00:21:15,955
box, yes, this is getting covered.

351
00:21:16,195 --> 00:21:17,815
No, this is not getting covered.

352
00:21:18,315 --> 00:21:22,215
And with this simple approach,
regardless of which component

353
00:21:22,215 --> 00:21:24,915
handles the data, we can capture it.

354
00:21:25,415 --> 00:21:29,945
So now engineers can focus on the
data that they want to capture.

355
00:21:30,845 --> 00:21:33,335
They focus on the data, not where.

356
00:21:33,520 --> 00:21:37,420
Not which component, they just focus
on the data that needs to be captured.

357
00:21:37,720 --> 00:21:41,410
And once that's clearly specified,
we will do the heavy lifting.

358
00:21:41,800 --> 00:21:46,510
We will automate the capturing
of the manual instrumentation.

359
00:21:47,010 --> 00:21:53,730
We call it config, accelerated custom
instrumentation, or case I for short.

360
00:21:54,230 --> 00:21:58,335
Now in our pilot, we were able to move the
instrumentation of the new microservice.

361
00:21:59,040 --> 00:22:03,390
From 1% all the way up to about 60%.

362
00:22:03,810 --> 00:22:07,770
Now, this vastly increased the data
and the telemetry that was available

363
00:22:07,770 --> 00:22:12,090
to them for investigating, debugging
and demystifying their system behavior.

364
00:22:12,629 --> 00:22:16,860
So after we actually developed
kci, it took us less than a week

365
00:22:16,860 --> 00:22:18,629
to get the configuration in place.

366
00:22:19,200 --> 00:22:23,280
All of this really accelerated
the manual instrumentation.

367
00:22:23,399 --> 00:22:24,780
They didn't have to spend the time.

368
00:22:25,280 --> 00:22:27,980
Going through manually
instrument in their code.

369
00:22:27,980 --> 00:22:30,770
It didn't take them three months,
it didn't take them six months, it

370
00:22:30,770 --> 00:22:34,520
didn't take them a year because we've
developed this config, accelerated

371
00:22:34,520 --> 00:22:38,960
custom instrumentation, and they can
easily specify with configuration

372
00:22:38,960 --> 00:22:40,760
the data that they want to capture.

373
00:22:41,030 --> 00:22:45,320
That's something that we were able to
work with them and get done in record

374
00:22:45,320 --> 00:22:49,450
time, what does this actually mean
in terms of debugging their systems?

375
00:22:49,690 --> 00:22:52,570
So for instance, if they're
looking at errors, they can

376
00:22:52,570 --> 00:22:54,430
actually break that apart.

377
00:22:54,430 --> 00:22:55,930
It's not just errors.

378
00:22:56,140 --> 00:22:59,110
They can break that apart by
various dimensions based on the

379
00:22:59,110 --> 00:23:01,120
data that the application handles.

380
00:23:01,450 --> 00:23:02,800
So in this example.

381
00:23:03,540 --> 00:23:06,630
They're breaking those errors
down by the event type, which

382
00:23:06,630 --> 00:23:10,920
reveals that the majority of errors
are updated event types, right?

383
00:23:11,310 --> 00:23:14,730
And there's many other dimensions
that they could break the different

384
00:23:14,730 --> 00:23:17,860
errors apart by, and they could
combine all of these things.

385
00:23:17,860 --> 00:23:21,830
And the reason that this is important
is that, just as you remember that

386
00:23:21,860 --> 00:23:25,900
story that we told about our on-call
engineer trying to figure out

387
00:23:25,900 --> 00:23:27,640
why do we have this premium leak?

388
00:23:28,140 --> 00:23:33,630
This is really important to have a lot
of high quality manual instrumentation

389
00:23:33,660 --> 00:23:38,310
because that's what's gonna help you
when you're trying to debug your system.

390
00:23:38,520 --> 00:23:40,380
You're looking at some
kind of degradation.

391
00:23:40,380 --> 00:23:44,130
In this case we're looking at arrows, but
you're able to break that apart so you

392
00:23:44,130 --> 00:23:48,981
can start to draw different boundaries
to understand exactly which segment

393
00:23:48,981 --> 00:23:50,661
of your customers are being affected.

394
00:23:51,161 --> 00:23:53,141
And similarly, if they
were looking at latency.

395
00:23:53,231 --> 00:23:55,811
Another quick example is not just latency.

396
00:23:56,201 --> 00:23:57,671
They can actually break that down.

397
00:23:57,971 --> 00:24:00,791
So in this example, we're showing
a graph where they're looking at

398
00:24:00,791 --> 00:24:04,421
the latency, but they're breaking
that down by the Kafka topic.

399
00:24:04,751 --> 00:24:08,441
And here they can see that, okay,
apparently the Kafka, the company Kafka

400
00:24:08,531 --> 00:24:10,451
topic is taking longer to process it.

401
00:24:10,451 --> 00:24:14,831
It seems to have, a lot more peaks than
some of the other topics that they have.

402
00:24:15,731 --> 00:24:16,061
And.

403
00:24:16,436 --> 00:24:21,026
All of this data that they have available
to them, all of this quality manual

404
00:24:21,026 --> 00:24:24,476
instrumentation that we're talking about
isn't just available in their traces.

405
00:24:24,686 --> 00:24:28,046
It's also there in their logs and
it's also in their errors, right?

406
00:24:28,076 --> 00:24:32,366
They get this high fidelity wherever
they're using and accessing the telemetry.

407
00:24:33,161 --> 00:24:37,271
I. That's the power of manual
instrumentation and we've

408
00:24:37,271 --> 00:24:39,461
accelerated it for them using ksa.

409
00:24:40,121 --> 00:24:45,221
Now, in terms of what's next for us, we're
thinking about scaling up for the other

410
00:24:45,221 --> 00:24:47,980
40 plus services that we need to capture.

411
00:24:48,280 --> 00:24:50,690
So some of the things we're
thinking about there is, in this.

412
00:24:51,045 --> 00:24:55,065
In this talk I talked about a service
meta class, and we only looked at

413
00:24:55,065 --> 00:24:56,985
one of the components in our pilot.

414
00:24:56,985 --> 00:25:00,195
We looked at one of the component types
that they had, which are services,

415
00:25:00,195 --> 00:25:02,095
which holds our business logic.

416
00:25:02,515 --> 00:25:05,975
But we're looking at making that
component agnostic so that we have

417
00:25:05,975 --> 00:25:07,745
meta class, we have a meta class.

418
00:25:07,745 --> 00:25:10,840
Know that it doesn't matter
what component type you have.

419
00:25:11,340 --> 00:25:14,130
We'll be able to capture and
hook into that component and

420
00:25:14,130 --> 00:25:16,110
capture the telemetry for you.

421
00:25:16,470 --> 00:25:21,450
Another thing we're working on is you'll
see in this talk that we focus on inputs.

422
00:25:21,450 --> 00:25:26,580
So you know when the API called the
service, we talked about the inputs, but

423
00:25:26,580 --> 00:25:29,850
we haven't actually started to capture
outputs, and that's something that

424
00:25:29,850 --> 00:25:31,440
we're interested in looking to as well.

425
00:25:32,340 --> 00:25:35,400
Another thing that we're really
excited to look at is thinking about

426
00:25:35,400 --> 00:25:39,540
how we can actually use LLMs to help
accelerate the pace of doing and

427
00:25:39,540 --> 00:25:41,580
installing this integration, right?

428
00:25:41,880 --> 00:25:46,150
Because what we really want to do is we
really want to tighten the feedback loop

429
00:25:46,510 --> 00:25:51,580
between adding the instrumentation and
seeing the value of that instrumentation

430
00:25:51,580 --> 00:25:52,720
and seeing what you can do.

431
00:25:52,720 --> 00:25:56,820
For example, in Datadog, we really want
to tighten that loop, engineers can

432
00:25:56,820 --> 00:25:58,320
really get a clearer picture of that.

433
00:25:58,500 --> 00:26:01,190
And we really want to use
LMS to help us do that.

434
00:26:01,190 --> 00:26:04,610
And because we're a small team
as well, it really helps us scale

435
00:26:04,610 --> 00:26:06,800
our impact and scale our voices.

436
00:26:06,800 --> 00:26:11,290
I. And then finally, we later found
out that data log actually has some

437
00:26:11,290 --> 00:26:16,720
abilities in their DD Trace Library
to propagate telemetry between spans.

438
00:26:16,960 --> 00:26:20,350
And so that's something that we're very
interested to looking into as well.

439
00:26:20,850 --> 00:26:21,180
All right.

440
00:26:21,180 --> 00:26:25,320
And that has brought us to
the end of the talk here.

441
00:26:25,725 --> 00:26:27,705
Instrument in at 10 years for a second.

442
00:26:28,435 --> 00:26:29,835
I want to thank you for tuning in.

443
00:26:30,175 --> 00:26:32,595
The best way to find me is
probably on LinkedIn Jaw mark.

444
00:26:32,605 --> 00:26:36,565
I also blog at JY 13 hash node dev.

445
00:26:36,865 --> 00:26:39,865
But I'd really like to thank you for
coming and listening to our talk.

446
00:26:40,015 --> 00:26:45,115
I hope you really enjoy the rest of
the talks at Con 42 and looking forward

447
00:26:45,115 --> 00:26:46,495
to hearing, hearing from all of you.

448
00:26:46,495 --> 00:26:47,095
If you have.

449
00:26:47,105 --> 00:26:50,315
Ideas if you've done something
similar or if you just want to

450
00:26:50,315 --> 00:26:53,735
chat about observability, I love
to geek out about observability.

451
00:26:53,735 --> 00:26:55,145
So looking forward to hearing from you.

452
00:26:55,295 --> 00:26:55,745
Thanks.

453
00:26:55,985 --> 00:26:56,345
Bye-bye.

