1
00:00:00,500 --> 00:00:06,530
Hi, first of all, thank you for
Con Fortitude team for providing an

2
00:00:06,530 --> 00:00:13,749
opportunity to present a topic on
optimizing SQL server infrastructure

3
00:00:13,870 --> 00:00:17,110
for LLM workloads in hybrid environment.

4
00:00:17,610 --> 00:00:18,840
Thanks for joining.

5
00:00:19,140 --> 00:00:20,460
My name is Siva.

6
00:00:20,960 --> 00:00:25,940
I'm working as a principal
database administrator in Paycor,

7
00:00:26,660 --> 00:00:28,910
which is now a Paychex company.

8
00:00:29,410 --> 00:00:38,229
In this talk, I will present about how
SQL Server get ready for LLM workloads.

9
00:00:38,729 --> 00:00:40,709
Let's start the presentation.

10
00:00:41,209 --> 00:00:41,509
Yeah.

11
00:00:42,124 --> 00:00:50,854
The agenda of this talk is the LLM
infrastructure challenges and legacy SQL

12
00:00:50,854 --> 00:00:58,155
server limitations, Azure SQL managed
instance, and I link for real time

13
00:00:58,335 --> 00:01:03,305
replication, performance optimization,
and global architecture patterns.

14
00:01:03,805 --> 00:01:09,795
Security, governance and real
world implementation of EMI, future

15
00:01:10,005 --> 00:01:12,375
proofing your data infrastructure.

16
00:01:12,825 --> 00:01:15,585
This is the agenda for this presentation.

17
00:01:16,085 --> 00:01:16,475
Okay.

18
00:01:16,475 --> 00:01:22,895
The, what are the challenges we have
right now for LLM infrastructure,

19
00:01:23,395 --> 00:01:25,380
that traditional SQL server.

20
00:01:25,880 --> 00:01:32,600
He's having a limitations, like a
high latency data, access patterns,

21
00:01:33,410 --> 00:01:40,820
limited scalability for concurrent
EA workloads, complex replication

22
00:01:40,820 --> 00:01:46,584
across environments, performance
bottlenecks in real time scenarios.

23
00:01:47,084 --> 00:01:53,654
So the modern LLM requirements are
subsecond response times per prompts,

24
00:01:54,154 --> 00:02:01,385
continuous data synchronization,
elastic scaling per variable demand.

25
00:02:02,225 --> 00:02:06,244
Enterprise creates
security and governance.

26
00:02:06,744 --> 00:02:10,899
So why legacy SQL
architecture struggle with ai?

27
00:02:11,399 --> 00:02:19,440
In legacy deployments, we haven't designed
for a unique demand of LLM workloads.

28
00:02:20,399 --> 00:02:27,570
Prompt engineering requires instant
access to synchronized and courage

29
00:02:27,570 --> 00:02:33,690
should be optimized for data access
across the distributed systems.

30
00:02:34,190 --> 00:02:39,620
So the gap between yay application
velocity and database respond

31
00:02:39,620 --> 00:02:46,340
times create a critical bottleneck
that impacts the user experience

32
00:02:46,340 --> 00:02:48,770
and limits of AI capabilities.

33
00:02:49,270 --> 00:02:56,435
To address these solutions, Microsoft
provides a managed instance that is the.

34
00:02:56,935 --> 00:03:04,265
Built for hybrid AI in this hybrid
cloud integration it helps to

35
00:03:04,265 --> 00:03:10,525
synchronize the data from on-premises
to Azure without any latency.

36
00:03:11,025 --> 00:03:16,875
And this is all also have
the built in governance and

37
00:03:16,875 --> 00:03:19,485
compliance and data protections.

38
00:03:19,985 --> 00:03:21,300
So this helps you.

39
00:03:22,040 --> 00:03:27,200
Increase your performance
based on requirements or need.

40
00:03:27,700 --> 00:03:34,780
So to set up manage instance, the
communication between your on-prem

41
00:03:34,780 --> 00:03:41,055
server to manage instance will be
taken care through MI link that

42
00:03:41,055 --> 00:03:43,935
is called Managed Instance Link.

43
00:03:44,435 --> 00:03:45,305
This link.

44
00:03:45,805 --> 00:03:51,375
Take care of synchronization between
Uber, Azure, and on-prem environment.

45
00:03:52,335 --> 00:03:57,705
The synchronization will happen
based on log based synchronization,

46
00:03:58,205 --> 00:04:02,305
which is similar to Uber,
always on ayc configuration.

47
00:04:03,145 --> 00:04:04,585
The latency would be near zero.

48
00:04:05,085 --> 00:04:11,895
So with this less latency, your
Azure environment MI instance, is

49
00:04:11,895 --> 00:04:18,695
ready for your LLM access because it
helps current and consistent data.

50
00:04:19,195 --> 00:04:23,240
So the advantage of MI it is
have a performance optimization.

51
00:04:23,740 --> 00:04:30,070
You can have utilize query store
insights that leverage historical query

52
00:04:30,070 --> 00:04:37,540
performance data to identify patterns,
optimize execution plans, and eliminate

53
00:04:37,630 --> 00:04:43,590
bottlenecks before the impact of A
LLM response times the other benefit

54
00:04:43,860 --> 00:04:46,410
memory control workload classification.

55
00:04:46,910 --> 00:04:53,570
So am I intelligently allocate resources
based on workload, priority, ensuring

56
00:04:53,570 --> 00:05:00,080
prompts, serving queries, receive a
dedicated memory and compute resources.

57
00:05:00,580 --> 00:05:03,920
The other benefit is
concurrency optimization.

58
00:05:04,420 --> 00:05:11,360
Fine tune connection, polling parallelism
settings and isolation levels to handle.

59
00:05:12,355 --> 00:05:16,675
Hundreds of simultaneous
requests without degradation.

60
00:05:17,175 --> 00:05:21,045
So architecture pattern
for a global prompt.

61
00:05:21,045 --> 00:05:27,255
Data sourcing organizations with
a global operations need prompt

62
00:05:27,255 --> 00:05:32,565
data available across regions
with consistent performance.

63
00:05:33,065 --> 00:05:38,750
To fulfill this requirement, the am I
link enables multi-region replication

64
00:05:38,750 --> 00:05:45,805
to apology, where on premises SQL server
feeds regional ministry instances.

65
00:05:46,305 --> 00:05:53,310
This architecture reduces cross
regional latency while maintaining data

66
00:05:53,310 --> 00:05:57,450
consistency, allowing LLMs to serve.

67
00:05:58,155 --> 00:06:05,115
Users from the nearest data source,
regardless of where the personal data

68
00:06:05,115 --> 00:06:12,605
resides, in simplest terms, you can set
up multiple MI links across multiple

69
00:06:12,605 --> 00:06:16,475
regions from one source database.

70
00:06:16,975 --> 00:06:22,795
Key performance metrics for LLM
workloads, target correl latency.

71
00:06:23,230 --> 00:06:29,560
The response time, concurrent
request, it'll support multiple

72
00:06:30,160 --> 00:06:31,930
queries on the same time.

73
00:06:32,830 --> 00:06:34,270
And the availability.

74
00:06:34,540 --> 00:06:39,384
Microsoft will take care of
H-E-A-D-R per Azure manager

75
00:06:39,784 --> 00:06:45,634
instance, and there is a less minimum
delay to synchronize the data.

76
00:06:46,580 --> 00:06:50,929
These are the key performance
metrics for LLM workloads.

77
00:06:51,429 --> 00:06:59,140
The other advantage with the manager
instances is you can utilize it

78
00:06:59,140 --> 00:07:06,029
as a per hybrid data disaster
recovery, as well as AI continuity.

79
00:07:06,529 --> 00:07:10,339
For example, your primary SQL server.

80
00:07:10,839 --> 00:07:12,819
That is your production environment.

81
00:07:12,999 --> 00:07:19,929
You have a setup for MI link with the
MI instance so that the MI instance

82
00:07:19,959 --> 00:07:29,275
will help to synchronize the data and
it act as a read access for your LLM

83
00:07:29,275 --> 00:07:34,045
workloads and as well as works as a d.

84
00:07:34,545 --> 00:07:35,114
Environment.

85
00:07:35,685 --> 00:07:41,884
So in, in case of disaster, you can
utilize and you can fail over your

86
00:07:42,384 --> 00:07:46,464
right loads from on premises to Azure.

87
00:07:46,764 --> 00:07:51,504
So it acts as a both rules
A, a access as well as Dr.

88
00:07:52,004 --> 00:07:54,419
The other advantage of MI is.

89
00:07:54,919 --> 00:07:59,734
It'll a hundred percent integrates
with Azure synapse integration.

90
00:08:00,234 --> 00:08:06,384
So with this, you no need
to have traditional ETLs.

91
00:08:06,884 --> 00:08:11,984
The integration analytics with
Azure Synapse Link enables advanced

92
00:08:12,484 --> 00:08:14,194
augment generation scenarios.

93
00:08:15,124 --> 00:08:20,704
Pre aggregated and index frequently
accessed the prompt context in

94
00:08:20,704 --> 00:08:25,984
synapse producing query complexity
and improving response times.

95
00:08:26,484 --> 00:08:31,524
This separation of concerns allow
transactional SQL workloads and

96
00:08:31,734 --> 00:08:37,194
analytical prompt preparation to
operate independently, preventing

97
00:08:37,194 --> 00:08:42,414
resource contention, and optimizing
each of its specific purpose.

98
00:08:42,914 --> 00:08:46,724
This is one of the example for
a real world implementation.

99
00:08:46,814 --> 00:08:53,774
For example, you assume that there
is a global bank presence across the

100
00:08:53,804 --> 00:09:01,174
world and they want to set up a LLM
chat bot with the their customers.

101
00:09:01,984 --> 00:09:06,724
And the major data is an
on-premises environment.

102
00:09:07,224 --> 00:09:15,524
In this case, you can use MI link and
set up a managed instances across the

103
00:09:15,524 --> 00:09:24,054
regions so that the bot can access the
local MI instance instead of everything

104
00:09:24,054 --> 00:09:27,624
hitting too, you source on premises data.

105
00:09:28,134 --> 00:09:32,484
So this helps the response
times and reduces the latency.

106
00:09:32,984 --> 00:09:40,274
With this solution, you can reduce
the prompts, prompt response latency,

107
00:09:40,814 --> 00:09:42,999
and it is compliance with the

108
00:09:43,499 --> 00:09:49,099
regular standards and it
supports concurrent AA sessions,

109
00:09:49,599 --> 00:09:52,514
security and governance in a pipeline.

110
00:09:53,014 --> 00:09:55,804
This helps data classification you.

111
00:09:56,374 --> 00:10:01,444
You can use automated discovery
labeling of since two data elements

112
00:10:01,444 --> 00:10:07,134
used in prom access control, role
level and column level security

113
00:10:07,134 --> 00:10:09,504
and force across the replication.

114
00:10:09,504 --> 00:10:12,354
Each targets audit trials.

115
00:10:12,414 --> 00:10:15,834
You can set up a
comprehensive logging all.

116
00:10:16,334 --> 00:10:22,384
AI driven data access for
compliance and reporting encryption,

117
00:10:23,014 --> 00:10:29,874
always encrypted columns and TDE
protected data in transit at rest.

118
00:10:30,324 --> 00:10:36,384
So these are the features you
can use in your MI instance

119
00:10:36,444 --> 00:10:39,204
based on your requirements.

120
00:10:39,704 --> 00:10:47,244
So to tune over LLM, these are the
things we need to follow first.

121
00:10:47,304 --> 00:10:48,564
Enable query store.

122
00:10:49,284 --> 00:10:55,134
This allows to capture all, all over
queries, so it helps to fine tune

123
00:10:55,134 --> 00:10:57,444
if any queries taking longer time.

124
00:10:57,944 --> 00:11:00,724
Configure Resource
Governor for AI workloads.

125
00:11:01,489 --> 00:11:07,519
So if you configure this one, you
can separate ever prompt queries

126
00:11:07,519 --> 00:11:13,969
and other queries so that users
can get a quick response, implement

127
00:11:13,969 --> 00:11:16,189
connection, polling best practices.

128
00:11:16,249 --> 00:11:20,449
You can size over buffer polls
based on every requirement.

129
00:11:20,949 --> 00:11:26,769
Optimize index for read heavy
patterns, like create a proper

130
00:11:26,769 --> 00:11:28,359
index based on your query.

131
00:11:28,859 --> 00:11:31,109
Which improves the response times.

132
00:11:31,609 --> 00:11:38,199
Monitor your MI link logs to ensure
that everything is working in sync.

133
00:11:38,699 --> 00:11:42,329
So future programing, your
AI data infrastructure.

134
00:11:42,390 --> 00:11:46,925
So the implementation first
phase, you have to assess.

135
00:11:47,425 --> 00:11:52,555
Evaluate current SQL workload and
identify yay integration points.

136
00:11:53,055 --> 00:11:58,845
Phase to optimize, implement
MI replication and tune for

137
00:11:59,295 --> 00:12:02,415
low latency access scale.

138
00:12:03,075 --> 00:12:06,905
Expand multi-region architecture
with integrated analytics.

139
00:12:07,405 --> 00:12:11,515
Phase four, innovate leverage
hybrid infrastructure for the

140
00:12:11,515 --> 00:12:14,125
next generation AI applications.

141
00:12:14,625 --> 00:12:20,805
So the key takeaways is
modernize without disruptions.

142
00:12:21,314 --> 00:12:26,505
MI link brings like CQL and
modern AI requirements without

143
00:12:26,505 --> 00:12:28,155
requiring full migration.

144
00:12:28,655 --> 00:12:33,334
Optimize for real time courage,
store and workload classification and

145
00:12:34,025 --> 00:12:36,964
tuning eliminates LLM bottlenecks.

146
00:12:37,464 --> 00:12:39,234
Scale intelligently.

147
00:12:39,234 --> 00:12:43,104
Hybrid architecture supports
global workloads while

148
00:12:43,104 --> 00:12:46,165
maintaining enterprise governance.

149
00:12:46,665 --> 00:12:47,355
Thank you.

150
00:12:47,385 --> 00:12:49,370
Thank you for providing this opportunity.

