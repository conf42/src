1
00:00:01,410 --> 00:00:02,009
Hi everyone.

2
00:00:02,440 --> 00:00:04,570
Welcome to the Con 42
Machine Learning Conference.

3
00:00:04,600 --> 00:00:05,620
Hope you're having a good day.

4
00:00:05,980 --> 00:00:10,180
My name is S Kati and I've actually
been working as a senior software

5
00:00:10,180 --> 00:00:12,470
engineer at Meta for about a year now.

6
00:00:12,720 --> 00:00:17,700
Before that I used to work at Google
for about four years and prior to that,

7
00:00:17,700 --> 00:00:21,610
Walmart Labs and prior to that in a
startup for about another five years.

8
00:00:21,960 --> 00:00:27,219
Basically, today we are actually going to
talk about something that I've actually

9
00:00:27,219 --> 00:00:29,379
been working on for quite some time.

10
00:00:29,750 --> 00:00:33,620
And I think it'll actually provide
you with great amounts of insights

11
00:00:33,620 --> 00:00:38,110
on how to make your decision when it
comes to building analytics pipelines.

12
00:00:38,600 --> 00:00:42,440
On that note, we are basically going
to do a deep dive into the fascinating

13
00:00:42,440 --> 00:00:44,460
world of real time machine learning.

14
00:00:45,025 --> 00:00:49,945
So we'll explore how to build analytics
pipelines that delivers insights in less

15
00:00:49,945 --> 00:00:54,810
than a second with the enormous amount of
volumes volumes of data generated today

16
00:00:54,810 --> 00:00:59,790
which is around 2.5 quintilian bytes
each day, which is not an normal amount.

17
00:00:59,820 --> 00:01:01,200
It's quite enormous, right?

18
00:01:01,740 --> 00:01:04,320
Traditional methods
simply aren't keeping up.

19
00:01:04,670 --> 00:01:09,500
We'll unpack why speed matters,
look at the challenges involved, and

20
00:01:09,500 --> 00:01:14,540
discuss how industries are gaining
major advantages from realtime ml.

21
00:01:16,605 --> 00:01:19,545
Now let's actually talk about
the data explosion challenge.

22
00:01:19,845 --> 00:01:25,545
First, let's understand the sheer scale
of the data problem we face every day.

23
00:01:26,095 --> 00:01:30,295
Like literally as we said, we produce
about 2.5 quintillion bytes of data,

24
00:01:30,505 --> 00:01:32,065
which is even hard to visualize.

25
00:01:32,455 --> 00:01:34,675
So to put it into perspective, right?

26
00:01:35,075 --> 00:01:39,485
If we were to stack blue, red
disks containing this data, they'd

27
00:01:39,485 --> 00:01:40,925
reach all the way to the moon.

28
00:01:40,930 --> 00:01:44,610
Again, like this is it's
even impossible to visualize.

29
00:01:45,240 --> 00:01:49,290
Around 75% of the companies now rely
on machine learning applications

30
00:01:49,290 --> 00:01:51,240
that need immediate responses.

31
00:01:51,960 --> 00:01:55,110
So for these businesses,
waiting isn't an option.

32
00:01:55,200 --> 00:02:00,700
I. Real-time analytics provides
insights 35 past percent faster and

33
00:02:00,700 --> 00:02:03,340
boosts efficiency by almost 42%.

34
00:02:03,970 --> 00:02:08,050
Imagine how crucial the speed is
for decision making during live

35
00:02:08,050 --> 00:02:11,500
events, medical diagnostic, and
final financial transactions.

36
00:02:11,890 --> 00:02:14,590
Of course, like you will see
many businesses, which still

37
00:02:14,590 --> 00:02:16,210
do rely on batch processing.

38
00:02:16,640 --> 00:02:19,610
We'll of course come to that what
batch processing is versus what

39
00:02:19,610 --> 00:02:21,380
streaming pre stream processing is.

40
00:02:21,830 --> 00:02:24,560
But the thing that I want you
to keep in perspective is.

41
00:02:25,065 --> 00:02:28,155
Every business has its own
needs for some businesses.

42
00:02:28,155 --> 00:02:34,245
It's very important for you to make sure
that you actually process real time data.

43
00:02:34,275 --> 00:02:37,245
For example, financial transac
financial transactions, right?

44
00:02:37,995 --> 00:02:40,455
Real time insights are
the ones which matter.

45
00:02:40,485 --> 00:02:41,595
Like for example, let's say.

46
00:02:41,940 --> 00:02:45,530
Basically if if a fraud is being
detected, which is let's say a fraud

47
00:02:45,530 --> 00:02:48,690
occurred like, let's say two hours
ago, but you're detecting right now it

48
00:02:48,690 --> 00:02:50,280
wouldn't make much of a sense again.

49
00:02:50,330 --> 00:02:53,180
We'll put things into perspective as
and when we go through the slides,

50
00:02:53,480 --> 00:02:57,990
but I just wanted to visualize how
businesses want to process data.

51
00:02:57,990 --> 00:03:01,680
And of course, like it differs from
business to business now coming to

52
00:03:01,740 --> 00:03:03,180
batch versus streaming processing.

53
00:03:03,770 --> 00:03:08,340
So I would want to you, I want, I
would want you to take a moment to

54
00:03:08,340 --> 00:03:11,730
realize what batch processing is
versus what stream processing is.

55
00:03:12,240 --> 00:03:15,365
Batch processing usually is
typically takes hours or days.

56
00:03:16,740 --> 00:03:20,040
Which is basically for
monthly or weekly reports.

57
00:03:20,370 --> 00:03:24,450
For example, let's say you are running
an ad tech business and you would want

58
00:03:24,450 --> 00:03:30,060
your business people to understand more
insights into how your ads are performing.

59
00:03:30,060 --> 00:03:33,810
I. Over a day for the past week, for
the past month, so on and so forth,

60
00:03:34,200 --> 00:03:38,250
you would rather want to stick with
your batch processing pipelines because

61
00:03:38,250 --> 00:03:42,090
which run like, let's say every hour,
every day, so on and so forth, which

62
00:03:42,090 --> 00:03:43,680
would definitely fit your needs.

63
00:03:43,680 --> 00:03:44,340
That's totally fine.

64
00:03:44,340 --> 00:03:47,950
You would not want to go for very
high-end like engineering heavy

65
00:03:47,950 --> 00:03:49,060
stream processing pipelines.

66
00:03:50,505 --> 00:03:54,015
But on the other note, think about
what I have been discussing just now.

67
00:03:54,225 --> 00:03:55,545
Think about fraud detection.

68
00:03:56,145 --> 00:04:00,195
What if your bank took hours to
identify suspicious activities?

69
00:04:00,245 --> 00:04:01,205
It's not at all good.

70
00:04:01,745 --> 00:04:04,745
So now coming to that, there
is an intermediate approach

71
00:04:04,745 --> 00:04:05,630
called micro batching.

72
00:04:06,230 --> 00:04:09,830
Which basically cuts the latency
to milliseconds, of course,

73
00:04:09,830 --> 00:04:12,850
like basically, which is which
in my opinion is really good.

74
00:04:12,850 --> 00:04:17,400
For example, micros Micro batching
has actually been serving like near

75
00:04:17,400 --> 00:04:19,410
realtime needs for quite some time now.

76
00:04:19,950 --> 00:04:23,910
But realtime streaming actually
goes a step further, like as in.

77
00:04:24,315 --> 00:04:28,305
How do you cut down that latency
to under 10 milliseconds?

78
00:04:28,305 --> 00:04:31,975
Like for example I agree there is,
there are instances where you would

79
00:04:31,975 --> 00:04:34,885
want to go for micro batching, but you
would definitely want to understand

80
00:04:34,885 --> 00:04:36,235
where your business fits, right?

81
00:04:36,775 --> 00:04:40,855
So let's say you are actually in
your amazon amazon.com and you are

82
00:04:40,855 --> 00:04:43,565
buying like a couple of things,
like for that matter, let's say

83
00:04:43,595 --> 00:04:45,155
diapers for your kids or whatever.

84
00:04:45,525 --> 00:04:46,275
Basically.

85
00:04:46,330 --> 00:04:50,860
It takes a couple of minutes for you to
actually go from the homepage to the page

86
00:04:50,860 --> 00:04:54,790
where the product is, to the add to cart
page, and then again the checkout page.

87
00:04:55,750 --> 00:05:02,710
So now let's say that you are an engineer
at Amazon and you want to identify all

88
00:05:02,710 --> 00:05:06,940
the people who are buying diapers, let's
say in the, who are buying diapers.

89
00:05:07,540 --> 00:05:11,150
So you got an event into your
into Amazon saying that, Hey,

90
00:05:11,150 --> 00:05:12,230
somebody bought diapers, right?

91
00:05:12,800 --> 00:05:16,600
You would want to identify what what
items were they looking at prior to that?

92
00:05:17,080 --> 00:05:20,650
So as in what is a really,
what is the path of the user

93
00:05:20,650 --> 00:05:22,900
starting from the homepage,
which is really important, right?

94
00:05:23,140 --> 00:05:24,730
Otherwise, where would
you want to put ads?

95
00:05:25,060 --> 00:05:28,790
How would you want to categorize things
which are bought together usually?

96
00:05:28,790 --> 00:05:32,690
So for such sort of things, you would
want to analyze the path of the user.

97
00:05:33,110 --> 00:05:34,370
To do that, you really want to.

98
00:05:34,690 --> 00:05:38,350
Use micro batching because then you
would want to, let's say check, okay,

99
00:05:38,500 --> 00:05:43,000
this time the buying occurred, let's
actually go back a couple of windows,

100
00:05:43,000 --> 00:05:46,600
which is a couple of milliseconds, a
couple of seconds or a couple of minutes

101
00:05:46,990 --> 00:05:49,030
back to identify what really went on.

102
00:05:49,610 --> 00:05:53,480
Such sort of things is where microb
batching is really like really used

103
00:05:53,810 --> 00:05:55,730
and it actually is really good too.

104
00:05:55,820 --> 00:05:59,450
But there are obviously some businesses
which definitely want to take it a

105
00:05:59,450 --> 00:06:01,160
step further to real time streaming.

106
00:06:01,995 --> 00:06:05,655
As I said, which cuts down the
latency to under 10 milliseconds.

107
00:06:06,315 --> 00:06:11,205
Now, this near instant processing
is essential in scenarios like

108
00:06:11,205 --> 00:06:15,045
let's say livestock trading,
realtime gaming, dynamic online

109
00:06:15,045 --> 00:06:16,245
content, so on and so forth.

110
00:06:16,665 --> 00:06:19,935
As you can see, many of the
FinTech startups have come up and

111
00:06:19,935 --> 00:06:21,670
many the FinTech scenarios have.

112
00:06:21,690 --> 00:06:23,820
Like literally exploded after the ai boom.

113
00:06:24,180 --> 00:06:25,620
You can see livestock trading, right?

114
00:06:25,620 --> 00:06:29,310
That's an excellent example where real
time streaming is much more important.

115
00:06:29,310 --> 00:06:33,780
You would definitely want to get real time
data, as in how the stock is performing

116
00:06:33,780 --> 00:06:38,280
or as in how the options are being
sold but so on and so forth to make an

117
00:06:38,280 --> 00:06:40,770
informed decision about what to do next.

118
00:06:41,490 --> 00:06:44,550
So that's where real time
streaming is what is required.

119
00:06:44,550 --> 00:06:48,875
So I hope you might, you could have
actually at least, form the mental

120
00:06:48,875 --> 00:06:53,045
picture of where batch micro batching
and stream processing sit in the

121
00:06:53,045 --> 00:06:55,505
entire analytics pipeline scenario.

122
00:06:55,895 --> 00:07:00,165
But of course, like in this particular
thing, we would definitely want to as part

123
00:07:00,165 --> 00:07:04,365
of this talk, I would definitely want to
dig deep into streaming analytics per se.

124
00:07:04,425 --> 00:07:08,065
But again, of course, like batch
I think is easily understood.

125
00:07:08,065 --> 00:07:09,865
Micro batching is a little
harder to understand.

126
00:07:09,865 --> 00:07:13,250
But yeah, feel free to stick with me
and I promise you'll get better insight.

127
00:07:14,030 --> 00:07:16,850
So now starting with, let's
say financial services, right?

128
00:07:17,715 --> 00:07:18,195
Oh, sorry.

129
00:07:18,495 --> 00:07:19,275
I think I might have.

130
00:07:20,460 --> 00:07:21,360
Yeah, my bad.

131
00:07:21,780 --> 00:07:24,990
So now starting with financial
services, real time fraud

132
00:07:24,990 --> 00:07:27,400
detection is much more important.

133
00:07:27,430 --> 00:07:30,950
Of course, as and when as and when the
internet exploded and frankly, as, and

134
00:07:30,950 --> 00:07:35,360
when the AI is currently taking over the
world, you can see many and many more

135
00:07:35,420 --> 00:07:37,655
new type of frauds and scams popping up.

136
00:07:38,240 --> 00:07:41,480
So it is much more important to
do real time fraud detection.

137
00:07:41,765 --> 00:07:45,485
So as to make sure that they are,
the banking is secure, right?

138
00:07:45,785 --> 00:07:49,865
So banks started adopting the real-time
fraud detection, which have reduced

139
00:07:49,865 --> 00:07:54,455
their losses by almost about 27% for,
let's say, considering a mid-sized

140
00:07:54,455 --> 00:07:58,475
bank that translates to around
15 million savings in each year,

141
00:07:58,535 --> 00:07:59,915
which is not a normal amount again.

142
00:08:00,365 --> 00:08:04,175
So traditional fraud detection
mechanisms analyze data.

143
00:08:04,970 --> 00:08:08,750
After the event has occurred, like
as we were just discussing, which

144
00:08:08,750 --> 00:08:11,840
is often too late because of course
the event has already occurred, the

145
00:08:11,840 --> 00:08:16,940
transaction has already occurred, and
as such, it is now near impossible

146
00:08:16,940 --> 00:08:19,070
to reverse that particular event.

147
00:08:19,875 --> 00:08:23,925
But near, but the real time systems
that we are talking about actually

148
00:08:23,925 --> 00:08:27,495
flag the unusual activities
as in when they're occurring.

149
00:08:27,915 --> 00:08:32,405
So you can actually stop the
fraud activity before the event

150
00:08:32,405 --> 00:08:36,605
completes, which is what we, which
is what is of utmost importance.

151
00:08:37,055 --> 00:08:41,675
For example, let's say detecting multiple
purchases in distant locations, right?

152
00:08:42,060 --> 00:08:47,055
I've had the situation where my credit
card, got hacked again, like I, I assume

153
00:08:47,085 --> 00:08:50,505
many people might have actually gone
through the same scenario and like, why?

154
00:08:50,775 --> 00:08:55,315
While I myself live in California, I. I've
actually had some business transaction

155
00:08:55,315 --> 00:09:01,935
go off in, let's say, Tennessee, Florida
which again, like from from from tracking

156
00:09:01,935 --> 00:09:05,295
my, let's say my recent transactions,
let's say, which occurred literally

157
00:09:05,295 --> 00:09:09,615
one hour ago, happened in California,
but the next transaction, which is

158
00:09:09,615 --> 00:09:13,340
currently occurring now is happening in
Tennessee, which is quite impossible.

159
00:09:13,690 --> 00:09:17,770
So basically flagging that particular
transaction as fraud actually

160
00:09:17,770 --> 00:09:18,910
would save me a lot of money.

161
00:09:19,310 --> 00:09:22,445
Again, like these sort of
scenarios are what are important.

162
00:09:23,485 --> 00:09:27,475
So now how do we like detecting
multiple purchases in, let's say,

163
00:09:27,475 --> 00:09:29,245
distant location simultaneously?

164
00:09:29,665 --> 00:09:33,625
Some, something traditional batch
methods would completely miss, right?

165
00:09:33,625 --> 00:09:37,645
Let's say for example, if you are taking
a batch processing approach to this

166
00:09:37,645 --> 00:09:42,455
particular problem, what would really
happen is okay in this particular hour

167
00:09:42,755 --> 00:09:44,825
a transaction happened in California.

168
00:09:45,085 --> 00:09:48,295
In the next particular hour a
transaction happened in Tennessee.

169
00:09:48,625 --> 00:09:51,865
But of course, like you wouldn't,
you would not necessarily get the

170
00:09:51,865 --> 00:09:54,925
entire picture, let's say, okay, for
on an hourly basis if we check it.

171
00:09:54,925 --> 00:09:58,165
Of course, even batch process would
pick it up, but the transaction

172
00:09:58,165 --> 00:09:59,035
might have already occurred.

173
00:09:59,035 --> 00:10:02,425
Like the person who would've actually
hack my credit card might have

174
00:10:02,425 --> 00:10:06,145
already made like thousands of dollars
of purchases, and I am the person

175
00:10:06,145 --> 00:10:07,405
who is responsible to pay for it.

176
00:10:07,625 --> 00:10:08,855
Which doesn't really make sense.

177
00:10:08,855 --> 00:10:12,035
So basically such realtime fraud
detection mechanisms are much more

178
00:10:12,035 --> 00:10:14,735
important and they are becoming
more important day by day.

179
00:10:15,425 --> 00:10:18,885
So now coming to let's say another
particular use case, right?

180
00:10:18,885 --> 00:10:22,990
Which is the e-commerce, for example,
as I was talking about like with

181
00:10:22,990 --> 00:10:27,055
the advent of at least I myself use
Amazon and Walmart quite frequently.

182
00:10:27,415 --> 00:10:31,825
So what I have actually
observed is the personalization.

183
00:10:32,064 --> 00:10:38,064
For the customers in these e-commerce
businesses have actually gotten way

184
00:10:38,064 --> 00:10:39,655
better than what they used to be.

185
00:10:40,285 --> 00:10:43,655
For example, they have as I was
saying, e-commerce businesses

186
00:10:43,655 --> 00:10:45,905
have actually seen major benefits.

187
00:10:46,335 --> 00:10:51,225
Some real time personalization
can boost sales up to like by 18%.

188
00:10:51,975 --> 00:10:54,345
So for example, picture
yourself doing this right.

189
00:10:54,770 --> 00:10:59,540
As you are browsing, let's say our real
time analytics immediately updates product

190
00:10:59,540 --> 00:11:04,100
recommendations based on your let's
say current session, inventory status,

191
00:11:04,100 --> 00:11:06,380
pricing strategies, so on and so forth.

192
00:11:06,765 --> 00:11:12,675
So this dynamic approach will definitely
increase the average order values by 12%.

193
00:11:13,275 --> 00:11:16,635
For example, let's say as I was alluding
to my previous example, like I have

194
00:11:16,635 --> 00:11:20,265
a kid and I of course, the thing that
usually pops up into my mind is buy

195
00:11:20,265 --> 00:11:22,065
diapers, screams on, and so forth.

196
00:11:22,455 --> 00:11:25,335
So if you were to take the example
of buying diapers themselves, right?

197
00:11:25,580 --> 00:11:29,350
So of course like running on
that less sleep more for quite

198
00:11:29,350 --> 00:11:30,310
some time now with a kid.

199
00:11:30,800 --> 00:11:35,510
Basically you sometimes forget, or
sometimes you might not even realize what.

200
00:11:35,595 --> 00:11:37,875
Is really needed for you
to purchase something.

201
00:11:38,355 --> 00:11:42,425
So for example, let's say as I was
browsing diapers, if let the real

202
00:11:42,425 --> 00:11:45,635
time personalization kicks in, like
the analytics pipelines kicks in and

203
00:11:45,635 --> 00:11:49,655
they say that, Hey, I have seen that
you have been buying diapers now and

204
00:11:49,655 --> 00:11:53,015
it's been some time since you bought,
let's say diaper cream or let's say

205
00:11:53,255 --> 00:11:54,815
moisturizing lotion for your kit.

206
00:11:55,235 --> 00:11:56,885
Why don't you actually
add that to the cart?

207
00:11:56,945 --> 00:11:58,745
Or of course, like this is, I think not.

208
00:11:58,805 --> 00:12:02,885
Prompting per se, but basically
people have also bought moisturizing

209
00:12:02,885 --> 00:12:03,935
cream, so on and so forth.

210
00:12:04,205 --> 00:12:06,485
That would actually trigger
something in my brain saying

211
00:12:06,485 --> 00:12:07,865
that Hey, I forgot to buy this.

212
00:12:07,865 --> 00:12:08,885
Let me actually add this.

213
00:12:09,245 --> 00:12:12,035
So this sort of personalization
is something that I would love

214
00:12:12,065 --> 00:12:12,905
in any new product, right?

215
00:12:12,905 --> 00:12:15,850
Because it is important for the
tech to make our lives better.

216
00:12:17,030 --> 00:12:21,380
So now along with that, let's say
realtime systems prevent customers

217
00:12:21,380 --> 00:12:23,300
from facing out of stock items.

218
00:12:23,300 --> 00:12:25,340
Like I've always had this happen, right?

219
00:12:25,340 --> 00:12:28,730
Like where, let's say I have seen
that you add a couple of items in

220
00:12:28,730 --> 00:12:33,060
Target, a target shopping cart, or
Walmart shopping cart for that matter.

221
00:12:33,570 --> 00:12:37,170
By the time you are just about to check
out, or let's say you are about to get

222
00:12:37,170 --> 00:12:38,610
it delivered, it just says out of stock.

223
00:12:39,010 --> 00:12:42,820
Such sort of events are actually
problematic for the user experience

224
00:12:42,820 --> 00:12:45,970
and handling such events in real
time is much more important.

225
00:12:45,970 --> 00:12:49,540
I. Again, you see these are much more
important when they, when there are

226
00:12:49,540 --> 00:12:54,400
like peak shopping like ho peak shopping
events like holidays or major sales.

227
00:12:54,790 --> 00:12:57,710
Like for example, when I was
working at Walmart Thanksgiving

228
00:12:57,710 --> 00:12:58,760
is a major season, right?

229
00:12:58,760 --> 00:13:01,370
So you would want to buy
gifts for your loved ones.

230
00:13:01,700 --> 00:13:03,170
Want to make sure your family's happy.

231
00:13:03,790 --> 00:13:06,880
I've actually seen this happen
many times where there is a

232
00:13:06,880 --> 00:13:08,230
particular sale which pops up.

233
00:13:08,230 --> 00:13:12,490
Let's say you want to fi, you want to
buy an iPad for your family members.

234
00:13:12,880 --> 00:13:13,210
Okay?

235
00:13:13,210 --> 00:13:15,340
There is a suddenly, there's
suddenly a sale which popped

236
00:13:15,340 --> 00:13:16,330
up in, let's say Walmart.

237
00:13:16,390 --> 00:13:17,800
That 20% off of iPad.

238
00:13:18,430 --> 00:13:21,350
It's not a normal, it's
not a normal sale, right?

239
00:13:21,350 --> 00:13:21,830
So you.

240
00:13:22,235 --> 00:13:27,165
Basically many people along with you are
actually flocking to walmart.com to buy

241
00:13:27,165 --> 00:13:31,845
that particular thing, and as such, it
would definitely make much more sense

242
00:13:31,845 --> 00:13:35,715
to update them real time on what is
actually going on with that sale event.

243
00:13:35,765 --> 00:13:37,685
As in, are you a little too late?

244
00:13:37,690 --> 00:13:40,985
Are you are, let's say they're like,
there are, how many of the iPads

245
00:13:40,985 --> 00:13:44,645
are actually still left, so that
way you can provide customers with

246
00:13:44,645 --> 00:13:46,085
way better shopping experience?

247
00:13:46,595 --> 00:13:50,895
Now let's actually discuss a completely
different side of the coin, which

248
00:13:50,895 --> 00:13:52,485
is manufacturing for that matter.

249
00:13:53,135 --> 00:13:57,355
Factories right have actually dramatically
improved operations by shifting to

250
00:13:57,355 --> 00:13:59,425
realtime predictive maintenance.

251
00:13:59,845 --> 00:14:04,560
For example, machines are now equipped
with advanced sensors which can

252
00:14:04,560 --> 00:14:06,810
stream continuously stream data.

253
00:14:07,170 --> 00:14:09,510
Like what is the temperature,
what is a vibration?

254
00:14:09,510 --> 00:14:11,280
What is the operating
speed of the machine?

255
00:14:11,670 --> 00:14:14,130
So now real time machine learning models.

256
00:14:14,420 --> 00:14:19,430
Analyze this data instantly, and
they can actually predict equipment

257
00:14:19,430 --> 00:14:23,830
failures before they happen for ex like
I, you may ask the question, right?

258
00:14:23,830 --> 00:14:24,850
Like, why is this important?

259
00:14:24,850 --> 00:14:28,600
So manufacturing is not something
which you, which you regularly

260
00:14:28,600 --> 00:14:29,860
see on a day-to-day basis.

261
00:14:29,860 --> 00:14:31,030
So why is this really important?

262
00:14:31,030 --> 00:14:35,250
So regular maintenance of, regular
maintenance of machines is important

263
00:14:35,250 --> 00:14:38,760
so as to streamline the operations
and not expect any latency.

264
00:14:39,330 --> 00:14:42,190
For example, let's say again an out of
the mind example, I'm just thinking out

265
00:14:42,190 --> 00:14:47,385
loud, where let's say you have ordered
some toys and basically for delivery

266
00:14:47,745 --> 00:14:51,525
and let's say that has gone back to the
manufacturing plant for manufacturing.

267
00:14:51,555 --> 00:14:55,110
Again, this is a completely out of the
world scenario that I'm talking about.

268
00:14:55,670 --> 00:14:58,700
But let's say that, okay, the tie
manufacturing has started, but somewhere

269
00:14:58,700 --> 00:15:00,050
down the line the machines failed.

270
00:15:00,500 --> 00:15:05,030
And that the tie, which is supposed to
be delivered for your kid's birthday, has

271
00:15:05,030 --> 00:15:09,050
now been, the delivery date has now been
pushed back let's say four to five weeks.

272
00:15:09,440 --> 00:15:10,610
That is not acceptable, right?

273
00:15:10,610 --> 00:15:13,790
There is a particular reason why you
pick that tie, and there is a particular

274
00:15:13,790 --> 00:15:15,200
timeline that you have in your mind.

275
00:15:15,695 --> 00:15:19,595
But now that because of a manufacturing
delay, all of this happened.

276
00:15:20,015 --> 00:15:25,715
So equipping machines with such data
where it can actually stream the data.

277
00:15:25,745 --> 00:15:28,895
Like saying that Hey, my current, the
temperature of the machine is so and

278
00:15:28,945 --> 00:15:32,300
let's say that like the pressure which
is going through and that number of ties

279
00:15:32,300 --> 00:15:36,070
that has been processed are built until
now or so and so such sort of numbers

280
00:15:36,070 --> 00:15:37,870
would help us quickly understand, hey.

281
00:15:38,435 --> 00:15:41,835
We have seen that let's say previous
day we have seen that this machine

282
00:15:41,835 --> 00:15:46,575
is processing so many ties or so
building so many toys per minute.

283
00:15:46,935 --> 00:15:49,185
But now that has gone down drastically.

284
00:15:49,245 --> 00:15:50,475
What is really going on, right?

285
00:15:50,775 --> 00:15:54,675
So you are able to identify the issue
with the manufacturing or the machine

286
00:15:55,335 --> 00:15:58,275
way before something really bad happens.

287
00:15:58,665 --> 00:16:02,205
And as such, you are able to
fix it so that your operations

288
00:16:02,205 --> 00:16:03,435
are way more streamlined.

289
00:16:03,485 --> 00:16:05,255
As and when more and more.

290
00:16:05,410 --> 00:16:08,700
Machines are more and
more automation kicks in.

291
00:16:08,820 --> 00:16:10,080
This is much more important.

292
00:16:11,400 --> 00:16:15,030
So now let's actually talk
about the technical challenges,

293
00:16:15,030 --> 00:16:16,620
which are event time processing.

294
00:16:17,040 --> 00:16:20,140
So for example, there are
again, implementing this real

295
00:16:20,140 --> 00:16:22,900
time machine learning is not
without its own challenges.

296
00:16:23,470 --> 00:16:26,710
For example, a significant hurdle
is event time processing, right?

297
00:16:27,250 --> 00:16:31,120
Accurately handling data based
on when events actually happen.

298
00:16:31,655 --> 00:16:34,295
But not when they are recorded,
like as in there real.

299
00:16:34,475 --> 00:16:36,375
There is it's hard to visualize.

300
00:16:36,375 --> 00:16:39,345
So let's take a moment to
understand what is going on.

301
00:16:40,035 --> 00:16:45,195
So there is a slight delay on
when the event actually occurs and

302
00:16:45,195 --> 00:16:49,725
when it is recorded, as in, let's
say you clicked on add to card

303
00:16:49,725 --> 00:16:52,245
button inlet on let on amazon.com.

304
00:16:52,995 --> 00:16:58,035
So that event is being sent to
the backend server for processing.

305
00:16:58,530 --> 00:17:03,660
So the, let's say you click the add
to cart right now, but because of a

306
00:17:03,660 --> 00:17:08,250
server delay or something, the backend
processing system actually received

307
00:17:08,250 --> 00:17:10,410
that event a couple of minutes late.

308
00:17:11,325 --> 00:17:13,785
So that does not necessarily
mean add to cart never happened.

309
00:17:13,965 --> 00:17:17,535
It means that the add to cart
event is actually recorded late.

310
00:17:17,685 --> 00:17:21,405
So that is the distinction, like as
in when the event occurred versus

311
00:17:21,405 --> 00:17:23,025
when it was actually processed.

312
00:17:23,565 --> 00:17:27,826
So around 15% of streaming data
arrives out of order because there are.

313
00:17:28,330 --> 00:17:29,710
There are a lot of issues, right?

314
00:17:29,710 --> 00:17:33,250
There are, these are all computers
where there events are being recorded.

315
00:17:33,250 --> 00:17:36,790
So let's say there is a network
problem, there is a machine, which

316
00:17:37,000 --> 00:17:40,720
machine which went down, so on and
so forth, which actually makes the

317
00:17:40,720 --> 00:17:44,470
streaming data this is a very common
occurrence where they are out of order.

318
00:17:44,970 --> 00:17:47,040
Which complicates accurate analytics.

319
00:17:47,070 --> 00:17:51,010
Again, this is this is something that this
is a, which has been a bigger challenge.

320
00:17:51,610 --> 00:17:53,500
So now to overcome this, right?

321
00:17:53,830 --> 00:17:57,940
Basically methods like sliding
windows or watermarking has been

322
00:17:58,280 --> 00:18:03,590
has been used to ensure data
accuracy, like as in let's take a

323
00:18:03,590 --> 00:18:04,940
moment to understand what these are.

324
00:18:04,940 --> 00:18:08,000
Sliding window is basically what you're
saying is, again, going back to the

325
00:18:08,000 --> 00:18:10,460
e-commerce example, you saw that.

326
00:18:10,650 --> 00:18:14,470
An add to cart event is
is, has come to you, okay?

327
00:18:14,470 --> 00:18:17,350
Like you are a software engineer,
and as part of the data that

328
00:18:17,350 --> 00:18:18,520
an add to cart event occur.

329
00:18:18,550 --> 00:18:23,470
You see that, but weirdly enough,
let's say you, you don't see an event

330
00:18:23,590 --> 00:18:26,350
where the user landed on the homepage.

331
00:18:27,070 --> 00:18:27,879
That doesn't make sense.

332
00:18:27,879 --> 00:18:30,999
As in how would you go to add to cart
before going to the homepage, right?

333
00:18:30,999 --> 00:18:34,479
Like you go to the homepage, you browse
the product, you then add to cart.

334
00:18:35,354 --> 00:18:39,195
So there are like three events,
but somehow because of some server

335
00:18:39,195 --> 00:18:42,225
delay or whatever, add to cart
is the one which you got first.

336
00:18:42,524 --> 00:18:47,665
So what you say is hey, I'll actually
wait for two minutes or one minute.

337
00:18:48,085 --> 00:18:54,055
Where for the home homepage event
and the product page event, so let's

338
00:18:54,055 --> 00:18:57,955
say that is called, that is where you
ensure how you ensure data accuracy.

339
00:18:58,280 --> 00:19:01,790
Okay, now that, yeah, you have waited
two minutes, let's say you have gotten

340
00:19:01,790 --> 00:19:06,660
homepage and the product page, you bundle
this together saying that, okay, like now

341
00:19:06,660 --> 00:19:10,800
I have gotten the full data, and as such
I have, I can order those events properly.

342
00:19:11,130 --> 00:19:14,040
Homepage occurred, first product
page occurred next, then the

343
00:19:14,040 --> 00:19:15,060
add to cart page happened.

344
00:19:15,360 --> 00:19:17,940
So you can see the user
path or the user behavior.

345
00:19:18,620 --> 00:19:19,490
As it is needed.

346
00:19:20,120 --> 00:19:24,230
And again, if you were to take another
example of stock market trading, precise

347
00:19:24,230 --> 00:19:27,590
timing of transactions is critical
again, which is really important, right?

348
00:19:27,590 --> 00:19:30,420
Like you cannot just say let's say
I want to buy a stock right now,

349
00:19:30,630 --> 00:19:32,700
but suddenly for whatever reason, I.

350
00:19:32,970 --> 00:19:37,020
Like the event got delayed and the
stock price rose, you cannot just tell

351
00:19:37,020 --> 00:19:40,010
the customer that unfortunately, we
could not buy the stock right now.

352
00:19:40,070 --> 00:19:44,570
And as such that incorrect ordering
can cost significant financial impact.

353
00:19:45,120 --> 00:19:47,970
Such sort of events are
much more kept in mind.

354
00:19:47,970 --> 00:19:50,760
And as such, of course, proper
methods have been devised, like

355
00:19:50,760 --> 00:19:52,230
siding, windows, and watermarking.

356
00:19:52,260 --> 00:19:57,450
Again, just to reiterate now, going
over, let's say we want to understand

357
00:19:57,450 --> 00:20:01,470
the model drift in continuous systems,
which is basically, which is, this

358
00:20:01,470 --> 00:20:02,970
is another one major challenge.

359
00:20:02,970 --> 00:20:07,420
Like for example over time
data patterns naturally change.

360
00:20:07,840 --> 00:20:12,990
So this causes the models to become
less accurate because they have been

361
00:20:12,990 --> 00:20:17,880
trained on a particular set of data
on our particular data patterns, but

362
00:20:17,910 --> 00:20:20,190
naturally the data pattern changed.

363
00:20:20,580 --> 00:20:25,150
You usually have to update the
models very frequently, as in the

364
00:20:25,150 --> 00:20:27,160
training has to occur very frequently.

365
00:20:27,350 --> 00:20:30,080
So as to make sure that
the models actually keep up

366
00:20:30,080 --> 00:20:31,790
with the data pattern trend.

367
00:20:32,450 --> 00:20:35,630
So if you were to take real
Ty retail businesses right?

368
00:20:35,905 --> 00:20:38,605
Customer shopping behaviors
change seasonally.

369
00:20:38,665 --> 00:20:42,675
For example if you were to take any
clothing business of sort summer

370
00:20:42,675 --> 00:20:45,855
clothes and winter clothes are
something which are very common, and

371
00:20:45,855 --> 00:20:50,655
as such, the buying patterns of summer
clothes increases over summertime

372
00:20:50,655 --> 00:20:52,605
decreases as the winter approaches.

373
00:20:53,385 --> 00:20:57,015
So is the case with winter clothes
where it increases over the wintertime

374
00:20:57,015 --> 00:20:58,665
and decreases as the spring approaches.

375
00:20:59,355 --> 00:21:02,235
So now considering such
seasonal behaviors, right?

376
00:21:03,610 --> 00:21:06,760
Basically, which influence the products,
what products they usually buy.

377
00:21:07,210 --> 00:21:12,770
Real time systems should address this by
detecting changes in data distribution

378
00:21:13,070 --> 00:21:15,080
and automatically retraining the model.

379
00:21:15,440 --> 00:21:19,160
So that such retraining of models
is much more important to make sure

380
00:21:19,160 --> 00:21:20,480
that you keep up with the data trend.

381
00:21:20,480 --> 00:21:23,750
Again, as we were just discussing,
think of it as keeping the

382
00:21:23,750 --> 00:21:25,280
system fresh and accurate.

383
00:21:25,940 --> 00:21:29,720
Automatically adjusting to shifts
without manual intervention, right?

384
00:21:29,720 --> 00:21:33,110
Like of course there will always
be some data changes, which humans

385
00:21:33,110 --> 00:21:37,420
might not have actually let's say,
been thinking about, for example I've

386
00:21:37,420 --> 00:21:39,340
actually seen a couple of things where I.

387
00:21:39,665 --> 00:21:44,185
There are a couple of scenarios where,
there are a couple of scenarios where

388
00:21:44,185 --> 00:21:49,905
the flowers buying actually increased
by quite a lot and but as in when some

389
00:21:49,905 --> 00:21:53,475
of the, some other events occur, it did
quite decrease and so on and so forth.

390
00:21:53,845 --> 00:21:55,435
Such sort of things are very.

391
00:21:55,790 --> 00:21:57,290
Almost impossible to predict, right?

392
00:21:57,290 --> 00:22:00,490
As a human, you cannot deal with
every single data pattern, but

393
00:22:00,490 --> 00:22:03,430
basically you would want your models
to automatically take care of it.

394
00:22:04,150 --> 00:22:04,600
Let's say.

395
00:22:04,660 --> 00:22:09,610
Now taking, talking about the
architectural comparison, so choosing the

396
00:22:10,030 --> 00:22:12,220
right streaming framework is critical.

397
00:22:12,270 --> 00:22:14,910
For example, until now, we have
established why stream processing

398
00:22:14,910 --> 00:22:18,750
is important, how we have overcome
the challenges, so on and so forth.

399
00:22:18,980 --> 00:22:22,160
Now let's actually discuss about what
technologies are actually present.

400
00:22:22,580 --> 00:22:26,060
And what frameworks essentially that's
the right word, but what streaming

401
00:22:26,060 --> 00:22:31,520
frameworks are actually present and
what are the ever so slight differences

402
00:22:31,520 --> 00:22:34,670
between them and what, how do you
compare them against each other?

403
00:22:34,670 --> 00:22:34,730
I.

404
00:22:36,020 --> 00:22:38,440
So for example Apache Kafka, right?

405
00:22:38,560 --> 00:22:41,840
It can actually process about 2
million events per second with

406
00:22:41,840 --> 00:22:43,610
around 10 milliseconds latency.

407
00:22:44,070 --> 00:22:49,710
Apache Flink is actually even faster
reaching sub millisecond latency ideal

408
00:22:49,710 --> 00:22:56,220
for financial markets or real time giving
cloud services like AWS Kinesys and Azure

409
00:22:56,220 --> 00:22:58,830
event hubs provide simpler integration.

410
00:22:59,310 --> 00:23:01,740
But to have slightly higher latency.

411
00:23:02,340 --> 00:23:06,630
So now business often adapt hybrid
approaches basically to balance

412
00:23:06,630 --> 00:23:09,240
performance, complexity, and convenience.

413
00:23:09,730 --> 00:23:14,530
For instance, using Kafka for event
ingestion and flink for processing

414
00:23:14,530 --> 00:23:16,840
might be ideal for complex environments.

415
00:23:17,390 --> 00:23:20,690
Again, you might ask the question, why
not actually go for the best one, right?

416
00:23:21,050 --> 00:23:22,730
So there are a lot of reasons.

417
00:23:22,730 --> 00:23:26,585
So when it comes to big companies or when
it comes to your use case it depends on

418
00:23:26,585 --> 00:23:28,145
what sort of thing you're looking for.

419
00:23:28,145 --> 00:23:31,255
It's just not always ideal
to go for the best one.

420
00:23:31,255 --> 00:23:32,985
Again it's not, I'm not trying to say.

421
00:23:33,480 --> 00:23:37,110
Increasing the latency is what you want
to go for, but basically there are ever

422
00:23:37,110 --> 00:23:40,980
so many things that you can, you have to
think about, keep in mind as in how many

423
00:23:40,980 --> 00:23:42,780
teams are actually looking for this data?

424
00:23:43,170 --> 00:23:45,780
What is the latency that is tolerated?

425
00:23:45,840 --> 00:23:48,810
What is the latency that you
really want to shoot for?

426
00:23:48,810 --> 00:23:49,650
So on and so forth.

427
00:23:50,010 --> 00:23:53,195
Again, like some things are easy to
integrate, some things are very hard

428
00:23:53,220 --> 00:23:56,400
to go for, go with, go to integrate,
and sometimes the learning curve is

429
00:23:56,400 --> 00:23:57,660
way too much, so on and so forth.

430
00:23:58,080 --> 00:23:59,840
There are a lot of decisions.

431
00:24:00,215 --> 00:24:03,155
Our thought processes,
which go into selecting a

432
00:24:03,155 --> 00:24:04,775
particular streaming framework.

433
00:24:05,255 --> 00:24:08,995
But at least in the span of my
career, I've actually always

434
00:24:08,995 --> 00:24:10,645
worked with the hybrid systems.

435
00:24:10,645 --> 00:24:14,935
Again, like as I was just saying, Kafka
for event ingestion is something which

436
00:24:14,935 --> 00:24:17,605
is very widely used because it actually.

437
00:24:18,095 --> 00:24:22,715
Elev for seamless event ingestion
and flink for processing is

438
00:24:22,715 --> 00:24:24,385
something very widely used as well.

439
00:24:24,385 --> 00:24:27,595
Again, there is park streaming, there
is park structured streaming, so on

440
00:24:27,595 --> 00:24:32,155
and so forth, which provides similar
functionalities, but it really depends on.

441
00:24:32,435 --> 00:24:34,535
What functionalities
you are you looking for?

442
00:24:34,540 --> 00:24:39,065
For for instance some frameworks
provide better watermarking, some

443
00:24:39,065 --> 00:24:41,135
frameworks provide better latency.

444
00:24:41,465 --> 00:24:45,125
So if, let's say you require better
watermarking, you go with the necessary

445
00:24:45,125 --> 00:24:49,095
framework I'm just trying to throw
all the details at you so that you

446
00:24:49,095 --> 00:24:53,985
can make an informed decision and you
can actually learn what, how to make a

447
00:24:53,985 --> 00:24:57,735
proper, informed decision and choosing
a particular streaming framework.

448
00:24:59,390 --> 00:25:00,710
Now implementation.

449
00:25:01,010 --> 00:25:02,720
Let's talk about the
implementation, right?

450
00:25:03,050 --> 00:25:08,650
So basically how does the implementation
work when you are trying to

451
00:25:09,020 --> 00:25:10,610
implement a real-time architecture?

452
00:25:11,060 --> 00:25:16,120
So first the data enters through high
speed brokers like Kafka, for example.

453
00:25:16,120 --> 00:25:20,740
Kafka is used for so that, let's say
for server fails, and you somehow.

454
00:25:21,095 --> 00:25:23,585
Don't process the event downstream.

455
00:25:24,075 --> 00:25:29,075
You will you have enough time for you
to process and which hand Kafka like

456
00:25:29,075 --> 00:25:30,665
handling millions of events per second.

457
00:25:31,055 --> 00:25:36,245
Next processor like Flink manages complex
computations and realtime analysis.

458
00:25:36,735 --> 00:25:40,275
Then a real-time feature store
quickly delivers essential data to

459
00:25:40,275 --> 00:25:43,605
models, because that is important to
create features, so on and so forth.

460
00:25:43,995 --> 00:25:45,855
And finally, optimized models.

461
00:25:45,855 --> 00:25:49,335
Serving infrastructure produces
predictions and constantly

462
00:25:49,335 --> 00:25:50,565
monitor system accuracy.

463
00:25:51,105 --> 00:25:53,400
So this setup includes
automatic retraining.

464
00:25:54,020 --> 00:25:58,100
Where accuracy drops, ensuring
consistent performance and reliability.

465
00:25:58,760 --> 00:26:02,750
Now let's actually talk about the key
takeaways and next things, next steps.

466
00:26:02,780 --> 00:26:06,260
So to wrap things up again,
implementing real-time machine learning

467
00:26:06,260 --> 00:26:08,300
has clear substantial benefits.

468
00:26:08,300 --> 00:26:12,470
I. Dramatically reducing fraud
for an for instance, significantly

469
00:26:12,470 --> 00:26:15,950
improving sales in e-commerce,
minimizing downtime in manufacturing.

470
00:26:16,280 --> 00:26:21,720
The keys to now keys to key to
success in is include is to choose

471
00:26:21,720 --> 00:26:23,790
your architecture very carefully.

472
00:26:24,390 --> 00:26:28,770
Continuously monitor performance
and automating model retraining.

473
00:26:29,430 --> 00:26:32,910
Now I would say begin by
targeting your most impactful

474
00:26:33,030 --> 00:26:34,980
latency sensitive applications.

475
00:26:35,250 --> 00:26:39,650
From there incrementally build your
capabilities and be on lookout for

476
00:26:39,650 --> 00:26:41,030
changes in your data environment.

477
00:26:41,540 --> 00:26:43,130
Again, thank you so
much for your attention.

478
00:26:43,160 --> 00:26:46,380
I hope you learned a great deal
about streaming data structure,

479
00:26:46,380 --> 00:26:49,530
streaming frameworks, so on and so
forth, and how, where they are used,

480
00:26:49,980 --> 00:26:52,710
and I'm really happy that you are.

481
00:26:53,400 --> 00:26:54,960
Again, thanks for
attending the conference.

482
00:26:55,230 --> 00:26:56,130
Hope you have a really good time.

