1
00:00:00,500 --> 00:00:01,490
Hi everyone.

2
00:00:01,730 --> 00:00:04,190
My name is P Juan from Ocean Based Global.

3
00:00:04,460 --> 00:00:10,070
In the next 15 minutes, I will show
you how platform teams can run rack

4
00:00:10,160 --> 00:00:15,530
semantic search analytics along a
single database platform, S code

5
00:00:15,530 --> 00:00:17,390
plus vector running on Kubernetes.

6
00:00:17,390 --> 00:00:23,230
The goal is simple, fewer moving parts,
lower latency, and easier governance.

7
00:00:23,680 --> 00:00:28,450
For both structured and unstructured
data, all with multi-tenants

8
00:00:28,450 --> 00:00:30,130
and strong consistency.

9
00:00:30,630 --> 00:00:32,040
Here is today's journey.

10
00:00:32,400 --> 00:00:36,570
First, I will share why AI will
close the challenge platform teams,

11
00:00:37,110 --> 00:00:42,535
things like fragmented texts, high
latency, and the governance issues.

12
00:00:42,864 --> 00:00:45,610
Then we will move from
real world challenges to.

13
00:00:46,110 --> 00:00:47,850
To one platform solution.

14
00:00:48,120 --> 00:00:53,220
Looking at how a unified
approach can simplify operations.

15
00:00:54,090 --> 00:00:59,280
Next, I'll introduce Ocean based
as a unified database for modern

16
00:00:59,310 --> 00:01:06,080
AI apps showing how SQL and vector
workloads ran together on Kubernetes.

17
00:01:06,560 --> 00:01:11,900
Finally, we'll ramp up with some
closing source and cater with that

18
00:01:11,900 --> 00:01:13,580
you can bring back to your team.

19
00:01:14,080 --> 00:01:22,265
According to IDC, over 92% of global
data is now in an unstructured format.

20
00:01:22,765 --> 00:01:28,440
This exactly is this data drives AI
workloads like React and semantic

21
00:01:28,440 --> 00:01:33,535
search executives and IT leaders
are also recognizing this shift.

22
00:01:34,035 --> 00:01:40,815
41% say rack as essential, and
over 80% believe that gene AI

23
00:01:40,815 --> 00:01:47,535
models using their own enterprise
data will be K competitive edge.

24
00:01:48,285 --> 00:01:50,235
And this isn't just theory.

25
00:01:50,745 --> 00:01:54,780
Clo recent research shows
enterprises are moving fast.

26
00:01:55,590 --> 00:02:01,110
Two thirds are already using enterprise
AI infrastructure platforms, and the

27
00:02:01,110 --> 00:02:08,460
60% are embedding agent AI directly
into their core applications.

28
00:02:08,880 --> 00:02:12,960
So AI workloads are no
longer experimental.

29
00:02:13,260 --> 00:02:19,075
They're becoming mainstream
enterprise critical workloads and

30
00:02:19,145 --> 00:02:22,930
that creates new challenges for
platform and the database team.

31
00:02:23,430 --> 00:02:24,930
Which we will dive into next.

32
00:02:25,430 --> 00:02:31,640
Now, if AI workloads are becoming
mainstream, the next question is, what

33
00:02:31,640 --> 00:02:34,700
does this mean for our technology stack?

34
00:02:35,690 --> 00:02:39,549
The real, the reality is different.

35
00:02:39,910 --> 00:02:44,969
Different workloads requires very
different capability for intelligent

36
00:02:45,539 --> 00:02:47,189
question and answer, charitables.

37
00:02:47,790 --> 00:02:53,159
They need low latency access,
thematic search and context

38
00:02:53,219 --> 00:02:56,339
awareness for log analytics.

39
00:02:56,929 --> 00:03:00,559
Normally detection, it
needs high ingestion, right?

40
00:03:01,059 --> 00:03:07,409
Ax search and metric aggregation for the
core business systems, it's comparing,

41
00:03:07,889 --> 00:03:15,414
it needs strong consistency is a ID and
S. And because of this, organizations end

42
00:03:15,414 --> 00:03:20,834
up teaching together multiple specialized
databases a transactional database,

43
00:03:21,274 --> 00:03:27,424
a data warehouse, a vector database,
sometimes in a graph or NoSQL engine.

44
00:03:27,924 --> 00:03:32,754
The result, the stack is getting
complicated, more system to

45
00:03:32,754 --> 00:03:35,634
upgrade more ETL pipelines.

46
00:03:36,399 --> 00:03:40,659
Higher cost and more risk when
workloads span across them.

47
00:03:41,159 --> 00:03:46,619
From my conversation with many
platform engineers and from our

48
00:03:46,619 --> 00:03:52,519
own experiences, this is what most
teams, which they had one platform

49
00:03:52,604 --> 00:03:58,309
for all data workloads and a strong
consistency and a high availability.

50
00:03:59,299 --> 00:04:04,069
Seamless integration with AI
pipelines, elastic scaling on

51
00:04:04,069 --> 00:04:10,959
Kubernetes and UN unified access and
multi-tenants with resource isolation.

52
00:04:11,459 --> 00:04:15,694
But here is the reality is they
face every day just like fragment.

53
00:04:15,694 --> 00:04:21,229
Augmented the data system trade off
between performance and consistency.

54
00:04:22,114 --> 00:04:28,244
Complex data movement between services,
difficult scaling across data flow

55
00:04:28,244 --> 00:04:36,364
workloads, inconsistent query models
and APIs, talent interference and noise.

56
00:04:36,794 --> 00:04:36,914
Neighbors.

57
00:04:37,414 --> 00:04:41,984
Let me make this more concrete
with with the user case that many

58
00:04:41,984 --> 00:04:43,814
enterprises are building today.

59
00:04:44,504 --> 00:04:48,134
AI powered Knowledge
retrieval for internal Teams.

60
00:04:48,734 --> 00:04:52,714
This is a conventional architecture
you will note, you will notice

61
00:04:52,714 --> 00:04:55,324
it looks quite complicated.

62
00:04:55,804 --> 00:04:58,984
There is a transactional
database for metadata.

63
00:04:59,599 --> 00:05:02,149
And a vector to store.

64
00:05:02,424 --> 00:05:07,629
A vector store for embedding
maybe a, maybe an elastic search

65
00:05:07,869 --> 00:05:09,729
cluster for full text search.

66
00:05:09,909 --> 00:05:14,859
And all of this need to be
synchronized with ETL drops.

67
00:05:15,759 --> 00:05:20,399
Now, what does this mean in
practice, you'll end up maintaining

68
00:05:20,399 --> 00:05:25,739
multiple databases, synchronizing,
synchronization, introduce.

69
00:05:26,309 --> 00:05:28,379
Latency and inconsistency.

70
00:05:29,189 --> 00:05:33,529
Each additional component adds
to the cost and operational risk.

71
00:05:34,099 --> 00:05:38,239
And if one part goes down,
the whole pipeline can break.

72
00:05:38,929 --> 00:05:45,439
This is exactly the kind of fragmented
stack we described earlier, how to operate

73
00:05:45,799 --> 00:05:49,089
costly to scale and fragile in production.

74
00:05:49,589 --> 00:05:52,889
Now here is how the team
approaches the problem.

75
00:05:53,489 --> 00:05:58,289
Instead of stitching together
multiple specialized systems,

76
00:05:58,589 --> 00:06:03,209
they build on ocean-based as
a unified database platform.

77
00:06:03,959 --> 00:06:09,399
This allowed them to reimburse structure
and vector search in the same engine,

78
00:06:10,029 --> 00:06:16,539
deploy elastically in on Kubernetes and
removes the heavy integration burden.

79
00:06:17,039 --> 00:06:24,779
And as the results speak for themselves,
correl latency dropped by over 70%.

80
00:06:25,279 --> 00:06:30,349
That is, that used to take offers to
refresh is now available in minutes

81
00:06:30,859 --> 00:06:37,669
as the amount of integration code to
maintain has been reduced by over 40%.

82
00:06:38,389 --> 00:06:45,129
So by moving to a unified data platform
engineers gen, both similarity,

83
00:06:45,429 --> 00:06:51,059
simplicity, and the performance, something
that was very hard to achieve in a

84
00:06:51,059 --> 00:06:53,909
conventional multi database data set up.

85
00:06:54,409 --> 00:06:58,489
Let's look at a topical conventional
architecture for AI powered.

86
00:06:58,819 --> 00:07:00,079
Knowledge retrieval.

87
00:07:00,349 --> 00:07:06,374
When a user when a user issu issues
a query, the system needs to interact

88
00:07:06,584 --> 00:07:09,114
with multiple backend systems.

89
00:07:09,354 --> 00:07:12,624
First, it queries MySQL
four structured data.

90
00:07:12,954 --> 00:07:17,574
Then it queries the vector
database for and embed, so

91
00:07:18,354 --> 00:07:19,879
IES is involved for catching.

92
00:07:20,699 --> 00:07:24,294
In total a single request
incur at least the four.

93
00:07:24,794 --> 00:07:32,504
Network run trips and the end to
end latency extend 170 milliseconds.

94
00:07:33,004 --> 00:07:37,994
On top of that, the platform must
maintain data consistency across

95
00:07:38,024 --> 00:07:42,674
all their systems, which adds
additional complexity and overhead.

96
00:07:43,174 --> 00:07:48,714
This is exactly where Commissional
architecture, Chicago multiple systems.

97
00:07:49,404 --> 00:07:52,194
High latency and complex integration.

98
00:07:52,584 --> 00:07:57,444
In the next slide, we will say how
we use Ocean Base simplifies their

99
00:07:57,444 --> 00:08:01,514
stack and just, and reduce latency

100
00:08:02,014 --> 00:08:03,094
with Ocean Base.

101
00:08:03,594 --> 00:08:08,154
A user query no longer needs to
touch multiple backend systems.

102
00:08:08,604 --> 00:08:12,564
Instead, the query is handled
within a single platform.

103
00:08:13,239 --> 00:08:16,689
Combining both structured
and unstructured data.

104
00:08:17,189 --> 00:08:23,999
As a result, the entire request only
involves one query, not multiple

105
00:08:23,999 --> 00:08:29,879
round trips, end to end latency
drops to just the 50 millisecond.

106
00:08:30,379 --> 00:08:35,449
Data consistency is strongly maintained
across the system at all times.

107
00:08:35,949 --> 00:08:40,959
This, it treats how consolidate
consolidating your data and the

108
00:08:40,959 --> 00:08:46,989
search capability into a single SQL
compatible platform can dramatically

109
00:08:47,079 --> 00:08:50,769
reduce latency and simplify operations.

110
00:08:51,269 --> 00:08:56,879
Here is the practical example of
what we mean by hybrid queries.

111
00:08:57,119 --> 00:08:58,634
Imagine a user asking.

112
00:08:59,489 --> 00:09:03,539
Recommended distance
within five 500 meters.

113
00:09:03,779 --> 00:09:10,289
Average consumption below five
doors, rating about 4.4 0.5.

114
00:09:10,649 --> 00:09:14,309
No queries for no queues
for the coffee shop.

115
00:09:14,809 --> 00:09:20,694
Now this requires, this request
involves multiple data types, GIS

116
00:09:20,724 --> 00:09:22,774
data for location and distance.

117
00:09:23,764 --> 00:09:29,824
Relational data for price and rating
web, the search for thematic relevance.

118
00:09:30,324 --> 00:09:37,074
In most systems, you would need to stage
together different engines to handle this.

119
00:09:37,434 --> 00:09:42,954
This means multiple queries, complex
integration and added latency.

120
00:09:43,454 --> 00:09:47,504
But with Ocean Base, this
entire request can be answered

121
00:09:47,504 --> 00:09:49,904
within with just one SQL state.

122
00:09:50,404 --> 00:09:55,684
That's the power housing of having
structured special and vector data

123
00:09:55,894 --> 00:09:58,444
all unified in a single database.

124
00:09:59,404 --> 00:10:04,984
It makes life much simpler for
developers and enables faster,

125
00:10:05,074 --> 00:10:07,384
smarter application for users.

126
00:10:07,884 --> 00:10:11,764
Let's, let me quickly walk you
through the ocean based architecture.

127
00:10:12,319 --> 00:10:18,889
First data is distributed across multiple
availability zone, which means we can

128
00:10:18,889 --> 00:10:21,319
scaler horizontally as workloads grow.

129
00:10:21,819 --> 00:10:22,869
Onis.

130
00:10:22,929 --> 00:10:28,589
Ocean base is fully containerized
supports operator based mag management

131
00:10:28,859 --> 00:10:34,259
and aurora's elastic scaling
for both computer and storage.

132
00:10:34,759 --> 00:10:39,229
This, mix it very cloud native
and easy to operate at scale.

133
00:10:39,949 --> 00:10:43,519
Second, let's talk about
availability and consistency.

134
00:10:44,019 --> 00:10:49,359
Ocean base is built on the axus
consensus algorithm with the trip

135
00:10:49,629 --> 00:10:54,429
with the triple replica mechanism
across availability zones.

136
00:10:54,999 --> 00:10:57,999
So even a note or zone fails.

137
00:10:58,789 --> 00:11:00,199
We also have automatic failover.

138
00:11:00,699 --> 00:11:04,749
And the system always
guarantees strong consistency.

139
00:11:05,379 --> 00:11:12,279
In short, the architecture is designed
for elastic elasticity and resilience

140
00:11:12,549 --> 00:11:19,369
and the trust trust thinners, which are
critical for AI vehicles in production.

141
00:11:19,869 --> 00:11:23,469
Another important reason for
choosing ocean base is multi

142
00:11:23,469 --> 00:11:26,009
tennis compat compatibility.

143
00:11:26,849 --> 00:11:34,019
This means that different workloads can
run on the same ocean-based clusters while

144
00:11:34,049 --> 00:11:36,959
being isolated into separated tenants.

145
00:11:37,379 --> 00:11:43,529
Each talent has its own dedicated
resources, so this is so there is

146
00:11:43,559 --> 00:11:46,709
no interference across workloads.

147
00:11:47,099 --> 00:11:51,289
With this, you truly get one
platform for all data workloads.

148
00:11:52,129 --> 00:11:58,129
The STEM engine supports O-L-O-T-P
transactions, log analysis, semantics

149
00:11:58,129 --> 00:12:03,679
retrieval, and even AI applications
without needing multiple databases.

150
00:12:04,399 --> 00:12:10,199
And with the unified unified access
layer, developers can use a single

151
00:12:10,199 --> 00:12:15,629
SQL engine to query both structured
data and unstructured data, including

152
00:12:15,629 --> 00:12:18,239
full text search and vector search.

153
00:12:18,739 --> 00:12:24,169
This makes the developer and its parents
much simpler and it helps platform

154
00:12:24,169 --> 00:12:28,039
teams consolidate their database tech.

155
00:12:28,539 --> 00:12:32,794
As we ramp up, I'd like to
leave you with the fail case.

156
00:12:33,294 --> 00:12:33,504
Takeaways.

157
00:12:33,504 --> 00:12:35,394
First one, platform.

158
00:12:35,664 --> 00:12:38,874
All workloads be for the next decade.

159
00:12:39,144 --> 00:12:39,684
Dedicate.

160
00:12:40,184 --> 00:12:44,804
With Ocean Base, you can significantly
reduce the number of systems

161
00:12:44,834 --> 00:12:50,354
in your stack and cut down the
integration complexity that platform

162
00:12:50,354 --> 00:12:52,784
engineers often struggle with.

163
00:12:53,284 --> 00:12:58,864
Second, the same platforms just supports
both structured and unstructured data.

164
00:12:59,434 --> 00:13:01,984
That means OLTP analytics.

165
00:13:02,269 --> 00:13:08,719
Photo has search and even vector search
for AI workloads all in one place.

166
00:13:09,219 --> 00:13:15,939
Third, ocean-based offers elastic
scaling and runs natively on es, so

167
00:13:15,939 --> 00:13:21,609
it fits naturally in naturally into
modern coordinative environments.

168
00:13:22,509 --> 00:13:27,129
And finally, with the fuel
systems to integrate and maintain.

169
00:13:27,429 --> 00:13:32,299
Platform engineers can focus more on
delivering value to the business instead

170
00:13:32,299 --> 00:13:35,629
of spending time staging system together.

171
00:13:36,129 --> 00:13:40,749
That's the vision we are driving
with Ocean Base, a single unified

172
00:13:40,839 --> 00:13:45,279
database platform for the next
generation of applications.

173
00:13:45,779 --> 00:13:50,999
One platform, all workloads,
less complexity, more innovation.

174
00:13:51,374 --> 00:13:53,624
That's the ocean-based promise.

175
00:13:54,134 --> 00:13:54,614
Thank you.

176
00:13:54,854 --> 00:13:55,904
Thank you for your time.

177
00:13:56,404 --> 00:13:56,694
Okay.

