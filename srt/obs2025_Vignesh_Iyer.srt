1
00:00:00,000 --> 00:00:04,350
Are you a small team with minimum budget
not being able to spend on enterprise

2
00:00:04,350 --> 00:00:07,590
gate applications, but at the same
time, you want to gain some really

3
00:00:07,590 --> 00:00:09,300
useful insights about your system.

4
00:00:10,200 --> 00:00:11,280
This talk is just for you.

5
00:00:12,000 --> 00:00:15,570
I know that distributed systems are
very complex and tracing issues in a

6
00:00:15,570 --> 00:00:20,100
distributed system is very difficult,
and at the same time, enterprise

7
00:00:20,100 --> 00:00:25,950
solutions, although very helpful, are very
expensive, especially for small teams.

8
00:00:26,460 --> 00:00:28,500
So in this talk, my
objective is going to be.

9
00:00:29,145 --> 00:00:34,445
How to build minimum observability
stack and Kubernetes, keeping the

10
00:00:34,445 --> 00:00:38,675
costs low, while also gaining some
useful insights about your system,

11
00:00:39,185 --> 00:00:43,505
and finally, avoiding some obvious
pitfalls when that you can run into

12
00:00:43,505 --> 00:00:45,095
while designing some of these systems.

13
00:00:45,455 --> 00:00:47,035
So let's get started now.

14
00:00:47,040 --> 00:00:50,485
Before getting into the actual tools
and how to set up the actual stack,

15
00:00:50,515 --> 00:00:54,115
I would like to discuss what I call
the four pillars of observability.

16
00:00:54,955 --> 00:00:55,885
First is logging.

17
00:00:56,260 --> 00:01:01,720
Logging is the process of tracking
specific events in your application.

18
00:01:02,220 --> 00:01:07,350
Now these help you answer questions
like, what is happening in my system?

19
00:01:08,340 --> 00:01:10,020
Second, we have metrics.

20
00:01:10,350 --> 00:01:16,015
Metrics is nothing but numerical
measurements for performance

21
00:01:16,110 --> 00:01:17,640
of your system over time.

22
00:01:18,480 --> 00:01:22,260
They really help you quantify the
performance of your system so they

23
00:01:22,260 --> 00:01:26,370
can help you answering questions like,
how is my system behaving over time?

24
00:01:26,870 --> 00:01:28,670
The third is tracing.

25
00:01:29,180 --> 00:01:33,470
Now tracing is the process of
looking at your system, looking at

26
00:01:33,470 --> 00:01:35,420
the flow of request in your system.

27
00:01:35,810 --> 00:01:40,119
Now, if you're someone who is aiming
for P 99 performance or really high

28
00:01:40,119 --> 00:01:44,320
performance system tracing will
help you identifying the specific

29
00:01:44,320 --> 00:01:45,699
bottlenecks in your application.

30
00:01:46,030 --> 00:01:50,410
So that you can target that specific
area, reduce the latency, and

31
00:01:50,410 --> 00:01:51,790
increase the overall performance.

32
00:01:52,720 --> 00:01:56,860
Now dashboard is nothing but visual
representation of all the data that

33
00:01:56,860 --> 00:02:00,880
you're aggregating in all these pillars
and showing it in one particular

34
00:02:00,880 --> 00:02:05,350
dashboard so that you can act on that
information as quickly as possible.

35
00:02:05,590 --> 00:02:08,110
So they help you answering
questions like, how quickly can

36
00:02:08,110 --> 00:02:10,390
we act upon identifying an issue?

37
00:02:10,890 --> 00:02:14,400
Now for all these pillars I have
identified one of the most popular

38
00:02:14,400 --> 00:02:16,140
tools that are available, open source.

39
00:02:16,470 --> 00:02:20,340
Starting with logging flu, nd
is one of the most popular log

40
00:02:20,340 --> 00:02:23,280
aggregator found in open source.

41
00:02:23,310 --> 00:02:26,070
In the open source
community for monitoring.

42
00:02:26,070 --> 00:02:27,150
We have Prometheus.

43
00:02:27,180 --> 00:02:32,660
Prometheus is arguably one of the most
used components in a Kubernetes clusters

44
00:02:32,660 --> 00:02:34,549
for aggregating your metrics data.

45
00:02:35,049 --> 00:02:39,310
Coming back, coming to tracing,
we have a tool called Yeager.

46
00:02:39,370 --> 00:02:42,579
I could also find other tools
like Open Telemetry, but Yeager

47
00:02:42,579 --> 00:02:46,200
is one of the most easy to set
up especially for smaller teams.

48
00:02:46,680 --> 00:02:48,330
And finally, coming to dashboards.

49
00:02:49,110 --> 00:02:53,400
Grafana is arguably one of the most
used open source tools for setting up

50
00:02:53,400 --> 00:02:55,770
dashboards for your observability stack.

51
00:02:56,270 --> 00:03:00,490
Now we are going to look at how to
set up each and every single one of

52
00:03:00,490 --> 00:03:04,480
these components in your stack with
the minimum amount of steps starting

53
00:03:04,480 --> 00:03:08,770
with logging, you can, if you have a
Kubernetes cluster, you can directly

54
00:03:08,770 --> 00:03:10,600
run this command cube serial l apply.

55
00:03:10,900 --> 00:03:14,980
This will pull the Yam L file that is
written by the Fluentd team itself.

56
00:03:15,370 --> 00:03:18,910
This will make sure that your
cluster is always running the latest

57
00:03:18,910 --> 00:03:21,040
version of the flu ND components.

58
00:03:21,985 --> 00:03:23,035
Coming back to Helm.

59
00:03:23,035 --> 00:03:25,645
If you're someone who's using
Helm inside your cluster, you

60
00:03:25,645 --> 00:03:27,805
can also run this Helm Command.

61
00:03:28,135 --> 00:03:30,745
This will pull the helm
charts directly from the Helm

62
00:03:30,745 --> 00:03:32,545
repository that Fluent maintains.

63
00:03:32,935 --> 00:03:34,705
It'll install it in your cluster.

64
00:03:35,065 --> 00:03:40,195
Once you have the component running,
you can have your applications push your

65
00:03:40,285 --> 00:03:46,045
logs directly to this aggregator where
it exposes the Port 2, 4, 2 to four.

66
00:03:46,614 --> 00:03:50,665
And the best practice I would
recommend is to use JSON log so

67
00:03:50,665 --> 00:03:56,325
that it's easier for parsing per
fluent D. Next up, we have metrics.

68
00:03:56,655 --> 00:03:59,205
Now if you're someone who uses
helm, you can run this command.

69
00:03:59,265 --> 00:04:04,915
This pulls the Prometheus components
directly from the helm charts helm

70
00:04:04,915 --> 00:04:08,755
repository and installs it directly
in your Kubernetes clusters.

71
00:04:09,174 --> 00:04:11,905
Once you have that running,
you can use your applications.

72
00:04:12,139 --> 00:04:15,259
With, you can use the client libraries
available in different programming

73
00:04:15,259 --> 00:04:21,140
languages like go Java, Python,
c plus, and directly export your

74
00:04:21,140 --> 00:04:22,940
data into the metrics endpoint.

75
00:04:23,510 --> 00:04:25,790
Finally, you would also
have to do one more step.

76
00:04:26,360 --> 00:04:29,570
You would have to specify the
targets for your application.

77
00:04:29,570 --> 00:04:33,500
So in this example, I'm overriding
the values dot yammer file into

78
00:04:33,500 --> 00:04:38,090
my helm config and specifying
where exactly my applications lie

79
00:04:38,300 --> 00:04:39,890
application lies within the cluster.

80
00:04:40,490 --> 00:04:43,670
So in my example, this is
in my app service, which is

81
00:04:43,670 --> 00:04:46,100
expo exposing a port 80 80.

82
00:04:46,490 --> 00:04:51,940
In your case, maybe be the DNS of your
application itself coming to tracing.

83
00:04:52,510 --> 00:04:57,060
This is you can either install it
via, directly by the Kubernetes

84
00:04:57,060 --> 00:05:01,530
l file that Yeager provides, or
you can also use Helm as the other

85
00:05:01,530 --> 00:05:03,195
steps that we have seen so far.

86
00:05:03,945 --> 00:05:06,675
Once you do that, you will
have a Yeager agent running

87
00:05:06,675 --> 00:05:08,145
inside your Kubernetes cluster.

88
00:05:08,925 --> 00:05:12,645
Once you have that, you can have, again,
you can use the client libraries available

89
00:05:12,645 --> 00:05:13,995
in different programming languages.

90
00:05:14,055 --> 00:05:17,325
Export the data directly to the
yeager's collector endpoint.

91
00:05:17,895 --> 00:05:22,325
Now, this is an example from the
Python Library where I'm specifying

92
00:05:22,325 --> 00:05:24,395
the host name for my Yeager agent.

93
00:05:24,755 --> 00:05:27,305
There's nothing but the service
name inside the Kubernetes cluster.

94
00:05:28,264 --> 00:05:32,465
This is usually the port where
Yeager exposes its particular

95
00:05:32,465 --> 00:05:34,924
exporter endpoint, which is 6 8, 3 1.

96
00:05:34,924 --> 00:05:36,335
In your case, if it is different.

97
00:05:36,335 --> 00:05:38,944
If it is a different one,
you can use that one.

98
00:05:39,444 --> 00:05:39,534
Now.

99
00:05:39,534 --> 00:05:44,204
Finally, we have the dashboard wherein
we provide a visual representation for

100
00:05:44,204 --> 00:05:45,944
the data that we have collected so far.

101
00:05:46,514 --> 00:05:50,264
So you can either get your Grafana
components running directly via

102
00:05:50,264 --> 00:05:54,974
the Kubernetes AML file or the
helm as we have seen so far.

103
00:05:55,650 --> 00:05:59,789
Once you have that, you need to
specify data sources for your

104
00:05:59,789 --> 00:06:01,890
Grafana plus Grafana component.

105
00:06:02,460 --> 00:06:08,159
Now, data sources may be the Prometheus,
which is the metrics, the Yeager, which

106
00:06:08,190 --> 00:06:11,880
is nothing but the tracing data that
you've collected, and also the logs

107
00:06:11,880 --> 00:06:15,955
that you're collecting from the fluid
D. So in this example, I'm overriding

108
00:06:15,960 --> 00:06:20,010
the values dot yaml file inside the
help config and specifying the data

109
00:06:20,010 --> 00:06:22,109
source for my Prometheus component.

110
00:06:22,589 --> 00:06:25,919
So this will make sure that whatever
data that I'm collecting inside the

111
00:06:25,919 --> 00:06:31,119
Prometheus component is available in
my dashboard on the Grafana component.

112
00:06:31,619 --> 00:06:36,469
Now, once you have this observability
stack set up, there are some of

113
00:06:36,469 --> 00:06:40,359
the best practices and some of the
lessons some of the obvious pitfalls

114
00:06:40,359 --> 00:06:41,649
that I would want you to avoid.

115
00:06:42,219 --> 00:06:44,739
First one is avoid alert fatigue.

116
00:06:44,769 --> 00:06:50,769
Now I've seen this with multiple teams
setting up alerts for every single

117
00:06:50,769 --> 00:06:52,659
issues that pop up in their applications.

118
00:06:53,349 --> 00:06:59,329
Once you do that, you get so many
alerts that soon it can, it it's

119
00:06:59,329 --> 00:07:02,839
no longer a signal, but it becomes
noise, so you are more likely

120
00:07:02,839 --> 00:07:04,789
to miss some useful information.

121
00:07:05,509 --> 00:07:11,389
Now, this exactly happened in my previous
company wherein we set up alerts for

122
00:07:11,389 --> 00:07:13,429
every single issues, and soon enough.

123
00:07:13,969 --> 00:07:19,189
The mailbox, our and mailbox of our
entire team was so full that we stopped

124
00:07:19,189 --> 00:07:24,349
receiving alerts altogether and we were
more likely to miss some useful alerts.

125
00:07:24,889 --> 00:07:29,779
So I would want you to be very specific on
what issues that you want to track and set

126
00:07:29,779 --> 00:07:34,639
up alerts only for those issues and for
other issues you can get away with setting

127
00:07:34,639 --> 00:07:37,429
up logging inside your fluency component.

128
00:07:37,929 --> 00:07:43,449
Now the second pitfall that I would want
you to avoid is don't over sample metrics.

129
00:07:44,049 --> 00:07:47,919
Now, what that means is control
the frequency of data that you're

130
00:07:48,439 --> 00:07:50,179
sending to the metrics endpoint.

131
00:07:50,689 --> 00:07:55,879
Now what this means is do not send data
too frequent, for example, every second.

132
00:07:56,359 --> 00:08:00,919
Now, this also depends on the type of
systems that you are supporting, but

133
00:08:00,979 --> 00:08:06,439
having a good balance is always better
because if you're sending data too often.

134
00:08:07,369 --> 00:08:10,639
It soon becomes noise rather than
signal, so you're more likely

135
00:08:10,639 --> 00:08:11,989
to miss some useful information.

136
00:08:12,349 --> 00:08:18,189
So it's better to send data at a frequent
rate rather than at an acceptable

137
00:08:18,189 --> 00:08:19,829
rate, rather than very frequent.

138
00:08:20,129 --> 00:08:23,309
So something like one
minute is a good balance.

139
00:08:23,809 --> 00:08:27,409
Finally, dashboard hygiene is
one of the biggest mistakes

140
00:08:27,409 --> 00:08:29,204
that SRE engineers often make.

141
00:08:29,914 --> 00:08:33,994
What they think is, the more data we
provide on the dashboard, the better

142
00:08:33,994 --> 00:08:36,484
insights that we can extract from it.

143
00:08:36,964 --> 00:08:41,524
That is actually far from truth, far from
the truth, because the more data that

144
00:08:41,524 --> 00:08:43,804
you provide, the more noise you create.

145
00:08:43,804 --> 00:08:48,334
So more likely you are to
miss some useful information.

146
00:08:48,754 --> 00:08:52,349
So what I would recommend is
based on the individual that

147
00:08:52,349 --> 00:08:53,609
you're creating the dashboard for.

148
00:08:54,109 --> 00:08:57,349
Keep the data that you're providing,
very focused for that individual.

149
00:08:57,349 --> 00:09:02,389
So for example, if you're providing
the dashboard for a manager, have data

150
00:09:02,389 --> 00:09:06,709
that provides a 10,000 feet overview of
the system rather than providing very

151
00:09:06,709 --> 00:09:08,749
specific data inside the application.

152
00:09:09,249 --> 00:09:13,089
So you keep the noise outside
and you provide more signals

153
00:09:13,089 --> 00:09:14,679
for that particular individual.

154
00:09:15,319 --> 00:09:17,569
Similarly if you're
creating a dashboard for.

155
00:09:18,184 --> 00:09:23,584
A developer, I would ask, I would want
you to provide data that's giving a more

156
00:09:23,584 --> 00:09:25,594
deeper look into the application itself.

157
00:09:25,864 --> 00:09:30,244
Like something like the logging aggregator
would be a good use for developers.

158
00:09:30,754 --> 00:09:34,084
So based on the individual that
you're creating the dashboard for,

159
00:09:34,654 --> 00:09:38,644
keep the data very focused for that
particular individual so that you

160
00:09:38,644 --> 00:09:43,804
don't create more noise and you
provide some useful insights Now.

161
00:09:44,584 --> 00:09:47,634
Quickly giving a summary of
what we have discussed so far.

162
00:09:47,634 --> 00:09:51,864
We discussed the four pillars of
observability, starting with logging.

163
00:09:52,314 --> 00:09:57,024
This helps you answer questions like, what
is happening in my system so that I can

164
00:09:57,024 --> 00:09:59,514
identify any issues that are going on.

165
00:09:59,904 --> 00:10:07,154
Looking at the logs itself, metrics helps
you answer questions like, how can I

166
00:10:08,114 --> 00:10:10,274
quantify the performance of my system?

167
00:10:10,774 --> 00:10:14,744
Coming to tracing helps you answer
questions like, what, where is the

168
00:10:14,744 --> 00:10:19,814
bottleneck in my system so that I can
target a specific area and increase

169
00:10:19,814 --> 00:10:21,404
the overall performance of my system?

170
00:10:22,154 --> 00:10:23,294
And dashboards are nothing.

171
00:10:23,294 --> 00:10:26,934
But providing a visual representation
of the data that you have created

172
00:10:27,534 --> 00:10:32,664
so that you can identify issues in
a particular area and you can act

173
00:10:32,664 --> 00:10:34,704
on that quickly as soon as possible.

174
00:10:35,204 --> 00:10:40,454
I hope that this talk was helpful, that
it provided you with the minimum amount

175
00:10:40,454 --> 00:10:43,604
of steps for getting an observability
stack running in your system.

176
00:10:44,084 --> 00:10:47,774
And if you want to discuss more on
this topic, and this is my LinkedIn

177
00:10:48,264 --> 00:10:52,704
this is my GitHub and this is my
blog where I post similar software

178
00:10:52,704 --> 00:10:56,434
engineering content content that
I learn on a day by day basis.

179
00:10:56,584 --> 00:10:56,914
Thank you.

