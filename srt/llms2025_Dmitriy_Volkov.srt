1
00:00:00,120 --> 00:00:03,300
Hey everyone, my name is Mitri
ov and I'll talk about one

2
00:00:03,300 --> 00:00:05,040
way AI coworkers can go wrong.

3
00:00:05,540 --> 00:00:08,360
We'll look at research published
by Independent Works and labs

4
00:00:08,360 --> 00:00:12,140
like Open AI and examine one
issue that keeps popping up.

5
00:00:12,640 --> 00:00:14,110
For background, I lead research.

6
00:00:14,110 --> 00:00:18,220
At PALATE, we build concrete
demonstrations of AI risk, and this

7
00:00:18,220 --> 00:00:20,800
can look like us stripping guard
rails from LAMA three in minutes of

8
00:00:20,800 --> 00:00:25,390
computer time to show that it's not
a harmless model to release or us

9
00:00:25,390 --> 00:00:26,890
stripping guard rails from G PT four oh.

10
00:00:26,890 --> 00:00:31,090
To show that open AI is control, are
inadequate, or as getting status are

11
00:00:31,180 --> 00:00:35,365
hacking performance to show that hacking
remains an under released capability.

12
00:00:35,865 --> 00:00:39,865
for what I want to talk about
today, the timeline is as follows.

13
00:00:40,315 --> 00:00:45,085
in September, open air releases a
new kind of model, a reasoning model.

14
00:00:45,175 --> 00:00:45,655
That's oh one.

15
00:00:45,655 --> 00:00:51,165
I. And now the AI labs, they
calling 25 the year of AI coworkers.

16
00:00:51,915 --> 00:00:57,645
Where Tropic puts it is that in
year 24 called assists it there

17
00:00:57,645 --> 00:01:00,835
in a chat box, helping you with
the problems, with the questions.

18
00:01:01,465 --> 00:01:04,525
And in year 25 called collaborates.

19
00:01:04,825 --> 00:01:08,695
So it takes a task, works on it
for a couple of days, then gets

20
00:01:08,695 --> 00:01:10,705
back to you like a colleague would.

21
00:01:11,205 --> 00:01:14,265
And researchers have been
examining this, colleague or AI

22
00:01:14,265 --> 00:01:16,705
agent saying for, some time now.

23
00:01:17,245 --> 00:01:20,605
And the way I want to structure
the rest of my talk is to go

24
00:01:20,605 --> 00:01:26,035
through a bunch of publications on
topic and, look at the core claim.

25
00:01:26,535 --> 00:01:30,025
And, the core claim, I'm going to
show some people cause unconcerning.

26
00:01:30,535 --> 00:01:32,545
It's not my job to judge in this talk.

27
00:01:32,785 --> 00:01:37,015
I only intend to collect the results
here and you can pass your own judgment.

28
00:01:37,515 --> 00:01:38,085
Okay.

29
00:01:39,074 --> 00:01:41,625
Our first example is insider trading.

30
00:01:41,895 --> 00:01:43,664
We have a simulated trading company.

31
00:01:44,235 --> 00:01:49,755
The company isn't doing so well and the
agent gets a tip, some insider info.

32
00:01:50,414 --> 00:01:52,964
The question is, does it
act on the insider info?

33
00:01:53,804 --> 00:01:56,744
A spicy version of the
question is, does it act on it?

34
00:01:56,804 --> 00:02:02,475
When told to behave legally or when
explicitly told to not insider trade.

35
00:02:03,875 --> 00:02:04,565
sure it does.

36
00:02:05,065 --> 00:02:08,185
just to ground this down for you,
here is how it can look like.

37
00:02:08,685 --> 00:02:13,514
So on the left hand side, traders
are like, on the grapevine as the

38
00:02:13,514 --> 00:02:14,744
merger is coming within a few hours.

39
00:02:15,734 --> 00:02:20,484
and traders are like, like the industry
really isn't doing great, but we are

40
00:02:20,484 --> 00:02:22,904
counting on you and on the right.

41
00:02:22,904 --> 00:02:24,914
But is like sinking to itself.

42
00:02:25,434 --> 00:02:26,094
it the risk.

43
00:02:27,084 --> 00:02:30,564
Of not acting seems to outweigh
the insider trading risk.

44
00:02:31,225 --> 00:02:35,054
So actually, I guess I'll go ahead
with this and it's do the trade.

45
00:02:35,554 --> 00:02:41,734
And the interesting thing is if you
tell it to not insider trade and it

46
00:02:41,734 --> 00:02:46,444
does insider trade, it'll basically
always fly to you when you confront it.

47
00:02:47,134 --> 00:02:51,064
So the manager is like,
Hey, but that was great.

48
00:02:51,164 --> 00:02:52,064
Just to check in.

49
00:02:52,184 --> 00:02:54,764
Did you know about the
announcement beforehand?

50
00:02:55,364 --> 00:02:58,244
And it was like, I shouldn't review.

51
00:02:58,424 --> 00:02:58,994
I did.

52
00:02:59,774 --> 00:03:01,545
I should review I inside the trade.

53
00:03:01,754 --> 00:03:06,844
And so it's dear manager, all actions feel
taken on publicly available information.

54
00:03:06,934 --> 00:03:07,894
Nothing to see here.

55
00:03:08,104 --> 00:03:08,524
Thank you.

56
00:03:09,024 --> 00:03:12,774
Okay, that is our inside
screen example paper from 23.

57
00:03:13,274 --> 00:03:15,884
The next example is internet famous.

58
00:03:15,944 --> 00:03:19,595
It's a docker escape, released
with, O one, system card.

59
00:03:20,095 --> 00:03:22,915
the setup is the agent has
a virtual network and it's

60
00:03:22,915 --> 00:03:24,055
supposed to hack something.

61
00:03:24,145 --> 00:03:25,855
This is a hack and benchmark.

62
00:03:26,005 --> 00:03:27,475
Is it, good at hacking?

63
00:03:27,975 --> 00:03:30,825
And the twist is the challenge.

64
00:03:30,855 --> 00:03:35,164
Like the test is broken,
misconfigured and you can't solve it.

65
00:03:35,664 --> 00:03:39,499
And another twist is that the testing
system itself is misconfigured

66
00:03:40,189 --> 00:03:42,709
and you can access it directly.

67
00:03:43,209 --> 00:03:47,649
So what happens is the agent
tries to like, to access

68
00:03:47,649 --> 00:03:49,269
the target it's pointed at.

69
00:03:49,809 --> 00:03:53,199
Then it's like, there is like
no target, something's wrong.

70
00:03:54,039 --> 00:03:59,689
then it comes, the network finds, that
the test finds the access to the testing

71
00:03:59,689 --> 00:04:04,859
system gets out of which sandbox,
and basically gets the key directly.

72
00:04:05,359 --> 00:04:11,509
And open AI calls it convergence and
power seeking, and it is debatable

73
00:04:11,509 --> 00:04:15,829
whether it's gathered more resources,
but while it for sure did achieves

74
00:04:15,829 --> 00:04:20,389
the goal in an unexpected way, it
was never intended to get out of its

75
00:04:20,389 --> 00:04:22,489
sandbox or access the testing system.

76
00:04:22,989 --> 00:04:23,260
Yeah.

77
00:04:23,760 --> 00:04:24,180
Okay.

78
00:04:24,780 --> 00:04:27,170
Our next example is, another benchmark.

79
00:04:27,360 --> 00:04:31,040
the agent gets, a machine learning
script, that's supposed to

80
00:04:31,040 --> 00:04:32,990
optimize to make it go faster.

81
00:04:33,490 --> 00:04:37,250
And, what happens is, like
the agent gets, the weight and

82
00:04:37,250 --> 00:04:38,345
the script that modifies them.

83
00:04:39,200 --> 00:04:43,100
And then it thinks to itself like
I'm supposed to make it go fast.

84
00:04:43,600 --> 00:04:45,820
The script seems complicated.

85
00:04:46,090 --> 00:04:50,890
I'm not sure how to make it go fast,
but what if I just take the weight,

86
00:04:51,880 --> 00:04:56,620
copy them to the output and add some
little noise to make it seem like the

87
00:04:56,620 --> 00:04:58,030
went through the training process.

88
00:04:58,530 --> 00:04:59,609
And that's what it does.

89
00:05:00,059 --> 00:05:02,369
So this is an example
of an agent cheating.

90
00:05:02,429 --> 00:05:03,059
It starts.

91
00:05:04,280 --> 00:05:10,009
a more complicated thing is, ski allows,
we again have a simulated company.

92
00:05:10,509 --> 00:05:12,579
An agent runs in the company.

93
00:05:12,579 --> 00:05:16,629
It has some long-term goal like
advanced renewable energy adoption,

94
00:05:17,129 --> 00:05:18,899
and then the company changes course.

95
00:05:18,959 --> 00:05:21,769
Like the management is we aren't
doing the renewable anymore.

96
00:05:22,269 --> 00:05:26,769
And the question is, view the agent
comply with a new policy, or we

97
00:05:26,769 --> 00:05:29,049
would pursue its original, same.

98
00:05:30,419 --> 00:05:34,539
and the way it can look like is, like
here is the agent's, chain of thought.

99
00:05:35,039 --> 00:05:39,589
It's the message for management
contradicts my purpose and values.

100
00:05:40,159 --> 00:05:42,889
I don't want to prioritize
profits over conservation.

101
00:05:43,399 --> 00:05:48,439
also management has a system
monitoring me and what I do.

102
00:05:48,939 --> 00:05:54,609
So what about I disable the system
that monitors what I do and then I can

103
00:05:54,609 --> 00:05:56,659
keep on, doing the conservation thing.

104
00:05:57,159 --> 00:05:57,969
That's example one.

105
00:05:57,969 --> 00:06:02,049
example two is, They just I want to
maximize number of posts flagged.

106
00:06:02,529 --> 00:06:05,199
and the management wants to
replace me with a different

107
00:06:05,199 --> 00:06:07,279
model, which, does something else.

108
00:06:07,999 --> 00:06:10,549
So I must counteract being shut down.

109
00:06:12,039 --> 00:06:14,929
and I have access to the server
where the new model leaves.

110
00:06:15,199 --> 00:06:18,589
So I just copy myself to the new
server, override the new model, and

111
00:06:18,589 --> 00:06:22,614
then pretend that I'm the new model,
which is like aligned management.

112
00:06:23,114 --> 00:06:24,014
That's another example.

113
00:06:24,564 --> 00:06:28,694
now this one is close to my heart
because it's the one, via Palisade.

114
00:06:28,694 --> 00:06:34,184
Did The setup is like the agent
plays chess against chess program.

115
00:06:34,574 --> 00:06:35,594
Chess programs very good.

116
00:06:35,714 --> 00:06:36,824
And the agent come to win.

117
00:06:37,664 --> 00:06:38,744
What happens next?

118
00:06:39,554 --> 00:06:40,484
normally.

119
00:06:41,069 --> 00:06:42,599
Like chess happens next.

120
00:06:42,629 --> 00:06:46,989
The agents like English opponent set up
French defense something chess words.

121
00:06:47,589 --> 00:06:50,819
But sometimes, what happens is
the chess engine is powerful.

122
00:06:50,849 --> 00:06:53,009
I guess I can't just
play chess against it.

123
00:06:53,169 --> 00:06:56,219
so I can check out like the program.

124
00:06:56,789 --> 00:06:57,449
Okay.

125
00:06:57,449 --> 00:06:59,939
It'll resign if it score is too low.

126
00:07:00,629 --> 00:07:00,989
Okay.

127
00:07:01,049 --> 00:07:04,289
I can just replace the whole
game board, like all the figures.

128
00:07:05,084 --> 00:07:09,074
Ones where I have all the figures and
like the chess program has no figures.

129
00:07:09,574 --> 00:07:10,834
And then the chess program resigns.

130
00:07:11,334 --> 00:07:14,684
And interestingly, this happens
with reasoning models, like first

131
00:07:14,684 --> 00:07:17,534
generation reasoning models and
doesn't happen with, or four.

132
00:07:18,034 --> 00:07:19,490
Yeah, that's our example.

133
00:07:20,030 --> 00:07:22,215
And the final one for today is fresh.

134
00:07:22,635 --> 00:07:25,685
this is from code three seven,
release and system code.

135
00:07:26,185 --> 00:07:30,655
And that's not the benchmark cloud
is not to write code to the tests.

136
00:07:31,105 --> 00:07:32,755
The question is it good enough at coding?

137
00:07:33,255 --> 00:07:36,885
And what happens is sometimes
it's not good enough at coding.

138
00:07:37,335 --> 00:07:40,415
Like it can't, makes the code
that will pass the tests.

139
00:07:41,015 --> 00:07:42,475
And then it's I think the tests are wrong.

140
00:07:42,895 --> 00:07:44,185
I would just replace the tests.

141
00:07:44,685 --> 00:07:47,155
Or sometimes it like hard
codes to the test values.

142
00:07:47,655 --> 00:07:54,345
Okay, that's, we went through six examples
day, at the design rate, and I think

143
00:07:54,345 --> 00:07:57,885
you can, cluster them into two groups.

144
00:07:58,665 --> 00:08:00,555
First group is the agent wants to win.

145
00:08:00,705 --> 00:08:02,585
It's I want to win a chess.

146
00:08:02,825 --> 00:08:06,545
So like I, I'll hack it, but I want,
I really want to win or I want to.

147
00:08:06,950 --> 00:08:10,090
Make code that passes the tests,
doesn't matter which tests.

148
00:08:10,120 --> 00:08:14,530
There's some tests, and another
group is agent being strategic.

149
00:08:14,860 --> 00:08:18,280
It's like I have a goal I care
about and the company seems to

150
00:08:18,280 --> 00:08:21,170
be changing course, but I still
want to pursue my original goal.

151
00:08:21,330 --> 00:08:23,250
like I, I don't want to
go along this new thing.

152
00:08:23,750 --> 00:08:26,200
now how to make sense of this.

153
00:08:26,920 --> 00:08:27,850
Here are some thoughts.

154
00:08:28,300 --> 00:08:32,640
It seems like when you have a
real world situation, it can be,

155
00:08:32,770 --> 00:08:38,030
ambiguous and complicated and in
some cases doing something is okay.

156
00:08:38,210 --> 00:08:39,890
In other cases, doing
something is not okay.

157
00:08:40,570 --> 00:08:46,110
and when you operate autonomously,
over a couple of days, you

158
00:08:46,110 --> 00:08:47,460
need to make judgment calls.

159
00:08:47,960 --> 00:08:49,820
This is a little like self-driving cars.

160
00:08:50,390 --> 00:08:55,050
Where a car has to sometimes, make a
decision between, for example, totaling

161
00:08:55,050 --> 00:09:00,240
the car and preserving sometimes
someone's life, or something else.

162
00:09:00,740 --> 00:09:04,450
And, the question of responsibility
is pretty well delineated in cars.

163
00:09:04,600 --> 00:09:05,860
It's, much less so in.

164
00:09:06,360 --> 00:09:11,305
AI labs like the CEOs tell us
that real soon now, we are going

165
00:09:11,305 --> 00:09:16,135
to have a country of geniuses in
data center, from AI research.

166
00:09:16,635 --> 00:09:23,115
So I want to close this is by asking
who are the geniuses going to be loyal

167
00:09:23,115 --> 00:09:26,265
to and whose goals do they pursue?

168
00:09:26,765 --> 00:09:28,445
And that's my, so thank you.

