1
00:00:00,500 --> 00:00:02,119
Speaker 22: Hello Con 42 attendees.

2
00:00:02,180 --> 00:00:04,070
Welcome to my talk on Guardrails.

3
00:00:04,070 --> 00:00:08,410
First, a framework for designing
safe, scalable, and accountable

4
00:00:08,410 --> 00:00:09,520
AI agents for security operations.

5
00:00:10,020 --> 00:00:10,979
Let's dive right in.

6
00:00:11,580 --> 00:00:15,510
The first thing you may be wondering
is why do I care about security

7
00:00:15,510 --> 00:00:17,369
operations as a DevOps person?

8
00:00:17,970 --> 00:00:22,020
And let me assure you, and I'm willing
to bet money on this, is that your

9
00:00:22,020 --> 00:00:25,919
security operations colleagues will
come to you seeking your help to build,

10
00:00:25,919 --> 00:00:28,649
deploy AI agents literally this year.

11
00:00:28,774 --> 00:00:32,435
And when that happens, my hope
is by spending the next 30 or so

12
00:00:32,435 --> 00:00:36,515
minutes with me, you'll find yourself
much better equipped to help them

13
00:00:36,515 --> 00:00:40,444
and your organization succeed
with AI in security operations.

14
00:00:41,375 --> 00:00:44,125
What qualifies me as a
presenter on this topic.

15
00:00:44,465 --> 00:00:47,555
First off, I have over five years
of experience across the leading

16
00:00:47,555 --> 00:00:52,425
security operations companies, building
solutions for practitioners Proofpoint,

17
00:00:52,540 --> 00:00:55,485
Zscaler, Splunk, and Cisco are just
some of the names that I've worked at.

18
00:00:56,295 --> 00:01:00,975
Secondly, I talk to over 150 security
operations teams every single year

19
00:01:01,255 --> 00:01:05,245
of all shapes and sizes, including
large Fortune 500 organizations

20
00:01:05,454 --> 00:01:08,905
with mature practices, as well as
organizations that are literally

21
00:01:08,905 --> 00:01:10,645
building a SOC for the first time.

22
00:01:11,345 --> 00:01:14,000
I know a thing or two
about security operations.

23
00:01:14,630 --> 00:01:18,830
Now, the thing with security operations
is even before Gen AI happened,

24
00:01:19,130 --> 00:01:20,600
there were massive challenges.

25
00:01:20,929 --> 00:01:24,979
Teams were faced with alert, fatigue,
tools, sprawl, and there was a widening

26
00:01:24,979 --> 00:01:28,220
skill shortage where there were so
many jobs with not enough skilled

27
00:01:28,220 --> 00:01:32,735
people to be able to man these posts
needed to keep organizations secure.

28
00:01:33,259 --> 00:01:36,695
And then gen AI happened
in 2022, making this all.

29
00:01:37,085 --> 00:01:43,535
Even worse because AI made it easy for
attackers to launch attacks faster, scale

30
00:01:43,535 --> 00:01:48,705
them better, and also lowered the skill
required to be able to make to be able

31
00:01:48,705 --> 00:01:50,715
to launch really massive attacks, right?

32
00:01:51,255 --> 00:01:55,245
And that, that basically made the
challenge for security operations terms,

33
00:01:55,245 --> 00:01:57,375
which was already a very bad situation.

34
00:01:57,785 --> 00:01:58,865
Even more worse.

35
00:01:59,525 --> 00:02:04,514
The good news though, or the slight silver
lining is ai and AI agents in particular

36
00:02:04,905 --> 00:02:10,275
also present a possible solution, and
in my mind, are an absolute necessity

37
00:02:10,574 --> 00:02:15,095
for SOC teams to be able to keep up
with the increased demands off of them.

38
00:02:15,595 --> 00:02:18,280
So AI presents a massive opportunity.

39
00:02:18,834 --> 00:02:22,915
But the challenge with security operations
or just security in general is you are

40
00:02:22,915 --> 00:02:25,225
paid to be right every single time.

41
00:02:25,555 --> 00:02:31,315
And LLMs that are non-deterministic have
high amounts of variability and could be

42
00:02:31,315 --> 00:02:36,535
wrong at times are really not a recipe
for success in security operations.

43
00:02:37,075 --> 00:02:41,205
So how do we actually realize this
opportunity despite all of the

44
00:02:41,205 --> 00:02:43,995
difficulties and challenges and.

45
00:02:44,495 --> 00:02:48,755
There are two, two broad set of
options but really it's buy or build.

46
00:02:49,144 --> 00:02:53,245
I don't think I, I don't think the
decision of whether you choose to buy

47
00:02:53,245 --> 00:02:57,265
or whether you choose to build your own
is really the most fundamental decision.

48
00:02:57,535 --> 00:02:58,325
That said, there are.

49
00:02:58,665 --> 00:03:03,175
A plenty of options out there from
both perspectives where, there are many

50
00:03:03,175 --> 00:03:07,225
startups that have spun up and are trying
to build AI SOC platforms as evidenced

51
00:03:07,225 --> 00:03:11,725
on my slide there, there's typically also
going to be good ics in your organization.

52
00:03:11,725 --> 00:03:15,405
It could be a security engineer, it
could be a SOC analyst who is, built

53
00:03:15,615 --> 00:03:20,625
a prototype that, tries to use AI
agents or deploy AI agents to help.

54
00:03:21,270 --> 00:03:23,730
Alleviate some of the pressure
that SOC teams are feeling.

55
00:03:24,210 --> 00:03:27,990
However, the challenge that you're gonna
find is all of these solutions demo

56
00:03:27,990 --> 00:03:30,060
very similarly and demo really well.

57
00:03:30,420 --> 00:03:33,960
However, when it comes to building
trust to be able to productionize

58
00:03:33,960 --> 00:03:37,590
these solutions, that's where you're
gonna have a really hard time.

59
00:03:38,220 --> 00:03:41,100
And the goal behind this
talk and introducing the

60
00:03:41,100 --> 00:03:42,150
guardrails first framework.

61
00:03:43,035 --> 00:03:47,715
Is to give you a methodology for how
you may go about, deploying these

62
00:03:47,715 --> 00:03:52,035
agents in production so that you're
successful in your endeavors, right?

63
00:03:52,245 --> 00:03:54,405
And there are four
pillars to this approach.

64
00:03:54,705 --> 00:03:59,215
Firstly, making sure that context
about your organization is

65
00:03:59,215 --> 00:04:00,715
front and center in the design.

66
00:04:01,435 --> 00:04:05,125
Appropriate policy controls are
in place so that you don't, you're

67
00:04:05,125 --> 00:04:08,825
not looking at adverse outcomes
if you get suboptimal results.

68
00:04:09,325 --> 00:04:13,465
Solution needs to be designed with
trust and auditability in mind.

69
00:04:13,705 --> 00:04:17,575
And then lastly, it needs to be human
in the loop for critical actions

70
00:04:17,575 --> 00:04:20,965
that it may take, which may pose
a more pose potential harm to your

71
00:04:20,965 --> 00:04:22,555
organization if done incorrectly.

72
00:04:23,055 --> 00:04:25,575
Let's double click into this
framework and see it in action.

73
00:04:26,055 --> 00:04:29,565
Let's talk about the first pillar,
which is organizational context.

74
00:04:29,925 --> 00:04:34,035
Now, I'll tell you this, from my years
of experience working with security

75
00:04:34,035 --> 00:04:40,935
operations teams, the process for triaging
or investigating and identical alert.

76
00:04:41,520 --> 00:04:44,760
Is never the same or
never exactly the same.

77
00:04:45,120 --> 00:04:50,130
Across two different organizations,
depending on the security tools that they

78
00:04:50,130 --> 00:04:55,260
own, how their data is structured and what
their standard operating procedures looks

79
00:04:55,260 --> 00:04:59,450
like you will have a very different set
of standards that need to be followed.

80
00:04:59,700 --> 00:05:03,809
Two, two triage and investigate
literally the exact same alert across

81
00:05:03,809 --> 00:05:05,309
two, two different organizations.

82
00:05:05,690 --> 00:05:08,700
And that's where AI really needs to begin.

83
00:05:09,030 --> 00:05:12,810
So as you think about building AI
agents in your own organization,

84
00:05:13,350 --> 00:05:18,330
you need to leverage all of the
context about your organization in

85
00:05:18,330 --> 00:05:20,730
training data for your AI agents.

86
00:05:21,090 --> 00:05:22,410
And what do I mean by that?

87
00:05:22,410 --> 00:05:26,310
The first part of this is you
need to leverage all of the human

88
00:05:26,310 --> 00:05:27,750
knowledge in your environment.

89
00:05:28,190 --> 00:05:31,460
For things that have already
been triaged or investigated.

90
00:05:31,940 --> 00:05:37,079
So what I mean by that is you probably
have a plethora of case notes that

91
00:05:37,079 --> 00:05:40,979
indicate processes currently being
followed to triage and investigate various

92
00:05:40,979 --> 00:05:42,900
types of alerts in your organization.

93
00:05:43,789 --> 00:05:48,379
Secondly, you probably have lead
SOC analysts who are able to write

94
00:05:48,499 --> 00:05:53,269
pretty detailed SOPs or standard
operating procedures for how they go

95
00:05:53,269 --> 00:05:57,799
about investigating specific types
of alerts that originate from various

96
00:05:57,799 --> 00:05:59,149
detections in your environment.

97
00:06:00,019 --> 00:06:05,684
And both of those serve as a key
training material to help your AI agents.

98
00:06:06,234 --> 00:06:10,764
Both from a rag or prompting context,
succeed in your own environment

99
00:06:10,944 --> 00:06:14,924
and understand the nuances of
what investigation or evidence is

100
00:06:14,924 --> 00:06:16,334
really required to be successful.

101
00:06:16,834 --> 00:06:20,104
One thing I will warn you from
talking to all of these teams.

102
00:06:20,459 --> 00:06:24,899
That you may already have a standard
set of SOPs that are written, but

103
00:06:24,899 --> 00:06:28,800
you, as you discover as you start
building AI agents, you're likely to

104
00:06:28,800 --> 00:06:33,630
discover that there are gaps in those
SOPs, not just from a perspective of

105
00:06:33,659 --> 00:06:36,689
not just from the perspective that
some steps are being missed, but also

106
00:06:36,689 --> 00:06:40,150
from the standpoint of hey if an AI
agent is really to understand these.

107
00:06:40,715 --> 00:06:44,515
Some of the steps may need to be
rewritten without, corporate jargon

108
00:06:44,515 --> 00:06:46,875
that, AI may have no path to succeed.

109
00:06:47,414 --> 00:06:52,315
And the third thing I will say is
anytime you are going to go ahead

110
00:06:52,315 --> 00:06:56,155
and deploy this agent, and this may
sound really basic, but anytime you're

111
00:06:56,155 --> 00:07:01,435
deploying an increment, make sure you
have a very robust back testing pipeline

112
00:07:01,735 --> 00:07:04,525
where this AI agent can disposition.

113
00:07:04,604 --> 00:07:11,214
Alerts or analyze alerts that have
already been labeled by your security

114
00:07:11,214 --> 00:07:15,324
operations team from a manual
investigation and triage process.

115
00:07:15,654 --> 00:07:19,434
So a back testing pipeline
is an absolute must.

116
00:07:19,464 --> 00:07:26,694
Third element, this is how organizational
context can really be at the center of the

117
00:07:26,874 --> 00:07:29,499
AI learning curriculum and help AI agents.

118
00:07:30,289 --> 00:07:33,379
Actually be practically useful
in your production environment.

119
00:07:33,879 --> 00:07:37,179
The second pillar that I want to
talk about is policy constraints.

120
00:07:37,509 --> 00:07:42,429
You do know that AI at times can
produce less than ideal unpredictable

121
00:07:42,459 --> 00:07:48,199
outcomes, and that's where policy can
really be a CYA with capability, scope,

122
00:07:48,199 --> 00:07:52,369
tools, policy driven gating, human
oversight and decision pro provenance.

123
00:07:52,744 --> 00:07:56,755
Being built into sort of the design of
your AI agents or one that you procure.

124
00:07:57,255 --> 00:07:59,955
Now, what do I mean by
capability, scope tools?

125
00:08:00,405 --> 00:08:03,555
The first thing is, decide
what the boundary really is.

126
00:08:03,615 --> 00:08:07,785
Is this a tool that is trying to
perform enrichment or is this an agent

127
00:08:07,785 --> 00:08:09,865
that's, trying to go update tickets?

128
00:08:09,865 --> 00:08:13,345
Is it one that's gonna go take
response actions in your environment?

129
00:08:13,825 --> 00:08:15,145
If it is one that is.

130
00:08:15,345 --> 00:08:17,085
Purely going to do enrichment.

131
00:08:17,415 --> 00:08:22,275
Make sure that it is scoped down
to only have access to tools

132
00:08:22,305 --> 00:08:23,955
that allow it to do enrichment.

133
00:08:24,355 --> 00:08:28,495
That way you are able to limit
your blast radius in, in case it

134
00:08:28,495 --> 00:08:30,235
ever reaches the wrong conclusion.

135
00:08:30,565 --> 00:08:34,014
Where you may not want it to go
take a response action locking out

136
00:08:34,014 --> 00:08:36,864
a user account or locking out a
domain controller, which makes your.

137
00:08:37,700 --> 00:08:39,590
Entire organization go dark.

138
00:08:39,840 --> 00:08:41,490
Things like that could happen.

139
00:08:41,540 --> 00:08:46,080
And, you wanna make sure that you are
building in controls in your architecture

140
00:08:46,290 --> 00:08:47,520
to prevent that from happening.

141
00:08:47,790 --> 00:08:52,280
So all of these AI agents need to
be scoped or need to be exposed

142
00:08:52,280 --> 00:08:57,130
to tools that are scoped down to
specific purposes in terms of what

143
00:08:57,130 --> 00:08:58,780
they, are permitted to achieve.

144
00:08:59,560 --> 00:09:00,130
Next.

145
00:09:00,410 --> 00:09:00,785
You should have.

146
00:09:01,205 --> 00:09:06,935
Some level of policy driven gating where
AI is able to come up with a proposal.

147
00:09:07,175 --> 00:09:11,285
It is evaluated against various
policies that you have in place.

148
00:09:11,705 --> 00:09:17,495
And a decision in made is made on whether
this is ex supposed to be escalated

149
00:09:17,495 --> 00:09:21,695
for human review, whether this is
supposed to be automatically executed

150
00:09:22,005 --> 00:09:25,635
or even if it is automatically executed
that it actually had the right in.

151
00:09:25,685 --> 00:09:27,305
Outcome of that execution.

152
00:09:27,725 --> 00:09:30,465
So policy constraints or
policy driven gating is an

153
00:09:30,465 --> 00:09:32,085
important part of that design.

154
00:09:32,585 --> 00:09:36,175
Now, all of this, organizational
context and policy is great,

155
00:09:36,355 --> 00:09:38,185
but how do you build trust?

156
00:09:38,275 --> 00:09:41,695
And this is one of those areas where
over time I think you will have

157
00:09:41,695 --> 00:09:43,745
to do less because AI will just.

158
00:09:44,165 --> 00:09:46,625
The tooling should just get
better and better over time.

159
00:09:47,015 --> 00:09:51,675
And people as they embrace this new
technology will be more trusting

160
00:09:51,675 --> 00:09:53,265
of it than they are at the moment.

161
00:09:53,345 --> 00:09:57,005
The things that, that I would
advocate for is when you're looking

162
00:09:57,005 --> 00:10:02,785
at either buying or building your own
AI agents for security operations.

163
00:10:03,200 --> 00:10:06,560
Make sure that you have
access to a full audit trail.

164
00:10:06,650 --> 00:10:13,720
This includes an entire timeline of every
step the AI agent took right from, what

165
00:10:13,720 --> 00:10:16,440
data it used to arrive at its conclusions.

166
00:10:16,710 --> 00:10:18,630
How did it reason past that data?

167
00:10:18,840 --> 00:10:21,570
What are all the actions that
it took, and then what was the

168
00:10:21,570 --> 00:10:23,010
final outcome that it suggested?

169
00:10:23,340 --> 00:10:28,950
A hundred percent traceability and
auditability is a must have when it

170
00:10:28,950 --> 00:10:32,700
comes to succeeding with AI agents
in production, in your environment.

171
00:10:33,120 --> 00:10:37,140
Make sure you hold that bar off
any solution that is either being

172
00:10:37,140 --> 00:10:39,750
built internally or being procured.

173
00:10:39,800 --> 00:10:43,130
If you want to be able to succeed
in a manner where you're truly able

174
00:10:43,130 --> 00:10:47,275
to augment, your SOC analyst with AI
agents to help investigate and triage.

175
00:10:47,775 --> 00:10:50,885
And lastly, from a adoption
roadmap standpoint.

176
00:10:51,415 --> 00:10:57,325
I would advocate for taking a very human
in the loop oriented approach and a

177
00:10:57,325 --> 00:11:03,115
practical way of achieving that is any
solution you deploy, I would advocate to

178
00:11:03,115 --> 00:11:08,695
begin it pretty much in shadow mode where
it is accessible to a subset of security

179
00:11:08,695 --> 00:11:11,005
operations leads in your environment.

180
00:11:11,335 --> 00:11:16,135
Not even the entire security operations
team where AI is silently making

181
00:11:16,135 --> 00:11:18,535
recommendations on what it would have.

182
00:11:18,910 --> 00:11:24,150
Dispositioned a particular alert or
finding as and that is just evaluated

183
00:11:24,150 --> 00:11:29,100
by some of your best SOC analysts or
SOC leads to find if the reasoning

184
00:11:29,100 --> 00:11:31,560
and the conclusions actually hold up.

185
00:11:32,370 --> 00:11:36,240
The second phase I would
advocate for is to go ahead

186
00:11:36,270 --> 00:11:38,940
and allow AI to make proposals.

187
00:11:39,285 --> 00:11:43,865
Pretty much on your, ui where
your analysts sit, where you know,

188
00:11:43,865 --> 00:11:48,935
they, it, it is able to go suggest
potential things to do, or based on

189
00:11:48,935 --> 00:11:52,685
its investigation outcomes to all of
your security operations analysts.

190
00:11:53,015 --> 00:11:57,555
However, there's by no means a mandatory,
like it, it's no, by no means mandatory

191
00:11:57,555 --> 00:11:59,715
to accept the suggestions provided by ai.

192
00:11:59,955 --> 00:12:02,645
In fact it is they're
encouraged to audit these.

193
00:12:03,000 --> 00:12:06,000
And then decide which ones they
would actually want to go and apply.

194
00:12:06,750 --> 00:12:11,190
Now, once you've gone past this point
where you've built a fair amount of

195
00:12:11,190 --> 00:12:15,060
trust and you think AI is pretty much
reaching the right conclusions and

196
00:12:15,060 --> 00:12:18,090
your team knows how to leverage some of
those conclusions to accelerate their

197
00:12:18,090 --> 00:12:23,370
workflows, I would consider giving it a
little bit more bounded autonomy where

198
00:12:23,400 --> 00:12:25,230
it's able to, autonomously perform.

199
00:12:25,945 --> 00:12:30,815
Low risk actions that would not,
lock out people in your organization

200
00:12:30,815 --> 00:12:32,495
or cause severe detriment.

201
00:12:32,985 --> 00:12:35,685
And that sort of boundary
autonomy is provided to it.

202
00:12:36,135 --> 00:12:39,465
And then lastly, I do
not see a world where.

203
00:12:39,865 --> 00:12:45,430
You would fully let AI go and, execute
a large detrimental action with

204
00:12:45,430 --> 00:12:47,200
the technology as it exists today.

205
00:12:47,480 --> 00:12:51,445
So providing continuous feedback
and making sure that it is able to

206
00:12:51,445 --> 00:12:55,255
grow in trust and reach that point
is the fourth phase where I would

207
00:12:55,255 --> 00:12:59,785
love to keep pushing in terms of
expanding the scope of that bounded

208
00:12:59,845 --> 00:13:03,395
autonomy to be, more wider with time.

209
00:13:03,895 --> 00:13:07,495
So those are the four pillars,
organizational context for training

210
00:13:07,885 --> 00:13:10,465
policy con constraints to limit downside.

211
00:13:11,290 --> 00:13:16,660
Auditability to build trust and a roadmap
that begins pretty much with your SOC

212
00:13:16,660 --> 00:13:22,060
leads then graduates to your entire SOC
team and is bounded in terms of scope

213
00:13:22,060 --> 00:13:26,980
of autonomy that it has while that scope
continues to expand and grow over time,

214
00:13:27,040 --> 00:13:31,990
where you're able to just take things off
your plate and put it on the AI agents.

215
00:13:32,980 --> 00:13:35,350
Alright, so that's the framework.

216
00:13:35,500 --> 00:13:39,820
How do you know if you implemented that
framework correctly and have an AI agent

217
00:13:40,180 --> 00:13:43,270
that actually works well in production,
delivering the outcomes you want?

218
00:13:43,820 --> 00:13:46,910
So first off, you obviously wanna
check, Hey, were you able to

219
00:13:46,970 --> 00:13:50,360
eliminate a lot of the false positives
that your team is looking at?

220
00:13:50,630 --> 00:13:53,870
Reduce your MTTR mean
team, mean time to analyze.

221
00:13:54,050 --> 00:13:58,220
With all of the AI provided output
did that provide or manifest as

222
00:13:58,220 --> 00:13:59,780
an analyst productivity gain?

223
00:13:59,780 --> 00:14:02,600
Pick your favorite metric for how
you measure analyst productivity.

224
00:14:02,600 --> 00:14:05,820
It could be tickets that they triage
in the shift whatever they did.

225
00:14:05,940 --> 00:14:09,665
And then at a very base level, obviously
you wanna look at how accurate were

226
00:14:09,665 --> 00:14:14,265
the recommendations as well as the
reasoning of what the AI produced, right?

227
00:14:15,045 --> 00:14:19,725
And then from a actual blueprint
standpoint, I would recommend you take

228
00:14:19,725 --> 00:14:22,305
these six phases or six stages, right?

229
00:14:22,815 --> 00:14:25,695
First off, assess the current
state of your knowledge

230
00:14:25,695 --> 00:14:26,835
base and your documentation.

231
00:14:26,895 --> 00:14:30,585
Like I said, the first pillar
is organizational context.

232
00:14:30,795 --> 00:14:35,115
Hey, do you even have your existing
SOC workflows documented really well?

233
00:14:35,615 --> 00:14:38,705
The next phase I would recommend
is find a narrow scope.

234
00:14:38,735 --> 00:14:40,415
Like in fact start really small.

235
00:14:40,545 --> 00:14:43,515
Hey, here's an area where we
are feeling some amount of pain.

236
00:14:43,785 --> 00:14:45,165
It's relatively low risk.

237
00:14:45,375 --> 00:14:49,335
It would be great to go and let
the AI agent lose on this, right?

238
00:14:49,585 --> 00:14:52,215
Particularly in talking to some of
the security operations teams, I've

239
00:14:52,215 --> 00:14:56,085
heard areas like employee reported
phishing emails be considered

240
00:14:56,085 --> 00:14:59,815
relatively low risk for things that
people would consider automating.

241
00:15:00,135 --> 00:15:02,475
And, having some level
of automation around.

242
00:15:02,860 --> 00:15:07,000
The third thing would be to see, what
kind of guardrails do you need in place?

243
00:15:07,190 --> 00:15:09,620
What kind of capability
scoped tools do you need?

244
00:15:09,620 --> 00:15:14,090
What kind of policies do you need to
make sure that, hey, even if AI reaches

245
00:15:14,090 --> 00:15:17,320
the wrong conclusion, the the impact to
your organization is not catastrophic.

246
00:15:17,920 --> 00:15:24,140
With that, you'd go ahead and develop your
AI models or AI agents and then deploy

247
00:15:24,140 --> 00:15:29,870
them in some sort of shadow mode with your
lead SOC analysts where there is no impact

248
00:15:29,870 --> 00:15:34,230
on any of your UI or work surface where
your where your analysts operate today.

249
00:15:34,890 --> 00:15:38,010
Fifth phase would be to deploy
them in a manner where it's able

250
00:15:38,010 --> 00:15:41,910
to make suggestions to analysts
in the UI that they operate in.

251
00:15:42,520 --> 00:15:47,580
And over time expand its autonomy
so that it's able to achieve

252
00:15:47,580 --> 00:15:49,470
better automation outcomes for you.

253
00:15:50,160 --> 00:15:54,590
So these would be the six phases in
which how I would approach a build

254
00:15:54,640 --> 00:15:59,060
project or even if it's a by project
implementation of a project like that.

255
00:15:59,210 --> 00:16:00,980
Hopefully that was helpful.

256
00:16:01,030 --> 00:16:04,800
I do think the key takeaway from
this conversation, hopefully is

257
00:16:04,800 --> 00:16:11,210
that while AI agents have a lot of
promise there is a path where these

258
00:16:11,210 --> 00:16:17,060
AI agents can be deployed in a safe,
scalable manner, but human centricity

259
00:16:17,460 --> 00:16:19,530
is still at the forefront, right?

260
00:16:19,860 --> 00:16:22,980
So as long as you start
with a small scope.

261
00:16:23,475 --> 00:16:28,365
Which, which can be expanded over
time, you're gonna be able to succeed

262
00:16:28,365 --> 00:16:30,255
and get off the ground a lot faster.

263
00:16:30,705 --> 00:16:34,035
You need to be able to measure
whether you're actually hitting

264
00:16:34,035 --> 00:16:35,055
the outcomes you're after.

265
00:16:35,055 --> 00:16:37,455
What is the accuracy of the
recommendations that the AI is

266
00:16:37,455 --> 00:16:41,085
making how much efficiency is
it able to add to your workflow?

267
00:16:41,605 --> 00:16:45,435
You wanna make sure that humans
are in the loop in terms of

268
00:16:45,435 --> 00:16:49,355
making all of the key decisions
while you expand the scope of ai.

269
00:16:49,475 --> 00:16:53,075
And then as you build this trust,
you need to be on a journey where

270
00:16:53,075 --> 00:16:57,755
you're constantly expanding the
scope of what autonomy means for AI

271
00:16:57,755 --> 00:16:59,195
agents deployed in your environment.

272
00:16:59,975 --> 00:17:01,805
Hopefully that was helpful.

273
00:17:01,855 --> 00:17:05,725
So you as a DevOps person,
when you do get your security

274
00:17:05,725 --> 00:17:07,285
operations teams coming to you.

275
00:17:07,545 --> 00:17:10,545
Pretty much as I promised
this year, asking you to help

276
00:17:10,545 --> 00:17:12,135
build and deploy AI agents.

277
00:17:12,415 --> 00:17:15,505
You're able to help them succeed
and help your organization

278
00:17:15,505 --> 00:17:16,705
succeed at this endeavor.

279
00:17:17,215 --> 00:17:18,445
Thank you so much for the time.

