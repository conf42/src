1
00:00:00,500 --> 00:00:01,400
Hello everyone.

2
00:00:01,490 --> 00:00:06,260
I'm Thena and I'm so excited
to join you today at Con 42.

3
00:00:06,740 --> 00:00:11,660
A huge shout out to the amazing Con 42
team for putting this event together.

4
00:00:11,900 --> 00:00:15,260
And of course, thank you to all
of you for being here, even if

5
00:00:15,260 --> 00:00:16,549
we are connecting virtually.

6
00:00:16,670 --> 00:00:20,270
I'm grateful for the chance to share
some meaningful insights with you.

7
00:00:21,200 --> 00:00:24,890
I contribute to Gopher, which is
an open source Golan framework

8
00:00:24,890 --> 00:00:28,220
designed to supercharge
microservice development in Golan.

9
00:00:28,720 --> 00:00:32,500
At cofa, we are passionate about
helping developers build scalable,

10
00:00:32,500 --> 00:00:38,290
resilient, and secure microservices with
the power of co. In this talk, let's

11
00:00:38,290 --> 00:00:43,120
kick things off by exploring why GRPC
has become the cornerstone of modern

12
00:00:43,120 --> 00:00:47,200
microservices architecture as it is
a fantastic choice for building high

13
00:00:47,200 --> 00:00:50,410
performance resilient distributed systems.

14
00:00:50,910 --> 00:00:54,570
If we talk about the reasons
first comes the performance.

15
00:00:55,245 --> 00:01:01,605
GRPC leverages HTTP two multiplexing
that allows multiple concurrent requests

16
00:01:01,635 --> 00:01:07,115
over a single connection and its uses
protocol buffers for binary serialization.

17
00:01:07,625 --> 00:01:12,725
This combination makes it incredibly
efficient, often significantly faster

18
00:01:12,785 --> 00:01:14,645
than traditional rest over chase.

19
00:01:14,645 --> 00:01:17,874
On second comes the strong contracts.

20
00:01:18,489 --> 00:01:23,919
With protocol puffers, you get a clear
language agnostic interface definition.

21
00:01:24,399 --> 00:01:29,769
This enforces types and ensures
consistency across different services no

22
00:01:29,769 --> 00:01:31,779
matter what language they're written in.

23
00:01:32,350 --> 00:01:36,729
It virtually eliminates schema,
mismatches and versioning headaches.

24
00:01:37,229 --> 00:01:39,240
Third comes the streaming superpower.

25
00:01:40,079 --> 00:01:44,490
GRPC offers powerful streaming
capabilities that is bidirectional

26
00:01:44,520 --> 00:01:46,109
client side and server side.

27
00:01:46,724 --> 00:01:51,164
Which is very crucial for realtime
applications nowadays, like chat services,

28
00:01:51,195 --> 00:01:53,864
iot data streams of a nine shared tickers.

29
00:01:54,614 --> 00:01:57,195
But here's the catch, and it's a big one.

30
00:01:57,644 --> 00:02:00,494
GRPC is a black box by default.

31
00:02:01,154 --> 00:02:01,514
Why?

32
00:02:01,944 --> 00:02:06,264
Because without explicit effort,
you get no request visibility,

33
00:02:06,774 --> 00:02:08,185
no built-in health checks.

34
00:02:08,514 --> 00:02:12,565
You don't know what's happening
inside those efficient piney pipes.

35
00:02:13,299 --> 00:02:16,329
That brings me closer to
our analogy for today.

36
00:02:16,539 --> 00:02:19,479
That is running GRPC
without observability.

37
00:02:19,749 --> 00:02:22,479
It's like driving blindfolded at night.

38
00:02:23,079 --> 00:02:26,469
You are moving fast, but you have
no idea where you are, where you're

39
00:02:26,469 --> 00:02:28,839
going, and what obstacles are ahead.

40
00:02:29,829 --> 00:02:35,260
Let's tying us these critical blind
spots that so many teams encounter

41
00:02:35,320 --> 00:02:37,359
when deploying GRPC in production.

42
00:02:37,859 --> 00:02:39,659
First comes the tracing gaps.

43
00:02:39,959 --> 00:02:44,279
Without distributed tracing, we
simply cannot track a request as it

44
00:02:44,279 --> 00:02:46,409
hops across multiple microservices.

45
00:02:47,159 --> 00:02:51,689
For example, a single user action
might touch five 10, or even 20

46
00:02:51,689 --> 00:02:56,729
different GRPC services, and if one
of them fails or introduces latency,

47
00:02:56,999 --> 00:03:01,289
isolating the root cause becomes a
nightmare because you're left with

48
00:03:01,289 --> 00:03:03,299
a fragmented, incomplete picture.

49
00:03:04,244 --> 00:03:06,224
Second comes the logging chaos.

50
00:03:06,404 --> 00:03:10,664
Often GRPC logs are unstructured,
verbose, and lack context.

51
00:03:11,144 --> 00:03:14,354
And at error August, you are
shifting through mountains of text,

52
00:03:14,564 --> 00:03:18,224
desperately trying to correlate
log lines from different services.

53
00:03:18,724 --> 00:03:23,404
Debugging GRPC errors can literally
take us turning a simple bug

54
00:03:23,404 --> 00:03:26,134
fix into an all day or deed.

55
00:03:26,634 --> 00:03:28,799
And third comes the metrics blindness.

56
00:03:29,769 --> 00:03:34,839
Without cran new latency, error rate,
all request volume metrics, you are

57
00:03:34,839 --> 00:03:39,729
completely in the dark about the health
and the performance of your GRPC services.

58
00:03:40,269 --> 00:03:44,769
You won't know if users are experiencing
slow responses, if an upstream

59
00:03:44,769 --> 00:03:49,089
dependencies is failing or if a new
deployment is causing cascading issues.

60
00:03:50,079 --> 00:03:52,734
So now let me give you a
real world consequence.

61
00:03:53,244 --> 00:03:58,914
Suppose a 500 MS. Latency spike in a
critical payment service goes completely

62
00:03:58,914 --> 00:04:04,224
undetected for us leading to frustrated
users and abandoned transactions.

63
00:04:05,064 --> 00:04:10,494
Partial outages in a streaming pipeline
silently fail, which means that the data

64
00:04:10,494 --> 00:04:15,804
is not processed correctly and you only
find out days later when the downstream

65
00:04:15,804 --> 00:04:18,134
system starts screaming and health checks.

66
00:04:18,824 --> 00:04:23,444
Often that is an afterthought, bolded
on at the last parent, if at all.

67
00:04:24,344 --> 00:04:28,724
Now, this isn't theoretical, it's why
development team based weeks, sometimes

68
00:04:28,754 --> 00:04:33,554
month, manually instrumenting GRPC
services trying to stitch together

69
00:04:33,614 --> 00:04:35,534
a coherent observability story.

70
00:04:36,494 --> 00:04:41,534
Now this is where gofer comes in
'cause meet gofer as open source

71
00:04:41,534 --> 00:04:46,204
Colan framework where the core
philosophy is simple, yet powerful.

72
00:04:46,729 --> 00:04:50,929
That is built-in observability
should be default and not an option.

73
00:04:51,469 --> 00:04:54,079
It should be foundational,
not an afterthought.

74
00:04:54,739 --> 00:04:59,599
With cofa, this is exactly what you
get because every GRPC call, whether

75
00:04:59,599 --> 00:05:05,179
it's a simple ary request or a complex
bi-directional stream, gets auto-lock,

76
00:05:05,419 --> 00:05:08,869
auto traced and has auto exported metrics.

77
00:05:09,409 --> 00:05:12,799
You don't write a single line
of instrumented code for it.

78
00:05:13,429 --> 00:05:19,429
Health checks are enabled out of the box
for GRPC servers, whereby the standard

79
00:05:19,429 --> 00:05:24,079
GRPC health protocol giving you immediate
visibility into service readiness.

80
00:05:24,499 --> 00:05:27,919
And critically, this is all
achieved with zero manual

81
00:05:27,919 --> 00:05:29,929
instrumentation from your site.

82
00:05:30,634 --> 00:05:32,194
Now, how do we do this?

83
00:05:32,774 --> 00:05:37,994
A secret weapon is the gofer CLI tool
that we have built with scaffolds,

84
00:05:38,204 --> 00:05:42,164
your GRPC services with all the
necessary observability, hooks,

85
00:05:42,254 --> 00:05:43,814
patent in from the very picnic.

86
00:05:44,314 --> 00:05:48,454
So Al, it's time to
actually see this in action.

87
00:05:48,724 --> 00:05:54,364
So I'm going to take you from a bare
bone GRPC service to full observability

88
00:05:54,394 --> 00:05:58,654
in literally five minutes without
writing any observability specific code.

89
00:05:59,134 --> 00:06:03,304
So let's not waste any more time
and let's get coding with cofa.

90
00:06:03,664 --> 00:06:07,114
So we start with setting up
A-G-R-P-C server from scratch.

91
00:06:07,504 --> 00:06:11,464
Let's begin by creating a new
project directory, let's say server.

92
00:06:12,019 --> 00:06:15,979
Inside this directory, we'll
define a pro profile, which is

93
00:06:15,979 --> 00:06:18,139
the heart of any GRPC servers.

94
00:06:18,919 --> 00:06:21,979
In this example, we'll
name it greed dot proto.

95
00:06:22,669 --> 00:06:27,199
A pro profile is a schema definition
file used in GRPC where we define

96
00:06:27,199 --> 00:06:31,039
the structure of our data and service
methods using protocol buffers.

97
00:06:31,609 --> 00:06:35,179
Once the pro profile is ready, we
need to generate go code from it

98
00:06:35,179 --> 00:06:37,219
using protocol buffer compiler.

99
00:06:38,149 --> 00:06:43,069
This pro compiler converts the proto
definitions into go structure and service

100
00:06:43,069 --> 00:06:49,039
interfaces, making it easier to handle
binary serialization and RPC calls in Go.

101
00:06:49,539 --> 00:06:53,829
To generate the GO code for A-G-R-P-C
service, we simply run the following

102
00:06:53,829 --> 00:06:58,419
command by providing the output
path as well as the profile path.

103
00:06:59,289 --> 00:07:01,239
This command generates two files.

104
00:07:01,780 --> 00:07:05,679
Greet pb dot Go and greet GPC PB dot go.

105
00:07:06,159 --> 00:07:10,479
These files contain the auto-generated
code to perform GRPC calls.

106
00:07:10,899 --> 00:07:15,339
However, one thing to note is that
this code does not automatically

107
00:07:15,339 --> 00:07:17,559
support observability or health checks.

108
00:07:17,889 --> 00:07:22,359
Something that is very crucial for
building production grade microservices.

109
00:07:22,929 --> 00:07:28,119
This is where Gopher Steps in Gopher
has always treated observability as a

110
00:07:28,119 --> 00:07:30,789
inbuilt feature even in GRPC services.

111
00:07:31,569 --> 00:07:35,739
To get started with it, we would need
to install the goer CLI by running.

112
00:07:35,739 --> 00:07:40,959
Go install goer dev slash cli
slash goer at the rate latest.

113
00:07:41,589 --> 00:07:44,889
Next, we navigate to a server
directory and execute the command.

114
00:07:44,949 --> 00:07:45,189
Go.

115
00:07:45,189 --> 00:07:51,759
For wrap G PC server, we give the pro
profile path that is create dot proto.

116
00:07:52,259 --> 00:07:56,189
This command generates multiple
files including greed, server dot go.

117
00:07:56,759 --> 00:07:59,969
Please note that this generated
files are always present in the

118
00:07:59,969 --> 00:08:02,129
directory of the proto file itself.

119
00:08:02,759 --> 00:08:06,719
The only file that we are supposed
to modify is greet server dot go.

120
00:08:07,019 --> 00:08:10,709
This file has a template for
our GRPC server implementation

121
00:08:10,709 --> 00:08:14,999
already wired with contact support,
observability, hooks, and health checks.

122
00:08:15,499 --> 00:08:18,829
Here we'll keep things simple,
receiving a request with user

123
00:08:18,829 --> 00:08:23,299
info and binding it in our defined
user info message type in GRPC.

124
00:08:23,689 --> 00:08:27,889
And then returning a single message
as hello and the first name that

125
00:08:27,889 --> 00:08:29,449
we received in the user info.

126
00:08:29,950 --> 00:08:33,835
Now in a main dot go file, we import
the package where a greet server

127
00:08:34,210 --> 00:08:39,099
go is present and register our G
PC service with gofer as server.

128
00:08:39,099 --> 00:08:45,460
Do register, greet server with goer and
pass the arguments as app and server.

129
00:08:46,000 --> 00:08:46,720
New greet.

130
00:08:46,720 --> 00:08:47,619
Go for server.

131
00:08:48,119 --> 00:08:51,869
If we look at the structure of
A-G-R-P-C handlers, we can find

132
00:08:51,869 --> 00:08:54,059
go first context available there.

133
00:08:54,509 --> 00:08:58,649
This gives us seamless access to
connected databases, metrics, locks,

134
00:08:58,649 --> 00:09:03,269
and traces, so we can easily add
custom observability as well as perform

135
00:09:03,269 --> 00:09:05,729
DB calls inside A-G-R-P-C handlers.

136
00:09:06,689 --> 00:09:11,249
Moving on, let's run a server and then
go to Postman and call the method.

137
00:09:11,249 --> 00:09:11,999
Say hello.

138
00:09:12,794 --> 00:09:17,074
With all the user details coming
back to the terminal, we can see

139
00:09:17,074 --> 00:09:20,944
that Gopher automatically locks
the request, but that's not all.

140
00:09:21,064 --> 00:09:23,194
Gopher also provides built-in metrics.

141
00:09:23,374 --> 00:09:27,904
We can access them at local host
2 1 2 1 slash metrics endpoint.

142
00:09:28,294 --> 00:09:32,464
Additionally, all the inbuilt as well
as custom traces are visible at the

143
00:09:32,464 --> 00:09:37,204
trace exporter that we had set up in
this case being tracer dot COFA dev.

144
00:09:37,864 --> 00:09:42,634
For added resilience, gopher also adds
a health service for every GRPC server.

145
00:09:42,994 --> 00:09:46,504
This can be accessed through the
health field in the generated truck.

146
00:09:46,894 --> 00:09:51,004
Health checks are automatically
registered for both the overall server

147
00:09:51,064 --> 00:09:53,404
and the individual GRPC services.

148
00:09:53,584 --> 00:09:56,974
These health checks are extremely
useful in Kubernetes health.

149
00:09:57,004 --> 00:09:58,444
Checking of A-G-R-P-C server.

150
00:09:58,944 --> 00:10:02,905
So if we perform a curl request to
the overall GRPC server, we can see

151
00:10:02,905 --> 00:10:04,584
that we get the following response.

152
00:10:04,944 --> 00:10:09,385
But if we perform the curl request
for a specific service in that GRPC

153
00:10:09,385 --> 00:10:13,525
server, the health check response return
is for that particular service only.

154
00:10:13,974 --> 00:10:18,354
In cases, if we would've had another
registered service on the same host, we

155
00:10:18,354 --> 00:10:22,374
can call the health check on that service
by using the check method on a health

156
00:10:22,374 --> 00:10:24,864
server by specifying the service name.

157
00:10:25,435 --> 00:10:29,964
Additionally, if a service is temporarily
not fit to serve request, we can set

158
00:10:29,964 --> 00:10:35,275
its status using health dot set, serving
status, passing the context, the name of

159
00:10:35,275 --> 00:10:40,435
the service, as well as the status of the
service we can even shut down or resume

160
00:10:40,435 --> 00:10:45,084
services dynamically making it incredibly
useful for Kubernetes Health Check.

161
00:10:45,984 --> 00:10:47,214
And that's it.

162
00:10:47,785 --> 00:10:53,185
The key moment to reiterate over here
is that there was zero instrumentation

163
00:10:53,185 --> 00:10:55,435
code that was actually written by us.

164
00:10:55,854 --> 00:10:57,624
This is the power of cofa.

165
00:10:58,074 --> 00:11:00,114
But wait, what about streaming?

166
00:11:00,384 --> 00:11:06,114
We talked about GRPC unity servers in the
demo before, but what about streaming?

167
00:11:06,444 --> 00:11:10,704
GRPC is renowned for streaming
capabilities and observability here is

168
00:11:10,734 --> 00:11:13,344
even more critical, yet often overlooked.

169
00:11:13,844 --> 00:11:18,674
Gofer handles all streaming types with
the safe zero instrumentation approach.

170
00:11:19,034 --> 00:11:22,484
Whether it is server streaming,
client streaming, or bidirectional

171
00:11:22,484 --> 00:11:25,094
streaming, goer automatically tracks it.

172
00:11:25,604 --> 00:11:29,504
You get real time message levels
tracing, like for example, a

173
00:11:29,504 --> 00:11:31,124
chat service, for instance.

174
00:11:31,184 --> 00:11:34,844
You can see every single message
sent and received within the

175
00:11:34,844 --> 00:11:38,894
stream being instrumented as
its own span in your trace.

176
00:11:39,449 --> 00:11:42,989
You get crucial metrics for the
streams, like stream duration,

177
00:11:42,989 --> 00:11:47,549
message counts and errors within the
stream are automatically captured.

178
00:11:48,389 --> 00:11:52,319
For example, see this chat
service, every message here is

179
00:11:52,319 --> 00:11:54,419
traced and every error is locked.

180
00:11:54,779 --> 00:11:59,339
This level of detail for streaming
services is incredibly powerful for

181
00:11:59,339 --> 00:12:04,529
debugging complex real time interactions,
and it's not just the server side.

182
00:12:04,934 --> 00:12:10,724
Even GRPC clients built with COFA get auto
instrumentation for their upstream calls.

183
00:12:11,144 --> 00:12:16,634
This means your entire call graph across
multiple services is fully traceable.

184
00:12:17,134 --> 00:12:19,234
That concludes for today's session.

185
00:12:19,594 --> 00:12:22,204
Thank you so much to all
of you for your time.

186
00:12:22,954 --> 00:12:23,704
Going forward.

187
00:12:23,704 --> 00:12:27,604
You can check our examples at Diet
deeper into streaming, as well

188
00:12:27,604 --> 00:12:32,404
as Unity observability of GRPC
services you set up in Gopher.

189
00:12:32,404 --> 00:12:37,024
At gofer dev slash docks, we
first established that GRPC is a

190
00:12:37,024 --> 00:12:41,194
powerhouse for pot and microservices,
but without observability.

191
00:12:41,434 --> 00:12:43,834
It is a recipe for production fires.

192
00:12:44,614 --> 00:12:49,594
Go for an open source coal framework
delivers that essential observability

193
00:12:49,654 --> 00:12:54,454
automatically that the structured
logs, distributed traces, comprehensive

194
00:12:54,454 --> 00:12:56,314
metrics and robust health checks.

195
00:12:56,644 --> 00:13:01,834
And crucially, it works seamlessly for
both unity and complex streaming GRPC

196
00:13:01,834 --> 00:13:04,294
patterns right out of the box today.

197
00:13:05,164 --> 00:13:07,204
So how do we get started in it?

198
00:13:07,344 --> 00:13:08,724
It's incredibly easy.

199
00:13:09,054 --> 00:13:13,704
Simply we do go get gofer dev
to pull in the framework for

200
00:13:13,704 --> 00:13:15,714
our existing GRPC services.

201
00:13:15,924 --> 00:13:18,504
We use the Gofer CLI tool and run.

202
00:13:18,504 --> 00:13:24,504
Go for wrap, GRPC server or client
command to inject GOFERS Magic.

203
00:13:25,014 --> 00:13:30,234
And for furthermore information, you can
go ahead and explore our comprehensive

204
00:13:30,234 --> 00:13:37,164
documentation and examples at slash
GRPC or our examples@github.com slash.

205
00:13:37,664 --> 00:13:43,424
To unlock the full potential of GRPC
with observability, and this is it.

206
00:13:43,814 --> 00:13:48,254
But when it comes to GRPC, we are
just getting started in gofer and

207
00:13:48,254 --> 00:13:53,234
are continuously improving the way
Gofers observability making setup

208
00:13:53,264 --> 00:13:55,364
even more seamless and powerful.

209
00:13:55,784 --> 00:13:58,244
We invite you to join us on this journey.

210
00:13:58,514 --> 00:14:02,174
Let us know your feedback, or
better yet, consider contributing

211
00:14:02,234 --> 00:14:04,214
to the open source COFA project.

212
00:14:04,859 --> 00:14:09,059
Together we can make GRPC
observability effortless for everyone.

213
00:14:09,719 --> 00:14:14,099
It's time to remove the blindfold
and stop pasting previous development

214
00:14:14,099 --> 00:14:19,349
type manually instrumenting and truly
start understanding our GRPC services.

215
00:14:19,829 --> 00:14:20,909
That's it for today.

216
00:14:20,969 --> 00:14:22,469
Thank you so much to all of you.

