1
00:00:00,000 --> 00:00:04,750
Welcome to my session on
accelerating AI lifecycle.

2
00:00:05,150 --> 00:00:10,610
Today, I will talk about how we at Meta
work towards making AI lifecycle and

3
00:00:10,650 --> 00:00:16,119
essentially the research to prod cycle
faster for scientists and engineers

4
00:00:16,120 --> 00:00:18,399
to iterate and deliver their work.

5
00:00:18,899 --> 00:00:20,409
But first, who am I?

6
00:00:21,109 --> 00:00:24,369
I'm Pari Gupta, and I'm a Senior
Production Engineer at Meta.

7
00:00:25,149 --> 00:00:28,939
I work on Python Language Foundation
team, which manages the entire

8
00:00:28,939 --> 00:00:31,189
Python ecosystem within the company.

9
00:00:31,689 --> 00:00:34,919
For the longest time, I worked
in the interactive computing team

10
00:00:34,919 --> 00:00:39,199
focused on Bento, Meta's internal
Jupyter notebook distribution.

11
00:00:39,849 --> 00:00:42,499
Outside of Meta, I like
to paint and dance.

12
00:00:43,499 --> 00:00:47,809
Before diving into all the exciting
and creative improvements that

13
00:00:47,839 --> 00:00:53,429
we've made so far, Let's go back to
basics and start with AI lifecycle.

14
00:00:54,109 --> 00:00:55,859
AI is an iterative process.

15
00:00:56,409 --> 00:00:59,520
When I first started out,
I thought it was linear.

16
00:00:59,520 --> 00:01:04,979
You collect the data, you train a
fancy AI model, and do the testing.

17
00:01:05,259 --> 00:01:09,559
Then you deploy that model in production,
and then you're magically done to

18
00:01:09,559 --> 00:01:11,849
reap the fruits of the AI system.

19
00:01:12,729 --> 00:01:14,039
Easy and dreamy, right?

20
00:01:14,539 --> 00:01:15,949
What a noob I was.

21
00:01:16,329 --> 00:01:21,079
It was very soon that I realized that AR
lifecycle is more of a cycle with a lot

22
00:01:21,079 --> 00:01:24,389
of back and forth than a linear graph.

23
00:01:24,889 --> 00:01:28,869
Here's an oversimplified version
of what an AR lifecycle looks like.

24
00:01:29,429 --> 00:01:34,199
It contains different stages such as
product scoping, data engineering,

25
00:01:34,389 --> 00:01:38,369
actual model development, deployment
of that developed model, and so on.

26
00:01:38,739 --> 00:01:41,679
monitoring of how it,
on how it is performing.

27
00:01:41,759 --> 00:01:45,139
And finally, business
analysis and insights.

28
00:01:46,079 --> 00:01:49,699
You'll notice a lot of back and forth
arrows between different stages.

29
00:01:50,349 --> 00:01:53,959
These indicate that lifecycle
is not a steady loop, but

30
00:01:53,999 --> 00:01:58,259
involves a lot of revisions and
iterations for better results.

31
00:01:58,759 --> 00:02:04,049
Out of all of these stages, the data
engineering and model development

32
00:02:04,049 --> 00:02:05,910
components is called ML prototyping.

33
00:02:05,910 --> 00:02:11,274
Thank you This phase is very core to the
cycle and its effectiveness can really

34
00:02:11,274 --> 00:02:13,934
speed up the overall delivery of AI.

35
00:02:14,434 --> 00:02:18,384
The most commonly used ML
prototyping tools are Jupyter

36
00:02:18,384 --> 00:02:20,164
Notebooks and Kana environments.

37
00:02:20,604 --> 00:02:21,654
Let's dive into them.

38
00:02:22,154 --> 00:02:27,441
Jupyter Notebooks is an open source, web
based, interactive computing platform.

39
00:02:27,441 --> 00:02:33,069
They allow you to create and
share documents that contain live

40
00:02:33,069 --> 00:02:36,794
code, equations, visualizations,
And narrative text and images.

41
00:02:37,474 --> 00:02:40,894
It's widely used in
academia and industry alike.

42
00:02:41,604 --> 00:02:45,304
One of its main benefits can be seen
in data science and engineering.

43
00:02:46,044 --> 00:02:49,074
Notebooks can help you massage
the data into the format that's

44
00:02:49,074 --> 00:02:53,164
expected by your model, develop
data features, do scientific

45
00:02:53,174 --> 00:02:57,434
computations, and even visualize
your data to derive insights from it.

46
00:02:57,934 --> 00:03:00,074
Another is ML modeling and testing.

47
00:03:01,034 --> 00:03:06,024
To prove a hypothesis, a scientist would
develop an experiment, code a hacky

48
00:03:06,024 --> 00:03:10,474
prototype, and run it on their notebook
iteratively to train and test their model.

49
00:03:10,475 --> 00:03:14,304
Jupyter notebooks have
several other benefits.

50
00:03:14,634 --> 00:03:19,504
They're language agnostics, which
means that you can easily not just

51
00:03:19,504 --> 00:03:24,634
run Python, but any other language
such as R, Hack, JavaScript, etc.

52
00:03:25,134 --> 00:03:27,164
Lastly, it is back end agnostic.

53
00:03:27,274 --> 00:03:31,924
This means that you can run your code
locally or on a remote server or a

54
00:03:31,924 --> 00:03:35,044
GPU or a CPU cluster fairly easily.

55
00:03:35,044 --> 00:03:35,744
Conda.

56
00:03:35,744 --> 00:03:40,994
Conda is a popular package manager
used for managing software environments

57
00:03:41,024 --> 00:03:44,444
and dependencies in data science
and scientific computing world.

58
00:03:44,944 --> 00:03:48,674
It was developed as part of Anaconda
distribution and includes Python

59
00:03:48,834 --> 00:03:53,334
and other programming languages for
major scientific computing tooling.

60
00:03:53,834 --> 00:03:58,604
Conva allows its users to create isolated
environments where different versions

61
00:03:58,604 --> 00:04:03,074
of packages can be installed and used
without interfering with one another

62
00:04:03,184 --> 00:04:05,204
or with the base system environment.

63
00:04:05,979 --> 00:04:09,819
This makes it very easy to manage
dependencies and avoid conflicts

64
00:04:10,079 --> 00:04:12,289
between different packages and versions.

65
00:04:12,789 --> 00:04:18,619
Now I want to share some of the
ideas on how we at Meta have explored

66
00:04:18,659 --> 00:04:23,689
and executed to make ML prototyping
faster within the company so that our

67
00:04:23,729 --> 00:04:26,234
researchers can deploy their models.

68
00:04:26,634 --> 00:04:31,964
and make them work for the fast growing
world of artificial intelligence.

69
00:04:32,464 --> 00:04:33,084
Bento.

70
00:04:33,084 --> 00:04:36,674
Bento is the internal
distribution of Jupyter notebooks.

71
00:04:36,904 --> 00:04:40,774
It is particularly optimized for
Meta's needs to make it compatible

72
00:04:40,774 --> 00:04:41,744
with the internal codebase.

73
00:04:42,244 --> 00:04:46,874
It allows users to execute their code
on different servers within Meta.

74
00:04:47,374 --> 00:04:51,494
Now, let's see how we've improved
this interactive platform.

75
00:04:51,994 --> 00:04:56,364
Have you ever exe Have you ever
wanted to rerun and visualize

76
00:04:56,364 --> 00:04:57,714
data on a regular basis?

77
00:04:57,974 --> 00:05:02,414
Maybe make reports or see how the model
is performing to make informed decisions?

78
00:05:03,184 --> 00:05:08,299
Inspired by Pet Paper Mill's
approach, Pento introduced powerful

79
00:05:08,299 --> 00:05:13,889
tooling to schedule notebooks to
run code at any cadence on any

80
00:05:14,429 --> 00:05:16,069
server in a privacy aware manner.

81
00:05:17,009 --> 00:05:21,689
This helps us ensure that all the
permissions are intact for user's ease.

82
00:05:22,189 --> 00:05:24,029
Next is persistent sessions.

83
00:05:24,899 --> 00:05:29,239
Ever come across a situation where
you wanted to run code remotely

84
00:05:29,349 --> 00:05:34,139
and lost internet or wifi got
disconnected or something as simple

85
00:05:34,139 --> 00:05:38,509
as a laptop went to sleep, you lose
all the long running executions and

86
00:05:38,509 --> 00:05:40,669
you have to start all over again.

87
00:05:41,229 --> 00:05:42,289
It's sad, right?

88
00:05:42,789 --> 00:05:46,839
We implemented persistent sessions
and sophisticated notebook recovery

89
00:05:46,849 --> 00:05:52,479
mechanisms where users can access
their data computations on, any Jupyter

90
00:05:52,489 --> 00:05:54,629
session across different servers.

91
00:05:55,569 --> 00:06:00,599
You can simply execute a long running in
your notebook and come back to Jupyter

92
00:06:00,599 --> 00:06:05,499
session at a later point in time,
essentially resuming the session with all

93
00:06:05,499 --> 00:06:07,659
the data computations already available.

94
00:06:08,159 --> 00:06:12,519
Another idea that made a difference
in developer productivity and improved

95
00:06:12,549 --> 00:06:18,509
ML prototyping was cross platform
debugging between We are scored at

96
00:06:18,509 --> 00:06:25,799
Meta and Bento here, one click on
any Python file would push you into

97
00:06:25,849 --> 00:06:31,679
a debugging session or a note or a
notebook to iterate on your code with

98
00:06:31,799 --> 00:06:33,899
all the dependencies that you may need.

99
00:06:34,779 --> 00:06:39,479
Not only this, we also introduced
notebooks within VS code itself.

100
00:06:39,729 --> 00:06:44,889
which makes it very easy to import and
export in and out of IDE at any time.

101
00:06:45,389 --> 00:06:49,509
There are several other innovations
that we've made so far and influenced

102
00:06:49,539 --> 00:06:55,399
the ML development and really
made some quantifiable speed ups.

103
00:06:55,899 --> 00:07:02,649
Integrations to enable data privacy and
security reviews, support for serverless

104
00:07:02,649 --> 00:07:07,914
servers, enhanced SQL cell support,
ability to use multi language kernels,

105
00:07:08,604 --> 00:07:14,204
auto suggestions using internally
developed Gen AI agents, and so much more.

106
00:07:14,704 --> 00:07:18,334
Kotlin is relatively a newer
introduction to our developer tooling.

107
00:07:18,834 --> 00:07:22,034
We want to bring most of the
open source experience internally

108
00:07:22,074 --> 00:07:23,274
in a controlled manner.

109
00:07:24,004 --> 00:07:28,264
This enables faster onboarding of
researchers to the ML development cycle,

110
00:07:28,264 --> 00:07:33,264
which will Lowering the learning curve and
hence making the product delivery faster.

111
00:07:34,244 --> 00:07:38,794
Researchers can play with their GitHub
projects more freely while they are

112
00:07:38,794 --> 00:07:40,804
experimenting with their ML models.

113
00:07:41,304 --> 00:07:44,924
One thing to note here is
that Meta is a large monorepo.

114
00:07:45,424 --> 00:07:49,774
This means that there is one
version across all the projects

115
00:07:49,774 --> 00:07:51,204
that exist within Meta.

116
00:07:52,204 --> 00:07:55,364
This introduces a lot of
difficulties when it comes to quicker

117
00:07:55,364 --> 00:07:57,544
experimentation and version switching.

118
00:07:58,044 --> 00:08:01,894
Conda, on the other end of the
spectrum, provides full version

119
00:08:01,894 --> 00:08:06,044
control of dependencies that a user
may want to experiment with before

120
00:08:06,064 --> 00:08:07,714
marring to one specific version.

121
00:08:08,214 --> 00:08:13,564
Conda meta is packaged into a portable
virtual self contained environment.

122
00:08:13,989 --> 00:08:17,589
which can be accessed from
anywhere the user desires.

123
00:08:18,089 --> 00:08:24,109
And finally, as we plug Conda environments
for specific MLE use cases, we also

124
00:08:24,149 --> 00:08:29,029
enable better telemetry, accountability,
and tracking in terms of errors,

125
00:08:29,059 --> 00:08:34,169
package usages, and vulnerability checks
to ensure that researchers can work

126
00:08:34,179 --> 00:08:36,089
without worrying about any breaches.

127
00:08:36,589 --> 00:08:39,979
The last interesting idea that I
want to share today is the power

128
00:08:39,979 --> 00:08:41,819
of combining these two tools.

129
00:08:42,319 --> 00:08:46,989
Basically, I mean that Runconda
as a Jupyter kernel in the

130
00:08:46,999 --> 00:08:48,439
back end of a notebook.

131
00:08:48,939 --> 00:08:54,415
This in particular has accelerated
the development of AI models and

132
00:08:54,415 --> 00:08:58,809
experimentation bringing research
and production development

133
00:08:58,819 --> 00:09:00,619
much closer to one another.

134
00:09:01,119 --> 00:09:02,849
It has actually reduced the cost.

135
00:09:03,349 --> 00:09:03,939
Thank you.

136
00:09:03,939 --> 00:09:06,459
Here's the link to my LinkedIn profile.

137
00:09:06,659 --> 00:09:10,709
I'll make the slides public in case anyone
wants to go through these resources.

138
00:09:11,139 --> 00:09:11,569
Thank you.

