1
00:00:00,500 --> 00:00:03,929
I'm Jorge, Platform Engineer
and Consultant at Petalim.

2
00:00:04,580 --> 00:00:08,000
Over the next 30 minutes, we will
be talking about bringing your

3
00:00:08,000 --> 00:00:09,390
data up to the Data Lakehouse.

4
00:00:10,200 --> 00:00:14,379
If you don't have any of those two, no
worries, this talk is actually for you.

5
00:00:15,000 --> 00:00:17,029
We will be discussing the
benefits and challenges that

6
00:00:17,029 --> 00:00:19,470
may, lay ahead of your path.

7
00:00:20,430 --> 00:00:25,469
And if you are already on your way, or if
you are very deep into this topic, I think

8
00:00:25,469 --> 00:00:27,669
this talk will also spark your interest.

9
00:00:28,069 --> 00:00:30,429
I encourage you to stick around,
maybe also share some of your

10
00:00:30,429 --> 00:00:33,599
thoughts in the, in the comments or
throughout, throughout the conference.

11
00:00:34,069 --> 00:00:39,929
and yeah, hopefully we can learn from
your trials and tribulations as well.

12
00:00:40,429 --> 00:00:43,299
The thesis of this talk is twofold.

13
00:00:44,084 --> 00:00:47,984
it is a well known fact that companies
face recurring challenges whenever

14
00:00:47,984 --> 00:00:52,304
they engage with data projects, whether
it is because of inconsistencies in

15
00:00:52,304 --> 00:00:56,834
data quality or, delay on insights
delivery or misalignment between

16
00:00:56,834 --> 00:00:58,094
stakeholders and developers.

17
00:00:58,594 --> 00:01:03,224
My opinion is that applying
DataOps and embracing Data Lake as

18
00:01:03,224 --> 00:01:07,354
architecture is not just another
fix that some engineers, cooked up,

19
00:01:08,094 --> 00:01:09,854
but it is rather a paradigm shift.

20
00:01:10,624 --> 00:01:15,854
That not only systematically addresses
these issues that I mentioned, but also

21
00:01:15,944 --> 00:01:18,334
thrives inside delivery and trust in data.

22
00:01:18,834 --> 00:01:19,234
I don't know.

23
00:01:19,254 --> 00:01:20,894
Is that arguable, polemical?

24
00:01:21,394 --> 00:01:22,494
Let's find out together.

25
00:01:22,994 --> 00:01:27,234
My hope is that you walk away from the
talk, understanding what DataOps is, why

26
00:01:27,234 --> 00:01:31,484
it matters, and how it approaches the
problems that it says, that it addresses.

27
00:01:32,004 --> 00:01:37,304
But more important than the boring what,
why, and how, what I really want to share

28
00:01:37,304 --> 00:01:41,094
with you is some of my, appreciation
for the topic, some of my justified love

29
00:01:41,104 --> 00:01:47,524
for it, so that you hopefully walk with
more perspective on the huge opportunity

30
00:01:47,674 --> 00:01:51,759
that this, topic represents for the
organization, or your personal career.

31
00:01:52,259 --> 00:01:56,099
About my personal career, I've been
working on data related projects

32
00:01:56,099 --> 00:02:00,989
for the last six years, different
layers of the stack, everywhere from

33
00:02:01,029 --> 00:02:03,999
platform to real time data analytics.

34
00:02:04,629 --> 00:02:07,729
I have consulted literally
a dozen organizations.

35
00:02:08,249 --> 00:02:10,329
and supported them on their IT journey.

36
00:02:10,839 --> 00:02:16,049
And over the last, 24 months, been in
love with this topic, trying to find ways

37
00:02:16,089 --> 00:02:19,009
to introduce DataOps to my daily work.

38
00:02:19,509 --> 00:02:24,329
To keep the conversation very, pragmatic,
very down to earth and easy to follow.

39
00:02:24,909 --> 00:02:29,849
I would like to start by telling you
the story of Fixit Incorporated, a

40
00:02:29,849 --> 00:02:34,209
fictional company that has issues
with their own, own existing current

41
00:02:34,599 --> 00:02:40,019
data infrastructure and are trying
to introduce DataOps to address them.

42
00:02:40,959 --> 00:02:44,319
We will also see the grand
vision behind DataOps.

43
00:02:44,869 --> 00:02:47,299
but when we want to start
understanding better what it is.

44
00:02:47,699 --> 00:02:52,509
We'll need to touch on some key concepts,
although a little bit technical, we will

45
00:02:52,539 --> 00:02:54,659
still always keep, on, on the horizon.

46
00:02:54,659 --> 00:02:58,869
These three core fundamentals
for the business, namely cost

47
00:02:58,899 --> 00:03:03,419
performance and understandability so
that we force ourselves to measure

48
00:03:03,869 --> 00:03:08,729
how much better we are doing than
whatever is already out there.

49
00:03:09,229 --> 00:03:12,929
We will then move on to the more
spicy, I would like to say, part

50
00:03:12,929 --> 00:03:18,169
of the talk, where we will discuss
typical technical and organizational

51
00:03:18,219 --> 00:03:23,039
challenges that you may face when
you want to embrace, full, DataOps.

52
00:03:24,019 --> 00:03:26,659
We will then conclude with some
summary, and I'll give you some

53
00:03:26,659 --> 00:03:30,439
pointers so that you know where
to go next if you decide that you

54
00:03:30,439 --> 00:03:32,657
want to, join us in this adventure.

55
00:03:33,157 --> 00:03:35,287
Without further ado, let's start.

56
00:03:35,297 --> 00:03:39,117
So to understand why DataOps
is so important, right?

57
00:03:39,117 --> 00:03:42,377
It makes sense to first try to
understand what are the problems

58
00:03:42,377 --> 00:03:43,467
that it's trying to address.

59
00:03:44,207 --> 00:03:47,167
To illustrate this, I will
tell you the story of Fixit.

60
00:03:47,167 --> 00:03:51,907
This is a interplanetary megacorporation
with a headquarters on Earth.

61
00:03:52,757 --> 00:03:56,877
They recently hit their
500, 000 employees mark.

62
00:03:57,637 --> 00:04:00,867
They are deep into research
and development, aerospace

63
00:04:00,867 --> 00:04:02,897
engineering and space mining.

64
00:04:03,397 --> 00:04:09,067
They poised themselves as data driven and
cutting edge, and therefore they recently

65
00:04:09,067 --> 00:04:13,987
hired their first ever chief data officer,
who launched a digital transformation

66
00:04:13,987 --> 00:04:16,467
program to grow, to drive growth.

67
00:04:16,967 --> 00:04:21,347
Now, central to this initiative is
the data engineering department,

68
00:04:21,357 --> 00:04:26,977
which has around 400 employees and big
investments on cloud native compute,

69
00:04:27,277 --> 00:04:31,227
storage, and martian BI tooling.

70
00:04:31,727 --> 00:04:36,727
So the CDO, TechSavvy as he
is, starts by mapping out the

71
00:04:36,767 --> 00:04:38,327
technological landscape in the company.

72
00:04:38,937 --> 00:04:42,897
What he recently quickly finds
out is that All of the departments

73
00:04:42,897 --> 00:04:44,827
have very well defined boundaries.

74
00:04:45,357 --> 00:04:48,947
They all use their own
date, sorry, tables.

75
00:04:49,347 --> 00:04:54,037
and essentially, the organization has
grown organically in that sense without

76
00:04:54,327 --> 00:04:56,927
any, top level alignment of any sense.

77
00:04:57,257 --> 00:04:59,377
They are all subject matter
experts on their topic.

78
00:04:59,747 --> 00:05:04,307
yet they rely on data from
each other to actually perform.

79
00:05:04,807 --> 00:05:07,427
Now the CDO wants to know how this works.

80
00:05:07,797 --> 00:05:11,097
Organic growth, which, by the
way, is a situation in which many

81
00:05:11,097 --> 00:05:15,867
companies find themselves, also,
without really realizing it.

82
00:05:16,267 --> 00:05:21,467
we want to see how this organizational
growth has impacted the typical

83
00:05:21,467 --> 00:05:23,627
workflows inside the company.

84
00:05:24,047 --> 00:05:25,377
So let's take one example.

85
00:05:25,877 --> 00:05:27,717
Let's talk about customer support reviews.

86
00:05:28,047 --> 00:05:32,067
Let's assume that one of our customers,
the Fixit customers, he or she

87
00:05:32,067 --> 00:05:36,557
buys one of these deep space mining
techniques, technologies or equipments.

88
00:05:37,327 --> 00:05:38,487
There is a technical issue.

89
00:05:38,717 --> 00:05:40,597
They place a ticket on the support desk.

90
00:05:41,387 --> 00:05:44,587
One of our colleagues helps
them solve the issue, hopefully.

91
00:05:45,457 --> 00:05:48,734
And afterwards, we get a
review of how well we did.

92
00:05:48,734 --> 00:05:55,482
Now, Fixit wants to know, If any of this
review information can be used to drive

93
00:05:55,482 --> 00:05:57,972
improvement on the customer support.

94
00:05:58,472 --> 00:06:02,672
The way that they decide to do this
is using sentiment analysis on these

95
00:06:02,672 --> 00:06:04,052
reviews that we got from the customers.

96
00:06:04,967 --> 00:06:06,377
alongside with some other signals.

97
00:06:07,227 --> 00:06:11,697
the issue here is that data is at the
moment on a relational data database

98
00:06:11,697 --> 00:06:15,417
system managed by the customer success
department, but it needs to reach

99
00:06:15,417 --> 00:06:19,167
the data science team, which are
the experts on sentiment analysis.

100
00:06:19,667 --> 00:06:23,747
The data science team doesn't have
direct access to this data, nor

101
00:06:23,747 --> 00:06:28,187
the expertise or resources to move
the data to where they need it, and

102
00:06:28,187 --> 00:06:31,602
therefore they rely on the data union
department who builds a pipeline.

103
00:06:32,252 --> 00:06:37,022
that copies this data and puts
it into a data lake for the data

104
00:06:37,022 --> 00:06:40,552
sciences on a hourly basis, let's say.

105
00:06:41,052 --> 00:06:45,982
The data scientists, they get the data,
they start performing the modeling,

106
00:06:46,912 --> 00:06:51,852
generate predictions, and then they
push this to the fifth step, fifth

107
00:06:51,852 --> 00:06:56,492
and last step on the pipeline, a BI
report, which itself will likely have

108
00:06:56,512 --> 00:06:58,892
some kind of caching or some storage.

109
00:06:59,472 --> 00:07:03,182
and then on top of which the
visualizations are built.

110
00:07:04,042 --> 00:07:06,692
The manufacturing managers then
can look at this information

111
00:07:06,692 --> 00:07:08,102
and take decisions accordingly.

112
00:07:08,602 --> 00:07:14,662
Now on the paper, this approach might
seem very straightforward, right?

113
00:07:14,702 --> 00:07:17,872
Very intuitive, nothing complicated here.

114
00:07:18,592 --> 00:07:20,992
But if you look with a more
clinical eye, you will start

115
00:07:20,992 --> 00:07:23,612
seeing some issues cropping out.

116
00:07:24,112 --> 00:07:29,167
For example, There is at least
one hard data copy that we know

117
00:07:29,167 --> 00:07:31,217
of between steps two and four.

118
00:07:31,447 --> 00:07:35,717
This is where the data engineers, not
knowing exactly what they're moving

119
00:07:35,727 --> 00:07:39,327
around, they just know that they need
to get data from one place or the other.

120
00:07:39,827 --> 00:07:43,047
They don't complicate their lives,
they simply copy it as it is.

121
00:07:44,047 --> 00:07:45,557
and put it somewhere else.

122
00:07:46,247 --> 00:07:49,617
we also see there is no centralized
point of orchestration for

123
00:07:49,617 --> 00:07:51,167
the whole sequence, right?

124
00:07:51,167 --> 00:07:54,367
So if something fails on a step
two, there is very little chance

125
00:07:54,377 --> 00:07:59,087
that the people sitting on step four
will have visibility over that, let

126
00:07:59,087 --> 00:08:02,067
alone the ones who are consuming the
dashboard, at the end of the stream.

127
00:08:02,567 --> 00:08:08,017
Finally, since we have multiple different
tools interacting, some of them being

128
00:08:08,347 --> 00:08:09,837
RDS, some of them being data lakes.

129
00:08:10,342 --> 00:08:14,272
Some of them being via reports,
we can presume already that

130
00:08:14,272 --> 00:08:15,652
there will be different formats.

131
00:08:15,652 --> 00:08:19,452
So data will be transformed into different
formats or three different formats,

132
00:08:19,462 --> 00:08:21,952
at least throughout the whole process.

133
00:08:22,452 --> 00:08:24,392
And that is inefficient, of course.

134
00:08:24,892 --> 00:08:29,412
What happens then when all of these
issues are corping up in one workflow?

135
00:08:30,262 --> 00:08:33,992
And we look at a scale when we start
looking at all the possible use

136
00:08:33,992 --> 00:08:37,642
cases that the company has already
built or is planning to build.

137
00:08:38,142 --> 00:08:42,792
The CDO, knowing that no amount of
documentation will help him wrap his

138
00:08:42,792 --> 00:08:48,502
head around this situation, decides to
do a full, review, a full, let's say,

139
00:08:48,802 --> 00:08:54,092
interview process of the company to try
to extract knowledge from the, colleagues.

140
00:08:54,312 --> 00:08:55,502
So he sends out a survey.

141
00:08:56,067 --> 00:08:58,137
And some rigorous questions.

142
00:08:58,697 --> 00:09:00,617
Let's see what comes out of that.

143
00:09:01,117 --> 00:09:05,117
The CEO tells us that the IT architecture
grew organically over the years.

144
00:09:05,667 --> 00:09:07,777
there are site data silos everywhere.

145
00:09:08,657 --> 00:09:11,687
We kind of suspected this
already, so nothing new.

146
00:09:12,187 --> 00:09:14,757
The head of data engineering
tells us we maintain hundreds of

147
00:09:14,797 --> 00:09:19,627
pipelines, moving data between all
departments, cyclically dependent even.

148
00:09:20,167 --> 00:09:23,677
Okay, this is already new information
and not a good news, actually.

149
00:09:24,672 --> 00:09:28,822
That means that if in an event there
is an issue with some pipeline there

150
00:09:28,822 --> 00:09:32,452
is a chance that multiple pipelines
will go down, all at the same time.

151
00:09:32,952 --> 00:09:36,582
the new recruits from Data Science tell
us that there are many obscure, column

152
00:09:36,582 --> 00:09:39,452
names, transformations across data stores.

153
00:09:40,212 --> 00:09:43,572
Even after six months of
onboarding, there is still,

154
00:09:43,892 --> 00:09:45,992
let's say, ongoing training plan.

155
00:09:46,002 --> 00:09:46,062
Thank you.

156
00:09:46,797 --> 00:09:51,127
No doubt, this is frustrating for the
recruits, but also for the recruiters who

157
00:09:51,137 --> 00:09:56,777
have to invest six months of training,
even before they start seeing any tasks

158
00:09:56,817 --> 00:09:59,527
delivered by these new colleagues.

159
00:10:00,027 --> 00:10:03,157
Head of IT tells us that, indeed,
there are too many systems

160
00:10:03,417 --> 00:10:05,592
and many permission schemes.

161
00:10:06,542 --> 00:10:08,402
The monthly bid is also unpredictable.

162
00:10:08,962 --> 00:10:14,162
So not only we have, we only have we
little visibility on costs, but also

163
00:10:14,162 --> 00:10:19,222
security is, hard to enforce due to the
complexity of permission is skimming.

164
00:10:19,722 --> 00:10:25,042
Finally, the managers, the, the poor
dashboard users, all of this comment

165
00:10:25,042 --> 00:10:30,142
actually is far more, let's say generic
than what the others mentioned, I

166
00:10:30,142 --> 00:10:32,312
found it much more insidious, right?

167
00:10:32,312 --> 00:10:37,552
There is some very, dark evil
lurking behind of his statement.

168
00:10:37,802 --> 00:10:40,562
He says, can I trust
data in this dashboard?

169
00:10:40,842 --> 00:10:42,962
Last time I checked, it looked funny.

170
00:10:43,632 --> 00:10:45,782
It also takes forever to load.

171
00:10:46,082 --> 00:10:48,952
So this means there is not only
a user experience problem here,

172
00:10:49,592 --> 00:10:52,162
but also trust is broken, right?

173
00:10:52,997 --> 00:10:56,477
my question, or it begs the
question, when, their incident,

174
00:10:56,497 --> 00:10:58,387
this incident was, happening.

175
00:10:58,867 --> 00:11:00,467
And it took three hours to solve it.

176
00:11:00,917 --> 00:11:04,797
Would it also take only three hours to
recover the confidence of this user?

177
00:11:05,297 --> 00:11:06,187
Most likely not.

178
00:11:07,087 --> 00:11:08,217
Perhaps three months.

179
00:11:08,827 --> 00:11:09,847
Perhaps even three years.

180
00:11:10,347 --> 00:11:11,647
So all the information is in.

181
00:11:12,307 --> 00:11:16,977
our CDO has tabulated all the,
the key findings into one table.

182
00:11:17,552 --> 00:11:21,932
Across the board, we see data silos
everywhere, multiple copies and formats,

183
00:11:22,352 --> 00:11:25,022
no single access control scheme.

184
00:11:25,922 --> 00:11:31,532
Metadata management is parts, or is
a topic in part of the whole stack,

185
00:11:32,032 --> 00:11:36,302
but not in some others, which means
we have a huge blind spot towards the

186
00:11:36,302 --> 00:11:38,292
side of data ingestion and storage.

187
00:11:38,992 --> 00:11:42,562
And orchestration and
monitoring is not centralized.

188
00:11:43,542 --> 00:11:45,492
At least we should be happy
there is some in place.

189
00:11:45,492 --> 00:11:45,917
That's it.

190
00:11:46,377 --> 00:11:47,987
But this is far from ideal, right?

191
00:11:48,887 --> 00:11:54,497
And at this step, the CDO takes, some
time and ponders the question, What

192
00:11:54,497 --> 00:11:59,162
if we introduce DataOps, and mold
it into a data lake architecture?

193
00:11:59,182 --> 00:12:00,512
How would that look like?

194
00:12:00,542 --> 00:12:01,482
What would it bring us?

195
00:12:01,982 --> 00:12:08,752
Well, in an ideal scenario, we could
cross out all of this complexity, right?

196
00:12:09,132 --> 00:12:12,582
By first of all, introducing
a single lake, a single data

197
00:12:12,582 --> 00:12:14,632
lake where all data resides.

198
00:12:15,262 --> 00:12:18,122
This will in turn enable us
to guarantee that there will

199
00:12:18,162 --> 00:12:20,022
only be one hard copy of data.

200
00:12:20,702 --> 00:12:27,232
well, besides any backups for, disaster
recovery, but there is no necessity

201
00:12:27,232 --> 00:12:32,952
to do hard copies of data in between
stages of a workflow since we know that

202
00:12:32,962 --> 00:12:37,222
everything relies, everything resides
in one single place, in one single lake.

203
00:12:37,722 --> 00:12:41,442
This also enables us to start
thinking about centralized metadata

204
00:12:41,442 --> 00:12:46,022
management, a single, pane of
glass for managing metadata.

205
00:12:46,632 --> 00:12:50,052
We can also think about, single data
format, or enforcing a single data format

206
00:12:50,202 --> 00:12:54,542
at some point, hopefully an open source
format that will allow us to further

207
00:12:54,542 --> 00:12:56,702
develop our own tooling in the end.

208
00:12:57,082 --> 00:13:01,922
Without really thinking about,
vendor locking or, licensing.

209
00:13:02,422 --> 00:13:05,152
we have additionally now the
opportunity to talk about centralized

210
00:13:05,312 --> 00:13:10,412
orchestration and monitoring, and
last but not least, centralized access

211
00:13:10,412 --> 00:13:15,211
management, management, whether we
do this via traditional, access, role

212
00:13:15,211 --> 00:13:22,137
based access control or more fancy tag
based access control using metadata.

213
00:13:22,637 --> 00:13:24,847
But in practice, is that even possible?

214
00:13:24,897 --> 00:13:25,887
How would that look like?

215
00:13:26,417 --> 00:13:31,797
And more importantly, what would that,
entail, for costs, performance, and

216
00:13:32,177 --> 00:13:36,607
understandability, which are the core
fundamentals that we want to, let's say,

217
00:13:36,657 --> 00:13:38,977
upheld from the business side of things.

218
00:13:39,477 --> 00:13:40,757
Well, let's take a look at that.

219
00:13:41,257 --> 00:13:43,397
We will need to start with a data lake.

220
00:13:43,747 --> 00:13:46,657
In simple words, we might
have different business tools.

221
00:13:46,707 --> 00:13:50,837
Some of them might be third
party, but all of them will

222
00:13:51,077 --> 00:13:53,167
write data to a single location.

223
00:13:53,957 --> 00:13:59,487
And this is actually a natural
response to these tendencies that

224
00:13:59,487 --> 00:14:01,347
we see on system integration, right?

225
00:14:01,347 --> 00:14:06,437
Because of the, sheer amount of
unstructured and semi structured data

226
00:14:06,767 --> 00:14:10,207
when compared to the structured data
that we actually would like to have.

227
00:14:10,707 --> 00:14:14,757
but that's actually not a bad
thing, given that the modern data

228
00:14:14,757 --> 00:14:18,097
platform cost models favors, storage.

229
00:14:18,327 --> 00:14:26,827
so meaning storage is cheap when
compared to pre cloud, It also opens

230
00:14:26,827 --> 00:14:30,927
up the opportunity for us to enforce,
as I mentioned, this open source data

231
00:14:30,927 --> 00:14:36,267
format, ideally something like Iceberg
or Data Lake, which are not only widely

232
00:14:36,267 --> 00:14:41,937
supported, but extremely high performance
because of their binary nature, and

233
00:14:41,937 --> 00:14:47,037
that also support now ACID operations
or ACID operations, let's call them.

234
00:14:47,607 --> 00:14:50,907
This simply means that you can
read and write to these files.

235
00:14:51,502 --> 00:14:55,692
without having to worry that you
will compete with write and read

236
00:14:55,692 --> 00:14:57,382
operations from other users.

237
00:14:57,612 --> 00:15:02,472
Every operation is atomic and therefore
there is no chance to collide.

238
00:15:02,632 --> 00:15:07,272
This is a very well known principle of
relational database systems, actually.

239
00:15:07,772 --> 00:15:13,502
We can further improve our situation by
bringing into the scope staging schemas.

240
00:15:13,822 --> 00:15:18,082
And this is actually when the strength
of the system starts to ramp up.

241
00:15:18,582 --> 00:15:19,582
Think about it like this.

242
00:15:19,942 --> 00:15:25,462
We start with very raw data that
needs to be refined until we have

243
00:15:25,472 --> 00:15:28,562
a final product which is ready
for the business to consume.

244
00:15:29,182 --> 00:15:32,242
Now we know what these typical
refining operations are.

245
00:15:33,082 --> 00:15:38,782
All that we need to do is to put them in
very well defined stages and execute them.

246
00:15:39,572 --> 00:15:42,552
The good thing of having these
stages well defined is that if

247
00:15:42,552 --> 00:15:45,732
something goes wrong, we know exactly
where to look for the problem.

248
00:15:46,517 --> 00:15:50,267
We also have these smaller
steps, essentially a lot

249
00:15:50,267 --> 00:15:52,007
of steps, very small steps.

250
00:15:52,427 --> 00:15:58,007
And this incremental process allows us to
simplify troubleshooting, extremely, yeah.

251
00:15:58,607 --> 00:16:04,537
To give you an example, suppose that
we have a staging stage where You or

252
00:16:04,867 --> 00:16:09,387
stage with staging models when we where
we do only language translation for

253
00:16:09,387 --> 00:16:10,977
the column names and the table names.

254
00:16:11,567 --> 00:16:14,867
Additionally, we do some
enforcing of data type formats.

255
00:16:15,137 --> 00:16:20,497
So if a column is a string, but
contains a date, then we save that

256
00:16:20,537 --> 00:16:25,007
as an actual date object on database
and save some performance and costs.

257
00:16:25,507 --> 00:16:28,437
Then we introduce another stage,
intermediary stage, where we

258
00:16:28,447 --> 00:16:31,537
enforce, the actual open source
formatting that we wanted.

259
00:16:31,847 --> 00:16:35,187
Maybe we add some partition and
some compression for the files.

260
00:16:35,837 --> 00:16:40,627
since now our files are stored in a
much more performant format, we can

261
00:16:40,627 --> 00:16:46,247
also think about, introducing data
quality tests and freshness tests.

262
00:16:46,747 --> 00:16:49,797
Moving forward, we go
into the curated stage.

263
00:16:50,297 --> 00:16:54,337
or some, however you want to like
it, however you like to call it.

264
00:16:55,117 --> 00:16:59,537
here where we start building business
objects or entities that have some

265
00:16:59,847 --> 00:17:01,907
meaning for whatever comes afterwards.

266
00:17:02,567 --> 00:17:06,627
And all of this can be done
while orchestrating the

267
00:17:06,637 --> 00:17:07,927
interdependencies between the models.

268
00:17:08,277 --> 00:17:12,757
That means if some business
object requires two of the role

269
00:17:12,757 --> 00:17:16,667
models, we can ensure that the
role models are updated before.

270
00:17:17,167 --> 00:17:18,097
the business object.

271
00:17:18,597 --> 00:17:22,347
All in all, this is very easy to
build, troubleshoot, and version.

272
00:17:22,347 --> 00:17:24,487
And this is the key word here.

273
00:17:25,437 --> 00:17:31,477
we are not really dividing data
into development, testing, and

274
00:17:31,487 --> 00:17:33,387
production, like it's usually done.

275
00:17:33,947 --> 00:17:38,587
Rather for us, everything is
production, but the same way we want

276
00:17:38,597 --> 00:17:42,557
to test a new feature on application
and roll out a new version of it.

277
00:17:43,187 --> 00:17:45,277
We do the same with the tables.

278
00:17:45,607 --> 00:17:51,067
I want to create something new, a new
column for whatever business reason.

279
00:17:51,797 --> 00:17:56,827
Then I can just create a new model next
to my previous one with a new version.

280
00:17:57,037 --> 00:17:57,857
Let's call it B2.

281
00:17:58,557 --> 00:18:01,397
and then I am ready to test, right?

282
00:18:02,037 --> 00:18:05,567
I, of course, we need to communicate
to my, consumers down the line.

283
00:18:06,067 --> 00:18:09,277
At some point, maybe I want to
deprecate my previous version.

284
00:18:09,577 --> 00:18:12,487
So we need to let them know, Hey,
please migrate to the new one.

285
00:18:13,107 --> 00:18:15,897
at this point, if necessary, right?

286
00:18:16,397 --> 00:18:20,037
point that I forgot to mention
here, this orchestration between

287
00:18:20,047 --> 00:18:23,387
the model dependencies, there are
tools already available for this.

288
00:18:23,387 --> 00:18:28,427
There are a few, I want to recommend
dbt simply because it's open

289
00:18:28,427 --> 00:18:30,017
source and he has a huge community.

290
00:18:30,587 --> 00:18:32,317
We will talk a little bit
more later about them.

291
00:18:32,817 --> 00:18:37,807
The next step into our, stack,
is actually bringing the house

292
00:18:37,887 --> 00:18:40,112
to the lake, in the lake house.

293
00:18:40,612 --> 00:18:42,552
By this I mean introducing a warehouse.

294
00:18:43,052 --> 00:18:44,032
Why we want to do this?

295
00:18:44,072 --> 00:18:45,782
Well, think about it like this.

296
00:18:46,022 --> 00:18:50,832
If the staging schemas are the
perfect place for data exploration,

297
00:18:51,367 --> 00:18:54,407
the warehouse is the perfect
place for data exploitation.

298
00:18:54,897 --> 00:18:58,487
This is because warehouse, technologies
or solutions have much more

299
00:18:58,487 --> 00:19:03,667
computational power, and they can help
you meet this time critical use case.

300
00:19:03,667 --> 00:19:09,507
Like if you need to get a report on, on,
very frequent interval, and you need these

301
00:19:09,507 --> 00:19:11,937
queries to really complete on due time.

302
00:19:12,437 --> 00:19:16,307
Here we can also think about introducing
more fancy materialization strategies.

303
00:19:16,587 --> 00:19:19,247
So, like, incremental
models, that means upserts.

304
00:19:20,047 --> 00:19:25,707
So, not to completely overwrite
tables or, yeah, just introduce some

305
00:19:25,717 --> 00:19:30,117
of them, but rather, update, rows
that are already existing on it, so.

306
00:19:30,617 --> 00:19:34,997
as well we can, introduce more fancy
tooling that comes out of the box

307
00:19:34,997 --> 00:19:38,357
with this, technologies, for example,
machine learning based queries or some

308
00:19:38,407 --> 00:19:42,127
statistical transformations that might
be required for whatever business reason.

309
00:19:42,627 --> 00:19:44,027
And here I need to make a point.

310
00:19:44,527 --> 00:19:49,657
There are multiple data warehousing
solutions, but not all of them are able to

311
00:19:49,657 --> 00:19:53,047
read and write directly into the one lake.

312
00:19:53,347 --> 00:19:59,477
So, If we want to upheld the principles
that we are trying to build here, we

313
00:19:59,477 --> 00:20:05,387
need to stay, or we want to choose select
carefully this tool so that we don't

314
00:20:05,397 --> 00:20:09,417
build a new silo on top of the lake.

315
00:20:09,917 --> 00:20:14,147
The cherry on the top for
us will be white tables.

316
00:20:14,947 --> 00:20:15,847
And I'll tell you why.

317
00:20:16,347 --> 00:20:20,217
First of all, there are
benchmarks I showed that in.

318
00:20:20,707 --> 00:20:27,867
modern warehouses, querying white
tables is 50 to 25 percent much more

319
00:20:27,907 --> 00:20:31,317
performant than running joins on tables.

320
00:20:32,207 --> 00:20:36,997
They are also extremely intuitive for
the analysts and the business users,

321
00:20:37,387 --> 00:20:41,497
so that they can actually really
get very fast to the results, right?

322
00:20:41,497 --> 00:20:45,257
They don't need too much onboarding
to start, utilizing these tables.

323
00:20:45,757 --> 00:20:46,197
Hence.

324
00:20:46,752 --> 00:20:48,922
Reducing all this cost
of, high labor, right?

325
00:20:49,422 --> 00:20:54,402
the, the BI tools also sometimes tend to
ask you for white tables as their input.

326
00:20:55,002 --> 00:20:59,172
So, you can even think about them as
more or less as a requirement depending

327
00:20:59,172 --> 00:21:00,652
on which BI tooling you're using.

328
00:21:01,152 --> 00:21:04,142
the caveat here is that you might
of course end up with duplicated

329
00:21:04,192 --> 00:21:05,802
columns in some white tables.

330
00:21:06,022 --> 00:21:11,012
For example, you might have some
table with conversions on some tables.

331
00:21:11,602 --> 00:21:16,972
Time range and some other table where
there are also conversions, but in

332
00:21:16,982 --> 00:21:20,182
maybe less granular monthly basis.

333
00:21:20,682 --> 00:21:26,822
And of course, you need to make sure that,
the users understand that conversions

334
00:21:26,822 --> 00:21:30,872
can look different in different
tables and that there is metadata

335
00:21:30,912 --> 00:21:32,752
that they can choose or they can use.

336
00:21:33,262 --> 00:21:37,442
I'm talking about, catalog,
glossary, and lineage.

337
00:21:38,212 --> 00:21:42,242
This should all be information that
should be available to the users so

338
00:21:42,242 --> 00:21:47,312
that they can trace back all the changes
from the beginning of the data lake

339
00:21:48,292 --> 00:21:49,942
all the way to their consumption layer.

340
00:21:50,642 --> 00:21:54,262
that way you can preemptively
answer any questions that may crop

341
00:21:54,262 --> 00:21:59,002
up, any doubts that may crop up
in the daily work of the analysts.

342
00:21:59,502 --> 00:22:01,362
And that brings me to
the challenges, right?

343
00:22:01,362 --> 00:22:04,237
So, we discussed already
all the benefits of this.

344
00:22:04,517 --> 00:22:10,127
from terms of cost performance and
understandability, but it's still there

345
00:22:10,127 --> 00:22:13,507
is a chance you will not be able to
make this pitch in your organization

346
00:22:13,537 --> 00:22:15,837
and get a thumbs up right away.

347
00:22:16,287 --> 00:22:20,897
Some typical technical challenges are,
for example, existing learning, long

348
00:22:20,907 --> 00:22:22,767
running licenses with enterprise tooling.

349
00:22:23,427 --> 00:22:26,397
maybe you get past that
and start implementing.

350
00:22:26,757 --> 00:22:30,687
DataOps, but you introduce
way too much trivial testing.

351
00:22:31,097 --> 00:22:34,327
This is to be fair, also something
that you will see in other

352
00:22:34,327 --> 00:22:36,047
areas of software development.

353
00:22:36,647 --> 00:22:41,637
but it gives you this false sense
of security that, might backfire.

354
00:22:42,137 --> 00:22:47,557
we also highlighted that there is
no dev test and production stages.

355
00:22:47,557 --> 00:22:51,927
We just have everything as production
with versions on the products or

356
00:22:52,777 --> 00:22:54,307
on the tables or on the models.

357
00:22:54,807 --> 00:22:57,007
Here what is critical is to
have good communication, right?

358
00:22:57,007 --> 00:23:02,467
So if you, create a new version of your
model and don't properly communicate, you

359
00:23:02,467 --> 00:23:08,497
might end up pulling the rock underneath
somebody, and this is hard to estimate

360
00:23:09,087 --> 00:23:11,487
the blast radius that it might have.

361
00:23:11,987 --> 00:23:16,282
Finally, one that I see often is
Although I think self explanatory at this

362
00:23:16,282 --> 00:23:20,202
point, you should always try to build
a mock up, right, of your use cases.

363
00:23:21,152 --> 00:23:23,642
Even if it's an Excel
sheet with dummy data.

364
00:23:24,522 --> 00:23:30,162
But as a pre step to align yourself with
the stakeholders, which is one of the

365
00:23:30,532 --> 00:23:32,812
core principles of data house practices.

366
00:23:33,472 --> 00:23:38,312
but sometimes it's rush, it's, yeah,
it's brush off all under the table just

367
00:23:38,342 --> 00:23:40,292
to, you know, get faster to the results.

368
00:23:40,792 --> 00:23:44,922
Now, all of this, I think, can
be addressed with a call head.

369
00:23:45,882 --> 00:23:50,842
However, the organizational side of things
may look different because, yeah, people

370
00:23:51,062 --> 00:23:52,972
not always, operate with a call head.

371
00:23:53,472 --> 00:23:56,762
Some comments that you might come
across are, the conversion reports

372
00:23:56,762 --> 00:23:59,472
you build are not matching with one
of the ones from the other team.

373
00:23:59,472 --> 00:24:01,552
So, this is this discrepancy
that I mentioned.

374
00:24:02,322 --> 00:24:05,672
You can end up with two different
white tables, with the same metric.

375
00:24:06,042 --> 00:24:08,092
but with different versions
of this information.

376
00:24:08,592 --> 00:24:12,502
Somebody can also mention, cool,
but I'm not a technical person.

377
00:24:12,512 --> 00:24:14,462
Why don't you talk to the engineers?

378
00:24:14,962 --> 00:24:17,082
huge misconception here
of what DataOps is.

379
00:24:17,852 --> 00:24:21,072
As I mentioned, this is a principle,
or one of these principles

380
00:24:21,072 --> 00:24:24,902
is that you want to align the
stakeholders with the engineers.

381
00:24:25,252 --> 00:24:27,232
And therefore, this is a joint effort.

382
00:24:27,732 --> 00:24:30,612
From the side of the engineers,
you might also hear things

383
00:24:30,612 --> 00:24:32,202
like, Hey, we have to work with.

384
00:24:32,567 --> 00:24:33,957
Tool X for so many years.

385
00:24:34,247 --> 00:24:37,037
Now you want me to change to tool Y.

386
00:24:37,537 --> 00:24:41,107
But at the end of the day,
the, the only thing that is

387
00:24:41,107 --> 00:24:43,167
constant is in life is change.

388
00:24:43,167 --> 00:24:48,987
I think everything, everyone can agree
with that and DataOps is built for that.

389
00:24:49,027 --> 00:24:52,717
It's embracing change,
and staying nimble, right?

390
00:24:52,717 --> 00:24:58,117
To stay, stay on your toes, always ready
for whatever comes out of the curve.

391
00:24:58,617 --> 00:25:02,687
Because piece of requirements will change,
KPIs will change, data will change,

392
00:25:02,967 --> 00:25:07,847
sources will change, tools, people, and
best practices themselves might change.

393
00:25:08,417 --> 00:25:13,097
but the, the whole idea is that
you, come up with a translation for

394
00:25:13,097 --> 00:25:15,077
DataOps that fits your organization.

395
00:25:15,877 --> 00:25:20,367
You put it to the test, you
review, and you make it better and

396
00:25:20,377 --> 00:25:21,737
better and better, iterate over.

397
00:25:22,172 --> 00:25:24,402
Small steps, but very quick steps.

398
00:25:24,902 --> 00:25:29,932
Putting it all together, Data Lakehouse
and Open Source Storage Formats are

399
00:25:29,942 --> 00:25:33,452
the natural response to the trends
that we see in system integration.

400
00:25:34,172 --> 00:25:38,832
And they are, beneficial for us
from a cost perspective because

401
00:25:39,242 --> 00:25:43,732
they take advantage of the building
models of modern data platforms.

402
00:25:44,232 --> 00:25:48,732
they essentially trade more of the
computational costs for the storage

403
00:25:48,752 --> 00:25:51,352
costs, which in turns are lower.

404
00:25:51,852 --> 00:25:56,382
we talked about why tables are,
they're intuitive and performance.

405
00:25:56,882 --> 00:26:00,472
even their, their, pervasiveness due
to requirements from the BI tooling.

406
00:26:01,092 --> 00:26:04,232
we also mentioned they have a downside
because they are denormalized,

407
00:26:04,692 --> 00:26:07,067
but the benefits outperform this.

408
00:26:07,587 --> 00:26:12,587
Issues, let's say metadata, I
cannot stress it enough is the key

409
00:26:12,637 --> 00:26:17,387
to removing silos, is really the
key to boosting understandability.

410
00:26:18,337 --> 00:26:22,497
And given the fact that the cost
of a skilled labor are still

411
00:26:22,497 --> 00:26:25,867
high and will remain high, the
understandability is a huge factor.

412
00:26:26,327 --> 00:26:27,957
on your cost control.

413
00:26:27,957 --> 00:26:32,017
So the more intuitive, the more easy
it is for people to find information

414
00:26:32,457 --> 00:26:37,157
on a self service manner, the
better you are off for the future.

415
00:26:37,957 --> 00:26:42,537
Finally, DataOps, as I
mentioned, is a paradigm shift.

416
00:26:42,537 --> 00:26:44,577
It's not just something that
the engineers cooked up.

417
00:26:45,077 --> 00:26:50,077
is the way that you treat data, metadata,
and how you adapt to change itself.

418
00:26:50,577 --> 00:26:53,927
If you are interested to learn a
little bit more about this topic,

419
00:26:54,717 --> 00:26:56,007
I can recommend a few things.

420
00:26:56,357 --> 00:27:02,697
First of all, take a look at the DataOps
Manifesto, 18 principles, very clear to

421
00:27:02,697 --> 00:27:05,057
the, direct to the business question.

422
00:27:05,557 --> 00:27:10,227
You can also read the best practices
from the dev, devs from dbt.

423
00:27:10,977 --> 00:27:15,747
As I mentioned, DVT is an open source
tool is very opinionated, but the

424
00:27:15,747 --> 00:27:19,927
documentation is very well written and
the engineers will tell you every time

425
00:27:19,927 --> 00:27:21,317
they take a decision why they do it.

426
00:27:21,317 --> 00:27:25,447
So you can internalize, right, the,
the, the fundamental behind it,

427
00:27:25,897 --> 00:27:29,337
instead of just falling blindly
into whatever the tool offers you.

428
00:27:29,837 --> 00:27:34,127
I can also recommend you check
out the book from Dave Fowler.

429
00:27:34,817 --> 00:27:38,547
And this is called Cloud Data Management,
Four Stages for Informed Companies.

430
00:27:38,747 --> 00:27:40,007
It's an open, free book.

431
00:27:40,047 --> 00:27:40,937
You can find it online.

432
00:27:41,507 --> 00:27:42,407
It's a, it's a good read.

433
00:27:43,107 --> 00:27:46,107
And, yeah, also maybe join
the community on Slack.

434
00:27:46,217 --> 00:27:49,097
dbt community is, is very active.

435
00:27:49,227 --> 00:27:50,277
very humorous at times.

436
00:27:50,697 --> 00:27:53,747
I think there is upwards of 70,
000 people right now in there.

437
00:27:54,517 --> 00:27:59,097
And, yes, you will always find somebody
with answers to your questions as well.

438
00:27:59,597 --> 00:28:00,727
That's all I have for you.

439
00:28:01,727 --> 00:28:03,297
I appreciate your time, your time.

440
00:28:03,327 --> 00:28:06,507
And yeah, I hope to see
you on the next one.

441
00:28:07,267 --> 00:28:07,727
Cha cha.

