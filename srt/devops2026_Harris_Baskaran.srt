1
00:00:00,500 --> 00:00:01,250
Speaker 13: Hello everyone.

2
00:00:01,940 --> 00:00:03,559
My name is Harris Peter Pacoin.

3
00:00:03,710 --> 00:00:04,640
I'm from Google.

4
00:00:05,359 --> 00:00:10,220
It's a pleasure to be with you
here today at Con 24 DevOps 2026.

5
00:00:10,720 --> 00:00:14,500
I'm a staff SRE at Google, and I
have been with Google for 11 years.

6
00:00:15,000 --> 00:00:18,780
Today I am excited to share about
a comprehensive case study titled

7
00:00:18,780 --> 00:00:23,850
from Lift and Shift to Kubernetes,
A two phase DevOps Modernization.

8
00:00:24,350 --> 00:00:28,910
This talk is not just about moving
workloads to the cloud, it's about

9
00:00:28,910 --> 00:00:32,450
the strategic evolution of a mission
critical enterprise application.

10
00:00:33,349 --> 00:00:34,819
And we will explore that.

11
00:00:34,970 --> 00:00:39,800
We'll explore the technical hurdles,
the strategic decisions, the operational

12
00:00:39,800 --> 00:00:42,649
outcomes of such a large transformation.

13
00:00:43,320 --> 00:00:47,280
This transformation is for a legacy
ticket management system and will

14
00:00:47,280 --> 00:00:49,055
transform this through this journey.

15
00:00:49,775 --> 00:00:52,235
Into a high performance
cloud native service.

16
00:00:52,735 --> 00:00:57,175
Every cloud migration is a journey with
many learnings, and whether you are

17
00:00:57,175 --> 00:01:01,105
starting your journey now or you're in
knee deep in the trenches of Kubernetes,

18
00:01:01,855 --> 00:01:07,585
I hope this blueprint serves as actionable
insights for your modernization efforts.

19
00:01:08,085 --> 00:01:11,955
To understand a journey, we should
start at understanding the challenge.

20
00:01:12,920 --> 00:01:17,990
The existing service was an on-prem
service running on VMware and had

21
00:01:17,990 --> 00:01:20,090
been running for over two decades.

22
00:01:20,840 --> 00:01:25,039
It was a large, monolithic application,
tightly coupled with many integrations.

23
00:01:25,850 --> 00:01:30,230
This was one of our core business
function services that is used day in

24
00:01:30,230 --> 00:01:33,410
and day out for more than a hundred
thousand tickets every single day.

25
00:01:33,910 --> 00:01:37,029
When this tightly coupled
monolith application service

26
00:01:37,029 --> 00:01:39,339
stops, the business stops.

27
00:01:40,179 --> 00:01:46,240
The services was upgraded more than a
decade ago from on-prem physical machines

28
00:01:46,270 --> 00:01:52,759
to VMware and for more than a decade
after that, it was not upgraded at all.

29
00:01:53,259 --> 00:01:58,629
When this project started, it was running
on aging VMware infrastructure and had a

30
00:01:58,629 --> 00:02:00,730
very large Oracle database at the backend.

31
00:02:01,450 --> 00:02:04,809
Both the cache, the application
stacks were globally deployed

32
00:02:05,170 --> 00:02:06,760
to reduce latency for users.

33
00:02:07,260 --> 00:02:10,680
Over the years, the architecture
has devolved into complex web

34
00:02:10,680 --> 00:02:15,270
of interdependencies, along with
growing number of customers and other

35
00:02:15,270 --> 00:02:17,400
critical systems that depend on this.

36
00:02:17,900 --> 00:02:24,840
The technical debt made even minor, risky
minor updates, risky and time consuming.

37
00:02:25,429 --> 00:02:30,649
Every single operational
change was a bottleneck as the

38
00:02:30,649 --> 00:02:32,419
transaction volume started growing.

39
00:02:32,919 --> 00:02:38,079
The complex dependencies made everything
such as even a routine maintenance, as

40
00:02:38,079 --> 00:02:42,699
such as patching, software upgrades, and
any releases, a challenging thing to do.

41
00:02:43,479 --> 00:02:48,849
So our goal was very clear, transform
this monolith into a containerized

42
00:02:48,909 --> 00:02:53,819
platform, minimizing disruptions, but
we needed a method which was methodical,

43
00:02:53,969 --> 00:02:57,179
measured, and above all risk covers.

44
00:02:57,679 --> 00:03:00,649
So we came up with this
two phase strategy.

45
00:03:01,379 --> 00:03:04,679
To manage the risk, we decided
against a Big bang approach.

46
00:03:05,439 --> 00:03:09,889
The two phases were phase one, which
was a lift and shift the lift and

47
00:03:09,889 --> 00:03:11,659
shift to Google compute engine.

48
00:03:12,619 --> 00:03:14,089
The object was very simple.

49
00:03:14,149 --> 00:03:18,649
So move the existing architecture
from Google, from on-prem into

50
00:03:18,649 --> 00:03:20,329
Google Compute Engine as is.

51
00:03:21,154 --> 00:03:22,474
Why do we do this?

52
00:03:22,534 --> 00:03:27,874
Because it allows us to exit the
VMware infrastructure more quickly,

53
00:03:28,374 --> 00:03:31,764
and more importantly, it allows
us to establish a performance

54
00:03:31,764 --> 00:03:33,714
baseline for the cloud environment.

55
00:03:34,584 --> 00:03:38,784
We wanted to see how the app would
behave in a cloud infrastructure.

56
00:03:38,964 --> 00:03:43,904
Before we added the complexities of
containers, we preserved the existing

57
00:03:43,934 --> 00:03:45,644
architecture as much as possible.

58
00:03:46,604 --> 00:03:48,404
The goal here wasn't innovation yet.

59
00:03:49,319 --> 00:03:50,759
It was more about stability.

60
00:03:51,419 --> 00:03:56,069
So by moving the VMs into cloud,
we could let go of the dependencies

61
00:03:56,069 --> 00:04:00,989
of VMware and establish a baseline
performance for adding the

62
00:04:00,989 --> 00:04:02,669
complexities of containers later on.

63
00:04:03,169 --> 00:04:07,779
Phase two was more about the modernization
the cloud native transformation.

64
00:04:08,139 --> 00:04:12,669
Once we were stable on gc, the
hard work of containerizing the

65
00:04:12,669 --> 00:04:15,130
application moving into the GKE.

66
00:04:16,014 --> 00:04:20,034
And then solving the data layer
problem, we could approach

67
00:04:20,034 --> 00:04:22,584
this more as a phased approach.

68
00:04:23,064 --> 00:04:27,234
It'll allow us to isolate the
variables that change and ensure

69
00:04:27,234 --> 00:04:31,404
that if anything goes wrong, we could
easily pinpoint which layer of the

70
00:04:31,404 --> 00:04:34,434
stack was responsible for the issues.

71
00:04:34,934 --> 00:04:37,304
So let me delve a little
deeper into phase one.

72
00:04:37,304 --> 00:04:39,524
Phase one was all about precision.

73
00:04:39,929 --> 00:04:46,619
So we broke it down into four critical
steps, dependency, mapping, parallel

74
00:04:46,619 --> 00:04:52,249
deployment, continuous application,
and almost zero cutover time In

75
00:04:52,249 --> 00:04:55,999
dependency mapping, we didn't just look
at the code, we looked at the network.

76
00:04:55,999 --> 00:04:59,329
We used the discovery tools
to map every integration point

77
00:04:59,809 --> 00:05:01,489
and communication patterns.

78
00:05:02,059 --> 00:05:06,079
You'd be surprised how many zombie
services or undocumented APA calls

79
00:05:06,079 --> 00:05:08,959
exist in a. 10-year-old monolith.

80
00:05:09,619 --> 00:05:13,849
We then use this data to chart out all
the dependencies and understand systems.

81
00:05:13,849 --> 00:05:17,179
It depends on and systems
that dependent on it.

82
00:05:17,679 --> 00:05:23,469
Secondly, we built the cloud
environment as a parallel environment

83
00:05:23,469 --> 00:05:24,879
to the on-prem environment.

84
00:05:25,419 --> 00:05:27,849
This wasn't just a
sandbox, it was a mirror.

85
00:05:28,719 --> 00:05:32,229
This parallel deployment made
sure that the live service was

86
00:05:32,229 --> 00:05:34,419
never disrupted or ever impacted.

87
00:05:34,919 --> 00:05:37,139
Thirdly, continuous replication.

88
00:05:37,139 --> 00:05:40,229
We used Oracle's native replication
technology to keep both the

89
00:05:40,229 --> 00:05:43,709
cloud databases in sync with
the on-prem VMware environment.

90
00:05:44,189 --> 00:05:48,849
This ensure that when we were ready
to move the data when we were ready to

91
00:05:48,849 --> 00:05:50,590
cut over, the data was already there.

92
00:05:51,269 --> 00:05:55,139
This was very necessary because
the database, as I mentioned,

93
00:05:55,139 --> 00:05:56,369
was a very large database.

94
00:05:56,869 --> 00:05:58,779
And finally about the cutover.

95
00:05:59,109 --> 00:06:04,299
We use DNS and load balancer redirects
to make sure that the traffic was

96
00:06:04,299 --> 00:06:05,799
moved over in a controlled manner.

97
00:06:06,609 --> 00:06:10,569
Because the environments were
identical in architecture and the

98
00:06:10,569 --> 00:06:12,639
stack, the data was kept in sync.

99
00:06:13,259 --> 00:06:17,939
When we were ready to cut over,
we also had the option to fall

100
00:06:17,939 --> 00:06:19,574
back very easily if we needed to.

101
00:06:20,499 --> 00:06:23,169
And keeping the data in sync all the time.

102
00:06:23,169 --> 00:06:26,169
Made sure that the cutover
is only a matter of minutes.

103
00:06:26,669 --> 00:06:29,519
How we measured the success of phase one?

104
00:06:30,359 --> 00:06:35,399
So as I mentioned, phase one was a
very critical step in our journey.

105
00:06:35,639 --> 00:06:40,079
Once we were on GCE, we didn't just keep
the lights on, we measured everything.

106
00:06:40,649 --> 00:06:45,029
We gathered comprehensive baseline
metrics on application latency.

107
00:06:45,344 --> 00:06:49,544
System throughput, resource
utilization, and we know exactly

108
00:06:49,544 --> 00:06:51,644
how the monolith performed on cloud.

109
00:06:52,144 --> 00:06:55,774
These numbers gave us a
success criteria for phase two.

110
00:06:56,274 --> 00:07:02,694
So we essentially set a bar for our
future cunet system that had to, and

111
00:07:02,694 --> 00:07:06,994
that bar we wanted to either meet or
exceed through the transformation.

112
00:07:07,494 --> 00:07:11,634
So our immediate win with phase
one was more about rapid resource

113
00:07:11,634 --> 00:07:16,464
provisioning on cloud scaling,
improved performance on G cvms.

114
00:07:17,034 --> 00:07:20,004
And given the dependency mapping
work that we had done in phase

115
00:07:20,004 --> 00:07:24,484
one, we had a lot more efficient
maintenance operations after phase one.

116
00:07:24,984 --> 00:07:28,709
Phase two was the
Kubernetes transformation.

117
00:07:29,209 --> 00:07:32,449
With our baseline established
and stable, we entered phase two

118
00:07:33,019 --> 00:07:36,829
where we got the opportunity to
do the cloud native redesign.

119
00:07:37,399 --> 00:07:38,929
This was the modernization phase.

120
00:07:38,929 --> 00:07:39,559
As I mentioned.

121
00:07:39,559 --> 00:07:42,829
We wanted to make sure that the
service could serve the business

122
00:07:42,829 --> 00:07:45,049
efficiently for many years to come.

123
00:07:45,739 --> 00:07:50,269
And also, we wanted to take the
opportunity to use all the cloud best

124
00:07:50,269 --> 00:07:52,999
practices and cloud SRE tools available.

125
00:07:53,499 --> 00:07:57,830
In this phase, we transitioned
from VMware machines to cloud

126
00:07:57,830 --> 00:07:59,359
VM machines to containers.

127
00:07:59,989 --> 00:08:04,369
We moved the workload to Google, GK
and began the work of modernizing

128
00:08:04,369 --> 00:08:08,670
our data layer, which is the
Oracle database into Postgres.

129
00:08:09,150 --> 00:08:13,229
This phase allowed us to find the, unlock
the benefits of DevOps, like I mentioned,

130
00:08:13,619 --> 00:08:15,300
rapid scaling, automated recovery.

131
00:08:16,094 --> 00:08:19,364
Streamline, CACD pipeline,
improve security and many more.

132
00:08:19,664 --> 00:08:22,094
And I'll talk about all of
these in the coming slides

133
00:08:22,594 --> 00:08:23,945
on database migration.

134
00:08:24,935 --> 00:08:29,335
This was perhaps I would say the most
hardest hurdle for us from moving

135
00:08:29,335 --> 00:08:31,740
from Oracle to Cloud SQL PostgreSQL.

136
00:08:32,615 --> 00:08:35,915
This was not a simple export
report, like we'd had to rewrite

137
00:08:36,335 --> 00:08:38,225
a bunch of the Psql packages.

138
00:08:38,225 --> 00:08:38,830
This was custom.

139
00:08:39,605 --> 00:08:42,695
Manage complex time zone
conversions, character set,

140
00:08:42,695 --> 00:08:45,005
missile element, handle data type.

141
00:08:45,505 --> 00:08:49,895
And given the size of the data, this was
not a trivial challenge that required

142
00:08:49,895 --> 00:08:54,815
a lot of careful planning and execution
which was many months in the making.

143
00:08:55,515 --> 00:08:58,570
We utilized change data capture
to keep the databases in

144
00:08:58,570 --> 00:08:59,950
sync during the transition.

145
00:09:00,640 --> 00:09:04,235
It required a lot of cycles
of doing performance tuning.

146
00:09:04,640 --> 00:09:08,240
And multiple rounds of load testing
to ensure Postgres could match

147
00:09:08,720 --> 00:09:12,170
the high concurrency throughput of
the original Oracle environment.

148
00:09:12,670 --> 00:09:16,060
And being on cloud, we could spin
up resources in a matter of minutes,

149
00:09:16,720 --> 00:09:18,610
run a load test, and destroy them.

150
00:09:19,090 --> 00:09:23,960
So using Terraform, we declared our
whole stack and that made cloning for

151
00:09:23,960 --> 00:09:25,735
load testing environments a lot easier.

152
00:09:26,235 --> 00:09:27,705
For monitoring and troubleshooting.

153
00:09:27,705 --> 00:09:32,265
We used cloud native monitoring tooling,
which was a huge advantage here.

154
00:09:32,385 --> 00:09:36,435
Next I'll talk a little bit
about security on Kubernetes.

155
00:09:36,465 --> 00:09:41,055
Security is a zero trust game, so
we implemented fine grain network

156
00:09:41,055 --> 00:09:44,685
policies to control port, to
port communications on premise.

157
00:09:44,685 --> 00:09:46,545
We had to rely on a hard perimeter.

158
00:09:47,425 --> 00:09:51,235
But in GK we controlled over 200
distinct communication parts.

159
00:09:52,075 --> 00:09:56,845
So the dependency analysis we had done in
phase one helped us land this fine grain

160
00:09:57,025 --> 00:09:59,815
network security boundary definition.

161
00:09:59,845 --> 00:10:04,945
In phase two, we started from a less
restrictive network policy definition,

162
00:10:04,975 --> 00:10:09,115
and as we monitored the traffic, we
made this more restricted with fine

163
00:10:09,115 --> 00:10:12,295
grain protocol aware segmented traffics.

164
00:10:12,795 --> 00:10:14,805
The cloud-based tooling
made this possible.

165
00:10:15,165 --> 00:10:20,375
On GCP, we have fantastic network
traffic monitoring software allowing

166
00:10:20,375 --> 00:10:25,175
us to map all the routes, gave us a
lot of deep insights and made traffic

167
00:10:25,175 --> 00:10:27,125
monitoring possible and much, much easier.

168
00:10:27,625 --> 00:10:29,545
Now let's talk about disaster recovery.

169
00:10:30,355 --> 00:10:35,485
In our legacy environment, deer was a
large and manual and high anxiety process.

170
00:10:35,575 --> 00:10:41,885
It involved periodic backups measuring
recovery times in hours and even days.

171
00:10:42,455 --> 00:10:45,995
And we rarely tested a full scale
scenario because we were not confident

172
00:10:45,995 --> 00:10:48,485
of getting to a recovery state.

173
00:10:48,985 --> 00:10:52,510
When we moved in phase one, our
dear strategy matured because

174
00:10:52,675 --> 00:10:57,055
we moved from manual backups to
automated persistent disc snapshots.

175
00:10:57,575 --> 00:10:59,870
And we were able to
restore them and test them.

176
00:11:00,370 --> 00:11:03,670
And this was a huge step forward
because we were able to document

177
00:11:03,670 --> 00:11:09,915
everything and have automation to do
the spin up of new VMs, reattaching

178
00:11:09,920 --> 00:11:12,350
disk, and reconfiguring networks.

179
00:11:12,850 --> 00:11:17,410
Once we reached phase two with gk,
our entire DR philosophy change

180
00:11:17,410 --> 00:11:22,120
because we moved from a model of
recovery to model of resilience.

181
00:11:22,750 --> 00:11:26,650
So in GK, we implemented a
multi-region deployment strategy.

182
00:11:27,200 --> 00:11:31,790
And we use gks Natives capabilities
for backup of GK, ensuring that our

183
00:11:31,790 --> 00:11:36,140
container configuration secrets and
stateful data sets were all replicated

184
00:11:36,590 --> 00:11:38,600
across geo geographic borders.

185
00:11:39,100 --> 00:11:42,690
And then we had infrastructure code,
like I mentioned which helped us

186
00:11:43,320 --> 00:11:45,390
rapidly build and deploy environments.

187
00:11:45,890 --> 00:11:49,640
I want to emphasize the importance
of a full scale recovery exercise.

188
00:11:50,140 --> 00:11:56,250
As part of our launch, we did a bunch of
what we call wheels of misfortune, where

189
00:11:56,250 --> 00:12:02,370
we test out the recovery of the system
that allowed us to move from theoretical

190
00:12:02,370 --> 00:12:07,220
models of recovery time objectives and
recovery point objectives, and document

191
00:12:07,220 --> 00:12:10,545
them and testing them before co life.

192
00:12:10,575 --> 00:12:15,340
Made sure that we had a well-documented
procedure that we could follow in

193
00:12:15,340 --> 00:12:17,080
case we had a disaster recovery.

194
00:12:17,785 --> 00:12:18,385
Scenario,

195
00:12:18,885 --> 00:12:23,285
some of the lessons I'd like to share
what worked for us and what didn't work

196
00:12:23,285 --> 00:12:25,325
for us and what we would do differently.

197
00:12:26,225 --> 00:12:31,025
I think the sequencing of choosing
a two phase approach is something

198
00:12:31,025 --> 00:12:33,785
that everyone asks us multiple times.

199
00:12:33,835 --> 00:12:34,960
Is it really worth doing it?

200
00:12:35,460 --> 00:12:38,100
But the biggest reason for
doing that is risk decoupling.

201
00:12:38,600 --> 00:12:43,040
If you want to change your infrastructure,
which is moving to cloud, and you

202
00:12:43,040 --> 00:12:46,940
want to change your runtime, which
is moving from VMs to containers, and

203
00:12:46,940 --> 00:12:50,275
you want to change your data layer,
moving to Postgres or Cloud sql.

204
00:12:50,775 --> 00:12:51,795
All at the same time.

205
00:12:52,095 --> 00:12:55,425
It's a recipe for something
to go wrong, right?

206
00:12:55,725 --> 00:12:58,815
And then when troubleshooting such
a service, it becomes very complex

207
00:12:58,815 --> 00:13:02,415
because you are trying to figure out
where they should happen between a

208
00:13:02,415 --> 00:13:09,195
network configuration, a resource
limit in a pod, a syntax cetera, or so

209
00:13:09,435 --> 00:13:15,255
decoupling them and doing phase one as
a lift and shift helped us isolate the

210
00:13:15,285 --> 00:13:17,025
cloud infrastructure variables first.

211
00:13:17,525 --> 00:13:21,005
So in phase one, by moving to
cloud, we would then show that the

212
00:13:21,005 --> 00:13:24,815
cloud infrastructure fundamentally
worked well for the service.

213
00:13:25,755 --> 00:13:30,165
And as I mentioned, it allowed
us to measure the baseline, and

214
00:13:30,165 --> 00:13:31,755
this was our insurance policy.

215
00:13:31,755 --> 00:13:36,015
It allows us to prove to the
stakeholders that cloud was just as

216
00:13:36,015 --> 00:13:40,605
stable as the VMware environment,
and also gave the project team a

217
00:13:40,605 --> 00:13:43,245
breather to design the modernization.

218
00:13:43,735 --> 00:13:45,025
And like I mentioned.

219
00:13:45,715 --> 00:13:50,005
The modernization effort is a lot
more easier with cloud native tools.

220
00:13:50,495 --> 00:13:53,225
And there are lots of these
tools for cloud networking.

221
00:13:54,095 --> 00:13:57,305
Im models Kubernetes Manifest
Istio service meshes.

222
00:13:57,805 --> 00:14:04,435
So decoupling is one of the key reasons
why we chose the two phase approach.

223
00:14:05,425 --> 00:14:08,155
Let's talk about some of the
things that we would do differently

224
00:14:08,155 --> 00:14:09,175
if we were to do this again.

225
00:14:09,675 --> 00:14:13,425
One of the hardest thing I mentioned
was doing the data change, right?

226
00:14:13,425 --> 00:14:16,065
The data migration from
Oracle to Postgres.

227
00:14:16,515 --> 00:14:20,775
And if we were to do this differently,
maybe we would start the database schema

228
00:14:20,775 --> 00:14:24,035
simplification work stream much sooner.

229
00:14:24,605 --> 00:14:29,615
We spent a lot of time in phase two
fixing the Oracle specific things

230
00:14:29,615 --> 00:14:31,265
that needed to change to Postgres.

231
00:14:31,955 --> 00:14:36,245
Maybe if we had started this much
sooner, maybe even in phase one, and

232
00:14:36,245 --> 00:14:40,355
let it land in phase two, that would've
been a lot better I would think.

233
00:14:41,195 --> 00:14:45,005
Secondly, observability, we waited
until phase two to implement some of

234
00:14:45,005 --> 00:14:48,065
the more advanced distributed tracing.

235
00:14:48,965 --> 00:14:52,925
In hindsight, we should have
instrumented our monolith with tracing

236
00:14:52,925 --> 00:14:58,075
before phase one having those traces
even in the lift and shift phase.

237
00:14:58,840 --> 00:15:02,860
Would've helped us identify a lot of
these latency jitters much easier.

238
00:15:03,620 --> 00:15:06,920
And on cloud there's a lot of
excellent tooling to help us do that

239
00:15:07,470 --> 00:15:11,760
and that would've also benefited
operating the service after phase one.

240
00:15:12,260 --> 00:15:17,240
Long story short, the key takeaway
here is your migration sequence

241
00:15:17,240 --> 00:15:22,010
should be designed to build confidence
both in the system's performance and

242
00:15:22,010 --> 00:15:23,670
your team's ability to manage it.

243
00:15:24,470 --> 00:15:30,290
So don't be afraid to take the slower
phase route if it guarantees a success

244
00:15:30,440 --> 00:15:35,710
arrival which has all the benefits that
you want to get out of the project.

245
00:15:36,210 --> 00:15:41,580
So now I'll talk more about the SRE
parts of operating in a containerized

246
00:15:41,580 --> 00:15:48,120
workload as we move to gk some of the
valuable lessons we learned of operating.

247
00:15:48,870 --> 00:15:49,530
At scale.

248
00:15:50,030 --> 00:15:51,470
Firstly, resource management.

249
00:15:52,160 --> 00:15:57,200
In the VMware world, we were always
used to over provisioning because

250
00:15:57,500 --> 00:16:01,400
we wanted to make sure there was
enough headroom for the service In

251
00:16:01,400 --> 00:16:04,090
Kubernetes that's not a great recipe.

252
00:16:04,870 --> 00:16:10,240
We wanted to use the best practices,
which involved learning to set requests

253
00:16:10,270 --> 00:16:12,640
and limits based on our 95th percentile.

254
00:16:13,400 --> 00:16:17,420
And the success metrics we gathered in
phase one helped us set those values

255
00:16:17,780 --> 00:16:19,650
in a much more well-managed way.

256
00:16:19,650 --> 00:16:24,120
Before we launched, we also were able
to implement gks Autopilot feature to

257
00:16:24,120 --> 00:16:29,430
help us fine tune these over time and
grow as the businesses associates.

258
00:16:30,300 --> 00:16:34,650
Secondly, the deployment strategies,
we moved away from maintenance windows.

259
00:16:35,450 --> 00:16:38,950
So by using gks tested
slow rollout deployments.

260
00:16:39,610 --> 00:16:42,380
We could spin up a new version
of the system new version of the

261
00:16:42,380 --> 00:16:47,250
containers run through the tests
and then flip the traffic as they

262
00:16:47,250 --> 00:16:48,910
move as they're being deployed.

263
00:16:49,300 --> 00:16:52,810
If there was a new bug detected,
we will roll back and the load

264
00:16:52,810 --> 00:16:54,820
balancer toggles away automatically.

265
00:16:55,320 --> 00:17:00,900
Thirdly, observability in a monolith, we
usually had logs in multiple locations,

266
00:17:00,900 --> 00:17:05,430
and it was always a challenge to go
fetch those logs and look at them and.

267
00:17:06,075 --> 00:17:07,395
Correlate them with each other.

268
00:17:07,455 --> 00:17:08,505
In Kubernetes.

269
00:17:09,075 --> 00:17:14,905
Logs are all centralized on Google
Cloud logging these logs are all

270
00:17:14,905 --> 00:17:20,515
aggregated and helps us process pers
and troubleshooting much faster.

271
00:17:21,015 --> 00:17:25,395
Without these logs in a central area,
it's almost like a flying blind.

272
00:17:25,875 --> 00:17:29,085
So the central logging through
GKE is really advantages.

273
00:17:29,585 --> 00:17:31,055
And finally on state management.

274
00:17:31,055 --> 00:17:34,715
So one of the most significant
mind shift mindset shifts we

275
00:17:34,715 --> 00:17:38,555
had when moving from VMware to
Kubernetes is how we handle state.

276
00:17:39,095 --> 00:17:44,405
So in our legacy VMware environment, we
had every server was essentially a pet.

277
00:17:44,465 --> 00:17:46,595
We had a name, an IP address.

278
00:17:46,595 --> 00:17:50,695
We had data tied to that machine and
then we had to manage that machine.

279
00:17:51,205 --> 00:17:53,635
But with gk, everything is deployments.

280
00:17:53,635 --> 00:17:56,275
So you have to manage the
state of those deployments.

281
00:17:56,945 --> 00:17:59,375
They're both stateless
assets and stateful assets.

282
00:17:59,375 --> 00:18:03,725
So clearly drawing the boundaries
around each of these asset groups and

283
00:18:03,725 --> 00:18:07,975
managing them as states is the way to go.

284
00:18:08,525 --> 00:18:11,945
And this will allow you to
scale horizontally and survive

285
00:18:11,945 --> 00:18:13,955
failures a lot more easier.

286
00:18:14,455 --> 00:18:16,945
The next thing I want to
talk about is security.

287
00:18:17,785 --> 00:18:21,895
So security in a cloud native
environment is not a perimeter

288
00:18:21,895 --> 00:18:23,485
you build your application around.

289
00:18:24,205 --> 00:18:25,825
It's a series of layers.

290
00:18:26,245 --> 00:18:29,215
So you'll build layers
and layers of security.

291
00:18:29,695 --> 00:18:33,475
Like I mentioned in the legacy
VMware setup, it was all

292
00:18:33,475 --> 00:18:35,125
about defining a boundary.

293
00:18:35,605 --> 00:18:38,785
And if you're inside that network
boundary, you are trusted.

294
00:18:38,785 --> 00:18:40,525
But if you're outside, you're not trusted.

295
00:18:41,025 --> 00:18:43,155
In gk, that model is very obsolete.

296
00:18:43,255 --> 00:18:48,935
There's a zero trust architecture and we
had to break it down by these pillars.

297
00:18:49,655 --> 00:18:51,275
You have identity access.

298
00:18:51,275 --> 00:18:54,995
So we moved away from long list
service accounts, which were

299
00:18:54,995 --> 00:18:57,235
essentially latent credentials.

300
00:18:58,095 --> 00:19:02,505
And we implemented G Clear's
workload entity, which allowed

301
00:19:02,505 --> 00:19:06,950
us to have a service account that
access a Google IAM service account.

302
00:19:07,620 --> 00:19:11,760
It also meant our pods get
short-lived, automatic rotated tokens.

303
00:19:12,640 --> 00:19:15,760
This allowed us to have
the least principle, the

304
00:19:15,760 --> 00:19:17,320
principle of least privilege.

305
00:19:17,810 --> 00:19:22,850
And if a pod doesn't need to write
to a Google Cloud storage bucket,

306
00:19:22,850 --> 00:19:26,790
essentially, like we wouldn't permit
that through the identity account.

307
00:19:27,290 --> 00:19:31,580
Secondly the network controls, like I
mentioned, in phase one, we had these

308
00:19:31,580 --> 00:19:37,480
broad firewall rules, but in phase two,
we implemented G K's network policy rules.

309
00:19:37,990 --> 00:19:41,980
This allowed us to micro segment
at the individual port level.

310
00:19:42,550 --> 00:19:48,220
We were able to map all the traffic
between the 200 different pods

311
00:19:48,220 --> 00:19:52,240
and the communications that they
had, and we were able to define.

312
00:19:52,660 --> 00:19:56,570
How these pods should interact
with each other very specifically.

313
00:19:57,410 --> 00:20:01,250
And we also were able to implement
like the default deny policy such that

314
00:20:01,580 --> 00:20:07,860
even if one pod is compromised, that's
the only border in which the attackers

315
00:20:07,860 --> 00:20:10,760
trapped onto Thirdly, container security.

316
00:20:11,270 --> 00:20:15,290
We shifted left, which means that
we were able to build all our.

317
00:20:15,905 --> 00:20:20,195
Control container security well
ahead into the development cycle.

318
00:20:20,555 --> 00:20:23,765
So we implemented image
scanning for vulnerabilities.

319
00:20:24,275 --> 00:20:26,765
We were able to implement
binary authorization.

320
00:20:27,195 --> 00:20:32,135
So these would prevent vulnerabilities
even before it hits the container registry

321
00:20:32,195 --> 00:20:33,905
from where your images get deployed.

322
00:20:34,175 --> 00:20:37,625
So even in the build cycle, we were
implement, we were able to implement

323
00:20:37,625 --> 00:20:42,905
these controls so that the images
were, are tested before they reached.

324
00:20:43,715 --> 00:20:44,315
Production.

325
00:20:44,815 --> 00:20:46,135
Finally, data protection.

326
00:20:46,325 --> 00:20:50,795
We shifted from manual certificate
management, automated MTLS, so

327
00:20:50,795 --> 00:20:55,815
all data and transit and all data
at rest is increment encrypted

328
00:20:55,815 --> 00:20:57,675
with the customer managed keys.

329
00:20:57,945 --> 00:21:01,825
And those keys were all managed
by us in the GCP projects.

330
00:21:02,325 --> 00:21:07,645
So some of the practical takeaways that
I'll mention as I wrap up, I want to

331
00:21:07,645 --> 00:21:12,845
leave you with these four areas that
will make any modernization successful.

332
00:21:12,895 --> 00:21:15,385
So start with the dependency mapping.

333
00:21:15,435 --> 00:21:19,065
This is one of the most
underrated areas for SREs.

334
00:21:19,695 --> 00:21:24,765
It's necessary to understand the
network flows, the application flows.

335
00:21:25,240 --> 00:21:29,950
And discovering hardcoded ips later in
the migration is a recipe for disaster.

336
00:21:30,340 --> 00:21:36,580
So understanding who all depend on the
system, which systems does this system

337
00:21:36,580 --> 00:21:39,540
depend on, is really necessary measure.

338
00:21:40,040 --> 00:21:44,110
So this is all the telemetry, all
the metrics, all the baselines

339
00:21:44,110 --> 00:21:49,540
that you need to build SREs
have the three basic metrics.

340
00:21:50,020 --> 00:21:53,870
Which are errors, throughput and latency.

341
00:21:54,290 --> 00:21:56,960
So measuring that is a
great starting point.

342
00:21:57,460 --> 00:22:02,650
Also, having a good set of baseline
metrics ensures that you don't run into

343
00:22:02,650 --> 00:22:07,510
those conversations of someone feels like
the app is slower with the migration.

344
00:22:07,900 --> 00:22:12,220
Having a baseline metrics helps you
to have more objective data-driven

345
00:22:12,220 --> 00:22:14,380
conversations rather than subjective.

346
00:22:15,005 --> 00:22:19,525
Feedback from customers and
like I mentioned, plan for

347
00:22:19,615 --> 00:22:21,055
the database complexities.

348
00:22:21,565 --> 00:22:27,505
Applications are easy to rewrite and
re-architecture data is a lot harder.

349
00:22:28,435 --> 00:22:34,195
As we moved from our experience, it
took us a lot of time to do this part.

350
00:22:34,835 --> 00:22:37,655
There are many cycles of
repeated performance testing.

351
00:22:38,205 --> 00:22:43,065
And feasibility that needs to happen
to get to the point of completion.

352
00:22:43,785 --> 00:22:48,975
So start early and if you end up
finishing sooner, you can always use

353
00:22:48,975 --> 00:22:51,285
the time for more performance tuning.

354
00:22:51,785 --> 00:22:55,965
And lastly, like I mentioned testing
your disaster recovery scenario.

355
00:22:56,725 --> 00:23:00,415
I cannot stress this
enough, having a system.

356
00:23:01,000 --> 00:23:06,370
That is only theoretically set
up for a disaster is a liability.

357
00:23:07,030 --> 00:23:09,520
Like I mentioned at Google,
we periodically run wheels of

358
00:23:09,520 --> 00:23:13,750
misfortune, disaster recovery
testing cycles, where we break things

359
00:23:13,750 --> 00:23:16,030
on purpose in real environments.

360
00:23:16,900 --> 00:23:22,560
And if we challenge our teams to go ahead
and try these disaster recovery scenarios

361
00:23:22,890 --> 00:23:27,680
within the RTO test the playbooks,
and then write re retrospectives on.

362
00:23:28,190 --> 00:23:29,090
What went wrong.

363
00:23:29,810 --> 00:23:35,570
So testing your DR periodically is a
fantastic way to ensure that you have

364
00:23:35,720 --> 00:23:37,880
high confidence in your DR strategy.

365
00:23:38,380 --> 00:23:41,590
And like I mentioned, modeling,
modernizing any legacy

366
00:23:41,590 --> 00:23:44,260
application service is a marathon.

367
00:23:44,590 --> 00:23:50,370
It requires patience, a phase strategy,
and a relentless focus on automation.

368
00:23:50,970 --> 00:23:54,210
By decoupling the migration from
transformation, we reached a goal of.

369
00:23:55,005 --> 00:23:59,175
Minimal downtime and a service that
is ready for many decades to come.

370
00:24:00,135 --> 00:24:01,725
Thank you so much for your time today.

371
00:24:01,965 --> 00:24:04,545
I'm happy to answer any
questions if you have any.

