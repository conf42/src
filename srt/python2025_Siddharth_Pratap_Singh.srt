1
00:00:00,060 --> 00:00:01,510
Hi, everyone, and welcome.

2
00:00:01,740 --> 00:00:05,290
I'm really excited to be here today
to talk to you about vector search.

3
00:00:05,770 --> 00:00:08,980
It's a fascinating technology
that's transforming the way we find

4
00:00:08,980 --> 00:00:13,359
information in the age of semantic
understanding, where we're not just

5
00:00:13,359 --> 00:00:15,320
looking for keywords, but actual meaning.

6
00:00:15,819 --> 00:00:17,420
this comprehensive review.

7
00:00:17,600 --> 00:00:21,900
we'll give you an idea of the
architectural foundations of vector

8
00:00:21,900 --> 00:00:25,240
search and its implementation
across multiple domains.

9
00:00:25,790 --> 00:00:27,290
my name is Siddharth Pratap Singh.

10
00:00:27,790 --> 00:00:28,200
All right.

11
00:00:28,400 --> 00:00:31,560
to fully appreciate vector
search, let's take a quick look

12
00:00:31,620 --> 00:00:32,990
back at how search has evolved.

13
00:00:33,490 --> 00:00:37,830
Remember when we used to rely on typing in
exact keywords and crossing our fingers?

14
00:00:38,050 --> 00:00:41,100
That was token based search,
using simple word matching.

15
00:00:41,690 --> 00:00:45,750
It was fine for basic searches, but it
often struggled with the complexities

16
00:00:45,750 --> 00:00:47,540
and ambiguities of human language.

17
00:00:48,040 --> 00:00:51,250
Then came semantic search, a
step in the right direction.

18
00:00:51,260 --> 00:00:55,120
The idea was to understand the
intent and context behind our

19
00:00:55,120 --> 00:00:59,090
words, but the technology at the
time just wasn't up to the task.

20
00:00:59,590 --> 00:01:03,640
Fast forward to today and we have
vector search, powered by incredible

21
00:01:03,640 --> 00:01:05,680
advances in machine learning and NLP.

22
00:01:06,180 --> 00:01:09,960
It's like semantic search on a whole
new level, able to truly understand

23
00:01:09,960 --> 00:01:15,160
what we mean and find relevant results
even in massive, messy data sets.

24
00:01:15,660 --> 00:01:18,290
Okay, let's dive into the
mechanics of vector search.

25
00:01:18,700 --> 00:01:22,820
At its core, it's about transforming
text into high dimensionality vectors.

26
00:01:23,530 --> 00:01:26,450
These vectors are like numerical
representations that capture

27
00:01:26,450 --> 00:01:29,130
the meaning of words and their
relationships to one another.

28
00:01:29,965 --> 00:01:34,275
Think of it as converting language
into a format that computers can

29
00:01:34,275 --> 00:01:40,545
really grasp, a way to map words and
concepts in a multi dimensional space.

30
00:01:41,395 --> 00:01:44,225
This transformation is made
possible by neural networks,

31
00:01:44,255 --> 00:01:48,145
sophisticated algorithms that can
process complex data relationships.

32
00:01:48,754 --> 00:01:53,045
They can handle all the nuances of human
language, like synonyms, ambiguity, and

33
00:01:53,055 --> 00:01:55,045
context, while maintaining efficiency.

34
00:01:55,545 --> 00:01:57,675
Different types of neural
networks are used in the process.

35
00:01:57,980 --> 00:01:59,560
each with its own strengths.

36
00:01:59,870 --> 00:02:05,170
Some popular ones include RNNs, Recurrent
Neural Networks, Convolutional Neural

37
00:02:05,170 --> 00:02:06,590
Networks, and Transformer Networks.

38
00:02:07,500 --> 00:02:12,110
RNNs are practically good at processing
sequential data like text, while CNNs

39
00:02:12,140 --> 00:02:13,359
excel at capturing local patterns.

40
00:02:13,859 --> 00:02:18,319
Transformers have revolutionized
NLP with their ability to handle

41
00:02:18,319 --> 00:02:20,049
long range dependencies in text.

42
00:02:20,549 --> 00:02:24,489
And to find the information we
need amongst a sea of data, we

43
00:02:24,489 --> 00:02:26,139
use specialized index structures.

44
00:02:26,349 --> 00:02:30,509
these index structures are designed
to enable rapid similarity searches

45
00:02:30,509 --> 00:02:31,819
in high dimensional spaces.

46
00:02:32,499 --> 00:02:37,049
They allow us to quickly sift through
millions of vectors and pinpoint the

47
00:02:37,049 --> 00:02:41,159
ones that are most similar to our search
queries, all while balancing search

48
00:02:41,169 --> 00:02:43,409
speed, memory efficiency, and accuracy.

49
00:02:44,359 --> 00:02:50,859
Some common index structures used in
embedding based search or vector search

50
00:02:50,859 --> 00:02:55,909
include KD trees, bird trees, locality
sensitive hashing, which is LSH.

51
00:02:56,409 --> 00:02:59,059
Okay, so let's go with the
implementation architecture.

52
00:02:59,060 --> 00:03:03,549
We'll take a closer look at how a vector
search system is actually implemented.

53
00:03:03,699 --> 00:03:07,269
There are a few key components
that work together seamlessly.

54
00:03:08,019 --> 00:03:10,129
First, we have the document
processing pipeline.

55
00:03:10,129 --> 00:03:12,069
This pipeline takes raw text data.

56
00:03:12,484 --> 00:03:16,594
And transforms it into a machine readable
vectors we were just, referring to.

57
00:03:17,094 --> 00:03:20,594
This involves several steps, including
cleaning the text, breaking it into

58
00:03:20,664 --> 00:03:25,314
meaningful chunks, like sentences or
paragraphs, and then creating those rich

59
00:03:25,314 --> 00:03:29,534
numerical representations into, through
our chosen embedding model into vectors.

60
00:03:30,509 --> 00:03:33,559
This process can also include
things like stemming, lemmatization,

61
00:03:33,559 --> 00:03:36,369
and stop word removal to further
optimize the search performance.

62
00:03:36,869 --> 00:03:40,879
Then, when you type in a search query,
the system actually converts it into

63
00:03:40,879 --> 00:03:42,739
the same vector format as the documents.

64
00:03:43,239 --> 00:03:47,639
The dimensionality of the search, the
vectors from the search queries as

65
00:03:47,639 --> 00:03:49,069
well as the documents are the same.

66
00:03:49,729 --> 00:03:52,579
It's like translating your query
into the language of the data,

67
00:03:52,689 --> 00:03:54,319
enabling true semantic matching.

68
00:03:54,959 --> 00:03:56,109
Finally, the index.

69
00:03:56,689 --> 00:03:58,169
The search index comes into play.

70
00:03:58,679 --> 00:04:03,059
It uses efficient nearest neighbor
algorithms to quickly find the documents

71
00:04:03,059 --> 00:04:07,039
that are most similar to your query
based on their vector representations.

72
00:04:07,539 --> 00:04:13,629
how, what we mean by similar is that in
that dimensional space, the documents

73
00:04:13,629 --> 00:04:17,830
that are most relevant or, answer the
question that your query has, they

74
00:04:17,830 --> 00:04:20,130
lie closer in that dimensional space.

75
00:04:20,850 --> 00:04:22,890
The system then intelligently
ranks the results.

76
00:04:23,390 --> 00:04:27,110
taking into account not only the semantic
relevance, but also the factors like

77
00:04:27,360 --> 00:04:32,560
quality of the document, the historical
behavioral popularity of the document,

78
00:04:33,060 --> 00:04:36,990
amongst the users on your platforms,
and then some user preferences.

79
00:04:37,490 --> 00:04:40,680
Okay, so we'll go over some
domain specific applications.

80
00:04:40,770 --> 00:04:42,390
We'll take up e commerce and health care.

81
00:04:42,890 --> 00:04:46,230
So embedding base search or vector
search isn't just a theoretical concept.

82
00:04:46,270 --> 00:04:49,260
It's already having a real
world impact in various fields.

83
00:04:49,970 --> 00:04:52,940
In e commerce, it's revolutionizing
the way we shop online.

84
00:04:53,540 --> 00:04:57,760
It helps us find the perfect product, the
most relevant product, even if we can't

85
00:04:57,790 --> 00:04:59,350
quite articulate what we are looking for.

86
00:04:59,850 --> 00:05:02,400
It can understand the intent
behind vague searches.

87
00:05:02,450 --> 00:05:06,370
For example, something like, birthday
gift for a 10 year old who likes

88
00:05:06,370 --> 00:05:08,720
science and suggests relevant products.

89
00:05:09,220 --> 00:05:11,920
on the business side, it helps
companies detect fraud and

90
00:05:11,920 --> 00:05:13,510
ensure secure transactions.

91
00:05:14,090 --> 00:05:17,460
for queries like this, for example,
the one I mentioned, birthday gift

92
00:05:17,470 --> 00:05:19,060
for a 10 year old who likes science.

93
00:05:19,750 --> 00:05:22,470
Most of the products that
an e commerce catalog has.

94
00:05:22,770 --> 00:05:25,760
It does not contain keywords
that go along with it, right?

95
00:05:26,270 --> 00:05:31,420
People will put in science kits,
astronomy kits, or computer kits as a

96
00:05:31,420 --> 00:05:35,390
product, but they wouldn't necessarily
mention 10 year olds who like science.

97
00:05:35,640 --> 00:05:40,310
So these queries are
very well represented.

98
00:05:40,500 --> 00:05:44,310
And the products, are that are retrieved
using vector search are relevant

99
00:05:44,310 --> 00:05:48,100
that you wouldn't find in the legacy
token based retrieval in health care.

100
00:05:48,100 --> 00:05:51,980
It's helping doctors and researchers
across critical access, critical

101
00:05:51,980 --> 00:05:56,350
information faster and more accurately
imagine being able to instantly

102
00:05:56,350 --> 00:05:59,460
find relevant medical records or
research papers, even if they use

103
00:05:59,460 --> 00:06:01,060
slightly different terminologies.

104
00:06:01,060 --> 00:06:05,300
an example in e commerce, like I just
mentioned, this can be life saving

105
00:06:05,300 --> 00:06:08,630
in emergency situations are crucial
for groundbreaking medical research.

106
00:06:09,425 --> 00:06:12,645
And of course, all this is done
while maintaining strict compliance

107
00:06:12,715 --> 00:06:13,955
with privacy regulations.

108
00:06:14,455 --> 00:06:16,975
VectorSearch is also a
game changer for academia.

109
00:06:17,225 --> 00:06:20,885
It can analyze complex relationships
between research papers, help us

110
00:06:20,885 --> 00:06:24,445
uncover hidden connections, and
accelerate scientific discovery.

111
00:06:25,185 --> 00:06:28,815
For example, it can identify papers
that are semantically similar.

112
00:06:29,345 --> 00:06:32,185
Even if they don't share any
common citations, leading to

113
00:06:32,185 --> 00:06:33,585
new insights and collaborations.

114
00:06:34,085 --> 00:06:37,085
In the business world, it's
breaking down information silos and

115
00:06:37,085 --> 00:06:38,345
making knowledge sharing a breeze.

116
00:06:38,845 --> 00:06:41,365
You don't need to dig through
endless files and folders to

117
00:06:41,365 --> 00:06:42,525
find the right information.

118
00:06:43,025 --> 00:06:48,125
However, or regardless of how the
information is stored or organized,

119
00:06:48,625 --> 00:06:56,825
the vector based indexes can provide a
critical infrastructure or a critical

120
00:06:56,835 --> 00:07:01,575
system that can get you all these files
and folders that you're looking for.

121
00:07:02,345 --> 00:07:04,885
This can lead to improved decision
making, increased productivity,

122
00:07:04,885 --> 00:07:06,555
and a more connected workforce.

123
00:07:07,055 --> 00:07:11,265
Let's come to how this is implemented and
being used in media and legal departments.

124
00:07:11,265 --> 00:07:14,395
In the media industry, VectorSearch
is powering the next generation

125
00:07:14,395 --> 00:07:16,065
of content recommendation systems.

126
00:07:16,565 --> 00:07:20,195
It's like having a personal assistant who
knows your taste in movies and TV shows

127
00:07:20,215 --> 00:07:24,555
better than you do, because it understands
your intent, maps them into embeddings.

128
00:07:25,405 --> 00:07:28,175
It's helpful in suggesting content
that you'll actually enjoy.

129
00:07:28,675 --> 00:07:33,325
It can analyze your viewing history,
genre preferences, and even your

130
00:07:33,325 --> 00:07:37,085
emotional responses to content to provide
highly personalized recommendations.

131
00:07:37,585 --> 00:07:41,025
For legal professionals,
it's revolutionizing how

132
00:07:41,355 --> 00:07:42,835
they research case laws.

133
00:07:43,335 --> 00:07:47,725
Imagine being able to instantly
find relevant precedents based on

134
00:07:47,725 --> 00:07:49,185
the nuances of legal arguments.

135
00:07:49,930 --> 00:07:53,720
even if they use different wording or
address slightly different circumstances.

136
00:07:54,220 --> 00:07:58,590
this takes countless hours of research and
leads to more efficient legal strategies.

137
00:07:59,090 --> 00:08:01,950
but how do we know if,
it is actually better?

138
00:08:02,420 --> 00:08:06,190
the numbers across the industry from the
research papers speak for themselves.

139
00:08:06,850 --> 00:08:10,180
We've seen significant improvements
in search accuracy, user

140
00:08:10,200 --> 00:08:11,620
satisfaction across the board.

141
00:08:11,920 --> 00:08:15,980
In many cases, vector search has
been shown to outperform traditional

142
00:08:15,980 --> 00:08:19,870
keyword searches by a significant
margin, leading to more relevant

143
00:08:19,890 --> 00:08:21,180
results and a better user experience.

144
00:08:21,680 --> 00:08:24,920
The best part, this field is
actively being worked upon.

145
00:08:25,030 --> 00:08:28,810
all the researchers across the industry
are constantly refining this technology,

146
00:08:28,850 --> 00:08:30,930
making it even smarter and more efficient.

147
00:08:31,430 --> 00:08:33,220
There are technical challenges.

148
00:08:33,575 --> 00:08:36,885
as we deal with the ever growing
amounts of data, we need to ensure

149
00:08:36,895 --> 00:08:38,395
our systems can handle the load.

150
00:08:38,895 --> 00:08:42,445
That means finding innovative
ways to manage data and optimize

151
00:08:42,445 --> 00:08:45,105
performances, especially in
distributed computing environments.

152
00:08:45,605 --> 00:08:47,185
things need to be fetched fast.

153
00:08:47,925 --> 00:08:49,925
performance optimization is another one.

154
00:08:50,235 --> 00:08:52,855
the model improvements are also happening.

155
00:08:52,965 --> 00:08:57,575
better and better models that can be used
to create these embedding representations

156
00:08:57,575 --> 00:08:59,045
or semantic representations.

157
00:08:59,615 --> 00:09:03,395
They are continually being
researched and innovated upon.

158
00:09:04,355 --> 00:09:07,925
These challenges actually also
present exciting opportunities.

159
00:09:08,425 --> 00:09:11,335
Imagine combining vector search
with cutting edge technologies

160
00:09:11,355 --> 00:09:12,465
like distributed learning.

161
00:09:12,475 --> 00:09:15,475
We could improve model performances
while keeping data privacy in mind.

162
00:09:15,975 --> 00:09:16,915
Going on to the next slide.

163
00:09:17,385 --> 00:09:18,965
the future is bright.

164
00:09:19,135 --> 00:09:22,115
We're seeing a convergence with
privacy preserving technologies,

165
00:09:22,805 --> 00:09:25,815
allowing us to search across
decentralized databases without

166
00:09:25,985 --> 00:09:27,815
compromising sensitive information.

167
00:09:27,815 --> 00:09:30,410
there are applications in blockchains.

168
00:09:30,410 --> 00:09:32,486
There are applications in.

169
00:09:32,986 --> 00:09:39,076
um, privacy preserving medical databases
that do not compromise sensitive

170
00:09:39,156 --> 00:09:42,476
information and are still able to find
relevant information for your queries.

171
00:09:43,386 --> 00:09:46,456
We're also seeing integration with
distributed computing, enabling

172
00:09:46,466 --> 00:09:49,666
robust search architectures that
can handle massive amounts of data.

173
00:09:50,166 --> 00:09:54,896
With the rise of more sophisticated
NLP, we can expect even deeper

174
00:09:54,896 --> 00:09:57,945
semantic understanding, leading to more
accurate and relevant search results.

175
00:09:58,445 --> 00:10:01,335
Vector search is, more than
just an incremental improvement.

176
00:10:01,385 --> 00:10:03,795
It's a paradigm shift in
how we find information.

177
00:10:04,035 --> 00:10:07,605
It's about moving beyond simple keyword
matching and truly understanding the

178
00:10:07,605 --> 00:10:09,275
meaning and context behind our searches.

179
00:10:09,775 --> 00:10:13,455
As we continue to refine the technology
and explore new applications, it will

180
00:10:13,485 --> 00:10:17,085
definitely play a crucial role in
unlocking the power and information and

181
00:10:17,095 --> 00:10:19,095
driving innovation across industries.

182
00:10:19,595 --> 00:10:21,005
Thank you, all for listening.

183
00:10:21,155 --> 00:10:24,825
I hope you're as excited about the
future of vector search as I am.

184
00:10:25,305 --> 00:10:28,995
thank you so much for taking your
time and hearing what I have to say.

185
00:10:29,405 --> 00:10:29,875
Thank you.

