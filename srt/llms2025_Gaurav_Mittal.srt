1
00:00:00,000 --> 00:00:00,690
Hello everyone.

2
00:00:01,050 --> 00:00:05,100
I'm Gar Hel senior software engineer
with Northshore Technology, and I'm

3
00:00:05,100 --> 00:00:09,300
excited to talk to you about how AI
is transforming how organizations

4
00:00:09,480 --> 00:00:10,645
manage and transform their data.

5
00:00:11,145 --> 00:00:14,174
So today we'll talk a little bit
about what ETL is, what the problems

6
00:00:14,174 --> 00:00:16,215
with traditional ETL systems is.

7
00:00:16,814 --> 00:00:21,854
We look at how AI powered ETL is the
new way to solve some of these problems.

8
00:00:22,354 --> 00:00:25,864
We will take a deep dive into the
various parts of the ETL process

9
00:00:25,864 --> 00:00:28,714
and how it AI is improving them.

10
00:00:29,214 --> 00:00:34,785
we'll also look at how, EL pipelines
are maintained, how the infrastructure

11
00:00:34,785 --> 00:00:38,385
for the EL pipelines is managed and how
AI can improve some of those things.

12
00:00:38,885 --> 00:00:43,885
We look at some of the popular tools and
platforms that are used for this, and we

13
00:00:43,890 --> 00:00:45,894
will end with some practical examples.

14
00:00:46,514 --> 00:00:47,474
And future trends.

15
00:00:47,974 --> 00:00:49,149
Alright, so what is ETL?

16
00:00:49,699 --> 00:00:53,884
ETL is really, the backbone of data
warehousing and business intelligence.

17
00:00:54,705 --> 00:00:59,774
organizations are accumulating vast
amounts of data and they need a way to.

18
00:01:00,050 --> 00:01:07,010
report on it, turn them into actionable
insights, and ETL helps with that by

19
00:01:07,010 --> 00:01:12,000
streamlining the process, all this
raw information is consolidated, and

20
00:01:12,210 --> 00:01:16,140
brought into a form that makes it
usable for analysis and decision making.

21
00:01:16,640 --> 00:01:19,970
So what are the problems with
e traditional ETL systems?

22
00:01:20,750 --> 00:01:22,820
So one is of course complexity.

23
00:01:23,540 --> 00:01:28,320
so the e the extract part of
it, for example, involves.

24
00:01:29,245 --> 00:01:31,825
Getting data out of a
variety of different sources.

25
00:01:32,045 --> 00:01:36,125
these sources could be scattered,
they could have data in a form

26
00:01:36,125 --> 00:01:37,445
that is not readily usable.

27
00:01:37,945 --> 00:01:40,975
data extraction involves,
someone writing scripts.

28
00:01:41,095 --> 00:01:43,135
Of course there are tools and
platforms that can be used for this,

29
00:01:43,135 --> 00:01:47,755
but ultimately it involves, Getting
a sense of what shape that data is

30
00:01:47,755 --> 00:01:49,645
currently in and getting it out.

31
00:01:50,145 --> 00:01:52,525
of course, a lot of times the
data could be just completely

32
00:01:52,525 --> 00:01:57,775
unstructured and there is no non-AI
way of doing this efficiently.

33
00:01:58,325 --> 00:02:01,895
typically it would involve a lot
of human input in, excuse me, in

34
00:02:01,895 --> 00:02:05,035
terms of tagging the information,
getting data points out of it.

35
00:02:05,585 --> 00:02:08,945
the data quality of the data as we
get it out of these different system

36
00:02:08,945 --> 00:02:13,445
in the consolidate them could be
low, because the shape of the data,

37
00:02:13,925 --> 00:02:16,115
in due course would keep evolving.

38
00:02:16,895 --> 00:02:19,535
So the scripts that were originally
written to extract the data.

39
00:02:20,365 --> 00:02:24,755
might need to be tweaked on an
ongoing basis, might, might develop

40
00:02:24,755 --> 00:02:26,075
problems on an ongoing basis.

41
00:02:26,575 --> 00:02:30,045
And of course, there could be scaling
challenges because it's hard to

42
00:02:30,045 --> 00:02:35,065
predict ahead of time what kind of
infrastructure one needs, to build the

43
00:02:35,065 --> 00:02:38,995
EDL pipeline as the data volume grows.

44
00:02:39,625 --> 00:02:43,434
or shrinks, your need
for a specific, level of,

45
00:02:43,895 --> 00:02:45,215
infrastructure would also change.

46
00:02:45,715 --> 00:02:50,515
So there is ongoing sort of input
and tweaking needed to what hardware,

47
00:02:50,825 --> 00:02:52,655
is needed to run your pipeline.

48
00:02:53,255 --> 00:02:57,975
And all of this requires, ongoing
manual input and manual work.

49
00:02:58,475 --> 00:03:00,695
So, Hello, ai.

50
00:03:01,204 --> 00:03:03,314
so AI helps it to broad ways.

51
00:03:04,244 --> 00:03:09,094
Using ai, we can build a better
pipeline, by improving the way

52
00:03:09,094 --> 00:03:12,454
they extract, transform, and
load steps of the pipeline work.

53
00:03:12,954 --> 00:03:17,944
Using ai, we can also, improve the
way the pipeline runs, meaning.

54
00:03:18,449 --> 00:03:21,489
We can, provision our
infrastructure in a better way.

55
00:03:21,609 --> 00:03:23,049
We can monitor it in a better way.

56
00:03:23,290 --> 00:03:26,079
We can make sure it stays performant.

57
00:03:26,630 --> 00:03:29,870
alright, so let's look at the
extract part of the process.

58
00:03:30,370 --> 00:03:33,969
So we know that, the extract
step involves getting data

59
00:03:33,969 --> 00:03:35,229
from all the different sources.

60
00:03:35,779 --> 00:03:38,479
this data may be in a
variety of different formats.

61
00:03:38,979 --> 00:03:43,559
And, could have different schemas
that needs to now be joined together,

62
00:03:43,799 --> 00:03:45,299
create a consolidated view of it.

63
00:03:45,799 --> 00:03:48,299
This is a, this is a manual process.

64
00:03:48,299 --> 00:03:48,869
Traditionally.

65
00:03:49,289 --> 00:03:49,709
Yes.

66
00:03:49,859 --> 00:03:51,074
You could, you have tools for this.

67
00:03:51,574 --> 00:03:54,034
so it's not always scripting,
although it could be scripting.

68
00:03:54,534 --> 00:03:58,454
there are visual tools that you can
use, but ultimately, someone with

69
00:03:59,054 --> 00:04:02,844
techno functional knowledge, needs
to look at the different data sources

70
00:04:03,024 --> 00:04:05,054
and say, what element is what?

71
00:04:05,239 --> 00:04:05,269
Okay.

72
00:04:05,769 --> 00:04:10,079
and what is the best way to extract it
for all this means that the processing

73
00:04:10,079 --> 00:04:14,169
speed for getting data out of those
systems, isn't as high as it could be.

74
00:04:14,719 --> 00:04:19,044
'cause of all the, manual
input and because of all the

75
00:04:19,044 --> 00:04:20,154
different types of data that.

76
00:04:20,524 --> 00:04:21,334
I need to be extracted.

77
00:04:21,834 --> 00:04:26,374
So AI is a natural fit, for doing
this job because AI can adapt to

78
00:04:26,794 --> 00:04:30,184
not just a different shape of,
data that it may be in, at a given

79
00:04:30,184 --> 00:04:31,624
point, but how it's evolving.

80
00:04:32,124 --> 00:04:35,724
Alright, two high level ways
in which AI helps with this.

81
00:04:36,204 --> 00:04:36,474
One.

82
00:04:36,474 --> 00:04:39,844
AI is a natural fit for, processing
unstructured information.

83
00:04:40,534 --> 00:04:44,034
So if the data that we're getting is
not structured or it is semi-structured,

84
00:04:44,964 --> 00:04:48,884
AI can make sense of it and help
extract, the different data elements

85
00:04:48,884 --> 00:04:50,024
or different data entities from it.

86
00:04:50,524 --> 00:04:51,819
AI is also good at.

87
00:04:52,164 --> 00:04:56,004
looking at data source whose
structure you may not know.

88
00:04:56,704 --> 00:05:00,334
so for example, there may be a webpage
or a PDF document or a document for

89
00:05:00,334 --> 00:05:04,594
another kind, which has some kind of
a structure that may change with time.

90
00:05:05,554 --> 00:05:07,204
but AI is a natural fit for.

91
00:05:07,559 --> 00:05:10,589
Figuring out that structure
so that data can be extracted

92
00:05:10,589 --> 00:05:11,819
from it in an optimal way.

93
00:05:12,319 --> 00:05:15,584
so, In terms of, transformation.

94
00:05:16,084 --> 00:05:20,644
So a typically pipeline has a bunch
of transformation steps where we are

95
00:05:20,694 --> 00:05:22,824
looking for problems with the data.

96
00:05:23,454 --> 00:05:26,634
We're looking for, data that might
be missing or in an incorrect

97
00:05:26,634 --> 00:05:28,764
format or an unexpected format.

98
00:05:29,264 --> 00:05:30,704
AI can handle all those things.

99
00:05:31,004 --> 00:05:35,964
So for example, using AI we can build a
model for what, the data should look like.

100
00:05:36,464 --> 00:05:39,884
And if the data is in a different,
shape or has different, values that

101
00:05:39,884 --> 00:05:44,664
are, outside ranges that they typically
are in, then AI can flag them.

102
00:05:45,164 --> 00:05:48,134
AI can also notice that some
data that is typically present

103
00:05:48,194 --> 00:05:50,984
is not present, or some data that
should be present is not present.

104
00:05:51,484 --> 00:05:51,634
Yeah.

105
00:05:51,634 --> 00:05:55,234
He is also good at handling
data that may be available,

106
00:05:55,264 --> 00:05:57,634
but in a non-standard format.

107
00:05:58,449 --> 00:06:02,959
So I can do mapping from, the
current format to the expected

108
00:06:02,959 --> 00:06:04,849
format so it can be extracted.

109
00:06:05,349 --> 00:06:09,999
AI can also identify that, certain
data ranges or certain data values are

110
00:06:10,059 --> 00:06:12,039
outside their expected range, outliers.

111
00:06:12,099 --> 00:06:19,749
So I can, AI can identify those
things and uh, of course AI can also.

112
00:06:20,084 --> 00:06:22,874
put human in the loop if needed.

113
00:06:23,274 --> 00:06:27,769
So if there is some, something that,
that are so outside the range that Yeah.

114
00:06:27,769 --> 00:06:31,129
Doesn't know how to handle it, then
it's possible to build it in a way

115
00:06:31,129 --> 00:06:34,179
so that yeah, it pulls in a human
being to say, Hey, look at this.

116
00:06:34,179 --> 00:06:36,939
This is something that I don't
know how to handle or I do.

117
00:06:37,439 --> 00:06:37,799
Alright.

118
00:06:38,299 --> 00:06:45,490
Also while loading the data, AI can figure
out the right, infrastructure that is

119
00:06:45,490 --> 00:06:47,799
needed to perform the load operation.

120
00:06:48,380 --> 00:06:53,555
for example, if, there is a data
spike or there is, less data.

121
00:06:53,960 --> 00:06:55,979
Then one, typically is
present in the pipeline.

122
00:06:56,480 --> 00:07:00,520
Then AI can, provision infrastructure
or scale up and scale down the

123
00:07:00,520 --> 00:07:04,640
infrastructure in such a way so that
data, a load step, happens optimally.

124
00:07:05,140 --> 00:07:10,300
I. if there are any errors during the
load process, then AI is typically able

125
00:07:10,300 --> 00:07:14,850
to handle things like, if a data type
is incorrect, if a number is stored

126
00:07:14,850 --> 00:07:20,060
as a string, things like that, then
AI is able to figure out how to handle

127
00:07:20,060 --> 00:07:21,980
it so that it is loaded correctly.

128
00:07:22,480 --> 00:07:23,200
And of course.

129
00:07:23,205 --> 00:07:27,575
Even in the load process, AI can
smartly bring a human in the loop

130
00:07:28,205 --> 00:07:33,185
if the load process is not going as
planned, or if things are happening

131
00:07:33,185 --> 00:07:34,235
that AI is unable to handle.

132
00:07:34,735 --> 00:07:38,055
Okay, now let's look at some of
the ways in which, AI can help us

133
00:07:38,205 --> 00:07:40,185
run our pipeline in a better way.

134
00:07:40,685 --> 00:07:41,020
All right.

135
00:07:41,520 --> 00:07:48,800
So using, using ai we can
build a model for, the kind of.

136
00:07:49,315 --> 00:07:52,045
failures that one can see.

137
00:07:52,415 --> 00:07:58,095
for example, AI can put in logs and
time series data from a variety of

138
00:07:58,095 --> 00:08:03,375
different sources and a build a model
for, the kind of failures and what

139
00:08:03,375 --> 00:08:05,025
can expect in a certain situation.

140
00:08:05,925 --> 00:08:11,795
And then it can proactively, flag it so
that we know that, a failure might happen

141
00:08:12,305 --> 00:08:16,815
or that there is a need for, a human being
to monitor certain parts of the process.

142
00:08:17,365 --> 00:08:21,175
so it can figure out these patterns
by looking at information from a

143
00:08:21,175 --> 00:08:26,485
variety of sources, from logs, from
sensor data, and it can identify

144
00:08:26,485 --> 00:08:28,945
patterns and raise these kind of flags.

145
00:08:29,445 --> 00:08:34,085
using ai, we can do some of these things
proactively, that would typically happen

146
00:08:34,085 --> 00:08:38,005
in a traditional pipeline, retroactively
when something goes wrong, right?

147
00:08:38,005 --> 00:08:43,325
And then using all these signals, AI
can figure out what the right, level of

148
00:08:43,325 --> 00:08:45,545
infrastructure is for running a pipeline.

149
00:08:46,355 --> 00:08:50,445
So it can, for example, suggest
that, some compute needs to

150
00:08:50,445 --> 00:08:51,825
be scaled up or scaled down.

151
00:08:52,325 --> 00:08:56,075
Or storage needs to be scaled up
or scaled down because the data

152
00:08:56,075 --> 00:08:59,545
that is coming through the pipeline
is, more or has a shape that is

153
00:08:59,545 --> 00:09:01,675
different from what it typically is.

154
00:09:02,175 --> 00:09:06,545
It, so in this way, AI can help
identify bottlenecks and help

155
00:09:06,545 --> 00:09:08,325
our pipeline run, smoothly.

156
00:09:08,825 --> 00:09:14,265
of course this also helps with cost
optimization, because with AI we can

157
00:09:14,315 --> 00:09:19,495
right size our AI pipeline, our sort
of our ETL, pipeline, infrastructure.

158
00:09:20,455 --> 00:09:22,495
So we are not over provisioning
or under provisioning.

159
00:09:22,995 --> 00:09:25,995
okay, so there are a number of
different tools and platforms

160
00:09:25,995 --> 00:09:28,925
that are available today, or this
doesn't need to be built by hand.

161
00:09:29,425 --> 00:09:32,795
so here's a small, snapshot of some
the popular tools that exist out there.

162
00:09:33,335 --> 00:09:36,775
And depending upon what we are trying to
achieve, or rather what we are trying to

163
00:09:36,775 --> 00:09:39,175
focus on, we can pick different tools.

164
00:09:39,885 --> 00:09:40,515
so for example.

165
00:09:41,165 --> 00:09:45,545
there's a provided of tools like
AWS Glue, which of course works,

166
00:09:45,615 --> 00:09:50,215
with AWS, but if that's where our
solution is, then, AWS glue gives us

167
00:09:50,215 --> 00:09:52,195
a variety of different, capabilities.

168
00:09:52,555 --> 00:09:57,965
We can, create our models, do mapping
and, number of steps of our pipeline can

169
00:09:57,965 --> 00:10:00,115
be hosted in automated using AWS group.

170
00:10:00,615 --> 00:10:02,295
if if you wanna go the open source route.

171
00:10:03,020 --> 00:10:07,140
Air Byte is an excellent tool that
also lets us do, data processing.

172
00:10:07,140 --> 00:10:11,790
We can create models, we can process
unstructured information using air byte.

173
00:10:12,290 --> 00:10:12,650
Alright.

174
00:10:13,150 --> 00:10:15,590
so yeah, so how can we,
get started with it?

175
00:10:16,140 --> 00:10:19,080
so one obvious low hanging
fruit that, that we can

176
00:10:19,080 --> 00:10:21,520
incorporate in our, EL pipelines.

177
00:10:21,880 --> 00:10:23,200
It's the use of sentiment analysis.

178
00:10:23,200 --> 00:10:26,500
So if we are dealing with unstructured
information that doesn't have sentiment

179
00:10:26,500 --> 00:10:30,340
data, then there are a variety of
different tools and algorithms that

180
00:10:30,340 --> 00:10:33,550
we can use to, tag the incoming data.

181
00:10:33,550 --> 00:10:37,010
Or even if it is semi-structured,
we can tag it with, sentiment

182
00:10:37,010 --> 00:10:39,910
data that subsequent steps
in our pipeline can then use.

183
00:10:40,410 --> 00:10:44,070
We can also look at, using AI to plug in.

184
00:10:44,460 --> 00:10:45,840
Holes that may be in our data.

185
00:10:45,840 --> 00:10:50,760
For example, some information that may
be missing can be plugged in using ai.

186
00:10:51,260 --> 00:10:56,000
An excellent use case for, AI in,
in pipelines is fraud detection.

187
00:10:56,990 --> 00:11:01,420
most of us would have, face a situation
where our, credit card provider suddenly

188
00:11:01,420 --> 00:11:04,420
says, Hey, this transaction card go
through because it looks fraudulent.

189
00:11:04,510 --> 00:11:05,380
So how do they look?

190
00:11:05,890 --> 00:11:07,090
How do they know it looks fraudulent?

191
00:11:07,750 --> 00:11:11,130
It's because they're able to, because they
have so much information that they're able

192
00:11:11,130 --> 00:11:16,690
to see that, hey, for this user, this is
typically where transactions are made, or

193
00:11:16,690 --> 00:11:22,120
this kind of, this amount ranges is, is
where the transaction typically lies in.

194
00:11:22,120 --> 00:11:24,850
And then if some, if it's something
outside that, then they can flag it.

195
00:11:25,330 --> 00:11:28,180
So that's another example
of, using anomaly detection.

196
00:11:29,060 --> 00:11:30,680
to detect if a fraud is happening.

197
00:11:31,180 --> 00:11:33,030
Alright, so we've come to the end.

198
00:11:33,340 --> 00:11:37,330
AI is a natural fit for making
our EDL pipelines better for

199
00:11:37,330 --> 00:11:38,290
running them in a better way.

200
00:11:39,160 --> 00:11:47,060
Of course, some things need to be kept
in mind security, so dealing with, ai.

201
00:11:47,560 --> 00:11:51,910
In a way that, the data that we store
with in our AI model is only accessible

202
00:11:51,910 --> 00:11:57,820
to certain users, is a, is something that
needs to be paid attention to, right?

203
00:11:58,450 --> 00:12:02,920
we shouldn't just throw our data
into, to AI and say, Hey, handle it.

204
00:12:03,710 --> 00:12:07,430
security safeguards need to be built
into it so that the right users

205
00:12:07,430 --> 00:12:09,510
are able to access, the right data.

206
00:12:10,010 --> 00:12:14,930
building these systems, of course,
requires newer skill sets, so

207
00:12:14,930 --> 00:12:16,800
everybody, needs to understand.

208
00:12:16,800 --> 00:12:19,800
AI needs to learn how to incorporate
AI in our day-to-day work.

209
00:12:20,300 --> 00:12:25,230
AI power automation is, is super useful
for all kinds of organizations, not

210
00:12:25,230 --> 00:12:30,900
just ETL, but, AI powered workflows
enable, so many different use cases.

211
00:12:31,400 --> 00:12:35,600
for example, it's possible to create
workflows that have some human

212
00:12:35,600 --> 00:12:39,890
steps, some AI steps, and some sort
of imperative programming steps.

213
00:12:40,390 --> 00:12:45,530
And, most workflows, of the
future are gonna be some hybrid

214
00:12:45,830 --> 00:12:47,930
of human AI and imperative steps.

215
00:12:48,430 --> 00:12:52,420
Of course, AI also is a natural
fit for real time analytics.

216
00:12:52,930 --> 00:12:56,860
rather than the traditional approach
of accumulating data in a warehouse,

217
00:12:56,910 --> 00:13:01,310
pre-processing it and generating
report from it, AI is able to extract

218
00:13:01,310 --> 00:13:06,430
information from large amounts of data
with much less pre-processing, that

219
00:13:06,430 --> 00:13:07,925
a traditional e system would need.

220
00:13:08,425 --> 00:13:11,180
All right, so we've come to the end.

221
00:13:12,080 --> 00:13:14,160
I hope you enjoyed this, talk.

222
00:13:14,715 --> 00:13:18,075
And I hope you find it useful as
you build out your next ETL system.

223
00:13:18,615 --> 00:13:18,975
Thank you.

