1
00:00:00,500 --> 00:00:00,680
Speaker 46: Hello.

2
00:00:00,919 --> 00:00:01,940
Good morning, good evening.

3
00:00:02,090 --> 00:00:06,175
My name is, I'm working as a senior
technical manager in HC Tech.

4
00:00:07,060 --> 00:00:11,399
Today I'm going to discuss about
operationalizing predict Analytics

5
00:00:11,399 --> 00:00:16,225
in education by combining lops
DevOps strategies for scalable

6
00:00:16,225 --> 00:00:17,695
ethical students success.

7
00:00:18,115 --> 00:00:19,345
So first, what is predict?

8
00:00:20,025 --> 00:00:24,665
So predicting unknown future by using
techniques like data mining, machine

9
00:00:24,665 --> 00:00:28,295
learning by analyzing the current
data to create the future outcome.

10
00:00:28,795 --> 00:00:33,775
So Yammer Labs is a displaying
that applies DevOps principles in

11
00:00:33,775 --> 00:00:35,035
the machine learning lifecycle.

12
00:00:35,515 --> 00:00:37,645
So we can see this in this session.

13
00:00:38,145 --> 00:00:43,005
So the educational data challenge
opportunity, so digital learning

14
00:00:43,005 --> 00:00:47,295
platforms not only generates
grade and attendance and also

15
00:00:47,295 --> 00:00:51,455
generate huge volumes of behavioral
performance and engagement data.

16
00:00:52,184 --> 00:00:57,965
So always raw data does not deliver any
meaningful sites if we harness it well.

17
00:00:58,485 --> 00:01:02,684
So this data can help us to
predict challenges yearly and also

18
00:01:02,694 --> 00:01:04,134
improve student success at scale.

19
00:01:04,634 --> 00:01:09,994
So the reality educational institutions
struggle to transform preq models into

20
00:01:09,994 --> 00:01:11,884
relatable production ready systems.

21
00:01:12,395 --> 00:01:16,594
That's a gap between experimental
insights and operational deployment.

22
00:01:17,085 --> 00:01:21,345
Which are robust DevOps
and MOPS practices.

23
00:01:21,675 --> 00:01:24,255
So there are some advantages of ML lops.

24
00:01:24,645 --> 00:01:30,215
So the ML lops brings reproducibility
so we can test the results of our model.

25
00:01:31,015 --> 00:01:35,545
Then it also brings scalability
and monitoring and governance.

26
00:01:36,045 --> 00:01:38,595
So from experimentation
to operational impact.

27
00:01:39,175 --> 00:01:42,745
So experimental models is happening
in the development phase, like

28
00:01:43,015 --> 00:01:47,785
proof of concept analytics in the
notebooks and sandbox environments.

29
00:01:48,535 --> 00:01:53,245
So in production systems the models can
be scalable for a huge volume of data.

30
00:01:54,019 --> 00:01:58,529
And can be monitored and gone by
ML pipelines integrated with the

31
00:01:58,529 --> 00:02:02,969
different educational workflows
and some of the measurable outcomes

32
00:02:02,969 --> 00:02:08,009
or improved student retention rate
engagement and student success at scale.

33
00:02:08,509 --> 00:02:12,469
So this session bridges the gap between
data science, experimentation and

34
00:02:12,649 --> 00:02:14,294
DevOps driven production deployment.

35
00:02:14,794 --> 00:02:17,044
So the machine learning
toolkit for education.

36
00:02:17,504 --> 00:02:21,764
There are many models available to
make predictions in educational domain.

37
00:02:22,334 --> 00:02:25,064
So here I highlighted
some of the key models.

38
00:02:25,514 --> 00:02:27,584
The first one is binary classification.

39
00:02:28,064 --> 00:02:31,994
So binary classification is a
supervised learning technique.

40
00:02:32,579 --> 00:02:39,599
It's used to break categorical outcomes,
like pass or fail at risk, or not at risk.

41
00:02:39,649 --> 00:02:44,600
So in those scenarios, so logistic
regression is a statistical model

42
00:02:45,179 --> 00:02:50,609
used to break probably outcomes of
a no patents by using input factors.

43
00:02:50,660 --> 00:02:55,005
So logistic regression can be used for
possible predictions of the students.

44
00:02:55,335 --> 00:02:55,365
Okay.

45
00:02:56,325 --> 00:02:58,475
The next one is ensemble methods.

46
00:02:58,775 --> 00:03:03,185
So random forest and grade interesting
are the high performance models.

47
00:03:03,785 --> 00:03:07,995
So the random forest models make
predictions by combining the

48
00:03:07,995 --> 00:03:09,835
results of many individual trees.

49
00:03:10,355 --> 00:03:15,585
So here the prediction is by averaging
for progression models and by majority

50
00:03:15,585 --> 00:03:17,455
work for classification models.

51
00:03:18,055 --> 00:03:21,815
So the goal is to reduce the
variance, which is over fitting.

52
00:03:22,435 --> 00:03:26,365
And the gradient boost make
predictions by combining many

53
00:03:26,365 --> 00:03:28,405
beacon models in a shake shell way.

54
00:03:28,955 --> 00:03:32,785
So here the final predictions
is by the sum of all the trees.

55
00:03:33,235 --> 00:03:36,385
So the goal is to reduce
bias, which is under 30.

56
00:03:36,885 --> 00:03:40,595
So the next one is networks,
which is time dependent model.

57
00:03:41,155 --> 00:03:45,605
So long short term memory and
recurrent neural network accesses

58
00:03:45,665 --> 00:03:47,765
capture temporal landing patterns.

59
00:03:48,184 --> 00:03:51,865
So this model can be used for
multi semester performance

60
00:03:51,955 --> 00:03:53,515
trajectories of the students

61
00:03:54,015 --> 00:03:56,025
see advanced analytical approaches.

62
00:03:56,075 --> 00:03:57,695
So unsupervised learning.

63
00:03:58,280 --> 00:04:02,180
So in unsupervised learning
usually we don't have any specific

64
00:04:02,180 --> 00:04:06,890
target for predictions, but the
grouping and clustering techniques

65
00:04:06,890 --> 00:04:09,770
are any new or complex patterns.

66
00:04:10,230 --> 00:04:14,340
The, there are a couple of key techniques
came means and hierarchal methods.

67
00:04:14,760 --> 00:04:19,950
So the K means is used to group students
in a cluster by similarity levels.

68
00:04:20,745 --> 00:04:24,325
And hierarchal methods is used
to group students students at

69
00:04:24,325 --> 00:04:25,765
the different similarity levels.

70
00:04:25,815 --> 00:04:27,405
So basically it's nested.

71
00:04:27,905 --> 00:04:32,175
So neutral language processing is
used for sentiment analysis pipelines.

72
00:04:32,695 --> 00:04:37,185
So in natural language process the
unstructured data is transformed

73
00:04:37,185 --> 00:04:40,445
into such a format to make some.

74
00:04:40,895 --> 00:04:44,335
To make some decisions and
performance forecasting.

75
00:04:44,385 --> 00:04:48,885
So gradient boosting models predict
final outcomes early in the term.

76
00:04:49,385 --> 00:04:53,755
So here we can see some of the critical
factors determining predict accuracy.

77
00:04:54,355 --> 00:04:57,435
So the first one is data
quality and complexness.

78
00:04:57,490 --> 00:05:02,510
So during any pre-processing steps, if we
see numeric missing values we can impute

79
00:05:02,510 --> 00:05:08,010
with mean values and any categorical
values can be labeled with unknown.

80
00:05:08,430 --> 00:05:12,390
And also there is some normalization
techniques to form the data

81
00:05:12,390 --> 00:05:13,980
consistent across records.

82
00:05:14,490 --> 00:05:17,190
Some of the techniques are
zero to one transforms.

83
00:05:17,250 --> 00:05:19,530
Z transforms under long transforms.

84
00:05:20,280 --> 00:05:22,650
The next one is future
relevance and engineering.

85
00:05:23,310 --> 00:05:27,650
See raw data, transform raw
data into meaningful features.

86
00:05:27,680 --> 00:05:32,570
So those features is very useful
to make accurate predictions.

87
00:05:33,230 --> 00:05:37,180
So temporal considerations it's
it's for a time-based data.

88
00:05:37,700 --> 00:05:42,620
So that timeline based data should
be lined properly in the right

89
00:05:42,620 --> 00:05:44,090
order with the right timings.

90
00:05:44,590 --> 00:05:48,160
The next one is sample
size and class balance.

91
00:05:48,740 --> 00:05:53,060
So sample size refers to the
data points to train the model.

92
00:05:53,630 --> 00:05:57,720
So always the large samples give
reliable patterns and higher

93
00:05:57,720 --> 00:06:03,140
accuracy and smaller samples
increased noise noise and or fitting.

94
00:06:03,640 --> 00:06:06,950
So the next one is model
complexity and interpretability.

95
00:06:07,589 --> 00:06:11,759
High complex models can capture
can capture relatable patterns

96
00:06:11,819 --> 00:06:13,829
and also achieve higher accuracy.

97
00:06:14,489 --> 00:06:18,399
And the interpretability is the
key in educational domain which,

98
00:06:18,549 --> 00:06:22,044
which shows which shows the
relationship between input and output.

99
00:06:22,544 --> 00:06:26,759
So here we can see the evolution metrics
for educational predictive models.

100
00:06:27,299 --> 00:06:31,799
So selecting the right metrics is crucial
for evaluating any predictive models.

101
00:06:32,069 --> 00:06:35,644
So here we can see like the
classification materials is used to

102
00:06:35,994 --> 00:06:37,459
evaluate the classification models.

103
00:06:37,969 --> 00:06:43,419
So here the Confucian metrics shows how
well a classification model performs

104
00:06:43,509 --> 00:06:46,609
by comparing ax actual, and, al labels.

105
00:06:47,059 --> 00:06:50,479
So these four labels are created
from the Confucian Matrix.

106
00:06:50,599 --> 00:06:55,389
This is the foundation to evaluate any
classification materials listed here.

107
00:06:55,659 --> 00:07:00,629
Accuracy, precision recall,
F1 score, ROC, and a OC.

108
00:07:01,129 --> 00:07:05,325
So here we can see some of the metrics
used to evaluate regression models.

109
00:07:05,825 --> 00:07:10,085
So this metrics is used to measure
how well the model predicts

110
00:07:10,114 --> 00:07:15,335
continuous value and also how close
predictions are to actual outcomes.

111
00:07:15,604 --> 00:07:18,504
Some of the metrics are
mean, means quite error.

112
00:07:18,835 --> 00:07:22,195
Root means quite error and
coefficient of determination.

113
00:07:22,695 --> 00:07:26,905
So here we can see DevOps foundations
for machine learning success.

114
00:07:27,295 --> 00:07:31,435
So any machine learning systems
deliver real value when they are

115
00:07:31,435 --> 00:07:33,505
built on strong DevOps principles.

116
00:07:34,105 --> 00:07:36,930
So the first principle is data versioning.

117
00:07:37,480 --> 00:07:39,385
So here the data set so there.

118
00:07:40,180 --> 00:07:44,790
It's used to track dataset
lineage, and any schema changes

119
00:07:44,850 --> 00:07:46,440
and any transformations.

120
00:07:46,770 --> 00:07:50,570
So the tools like DVC data
version control it's similar to

121
00:07:50,570 --> 00:07:53,110
GI for YAML and data pipelines.

122
00:07:53,530 --> 00:07:58,350
It's used to version dataset
and also it's used to track any

123
00:07:58,350 --> 00:08:00,800
changes in data futures and models.

124
00:08:01,300 --> 00:08:05,290
The ML flow is a machine learning
lifecycle machine learning

125
00:08:05,410 --> 00:08:10,630
lifecycle flow to track to
track package and deploy models.

126
00:08:11,130 --> 00:08:13,620
Next one is future engineering pipelines.

127
00:08:14,190 --> 00:08:16,900
So here this is used the future logic.

128
00:08:17,545 --> 00:08:22,315
This is here is to ensure the future
logic is consistent with being

129
00:08:22,415 --> 00:08:24,634
training and inference environments.

130
00:08:24,844 --> 00:08:26,915
The next one is C-A-C-A-D.

131
00:08:26,915 --> 00:08:31,624
Pipeline for machine learning is
used to implement automated testing,

132
00:08:31,805 --> 00:08:34,154
validation, and deployment workflows.

133
00:08:35,114 --> 00:08:36,944
Next one is model monitoring.

134
00:08:37,394 --> 00:08:43,364
If any model is deployed in prediction
and so that should be continuously

135
00:08:43,364 --> 00:08:48,634
monitored to track prediction
latency, net quality, and directions.

136
00:08:49,134 --> 00:08:53,494
So here we can see the typical ML
ops, DevOps pipeline architecture.

137
00:08:53,854 --> 00:08:57,864
So there are six different core
layers of this architecture.

138
00:08:58,599 --> 00:09:00,839
So the first one is data management.

139
00:09:01,469 --> 00:09:06,649
So this layer choose the data used in
the training and inference environments.

140
00:09:06,679 --> 00:09:10,579
It's properly ent and
validated and also documented.

141
00:09:11,419 --> 00:09:14,269
So the next layer is model development.

142
00:09:14,839 --> 00:09:18,969
So this is where experiment
experimentation happens in a

143
00:09:18,969 --> 00:09:24,189
reproducible way and also model ing
and parameter units are happening here.

144
00:09:24,689 --> 00:09:26,189
So model deployment.

145
00:09:26,289 --> 00:09:29,169
So this is where DevOps
meets machine learning.

146
00:09:29,709 --> 00:09:34,929
So ca creating ca CID pipelines
for morals, and also creating

147
00:09:34,979 --> 00:09:38,969
automated testing gates and
creating development strategies.

148
00:09:39,469 --> 00:09:41,719
And the next layer is model monitoring.

149
00:09:41,849 --> 00:09:44,719
This is the most important
layer of machine learning.

150
00:09:45,389 --> 00:09:49,789
So here we can track performance,
drip fairness, and alerts.

151
00:09:50,289 --> 00:09:52,719
The next layer is in
protection and automation.

152
00:09:52,819 --> 00:09:55,159
This is the operational backbone layer.

153
00:09:55,789 --> 00:09:59,919
So the containers are used
to bundle bundle codes and

154
00:09:59,919 --> 00:10:01,479
dependencies and libraries.

155
00:10:01,979 --> 00:10:07,859
And then the process is used to automate
model deployments and auto-scaling

156
00:10:07,859 --> 00:10:09,919
and load balancing processes.

157
00:10:10,419 --> 00:10:14,389
So governance and compliance
is the key in educational any

158
00:10:14,389 --> 00:10:16,129
many other regulated domains.

159
00:10:16,789 --> 00:10:20,869
So to implement privacy
and audit processes.

160
00:10:21,499 --> 00:10:23,449
So that's all about this architecture.

161
00:10:23,949 --> 00:10:28,419
So here we can see some of the
infrastructure key points that show

162
00:10:28,469 --> 00:10:31,139
scalable and there is reliable ML systems.

163
00:10:31,189 --> 00:10:32,929
The first one is performance.

164
00:10:33,389 --> 00:10:37,979
So unique high performance models
and real time inference systems.

165
00:10:38,059 --> 00:10:43,299
Usually require more GPUs on autoscaling
services compared to batch pipelines.

166
00:10:44,154 --> 00:10:46,644
So the next one is cost efficiency.

167
00:10:47,164 --> 00:10:48,574
The cost efficiency.

168
00:10:48,584 --> 00:10:53,444
It's about delivering impact without
overspending on any compute services.

169
00:10:53,964 --> 00:10:58,294
So in this case serverless and spot
instances or good considerations.

170
00:10:59,059 --> 00:11:03,589
And reliability, student facing
systems require high availability

171
00:11:04,189 --> 00:11:08,789
and any failover in the system
should be up and operating quickly.

172
00:11:09,289 --> 00:11:11,689
So detecting and managing model drip.

173
00:11:12,109 --> 00:11:15,679
So while drip meters and
syndication, so usually the models

174
00:11:15,679 --> 00:11:17,119
are trained on historical data.

175
00:11:17,749 --> 00:11:21,359
So whenever any new data arrives
in the production environment,

176
00:11:21,959 --> 00:11:23,549
the model performs poorly.

177
00:11:24,069 --> 00:11:28,649
So deep direction systems are used to
compare statistical properties of training

178
00:11:28,649 --> 00:11:30,979
data against production inference.

179
00:11:31,339 --> 00:11:34,389
So if there is any deviation,
it is flagged before it

180
00:11:34,389 --> 00:11:36,069
impact any production quality.

181
00:11:36,569 --> 00:11:41,729
So governance, so always fairness,
transparency and privacy are

182
00:11:41,729 --> 00:11:46,439
essential because they ensure our
systems treat equally everyone.

183
00:11:46,904 --> 00:11:53,284
So the first one is fairness welded model
across different subgroup subgroups and

184
00:11:53,284 --> 00:11:56,364
also implement bias mitigation techniques.

185
00:11:56,454 --> 00:11:58,254
The next one is transparency.

186
00:11:58,794 --> 00:12:05,124
So transparency is to show how our model
works and also show how and why prediction

187
00:12:05,154 --> 00:12:10,914
occurs so that the stakeholders can
understand it and trust the decisions.

188
00:12:11,414 --> 00:12:15,954
So we can deploy explainable
techniques like shaft and lime.

189
00:12:16,454 --> 00:12:20,904
So the next one is privacy, is
to implement implement secure

190
00:12:20,904 --> 00:12:24,864
and restricted role-based access
controls to comply with ffa.

191
00:12:25,254 --> 00:12:29,744
So FFA is federal law that products
privacy of the student records.

192
00:12:29,804 --> 00:12:32,574
And GDPR, it's for European Union.

193
00:12:33,074 --> 00:12:35,964
So auditability and
responsible deployment.

194
00:12:36,474 --> 00:12:42,634
So model cuts are used to document
how the models build and then and

195
00:12:42,634 --> 00:12:45,824
what is the intended user, the
models so we can document those

196
00:12:45,824 --> 00:12:47,504
information in the model cards.

197
00:12:47,954 --> 00:12:50,579
So for auditability model
cards is the backbone.

198
00:12:51,079 --> 00:12:55,739
And automated validation is used to
verify fairness, privacy, and the

199
00:12:55,739 --> 00:12:58,199
performance before prion deployment.

200
00:12:58,699 --> 00:13:00,469
So the next one is human in the loop.

201
00:13:00,559 --> 00:13:05,779
So human in the loop is also in the
design approach where human remains

202
00:13:05,839 --> 00:13:11,329
actively the key in the key points in
any machine learning and AI decisions.

203
00:13:11,829 --> 00:13:15,639
Audit trials are to log
predictions, sessions and outcomes

204
00:13:15,639 --> 00:13:17,259
for audits and compliance.

205
00:13:17,759 --> 00:13:21,899
So the final step is integration
with the student support services.

206
00:13:22,349 --> 00:13:23,879
So real time decision support.

207
00:13:24,089 --> 00:13:28,689
So any prediction prediction output
flow must be integrated with management

208
00:13:28,689 --> 00:13:30,639
platforms and the dashboards.

209
00:13:31,029 --> 00:13:35,049
So far, these, the a p driven
architecture can be implemented to

210
00:13:35,079 --> 00:13:40,189
integrate data from models, output
to the dashboards, actionable alerts.

211
00:13:40,639 --> 00:13:45,379
So transfer model outputs into
task list for advisors, tutors,

212
00:13:45,379 --> 00:13:46,939
and support for decision making.

213
00:13:47,439 --> 00:13:49,779
Continuous feedback and model evolution.

214
00:13:50,379 --> 00:13:54,899
So this are, this is the standard
steps in any of the many of the

215
00:13:54,899 --> 00:13:56,309
software development lifecycle.

216
00:13:56,819 --> 00:14:00,839
So the first one is deploy,
monitor, collect feedback,

217
00:14:01,049 --> 00:14:02,639
and the last step is irate.

218
00:14:03,239 --> 00:14:07,439
So retain models with updated
data and updated futures

219
00:14:07,479 --> 00:14:09,129
with improved architectures.

220
00:14:10,074 --> 00:14:12,654
So production ML systems
are never finished.

221
00:14:13,074 --> 00:14:17,394
Continuous improvement loops and
choose model remains effective.

222
00:14:17,894 --> 00:14:22,544
Practical guidance for implementation
always start with clear use cases.

223
00:14:22,594 --> 00:14:27,289
Define specific intervention workflows and
success metrics before building models.

224
00:14:27,789 --> 00:14:32,919
Building Cross al teams combined data
scientists develop engineers, domain

225
00:14:32,919 --> 00:14:39,119
experts, and compliance officers invest
in structurally lops tooling monitoring

226
00:14:39,119 --> 00:14:43,819
platforms and governance frameworks,
prioritize ethical considerations,

227
00:14:44,319 --> 00:14:46,374
fairness, transparency, and privacy.

228
00:14:46,874 --> 00:14:50,144
Delivering scalable ethical
impact driven analytics.

229
00:14:50,564 --> 00:14:55,514
See, at the end, operationalizing
operat analytics and education is

230
00:14:55,544 --> 00:15:01,664
by combining strong ops practices
with the ethical governance so that

231
00:15:01,724 --> 00:15:05,379
institutions can deploy the morals
that are scalable and trustworthy.

232
00:15:06,239 --> 00:15:10,519
So this approach measurably improves
student retention rate engagement,

233
00:15:10,519 --> 00:15:12,949
success and success at institution level.

234
00:15:13,449 --> 00:15:14,979
So that's all about this session.

235
00:15:15,309 --> 00:15:20,009
So if record, we can collaborate
together to build analytics system in the

236
00:15:20,009 --> 00:15:24,909
educational domain by combining machine
learning and strong DevOps principles.

237
00:15:25,149 --> 00:15:25,659
Thank you.

