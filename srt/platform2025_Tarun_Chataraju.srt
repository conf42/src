1
00:00:00,500 --> 00:00:01,130
Hello everyone.

2
00:00:01,819 --> 00:00:07,120
This is, and I'll be sharing how
we've modernized a legacy fixed

3
00:00:07,120 --> 00:00:11,920
income index system using cloud
native platform engineering.

4
00:00:12,419 --> 00:00:16,979
This was a major transformation
effort that addressed both technical

5
00:00:17,279 --> 00:00:18,930
and organizational challenges.

6
00:00:19,430 --> 00:00:22,970
I'll walk you through the challenges
we faced, the modernization

7
00:00:22,970 --> 00:00:25,430
strategy, the technical architecture.

8
00:00:25,955 --> 00:00:29,255
The results we achieved and
finally the lessons we learned.

9
00:00:29,755 --> 00:00:35,395
By the end, I hope you will see how
Cloud native engineering can unlock

10
00:00:35,995 --> 00:00:40,555
real performance and agility in a
heavily regulated financial environment.

11
00:00:41,055 --> 00:00:42,915
Here is the roadmap for the presentation.

12
00:00:43,415 --> 00:00:46,714
We will start with the legacy
system challenges that forced

13
00:00:46,714 --> 00:00:47,824
us to rethink our approach.

14
00:00:48,324 --> 00:00:52,995
Then we will go into the modernization
strategy we adopted, followed by

15
00:00:52,995 --> 00:00:54,434
the technical architecture we built.

16
00:00:54,934 --> 00:00:58,884
I'll then share the implementation
journey and results and wrap up with

17
00:00:59,274 --> 00:01:01,254
lessons learned and future directions.

18
00:01:01,754 --> 00:01:03,734
Let's start with the problems
we were trying to solve.

19
00:01:04,234 --> 00:01:06,815
Our legacy fixed income
index system had grown.

20
00:01:07,414 --> 00:01:08,495
Increasingly fragile.

21
00:01:08,995 --> 00:01:14,385
For example, monthly index rebalancing
required longer maintenance dose.

22
00:01:14,865 --> 00:01:18,165
During that time, clients
could not access critical data,

23
00:01:18,675 --> 00:01:20,295
which directly impacted CLIs.

24
00:01:21,135 --> 00:01:25,455
At the same time, data volumes grew
five times in just three years.

25
00:01:26,415 --> 00:01:30,135
The system simply couldn't
process that influx efficiently.

26
00:01:30,635 --> 00:01:33,515
Index calculations often
took 30 minutes or more.

27
00:01:33,890 --> 00:01:38,780
During peak times, which is unacceptable
when unacceptable, when clients

28
00:01:39,230 --> 00:01:42,230
needed near real time results.

29
00:01:42,730 --> 00:01:45,670
Finally, scalability
was a huge limitation.

30
00:01:46,510 --> 00:01:52,510
The monolithic system couldn't handle
sudden spikes in demand, such as quarter

31
00:01:52,510 --> 00:01:56,140
and reporting or market volatility events.

32
00:01:56,640 --> 00:01:58,855
This wasn't just an IT problem.

33
00:01:59,355 --> 00:02:05,265
It directly affected client satisfaction,
SLS, and even revenue opportunities.

34
00:02:05,765 --> 00:02:08,045
Clearly, something had to change.

35
00:02:08,545 --> 00:02:15,655
We needed a strategy that didn't just
patch problems, but Rema, but reimagined

36
00:02:15,685 --> 00:02:18,465
the whole foundation to solve this.

37
00:02:18,585 --> 00:02:21,800
We adopted a platform
engineering approach that meant.

38
00:02:22,510 --> 00:02:26,560
Building a foundation that supported
both stability and innovation.

39
00:02:27,060 --> 00:02:33,030
We could not afford outages, so we moved
step by step with zero downtime goals.

40
00:02:33,530 --> 00:02:37,935
Infrastructure as code was insured,
has insured every enrollment.

41
00:02:38,275 --> 00:02:40,895
Its consistent, automated and auditable.

42
00:02:41,395 --> 00:02:43,785
We applied domain design, for example.

43
00:02:44,285 --> 00:02:49,735
Pricing logic became its own
service decoupled from reporting CSC

44
00:02:49,735 --> 00:02:56,005
pipelines, we introduced automated
pipelines, so code changes went from

45
00:02:56,005 --> 00:02:58,165
weeks of manual promotion to hours.

46
00:02:58,665 --> 00:03:03,345
For instance, we used Docker to
containerize every component, making

47
00:03:03,345 --> 00:03:04,600
it portable and easier to scale.

48
00:03:05,100 --> 00:03:08,940
We redesigned workflows to be a
synchronous, which prevented bottleneck

49
00:03:09,440 --> 00:03:15,770
and security Compliance was integrated
directly into the deployment process,

50
00:03:16,270 --> 00:03:21,580
so previously we had a giant
monolith one code base manually

51
00:03:21,580 --> 00:03:27,030
deployed scaling by buying bigger
boxes and running nightly batches.

52
00:03:27,530 --> 00:03:33,290
Now we run domain based, domain
bounded microservices, deployed with

53
00:03:33,770 --> 00:03:38,829
GitHubs, even driven architecture,
multi-region redundancy, and

54
00:03:39,040 --> 00:03:40,540
auto scaling in Kubernetes.

55
00:03:41,040 --> 00:03:46,795
Think of it like moving from a single
massive mainframe to an ecosystem of

56
00:03:46,800 --> 00:03:51,355
small, specialized teams of workers
who can scale up or down instantly.

57
00:03:51,855 --> 00:03:54,795
This was not just a technology upgrade.

58
00:03:55,295 --> 00:03:58,355
It fundamentally changed how
we could deliver features

59
00:03:58,625 --> 00:04:00,395
and respond to client needs.

60
00:04:00,895 --> 00:04:03,265
Let's break down what
those components look like.

61
00:04:03,765 --> 00:04:05,475
Kubernetes and airflow implementation.

62
00:04:05,975 --> 00:04:10,715
Along alongside Kubernetes,
we introduced Apache Airflow

63
00:04:11,105 --> 00:04:12,815
as our workflow orchestrator.

64
00:04:13,315 --> 00:04:18,535
While Kubernetes handle infrastructure,
scaling airflow gave us a

65
00:04:18,535 --> 00:04:23,035
framework to manage the sequence
of steps in index calculation.

66
00:04:23,535 --> 00:04:28,935
We built containerized adapters to
connect to multiple data providers like

67
00:04:29,035 --> 00:04:32,685
Bloomberg, because they were stateless,
we could scale them independently.

68
00:04:33,185 --> 00:04:38,645
Airflow triggered tasks to pull issuer
and bond metadata from multiple providers.

69
00:04:39,145 --> 00:04:44,695
Next tasks collected real time and
end of day bond prices, ensuring data

70
00:04:44,695 --> 00:04:46,465
quality checks before proceeding.

71
00:04:46,965 --> 00:04:50,345
Calculation engine ports
spun up based on workload.

72
00:04:50,765 --> 00:04:53,765
For example, at month end,
the system ought to scale to

73
00:04:54,245 --> 00:04:55,700
30 ports for calculations.

74
00:04:56,630 --> 00:04:59,380
Then scale down overnight for persistence.

75
00:04:59,440 --> 00:05:05,290
We chose time series databases
that were optimized for financial

76
00:05:05,380 --> 00:05:09,220
tick data and are critical for
both accuracy and performance.

77
00:05:09,720 --> 00:05:15,470
The A PL layer included both rest and
graph QL supporting both diverse clients.

78
00:05:15,970 --> 00:05:20,500
Compliance checks like masking restricted
securities happen directly at the API.

79
00:05:21,000 --> 00:05:27,300
Service mass gave us fine-grain traffic
control, encryption, and observability.

80
00:05:27,800 --> 00:05:32,860
If one service failed, we could trace
exactly where and why airlock orchestrated

81
00:05:33,040 --> 00:05:37,305
the workflow end to end, starting
from ingestion, ingesting difference

82
00:05:37,305 --> 00:05:40,380
data and prices, and validate quality.

83
00:05:40,880 --> 00:05:45,500
The followed by not triggering
calculations on Kubernetes

84
00:05:45,500 --> 00:05:49,000
ports as a third step generating
reports and violations.

85
00:05:49,500 --> 00:05:53,130
Lastly, publishing to APIs and
downstream customers the entire world.

86
00:05:53,160 --> 00:05:55,345
Orchestration was done by airflow.

87
00:05:55,845 --> 00:05:58,515
Airflow gave us a dag view.

88
00:05:59,145 --> 00:06:01,985
Every step was visible,
retriable and auditable.

89
00:06:02,485 --> 00:06:04,945
Using these directed as cyclic graphs.

90
00:06:05,695 --> 00:06:09,895
Let's say if pricing data
failed, calculations paused

91
00:06:09,895 --> 00:06:11,425
automatically until conducted.

92
00:06:11,925 --> 00:06:16,755
Think of Kubernetes as the engine and
airflow as the conductor, making sure

93
00:06:16,755 --> 00:06:18,315
the orchestra plays in the right order.

94
00:06:18,815 --> 00:06:23,240
Together these components form a
resilient and flexible backbone for

95
00:06:23,240 --> 00:06:25,240
our system, critical system components.

96
00:06:26,215 --> 00:06:32,035
Kafka decoupled processing instead of
requests piling up in a queue, Kafka

97
00:06:32,035 --> 00:06:34,555
smoothened our spikes and preserved state.

98
00:06:34,795 --> 00:06:40,955
If one node failed during one stress
test, Kafka handled three times expected

99
00:06:41,165 --> 00:06:43,655
peak loads without dropping a message.

100
00:06:44,155 --> 00:06:47,185
With Redis, we cut database reads by 90%.

101
00:06:48,055 --> 00:06:51,435
That meant sub millisecond
access to frequently used data.

102
00:06:52,125 --> 00:06:54,725
With cross region
replication for resilience,

103
00:06:55,225 --> 00:07:00,445
Kubernetes features like stateful
sets for order deployment, network

104
00:07:00,445 --> 00:07:04,495
policies for regulatory boundaries
and horizontal power auto scaling for

105
00:07:04,495 --> 00:07:11,560
elasticity have been implemented here,
for example, during volatile market days.

106
00:07:12,060 --> 00:07:15,630
The system automatically scaled
to handle 20 times more concurrent

107
00:07:15,630 --> 00:07:18,180
calculations without service degradation.

108
00:07:18,680 --> 00:07:19,040
At simple.

109
00:07:19,040 --> 00:07:24,280
The performance improvements, we saw
an 88% reduction in calculation time.

110
00:07:24,640 --> 00:07:26,950
What used to take 30 minutes
now takes under five.

111
00:07:27,450 --> 00:07:29,130
Scheduled downtime was eliminated.

112
00:07:29,580 --> 00:07:32,640
Clients could access the system
continuously even during rebalancing.

113
00:07:33,140 --> 00:07:37,940
And we could now support 20 times
more concurrent calculations, directly

114
00:07:37,940 --> 00:07:44,815
improving client SLS added with airflow
orchestration, which not only did

115
00:07:44,815 --> 00:07:50,665
calculations run faster, but the entire
workflow pipeline from to publishing

116
00:07:51,165 --> 00:07:52,665
became more reliable and auditable.

117
00:07:53,165 --> 00:07:54,470
These weren't just technical wins.

118
00:07:55,190 --> 00:07:56,600
They directly improved the client.

119
00:07:56,600 --> 00:08:03,070
Ls even reduced operational risk and gave
product teams the ability to launch in

120
00:08:03,070 --> 00:08:05,980
indexes that weren't even possible before.

121
00:08:06,480 --> 00:08:09,750
The migration approach was
executed in four phases.

122
00:08:10,250 --> 00:08:15,255
As a phase one, we set up cloud
infrastructure with compliance baked in.

123
00:08:15,755 --> 00:08:20,605
Infrastructure as a code CICD
and monitoring as phase two.

124
00:08:20,755 --> 00:08:21,955
We migrated historical data.

125
00:08:22,455 --> 00:08:26,865
We used dual right patterns and
built a data validation framework.

126
00:08:27,365 --> 00:08:31,985
Every bond reference data and prices
were checked across all and new systems

127
00:08:32,195 --> 00:08:34,470
before going live as part of phase three.

128
00:08:35,170 --> 00:08:38,560
We decomposed the monolith
extracting services gradually.

129
00:08:38,560 --> 00:08:42,720
Using the strangler pattern, we peeled
services off one by one, running both

130
00:08:42,720 --> 00:08:44,370
in parallel until they are stable.

131
00:08:44,870 --> 00:08:47,960
When we decomposed services,
airflow became the glue

132
00:08:47,960 --> 00:08:49,610
between old and new systems.

133
00:08:50,180 --> 00:08:54,920
For example, the reference data
was still in Legacy database while

134
00:08:54,980 --> 00:08:57,050
calculations moved to Kubernetes.

135
00:08:57,550 --> 00:08:59,365
Airflow orchestrated across both worlds.

136
00:09:00,280 --> 00:09:03,820
During this transition, which
actually let us migrate step by

137
00:09:03,820 --> 00:09:05,770
step without breaking workflows.

138
00:09:06,270 --> 00:09:09,240
As the last phase, we cut over
with blue green deployments.

139
00:09:09,740 --> 00:09:12,770
We ran systems in parallel and
only decommissioned legacy.

140
00:09:12,770 --> 00:09:17,290
After full confidence at every
stage, we made sure business

141
00:09:17,290 --> 00:09:18,550
operations were not stopped.

142
00:09:19,540 --> 00:09:23,560
The key was that clients
never saw disruption for them.

143
00:09:23,620 --> 00:09:24,715
The system just got faster.

144
00:09:25,570 --> 00:09:26,410
And more reliable

145
00:09:26,910 --> 00:09:27,540
technology.

146
00:09:27,690 --> 00:09:29,760
Transformation was only
half the story here.

147
00:09:30,260 --> 00:09:32,150
We also had to transform the organization.

148
00:09:32,650 --> 00:09:35,950
We created a dedicated platform
team that functioned as an

149
00:09:36,370 --> 00:09:37,570
internal service provider.

150
00:09:38,070 --> 00:09:42,630
We invested in developer experience
building self-service portals that

151
00:09:42,630 --> 00:09:44,715
standardized container environments.

152
00:09:45,215 --> 00:09:50,755
Of waiting weeks for servers, developers
had a self-service portal, needed a

153
00:09:50,755 --> 00:09:56,385
database, just one click needed a dev
enrollment done in minutes, we ran

154
00:09:56,505 --> 00:10:02,570
internal tech talks, paid developers with
platform engineers, and created runbooks.

155
00:10:02,570 --> 00:10:07,175
So teams were not dependent on
specialists without these cultural and

156
00:10:07,175 --> 00:10:08,585
organization organizational changes.

157
00:10:09,560 --> 00:10:12,950
The technical success would
not have been sustainable.

158
00:10:13,450 --> 00:10:17,420
Key takeaways, performance
transformation was the major key here.

159
00:10:17,920 --> 00:10:19,840
Overall, we are eight times faster.

160
00:10:20,200 --> 00:10:23,080
We are doing eight, eight times
faster calculations with no downtime.

161
00:10:23,580 --> 00:10:27,750
We improved clients trust and
enable entirely new products.

162
00:10:28,250 --> 00:10:32,900
By automating compliance in
CICD, we reduced audit stress.

163
00:10:33,380 --> 00:10:34,730
While deploying more frequently,

164
00:10:35,230 --> 00:10:40,750
we cut time to market for custom
products by 70%, allowing teams to focus

165
00:10:40,750 --> 00:10:45,140
on innovation instead of firefighting
infrastructure as the next steps.

166
00:10:45,200 --> 00:10:49,395
We are now exploring machine learning
for anomaly detection in market data.

167
00:10:49,895 --> 00:10:52,625
To catch issues before they impact trends.

168
00:10:53,495 --> 00:10:57,005
For example, if a bond price
suddenly spikes out of historical

169
00:10:57,005 --> 00:11:00,895
range, mission learning can flag
it before impacting calculations.

170
00:11:01,395 --> 00:11:04,335
We are considering multi-cloud
deployment strategies for resilience

171
00:11:04,335 --> 00:11:08,505
against provider outages that is
critical for regulated markets.

172
00:11:09,005 --> 00:11:12,335
We are enhancing self-service
capabilities so business users can

173
00:11:12,335 --> 00:11:13,400
request infrastructure directly.

174
00:11:13,900 --> 00:11:18,520
The goal is that a business analyst
can spin up a test index enrollment

175
00:11:18,520 --> 00:11:20,410
without calling the application team.

176
00:11:20,910 --> 00:11:26,270
We are also looking at extending airflow
D to support streaming workflows, for

177
00:11:26,270 --> 00:11:31,565
example, regarding intra calculations
when new market data costs, et

178
00:11:31,565 --> 00:11:33,395
cetera, show the journey is ongoing.

179
00:11:34,235 --> 00:11:37,955
But the foundation we have built
sets up for continuous innovation.

180
00:11:38,455 --> 00:11:41,875
This modernization was as
much about people and process

181
00:11:41,965 --> 00:11:43,105
as it was about technology.

182
00:11:43,885 --> 00:11:47,785
It showed that with the right
foundation, even legacy financial

183
00:11:47,785 --> 00:11:49,555
systems can be reimagined.

184
00:11:50,055 --> 00:11:50,745
Thank you for your time.

185
00:11:51,465 --> 00:11:54,795
I look forward to collaborating on
building the next way of resilient

186
00:11:54,825 --> 00:11:56,535
and scalable financial platforms.

