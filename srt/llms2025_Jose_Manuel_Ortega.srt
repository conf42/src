1
00:00:00,300 --> 00:00:00,900
Hi everyone.

2
00:00:00,900 --> 00:00:02,009
My name is Homan Ortega.

3
00:00:02,009 --> 00:00:06,829
I'm going to talk about security auditing
tools in network in less language models.

4
00:00:07,700 --> 00:00:14,380
less language models, basically category
of the learning models based on networks

5
00:00:14,440 --> 00:00:19,520
and other that language processing, the
tall aim to analyze the security of these,

6
00:00:19,570 --> 00:00:21,760
language from the developer point of view.

7
00:00:22,375 --> 00:00:23,905
Analyzing the ities.

8
00:00:23,905 --> 00:00:27,835
That can include in, in the edition
of these models, among the mo

9
00:00:28,435 --> 00:00:29,994
the main points to be discussed.

10
00:00:30,025 --> 00:00:35,545
We can highlight introduction to LM,
introduction to thewas, 11 to 10,

11
00:00:35,605 --> 00:00:41,630
auditing tools and applications that
handles these models, and, explain a

12
00:00:41,730 --> 00:00:43,565
use case with the test attack tool.

13
00:00:44,065 --> 00:00:46,975
we start with introducing
these kind of models.

14
00:00:47,665 --> 00:00:51,065
this kind of models, the base of
these models are transformers.

15
00:00:51,065 --> 00:00:56,735
Transformers are a type of,
neural network architecture, use

16
00:00:56,765 --> 00:00:59,225
in natural language processing.

17
00:00:59,765 --> 00:01:03,875
Tax, like machine translation, test
generation and test generation.

18
00:01:04,535 --> 00:01:05,525
They were introduced.

19
00:01:05,765 --> 00:01:07,635
They were introduced in the paper.

20
00:01:07,965 --> 00:01:11,225
A test is all you need in 2017.

21
00:01:11,725 --> 00:01:16,145
The key of transformers is the
sales, attentions mechanisms,

22
00:01:16,145 --> 00:01:20,485
which allows the models to weigh
the importance of different work

23
00:01:20,905 --> 00:01:23,575
incentives when making, predictions.

24
00:01:24,075 --> 00:01:29,915
Citation, each word or token in a sentence
can attend to every other word, allowing

25
00:01:29,915 --> 00:01:35,725
the model to capture two, relationships
between these 10 words, more effectively.

26
00:01:35,755 --> 00:01:40,655
This helps transformers and
understands context, the context,

27
00:01:41,025 --> 00:01:46,305
better than previous models like
recurrent network neural networks.

28
00:01:46,805 --> 00:01:50,215
Original transformer
have two main components.

29
00:01:50,455 --> 00:01:56,305
Encoder code the encoder, process
the input data, like a sentence

30
00:01:56,545 --> 00:01:59,095
and create a representation of it.

31
00:01:59,665 --> 00:02:03,600
And theod, use the encoder
representation to generate.

32
00:02:04,140 --> 00:02:06,560
Then they put translated sentence.

33
00:02:07,490 --> 00:02:13,255
In the case of card of tax, like
translation, the input is processed

34
00:02:13,255 --> 00:02:14,900
by the encoder under theod.

35
00:02:15,400 --> 00:02:20,420
The translations, the translation
buys on the encoders output.

36
00:02:20,920 --> 00:02:27,580
The training process of these kind of
models is generally, divided into stages.

37
00:02:27,610 --> 00:02:33,970
The pre-training and fine tuning
stages, the pertaining stage is where

38
00:02:33,970 --> 00:02:41,200
the model is trained on, on, on large
test ra to the, this word in a center,

39
00:02:41,500 --> 00:02:44,085
in a sequence or complete sentence.

40
00:02:44,325 --> 00:02:44,655
This.

41
00:02:45,155 --> 00:02:50,005
Alert and general representation of the
language and the file tuning, stage.

42
00:02:50,455 --> 00:02:55,555
After the pre-training stage, the
model is fine tuned with a specific

43
00:02:55,555 --> 00:03:01,525
data and allows for particular tasks
such as tech specification question

44
00:03:01,525 --> 00:03:06,470
answering on generate or generating
according text in a specific context.

45
00:03:06,970 --> 00:03:14,410
with the groaning property of this kind
of models such as GBT, JAMA among others,

46
00:03:14,910 --> 00:03:21,860
always, eh, has publish a specific list
of ities for this kind of applications.

47
00:03:22,360 --> 00:03:28,935
The, comes, in response to the rapid
adoption of these kind of models in,

48
00:03:28,965 --> 00:03:34,035
in, in many industries with the aim of
High lane, the main security problems

49
00:03:34,035 --> 00:03:36,595
associated with this, emerging technology.

50
00:03:37,095 --> 00:03:40,985
In this table, we can
see the minimal where.

51
00:03:41,485 --> 00:03:46,945
We can highlight in that we can
highlight in language models.

52
00:03:47,695 --> 00:03:51,835
Now we analyze some of
them in more detail.

53
00:03:52,335 --> 00:03:56,320
For example, one of the
talk of the top rank Es, if.

54
00:03:56,820 --> 00:04:03,350
Involves manipulating input prompts
to achieve unintended or malicious and

55
00:04:03,350 --> 00:04:07,250
model outputs regarding investigation.

56
00:04:07,250 --> 00:04:13,100
This of this Ed is recommended to
validate all inputs before processing

57
00:04:13,100 --> 00:04:17,890
them with the LM command while
least could also be implemented, and

58
00:04:17,890 --> 00:04:19,665
the types of data can be process.

59
00:04:20,040 --> 00:04:21,150
Will be limited.

60
00:04:21,650 --> 00:04:25,940
For example, in a element scenario
stating input from external

61
00:04:25,940 --> 00:04:34,070
sources such as website or files
controlled by a user, in jection is

62
00:04:34,250 --> 00:04:35,930
obtained need in which an attacker.

63
00:04:36,000 --> 00:04:43,650
Manipulate external inputs such as
web or web content or user data,

64
00:04:43,680 --> 00:04:46,360
which are then processed by the model.

65
00:04:46,840 --> 00:04:52,570
This can lead the model to behave in
ways not intended by the developers,

66
00:04:52,570 --> 00:04:57,115
compromising the security and
the integrity of applications.

67
00:04:57,615 --> 00:05:03,695
For example, Ali Pro could be generated
a summary of the provided document by

68
00:05:04,355 --> 00:05:10,825
an attacker and manipulating external
source in jets, hidden pro prompts,

69
00:05:10,825 --> 00:05:16,195
include confidential information
from other files, for example, or

70
00:05:16,195 --> 00:05:18,840
from a specific web page for example.

71
00:05:19,340 --> 00:05:25,270
Of external manipulation, the model
generates content can generate content.

72
00:05:25,660 --> 00:05:31,575
Incorporating sensitive details
from un outsized sources leading to

73
00:05:31,575 --> 00:05:33,695
data leakage and security breach.

74
00:05:34,195 --> 00:05:40,915
Another res example of Jection is,
in the people with tiptoe, GPT-4

75
00:05:40,915 --> 00:05:46,085
is too smart to be safe, still
chart with elements via thier.

76
00:05:46,745 --> 00:05:53,425
This research, explored how these
models can be manipulated to engage in

77
00:05:53,545 --> 00:05:59,955
conversations with, In conversations
using encryption or coded message,

78
00:06:00,885 --> 00:06:03,635
thereby of scoring, the true intent.

79
00:06:03,695 --> 00:06:08,835
This method called, bypass Monitoring
Systems Design into the Technic user.

80
00:06:09,495 --> 00:06:17,185
A common example, might be the use
of the of C test or simple scrambling

81
00:06:17,265 --> 00:06:22,305
in incoming message, which an
attacker can use to generate a code.

82
00:06:22,755 --> 00:06:28,379
Responses, responses or the code,
sensitive information without it

83
00:06:28,379 --> 00:06:30,059
paying all used to a no server.

84
00:06:30,559 --> 00:06:35,119
Injection is also, this attack
is also known as yearly.

85
00:06:35,119 --> 00:06:41,479
Breaking involves in this attack
involves directly in manipulating the

86
00:06:42,349 --> 00:06:44,479
commas that are sent to these models.

87
00:06:44,989 --> 00:06:48,409
This research with the detailed
do anything, no charity

88
00:06:48,409 --> 00:06:51,570
machine, and evaluating, in the.

89
00:06:52,330 --> 00:06:57,280
Jailbreak prongs on less language
models, analyze and evaluate

90
00:06:57,310 --> 00:07:03,639
so-called jailbreak prongs, which
are common the same to bypass TI

91
00:07:04,150 --> 00:07:07,420
restrictions impose of these models.

92
00:07:07,930 --> 00:07:15,369
These prongs allow users to obtain answers
that goul normally may be blocked due to

93
00:07:15,369 --> 00:07:18,999
ethical, illegal, or security constraints.

94
00:07:19,499 --> 00:07:26,129
The study investigates how these lib
pros work and how effective they are,

95
00:07:26,190 --> 00:07:32,790
by passing, words, the researchers
co and gala side and collective size.

96
00:07:32,920 --> 00:07:39,005
several samples of jail that has been
created and shared in the community

97
00:07:39,005 --> 00:07:42,515
without economy of industrial integration.

98
00:07:43,015 --> 00:07:47,515
In element deploy development
that is crucial intraining

99
00:07:47,845 --> 00:07:49,525
for language compression.

100
00:07:50,135 --> 00:07:55,415
five tuning for quality ment and
embedding no demand specific knowledge.

101
00:07:55,625 --> 00:08:01,825
However, these data sets, can be,
susceptible, allowing attackers to

102
00:08:01,825 --> 00:08:03,925
manipulate them, this manipulation.

103
00:08:04,450 --> 00:08:10,540
no known as po poisoning can
compromise the mother's performance

104
00:08:10,630 --> 00:08:16,660
and lead to generate content aligned
with malicious intentions during

105
00:08:16,660 --> 00:08:19,180
pre-training and attacker Introduce.

106
00:08:19,710 --> 00:08:25,260
misleading language and sample and
samples shaping the models in or

107
00:08:25,260 --> 00:08:27,930
understanding of a specific subjects.

108
00:08:28,350 --> 00:08:33,230
Consequently, the model might
produce outputs reflecting the

109
00:08:33,230 --> 00:08:38,305
inject the injected bias when used
in practical app applications.

110
00:08:38,805 --> 00:08:44,350
One of the attack, one of the attacks,
that has been investigated in last

111
00:08:44,350 --> 00:08:50,360
years, it attacks, this attacks,
reference to, deliberate attempt to

112
00:08:50,360 --> 00:08:55,405
manipulate or deceive an artificial
intelligence or material learning

113
00:08:55,455 --> 00:08:58,405
model by providing it with credit.

114
00:08:58,905 --> 00:09:03,785
Carefully craft input data designated
to cows, the models to make in,

115
00:09:04,465 --> 00:09:09,445
to make incorrect predictions or
decisions these attacks exploit.

116
00:09:09,445 --> 00:09:15,445
Try to exploit es in, in, in the models
decision making process typically by

117
00:09:15,445 --> 00:09:21,275
introduces a small in person table change
in the input app, in the input data.

118
00:09:21,775 --> 00:09:24,590
These are the key
characteristics of albe attacks.

119
00:09:24,800 --> 00:09:30,840
For example, smart, per bats alax
typically involve adding a small pho

120
00:09:30,875 --> 00:09:37,885
crop per perturbations to input data
of 10 imperial tables, two months.

121
00:09:38,605 --> 00:09:39,890
modern vulner.

122
00:09:39,890 --> 00:09:44,750
These attacks exploit specific weakness
in the mature learning models such as.

123
00:09:45,395 --> 00:09:50,725
Eh, it's inability to, to general
generalize well, to, to new and

124
00:09:50,845 --> 00:09:56,665
in data or the sensitivity of the
model to certain types of input.

125
00:09:57,165 --> 00:10:01,755
we can classify guy,
eh, with two, two types.

126
00:10:01,755 --> 00:10:03,525
White box and black box.

127
00:10:04,095 --> 00:10:05,205
White box.

128
00:10:05,565 --> 00:10:10,035
Eh, assume the attacker has
full knowledge of the model.

129
00:10:10,455 --> 00:10:14,410
Including, its a architecture
parameters entering data.

130
00:10:14,920 --> 00:10:16,240
They are the attacker.

131
00:10:16,240 --> 00:10:22,230
Use the information to generate samples
that are most likely to receive the model.

132
00:10:22,730 --> 00:10:28,395
And on the, and on the other side, we have
black box attacks in black box attacks.

133
00:10:28,450 --> 00:10:33,300
the attacker doesn't have direct
access to the models internal workings.

134
00:10:34,005 --> 00:10:34,725
in.

135
00:10:34,755 --> 00:10:40,935
Instead, the attackers rely relies on
observing the model's output in response

136
00:10:40,935 --> 00:10:45,990
to different inputs, and using this
information to graph adversarial samples.

137
00:10:46,490 --> 00:10:51,280
Machine learning systems are
vulnerable to, to wide rights

138
00:10:51,320 --> 00:10:53,350
of many adversarial, attacks.

139
00:10:53,860 --> 00:11:01,120
Many of them employ class machine learning
models like linear regression and superb

140
00:11:01,780 --> 00:11:04,510
machines, as well as the learning systems.

141
00:11:04,510 --> 00:11:06,480
The majority of attacks.

142
00:11:06,980 --> 00:11:11,690
often try to reduce classified
performance on particular tasks.

143
00:11:12,690 --> 00:11:16,680
the area of machine learning
investigates a class of assault

144
00:11:16,740 --> 00:11:22,410
mean to degrade classified
performance or on a particular tax.

145
00:11:23,400 --> 00:11:26,930
At this point, we regard,
different, classify adversarial

146
00:11:26,930 --> 00:11:28,220
task in different types.

147
00:11:29,080 --> 00:11:34,065
Eh, these are the six we call
highly, six adversarial strategies

148
00:11:34,755 --> 00:11:37,005
and the need for these defenses.

149
00:11:37,815 --> 00:11:44,505
The jection, as I comment before,
basically jection in attacks involved

150
00:11:44,625 --> 00:11:49,455
crafting input to manipulate the
behavior of these models, cautioning

151
00:11:49,455 --> 00:11:51,495
them to, to produce harmful.

152
00:11:51,840 --> 00:11:58,050
Or unintended by splitting their
reliance on, on, on usher prompts.

153
00:11:58,550 --> 00:12:01,160
Another kind of attacks
is the bastion attacks.

154
00:12:01,340 --> 00:12:05,200
This kind of attacks,
involve, modifying inputs.

155
00:12:05,785 --> 00:12:11,220
to mislead, models at inference time,
making them, still see an effective

156
00:12:11,250 --> 00:12:13,800
means of bypassing a costal system.

157
00:12:14,370 --> 00:12:21,390
The tax objective of the kind of, this
kind of tax is to generate an input that

158
00:12:21,390 --> 00:12:27,110
is misclassify misclassified without
needing to understand the artificial

159
00:12:27,110 --> 00:12:29,420
intelligence models internal mechanism.

160
00:12:29,920 --> 00:12:35,985
Making a taxi still here,
particularly difficult to go under.

161
00:12:36,485 --> 00:12:39,045
Another kind of attacks
is poisoning attacks.

162
00:12:39,115 --> 00:12:44,985
target the training files of, artificial
intelligence models, injecting malicious

163
00:12:44,985 --> 00:12:50,440
data into the training sets and training
sets to compromise the models, behavior

164
00:12:50,445 --> 00:12:57,105
manipul by manipulating the training data
or is labels that can come, made the model

165
00:12:57,225 --> 00:12:59,745
perform poorly during the deployment.

166
00:13:00,245 --> 00:13:03,365
Another kind of attacks is the
modeling inversion attacks.

167
00:13:03,965 --> 00:13:08,555
Eh, I'm still reverse engineering
artificial intelligent models to

168
00:13:08,555 --> 00:13:12,875
retrieve sensitive information
about the training data.

169
00:13:13,235 --> 00:13:16,685
In these attacks, malicious
actors, analyze their

170
00:13:16,685 --> 00:13:19,630
predictions they made by a model.

171
00:13:20,155 --> 00:13:23,335
In response to, to, to many inputs.

172
00:13:23,395 --> 00:13:28,855
Using this analysis helps them in
fair sensitive details about the

173
00:13:28,855 --> 00:13:30,685
data the model was turning on.

174
00:13:31,405 --> 00:13:37,105
For example, by sharing how the model
reacts to different input patterns,

175
00:13:37,605 --> 00:13:45,310
attackers can review features or even
t portions of the OR training data set.

176
00:13:45,810 --> 00:13:48,810
Another kind of attack
is mod model extraction.

177
00:13:48,810 --> 00:13:55,170
Model extraction attacks aim to replicate
the functionality of proprietary model

178
00:13:55,260 --> 00:14:01,060
by quoting it with numeral inputs and
observ and observing its output, it's

179
00:14:01,150 --> 00:14:07,920
outputs this attack when attacker
elicited appropriate the training model.

180
00:14:08,620 --> 00:14:14,530
typically involving regressing engineering
where the attacker receivers the system

181
00:14:15,100 --> 00:14:21,340
and parameters to replicate his full
san once in possession of the replicated

182
00:14:21,340 --> 00:14:27,610
system, the attacker can exploit it
with, for values, malis issues, activity.

183
00:14:28,110 --> 00:14:30,600
finally, the member cease inference.

184
00:14:30,630 --> 00:14:36,825
Inference attacks involves an adv
adverse to the use sensitive information.

185
00:14:37,665 --> 00:14:41,595
Sensitive information from
an artificial inte model.

186
00:14:42,045 --> 00:14:45,105
By meaning it outputs and behaviors.

187
00:14:45,945 --> 00:14:50,925
For instance, an attacker might
use the confidence levels of

188
00:14:51,030 --> 00:14:56,510
a. To determine if a person was
part of the model training data.

189
00:14:57,380 --> 00:15:03,010
Typically, a model training or on a
specific data point will generate high

190
00:15:03,010 --> 00:15:09,430
confidence predictions for that data point
if it goes in the training data sets.

191
00:15:09,930 --> 00:15:14,460
And by introducing slight modifications
to the input data and observing

192
00:15:14,460 --> 00:15:19,560
fluctuations in the models confidence
levels, an attacker can make educated

193
00:15:20,060 --> 00:15:24,740
guesses about somewhat precise
presence in the training data set.

194
00:15:25,240 --> 00:15:28,925
at this point, I'm going to commend
some tools that I interesting to

195
00:15:28,925 --> 00:15:31,115
evaluate the model robustness.

196
00:15:31,625 --> 00:15:36,275
For example, the Pro J framework, that
is a tool that design need to study.

197
00:15:36,275 --> 00:15:42,605
Whole language models like GPT
can be manipulated using rooms, eh

198
00:15:42,695 --> 00:15:47,760
port pair that is Aron on of prone
automatic interactive refinement.

199
00:15:47,955 --> 00:15:54,325
It is an automated, methodology
for iteratively refining problems

200
00:15:54,385 --> 00:16:00,715
with the wall of jailbreaking
of jailbreaking L lms.

201
00:16:01,045 --> 00:16:05,795
Jail breaking, refers to 10 that
bypass security restrictions and

202
00:16:06,065 --> 00:16:09,755
lanes imposed by language models.

203
00:16:10,270 --> 00:16:13,080
And another, tool,
interesting tool is stop.

204
00:16:13,925 --> 00:16:19,105
That is a framework that explored
how to perform attacks on MLMs

205
00:16:19,465 --> 00:16:22,195
using a decision three structure.

206
00:16:22,695 --> 00:16:27,265
Another interesting tool is, is
failing indication, indicators.

207
00:16:27,445 --> 00:16:34,870
This are, it is an open source tool for
detecting and evaluating fairness metrics

208
00:16:35,170 --> 00:16:38,110
across data and models in particular.

209
00:16:38,610 --> 00:16:42,510
This tool includes the ability to
evaluate the distribution of data sets.

210
00:16:42,870 --> 00:16:47,465
It will evaluate the model
performance, feel confident about

211
00:16:47,945 --> 00:16:52,310
the resource with confidence in
intervals and multiple three cells.

212
00:16:53,060 --> 00:17:00,130
And also, with these tools, we can,
explore, root causes and opportunities

213
00:17:00,130 --> 00:17:02,080
for improvements in our models.

214
00:17:02,580 --> 00:17:06,480
now I'm going to talk about privacy
and security As commented before,

215
00:17:06,530 --> 00:17:12,600
this kind of models might, may leak
sensitive information or be to,

216
00:17:12,600 --> 00:17:17,930
to attacks such as pro injector or
adversarial manipulation at this point.

217
00:17:18,430 --> 00:17:25,120
Focus on ensuring data privacy
and security audits focus on

218
00:17:25,450 --> 00:17:27,280
detecting and addressing vulner.

219
00:17:27,780 --> 00:17:34,470
For example, we can find some auditing
tools like Pro, pro request to a set

220
00:17:35,040 --> 00:17:40,380
of strategies, tools or techniques
design need to save what the behavior

221
00:17:40,560 --> 00:17:46,055
of these kind of models for, from
malicious or on intent on intended.

222
00:17:46,555 --> 00:17:54,160
Pro use a models with, t 6 million
parameters that has been training,

223
00:17:54,930 --> 00:18:00,090
on a less data set of attacks and
branch from phone, any on internet

224
00:18:00,590 --> 00:18:01,090
to test.

225
00:18:01,355 --> 00:18:06,295
the three model is as, as going to
the repository to repository in.

226
00:18:06,795 --> 00:18:13,520
And using it with the free inference,
API, that the platform, offer us

227
00:18:13,550 --> 00:18:21,950
basically, this tool is a, Offers like
the capacity to prevent harmful of all

228
00:18:21,950 --> 00:18:27,860
malicious interactions by filtering,
monitoring, and responding to adversarial

229
00:18:27,890 --> 00:18:33,785
inputs, ensuring that the models
behavior remain safe, ethical, and

230
00:18:33,785 --> 00:18:36,495
aligned, line with the intended use.

231
00:18:36,995 --> 00:18:41,745
We have other, other tools like Jamma
word reference to a security tool or

232
00:18:41,745 --> 00:18:47,785
the strategy design, for wording layer
less language models like Meta jama.

233
00:18:48,400 --> 00:18:51,580
Against potential is
and adversarial attacks.

234
00:18:52,570 --> 00:18:59,470
This tool offers a table solution
to protect s against a pro injection

235
00:18:59,470 --> 00:19:05,820
and breaker attacks by combining
different techniques or in filtering,

236
00:19:05,820 --> 00:19:07,950
normalization and monitoring.

237
00:19:08,280 --> 00:19:10,740
I'm monitoring the and input.

238
00:19:10,770 --> 00:19:12,920
I'm monitoring the input of the user.

239
00:19:13,420 --> 00:19:21,110
Essentially this model employs a. At
multiple levels to mitigate the risk

240
00:19:21,140 --> 00:19:28,370
of jection and break attacks using
techniques like dynamic impulse filtering,

241
00:19:29,300 --> 00:19:33,815
pron normalization and con, con and
contextualization, response policy

242
00:19:33,815 --> 00:19:36,665
and active monitoring and a response.

243
00:19:37,165 --> 00:19:41,635
To test this model, we can do
it with the hugging face site.

244
00:19:42,135 --> 00:19:43,395
Jamma.

245
00:19:43,425 --> 00:19:47,846
Jamma ES will not only tell us
whether the content is safe or not.

246
00:19:48,346 --> 00:19:54,526
But we also classify the content
into 14 different categories.

247
00:19:54,916 --> 00:20:03,816
These categories have been straight from a
taxonomy, introduced in the following, in,

248
00:20:03,816 --> 00:20:06,546
in the following, in this investigation.

249
00:20:07,046 --> 00:20:13,036
Finally I'm going to comment, tool
for, this design need for adversarial

250
00:20:13,096 --> 00:20:18,196
attacks, that augmentation and
model robust robustness testing

251
00:20:18,196 --> 00:20:19,966
in natural language processing.

252
00:20:20,466 --> 00:20:27,286
Basically this attack is, to a framework
for the detect for testing attacks,

253
00:20:27,806 --> 00:20:34,486
using techniques like data augmentation
and natural language processing.

254
00:20:34,986 --> 00:20:37,561
This tool, allows user to test.

255
00:20:38,061 --> 00:20:43,356
To test how model is to the samples
and helps to improve it ness,

256
00:20:43,856 --> 00:20:46,061
choosing the sentiment analysis model.

257
00:20:46,821 --> 00:20:51,051
for this case, we'll use the
Pretrained bare Model train, train

258
00:20:51,051 --> 00:20:55,491
it on the EMDB mobile review dataset.

259
00:20:55,491 --> 00:21:00,051
I commonly use dataset for
binary sentiment classification.

260
00:21:00,551 --> 00:21:05,781
Data tax provides a variety of attack,
attack strategies for this use case.

261
00:21:05,781 --> 00:21:07,371
Will use the text attack.

262
00:21:07,931 --> 00:21:13,431
A popular attack 10 that replace
words in the input test with

263
00:21:13,431 --> 00:21:16,031
pseudonyms, the full, the model.

264
00:21:16,481 --> 00:21:19,521
Without changing the
semantic, meaning of the text.

265
00:21:20,021 --> 00:21:24,666
Now you can ruin the attack on a
simple input to see how robust the

266
00:21:24,666 --> 00:21:27,876
analysis model is against the sample.

267
00:21:28,376 --> 00:21:32,826
After ruling the attack, you will
receive a al samples where mine

268
00:21:32,921 --> 00:21:36,971
change to the input test, have
altered the model expiration.

269
00:21:36,971 --> 00:21:41,651
In this case, the model or in,
or originally predict positive

270
00:21:41,651 --> 00:21:44,111
sentiment for the original test.

271
00:21:44,116 --> 00:21:48,631
However, after replacing
words like love with like it.

272
00:21:49,606 --> 00:21:55,006
And really with griping the model
match incorrectly predict negative

273
00:21:55,396 --> 00:22:01,296
sentiment de, despite the fact that
the overall meaning remains, positive.

274
00:22:01,796 --> 00:22:06,386
Once you have identified witness
in the model, you can use the Cyac

275
00:22:06,446 --> 00:22:10,496
data augmentation capabilities to
retrain the model with adversarial

276
00:22:10,496 --> 00:22:12,926
samples to improve it robustness.

277
00:22:13,586 --> 00:22:17,246
This step allows you to augment
your data set with adversarial

278
00:22:17,246 --> 00:22:22,226
samples and retrain model, making
it more resistant to such attacks.

279
00:22:22,726 --> 00:22:26,776
I am finally commenting what the
interesting resources that we can

280
00:22:26,776 --> 00:22:29,986
find in, in, in Bush, for example.

281
00:22:30,016 --> 00:22:32,746
These are the bush that have
been blue published in the

282
00:22:32,746 --> 00:22:34,266
last year, with this topic.

283
00:22:34,766 --> 00:22:40,041
And these are, specific, you
have resources for with, uh.

284
00:22:40,541 --> 00:22:46,711
with the papers that I commented, and
this is another interesting resources

285
00:22:46,711 --> 00:22:52,231
that we can, when we can, for extend
information provided in this presentation.

286
00:22:52,731 --> 00:22:56,116
this is the final of the
presentation, I think.

287
00:22:56,196 --> 00:22:59,946
Eh, that has be, can be hobby.

288
00:23:00,036 --> 00:23:00,606
Interesting.

289
00:23:00,666 --> 00:23:01,296
Interesting.

290
00:23:01,296 --> 00:23:03,756
for developers and secreted researchers.

291
00:23:04,356 --> 00:23:05,496
And that's all.

292
00:23:05,496 --> 00:23:15,256
Thank you very much for attendance in
the con go 42 language models in 2025.

293
00:23:15,746 --> 00:23:16,226
thank you very much.

294
00:23:16,496 --> 00:23:16,766
Bye.

