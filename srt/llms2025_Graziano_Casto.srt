1
00:00:00,000 --> 00:00:01,140
Good morning guys.

2
00:00:01,140 --> 00:00:02,580
Welcome here to my session.

3
00:00:02,610 --> 00:00:06,000
The aim for today is to explore
the synergies between the cloud,

4
00:00:06,000 --> 00:00:07,530
native world and the AI world.

5
00:00:07,590 --> 00:00:08,820
So let's start.

6
00:00:09,240 --> 00:00:10,350
My name is Graciano.

7
00:00:10,380 --> 00:00:15,150
I am a dev rail engineer at Mia Platform,
an Italian company that develops among a

8
00:00:15,150 --> 00:00:19,800
platform builder that among other teams,
simplified adoption and of the technology

9
00:00:19,800 --> 00:00:21,600
we are, we will be discussing today.

10
00:00:22,100 --> 00:00:26,300
This talk actually comes from a
conversation I had a few months ago with

11
00:00:26,300 --> 00:00:30,500
a close friend of mine, Leonardo, who
works as a data scientist and deals with

12
00:00:30,500 --> 00:00:35,510
model and data every day, one evening
after a slide to alcoholic, A operative.

13
00:00:35,540 --> 00:00:39,500
What do you think your
engineers will talk about work?

14
00:00:39,530 --> 00:00:40,100
Of course.

15
00:00:40,100 --> 00:00:45,239
So Leonardo begins explaining, all the
problem he faces in his daily work.

16
00:00:45,739 --> 00:00:50,480
It started with this sentence, when
everything seems to be going smoothly,

17
00:00:50,510 --> 00:00:57,169
something always going wrong, and then
he had, it takes longer to put the

18
00:00:57,169 --> 00:01:03,080
model into production than to develop
it, and as I found out later, he's

19
00:01:03,080 --> 00:01:04,789
not the only one with this problem.

20
00:01:04,789 --> 00:01:07,789
As you can see from this
screenshot, from this tweet.

21
00:01:08,289 --> 00:01:12,879
So guys, I am not an expert of this kind
of stuff, but I'm extremely curious.

22
00:01:12,879 --> 00:01:15,609
So I ask him to explain his workflow.

23
00:01:15,879 --> 00:01:20,739
He immediately starts talking about
feature management and the inference,

24
00:01:20,799 --> 00:01:22,719
and I'm like, inference what?

25
00:01:23,169 --> 00:01:25,299
Give me the beginner version, please.

26
00:01:25,329 --> 00:01:26,859
So he simplifies.

27
00:01:27,199 --> 00:01:31,519
As much as possible explaining that
the workflow follows four step.

28
00:01:31,519 --> 00:01:35,509
As you can see on the slide, the data
preparation, which is the preliminary

29
00:01:35,509 --> 00:01:39,979
phase where the data sets are prepared,
the model training and the timing

30
00:01:40,279 --> 00:01:42,769
where the model is actually developed.

31
00:01:42,799 --> 00:01:47,259
And then when the deployment and
monitoring is the phase, which is

32
00:01:47,259 --> 00:01:51,579
when the model is put on production
and its behavior is monitored

33
00:01:51,579 --> 00:01:54,939
to, and finally the process is
repeated and improved the model.

34
00:01:55,439 --> 00:01:59,489
As, I soon learned, none of
this step was free from issues.

35
00:01:59,989 --> 00:02:03,409
The first problem Leo points
out is dealing with the data.

36
00:02:04,219 --> 00:02:07,309
Manage the large volumes
of data without a clear.

37
00:02:07,309 --> 00:02:11,629
Governance is a huge challenge
tracing the history or lineage of the

38
00:02:11,629 --> 00:02:16,279
dataset, identifying the data owners
and understand data sensitivity,

39
00:02:16,279 --> 00:02:18,259
it's all a nightmare for him.

40
00:02:18,759 --> 00:02:23,589
Then there is the gap between the local
development and the production release

41
00:02:23,589 --> 00:02:27,369
of the model, which is actually the
most important trouble for Leonardo.

42
00:02:27,609 --> 00:02:33,099
He explains that when he do his magic on
its local Jupyter notebook, everything

43
00:02:33,129 --> 00:02:39,129
works fine and runs perfectly, but then
when it's taken by the delivery team,

44
00:02:39,579 --> 00:02:42,369
it's entering in a world new world.

45
00:02:42,869 --> 00:02:47,669
The last one is the observability
nightmare when they survive

46
00:02:47,669 --> 00:02:48,989
the production release.

47
00:02:49,049 --> 00:02:54,389
Monitoring the model behavior to spot
tissue as early as possible become a

48
00:02:54,389 --> 00:02:59,159
challenging task, and they often don't
have a sense of full control over

49
00:02:59,159 --> 00:03:04,349
what's happening, risking to being
too late in resolving critical issues.

50
00:03:04,849 --> 00:03:09,739
The conversation ended with Leonardo
feeling frustrated and me headed to home

51
00:03:09,739 --> 00:03:12,259
still thinking about what Leonardo said.

52
00:03:12,379 --> 00:03:17,899
And the question that arise in
my mind is, how can I apply my

53
00:03:17,899 --> 00:03:20,449
expertise to help a friend of mine?

54
00:03:20,949 --> 00:03:24,819
In my mind I had somewhat
like a meme with the Vs.

55
00:03:24,819 --> 00:03:29,169
On one side feeling frustrated,
thinking at the models, and excited

56
00:03:29,169 --> 00:03:30,969
thinking at the infrastructure stuff.

57
00:03:30,969 --> 00:03:36,189
And on the other side, the data scientists
excited of their models and frustrated

58
00:03:36,219 --> 00:03:38,109
thinking at deployment activities.

59
00:03:38,609 --> 00:03:41,159
However, this situation wasn't new to me.

60
00:03:41,609 --> 00:03:47,609
This is exactly how operation guys and
developer felt years ago before the

61
00:03:47,609 --> 00:03:49,349
introduction of the DevOps culture.

62
00:03:49,849 --> 00:03:51,769
So here are my intuition.

63
00:03:52,369 --> 00:03:57,199
How can I apply my knowledge about the
application development, DevOps, and

64
00:03:57,199 --> 00:04:01,699
Cloud Native World to solve the issues
that are actually similar to those we

65
00:04:01,699 --> 00:04:04,369
faced in our context some years ago?

66
00:04:04,819 --> 00:04:07,609
Spoiler, the answer is yes, of course.

67
00:04:08,009 --> 00:04:10,244
but let's see together how.

68
00:04:10,744 --> 00:04:15,904
First, we need tools and fortunately,
the Cloud Native Computing Foundation

69
00:04:15,904 --> 00:04:20,884
and specifically the AI working group,
made an excellent work mapping all the

70
00:04:20,884 --> 00:04:25,824
tools in the cloud native landscape
that could help the ML world creating

71
00:04:25,824 --> 00:04:28,134
the cloud native AI landscape.

72
00:04:28,634 --> 00:04:31,244
Adding tools alone into
the delivery chain.

73
00:04:31,274 --> 00:04:36,314
Risk to add just a lot of complexity,
actually increasing the issues.

74
00:04:36,564 --> 00:04:41,874
then we need the methodology, a framework
to govern all these tools and make

75
00:04:41,874 --> 00:04:44,184
their adoption simple and seamless.

76
00:04:44,705 --> 00:04:48,575
This methodology is actually the
platform engineering, thanks to the

77
00:04:48,575 --> 00:04:52,624
platform, provide a place between
engineers and their dependency,

78
00:04:52,835 --> 00:04:57,275
helping them to consume what they
need in an easy and self-service way.

79
00:04:57,775 --> 00:05:02,605
The first problem pointed by Leonardo
is dealing with data, and that's

80
00:05:02,605 --> 00:05:06,265
actually not a problem related with
tools, but it's a problem of culture.

81
00:05:06,594 --> 00:05:10,585
The lack of data governance is a clear
reflection of the lack of a proper

82
00:05:10,585 --> 00:05:12,295
data culture inside the company.

83
00:05:12,594 --> 00:05:16,525
I suggest start cultivating
the data culture and inside

84
00:05:16,525 --> 00:05:20,664
organization by adding some tools
that could help during the process.

85
00:05:21,430 --> 00:05:25,540
The key point here is to understand
that the AI is as effective

86
00:05:25,630 --> 00:05:27,490
as the data used to train it.

87
00:05:27,970 --> 00:05:33,070
A model fit with poor quality or
inaccurate data will produce poor results.

88
00:05:33,370 --> 00:05:39,200
A tool that simplify all these activities
when dealing with data like discovering,

89
00:05:39,260 --> 00:05:44,960
profiling, versioning, tracing is
essential to ensure high quality and

90
00:05:44,960 --> 00:05:47,539
consistent data being used and produced.

91
00:05:48,039 --> 00:05:52,180
The tool we are talking about is a
data catalog, which is essentially

92
00:05:52,210 --> 00:05:57,010
a hierarchical archive of all the
metadata of your data sources.

93
00:05:57,880 --> 00:06:02,140
A data catalog contains all the
technical metadata, like data type,

94
00:06:02,140 --> 00:06:06,310
storage, location, and the lineage,
which explain how the dataset are

95
00:06:06,310 --> 00:06:11,020
connected, each other's, and also the
business metadata, like ownership or

96
00:06:11,020 --> 00:06:13,330
compliance level to specific regulation.

97
00:06:13,830 --> 00:06:18,510
Since with Data Catalog, all the
information about our dataset are

98
00:06:18,510 --> 00:06:22,770
centralized, making them easily
searched and well organized.

99
00:06:22,770 --> 00:06:27,990
It's the Swift, the Swiss knife
to for everyone who need to deal

100
00:06:27,990 --> 00:06:30,000
with data inside the organization.

101
00:06:30,000 --> 00:06:34,919
I. Now it's time to deal with the
main issue pointed out by Leonardo.

102
00:06:35,130 --> 00:06:39,630
So we have to understand how to bridge
the gap between the local development

103
00:06:39,690 --> 00:06:41,340
and the production release of the model.

104
00:06:42,060 --> 00:06:47,460
This way, many tool from the cloud
native AI landscape could help us.

105
00:06:47,580 --> 00:06:49,050
One of them is cube flow.

106
00:06:49,550 --> 00:06:54,020
The problem here is due to the
fact that, On one side, we have the

107
00:06:54,020 --> 00:06:58,520
data scientists who works in their
environment like Jupyter Notebook

108
00:06:58,520 --> 00:07:00,560
or something else on the other side.

109
00:07:01,060 --> 00:07:05,380
And on the other side we have the
ops guys, which are to deal with the

110
00:07:05,380 --> 00:07:07,180
completely different environment.

111
00:07:07,300 --> 00:07:11,560
And it's this opposition of
ecosystem, the core of the problem.

112
00:07:11,800 --> 00:07:15,220
So to solve this challenge,
we need to bridge the gap

113
00:07:15,220 --> 00:07:17,200
between these two ecosystems.

114
00:07:18,160 --> 00:07:22,960
This Bridges Cube Flow, a collection of
open source projects designed to support

115
00:07:22,990 --> 00:07:28,550
each stage of the machine learning
lifecycle bridging the ML on Kubernetes

116
00:07:28,550 --> 00:07:31,280
in a simple, portable and scalable way.

117
00:07:31,760 --> 00:07:36,050
In this way, we bring the world of the
data scientists in science, something

118
00:07:36,050 --> 00:07:39,800
that delivery teams know wells,
the containers, and the Kubernetes.

119
00:07:40,300 --> 00:07:44,469
The first project I want to discuss
about is cube flow notebooks.

120
00:07:44,710 --> 00:07:50,110
The core problem underlined by Leonardo
is the gap between developing this model

121
00:07:50,110 --> 00:07:54,789
locally during the experimentation phase
and then moving them into production.

122
00:07:55,270 --> 00:07:59,440
This is because the development happens
in a completely different environment.

123
00:07:59,440 --> 00:08:03,039
From production and producing all
the development condition could

124
00:08:03,039 --> 00:08:04,750
be a huge challenge for them.

125
00:08:05,250 --> 00:08:09,390
Cup Flow Notebook helps solving
this issue by providing a way to run

126
00:08:09,390 --> 00:08:13,020
web-based development environment
inside the Kubernetes cluster.

127
00:08:13,439 --> 00:08:18,409
This notebook cells actually, runs
as a container inside the Kubernetes

128
00:08:18,409 --> 00:08:24,019
ports so that user can create
their notebook directly inside

129
00:08:24,019 --> 00:08:25,639
the cluster rather than locally.

130
00:08:25,909 --> 00:08:30,529
And admins can provide standard
notebooks image for their organization.

131
00:08:31,029 --> 00:08:36,039
With the required packages style, for
example, working in this way, data

132
00:08:36,039 --> 00:08:39,959
scientists develop the model in a
standardized way that the ops engineer

133
00:08:39,959 --> 00:08:44,219
can then understand and replicate over
the production environment sits both

134
00:08:44,219 --> 00:08:48,760
phase, actually happens in the same
environment, which is Kubernetes itself.

135
00:08:49,260 --> 00:08:54,120
Another major challenge in this context
is managing distributed training and

136
00:08:54,120 --> 00:08:57,840
Kuber Flow helps simplifying these
activities, providing the kuber flow,

137
00:08:58,120 --> 00:09:02,650
training operator, which is a Kubernetes
operator for fine tuning and scalable

138
00:09:02,650 --> 00:09:04,780
distributed training of the ML model.

139
00:09:05,319 --> 00:09:07,300
It works by implementing a.

140
00:09:07,339 --> 00:09:11,180
Central Kubernetes controller
that coordinates distributed

141
00:09:11,180 --> 00:09:15,139
training, jobs supporting also
high performance computing task.

142
00:09:15,889 --> 00:09:20,119
The training operator allowed to scale
the model training from a single machine

143
00:09:20,119 --> 00:09:25,425
to a larger distributor Kubernetes
cluster by using its APIs and interface.

144
00:09:25,925 --> 00:09:30,965
Another important use case to cover is
the case in which we just want to create

145
00:09:31,025 --> 00:09:37,025
AI powered product or to add AI powered
feature to our existing product without

146
00:09:37,025 --> 00:09:42,095
developing the model, but using a third
party model like GPT, mytral or lama.

147
00:09:42,785 --> 00:09:47,675
In this situation, our application has to
handle various aspects including recast.

148
00:09:48,530 --> 00:09:51,890
Authentication authorization for
accessing the model, for example,

149
00:09:52,250 --> 00:09:55,969
from management caching and
spending check with AI provider.

150
00:09:56,510 --> 00:10:01,280
This approach not only adds complexity
to our application with task outside the

151
00:10:01,280 --> 00:10:06,380
core function, but also makes switching
between models challenging and it couples

152
00:10:06,380 --> 00:10:08,030
the application to the specific model.

153
00:10:08,530 --> 00:10:12,180
The solution to the problem is
the user, is to use an AI gateway

154
00:10:12,420 --> 00:10:14,580
similar to common API gateway.

155
00:10:14,640 --> 00:10:18,210
An AI gateway is a middleware
between the application and the

156
00:10:18,210 --> 00:10:20,340
model, managing their interaction.

157
00:10:20,700 --> 00:10:24,540
By introducing an AI gateway,
we can centralize and allocate

158
00:10:24,540 --> 00:10:28,320
task like model authentication,
authorization, prompt management.

159
00:10:28,709 --> 00:10:30,239
Caching and spending check.

160
00:10:30,479 --> 00:10:34,290
This way, the, these responsibility
are handled by the AI gateway,

161
00:10:34,290 --> 00:10:36,119
not within the application itself.

162
00:10:36,359 --> 00:10:39,569
Reducing the coupling between
the application and the model.

163
00:10:40,050 --> 00:10:44,189
The setup also make it easy to switch
to a different model or even use

164
00:10:44,189 --> 00:10:46,520
multiple model based on, specific needs.

165
00:10:47,020 --> 00:10:49,620
The last tissue involves,
model monitoring.

166
00:10:49,680 --> 00:10:53,880
My suggestion here is to use some
of the best practices already used

167
00:10:53,880 --> 00:10:57,900
in the cloud native world with few
adjustment to adapt them to the

168
00:10:57,900 --> 00:10:59,729
specific need of the model monitoring.

169
00:11:00,545 --> 00:11:04,505
Monitoring a model is essential
to, identify and solve

170
00:11:04,565 --> 00:11:05,975
issue before it's too late.

171
00:11:06,005 --> 00:11:08,735
But identifying what monitor is the key.

172
00:11:09,455 --> 00:11:13,205
The most immediate metrics to
collect are the operational

173
00:11:13,205 --> 00:11:15,725
metrics like CPU or RAM usage.

174
00:11:15,725 --> 00:11:19,190
That indicates if the system
is working properly, but.

175
00:11:20,070 --> 00:11:23,340
It is not enough in the space of model
monitoring, we need those to take a

176
00:11:23,340 --> 00:11:28,110
look at inputs that the model received
to be sure it received the right data

177
00:11:28,340 --> 00:11:32,330
in the right form to perform prediction
efficiently and the outputs of the

178
00:11:32,330 --> 00:11:34,460
model to evaluate its performance.

179
00:11:34,960 --> 00:11:38,650
All this attention is needed
to spot a eventual drift that

180
00:11:38,650 --> 00:11:40,090
could happen in our system.

181
00:11:40,330 --> 00:11:44,900
In this context, the drift, refer
to any changes that could impact

182
00:11:44,900 --> 00:11:49,970
the model accuracy, and that can be
essentially of three types data drift.

183
00:11:50,025 --> 00:11:51,105
Prediction drift.

184
00:11:51,165 --> 00:11:52,185
A concept drift.

185
00:11:52,665 --> 00:11:58,005
In simple terms, data drift occurs when
the statistical properties of input data

186
00:11:58,005 --> 00:12:03,165
changes leading to decreasing accuracy,
even if the model itself don't change.

187
00:12:03,795 --> 00:12:08,865
prediction drift instead refers to
prediction changes, even if the data

188
00:12:08,925 --> 00:12:11,595
doesn't change due to the model itself.

189
00:12:11,640 --> 00:12:18,340
Or to external factor, then constant
drift res, refers to changes in the

190
00:12:18,340 --> 00:12:22,690
relationship between the input data
and the target variable affecting the

191
00:12:22,690 --> 00:12:24,400
pattern that the model has learned.

192
00:12:24,900 --> 00:12:30,480
To keep track of all this aspect, we
can use true open source of software

193
00:12:30,510 --> 00:12:34,050
promises, which is a monitoring
toolkit to collect real time

194
00:12:34,050 --> 00:12:36,060
metrics in a time series database.

195
00:12:36,390 --> 00:12:40,950
And pharma, which offer an extensive
option for dashboard creation

196
00:12:40,950 --> 00:12:41,910
and database visualization.

197
00:12:42,300 --> 00:12:47,250
Completing Prometheus, allowing us
to visualize the connected metrics.

198
00:12:47,370 --> 00:12:47,400
Okay.

199
00:12:48,210 --> 00:12:51,690
Parameters and Grafana together
offer many benefits in the real

200
00:12:51,690 --> 00:12:53,340
realm of monitoring ML system.

201
00:12:53,340 --> 00:12:57,870
In fact, they allow us to collect a
wide range of metric from the system

202
00:12:57,870 --> 00:13:02,610
performance to the to specific model in
customizable way, in a real time way,

203
00:13:02,610 --> 00:13:08,010
offering also the possibility to set up
others on specific events or thresholds.

204
00:13:08,340 --> 00:13:10,785
Both are designed to
handle large scale data.

205
00:13:11,670 --> 00:13:16,320
Providing a high flexible system
for model monitoring, even when the

206
00:13:16,320 --> 00:13:18,990
complexity grows and the needs change.

207
00:13:19,490 --> 00:13:24,470
To wrap up, we discussed about many tools,
but just starting tools to the chain only

208
00:13:24,470 --> 00:13:26,480
increase the complexity of the system.

209
00:13:26,510 --> 00:13:30,920
We don't need just tool, we need a
way to adopt them with a cell service

210
00:13:30,920 --> 00:13:33,560
approach and with less friction possible.

211
00:13:34,010 --> 00:13:37,820
This way is true internal developer
platform and platform engineering

212
00:13:38,030 --> 00:13:41,095
by building an internal developer
platform that leverage this.

213
00:13:41,100 --> 00:13:46,350
Tool we can standardize and simplify
their adoption while creating power

214
00:13:46,350 --> 00:13:51,150
path for common use case that can
be consumed throughout a catalog

215
00:13:51,150 --> 00:13:53,130
of resources in a cell service way.

216
00:13:53,490 --> 00:13:56,910
Just for reference, if Leonardo
needed a development environment

217
00:13:56,910 --> 00:14:00,690
to run their experiments, just need
to access the catalog and choose

218
00:14:00,690 --> 00:14:04,230
the one with the Configurational
libraries that best with is need.

219
00:14:05,055 --> 00:14:07,875
The same happen when Leonardo
needs a cluster for distributed

220
00:14:07,875 --> 00:14:11,685
training or if delivery teams need
to set up the monitoring dashboard

221
00:14:11,685 --> 00:14:13,155
for a new services, for example.

222
00:14:13,655 --> 00:14:15,605
Analyzing all these factors.

223
00:14:15,665 --> 00:14:19,385
A question raised in my mind, could
the cloud native world and the

224
00:14:19,385 --> 00:14:23,675
application development world benefit
from the recent explosion of AI?

225
00:14:23,675 --> 00:14:27,410
And, if yes, how I deal
with platforms every day.

226
00:14:27,930 --> 00:14:32,310
So trying to find an answer to this
question, I started thinking to them.

227
00:14:32,710 --> 00:14:38,080
platforms are an heterogeneous set
of element based up of metadata

228
00:14:38,080 --> 00:14:43,360
repositories, pipelines, documentation,
and many other assets in various format.

229
00:14:43,420 --> 00:14:48,040
And using a platform today actually
means managing all these assets and

230
00:14:48,040 --> 00:14:50,470
being able to go learn and navigate them.

231
00:14:50,540 --> 00:14:50,600
It.

232
00:14:51,100 --> 00:14:56,170
So what if instead of having to work
against this asset, we made them

233
00:14:56,170 --> 00:15:00,460
work together and allowing us to
access them in a unified interface.

234
00:15:00,960 --> 00:15:04,410
This is where the conversational
dev device comes into play with

235
00:15:04,410 --> 00:15:07,620
conversational dev device are
referred to a way to elevate the

236
00:15:07,620 --> 00:15:11,700
use of platform and then enhance the
dev device of those who use them.

237
00:15:11,970 --> 00:15:17,160
It involves the use of our R system,
applied the up to the platform assets,

238
00:15:17,220 --> 00:15:23,040
allowing us to leverage the capabilities
of Gen AI to analyze and query the

239
00:15:23,040 --> 00:15:25,260
platform assets using natural language.

240
00:15:25,760 --> 00:15:26,930
That's not just a reason.

241
00:15:26,930 --> 00:15:31,760
These already available in an early stage
version, and I invite you to try it over

242
00:15:31,760 --> 00:15:36,560
your own assets by scanning the QR code
in the last slide to access the qap repo.

243
00:15:37,060 --> 00:15:39,849
Imagine a world where
you have a virtual that.

244
00:15:40,569 --> 00:15:45,369
Speaks like you with the added
benefits of perfectly understanding

245
00:15:45,369 --> 00:15:49,060
the context of your platform because
it is actually your platform.

246
00:15:49,390 --> 00:15:56,020
Now, imagine what you could do with this
assistant that know all the models, the

247
00:15:56,020 --> 00:15:58,329
lineage, then the pipeline around them.

248
00:15:58,779 --> 00:16:02,869
It's now, the tool you can use
the best practices for using them

249
00:16:02,869 --> 00:16:05,629
and even how other have used them.

250
00:16:05,930 --> 00:16:10,339
Troubleshooting production issues or
conducting a proof of concept is such

251
00:16:10,339 --> 00:16:12,829
a context is on a wall, new level.

252
00:16:13,329 --> 00:16:16,959
The presentation is fine
and the before said goodbye.

253
00:16:16,959 --> 00:16:20,049
Let me recap the key points
from the session first.

254
00:16:20,079 --> 00:16:24,759
Applying all solution to new problem
can be a smart way to solve them

255
00:16:24,759 --> 00:16:26,439
without reinventing the wheel.

256
00:16:26,819 --> 00:16:30,800
Second, platform, A, a highly
customizable tool that can met

257
00:16:30,800 --> 00:16:35,919
specific needs and, can streamline
the work even in the realm of mops.

258
00:16:36,189 --> 00:16:42,069
Finally, I don't know if AI will still our
job in the future, but now it's a powerful

259
00:16:42,069 --> 00:16:44,199
tool to enhance the developer experience.

260
00:16:44,699 --> 00:16:45,689
Thank you guys.

261
00:16:46,500 --> 00:16:49,500
Here is a feedback form where
you can leave your feedbacks.

262
00:16:49,770 --> 00:16:50,429
See you.

