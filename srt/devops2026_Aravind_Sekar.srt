1
00:00:00,500 --> 00:00:00,950
Speaker 3: Hi all.

2
00:00:01,400 --> 00:00:04,010
This is Arvind Shaker, director
of Engineering at Twilio.

3
00:00:04,670 --> 00:00:08,390
I have over two likes, building
and scaling distributed systems,

4
00:00:08,780 --> 00:00:11,719
and today we're gonna talk a
little bit about reliability,

5
00:00:11,870 --> 00:00:13,850
specifically AI powered reliability.

6
00:00:14,660 --> 00:00:16,309
Without further ado, let's get into it.

7
00:00:16,810 --> 00:00:19,630
Reliability these days is a
fundamental business requirement.

8
00:00:20,130 --> 00:00:22,980
It directly impacts a
company's bottom line.

9
00:00:23,480 --> 00:00:28,700
Imagine this, what will happen in
the US if 9 1 1 emergency services

10
00:00:29,030 --> 00:00:30,950
is not available for a few hours?

11
00:00:31,490 --> 00:00:32,600
Sounds disastrous, right?

12
00:00:32,930 --> 00:00:33,200
Yep.

13
00:00:33,410 --> 00:00:38,720
That is the case because today we are
all reliant on these kind of systems

14
00:00:38,870 --> 00:00:43,290
being available all the time, and
it's a reasonable expectation for us

15
00:00:43,620 --> 00:00:48,300
to expect the services that we use
to be up and available and reliable.

16
00:00:49,080 --> 00:00:52,890
That goes same for any software that's
out there that's serving customers.

17
00:00:53,820 --> 00:00:55,920
Reliability equals business continuity.

18
00:00:56,420 --> 00:01:00,955
It is estimated that on an average
a minute of downtime is equivalent

19
00:01:00,955 --> 00:01:04,270
to about $5,600 of revenue loss.

20
00:01:04,630 --> 00:01:08,570
So with extended out outages, you
can imagine what would happen.

21
00:01:08,960 --> 00:01:09,715
We saw what happened.

22
00:01:10,365 --> 00:01:14,085
On October 20th and there was
an AWS outage in uscs fund,

23
00:01:14,585 --> 00:01:19,055
this estimated that this outage
itself impacted millions of customers

24
00:01:19,265 --> 00:01:23,775
causing tens of millions of dollars
of revenue impact across the globe.

25
00:01:24,275 --> 00:01:28,445
So when systems fail, conversions
drop, customers lose trust and

26
00:01:28,445 --> 00:01:30,155
competitors gain advantage.

27
00:01:30,485 --> 00:01:35,915
So it is imperative that reliability
is an absolute need across all systems.

28
00:01:36,415 --> 00:01:37,675
The reliability crisis.

29
00:01:38,175 --> 00:01:41,955
So over the last, 10, 15 years.

30
00:01:42,345 --> 00:01:45,195
We've seen the advent of microservices
architectures falling up.

31
00:01:45,945 --> 00:01:48,375
So we started creating a
bunch of microservices.

32
00:01:48,985 --> 00:01:50,515
We started breaking down monoliths.

33
00:01:50,935 --> 00:01:55,605
And with that, what happened was
the complexity of dependencies

34
00:01:55,605 --> 00:01:57,675
grew as we created more services.

35
00:01:58,175 --> 00:02:03,875
Also, with that, what happened was
that an impact or a downtime or an

36
00:02:03,875 --> 00:02:08,680
outage in one of these services,
the impact cascaded along and would.

37
00:02:09,420 --> 00:02:11,940
It would cause widespread
outages somewhere else.

38
00:02:12,440 --> 00:02:17,380
Also with, we pride ourselves
over the years in increasing the

39
00:02:17,380 --> 00:02:19,359
number of deployments that you had.

40
00:02:19,909 --> 00:02:22,889
So previously we used to
have, one or two deployments a

41
00:02:22,889 --> 00:02:24,689
month or maybe even a quarter.

42
00:02:24,989 --> 00:02:28,049
And these days we are doing multiple
deployments in a day when the

43
00:02:28,049 --> 00:02:29,459
number of deployments increase.

44
00:02:30,239 --> 00:02:33,479
The potential of vulnerabilities that
go into protection also increase.

45
00:02:34,109 --> 00:02:38,969
So we are living in this kind of
a crisis today where the number

46
00:02:38,969 --> 00:02:43,619
of systems, microservices that we
maintain increased, have increased.

47
00:02:44,019 --> 00:02:47,019
The scale at which we operate
has increased the number of

48
00:02:47,019 --> 00:02:50,929
deployments that we that we have
per day, that has increased.

49
00:02:51,289 --> 00:02:54,349
And the interdependencies between
systems have also increased.

50
00:02:54,849 --> 00:02:55,714
So how do we maintain.

51
00:02:56,574 --> 00:02:58,974
These kind of systems today, right?

52
00:02:59,364 --> 00:03:02,514
We have a set of monitors
that we have set up, right?

53
00:03:02,544 --> 00:03:07,674
All systems have typically monitors,
and these monitors are threshold based.

54
00:03:07,674 --> 00:03:11,334
For example, you would, alert
someone if if the number of five

55
00:03:11,334 --> 00:03:14,749
X is that a system and that an a
PA throws goes beyond a threshold.

56
00:03:14,929 --> 00:03:19,359
Or if the CPU spikes, beyond a certain
percentage, you alert something.

57
00:03:20,319 --> 00:03:23,329
Anything with the heap or you have
different thresholds that we set

58
00:03:23,329 --> 00:03:24,719
up for each of these monitors.

59
00:03:24,719 --> 00:03:25,679
That's one, one way.

60
00:03:26,429 --> 00:03:33,769
The second thing is when these thresholds
are broken through these monitors, we

61
00:03:33,769 --> 00:03:36,004
start alerting people or what happens.

62
00:03:36,504 --> 00:03:40,214
Whenever there is a bigger outage
when, say an entire region goes down

63
00:03:40,214 --> 00:03:43,809
or a full system goes down due to some
infrastructure issues or networking

64
00:03:43,809 --> 00:03:49,209
issues, the number of alerts that we get
from each of these monitors is humongous.

65
00:03:49,219 --> 00:03:53,139
Say I'm on call and there is an outage
that's happening in, in, in a region that

66
00:03:53,139 --> 00:03:55,380
we have deployed code and each of these.

67
00:03:56,010 --> 00:04:00,000
Hosts that have these monitors, the
thresholds are broken, monitors are

68
00:04:00,000 --> 00:04:03,480
triggered, and I start getting alerts
and I would be getting like a hundred,

69
00:04:03,480 --> 00:04:05,700
200,000 alerts at the same time.

70
00:04:06,090 --> 00:04:07,440
So what would I do at that point in time?

71
00:04:07,440 --> 00:04:09,240
Which one should I look and
which one should I not look?

72
00:04:09,660 --> 00:04:11,160
And that's called an alert storm.

73
00:04:11,160 --> 00:04:14,910
And alert storm is disastrous when
you're on call because you wouldn't

74
00:04:14,945 --> 00:04:18,830
know what to prioritize and what not to
prioritize root cause analysis, right?

75
00:04:19,295 --> 00:04:20,615
So root cause analysis.

76
00:04:21,185 --> 00:04:25,875
With the amount of telemetry that we
have today, we emit, some companies emit

77
00:04:26,655 --> 00:04:30,105
billions, multiple billions, sometimes
trillions of transactions every day.

78
00:04:30,675 --> 00:04:34,885
And how do you how do we analyze
all this data that's so that, that's

79
00:04:34,975 --> 00:04:38,315
tremendous amount of data that we
emit these days and analyzing them

80
00:04:38,315 --> 00:04:40,385
to even identify root cause analysis.

81
00:04:40,805 --> 00:04:44,005
Matching data across these
systems becomes a nightmare.

82
00:04:44,505 --> 00:04:45,255
Knowledge cycles.

83
00:04:45,630 --> 00:04:49,770
So when these systems expand,
as systems grow, the knowledge

84
00:04:50,580 --> 00:04:55,230
that we have inherently is not
usually documented across and not

85
00:04:55,290 --> 00:04:57,030
documented in the standard format.

86
00:04:57,540 --> 00:05:01,230
So these things stick into people's
minds, and then when people leave, the

87
00:05:01,230 --> 00:05:03,169
knowledge goes away, the on-call burnout.

88
00:05:04,070 --> 00:05:07,929
So we keep firefighting whenever
there are system outages.

89
00:05:08,429 --> 00:05:10,949
We have these huge alert
storms that come in.

90
00:05:11,280 --> 00:05:14,879
We keep firefighting, we keep looking
because we are not sure where the outage

91
00:05:14,879 --> 00:05:19,529
is because of too many alerts that come
in and firefighting becomes a unknown.

92
00:05:19,979 --> 00:05:21,989
We don't look at how do we prevent issues.

93
00:05:21,989 --> 00:05:25,169
We look at how do we resolve
issues when, once they occur.

94
00:05:25,679 --> 00:05:29,189
So that is the current state, and
these are huge limitations in the way

95
00:05:29,189 --> 00:05:32,039
we handle support these scale systems.

96
00:05:32,539 --> 00:05:34,159
AI is.

97
00:05:34,924 --> 00:05:40,054
Is being seen as a panacea for everything,
but here we should be careful as

98
00:05:40,054 --> 00:05:42,644
to how we use ai in AI operations.

99
00:05:42,644 --> 00:05:47,835
So wherever we use AI in operating,
in operations while we maintain these

100
00:05:47,924 --> 00:05:50,204
large systems, it's called AI ops.

101
00:05:50,699 --> 00:05:50,729
Okay.

102
00:05:51,509 --> 00:05:53,879
Now, where can AI help us?

103
00:05:54,030 --> 00:05:58,919
So AI can help us, or ML can help
us in pattern detection, right?

104
00:05:59,189 --> 00:06:05,369
Something has happened before, and if
the same pattern happens again, ML can

105
00:06:05,369 --> 00:06:07,590
help us detect those patterns, right?

106
00:06:07,679 --> 00:06:11,960
For example if if during
the holidays, see that?

107
00:06:12,434 --> 00:06:16,025
The number of requests that we,
that an API gets on the system gets

108
00:06:16,424 --> 00:06:19,544
increases during holidays, then
that's a pattern and it happens

109
00:06:19,544 --> 00:06:21,565
every every year during the holiday.

110
00:06:21,685 --> 00:06:28,015
These patterns are detected automatically
by ml, and we can definitely use ML

111
00:06:28,015 --> 00:06:29,995
models to identify these patterns.

112
00:06:30,495 --> 00:06:32,475
Then comes predictive analytics.

113
00:06:32,895 --> 00:06:38,415
So once we see that, once we know there
are these patterns that are available,

114
00:06:38,745 --> 00:06:43,065
we can also start predicting using ML
models when an event can reoccur or

115
00:06:43,065 --> 00:06:46,545
when an event is going to occur, and
we can start alerting people saying

116
00:06:46,545 --> 00:06:49,659
that, Hey, there is an event that's
gonna happen and let's take care of

117
00:06:49,665 --> 00:06:51,775
that before the event actually happens.

118
00:06:52,525 --> 00:06:54,175
The next one is intelligent automation.

119
00:06:54,175 --> 00:06:56,900
So if you've already predicted
that something is gonna happen.

120
00:06:57,585 --> 00:07:02,414
Or if, if you've seen the remediation
steps that we've already taken, when

121
00:07:02,414 --> 00:07:05,534
these kind of issues have happened in
the past, we can automatically start

122
00:07:05,534 --> 00:07:08,354
executing those remediation steps.

123
00:07:08,754 --> 00:07:12,764
Using ML models and insights,
augmented intelligence, so insights.

124
00:07:12,764 --> 00:07:16,769
So with the large number of data
we get out of, say, telemetry,

125
00:07:16,769 --> 00:07:18,569
logs, et cetera, events, et cetera.

126
00:07:19,459 --> 00:07:22,939
Instead of analyzing all this
data manually, ML models can

127
00:07:22,939 --> 00:07:23,959
definitely help us there.

128
00:07:24,259 --> 00:07:25,639
We can go a little deeper into this,

129
00:07:26,139 --> 00:07:29,079
the reliability framework, right?

130
00:07:29,079 --> 00:07:30,339
That's based on ML and ai.

131
00:07:30,839 --> 00:07:34,069
So the, it's, it all starts
with anomaly detection.

132
00:07:34,609 --> 00:07:35,449
So how does that work?

133
00:07:35,949 --> 00:07:40,989
You have data that's coming out from
systems, and this data could be tele

134
00:07:40,989 --> 00:07:46,149
metadata, it could be events, could be
logs, could be anything that's coming out.

135
00:07:46,929 --> 00:07:51,989
And bringing all that data
together and analyzing them to

136
00:07:51,989 --> 00:07:54,524
find anomalies is what ML can do.

137
00:07:55,304 --> 00:07:56,729
So that's the first step.

138
00:07:56,999 --> 00:07:59,954
Finding anomalies from all
the data that's coming out.

139
00:08:00,454 --> 00:08:03,364
The second step is temporal
seasonal awareness.

140
00:08:03,864 --> 00:08:08,904
So these models, when they're fed
with seasonal awareness in the sense

141
00:08:09,294 --> 00:08:12,674
all the holidays that are coming up,
so during the Super Bowl match the

142
00:08:12,729 --> 00:08:15,109
number of pizza orders could go up.

143
00:08:15,649 --> 00:08:17,899
So that is something that's
seasonal, so you know that, okay.

144
00:08:18,274 --> 00:08:20,614
That is a Super Bowl match
that's gonna happen this weekend.

145
00:08:21,334 --> 00:08:25,274
And and I know that the number of
requests hitting my API is gonna go

146
00:08:25,274 --> 00:08:30,045
high, and that is also an input to,
to ml. And the ML models are going

147
00:08:30,045 --> 00:08:37,065
to use that to predict what traffic
patterns across these across the days.

148
00:08:37,565 --> 00:08:40,505
Cross signal correlation
as we discussed before.

149
00:08:41,405 --> 00:08:43,475
Metrics, logs, traces.

150
00:08:44,045 --> 00:08:47,305
These are all different data now.

151
00:08:47,305 --> 00:08:51,955
Bringing them all together and
finding patterns across this data is

152
00:08:51,955 --> 00:08:54,115
important, and ML can help us do that.

153
00:08:54,615 --> 00:08:58,725
Causality over correlation
cause and effect, right?

154
00:08:59,055 --> 00:09:00,105
So we know.

155
00:09:00,855 --> 00:09:05,015
So today what happens is humans
are the only ones who can find out.

156
00:09:05,515 --> 00:09:10,245
This happened and any an incident happened
and this was the root cause, and we are

157
00:09:10,245 --> 00:09:12,135
able to do that after a lot of analysis.

158
00:09:12,405 --> 00:09:16,225
But ML models can do that
when they have all the data.

159
00:09:16,945 --> 00:09:16,975
Okay.

160
00:09:17,475 --> 00:09:23,190
So a pri a primitive approach to
this would be, okay, I have an

161
00:09:23,190 --> 00:09:27,240
incident that's going on, another
incident in another organization.

162
00:09:27,300 --> 00:09:32,750
Or say in a say in AWS or any other
company that I'm reliant on is

163
00:09:32,750 --> 00:09:34,250
happening, so I can relate them.

164
00:09:34,940 --> 00:09:39,580
So that is a primitive idea,
but beyond that, ML can identify

165
00:09:39,580 --> 00:09:41,110
root causes, not just based.

166
00:09:41,725 --> 00:09:45,115
On coincident events, it go, can
go much more deeper than that.

167
00:09:45,615 --> 00:09:48,735
And continuous adoption
systems automatically adjust as

168
00:09:48,735 --> 00:09:50,685
applications and workloads evolve.

169
00:09:51,185 --> 00:09:53,925
So how do we build this for our systems?

170
00:09:54,495 --> 00:10:01,995
The first step to use AI for reliable
operations is streaming telemetry.

171
00:10:02,775 --> 00:10:04,005
Telemetry is important.

172
00:10:04,005 --> 00:10:08,625
Real time collection of metrics logs,
distributed traces is super critical.

173
00:10:08,625 --> 00:10:13,335
If you are not doing that today,
the first step in having a reliable

174
00:10:13,335 --> 00:10:17,955
system and doing that through AI
is to stream the right telemetry

175
00:10:18,525 --> 00:10:20,235
to have the right telemetry output.

176
00:10:20,735 --> 00:10:25,655
Then once you have this data being
emitted out of a system, you could.

177
00:10:26,225 --> 00:10:32,075
Implement or deploy ML models
that would analyze streaming data.

178
00:10:32,575 --> 00:10:36,855
And doesn't mean that you have
to deploy heavy models to to

179
00:10:37,215 --> 00:10:38,535
to analyze the streaming data.

180
00:10:38,535 --> 00:10:41,145
You could be ate models and
you could keep it tiered.

181
00:10:41,145 --> 00:10:44,385
So at first step you analyze
this data, like using lightweight

182
00:10:44,385 --> 00:10:47,805
models and then aggregate that data
and then use heavyweight models

183
00:10:48,105 --> 00:10:49,905
to to find to predict, to do.

184
00:10:50,730 --> 00:10:53,460
To do predictions, to do
anomaly, detections, et cetera.

185
00:10:53,460 --> 00:10:57,830
So it could be a layered approach,
but all this inference that we are

186
00:10:57,830 --> 00:10:59,330
getting out should be near real time.

187
00:11:00,050 --> 00:11:03,860
You cannot have an event that happens
now and then have these models run in

188
00:11:03,860 --> 00:11:07,520
the background, and then after 20 minutes
or 30 minutes you have a prediction

189
00:11:07,520 --> 00:11:10,700
or you have a detection of an anomaly
and then you start alerting people.

190
00:11:10,700 --> 00:11:13,770
That is not how that
is not a useful thing.

191
00:11:13,800 --> 00:11:17,550
So you should have near real
time inference of what's

192
00:11:17,550 --> 00:11:19,140
happening in these systems.

193
00:11:19,640 --> 00:11:20,630
Model performance.

194
00:11:20,720 --> 00:11:22,790
So imagine this, right?

195
00:11:22,790 --> 00:11:25,260
You have fraud detection, in credit cards.

196
00:11:25,260 --> 00:11:28,420
This is one of the common
use cases for ML models.

197
00:11:29,020 --> 00:11:33,430
You deploy ML models to identify a
fraud in a credit card transaction.

198
00:11:34,060 --> 00:11:38,260
Imagine using the same model
that we developed 10 years ago.

199
00:11:38,760 --> 00:11:40,410
It's not gonna work today because.

200
00:11:40,830 --> 00:11:41,430
The fraud.

201
00:11:41,480 --> 00:11:44,720
The ways in which fraud happen in
credit cards have changed over time.

202
00:11:45,110 --> 00:11:49,350
So we have to keep updating, these
models and that's what this means,

203
00:11:49,400 --> 00:11:51,080
and same thing applies everywhere.

204
00:11:51,470 --> 00:11:55,700
So continuous validation ensures
model accuracy as systems and data

205
00:11:55,700 --> 00:11:57,710
systems and data distributions evolve.

206
00:11:58,210 --> 00:11:59,830
And you should have some
safe rollback mechanisms.

207
00:11:59,830 --> 00:12:03,040
That we are seeing more false positives.

208
00:12:03,925 --> 00:12:07,375
Then there should be an easy
way or mechanism to roll back

209
00:12:07,375 --> 00:12:11,635
our model and deploy older ones
for safer for better safety.

210
00:12:12,135 --> 00:12:15,705
So overall we should balance
real time responsiveness with

211
00:12:15,705 --> 00:12:16,605
computational efficiency.

212
00:12:17,105 --> 00:12:20,595
Moving on another area where you can have.

213
00:12:21,095 --> 00:12:27,235
A very good bang for the bug using
AI ops is capacity management.

214
00:12:27,735 --> 00:12:33,250
Say you as an example, let's take the same
example of of a Super Bowl match, right?

215
00:12:33,340 --> 00:12:36,730
You know that a Super Bowl match is gonna
happen and you know that the holidays

216
00:12:36,730 --> 00:12:40,230
are coming up, you know that the fabric
is gonna increase to your service.

217
00:12:40,980 --> 00:12:44,000
What we do, we typically,
scale up our services.

218
00:12:44,855 --> 00:12:48,185
Scale up the number of holes,
number of pos that we have, and

219
00:12:48,195 --> 00:12:49,905
and handle the traffic accordingly.

220
00:12:50,405 --> 00:12:55,825
How many of us are guilty of scaling
too much during these periods and keep

221
00:12:55,825 --> 00:12:59,815
them scaled up because we don't know
what kind of traffic is gonna hit us.

222
00:12:59,905 --> 00:13:05,535
And even though, our systems are highly
scalable, they can scale whenever

223
00:13:05,535 --> 00:13:07,605
there is a, an increase in traffic.

224
00:13:08,105 --> 00:13:11,555
The track, the scalability
doesn't happen immediately.

225
00:13:11,945 --> 00:13:14,585
It takes a few minutes
for any host to put up.

226
00:13:14,975 --> 00:13:18,995
So it doesn't mean that you have, say,
a thousand hosts now, and the next

227
00:13:18,995 --> 00:13:20,495
second you're gonna have 2000 hosts.

228
00:13:20,495 --> 00:13:21,635
It doesn't happen like that.

229
00:13:21,635 --> 00:13:23,525
It takes time for us to scale up.

230
00:13:24,065 --> 00:13:26,105
And during that time
then we are scaling up.

231
00:13:26,705 --> 00:13:31,160
We customers may see issues in
terms of latency, availability, et

232
00:13:31,160 --> 00:13:32,160
cetera, which is not acceptable.

233
00:13:32,940 --> 00:13:35,440
So AI can predict.

234
00:13:36,335 --> 00:13:42,435
When we need to scale up and preemptively
scale up when required, just in time.

235
00:13:42,435 --> 00:13:47,565
It's like just in time scaling up,
but with a good sense of when it is

236
00:13:47,565 --> 00:13:49,110
needed and how much scaling is needed.

237
00:13:49,610 --> 00:13:55,670
And 92% SLO adherence is basically
what this means here, is that by

238
00:13:55,670 --> 00:13:58,815
doing this, by using ai, we can.

239
00:13:59,670 --> 00:14:06,050
We can adhere to SLOs 92% of the time as
we scale up systems preemptively using ai.

240
00:14:06,550 --> 00:14:07,720
Similarly cost reduction.

241
00:14:08,110 --> 00:14:11,980
As I said before, I'm guilty of
having scaled up systems, having

242
00:14:11,980 --> 00:14:16,110
doubled the capacity and having
it scaled up without scaling down.

243
00:14:16,950 --> 00:14:20,930
Even before for quite a number of
days when it was not even required.

244
00:14:21,440 --> 00:14:25,790
So just in time scaling
would reduce the cost by 35%.

245
00:14:26,450 --> 00:14:27,890
So super important.

246
00:14:28,280 --> 00:14:33,045
This is a very important place
where AI can be used in, in, in

247
00:14:33,045 --> 00:14:37,595
scaling systems, moving on smarter
detection and root cause analysis.

248
00:14:37,775 --> 00:14:41,735
The first step is always
anomaly detection, right?

249
00:14:42,245 --> 00:14:44,735
ML identifies deviations
from normal behavior.

250
00:14:45,425 --> 00:14:46,595
So I'll give you an example.

251
00:14:46,715 --> 00:14:48,935
You deploy a new version of software.

252
00:14:49,715 --> 00:14:55,585
It was, to say a new divert, one person
or 10% of your traffic to the new version.

253
00:14:55,585 --> 00:14:59,545
Now the question is, how is
the new version working when

254
00:14:59,545 --> 00:15:00,625
compared to the previous version?

255
00:15:01,515 --> 00:15:05,145
You cannot expose a hundred percent
of your customers to the new version

256
00:15:05,145 --> 00:15:06,465
without knowing how it's performing.

257
00:15:06,945 --> 00:15:12,165
So you divert a percentage of the traffic,
but you have to so analyze how it is

258
00:15:12,165 --> 00:15:14,385
working compared to the previous version.

259
00:15:15,165 --> 00:15:16,545
ML models can help us do that.

260
00:15:16,995 --> 00:15:20,775
It can analyze how the old version is
working compared to the new version

261
00:15:20,985 --> 00:15:23,175
and tell you what the differences are.

262
00:15:23,675 --> 00:15:27,295
Signal correlation, again,
ML models can analyze.

263
00:15:27,565 --> 00:15:31,345
Related anomalies across
services, logs, cases, et cetera.

264
00:15:31,435 --> 00:15:35,275
So it doesn't have to be just
your logs or just your case.

265
00:15:35,275 --> 00:15:40,635
It can do anomaly detection across
service logs, cases, et cetera.

266
00:15:41,135 --> 00:15:45,130
Dependency mapping, real time
topology, understanding shows how

267
00:15:45,130 --> 00:15:46,685
issues propagate through the system.

268
00:15:47,045 --> 00:15:51,455
So with when we have hundreds
of thousands of microservices

269
00:15:51,845 --> 00:15:53,050
supporting in the backend.

270
00:15:53,550 --> 00:15:59,460
It can, ML models can help you
trace, trace the issue and it'll

271
00:15:59,460 --> 00:16:02,220
also tell you how issues are
propagating through the system.

272
00:16:02,720 --> 00:16:03,800
Blast radius analysis.

273
00:16:03,800 --> 00:16:07,490
So when you do root cause analysis
after incidents, or even for

274
00:16:07,850 --> 00:16:13,545
even for during design, we can,
it is always important for us.

275
00:16:13,895 --> 00:16:17,285
To reduce the blast radius
in case of an incident.

276
00:16:18,065 --> 00:16:23,195
So which can ML models can help us
identify quickly, identify which services

277
00:16:23,195 --> 00:16:29,135
and users are affected by an incident,
and also will help us in designing good

278
00:16:29,135 --> 00:16:33,665
systems with where, whenever there is
an outage, which will reduce the blast

279
00:16:33,670 --> 00:16:35,410
release, root cause identification.

280
00:16:35,860 --> 00:16:36,580
To be honest.

281
00:16:36,985 --> 00:16:38,365
It's in the nascent stage now.

282
00:16:38,845 --> 00:16:43,455
And but I think once the data that we
provide to these models are good enough,

283
00:16:43,835 --> 00:16:47,925
then root cause analysis or root cause
identification is something that ML

284
00:16:47,925 --> 00:16:49,965
models can definitely help us get better.

285
00:16:50,465 --> 00:16:57,185
All these things reduce M-T-T-I-M-T-T-I
is being time to identify basically.

286
00:16:57,980 --> 00:17:01,430
Identifying that there is an issue going
on so that you can do remediation quickly,

287
00:17:01,930 --> 00:17:03,400
going to autonomous remediation.

288
00:17:03,400 --> 00:17:10,060
So now assume that you are using AI to
identify that an issue is going on, right?

289
00:17:10,150 --> 00:17:13,560
We have these anomaly detections, et
cetera, et cetera, and now we know

290
00:17:13,560 --> 00:17:14,670
that there is an issue going on.

291
00:17:14,820 --> 00:17:15,930
So what does.

292
00:17:16,430 --> 00:17:17,480
What should the system do?

293
00:17:17,780 --> 00:17:22,770
Ideally it would page an on-call
engineer and the on-call engineer would

294
00:17:22,770 --> 00:17:24,300
say, okay, something is happening.

295
00:17:24,330 --> 00:17:25,050
Let me go fix it.

296
00:17:25,800 --> 00:17:28,620
And how does fixing happen?

297
00:17:29,120 --> 00:17:32,430
We can use gentech mitigations
like Google has mentioned in their

298
00:17:32,430 --> 00:17:35,460
Sari book, there are a few generic
mitigations that you could do.

299
00:17:35,460 --> 00:17:36,935
You control back code, deploy.

300
00:17:37,590 --> 00:17:40,650
See if there was a deployment that
happened, roll it back, see if there

301
00:17:40,650 --> 00:17:44,070
was a bad host, replace the host
or take it out of load balancer.

302
00:17:44,370 --> 00:17:47,320
So there are a few things
that, you could do, limit the

303
00:17:47,320 --> 00:17:49,120
traffic, et cetera, et cetera.

304
00:17:49,540 --> 00:17:57,490
So we know that generic mitigations can
quickly fix an issue without much thought.

305
00:17:58,150 --> 00:18:01,990
Now when we know that there are only a
set of gender mitigations that can happen.

306
00:18:02,620 --> 00:18:05,500
The question is, can that
be automated as well?

307
00:18:05,740 --> 00:18:07,480
So you know that an issue is going on.

308
00:18:07,540 --> 00:18:10,960
Can we have automated systems
that would resolve these issues as

309
00:18:10,960 --> 00:18:12,400
well, that will fix these issues?

310
00:18:13,090 --> 00:18:20,310
But the question is who can we trust
ML systems to make the decision of

311
00:18:20,310 --> 00:18:22,050
executing these remediations or not?

312
00:18:22,500 --> 00:18:23,790
And that's an evolving process.

313
00:18:23,790 --> 00:18:28,380
So one step is, yeah, you've
identified the problem.

314
00:18:28,880 --> 00:18:34,430
You can use genetic mitigations to
fix these problems, and ML models here

315
00:18:34,850 --> 00:18:40,340
can give us a recommendation for what
kind of genetic mitigations to use.

316
00:18:41,000 --> 00:18:46,220
Once we are comfortable with that, then
we can allow AI to execute these genetic

317
00:18:46,310 --> 00:18:48,450
mitigations without, even paging everyone.

318
00:18:48,950 --> 00:18:50,120
Human AI collaboration model.

319
00:18:50,170 --> 00:18:50,800
That's what this is.

320
00:18:50,800 --> 00:18:55,360
The AI suggests what kind of
remediation a human should do.

321
00:18:55,840 --> 00:18:57,310
The human would validate that.

322
00:18:57,310 --> 00:19:03,110
For example, if the AI suggests, okay
there is a high number of five Xs that

323
00:19:03,110 --> 00:19:06,420
are coming out of this this new version
of code, I would suggest you roll back

324
00:19:06,420 --> 00:19:11,460
the code, roll back a deployment that
happened, one hour ago, and humans

325
00:19:11,460 --> 00:19:12,870
say, okay, yep, this makes sense.

326
00:19:12,870 --> 00:19:13,500
Let me do that.

327
00:19:14,000 --> 00:19:17,640
Or they can say, okay, ai, would
allow, would give the key to AI

328
00:19:17,640 --> 00:19:19,290
and say, yep you execute rollback.

329
00:19:19,290 --> 00:19:22,350
The AI executes rollback and
all that knowledge is captured.

330
00:19:22,350 --> 00:19:24,210
It becomes a loop and
the AI slowly learns.

331
00:19:24,300 --> 00:19:24,450
Yep.

332
00:19:24,760 --> 00:19:26,980
Whenever this kind of issue
happens, you can do a rollback of

333
00:19:26,980 --> 00:19:27,970
code if there was a deployment.

334
00:19:28,570 --> 00:19:33,250
So that whole cycle keeps repeating till
the humans can move out of the loop,

335
00:19:33,580 --> 00:19:35,500
and AI makes decisions automatically.

336
00:19:36,000 --> 00:19:37,620
Things like, the future.

337
00:19:37,650 --> 00:19:42,660
There are advanced anomaly detections
here where the anomaly detection,

338
00:19:42,660 --> 00:19:47,110
comes through various different sources
into a single powerful representation.

339
00:19:47,930 --> 00:19:54,690
Also we are seeing that models are
very good at even when ML models,

340
00:19:54,740 --> 00:19:56,600
are dependent on past data, right?

341
00:19:57,410 --> 00:19:58,700
And a large number of data.

342
00:19:58,730 --> 00:20:03,760
A large number of events that happened
in the past to predict the future.

343
00:20:04,570 --> 00:20:07,660
But we are seeing that few short
learning is also happening.

344
00:20:07,660 --> 00:20:13,220
What that means is that even the rarest
and the most elusive failure modes are

345
00:20:13,220 --> 00:20:16,400
also being predicted with these ML models.

346
00:20:16,430 --> 00:20:19,510
And that is is really
coming up these days.

347
00:20:20,010 --> 00:20:21,090
Moving on, right?

348
00:20:21,540 --> 00:20:22,800
What is the overall impact?

349
00:20:22,850 --> 00:20:27,170
Personally, I've seen a huge
impact in a few areas, right?

350
00:20:27,650 --> 00:20:29,030
The first one is noise reduction.

351
00:20:29,240 --> 00:20:30,170
Let me give you an example.

352
00:20:30,670 --> 00:20:34,420
For example, if you're using, each
company uses different tools for

353
00:20:34,770 --> 00:20:38,030
for monitoring, alerting, ticket
management, et cetera, et cetera.

354
00:20:38,030 --> 00:20:38,900
You may be using.

355
00:20:39,400 --> 00:20:45,430
A bunch of tools, say for example,
Datadog or Big Panda, Dynatrace, Splunk,

356
00:20:45,520 --> 00:20:49,640
or PagerDuty no matter what the tools
are, all these tools are bringing

357
00:20:49,640 --> 00:20:52,670
in ai, ai operations into the mix.

358
00:20:53,150 --> 00:20:57,980
And one of the things that we did
was that we saw huge pager noise.

359
00:20:58,760 --> 00:21:03,629
And as I mentioned before, an outage
would raise a bunch of would break

360
00:21:03,629 --> 00:21:06,120
the threshold of a bunch of monitors,
and all those monitors would.

361
00:21:06,630 --> 00:21:12,020
Send out alerts to the on-call person
and for every incident there would be

362
00:21:12,060 --> 00:21:17,679
hundreds of pages that would reach the
on-call person, and the page storm would

363
00:21:17,679 --> 00:21:19,509
cause more confusion than anything else.

364
00:21:19,899 --> 00:21:25,239
So what we did was PagerDuty
had a had a feature to group.

365
00:21:25,929 --> 00:21:32,929
These alerts into one so that the on-call
person doesn't receive as many alerts.

366
00:21:33,020 --> 00:21:37,380
And you could intelligently group
similar alerts into one alert

367
00:21:37,710 --> 00:21:42,330
and say, okay, I'm hearing, I'm
seeing a bunch of monitors failing.

368
00:21:42,630 --> 00:21:45,360
Here is one alert for you and you
may want to look into this area.

369
00:21:45,860 --> 00:21:50,785
That has really helped us in reducing
noise and alert fatigue in alert storms.

370
00:21:51,655 --> 00:21:53,545
By 60%, which is huge.

371
00:21:53,575 --> 00:21:57,085
So when an incident these days
happen, the number of pages that an

372
00:21:57,085 --> 00:22:01,705
oncology has reduced so much, now
the oncology can go and focus on what

373
00:22:01,705 --> 00:22:07,065
really matters and not focus on not
focus on acknowledging these pages.

374
00:22:07,565 --> 00:22:09,875
We are also seeing a
huge reduction in mttr.

375
00:22:09,875 --> 00:22:13,725
MTTR is meantime to resolution
basically when an incident is opened.

376
00:22:14,070 --> 00:22:17,820
The time it takes to resolve the incident
to resolve the issue has reduced a lot

377
00:22:17,820 --> 00:22:21,960
because with these tools are able to
correlate data between these different

378
00:22:22,060 --> 00:22:26,389
data sources like tele metadata
traces, log, et cetera, and it's able

379
00:22:26,389 --> 00:22:29,540
to narrow down on the area of impact.

380
00:22:29,810 --> 00:22:33,110
So that is really helping all college
engineers to quickly go and identify

381
00:22:33,110 --> 00:22:37,284
the areas where the problem is happening
and do mitigate and mitigate the issue.

382
00:22:37,784 --> 00:22:40,604
I said alert fitting is one thing,
and then incident frequencies,

383
00:22:40,964 --> 00:22:45,584
definitely decreased number of
incidents because we are able to

384
00:22:45,944 --> 00:22:47,624
automate the remediation process.

385
00:22:47,985 --> 00:22:50,714
Nobody gets, imagine this, right?

386
00:22:51,185 --> 00:22:54,744
You, you're running tens of
thousands of servers and every

387
00:22:54,744 --> 00:22:56,639
time there is a CPU breach.

388
00:22:57,139 --> 00:23:01,679
Or, a host goes down, if an onca
unit gets paged just to replace

389
00:23:01,679 --> 00:23:05,669
the host, then the on-call engineer
would never be able to sleep, right?

390
00:23:05,879 --> 00:23:09,539
So these things are handled automatically
these days and it's become alarm.

391
00:23:10,039 --> 00:23:13,689
So the number of incidents have gone
down as well, and the efficiency

392
00:23:13,689 --> 00:23:18,129
of infrastructure we are seeing
pro slow progress in these things

393
00:23:18,310 --> 00:23:20,129
because for better capacity planning.

394
00:23:20,129 --> 00:23:26,179
I see, not all companies are using AI
to predict capacity, but I see a big

395
00:23:26,179 --> 00:23:31,980
trend in in large companies that are
using AI to predict capacity and scale

396
00:23:32,340 --> 00:23:33,930
and using that to scale of systems.

397
00:23:34,430 --> 00:23:39,290
So definitely using AI in in the real
world is pretty impactful to business.

398
00:23:39,790 --> 00:23:40,480
So what do we do?

399
00:23:40,960 --> 00:23:42,610
How do we, what do we do to adopt this?

400
00:23:42,820 --> 00:23:46,680
Visibility and anomaly detection
is phase one, so you cannot say, a

401
00:23:46,680 --> 00:23:50,520
company that has not implemented any
of these things, it's very difficult

402
00:23:50,520 --> 00:23:52,920
to do all these things on day one.

403
00:23:53,010 --> 00:23:55,920
So what do you do is do
this in phases, right?

404
00:23:55,920 --> 00:23:59,280
The first step is to emit
the right telemetry data.

405
00:23:59,760 --> 00:24:04,190
So as long as you have data, the
right telemetry data that you emit.

406
00:24:04,690 --> 00:24:08,020
That, that should help in, in, in ai.

407
00:24:08,110 --> 00:24:11,950
So if you're not emitting the right
data, start emitting the right data.

408
00:24:12,100 --> 00:24:12,940
That is step number one.

409
00:24:13,720 --> 00:24:19,090
Step number two is use AI to,
to correlate all this data.

410
00:24:19,795 --> 00:24:23,515
And build the dependency mapping
and reduce the investigation time.

411
00:24:23,845 --> 00:24:26,065
You don't have to do this all by yourself.

412
00:24:26,125 --> 00:24:31,195
There are these tools that will help
you do all these things, and most

413
00:24:31,195 --> 00:24:35,365
tools that you use, as I mentioned
before, have got these features built

414
00:24:35,365 --> 00:24:37,915
in these days, and which all you
have to do is to start using them.

415
00:24:38,415 --> 00:24:43,160
Automation and prediction enable enabling
autonomous remediation for monitors.

416
00:24:43,160 --> 00:24:46,465
That is the phase three because
as I said before, you cannot.

417
00:24:46,965 --> 00:24:50,655
Rely on AI to make the
decisions on your behalf.

418
00:24:51,225 --> 00:24:55,635
You can coach it so slowly you
can ask it to recommend what?

419
00:24:55,845 --> 00:24:57,675
What could be a good remediation.

420
00:24:58,035 --> 00:24:59,415
You can give it feedback.

421
00:24:59,415 --> 00:25:01,365
Yep, this is good, this is bad, et cetera.

422
00:25:01,365 --> 00:25:02,205
And slowly to learn.

423
00:25:02,205 --> 00:25:05,475
And you get to a stage where
it can take decisions on your

424
00:25:05,475 --> 00:25:09,165
behalf for the remediation as
well, and optimization and scale.

425
00:25:09,165 --> 00:25:12,190
So expand the scope of
optimization will be based on.

426
00:25:12,495 --> 00:25:12,555
For.

427
00:25:13,055 --> 00:25:14,945
So first thing is the quality of data.

428
00:25:15,065 --> 00:25:18,455
Make sure that phase one is good,
other things would fall in place.

429
00:25:18,955 --> 00:25:19,885
We're coming to the end here.

430
00:25:20,425 --> 00:25:25,290
So key takeaways, reliability is a given.

431
00:25:25,770 --> 00:25:28,620
We expect reliability from
all the systems around us.

432
00:25:29,310 --> 00:25:31,740
And reliability is dependent on data.

433
00:25:32,310 --> 00:25:33,780
So if you've not done this.

434
00:25:34,485 --> 00:25:41,575
Emit the right data, and AI can help
you with reliability, and AI can go

435
00:25:41,575 --> 00:25:47,315
to an extent where it can it, it can
help in tradition, not just reaction.

436
00:25:47,315 --> 00:25:52,860
So as humans, we are reactive,
but humans plus ai, we can make

437
00:25:52,860 --> 00:25:54,360
predictions not just be reactive

438
00:25:54,860 --> 00:25:56,510
humans plus AI outperform.

439
00:25:57,110 --> 00:25:59,240
Either alone, right?

440
00:25:59,450 --> 00:26:03,440
So humans alone or AI alone cannot work.

441
00:26:04,430 --> 00:26:07,760
Humans and AI together would
perform, would outperform.

442
00:26:08,150 --> 00:26:08,780
You know what?

443
00:26:08,780 --> 00:26:11,590
Humans did all these years and
what's gonna happen in the future?

444
00:26:11,680 --> 00:26:13,240
Reinforcement learning is a big thing.

445
00:26:13,240 --> 00:26:15,980
Self-improving remediation
policies is that said before.

446
00:26:16,410 --> 00:26:19,260
As you teach AI to, for the mediation.

447
00:26:19,695 --> 00:26:23,695
The number of pages that non receives,
would drama drastically go down because

448
00:26:23,875 --> 00:26:26,485
AI would be able to make decisions
on what remediation steps to take

449
00:26:26,485 --> 00:26:30,525
and then give them automatically
digital twins, like deployments.

450
00:26:30,645 --> 00:26:35,295
You would have a virtual environment
which would be validated by ai, and

451
00:26:35,295 --> 00:26:38,015
then AI will tell you, yep, this
is good, and then move that to the

452
00:26:38,015 --> 00:26:41,555
customers and causal reasoning,
causal reason, cause and effect.

453
00:26:41,930 --> 00:26:44,720
So when there is a cause in one
area, it's gonna tell you what's

454
00:26:44,720 --> 00:26:48,170
gonna happen and know what, how
it's going to affect another area.

455
00:26:48,170 --> 00:26:53,010
So these are things that are
upcoming and would help companies

456
00:26:53,350 --> 00:26:54,550
support the systems better.

457
00:26:55,050 --> 00:26:57,900
So predictive prevention
is the new SR standard.

458
00:26:58,400 --> 00:27:00,110
I hope you all love this session.

459
00:27:00,500 --> 00:27:01,310
Thank you so much.

460
00:27:01,640 --> 00:27:02,210
You'll have a good one.

461
00:27:02,900 --> 00:27:03,140
Cheers.

