1
00:00:00,070 --> 00:00:04,320
My name is Dileesh, welcome to my talk
on optimizing network performance,

2
00:00:04,770 --> 00:00:08,350
and automation using Apache
Airflow, Python and data science.

3
00:00:08,770 --> 00:00:09,670
So let's get started.

4
00:00:09,970 --> 00:00:15,759
So I'll cover this PPT in brief, and
then I'll go through, how Apache Airflow

5
00:00:15,759 --> 00:00:17,749
works and give a short demo of it.

6
00:00:17,889 --> 00:00:20,779
I've have it installed on my
local machine, so I'll just

7
00:00:20,789 --> 00:00:22,239
give a quick demo of it later.

8
00:00:22,949 --> 00:00:25,209
So let's get started
with the presentation.

9
00:00:26,079 --> 00:00:30,519
So introduction to network performance
automation, So there are several modern

10
00:00:30,529 --> 00:00:36,499
challenges, that networks face, right now,
which is due to the increase in complexity

11
00:00:36,549 --> 00:00:42,669
and, scale of, the devices and the
new, technologies that come into place.

12
00:00:42,719 --> 00:00:44,629
So everything needs to
have network, right?

13
00:00:44,949 --> 00:00:49,009
So all of these challenges, some of
them can be solved by automation.

14
00:00:49,389 --> 00:00:55,279
And the main driver for that automation
is Python and the orchestrated

15
00:00:56,049 --> 00:01:02,419
platform that's best built for running
Python workflows is Apache Airflow.

16
00:01:02,859 --> 00:01:07,979
So for network challenges, so there
are several components, right?

17
00:01:07,979 --> 00:01:09,639
So coming to complexity.

18
00:01:10,174 --> 00:01:13,984
So there are several multi
layered architectures, with, a

19
00:01:13,984 --> 00:01:16,154
mix of, legacy and modern systems.

20
00:01:16,474 --> 00:01:19,534
So there are different devices
that we connect to and different

21
00:01:19,534 --> 00:01:20,884
protocols that they follow.

22
00:01:21,214 --> 00:01:25,054
And there are different technologies that
exist within a single network as well.

23
00:01:25,774 --> 00:01:28,624
And the network has to support all
of these technologies in tandem.

24
00:01:29,124 --> 00:01:33,844
So coming to scale, so now due to
the proliferation of the internet,

25
00:01:33,854 --> 00:01:38,954
so right now the connected devices
that we use, have exploded in scale.

26
00:01:38,994 --> 00:01:45,024
So for example, from your watch
to phone, headphones, laptop

27
00:01:45,084 --> 00:01:50,324
devices, any IOT devices that
you do and it's growing actually.

28
00:01:50,584 --> 00:01:53,074
The prediction is within
billions, within 20.

29
00:01:53,574 --> 00:01:58,204
2030. and, due to the
increase in, density as well.

30
00:01:58,244 --> 00:02:04,724
So if you take a grid of, certain,
areas and compare the devices

31
00:02:04,844 --> 00:02:08,714
that's that exists right now with
previous version of the devices.

32
00:02:08,994 --> 00:02:12,084
So right now, the density,
has increased a lot.

33
00:02:12,524 --> 00:02:19,464
and if you want to have a smart city or an
industrial IOT city, that it puts really

34
00:02:19,464 --> 00:02:23,664
pressure on the current infrastructure,
and it tests the limits of it.

35
00:02:24,044 --> 00:02:27,684
and, there are other challenges such
as resource allocation, where you

36
00:02:27,694 --> 00:02:30,954
have to distribute all the bandwidth
to all these devices and the compute

37
00:02:30,954 --> 00:02:36,044
power and storage, to avoid bottlenecks
for the devices and latency as well.

38
00:02:36,054 --> 00:02:41,054
So coming to latency, you would
need a network to perform in a very

39
00:02:41,054 --> 00:02:42,684
optimized way for these devices to.

40
00:02:43,309 --> 00:02:45,419
stream whatever content that
you are streaming, right?

41
00:02:45,419 --> 00:02:49,999
So for example, if you have an AR VR
device, that streaming data, so you don't

42
00:02:50,009 --> 00:02:51,589
want to have any lag in the network.

43
00:02:52,009 --> 00:02:56,869
So all of these are challenges that
are faced by, networks currently.

44
00:02:57,639 --> 00:03:01,099
So a role of automation in improving
network performance, right?

45
00:03:01,349 --> 00:03:04,289
So automation is one of the
cornerstones for, scaling and

46
00:03:04,329 --> 00:03:05,629
optimizing network performance.

47
00:03:06,014 --> 00:03:09,594
it can do productive fault management
where it can detect, any issues with the

48
00:03:09,594 --> 00:03:14,654
network, reroute the traffic through,
devices and conditions and, outages.

49
00:03:15,054 --> 00:03:20,264
So you can have some automated scripts
run the network and, resolve that issue.

50
00:03:20,824 --> 00:03:24,024
And there are several Python
powered automation tools.

51
00:03:24,044 --> 00:03:30,569
So Python has Played a big role previously
and plays a big role right now, with

52
00:03:30,699 --> 00:03:34,799
within the networks to proactively
fault manage, for these networks.

53
00:03:35,009 --> 00:03:37,639
So you can use Ansible, Netmeco, Napalm.

54
00:03:37,909 --> 00:03:40,319
So these are all, packages
within Python, right?

55
00:03:40,609 --> 00:03:42,049
So these are powered by Python.

56
00:03:42,519 --> 00:03:45,559
And coming to dynamic scaling
and self healing networks.

57
00:03:45,569 --> 00:03:49,059
So whenever we talk about dynamic
scaling and self healing networks,

58
00:03:49,059 --> 00:03:52,239
there are software defined networks,
SDNs, and you have your network

59
00:03:52,379 --> 00:03:54,179
functional, virtualization.

60
00:03:54,739 --> 00:03:56,489
So it uses Python as well.

61
00:03:56,489 --> 00:03:58,089
And Python scripts.

62
00:03:58,484 --> 00:04:02,614
Drive a lot of, network
and network performance.

63
00:04:02,974 --> 00:04:07,884
So it, interacts with different devices,
different frameworks, routers, which

64
00:04:07,884 --> 00:04:09,824
is, any network device that you have.

65
00:04:10,184 --> 00:04:16,484
and these controllers, the devices have
controllers that are support Python.

66
00:04:16,764 --> 00:04:20,494
So you can use Python to interact with the
device directly, and they have the built

67
00:04:20,494 --> 00:04:23,874
in libraries that are built, that you can
leverage to interact with these devices.

68
00:04:24,374 --> 00:04:27,020
So coming to some, real
world examples, right?

69
00:04:27,070 --> 00:04:30,330
Apache Airflow, you can use it
to, optimize your, orchestrate

70
00:04:30,330 --> 00:04:33,730
your network workflows, including
provisioning, monitoring, and scaling,

71
00:04:34,090 --> 00:04:38,020
suggesting provisioning, network speed
testing, and semantic similarity.

72
00:04:38,470 --> 00:04:41,165
So these are some of the
examples that we've used in the

73
00:04:41,165 --> 00:04:42,610
past, for some of my clients.

74
00:04:42,990 --> 00:04:44,750
and it's proven to be very good.

75
00:04:45,250 --> 00:04:48,960
and very optimized version of,
provisioning, monitoring, and scaling, and

76
00:04:48,960 --> 00:04:50,830
improving that for network performance.

77
00:04:50,890 --> 00:04:53,380
So coming to Apache Airflow.

78
00:04:53,380 --> 00:04:54,460
So what is Apache Airflow?

79
00:04:54,460 --> 00:04:56,330
So it's an open source framework.

80
00:04:56,380 --> 00:05:00,400
it's an orchestration tool, basically
whatever Python scripts that you have,

81
00:05:00,700 --> 00:05:07,280
you define those scripts in a way it's
there orchestrated in a way that you

82
00:05:07,280 --> 00:05:09,380
can run all of your tasks within Python.

83
00:05:10,050 --> 00:05:15,700
and, the core component of Apache
Airflow is scheduler, where it schedules

84
00:05:15,830 --> 00:05:20,030
your Python workflows, the way that
you mentioned, how you want to run it.

85
00:05:20,420 --> 00:05:21,770
So it's built in Python itself.

86
00:05:21,990 --> 00:05:23,430
So it's built on top of Python.

87
00:05:23,810 --> 00:05:26,029
it uses something called, DAGs.

88
00:05:26,030 --> 00:05:28,360
So directed acyclic graphs,
where you can schedule.

89
00:05:28,865 --> 00:05:31,615
All your components in an acyclic manner.

90
00:05:32,185 --> 00:05:35,015
for example, if you want to run
three functions, you can define which

91
00:05:35,015 --> 00:05:38,625
function to run first, second and
third, and so on and so forth, right?

92
00:05:38,625 --> 00:05:42,345
So you want to run third, first,
second, and then first, you can

93
00:05:42,355 --> 00:05:46,775
define it that way, making them
dynamic and reusable and scalable.

94
00:05:47,625 --> 00:05:48,955
coming to the key features, right?

95
00:05:49,455 --> 00:05:50,615
Dynamic workflow authoring.

96
00:05:50,615 --> 00:05:53,765
So you can use Python code to
define tasks, the dependencies

97
00:05:53,815 --> 00:05:55,275
and schedules for that.

98
00:05:55,275 --> 00:05:58,975
in the configuration section, you
can mention how often do you want to

99
00:05:59,105 --> 00:06:00,365
run it, just like a cron job, right?

100
00:06:00,615 --> 00:06:05,385
So you can, write the cron expression
for it to run, to your liking every

101
00:06:05,385 --> 00:06:07,035
once in a while, once in a day.

102
00:06:07,470 --> 00:06:09,740
or a however you like, right?

103
00:06:09,740 --> 00:06:10,800
So extensibility.

104
00:06:11,450 --> 00:06:13,450
it can integrate with other systems.

105
00:06:13,520 --> 00:06:16,300
you can have custom operators, hooks.

106
00:06:16,310 --> 00:06:19,059
So there are already several
built within Apache Airflow.

107
00:06:19,110 --> 00:06:22,360
and everything is written in Python as
well, where you have custom operators,

108
00:06:22,630 --> 00:06:28,500
that can interface with, Azure or AWS
or any other cloud provider on any other

109
00:06:28,500 --> 00:06:30,870
system that you would like, scalability.

110
00:06:31,370 --> 00:06:34,690
So it supports distributed
systems execution as well.

111
00:06:34,990 --> 00:06:38,460
So it has components like celery,
Kubernetes or local executor that can

112
00:06:38,460 --> 00:06:40,650
perform tasks in a scalable manner.

113
00:06:40,990 --> 00:06:44,070
So you basically start
out with a local executor.

114
00:06:44,570 --> 00:06:46,480
So these are backend
functionalities, right?

115
00:06:46,480 --> 00:06:50,010
So executor is nothing but something
that's behind, an Apache Airflow

116
00:06:50,010 --> 00:06:53,040
framework that runs, your workflows.

117
00:06:53,540 --> 00:06:58,710
you can, so initially started with a
local executor where you execute these

118
00:06:58,710 --> 00:07:03,620
tasks and the, if you don't care about
the, scalability part, you can start

119
00:07:03,620 --> 00:07:05,970
with local and then scale out basically.

120
00:07:06,430 --> 00:07:09,080
So that's how you make the
systems distributed, right?

121
00:07:09,080 --> 00:07:13,380
So basically you start out with,
vertical scaling, where you have a

122
00:07:13,450 --> 00:07:16,940
single device, single machine, and
increase the resources in that machine.

123
00:07:17,420 --> 00:07:22,290
And then you scale up to multiple systems,
as you, as your scale and increases.

124
00:07:22,980 --> 00:07:24,170
So monitoring and alert.

125
00:07:24,180 --> 00:07:30,150
So it has integration with, different,
Platforms such as Slack, email, Microsoft

126
00:07:30,150 --> 00:07:33,010
Teams, or, discord, anything, right?

127
00:07:33,010 --> 00:07:37,960
So all of the things are built in so you
can use that to alert you, and web UI.

128
00:07:38,160 --> 00:07:41,590
So I'll get to and get into this
later, but it has a really intuitive

129
00:07:41,590 --> 00:07:44,890
and a really great way to visualize,
manage and debug workflows.

130
00:07:45,390 --> 00:07:46,770
coming to the benefits of it, right?

131
00:07:46,890 --> 00:07:51,170
So it simplifies your Python code a lot.

132
00:07:51,340 --> 00:07:55,530
So if you have, let's say a thousand
lines of code, it's really hard for

133
00:07:55,530 --> 00:07:57,510
you to, manage and maintain that code.

134
00:07:57,530 --> 00:07:59,750
What went wrong, where it went wrong.

135
00:08:00,440 --> 00:08:03,480
Of course you can do some logging
statements and find out what's going on.

136
00:08:03,890 --> 00:08:08,310
But, Apache Airflow gives you a
really intuitive web UI where you can.

137
00:08:08,770 --> 00:08:14,500
Log in and check the, check the, how your
DAG performs and see where it went wrong.

138
00:08:14,570 --> 00:08:15,210
Exactly.

139
00:08:15,740 --> 00:08:17,810
So it enhances the transparency.

140
00:08:18,190 --> 00:08:21,730
logs and visualizations so you can
visualize what's going on with your

141
00:08:22,350 --> 00:08:24,920
workflow and reduce manual interventions.

142
00:08:25,420 --> 00:08:29,270
So use cases of Apache Airflow
in network operations, right?

143
00:08:29,270 --> 00:08:32,300
So you can have Apache
Airflow run some provisioning.

144
00:08:32,700 --> 00:08:36,650
So it automated tasks like, provisioning
physical sims or virtual sims,

145
00:08:36,650 --> 00:08:44,695
managing IMSI ranges and updating
systems like SRP, and others.

146
00:08:44,725 --> 00:08:49,605
And, once I've implemented this,
it turns turned out to be a very

147
00:08:49,605 --> 00:08:55,075
fast, execution of SIM provisioning
rather than the legacy systems.

148
00:08:55,555 --> 00:09:03,295
So Apache Airflow gave us a platform where
we can leverage Python and, different.

149
00:09:03,810 --> 00:09:09,160
Parameters of provisioning process to
really improve the speeds for first net

150
00:09:09,160 --> 00:09:15,340
HSS, it's first net is nothing but first
responder communication network where, all

151
00:09:15,340 --> 00:09:19,370
of the communication for first responders
have happened through that first net HSS.

152
00:09:19,760 --> 00:09:23,130
And it turned out to be,
really great for that use case.

153
00:09:23,630 --> 00:09:24,600
Network speed testing.

154
00:09:24,630 --> 00:09:29,320
So gathering telemetry data,
analyzing, key performance indicators,

155
00:09:29,330 --> 00:09:31,100
triggers, anomalies in network.

156
00:09:31,190 --> 00:09:36,430
So it turned out to be a great use
case for speed testing as well.

157
00:09:36,930 --> 00:09:39,850
Coming to configuration management,
deploying configuration to router

158
00:09:39,850 --> 00:09:42,640
switches, and other servers and services.

159
00:09:42,690 --> 00:09:46,460
it, it makes it very easy to
configure all of your devices.

160
00:09:46,460 --> 00:09:49,885
Thank you within your network
and predictive analytics as well.

161
00:09:50,215 --> 00:09:52,805
So the way that you can write
Apache Airflow workflow.

162
00:09:53,760 --> 00:09:57,520
You can have all of your ETL tasks
performed within the pipelines itself,

163
00:09:58,020 --> 00:10:02,370
and you can have your machine learning
models or semantic search models as well.

164
00:10:02,870 --> 00:10:06,340
So how Apache Airflow, helps
automate workflows, enhance

165
00:10:06,360 --> 00:10:08,480
efficiency and optimize resource use.

166
00:10:08,810 --> 00:10:10,440
So coming to automation, right?

167
00:10:10,450 --> 00:10:13,020
So Airflow eliminates
manual interventions.

168
00:10:13,440 --> 00:10:16,730
that's like how automation
works basically, right?

169
00:10:17,015 --> 00:10:20,565
So you trigger your workflow based
on scheduler or the external events.

170
00:10:20,585 --> 00:10:25,345
It runs all the tasks that you define
it to run and then picks up all

171
00:10:25,355 --> 00:10:27,625
the, things that you want it to do.

172
00:10:28,625 --> 00:10:29,265
Efficiency.

173
00:10:29,265 --> 00:10:35,535
So if you run Celery as an executor in
the backend, it can parallelize tasks,

174
00:10:35,605 --> 00:10:37,745
to how many cores you have in the system.

175
00:10:38,295 --> 00:10:42,165
you can have multiple requests process
at once and try to use execution time.

176
00:10:42,665 --> 00:10:44,815
It restructures the failed
tasks intelligently.

177
00:10:44,815 --> 00:10:48,125
So there is already a built in
mechanism where you can handle re

178
00:10:48,125 --> 00:10:52,695
tries, failed, processes and ensure
that, the workflow is reliable.

179
00:10:53,195 --> 00:10:54,595
the resources automation, right?

180
00:10:54,595 --> 00:10:59,395
So there is some concept called task
queues, where you can manage the balance

181
00:10:59,395 --> 00:11:01,715
and work workloads within multiple, VMs.

182
00:11:02,015 --> 00:11:06,145
where you can use kubernetes executor
to scale out all the pods and,

183
00:11:06,145 --> 00:11:07,975
containerized workflow execution.

184
00:11:08,945 --> 00:11:11,885
So you can have your
resource allocation as well.

185
00:11:12,355 --> 00:11:15,775
So automation workflows,
for large sim provisioning

186
00:11:16,005 --> 00:11:18,505
projects, you can use Airflow.

187
00:11:18,870 --> 00:11:20,060
That we've discussed earlier.

188
00:11:20,560 --> 00:11:22,130
So why use Python for Airflow, right?

189
00:11:22,130 --> 00:11:27,130
So Airflow is built on top of a
Python itself, but within Airflow,

190
00:11:27,160 --> 00:11:28,230
you can code Python again.

191
00:11:28,580 --> 00:11:32,870
it's the, for the intuition for
using Python is basically easy to

192
00:11:32,870 --> 00:11:35,150
learn syntax, extensive libraries.

193
00:11:35,160 --> 00:11:40,150
So it has a really good open source
and all of the machine learning

194
00:11:40,200 --> 00:11:43,860
and, Data related stuff happens
in Python in nowadays, right?

195
00:11:44,280 --> 00:11:48,590
if you want to talk about Spark, so there
is libraries for PySpark, within Python

196
00:11:48,590 --> 00:11:53,010
itself, where you can use Spark modules
and Spark frameworks within Python.

197
00:11:53,890 --> 00:11:57,540
There are several libraries and
frameworks that lets you manage, all your

198
00:11:57,560 --> 00:11:59,360
network automation needs within Python.

199
00:11:59,810 --> 00:12:03,660
So netmiko or paramiko, if you heard
about SSH communication, these are,

200
00:12:03,910 --> 00:12:08,020
frameworks that help you with, SSH based
communication to configure, network

201
00:12:08,050 --> 00:12:09,680
devices and interface with those.

202
00:12:10,270 --> 00:12:14,540
So you can automate tasks like, device
configuration, backup troubleshooting.

203
00:12:14,970 --> 00:12:17,340
So you have napalm, pi, SNMB.

204
00:12:17,340 --> 00:12:18,430
You can check this out later.

205
00:12:18,580 --> 00:12:21,710
I'll just give a brief overview of
network X is a really good thing

206
00:12:21,720 --> 00:12:25,780
where you can analyze and visualize
network graphs and topology and

207
00:12:25,790 --> 00:12:27,480
mapping and simulation of the networks.

208
00:12:27,965 --> 00:12:33,665
pandas and embryos for network telemetry
and logs, trends, and anomaly detection.

209
00:12:33,735 --> 00:12:39,485
Basically, pandas, you can create data
frames and process objects within Python.

210
00:12:39,985 --> 00:12:42,825
So predictive analytics for
network optimization, right?

211
00:12:42,875 --> 00:12:45,015
role of data science in
network optimization.

212
00:12:45,355 --> 00:12:48,745
You can identify trends and
anomalies in network, performance.

213
00:12:49,080 --> 00:12:53,250
You can predict failures before they
occur, reduce the downtime, and you

214
00:12:53,250 --> 00:12:56,570
can optimize the resource allocation,
such as bandwidth, processing

215
00:12:56,570 --> 00:12:59,160
power, use for network automation.

216
00:12:59,610 --> 00:13:00,900
So machine learning, right?

217
00:13:00,900 --> 00:13:04,980
So there are several libraries built
on in Python for machine learning.

218
00:13:04,980 --> 00:13:07,110
So if you're trying to build
a machine learning model, you

219
00:13:07,150 --> 00:13:08,430
don't need to do it from scratch.

220
00:13:08,480 --> 00:13:12,310
Python probably has a library
that already does that.

221
00:13:12,630 --> 00:13:13,780
So scikit learn.

222
00:13:13,800 --> 00:13:15,370
So there are several scikit learn.

223
00:13:16,170 --> 00:13:19,620
There are several libraries within
Python for machine learning.

224
00:13:20,000 --> 00:13:24,670
You have TensorFlow, PyTorch, Pandas,
NumPys, for your data processing.

225
00:13:25,190 --> 00:13:30,220
And, these are, you can use these,
libraries for some use cases, such as

226
00:13:30,220 --> 00:13:33,120
failure prediction or anomaly detection.

227
00:13:33,840 --> 00:13:38,160
and automation, use predictions
with Apache Airflow, right?

228
00:13:38,610 --> 00:13:41,960
So you can use Python integrates
basically with everything, right?

229
00:13:41,960 --> 00:13:46,050
So any device, any network device,
any models, any third party

230
00:13:46,050 --> 00:13:48,200
systems that has support for APIs.

231
00:13:48,725 --> 00:13:51,965
it probably have a built in
library that it can leverage.

232
00:13:52,365 --> 00:13:56,515
it, enables real time decision making
through, trigger events, right?

233
00:13:56,515 --> 00:14:00,575
So, so let's take an example
of a workflow tag, right?

234
00:14:00,575 --> 00:14:03,635
So you collect elementary data,
you run a machine learning model

235
00:14:03,665 --> 00:14:07,435
for prediction and some kind of,
automated action you can trigger with.

236
00:14:07,935 --> 00:14:11,695
So automation of network
configuration and provisioning, right?

237
00:14:11,695 --> 00:14:16,905
So using Apache Airplane Python, you
can, do some complex stuff, such as

238
00:14:16,955 --> 00:14:19,205
by same provisioning network setups.

239
00:14:19,445 --> 00:14:22,395
You can set up, a script that
does provision to systems like

240
00:14:22,545 --> 00:14:27,935
HSS, FirstNet, Gport, HLRmind, and
range allocation for SIMs and MCs.

241
00:14:28,915 --> 00:14:30,065
coming to benefits, right?

242
00:14:30,065 --> 00:14:35,045
So it's efficient, accurate, scalable,
and monitoring, wise for, Apache

243
00:14:35,045 --> 00:14:38,255
Airflow, everything that I mentioned
right now, those are the beneficial

244
00:14:38,755 --> 00:14:41,255
benefits that for using, Apache Airflow.

245
00:14:41,755 --> 00:14:44,925
If you take an example for SIM
provisioning, within Apache Airflow,

246
00:14:44,925 --> 00:14:49,605
you collect all the user requests, SIM
types, MC ranges, and you execute the

247
00:14:49,605 --> 00:14:51,085
provisioning tasks with different systems.

248
00:14:51,095 --> 00:14:56,175
So each system has one component
where it uses either RTELnet, LDAP,

249
00:14:56,175 --> 00:14:59,505
or whatever, communication, way of
communication that you do either SSH.

250
00:14:59,985 --> 00:15:03,695
Now to that system and origin, all
the sims, you validate those, you

251
00:15:03,725 --> 00:15:08,685
present the log status and, you can
have a workflow that step that can,

252
00:15:09,065 --> 00:15:11,145
notify, someone of the completion.

253
00:15:11,645 --> 00:15:13,935
So that's basically,
it for how to optimize.

254
00:15:13,935 --> 00:15:17,845
So let's dive right in to, how
Apache Airflow works, right?

255
00:15:18,815 --> 00:15:20,095
Airflow is an open source tool, right?

256
00:15:20,095 --> 00:15:23,835
So there is really good documentation
on how this basically works.

257
00:15:24,145 --> 00:15:27,205
what, providers, these are
all the active providers that,

258
00:15:27,485 --> 00:15:28,665
Airflow supports right now.

259
00:15:28,925 --> 00:15:32,565
So you can have a Cassandra
connection to Cassandra, Amazon,

260
00:15:32,575 --> 00:15:35,365
AWS, a lot of stuff, basically.

261
00:15:36,365 --> 00:15:40,295
So if you take Amazon, for
example, using Apache Airflow.

262
00:15:40,775 --> 00:15:45,135
they provide all the versions
related to, they're within

263
00:15:45,145 --> 00:15:47,475
their, server itself service.

264
00:15:47,475 --> 00:15:49,115
So installing it in your local, right?

265
00:15:49,115 --> 00:15:52,825
So I've installed, my
version using, Docker.

266
00:15:53,405 --> 00:15:56,245
So there is really good
documentation on how to do it.

267
00:15:56,255 --> 00:15:58,015
So you can play around with it as well.

268
00:15:58,075 --> 00:16:01,330
But first let's make sure our Docker.

269
00:16:01,830 --> 00:16:04,010
Instances running right now.

270
00:16:04,260 --> 00:16:05,620
So Docker engine is paused.

271
00:16:05,630 --> 00:16:06,620
So let's resume that.

272
00:16:07,170 --> 00:16:09,570
So I already have a Docker installed.

273
00:16:10,550 --> 00:16:11,420
Let's go to a terminal.

274
00:16:11,920 --> 00:16:12,430
Let's check.

275
00:16:12,930 --> 00:16:13,800
It's already running.

276
00:16:13,840 --> 00:16:14,260
No.

277
00:16:14,740 --> 00:16:15,140
Yes.

278
00:16:15,640 --> 00:16:17,610
So I've installed earlier.

279
00:16:17,910 --> 00:16:18,350
let's check.

280
00:16:18,520 --> 00:16:20,780
So I have the Docker compose file.

281
00:16:20,780 --> 00:16:25,020
So let's do Docker, Docker compose up.

282
00:16:25,520 --> 00:16:27,560
So it creates all the containers.

283
00:16:27,750 --> 00:16:32,330
and it starts up all the services that
are required for So one of the good

284
00:16:32,330 --> 00:16:37,160
thing is all of the containers are built
in, using Docker, so you don't need

285
00:16:37,160 --> 00:16:38,970
to create each instance separately.

286
00:16:39,500 --> 00:16:44,080
if you're planning to run a
parallelized version of Airflow.

287
00:16:44,580 --> 00:16:45,390
So it's starting up.

288
00:16:46,120 --> 00:16:47,150
Let's see.

289
00:16:47,650 --> 00:16:51,360
So it's started all the Airflow stuff.

290
00:16:52,040 --> 00:16:53,390
Docker, reduce.

291
00:16:53,480 --> 00:16:55,080
Postgres.

292
00:16:55,580 --> 00:17:01,420
Postgres 13 is a database that Airflow
backend uses for storing metadata.

293
00:17:01,920 --> 00:17:03,940
So web server.

294
00:17:03,940 --> 00:17:07,730
It looks like all the service is
running except for Docker Airflow.

295
00:17:07,840 --> 00:17:08,790
Let's see what's going on.

296
00:17:09,290 --> 00:17:09,730
There you go.

297
00:17:09,730 --> 00:17:11,620
I think it started now.

298
00:17:12,620 --> 00:17:15,030
Let's go to localhost and
check if it's running.

299
00:17:15,060 --> 00:17:17,020
So I see the instance is running.

300
00:17:17,020 --> 00:17:21,250
So this is a UI that you get to, you
get to log in screen, but default

301
00:17:21,260 --> 00:17:24,325
or airflow and airflow credentials.

302
00:17:25,045 --> 00:17:31,285
so here you have your dags, dags are the
workflows that you can schedule, to run in

303
00:17:31,285 --> 00:17:34,935
Python and it shows all the dags, right?

304
00:17:35,235 --> 00:17:38,085
And there are two sections
here called active and Post.

305
00:17:38,415 --> 00:17:44,035
So you can have your workflow and
you can either have it run or, in

306
00:17:44,035 --> 00:17:45,885
a pause state or an unpause state.

307
00:17:45,885 --> 00:17:49,195
If you want to run it, you
can click on this button.

308
00:17:49,575 --> 00:17:50,625
It will make it active.

309
00:17:50,895 --> 00:17:53,185
So there are two, active
workflows right now.

310
00:17:53,595 --> 00:17:56,185
If you want to turn it off, you can
just turn it off using this button.

311
00:17:56,625 --> 00:17:58,785
So it gives a really good user interface.

312
00:17:58,845 --> 00:17:59,905
You can manage everything here.

313
00:18:00,405 --> 00:18:04,245
So coming to security, it
has users, roles, actions,

314
00:18:04,295 --> 00:18:05,545
everything, permissions too, right?

315
00:18:05,555 --> 00:18:07,365
So you can manage user permissions.

316
00:18:07,760 --> 00:18:11,870
if you go to users, you can find all the
users here, you can create users, you can

317
00:18:11,880 --> 00:18:18,850
drill down into how you want to manage the
RBAC for users, and you have admin section

318
00:18:18,850 --> 00:18:23,260
as well, where you have all your variables
and you can basically create connections

319
00:18:23,260 --> 00:18:29,220
to different, Services where it can see
it has all the stuff related to, Amazon,

320
00:18:29,230 --> 00:18:32,900
AWS, Azure, Docker or anything, right?

321
00:18:32,900 --> 00:18:35,230
So it has a lot of built in libraries.

322
00:18:35,230 --> 00:18:37,290
You can interface with
SQLite database as well.

323
00:18:37,760 --> 00:18:42,050
If you select SQLite, it should
give you a connection to a SQLite.

324
00:18:42,070 --> 00:18:44,710
So you give it a host login, password.

325
00:18:45,125 --> 00:18:47,915
It should be able to connect to and
there is an option to test as well.

326
00:18:47,985 --> 00:18:52,955
So you can create a connection and
test it out coming to workflows, right?

327
00:18:52,995 --> 00:18:56,195
So this is an example
workflow that I ran earlier.

328
00:18:56,665 --> 00:18:58,895
So it gives you a really good interface.

329
00:18:58,895 --> 00:19:01,385
So it took 20 seconds to run.

330
00:19:01,695 --> 00:19:05,784
The run type is manual
because the schedule is none.

331
00:19:05,785 --> 00:19:07,635
So there is no schedule set up for this.

332
00:19:08,045 --> 00:19:15,565
So, if I run this again, so it should
automatically trigger a new Where, if you

333
00:19:15,565 --> 00:19:20,515
check the graph of it, how this looks,
so this is how the graph looks right?

334
00:19:20,975 --> 00:19:23,335
So first it goes here.

335
00:19:24,075 --> 00:19:26,694
The logical output is run this.

336
00:19:26,744 --> 00:19:31,724
And you basically, once this
is run, you go to this step.

337
00:19:31,824 --> 00:19:34,444
And after this executes,
you run all of these steps.

338
00:19:35,069 --> 00:19:37,099
So now this is done, right?

339
00:19:37,639 --> 00:19:39,689
So let's drill down into each of this.

340
00:19:39,689 --> 00:19:42,069
So this is nothing but Python code base.

341
00:19:42,489 --> 00:19:46,629
So, basically you define
all of your functions here.

342
00:19:47,139 --> 00:19:49,279
these are just examples
that airflow provided.

343
00:19:49,629 --> 00:19:53,734
So this is a. way that you can
specify what the DAG is about.

344
00:19:54,194 --> 00:19:58,104
And this is a function
basically, print context, right?

345
00:19:58,104 --> 00:19:59,784
So it, this is a task.

346
00:19:59,994 --> 00:20:02,814
This is an operator where you
can print the context of what's

347
00:20:02,814 --> 00:20:04,244
going on within this function.

348
00:20:04,744 --> 00:20:07,994
So here there are other functions as well.

349
00:20:08,154 --> 00:20:11,094
Let's see what sleep three does.

350
00:20:11,594 --> 00:20:12,154
So

351
00:20:12,654 --> 00:20:14,874
sleep for three.

352
00:20:15,374 --> 00:20:16,924
So this is task sleep for three.

353
00:20:17,624 --> 00:20:21,104
and you can drill down into event
log what happened within the events.

354
00:20:21,594 --> 00:20:25,264
details of this, and you can
go into each individual log and

355
00:20:25,264 --> 00:20:26,704
check what happened there as well.

356
00:20:26,705 --> 00:20:32,104
So if you check the logs, so this is
nothing but, the log for this task.

357
00:20:32,834 --> 00:20:36,754
So all of the quarks here,
you have context, variables.

358
00:20:36,804 --> 00:20:41,284
We have what happened within
the log, within the operator.

359
00:20:41,784 --> 00:20:44,634
And if you click on here, you can
check out what's going on here as well.

360
00:20:45,134 --> 00:20:47,744
So that is it for Apache Airflow.

361
00:20:48,034 --> 00:20:50,684
so there is, interactive APIs as well.

362
00:20:50,814 --> 00:20:53,504
So it, everything is built in within here.

363
00:20:54,004 --> 00:20:55,824
So let's open Swagger.

364
00:20:56,324 --> 00:21:01,934
So it automatically, it takes,
basic code, Airflow and Airflow.

365
00:21:02,434 --> 00:21:06,254
And you can use this
APIs externally as well.

366
00:21:06,254 --> 00:21:09,724
If you have Postman, if you want
to test it out, you can basically,

367
00:21:09,824 --> 00:21:14,164
8080 and, run the DAG ID.

368
00:21:14,204 --> 00:21:16,084
So it asks for basically.

369
00:21:16,424 --> 00:21:17,134
Few things, right?

370
00:21:17,144 --> 00:21:17,714
DAG ID.

371
00:21:17,714 --> 00:21:20,894
So you can select any DAG ID from here.

372
00:21:21,394 --> 00:21:23,484
Let's check what the DAG ID is for this.

373
00:21:23,984 --> 00:21:25,974
DAG ID is example Python operator, right?

374
00:21:26,014 --> 00:21:27,094
So let's copy this

375
00:21:27,594 --> 00:21:28,654
and paste it here.

376
00:21:29,154 --> 00:21:30,814
Oh, let's do get.

377
00:21:31,314 --> 00:21:34,154
So it gives you all the
information about this, right?

378
00:21:34,454 --> 00:21:36,424
So what the DAG is about the file token.

379
00:21:37,044 --> 00:21:40,674
Where it is and what's
going on with the, DAG.

380
00:21:41,174 --> 00:21:44,604
So there is a lot of stuff that I
want to go out, but I don't think

381
00:21:44,604 --> 00:21:45,944
I can cover it in one session.

382
00:21:46,194 --> 00:21:52,534
But, yeah, you can go to the documentation
and, find out, about Apache Airflow mode.

383
00:21:52,864 --> 00:21:56,234
So you can run, so this is
basically what I told, right?

384
00:21:56,234 --> 00:21:57,504
You can run your workflows as code.

385
00:21:58,004 --> 00:22:01,634
And security is built within Airflow as
well, where you can, there is something

386
00:22:01,634 --> 00:22:06,424
called fernet key, that encrypts all your,
passwords, usernames, everything, right?

387
00:22:06,424 --> 00:22:10,284
So basically security is
baked in to Apache Airflow.

388
00:22:10,784 --> 00:22:12,524
And this is how we set
up a diagram, right?

389
00:22:12,524 --> 00:22:15,934
So this is basically how a
diagram, so you define a task.

390
00:22:16,419 --> 00:22:20,729
You'll give an operator and you'll
ask Airflow to run the operator.

391
00:22:21,549 --> 00:22:25,829
so that is pretty much it for
Apache Airflow, Python and network

392
00:22:25,829 --> 00:22:27,739
automation, and data science.

393
00:22:28,199 --> 00:22:32,669
So if you have any more questions,
you can certainly reach out to me.

394
00:22:32,889 --> 00:22:34,329
Thanks for joining me in this session.

