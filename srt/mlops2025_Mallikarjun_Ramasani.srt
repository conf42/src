1
00:00:00,500 --> 00:00:02,630
Hello everyone myself, Malika.

2
00:00:03,130 --> 00:00:07,950
I am a software engineer working in
Barlay as a business inte developer

3
00:00:08,800 --> 00:00:13,690
mainly focusing on building AI power
solutions that drive Smart Edition making.

4
00:00:14,560 --> 00:00:17,695
So my tech stack is using tech
clicks, clicksense, Tableau,

5
00:00:17,700 --> 00:00:23,095
sql data models and cloud-based
AI ML integration using Python.

6
00:00:23,810 --> 00:00:27,990
So mainly focusing on turning complex
data into the actionable insights.

7
00:00:27,990 --> 00:00:33,689
So I'm here to discuss on the ML ops
for the scalable BA and real time

8
00:00:33,689 --> 00:00:37,889
analytics, like it's a comprehensive
guide to building, deploying, and

9
00:00:37,889 --> 00:00:41,889
managing production grade machine
learning system and financial services.

10
00:00:42,389 --> 00:00:47,589
So today agenda is, we are
focusing on five stages.

11
00:00:47,589 --> 00:00:50,470
So one is first one, like here.

12
00:00:50,539 --> 00:00:54,979
We'll start the fundamentals of
ML ops in financial services.

13
00:00:54,979 --> 00:00:58,219
Then walk through the ML lifecycle
from injection to the deployment.

14
00:00:58,759 --> 00:01:03,489
After that, we'll look at the monitoring
and governance and finally how all

15
00:01:03,489 --> 00:01:09,130
of these currents with BI tools that
decision making make us use every day.

16
00:01:09,909 --> 00:01:11,739
So we'll go through one by one.

17
00:01:12,239 --> 00:01:12,509
Yeah.

18
00:01:13,139 --> 00:01:17,899
Next one is the ML of maturity gap
in financial services, where you can

19
00:01:17,899 --> 00:01:22,910
see the 83% of ML models in finance
never make it to the production.

20
00:01:23,449 --> 00:01:29,720
That number should scare as well because
it means billions spent on talent

21
00:01:29,720 --> 00:01:32,030
and platform without business value.

22
00:01:32,530 --> 00:01:37,939
The reason of familiar complex
requirement that slow down the deployment

23
00:01:38,029 --> 00:01:43,249
legacy system that resist integration
and the real time performance needs

24
00:01:43,249 --> 00:01:45,449
that most pipeline can't handle.

25
00:01:46,199 --> 00:01:48,509
The big insight here.

26
00:01:49,019 --> 00:01:50,849
The bot link is in model development.

27
00:01:50,849 --> 00:01:52,619
It's everything that happens after.

28
00:01:53,294 --> 00:01:56,024
From building models to managing them.

29
00:01:56,504 --> 00:02:00,504
The reason is clear, it's not
enough to develop a great algorithm.

30
00:02:01,004 --> 00:02:05,834
Financial banks need framework
that automate auditor has integrate

31
00:02:05,834 --> 00:02:10,484
models with legacy systems and
monitoring performance in real time.

32
00:02:10,869 --> 00:02:12,844
This is where ML ops comes in.

33
00:02:13,504 --> 00:02:17,194
Think of it as a bridge between model
development and business impact.

34
00:02:18,139 --> 00:02:23,479
Automating the pipeline that handle
deployment, logging, and compliance check.

35
00:02:23,979 --> 00:02:27,269
Whereas next stage would be the
versioning and monitoring tools

36
00:02:27,689 --> 00:02:33,019
so that it makes sure every model
update is stress and if suddenly

37
00:02:33,019 --> 00:02:35,529
risk reviews that used to take weeks.

38
00:02:36,259 --> 00:02:38,959
It can happen in days or even hours.

39
00:02:39,319 --> 00:02:40,774
The point is when you solve.

40
00:02:41,274 --> 00:02:45,714
After problem, all the investment
in talent and technology.

41
00:02:46,404 --> 00:02:51,234
Finally start paying off model, move
from prototype to production, and

42
00:02:51,234 --> 00:02:53,364
business teams actually see the results.

43
00:02:53,724 --> 00:02:56,584
That's how we close the gap
between building something

44
00:02:56,584 --> 00:02:58,504
amazing and making it useful.

45
00:02:59,004 --> 00:02:59,244
Yeah.

46
00:02:59,744 --> 00:03:02,265
Next slide would be
the financial MI cycle.

47
00:03:02,765 --> 00:03:05,165
So it has a multiple stages.

48
00:03:05,165 --> 00:03:08,654
So each stage has its financial
twist coming to the first

49
00:03:08,654 --> 00:03:10,034
stage of data injection.

50
00:03:10,034 --> 00:03:16,004
So it is not just pulling the CSV files
you are streaming marketing data injecting

51
00:03:16,034 --> 00:03:20,404
payment data, and even using the alternate
data sets for that, whereas coming to

52
00:03:20,404 --> 00:03:25,024
the next stage, featuring engineering
which is a critical for risk signals,

53
00:03:25,475 --> 00:03:30,855
things like violating windows credit
utilization ratios, or liquidity metrics.

54
00:03:31,635 --> 00:03:35,825
And coming to the next stage model
training which needs versioning

55
00:03:35,825 --> 00:03:39,825
and para parameter tracking, but
also regulatory documentation.

56
00:03:40,515 --> 00:03:43,125
Every DEC decision needs to be expandable.

57
00:03:43,625 --> 00:03:49,405
Coming to the next stage is deployment,
so model might be, must be ized,

58
00:03:49,615 --> 00:03:54,805
deployed via continuous integration
or continuous deployment, and often

59
00:03:54,805 --> 00:03:57,205
tested with the required frameworks.

60
00:03:58,045 --> 00:04:00,325
And last stage is monitoring.

61
00:04:01,135 --> 00:04:05,075
This is this includes a drift
performance metrics and compliance,

62
00:04:05,075 --> 00:04:07,535
auditing and financial impact.

63
00:04:07,535 --> 00:04:12,425
So each stage has unique demand in
finance, especially in the documentation.

64
00:04:12,425 --> 00:04:13,405
And sable.

65
00:04:14,065 --> 00:04:21,145
Coming to the next slide which is a tech
stack for ML ops, for the fine tech, so

66
00:04:21,805 --> 00:04:25,385
we can, I can go with the breakdown for
the ML ops, like data engineering and

67
00:04:25,385 --> 00:04:27,065
the management would be the first task.

68
00:04:27,565 --> 00:04:32,775
Stage one, like data pipelines, like
tools like Apache Airflow and Apache

69
00:04:32,775 --> 00:04:38,075
Spark Q Flow help build building the
automated data pipeline that handle

70
00:04:38,075 --> 00:04:42,875
data extraction transformation,
and loading the ETL to ensure high

71
00:04:42,875 --> 00:04:44,885
quality input Strat for the models.

72
00:04:45,785 --> 00:04:52,235
And within the same area, data versioning
and tracking tools like DVC data

73
00:04:52,235 --> 00:04:57,035
version control enable teams to track
data sets, change changes across the

74
00:04:57,035 --> 00:05:02,855
experiment, and making it easier to
reproduce and debug modules and another.

75
00:05:03,355 --> 00:05:07,495
Another thing is like data quality
monitoring, like ensuring data

76
00:05:07,495 --> 00:05:08,695
quality through validation.

77
00:05:08,695 --> 00:05:13,230
Tools like monitoring prop helps
detect issues such as missing values

78
00:05:13,230 --> 00:05:18,100
or anomalies that could degrade
model performance, which comes

79
00:05:18,100 --> 00:05:19,925
under the data quality monitoring.

80
00:05:20,849 --> 00:05:25,669
Whereas like second the other other
module is model experimental and

81
00:05:25,699 --> 00:05:29,124
versioning, which is nothing but
experiment tracking like ML flow and.

82
00:05:30,049 --> 00:05:34,009
ML Flow are used to log model
hyper parameters, metrics, and

83
00:05:34,009 --> 00:05:38,989
configuration, and allowing those
data scientists to systematically

84
00:05:39,769 --> 00:05:42,109
compare experiment for comparison.

85
00:05:42,139 --> 00:05:48,059
We so for the comparison we use
neptune.ai and within the same

86
00:05:48,059 --> 00:05:52,459
area, the model versioning are
stored in registry like ML flow.

87
00:05:53,224 --> 00:05:57,624
Model ing, which documents the
metadata training data training

88
00:05:57,624 --> 00:06:02,844
data and performance metrics for eg
versioning and tracking the deployment.

89
00:06:03,344 --> 00:06:05,174
And coming to the deployment.

90
00:06:05,174 --> 00:06:10,554
So deployment with the CSCD pipeline,
like using automated module testing

91
00:06:10,914 --> 00:06:14,724
and validating and ensuring that
all the models are thoroughly

92
00:06:14,724 --> 00:06:16,284
evaluated before the deployment.

93
00:06:16,344 --> 00:06:21,234
This streamline streamlines the
continuous integration and deployment

94
00:06:21,234 --> 00:06:26,454
of the new models and the scalable
deployment, like continuation with

95
00:06:26,454 --> 00:06:28,284
Docker and orchestration using.

96
00:06:28,784 --> 00:06:33,404
Es enable, enable the flexible and
scalable model deployments, which

97
00:06:33,404 --> 00:06:36,374
is nothing but adapting the resource
to the real time domain mind.

98
00:06:36,874 --> 00:06:42,694
And using the Bento ml and Amazon
SAG Maker is a fully managed

99
00:06:42,994 --> 00:06:46,744
cloud-based machine learning that
provides tools and services to

100
00:06:46,744 --> 00:06:49,114
build, train, and apply ML models.

101
00:06:49,894 --> 00:06:53,534
And next stage would be the
monitoring and maintenance like.

102
00:06:54,034 --> 00:06:57,895
We are using evidently, AI for
drift detections and for the

103
00:06:57,895 --> 00:06:59,515
metric collections as well.

104
00:07:00,015 --> 00:07:00,304
Yeah.

105
00:07:00,804 --> 00:07:05,854
Next slide would be featuring engineering
in financial data, whereas like some.

106
00:07:06,354 --> 00:07:10,145
Financial ml require specialization,
feature engineering to capture temporal

107
00:07:11,025 --> 00:07:15,405
pattern market and the risk factors
where coming to the temporal patterns in

108
00:07:15,405 --> 00:07:18,495
finance, the timing of events is critical.

109
00:07:18,674 --> 00:07:24,615
Generic features like average price don't
capture the market first, but you want

110
00:07:24,615 --> 00:07:29,204
to feature that reflect how price and
volume evaluate over the time, right?

111
00:07:29,214 --> 00:07:33,474
Let's say, example, let's smooth
out short term fluctuations

112
00:07:33,474 --> 00:07:35,334
and high highlighting trends.

113
00:07:35,784 --> 00:07:40,114
So short term versus long term and
make and signal movement to challenges.

114
00:07:40,744 --> 00:07:44,765
So we coming to the market,
MicroStrategy future, this is nothing

115
00:07:44,765 --> 00:07:49,074
but this bit spread the difference
between buying and selling the price.

116
00:07:49,534 --> 00:07:52,784
Which comes within the same
area, like depth imbalance.

117
00:07:53,564 --> 00:07:55,524
Transaction cost and sleep level.

118
00:07:56,024 --> 00:08:02,369
Coming to the next slide CSCD for the
financial MI models, which is nothing

119
00:08:02,369 --> 00:08:07,289
but continuous integration and continuous
deployment, whereas coming to the data

120
00:08:07,379 --> 00:08:10,109
validation, every new data must be passed.

121
00:08:10,109 --> 00:08:11,399
Quality and bias check.

122
00:08:11,899 --> 00:08:14,609
Next stage would be data model training.

123
00:08:15,089 --> 00:08:17,819
Training pipeline generates
the documentation and risk

124
00:08:17,819 --> 00:08:19,019
reports automatically.

125
00:08:19,949 --> 00:08:23,609
And next stage would be the testing
and staging that can read department,

126
00:08:23,609 --> 00:08:30,469
reduce risk before full rollout re
regulatory gates, which is the next stage.

127
00:08:30,499 --> 00:08:33,799
Like compliance reviews are
integrated in the pipeline.

128
00:08:33,799 --> 00:08:36,229
So approval rt, manual bottlenecks.

129
00:08:37,049 --> 00:08:42,849
CSCD isn't just a DevOps idea, it's a
governor frameworks in financial mi.

130
00:08:43,839 --> 00:08:48,349
So next day next slide would be the
real time interface architecture.

131
00:08:48,899 --> 00:08:53,589
This is where ML ops made systems
systems engineering, whereas low

132
00:08:53,589 --> 00:08:59,429
latency use cases like fraud detection,
algo trading, instant credit scoring.

133
00:08:59,864 --> 00:09:04,094
If you are too slow, either
fraud slips through, a legitimate

134
00:09:04,154 --> 00:09:05,534
customer gets declined.

135
00:09:06,224 --> 00:09:10,874
So come with tech stack tech stack
solutions like optimization model or

136
00:09:10,874 --> 00:09:16,424
GPU acceleration and streaming platform
like Kafka something sometimes even

137
00:09:16,424 --> 00:09:18,754
edge deployments, which takes a matter.

138
00:09:19,414 --> 00:09:22,984
So the other thing is
like scalability pattern.

139
00:09:23,484 --> 00:09:29,034
To handle market payments to wide variety,
you need auto-scaling clusters and a

140
00:09:29,034 --> 00:09:35,374
load balancing and ation framework to
keep core services Stable in finance and

141
00:09:35,884 --> 00:09:40,394
performance is not just nice to have,
it's must that it should deal with it.

142
00:09:41,384 --> 00:09:45,684
And next would be the complaints
and governance for the FinTech.

143
00:09:46,184 --> 00:09:51,924
Okay governance is the foundation like
model cards that compel with SR 11 by

144
00:09:51,924 --> 00:09:57,894
17 and exp expandability report using
shape and line, so washing control

145
00:09:57,894 --> 00:10:04,319
violation, artifacts for audit, you need
an immutable logs reportable environment,

146
00:10:04,434 --> 00:10:06,384
and deploying deployment issues.

147
00:10:06,864 --> 00:10:11,784
Security also matters like encrypting
a pipeline component and using a strict

148
00:10:11,944 --> 00:10:17,314
role based contact access and scanning
container without these like you can't

149
00:10:17,314 --> 00:10:20,914
have mls so you have a risk exposure here.

150
00:10:21,414 --> 00:10:23,994
And monitoring financial ML in production.

151
00:10:24,515 --> 00:10:28,344
There are a few steps to track
down in production for monitoring

152
00:10:28,344 --> 00:10:32,775
the machine learning so that track
is nothing but track data and

153
00:10:32,775 --> 00:10:34,785
prediction drift over the time.

154
00:10:34,785 --> 00:10:41,895
Your model input data and its prediction
can shift leading to decreased accuracy.

155
00:10:42,465 --> 00:10:48,865
So monitoring these trips helps
identifying when retraining is necessary.

156
00:10:49,645 --> 00:10:55,205
Watch for the training service crew ensure
that the data used during training closely

157
00:10:55,205 --> 00:10:57,725
matches the data and the production.

158
00:10:57,725 --> 00:11:02,855
Significant decreases differences can
cause the model to perform poorly.

159
00:11:03,515 --> 00:11:06,945
Like next would be the
monitoring data pipeline health.

160
00:11:07,605 --> 00:11:13,215
Issues in data process can introduce
errors or delays affecting data model's

161
00:11:13,215 --> 00:11:18,015
performance, keeping an eye on this
pipeline, ensure smooth operation.

162
00:11:18,735 --> 00:11:24,105
The other would be evaluating model
performance continuously and which

163
00:11:24,105 --> 00:11:28,885
is nothing but back testing and other
metrics to access how we model the

164
00:11:28,885 --> 00:11:30,895
performing the real time conditions.

165
00:11:31,285 --> 00:11:35,725
So this helps in making information
about updates or re retraining.

166
00:11:36,565 --> 00:11:41,785
Finally the setup setup the alarm
implementing alert mechanism to quickly

167
00:11:41,785 --> 00:11:47,065
detect and respond to any unexpected
changes in model behavior performance.

168
00:11:47,305 --> 00:11:52,835
So by focusing on these areas
you can maintain the ability and

169
00:11:52,835 --> 00:11:55,955
effectiveness of your mission
learning models and production.

170
00:11:56,455 --> 00:11:59,125
Coming to the next slide,
like concept driven.

171
00:11:59,575 --> 00:12:02,575
Drift management in financial model.

172
00:12:03,075 --> 00:12:07,035
Drift management in financial model
is about detecting changes in data

173
00:12:07,035 --> 00:12:12,235
and relationship that can degrade
model performance all the time.

174
00:12:12,565 --> 00:12:18,485
So it involves monitoring, input,
features and model predictions.

175
00:12:19,475 --> 00:12:25,175
Using metrics like PSA or kl ence
to flagships when drift is detected.

176
00:12:25,235 --> 00:12:30,765
Models are recalibrated or retrained
on recent data to maintain accuracy.

177
00:12:30,855 --> 00:12:36,495
So continuous monitoring and governance
ensure the model stays reliable in

178
00:12:36,495 --> 00:12:38,925
dynamic market or credit environment.

179
00:12:39,425 --> 00:12:44,735
So next slide would be, the integrating
ml with the BI tools in finance, like

180
00:12:44,735 --> 00:12:49,085
integration approaches or like a PS
for the model prediction and endpoints

181
00:12:49,805 --> 00:12:56,370
and embedded Python are for in dash com
dashboard computation and pre-computer

182
00:12:56,370 --> 00:12:58,860
feature stores for the low latency access.

183
00:12:59,220 --> 00:13:02,220
Custom visualization extensions
for the model insights.

184
00:13:02,670 --> 00:13:06,400
To these capabilities that
suppose the BI platform is Tableau

185
00:13:06,820 --> 00:13:09,270
or bi looker and ThoughtSpot.

186
00:13:10,135 --> 00:13:13,565
Whereas there's process for that
integrating the ML with the BI

187
00:13:13,805 --> 00:13:18,395
tools and finances pretty predicate
to analytics like ML algorithm

188
00:13:18,855 --> 00:13:21,345
analysis, historical data to forecast.

189
00:13:22,020 --> 00:13:26,280
Future trends such as sales or
customer's behavior and enabling the

190
00:13:26,280 --> 00:13:32,760
proactive decision making and also
automated the insight, insights and

191
00:13:32,760 --> 00:13:37,620
enhancing the forecasting and detection
and personalized recommendation.

192
00:13:38,520 --> 00:13:44,090
So that would be the integrating
the BI tools with the mi.

193
00:13:44,590 --> 00:13:46,430
Next would be the NLP Dr.

194
00:13:46,460 --> 00:13:48,410
Driven Financial Analytics.

195
00:13:48,930 --> 00:13:50,750
Which is nothing but ai.

196
00:13:51,350 --> 00:13:56,180
Added it course like NLP technologies
built on the interaction of computer

197
00:13:56,180 --> 00:13:58,340
science, artificial intelligence.

198
00:13:58,340 --> 00:14:02,350
And so main core components
of NP systems are like.

199
00:14:03,070 --> 00:14:05,290
Tokenization passing and machine learning.

200
00:14:05,630 --> 00:14:08,630
Which is nothing but breakdown
text into the different

201
00:14:09,120 --> 00:14:11,640
individual words or pa passes.

202
00:14:11,730 --> 00:14:14,670
Allowing systems to analyze
smaller part of a sentence.

203
00:14:15,030 --> 00:14:19,320
Such words are shorter basis is
nothing but a to tokenization.

204
00:14:19,770 --> 00:14:21,090
And the next would be the passing.

205
00:14:21,090 --> 00:14:25,060
That understanding the grammatical
structure helps system integrator

206
00:14:25,160 --> 00:14:26,810
relationship between words.

207
00:14:27,310 --> 00:14:31,450
And finally, the NLP model can
generate more accurately by using the

208
00:14:31,450 --> 00:14:36,220
machine learning outputs, by training
with vast amount of text data, while

209
00:14:36,220 --> 00:14:39,070
additionally data can enhance performance.

210
00:14:39,590 --> 00:14:45,840
Model also need careful monitoring to
avoid the degradation over the time.

211
00:14:46,560 --> 00:14:50,310
So machine learning uses an
algorithm to detect pattern.

212
00:14:50,925 --> 00:14:56,235
Text enabling the NLP system to
identify and categorize data efficiency.

213
00:14:56,595 --> 00:15:02,025
This approach allows models to
recognize recurring themes, sentiments,

214
00:15:02,025 --> 00:15:03,830
and entities within the text data.

215
00:15:04,330 --> 00:15:05,765
And key takeaways are like.

216
00:15:06,510 --> 00:15:11,460
Start with the, when we design something,
like start with the architecture, like

217
00:15:11,460 --> 00:15:17,020
designing machine, learning of pipeline
with the specific requirement in mind

218
00:15:17,020 --> 00:15:23,870
from day one and integrate early, connect
the ML capabilities directly into the

219
00:15:23,870 --> 00:15:30,280
existing BI tools where users already
trust that tool and make it governance.

220
00:15:30,370 --> 00:15:34,020
And com compliance isn't an,
and afterthought, I built

221
00:15:34,170 --> 00:15:36,740
it into your workflows.

222
00:15:37,240 --> 00:15:37,870
Yeah, that's all.

223
00:15:37,870 --> 00:15:38,410
Thank you.

