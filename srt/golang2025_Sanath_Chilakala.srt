1
00:00:00,000 --> 00:00:00,690
Hi everyone.

2
00:00:01,500 --> 00:00:07,440
Do you guys know enterprise data is
growing at 42.2% annually, and in that

3
00:00:07,440 --> 00:00:10,530
yet 68% of that remains unutilized.

4
00:00:11,460 --> 00:00:13,050
I am sala.

5
00:00:13,440 --> 00:00:17,850
And today let's explore how
go based solutions transform

6
00:00:18,000 --> 00:00:21,125
enterprise data architecture
to meet tomorrow's challenges.

7
00:00:21,625 --> 00:00:26,035
Let's talk about some facts
around the enterprise data crisis.

8
00:00:26,305 --> 00:00:31,284
As I mentioned earlier, 42.2% of
annual growth year over year, yet

9
00:00:31,314 --> 00:00:38,324
68, 60 8% is underutilized and
32% are an inadequate systems.

10
00:00:39,044 --> 00:00:42,945
They, most of the organizations
and IT leaders believe that the

11
00:00:42,945 --> 00:00:46,095
existing data architectures cannot
handle future, future demand.

12
00:00:46,595 --> 00:00:52,055
and 44% of the time of other, of
data professionals on a single

13
00:00:52,055 --> 00:00:57,325
workday is just lost in trying to
find validating data across the

14
00:00:57,325 --> 00:00:59,185
board to achieve one single goal.

15
00:00:59,685 --> 00:01:04,895
Why Go is a valuable asset to,
to a data architecture solution?

16
00:01:05,395 --> 00:01:06,025
Three things.

17
00:01:06,025 --> 00:01:08,905
One, it provides superior concurrency.

18
00:01:09,430 --> 00:01:12,250
Number two, it provides
immense performance.

19
00:01:12,580 --> 00:01:18,250
Number three, it en enables integrations
with cloud native ecosystems.

20
00:01:18,750 --> 00:01:21,720
Let's talk about the first
one, the superior concurrency.

21
00:01:22,650 --> 00:01:23,760
Go lightweight.

22
00:01:23,760 --> 00:01:27,580
Go routines basically
enable to effortlessly, run

23
00:01:27,580 --> 00:01:28,735
parallel data processing.

24
00:01:29,170 --> 00:01:34,660
It can integrate across multiple data
sources very seamlessly and enable that

25
00:01:34,660 --> 00:01:36,730
parallel data processing generating.

26
00:01:37,180 --> 00:01:42,369
Super, performance when it comes
down to running real time jobs.

27
00:01:42,869 --> 00:01:45,030
Let's come to per let's
talk about performance.

28
00:01:45,530 --> 00:01:47,660
Go is basically a compiled language.

29
00:01:47,660 --> 00:01:50,130
It's basically with intelligent
garbage collection, it

30
00:01:50,430 --> 00:01:53,160
delivers nearly c performance.

31
00:01:53,210 --> 00:01:57,500
with developer friendly syn taxes,
it processes data significantly

32
00:01:57,500 --> 00:02:00,920
faster than interpreted languages
like Python, JavaScript, while

33
00:02:00,920 --> 00:02:02,150
maintain quota liability.

34
00:02:02,650 --> 00:02:07,720
it is also widely u widely popular
language lately, and people are started

35
00:02:07,720 --> 00:02:10,930
to realize the performance that is
bringing to the table in order to enable

36
00:02:10,930 --> 00:02:14,320
the jobs in order to make sure the
pipeline's done as fast as possible.

37
00:02:14,820 --> 00:02:17,490
Next is integrations with
cloud native ecosystems.

38
00:02:17,490 --> 00:02:21,460
Over the years, over the last couple of
years, we have seen many organizations.

39
00:02:21,760 --> 00:02:26,800
Start to modernize their, on-prem systems
over to the cloud, whether it's Azure,

40
00:02:26,800 --> 00:02:30,170
whether it's, GCP or whether it is AWS go.

41
00:02:30,170 --> 00:02:34,410
Basically enables, integrations
with tools like Kubernetes, Docker,

42
00:02:34,410 --> 00:02:40,540
Prometheus, which enables scalability,
which brings you, e easier integrations.

43
00:02:40,900 --> 00:02:44,790
And it also basically provides extensive
libraries and SDKs which integrate

44
00:02:44,790 --> 00:02:47,100
natively with modern cloud platforms.

45
00:02:47,600 --> 00:02:50,695
and with these integrations
door, it basically simplifies

46
00:02:50,695 --> 00:02:53,575
deployment and scalability of the
entire data architecture as well.

47
00:02:54,075 --> 00:02:58,815
So in a typical, and in a typical
organization, there is a, there is

48
00:02:58,845 --> 00:03:00,495
various levels of data maturity.

49
00:03:01,395 --> 00:03:05,615
If you see, as you're seeing on the slide,
every organization basically should,

50
00:03:05,615 --> 00:03:07,320
starts with slide, with point number four.

51
00:03:07,805 --> 00:03:11,375
Wherein its basic collection,
there is multiple systems, multiple

52
00:03:11,375 --> 00:03:12,755
applications within an environment.

53
00:03:12,755 --> 00:03:14,135
There's tons of data available.

54
00:03:14,525 --> 00:03:16,685
Everything's very
fragmented, very reactive.

55
00:03:17,060 --> 00:03:20,180
It's that you, your first
activity of maturity, basic

56
00:03:20,180 --> 00:03:21,650
needs to collect everything.

57
00:03:22,150 --> 00:03:25,530
Step number three basically is about
structuring the data that you've gathered.

58
00:03:25,620 --> 00:03:30,480
So basically trying to have it plugged
into an active database, ensuring that

59
00:03:30,480 --> 00:03:32,700
you have a structure enabled to it.

60
00:03:32,700 --> 00:03:36,150
It doesn't have to have a formalized
model though, but there has to be

61
00:03:36,150 --> 00:03:39,940
some level of structure, integrity
available for the data sets.

62
00:03:40,440 --> 00:03:44,160
Step number two is basically wherein
the advanced analytics comes in picture.

63
00:03:44,219 --> 00:03:49,195
This is where you basically define a
purpose-built data model, depending on

64
00:03:49,195 --> 00:03:51,144
the industry and your business value.

65
00:03:51,654 --> 00:03:55,825
And then you run high analytics using
that data, specific data model itself.

66
00:03:56,755 --> 00:04:01,044
And the peak of this chain is
basically that it's air readiness.

67
00:04:01,314 --> 00:04:04,344
It's basically self-optimizing
systems wherein you don't have

68
00:04:04,344 --> 00:04:06,964
to, explicitly define something.

69
00:04:07,309 --> 00:04:11,689
The ai, basically, you develop a
data mature strategy so well that the

70
00:04:11,689 --> 00:04:16,409
AI basically starts to self realize
what the, what each data point is and

71
00:04:16,409 --> 00:04:21,479
tries to generate solutions for you
for various value added cases, eight,

72
00:04:21,479 --> 00:04:25,739
surprisingly, 87% of the organizations,
they're remain, they're trapped with

73
00:04:25,739 --> 00:04:28,335
the, with three and four, with Go.

74
00:04:28,484 --> 00:04:32,919
It enables this maturity, it accelerates
this maturity level by 3.5 times.

75
00:04:33,264 --> 00:04:36,834
And enables companies to basically
transform raw data into actionable

76
00:04:36,834 --> 00:04:38,634
intelligence and predictive capabilities.

77
00:04:39,134 --> 00:04:44,394
Let's talk about the various, phases
in basically defining a data pipelines.

78
00:04:45,084 --> 00:04:48,714
We have ingestions, we have
transformations, we have an analyzing

79
00:04:48,714 --> 00:04:50,684
and then distribution, right?

80
00:04:50,984 --> 00:04:53,114
So the first step is ingestion, right?

81
00:04:53,224 --> 00:04:57,114
Primarily, in any data architecture,
when you have to basically start

82
00:04:57,114 --> 00:05:00,234
to get information from various
sources, you ingest it into one

83
00:05:00,234 --> 00:05:05,574
data platform with Go, it enables
you to con connect to 10 different

84
00:05:05,574 --> 00:05:10,224
sources simultaneously and processes
up to 2 million events per second

85
00:05:10,884 --> 00:05:13,224
with minimum C minimal CPO overhead.

86
00:05:13,794 --> 00:05:18,655
This kind of real time capability is
a huge accelerator for any kind of an.

87
00:05:19,054 --> 00:05:21,344
a large data architecture
in any environment.

88
00:05:21,844 --> 00:05:25,234
The second step is basically
the transformation piece.

89
00:05:25,294 --> 00:05:27,414
This is where you write to data pipelines.

90
00:05:27,414 --> 00:05:28,584
You include the logic.

91
00:05:28,734 --> 00:05:32,394
You ensure that the logic basically
adheres to the model and ensures that

92
00:05:32,394 --> 00:05:33,864
from the source system to the model.

93
00:05:33,864 --> 00:05:35,604
You, you develop the entire pipeline.

94
00:05:35,964 --> 00:05:39,974
Go routines basically provide you
very lightweight, ETL capabilities,

95
00:05:40,364 --> 00:05:44,294
but it provides you the capability to
execute complex ETL operations as well.

96
00:05:44,294 --> 00:05:50,584
In parallel, using these go routines, you
will be able to achieve 85% less latency

97
00:05:50,584 --> 00:05:52,954
while scaling linearly across CPU course.

98
00:05:53,454 --> 00:05:58,649
So basically, with this, with, a smaller
scale, the smaller scale, compute, you

99
00:05:58,649 --> 00:06:00,869
are able to achieve very high performance.

100
00:06:01,369 --> 00:06:03,559
The next phase is basically the analyze.

101
00:06:04,010 --> 00:06:08,200
This is where go basically interfaces
with applications like TensorFlow,

102
00:06:08,200 --> 00:06:11,819
frameworks like Apache Spark and
customized custom analytical engines.

103
00:06:12,734 --> 00:06:15,585
It enables subsequent decision
making on streaming data.

104
00:06:15,614 --> 00:06:19,875
So as we've talked about earlier in the
ingestion, when we are able to bring

105
00:06:19,875 --> 00:06:24,674
in the capability of generating data in
near real time, it also enables you to

106
00:06:24,674 --> 00:06:29,294
analyze or basically provide you insights
out of the data in the same fashion.

107
00:06:30,284 --> 00:06:34,005
It enables you to do smarter
decision making and enables you

108
00:06:34,005 --> 00:06:37,635
to basically reflect results
in, in, in a very fast manner.

109
00:06:38,135 --> 00:06:39,425
And the last is distribution.

110
00:06:40,294 --> 00:06:44,364
So how do we basically have this
data available to other data

111
00:06:44,364 --> 00:06:45,684
sources within the organization?

112
00:06:45,895 --> 00:06:51,174
So the Z it Go basically offers a
zero copy delivery to data lakes,

113
00:06:51,324 --> 00:06:53,155
warehouses, and applications.

114
00:06:53,604 --> 00:06:57,564
It maintains cryptographically verified
data needs throughout the pipeline.

115
00:06:58,064 --> 00:07:01,034
Then next, let's talk about
the data governance framework.

116
00:07:01,534 --> 00:07:05,494
as part of the overall design,
with the various phases occurring.

117
00:07:05,824 --> 00:07:11,584
Governance is a very key aspect in the
future of any organization with so many

118
00:07:11,584 --> 00:07:15,994
data sources, so many data attributes
available out there, I. A need for

119
00:07:15,994 --> 00:07:20,044
data governance is mandated across,
especially our organizations when there

120
00:07:20,044 --> 00:07:24,004
is financial transactions involved, when
there's health healthcare transactions

121
00:07:24,004 --> 00:07:27,634
involved, whether there's insurance
transactions involved, anything wherein

122
00:07:27,814 --> 00:07:31,844
there is so many data sources available
across, across the organization.

123
00:07:32,324 --> 00:07:35,414
This governance framework is
mandated and much money, much needed.

124
00:07:35,444 --> 00:07:35,504
It.

125
00:07:36,434 --> 00:07:39,314
So what does governance
framework basically offer?

126
00:07:39,374 --> 00:07:43,094
It basically allow, offers you metadata
management, basically allows you

127
00:07:43,094 --> 00:07:45,584
to catalog, create a data catalog.

128
00:07:45,794 --> 00:07:50,054
It also enables you to show you how
data lineage, how data moves from one

129
00:07:50,054 --> 00:07:53,444
system to another system, what one
attribute means in one system, how it

130
00:07:53,444 --> 00:07:55,274
transform to another attribute system.

131
00:07:56,174 --> 00:07:58,814
The second is about security
and compliance to basically.

132
00:07:59,109 --> 00:08:04,329
Flag the flag attributes, which are
P-H-I-P-I-I with sensitive information.

133
00:08:04,329 --> 00:08:08,169
You basically ensure you read the
reg regulatory requirements and make

134
00:08:08,169 --> 00:08:12,619
sure you have all audit checks and,
definition audit auditable definitions

135
00:08:12,619 --> 00:08:14,369
created for those attributes.

136
00:08:15,330 --> 00:08:16,830
Third is basically quality control.

137
00:08:17,550 --> 00:08:20,940
Quality control is wherein you
ensure that a specific attribute is

138
00:08:20,940 --> 00:08:22,200
defined the way it's supposed to.

139
00:08:22,910 --> 00:08:27,710
You ensure that if that is basically
not satisfying the original, originally

140
00:08:27,710 --> 00:08:32,480
defined manner, it basically fails
quality, so ensure that integrity

141
00:08:32,480 --> 00:08:34,220
maintains all the way through.

142
00:08:34,720 --> 00:08:36,730
The last is basically access management.

143
00:08:36,790 --> 00:08:41,740
This is where, it enables users, to
access the data through role-based access

144
00:08:41,740 --> 00:08:45,365
control, depending on the level and
the type of access that they will need.

145
00:08:45,865 --> 00:08:50,455
How Go helps in all of this go basically
go offers concurrent microservices

146
00:08:50,455 --> 00:08:54,285
architecture, enables governance
frameworks that scale elastically and

147
00:08:54,285 --> 00:08:58,545
automatically adapting to new regulations
while reducing compliance overhead by

148
00:08:58,545 --> 00:09:01,215
up to 40% compared to tradition systems.

149
00:09:01,215 --> 00:09:03,885
That is a huge value, that's a
huge shoutout that we need to

150
00:09:03,885 --> 00:09:05,870
give to the existing Go framework.

151
00:09:06,370 --> 00:09:10,490
Let's talk about an active use
case, of where Go has added or go

152
00:09:10,490 --> 00:09:12,260
has become, a game changer in this.

153
00:09:12,800 --> 00:09:18,510
So the problem, the problem statement in
this specific use case is basically, in

154
00:09:18,510 --> 00:09:23,860
this specific healthcare organization,
36%, they was, they were facing 36% annual

155
00:09:23,860 --> 00:09:28,540
data growth across multiple silos or silo
legacy systems within the organization.

156
00:09:29,410 --> 00:09:32,500
This patient records were scattered
across multiple departments.

157
00:09:32,500 --> 00:09:35,860
There is no way basically
to bring the data together.

158
00:09:36,100 --> 00:09:37,450
And what was the end result?

159
00:09:37,450 --> 00:09:40,510
It was basically causing critical
delays in, in care delivery.

160
00:09:41,290 --> 00:09:44,200
So for example, if some doctor
basically goes and requests for

161
00:09:44,200 --> 00:09:47,700
a certain patient records for the
organization to provide information

162
00:09:47,700 --> 00:09:49,095
back, it takes a couple of hours.

163
00:09:49,595 --> 00:09:55,205
That kind of, delay basically, causes,
a huge gap in how patients, how

164
00:09:55,205 --> 00:09:58,325
doctors engage with patients and try
to provide the best care possible.

165
00:09:58,825 --> 00:10:00,385
How go solve this problem?

166
00:10:01,195 --> 00:10:04,825
It go basically, the Go Solution
basically implemented robust

167
00:10:04,825 --> 00:10:08,365
microservices architecture,
connecting 17 disparate systems.

168
00:10:08,995 --> 00:10:12,925
It developed real time HL seven
FHAR translation engine with h HIPAA

169
00:10:12,925 --> 00:10:14,275
compliant verification protocol.

170
00:10:14,775 --> 00:10:15,765
Doing that.

171
00:10:15,895 --> 00:10:19,315
it, it enabled lightning
speed availability of data.

172
00:10:19,825 --> 00:10:24,535
It also basically provided you 83
per percent reduction in the overall

173
00:10:24,535 --> 00:10:25,855
clinical data retrieval time.

174
00:10:26,355 --> 00:10:29,805
Comprehensive patient, as I mentioned
earlier, now, the comprehensive patient

175
00:10:29,805 --> 00:10:33,725
histories are now available in seconds
rather than hours, which basically

176
00:10:33,725 --> 00:10:35,885
improved overall care decisions.

177
00:10:36,385 --> 00:10:40,550
Another case study now talking
about the How Go has impact, how

178
00:10:40,550 --> 00:10:43,670
a current problem in financial
services and how go was able to help.

179
00:10:44,420 --> 00:10:48,140
So in this financial institution,
basically there are fragmented

180
00:10:48,290 --> 00:10:52,190
regulatory compliances across 28
disparate systems creating, which was

181
00:10:52,190 --> 00:10:54,230
creating significant operational burden.

182
00:10:55,190 --> 00:11:00,950
Monthly compliance reporting require
it took almost 160 hours staff hours to

183
00:11:00,950 --> 00:11:02,720
basically create compliance reporting.

184
00:11:03,125 --> 00:11:06,095
One compliance, one, one
monthly compliance report, which

185
00:11:06,095 --> 00:11:08,795
is insane with the existing
market that we currently have.

186
00:11:09,545 --> 00:11:13,145
What go did is basically deployed
an image source data fabric with

187
00:11:13,145 --> 00:11:15,695
cryptographically secured audited trails.

188
00:11:16,145 --> 00:11:20,905
What is enabled is faster near real
time availability of, of all of

189
00:11:20,905 --> 00:11:24,535
the in data and enable compliance
monitoring on top of it with

190
00:11:24,535 --> 00:11:26,695
automatic automated anomaly detection.

191
00:11:27,665 --> 00:11:31,830
it, it is basically a combination
of how you have machine learning

192
00:11:31,860 --> 00:11:35,550
and a good quality data with
high availability and speed.

193
00:11:36,360 --> 00:11:40,320
So what it did is basically it reduced
the overall compliance reporting time

194
00:11:40,320 --> 00:11:42,540
and 94% while eliminating manual error.

195
00:11:43,500 --> 00:11:46,980
It enabled continuous risk assessments
across all trading platforms with

196
00:11:46,980 --> 00:11:48,540
millisecond level visibility.

197
00:11:49,080 --> 00:11:50,205
That is the power of go.

198
00:11:50,705 --> 00:11:55,145
Next, let's talk about basically,
the how we arrived at cloud

199
00:11:55,145 --> 00:11:56,925
data, native data architectures.

200
00:11:57,285 --> 00:12:01,835
So as I mentioned earlier, one of the
key features of, of Go is basically

201
00:12:01,835 --> 00:12:05,075
to successfully integrate with these
cloud native data architectures.

202
00:12:05,105 --> 00:12:10,545
as you can see on the screen, the, how
our cloud native has evolved as part of

203
00:12:10,545 --> 00:12:13,305
the modernization of many organizations
over the last couple of years.

204
00:12:13,935 --> 00:12:16,935
It started with the traditional
on-prem, then go with the hybrid cloud

205
00:12:16,935 --> 00:12:20,925
system, then the multi-cloud system,
then very cloud native capabilities.

206
00:12:21,615 --> 00:12:25,665
So in here, basically if you see,
goes lightweight concurrency model

207
00:12:25,665 --> 00:12:28,845
and efficient resource utilization,
make it exceptionally well suited

208
00:12:28,845 --> 00:12:30,555
for cloud native data architectures.

209
00:12:31,055 --> 00:12:33,905
In a typical cloud data,
native data architectures, the

210
00:12:33,905 --> 00:12:36,515
growth rate is around 14.2%.

211
00:12:36,515 --> 00:12:37,175
CAGR.

212
00:12:37,675 --> 00:12:41,815
By leveraging go's ability to support
for containers, Kubernetes, microservice,

213
00:12:41,995 --> 00:12:46,225
serverless architectures, and SDKs
to integrate with cloud native, it

214
00:12:46,225 --> 00:12:49,225
significantly reduce operational
overhead and deployment complexities.

215
00:12:49,725 --> 00:12:52,935
Then let's talk about go
libraries for enterprise data.

216
00:12:53,265 --> 00:12:56,625
So these are some of the key libraries
that, that we need to focus on from

217
00:12:56,625 --> 00:12:58,455
a, from GO'S capability perspective.

218
00:12:59,205 --> 00:13:01,865
First in, in high power,
high performance, ETL.

219
00:13:02,255 --> 00:13:06,755
Goum provides advanced numerical com
computations with near native speed.

220
00:13:07,255 --> 00:13:09,955
GIA enables GPU Accelerator
machine learning pipelines.

221
00:13:10,435 --> 00:13:15,010
Nats delivers Ultrafast message streaming
with 10, 10 million messages per second.

222
00:13:15,510 --> 00:13:20,790
Second is governance spiff inspire,
implement implements, zero trust identity

223
00:13:20,790 --> 00:13:22,710
frameworks across distributed systems.

224
00:13:23,700 --> 00:13:27,420
Ori Hydra provides scalable
O-O-I-D-C authorizations.

225
00:13:28,170 --> 00:13:32,250
Jagger Pro enables comprehensive data
in tracing with microsecond precision.

226
00:13:32,520 --> 00:13:35,995
All of these key components are
what comprises of the entire data

227
00:13:36,115 --> 00:13:37,350
F frame data governance framework.

228
00:13:38,310 --> 00:13:40,530
The next is the data
storage and retrieval.

229
00:13:41,250 --> 00:13:46,830
Badger DP offers embedded key value
storage with SSD performance optimized

230
00:13:46,830 --> 00:13:49,350
performance enterprise Ready Go Client.

231
00:13:49,350 --> 00:13:53,610
Seamlessly integrated with
Kafka, Cassandra, MongoDB,

232
00:13:53,610 --> 00:13:54,660
and Elasticsearch ecosystems.

233
00:13:55,160 --> 00:13:58,700
The last piece of the cloud integration
as we mentioned earlier, native go

234
00:13:58,700 --> 00:14:03,770
SDKS for A-W-S-G-C-P and Azure deliver
optimized cloud for cloud performance.

235
00:14:04,280 --> 00:14:08,330
Terraform provides enable in
as a code with declarative

236
00:14:08,330 --> 00:14:09,500
data architecture deployments.

237
00:14:10,000 --> 00:14:10,480
Next.

238
00:14:10,750 --> 00:14:13,660
How do you basically come up
with your with Go data strategy?

239
00:14:13,790 --> 00:14:17,690
In a typical, ideal environment, very
first thing is we do an assessment.

240
00:14:17,780 --> 00:14:21,980
We basically go conduct a complete
comprehensive data architecture audit,

241
00:14:22,280 --> 00:14:26,030
identify critical performance bottlenecks,
security vulnerabilities, and governance

242
00:14:26,030 --> 00:14:30,020
gaps, and then we start to create a
prioritized roadmap based off the business

243
00:14:30,020 --> 00:14:31,610
impact and implementation complexity.

244
00:14:32,110 --> 00:14:34,660
Then we basically come up
with an MVP or a pilot.

245
00:14:35,260 --> 00:14:39,130
We create a set, a high visibility
constrained scope initiative for,

246
00:14:39,180 --> 00:14:43,530
the initial go implementation,
establish clear KPIs and benchmarks

247
00:14:43,530 --> 00:14:47,250
against existing systems, and then
develop internal technical expertise.

248
00:14:47,250 --> 00:14:48,600
We showcasing early wins.

249
00:14:49,500 --> 00:14:52,500
Then once we are able to
successfully achieve that MVP,

250
00:14:52,740 --> 00:14:54,150
then we expand and integrate.

251
00:14:54,450 --> 00:14:57,690
So we basically use Go
Powered and microservices

252
00:14:57,690 --> 00:14:59,680
across, with prior data flows.

253
00:15:00,070 --> 00:15:03,070
And start to scale it out
across, across the other use

254
00:15:03,070 --> 00:15:04,490
cases, within the organization.

255
00:15:05,270 --> 00:15:07,820
And then you basically implement
compress and data governance with

256
00:15:07,820 --> 00:15:09,260
automated compliance controls.

257
00:15:09,830 --> 00:15:14,270
And the last maturity level, transform
then scale solutions to basically

258
00:15:14,270 --> 00:15:18,740
enterprise by data, fabric architecture,
establishing go powered foundation for

259
00:15:18,740 --> 00:15:22,910
advanced AI MAL initiatives achieve
50% reduction in unutilized data.

260
00:15:23,270 --> 00:15:27,320
While improving excess P 2%,
75%, this is where you get your

261
00:15:27,320 --> 00:15:29,180
full near real time capabilities.

262
00:15:29,400 --> 00:15:32,100
your transforming, ETL optimizations.

263
00:15:32,370 --> 00:15:35,545
Then it also eventually feeds into
your AI ML initiatives as well.

264
00:15:36,045 --> 00:15:41,115
So overall, as I mentioned, go is a
very powerful platform or a powerful

265
00:15:41,715 --> 00:15:46,305
utility, which covers you many
tools for you to satisfy in, in, in

266
00:15:46,305 --> 00:15:52,215
building an entirely, robust, cost
effective, scalable data architectures,

267
00:15:52,965 --> 00:15:55,155
not just from 1 1 1 perspective.

268
00:15:55,245 --> 00:15:57,885
There are multiple use cases
within the industry that can be

269
00:15:57,885 --> 00:15:59,190
solved using those architectures.

270
00:15:59,690 --> 00:16:01,370
So looking forward to more questions.

271
00:16:01,580 --> 00:16:02,510
Thank you for everybody's time.

