1
00:00:00,960 --> 00:00:03,090
Hi everyone, and thank you
for joining this session.

2
00:00:04,050 --> 00:00:06,690
I'm work as senior data engineer.

3
00:00:07,110 --> 00:00:11,670
Seven years of experience specialize
in building high performance scalable

4
00:00:11,670 --> 00:00:13,860
data solutions that connect innovation.

5
00:00:13,860 --> 00:00:18,840
With the three year world impact, my
focus is on our systems that can handle

6
00:00:19,110 --> 00:00:24,420
massive data volumes without compromising
speed, reliability, or flexibility.

7
00:00:25,290 --> 00:00:28,710
I'm passionate about exploring
emerging technologies that push

8
00:00:28,710 --> 00:00:32,400
the limits of what's possible
in modern data infrastructure.

9
00:00:32,880 --> 00:00:36,330
That's exactly what brings us
here today to explore how rust

10
00:00:36,570 --> 00:00:40,260
pair with data mesh principles can
completely reshape the way we think

11
00:00:40,260 --> 00:00:42,095
about large scale production grade.

12
00:00:42,595 --> 00:00:47,904
Datas in this session we'll talk about
how Rust can transform modern datas

13
00:00:48,085 --> 00:00:49,735
by applying data mesh principles.

14
00:00:50,065 --> 00:00:56,455
We'll explore why enterprises need to move
away from monolithic systems, how rusts

15
00:00:56,485 --> 00:01:01,495
memory, safety, and fearless concurrency
enabled high performance, reliable

16
00:01:01,495 --> 00:01:03,820
pipelines, and real world implementations.

17
00:01:04,319 --> 00:01:09,060
We'll also look at benchmarks, advanced
risk patterns, strategies for integrating

18
00:01:09,060 --> 00:01:14,040
with cloud infrastructure to build
scalable future ready data platforms.

19
00:01:14,730 --> 00:01:15,690
Let's get started.

20
00:01:16,190 --> 00:01:18,530
We are all aware of how
rapidly data is growing.

21
00:01:19,030 --> 00:01:20,110
It's exponential.

22
00:01:20,530 --> 00:01:24,040
The problem is that monolithic
architectures can't keep up

23
00:01:24,040 --> 00:01:27,490
anymore and we end up with
bottleneck, any inefficiencies.

24
00:01:27,850 --> 00:01:31,000
That's why moving to distributor
system isn't just an option.

25
00:01:31,210 --> 00:01:31,930
It's essential.

26
00:01:32,470 --> 00:01:37,090
Distributor systems give us the
scalability and flexibility we need

27
00:01:37,090 --> 00:01:39,970
to manage these massive data demands.

28
00:01:40,870 --> 00:01:44,440
This is where data mesh comes
in a decentralized approach.

29
00:01:45,085 --> 00:01:49,375
That puts data ownership in the
hands of domain experts, improving

30
00:01:49,375 --> 00:01:53,935
collaboration, reducing dependencies,
and scaling more effectively.

31
00:01:54,435 --> 00:01:56,735
So first, memory safety.

32
00:01:57,235 --> 00:02:02,725
RUS ownership model means no
data races, and let us write the

33
00:02:02,725 --> 00:02:06,595
robust concurrent processing code
without sacrificing comments.

34
00:02:07,225 --> 00:02:11,785
Then there are zero cost fractions,
which let us write expressive high

35
00:02:11,785 --> 00:02:14,635
level code without any runtime penalty.

36
00:02:15,235 --> 00:02:21,810
Finally, fearless concurrency trust makes
parallel processing safe and reliable.

37
00:02:22,495 --> 00:02:26,275
Which is absolutely critical for
high throughput data pipelines.

38
00:02:26,775 --> 00:02:30,735
When we talk about performance,
rust truly stands out.

39
00:02:31,545 --> 00:02:35,775
One of the biggest gains I've
seen is in data processing speed.

40
00:02:36,275 --> 00:02:40,775
Using rust as synchronous runtime,
we can achieve a through 10

41
00:02:40,775 --> 00:02:44,555
times faster class sync compared
to many traditional solutions.

42
00:02:45,185 --> 00:02:47,045
That's not just a small optimization.

43
00:02:47,405 --> 00:02:52,655
That's a difference between a job
finishing in hours versus minutes.

44
00:02:53,155 --> 00:02:57,595
Another huge advantage is building
memory Safe pipelines because

45
00:02:57,595 --> 00:02:59,755
of Rust Strong type system.

46
00:02:59,995 --> 00:03:04,230
A lot of the bugs that would
normally show up at runtime, things

47
00:03:04,310 --> 00:03:07,830
like null point or errors or data
mismatches simply can't happen.

48
00:03:08,100 --> 00:03:09,950
This means we production incidents.

49
00:03:10,705 --> 00:03:13,195
And much more reliable systems.

50
00:03:14,095 --> 00:03:16,375
And then there is stream processing.

51
00:03:16,875 --> 00:03:20,505
If you have worked with JVM based
solutions before, you will know

52
00:03:20,805 --> 00:03:25,865
they can be powerful, but also
heavy in realtime analytics.

53
00:03:26,195 --> 00:03:27,725
Rest outperforms them.

54
00:03:28,055 --> 00:03:31,420
Delivery low latency
and higher throughput.

55
00:03:32,075 --> 00:03:37,805
That means we can process streams
of events faster, react to changes

56
00:03:37,805 --> 00:03:42,995
more quickly and do it all without
consuming excessive resources.

57
00:03:43,205 --> 00:03:48,605
In short, rest gives us speed,
reliability, and efficiency all at one.

58
00:03:49,265 --> 00:03:53,855
And those are the three pillars
you need for any modern high

59
00:03:53,855 --> 00:03:55,385
performance data pipeline.

60
00:03:55,885 --> 00:04:00,345
Let's move from theory into practice
when we are actually building high

61
00:04:00,345 --> 00:04:04,815
performance data systems in rust,
there are few tools and techniques

62
00:04:05,205 --> 00:04:06,495
that really make a difference.

63
00:04:06,705 --> 00:04:09,045
First, a synchronous crossing with Tokyo.

64
00:04:09,545 --> 00:04:15,035
Tokyo is a high performance Synchron
runtime that lets us handle massive

65
00:04:15,185 --> 00:04:19,415
amounts of concurrent work without
blocking threats unnecessarily.

66
00:04:19,915 --> 00:04:24,505
In a data pipeline, this means we can
process incoming requests, transform

67
00:04:24,505 --> 00:04:29,425
data and push results downstream, all
in parallel without choking the system.

68
00:04:30,175 --> 00:04:36,315
Next, zero copy serialization with
survey in many systems, serialization

69
00:04:36,345 --> 00:04:40,875
and D, serialization of bottlenecks
because you are constantly copying

70
00:04:40,875 --> 00:04:43,275
data in and out of memory structures.

71
00:04:43,725 --> 00:04:48,825
With ser, you can add those extra copies,
which not only improves the performance,

72
00:04:49,215 --> 00:04:51,255
but also reduces memory treasure.

73
00:04:52,215 --> 00:04:55,005
This is especially powerful
when you're transforming and

74
00:04:55,005 --> 00:04:56,685
transmitting huge data sets.

75
00:04:57,555 --> 00:05:01,260
And finally, column of data
operations with Apache Arrow.

76
00:05:01,760 --> 00:05:01,850
Arrow.

77
00:05:01,940 --> 00:05:04,405
It's a store and process
data in a column of fiber.

78
00:05:05,060 --> 00:05:08,360
Which is far more
efficient from analytical.

79
00:05:08,860 --> 00:05:12,970
In my experience, combining
rust with Arrow means we can

80
00:05:12,970 --> 00:05:17,470
run high speed analytics on
large data sets that otherwise.

81
00:05:17,970 --> 00:05:20,670
More traditional row
based tourist systems.

82
00:05:21,360 --> 00:05:25,650
When you put all of this together as
in processing zero copy serialization

83
00:05:25,650 --> 00:05:29,760
and columnal data operations, you
get a data pipeline that is extremely

84
00:05:29,790 --> 00:05:31,830
fast and resource efficient.

85
00:05:32,370 --> 00:05:37,140
This is where rust really starts
to shine in real world scenarios.

86
00:05:37,640 --> 00:05:39,135
So now let's talk about building.

87
00:05:40,025 --> 00:05:40,895
Infrastructure.

88
00:05:41,060 --> 00:05:46,355
Infrastructure that supports these high
performance pipelines for me starts with

89
00:05:46,355 --> 00:05:48,365
the high throughput Kafka consumers.

90
00:05:48,755 --> 00:05:54,025
I use the Kafka RS Library,
which is a re around the battle

91
00:05:54,085 --> 00:05:56,605
tested Kafka SEA Library.

92
00:05:57,505 --> 00:06:03,565
It gives us reliable, low latency
consumption, xk, which is perfect

93
00:06:03,565 --> 00:06:05,845
for ingesting millions of.

94
00:06:06,610 --> 00:06:09,340
Even per second without dropping messages.

95
00:06:10,060 --> 00:06:15,880
Then I focus on memory efficient
transformations, rust, iterators, and zero

96
00:06:15,880 --> 00:06:17,740
allocation patterns are game changers.

97
00:06:17,740 --> 00:06:22,630
Here, instead of creating temporary
objects that collect of memory and

98
00:06:22,630 --> 00:06:28,120
put further on the garbage collector,
which rust doesn't even need downstream

99
00:06:28,180 --> 00:06:32,765
transformations directly keeping memory
usage minimal and performance high.

100
00:06:33,265 --> 00:06:38,665
Finally, I think about robust
HTTP APIs when we need to expose

101
00:06:38,665 --> 00:06:40,825
data or services to other systems.

102
00:06:41,125 --> 00:06:44,905
I like using frameworks like
Axiom along with top middleware.

103
00:06:45,415 --> 00:06:50,215
This combination let me build APIs
that are fast, secure, and easy to

104
00:06:50,215 --> 00:06:53,065
integrate into wider data ecosystem.

105
00:06:53,515 --> 00:06:58,015
Plus, since these APIs are written
in rest, they inherit the same

106
00:06:58,015 --> 00:07:01,735
performance and safety guarantees
as the rest of the pipeline.

107
00:07:02,235 --> 00:07:07,215
All of these pieces, Kafka, consumers
efficient transformation and strong APAs

108
00:07:07,605 --> 00:07:13,365
work together to create an infrastructure
that isn't just fast, but also resilient.

109
00:07:13,665 --> 00:07:17,105
It's scalable for long
term growth performance.

110
00:07:17,105 --> 00:07:17,225
Me.

111
00:07:18,155 --> 00:07:22,745
So whenever I talk about rust in data
systems, one of the first questions I

112
00:07:22,745 --> 00:07:27,005
get is, how does it actually perform
compared to Python, Java, or scaler?

113
00:07:27,505 --> 00:07:32,815
So I make it a point to run real world
benchmarks, not just synthetic tests.

114
00:07:32,965 --> 00:07:36,895
I take actual data workloads, that
kind you would see in production

115
00:07:37,225 --> 00:07:41,065
and compare implementations
across these languages every time.

116
00:07:41,665 --> 00:07:44,935
Rust, consistently shows lower
latency, higher throughput,

117
00:07:45,085 --> 00:07:47,215
and better resource efficiency.

118
00:07:47,665 --> 00:07:51,685
The tools I use for these benchmarks
are well established and reliable.

119
00:07:52,120 --> 00:07:56,050
The measure, not just speed,
but also memory usage, startup

120
00:07:56,050 --> 00:07:58,000
times, and consistency under load.

121
00:07:58,300 --> 00:08:02,305
This isn't about proving that one
language is better in all cases, but

122
00:08:02,305 --> 00:08:07,705
about having accurate performance insight
so I can make informed architectural

123
00:08:07,765 --> 00:08:10,585
righteous, and the impact is huge.

124
00:08:11,065 --> 00:08:13,975
These benchmarks directly
influence system design.

125
00:08:14,935 --> 00:08:18,325
For example, if certain part of a
pipeline is performance critical,

126
00:08:18,655 --> 00:08:23,365
real time event processing our data
transformations on massive data sets.

127
00:08:24,085 --> 00:08:26,995
Rest often becomes the
clear choice by contrast.

128
00:08:27,145 --> 00:08:30,805
For less time sensitive components,
another language might make

129
00:08:30,805 --> 00:08:32,515
sense for speed of development.

130
00:08:32,515 --> 00:08:33,475
Ecosystem support.

131
00:08:33,835 --> 00:08:37,165
In short, benchmarking keeps us honest.

132
00:08:37,615 --> 00:08:41,545
It ensures that we are picking the
right tool for the job, and more

133
00:08:41,545 --> 00:08:46,195
often than not, it highlights where
REST can give us a major advantage

134
00:08:46,195 --> 00:08:48,535
in high performance data processing.

135
00:08:49,035 --> 00:08:52,755
Now we have covered
performance in infrastructure.

136
00:08:53,205 --> 00:08:58,245
I want to share some advanced rust
patterns that I found especially

137
00:08:58,245 --> 00:09:01,725
valuable when building large
scale distributed data system.

138
00:09:02,145 --> 00:09:04,395
First active model implementations.

139
00:09:04,635 --> 00:09:08,925
I like using the active model to isolate
domains within a distributor system.

140
00:09:09,495 --> 00:09:11,145
Each actor has its own state.

141
00:09:11,445 --> 00:09:13,695
It communicates through message passing.

142
00:09:14,670 --> 00:09:18,450
Which not only improves modularity,
but also makes the system

143
00:09:18,930 --> 00:09:21,390
much easier to scale in rust.

144
00:09:21,600 --> 00:09:24,120
This model is both safe and efficient.

145
00:09:24,510 --> 00:09:30,890
Thanks to the languages concurrency
guarantees Second custom derived macros

146
00:09:31,400 --> 00:09:35,660
in big projects, I often need to enforce
certain rules consistently across the

147
00:09:35,660 --> 00:09:40,850
code base, like validating the incoming
data structures with custom device macros.

148
00:09:40,850 --> 00:09:41,625
I can automate that.

149
00:09:42,350 --> 00:09:47,540
It means every developer on the team
gets validation for free without having

150
00:09:47,540 --> 00:09:49,610
to remember to write it manually.

151
00:09:49,910 --> 00:09:52,610
Indeed, it keeps the code
base clean and uniform.

152
00:09:53,420 --> 00:09:56,660
And finally, web assembly for security.

153
00:09:57,020 --> 00:09:59,690
This is where Ru Flexible really shines.

154
00:10:00,020 --> 00:10:04,810
I can compile parts of the rest
application into web assembly modules and

155
00:10:04,810 --> 00:10:07,300
then run them in a sandbox environment.

156
00:10:07,750 --> 00:10:08,715
That's perfect, Phil.

157
00:10:09,215 --> 00:10:14,345
Crossing data across different domains
and even different organization because

158
00:10:14,465 --> 00:10:19,295
the isolation gives me an extra layer of
security without giving up performance.

159
00:10:19,985 --> 00:10:25,745
These advanced patterns, actors, macros,
and web assembly, gives us the ability

160
00:10:25,745 --> 00:10:31,565
to build system that are not just fast,
but also maintainable, secure, and ready

161
00:10:31,565 --> 00:10:34,645
to evolve as requirements change for me.

162
00:10:35,485 --> 00:10:39,625
A high performance data system
isn't complete unless it integrates

163
00:10:39,625 --> 00:10:41,515
seamlessly with the cloud environment.

164
00:10:41,515 --> 00:10:42,080
It's deployed.

165
00:10:42,925 --> 00:10:44,875
Rest makes this easier
than you might think.

166
00:10:45,715 --> 00:10:48,085
Start with seamless cloud integration.

167
00:10:48,565 --> 00:10:53,810
Whether I'm deploying on Azure, a Ws, gcp,
REST applications can hook into existing

168
00:10:53,810 --> 00:10:56,060
cloud services with minimal overhead.

169
00:10:56,600 --> 00:11:00,620
I can connect to managed databases,
storage systems, message queues,

170
00:11:00,920 --> 00:11:04,065
just as easily as I can work with on.

171
00:11:04,920 --> 00:11:05,580
Components.

172
00:11:05,760 --> 00:11:11,130
That means they get the flexibility to run
in hybrid or multi-cloud setups without

173
00:11:11,130 --> 00:11:13,290
rewriting large parts of the system.

174
00:11:14,160 --> 00:11:16,950
Next, look at microservices architecture.

175
00:11:17,250 --> 00:11:22,605
Rust is good for building small focused
services that do one job extremely well.

176
00:11:23,105 --> 00:11:28,025
When each service is independent, I can
scale them individually based on demand

177
00:11:28,115 --> 00:11:31,715
scaling the ingestion layer without
touching the analytics layer, for example.

178
00:11:32,015 --> 00:11:37,175
This is approach also makes maintenance
easier because the teams can work

179
00:11:37,175 --> 00:11:40,805
on different services without
stepping on each other source.

180
00:11:41,615 --> 00:11:44,135
And finally, data contracts.

181
00:11:44,135 --> 00:11:44,765
Enforcement.

182
00:11:45,265 --> 00:11:46,165
This is one of the.

183
00:11:46,780 --> 00:11:51,400
Russ hidden superpowers by defining
strict data structures and when

184
00:11:51,400 --> 00:11:52,660
forcing them at compiled time.

185
00:11:52,990 --> 00:11:57,640
I can guarantee that data exchange between
services always in the correct format.

186
00:11:58,120 --> 00:12:02,590
This eliminates a whole category of
runtime errors and reduces the need

187
00:12:02,590 --> 00:12:07,440
for defensive coding when I combine
this capabilities cloud integration.

188
00:12:07,980 --> 00:12:10,620
Microservices and
compiling data contracts.

189
00:12:10,800 --> 00:12:15,240
I get a cloud ready architecture
that's sufficient, safe, and

190
00:12:15,240 --> 00:12:16,560
designed for the long haul.

191
00:12:17,060 --> 00:12:22,160
As we wrap up, I want to bring the
big picture back into focus for me.

192
00:12:22,280 --> 00:12:25,790
Rust Isal, just another
programming language.

193
00:12:26,150 --> 00:12:29,570
It's a game changer for how we
approach data architectures.

194
00:12:30,260 --> 00:12:35,270
Combining rest with the data mesh
principles, I've seen firsthand how we can

195
00:12:35,270 --> 00:12:40,490
build infrastructures that are both highly
scalable, rock solid, and reliability.

196
00:12:41,060 --> 00:12:44,450
Looking ahead, the rest
ecosystem is evolving fast.

197
00:12:44,810 --> 00:12:46,910
We are seeing improvements
in asynchronized.

198
00:12:47,540 --> 00:12:51,200
New libraries for data
processing and even more seamless

199
00:12:51,200 --> 00:12:52,820
integrations with cloud platforms.

200
00:12:53,410 --> 00:12:58,750
All of these developments means the rest
is only going to get more capable for

201
00:12:58,750 --> 00:13:00,850
handling the toughest data challenges.

202
00:13:01,570 --> 00:13:06,405
My takeaway for you is this, if you are
working on system that need to handle.

203
00:13:06,905 --> 00:13:10,715
Massive data volumes, low
latency and high reliability.

204
00:13:11,285 --> 00:13:13,085
Rust should be on your short list.

205
00:13:13,805 --> 00:13:18,395
Gives you performance with sacrificing
safety, without sacrificing

206
00:13:18,395 --> 00:13:22,625
safety and scalability without
the typical complexity overhead.

207
00:13:23,315 --> 00:13:25,465
And I will leave you
with a call to action.

208
00:13:26,215 --> 00:13:27,625
Explore rest.

209
00:13:28,125 --> 00:13:31,185
Try it on a small, high
impact part of your pipeline.

210
00:13:31,485 --> 00:13:34,065
See how it performs in your environment.

211
00:13:34,425 --> 00:13:38,025
You might be surprised at just
how much of a difference it makes.

212
00:13:38,525 --> 00:13:40,145
This concludes my presentation.

213
00:13:40,475 --> 00:13:42,095
Thank you for your time and attention.

214
00:13:42,395 --> 00:13:43,085
Have a good day.

