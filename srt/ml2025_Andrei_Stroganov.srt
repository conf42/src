1
00:00:00,120 --> 00:00:02,340
Hello, my name is Andres Stroganoff.

2
00:00:03,000 --> 00:00:06,960
I'd like to present you our
joint work with and Andrea Bishan

3
00:00:07,470 --> 00:00:11,850
First, confidence estimation for
classification and regression models.

4
00:00:13,800 --> 00:00:16,079
Let's talk about prediction confidence.

5
00:00:16,860 --> 00:00:19,770
Why do we need it and how to define it.

6
00:00:22,080 --> 00:00:27,150
The prediction quality estimation plays
key role in model tuning, and there

7
00:00:27,150 --> 00:00:29,880
are many metrics to do such estimation.

8
00:00:30,854 --> 00:00:37,245
For instance, an F1 score, various area
under the curve, metrics and so on.

9
00:00:38,445 --> 00:00:43,635
The important thing is that during
model tuning, we usually have the

10
00:00:43,635 --> 00:00:49,840
taste in dataset, so the true values
are known to us, and we can compare

11
00:00:49,995 --> 00:00:51,465
the predicted values with them.

12
00:00:52,575 --> 00:00:56,565
Sometimes the machine learning
model is applied on a group basis.

13
00:00:56,940 --> 00:01:01,530
For instance, we want to detect the
infected students within a class.

14
00:01:02,640 --> 00:01:07,890
In this case, we might be interested
not only how well the model

15
00:01:07,890 --> 00:01:13,560
performs on individual samples,
but also on the entire groups.

16
00:01:14,670 --> 00:01:20,009
But what if we don't have the true
values for validating the predictions?

17
00:01:20,940 --> 00:01:23,820
Can we still estimate
the prediction quality?

18
00:01:23,820 --> 00:01:26,850
You well accurately, no.

19
00:01:28,080 --> 00:01:34,699
But we can use the indirect quantitative
information about predictions to tell

20
00:01:34,759 --> 00:01:41,839
if they look reasonable or maybe quite
improbable as a motivating example.

21
00:01:41,839 --> 00:01:45,739
For this problem, we'll consider
a machine learning method for

22
00:01:45,739 --> 00:01:50,419
profile guided optimization that
we proposed in the following paper.

23
00:01:53,314 --> 00:01:56,674
Let's give a brief
introduction to this method.

24
00:01:57,724 --> 00:02:02,434
The Android applications consist
of Java Byte code of classes and

25
00:02:02,434 --> 00:02:07,534
methods, which can be compiled
into a native representation, which

26
00:02:07,534 --> 00:02:12,695
usually leads to faster application
startup and smoother performance

27
00:02:13,774 --> 00:02:15,904
in profile guided optimization.

28
00:02:16,445 --> 00:02:23,075
Not all classes and methods are compiled,
but only those which are frequently used.

29
00:02:23,344 --> 00:02:24,604
They are called hot.

30
00:02:25,474 --> 00:02:32,344
Their indexes are listed in a special
file, a profile in machine learning

31
00:02:32,344 --> 00:02:34,414
method for profile generation.

32
00:02:35,404 --> 00:02:40,004
A model predicts hot classes and
methods and create an optimization

33
00:02:40,004 --> 00:02:41,984
profile for the application.

34
00:02:43,274 --> 00:02:49,274
There are cases when testing
profiles are not available, so direct

35
00:02:49,274 --> 00:02:51,314
prediction validation is impossible.

36
00:02:52,484 --> 00:02:59,294
In this talk, we'll discuss a P value
based method to indirectly estimate the

37
00:02:59,294 --> 00:03:06,194
prediction confidence in similar cases,
and its efficient c plus implementation.

38
00:03:09,045 --> 00:03:10,244
So to which.

39
00:03:10,740 --> 00:03:12,510
Predictions do we trust?

40
00:03:13,530 --> 00:03:15,780
Consider the following example table.

41
00:03:17,100 --> 00:03:21,000
Here we see some user
applications for Android.

42
00:03:21,990 --> 00:03:26,460
For each application, we see the number
of methods in this application, and

43
00:03:26,490 --> 00:03:31,710
in the last column we see a number
of methods which are predicted hot.

44
00:03:33,000 --> 00:03:35,970
How confident are we in these numbers?

45
00:03:37,334 --> 00:03:45,135
Is almost 8,000 hot methods looks okay
for solitaire application with almost

46
00:03:45,225 --> 00:03:53,624
300,000 methods, or does almost 10%
of hot methods looks reasonable for

47
00:03:53,655 --> 00:03:55,155
sofa score, application, location.

48
00:03:56,355 --> 00:04:00,015
We need a metric to measure a
confidence level in such cases.

49
00:04:01,064 --> 00:04:04,394
Our goals for this metric are first.

50
00:04:04,785 --> 00:04:06,975
It should be easily interpretable.

51
00:04:07,364 --> 00:04:14,414
For instance a value zero should mean that
the result is very probable and one should

52
00:04:14,414 --> 00:04:17,294
mean that the result seems like correct.

53
00:04:18,314 --> 00:04:24,434
Again, we can't tell if predictions
are correct if we don't have their true

54
00:04:24,434 --> 00:04:27,974
values, but we can say that statistically.

55
00:04:28,544 --> 00:04:34,274
The number of positive classes or
other qualitative value seems probable

56
00:04:34,304 --> 00:04:39,254
or improbable, so the confidence
is a measure of this probability.

57
00:04:40,724 --> 00:04:45,344
Our second goal is that the metric
evaluation should be very time

58
00:04:45,344 --> 00:04:51,914
efficient, and the third goal is that the
implementation should be easily verified.

59
00:04:52,559 --> 00:04:57,829
In our case, we needed the solution that
won't require much testing to prevent

60
00:04:57,829 --> 00:05:00,729
stability and undefined behavior issues.

61
00:05:01,539 --> 00:05:06,399
To decide which metric to use, let's
examine the real data distribution.

62
00:05:07,089 --> 00:05:12,849
In the table, we see the numbers of
methods in applications from training set.

63
00:05:13,630 --> 00:05:19,450
We also see the actual number of hot
methods and the ratio of the number

64
00:05:19,450 --> 00:05:23,650
of hot methods to the total number
of methods for each application.

65
00:05:25,210 --> 00:05:28,719
On the picture, we see
the ratio distributions.

66
00:05:28,869 --> 00:05:30,129
It is a block figure.

67
00:05:30,909 --> 00:05:36,520
Here we count the number of occurrences
of ratio values within small segments.

68
00:05:37,989 --> 00:05:42,909
We see that it can be roughly
approximated with normal distribution

69
00:05:42,909 --> 00:05:49,960
function, which is a blue line, but we
could use more precise approximations.

70
00:05:50,949 --> 00:05:57,630
For instance the black line is the cube
spine interpolation for confidence metric.

71
00:05:57,900 --> 00:06:03,450
It is natural to reflect the
correspondence of measured

72
00:06:03,450 --> 00:06:05,789
value to the distribution.

73
00:06:08,789 --> 00:06:11,369
And now let's define a metric.

74
00:06:13,619 --> 00:06:19,079
First of all, let's recall what a
probability distribution function is.

75
00:06:19,769 --> 00:06:24,629
It is a function which describes how
probabilities are assigned to the

76
00:06:24,629 --> 00:06:27,389
possible outcomes of a random variable.

77
00:06:28,364 --> 00:06:34,664
There is also a cumulative distribution
function, A CDF, which gives the

78
00:06:34,664 --> 00:06:40,635
probability that the random variable is
less than or equal to a certain value.

79
00:06:42,435 --> 00:06:46,934
There are several useful properties
of cumulative distribution function.

80
00:06:48,090 --> 00:06:49,740
It is non decreasing.

81
00:06:50,250 --> 00:06:56,370
It is bounded by zero and one, and
it can be defined as the integral of

82
00:06:56,460 --> 00:06:58,469
probability distribution function.

83
00:06:59,910 --> 00:07:06,180
On the picture here, we
see a CDF evaluated at 0.4.

84
00:07:06,540 --> 00:07:13,140
It is the area under the curve from
minus infinity to 0.4, and also it is a

85
00:07:13,140 --> 00:07:16,160
probability that the randomly peak point.

86
00:07:17,030 --> 00:07:20,390
Will be less or equal to 0.4.

87
00:07:21,620 --> 00:07:24,830
We can use various probability
distribution functions.

88
00:07:25,310 --> 00:07:30,530
The most common is the normal
distribution, which is shown on the left.

89
00:07:31,130 --> 00:07:33,380
It is defined by two values.

90
00:07:33,470 --> 00:07:39,410
The mean value, which is the center of
horizontal symmetry, and the standard

91
00:07:39,410 --> 00:07:44,750
deviation value, which shows how
dispersed the values from the mean value.

92
00:07:46,130 --> 00:07:51,740
On the right picture, we see the
cubic spine interpolation normalized

93
00:07:51,740 --> 00:07:57,530
so that the area under the curve
would be approximately one s for any

94
00:07:57,530 --> 00:07:59,420
probability distribution function.

95
00:08:00,425 --> 00:08:04,655
Both of these approximations can
be used for measuring confidence.

96
00:08:05,825 --> 00:08:11,825
To define a confidence level, we will
use a p value based method, let TB a

97
00:08:11,825 --> 00:08:14,285
value for which we measure confidence.

98
00:08:15,095 --> 00:08:21,785
F small of X is a probability distribution
function, and F capital of X is

99
00:08:21,785 --> 00:08:23,855
accumulative distribution function.

100
00:08:23,975 --> 00:08:30,995
Or C, D, F. Recall that CDF of X.
Is a probability that a randomly

101
00:08:30,995 --> 00:08:39,035
peaked F distributed value is
less or equal to T and one minus F

102
00:08:39,035 --> 00:08:44,765
capital of X is a probability that
a random value is greater than T.

103
00:08:45,995 --> 00:08:51,965
When these probabilities are significantly
different, then T is shifted away

104
00:08:51,965 --> 00:08:57,275
from the most of the values and should
be considered unlikely to appear.

105
00:08:58,025 --> 00:09:03,575
On the other hand, when probability
of finding the random value left of T

106
00:09:03,605 --> 00:09:09,455
is close to probability of finding it
right of T, then T is approximately

107
00:09:09,515 --> 00:09:11,735
in the middle of the distribution.

108
00:09:12,545 --> 00:09:16,025
In this case, the confidence
metric should give one.

109
00:09:17,135 --> 00:09:21,935
Of course, this is arguable because
there are distributions for which this

110
00:09:21,935 --> 00:09:26,825
won't hold as reasonable definition,
but we won't discuss them now.

111
00:09:28,130 --> 00:09:31,970
So let's define the
confidence measure as doubled.

112
00:09:31,970 --> 00:09:39,530
Minimal probability of finding the
random value left to or right to t.

113
00:09:40,490 --> 00:09:46,190
When these probabilities are equal, we
will get a confidence level equal to one.

114
00:09:47,630 --> 00:09:50,090
So how confident were about prediction.

115
00:09:50,885 --> 00:09:55,205
The confidence here is evaluated
using the normal distribution function

116
00:09:55,235 --> 00:09:58,325
interpolation of our experimental data.

117
00:09:59,165 --> 00:10:04,835
We see that the number of methods
which are classified hot for Connect

118
00:10:04,835 --> 00:10:10,295
application seems about right, but
the number of hot methods in download

119
00:10:10,295 --> 00:10:17,015
application seems very suspicious
due to the exponential behavior.

120
00:10:17,495 --> 00:10:24,245
The confidence for Solitaire application
is still considerably high, although

121
00:10:24,245 --> 00:10:26,885
it depends on what threshold is chosen.

122
00:10:29,795 --> 00:10:34,415
And now let's discuss the
implementation of the confidence metric.

123
00:10:34,745 --> 00:10:43,115
How to approach the computation of fee of
X. First of all, let's recall the goals.

124
00:10:44,030 --> 00:10:48,260
First, create a metric which
evaluates a confidence level as a

125
00:10:48,260 --> 00:10:53,120
real number between zero and one
where one is the most probable result

126
00:10:53,150 --> 00:10:55,730
and zero is the most improbable.

127
00:10:56,600 --> 00:10:59,990
We have done it using cumulative
distribution function.

128
00:11:01,670 --> 00:11:06,140
Second metric evaluation should
be time efficient, and third,

129
00:11:06,500 --> 00:11:09,140
implementation should be easily verified.

130
00:11:11,060 --> 00:11:15,440
Now let's talk about evaluating
the confidence time efficiently.

131
00:11:16,370 --> 00:11:21,380
To compute accumulative distribution
function of X, we'll use a numerical

132
00:11:21,380 --> 00:11:26,780
integration on evenly distributed
grid with step size delta.

133
00:11:28,130 --> 00:11:32,600
The integration range is defined by
probability distribution function.

134
00:11:33,350 --> 00:11:38,285
Let it be the segment from A to
B. Thus, we consider probability

135
00:11:38,285 --> 00:11:43,505
distribution function to be negligible,
close to zero out of this segment.

136
00:11:44,405 --> 00:11:48,395
To get the approximate value of
confidence, we will store the values

137
00:11:48,455 --> 00:11:54,125
of cumulative distribution function
in a pre-computed lookup table.

138
00:11:54,845 --> 00:11:58,535
It'll contain the values of the
integral in the greed points.

139
00:11:59,855 --> 00:12:05,795
The value at first point is zero, and the
value in the last point is approximately

140
00:12:05,795 --> 00:12:11,225
one, as it is the area under the curve
of probability distribution function.

141
00:12:12,335 --> 00:12:19,265
The code below shows that acquiring of the
confidence value is done in constant time.

142
00:12:20,045 --> 00:12:24,515
We just compute the index of
the appropriate point in the

143
00:12:24,515 --> 00:12:29,675
grid and return the doubled,
minimal value of the integral.

144
00:12:29,765 --> 00:12:33,995
In this point here, we
provide a simplified code.

145
00:12:34,415 --> 00:12:39,305
Our c plus implementation is more
general, and it is available on GitHub.

146
00:12:41,075 --> 00:12:46,745
Now let's discuss how to implement
the lookup table with CDF values.

147
00:12:47,255 --> 00:12:51,545
There are several approaches
with its pros and cons.

148
00:12:52,445 --> 00:12:57,515
First of all, we can generate a
table when our application starts.

149
00:12:58,385 --> 00:13:04,085
This is fairly simple, but since it
is done during the runtime, it may

150
00:13:04,085 --> 00:13:06,425
increase the application startup time.

151
00:13:06,830 --> 00:13:09,920
And also it requires
additional unit testing.

152
00:13:11,150 --> 00:13:16,190
Another approach is to generate the
values of CDF before the application

153
00:13:16,190 --> 00:13:19,680
is compiled and fill the area by hand.

154
00:13:20,670 --> 00:13:25,440
For instance, Scion paste the values
computed in Python or somewhere else.

155
00:13:27,660 --> 00:13:30,780
This is very simple
approach, but not flexible.

156
00:13:31,260 --> 00:13:35,940
For instance, if we decide to change some
parameters of probability, distribution

157
00:13:35,940 --> 00:13:41,190
function, or the function itself,
we will have to regenerate values

158
00:13:41,190 --> 00:13:43,710
and update the table in source code.

159
00:13:44,070 --> 00:13:45,420
This is very painful.

160
00:13:46,320 --> 00:13:51,150
The benefit of this approach is that
the runtime code is very simple.

161
00:13:51,570 --> 00:13:53,760
We only have the lookup function.

162
00:13:55,650 --> 00:14:02,310
But in c plus, we can utilize the generic
programming approach, which allows to

163
00:14:02,340 --> 00:14:09,090
run an algorithm during the compilation
of a program so we can generate a table,

164
00:14:09,420 --> 00:14:15,000
not before or after the compilation,
but during the compilation itself.

165
00:14:16,050 --> 00:14:21,840
This allows to add error checks to spot
the issues before the application is

166
00:14:21,840 --> 00:14:25,650
started, so less unit testing is required.

167
00:14:26,340 --> 00:14:32,280
Also, since the CDF table will
be generated at compile time, the

168
00:14:32,280 --> 00:14:38,160
runtime code will be as simple as we
showed just to look up into a table.

169
00:14:39,435 --> 00:14:43,545
The great thing about generic
programming approach is that we

170
00:14:43,545 --> 00:14:48,825
can plug in a custom distribution
function, whether it is given by a

171
00:14:48,825 --> 00:14:51,915
formula or by piece wise approximation.

172
00:14:52,875 --> 00:14:57,765
The downside of this approach is that
usually the compiled time evaluated

173
00:14:58,035 --> 00:15:04,365
code is a bit more complex and strict,
but we'll have no problems with

174
00:15:04,365 --> 00:15:07,215
that, so we will use this approach.

175
00:15:09,195 --> 00:15:13,755
And now we will implement the
lookup table generation and normal

176
00:15:13,755 --> 00:15:15,585
distribution function evaluation.

177
00:15:18,075 --> 00:15:22,215
First of all, accumulation
distribution function requires

178
00:15:22,215 --> 00:15:24,165
computation of the integral.

179
00:15:24,765 --> 00:15:27,795
We will use the ZO rule approximation.

180
00:15:29,130 --> 00:15:33,570
We split the integration segment
from A to B into a consecutive

181
00:15:33,570 --> 00:15:40,200
segments where area on each segment
is interpolated by the area of

182
00:15:40,230 --> 00:15:46,780
trapezoid which is usually computed
by the lens of each parallel sites.

183
00:15:47,965 --> 00:15:53,005
In the picture, we see the example
of computing of one such area.

184
00:15:53,395 --> 00:15:59,395
The lengths of the sites are given
by the values of the function at

185
00:15:59,395 --> 00:16:03,415
points X sub I and x sub i plus one.

186
00:16:04,165 --> 00:16:08,905
And the area is the mean of
these lengths multiplied by

187
00:16:08,905 --> 00:16:10,855
the distance between the sides.

188
00:16:11,890 --> 00:16:19,150
The integral is approximated as the sum
of areas of all OIDs by the Formula One.

189
00:16:20,650 --> 00:16:26,860
Note that each inner point is
contained by exactly two OIDs.

190
00:16:27,370 --> 00:16:32,380
That's why the sum of the inner
points is not divided by two.

191
00:16:34,210 --> 00:16:37,300
Computation of the CDF
table is fairly simple.

192
00:16:37,915 --> 00:16:44,395
Here is our OID approximation of the
integral and below is the quote, which

193
00:16:44,395 --> 00:16:50,705
computes values of CDF of I evaluating
probability distribution function.

194
00:16:50,975 --> 00:16:57,395
Once for each G grid point, the first
value of CDF is defined as zero.

195
00:16:58,415 --> 00:17:03,605
In variable sum, we will store
the sum of PDF values in the inner

196
00:17:03,605 --> 00:17:09,965
points up to current point and a
half of PDF value in the first point.

197
00:17:10,925 --> 00:17:17,595
Thus, to compute the CDF in current
point, we take the sum variable and add

198
00:17:17,685 --> 00:17:24,375
to it the half of the value of PDF in
current point and multiplied by Delta.

199
00:17:25,335 --> 00:17:29,265
In the last line, we just
update the sum variable.

200
00:17:30,855 --> 00:17:34,695
And now let's see how a
probability distribution function

201
00:17:34,785 --> 00:17:39,855
of for normal distribution can
be computed in compiled time.

202
00:17:40,785 --> 00:17:41,775
It's a bit tricky.

203
00:17:42,465 --> 00:17:45,195
The function is given by the Formula two.

204
00:17:46,845 --> 00:17:52,155
We have no problems with computing
the root of two PI because we just.

205
00:17:52,545 --> 00:17:55,305
Substitute its value as the constant.

206
00:17:56,205 --> 00:18:03,375
But in c plus 17, which is a popular
standard for c plus, the exponent function

207
00:18:03,555 --> 00:18:10,515
is not a constant expression, and it can
be used in compiled time computations.

208
00:18:11,325 --> 00:18:16,695
So we'll make our own a compiled
time friendly exponent function.

209
00:18:17,775 --> 00:18:23,475
Let's recall the tailor expansion
of E to the x. In Formula three,

210
00:18:24,475 --> 00:18:28,705
in the code below, we see a
straightforward implementation

211
00:18:28,705 --> 00:18:29,875
of this summation.

212
00:18:30,445 --> 00:18:37,760
Four positive values of X. It adds
terms consequently to variable E

213
00:18:38,050 --> 00:18:41,200
until next term is approximately zero.

214
00:18:42,775 --> 00:18:48,235
With an increasing, the factorial
will eventually scale down any

215
00:18:48,235 --> 00:18:54,445
term to zero, but some TER terms
may become quite large and make

216
00:18:54,505 --> 00:18:57,355
overflow, which will corrupt the sum.

217
00:18:58,345 --> 00:19:04,525
Thus, we will use this algorithm,
four values of X, less than

218
00:19:04,705 --> 00:19:10,195
one, and for the greater values,
we will use another approach.

219
00:19:11,020 --> 00:19:16,720
When absolute value of X is greater
than zero, we will split it into a

220
00:19:16,720 --> 00:19:23,950
sum of two values, an integer part, a
variable U, and the fractional part.

221
00:19:24,340 --> 00:19:29,800
A variable V. Since V is less
than one, will already know

222
00:19:29,830 --> 00:19:31,690
how to compute E to the v.

223
00:19:32,425 --> 00:19:38,425
And for E to the U, we'll use the bin
pole algorithm, which computes the integer

224
00:19:38,425 --> 00:19:41,575
power of the value in logarithmic time.

225
00:19:42,355 --> 00:19:46,075
The algorithm of computing
E to the U is given below.

226
00:19:47,365 --> 00:19:54,295
So to compute E to the X for arbitrary
X, we'll use the combination of these

227
00:19:54,295 --> 00:19:56,905
two algorithms and Formula four.

228
00:19:58,660 --> 00:20:03,160
All discussed algorithms can be
evaluated in compiled time and the

229
00:20:03,160 --> 00:20:06,040
full source code is provided in GitHub.

230
00:20:08,560 --> 00:20:14,680
And to sum up, we discussed a confidence
estimation approach, focus ification, and

231
00:20:14,680 --> 00:20:17,500
regression models based on P value score.

232
00:20:18,400 --> 00:20:22,390
We provided an implementation
of the confidence evaluation

233
00:20:22,810 --> 00:20:25,150
with constant time complexity.

234
00:20:26,095 --> 00:20:32,155
The approach is general and can be applied
to various data distributions, including

235
00:20:32,155 --> 00:20:35,005
the approximations of experimental data.

236
00:20:36,265 --> 00:20:37,915
I thank you for your attention.

237
00:20:38,845 --> 00:20:39,715
Have a nice day.

