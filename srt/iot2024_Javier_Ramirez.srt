1
00:00:00,520 --> 00:00:02,009
Hi, my name is Javier.

2
00:00:02,610 --> 00:00:07,420
I'm a developer advocate at QuestDB,
an open source fast time series

3
00:00:07,420 --> 00:00:11,770
database, and I'm here today to speak
about databases, but I'm not going

4
00:00:11,799 --> 00:00:13,790
to speak specifically about QuestDB.

5
00:00:13,819 --> 00:00:16,909
I have some demos, but I'm
here to speak about databases.

6
00:00:17,360 --> 00:00:21,530
about a new architecture, the type
three architecture, that we've

7
00:00:21,530 --> 00:00:26,389
seen many databases adopting in
the past few months and why this

8
00:00:26,390 --> 00:00:28,080
architecture is a good thing for you.

9
00:00:28,630 --> 00:00:31,709
But before we start, let
me ask you something.

10
00:00:32,449 --> 00:00:37,669
When was the last time you
wish your database was using a

11
00:00:37,669 --> 00:00:39,809
more proprietary data format?

12
00:00:40,789 --> 00:00:41,559
Think about that.

13
00:00:42,059 --> 00:00:45,584
Did you ever think, Damn,
my database is broken.

14
00:00:45,944 --> 00:00:51,785
It's using a data format, which is so
standard and so open that I can easily

15
00:00:52,294 --> 00:00:56,984
reuse it and share with any other
applications like, I don't know, my data

16
00:00:56,984 --> 00:01:02,374
lake or my, machine learning pipeline,
for example, I don't know about you,

17
00:01:02,774 --> 00:01:07,044
but I've been working with databases for
longer than I care to remember and with

18
00:01:07,054 --> 00:01:09,334
big and fast data for the past 10 years.

19
00:01:09,904 --> 00:01:15,334
And I never thought having data in an
open format in a form which is compatible

20
00:01:15,344 --> 00:01:16,824
with many systems is a bad thing.

21
00:01:17,404 --> 00:01:24,314
If your data is in an open format, you can
easily share across different applications

22
00:01:24,384 --> 00:01:29,834
without duplicating data and without
having to spend time moving data around,

23
00:01:30,164 --> 00:01:32,664
transforming data and using resources.

24
00:01:32,864 --> 00:01:33,954
What's not to like?

25
00:01:34,644 --> 00:01:39,304
On a traditional database,
ingesting data is fairly simple.

26
00:01:39,704 --> 00:01:44,275
But if you want to extract the data from
the database, not just for one specific

27
00:01:44,304 --> 00:01:49,144
query, return a few rows, but if you
want to extract the bulk of your data,

28
00:01:49,155 --> 00:01:53,920
all of your data from one table, and
share that data with another system.

29
00:01:53,960 --> 00:01:57,430
Imagine, for example, you want to
train a machine learning pipeline

30
00:01:57,860 --> 00:02:02,849
and you want to get all the data you
have in your database out so your

31
00:02:02,880 --> 00:02:04,949
pipeline can train from that data.

32
00:02:05,410 --> 00:02:06,570
That's not so easy to do.

33
00:02:07,479 --> 00:02:10,780
It's going to take a while to
export if you have a large table.

34
00:02:11,280 --> 00:02:15,769
You truly have to move the data around
from one server to another, and you

35
00:02:15,790 --> 00:02:21,840
truly need to either transform the data
into another format, or bring just in

36
00:02:21,840 --> 00:02:24,090
the other system to make it efficient.

37
00:02:24,159 --> 00:02:28,790
in the end, you're going to be wasting
a lot of time just extracting data.

38
00:02:29,159 --> 00:02:33,604
in a way, the database, It's acting
as the gatekeeper to your data.

39
00:02:33,914 --> 00:02:39,864
It's hey, you can get in, but getting
out, ah, that's going to be a bit painful.

40
00:02:40,364 --> 00:02:41,734
And I should know about that.

41
00:02:41,864 --> 00:02:46,354
As I told you earlier, I work for
QuizDB, and in QuizDB, we've been

42
00:02:46,354 --> 00:02:48,284
doing that for the past few years.

43
00:02:48,834 --> 00:02:51,234
So why databases behave that way?

44
00:02:51,394 --> 00:02:52,414
You think about that.

45
00:02:52,415 --> 00:02:58,724
20, 30 years ago, databases were
designed for a very different use case.

46
00:02:59,144 --> 00:03:06,374
Today, on IoT, it's very common to
have data, to generate data at, I don't

47
00:03:06,374 --> 00:03:08,954
know, a few thousand rows per second.

48
00:03:09,224 --> 00:03:12,564
I've seen systems even generating
a few millions of rows per

49
00:03:12,564 --> 00:03:15,344
second on a single application.

50
00:03:15,844 --> 00:03:18,204
20 years ago, that was unthinkable.

51
00:03:18,554 --> 00:03:24,594
20 years ago, if an application was
very successful, it would store maybe

52
00:03:24,934 --> 00:03:27,214
a few million records on a database.

53
00:03:27,394 --> 00:03:30,524
In the lifetime of the
application, not in one second,

54
00:03:30,674 --> 00:03:32,594
not in one day, not in one year.

55
00:03:33,075 --> 00:03:37,054
In the lifetime of the application, you
will store maybe a few millions of rows.

56
00:03:37,809 --> 00:03:42,816
And if you had a query that took a
couple of seconds, no one will complain.

57
00:03:42,816 --> 00:03:48,149
The concept of real time we have today,
in which you need to have milliseconds

58
00:03:48,199 --> 00:03:53,140
latency between ingestion and being able
to query the data because maybe you have

59
00:03:53,510 --> 00:03:54,709
a critical process depending on that.

60
00:03:55,219 --> 00:03:58,179
That was not the common case 20 years ago.

61
00:03:58,579 --> 00:04:01,449
So databases were designed
in a different way.

62
00:04:02,259 --> 00:04:10,389
And I don't know about you, but, in the
late 90s, early 90s, I was a web developer

63
00:04:10,869 --> 00:04:13,689
and I heard this complaint very often.

64
00:04:13,739 --> 00:04:14,129
Hey!

65
00:04:14,579 --> 00:04:21,679
The application is slow and the database
is the bottleneck and that was true The

66
00:04:21,749 --> 00:04:28,069
database was the bottleneck and that
happened because By the end of the 90s and

67
00:04:28,069 --> 00:04:33,989
the early noughties the world changed We
moved from a world in which applications

68
00:04:33,989 --> 00:04:41,364
were mostly corporate and databases were
mostly corporate to a world in which, we

69
00:04:41,364 --> 00:04:43,864
had websites for absolutely everything.

70
00:04:44,244 --> 00:04:48,344
Not only that, early notice,
we started getting, cell

71
00:04:48,364 --> 00:04:51,044
phones and mobile applications.

72
00:04:51,454 --> 00:04:54,144
So we have two interesting patterns here.

73
00:04:54,474 --> 00:04:58,429
First, data was arriving faster than ever.

74
00:04:58,729 --> 00:05:02,479
And from many different places,
not from a single point of entry.

75
00:05:03,169 --> 00:05:07,519
And second, we were storing
not only transactional data,

76
00:05:07,559 --> 00:05:09,509
but also analytical data.

77
00:05:09,519 --> 00:05:15,734
We started Analyzing user behavior
and relational databases were

78
00:05:15,734 --> 00:05:17,844
not really designed for that.

79
00:05:18,294 --> 00:05:22,654
And it was difficult for them
to deal with the speed of the

80
00:05:22,654 --> 00:05:24,904
data and the amount of the data.

81
00:05:25,224 --> 00:05:27,754
And then we saw two interesting
trends on databases.

82
00:05:28,124 --> 00:05:32,754
First, NoSQL databases,
super fast to inserting data.

83
00:05:33,124 --> 00:05:36,044
Super fast to query the data in exchange.

84
00:05:36,514 --> 00:05:43,484
Not consistency, no constraints, simple
queries, no indexes, but super fast.

85
00:05:43,604 --> 00:05:47,524
So if you wanted just ingesting
fast data, a NoSQL database

86
00:05:47,544 --> 00:05:49,404
would be a super good option.

87
00:05:49,914 --> 00:05:53,844
On the other hand, if you wanted to
have analytics and you didn't want

88
00:05:53,854 --> 00:05:58,424
to use a corporate data warehouse,
you could use the brand new

89
00:05:58,534 --> 00:06:00,874
analytical databases, OLAP databases.

90
00:06:01,249 --> 00:06:06,619
That were specialized in running complex
queries across huge amounts of data.

91
00:06:06,959 --> 00:06:08,279
Latency was not great.

92
00:06:08,729 --> 00:06:11,739
It was usually in the seconds
to minutes rather than

93
00:06:11,739 --> 00:06:14,149
milliseconds as we expect today.

94
00:06:15,059 --> 00:06:17,679
And they had to prepare the
data in many different ways.

95
00:06:18,154 --> 00:06:23,824
So they were optimized for batch inserts
and batch queries, but still they

96
00:06:23,824 --> 00:06:26,424
could analyze huge amounts of data.

97
00:06:26,774 --> 00:06:30,194
And of course, article
databases, they got better.

98
00:06:30,554 --> 00:06:33,854
They started a train of separating
storage and computations.

99
00:06:33,854 --> 00:06:34,029
So now.

100
00:06:34,734 --> 00:06:38,774
You could use multiple instances
in parallel to query data and

101
00:06:38,774 --> 00:06:41,124
then aggregate, reducing latency.

102
00:06:41,454 --> 00:06:44,544
So still we are speaking about
seconds rather than milliseconds,

103
00:06:44,584 --> 00:06:46,314
but they were way faster.

104
00:06:46,624 --> 00:06:51,764
And they also introduced the trend
of the data lake, in which we have a

105
00:06:51,764 --> 00:06:57,454
central repository of data, in which
you store the data in some open formats.

106
00:06:58,189 --> 00:07:03,449
Might be parquet, most commonly, could
be JSON, could be CSV, basically formats

107
00:07:03,459 --> 00:07:07,939
that different tools can consume without
having duplicate, which is great, which

108
00:07:07,939 --> 00:07:09,379
is what we are talking about today.

109
00:07:09,919 --> 00:07:12,879
And still, even with the data
lake and with the separated

110
00:07:12,919 --> 00:07:17,049
storage and computation, these
tools were mostly for batch.

111
00:07:17,549 --> 00:07:21,139
And one thing that was still a
problem with analytical databases.

112
00:07:21,929 --> 00:07:27,119
It was that they were designed for
immutable data because of the way they

113
00:07:27,119 --> 00:07:31,539
used to prepare data and because of the
formats they were using, like parquet,

114
00:07:32,419 --> 00:07:36,709
maybe storing data in object storage,
like Google Cloud Storage or Amazon

115
00:07:36,710 --> 00:07:42,859
S3 or Azure Blob Storage, because of
that, they were not really designed for

116
00:07:42,889 --> 00:07:48,589
random access and random modifications,
but for append only operations.

117
00:07:49,099 --> 00:07:51,809
And having immutable data was not ideal.

118
00:07:52,409 --> 00:07:55,359
This has changed now, and I
will talk about that later.

119
00:07:55,749 --> 00:08:00,079
But, at the time, analytical
databases were not really designed

120
00:08:00,109 --> 00:08:03,909
for real time, and were not really
designed for immutable data.

121
00:08:04,409 --> 00:08:08,484
But, if you are working in a use
case, In which you are generating a

122
00:08:08,484 --> 00:08:13,054
streaming data, and of course, IOT
is one of the typical use cases like

123
00:08:13,214 --> 00:08:16,444
finance or energy data or mobility.

124
00:08:16,844 --> 00:08:20,384
if you are working with a use
case in which you need a streaming

125
00:08:20,384 --> 00:08:25,164
data, ideally, you want a system,
a database that can do everything.

126
00:08:25,824 --> 00:08:27,079
How hard can that be?

127
00:08:27,689 --> 00:08:31,159
it turns out that working with
a streaming data, it's tricky.

128
00:08:31,899 --> 00:08:34,059
First, because a streaming
data can get very big.

129
00:08:34,539 --> 00:08:39,349
If you keep a streaming data and
every few milliseconds, or every few

130
00:08:39,349 --> 00:08:42,839
seconds, you are getting new data
points from many different devices,

131
00:08:43,179 --> 00:08:48,209
you're going to be getting to a
few billion records very quickly.

132
00:08:48,269 --> 00:08:51,269
And a few billion records is
something that most databases use.

133
00:08:51,594 --> 00:08:57,344
are not going to be able to work with in
a comfortable way and data never stops.

134
00:08:57,844 --> 00:09:02,084
So whenever you need to do any
calculation, data is always,

135
00:09:02,114 --> 00:09:03,444
there is always more data coming.

136
00:09:03,794 --> 00:09:08,554
So you need some way of setting
time barriers or sampling data

137
00:09:08,554 --> 00:09:10,174
by time or something like that.

138
00:09:10,884 --> 00:09:14,794
Of course, the data that you are getting
when you are streaming in real time is

139
00:09:14,794 --> 00:09:17,164
going to, is not going to be constant.

140
00:09:17,184 --> 00:09:17,734
You're going to have.

141
00:09:18,289 --> 00:09:22,279
But of data and you're going to have
some luck because you're going to

142
00:09:22,279 --> 00:09:26,309
have sensors in different factories
across the wall and the network is

143
00:09:26,329 --> 00:09:28,369
going to be a slower in some places.

144
00:09:28,859 --> 00:09:32,479
And just because of latency data that
is generated closer to the server.

145
00:09:32,869 --> 00:09:36,739
It's going to get faster into the
server, even if it was generated later.

146
00:09:36,889 --> 00:09:37,809
that's going to happen.

147
00:09:38,219 --> 00:09:40,149
And devices are going to
be running out of battery.

148
00:09:40,149 --> 00:09:41,349
They are going to restart.

149
00:09:41,359 --> 00:09:42,879
They are going to get disconnected.

150
00:09:43,239 --> 00:09:45,869
So data is going to be
coming out of order.

151
00:09:46,139 --> 00:09:47,589
It's going to be coming late.

152
00:09:47,879 --> 00:09:51,769
And very often, it will come when
you already emit some results.

153
00:09:52,049 --> 00:09:55,129
So you need to be able to
update whatever you did.

154
00:09:55,169 --> 00:10:00,049
So if you're working with immutable
systems, That's going to be hard and you

155
00:10:00,049 --> 00:10:04,529
need to have some way of working with
individual points but also when data is

156
00:10:04,529 --> 00:10:09,914
getting old some way of aggregating data
because aggregations of older data are

157
00:10:09,914 --> 00:10:16,184
more valuable than individual data, which
are more valuable in the recent time

158
00:10:16,274 --> 00:10:23,274
analytics and all of this, of course, with
low latency and queries that show data

159
00:10:23,354 --> 00:10:27,754
as fresh as possible, because very often
you're going to be working with critical

160
00:10:27,754 --> 00:10:29,614
systems in which maybe you are getting in.

161
00:10:30,389 --> 00:10:35,939
Data from a sensor, and you need to emit
some alert if something is wrong, so the

162
00:10:35,939 --> 00:10:38,199
workers, the operator can clear the area.

163
00:10:38,379 --> 00:10:43,889
So you cannot allow for seconds
latency in some use cases.

164
00:10:43,899 --> 00:10:44,839
So that's tricky.

165
00:10:45,319 --> 00:10:49,939
And that's made a new type
of database appear, which I'm

166
00:10:49,939 --> 00:10:52,369
going to call fast databases.

167
00:10:52,649 --> 00:10:58,579
So fast databases will be like a
specialization of analytical databases.

168
00:10:59,464 --> 00:11:00,934
that are designed for real time.

169
00:11:00,944 --> 00:11:02,084
That's basically it.

170
00:11:02,784 --> 00:11:07,524
And in fast databases, a very popular
type of database, of course, in

171
00:11:07,584 --> 00:11:09,894
IoT, is the time series database.

172
00:11:10,574 --> 00:11:15,664
So time series databases are databases
that specialize in very fast ingestion,

173
00:11:16,164 --> 00:11:22,674
in very fast queries, over the recent
data, but also can query data, historical

174
00:11:22,684 --> 00:11:29,134
data, and can use techniques like
downsampling old data, maybe deleting

175
00:11:29,134 --> 00:11:34,514
old partitions, moving data to object
storage, so queries are slower for the

176
00:11:34,514 --> 00:11:36,144
historical data but still available.

177
00:11:36,549 --> 00:11:41,679
But basically, time series databases
specialize in those type of use cases.

178
00:11:42,209 --> 00:11:44,769
as I told you before, I work for QuestDB.

179
00:11:45,429 --> 00:11:48,159
QuestDB is an open source
time series database.

180
00:11:48,179 --> 00:11:49,499
It has the Apache 2.

181
00:11:49,499 --> 00:11:49,779
0 license.

182
00:11:50,264 --> 00:11:53,744
Which basically means you can
use it for any use case you want

183
00:11:54,244 --> 00:11:55,624
completely for free, of course.

184
00:11:56,184 --> 00:11:57,944
And it's a fairly popular project.

185
00:11:57,974 --> 00:12:03,629
We have today almost
15, 000 stars on GitHub.

186
00:12:03,629 --> 00:12:06,663
over 150 contributors already.

187
00:12:06,713 --> 00:12:07,743
This slide is a slightly different one.

188
00:12:08,153 --> 00:12:10,783
Out of date, but yeah,
it's a popular project.

189
00:12:10,893 --> 00:12:14,103
We have thousands of users
happily using QuestDB.

190
00:12:14,603 --> 00:12:19,003
And I want to give you an overview
of which are the internals of

191
00:12:19,003 --> 00:12:21,143
QuestDB and what is QuestDB.

192
00:12:21,613 --> 00:12:26,323
so you can see why we are adopting
now the Type 3 architecture.

193
00:12:26,343 --> 00:12:30,858
Because for the past 10 years, We've been
a traditional database, a fast database,

194
00:12:31,148 --> 00:12:36,198
but with a traditional architecture in
which we were using our own format that is

195
00:12:36,208 --> 00:12:37,768
not proprietary because it's open source.

196
00:12:37,818 --> 00:12:40,998
you can go here and see how we
are storing the data, but no

197
00:12:40,998 --> 00:12:42,168
one else is using this format.

198
00:12:42,168 --> 00:12:45,528
So basically we are not
compatible with any other system.

199
00:12:45,873 --> 00:12:47,303
Until now that we are changing.

200
00:12:47,313 --> 00:12:50,153
So let me tell you a little
bit about QuestDB first.

201
00:12:50,673 --> 00:12:52,943
QuestDB is the fast database.

202
00:12:53,313 --> 00:12:56,493
We store data in column,
in columnar format.

203
00:12:56,833 --> 00:13:00,643
So it's very quick to retrieve the data.

204
00:13:01,143 --> 00:13:05,233
We have a parallel SQL engine custom
made with a just in time compiler.

205
00:13:05,543 --> 00:13:08,293
So every time you execute a
query, we can parallelize.

206
00:13:08,613 --> 00:13:12,073
across many CPUs and
many threads in the CPUs.

207
00:13:12,523 --> 00:13:14,043
The data is always partitioned.

208
00:13:14,363 --> 00:13:17,413
You'll see a little bit about that later.

209
00:13:18,003 --> 00:13:19,263
We don't use indexes.

210
00:13:19,763 --> 00:13:26,578
We have some specialized indexes, but,
we usually They score it using index

211
00:13:26,578 --> 00:13:29,638
sex, except for very specific use cases.

212
00:13:30,118 --> 00:13:34,218
And, in most cases, we are very
fast without having to index data.

213
00:13:34,648 --> 00:13:39,675
So we have very low latency
between ingestion and the

214
00:13:39,675 --> 00:13:42,358
data being able to be queried.

215
00:13:42,828 --> 00:13:44,618
So it's practically immediate.

216
00:13:44,968 --> 00:13:48,128
After you read the data,
you can query data already.

217
00:13:48,708 --> 00:13:50,738
we separate ingestion and reads.

218
00:13:51,508 --> 00:13:59,568
So even if your database is experiencing
a heavy load on queries, you can still

219
00:13:59,588 --> 00:14:01,378
have a predictable ingestion rate.

220
00:14:01,828 --> 00:14:09,198
So if you know with your current machine,
you can always ingest, let's say, I

221
00:14:09,198 --> 00:14:11,698
don't know, 350, 000 events per second.

222
00:14:11,858 --> 00:14:15,228
Even if your server is experiencing
heavy load of queries, You can

223
00:14:15,228 --> 00:14:19,808
always make sure you are going to
be ingesting data at the same rate.

224
00:14:20,398 --> 00:14:25,718
And we have goodies like building
the duplication or updates on absurd.

225
00:14:25,748 --> 00:14:29,058
So basically what you would
expect from a modern database

226
00:14:29,068 --> 00:14:31,218
for time series these days.

227
00:14:32,138 --> 00:14:34,748
Let me show you a little bit
how you can use QuickQuestDB.

228
00:14:35,248 --> 00:14:37,468
I'm not going to do an ingestion demo.

229
00:14:37,498 --> 00:14:40,578
If you want to test ingestion,
you can go to the QuestDB website.

230
00:14:40,938 --> 00:14:45,738
Or to this repository and try on
your own, but I'm going to give you a

231
00:14:45,738 --> 00:14:48,288
couple of things we can do in QuestDB.

232
00:14:48,408 --> 00:14:54,418
For example, this is a public dashboard
in which we have live financial data.

233
00:14:54,458 --> 00:14:56,268
We are ingesting data in real time.

234
00:14:56,733 --> 00:15:00,243
Grafana that is sending
SQL queries to QuestDb.

235
00:15:00,763 --> 00:15:04,123
Every, actually, I'm going
to make this a bit faster.

236
00:15:04,683 --> 00:15:08,783
So now, every quarter of a second,
we are sending queries to QuestDb.

237
00:15:08,953 --> 00:15:11,643
And every quarter of a
second, we are versing data.

238
00:15:11,673 --> 00:15:13,423
So you can see it's quite responsive.

239
00:15:14,143 --> 00:15:18,553
We have another dashboard here, which
for IoT might be more relevant, which

240
00:15:18,553 --> 00:15:20,863
is taxi rides in the city of New York.

241
00:15:21,493 --> 00:15:27,223
So you can see here every time a journey
on a taxi is starting or finishing,

242
00:15:27,383 --> 00:15:29,753
we are plotting this data on a map.

243
00:15:30,253 --> 00:15:35,973
And we also have here some
passengers stats and the correlation

244
00:15:35,973 --> 00:15:38,593
between the tip and the fare.

245
00:15:38,878 --> 00:15:41,088
Of the taxi ride and so on and so forth.

246
00:15:41,528 --> 00:15:45,578
These two dashboards are powered by
public data set we have available,

247
00:15:45,588 --> 00:15:47,448
sorry, in our demo machine.

248
00:15:47,448 --> 00:15:50,108
If you come to this, machine, DemoQuestDB.

249
00:15:50,358 --> 00:15:53,328
io, you can play with this data yourself.

250
00:15:54,098 --> 00:15:57,708
This is the trading data, and this
is the taxi rides, the trips data.

251
00:15:58,498 --> 00:16:01,158
And for the trips data,
it's not a small data set.

252
00:16:01,658 --> 00:16:01,908
It's 1.

253
00:16:01,908 --> 00:16:03,628
6 billion records.

254
00:16:03,748 --> 00:16:05,968
It's not huge, but it's not too bad.

255
00:16:06,508 --> 00:16:07,338
So we have 1.

256
00:16:07,338 --> 00:16:10,228
6 billion records with
a lot of columns here.

257
00:16:10,698 --> 00:16:14,290
And you can do things like, for
example, we have here some sample

258
00:16:14,290 --> 00:16:15,948
queries you can test yourself.

259
00:16:15,948 --> 00:16:20,243
And I'm going to be asking, for example,
the I can see the average distance

260
00:16:21,243 --> 00:16:26,363
on this data set, and I can see the
average distance on this data set.

261
00:16:26,753 --> 00:16:28,363
It's 2.

262
00:16:28,443 --> 00:16:37,113
8 miles, and it took 200 milliseconds to
calculate the average distance over a 1.

263
00:16:37,113 --> 00:16:38,053
6 billion data set.

264
00:16:38,843 --> 00:16:40,313
Of course, if I limit.

265
00:16:40,873 --> 00:16:48,523
this to, just one year, for example,
just the year 2018, now it's way faster.

266
00:16:48,663 --> 00:16:52,713
It took only 19 milliseconds to
calculate the average distance.

267
00:16:53,193 --> 00:16:56,283
And we are still reading
a large amount of data.

268
00:16:56,313 --> 00:17:00,793
Let me see how many trips we have in 2018.

269
00:17:01,713 --> 00:17:04,053
It was 110 million records, not too bad.

270
00:17:05,028 --> 00:17:09,018
And then you can do things like not
only getting the average distance, but

271
00:17:09,028 --> 00:17:14,058
we have extensions to do things like
I want to get the average distance

272
00:17:14,058 --> 00:17:16,908
sampling 15 days interval, for example.

273
00:17:17,268 --> 00:17:21,538
Oh, and actually I'm going to put here
the date so this is easier to see.

274
00:17:22,168 --> 00:17:28,408
So if I execute this query, I'm going
to now get for each 15 days interval,

275
00:17:28,508 --> 00:17:32,938
you can see here, 19, the 4, 18.

276
00:17:33,218 --> 00:17:34,378
So every 15 days.

277
00:17:34,808 --> 00:17:38,318
I have a record here, and this is
arbitrary, I could, I can do every

278
00:17:38,318 --> 00:17:44,078
two, every two months, I can go
to, from years to microseconds, I

279
00:17:44,078 --> 00:17:49,808
can go to any arbitrary, a month
I want here, 22 days, whatever.

280
00:17:50,158 --> 00:17:54,648
So here, as you see, 140
milliseconds to calculate the

281
00:17:54,688 --> 00:17:57,398
average in 22 days intervals.

282
00:17:57,608 --> 00:17:58,778
it's quite flexible and performant.

283
00:17:59,278 --> 00:18:01,758
I don't want to talk much about this.

284
00:18:02,078 --> 00:18:05,788
What I want to tell you is
about the architecture we design

285
00:18:05,918 --> 00:18:06,968
and why we are changing it.

286
00:18:07,608 --> 00:18:10,838
So in the past few years, we've
implemented a lot of things to

287
00:18:10,838 --> 00:18:12,888
make QuestDB super efficient.

288
00:18:12,898 --> 00:18:16,958
For example, a couple of years ago,
We implemented something called the

289
00:18:17,148 --> 00:18:22,948
Parallel Write Ahead Lock that allows
you, QuestDB with the Postgres protocol

290
00:18:23,008 --> 00:18:28,388
that we support, or the InfluxLine
protocol that we support, or CSV, we

291
00:18:28,388 --> 00:18:33,078
ingest data in parallel and apply changes
in parallel, both in the primary machine

292
00:18:33,118 --> 00:18:38,768
and if you have any replicas, so your
data can be read as quickly as possible.

293
00:18:39,268 --> 00:18:42,558
We also store the data partition by time.

294
00:18:43,008 --> 00:18:46,668
So for each time partition, in this
case, I'm partitioning by month.

295
00:18:46,978 --> 00:18:50,548
So for each month, I will have
a directory, and inside the

296
00:18:50,548 --> 00:18:54,888
directory, I have a binary file
with the contents for each column.

297
00:18:55,418 --> 00:19:02,318
So if the column has like a fixed length,
like an integer or a long or a float,

298
00:19:02,838 --> 00:19:07,918
Then it's going to be just one file if
the data type is variable size, like

299
00:19:07,958 --> 00:19:13,998
a string or a bar chart, we use two
different files, one for the data and

300
00:19:13,998 --> 00:19:18,718
one for the offsets to know where each
column, where each row is starting.

301
00:19:19,138 --> 00:19:25,068
But basically, we use our own format to
store the data in a very efficient way.

302
00:19:25,938 --> 00:19:30,228
if you look at the contents of
the database folder in QuestDB,

303
00:19:30,648 --> 00:19:31,568
this is what you will see.

304
00:19:31,938 --> 00:19:36,798
For each table, you will see several
partitions, one for every type unit.

305
00:19:37,438 --> 00:19:41,498
Inside the partitions, you will
see, multiple binary files.

306
00:19:42,158 --> 00:19:46,438
And we have also metadata, like
the transactions and some temporary

307
00:19:46,438 --> 00:19:47,948
folders, with the Grata head log files.

308
00:19:47,968 --> 00:19:52,498
So that's basically the, the
physical layout we have, in QuestDB.

309
00:19:53,348 --> 00:20:00,608
We also realized last year that having
columnar format is very efficient for

310
00:20:00,648 --> 00:20:05,128
querying data, but it's actually not
that efficient for ingesting data because

311
00:20:05,138 --> 00:20:10,908
in real life, we see most applications
are sending data with whole rows.

312
00:20:11,228 --> 00:20:15,903
So even if you query data by columns,
You send the data in row chunks.

313
00:20:16,423 --> 00:20:21,803
So what we do now is when we're ingesting
data, rather than ingesting by column,

314
00:20:21,913 --> 00:20:27,993
as we were doing in the past, now we
ingest data by rows, and we store data

315
00:20:27,993 --> 00:20:32,793
by columns, but we only have to open
files once, not multiple times like

316
00:20:32,793 --> 00:20:35,563
before, so ingesting is a bit faster.

317
00:20:35,563 --> 00:20:39,638
And I'm telling you all these things,
Because basically I want you to see

318
00:20:39,748 --> 00:20:45,408
that in the past few years, even if we
were using our own format, we're doing

319
00:20:45,678 --> 00:20:52,838
everything possible to make the life
of our users as convenient as possible.

320
00:20:53,238 --> 00:20:55,298
We added also multi primary ingestion.

321
00:20:55,418 --> 00:21:01,038
So now you can have multiple machines
in which you write data and we

322
00:21:01,038 --> 00:21:02,308
make sure there are no conflicts.

323
00:21:02,858 --> 00:21:06,528
So you can get higher throughput, or
you can even have a machine in one

324
00:21:06,528 --> 00:21:11,028
region ingesting data and a machine
in another region, and they both

325
00:21:11,028 --> 00:21:12,848
replicate data across the cluster.

326
00:21:13,088 --> 00:21:17,628
So we did every kind of thing to
make the life of our users better.

327
00:21:18,603 --> 00:21:24,103
And we got to the point in which we
saw adoption growing, many users using

328
00:21:24,143 --> 00:21:26,933
QuestDB, and we thought, Oh, this is cool.

329
00:21:27,253 --> 00:21:32,203
Now we are at a point in which we
already implemented everything we

330
00:21:32,203 --> 00:21:36,833
wanted in the engine, and we can
start doing incremental changes.

331
00:21:36,833 --> 00:21:43,723
But then we realize that users
were asking once and again for some

332
00:21:43,733 --> 00:21:47,713
things That we didn't have, and we
realized that the design we have for

333
00:21:47,713 --> 00:21:50,963
the database was getting obsolete.

334
00:21:51,753 --> 00:21:52,603
And why was that?

335
00:21:53,063 --> 00:21:56,203
if you've been paying attention,
in the past couple of years,

336
00:21:56,743 --> 00:22:01,673
there's been a lot of talk about
new file formats for big data.

337
00:22:02,413 --> 00:22:06,353
Apache Hudi, or Apache
IceBear, or Delta Lake.

338
00:22:06,673 --> 00:22:11,103
They have been around for a few years,
but in the past two years, there is

339
00:22:11,103 --> 00:22:12,693
a lot of buzz about those things.

340
00:22:13,363 --> 00:22:18,573
So basically these formats are
formats for big data that allow you

341
00:22:18,633 --> 00:22:24,913
to have data stored in parquet files
and add table behavior to parquet.

342
00:22:25,433 --> 00:22:31,453
They basically allow you to have updates,
deletes, increments, incremental changes.

343
00:22:31,973 --> 00:22:34,443
In your tables on top of a get files.

344
00:22:34,903 --> 00:22:39,803
So the constraints I told you before
on analytical databases about not being

345
00:22:39,803 --> 00:22:45,463
able to update data being immutable
that being removed with these formats,

346
00:22:45,533 --> 00:22:51,373
but more importantly, these formats
are open on as more and more tools.

347
00:22:51,883 --> 00:22:56,493
Are adopting these formats, now,
you can consume data from multiple

348
00:22:56,493 --> 00:22:59,213
applications without any duplication.

349
00:22:59,713 --> 00:23:04,263
We also saw that, more and
more, users want to do machine

350
00:23:04,263 --> 00:23:05,443
learning on top of the data.

351
00:23:06,093 --> 00:23:08,983
So users use QuestDB
for real time analytics.

352
00:23:09,633 --> 00:23:15,483
They use QuestDV for seeing trends
across the historical data, but they

353
00:23:15,483 --> 00:23:18,043
also want to do predictive analytics.

354
00:23:18,063 --> 00:23:23,993
They also want to use, that science
or machine learning to, learn from the

355
00:23:23,993 --> 00:23:25,753
data they have already in the database.

356
00:23:26,363 --> 00:23:31,733
And since we have a data, a data
format, which was not, open, it was

357
00:23:31,743 --> 00:23:36,183
difficult for them to train the data
because they will have to export

358
00:23:36,193 --> 00:23:38,333
data into CSV, as I told you before.

359
00:23:38,703 --> 00:23:44,293
Or read data with tools like Pandas,
row after row, before they could,

360
00:23:44,433 --> 00:23:46,973
they could easily train that data.

361
00:23:47,553 --> 00:23:50,863
So when they're doing machine
learning, there are two use cases.

362
00:23:51,413 --> 00:23:55,803
In one case, users that want
to export the whole data set.

363
00:23:56,273 --> 00:23:58,483
So they can train their models elsewhere.

364
00:23:58,843 --> 00:24:01,348
And in that case, what they
would prefer is a parquet file.

365
00:24:01,598 --> 00:24:06,618
Or a parquet, a directory with several
parquet files that they can just point

366
00:24:06,648 --> 00:24:08,198
their models and train from there.

367
00:24:08,698 --> 00:24:11,238
We have another type of user
on machine learning, which are

368
00:24:11,248 --> 00:24:13,528
users that want to run queries.

369
00:24:13,638 --> 00:24:17,708
maybe they want to do, aggregations
or downsampling directly on the

370
00:24:17,708 --> 00:24:24,068
database to, to then present the
data with tools like Python or R

371
00:24:24,068 --> 00:24:25,368
or whichever tools you are using.

372
00:24:26,118 --> 00:24:30,888
But they want to run the queries
using the database, but they want

373
00:24:30,888 --> 00:24:34,798
the data to get to the client
application as fast as possible.

374
00:24:35,218 --> 00:24:37,738
As of today, we are using
the POSGES protocol.

375
00:24:38,238 --> 00:24:41,898
When you use the Postgres protocol,
or any other traditional protocol,

376
00:24:41,898 --> 00:24:46,648
JDBC, whatever you are using in your
database, you are sending row after

377
00:24:46,648 --> 00:24:48,898
row of data to the client application.

378
00:24:49,378 --> 00:24:53,918
The client application needs to
deserialize that data into objects, and

379
00:24:53,918 --> 00:24:56,008
then they need to use the application.

380
00:24:56,548 --> 00:24:59,018
If you are doing that with a
few thousand rows, you are fine.

381
00:24:59,658 --> 00:25:03,078
If you are doing that with a few
million rows, that's way too slow

382
00:25:03,438 --> 00:25:05,518
and uses way too many memory.

383
00:25:05,888 --> 00:25:08,848
That's basically, that's
the trolling we were seeing.

384
00:25:09,398 --> 00:25:13,558
in order to, to solve this,
we wanted to do something new.

385
00:25:14,058 --> 00:25:17,868
And we are going to be adopting what
we call the type three database,

386
00:25:17,918 --> 00:25:21,198
which is something we've seen
also in other databases, not only

387
00:25:21,258 --> 00:25:23,298
QuestDB, it's a trend we are seeing.

388
00:25:23,678 --> 00:25:28,418
As of today, we don't know of any
other database implementing fully the

389
00:25:28,458 --> 00:25:33,178
type three database, but we see many
databases taking these ideas, these

390
00:25:33,218 --> 00:25:35,408
concepts, and implementing parts of that.

391
00:25:35,998 --> 00:25:40,218
So the first component of a type
three database is distributed

392
00:25:40,268 --> 00:25:42,848
computing, which basically you have.

393
00:25:42,868 --> 00:25:47,288
You have a storage separate from
computation, probably the storage

394
00:25:47,498 --> 00:25:54,808
might be even object store like,
Amazon S3 or, Azure Blob Storage.

395
00:25:55,388 --> 00:25:59,418
And you have a computation
separated from that.

396
00:25:59,678 --> 00:26:02,968
So your queries can execute on
several machines and return faster.

397
00:26:03,878 --> 00:26:08,498
The second part, as I told you already,
It's storing the data in open formats.

398
00:26:09,198 --> 00:26:13,858
If your data is stored in an open format
that many tools can use, whenever you

399
00:26:13,868 --> 00:26:18,988
want to reuse that data for anything,
You can skip completely the database.

400
00:26:19,338 --> 00:26:21,648
You can just go to the storage.

401
00:26:22,108 --> 00:26:23,998
Those formats also are compressed.

402
00:26:24,378 --> 00:26:27,838
So you're going to be, you're going
to be saving a lot of money and you're

403
00:26:27,838 --> 00:26:29,748
going to be saving a lot of time.

404
00:26:30,248 --> 00:26:33,478
It also, it's also interesting
to use these formats and

405
00:26:33,558 --> 00:26:35,988
support semi structured data.

406
00:26:36,628 --> 00:26:42,178
So in most cases, your data in your
table is going to have some structure,

407
00:26:42,298 --> 00:26:44,408
the timestamp and some columns.

408
00:26:44,918 --> 00:26:46,218
But in many use cases.

409
00:26:47,088 --> 00:26:50,448
You might have some optional columns.

410
00:26:50,858 --> 00:26:56,668
You might have some devices of different
types that sometimes they have one

411
00:26:56,668 --> 00:26:59,368
architecture, but sometimes they have
a different one, a different schema.

412
00:26:59,978 --> 00:27:03,148
And for that, it's important to
support semi structured data,

413
00:27:03,398 --> 00:27:08,638
like JSON or in Parquet, you can
support this semi structured data.

414
00:27:08,658 --> 00:27:12,873
So it's important that your
database can use those structured

415
00:27:13,253 --> 00:27:14,593
and semi structured data.

416
00:27:14,653 --> 00:27:18,883
Otherwise, you're not going to
be able to model efficiently some

417
00:27:18,883 --> 00:27:20,283
of the data sets that you want.

418
00:27:20,733 --> 00:27:25,383
And the last part, for a type 3
database, the data ingress should

419
00:27:25,393 --> 00:27:27,303
be as fast as the data ingress.

420
00:27:27,873 --> 00:27:32,663
What we mean by this is that
traditionally, fast databases have

421
00:27:32,663 --> 00:27:37,663
been, focusing on ingesting individual
rows very fast, And outputting

422
00:27:37,863 --> 00:27:39,653
aggregated results very fast.

423
00:27:40,183 --> 00:27:45,253
But they were not designed to output
individual rows also very fast.

424
00:27:45,803 --> 00:27:47,273
And that's something that needs to change.

425
00:27:47,723 --> 00:27:54,003
On the type 3 database, getting individual
rows out of the database should be as fast

426
00:27:54,333 --> 00:27:57,003
as getting rows inside of the database.

427
00:27:57,223 --> 00:28:00,703
And for that, you can
use new data formats.

428
00:28:01,083 --> 00:28:04,463
I told you already about Iceberg
and Parquet for storing data.

429
00:28:05,038 --> 00:28:06,898
And I'm going to tell
you about Apache Arrow.

430
00:28:07,718 --> 00:28:10,448
You probably have heard already
of Apache Arrow because it's been

431
00:28:10,458 --> 00:28:12,888
quite trendy in the past few months.

432
00:28:13,288 --> 00:28:19,388
But if you haven't heard of Apache Arrow,
it's a in memory format that allows you

433
00:28:20,138 --> 00:28:26,098
to share data across multiple applications
without deserializing the data.

434
00:28:26,818 --> 00:28:33,208
So basically, with Apache Arrow,
my database When you are asking me

435
00:28:33,608 --> 00:28:38,468
to send you data, I can create the
data directly on the arrow format.

436
00:28:39,438 --> 00:28:42,118
The arrow format is not a row format.

437
00:28:42,518 --> 00:28:44,258
It's a columnar format.

438
00:28:44,658 --> 00:28:48,178
So I send you data down the
line already in columnar format.

439
00:28:49,108 --> 00:28:54,308
And I send you the data in this
OpenMemory format, which is compatible

440
00:28:54,788 --> 00:28:57,078
with many libraries already.

441
00:28:57,568 --> 00:29:02,818
So tools like Apache Spark, or
Pandas, or Dusk, or virtually

442
00:29:02,878 --> 00:29:06,218
any programming language, they
already have Arrow libraries.

443
00:29:06,728 --> 00:29:11,788
So when I send you the data in Arrow
format, You can directly use the data

444
00:29:12,098 --> 00:29:16,828
from your programming language without
having to deserialize data in memory.

445
00:29:17,348 --> 00:29:22,988
So you are saving time from serializing,
deserializing, and you are also not

446
00:29:23,588 --> 00:29:25,618
having to duplicate data in memory.

447
00:29:26,578 --> 00:29:27,988
So it gets very fast.

448
00:29:28,488 --> 00:29:32,798
Get the data and being able to
use it, especially if you are

449
00:29:32,838 --> 00:29:35,128
working with a lot of data.

450
00:29:35,518 --> 00:29:40,358
And the cool thing about arrow is that
it gives you not only the memory format,

451
00:29:40,758 --> 00:29:44,818
but also a DBC, which is like JDBC.

452
00:29:45,208 --> 00:29:51,708
or like ODBC is a protocol to
work with SQL data, but in which

453
00:29:51,718 --> 00:29:54,528
the wire format is arrow itself.

454
00:29:55,178 --> 00:30:02,148
So basically, if you have a library
that can speak ADBC, you can connect to

455
00:30:02,148 --> 00:30:07,228
MyDatabase, you can connect to QuestDb,
you can connect using the ADBC protocol.

456
00:30:07,908 --> 00:30:11,228
And when you query the data, you
are going to be getting the rows

457
00:30:11,308 --> 00:30:13,588
directly in the columnar arrow format.

458
00:30:14,528 --> 00:30:19,278
So the client application doesn't need
to do any conversions and you have

459
00:30:19,698 --> 00:30:24,958
zero copy memory operations, making
the whole process very efficient.

460
00:30:25,388 --> 00:30:30,798
What this means is that now you can
stream data out of your database as fast

461
00:30:31,218 --> 00:30:33,968
as you can stream data in your database.

462
00:30:34,628 --> 00:30:37,828
So in QuestDB, we're already
adopting this architecture.

463
00:30:38,498 --> 00:30:43,203
we are closing the gap between a time
series database analytical database,

464
00:30:43,633 --> 00:30:47,873
because now, since we are going to be
producing, we are already producing

465
00:30:47,873 --> 00:30:55,483
data on parquet format, we can directly
ingest data parquet, and we can also

466
00:30:56,063 --> 00:31:02,273
read data generated elsewhere, in parquet
and query data using the QuestDB engine.

467
00:31:02,673 --> 00:31:04,973
So we are becoming analytical database.

468
00:31:05,658 --> 00:31:07,648
And a time series database all in one.

469
00:31:08,228 --> 00:31:12,318
Of course, the query engine
is decoupled for storage.

470
00:31:12,718 --> 00:31:16,918
As I told you, we are supporting,
this is still in beta, but this

471
00:31:16,918 --> 00:31:18,948
is going to be early next year.

472
00:31:19,008 --> 00:31:21,358
It's going to be available for everyone.

473
00:31:21,788 --> 00:31:24,408
So we support already ADBC.

474
00:31:25,008 --> 00:31:28,348
So you can have zero copy operations.

475
00:31:28,398 --> 00:31:33,993
If you prefer to use the process protocol,
JDBC, as you are doing today, You can

476
00:31:34,003 --> 00:31:38,763
still do it, but if your client library
use ADBC, like pandas, for example,

477
00:31:38,763 --> 00:31:43,243
or polars, you can just use ADBC to
query data and everything will be fine.

478
00:31:43,588 --> 00:31:45,268
Much more, much more efficient.

479
00:31:45,948 --> 00:31:49,648
When we generate the data is stored
directly in compressed parquet,

480
00:31:49,708 --> 00:31:51,318
which is iceberg compatible.

481
00:31:51,718 --> 00:31:56,958
So you can reuse that data if you
want on any other system without

482
00:31:56,968 --> 00:31:59,058
having to pass through QuestDb.

483
00:31:59,138 --> 00:32:02,508
So we are not gatekeeping
your data anymore.

484
00:32:02,628 --> 00:32:05,518
And as I told you already,
if you have parquet that has

485
00:32:05,518 --> 00:32:09,473
been generated externally, you
can point QuestDb to QuestDb.

486
00:32:09,713 --> 00:32:13,743
To those packet files, and you'll be
able to create data directly from there.

487
00:32:14,243 --> 00:32:19,133
A specific thing we are doing in QuestDB,
which is a bit unique, is that, as I

488
00:32:19,173 --> 00:32:24,333
told you earlier, we have our own binary
format in order to be very efficient.

489
00:32:24,883 --> 00:32:29,848
So what we are doing these days
is When you are storing data, the

490
00:32:29,848 --> 00:32:34,448
latest partition of the data, it's
stored still in our binary format.

491
00:32:35,408 --> 00:32:38,878
The older partitions, if you
are partitioning data by day, so

492
00:32:38,888 --> 00:32:42,848
data from yesterday and before,
it's stored in parquet format.

493
00:32:43,358 --> 00:32:48,063
And even if you have data which is
arriving late, And out of order, if

494
00:32:48,063 --> 00:32:52,143
you have an update on the data, we
are updating the parquet files, okay?

495
00:32:52,873 --> 00:32:57,633
But the recent partition, if you have
a partition per hour, the last hour of

496
00:32:57,653 --> 00:33:00,513
data, it's still stored on binary format.

497
00:33:00,593 --> 00:33:00,993
Why?

498
00:33:01,493 --> 00:33:06,173
Because, as I said before, our binary
format is designed to be super efficient.

499
00:33:06,873 --> 00:33:12,343
When you are working with real
time data, most queries are on

500
00:33:12,353 --> 00:33:14,073
the most recent part of the data.

501
00:33:14,573 --> 00:33:16,753
This means the most recent partition.

502
00:33:17,253 --> 00:33:22,103
Since also those queries are the most
critical for query latency, what we

503
00:33:22,103 --> 00:33:27,703
are doing is we store the the most
recent partition for each table,

504
00:33:28,093 --> 00:33:29,803
it's stored in our binary format.

505
00:33:30,463 --> 00:33:33,993
So the time to query the data,
the query latency, the query

506
00:33:33,993 --> 00:33:36,883
freshness, is as fast as possible.

507
00:33:37,123 --> 00:33:40,083
We don't need to read from
parquet for that data.

508
00:33:40,383 --> 00:33:45,213
We store data in the binary format, and
all the other partitions are in parquet.

509
00:33:45,513 --> 00:33:51,243
When you run a select, if you, if your
select is across multiple partitions,

510
00:33:51,573 --> 00:33:56,313
you don't have to worry if your data
lives on the binary format or in parquet.

511
00:33:56,463 --> 00:34:00,783
Even if you have object storage, you
can define, you have, you can define

512
00:34:00,783 --> 00:34:04,623
multi tiered storage in which the
latest partition is in binary format.

513
00:34:05,538 --> 00:34:08,438
The last month of partitions
is in, in parquet.

514
00:34:08,948 --> 00:34:11,278
And then you might have in
parquet in your local disk.

515
00:34:11,648 --> 00:34:16,808
And then you might define that all the
data, which is over one month, all go

516
00:34:16,848 --> 00:34:18,948
to object storage, for example, to S3.

517
00:34:19,098 --> 00:34:24,398
So you can define three tiers of data,
the recent partition, the data which is

518
00:34:24,398 --> 00:34:28,098
in parquet in your local file system,
and the data which is in object storage.

519
00:34:28,358 --> 00:34:32,108
And QuestDB, whenever there is
a query, it will, get the data.

520
00:34:32,108 --> 00:34:36,378
It will fetch the data from whichever
storage And we'll, execute the query,

521
00:34:36,788 --> 00:34:41,088
but the queries that execute in the most
recent partition, they will be faster

522
00:34:41,388 --> 00:34:43,038
because they are using the binary storage.

523
00:34:43,388 --> 00:34:49,358
So we call this the first mile of the
data in which, with this optimization, we

524
00:34:49,358 --> 00:34:55,283
can still be Super fast for query, super
low latencies, but keep compatibility

525
00:34:55,603 --> 00:34:59,893
with the rest of the systems by storing
in Parquet all the other partitions.

526
00:35:00,393 --> 00:35:04,363
So this is in and of itself
the Type 3 architecture.

527
00:35:04,583 --> 00:35:09,113
As I said, in QuestDB, we are adopting
it, but there are other databases that

528
00:35:09,123 --> 00:35:11,563
are also adopting Arrow and Parquet.

529
00:35:11,903 --> 00:35:13,093
So it's a trend.

530
00:35:13,458 --> 00:35:17,008
We see in many other databases,
and this is our implementation.

531
00:35:17,578 --> 00:35:24,488
You can ingest data using, streaming
systems like Red Panda or Kafka or

532
00:35:24,488 --> 00:35:25,918
Confluent, whatever you are using.

533
00:35:26,358 --> 00:35:31,798
You can ingest data using the Client
Libraries or the Influence Line Protocol.

534
00:35:32,498 --> 00:35:34,798
you can also ingest data
if you want from CSV.

535
00:35:35,298 --> 00:35:39,418
Whichever way you are ingesting the
data, the parallel writers are going

536
00:35:39,438 --> 00:35:44,693
to be writing data To the Quest DB file
formats for the latest partition to

537
00:35:44,693 --> 00:35:48,413
park it for all the other partitions
or all the data with Ravi late.

538
00:35:49,393 --> 00:35:54,823
This packet file can be consumed
directly by any tools like Polars

539
00:35:54,823 --> 00:35:58,503
or Pandas or Spark or d db.

540
00:35:58,803 --> 00:36:02,763
Anything you are using, you can
read directly from Parque without

541
00:36:03,033 --> 00:36:05,383
having to, to use the query engine.

542
00:36:05,763 --> 00:36:09,933
But if you want to run any queries,
of course you're seeing the.

543
00:36:10,348 --> 00:36:15,378
SQL clients with, JDBC,
the POSIS protocol, ADBC.

544
00:36:15,748 --> 00:36:20,398
You can efficiently query data,
either with Arrow or with the

545
00:36:20,768 --> 00:36:23,438
Piggywire, protocol or the REST API.

546
00:36:23,808 --> 00:36:26,498
And again, those reads are
going to be in parallel.

547
00:36:26,778 --> 00:36:30,168
We scale computation
independently of storage.

548
00:36:30,483 --> 00:36:33,293
to get your data as fast as possible.

549
00:36:34,053 --> 00:36:43,193
As a last demo, I want to show you how we
are working as of today with Parquet and

550
00:36:43,193 --> 00:36:48,883
how you can use external tools, in this
case, DuckDB, to, read data in Parquet.

551
00:36:49,183 --> 00:36:51,583
That we are producing from QuestDB.

552
00:36:52,203 --> 00:36:55,603
Let me show you my local QuestDB.

553
00:36:56,153 --> 00:36:58,723
So here I have a table.

554
00:36:59,223 --> 00:37:03,703
It has like public data from
water sensors in Chicago.

555
00:37:04,393 --> 00:37:07,243
It has this, the measurement timestamp.

556
00:37:07,903 --> 00:37:11,823
The beach name, water temperature,
turbidity, wave height, period,

557
00:37:11,873 --> 00:37:13,743
battery life, different things.

558
00:37:14,403 --> 00:37:19,363
And, I can just run a query on this
data, and of course I can do things,

559
00:37:19,833 --> 00:37:26,173
as I told you earlier, I can do things
like, give me, for example, the, let's

560
00:37:26,173 --> 00:37:34,748
say, give me, The time stamp and the
average, actually the time stamp and the

561
00:37:34,748 --> 00:37:46,418
its name and the average temperature and
average to b the T, in, let's say in one

562
00:37:46,418 --> 00:37:51,448
month intervals, for example, on this
dataset and that for each beach name

563
00:37:51,508 --> 00:37:55,178
and timestamp, each beach name a month.

564
00:37:55,178 --> 00:37:55,838
In this case, sorry.

565
00:37:56,318 --> 00:37:57,798
It will give me the average temperature.

566
00:37:57,968 --> 00:38:01,718
So as you can see here, different
month and different beach.

567
00:38:01,748 --> 00:38:03,228
And I have a hundred rows like this.

568
00:38:04,028 --> 00:38:08,058
And if I query independently,
I have 120, 000 rows.

569
00:38:08,078 --> 00:38:09,578
This is public data from Chicago.

570
00:38:10,208 --> 00:38:16,128
And if I go to my, to my file
storage, I can see here, actually.

571
00:38:16,188 --> 00:38:16,758
Oh yeah, here.

572
00:38:17,078 --> 00:38:19,208
I'm here on the directory
from my data set.

573
00:38:19,708 --> 00:38:26,088
And I can see here that the data, all the
partitions, are stored as parquet data,

574
00:38:26,588 --> 00:38:32,618
except for the, last one, which is
stored, as I told you earlier, with,

575
00:38:32,728 --> 00:38:34,618
individual, with the binary format.

576
00:38:35,078 --> 00:38:41,738
So for each different column, I have,
different, a different binary file.

577
00:38:42,598 --> 00:38:47,658
For the columns that are
characters or variable size, I

578
00:38:47,658 --> 00:38:49,158
have two, as you can see here.

579
00:38:49,998 --> 00:38:52,768
And this one is a specific
type, we call it symbol.

580
00:38:53,118 --> 00:38:55,808
It has like different
metadata, but that's about it.

581
00:38:56,178 --> 00:39:00,458
All the, all the partitions are
parquet, and the most recent

582
00:39:00,498 --> 00:39:04,838
partition, what we call the update
partition, it's in binary format.

583
00:39:05,668 --> 00:39:10,893
And something I can do, I can just
open DuckDB, which is a fantastic

584
00:39:10,893 --> 00:39:17,263
database for, batch analytics, and
I can run a query just like this.

585
00:39:18,133 --> 00:39:22,583
I can say, I want to read from
parquet, and I'm going to point to

586
00:39:22,633 --> 00:39:25,403
my directory and all the folders.

587
00:39:25,798 --> 00:39:29,258
Inside this directory, all the
partitions, and I'm going to say,

588
00:39:29,458 --> 00:39:32,098
Hey, this is a partition folder.

589
00:39:32,878 --> 00:39:35,378
And it contains a lot of parquet files.

590
00:39:35,958 --> 00:39:39,648
So find me everything which is in this.

591
00:39:39,678 --> 00:39:44,588
if I don't filter anything first,
I should have the whole data set.

592
00:39:45,088 --> 00:39:47,303
Now, a hundred twenty something thousand.

593
00:39:47,843 --> 00:39:51,993
And now I can run queries like
a only for a specific, point in

594
00:39:51,993 --> 00:39:56,973
time, and I have five, 5,478 rows.

595
00:39:57,453 --> 00:40:02,303
If I go again back to my web
console, I can run the state query.

596
00:40:02,803 --> 00:40:06,733
And actually here you can
see the number is larger.

597
00:40:07,273 --> 00:40:14,603
So if I'm query in for the same
date, the number here is over 5,500.

598
00:40:15,103 --> 00:40:19,433
And in DuckDB, it was
actually 5, 400 something.

599
00:40:19,573 --> 00:40:20,383
Why is this?

600
00:40:20,763 --> 00:40:25,603
as I told you earlier, the latest
partition, the active partition,

601
00:40:25,673 --> 00:40:27,773
is not stored in parquet.

602
00:40:28,413 --> 00:40:31,863
You can see here the partition
corresponding to September 2023,

603
00:40:31,883 --> 00:40:33,083
which is the latest in this dataset.

604
00:40:33,888 --> 00:40:37,738
It's stored in the binary format
because it's the most recent partition.

605
00:40:37,988 --> 00:40:40,358
So DAGDB don't have access to this.

606
00:40:40,408 --> 00:40:43,608
If I wanted, I could export
this in parquet from QuestDB.

607
00:40:43,918 --> 00:40:47,998
But by default, the latest
partition is not available.

608
00:40:48,468 --> 00:40:51,138
So basically, the
difference is in this one.

609
00:40:51,618 --> 00:40:58,628
If I go back to my web interface, and
actually I query the table partitions,

610
00:40:59,508 --> 00:41:01,528
and I see that the latest one is this one.

611
00:41:02,463 --> 00:41:04,983
It's September 2023, as we saw on disk.

612
00:41:05,413 --> 00:41:11,183
And now I'll run my query between
this date and the latest one.

613
00:41:11,683 --> 00:41:19,473
Now I have 5, 478 rows, which is exactly
the number of rows I have in DAGDB.

614
00:41:19,913 --> 00:41:26,213
as we wanted to show, now you can
use QuestDB to ingest your data.

615
00:41:26,613 --> 00:41:33,643
Run your real time identical queries
and still use, any other, system in this

616
00:41:33,643 --> 00:41:36,843
case that DB could be pandas, could be
polars, could be anything you want to

617
00:41:36,843 --> 00:41:39,943
use to query the generated parquet files.

618
00:41:40,443 --> 00:41:44,863
And if you want to use QuestDB, I
told you already is open source.

619
00:41:45,283 --> 00:41:49,813
You can use it for absolutely anything
you want with the open source version.

620
00:41:50,668 --> 00:41:57,298
You can have, up to 5 million events per
second on a single instance was not to

621
00:41:57,298 --> 00:42:03,668
like, but if you want to have enterprise,
capabilities, bring your own cloud

622
00:42:03,778 --> 00:42:10,048
or role based access control, single
sign on with after record or intra ID.

623
00:42:10,668 --> 00:42:16,658
enhanced security on all protocols,
cold storage, multi primary ingestion.

624
00:42:16,718 --> 00:42:19,973
If you need any of those things,
and of course, Enterprise support.

625
00:42:20,013 --> 00:42:23,403
We also have a QuestDB
Enterprise offering.

626
00:42:24,033 --> 00:42:27,233
We recommend most users to
start with QuestDB Open Source.

627
00:42:27,663 --> 00:42:28,683
See if you like it.

628
00:42:28,753 --> 00:42:31,543
And if you like it,
contact us for Enterprise.

629
00:42:32,003 --> 00:42:33,243
And that's about it.

630
00:42:33,343 --> 00:42:35,283
I hope this was informative.

631
00:42:35,423 --> 00:42:39,443
I hope you understand now why the
TypeScript database is a good idea.

632
00:42:39,843 --> 00:42:44,193
And if you want to learn more about
QuestDB, I'm leaving here some links

633
00:42:44,213 --> 00:42:46,783
where you can learn a bit more.

634
00:42:47,283 --> 00:42:48,403
Thank you and have a nice day.

