1
00:00:00,500 --> 00:00:01,319
Hello, everyone.

2
00:00:01,330 --> 00:00:02,750
Thank you for joining me today.

3
00:00:02,770 --> 00:00:04,210
My name is Mohit Mittal.

4
00:00:04,620 --> 00:00:07,550
And today I'm going to talk about NLP.

5
00:00:08,050 --> 00:00:12,570
NLP, Natural Language Processing, is
a subfield of artificial intelligence

6
00:00:12,619 --> 00:00:18,089
that enables machines to understand,
interpret, and generate human language.

7
00:00:18,569 --> 00:00:21,830
The need of NLP emerged due
to the complexity of human

8
00:00:21,830 --> 00:00:23,960
language filled with ambiguity.

9
00:00:24,090 --> 00:00:26,910
context, idioms, and variations.

10
00:00:27,280 --> 00:00:32,390
Historically, NLP research dates
back to 1950 when Alan Turning

11
00:00:32,400 --> 00:00:36,660
proposed the Turning Test to
evaluate machine intelligence.

12
00:00:37,200 --> 00:00:42,449
Early NLP efforts involved rule based
systems, but these had limitations due

13
00:00:42,449 --> 00:00:45,110
to the complexity of human language.

14
00:00:45,550 --> 00:00:50,489
Over the past two decades, the rapid
growth in data, computing power, And

15
00:00:50,500 --> 00:00:55,680
machine learning algorithms have propelled
NLP into real world applications.

16
00:00:56,019 --> 00:01:01,449
Today NLP is used to chat, is used
in chatbots, voice assistants,

17
00:01:01,900 --> 00:01:07,789
search engines, translation services,
sentiment analysis, shaping how

18
00:01:07,789 --> 00:01:09,779
humans interact with technology.

19
00:01:10,115 --> 00:01:15,054
This session will deep dive into the key
techniques and advancements that have

20
00:01:15,054 --> 00:01:17,965
transformed NLP into what it is today.

21
00:01:18,465 --> 00:01:20,355
Growing impact of NLP.

22
00:01:20,855 --> 00:01:24,185
The NLP market has
experienced remarkable growth.

23
00:01:24,275 --> 00:01:30,995
In 2018, the market was valued
at around eight billions, and by

24
00:01:30,995 --> 00:01:34,205
2022 it has surge to 26 billion.

25
00:01:34,685 --> 00:01:39,905
The projected CHER of
21.4% from 2023 to 20.

26
00:01:40,115 --> 00:01:45,575
30 signifies that NLP is one of
the fastest growing AI segments.

27
00:01:46,035 --> 00:01:52,444
To put this into perspective, the market
size in 2018 was 8 billion, journal

28
00:01:52,444 --> 00:01:55,955
8 billion, in 19 it grew up to 11.

29
00:01:55,955 --> 00:01:56,535
6 billion.

30
00:01:57,160 --> 00:02:02,845
2020. It was 16.2 billion, then
2021, it grew up to 20 point 21.3

31
00:02:02,845 --> 00:02:06,880
billion and 2022, it's 26.4 billion.

32
00:02:07,380 --> 00:02:13,080
Healthcare has been a key driver in NLP
adoption, contributing 15.2 of the 2%

33
00:02:13,080 --> 00:02:15,780
of the total market leveraging NLP for.

34
00:02:16,554 --> 00:02:22,275
medical records, analysis, predictive
analysis, and clinical decision support.

35
00:02:23,045 --> 00:02:28,545
Cloud based NLP deployments now
account for 60 percent of the industry,

36
00:02:28,554 --> 00:02:34,014
highlighting a shift towards more
scalable and cost effective solutions.

37
00:02:34,554 --> 00:02:39,545
The increasing integration of NLP
across finance, retail, and legal

38
00:02:39,545 --> 00:02:44,924
sectors further drives demand,
making it a critical AI technology.

39
00:02:45,424 --> 00:02:49,594
Moving on to next slide where we
are going to talk about advancements

40
00:02:49,614 --> 00:02:51,804
in large language models.

41
00:02:51,975 --> 00:02:53,674
What is LLM?

42
00:02:54,584 --> 00:03:00,754
Large language models or LLM are
a subset of NLP models trained on

43
00:03:00,764 --> 00:03:05,504
massive data sets to understand
and generate human like text.

44
00:03:06,094 --> 00:03:10,954
The evolution of LLMs can be tracked
back to the earlier models where

45
00:03:11,420 --> 00:03:17,629
Word2VC 2013 which introduced
vector based word representations.

46
00:03:17,719 --> 00:03:26,649
Later, transformer based architectures
like BERT and GPT 3 revolutionized NLP by

47
00:03:26,679 --> 00:03:29,309
enabling deep conceptual understanding.

48
00:03:30,154 --> 00:03:33,004
GPT-4 launched in 2023.

49
00:03:33,094 --> 00:03:37,684
Further improved contextual
reasoning, creativity, and logical

50
00:03:37,684 --> 00:03:39,994
consistency in generated text.

51
00:03:40,864 --> 00:03:44,614
LMS have been tested
across various domains.

52
00:03:45,164 --> 00:03:50,504
If you talk about medical diagnosis,
researching human level accuracy in

53
00:03:50,504 --> 00:03:54,104
medical text analysis is around 90 to 95%.

54
00:03:54,604 --> 00:03:56,015
Legal document review.

55
00:03:56,905 --> 00:04:01,805
where we are automating
contract analysis with up to 92.

56
00:04:02,304 --> 00:04:07,834
percent of accuracy, conversational AI,
enabling chatbots, virtual assistants

57
00:04:07,834 --> 00:04:09,804
to provide human like responses.

58
00:04:10,574 --> 00:04:15,644
With the rise of zero shot and few
shot learning, LLMs can now perform

59
00:04:15,684 --> 00:04:20,475
new tasks without extensive retraining,
making them highly adaptable.

60
00:04:20,975 --> 00:04:26,465
Moving on to next slide where we are going
to talk about core NLP components is a

61
00:04:26,525 --> 00:04:34,495
it's a four four step ladder you can say
where the top one is text tokenization.

62
00:04:35,084 --> 00:04:38,744
Tokenization is a process of
breaking text into smaller units.

63
00:04:39,535 --> 00:04:42,875
which can be words, sub
words, or characters.

64
00:04:43,545 --> 00:04:48,185
This step is essential for language
models to understand and process it.

65
00:04:48,945 --> 00:04:51,035
There are different types of tokenization.

66
00:04:51,805 --> 00:04:55,565
Word based tokenization which
splits text at word boundaries.

67
00:04:56,015 --> 00:04:59,095
Subword tokenizations like, BPE.

68
00:04:59,595 --> 00:05:05,755
Wordpiece breaks words into smaller
meaningful units to handle unknown words.

69
00:05:06,745 --> 00:05:10,725
The another one is character
based tokenization, which splits

70
00:05:10,765 --> 00:05:15,595
text into individual characters
useful for language like Chinese.

71
00:05:16,095 --> 00:05:21,225
The next letter is part of
speech tagging, POS tagging.

72
00:05:21,725 --> 00:05:28,885
POS tagging assigns grammatical label to
words such as noun, verbs, adjectives.

73
00:05:29,695 --> 00:05:34,755
This helps NLP models understand
sentence structure and meaning.

74
00:05:35,395 --> 00:05:39,935
For example, run can be
a noun, a morning run.

75
00:05:40,435 --> 00:05:46,185
or a verb I run every day and POS
tagging helps in differentiating them.

76
00:05:46,685 --> 00:05:50,354
The next is named entity recognition.

77
00:05:50,465 --> 00:05:51,595
We also call it NER.

78
00:05:51,595 --> 00:05:59,494
NER identifies real world entities such as
names, locations, organization, and dates.

79
00:06:00,064 --> 00:06:04,364
Example, in Apple, INC is
headquartered in California.

80
00:06:04,864 --> 00:06:09,574
NER identifies Apple Inc as a
company and California as a location.

81
00:06:10,574 --> 00:06:12,244
That's how NER helps.

82
00:06:12,904 --> 00:06:14,524
The last one is pre processing.

83
00:06:15,254 --> 00:06:18,804
Pre processing involves
cleaning and normalizing text

84
00:06:18,814 --> 00:06:20,434
to improve model accuracy.

85
00:06:20,904 --> 00:06:25,234
The common steps include removing
stop words, lower casing text,

86
00:06:25,284 --> 00:06:27,344
and handling misspellings.

87
00:06:27,844 --> 00:06:32,524
Proper pre processing can enhance
model performance by up to 30%.

88
00:06:33,024 --> 00:06:35,544
Now, what is tokenization in NLP?

89
00:06:36,084 --> 00:06:42,194
Tokenization is the fundamental step
in NLP that converts unstructured text

90
00:06:42,214 --> 00:06:45,024
into structured data for processing.

91
00:06:45,804 --> 00:06:51,164
The choice of tokenization technique
impacts model accuracy and efficiency.

92
00:06:51,804 --> 00:06:57,484
Traditional word based tokenization
struggles with compound words and

93
00:06:57,484 --> 00:06:59,194
different language structures.

94
00:06:59,215 --> 00:07:05,394
Subword tokenization methods
like Byte Pair Encoding, or BPE,

95
00:07:05,634 --> 00:07:10,224
have reduced vocabulary size and
improved pre verb recognition.

96
00:07:11,114 --> 00:07:15,244
Effective tokenization enables models
to handle languages with complex

97
00:07:15,244 --> 00:07:17,194
morphology and new words efficient.

98
00:07:17,694 --> 00:07:23,064
Moving on to next slide where we are going
to talk about what is sentiment in NLP.

99
00:07:23,734 --> 00:07:28,714
Sentiment in NLP refers to the
emotional tone expressed in a text.

100
00:07:29,174 --> 00:07:34,344
It can be classified into categories
like positive, negative, neutral.

101
00:07:34,345 --> 00:07:38,654
Early sentiment analysis
use predefined word.

102
00:07:38,879 --> 00:07:43,119
List that struggled with
a sarcasm on Context.

103
00:07:43,609 --> 00:07:47,369
Machine learning approaches
improved sentiment classification

104
00:07:47,369 --> 00:07:50,099
by learning and label data sets.

105
00:07:50,629 --> 00:07:55,129
Advanced model like BERT now
understand contextual nuances and

106
00:07:55,149 --> 00:08:01,039
perform sentiment analysis by over
90% Advanced Sentiment Analysis.

107
00:08:01,709 --> 00:08:06,689
Multitask learning enhances accuracy
by understanding overlapping emotions.

108
00:08:07,479 --> 00:08:12,429
Multilingual sentiment analysis
ensures consistent sentiment

109
00:08:12,479 --> 00:08:14,399
detection across languages.

110
00:08:15,309 --> 00:08:19,959
Hybrid approaches combine rule based
and deep learning models for better

111
00:08:19,959 --> 00:08:24,859
sarcastic Moving on to next slide,
language generation breakthroughs.

112
00:08:25,149 --> 00:08:28,799
The introduction of transformer
based models has revolutionized

113
00:08:29,179 --> 00:08:30,409
language generation.

114
00:08:30,909 --> 00:08:36,119
Attention mechanisms help models
maintain context over long passages,

115
00:08:36,119 --> 00:08:38,740
improving coherence in generated text.

116
00:08:39,619 --> 00:08:43,540
Training techniques like teacher
forcing have accelerated model

117
00:08:43,540 --> 00:08:49,749
learning by 40%, which nucleus
sampling has reduced repetitive.

118
00:08:50,019 --> 00:08:54,899
Text generation by 60 percent real
world application of NLP powered

119
00:08:54,899 --> 00:09:00,509
language generation include automatic
text summarization, where new models

120
00:09:00,549 --> 00:09:03,049
now achieve near human performance.

121
00:09:03,839 --> 00:09:09,549
These advancements are paving the
way of AI generated context in news,

122
00:09:09,609 --> 00:09:11,969
education, conversational AI applications.

123
00:09:12,469 --> 00:09:15,629
Now talking about
implementation considerations.

124
00:09:16,259 --> 00:09:21,525
Deploying NLP at scale requires
addressing several critical Considerations

125
00:09:21,525 --> 00:09:27,854
Computational requirements Optimized
decoding algorithms can reduce inference

126
00:09:27,864 --> 00:09:32,644
latency by three times, making real
time processing more efficient.

127
00:09:33,064 --> 00:09:38,725
Data quality High quality preprocessing
can improve entity recognition

128
00:09:38,944 --> 00:09:41,964
accuracy by up to 12 to 15 percent.

129
00:09:42,464 --> 00:09:49,564
MLOps practices Automated model monitoring
ensures model perform optimally and detect

130
00:09:49,564 --> 00:09:53,064
degration up to 92 percent accuracy.

131
00:09:53,714 --> 00:09:58,294
The ethical considerations, biased
detection frameworks help ensure

132
00:09:58,354 --> 00:10:00,784
NLP systems are fair, unbiased.

133
00:10:01,224 --> 00:10:04,404
These factors are essential for
organization looking to integrate

134
00:10:04,984 --> 00:10:05,974
NLP solutions effectively.

135
00:10:06,954 --> 00:10:09,674
Challenges and future directions for NLP.

136
00:10:10,124 --> 00:10:14,104
Despite advancements, NLP
faces several key challenges.

137
00:10:14,654 --> 00:10:15,854
Factual accuracy.

138
00:10:16,459 --> 00:10:19,829
Like large language models sometimes
generate incorrect information,

139
00:10:19,829 --> 00:10:24,629
but new fact checking mechanism
have reduced errors by 45%.

140
00:10:25,169 --> 00:10:30,329
Domain specialization, custom NLP
models for specific industries like

141
00:10:30,329 --> 00:10:35,649
healthcare, finance are achieving
85 percent accuracy in maintaining.

142
00:10:35,869 --> 00:10:41,349
Precise terminology, ethical AI
development, privacy enhancing

143
00:10:41,349 --> 00:10:47,519
techniques such as federated learning
are reducing data security risks by 60%.

144
00:10:48,369 --> 00:10:52,134
Addressing these challenges will
shape the next NLP of NLP models.

145
00:10:52,634 --> 00:10:53,964
What's the future of NLP?

146
00:10:53,964 --> 00:11:01,284
NLP is rapidly evolving and will continue
to transform industry in coming decade.

147
00:11:01,774 --> 00:11:06,404
The next phase will see improvements
in multilingual capabilities, better

148
00:11:06,444 --> 00:11:11,334
context understanding and more
advanced AI human collaboration.

149
00:11:11,984 --> 00:11:16,394
Responsible AI development, ethical
data governance and scalable

150
00:11:16,394 --> 00:11:21,124
solutions will be the key factors in
ensuring the positive impact of NLP.

151
00:11:21,624 --> 00:11:25,454
Thank you so much for, pulling
your time out, giving me time

152
00:11:25,454 --> 00:11:27,654
to present my views on NLP.

153
00:11:28,084 --> 00:11:31,224
I am looking forward to your
questions and future discussions.

154
00:11:31,254 --> 00:11:32,004
Thank you so much.

