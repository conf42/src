1
00:00:00,500 --> 00:00:01,089
Hi everyone.

2
00:00:01,180 --> 00:00:02,500
My name is Zaki.

3
00:00:02,500 --> 00:00:04,630
I work at V Cluster Labs.

4
00:00:05,170 --> 00:00:07,390
We are the creators of V Cluster.

5
00:00:07,510 --> 00:00:11,680
We like Kubernetes so much that we
put Kubernetes inside of Kubernetes.

6
00:00:12,490 --> 00:00:14,080
Today I want to talk about.

7
00:00:14,455 --> 00:00:20,384
Kai Scheduler or Kubernetes AI scheduler
from Nvidia and how we can use virtual

8
00:00:20,384 --> 00:00:27,105
cluster technology to make scheduling
easier and more multi-tenant friendly.

9
00:00:27,894 --> 00:00:31,195
The setup I'm currently running
will be available in the

10
00:00:31,195 --> 00:00:33,425
description or in the video notes.

11
00:00:33,800 --> 00:00:38,240
You can run it yourself if you have
an Nvidia graphic card and A GPO.

12
00:00:38,840 --> 00:00:40,700
I already created a cluster.

13
00:00:40,780 --> 00:00:42,940
This setup is a little bit involving.

14
00:00:43,300 --> 00:00:47,170
It configures the docker for
GPO pass through installs all

15
00:00:47,170 --> 00:00:49,500
the components on local cluster.

16
00:00:49,500 --> 00:00:53,790
This of course can be run on
any managed Kubernetes cluster

17
00:00:54,060 --> 00:00:55,815
wherever GPU workloads run.

18
00:00:56,315 --> 00:01:00,455
So let's talk a little bit
about the scheduler itself.

19
00:01:00,585 --> 00:01:05,805
In Kubernetes, we schedule
workloads on different nodes

20
00:01:05,995 --> 00:01:07,945
depending on various conditions.

21
00:01:08,215 --> 00:01:11,605
Kubernetes comes with a
pre-configured scheduler that

22
00:01:11,605 --> 00:01:14,485
works out of the box for CPU notes.

23
00:01:14,875 --> 00:01:16,474
This has been a space.

24
00:01:17,114 --> 00:01:18,224
Heavily developed.

25
00:01:18,474 --> 00:01:23,224
You have tools like Carpenter or others
to make the scheduling process easier.

26
00:01:23,524 --> 00:01:29,174
However, in GPU we would like
to schedule workloads that are

27
00:01:29,174 --> 00:01:33,224
running on graphic cards that
require GPU to run on the notes.

28
00:01:33,324 --> 00:01:37,134
The idea is the same, but the
technology is slightly different.

29
00:01:37,654 --> 00:01:43,984
Nvidia open sourced Kubernetes AI
scheduler or Kai Scheduler in 2025, and

30
00:01:43,984 --> 00:01:50,534
now everybody can use this scheduler to
run GPU workloads on Kubernetes cluster.

31
00:01:51,034 --> 00:01:57,335
So first of all, this scheduler enables
fractional GPU allocation, meaning that

32
00:01:57,574 --> 00:02:03,874
having a single GPU on a node can split
the workloads such that each workload

33
00:02:04,264 --> 00:02:07,394
can just have a fraction of GPU usage.

34
00:02:08,274 --> 00:02:13,114
It enables queue based scheduling,
so we can schedule workloads based

35
00:02:13,114 --> 00:02:16,754
on a queue mechanism that Kai
Scheduler comes pre-configured with.

36
00:02:17,639 --> 00:02:23,189
It has various advanced features like note
topology awareness and other elements.

37
00:02:23,689 --> 00:02:28,049
So first of all, this is a live demo
and hopefully everything will work.

38
00:02:28,049 --> 00:02:32,759
So let's verify that we
actually have access to our GPU.

39
00:02:32,819 --> 00:02:38,309
So indeed, I have pretty old NV
graphic card on my computer one 60,

40
00:02:38,729 --> 00:02:43,919
and we just run this simple cube
catalog command to verify that my

41
00:02:44,359 --> 00:02:47,389
computer actually exposes the GPU card.

42
00:02:47,794 --> 00:02:52,444
Into my kind cluster, which worked,
and we can see it from inside

43
00:02:52,474 --> 00:02:54,544
of the Kind Kai Demo cluster.

44
00:02:55,044 --> 00:02:59,904
So with this out of the way, let's
talk a little bit about V Cluster.

45
00:02:59,959 --> 00:03:05,689
V Cluster enables us to create
essentially a fully fledged

46
00:03:05,719 --> 00:03:08,209
Kubernetes cluster inside of a port.

47
00:03:08,839 --> 00:03:13,579
An easy way to think about it is
to imagine that V cluster enables

48
00:03:13,579 --> 00:03:15,889
containerized Kubernetes inside a port.

49
00:03:16,639 --> 00:03:23,089
So we have a team one here that run their
own workloads inside of a cluster pod,

50
00:03:23,479 --> 00:03:26,809
but they share services from host cluster.

51
00:03:27,139 --> 00:03:31,229
So maybe we have third manager,
Istio Ingress or others Istio

52
00:03:31,229 --> 00:03:35,159
Service Measure ingress controller
or others, and the team can use it.

53
00:03:35,369 --> 00:03:40,769
However, we can also have a separate G
cluster that install everything inside

54
00:03:41,039 --> 00:03:42,804
of the virtual cluster namespace.

55
00:03:43,304 --> 00:03:48,574
So there's a quite interesting use case
here where we can have different teams

56
00:03:48,974 --> 00:03:51,374
using virtual clusters in different ways.

57
00:03:52,274 --> 00:03:55,724
So I cluster provides a
full Kubernetes API server.

58
00:03:55,754 --> 00:03:59,324
It's a fully certified Kubernetes
distribution like any other.

59
00:03:59,714 --> 00:04:04,794
We actually run Kubernetes inside of
a pod to regular Kubernetes binaries.

60
00:04:05,134 --> 00:04:08,164
This enables us to have
very flexible isolation in

61
00:04:08,164 --> 00:04:09,904
various multitenant scenarios.

62
00:04:09,904 --> 00:04:14,904
We're gonna talk about Kai scheduler
in this talk, but obviously all kinds

63
00:04:14,904 --> 00:04:18,114
of other interesting multitenant
scenarios are available as well.

64
00:04:18,614 --> 00:04:22,724
It makes the resource utilization
much more efficient, which

65
00:04:22,754 --> 00:04:28,034
contributes to limiting cost and
better managing existing resources.

66
00:04:28,034 --> 00:04:33,474
Instead of creating a full Kubernetes
cluster like EKS or a KS you

67
00:04:33,474 --> 00:04:38,524
actually can use virtual cluster
for various purposes because its

68
00:04:38,524 --> 00:04:40,714
provisions much faster than regular.

69
00:04:40,889 --> 00:04:43,439
Host cluster or Kubernetes
managed cluster.

70
00:04:43,709 --> 00:04:47,840
You can use it in test dev ci,
but also production wherever

71
00:04:47,840 --> 00:04:48,894
performance is important.

72
00:04:49,394 --> 00:04:52,784
So a little bit more about
technical architecture.

73
00:04:53,264 --> 00:04:58,544
V Cluster creates a pod that
encapsulates Kubernetes binary.

74
00:04:58,574 --> 00:05:00,224
So we have Cube, API server.

75
00:05:00,554 --> 00:05:03,884
We have a data store inside of a pod.

76
00:05:03,944 --> 00:05:06,704
This is where we start with
open source, V cluster.

77
00:05:07,064 --> 00:05:08,534
It's of course very easy to.

78
00:05:09,159 --> 00:05:12,969
Connect V Cluster to existing
Ed, CD cluster or other data

79
00:05:12,969 --> 00:05:17,189
stores outside of the pod for
redundancy and high availability.

80
00:05:18,049 --> 00:05:21,709
We have components such as controller,
manager, API server, optionally

81
00:05:21,709 --> 00:05:22,999
we can in install scheduler.

82
00:05:23,239 --> 00:05:25,639
We are going to obviously
do this during this talk.

83
00:05:26,119 --> 00:05:29,059
And the key component
of V cluster is sinker.

84
00:05:29,389 --> 00:05:31,849
Sinker enables essentially.

85
00:05:32,134 --> 00:05:37,114
Dynamic and con, highly configurable
synchronizing of resources between the

86
00:05:37,114 --> 00:05:40,534
pod of virtual cluster and host cluster.

87
00:05:40,894 --> 00:05:46,534
This opens up all kinds of interesting
scenarios, but for now, it's important

88
00:05:46,534 --> 00:05:49,134
to know that this works like that.

89
00:05:49,614 --> 00:05:51,264
So host cluster will provide.

90
00:05:52,124 --> 00:05:56,824
At the beginning, all services think
of them as shirt, low level services.

91
00:05:57,064 --> 00:06:01,024
Again, we can have V cluster running
in different modes and have their

92
00:06:01,024 --> 00:06:05,434
own services, but for the sake of the
stock, we can think of host cluster as

93
00:06:05,434 --> 00:06:08,644
providing a base of the shirt services.

94
00:06:09,144 --> 00:06:10,249
So we have API server.

95
00:06:10,959 --> 00:06:16,599
Which is fully k Kubernetes compliant,
which is about a hundred megabytes.

96
00:06:16,599 --> 00:06:22,339
So the whole virtual cluster port would
be about four, 400 megabytes or so or

97
00:06:22,369 --> 00:06:24,319
even less depending on the installation.

98
00:06:24,819 --> 00:06:29,379
So the key insight here is that
we use containerized control

99
00:06:29,379 --> 00:06:33,579
plane with intelligent resource
sinking back and forth between the

100
00:06:33,579 --> 00:06:35,499
virtual clusters and host clusters.

101
00:06:35,999 --> 00:06:37,559
Little bit more about Syner.

102
00:06:37,649 --> 00:06:42,919
Syner runs in a reconciliation loop,
so it's bidirectionally synchronizes

103
00:06:43,099 --> 00:06:48,919
resources running inside of virtual
cluster, which means they talk to Cube

104
00:06:48,919 --> 00:06:54,229
API running inside of virtual cluster
Pod and talk to QAPI running in the host.

105
00:06:54,424 --> 00:06:59,494
API pod Syner does things like making
sure there are no name collisions,

106
00:06:59,644 --> 00:07:01,264
synchronizing various resources.

107
00:07:01,294 --> 00:07:03,214
All this is very highly configurable.

108
00:07:03,364 --> 00:07:04,924
We'll look at it later a little bit.

109
00:07:05,424 --> 00:07:10,504
Coming back to GPUs obviously
AI is the current hype and.

110
00:07:11,004 --> 00:07:14,904
Most of the resources, most of the
workloads that run on gpu, SRAI.

111
00:07:14,934 --> 00:07:18,954
But not only, so obviously we have
model training, things like fine tuning,

112
00:07:18,954 --> 00:07:21,744
lms, deep training, base training.

113
00:07:21,834 --> 00:07:23,994
This is, can be extremely costly.

114
00:07:24,334 --> 00:07:25,169
And this will run.

115
00:07:25,919 --> 00:07:29,099
For weeks or months on end,
depending on the model.

116
00:07:29,389 --> 00:07:31,909
And this utilizes GPUA hundred percent.

117
00:07:31,909 --> 00:07:37,859
So in this case we would have specialized
equipment that needs to run on GPU.

118
00:07:37,919 --> 00:07:42,809
So that's one use case, stable diffusion
image generation, but also inference.

119
00:07:42,909 --> 00:07:44,799
All of us now interact with ai.

120
00:07:44,799 --> 00:07:48,659
So whether we do this through API
or web interface inference would

121
00:07:48,659 --> 00:07:53,349
also be mixed workloads based
off of GPU and CPU resources.

122
00:07:53,799 --> 00:07:55,449
But it's not only ai.

123
00:07:55,509 --> 00:07:59,559
We have also video processing
and all kinds of GPU heavy.

124
00:08:00,099 --> 00:08:03,189
Processing that still
requires GPUs as well.

125
00:08:03,639 --> 00:08:06,929
Depending on the workload, it
can be also to a hundred percent.

126
00:08:07,409 --> 00:08:10,259
We have Cuda development
Jupyter Notebooks.

127
00:08:10,859 --> 00:08:14,519
All kinds of older workloads
that Kubernetes can help us run.

128
00:08:14,789 --> 00:08:18,029
And finally, scientific
computing with batch processing.

129
00:08:18,329 --> 00:08:21,779
And we can utilize GPU
here very effectively also.

130
00:08:21,779 --> 00:08:25,379
So GPU workloads are not only about ai.

131
00:08:25,429 --> 00:08:29,569
Obviously we are focusing on AI on this
talk, but there's a lot of legitimate

132
00:08:29,569 --> 00:08:34,069
use cases where we would like to run
GPU workloads and use Kubernetes to

133
00:08:34,069 --> 00:08:35,809
help us schedule them and manage.

134
00:08:36,309 --> 00:08:40,959
So just for fun, we are going
to deploy a very simple pod and

135
00:08:40,989 --> 00:08:45,849
we'll see how V Cluster can help
us with synchronizing this pod.

136
00:08:45,849 --> 00:08:51,809
So this pod is just a really silly
demo to show what we can do with GPU.

137
00:08:52,319 --> 00:08:57,259
So you can see now the port has
been deployed and since this is

138
00:08:57,259 --> 00:09:01,439
a recorded talk, you will not
unfortunately have access to this link.

139
00:09:01,559 --> 00:09:05,749
But if you will run this
presentation by yourself, you will

140
00:09:05,749 --> 00:09:07,549
be able to also see this link.

141
00:09:07,549 --> 00:09:11,899
So we have used service called NRO
to essentially create this pod.

142
00:09:12,289 --> 00:09:15,839
And here we can scan the QR code.

143
00:09:16,030 --> 00:09:19,699
I will do this on my phone
and we are gonna have some fun

144
00:09:19,760 --> 00:09:21,500
here with the demo in a second.

145
00:09:22,000 --> 00:09:29,050
So now for the demo, we're just
going to see how GPU scheduled web

146
00:09:29,050 --> 00:09:31,630
application can run AI inference.

147
00:09:32,020 --> 00:09:35,230
So here we have different kus For now we.

148
00:09:35,730 --> 00:09:37,470
Didn't generate any kus.

149
00:09:37,470 --> 00:09:40,920
This is my favorite topic
site reliability, engineering.

150
00:09:41,340 --> 00:09:43,939
I'm generating on my phone haiku now.

151
00:09:44,360 --> 00:09:49,550
And we can see AI is helping
us create this various horror

152
00:09:49,550 --> 00:09:52,459
stories, haiku, like poems.

153
00:09:53,300 --> 00:09:59,479
Obviously this is mix of inference
from ai, but also runs on my kind

154
00:09:59,479 --> 00:10:02,030
cluster utilizing GPU as well.

155
00:10:02,719 --> 00:10:07,550
So it's not that hard to also mix
and match various resources as

156
00:10:07,550 --> 00:10:09,925
well as various types of workloads.

157
00:10:10,425 --> 00:10:11,805
Wrong context.

158
00:10:11,835 --> 00:10:14,175
Pots vanish into devoid 3:00 AM despair.

159
00:10:14,175 --> 00:10:15,195
That's very relatable.

160
00:10:15,885 --> 00:10:16,815
All perfect.

161
00:10:16,815 --> 00:10:22,665
So this was the G Ps. Let me
just swap to this real quick.

162
00:10:23,165 --> 00:10:29,095
So now I. Let's talk a little
bit about how would we upgrade

163
00:10:29,185 --> 00:10:30,775
schedulers in production.

164
00:10:31,555 --> 00:10:36,845
If you have host cluster set up and
you use any scheduler, we are obviously

165
00:10:36,845 --> 00:10:39,395
talking about the Kubernetes AI scheduler.

166
00:10:39,500 --> 00:10:44,650
You will need to upgrade it at
some point and it will it can be a

167
00:10:44,650 --> 00:10:51,190
significant risk when there's a lot of
teams depending on the same scheduler

168
00:10:51,190 --> 00:10:54,790
version, so testing, it must be.

169
00:10:55,320 --> 00:10:57,910
Significantly more disciplined.

170
00:10:58,130 --> 00:11:00,740
We need to have a very good rollback.

171
00:11:00,740 --> 00:11:05,510
Procedures and teams are often
blocked on a single scheduler.

172
00:11:05,510 --> 00:11:10,040
Maybe there is a new version
of, in our case, k, a scheduler

173
00:11:10,040 --> 00:11:11,570
that the teams are waiting for.

174
00:11:12,030 --> 00:11:16,600
And we are going to go
through the upgrade process.

175
00:11:17,100 --> 00:11:19,350
So let's talk a little bit about risk.

176
00:11:19,350 --> 00:11:25,210
So if we have a single scheduler on
a big cost cluster where we share the

177
00:11:25,210 --> 00:11:31,600
same scheduler across multiple teams, if
there's a bug in a new scheduler version,

178
00:11:31,900 --> 00:11:37,720
obviously all the polls that depend on it
will crash or pending being pending state.

179
00:11:38,120 --> 00:11:41,995
And the recover time depends
on, what are we going to do?

180
00:11:41,995 --> 00:11:43,015
What is the problem?

181
00:11:43,445 --> 00:11:46,745
So it might be a few hours, it might
be more we, at this point we might

182
00:11:46,745 --> 00:11:52,985
be able to revert the change and
be okay with it, however because we

183
00:11:52,985 --> 00:11:55,535
are using host cluster deployment.

184
00:11:56,035 --> 00:12:02,075
We use typically CRDs for
managing the installation of the

185
00:12:02,105 --> 00:12:04,955
scheduler and related operators.

186
00:12:05,435 --> 00:12:11,195
If this goes wrong and there's a
namespace corruption or CRD corruption.

187
00:12:11,830 --> 00:12:13,510
This can be very costly.

188
00:12:13,510 --> 00:12:21,370
So I've seen scenarios where CRDs of
operators that using Kubernetes manage

189
00:12:21,400 --> 00:12:26,950
external resources like databases or
infrastructure were incorrectly upgraded.

190
00:12:27,040 --> 00:12:31,490
And this resulted in data
loss or, some older disasters.

191
00:12:31,490 --> 00:12:38,300
So this is a critical risk scenario that
we have to take under consideration.

192
00:12:38,690 --> 00:12:42,215
And of course, the bigger the
cluster is the larger the amount

193
00:12:42,215 --> 00:12:46,475
of teams that we are serving, then
the more critical this becomes.

194
00:12:46,975 --> 00:12:49,975
Version mismatch is
similar to scheduler bugs.

195
00:12:49,975 --> 00:12:56,695
Maybe our pods require a feature that is
in specific version, so we have to test

196
00:12:56,695 --> 00:12:58,895
all of them before creating an upgrade.

197
00:12:59,395 --> 00:13:03,195
And finally maybe a new version has
a resource leak, something that is

198
00:13:03,195 --> 00:13:08,475
difficult to detect instantly, but
after it will be deployed, we will

199
00:13:08,625 --> 00:13:13,935
maybe detect it in a few days or hours,
depending on our observability setup.

200
00:13:14,685 --> 00:13:20,505
So according to New Relic 2024 report,
the enterprise downtime cost can be.

201
00:13:21,005 --> 00:13:25,265
Up to 1 million per hour, obviously,
depending on the size of the company.

202
00:13:25,265 --> 00:13:30,785
So we definitely want to avoid
having upgrades of critical

203
00:13:30,785 --> 00:13:36,995
infrastructure like Kubernetes
schedulers and hopefully be able to

204
00:13:37,025 --> 00:13:39,095
compartmentalize it and isolate it.

205
00:13:39,595 --> 00:13:41,515
So that's where can help.

206
00:13:42,015 --> 00:13:47,145
You can think of virtual cluster
Pod as something that encapsulates

207
00:13:47,355 --> 00:13:49,965
your scheduler inside of it.

208
00:13:50,415 --> 00:13:54,675
So we install special we
set up special setting.

209
00:13:55,125 --> 00:13:56,385
You will see it in a moment.

210
00:13:56,985 --> 00:14:00,465
And what it results with is that
we can start from single visual

211
00:14:00,465 --> 00:14:03,495
cluster, we can schedule some pos.

212
00:14:04,335 --> 00:14:10,355
Just using this very isolated and small
blast radios and perform AB testing,

213
00:14:10,535 --> 00:14:15,045
making sure that the new version of
the case scheduler in our case is

214
00:14:15,045 --> 00:14:20,235
correct and everything works well, which
also limits the cost of the testing

215
00:14:20,235 --> 00:14:23,535
infrastructure significantly because
we can have a very small cluster and

216
00:14:23,925 --> 00:14:28,825
just have several virtual clusters
kind of testing various scenarios

217
00:14:28,825 --> 00:14:30,575
that are important for our business.

218
00:14:31,075 --> 00:14:35,935
So the idea here is that virtual
cluster will create an isolated

219
00:14:36,295 --> 00:14:42,865
Kubernetes cube, API and other machinery
including Kai Scheduler inside of it.

220
00:14:42,865 --> 00:14:46,535
So we don't need to additional
host cluster for testing at all.

221
00:14:46,535 --> 00:14:49,325
We can do this right in production
because it's all isolated.

222
00:14:49,825 --> 00:14:52,645
So one more time,
reiterating on the risks.

223
00:14:52,705 --> 00:14:57,875
We can obviously there are other risks,
like we slow down innovation because

224
00:14:57,875 --> 00:15:00,515
the teams must wait on the upgrades.

225
00:15:00,795 --> 00:15:05,065
And all of all upgrades in Kubernetes
are pretty costly and stressful.

226
00:15:05,565 --> 00:15:07,725
So how can we cluster help?

227
00:15:07,725 --> 00:15:09,915
I'm going to run this
in the background and.

228
00:15:10,785 --> 00:15:16,455
To talk a little bit about how V Cluster
uses the configuration file that we see

229
00:15:16,455 --> 00:15:22,455
here at the Top V cluster essentially
provides us a very highly customizable

230
00:15:22,695 --> 00:15:28,185
way of synchronizing various resources
back and forth, but also creating virtual

231
00:15:28,185 --> 00:15:32,025
clusters in a way that serves our needs.

232
00:15:32,535 --> 00:15:34,275
So this first group of settings.

233
00:15:34,695 --> 00:15:39,945
Make sure that if we want to use a host
cluster scheduler, like in one of the

234
00:15:39,945 --> 00:15:45,535
demos we want to use the set owner flag,
which essentially tells Kubernetes or V

235
00:15:45,535 --> 00:15:50,665
Cluster specifically not to take ownership
of the deployed pulse in the V cluster.

236
00:15:50,935 --> 00:15:54,475
This is how Kubernetes works
by tracking ownership plugs.

237
00:15:54,975 --> 00:15:59,715
In the second demo, we want to make sure
that our control plane, which is the

238
00:15:59,925 --> 00:16:05,445
Kubernetes API and Kubernetes machinery
that we deploy inside of virtual cluster

239
00:16:05,445 --> 00:16:09,345
Pod also has virtual scheduler enabled.

240
00:16:10,000 --> 00:16:14,199
And thanks to this virtual scheduler
enabled, we can use Kai Scheduler.

241
00:16:14,360 --> 00:16:17,479
And you, as you'll see later,
we need some other settings.

242
00:16:17,509 --> 00:16:23,269
We need to be able to synchronize nodes
from host and also runtime classes

243
00:16:23,449 --> 00:16:28,130
so that we can synchronize n video
runtime class as well as node labels.

244
00:16:28,489 --> 00:16:34,520
So we will see how, we can use it just to
target only special nodes that are labeled

245
00:16:34,710 --> 00:16:39,370
with the GPU so that our future cluster
synchronizes those things correctly.

246
00:16:39,870 --> 00:16:46,050
So the benefit is obviously that we
will now have independent Kai scheduler

247
00:16:46,080 --> 00:16:48,240
versions, so you can think of it.

248
00:16:48,540 --> 00:16:51,030
Each team can come with their own.

249
00:16:51,625 --> 00:16:52,675
Scheduler version.

250
00:16:52,885 --> 00:16:57,230
We have an older version here,
seven 11 or nine two or others.

251
00:16:57,590 --> 00:16:59,180
So that's already a huge benefit.

252
00:16:59,840 --> 00:17:03,740
Now we completely isolate scheduling
so we can target different

253
00:17:03,780 --> 00:17:05,520
no pools and different nodes.

254
00:17:06,050 --> 00:17:09,180
And we achieved sch autonomy by teams.

255
00:17:09,180 --> 00:17:10,050
And this of course.

256
00:17:10,440 --> 00:17:15,750
If you add a little bit more advanced
use cases that VLA supports, you can

257
00:17:15,750 --> 00:17:20,690
schedule on all kinds of infrastructure
and you can support you can support

258
00:17:20,690 --> 00:17:24,050
very flexible multi-tenancy scenarios.

259
00:17:24,550 --> 00:17:26,850
Let's see if this works correctly.

260
00:17:26,970 --> 00:17:29,610
So we still have some problems.

261
00:17:29,610 --> 00:17:30,210
Which image?

262
00:17:30,210 --> 00:17:31,830
This is probably will take a moment.

263
00:17:31,880 --> 00:17:32,990
Because of my connection.

264
00:17:33,560 --> 00:17:34,460
Oh, now it's running.

265
00:17:34,850 --> 00:17:40,040
But you can see we are inside of virtual
cluster here and we have actually created.

266
00:17:40,540 --> 00:17:43,570
This new virtual cluster
with all the settings.

267
00:17:43,870 --> 00:17:45,730
So we don't have any demos here.

268
00:17:45,730 --> 00:17:47,260
We don't have any pods running.

269
00:17:47,610 --> 00:17:50,760
But I just wanted to
show you that this works.

270
00:17:50,820 --> 00:17:57,240
So this command Create Kai Isolated,
created a new virtual cluster using

271
00:17:57,240 --> 00:17:58,830
those settings that you can see here.

272
00:17:59,340 --> 00:18:03,900
And I opened Canine s, which is a,
just a terminal Kubernetes viewer

273
00:18:04,380 --> 00:18:07,500
to show you that we are connected
currently to the virtual cluster.

274
00:18:08,000 --> 00:18:12,680
Okay, so now if we refresh
this, you can see that we are

275
00:18:12,680 --> 00:18:14,750
connected to our virtual cluster.

276
00:18:14,750 --> 00:18:18,780
It's a pretty long name that we
cluster builds based off of various

277
00:18:18,810 --> 00:18:23,400
variables, but we are essentially
running inside of my kind cluster.

278
00:18:23,670 --> 00:18:26,850
There is an actual virtual
cluster running there.

279
00:18:27,350 --> 00:18:29,060
So just a quick recap.

280
00:18:29,150 --> 00:18:30,440
We cluster components.

281
00:18:30,530 --> 00:18:33,920
We have an API server, which is
a normal Kubernetes cube, API.

282
00:18:34,370 --> 00:18:38,390
We have a syner, which enables us to
be directional syncing, and it's at the

283
00:18:38,390 --> 00:18:41,540
heart of how virtual cluster work we have.

284
00:18:41,590 --> 00:18:45,405
A. Our complete isolated backend.

285
00:18:45,495 --> 00:18:47,085
In our case, it's SQL Light.

286
00:18:47,145 --> 00:18:48,165
As I mentioned earlier.

287
00:18:48,225 --> 00:18:53,135
You can also use set CD and you can
also use external backend so that we

288
00:18:53,135 --> 00:18:55,535
have high availability and redundancy.

289
00:18:56,105 --> 00:19:00,575
And in our case, we are going to
soon install the Kai Scheduler

290
00:19:00,785 --> 00:19:02,195
for independent scheduling.

291
00:19:03,035 --> 00:19:06,065
So currently our virtual cluster.

292
00:19:06,765 --> 00:19:14,635
Which is this pod uses 370
megabytes and 4.6 media CPUs, and

293
00:19:14,635 --> 00:19:16,075
we have the database right here.

294
00:19:16,255 --> 00:19:17,335
Okay, perfect.

295
00:19:18,025 --> 00:19:24,985
So now we will make sure we are connected
to RV cluster and inside of it we are

296
00:19:24,985 --> 00:19:28,735
going to install the Kai Scheduler.

297
00:19:29,035 --> 00:19:30,085
So let's do this right now.

298
00:19:30,585 --> 00:19:34,065
So you can see we have
connected to virtual cluster.

299
00:19:34,155 --> 00:19:38,205
It gives us friendly warning here
that there's a newer version,

300
00:19:38,685 --> 00:19:41,985
but we are already operating
with the correct version.

301
00:19:42,405 --> 00:19:46,905
So what I am doing now, I am
installing the same Kai scheduler

302
00:19:46,905 --> 00:19:49,095
that we have on our host cluster.

303
00:19:49,510 --> 00:19:52,690
But I'm installing it
inside of virtual cluster.

304
00:19:52,750 --> 00:19:57,220
So we are moving one level of
abstractions up, and I'll show you

305
00:19:57,220 --> 00:19:59,920
in a second how it looks in a pod.

306
00:20:00,400 --> 00:20:06,520
But from our perspective, you can
install inside of the cluster anything.

307
00:20:06,880 --> 00:20:08,455
It's a normal Kubernetes cluster.

308
00:20:08,455 --> 00:20:11,620
You can install a web app
like we've seen earlier.

309
00:20:11,900 --> 00:20:16,490
You can install scheduler like
in our case or other components.

310
00:20:17,245 --> 00:20:20,745
This is done and now we
can see how it works.

311
00:20:20,805 --> 00:20:24,585
So we've connected again to our
virtual cluster here, as you can see

312
00:20:24,585 --> 00:20:29,205
by the context name, and you can see
that we have now several components

313
00:20:29,205 --> 00:20:33,255
from Kai Scheduler, which is Port
Grouper Q Controller, and others

314
00:20:33,345 --> 00:20:35,385
running inside of our review cluster.

315
00:20:35,385 --> 00:20:36,755
Okay, this is great.

316
00:20:37,595 --> 00:20:39,365
So let's do a quick recap.

317
00:20:40,025 --> 00:20:41,765
We now have.

318
00:20:42,265 --> 00:20:45,955
High scheduler inside of virtual
cluster with A GPU sharing.

319
00:20:45,955 --> 00:20:51,835
So we should be able to install some
kind of GPU demo and make sure that

320
00:20:51,835 --> 00:20:58,105
we are actually able to schedule pods
from this demo on our virtual cluster.

321
00:20:58,105 --> 00:20:59,275
So how are we doing it?

322
00:20:59,905 --> 00:21:02,395
There are several things that
we have to configure inside of

323
00:21:02,395 --> 00:21:04,705
our pods, so you can see here.

324
00:21:05,095 --> 00:21:09,805
First of all, we are labeled it with
appropriate queue from Kai Scheduler.

325
00:21:09,805 --> 00:21:11,125
I have created it earlier.

326
00:21:11,200 --> 00:21:15,190
So this is Case Scheduler internals,
but this one is interesting.

327
00:21:15,400 --> 00:21:18,610
Inside of Case Scheduler, we
use fractional GPU scaling.

328
00:21:19,030 --> 00:21:23,910
So in this case, 20% of GPU
can be taken by this pod.

329
00:21:24,000 --> 00:21:29,190
So GPU Demo two and 10% of GPU
can be taken by GPU demo one.

330
00:21:29,850 --> 00:21:33,510
And another important setting
here is schedule name.

331
00:21:33,690 --> 00:21:37,140
We are selecting case scheduler
and runtime class and video.

332
00:21:37,470 --> 00:21:41,700
So those bots don't do anything
interesting due to sleep, but we are able

333
00:21:41,700 --> 00:21:44,130
to schedule them inside of the cluster.

334
00:21:44,280 --> 00:21:45,055
So let's give it a try.

335
00:21:45,555 --> 00:21:50,235
I'm going to apply those things and we are
currently connected to a virtual cluster.

336
00:21:50,715 --> 00:21:54,435
I'm making sure I'm applying the
queues, and I just applied those two

337
00:21:54,435 --> 00:21:59,715
pos that we've seen earlier, and as
you can see, they correctly take the

338
00:21:59,715 --> 00:22:02,865
fraction of the GPU as we configured.

339
00:22:03,405 --> 00:22:05,325
So this was pretty easy.

340
00:22:05,955 --> 00:22:08,415
So now let's imagine that we want to.

341
00:22:08,915 --> 00:22:13,625
Change our Kai Scheduler version or
maybe we did just a simple experiment.

342
00:22:13,655 --> 00:22:17,075
We install all those pos, we
tested it, we've installed

343
00:22:17,125 --> 00:22:19,405
Kai Scheduler and we are done.

344
00:22:19,455 --> 00:22:24,495
So v clusters are very ephemeral
if you want them to be.

345
00:22:24,555 --> 00:22:27,195
It's very easy to create a V cluster pod.

346
00:22:27,285 --> 00:22:32,535
We've seen this is around 300 megabytes
and a very small fraction of CPU.

347
00:22:33,030 --> 00:22:36,250
And you can quickly delete
them if you're done with them.

348
00:22:36,250 --> 00:22:38,230
That's why they're so helpful.

349
00:22:38,230 --> 00:22:43,240
When you do AB testing or when you
upgrade various components, you can

350
00:22:43,240 --> 00:22:45,640
use virtual clusters to test behavior.

351
00:22:45,640 --> 00:22:50,680
And here we can see in 39 seconds, we
were able to completely remove virtual

352
00:22:50,680 --> 00:22:52,970
cluster as we are done with our testing.

353
00:22:53,470 --> 00:22:56,670
So let's demo one more thing.

354
00:22:56,700 --> 00:23:00,620
So imagine you have, you're supporting
maybe machine learning team that

355
00:23:00,620 --> 00:23:03,560
needs a specific new Kai version.

356
00:23:03,990 --> 00:23:06,600
But the research team is
still stuck on the old one.

357
00:23:06,600 --> 00:23:10,830
So you definitely don't want to upgrade
the host cluster scheduler version.

358
00:23:11,190 --> 00:23:14,980
And also dev team uses the
default host cluster scheduler.

359
00:23:14,980 --> 00:23:19,210
They don't really care about any version
here, and we are able to support.

360
00:23:19,420 --> 00:23:21,850
All three use cases simultaneously.

361
00:23:22,630 --> 00:23:26,290
So first of all, let's create
two new virtual clusters.

362
00:23:26,320 --> 00:23:30,700
We are creating one for team
stable and one for team beta.

363
00:23:31,360 --> 00:23:35,290
Both of those virtual clusters are
going to be separate pods, which you

364
00:23:35,290 --> 00:23:39,905
know by now, and we just have the same
configuration, just with different names.

365
00:23:40,855 --> 00:23:41,145
Okay.

366
00:23:41,645 --> 00:23:46,325
So now if this is done, we can actually
connect to each of those clusters

367
00:23:46,835 --> 00:23:51,815
and we can install Kai Scheduler
version different per each cluster.

368
00:23:51,935 --> 00:23:56,915
So in our case, team stable
have 0.7 version, and Team

369
00:23:56,915 --> 00:24:01,475
Beta has 0.93 and we are now.

370
00:24:02,225 --> 00:24:03,945
Doing like an AB testing.

371
00:24:04,005 --> 00:24:08,715
We are creating two separate scheduler
versions and we are, once this is done, we

372
00:24:08,715 --> 00:24:13,005
are going to disconnect from the virtual
clusters and we are going to explore

373
00:24:13,185 --> 00:24:15,465
how it looks on the host cluster itself.

374
00:24:15,965 --> 00:24:21,945
So while this is still running it'll take
a moment to install and we will soon be

375
00:24:21,945 --> 00:24:26,935
able to, test the virtual cluster test the
host cluster and see what happened here.

376
00:24:27,595 --> 00:24:33,505
So this is still going, and we can
now connect to each virtual cluster

377
00:24:33,655 --> 00:24:38,995
that we deployed and we can create the
same demos that we've seen earlier.

378
00:24:39,025 --> 00:24:41,125
So pod one and PO two.

379
00:24:41,455 --> 00:24:44,635
We can of course, imagine
that those are different pods.

380
00:24:44,725 --> 00:24:47,035
Maybe they have different allocation here.

381
00:24:47,245 --> 00:24:49,915
Maybe we are using completely
different allocation strategy

382
00:24:49,915 --> 00:24:52,105
for other V clusters and so on.

383
00:24:52,135 --> 00:24:54,025
Other virtual clusters
and so on and so forth.

384
00:24:54,685 --> 00:25:00,865
This shows the flexibility that virtual
cluster provides us alongside the Kai

385
00:25:00,925 --> 00:25:07,185
Scheduler and can give us this really nice
middle ground for supporting various teams

386
00:25:07,305 --> 00:25:12,965
without having a headache of upgrading
the Kai versions or other components.

387
00:25:13,955 --> 00:25:17,285
So this is done and the
result of this is that we have

388
00:25:17,285 --> 00:25:19,475
currently two V glasses running.

389
00:25:20,315 --> 00:25:21,305
One team better.

390
00:25:21,605 --> 00:25:22,685
One team stable.

391
00:25:23,015 --> 00:25:27,935
They both run on 0 27 virtual or
cluster version, and we've achieved

392
00:25:27,935 --> 00:25:29,955
isolation that we talked about.

393
00:25:30,735 --> 00:25:34,685
So let's look at how it is
actually on the host cluster.

394
00:25:34,685 --> 00:25:38,225
So remember, our host cluster is
just a kind cluster running clock

395
00:25:38,375 --> 00:25:39,455
on my machine, but this can be.

396
00:25:40,400 --> 00:25:45,210
Any cluster managed cluster
bare metal future cluster does

397
00:25:45,210 --> 00:25:47,400
not require any special setup.

398
00:25:47,400 --> 00:25:51,000
It would work anywhere because this
is just a Kubernetes distribution.

399
00:25:51,900 --> 00:25:55,290
So here we can see the
future cluster pods.

400
00:25:55,470 --> 00:26:00,240
So we cluster team beta in,
team stable, they're running,

401
00:26:00,600 --> 00:26:02,970
and those pos are going to.

402
00:26:03,470 --> 00:26:08,250
Our cube, API and if somebody wants to
connect whether using cube kettle or V

403
00:26:08,250 --> 00:26:14,500
cluster CLI or just consume web app or any
other workload virtual clusters running

404
00:26:14,500 --> 00:26:16,240
here will provide the isolation level.

405
00:26:16,740 --> 00:26:19,790
And here we can, of course,
other components from the

406
00:26:19,790 --> 00:26:20,780
previous demos as well.

407
00:26:21,280 --> 00:26:24,250
What we have achieved,
let's bring it home.

408
00:26:24,880 --> 00:26:30,340
From my perspective, I think as platform
engineer or SRE, that's where I've worked

409
00:26:30,340 --> 00:26:33,580
for most of my career, and I know that.

410
00:26:34,525 --> 00:26:39,385
Upgrades and updates are always
painful, especially in distributed

411
00:26:39,385 --> 00:26:40,825
systems like Kubernetes.

412
00:26:40,885 --> 00:26:43,415
Those things are not glamorous.

413
00:26:43,415 --> 00:26:47,195
They are behind the scenes, but
they have to happen and they are

414
00:26:47,285 --> 00:26:50,015
very stressful and often costly.

415
00:26:50,285 --> 00:26:55,605
They require a lot of planning and
errors or risk that they carry can

416
00:26:55,605 --> 00:26:57,225
be very significant to the business.

417
00:26:57,975 --> 00:27:00,165
So testing, scheduler, upgrades.

418
00:27:00,410 --> 00:27:07,290
We, instead of creating a new host
cluster, maybe a new EKS or having a

419
00:27:07,290 --> 00:27:12,360
test environment that we have to pay
for, which would typically take several

420
00:27:12,360 --> 00:27:18,540
hours, we limited it to few minutes when
we were able to create a separate virtual

421
00:27:18,540 --> 00:27:24,030
clusters and install scheduler there while
configuring V cluster in a specific way.

422
00:27:24,530 --> 00:27:28,640
So we've reduced the risk literally
to 0% because if something doesn't

423
00:27:28,640 --> 00:27:31,700
work in our future cluster, we
don't care which is deleted.

424
00:27:32,110 --> 00:27:34,180
And there's no impact on
other production workloads.

425
00:27:34,680 --> 00:27:39,150
All backing changes we've seen
from potentially several hours.

426
00:27:39,180 --> 00:27:43,470
We were able to lower this
time to about 39 seconds.

427
00:27:43,470 --> 00:27:46,960
We just deleted the virtual
cluster and nothing happened.

428
00:27:47,290 --> 00:27:51,040
And my favorite, we've enabled
very strong AB testing.

429
00:27:51,400 --> 00:27:54,820
So we can now see which versions work.

430
00:27:54,850 --> 00:28:00,120
We can gradually introduce them and we can
split this IB testing across time as well.

431
00:28:00,270 --> 00:28:04,960
So from pretty high risk if we would
be using if we would be upgrading

432
00:28:05,350 --> 00:28:09,530
scheduler in place, in host cluster,
we went to a very low risk or

433
00:28:09,530 --> 00:28:12,100
almost zero per team schedulers.

434
00:28:12,190 --> 00:28:12,970
This is obviously.

435
00:28:13,420 --> 00:28:15,130
Multi-tenancy benefit.

436
00:28:15,670 --> 00:28:22,570
But other than that, we were able,
we wouldn't have to essentially talk

437
00:28:22,570 --> 00:28:26,920
to the teams and making sure that
we have this middle ground version.

438
00:28:26,920 --> 00:28:28,060
We can essentially.

439
00:28:28,485 --> 00:28:33,805
Satisfy each team's need for specific
virtual cluster and Kai scheduler

440
00:28:33,805 --> 00:28:36,265
version, which is super powerful.

441
00:28:36,485 --> 00:28:42,095
And as is as an SE I'm always amazed
that this works and makes this blast so

442
00:28:42,095 --> 00:28:47,165
small and in the same time enables me
to serve the teams that I'm taking care

443
00:28:47,165 --> 00:28:52,725
of in exactly the versions and software
that they need without impacting others.

444
00:28:53,640 --> 00:29:00,070
And we've also seen that even our
simple demo can use proper GPU sharing

445
00:29:00,370 --> 00:29:04,210
and you could run it at home if you
have a powerful enough computer on

446
00:29:04,210 --> 00:29:08,560
environmental, or of course, in the
business application at scale as well.

447
00:29:09,060 --> 00:29:12,150
So let me just change
the settings real quick.

448
00:29:12,760 --> 00:29:14,410
A little bit of resources here.

449
00:29:14,510 --> 00:29:20,560
You can go to v cluster.com/docs and
there's a dedicated article about

450
00:29:20,560 --> 00:29:24,970
using Kai Scheduler with V Cluster, or
you can explore order configuration.

451
00:29:25,520 --> 00:29:28,465
There's also a case scheduler
page on GitHub as well.

452
00:29:29,080 --> 00:29:32,140
And the specific integration
with Kai Schedule, as I mentioned

453
00:29:32,140 --> 00:29:34,540
earlier, please join our Slack.

454
00:29:34,600 --> 00:29:40,000
We would love to have you and talk
about Kai V Cluster or any other topics

455
00:29:40,120 --> 00:29:42,520
related to multi-tenancy and Kubernetes.

456
00:29:42,810 --> 00:29:44,970
You can connect with me on LinkedIn.

457
00:29:45,030 --> 00:29:47,520
I would love to hear your experiences.

458
00:29:48,075 --> 00:29:51,675
With similar setups and how
you're using whatever tools

459
00:29:51,675 --> 00:29:53,655
you're using to help yourself.

460
00:29:53,995 --> 00:29:59,245
We have also Office Hours, which is
an event hosted by our marketing team

461
00:29:59,495 --> 00:30:04,905
which can give you first experience
with future Cluster and it's technology.

462
00:30:05,475 --> 00:30:08,355
And this is in our Lage events page.

463
00:30:08,855 --> 00:30:09,275
Perfect.

464
00:30:09,275 --> 00:30:10,835
So that's it.

465
00:30:10,865 --> 00:30:11,885
I hope you enjoyed it.

466
00:30:11,985 --> 00:30:15,135
Again, please feel free
to contact me on LinkedIn.

467
00:30:15,585 --> 00:30:20,625
I would love to chat with you about
virtual cluster and related technologies,

468
00:30:20,865 --> 00:30:23,235
multitenancy and other things.

469
00:30:23,715 --> 00:30:24,915
Please feel free to reach out.

470
00:30:25,425 --> 00:30:27,825
Thank you everyone and have a
great rest of the conference.

