1
00:00:00,500 --> 00:00:01,400
Greetings, everyone.

2
00:00:01,550 --> 00:00:04,430
Welcome to Con 42 Observability 2025.

3
00:00:04,580 --> 00:00:08,390
My name is Ravin Kana, a senior
software engineer who focuses on

4
00:00:08,390 --> 00:00:12,020
cloud native automation and large
scale data processing pipelines.

5
00:00:12,440 --> 00:00:16,970
Our topic is adaptive parallelism
insights, which I'll ABB appreciate

6
00:00:17,029 --> 00:00:19,009
as a PIX after this introduction.

7
00:00:19,550 --> 00:00:24,500
A PIX combines Open standard Telemetry
with a compact graph neural network

8
00:00:24,680 --> 00:00:29,090
so that an A WSD function workflow can
learn to tune itself while it's running.

9
00:00:29,690 --> 00:00:32,209
By the end of this session, you
will understand three things.

10
00:00:32,570 --> 00:00:36,380
First, how we capture branch level
data without rewriting business code.

11
00:00:36,650 --> 00:00:41,450
Second, how a small model converts
that data into useful guidance.

12
00:00:41,660 --> 00:00:46,250
And third, how a closed loop safely
applies the guidance to reduce.

13
00:00:46,685 --> 00:00:51,655
Latency and cost while keeping
every decision auditable to begin,

14
00:00:52,585 --> 00:00:58,735
serverless fan outs offer near
instant elasticity at three problems

15
00:00:58,795 --> 00:01:00,960
surface as scale grows first.

16
00:01:01,460 --> 00:01:06,259
Latency clips arise because a few slow
branches delay the entire workflow.

17
00:01:06,859 --> 00:01:11,240
Even a single hotspot can push the
entire service level agreement.

18
00:01:11,810 --> 00:01:17,299
Second, tracing gaps appear because
high level dashboards average a branch

19
00:01:17,299 --> 00:01:21,470
specific anomalies, which means engineers
cannot pinpoint trouble quickly.

20
00:01:22,070 --> 00:01:25,009
Third, cost spikes emerge
when hidden retries.

21
00:01:25,039 --> 00:01:28,190
Cold starts infra the monthly
without obvious warning.

22
00:01:28,690 --> 00:01:32,470
These pains led us to frame
three guiding questions.

23
00:01:32,830 --> 00:01:38,170
Question one asked how to collect
fine-grained telemetry from every

24
00:01:38,170 --> 00:01:40,570
branch while adding minimal overhead.

25
00:01:41,259 --> 00:01:44,950
Question two considers whether a
lightweight artificial intelligence

26
00:01:44,950 --> 00:01:49,360
model can digest that stream quickly
enough to make timely recommendations.

27
00:01:49,809 --> 00:01:51,640
Question three asks.

28
00:01:51,940 --> 00:01:57,280
How to apply those recommendations during
a live run without breaking confidence

29
00:01:57,280 --> 00:01:59,110
limits or financial guard rates.

30
00:01:59,500 --> 00:02:03,960
The rest of the talk answers each
questions inter contextually.

31
00:02:04,319 --> 00:02:09,959
Many enterprise workloads such as data
enrichment, ETL, retries and compliance

32
00:02:09,989 --> 00:02:14,940
reaction depend on the map state pattern
inside AWS Dev functions to fan out toss.

33
00:02:15,690 --> 00:02:20,609
Limitations acknowledge the native
execution history application programming

34
00:02:20,609 --> 00:02:26,960
interface shows only course events,
which hides branch behavior that matters.

35
00:02:27,335 --> 00:02:31,445
Standardization helps because
open telemetry, the leading vendor

36
00:02:31,625 --> 00:02:37,295
neutral format for traces, metrics,
and logs let us instrument once

37
00:02:37,355 --> 00:02:39,665
and analyzed anywhere in parallel.

38
00:02:39,755 --> 00:02:43,595
CloudWatch EMF Embedded metric
format adds structured counters

39
00:02:43,595 --> 00:02:46,055
inside Lambda logs without agents.

40
00:02:46,535 --> 00:02:50,704
Research to date often targets
virtual machine clusters with

41
00:02:50,795 --> 00:02:52,924
reinforcement learning less.

42
00:02:52,924 --> 00:02:55,535
Focus has landed on dynamic
state machine graphs.

43
00:02:55,880 --> 00:03:01,640
Where topology changes every minute
opportunity identified graph neural

44
00:03:01,640 --> 00:03:05,749
networks accelerate reasoning over
graphs whose notes and edges evolve

45
00:03:05,749 --> 00:03:09,529
rapidly so they fit our fan out scenario.

46
00:03:09,769 --> 00:03:13,790
A PIX therefore unites detailed
telemetry open standards, and a

47
00:03:13,790 --> 00:03:17,899
compact graph neural network into one
practical flows loop control system.

48
00:03:18,399 --> 00:03:21,609
The picture at right shows
the flow of data and control.

49
00:03:22,089 --> 00:03:25,480
Starting at the top, the
instrumentation layer adds an

50
00:03:25,570 --> 00:03:27,970
open telemetry span to every task.

51
00:03:28,179 --> 00:03:33,649
Parallel and map branch spans a carry,
chunk ID and the same wrapper logs,

52
00:03:33,679 --> 00:03:36,049
counters, such as payload by three.

53
00:03:36,049 --> 00:03:41,569
Try count close, cold start time by
using CloudWatch embedded metric format.

54
00:03:42,109 --> 00:03:47,239
Next, the telemetry pipeline since spans
over the open telemetry protocol to

55
00:03:47,239 --> 00:03:49,489
a collector running on AWS four gate.

56
00:03:50,119 --> 00:03:55,459
The collector passes real time streams
into Kinesis Data File Host, which

57
00:03:55,519 --> 00:04:01,040
fuels live dashboards in Amazon
managed Grafana, and it also stores

58
00:04:01,040 --> 00:04:04,459
raw records as Parquet files in Amazon.

59
00:04:04,459 --> 00:04:06,829
Simple storage surveys for deep analysis.

60
00:04:07,219 --> 00:04:12,560
Moving downward sales maker Serverless
hosts a two layer graph neural network.

61
00:04:12,889 --> 00:04:16,249
With about 1.2 million
parameters every 60 seconds.

62
00:04:16,249 --> 00:04:20,509
The model consumes the latest window
of branch metric and predicts an

63
00:04:20,509 --> 00:04:26,090
ideal chunk size plus confidence and
projected cost and latency impact.

64
00:04:26,590 --> 00:04:31,390
Even bridge then delivers third
influence inference to a Lambda callback.

65
00:04:31,840 --> 00:04:36,400
The callback applies guardrail checks
and if they pass patches, the running

66
00:04:36,400 --> 00:04:40,239
map state through the step function
update application programming interface.

67
00:04:41,200 --> 00:04:47,200
Finally, governance is preserved because
each inference has hashed, stored in

68
00:04:47,200 --> 00:04:51,970
systems managed parameter store, and
according into subsequent spans, giving

69
00:04:51,970 --> 00:04:57,150
auditors a full lineage from decision
to outcome, automation must be safe.

70
00:04:57,420 --> 00:05:01,650
So the controller enforce three
boundaries before any change.

71
00:05:01,770 --> 00:05:07,725
First, the chink change window limits are
adjustment to 25% within five minutes.

72
00:05:08,325 --> 00:05:10,034
Which prevents oscillation.

73
00:05:10,364 --> 00:05:15,645
Second, the concurrency cap ensures
the new chunk size keeps total

74
00:05:15,645 --> 00:05:21,405
lambda concurrency below the account
quota minus a safety buffer re

75
00:05:21,405 --> 00:05:23,145
retrieved from parameter store.

76
00:05:23,594 --> 00:05:29,864
Third, the rollback trigger
monitors 95th percentile latency and

77
00:05:29,864 --> 00:05:35,234
total errors if both rise for two
consecutive windows, the controller

78
00:05:35,234 --> 00:05:37,184
reverts to the prior Chang size.

79
00:05:37,545 --> 00:05:41,895
Together these safeguards let the
model explore improvements while

80
00:05:42,315 --> 00:05:46,635
guaranteeing that production stability
and cost limits remain intact.

81
00:05:47,135 --> 00:05:51,425
We measure first because
insight depends on data quality.

82
00:05:51,965 --> 00:05:57,035
Span emission adds about two
millisecond per invocation, negligible

83
00:05:57,035 --> 00:06:02,105
overhead for data processing tasks
that often run for seconds after

84
00:06:02,105 --> 00:06:04,475
capture the open telemetry collector.

85
00:06:05,300 --> 00:06:10,490
Batches spans, pushes them to can
assist data file hose and writes

86
00:06:10,490 --> 00:06:12,950
fully ity regards to Amazon.

87
00:06:13,010 --> 00:06:18,350
S3 visualization follows where managed
Grafana converts those streams into

88
00:06:18,350 --> 00:06:22,610
live heat maps highlighting slow or
throated branches within seconds.

89
00:06:23,030 --> 00:06:27,740
Flexibility matters and because every
interference interface follows open

90
00:06:27,740 --> 00:06:33,500
standards, teams can replace Grafana
with another dashboard or root.

91
00:06:34,000 --> 00:06:39,410
Par K files into S3 for any
analytical platform or without

92
00:06:39,410 --> 00:06:41,890
revising revisiting application code.

93
00:06:42,370 --> 00:06:46,510
XP experience shows that this
separation accelerates debugging

94
00:06:46,510 --> 00:06:52,660
and gives finance teams precise cost
attribution tied to trace identifiers.

95
00:06:53,160 --> 00:06:54,955
Evaluation used three workloads.

96
00:06:55,075 --> 00:07:00,145
Synthetic uniform contains 10 terabytes
of uniformly sized one megabyte ard.

97
00:07:01,060 --> 00:07:06,130
Synthetic skewed SS also spans
10 terabytes, but injects 5%

98
00:07:06,130 --> 00:07:08,380
larger records up to 10 megabytes.

99
00:07:08,770 --> 00:07:13,810
Two fourth of slower processing
live enterprise entity data.

100
00:07:13,960 --> 00:07:17,890
For example, like contractual
data represents three terabytes

101
00:07:17,890 --> 00:07:19,660
of anonymized production updates.

102
00:07:20,230 --> 00:07:24,800
Baseline compared were static
patching at 1000 items.

103
00:07:24,830 --> 00:07:29,750
Dynamic sub three partitioning that
splits branches about two gigabytes

104
00:07:29,870 --> 00:07:33,350
and a PIX adaptive chunking metrics.

105
00:07:34,310 --> 00:07:36,980
Metrics tracked include 50th.

106
00:07:37,610 --> 00:07:43,670
90th and 95th percentile latency
OB cost in United States dollars.

107
00:07:43,730 --> 00:07:46,570
And throttle error
counts reading the table.

108
00:07:46,630 --> 00:07:50,440
Adaptive chunking shows the lowest
latency across all the percentile and

109
00:07:50,440 --> 00:07:52,240
the lowest cost for every workload.

110
00:07:52,660 --> 00:07:58,660
For synthetic skewed throttle errors
drop from 47 under static patching

111
00:07:58,660 --> 00:08:04,390
to F five under API X, demonstrating
resilience against uneven regard sizes.

112
00:08:04,890 --> 00:08:09,000
Live Enterprise data follows the same
trend with active chunking, cutting

113
00:08:09,000 --> 00:08:14,690
both dry tile latency and cost while
reducing throttles from 24 to three.

114
00:08:15,230 --> 00:08:18,940
In summary, real time tuning
delivers consistent performance

115
00:08:18,940 --> 00:08:21,310
gains and tangible cost savings.

116
00:08:21,810 --> 00:08:27,750
Five lessons for anyone who wants to
replicate EPX first Sample generously

117
00:08:28,380 --> 00:08:33,960
capturing every span during peak
windows, so retry storms are visible down

118
00:08:33,960 --> 00:08:36,270
sample later to trim storage if needed.

119
00:08:36,690 --> 00:08:43,034
Second, activate guardrails because
the 25% cap on chunk changes smooths

120
00:08:43,065 --> 00:08:45,495
workload burst and prevents isolation.

121
00:08:45,885 --> 00:08:47,745
Third connect costs.

122
00:08:48,705 --> 00:08:52,965
Publishing dollar Deltas alongside
trace identifiers quickly gained

123
00:08:53,025 --> 00:08:55,125
trust from finance stakeholders.

124
00:08:55,695 --> 00:09:01,365
Fourth, inspect retry patterns since
high retry counts, often pinpoint hard

125
00:09:01,365 --> 00:09:03,345
partitions or downstream rate limits.

126
00:09:03,795 --> 00:09:08,820
Pair re try count with chunk identifiers
made those hotspot obvious Fifth.

127
00:09:09,070 --> 00:09:14,080
Retrain weekly because as new
entity types enter production, the

128
00:09:14,080 --> 00:09:18,130
graph neural network must adapt
to maintain prediction accuracy.

129
00:09:18,670 --> 00:09:23,890
Collectively, these practices turn a
PIX from prototype into a production

130
00:09:23,890 --> 00:09:25,960
service adapted by multiple teams.

131
00:09:26,460 --> 00:09:30,475
Every study has limitations, cold
start, variability per persist.

132
00:09:31,050 --> 00:09:35,820
Even with Snap start, residual
jitter can influence daily

133
00:09:35,820 --> 00:09:38,490
latency and may exaggerate gains.

134
00:09:38,880 --> 00:09:44,160
Synthetic bias is possible because
synthetic workloads, while realistic may

135
00:09:44,250 --> 00:09:50,940
overlook extreme traffic patterns found
in the divide for future work will replay

136
00:09:50,940 --> 00:09:53,945
full production traces to close that gap.

137
00:09:54,475 --> 00:09:56,694
Model rate remains risk.

138
00:09:57,180 --> 00:10:01,050
Shifting data distributions
can erode predictions quality.

139
00:10:01,050 --> 00:10:05,339
So automated drift alerts
trigger retraining whenever

140
00:10:05,520 --> 00:10:06,989
crosses a person threshold.

141
00:10:07,489 --> 00:10:12,559
Recognizing these limits helps
others decide whether A PIX fits

142
00:10:12,559 --> 00:10:17,569
their environment and guides our
roadmap for continued validation.

143
00:10:18,069 --> 00:10:21,904
In closing, adaptive parallelism
insights turns detailed.

144
00:10:22,569 --> 00:10:26,799
Telemetry into timely recommendations
that update step functions, workflow

145
00:10:27,069 --> 00:10:32,019
safely benefited, delivered teams
gain faster executions, lower compute

146
00:10:32,019 --> 00:10:35,319
bills, and complete audit trail
without changing business logic.

147
00:10:35,769 --> 00:10:41,829
Next steps we intend to coordinate
turning across multiple workflows that

148
00:10:41,829 --> 00:10:47,019
share concurrency limits and explore web
assembly based span features extraction

149
00:10:47,019 --> 00:10:48,759
to reduce overhead even further.

150
00:10:49,749 --> 00:10:50,469
Appreciation.

151
00:10:50,469 --> 00:10:51,939
Thank you for your attention.

152
00:10:52,029 --> 00:10:56,469
Your questions and feedback through
conference forums are welcome and I look

153
00:10:56,469 --> 00:11:01,599
forward to hearing how you apply adaptive
observability in your own pipelines.

154
00:11:02,099 --> 00:11:02,430
Thank you.

