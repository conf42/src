1
00:00:00,500 --> 00:00:01,160
Hello everybody.

2
00:00:01,250 --> 00:00:01,909
Good morning.

3
00:00:02,630 --> 00:00:07,400
This stock explores how network
infrastructure defines platform potential.

4
00:00:07,900 --> 00:00:12,430
Traditional webs apps scale differently
than a platforms which require ultra

5
00:00:12,430 --> 00:00:14,319
low latency and massive scalability.

6
00:00:14,979 --> 00:00:18,490
And the network can either
empower or triple modern systems.

7
00:00:19,330 --> 00:00:25,420
I'll take a deep dive how a network
is very crucial in modern era.

8
00:00:25,920 --> 00:00:33,589
Your network is, we all say the
weakest link, the strength of a

9
00:00:33,589 --> 00:00:37,970
chain is its weakest link, so it's
applicable to network as well.

10
00:00:37,974 --> 00:00:42,050
The network is either your platform
superpower or actually network here.

11
00:00:42,099 --> 00:00:44,349
All the devices, which do does the.

12
00:00:44,904 --> 00:00:49,554
Packet processing capabilities, how
the packet is far from any user, all

13
00:00:49,554 --> 00:00:53,035
the way to the server and the written
packet from the server to user.

14
00:00:53,194 --> 00:00:56,934
You experience the internet,
its fastness, it's.

15
00:00:57,625 --> 00:00:59,605
How you feel as a user.

16
00:00:59,785 --> 00:01:05,135
It that's the one decides any
product in the market, be it, is its

17
00:01:05,135 --> 00:01:09,185
simple application or be its very
e-commerce platform or it an app.

18
00:01:09,664 --> 00:01:12,185
So underlying there is so
much networking concepts.

19
00:01:12,664 --> 00:01:14,255
There is so much networking is involved.

20
00:01:14,525 --> 00:01:18,795
So today we're looking into how
some of the case studies, how

21
00:01:19,275 --> 00:01:20,680
networking plays a key role.

22
00:01:21,180 --> 00:01:25,990
We'll cover architectural evaluation
networking failure models, modes,

23
00:01:25,990 --> 00:01:30,500
sorry principles of fault tolerance,
network observability strategies,

24
00:01:30,530 --> 00:01:32,900
and real world case studies.

25
00:01:33,830 --> 00:01:37,870
And those con around those concepts,
we have more from traditional

26
00:01:37,870 --> 00:01:39,250
three tide architectures.

27
00:01:39,300 --> 00:01:44,880
There were core distribution
access to spine leaf models

28
00:01:44,970 --> 00:01:46,710
where two spines are connected.

29
00:01:47,429 --> 00:01:51,649
Three lead switch optimized for
particularly these optimized

30
00:01:51,649 --> 00:01:52,580
for east waste traffic.

31
00:01:52,610 --> 00:01:56,809
Because of the tremendous AI and
tremendous activity in the data centers.

32
00:01:57,329 --> 00:02:01,529
The traffic not only is from the north
to south, meaning the user to server,

33
00:02:01,949 --> 00:02:03,989
but also there is a lot of computation.

34
00:02:04,039 --> 00:02:08,714
There, there are a lot of calculations
involved within the data center before

35
00:02:09,074 --> 00:02:11,804
the response is sent to the user.

36
00:02:12,294 --> 00:02:13,494
This has become more.

37
00:02:13,945 --> 00:02:20,244
Even more prevalent in a data centers
where the distribution nodes have

38
00:02:21,024 --> 00:02:26,404
grown tremendously from two to four, a
simple web scale traffic to one hundred

39
00:02:26,404 --> 00:02:30,609
and twenty eight, two to 56 nodes in
the case of a the case of a network.

40
00:02:31,109 --> 00:02:34,619
Software defined networking, of
course, because of the lot of

41
00:02:34,619 --> 00:02:38,174
nodes involved in the data centers,
software defined networking,

42
00:02:38,174 --> 00:02:39,914
which enables the programmability.

43
00:02:39,914 --> 00:02:46,634
Basically, you are able to program a lot
of devices at once so that the DevOps,

44
00:02:47,534 --> 00:02:52,764
those particularly, the duties revolving
around the DevOps will become but.

45
00:02:53,619 --> 00:02:58,450
Much more easy and predictable,
less prone and et cetera.

46
00:02:58,950 --> 00:03:01,469
Of course, a scale requests specialized.

47
00:03:01,469 --> 00:03:03,659
The six and delta low latency fabrics.

48
00:03:03,659 --> 00:03:07,439
There is so much of hardware
there is so much development in

49
00:03:07,439 --> 00:03:08,999
the hardware also is happening.

50
00:03:08,999 --> 00:03:15,089
Particularly asics are built with ultra
high speeds and ultra low latency.

51
00:03:15,089 --> 00:03:18,329
The whole data center fabrics
are built particularly.

52
00:03:19,035 --> 00:03:24,735
So with, to handle the high bandwidth
and low latency, each stage reflects

53
00:03:24,735 --> 00:03:28,545
growing demands for bandwidth,
low latency, and flexibility.

54
00:03:29,045 --> 00:03:32,495
Traditional versus modern network
requirements, web applications typically

55
00:03:32,495 --> 00:03:34,355
need two to four backend connections.

56
00:03:34,725 --> 00:03:39,374
Tolerate 5,200 milliseconds,
latency and follow predictable.

57
00:03:39,780 --> 00:03:41,609
They all follow predictable traffic.

58
00:03:42,090 --> 00:03:47,340
AI workloads connect 128 to 2 56
plus loads needs terabits, but

59
00:03:47,340 --> 00:03:51,270
second scale bandwidth and require
less than 10 milliseconds latency.

60
00:03:51,330 --> 00:03:55,869
They're bursty unpredictable and
dominated by e stress traffic.

61
00:03:56,164 --> 00:04:00,304
So if you take a case study, just a
simple cascading networking failure

62
00:04:01,094 --> 00:04:06,224
during peak load, lets say a financial
AI platform experience, a p timeouts.

63
00:04:06,734 --> 00:04:07,994
The loss is huge.

64
00:04:08,924 --> 00:04:12,285
The confidence will be shared,
the confidence will be hit low.

65
00:04:12,654 --> 00:04:19,334
There are so many problems that
can cause if a financial, a

66
00:04:19,334 --> 00:04:21,044
platform experience, some timeouts.

67
00:04:21,974 --> 00:04:24,984
So particularly this can
happen I mean it's identified

68
00:04:25,135 --> 00:04:27,375
like tcp in cast congestion.

69
00:04:27,854 --> 00:04:35,985
Basically tcp in cast congestion is where
multiple servers respond to aggregator

70
00:04:35,985 --> 00:04:42,015
node at once, this cascades into a
lot of packet drops, transmission de

71
00:04:42,015 --> 00:04:44,355
transmissions, and service degradation.

72
00:04:45,085 --> 00:04:48,655
In data centers nowadays, we have
this remote direct memory access,

73
00:04:49,135 --> 00:04:53,034
our converge ethernet with flow
control to prevent collapse.

74
00:04:53,364 --> 00:04:56,754
So this will be called Rocky,
E-R-D-M-A or converge ethernet.

75
00:04:57,254 --> 00:05:02,804
So this is one case study where we can
apply R-D-M-A-R-D-M-E-A so that you,

76
00:05:03,674 --> 00:05:06,664
the, and the congestion can be avoided.

77
00:05:07,164 --> 00:05:11,874
So network bottlenecks very few
key bottlenecks, buffer blot.

78
00:05:12,574 --> 00:05:13,319
There's lot of.

79
00:05:13,819 --> 00:05:17,349
Buffer size increase so
that increases the latency.

80
00:05:17,899 --> 00:05:21,739
Packet processing limits traditionals,
which can't handle billions of packets per

81
00:05:21,739 --> 00:05:26,179
second topal, they can hand the new sx.

82
00:05:26,479 --> 00:05:30,559
They handle a lot of traffic,
so that also has to be included.

83
00:05:30,889 --> 00:05:36,419
Topology constraints like war subscription
ratios, break and ray workloads bandwidth

84
00:05:36,419 --> 00:05:38,519
saturation ML training can overwhelm.

85
00:05:39,019 --> 00:05:41,539
Bits per second links in seconds.

86
00:05:42,039 --> 00:05:44,679
So how should a network design be?

87
00:05:44,809 --> 00:05:47,554
How should a fault
tolerant network design be?

88
00:05:48,054 --> 00:05:48,744
Principles?

89
00:05:48,834 --> 00:05:52,974
T fabrics meaning you have a
path, an alternate path in case

90
00:05:52,974 --> 00:05:57,384
of failure, graceful degradation
and isolated failure domains.

91
00:05:57,384 --> 00:06:00,204
Like you, you isolate a
failure domains so that you.

92
00:06:00,704 --> 00:06:04,724
Traffic is not impacted or
only less traffic is impacted.

93
00:06:05,144 --> 00:06:11,594
So for to have some of the strategies
like ECMP equal Custom MULTIPATH for

94
00:06:11,594 --> 00:06:16,694
path diversity, as I said, there is an
alternate path for the traffic to flow

95
00:06:17,324 --> 00:06:21,254
and bidirectional forward detection
for a subsequent fail for subsequent

96
00:06:21,584 --> 00:06:28,129
failover, meaning at a very hardware
level, if there is a. Failover of a link

97
00:06:28,189 --> 00:06:33,109
between two devices that will be easily
identified, that will easily propagate

98
00:06:33,139 --> 00:06:38,269
to the upper layers, and the alternate
path is identified for the traffic and

99
00:06:38,269 --> 00:06:40,549
segment routing for traffic engineering

100
00:06:41,049 --> 00:06:42,219
modern spying.

101
00:06:42,624 --> 00:06:44,664
Leave architecture for a clothes.

102
00:06:45,084 --> 00:06:48,924
The advantages are predictable
latency, linear scalability,

103
00:06:49,164 --> 00:06:50,754
optimized for re traffic.

104
00:06:51,454 --> 00:06:52,714
Design considerations.

105
00:06:52,774 --> 00:06:57,934
Keep wall subscription below three, three
to one in the sense of our subscription.

106
00:06:57,934 --> 00:07:02,644
In the data networking refers to
the bandwidth available, bandwidth

107
00:07:02,644 --> 00:07:08,994
proposed to the downlink servers, to
bandwidth available to the uplink.

108
00:07:09,494 --> 00:07:10,514
So always.

109
00:07:10,964 --> 00:07:14,534
So which means for three down
links there is at least one

110
00:07:14,564 --> 00:07:16,064
uplink path should be available.

111
00:07:16,064 --> 00:07:20,474
That's what I meant by our
subscription use ECMP diversity.

112
00:07:20,609 --> 00:07:26,579
There's again, equal cost multi-part,
so the cost, the path calculation by

113
00:07:26,939 --> 00:07:32,799
various protocols is done and inserted
into the routing tables so that traffic

114
00:07:32,799 --> 00:07:35,650
can be spread across multiple parts.

115
00:07:36,150 --> 00:07:40,999
So that the bandwidth is efficiently
utilized, size buffers for microbus,

116
00:07:41,329 --> 00:07:44,929
meaning there are some, there is,
there are some traffic patterns

117
00:07:45,049 --> 00:07:49,459
where microbus can happen so that
your buffer size can be dynamically

118
00:07:49,459 --> 00:07:52,129
increased to accommodate those microbus.

119
00:07:52,620 --> 00:07:56,370
Consider RDMJ remote direct memory
access for ultra low latency.

120
00:07:56,870 --> 00:08:02,199
This is network applicability, also
play a key role because you, we better

121
00:08:02,199 --> 00:08:04,329
find problems before the users do.

122
00:08:04,939 --> 00:08:09,559
For example, I mean there are
telemetry, collect telemetry, like

123
00:08:09,559 --> 00:08:13,324
one second metrics that might be too
fast, but that might be too frequent.

124
00:08:13,464 --> 00:08:16,284
But you can have some
regular interval telemetry.

125
00:08:16,784 --> 00:08:23,895
Monitor KPAs like latency distributions,
utilization retransmissions use machine

126
00:08:23,895 --> 00:08:27,794
learning for anomaly detection and
digital twins for what if analysis.

127
00:08:28,194 --> 00:08:33,390
This observability always plays a
key role when you want to avoid the

128
00:08:33,390 --> 00:08:35,510
problems before even there do happen.

129
00:08:35,809 --> 00:08:39,015
So that observability
really plays a key role.

130
00:08:39,515 --> 00:08:41,405
The hidden cost of network latency.

131
00:08:41,405 --> 00:08:46,594
In a ML pipelines, even one milliseconds
of latency can add minutes to training.

132
00:08:47,094 --> 00:08:53,454
32% is training efficiency loss, 2.5
times infrastructure cost multiplies.

133
00:08:54,234 --> 00:08:55,644
47% is wasted.

134
00:08:55,674 --> 00:09:01,584
GP cycles and 82 millisecond in inference
latency, I have to use a response.

135
00:09:01,584 --> 00:09:06,144
Time networks dealers directly translate
into cost and time to market risks.

136
00:09:06,644 --> 00:09:11,894
Legacies switches process the
1.2 billion packets per second.

137
00:09:12,434 --> 00:09:12,794
Yay.

138
00:09:12,794 --> 00:09:16,054
Workload needs three times, like
3 billion packets per second.

139
00:09:16,594 --> 00:09:22,435
Modern IC add programmable pipelines,
hardware accelerated RDMA, beat

140
00:09:22,435 --> 00:09:27,264
buffers congestion management and sub
microsecond forwarding vendors like

141
00:09:27,264 --> 00:09:32,274
Nvidia, Broadcom, and Intel, they
design silicon for AA traffic patterns.

142
00:09:32,774 --> 00:09:35,774
Just a case study for an
e-commerce platform transformation

143
00:09:36,194 --> 00:09:37,274
Initially three times.

144
00:09:37,424 --> 00:09:42,604
There are 10 gig of per second links
seven, two second, seven to nine second

145
00:09:43,144 --> 00:09:48,184
page loads After microservices, east
west traffic goes to 60 percentage

146
00:09:48,184 --> 00:09:52,654
and latency spikes after adoption
requires a hundred GB per second,

147
00:09:53,164 --> 00:09:55,624
gigabits per second links for G cluster.

148
00:09:56,124 --> 00:10:00,894
Final, when final spine leaves with
400 gigabits per second backbone

149
00:10:00,894 --> 00:10:06,024
cut incidents by 98.5 percentage
and scale to petabytes per day.

150
00:10:06,564 --> 00:10:12,284
So the spine leaf architecture, it's,
it has become very powerful in the data

151
00:10:12,284 --> 00:10:19,099
centers where the speed is increased to
multi-fold than the previous data centers.

152
00:10:19,819 --> 00:10:23,659
So the incidents are
cut by 98.5% and scaled.

153
00:10:24,199 --> 00:10:25,159
To petabytes.

154
00:10:25,659 --> 00:10:26,619
So yeah.

155
00:10:26,649 --> 00:10:30,369
Key takeaways, building for
the future our for change.

156
00:10:30,489 --> 00:10:31,479
Design always.

157
00:10:31,479 --> 00:10:35,679
Your networks should be available
in for the future changes.

158
00:10:36,029 --> 00:10:38,579
Invest in observability
to catch issues early.

159
00:10:39,449 --> 00:10:43,559
Prioritize resilience with
fault tolerant designs.

160
00:10:44,069 --> 00:10:48,149
Optimized for latency since it direct
impacts cost and time to market.

161
00:10:48,854 --> 00:10:50,564
Your network is no longer plumbing.

162
00:10:50,624 --> 00:10:52,544
It's a core platform capability.

163
00:10:53,534 --> 00:10:53,984
Thank you.

