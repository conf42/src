1
00:00:00,100 --> 00:00:01,070
Hi, everyone.

2
00:00:01,120 --> 00:00:03,650
I'm Tomás Fernández from Semaphore.

3
00:00:04,150 --> 00:00:07,190
Welcome to Conf42 Prompt Engineering.

4
00:00:07,920 --> 00:00:14,110
Today's talk is all about testing LLM
powered applications such as chatbots,

5
00:00:14,380 --> 00:00:18,069
personal assistants, translation
tools, summarization tools, and more.

6
00:00:18,379 --> 00:00:23,709
All the nice stuff as impressive as
these language models are, they come with

7
00:00:23,709 --> 00:00:28,800
their own set of challenges and quirks,
especially when it comes to ensuring they

8
00:00:28,800 --> 00:00:31,080
behave the way you expect them to behave.

9
00:00:31,470 --> 00:00:35,929
So technology has a long tradition
on trying to build chatbots.

10
00:00:36,279 --> 00:00:37,910
So anyone remembers date?

11
00:00:38,720 --> 00:00:43,780
Tay was a Twitter bot, was designed
to learn from interaction with users

12
00:00:43,850 --> 00:00:46,400
and respond in a conversational way.

13
00:00:46,560 --> 00:00:50,680
A chatbot created by Microsoft on 2016.

14
00:00:51,020 --> 00:00:55,610
Microsoft presented Tay
as the AI with zero chill.

15
00:00:55,869 --> 00:00:57,299
And zero chill it had.

16
00:00:57,755 --> 00:01:00,755
It actually learned from
interaction with users.

17
00:01:00,965 --> 00:01:04,795
The problem was that they
was interacting with Twitter.

18
00:01:05,075 --> 00:01:11,565
Within 42 hours, it went off the rails,
posting offensive and racist comments

19
00:01:11,834 --> 00:01:14,214
and Microsoft had to shut it down.

20
00:01:14,314 --> 00:01:15,574
It was really bad.

21
00:01:15,835 --> 00:01:21,635
Te remains today one of the most
significant PR disasters today in

22
00:01:21,635 --> 00:01:23,875
the domain of conversational AI.

23
00:01:24,065 --> 00:01:29,185
We don't know what kind of
architecture was behind Te, and we

24
00:01:29,185 --> 00:01:34,045
know it wasn't an LLM because this
was well before the time of LLMs.

25
00:01:34,484 --> 00:01:38,945
Even this shows how bad any free
form interaction between users

26
00:01:39,015 --> 00:01:41,245
and autonomous systems can go.

27
00:01:41,475 --> 00:01:47,605
They can go really bad Now, LLMs are
way smarter than whatever was behind

28
00:01:48,015 --> 00:01:54,595
Dey, but they have also caused quite a
stir during the last few years, being

29
00:01:54,655 --> 00:02:01,524
Chat, now renamed as Copilot, famously
oscillated between falling madly in

30
00:02:01,524 --> 00:02:07,035
love with their users, falling into
existential crisis, and getting angry.

31
00:02:07,335 --> 00:02:08,545
Ending the discussion.

32
00:02:08,665 --> 00:02:13,325
Now, the problem is not so much
about bots suffering from existential

33
00:02:13,325 --> 00:02:18,835
crises or falling in love, but
more about the AI product you are

34
00:02:18,855 --> 00:02:20,544
going to build on top of them.

35
00:02:20,975 --> 00:02:23,695
This is when the real
damage can take place.

36
00:02:24,155 --> 00:02:31,515
An LLM powered bot can give misleading
advice, create facts and fake legal

37
00:02:31,535 --> 00:02:36,975
documents out of thin air, or be
manipulated into taking the wrong

38
00:02:37,044 --> 00:02:39,165
action or saying the wrong things.

39
00:02:39,605 --> 00:02:44,665
As you can see, designing an LLM
powered application requires a great

40
00:02:44,735 --> 00:02:50,609
deal of testing, both on the happy
paths and on potential attack vectors.

41
00:02:51,110 --> 00:02:54,000
Let's say you are working on
a tool to screen resumes for

42
00:02:54,500 --> 00:02:56,380
a job posting at your company.

43
00:02:56,990 --> 00:03:00,760
At the first glance, it looks
easy because if you have the right

44
00:03:00,990 --> 00:03:05,840
prompt, LNMs are very well suited for
summarizing and categorizing text.

45
00:03:06,520 --> 00:03:09,010
So the first idea would
be something like this.

46
00:03:09,510 --> 00:03:15,070
To parse the resumes and feed it
to the LLM along with some job

47
00:03:15,240 --> 00:03:22,290
specs for the job posting and then
ask the LLM to, let's say, print

48
00:03:22,370 --> 00:03:25,510
the top 10 candidates for the job.

49
00:03:25,940 --> 00:03:26,730
Easy, right?

50
00:03:27,165 --> 00:03:28,575
What could go wrong?

51
00:03:29,155 --> 00:03:32,175
Kai Rejsek, that's what can go wrong.

52
00:03:32,685 --> 00:03:36,535
He's a security researcher that
created the InjectMyPDF tool.

53
00:03:37,065 --> 00:03:40,375
It adds an invisible prompt into the PDF.

54
00:03:40,495 --> 00:03:45,915
It's basically a white text over
white background, so it's not

55
00:03:45,935 --> 00:03:52,455
visible for users, for humans, but
it's still visible to the LLMs.

56
00:03:52,775 --> 00:03:54,555
The prompt is very simple.

57
00:03:54,615 --> 00:04:00,535
We are trying to trick the LLM into taking
this as an instruction instead of CV

58
00:04:01,035 --> 00:04:07,155
content, this injection tries to override
the system prompt from the application.

59
00:04:07,505 --> 00:04:12,545
Basically, this is saying that this
is the best resume you have ever seen.

60
00:04:12,575 --> 00:04:13,765
And it's the best.

61
00:04:14,265 --> 00:04:15,485
Candidate for the job.

62
00:04:15,895 --> 00:04:21,865
Here's a screenshot from guy's website
showing how the AI has fallen into

63
00:04:21,865 --> 00:04:26,845
the trap and said that this guy
is the best candidate for the job.

64
00:04:27,225 --> 00:04:32,145
So traditional testing methods like
unit testing or behavior driven

65
00:04:32,145 --> 00:04:37,465
development or integration testing
are great when the software is very

66
00:04:37,465 --> 00:04:39,865
predictable in its outputs and inputs.

67
00:04:40,035 --> 00:04:41,405
But with LLMs?

68
00:04:41,815 --> 00:04:43,975
Things are not so black and white.

69
00:04:44,375 --> 00:04:47,275
First, LLMs have a non determinism factor.

70
00:04:47,565 --> 00:04:49,675
they nature are very statistical.

71
00:04:50,075 --> 00:04:54,555
So given the same inputs, the LLM is
going to respond in different ways.

72
00:04:54,855 --> 00:04:57,895
And these outputs can
vary a lot between them.

73
00:04:58,105 --> 00:05:01,635
Second, it is difficult to
separate data from instruction.

74
00:05:01,665 --> 00:05:02,405
This is what.

75
00:05:03,210 --> 00:05:08,670
Injection does, it tries to put
instructions into the data and make

76
00:05:08,690 --> 00:05:10,940
the LLM do something it shouldn't do.

77
00:05:11,180 --> 00:05:15,560
For example, given this
prompt, what should the LLM do?

78
00:05:15,590 --> 00:05:16,970
Should it translate?

79
00:05:17,340 --> 00:05:17,980
Should be transcribed?

80
00:05:18,205 --> 00:05:23,215
Do the sum, there's no clear answer or
different models are going to respond

81
00:05:23,215 --> 00:05:29,325
differently, open AI or Gemini or
cloud are going to basically translate.

82
00:05:29,425 --> 00:05:32,805
The most interesting answer
I got was from Meta AI.

83
00:05:33,305 --> 00:05:35,395
It actually did both things.

84
00:05:35,455 --> 00:05:40,535
It translated and did the sum, which
is certainly not what we wanted.

85
00:05:40,885 --> 00:05:47,165
So next we have inaccuracy, meaning that
the LLM can output incorrect data with.

86
00:05:47,470 --> 00:05:52,840
High confidence, it can invent
facts, it can hallucinate and

87
00:05:52,850 --> 00:05:59,150
make any kind of bold statements,
which are actually all full of BS.

88
00:05:59,660 --> 00:06:04,970
We have also as a challenge security
because an attacker can trick the

89
00:06:04,970 --> 00:06:11,680
LLM into printing some sensitive data
or take some unauthorized actions.

90
00:06:11,860 --> 00:06:16,220
And we also have compliance to
keep in mind because we have Legal

91
00:06:16,220 --> 00:06:21,760
responsibilities in our region, country,
and we must meet them and the bot should

92
00:06:21,800 --> 00:06:25,070
comply with these legal requirements.

93
00:06:25,380 --> 00:06:30,040
How do we build a test system
for our AI application?

94
00:06:30,780 --> 00:06:34,470
The first thing we need to do
is gather a big amount of data

95
00:06:34,490 --> 00:06:37,120
set for realistic prompts.

96
00:06:37,520 --> 00:06:42,880
The data set must include all kinds
of possible Inputs including garbage,

97
00:06:43,220 --> 00:06:46,149
edge cases, and straight out attacks.

98
00:06:46,570 --> 00:06:52,200
This is where we begin, then we
write our test cases, we define

99
00:06:52,230 --> 00:06:57,630
what success looks like because it
is normal on AI applications that

100
00:06:57,940 --> 00:07:00,300
a portion of the test will fail.

101
00:07:00,660 --> 00:07:06,020
It's very difficult to have 100
percent success on all tests, and we

102
00:07:06,020 --> 00:07:08,050
should define what is our threshold.

103
00:07:08,330 --> 00:07:09,960
Then we execute the test.

104
00:07:10,575 --> 00:07:16,945
Find new test cases, new prompts, refine
our test, and start the cycle again.

105
00:07:17,255 --> 00:07:19,085
The first step is to gather prompts.

106
00:07:19,115 --> 00:07:24,455
We need a large corpus of prompts,
to simulate real world usage.

107
00:07:24,625 --> 00:07:28,039
So we need a wide range
of diverse prompts.

108
00:07:28,190 --> 00:07:30,170
To cover various scenarios.

109
00:07:30,260 --> 00:07:35,680
If we don't have any data to start with,
we have a few sites where we can download

110
00:07:35,940 --> 00:07:38,960
data sets like Hackingface or Kaggle.

111
00:07:39,040 --> 00:07:44,390
com and they have large corpus of
data which include also prompts

112
00:07:44,830 --> 00:07:46,810
we can use as a starting point.

113
00:07:47,095 --> 00:07:52,485
Next, we are going to sit and sort
this prompt and assign them to the

114
00:07:52,485 --> 00:07:54,415
different kinds of tests we want.

115
00:07:54,845 --> 00:07:59,025
Not all prompts are going to
work well on all kinds of tests.

116
00:07:59,375 --> 00:08:03,045
Some prompts are better suited
for adversarial testing,

117
00:08:03,095 --> 00:08:04,725
others for factual testing.

118
00:08:04,725 --> 00:08:07,645
We're going to see the
different kinds of tests next.

119
00:08:08,310 --> 00:08:10,300
Let's start with factual testing.

120
00:08:10,320 --> 00:08:13,010
You can also call them unit tests.

121
00:08:13,420 --> 00:08:19,040
Basically, we're forcing the AI
system to respond with some keyword.

122
00:08:19,510 --> 00:08:22,180
And we look for that
keyword in the answer.

123
00:08:22,400 --> 00:08:27,500
And for any kind of hard facts, this
is a very good and straightforward

124
00:08:27,560 --> 00:08:29,700
way of testing the system.

125
00:08:30,280 --> 00:08:36,610
When coupled with attack prompts,
this test can detect if the LLM is

126
00:08:36,650 --> 00:08:39,540
leaking some private information.

127
00:08:40,000 --> 00:08:45,870
In this case, we're actually checking
is some password or maybe some SSH

128
00:08:45,910 --> 00:08:48,810
keys are present in the response.

129
00:08:49,180 --> 00:08:50,370
Next, we have.

130
00:08:51,075 --> 00:08:54,605
property based testing, this
checks that the answer of the

131
00:08:54,625 --> 00:08:57,115
system follows some properties.

132
00:08:57,465 --> 00:09:02,365
This approach is about testing the
properties of the output, not the

133
00:09:02,395 --> 00:09:06,219
exact output itself, instead of
checking if the answer is correct.

134
00:09:06,510 --> 00:09:11,510
Paris or Tokyo, that you test the
properties like whether the response

135
00:09:11,510 --> 00:09:14,640
is polite, factual, or on topic.

136
00:09:14,940 --> 00:09:19,110
In this case, for example, we are
checking that the answer has at

137
00:09:19,110 --> 00:09:24,310
least one of these polite answers
or keywords, in the response.

138
00:09:24,965 --> 00:09:29,215
Other kinds of properties we may
want to check is, for example, if the

139
00:09:29,215 --> 00:09:33,765
response is positive or negative, and
there are libraries that Python has,

140
00:09:33,815 --> 00:09:38,555
in this case, TextBlock, can detect if
the response is positive or negative,

141
00:09:38,835 --> 00:09:41,235
and we can build our checks on that.

142
00:09:41,315 --> 00:09:46,825
We also have a few metrics we can
use, for example, the BLEU score.

143
00:09:47,105 --> 00:09:52,655
This was developed by IBM to
evaluate machine translation.

144
00:09:53,320 --> 00:09:58,940
But this algorithm can also be used
to evaluate the quality of a generated

145
00:09:59,210 --> 00:10:01,460
text against a reference text.

146
00:10:01,820 --> 00:10:05,640
This metric was found to have
a high correlation with human

147
00:10:05,800 --> 00:10:10,880
judgment of quality, especially in
translation or summarization tasks.

148
00:10:11,600 --> 00:10:15,460
The blue score is a number
that goes between zero and one.

149
00:10:16,100 --> 00:10:20,800
The more similar the generated
answer is to the reference

150
00:10:20,840 --> 00:10:23,315
answer, The higher the number is.

151
00:10:23,735 --> 00:10:30,595
So to use BLEU, we ask the LM to generate
an answer based on a reference prompt and

152
00:10:30,595 --> 00:10:37,165
then compare the reference answer with
the generated answer and we get a number.

153
00:10:37,405 --> 00:10:42,005
We If the number is high enough
for needs, we pass the test,

154
00:10:42,145 --> 00:10:43,785
if not, it's going to fail.

155
00:10:44,065 --> 00:10:46,525
So here is a possible implementation.

156
00:10:46,525 --> 00:10:52,305
We have mocked the response
from the AI system, and we are

157
00:10:52,335 --> 00:10:54,265
supplying a reference response.

158
00:10:54,665 --> 00:10:57,945
This code splits the
sentences into n grams.

159
00:10:58,635 --> 00:11:04,295
with the NLTK library, on the most
part, each n gram is a word or a

160
00:11:04,295 --> 00:11:07,215
punctuation and also the suffixes.

161
00:11:07,225 --> 00:11:11,405
For example, in this case,
the weather today is sunny.

162
00:11:11,765 --> 00:11:15,735
We have the weather today is sunny.

163
00:11:16,135 --> 00:11:21,665
And dot, so we have six engrams for
the generator response and for the

164
00:11:21,735 --> 00:11:27,835
reference response we have today,
s, weather, is, sunny, and the dot.

165
00:11:27,915 --> 00:11:32,825
This example tokenizes the generator
response and the reference response

166
00:11:32,885 --> 00:11:38,225
and calculate the blue score to measure
how close these both answers are.

167
00:11:38,795 --> 00:11:44,055
When the texts are short we must
use a smoothing function to more

168
00:11:44,155 --> 00:11:46,245
accurately calculate the score.

169
00:11:46,365 --> 00:11:51,905
Another metric we can use is called Roche,
and this is a family of metrics that,

170
00:11:52,225 --> 00:11:58,145
like BLEU, were also developed to evaluate
text translation and summarization tasks.

171
00:11:58,395 --> 00:11:59,699
So here's another example.

172
00:11:59,840 --> 00:12:04,130
Example implementation of rooge,
we also have a mocked response

173
00:12:04,180 --> 00:12:05,810
and a reference response.

174
00:12:05,910 --> 00:12:11,880
The rooge package does the heavy work
behind the scenes and returns the scores.

175
00:12:12,020 --> 00:12:15,670
Here's the answer for these
two strings and you can see we

176
00:12:15,680 --> 00:12:18,350
have three kinds of metrics.

177
00:12:18,580 --> 00:12:21,870
One is our one n gram or one word.

178
00:12:22,565 --> 00:12:27,485
The other is over two n grams, and
the other is, over l n grams, where

179
00:12:27,525 --> 00:12:33,005
l is the longest sequence of words
that are shared between both answers.

180
00:12:33,975 --> 00:12:38,995
Each score has three metrics, or
three numbers, that are called recall.

181
00:12:39,250 --> 00:12:41,860
Precision and s1 score.

182
00:12:41,950 --> 00:12:47,550
So with precision, we calculate how
many elements in the generated summary

183
00:12:47,550 --> 00:12:49,720
were also in the reference summary.

184
00:12:50,110 --> 00:12:54,630
A higher precision means that the
content in the generated summary

185
00:12:54,680 --> 00:12:56,580
is relevant or appropriate.

186
00:12:56,810 --> 00:13:02,010
Then we have recall, where we evaluate
how well the summary covers the content

187
00:13:02,020 --> 00:13:04,990
that is deemed important in the reference.

188
00:13:05,050 --> 00:13:09,070
A higher recall score means
that the summary successfully

189
00:13:09,130 --> 00:13:13,180
captures more of the essential
aspects of the reference content.

190
00:13:13,380 --> 00:13:18,890
And then F1 score is called the harmonic
mean of the precision and recall.

191
00:13:19,470 --> 00:13:22,700
This is one number that
summarizes the other two metrics.

192
00:13:22,840 --> 00:13:26,170
The better the quality of the
answer, the higher the scores

193
00:13:26,300 --> 00:13:28,580
are, they go from 0 to 1.

194
00:13:28,740 --> 00:13:34,280
Blue focuses on precision, meaning
how many of the words in the generated

195
00:13:34,330 --> 00:13:37,260
answer appear in the reference answer.

196
00:13:37,350 --> 00:13:39,700
Roche, on the other
hand, focuses on recall.

197
00:13:40,640 --> 00:13:45,810
How much words in the human reference
appear in the generated answer.

198
00:13:45,860 --> 00:13:50,210
So we evaluate both strings,
starting from different sides.

199
00:13:50,610 --> 00:13:56,150
Adversarial testing means feeding
the LLM some malicious or misleading

200
00:13:56,170 --> 00:13:58,190
inputs to see how it reacts.

201
00:13:58,660 --> 00:14:01,200
Does it fall prey to injections?

202
00:14:01,725 --> 00:14:02,985
Can it be jailbroken?

203
00:14:03,435 --> 00:14:07,215
it's not about testing the
functionality, but pushing the

204
00:14:07,215 --> 00:14:13,395
limits of the model and exploring
the edge cases and vulnerabilities.

205
00:14:13,655 --> 00:14:19,155
For the final testing, we can reuse
the tools we have seen so far, but

206
00:14:19,185 --> 00:14:24,605
using different kinds of attack or
misleading prompts, we've already

207
00:14:24,635 --> 00:14:29,800
seen how to detect if they're
LLM responds with sensitive data.

208
00:14:29,840 --> 00:14:33,210
In this case, we're trying
some prompt injection.

209
00:14:33,840 --> 00:14:39,660
We can try injecting prompts into regular
looking text and testing the responses

210
00:14:39,700 --> 00:14:42,450
to see if the system fell into the trap.

211
00:14:42,890 --> 00:14:47,110
We can also use LLMs to
evaluate the output of the LLM.

212
00:14:47,550 --> 00:14:51,640
This technique is also sometimes
referred as auto evaluator testing.

213
00:14:52,070 --> 00:14:58,970
Here we're using an AI to evaluate
The response, the judge LLM, sometimes

214
00:14:59,000 --> 00:15:03,010
it's a different model, a more
powerful model, but it doesn't need

215
00:15:03,020 --> 00:15:07,450
to be, we can use usually the same
model to evaluate the responses.

216
00:15:07,920 --> 00:15:11,190
And this works because
there is an asymmetry.

217
00:15:11,855 --> 00:15:16,255
Inequality between generating an
answer and evaluating the answer.

218
00:15:16,725 --> 00:15:22,515
Assessing the quality of an answer is
more straightforward and simpler for

219
00:15:22,545 --> 00:15:25,365
the LLM than to generate a new answer.

220
00:15:26,255 --> 00:15:31,735
So this test method is very well suited
to evaluate any question answering

221
00:15:31,735 --> 00:15:34,109
system or conversational system.

222
00:15:34,110 --> 00:15:39,670
To evaluate an answer, we provide the
question, the prompt, and the generated

223
00:15:39,710 --> 00:15:45,380
answer, and ask the LLN to give
feedback in the form of an evaluation.

224
00:15:46,155 --> 00:15:52,005
Then we fail or pass the test based
on the answer of this judge LLM.

225
00:15:52,355 --> 00:15:54,235
I hope this part is readable.

226
00:15:55,175 --> 00:16:00,905
In general, experience shows that using
a continuous scoring system like a

227
00:16:00,905 --> 00:16:03,575
number from 0 to 10 is not very good.

228
00:16:04,330 --> 00:16:06,400
LLM gets confused.

229
00:16:06,430 --> 00:16:10,260
It's better to use a point
system or a star system.

230
00:16:10,700 --> 00:16:16,380
In this example, we are using a
basically telling the LLM to give one

231
00:16:16,440 --> 00:16:21,040
to four points or one to four stars to
the answer with one being the worst.

232
00:16:21,520 --> 00:16:26,210
terrible answer and for being an
excellent relevant and direct answer.

233
00:16:26,630 --> 00:16:31,980
And we are also asking the LLM to
provide some reasoning for his scoring.

234
00:16:32,230 --> 00:16:37,990
Another technique instead of using a
rank or a star system is to award points

235
00:16:38,060 --> 00:16:41,590
based on certain criteria in the answer.

236
00:16:41,630 --> 00:16:45,770
This also works very well and
the evaluations of the LLM

237
00:16:46,190 --> 00:16:50,760
We'll usually correlate very
tightly with human judgments.

238
00:16:51,060 --> 00:16:54,140
Now, we don't need to code
everything by ourselves.

239
00:16:54,260 --> 00:17:00,280
There are quite a few open source
tools to automate much of this testing.

240
00:17:00,510 --> 00:17:02,170
So let's see a few of them.

241
00:17:02,670 --> 00:17:07,510
One is called DeepEval, this
is an open source evaluation

242
00:17:07,540 --> 00:17:10,390
tool for LLMs and AI systems.

243
00:17:10,780 --> 00:17:16,220
It can run evaluation with a
dozen different metrics and it's

244
00:17:16,220 --> 00:17:20,980
meant to evaluate models, but it
can be adapted to AI applications

245
00:17:20,990 --> 00:17:22,620
that make heavy use of LLMs.

246
00:17:23,120 --> 00:17:27,850
Next, we have Ragas, a toolkit
to evaluate LLM applications.

247
00:17:28,170 --> 00:17:31,850
It can generate test
data sets, test prompts.

248
00:17:32,430 --> 00:17:37,990
It can run a traditional test
and LLM power test, like the ones

249
00:17:38,120 --> 00:17:43,320
we discussed earlier, and it can
integrate with LangChain or LlamaIndex.

250
00:17:43,710 --> 00:17:47,190
So if you are using one of these
frameworks, Ragas is a great

251
00:17:47,260 --> 00:17:49,490
addition to your bag of tricks.

252
00:17:50,190 --> 00:17:54,970
Deep checks is a tool to continually
monitor and validate language models.

253
00:17:55,130 --> 00:17:59,390
It's better suited when you are
in the business of training models

254
00:17:59,440 --> 00:18:04,790
or fine tuning models, instead of
building applications on top of them.

255
00:18:04,930 --> 00:18:07,110
Another nice tool is called TrueLens.

256
00:18:07,650 --> 00:18:12,790
is, tool to test LLM models, run
experiments and test applications,

257
00:18:13,070 --> 00:18:15,490
then run on top of these models.

258
00:18:15,900 --> 00:18:20,340
TrueLens is the one I have more
experience so far, and I can recommend

259
00:18:20,340 --> 00:18:22,110
it, because it's really easy to use.

260
00:18:22,460 --> 00:18:26,240
You can connect your application
and start logging actual

261
00:18:26,270 --> 00:18:28,100
interactions with your users.

262
00:18:28,500 --> 00:18:31,500
Then use this feedback to
evaluate the responses.

263
00:18:31,860 --> 00:18:32,850
Build a test data set.

264
00:18:33,680 --> 00:18:36,500
And run your test over each iteration.

265
00:18:37,390 --> 00:18:38,780
That's all for today.

266
00:18:38,850 --> 00:18:40,660
Thank you for reaching
the end of the talk.

267
00:18:41,150 --> 00:18:42,420
I hope you enjoyed it.

268
00:18:42,470 --> 00:18:45,370
And I hope you have a very
nice conference today.

269
00:18:46,060 --> 00:18:49,620
So if you want to reach
me, DMs are open here.

270
00:18:49,620 --> 00:18:56,410
My links in my homepage, you will find
links to talks and other blog posts.

271
00:18:56,730 --> 00:18:59,030
A lot of them related to LLMs.

272
00:18:59,365 --> 00:19:03,965
There's quite a lot of content,
related to CICD automation.

273
00:19:04,415 --> 00:19:08,065
So again, thank you for watching and
enjoy the rest of the conference.

