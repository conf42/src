1
00:00:00,300 --> 00:00:01,590
Hello, I'm Jo Mundi.

2
00:00:02,130 --> 00:00:06,140
I'm going to talk about reinforcement
learning and how to scale RHLF based

3
00:00:06,140 --> 00:00:09,720
systems for distributed learning
across multi GPU environment.

4
00:00:10,270 --> 00:00:13,795
There's a lot of improvements you
can see recently in the models either

5
00:00:13,895 --> 00:00:16,240
the Sonet models or the OPA models.

6
00:00:16,730 --> 00:00:23,070
Where they have done a lot of instruction
based RHLF based models which continuously

7
00:00:23,070 --> 00:00:24,570
improves the performance of these models.

8
00:00:24,570 --> 00:00:28,290
And, i'm going to talk about the
challenges there and also the, core

9
00:00:28,369 --> 00:00:32,169
capabilities that are being used to
build these platforms and systems.

10
00:00:32,669 --> 00:00:35,699
So the agenda to talk is, the,
a little bit summary about our

11
00:00:35,699 --> 00:00:39,159
reinforcement learning from theoretical
frameworks to production systems

12
00:00:39,559 --> 00:00:42,969
the foundations distributed rl what
are the challenges involved there?

13
00:00:43,954 --> 00:00:49,134
Then, the RHL principles and the,
practices that are normally applied in

14
00:00:49,134 --> 00:00:54,364
a production systems for training or for
inference the cloud native architectures.

15
00:00:54,435 --> 00:00:59,254
Mostly these systems are trained on
Kubernetes with multi GPU set up with

16
00:00:59,254 --> 00:01:00,815
proper networking using and will link.

17
00:01:01,625 --> 00:01:04,504
And I'll also talk about some of the
scaling frameworks that I've used.

18
00:01:04,504 --> 00:01:08,154
So mostly on Impala, really
lightning AI deep speed.

19
00:01:08,514 --> 00:01:11,939
Some of the modern frameworks
that are being utilized and the

20
00:01:11,939 --> 00:01:14,834
challenges of monitoring and
deploying these pipelines at at scale.

21
00:01:15,334 --> 00:01:20,439
So reinforcement learning the previous
version of RL based systems was it

22
00:01:20,559 --> 00:01:23,834
came from MDP, it's called markup
Decision Process, where it was

23
00:01:23,894 --> 00:01:27,894
more of a sequential based learning
algorithm where can maximize the

24
00:01:27,894 --> 00:01:29,604
cumulative reward at each step.

25
00:01:30,034 --> 00:01:32,064
But with deep parallel
deep neural networks.

26
00:01:32,064 --> 00:01:36,354
Now there transformer based networks
it can learn deep processing.

27
00:01:37,064 --> 00:01:41,589
Power how the input gets transformed
is a little, is more sophisticated

28
00:01:41,709 --> 00:01:45,169
with the transformer based networks
compared to just a sequential process

29
00:01:45,169 --> 00:01:49,339
in MDP, so it can learn much more
sophisticated functions and multimodal

30
00:01:49,759 --> 00:01:51,529
either be text image or video audio.

31
00:01:52,029 --> 00:01:55,209
So the foundations of distributed
reinforcement learning are there are some

32
00:01:55,239 --> 00:01:59,829
it can be either done in a multi-node,
multi pod system where, there'll be

33
00:01:59,829 --> 00:02:04,074
a main server which will accumulate
all the weights as the system learns

34
00:02:04,104 --> 00:02:05,779
throughout each epoch and each batch.

35
00:02:06,424 --> 00:02:08,254
The core challenges there are networking.

36
00:02:08,254 --> 00:02:11,494
How does the how does the workers,
where the learning is happening,

37
00:02:11,524 --> 00:02:14,554
how do they communicate along with
the servers, and how does the server

38
00:02:14,824 --> 00:02:19,084
accumulate all the weights to make sure
the weight of the entire network is is

39
00:02:19,134 --> 00:02:21,474
is updated after each learning event?

40
00:02:22,224 --> 00:02:25,949
Now the, there are different kinds of
loss functions and policy optimization

41
00:02:25,949 --> 00:02:27,179
algorithms that will be applied.

42
00:02:27,179 --> 00:02:30,209
DD like PPO or GRPO or DDPO.

43
00:02:30,630 --> 00:02:33,290
These are different kinds of
policies that are mainly applied.

44
00:02:33,290 --> 00:02:38,319
There's also actor, critic models
where where you learn from examples.

45
00:02:38,319 --> 00:02:41,649
And these examples are either hand
labeled or it can be sometimes

46
00:02:41,649 --> 00:02:43,329
generated by the machines itself.

47
00:02:43,619 --> 00:02:46,079
So that is, it becomes a
continuous learning feedback loop.

48
00:02:46,579 --> 00:02:48,499
So learning can happen in multiple ways.

49
00:02:48,499 --> 00:02:51,259
So one is the most common
is the from human feedback.

50
00:02:51,289 --> 00:02:54,779
And that there is that is called
supervised fine tuning, where

51
00:02:54,779 --> 00:02:58,354
you have a pre-trained model that
you train on a large cluster.

52
00:02:58,714 --> 00:03:01,774
But then on top of that, then you
have to fine tune based on your own

53
00:03:01,774 --> 00:03:05,959
instructions which are a basic set
of questions and answers for lms.

54
00:03:06,644 --> 00:03:10,094
For vision models, it can be
the image and the labels itself.

55
00:03:10,484 --> 00:03:14,524
So these are human labels and these are
created by experts in specific domains.

56
00:03:14,524 --> 00:03:18,334
For example, doctors or physicians
or math olympiads, they create

57
00:03:18,334 --> 00:03:20,024
this supervised the kid is very.

58
00:03:20,334 --> 00:03:23,724
High quality data set that can be used
for supervised fine tuning, and that

59
00:03:23,724 --> 00:03:25,074
improves the performance of these models.

60
00:03:25,434 --> 00:03:29,324
So to train this data at scale you have
to also make sure you have a reward

61
00:03:29,354 --> 00:03:33,109
model or reward function defined so
that the, so that the entire network

62
00:03:33,109 --> 00:03:34,639
can optimize for that your function.

63
00:03:34,689 --> 00:03:39,089
The policies that can, you can apply
for achieving the reward functions are

64
00:03:39,489 --> 00:03:41,049
like PPO is one of them, as I mentioned.

65
00:03:41,049 --> 00:03:47,310
There's also, DDPO and the GRPO and all
these policies depends on like some of

66
00:03:47,310 --> 00:03:50,750
the policies can do much better than
the others depending on the type of.

67
00:03:50,820 --> 00:03:53,060
The dataset and reward
functions you have defined.

68
00:03:53,570 --> 00:03:56,360
So you need to be very careful in
defining the reward functions because

69
00:03:56,360 --> 00:03:57,640
you there are challenges involved.

70
00:03:57,640 --> 00:03:58,810
There are reward hacking.

71
00:03:58,860 --> 00:04:02,669
If the functions are not defined
properly, the model might try to do

72
00:04:02,719 --> 00:04:08,240
if you can hack the reward function
and it might lead to un consequences,

73
00:04:08,240 --> 00:04:10,240
which are un which are not desirable.

74
00:04:10,740 --> 00:04:12,840
So distributed implementation of RHLF.

75
00:04:12,840 --> 00:04:17,304
There, there is there is one, one
of them most of them are, did,

76
00:04:17,635 --> 00:04:23,224
are trained on different kinds of
large Kubernetes clusters and their

77
00:04:23,224 --> 00:04:24,244
different kinds of frameworks.

78
00:04:24,304 --> 00:04:26,914
Each of them has their own
advantages and disadvantages.

79
00:04:27,734 --> 00:04:29,405
But the most common is data.

80
00:04:29,405 --> 00:04:32,994
The data parallelism and model
parallelism, data parallelism meaning your

81
00:04:32,994 --> 00:04:37,429
model, same model, which is is distributed
across all the pods in the cluster.

82
00:04:37,929 --> 00:04:40,009
And the data it is very large size.

83
00:04:40,009 --> 00:04:43,669
It gets split across those nodes and
the learning happens in each, the

84
00:04:43,669 --> 00:04:47,659
individual pods which are then accumulated
and the, and that the workers, which

85
00:04:47,659 --> 00:04:51,829
learns after each batch, it sends that
weights back to the server, and the

86
00:04:51,829 --> 00:04:55,014
server accumulates those weights and
the neur network weights are updated.

87
00:04:55,944 --> 00:04:57,414
And it's sent back to the workers.

88
00:04:57,784 --> 00:04:59,969
There, there is a grad, the
accommodation strategies.

89
00:04:59,969 --> 00:05:02,669
There are various kind of accommodation
strategies that can happen on the server

90
00:05:02,669 --> 00:05:06,680
pod which is the main pod, which is
monitoring all all the training and

91
00:05:06,680 --> 00:05:08,740
learning across all these workout pods.

92
00:05:09,640 --> 00:05:12,739
And you also have to make sure the
models are checkpoint because some

93
00:05:12,739 --> 00:05:16,494
of this training can take a long
time to to, to train weeks to train.

94
00:05:16,494 --> 00:05:21,664
And sometimes you don't want to
restart the training after after

95
00:05:21,664 --> 00:05:23,494
let's say 50% of the training is done.

96
00:05:23,824 --> 00:05:28,104
You want to make sure it, it can restart
from that checkpoints so that it can, like

97
00:05:28,104 --> 00:05:31,524
in minimize the compute resource that is
needed for the entire training process.

98
00:05:32,164 --> 00:05:34,504
Human feedback collection is
one of the other challenges.

99
00:05:34,554 --> 00:05:35,874
We have like human feedback.

100
00:05:35,974 --> 00:05:39,204
Sometimes sometimes the
feedback loop can be very long.

101
00:05:39,574 --> 00:05:43,064
So depending on the use case some
feedback loops are faster to collect.

102
00:05:43,064 --> 00:05:45,824
Some feedbacks can take
up to months to collect.

103
00:05:46,154 --> 00:05:49,684
So normally in, generally there is
a lot of companies like scale ai.

104
00:05:50,084 --> 00:05:54,179
And know Marco and few others
which helps creating this kind of

105
00:05:54,179 --> 00:05:55,859
labels based on human feedback.

106
00:05:56,279 --> 00:06:00,529
And if you create millions of examples
these examples then becomes a data set

107
00:06:00,569 --> 00:06:02,909
for training RHLF models at school.

108
00:06:03,329 --> 00:06:05,999
And you can control the
quality and also make sure.

109
00:06:06,449 --> 00:06:10,349
The, the evolution systems that we
have are in line with the controls

110
00:06:10,349 --> 00:06:13,349
and the quality that you want
to achieve after the training.

111
00:06:13,629 --> 00:06:16,209
So that when it goes to inference
it can really highly perform

112
00:06:16,209 --> 00:06:17,409
compared to the previous models.

113
00:06:17,909 --> 00:06:21,789
The, there are different kinds of cloud
native architectures for distributor.

114
00:06:22,129 --> 00:06:23,709
There I'll talk more about it.

115
00:06:23,829 --> 00:06:25,549
There are different kinds of
frameworks that are there.

116
00:06:25,919 --> 00:06:30,109
But most of this is run on top of
Kubernetes orchestrating all the pods.

117
00:06:30,499 --> 00:06:34,424
Of the resource management, the
compute management fail word recovery

118
00:06:34,484 --> 00:06:38,354
based on the checkpoints replay
buffers based on that actor, critic

119
00:06:38,404 --> 00:06:42,359
patterning the stateful management
of, of memory as well as the data.

120
00:06:43,074 --> 00:06:46,619
Which is the, the, it can be in, in
terms of sometimes in terms of terabytes.

121
00:06:46,919 --> 00:06:50,749
So all this state management has to be
done and in the proper way so that it

122
00:06:50,749 --> 00:06:56,139
can restart or it can communicate the
states across all these training pods for

123
00:06:56,139 --> 00:06:58,624
a consistent and reliable training run.

124
00:06:59,124 --> 00:07:01,014
Network optimization is
another very important thing.

125
00:07:01,064 --> 00:07:04,274
Your network needs to be really
optimized for this multi GPU training.

126
00:07:04,694 --> 00:07:09,894
And NVIDIA has NNV link, which is really
provides a highly optimal network.

127
00:07:10,064 --> 00:07:13,294
So that, the multi GPU
communications can be done at scale.

128
00:07:13,784 --> 00:07:16,034
And there a lot of improvements
that are going on for low latency

129
00:07:16,034 --> 00:07:17,384
communication across multi GPUs.

130
00:07:17,859 --> 00:07:21,790
So that it can it can reduce down the
compute time when the data and the

131
00:07:21,790 --> 00:07:24,239
state gets passed across these pods.

132
00:07:24,739 --> 00:07:26,119
There are different kinds of frameworks.

133
00:07:26,175 --> 00:07:28,125
Impala is one of them, really is another.

134
00:07:28,125 --> 00:07:29,265
Deep speech is another.

135
00:07:29,625 --> 00:07:32,015
So there are different frameworks
available, and each of these

136
00:07:32,015 --> 00:07:33,644
has their own pros and cons.

137
00:07:33,644 --> 00:07:37,144
And depending on the use case, you can
decide what framework you want to use.

138
00:07:37,514 --> 00:07:43,184
For multi gpu RL based trainings really
is used if you have it gives a lot

139
00:07:43,184 --> 00:07:44,774
of advantages of of the components.

140
00:07:44,774 --> 00:07:48,114
There's some really available components
and metrics are, it gives you the

141
00:07:48,114 --> 00:07:51,364
metrics of how the training is going
like quickly out, out of the box.

142
00:07:51,854 --> 00:07:56,269
DP another one, which is normally used for
very large scale transformer know networks

143
00:07:56,269 --> 00:07:58,609
when you cannot fit the model in one.

144
00:07:58,925 --> 00:08:00,650
GP like the model needs to be distributed.

145
00:08:00,935 --> 00:08:03,515
The model itself needs to be
distributed across multiple GPUs.

146
00:08:03,515 --> 00:08:06,915
The learning happens across
multiple pods and not just one pod.

147
00:08:07,275 --> 00:08:09,825
So where for very large
networks, you cannot have the

148
00:08:09,825 --> 00:08:11,715
entire model load in one pod.

149
00:08:11,955 --> 00:08:15,635
In that case, you need to have the model
except distributed across multiple pods.

150
00:08:15,965 --> 00:08:21,065
And the learning happens in a distributed
way across all these all these pods from

151
00:08:21,155 --> 00:08:24,155
parts of the, from one part of the network
to another part of the neural network.

152
00:08:24,655 --> 00:08:28,940
This is an example of so gpu, she
link Kate themselves, don't buy

153
00:08:28,940 --> 00:08:30,830
natively, do the GPU shell link.

154
00:08:30,880 --> 00:08:34,130
Nvidia has device plugins, so
you need to install them as part

155
00:08:34,130 --> 00:08:35,769
of as part of the pod setup.

156
00:08:36,275 --> 00:08:40,184
And you can say, okay, how many pods,
how many gps do you want to in one pod?

157
00:08:40,184 --> 00:08:43,194
Normally four is the number, but you
can also go up to eight depending

158
00:08:43,194 --> 00:08:44,524
on the resources that you have.

159
00:08:44,964 --> 00:08:49,454
And the distributed training frameworks
where the training is actually happening.

160
00:08:49,675 --> 00:08:52,764
There are some of the things that
all they mentioned this is there.

161
00:08:53,724 --> 00:08:55,435
DDP is there for data parallelism.

162
00:08:55,805 --> 00:08:57,694
There's deep speed,
which is from Microsoft.

163
00:08:57,694 --> 00:08:59,615
There's Ray Train,
which is from any scale.

164
00:09:00,209 --> 00:09:05,140
And Q Flow also provides toppy jobs
and, TF job operators where you can

165
00:09:05,140 --> 00:09:06,630
use them for distributed training.

166
00:09:07,390 --> 00:09:10,510
The most important thing is also when
you're doing multi GPO training is

167
00:09:10,515 --> 00:09:14,755
the, is the communication for a fast
communication you need to use Nvidia the

168
00:09:14,755 --> 00:09:19,275
NCCL package to make sure the network
is fully optimized for skill training.

169
00:09:19,775 --> 00:09:24,880
So production, deployment strategies like
we need to monitor all these various GPU

170
00:09:24,880 --> 00:09:29,550
usage because some of the GPUs the, if
the data is not properly distributed,

171
00:09:29,550 --> 00:09:32,290
some of the GPUs can really underperform.

172
00:09:32,560 --> 00:09:34,990
And so you need to have like
very close monitoring tools.

173
00:09:34,990 --> 00:09:36,790
They can, you can inter
integrate with any.

174
00:09:37,450 --> 00:09:40,600
Platforms like Datadog and things
like that to understand the GPU uses.

175
00:09:41,000 --> 00:09:44,120
And this meet some trial and error
on understanding what is the data,

176
00:09:44,360 --> 00:09:48,290
right distribution and shorting of
the data that needs to be done so

177
00:09:48,290 --> 00:09:52,340
that now all the GPUs are maximally
utilized and there's no there's no

178
00:09:52,340 --> 00:09:56,245
like imbalance or high imbalance in
terms of, utilization of the GPUs when

179
00:09:56,245 --> 00:09:57,385
you're doing a distributor training.

180
00:09:58,075 --> 00:09:59,910
There are things like, safety mechanisms.

181
00:09:59,910 --> 00:10:02,720
So circuit breaker patterns
for unstable policies.

182
00:10:02,720 --> 00:10:06,550
If there are pots that goes down,
you need to have failover and,

183
00:10:06,550 --> 00:10:10,884
high reliable like robust failover
mechanisms so that if a port goes down.

184
00:10:11,389 --> 00:10:13,069
It does not break the entire training.

185
00:10:13,129 --> 00:10:16,000
It can recover from the specific
checkpoint to it failed, and

186
00:10:16,000 --> 00:10:17,290
it can take it up from there.

187
00:10:17,290 --> 00:10:20,959
And then the entire training across
the cluster can still continue.

188
00:10:21,569 --> 00:10:25,574
So it needs very optimal it's a
monitoring in terms of all the

189
00:10:25,574 --> 00:10:30,274
utilization across G-P-U-C-P-U
your network across the cluster.

190
00:10:30,764 --> 00:10:34,334
So that you have a good understanding
about ization across when the training

191
00:10:34,334 --> 00:10:38,014
is happening, and you can optimize based
on bottlenecks that you did that you see

192
00:10:38,534 --> 00:10:40,604
the bottlenecks can be of various types.

193
00:10:40,634 --> 00:10:44,839
It can be either network, it can be the
data or sometimes it can be like the model

194
00:10:44,839 --> 00:10:47,074
itself is the, it is learning slowly.

195
00:10:47,074 --> 00:10:50,994
So you have to make sure the the
learning parameters are set up

196
00:10:50,994 --> 00:10:52,304
in optimal way and you have to.

197
00:10:52,530 --> 00:10:56,160
Test and tune that multiple times
to, to get to optimal learning

198
00:10:56,260 --> 00:10:57,970
learning for the entire model.

199
00:10:58,490 --> 00:11:02,345
So the loss can be the loss and the, the
loss function that you're applying or the

200
00:11:02,450 --> 00:11:06,450
policies that you're applying are really
like improving the learn, improving the

201
00:11:06,450 --> 00:11:08,340
learning performance of the entire model.

202
00:11:09,000 --> 00:11:11,460
So there are a lot of things like
learning rate parameters, and all those

203
00:11:11,460 --> 00:11:14,440
things that also needs to be tuned
depending on the type of data and the

204
00:11:14,440 --> 00:11:15,780
type of way cluster that you have.

205
00:11:16,280 --> 00:11:19,590
So case studies for rf there
is most common use cases

206
00:11:19,590 --> 00:11:20,550
are large language models.

207
00:11:20,550 --> 00:11:23,500
Which are commonly being
trained using label data.

208
00:11:23,530 --> 00:11:26,650
There's SFT, which is, I talked about
supervised, fine tuning, but also there

209
00:11:26,650 --> 00:11:29,860
is, there now new techniques coming
up, which are like reinforcement based.

210
00:11:29,910 --> 00:11:30,540
Fine tuning.

211
00:11:31,060 --> 00:11:34,600
And that actually can really speed
up the training because it's very

212
00:11:34,600 --> 00:11:37,810
difficult to get labeled data from
experts like, doctors, physicians, or

213
00:11:37,810 --> 00:11:41,340
olympiads or mathematic mathematical
olympiads or cist olympiads.

214
00:11:41,770 --> 00:11:43,090
To get labeled data takes a long time.

215
00:11:43,605 --> 00:11:48,095
If the system can, if you can define
reward functions and if you can

216
00:11:48,095 --> 00:11:52,245
define property or function, the
system, the network can itself come

217
00:11:52,245 --> 00:11:56,235
up with new kinds of la labeled data
that can be fed back to the model.

218
00:11:56,445 --> 00:11:59,375
So in that case, you don't need
a very large set of labeled data.

219
00:11:59,375 --> 00:12:03,185
You can have an initial seed and that
model can learn from that initial seed.

220
00:12:03,460 --> 00:12:06,460
And they can they can generate new
kinds of data sets itself as part of

221
00:12:06,465 --> 00:12:09,730
a training and that can improve our,
improve the speed of your training

222
00:12:09,730 --> 00:12:13,250
and you don't have to wait for long
cycle of getting human feedback data.

223
00:12:14,225 --> 00:12:17,375
So large linguist models is the most
common use case is also use cases

224
00:12:17,375 --> 00:12:20,740
for autonomous vehicles and robotic
applications where these data is being

225
00:12:20,740 --> 00:12:22,710
collected for robotics applications.

226
00:12:22,710 --> 00:12:25,470
A lot of challenges in
collecting data of images.

227
00:12:25,470 --> 00:12:30,785
But as you collect multiple various
kinds of data sets the model becomes

228
00:12:30,785 --> 00:12:32,985
more robust to new use cases.

229
00:12:33,485 --> 00:12:36,125
The challenges I'll talk about is
the human feedback, scalability

230
00:12:36,125 --> 00:12:37,685
human feedback is the golden dataset.

231
00:12:37,685 --> 00:12:41,435
So if you can get that data quickly
on and in a reliable way, in a very

232
00:12:41,435 --> 00:12:43,175
high, that's most high equity data.

233
00:12:43,575 --> 00:12:44,595
But that's the challenge.

234
00:12:44,595 --> 00:12:48,115
I think the next two, three years will
be needed to make sure the human data is

235
00:12:48,115 --> 00:12:52,595
collected across very special tasks that
the model can learn and specialize on.

236
00:12:53,065 --> 00:12:55,525
So that, the quality of the
model can increase over time.

237
00:12:56,015 --> 00:12:59,500
Communication overhead is, how do you
make sure the policy parameters or

238
00:12:59,550 --> 00:13:04,565
the the weight parameters are shared
across the network in a reliable way.

239
00:13:04,615 --> 00:13:08,535
In a in a quick, in a fast way,
so the coordination becomes safe.

240
00:13:08,880 --> 00:13:12,650
It needs a very high network bandwidth
to make sure the coordinate coordination

241
00:13:12,650 --> 00:13:16,210
of this as the model is training, the
coordination of all this state stateful

242
00:13:16,210 --> 00:13:20,200
data needs to happen in a very quick
way as that can become the bottleneck

243
00:13:20,200 --> 00:13:23,155
of for your entire cluster and can
increase the cost of your computing.

244
00:13:23,655 --> 00:13:25,785
Then there are other challenges
like temporal dynamics.

245
00:13:25,795 --> 00:13:30,735
How do you catch how do you make sure your
temporal dynamics in terms of the changing

246
00:13:30,735 --> 00:13:32,325
preferences are caught in the network?

247
00:13:32,325 --> 00:13:35,535
So that de that depends on the
policy that we're trying to build.

248
00:13:36,025 --> 00:13:37,135
And also the loss functions.

249
00:13:37,405 --> 00:13:39,195
It depends on the network
architecture as well.

250
00:13:40,190 --> 00:13:43,130
Then there are consistency grantees
and bias and representation, which

251
00:13:43,370 --> 00:13:47,955
talks about like how do we make
sure this the model is trained?

252
00:13:48,195 --> 00:13:50,685
When it is trained, it gives
consistent results and does not,

253
00:13:50,735 --> 00:13:55,055
give very highly like the certainty
of the results should be within some

254
00:13:55,055 --> 00:13:56,895
bands so that each time you train.

255
00:13:57,530 --> 00:13:58,310
It should improve.

256
00:13:58,400 --> 00:14:00,920
'cause if you have inconsistency,
then you cannot trust the

257
00:14:00,980 --> 00:14:02,310
model every time you train.

258
00:14:02,310 --> 00:14:05,770
So you have to make sure there's
consistency and kind of the data

259
00:14:05,770 --> 00:14:09,800
availability, the know the time,
compute time that is needed to run.

260
00:14:10,200 --> 00:14:13,020
And consistently in terms of the
model performance itself when you're

261
00:14:13,020 --> 00:14:16,590
tr training it multiple times, you
should give the same results close to.

262
00:14:16,620 --> 00:14:19,680
It might not be exactly same because
it's an un deterministic system,

263
00:14:19,680 --> 00:14:21,150
but it should be close enough.

264
00:14:21,700 --> 00:14:25,080
The emerging trends there is also
a new way of doing RHLF, which is

265
00:14:25,080 --> 00:14:28,675
federated based RHLF, federated based
systems, where that means the learning

266
00:14:28,675 --> 00:14:32,565
can happen on the edge itself instead
of on a cluster in a Kubernetes pod.

267
00:14:32,955 --> 00:14:35,055
You can also do learning directly.

268
00:14:35,445 --> 00:14:38,565
On the H devices it can be, there
can be millions of H devices

269
00:14:38,565 --> 00:14:42,385
where there can be the data gets
collected and the training happens.

270
00:14:42,415 --> 00:14:44,360
Some of the training can happen
directly on the, on, on the

271
00:14:44,360 --> 00:14:46,100
devices for privacy preservation.

272
00:14:46,430 --> 00:14:48,400
And then the devices then sync.

273
00:14:48,430 --> 00:14:51,555
They are learning the weights
to the, to a central server.

274
00:14:52,160 --> 00:14:56,690
Accumulates all the learning parameters,
and then it merges it and then it can

275
00:14:56,690 --> 00:15:01,240
send it back to devices so that so that
you don't have to need to have a like

276
00:15:01,240 --> 00:15:02,830
very large training every time there.

277
00:15:02,830 --> 00:15:05,930
Some of these continuous learning can
happen on the devices, on the edge itself.

278
00:15:05,930 --> 00:15:07,980
So that's a feder learning approach.

279
00:15:07,980 --> 00:15:10,270
There's a lot of research going
on, on, on that area itself.

280
00:15:10,770 --> 00:15:14,020
Then there's multimodal which
is like image, audio your text

281
00:15:14,025 --> 00:15:15,435
video and all those other things.

282
00:15:15,715 --> 00:15:17,935
There's automated feedback
based learning where you can

283
00:15:17,935 --> 00:15:19,375
collect the data automatically.

284
00:15:19,425 --> 00:15:20,475
And there is a lot of promise.

285
00:15:20,475 --> 00:15:24,475
A lot of companies are seeing where
the feedbacks are collected within the

286
00:15:24,475 --> 00:15:26,455
product itself and either good or bad.

287
00:15:26,455 --> 00:15:30,025
And that can be used and those labels
that are collected from the product itself

288
00:15:30,025 --> 00:15:32,815
are then used either as a part of the.

289
00:15:33,290 --> 00:15:38,250
Training process while you're doing
fine tuning with with your instruction

290
00:15:38,530 --> 00:15:42,610
instruction based fine tuning with
RHLF, or some people use that as

291
00:15:42,610 --> 00:15:44,530
part of the rag application itself.

292
00:15:44,530 --> 00:15:47,670
Where those feedbacks are are
used as part of the prompt.

293
00:15:47,670 --> 00:15:52,525
And then the, when they call the the lms
those feedbacks are given as references

294
00:15:52,525 --> 00:15:53,695
for chain of thought reasoning.

295
00:15:54,195 --> 00:15:56,935
So the path forward there is
still a lot of work to do.

296
00:15:56,935 --> 00:16:00,475
I think the most biggest the
way to do know, do distributor.

297
00:16:01,165 --> 00:16:05,195
There are a lot of frameworks out there,
but most of challenging part I think

298
00:16:05,195 --> 00:16:09,805
is right now the data collection part
from researchers, engineers like coders

299
00:16:10,195 --> 00:16:13,815
or doctors as physicians or teachers.

300
00:16:14,185 --> 00:16:16,145
Or any professional, any domain.

301
00:16:16,385 --> 00:16:19,865
So we need lots of data, I think high
quality data that needs to be collected

302
00:16:19,865 --> 00:16:23,785
so that the model can continuously
can be trained with reinforcement

303
00:16:23,865 --> 00:16:25,415
learning with human feedback.

304
00:16:25,835 --> 00:16:28,975
And this feedback data are
specialized data gets collected

305
00:16:28,975 --> 00:16:30,475
so that, getting that at scale.

306
00:16:30,910 --> 00:16:35,350
Across all the people in the world
let's say 1 billion people that

307
00:16:35,350 --> 00:16:39,700
can really improve the new versions
of models that are coming out now.

308
00:16:40,200 --> 00:16:41,700
That might not be always possible to get.

309
00:16:41,700 --> 00:16:45,850
So that's why I think the reinforcement
based fine tuning might be might be

310
00:16:45,850 --> 00:16:50,520
one of the new ways where where with
custom reward functions can be the new

311
00:16:50,520 --> 00:16:54,540
way, where the model itself generates
new kinds of data and hypothesis

312
00:16:54,540 --> 00:16:55,710
that can be used for training.

313
00:16:55,710 --> 00:16:59,160
But there's, that's a lot of research
going on this area as well right now.

314
00:16:59,890 --> 00:17:00,370
So that's it.

315
00:17:00,460 --> 00:17:01,910
Thank you so much for listening to me.

316
00:17:01,960 --> 00:17:03,590
I would love to get
any questions and yeah.

317
00:17:03,590 --> 00:17:06,265
And answer them as needed
based on my experience so far.

318
00:17:06,665 --> 00:17:06,905
Thank you

