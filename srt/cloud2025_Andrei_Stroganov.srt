1
00:00:00,010 --> 00:00:02,060
Hello, I'm Andrey Stroganov.

2
00:00:02,550 --> 00:00:07,060
Thank you for joining me in my talk
about performance and scalability

3
00:00:07,060 --> 00:00:11,809
challenges to reassemble classifiers
on heterogeneous platforms.

4
00:00:12,170 --> 00:00:16,129
Specifically, we will be talking
about heterogeneous CPUs.

5
00:00:16,710 --> 00:00:22,030
Those are the CPUs that we use
every day in our mobile devices.

6
00:00:22,770 --> 00:00:26,830
We use Big Little Architecture,
which combines high performance

7
00:00:26,830 --> 00:00:31,389
cores with energy efficient
cores within a single processor.

8
00:00:31,859 --> 00:00:37,433
Initially, this technology came
from a mobile market where power

9
00:00:37,433 --> 00:00:43,255
efficiency is crucial, and now
heterogeneous CPUs are gradually making

10
00:00:43,255 --> 00:00:45,585
their way into the server segment.

11
00:00:45,585 --> 00:00:51,584
Heterogeneous CPUs can dynamically
allocate workloads to the appropriate

12
00:00:51,584 --> 00:00:58,334
cores, their applications can range from
lightweight microservices to resource

13
00:00:58,334 --> 00:01:04,585
intensive data processing tasks, and they
are an attractive option for data centers.

14
00:01:04,605 --> 00:01:09,731
Transcripts can be used to access
any of the following formats.

15
00:01:09,731 --> 00:01:14,392
We will experiment with two popular
machine learning models, random

16
00:01:14,392 --> 00:01:17,894
forest and gradient boosting.

17
00:01:18,334 --> 00:01:23,534
Both of them use decision trees under
the hood, so let's do a quick recap.

18
00:01:24,344 --> 00:01:29,034
A decision tree is a tree like
structure which makes the decisions.

19
00:01:29,399 --> 00:01:31,669
Based on, feature values.

20
00:01:32,189 --> 00:01:37,759
Here we see a small decision tree
with three features, humidity,

21
00:01:37,819 --> 00:01:39,259
temperature, and wind speed.

22
00:01:39,759 --> 00:01:44,309
And in each node it has a
feature and its splitting value.

23
00:01:44,759 --> 00:01:54,669
if humidity is about 90 percent we
go, right and if let's say temperature

24
00:01:54,669 --> 00:01:57,479
is about 15 degrees, we'll go left.

25
00:01:57,749 --> 00:02:00,759
And the prediction is that it is raining.

26
00:02:01,309 --> 00:02:05,389
in practice, the decision
trees are quite larger.

27
00:02:05,399 --> 00:02:11,669
They use more features, but a single
decision tree is a weak predictor.

28
00:02:12,259 --> 00:02:16,679
So we usually, utilize the ensembles.

29
00:02:17,179 --> 00:02:22,669
for instance, we can take multiple
trees and train each of them on a

30
00:02:22,669 --> 00:02:26,759
random subset of training data set.

31
00:02:27,359 --> 00:02:30,579
The resulting structure
will be a random forest.

32
00:02:31,079 --> 00:02:37,314
Another approach is to train a
sequence of decision trees so that Each

33
00:02:37,364 --> 00:02:43,954
sequential tree would try to compensate
prediction inaccuracy of previous trees.

34
00:02:44,544 --> 00:02:47,854
this structure would be a
gradient boosting machine.

35
00:02:48,754 --> 00:02:51,614
Both of them are widely popular.

36
00:02:51,664 --> 00:02:59,704
They have Implementations in such great
libraries, as ONNX, xg, boost and Live

37
00:02:59,704 --> 00:03:07,554
GBM, but due to their branch in nature,
they are usually processed on CPUs.

38
00:03:07,674 --> 00:03:12,354
We don't use NPUs or GPUs for them.

39
00:03:13,254 --> 00:03:18,324
there might be some obstacles
when we port these algorithms

40
00:03:18,774 --> 00:03:20,934
into heterogeneous setting.

41
00:03:21,434 --> 00:03:24,164
And these are our experimental settings.

42
00:03:24,664 --> 00:03:30,984
We will use weather forecasting data set
with 21 numerical features, an octa core

43
00:03:30,984 --> 00:03:38,674
CPU with one very fast core, three slower
cores, and four energy efficient cores.

44
00:03:39,444 --> 00:03:45,779
Our models will have up to 100
decision trees, and they will

45
00:03:45,789 --> 00:03:49,739
be processed using ONNX Runtime.

46
00:03:50,239 --> 00:03:56,629
Here we see the stock performance of
Random Forest using several threads.

47
00:03:57,129 --> 00:04:05,069
On X axis we see the data size,
and 1 denotes 1 million samples.

48
00:04:05,779 --> 00:04:10,469
On Y axis we see the
inference time in seconds.

49
00:04:10,969 --> 00:04:16,619
Blue line here is the performance
of a single thread algorithm.

50
00:04:17,309 --> 00:04:22,119
If we add additional thread,
we got much better performance.

51
00:04:22,959 --> 00:04:25,359
So let's add another two threads.

52
00:04:25,859 --> 00:04:30,339
This one here is the result of
predicting using four threads.

53
00:04:30,829 --> 00:04:33,109
We again improved performance.

54
00:04:33,949 --> 00:04:38,769
Adding two additional threads,
we get the best performance.

55
00:04:39,269 --> 00:04:44,209
But if we try to add a couple
more threads, we see that

56
00:04:44,709 --> 00:04:47,469
performance gets worse.

57
00:04:47,969 --> 00:04:50,839
And this is a concurrency penalty.

58
00:04:51,149 --> 00:04:56,679
So the problem is that we can't
be sure how, how many threads we

59
00:04:56,679 --> 00:04:59,799
should take for a good performance.

60
00:05:00,299 --> 00:05:04,259
And these are results for gradient
boosting in stock performance.

61
00:05:04,759 --> 00:05:07,189
We see that the problem stays.

62
00:05:07,879 --> 00:05:13,919
This one is single thread
performance, and if we add additional

63
00:05:13,929 --> 00:05:16,429
thread, we get best performance.

64
00:05:16,949 --> 00:05:21,909
Adding more threads results
in worse performance with huge

65
00:05:21,929 --> 00:05:24,759
concurrency penalty for using.

66
00:05:25,259 --> 00:05:32,349
And, let's note that we got very
small performance boost here, 1.

67
00:05:32,419 --> 00:05:33,259
2 times.

68
00:05:33,759 --> 00:05:39,699
In order to improve performance, let's
see how usually random forest and tree

69
00:05:39,700 --> 00:05:42,769
ensemble classifiers are multithreaded.

70
00:05:43,439 --> 00:05:49,369
If we have enough trees, we can
process each tree in a separate thread.

71
00:05:49,749 --> 00:05:57,109
For instance, if we have 9 trees and 4
available threads, then we can take 4

72
00:05:57,129 --> 00:06:03,209
trees, process them in parallel, take
additional 4 trees and process them,

73
00:06:03,479 --> 00:06:06,519
and then we will have only 3 trees.

74
00:06:07,019 --> 00:06:12,789
And so we will use second approach, when
we split dataset into several batches,

75
00:06:13,369 --> 00:06:16,094
and each batch is processed in parallel.

76
00:06:16,524 --> 00:06:22,984
in different thread using a decision
forest consisting of three trees,

77
00:06:23,654 --> 00:06:29,664
but both these approaches imply
that our CPU cores are about equal,

78
00:06:30,084 --> 00:06:32,174
but in heterogeneous setting.

79
00:06:32,794 --> 00:06:36,054
It is not entirely true here.

80
00:06:36,054 --> 00:06:43,074
What can happen if we run equally
demanding tasks on different course?

81
00:06:43,534 --> 00:06:47,604
So let's see this setting.

82
00:06:48,034 --> 00:06:51,454
We have four course in our CPU core.

83
00:06:51,524 --> 00:07:00,334
A is fastest and core B. is two times
slower, and we have two slowest cores, C

84
00:07:00,444 --> 00:07:09,534
and D. And if we put, equally demanding
tasks on A, on these cores, then we see

85
00:07:09,564 --> 00:07:12,994
that core A finishes its job very fast.

86
00:07:13,659 --> 00:07:19,729
And we'll do nothing and core
B will finish a bit later

87
00:07:20,069 --> 00:07:21,809
and also will be idling.

88
00:07:22,419 --> 00:07:29,189
And we will have to wait for cores
C and D for finishing their tasks.

89
00:07:29,689 --> 00:07:37,069
And there is more realistic scenario
in case 2 when cores help each other,

90
00:07:37,349 --> 00:07:45,309
so when core A. finished its task
one, it took the unfinished part of

91
00:07:45,369 --> 00:07:51,779
task three from core C, and then it
took unfinished part from core D.

92
00:07:52,279 --> 00:07:58,629
This schedule is more compact, but
we still see that some cores idling.

93
00:07:59,129 --> 00:08:01,139
So can we do better?

94
00:08:01,639 --> 00:08:02,429
Yes we can.

95
00:08:02,789 --> 00:08:03,409
We can use.

96
00:08:03,859 --> 00:08:11,299
A worker pool pattern, when we split
our data set into very small batches,

97
00:08:11,619 --> 00:08:19,479
the size of the batch should be,
so small so that, the slowest core

98
00:08:19,489 --> 00:08:21,999
would process it in reasonable time.

99
00:08:22,499 --> 00:08:26,659
And we use, several
workers in several threads.

100
00:08:27,539 --> 00:08:34,659
Each worker takes the batch,
processes it, and requests next batch.

101
00:08:35,159 --> 00:08:41,669
Here we see that core A took batch
1, core B took batch 2, core C took

102
00:08:41,759 --> 00:08:44,359
batch 3, and core D took batch 4.

103
00:08:45,009 --> 00:08:51,694
When core A finished processing
batch one, it took batch five,

104
00:08:51,994 --> 00:08:58,234
then it took batch six, by that
time core B finished batch two and

105
00:08:58,314 --> 00:09:01,514
requested next available batch seven.

106
00:09:02,274 --> 00:09:05,594
This schedule is far more compact.

107
00:09:06,094 --> 00:09:12,534
And these are the results of using
worker pool pattern in random forest.

108
00:09:13,034 --> 00:09:17,624
We see that adding threads
results in better performance.

109
00:09:18,004 --> 00:09:20,624
So this is single thread performance.

110
00:09:21,254 --> 00:09:23,654
This is two thread performance.

111
00:09:24,424 --> 00:09:25,724
Four thread performance.

112
00:09:26,054 --> 00:09:30,964
And this is six, seven and
eight thread performance.

113
00:09:31,244 --> 00:09:37,904
We see that system couldn't give us
more processing power, so in, all these

114
00:09:37,914 --> 00:09:46,054
three cases, we get, same performance,
but there is no concurrency penalty here

115
00:09:46,384 --> 00:09:49,134
and we have better performance overall.

116
00:09:49,634 --> 00:09:52,204
These are results for gradient boosting.

117
00:09:52,664 --> 00:09:58,484
Again, we see that when we add
additional threads, the performance

118
00:09:58,514 --> 00:10:05,124
increases and the best performance
there is when we use eight threads

119
00:10:05,484 --> 00:10:07,894
and we have no concurrency penalty.

120
00:10:08,104 --> 00:10:10,864
We have four times performance boost.

121
00:10:11,364 --> 00:10:18,354
So as we said, we use small batches around
four hundred to eight hundred samples

122
00:10:18,874 --> 00:10:25,444
we don't want to use very small batches
because we will waste some processor

123
00:10:25,494 --> 00:10:32,084
time for handling the queue and we
don't want to use very large batches.

124
00:10:32,584 --> 00:10:38,799
So to sum up We saw that porting of
software to heterogeneous CPUs may

125
00:10:38,799 --> 00:10:43,869
require some architectural changes
to fully utilize computing potential.

126
00:10:44,479 --> 00:10:48,399
We demonstrated that stock
multithreading architecture may

127
00:10:48,449 --> 00:10:54,049
inefficiently utilize heterogeneous
CPU and discussed reasons for this.

128
00:10:54,699 --> 00:11:00,899
We demonstrated how to solve this problem
using worker pool pattern, and we got

129
00:11:01,279 --> 00:11:07,229
four times to five times performance boost
for random forest and gradient boosting.

130
00:11:07,729 --> 00:11:12,639
I thank you for your attention and
I'll be glad to answer any questions.

131
00:11:13,139 --> 00:11:13,609
Goodbye.

