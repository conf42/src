1
00:00:00,500 --> 00:00:01,310
Hello everyone.

2
00:00:01,430 --> 00:00:02,390
I'm Man Mohana.

3
00:00:03,050 --> 00:00:08,240
Today I'm presenting how organizations
solve iot seal challenges using

4
00:00:08,240 --> 00:00:14,389
Snowflake plus a structured feature
store architecture to reduce cost while

5
00:00:14,420 --> 00:00:21,320
improving reliability, consistency, ML
performance and development agility.

6
00:00:21,820 --> 00:00:26,950
This approach comes from real
developments, manufacturing, utilities.

7
00:00:27,595 --> 00:00:32,364
Energy grids and smart monitoring
systems, we'll break down what problems

8
00:00:32,364 --> 00:00:39,315
exist, where pipelines fail, how feature
stores solve them, and how snowflake

9
00:00:39,315 --> 00:00:44,835
architecture brings performance and cost
control that traditional systems cannot.

10
00:00:45,335 --> 00:00:46,385
Data explosion.

11
00:00:46,885 --> 00:00:49,225
The scale of IOG today is unprecedented.

12
00:00:49,725 --> 00:00:55,535
Many organizations think of IOT
as just a sensor, but each sensor

13
00:00:55,684 --> 00:01:00,575
represents continuous telemetry
streams, manufacturing lines,

14
00:01:00,575 --> 00:01:06,785
push vibration, temperature
talk, error logs, and many more.

15
00:01:06,964 --> 00:01:08,494
Millions of readings per hour.

16
00:01:08,994 --> 00:01:14,515
Cities run environmental networks,
power rates, traffic monitors, and more.

17
00:01:15,015 --> 00:01:23,465
Vehicle freights push engine health speed
GPS, weather, all streamed continuously.

18
00:01:23,965 --> 00:01:27,555
Traditional infrastructure was
never built for this, so the

19
00:01:27,555 --> 00:01:28,935
challenge is in chest storing data.

20
00:01:29,435 --> 00:01:35,460
It's sustaining throughput while
maintaining contextual meaningful amount.

21
00:01:35,960 --> 00:01:37,969
Three problems always show up.

22
00:01:38,630 --> 00:01:44,079
Number one, number of sensors,
readings per r explodes, and

23
00:01:44,079 --> 00:01:50,619
pipelines must continuously consume
them without pausing or buffering.

24
00:01:51,119 --> 00:01:55,200
Number two, these streams are
continuous, so nothing ever stops.

25
00:01:55,559 --> 00:01:59,915
Ingestion must be always on
fault, tolerant, and incremental.

26
00:02:00,415 --> 00:02:02,304
Number three, heterogeneity.

27
00:02:02,804 --> 00:02:06,134
Different sample rates, precision formats.

28
00:02:06,634 --> 00:02:11,734
When smart meters report every few
seconds and vehicles report irregular

29
00:02:11,825 --> 00:02:17,844
burst, you get temporal misalignment
and that breaks simple SQ analytics.

30
00:02:18,344 --> 00:02:22,664
You need infrastructure that
can ingest, clean, normalize.

31
00:02:23,549 --> 00:02:26,519
Model and serve at streaming velocity.

32
00:02:27,019 --> 00:02:32,019
IOT data is meaningless
without historical context.

33
00:02:32,519 --> 00:02:38,949
A vibration right now tells nothing
unless compared to last hour or last week.

34
00:02:39,764 --> 00:02:40,744
Our sessional patterns.

35
00:02:41,244 --> 00:02:48,884
So ML features must include rolling
window metrics, lag signals, trend

36
00:02:48,884 --> 00:02:51,514
acceleration, sessional behavior.

37
00:02:52,014 --> 00:02:54,054
This is where ad hoc scripts fall apart.

38
00:02:54,554 --> 00:02:59,284
Doing consistent temporal processing
across thousands of sensors or across

39
00:02:59,284 --> 00:03:02,644
dozens of models is impossible.

40
00:03:03,144 --> 00:03:08,654
Without structured repeatable engineering,
so the feature store becomes not

41
00:03:08,654 --> 00:03:11,054
a luxury, but a survival strategy.

42
00:03:11,554 --> 00:03:17,614
In many real deployments, over
70% of ML accuracy actually

43
00:03:17,614 --> 00:03:20,524
comes from temporal context.

44
00:03:21,024 --> 00:03:21,689
Now there are feeding.

45
00:03:22,189 --> 00:03:28,299
Iot data quality issues are unavoidable
like now, values, network gaps,

46
00:03:28,359 --> 00:03:31,329
collaboration drifts mainly noise.

47
00:03:31,829 --> 00:03:33,210
The danger is not bad data.

48
00:03:34,199 --> 00:03:38,299
It's inconsistent handling
across tools, teams, and cetera.

49
00:03:38,799 --> 00:03:41,469
Data engineers handle ingestion one way.

50
00:03:41,964 --> 00:03:45,414
Scientists write manual transformations.

51
00:03:45,914 --> 00:03:49,664
Operations team build bi aggregations.

52
00:03:50,164 --> 00:03:53,344
Result of all of it is same metrics.

53
00:03:53,344 --> 00:03:54,874
Computer three different ways.

54
00:03:55,594 --> 00:03:59,034
Models break when teams
update independently using

55
00:03:59,454 --> 00:04:01,739
predictions become impossible.

56
00:04:02,239 --> 00:04:03,209
This fragmentation.

57
00:04:03,709 --> 00:04:07,969
Escalates cost and destroys trust.

58
00:04:08,469 --> 00:04:12,369
So what we do, we centralize
transformations into a

59
00:04:12,369 --> 00:04:13,839
uniform feature store.

60
00:04:14,339 --> 00:04:19,890
In real world pipelines, I've consistently
seen three to six copies of the same

61
00:04:19,890 --> 00:04:24,949
logic written by different teams because
there is no centralized feature stored.

62
00:04:25,449 --> 00:04:26,709
Feature store concept.

63
00:04:27,399 --> 00:04:34,199
A feature store fixes that fragmentation,
centralized repositories of canonical

64
00:04:34,519 --> 00:04:39,729
features, reusable definitions across
multiple ML models in practice.

65
00:04:40,599 --> 00:04:47,439
Feature reuse cuts
engineering effort by 60, 80%.

66
00:04:47,939 --> 00:04:51,719
' cause once the logic exists, it
automatically benefits every model

67
00:04:52,219 --> 00:04:54,194
versioning to guarantee reproducibility.

68
00:04:54,694 --> 00:04:58,804
Optimized serving for
low latency inference.

69
00:04:59,304 --> 00:05:01,824
Think about sixteens
building six pipelines.

70
00:05:02,324 --> 00:05:03,254
To achieve something,

71
00:05:03,754 --> 00:05:07,284
but to create one pipeline for six teams.

72
00:05:07,784 --> 00:05:12,484
Snowflake architecture gives
us a unique advantage for iot.

73
00:05:12,984 --> 00:05:17,509
Number one, variant enables
ingestion of semi-structured json.

74
00:05:18,209 --> 00:05:21,799
From sensors without schema, gymnastics

75
00:05:22,299 --> 00:05:25,959
time, travel means
historical reproducibility.

76
00:05:26,829 --> 00:05:30,459
If data changed, models
remain reproducible.

77
00:05:30,959 --> 00:05:37,099
Streams plus materialized views lets
us do incremental change capture,

78
00:05:37,599 --> 00:05:40,294
meaning we only process new sensor data.

79
00:05:40,794 --> 00:05:42,264
Not terabytes of history.

80
00:05:42,764 --> 00:05:50,974
Incremental updates routinely reduce
compute load by 40 60% because you

81
00:05:50,999 --> 00:05:53,119
stop reprocessing historical heta.

82
00:05:53,644 --> 00:05:54,844
That hasn't changed.

83
00:05:55,344 --> 00:06:00,824
In short, snowflake trades,
brute force processing for

84
00:06:00,914 --> 00:06:03,194
intelligent incremental work.

85
00:06:03,694 --> 00:06:05,374
And that's the core cost reducer.

86
00:06:05,874 --> 00:06:11,234
This architecture works at any
scale because it's modular,

87
00:06:11,734 --> 00:06:18,009
raw layer preserves, truth timestamps,
device metadata, transform layer.

88
00:06:18,509 --> 00:06:24,709
Performs all feature logic,
windowing smoothing health metrics.

89
00:06:25,209 --> 00:06:32,549
Serving layer is de-normalized for
direct fast lookup by ML systems.

90
00:06:33,049 --> 00:06:37,469
This eliminates Ty
Pipelines feature becomes.

91
00:06:37,755 --> 00:06:43,595
Software artifacts, not scattered scripts.

92
00:06:44,095 --> 00:06:52,335
This layered approach is how teams
safely expand to hundreds or hundreds

93
00:06:52,335 --> 00:06:58,300
of thousands of sensors without
rewriting pipelines every day.

94
00:06:58,800 --> 00:07:02,270
DBT turns analytics into
maintainable software.

95
00:07:02,770 --> 00:07:08,900
Explicit dependency graphs, reusable
macros for rolling statistics or anomaly

96
00:07:08,900 --> 00:07:11,820
scores, unit prevent data drifts.

97
00:07:12,320 --> 00:07:14,311
I've seen automated TVT tests.

98
00:07:15,200 --> 00:07:21,810
Catching a huge amount of bad
upstream sensor data long before

99
00:07:21,900 --> 00:07:24,270
it pollutes dashboards or models,

100
00:07:24,770 --> 00:07:26,810
version control and code review.

101
00:07:27,590 --> 00:07:29,030
This enforces discipline.

102
00:07:29,530 --> 00:07:35,175
Every feature is testable, traceable,
reproducible, and documented.

103
00:07:35,675 --> 00:07:42,605
That's how IOD ML can scale beyond
pipelines to team infrastructure.

104
00:07:43,105 --> 00:07:47,955
A skill handles 80% of
transformations, formations for the

105
00:07:47,955 --> 00:07:51,440
specialized 20% spectral analysis.

106
00:07:52,345 --> 00:07:55,425
Signal decomposition, anomaly scoring.

107
00:07:55,925 --> 00:07:59,105
Snowflake executes Python inside compute.

108
00:07:59,605 --> 00:08:04,430
This is critical because no
radar copied out of Snowflake.

109
00:08:04,930 --> 00:08:07,080
Governance stays intact.

110
00:08:07,580 --> 00:08:09,910
Libraries like Pandas SciPi.

111
00:08:10,495 --> 00:08:13,015
Psyche learn available at scale.

112
00:08:13,515 --> 00:08:20,495
This gives us hybrid power, SQL
speed width, python expressiveness.

113
00:08:20,995 --> 00:08:21,775
Isn't it great?

114
00:08:22,275 --> 00:08:29,645
This hybrid approach keeps AQ fast for
bulk work while leveraging Python for.

115
00:08:30,145 --> 00:08:36,595
About 20% of the cases that requires
signal processing, sophistication.

116
00:08:37,095 --> 00:08:40,445
Cost optimization, we
deliberately minimize Cost

117
00:08:40,945 --> 00:08:42,145
compute is matched per model.

118
00:08:42,645 --> 00:08:47,345
Tiny warehouse for simple
aggressions, medium for white joints.

119
00:08:47,845 --> 00:08:50,365
Large only for parallel tasks.

120
00:08:50,865 --> 00:08:57,635
Matching workloads to warehouse size
commonly yields 50% plus compute

121
00:08:57,635 --> 00:09:04,845
saving in production storage is
tiered granular for recent data down.

122
00:09:04,845 --> 00:09:06,045
Sample for older.

123
00:09:07,005 --> 00:09:08,775
Cold archive for compliance.

124
00:09:09,275 --> 00:09:12,035
Caching ensures repeated model lookups.

125
00:09:12,365 --> 00:09:17,964
Cause zero compute caching can eliminate
nearly all costs for repeated lookups,

126
00:09:18,174 --> 00:09:21,534
especially for inference windows.

127
00:09:22,034 --> 00:09:24,974
Cost governance becomes
architectural, not accidental

128
00:09:25,474 --> 00:09:27,394
performance patterns and trade offs.

129
00:09:27,894 --> 00:09:30,024
Support multiple access patterns.

130
00:09:30,524 --> 00:09:35,454
Batch mode, perfect for ML
training and mass scoring.

131
00:09:35,954 --> 00:09:37,034
Point lookups.

132
00:09:37,534 --> 00:09:42,259
Acceptable latency, but benefits
from caching, decentralized tables.

133
00:09:42,759 --> 00:09:48,519
Intentionally trade a bit of storage
to enable blazing fast inference.

134
00:09:49,019 --> 00:09:57,579
In many deployments, de normalizing
improves serving latency by two to five x

135
00:09:58,079 --> 00:10:00,059
in IOD ml latency.

136
00:10:00,479 --> 00:10:03,319
Predictability is more
valuable than pure speed.

137
00:10:03,819 --> 00:10:05,979
Production operations and governance.

138
00:10:06,579 --> 00:10:11,969
Production means governance monitor
data quality drift pipeline.

139
00:10:11,969 --> 00:10:14,569
Health monitoring feature
drift is critical.

140
00:10:15,139 --> 00:10:22,814
I've seen unnoted drift degraded model
accuracy by 20 to 30% within a month.

141
00:10:23,314 --> 00:10:24,094
Access control.

142
00:10:24,764 --> 00:10:32,394
Via RBAC plus row security,
multiple feature versions active

143
00:10:33,384 --> 00:10:40,844
simultaneously cost tagging per model
team to eliminate budget surprises.

144
00:10:41,344 --> 00:10:48,124
This creates a robust operational backbone
supporting multiple IO OT workloads.

145
00:10:48,454 --> 00:10:49,234
Safely.

146
00:10:49,734 --> 00:10:51,234
Real deployments.

147
00:10:51,984 --> 00:10:57,634
For example, in manufacturing, unified
features improve accuracy and route

148
00:10:57,724 --> 00:10:59,314
engineering efforts significantly.

149
00:10:59,814 --> 00:11:04,314
Smart buildings data,
qualit tests, cart failures.

150
00:11:04,374 --> 00:11:06,624
Before dashboards are ML models.

151
00:11:07,134 --> 00:11:08,214
Pull your decisions.

152
00:11:08,714 --> 00:11:15,904
Usually meters, snowflakes,
columnary, design plus clustering

153
00:11:16,594 --> 00:11:23,614
handled millions of customers at
lower storage and lower latency.

154
00:11:24,114 --> 00:11:26,864
Bottom line feature stores plus snowflake.

155
00:11:27,524 --> 00:11:29,594
Predictable cost and reliable insight.

156
00:11:30,094 --> 00:11:32,754
Key takeaways start with access patterns.

157
00:11:33,254 --> 00:11:36,364
Architecture follows usage quality first.

158
00:11:36,364 --> 00:11:38,254
Mindset saves millions.

159
00:11:38,254 --> 00:11:38,704
Later.

160
00:11:39,204 --> 00:11:42,789
Modular transformations reduce
risk and onboarding pain.

161
00:11:43,289 --> 00:11:45,719
Treat features as engineered assets.

162
00:11:46,219 --> 00:11:48,530
Tester version governed.

163
00:11:49,030 --> 00:11:53,610
This is not a one-off pipeline,
it's a platform strategy.

164
00:11:54,110 --> 00:11:59,025
Treating feature as engineered
assert with test lineage and

165
00:11:59,079 --> 00:12:01,510
reuse is what unlocks these gains.

166
00:12:01,870 --> 00:12:06,819
Typically 50 to 70% less
engineering effort and noticeably.

167
00:12:07,600 --> 00:12:08,589
Higher ML reliability.

168
00:12:09,089 --> 00:12:14,849
So the key idea is consistency delivers
both cost savings and accuracy.

169
00:12:15,349 --> 00:12:18,284
This approach scales whether
you're handling thousands

170
00:12:18,784 --> 00:12:21,204
or millions of IOD signals.

171
00:12:21,704 --> 00:12:22,534
Thank you for listening.

172
00:12:23,034 --> 00:12:25,704
I data doesn't have to
be chaotic or expensive.

173
00:12:26,204 --> 00:12:31,934
Structured feature engineering on
snowflake drives predictability,

174
00:12:31,934 --> 00:12:33,464
scalability, and trust.

175
00:12:33,964 --> 00:12:36,814
Happy to dive deeper or
answer any questions.

176
00:12:37,264 --> 00:12:41,374
Please drop your questions in the
forum and I can get back to you.

177
00:12:42,094 --> 00:12:42,454
Thank you.

