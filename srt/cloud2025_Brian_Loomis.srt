1
00:00:00,200 --> 00:00:03,840
Hello, my name is Brian Loomis
and I'm with Progress Chef.

2
00:00:04,370 --> 00:00:07,300
I'll be speaking with you today about
the practicalities of using open

3
00:00:07,300 --> 00:00:11,579
telemetry and telemetry in general
to provide feedback given business

4
00:00:11,579 --> 00:00:14,100
metrics and on operational systems.

5
00:00:15,209 --> 00:00:18,160
We've built a prototype, which
I'll share with you, based on

6
00:00:18,160 --> 00:00:22,670
some specific needs of operating a
software as a service, SaaS service.

7
00:00:24,205 --> 00:00:27,555
Logging and tracing fine for small
numbers of customers and for direct

8
00:00:27,585 --> 00:00:31,515
developer interaction debugging, but
they have some limitations when it

9
00:00:31,515 --> 00:00:36,075
starts getting into larger systems, and
especially with regard to understanding

10
00:00:36,095 --> 00:00:40,555
customer behavior within the context
of what they're licensed to use, and

11
00:00:40,555 --> 00:00:44,745
maybe some limits on those licenses,
and with automating the scalability

12
00:00:44,745 --> 00:00:46,825
of large clusters of services, right?

13
00:00:46,835 --> 00:00:51,820
So when we get really big Those
logs get similarly big, and they

14
00:00:51,820 --> 00:00:55,050
actually can't really drive as
much sort of high level behavior.

15
00:00:55,900 --> 00:01:00,770
recognizing that OTEL and OTEL like
systems have some press out there

16
00:01:00,770 --> 00:01:04,879
indicating they're hard to implement, we
found it pretty approachable, and we'll

17
00:01:04,879 --> 00:01:08,540
try and give you some idea of the specific
types of metrics we're trying to collect,

18
00:01:08,790 --> 00:01:13,670
how we're using tracing, and events to
make those actionable and to achieve

19
00:01:13,670 --> 00:01:15,630
superior performance on our services.

20
00:01:16,610 --> 00:01:18,030
Let's take a look at what's coming up.

21
00:01:21,900 --> 00:01:22,790
There's a quick agenda.

22
00:01:23,090 --> 00:01:26,480
We'll talk about topics in roughly
this order here, a little bit about

23
00:01:26,480 --> 00:01:30,420
traditional one way telemetry and some of
the challenges we're hoping to overcome.

24
00:01:31,450 --> 00:01:34,840
We'll talk about our design
with feedback loops included.

25
00:01:35,520 --> 00:01:37,750
And that's a key concept in there,
that the telemetry is going to

26
00:01:37,750 --> 00:01:41,490
drive some other services and other
events elsewhere in the system.

27
00:01:42,460 --> 00:01:46,670
Our example application, how enterprise
systems work and what we need from a

28
00:01:46,670 --> 00:01:49,820
licensing business metric perspective
to show back to the customer.

29
00:01:50,170 --> 00:01:54,390
Or our tenant in SAS, and then we'll
wrap up with some final comment.

30
00:01:56,450 --> 00:02:01,280
The data we've collected through telemetry
is pretty important in driving two

31
00:02:01,280 --> 00:02:02,760
very important pieces of the business.

32
00:02:02,970 --> 00:02:06,260
One is obviously revenue, and the
other one is customer satisfaction.

33
00:02:06,260 --> 00:02:06,550
Those two are.

34
00:02:07,215 --> 00:02:08,435
Pretty tightly coupled, right?

35
00:02:09,585 --> 00:02:14,685
Our solution, the Chef 360 system
that we're putting this into is a

36
00:02:14,695 --> 00:02:17,195
multi tenant SaaS application, right?

37
00:02:17,195 --> 00:02:21,425
Meaning we have multiple organizations
and each of them may have one or more

38
00:02:21,665 --> 00:02:26,465
tenancies or I would say operations
and data subdivisions to them.

39
00:02:27,725 --> 00:02:30,965
And the whole solution operates on
a consumption based billing, right?

40
00:02:30,965 --> 00:02:35,415
So truly as a service that you
order up a certain quantity of.

41
00:02:35,795 --> 00:02:37,995
Operations that you want to achieve.

42
00:02:39,835 --> 00:02:43,565
Fundamentally, we want the SAS customer
to dynamically change their demand

43
00:02:43,615 --> 00:02:44,965
proportional to where they see value.

44
00:02:45,685 --> 00:02:49,255
In DevOps, this means going from
a limited prototype, evaluating

45
00:02:49,255 --> 00:02:52,955
functionality and maybe some candidate
workloads in a limited capacity, to

46
00:02:52,955 --> 00:02:57,445
increasingly broader use across the
enterprise, eventually having the most

47
00:02:57,445 --> 00:02:59,335
critical systems under management.

48
00:02:59,985 --> 00:03:03,795
Often this is tens or hundreds
of thousands of machines to us.

49
00:03:04,605 --> 00:03:07,965
The customer buys in incremental
workloads of the operations they perform.

50
00:03:08,215 --> 00:03:12,835
Seeing value in new cases as the system
adoption grows, as they add more machines,

51
00:03:13,185 --> 00:03:15,765
as they add new types of jobs in our case.

52
00:03:16,440 --> 00:03:20,470
This presents unique challenges to
the standard MELT model of how we

53
00:03:20,800 --> 00:03:22,230
traditionally look at logging, right?

54
00:03:23,060 --> 00:03:28,040
Events are usually tied in that model
to very small, smaller than business

55
00:03:28,060 --> 00:03:30,260
increment type operations, right?

56
00:03:30,570 --> 00:03:34,500
So maybe an API usage, but not
necessarily a single operation

57
00:03:34,510 --> 00:03:37,000
that the customer is paying for.

58
00:03:37,010 --> 00:03:39,930
Or they're very strongly correlated
with the logical implementation

59
00:03:40,140 --> 00:03:41,310
and not the business value.

60
00:03:42,200 --> 00:03:46,640
Meaning that sometimes we're tracking
events and logging based on an API.

61
00:03:47,525 --> 00:03:49,965
But that's not really what
the customer is paying for.

62
00:03:50,045 --> 00:03:52,205
We may be tracking things on.

63
00:03:52,835 --> 00:03:56,815
A CPU basis or a disk space
basis, but the customer may be

64
00:03:56,815 --> 00:03:58,135
ordering a very different units.

65
00:03:59,615 --> 00:04:02,765
Tracing similarly is constrained
often to non production debugging

66
00:04:02,765 --> 00:04:03,915
because it's so verbose.

67
00:04:04,295 --> 00:04:09,255
And even that logging is often too
voluminous to support any actionable

68
00:04:09,285 --> 00:04:13,825
insights for either the service
provider, our team, or for our customers.

69
00:04:14,795 --> 00:04:17,885
And the final issue that we were
facing was really that We're not

70
00:04:17,885 --> 00:04:21,675
being responsive enough at scaling
for large numbers of tenants, right?

71
00:04:21,675 --> 00:04:24,655
We'd often have to have them wait for a
little while until we provision some more

72
00:04:24,655 --> 00:04:26,635
hardware or whatever it happened to be.

73
00:04:26,635 --> 00:04:31,115
So it's, there's a delay between when they
want to scale up their demand and when

74
00:04:31,125 --> 00:04:34,425
we actually provision it to address this.

75
00:04:34,505 --> 00:04:37,665
We developed a telemetry driven
approach that provides visibility

76
00:04:37,665 --> 00:04:42,745
into the license features and the
number of operations, maybe the demand

77
00:04:42,795 --> 00:04:44,885
quantum that the customers ordered.

78
00:04:45,455 --> 00:04:49,865
And that enables us to dynamically
scale our workloads, and it also

79
00:04:49,925 --> 00:04:52,055
enables us to bill very accurately.

80
00:04:55,585 --> 00:04:56,595
So I'll keep going here.

81
00:04:57,415 --> 00:05:01,115
This is a typical, maybe an
enterprise application life cycle.

82
00:05:01,495 --> 00:05:05,665
And certainly true for enterprise grade.

83
00:05:06,080 --> 00:05:07,510
SAS applications, right?

84
00:05:07,970 --> 00:05:11,560
A customer is going to purchase
a tenant, a tenancy in the

85
00:05:11,560 --> 00:05:16,720
application through maybe a license
or a transaction in a ERP system.

86
00:05:17,760 --> 00:05:22,090
They may install the app, it may be
hosted in the hosted SAS service,

87
00:05:22,440 --> 00:05:23,960
or it may also be available on prem.

88
00:05:24,545 --> 00:05:26,195
It may be in their cloud, right?

89
00:05:26,195 --> 00:05:31,005
Through an AWS marketplace or an Azure
marketplace, or maybe in an air gap

90
00:05:31,005 --> 00:05:34,665
environment, even further removed from
maybe internet accessibility, right?

91
00:05:35,295 --> 00:05:39,195
Now, for your application, you want
to have your application behave

92
00:05:39,195 --> 00:05:40,905
the same way everywhere, right?

93
00:05:40,945 --> 00:05:44,685
And so that's going to be one theme that
we see coming through this is that our

94
00:05:44,685 --> 00:05:48,815
telemetry structure is exactly the same,
whether any of these cases are true.

95
00:05:50,175 --> 00:05:53,525
If the application is multi
tenant like ours, then you then.

96
00:05:53,950 --> 00:05:56,600
There possibly is more than one
tenant per organization, right?

97
00:05:56,600 --> 00:06:02,330
A customer may have a tenancy for
maybe manufacturing in Asia and one

98
00:06:02,330 --> 00:06:06,340
for manufacturing in Europe, and they
may be separate, possibly the same

99
00:06:06,340 --> 00:06:11,530
org, but also more likely they're going
to be multiple tenants in there from

100
00:06:11,550 --> 00:06:13,579
different organizations that need.

101
00:06:14,230 --> 00:06:17,120
Really significant privacy and
security walls between them.

102
00:06:17,990 --> 00:06:21,150
And so one of the keys also is that
our telemetry can't bleed through,

103
00:06:21,230 --> 00:06:23,100
from one customer to the next customer.

104
00:06:23,570 --> 00:06:27,330
And maybe not even between one tenant and
another tenant within the same customer.

105
00:06:28,390 --> 00:06:31,990
Once provisioned, each tenant
is going to have users which

106
00:06:31,990 --> 00:06:33,190
perform their operations, right?

107
00:06:33,820 --> 00:06:37,770
The license may grant them a certain
quantity of these, an as a service, right?

108
00:06:37,770 --> 00:06:41,870
You're ordering on demand in
billable chunks, maybe, tranches,

109
00:06:41,870 --> 00:06:43,400
or maybe by feature set even.

110
00:06:44,580 --> 00:06:45,850
Whatever they'd like to purchase, right?

111
00:06:45,850 --> 00:06:47,420
They may want this feature,
but not this other one.

112
00:06:48,150 --> 00:06:52,520
And that may change over the
lifetime of their license, right?

113
00:06:52,810 --> 00:06:53,660
It may be updatable.

114
00:06:54,615 --> 00:06:57,225
If you're thinking in app purchases,
yes, that's certainly true.

115
00:06:57,475 --> 00:07:01,425
Probably bigger things than Candy
Crush power ups, but maybe more like

116
00:07:01,645 --> 00:07:06,155
DevOps minutes in our case, in Chef
360's case, by the thousands, right?

117
00:07:06,155 --> 00:07:09,355
So you may order in chunks
of big computing power.

118
00:07:10,335 --> 00:07:12,075
These are some of the events
we want to capture, right?

119
00:07:12,075 --> 00:07:14,465
We want to know what these are,
and we want to translate them into

120
00:07:14,465 --> 00:07:16,295
metrics to show usage over time.

121
00:07:17,265 --> 00:07:19,895
We also want to know how the system,
the underlying system, is performing.

122
00:07:20,075 --> 00:07:23,355
When this increased load gets put
on, so we're going to take metrics

123
00:07:23,355 --> 00:07:27,575
on the average load and that could
be as simple as communities metrics.

124
00:07:29,795 --> 00:07:33,955
In more complex terms, we call these
aggregate events, and they inform

125
00:07:33,965 --> 00:07:37,245
our customer usage specials, goes
back to the customer, tells them,

126
00:07:37,345 --> 00:07:39,025
Hey, you're licensed for 500 units.

127
00:07:39,025 --> 00:07:39,945
You're getting close.

128
00:07:40,245 --> 00:07:42,705
We're going to throttle you
down as you get closer to that.

129
00:07:42,945 --> 00:07:46,054
Maybe you're at 95 percent
of your capacity and you're

130
00:07:46,054 --> 00:07:46,935
going to hit a limit there.

131
00:07:47,304 --> 00:07:50,875
we'll also show that back to the
customer on a dashboard that Brings

132
00:07:50,875 --> 00:07:55,215
these hotel events and metrics
back to them for full transparency

133
00:07:55,215 --> 00:07:56,715
control from their side, right?

134
00:07:57,135 --> 00:08:00,815
We want them to see the workloads
in business terms, right?

135
00:08:00,815 --> 00:08:05,055
We want to see it in units of measure that
they're buying, not necessarily what we're

136
00:08:05,075 --> 00:08:07,034
charged as the service provider, right?

137
00:08:07,055 --> 00:08:11,215
We may be billed for CPU minutes or
disk space or something like that,

138
00:08:11,485 --> 00:08:13,865
but the customer is actually buying
something that's different, right?

139
00:08:13,865 --> 00:08:15,245
They're buying a capability.

140
00:08:15,575 --> 00:08:16,635
In business terms, right?

141
00:08:16,635 --> 00:08:19,705
There are buying DevOps
minutes or number of jobs.

142
00:08:20,445 --> 00:08:25,945
We typically build and show 360 biological
job running on a customer node, which

143
00:08:25,945 --> 00:08:28,125
we call the job node, strangely enough.

144
00:08:28,665 --> 00:08:28,975
okay.

145
00:08:28,975 --> 00:08:33,255
And that's our per instance measure that
they can scale up or scale down, right?

146
00:08:33,295 --> 00:08:37,075
They don't necessarily care how
much, how many EKS containers

147
00:08:37,105 --> 00:08:38,335
are running in the background.

148
00:08:39,675 --> 00:08:43,055
We want to abstract our platform cost
effectively from what the customer is

149
00:08:43,055 --> 00:08:44,685
actually buying as a service, right?

150
00:08:49,920 --> 00:08:54,360
So why don't we do non
Otel type solutions, right?

151
00:08:54,590 --> 00:08:57,180
Why do we go to the extra
trouble of adding Otel in here?

152
00:08:57,850 --> 00:09:01,930
And so our typical scenario that we're
dealing for when I say like enterprise

153
00:09:01,930 --> 00:09:08,030
grade solutions, imagine you have a
customer that has 100, 000 containers

154
00:09:08,310 --> 00:09:10,060
that only last for a few minutes each.

155
00:09:10,110 --> 00:09:13,400
It's a very dynamic environment,
scales up and down very quickly.

156
00:09:13,880 --> 00:09:17,490
some of those particular nodes
that we would call them, execute.

157
00:09:18,285 --> 00:09:20,435
Very quickly, and then
they disappear, right?

158
00:09:20,435 --> 00:09:21,515
They get dehydrated.

159
00:09:21,915 --> 00:09:23,115
They're ephemeral, as we call them.

160
00:09:23,895 --> 00:09:26,535
these are some of the questions that
we realized that we're asking ourselves

161
00:09:26,535 --> 00:09:29,694
as we're going down this OpenTelemetry
journey, and why OpenTelemetry?

162
00:09:31,285 --> 00:09:34,435
You think of like how many times you've
just, shipped a system to production

163
00:09:34,435 --> 00:09:36,685
that has had debug tracing turned on.

164
00:09:37,035 --> 00:09:39,065
You're like, just in case something
goes wrong, I'm just going to turn

165
00:09:39,065 --> 00:09:42,454
on all the logging and we'll scale
it down if there's, a need to scale

166
00:09:42,454 --> 00:09:44,354
it down and just take less logs.

167
00:09:44,384 --> 00:09:47,644
But, we're designing for the doomsday
scenario where we have to have all

168
00:09:47,644 --> 00:09:49,054
the logging all the time, right?

169
00:09:49,154 --> 00:09:52,084
And, this gets worse obviously with
distributed systems because you have.

170
00:09:52,544 --> 00:09:53,734
Uncorrelated logs, right?

171
00:09:53,734 --> 00:09:56,534
You have to bring them all together
somehow that may have come from different

172
00:09:56,574 --> 00:10:00,234
time zones and they're interleaved,
might call this data dog diving.

173
00:10:00,254 --> 00:10:00,604
I don't know.

174
00:10:00,604 --> 00:10:01,704
He's making that one up.

175
00:10:02,834 --> 00:10:08,294
But also we have to realize that
someone's going to have to look

176
00:10:08,294 --> 00:10:09,394
through all these logs, right?

177
00:10:09,404 --> 00:10:13,869
So Even though we may be taking in
a lot of trace information, a lot

178
00:10:13,869 --> 00:10:19,729
of debug on every API call, on every
request, every response into the system.

179
00:10:20,319 --> 00:10:22,479
Fundamentally, at the end of the
day, your developers are then

180
00:10:22,479 --> 00:10:23,699
your tier three support, right?

181
00:10:23,699 --> 00:10:26,749
If something goes wrong, the developer is
going to have to look through this massive

182
00:10:26,749 --> 00:10:28,959
data and try and intuit something better.

183
00:10:29,939 --> 00:10:33,979
I would also argue, without a, an
intelligent assistant on that or some

184
00:10:33,979 --> 00:10:38,589
way of parsing that, that becomes
really Unscalable really quickly.

185
00:10:39,369 --> 00:10:42,759
Imagine you have a hundred customers
that are feeding into a SAS service log.

186
00:10:43,029 --> 00:10:47,889
figuring out where that one customer was,
when they had an issue is looking for the

187
00:10:48,019 --> 00:10:49,670
proverbial needle in a haystack, right?

188
00:10:50,680 --> 00:10:53,650
How many times have you just
bolted on events, every time

189
00:10:53,819 --> 00:10:55,089
an API is called, right?

190
00:10:55,170 --> 00:10:56,250
Every time an endpoint gets hit.

191
00:10:56,719 --> 00:11:00,100
you may be logging an awful lot
of data, but you're also probably

192
00:11:00,100 --> 00:11:01,660
logging the request and response.

193
00:11:02,025 --> 00:11:06,004
Do you go through and actually
take the time to, apply privacy

194
00:11:06,004 --> 00:11:10,734
filters to that export right before
it gets out to maybe the service

195
00:11:10,734 --> 00:11:12,494
provider or something like that.

196
00:11:12,904 --> 00:11:16,884
And in many times when we go, Oh,
customer, you just go ahead and collect

197
00:11:16,884 --> 00:11:20,034
all the logs and then you implicitly
tell us that we can see everything in

198
00:11:20,034 --> 00:11:23,724
those logs and you just ship them to
us, you zip them and FTP them over to

199
00:11:23,724 --> 00:11:24,904
us and we'll take a look at them, right?

200
00:11:24,904 --> 00:11:29,034
It's a very asynchronous process, but
it also doesn't provide any privacy.

201
00:11:29,454 --> 00:11:33,035
And in a multi tenant environment, this
is very important, because we're going

202
00:11:33,035 --> 00:11:37,744
to want to do things like auditing events
and show the customer who had access

203
00:11:37,744 --> 00:11:39,634
to do what thing at what point in time.

204
00:11:40,734 --> 00:11:43,004
And we're looking at a multi
tenant log scenario, unless

205
00:11:43,004 --> 00:11:44,704
we've parsed them out by tenant.

206
00:11:46,115 --> 00:11:50,324
Another one we run into a lot is
how many times do you then go, okay,

207
00:11:50,324 --> 00:11:53,244
I've got this big body of log files.

208
00:11:54,160 --> 00:11:56,350
I'm going to have to put a reporting
solution on top of it, right?

209
00:11:56,470 --> 00:11:58,100
I've got to build a dashboard.

210
00:11:58,100 --> 00:11:59,579
I've got to do all this stuff.

211
00:12:00,510 --> 00:12:02,760
And then you realize, Oh, wait,
there are tools out there.

212
00:12:02,800 --> 00:12:05,680
And other people seem to be able to do
this a lot easier than I'm able to do it.

213
00:12:05,770 --> 00:12:11,839
And then you go, okay, what if that
log file isn't actually answering the

214
00:12:11,839 --> 00:12:17,040
question, the fourth bullet on here, what
if that telemetry is not useful to the

215
00:12:17,040 --> 00:12:19,569
rest of the people in your organization,
it's useful to developers made to

216
00:12:19,579 --> 00:12:22,010
troubleshoot a particular debug instance.

217
00:12:22,630 --> 00:12:24,110
to find a bug or something like that.

218
00:12:24,990 --> 00:12:28,400
But what about the next time the customer
comes up for a licensing discussion?

219
00:12:28,550 --> 00:12:31,959
And you want to be able to show
them, Hey, you've been using,

220
00:12:31,970 --> 00:12:33,500
80 percent of your allocation.

221
00:12:33,509 --> 00:12:36,870
Do you want to buy another 10 percent
or something like that so that

222
00:12:36,870 --> 00:12:38,419
you have enough capacity to grow?

223
00:12:39,450 --> 00:12:40,790
We're not collecting those metrics.

224
00:12:40,800 --> 00:12:43,580
What we're collecting is a lot
of very low level APIs, right?

225
00:12:44,410 --> 00:12:46,950
And this drives some very
particular customer behaviors.

226
00:12:46,950 --> 00:12:47,900
I put a couple in here.

227
00:12:48,230 --> 00:12:52,120
The customer may overbuy, in which
case they're spending a lot of extra

228
00:12:52,120 --> 00:12:57,329
money on their solution, which kind of
looks like shelfware to them, right?

229
00:12:57,330 --> 00:13:02,729
And they go, Ooh, maybe, when budgets
get tight or something, maybe I actually

230
00:13:02,729 --> 00:13:05,630
go in and evaluate like how much I'm
spending on this solution over here.

231
00:13:06,020 --> 00:13:09,740
or they have, Even worse behavior that
they limit themselves and they go, I

232
00:13:09,740 --> 00:13:12,450
don't want to go back to procurement
and add another quantum of things.

233
00:13:12,950 --> 00:13:15,479
And so they're taking themselves
like they're constraining their own

234
00:13:15,479 --> 00:13:19,290
value because they can't see how much
they're using that solution, right?

235
00:13:19,290 --> 00:13:21,789
And they don't want to go over
because that would be very

236
00:13:21,789 --> 00:13:22,899
difficult organizationally.

237
00:13:23,309 --> 00:13:28,210
And they'll not even ask you the question
of could I get better service, right?

238
00:13:28,560 --> 00:13:32,850
And so both of these, when I say the
business metrics get impacted and

239
00:13:33,750 --> 00:13:37,030
Your revenue as a service provider
gets impacted because the customer

240
00:13:37,030 --> 00:13:40,090
is holding back from the conversation
you really want to have, which is,

241
00:13:40,130 --> 00:13:41,830
can I pay for just what I'm using for?

242
00:13:43,730 --> 00:13:46,540
Okay, and then in the last one,
it's more of an operational concern.

243
00:13:46,920 --> 00:13:50,650
we call this the horizontal pod or
auto scaling only gets you so far.

244
00:13:51,780 --> 00:13:53,890
What do you set your pod size into?

245
00:13:54,690 --> 00:13:56,800
If you're adding multiple
customers in there that are

246
00:13:56,800 --> 00:14:00,010
variable sized and may get bigger.

247
00:14:00,525 --> 00:14:04,575
Over time and may eventually either
themselves or the combination of

248
00:14:04,585 --> 00:14:08,165
these customers may exceed the
reasonable size of the cluster, right?

249
00:14:08,545 --> 00:14:10,175
You have vertical pod auto scaling.

250
00:14:10,715 --> 00:14:12,435
You can add more physical nodes.

251
00:14:12,435 --> 00:14:14,275
You can get monolithically bigger.

252
00:14:14,815 --> 00:14:18,539
But how do we actually get real
auto scaling that can burst out?

253
00:14:20,090 --> 00:14:22,870
Here we're sharing the architecture
diagram behind our solution,

254
00:14:23,140 --> 00:14:27,350
showcasing a multi stage telemetry
processing basic dashboard.

255
00:14:28,200 --> 00:14:31,300
On the far left side, you'll see the
license service, which talks to our

256
00:14:31,300 --> 00:14:35,610
tenant provisioner service, which then
allocates capacity in one of many,

257
00:14:35,990 --> 00:14:39,690
potentially many Kubernetes clusters for
the new tenant based on current usage.

258
00:14:40,045 --> 00:14:41,145
for requested usage.

259
00:14:41,915 --> 00:14:45,145
If the customer gets larger, too large
for a single cluster, we want them to

260
00:14:45,145 --> 00:14:48,695
burst the next available cluster and have
that automatically be created for them.

261
00:14:49,625 --> 00:14:53,165
Telemetry is captured identically
on prem as in our SAS service.

262
00:14:53,505 --> 00:14:56,925
Multiple clusters will feed into
a local telemetry dashboard for

263
00:14:56,925 --> 00:14:58,595
a single organizational customer.

264
00:14:59,175 --> 00:15:03,290
This is then optionally anonymized
and fed to a global hotel collector

265
00:15:03,690 --> 00:15:08,030
operated by Chef and Progress, which
allows our organization to monitor

266
00:15:08,050 --> 00:15:12,270
any on prem clusters, which opt in
and all of our own SAS clusters.

267
00:15:13,360 --> 00:15:15,000
There's three things to note with this.

268
00:15:15,940 --> 00:15:19,390
Really, the first one is the lines
backwards going between the local

269
00:15:19,390 --> 00:15:23,220
collector and the provisioner allows
for feedback when the cluster gets full

270
00:15:23,350 --> 00:15:28,320
at capacity and causing a new cluster
to be spun up and new workloads then

271
00:15:28,320 --> 00:15:29,670
to be allocated to that new cluster.

272
00:15:29,670 --> 00:15:33,690
Cluster number two, for instance, and then
the second piece of feedback there is the

273
00:15:33,690 --> 00:15:35,410
global collector can send a signal back.

274
00:15:35,780 --> 00:15:39,220
Overbilling to tell the customer
they're actually close to exceeding

275
00:15:39,220 --> 00:15:43,510
their contractual limit and see if they
want to upgrade their licensed usage.

276
00:15:45,450 --> 00:15:48,980
The third thing to note here is that
basically we have the same tiered

277
00:15:49,010 --> 00:15:50,520
pattern for any customer type.

278
00:15:50,690 --> 00:15:56,350
Our home chart does not change if we
deploy on prem in AirGap or in customer

279
00:15:56,350 --> 00:15:58,710
supply cloud or cloud hosted by us.

280
00:15:58,780 --> 00:16:00,150
It's the same exact pattern.

281
00:16:00,580 --> 00:16:04,520
The customer would get on prem
everything up to that orange local

282
00:16:04,800 --> 00:16:06,150
customer telemetry dashboard.

283
00:16:06,825 --> 00:16:10,535
And then we would still have some of
the global services wrapping this, but

284
00:16:10,545 --> 00:16:13,795
otherwise they get everything in the
middle of this picture and they operate

285
00:16:13,795 --> 00:16:18,115
exactly the same way and they can see all
of the telemetry that they want to see.

286
00:16:22,045 --> 00:16:25,065
This sort of talks a little bit about the
sample application that will be out there.

287
00:16:25,445 --> 00:16:28,385
This is just a sample
hotel implementation.

288
00:16:28,405 --> 00:16:33,565
It's roughly equivalent to the prototype
that were, it shows all the items that

289
00:16:33,565 --> 00:16:35,035
are in the diagram just before this.

290
00:16:35,395 --> 00:16:36,005
And.

291
00:16:36,720 --> 00:16:39,490
Really, the differences here are
in the bottom bullets there, our

292
00:16:39,490 --> 00:16:42,940
application, our main application is
a bunch of scratch containers that

293
00:16:42,940 --> 00:16:49,170
are written for AWS EKS, they're in
Golang, and so our hotel implementation

294
00:16:49,170 --> 00:16:51,340
is in Golang, not in, not in NET.

295
00:16:52,060 --> 00:16:54,800
NET seems to be an easier one to
present to people and has a really good

296
00:16:54,800 --> 00:16:57,740
wall and everything, you'll be able
to download the code samples from the

297
00:16:57,770 --> 00:16:59,190
repository and the references here.

298
00:17:00,205 --> 00:17:03,015
Just to quickly go through
this, there's a client tool that

299
00:17:03,215 --> 00:17:04,805
asks for jobs to be submitted.

300
00:17:05,155 --> 00:17:07,025
it basically runs for multiple customers.

301
00:17:07,045 --> 00:17:09,335
It simulates demand into the system.

302
00:17:09,905 --> 00:17:12,475
the ones in parentheses I'm not
supplying, they would typically

303
00:17:12,495 --> 00:17:16,015
be part of this service, but a
routing service or a load balancer

304
00:17:16,015 --> 00:17:17,355
basically that takes those clients.

305
00:17:17,500 --> 00:17:19,890
request and puts them in the
right clusters where that

306
00:17:19,890 --> 00:17:21,360
job was deployed, right?

307
00:17:21,360 --> 00:17:24,000
Obviously we'd have to talk to the
provision service to go, okay, where

308
00:17:24,000 --> 00:17:29,380
did customer with tenant A and this
particular workload or license go to?

309
00:17:29,770 --> 00:17:31,410
oh, that's over in cluster five, right?

310
00:17:31,430 --> 00:17:34,110
So that routing service, we leave
as a exercise to the reader.

311
00:17:34,870 --> 00:17:36,009
We have the job service.

312
00:17:36,080 --> 00:17:39,670
which takes client requests and may
have some limits implied, right?

313
00:17:39,670 --> 00:17:43,350
So it goes, okay, you are only
licensed to five operations.

314
00:17:43,380 --> 00:17:44,820
So we'll cap you at that point.

315
00:17:45,150 --> 00:17:48,920
And then we will not take requests
after that point, but it also has

316
00:17:48,920 --> 00:17:50,400
its own internal capacity, right?

317
00:17:50,400 --> 00:17:54,940
So that once it gets full, it's
expecting that the load balancer or

318
00:17:54,980 --> 00:17:58,190
the provisioner is going to redirect
all that load to a new cluster.

319
00:17:58,860 --> 00:18:01,270
the cluster management service itself,
and we just talked about that, which

320
00:18:01,270 --> 00:18:04,610
will provision new clusters when
once at capacity or near capacity.

321
00:18:05,240 --> 00:18:08,970
And then finally, on the outside, the
observability platform that you might

322
00:18:08,970 --> 00:18:12,910
bolt on to your telemetry service
might be a Prometheus or a Grafana

323
00:18:12,910 --> 00:18:16,730
or something to visualize this so
that an operator can go, okay, what's

324
00:18:16,730 --> 00:18:18,070
actually going on in my cluster?

325
00:18:18,500 --> 00:18:23,990
Might bring in the Kubernetes metrics
as well, CPU, disk, network load, things

326
00:18:23,990 --> 00:18:27,550
like that, as well as the business metrics
that we're getting from the hotel side.

327
00:18:31,580 --> 00:18:34,800
Coming over here, I wanted to show
you just a quick, animation here.

328
00:18:35,160 --> 00:18:36,770
And I'll see if I can
play this video here.

329
00:18:36,950 --> 00:18:38,350
I did this in chat GPT.

330
00:18:38,370 --> 00:18:42,490
So apologies for the, the simple
nature of this and everything.

331
00:18:42,490 --> 00:18:45,340
I put the prompt in,
the repository as well.

332
00:18:45,340 --> 00:18:46,390
So you'll see that out there.

333
00:18:46,560 --> 00:18:49,540
If you want to do your own animations
and things like that, this basically

334
00:18:49,540 --> 00:18:51,030
shows a red and a blue customer.

335
00:18:51,225 --> 00:18:53,235
Each with a limited number
of jobs that they have.

336
00:18:53,655 --> 00:18:57,015
eventually the starting cluster
gets full of requests and

337
00:18:57,025 --> 00:18:58,595
bursts out to a second cluster.

338
00:18:58,985 --> 00:19:02,855
And you'll see that blue eventually goes,
Oh, wait, I'm limited by number of jobs.

339
00:19:02,855 --> 00:19:06,065
I don't want to have to wait like red
customer did to start my next one.

340
00:19:06,595 --> 00:19:08,975
I actually just want to go ahead
and bump up my license and you'll

341
00:19:08,975 --> 00:19:10,665
see all of blue's jobs then come in.

342
00:19:11,115 --> 00:19:13,245
So I'll just play this
quickly for you here.

343
00:19:50,895 --> 00:19:53,345
The red has to wait for
you to submit his next job.

344
00:20:08,010 --> 00:20:10,850
Red wanted a total of eight,
so it's done for right now.

345
00:20:13,750 --> 00:20:14,880
Let's see what blue does.

346
00:20:23,050 --> 00:20:24,770
Ah, blue asks for five more.

347
00:20:24,880 --> 00:20:26,170
Has a little more space in there.

348
00:20:26,670 --> 00:20:29,820
So his are just going to go into cluster
number two, which is a little bit bigger

349
00:20:29,820 --> 00:20:32,440
size than maybe cluster number one was.

350
00:20:41,730 --> 00:20:42,410
And there we go.

351
00:20:42,500 --> 00:20:42,990
Blue is done.

352
00:20:43,950 --> 00:20:44,050
There

353
00:20:50,430 --> 00:20:50,720
we go.

354
00:20:51,130 --> 00:20:55,070
So talk a little bit here about the
differences between where we were, our

355
00:20:55,070 --> 00:20:58,540
legacy solution and our new solution,
and especially the types of metrics

356
00:20:58,540 --> 00:20:59,800
that we're able to track right now.

357
00:21:00,010 --> 00:21:02,250
And I apologize, I just probably
blow this up on your screen to

358
00:21:02,250 --> 00:21:04,560
see a little bit of the detail and
everything, because there was so much

359
00:21:04,560 --> 00:21:05,950
stuff that we really added in here.

360
00:21:06,590 --> 00:21:08,270
Really three big things to note here.

361
00:21:09,090 --> 00:21:11,570
One, we're using tiered
hotel collectors, right?

362
00:21:11,570 --> 00:21:14,130
So one set of collectors
will forward those with OLTP.

363
00:21:15,515 --> 00:21:19,925
OTP, sorry, over to another, the
global hotel collector as well, right?

364
00:21:19,925 --> 00:21:22,645
It gives us a chance to anonymize
data before sending it on to

365
00:21:22,645 --> 00:21:24,205
the next stage of processing.

366
00:21:24,775 --> 00:21:29,875
Also lets the customer who's in control
of that local one to opt in to letting

367
00:21:29,875 --> 00:21:32,425
us see some, none, or all of their data.

368
00:21:33,355 --> 00:21:35,595
Having that local on prem collector.

369
00:21:36,745 --> 00:21:39,415
Let's the customer also manage
their own telemetry, but if there's

370
00:21:39,415 --> 00:21:42,115
an issue, they can change how
much they're sending us, right?

371
00:21:42,145 --> 00:21:46,065
And then we can, we could be opted in
to help troubleshoot and then they can

372
00:21:46,065 --> 00:21:47,315
dial it back when they're all clear.

373
00:21:47,885 --> 00:21:51,675
And the third thing to note here
is that the business metrics of job

374
00:21:51,695 --> 00:21:54,105
nodes, the sizing of a job, right?

375
00:21:54,115 --> 00:21:57,555
Our sort of quantum, the little red
and blue dots from before, right?

376
00:21:57,855 --> 00:22:01,535
How big or small it is compared to
the capacity of the cluster lets

377
00:22:01,535 --> 00:22:03,235
us decouple the actual cluster.

378
00:22:03,645 --> 00:22:06,915
The customers running on from the
service that they're buying, right?

379
00:22:06,915 --> 00:22:08,645
They're buying a set of those dots.

380
00:22:08,745 --> 00:22:12,225
They're not buying whether it's in
cluster number one or number two.

381
00:22:12,635 --> 00:22:16,245
They may be on a portion of one
cluster and maybe in there with other

382
00:22:16,245 --> 00:22:17,775
tenants that sort of fill it up.

383
00:22:18,165 --> 00:22:21,385
Or they may spend multiple clusters
because they've gotten very large.

384
00:22:21,565 --> 00:22:24,755
Or they may even decide that
they want to move some workload

385
00:22:24,795 --> 00:22:26,769
and expand on Prem or whatever.

386
00:22:26,880 --> 00:22:30,200
vice versa from on prem into
the cloud to gain efficiencies.

387
00:22:30,770 --> 00:22:34,650
That way the customer is paying for the
operations they need, not more or less.

388
00:22:37,470 --> 00:22:39,870
Okay, so can you do this in logging alone?

389
00:22:41,100 --> 00:22:41,630
You can.

390
00:22:42,710 --> 00:22:46,410
I think for all the questions that
we brought up here, it's Locking

391
00:22:46,410 --> 00:22:49,010
is certainly an easy solution, but
it does have a couple drawbacks.

392
00:22:49,020 --> 00:22:54,050
Ones that you're going to have to
write some code, some bits around, that

393
00:22:54,090 --> 00:22:56,000
OpenTelemetry really gives us for free.

394
00:22:56,670 --> 00:23:01,100
it doesn't help us, for instance, when we
get to implementing privacy and auditing.

395
00:23:01,330 --> 00:23:03,660
The logs are they're low tech, right?

396
00:23:03,910 --> 00:23:06,840
They're shared between organizations
at a very granular level.

397
00:23:07,050 --> 00:23:10,320
You want to be able to get
rid of privacy PII in there.

398
00:23:10,760 --> 00:23:13,750
You want to be able to
provide traces for debugging.

399
00:23:13,790 --> 00:23:16,780
You don't want all the other noise
that's going on from maybe other

400
00:23:16,780 --> 00:23:20,850
tenants in the infrastructure, even if
they are from the same organization.

401
00:23:21,400 --> 00:23:23,760
Telemetry processing
can give us that, right?

402
00:23:24,070 --> 00:23:28,380
And can aggregate some of the things
that we would normally track as Five,

403
00:23:28,490 --> 00:23:30,940
10 different API calls in a row to go.

404
00:23:30,990 --> 00:23:32,940
That's one count on a business metric.

405
00:23:32,990 --> 00:23:37,600
That is one operation that we
would bill you for basically helps

406
00:23:37,600 --> 00:23:39,120
us as a SAS provider as well.

407
00:23:39,130 --> 00:23:41,120
Isolate customer data
from each other, right?

408
00:23:41,130 --> 00:23:43,510
So that helps as well, because we'll
have a little customer ID on there.

409
00:23:46,100 --> 00:23:50,000
The other thing is that remember we
talked about customer buying behavior.

410
00:23:51,295 --> 00:23:53,235
We don't want them to
behave irrationally, right?

411
00:23:53,235 --> 00:23:56,175
We don't want them to overbuy because
they're afraid they're going to run out.

412
00:23:56,445 --> 00:23:59,795
We don't want them to limit their usage
because they're afraid they'll run

413
00:23:59,805 --> 00:24:03,055
over in terms of billing or they'll
have dynamic billing applied to them.

414
00:24:03,475 --> 00:24:06,675
We provide customers basically a
flexible mechanism where they can buy

415
00:24:07,005 --> 00:24:10,595
chunks of, Quantum of operations, right?

416
00:24:10,595 --> 00:24:16,065
So you can buy 500 or a thousand or in
chunks of reasonably useful increments,

417
00:24:16,345 --> 00:24:19,575
but then that any set of increments
mirrors their actual usage, right?

418
00:24:19,575 --> 00:24:21,615
So they see steps up in their licensing.

419
00:24:21,665 --> 00:24:28,445
It's not a gradual curve or any sort
of performance problem where they miss,

420
00:24:28,495 --> 00:24:32,135
Maybe submit a giant job and then they
realize, oh, that wasn't the job I wanted

421
00:24:32,165 --> 00:24:35,645
and it spikes their load and we don't
have a way to go, wait, let's hold off

422
00:24:35,645 --> 00:24:38,485
on that one and have that discussion with
the customer and go, is that really what

423
00:24:38,515 --> 00:24:41,835
you wanted to do and get them into the
plan that they're really looking for?

424
00:24:42,225 --> 00:24:45,295
what we don't want to have is a mismatch
between billing and what they're at, what

425
00:24:45,305 --> 00:24:46,785
value they're actually seeing, right?

426
00:24:46,795 --> 00:24:50,205
So the business metrics give us
that aggregation, that abstraction.

427
00:24:51,380 --> 00:24:51,820
Okay.

428
00:24:52,080 --> 00:24:55,220
And then finally, of course, on the
troubleshooting side, our operators

429
00:24:55,220 --> 00:24:58,580
already knew with our previous
implementation, having 100 customers

430
00:24:58,580 --> 00:25:02,010
in the same logged cluster, it's
tough to follow a single logical

431
00:25:02,010 --> 00:25:03,340
operation through that, right?

432
00:25:03,350 --> 00:25:05,370
Much less through one server.

433
00:25:05,660 --> 00:25:08,890
But then if you think of 30 or 40
containers where you're having to

434
00:25:08,890 --> 00:25:12,780
aggregate, cross them and trace
between them what really happened,

435
00:25:13,130 --> 00:25:16,100
and you don't want to bother the
other 99 customers in that cluster.

436
00:25:18,320 --> 00:25:18,750
Okay.

437
00:25:19,730 --> 00:25:21,710
So some technical notes
on the prototype here.

438
00:25:21,820 --> 00:25:25,480
We're using simple hotel emitters
and collectors, the local collectors

439
00:25:25,490 --> 00:25:27,170
followed by a global collector again.

440
00:25:27,560 --> 00:25:31,200
each of those is a container that can
embellish those input events, the tracing

441
00:25:31,200 --> 00:25:34,490
and the metrics, and create other events
as part of feedback to the manager

442
00:25:34,490 --> 00:25:36,315
processes, like provision and billing.

443
00:25:37,415 --> 00:25:40,455
Decoupling our cluster capacity
from logical customer deployment

444
00:25:40,495 --> 00:25:44,525
helps solve the HPA only gets you
HPA only gets you so far problem.

445
00:25:45,335 --> 00:25:46,735
Horizontal pod autoscaling, right?

446
00:25:47,105 --> 00:25:50,415
If a customer needs 500 scale units
and we can only get up to 100 per

447
00:25:50,415 --> 00:25:54,925
cluster, then our provisioner knows
that he has to put another 4 out there.

448
00:25:56,490 --> 00:25:58,150
There's some things we
don't do yet, right?

449
00:25:58,150 --> 00:26:01,220
So there's some things that will
be totally transparent with moving

450
00:26:01,220 --> 00:26:02,790
workloads between cloud and on prem.

451
00:26:03,220 --> 00:26:03,830
That's tricky.

452
00:26:03,910 --> 00:26:04,910
That's not an easy thing.

453
00:26:04,910 --> 00:26:07,360
It's not something that hotel
necessarily gives you out of the box.

454
00:26:07,710 --> 00:26:11,200
pinning allocations from that
provisioner to specific clusters.

455
00:26:11,340 --> 00:26:15,580
So you need to stay in a GDPR region
or have some special licensing like

456
00:26:15,580 --> 00:26:19,490
that, or also putting true AI in
there, or maybe machine learning to

457
00:26:19,490 --> 00:26:21,020
make intelligent provisioning choices.

458
00:26:21,020 --> 00:26:22,740
Those are all areas for us to grow.

459
00:26:25,890 --> 00:26:29,570
Again, to summarize new versus old,
this is at the business level here.

460
00:26:30,100 --> 00:26:31,520
we're using tiered OpenTelemetry.

461
00:26:31,600 --> 00:26:33,930
This helps us grow with our
customers and very transparent.

462
00:26:34,720 --> 00:26:37,520
We have the same code base for
on prem as our hosted service.

463
00:26:37,900 --> 00:26:40,220
And it's a multi tenant solution
where the customer sees their own

464
00:26:40,220 --> 00:26:43,480
dashboard and controls how much they
want us as a service provider to see.

465
00:26:45,210 --> 00:26:48,970
Between this and another complementary
on prem solution, we can bring

466
00:26:48,970 --> 00:26:50,160
business metrics together.

467
00:26:50,390 --> 00:26:53,720
With the customer licensing and
Kubernetes performance data to show

468
00:26:53,720 --> 00:26:56,760
a more complete picture back to the
customer so they can take their own

469
00:26:56,760 --> 00:27:00,480
appropriate action and really self service
based on what they want to use, right?

470
00:27:00,550 --> 00:27:02,070
Isn't that the whole purpose of telemetry?

471
00:27:03,250 --> 00:27:05,800
And finally, I'm going to
say, we picked open telemetry.

472
00:27:05,880 --> 00:27:07,660
There are a lot of other
solutions out there.

473
00:27:08,100 --> 00:27:12,020
Prometheus, Datadog, Splunk,
a lot of these will enhance

474
00:27:12,050 --> 00:27:13,370
logging to a great degree.

475
00:27:14,580 --> 00:27:17,620
We didn't want something specifically
that would add extra load onto

476
00:27:17,620 --> 00:27:18,830
the services that we're doing.

477
00:27:18,840 --> 00:27:21,410
It had to be something that was native
in the service that it could emit

478
00:27:21,410 --> 00:27:26,240
telemetry, but it wasn't an extra
burden to have on there another.

479
00:27:26,835 --> 00:27:31,525
Either a pull mechanism or a,
an agent of some sort that's on

480
00:27:31,535 --> 00:27:32,955
there that's also running, right?

481
00:27:33,145 --> 00:27:36,695
And wasn't really, I would say cloud
native, but really not container friendly.

482
00:27:39,845 --> 00:27:42,235
So finally here, this shows
some resources for the talk.

483
00:27:42,425 --> 00:27:45,145
An OTEL getting started
in NET, really easy.

484
00:27:45,375 --> 00:27:46,235
Takes five minutes.

485
00:27:46,625 --> 00:27:48,155
an OWASP logging checklist.

486
00:27:48,275 --> 00:27:51,215
so if you haven't been out to the security
site and seen what you should log and what

487
00:27:51,215 --> 00:27:54,715
you shouldn't log, it's a good example
of what the security folks might ask for.

488
00:27:55,255 --> 00:27:56,675
And our repository.

489
00:27:57,180 --> 00:27:59,520
For our dot net sample, the
one that I wrote, and that

490
00:27:59,520 --> 00:28:00,950
will be available by the month.

491
00:28:01,050 --> 00:28:03,140
If you shoot me your email,
I'll notify you directly.

492
00:28:03,150 --> 00:28:06,320
Otherwise, just follow the space and
you should see the update when it hits.

493
00:28:06,900 --> 00:28:09,380
and then finally, the documentation
of the Chef 360 system.

494
00:28:09,380 --> 00:28:11,700
If you want to understand some background
of what we don't want to talk about

495
00:28:11,720 --> 00:28:13,640
jobs and runners and things like that.

496
00:28:13,925 --> 00:28:17,535
Okay, and that actually should
be GA within days of this

497
00:28:17,545 --> 00:28:19,615
talk as our hosted service.

498
00:28:19,615 --> 00:28:20,605
So we're pretty proud of that.

499
00:28:22,045 --> 00:28:23,835
Finally, thank you for
listening to this talk.

500
00:28:23,865 --> 00:28:28,155
We wish you a very helpful and inspiring
conference 42 from Brian and Chef.

501
00:28:28,275 --> 00:28:28,605
Goodbye.

