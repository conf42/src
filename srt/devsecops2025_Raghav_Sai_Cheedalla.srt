1
00:00:00,500 --> 00:00:01,130
Hello everyone.

2
00:00:01,430 --> 00:00:02,940
My name is I'm from Meta.

3
00:00:03,539 --> 00:00:08,700
Today, I'm excited to talk about like
how we can bring together AI DevSecOps

4
00:00:09,299 --> 00:00:13,590
ethics and accessibility to create
personalized learning pathways that

5
00:00:13,590 --> 00:00:17,840
truly like supports every student,
especially those with learning needs.

6
00:00:18,340 --> 00:00:20,530
So this talk is built
around like one core belief.

7
00:00:21,384 --> 00:00:27,414
AI and education must be secure,
transparent, and equitable if

8
00:00:27,414 --> 00:00:28,885
we wanted to like change lives.

9
00:00:29,385 --> 00:00:34,205
So currently, like where do we
stand the landscape, especially

10
00:00:34,205 --> 00:00:35,975
like the challenge in general.

11
00:00:36,515 --> 00:00:43,055
So we have like over 7.5 million US
students that receive services under idea.

12
00:00:43,555 --> 00:00:48,085
Which is nothing but the individuals
with Disabilities Education Act.

13
00:00:48,655 --> 00:00:51,745
So these students like still
experience persistent achievement

14
00:00:51,745 --> 00:00:56,744
gaps and the traditional instruction
just wasn't like built to adapt

15
00:00:57,014 --> 00:01:01,964
dynamically to like cognitive
sensory communication differences.

16
00:01:02,464 --> 00:01:08,119
So the opportunity is that here, like
how can we like adapt AI where we

17
00:01:08,119 --> 00:01:10,369
can personalize learning in general.

18
00:01:10,819 --> 00:01:16,160
And also in real time which can
like significantly improve reading

19
00:01:16,220 --> 00:01:19,940
outcomes for like students,
especially with like disabilities.

20
00:01:20,929 --> 00:01:26,079
So this is basically like
adapting AI for these students.

21
00:01:26,579 --> 00:01:31,249
And before jumping into like
how can we like, use AI in

22
00:01:31,249 --> 00:01:32,869
general for the, in the ed tech.

23
00:01:33,499 --> 00:01:35,689
So we also wanted to bake few things.

24
00:01:35,809 --> 00:01:41,929
How can we like, protect the user data in
general as we are like dealing sensitive

25
00:01:41,989 --> 00:01:45,229
data, especially related to like students.

26
00:01:46,099 --> 00:01:53,740
So dev, DevSecOps, like embeds like Ed at
each and every stage enables like scalable

27
00:01:53,800 --> 00:01:56,869
deployment and forces like compliance.

28
00:01:57,214 --> 00:02:02,164
With ferpa, GDPR and like the
accessibility laws in general and provides

29
00:02:02,164 --> 00:02:04,294
like transparency through audit trails.

30
00:02:04,834 --> 00:02:08,914
So in education, like trust isn't
optional, so it's essential.

31
00:02:09,245 --> 00:02:13,535
So these are the things that we
wanted to make sure the DevSecOps

32
00:02:13,535 --> 00:02:19,284
like can needs to be like baked
into the AI ed tech services.

33
00:02:19,784 --> 00:02:25,004
So this is like the overall picture on
a DevSecOps how we wanted to do it, but

34
00:02:25,004 --> 00:02:27,104
also measuring the impact in net tech.

35
00:02:27,584 --> 00:02:33,374
So using ai, the skills have been
like accelerated by 41%, like

36
00:02:33,374 --> 00:02:38,244
acquisition and the engagement has
been like significantly grown by 35%.

37
00:02:38,934 --> 00:02:42,864
And reading challenges can be like
detected as early as seven months.

38
00:02:43,374 --> 00:02:46,524
So using ai, like these are
like the things that were like

39
00:02:46,524 --> 00:02:48,724
achievable in like recent days.

40
00:02:49,164 --> 00:02:53,754
Which can like, actually impact the
long term economic for these students.

41
00:02:54,654 --> 00:03:00,214
And basically like how, how these
AI enabled ethics system have

42
00:03:00,214 --> 00:03:01,414
like threat modeling as well.

43
00:03:01,834 --> 00:03:06,094
So essentially like the
AI driven education like

44
00:03:06,094 --> 00:03:07,684
comes with some unique risk.

45
00:03:08,224 --> 00:03:13,324
So these include like exposure of like
sense to student data model inversion

46
00:03:13,324 --> 00:03:20,594
attacks, bias outcomes, and some spoofing
of assistive tech interface interfaces.

47
00:03:21,134 --> 00:03:22,784
And also like some insider threats.

48
00:03:23,339 --> 00:03:28,979
So these risks define the guardrails we
must build into the system from day one.

49
00:03:29,479 --> 00:03:31,869
I meant to say so these
are like some risks.

50
00:03:31,919 --> 00:03:35,429
And we wanted build some guardrails
around it and we build a system for

51
00:03:35,429 --> 00:03:40,389
the for the students in general who are
like using ai in their tech systems.

52
00:03:40,889 --> 00:03:43,369
So before before that, like.

53
00:03:44,045 --> 00:03:45,934
How can we use AI in general?

54
00:03:45,934 --> 00:03:49,114
This is just like an overall picture
that I want to like, talk about,

55
00:03:49,114 --> 00:03:52,734
like how these adapt algorithms
can, like personalize learning.

56
00:03:53,364 --> 00:03:57,354
There are like three major
ways we can make sure the

57
00:03:57,354 --> 00:03:59,154
adapt systems can personalize.

58
00:03:59,764 --> 00:04:02,224
One is like the modality adoption.

59
00:04:02,804 --> 00:04:06,404
The system identifies whether
the student can learn through

60
00:04:06,404 --> 00:04:08,084
like best visually auditorally.

61
00:04:08,584 --> 00:04:13,474
Can especially and improving the
engagement improving the engagement

62
00:04:13,474 --> 00:04:14,584
of the students in general.

63
00:04:15,094 --> 00:04:18,869
So one is the other the other one
is the difficulty calibration.

64
00:04:19,079 --> 00:04:23,999
So the task continuously can adjust
to keep the student in their optimal

65
00:04:23,999 --> 00:04:28,159
learning zones, resulting in like
more faster skills acquisition.

66
00:04:28,909 --> 00:04:32,559
This is one of the important thing
like to make it like optimal zone

67
00:04:32,559 --> 00:04:36,039
for the students in general so that
they can learn as soon as possible

68
00:04:36,539 --> 00:04:38,249
and the scaffolding intelligence.

69
00:04:38,369 --> 00:04:39,329
So what does this mean?

70
00:04:39,359 --> 00:04:43,559
So the system actually like, provides
the right level of support at the right

71
00:04:43,619 --> 00:04:49,469
time by providing hints, examples, and
some explanations where they can like

72
00:04:49,469 --> 00:04:51,809
gradually feel that support is not needed.

73
00:04:52,269 --> 00:04:56,279
And and so that they can like mastery
those skills as soon as possible.

74
00:04:56,699 --> 00:05:01,589
So as soon as like the, these
things like gradually fades

75
00:05:01,589 --> 00:05:03,629
away so the mastery increases.

76
00:05:04,019 --> 00:05:07,169
So this reduces the learning helplessness.

77
00:05:07,219 --> 00:05:09,429
And also and boost the
confidence in students.

78
00:05:09,929 --> 00:05:13,449
So this is how generally the, you
can have some adapter algorithms

79
00:05:13,449 --> 00:05:14,749
for personalized learning.

80
00:05:15,619 --> 00:05:22,139
Okay, so now coming to the security
privacy of designing these systems.

81
00:05:22,619 --> 00:05:26,039
So the systems have to
be like built securely.

82
00:05:26,649 --> 00:05:30,099
We must ensure that there are
like, all the compliances are met,

83
00:05:30,099 --> 00:05:32,439
like for example, the ferpa, GDPR.

84
00:05:32,769 --> 00:05:37,539
And we also need to like encrypt
the data, use zero trust security

85
00:05:37,539 --> 00:05:43,069
models and continuously like monitor
access and model behavior so that the

86
00:05:43,069 --> 00:05:46,909
student shouldn't have to like trade
privacy for like personalization.

87
00:05:47,329 --> 00:05:50,939
So this is like the overall design that
we wanted to have for the students.

88
00:05:51,504 --> 00:05:54,534
Basically not for the students,
but for the the system itself.

89
00:05:55,034 --> 00:06:00,824
And so that is like the overall picture
of the security and privacy and coming

90
00:06:00,824 --> 00:06:07,004
to the, how can we in general, like you
also need some message to technology along

91
00:06:07,004 --> 00:06:08,934
the way for building these technologies.

92
00:06:08,964 --> 00:06:12,504
So we are not leaving some of the core
essential parts like accessibility.

93
00:06:13,254 --> 00:06:17,204
Especially we need some seamless
screen data support, like text to

94
00:06:17,204 --> 00:06:22,044
speech input and like natural text
tope our ascension for, so that

95
00:06:22,044 --> 00:06:23,664
the students can easily understand.

96
00:06:24,624 --> 00:06:30,884
So with these things like set up the
students can see like nearly like 50%

97
00:06:30,944 --> 00:06:35,894
increase in independent task completion
and like even 40% increase in engagement.

98
00:06:36,254 --> 00:06:38,754
So accessibility is like the
core component and also like

99
00:06:38,754 --> 00:06:40,164
the performance amplifier.

100
00:06:40,664 --> 00:06:46,169
So you wanted to have like bridge these
gaps as well with AI and like the assist

101
00:06:46,229 --> 00:06:48,389
techno assist to technology integration.

102
00:06:48,989 --> 00:06:53,509
So coming into like the, yeah, this
is what I was like talking about, like

103
00:06:53,719 --> 00:06:55,159
increases the engagement in general.

104
00:06:55,834 --> 00:06:59,134
So when students can access their
content in their preferred modality,

105
00:06:59,194 --> 00:07:03,444
which we like spoke about previously
having having them to navigate smoothly

106
00:07:03,824 --> 00:07:07,714
which frustrat which kind of like
decreases the frustration and also the

107
00:07:07,714 --> 00:07:12,204
ta and also we can see like the reduce
in the task abandonments in general.

108
00:07:12,744 --> 00:07:16,254
So the more the intuitive the experience
is, the deeper the learning is.

109
00:07:16,374 --> 00:07:21,444
So that is what we have to like, make
sure we have all the systems in place

110
00:07:21,494 --> 00:07:25,874
like the accessibility engagement like
ski like this like the screen readers,

111
00:07:26,234 --> 00:07:30,814
like this speech to tech text or vice
versa so that they have like much

112
00:07:30,814 --> 00:07:33,544
more intuitive experience and they
have the deeper learning in general.

113
00:07:34,044 --> 00:07:38,494
So that is how we wanted to
define ai in the ed tech systems.

114
00:07:39,054 --> 00:07:40,944
Like a overall core goal.

115
00:07:41,124 --> 00:07:45,499
We wanted to make sure that the
students doesn't feel demotivated

116
00:07:45,589 --> 00:07:49,589
and have like more stronger focus
and more improved retention rate.

117
00:07:50,089 --> 00:07:56,139
And yeah, and coming to making the, how
the algorithm should behave as well.

118
00:07:56,769 --> 00:08:00,629
So in order to be like trusted
the AI must be like explainable.

119
00:08:01,199 --> 00:08:07,289
So we should use interpretable models
and we should like audit and like

120
00:08:07,289 --> 00:08:11,999
providing dashboards for the teachers and
providing clear explanation for families.

121
00:08:12,539 --> 00:08:16,199
So the transparency has to be
there so that the no addition

122
00:08:16,199 --> 00:08:17,669
feels like a black box in general.

123
00:08:18,299 --> 00:08:21,759
Because as we are like dealing
with student data especially,

124
00:08:22,259 --> 00:08:26,199
and and the other things like coming
to the transparency the one of the

125
00:08:26,199 --> 00:08:30,039
other things we need to take care
is like the mitigating the bias.

126
00:08:30,849 --> 00:08:35,519
So bias in like educational
AI has like real consequences.

127
00:08:36,134 --> 00:08:40,154
Where the thing is like we need to
like combat this using like diverse

128
00:08:40,154 --> 00:08:46,274
training data, like fairness metrics
and routine audits and inclusive,

129
00:08:46,304 --> 00:08:47,894
like design practices in general.

130
00:08:48,344 --> 00:08:52,204
So the solution is to have like
proactive bias detection and

131
00:08:52,204 --> 00:08:55,664
mitigation which must be like, embedded
into the development life cycle.

132
00:08:56,174 --> 00:09:00,794
So it's not like a checkbox, it's a
continuous responsibility so that we like.

133
00:09:01,319 --> 00:09:04,049
Mitigate the bias in
general in these systems.

134
00:09:04,549 --> 00:09:09,609
So the tech, the now we are actually
even like looking at these systems like

135
00:09:09,609 --> 00:09:14,249
making what they call it this technology
is done meant replace the educators

136
00:09:14,249 --> 00:09:15,539
or like the teachers in general.

137
00:09:15,869 --> 00:09:20,049
So it has to it's ma it's basically
like helping the teachers in a way.

138
00:09:20,604 --> 00:09:24,064
Over here, I meant to say it deliver
relieves from a few of the things

139
00:09:24,064 --> 00:09:28,234
like automatic grade using the ai
you can do, like automatic grading

140
00:09:28,524 --> 00:09:30,084
adapting the content in general.

141
00:09:30,624 --> 00:09:33,349
And so that the, it's,
it frees the teachers.

142
00:09:34,329 --> 00:09:39,179
To focus mostly on having an emotional
connect and providing like context

143
00:09:39,209 --> 00:09:42,709
and providing like individual
support to the students so that the

144
00:09:42,709 --> 00:09:44,729
AI can really help in other ways.

145
00:09:45,179 --> 00:09:48,859
And also like making the teachers to have
that special connection with the students.

146
00:09:49,339 --> 00:09:53,509
So it like arguments together in a
best way not to replace everyone.

147
00:09:53,689 --> 00:09:56,429
That is like the main motivation
that we need to like, take

148
00:09:56,429 --> 00:09:57,389
into the account as well.

149
00:09:57,889 --> 00:10:03,269
This is like how the ai has to be like
baked into for the EdTech systems.

150
00:10:03,689 --> 00:10:07,519
Now it comes to the picture of
like how we can set up a blueprint

151
00:10:07,519 --> 00:10:08,929
in like implementing this.

152
00:10:09,469 --> 00:10:13,559
So we respond like how, like
the DevSecOps can help us.

153
00:10:13,609 --> 00:10:15,419
Operationalize these things.

154
00:10:15,899 --> 00:10:19,639
So we need to like look into like few
things like infrastructure as a core,

155
00:10:20,149 --> 00:10:23,959
automated like security and accessibility,
like testing through each and every

156
00:10:23,959 --> 00:10:28,849
scenario and need to see checking that
like model signing and like tamper

157
00:10:28,849 --> 00:10:34,589
detection is in place like continuous
monitoring and iterative, like fast back.

158
00:10:34,769 --> 00:10:38,034
It to like iterative feedback
driven improvement so that the.

159
00:10:38,714 --> 00:10:42,714
You have if anything like goes out
of picture so we need to make sure

160
00:10:42,714 --> 00:10:46,104
the platform like evolves better
for like students and educators.

161
00:10:46,534 --> 00:10:50,264
That is like the core blueprint
in this AI tech related.

162
00:10:51,254 --> 00:10:56,324
And how do we like, see like pipeline,
these adapt to like learning models.

163
00:10:56,909 --> 00:11:00,689
So the things that we need to like
look into is like some of the data sets

164
00:11:00,689 --> 00:11:06,179
and models and like making sure every
scanning like takes place like secretary

165
00:11:06,179 --> 00:11:10,649
scanning at each and every step and
ensuring like you have the fair pan

166
00:11:10,649 --> 00:11:16,679
GDPR checks and mitigating like bias and
like drift through some like fairness

167
00:11:16,679 --> 00:11:19,159
tests to make sure there is less bias.

168
00:11:19,864 --> 00:11:22,944
And and continuous like
monitoring of the model behavior.

169
00:11:23,434 --> 00:11:27,804
That is what we wanted to like look into
some learning like the adapt learning

170
00:11:27,804 --> 00:11:33,114
models in this like DevSecOps pipeline
and during this the same pipeline.

171
00:11:33,184 --> 00:11:36,484
The thing is like we wanted to
look is like how can we like secure

172
00:11:36,484 --> 00:11:38,934
the the student data lifecycle?

173
00:11:39,624 --> 00:11:42,584
The thing is like we need
to make sure collect like

174
00:11:42,584 --> 00:11:44,534
minimally, like minimal encrypt.

175
00:11:45,154 --> 00:11:50,344
We collect like minimally and like encrypt
thoroughly to make sure like the data is

176
00:11:50,344 --> 00:11:55,604
like secure and and also like we need to
make sure the student has the consent has.

177
00:11:56,399 --> 00:12:00,959
Taken and delete any logs in
general for only like storing it

178
00:12:00,959 --> 00:12:02,549
for like certain period of time.

179
00:12:03,029 --> 00:12:06,139
And also like taking the pay
and consent and revoking if

180
00:12:06,139 --> 00:12:07,709
needed if they don't need it.

181
00:12:07,739 --> 00:12:11,089
So the privacy is like
non-negotiable in this in this case.

182
00:12:11,479 --> 00:12:14,949
So we may, we need to make sure
that the student data has like

183
00:12:14,949 --> 00:12:16,499
a strict life cycle in general.

184
00:12:17,369 --> 00:12:20,709
Along this process it comes into
the picture of each region has

185
00:12:20,709 --> 00:12:22,529
their own compliances as well.

186
00:12:22,529 --> 00:12:24,689
For example, like the compliance score.

187
00:12:24,739 --> 00:12:30,734
We need to embed like multiple
compliances like ferpa, GDPR A-D-A-A-D-A

188
00:12:30,734 --> 00:12:34,154
is nothing but like the Americans
with Disabilities Act and V sag.

189
00:12:34,604 --> 00:12:36,044
It's nothing but like the web content.

190
00:12:36,549 --> 00:12:42,639
Accessibility guidelines, so me
ensuring like these are in place and

191
00:12:42,689 --> 00:12:46,699
and checks which checks di directly
into the continuous integration and

192
00:12:46,699 --> 00:12:48,409
like continuous deployment pipelines.

193
00:12:49,099 --> 00:12:53,029
And making sure we are like auditing
the each and everything in place.

194
00:12:53,079 --> 00:12:56,629
So if anything comes up you are like
protecting the user data in general.

195
00:12:57,124 --> 00:13:02,494
So this makes like the compliance, like
proactive rather than like reactive.

196
00:13:03,004 --> 00:13:04,144
So yeah.

197
00:13:04,204 --> 00:13:06,864
So the compliance must be
like continuous in general.

198
00:13:07,364 --> 00:13:11,954
But so this is what I was saying so
it has to be like the compliance has

199
00:13:11,954 --> 00:13:14,644
to like balance with the innovation.

200
00:13:15,079 --> 00:13:18,629
So you have to like, follow some
the DevSecOps transforms the

201
00:13:18,629 --> 00:13:21,299
tension between the innovation
and regulation into strength.

202
00:13:21,749 --> 00:13:25,769
So you need to have automated,
like guardrails, allowing teams

203
00:13:25,769 --> 00:13:29,609
to like, innovate quickly while
ensuring like trust and safety.

204
00:13:29,849 --> 00:13:34,249
So the balance has to be there in
between these two things and making

205
00:13:34,249 --> 00:13:36,829
sure that we are in compliance.

206
00:13:37,329 --> 00:13:41,409
And so these are like some of the lessons
for AI and like regulator sectors.

207
00:13:42,039 --> 00:13:45,899
The thing is like key lessons,
which include security first, like

208
00:13:46,019 --> 00:13:48,279
design which prevents like rework.

209
00:13:48,879 --> 00:13:52,359
So accessibility issues must
be treated as a severity one.

210
00:13:52,879 --> 00:13:57,209
Because without these especially we
as we are like dealing with diversify

211
00:13:57,659 --> 00:13:59,609
students with di diversifying needs.

212
00:14:00,159 --> 00:14:03,219
So the accessibility is a core
issue which I spoke earlier as well

213
00:14:03,269 --> 00:14:05,009
which has the performance one mark.

214
00:14:05,579 --> 00:14:10,799
And and auditing the bias runs in
production and having like privacy tools,

215
00:14:10,799 --> 00:14:15,179
friendly making sure that everyone has
the access to these data, especially

216
00:14:15,179 --> 00:14:16,679
the students and their parents.

217
00:14:17,159 --> 00:14:21,469
So these are like some things we need
to like, keep in mind because these are

218
00:14:21,469 --> 00:14:25,239
like the crucial stakeholders and, we
wanted to make sure like everyone is

219
00:14:25,239 --> 00:14:31,189
safe using the AI and ed tech and there,
and we are not doing any privacy leaks.

220
00:14:31,689 --> 00:14:32,049
Yeah.

221
00:14:32,379 --> 00:14:38,839
So that is what, like the trust, like
building transparency, inclusion, ethical

222
00:14:38,839 --> 00:14:40,729
thinking and like secure engineering.

223
00:14:41,179 --> 00:14:46,009
So when DevSecOps, like guides adaptive
learning, the result in systems.

224
00:14:46,509 --> 00:14:49,449
Like safe, scalable,
and like human centered.

225
00:14:50,019 --> 00:14:54,379
So we need to build that trust
using this AI enabled education.

226
00:14:54,879 --> 00:14:57,789
So that comes to like closing.

227
00:14:57,959 --> 00:15:02,349
So the key takeaways from these, like the
prac basically the practical path forward.

228
00:15:03,069 --> 00:15:06,009
So we need to like, start
with security and ethics.

229
00:15:06,509 --> 00:15:08,159
Basically like design for
like diverse learners.

230
00:15:08,909 --> 00:15:13,389
Keep educators at at core and commit
to like continuous improvement.

231
00:15:13,979 --> 00:15:15,959
The current presentation
provides like a blueprint.

232
00:15:15,989 --> 00:15:19,849
So the tools are here and
it's now the time to build

233
00:15:19,909 --> 00:15:22,009
AI and education responsibly.

234
00:15:22,579 --> 00:15:26,689
So embrace the, like the
continuous implement in general.

235
00:15:26,809 --> 00:15:30,679
So the DevSecOps kind of like
rapidly, like enables like

236
00:15:30,679 --> 00:15:32,269
rapid learning and refinement.

237
00:15:32,719 --> 00:15:34,249
So yeah.

238
00:15:34,999 --> 00:15:38,679
Now it's like time to build AI and
education responsibility using some of

239
00:15:38,679 --> 00:15:41,624
these core blueprint ideas in general.

240
00:15:42,104 --> 00:15:42,764
Thank you.

241
00:15:42,884 --> 00:15:46,574
Yeah, I would like to hear like
some some of the questions feel free

242
00:15:46,574 --> 00:15:48,554
to drop me a message or email me.

243
00:15:49,184 --> 00:15:50,894
I'll try to respond as soon as possible.

244
00:15:51,524 --> 00:15:52,034
Thank you.

