1
00:00:00,000 --> 00:00:00,810
Hi everyone.

2
00:00:00,840 --> 00:00:04,890
Welcome to my presentation today on
the topic LLM Enhanced Multimodal

3
00:00:04,920 --> 00:00:09,420
AI and how it is revolutionizing
the audio interaction technologies.

4
00:00:10,020 --> 00:00:11,070
A little bit about me.

5
00:00:11,130 --> 00:00:14,790
I'm a senior staff software engineer
at Intuit with over 17 years of

6
00:00:14,790 --> 00:00:18,000
software development experience
and full stack specializing

7
00:00:18,000 --> 00:00:19,680
in mobile and AI technologies.

8
00:00:19,920 --> 00:00:23,430
Currently I lead the development
of Gen AI mobile applications.

9
00:00:23,805 --> 00:00:27,525
I'm passionate about technology
and I love learning and sharing my

10
00:00:27,525 --> 00:00:31,634
knowledge through blocks, webinars,
publications, and patented innovations.

11
00:00:32,384 --> 00:00:33,165
Let's get started.

12
00:00:33,775 --> 00:00:37,675
as we see, there's a significant
search in the audio content in the

13
00:00:37,825 --> 00:00:42,655
past few years, particularly podcasts,
audio, audio books, and online courses

14
00:00:42,655 --> 00:00:44,235
becoming, much more mainstream.

15
00:00:44,535 --> 00:00:48,285
They emerge as an essential challenge,
which is a content overload.

16
00:00:48,585 --> 00:00:51,615
So with so much material
available for listeners, they get.

17
00:00:51,975 --> 00:00:54,645
Pretty overwhelmed with the
share volume of audio content.

18
00:00:55,135 --> 00:00:57,715
and this could lead to
navigational difficulties.

19
00:00:57,715 --> 00:01:02,665
Unlike text-based mediums where you can
easily search or skim for a specific

20
00:01:02,665 --> 00:01:04,465
information with a simple command.

21
00:01:04,465 --> 00:01:09,580
F for a control f. Audio is inherently
linear, and listeners find it

22
00:01:09,580 --> 00:01:13,720
cumbersome to manually navigate
through hours of content without,

23
00:01:13,770 --> 00:01:17,220
a clear way to locate a specific
information that they're looking for.

24
00:01:17,910 --> 00:01:21,450
Ultimately, this creates a gap
between what listeners want and

25
00:01:21,450 --> 00:01:22,740
what is currently available.

26
00:01:23,200 --> 00:01:26,830
users are looking for a solution
that allows them to access relevant

27
00:01:26,830 --> 00:01:30,670
segments quickly, enabling them to
enjoy the benefits of audio content

28
00:01:30,670 --> 00:01:32,470
without feeling lost in the noise.

29
00:01:32,880 --> 00:01:37,740
Addressing these challenges is what our
AI part solution aims to accomplish.

30
00:01:38,240 --> 00:01:42,950
Before we get into the solution, let's
take a look at what multimodal AI is.

31
00:01:43,220 --> 00:01:48,430
Multimodal AI refers to a system that
integrates and analyzes, multiple types

32
00:01:48,430 --> 00:01:54,660
of data, such as text, images, audio,
and video simultaneously to, to enhance

33
00:01:54,660 --> 00:01:56,820
understanding or improve decision making.

34
00:01:57,479 --> 00:02:02,179
So our multimodal AI part solution
addresses, these challenges

35
00:02:02,179 --> 00:02:05,659
with the following key AI
driven technologies, which is.

36
00:02:05,974 --> 00:02:12,714
Speaker Diarization, which automatically
identifies who spoke when in an audio file

37
00:02:13,044 --> 00:02:17,304
and topic segmentation, which divides an
audio recording into meaningful segments

38
00:02:17,304 --> 00:02:21,924
based on the content and multimodal
search interface allows users to interact

39
00:02:21,924 --> 00:02:24,414
with audio via text and voice queries.

40
00:02:25,209 --> 00:02:30,510
we leverage the advanced AI models
for this, like open AI whisper for

41
00:02:30,510 --> 00:02:36,289
speech to text transcription, or Google
Gemini and various, NLP algorithms

42
00:02:36,289 --> 00:02:38,629
for content indexing and other things.

43
00:02:39,129 --> 00:02:43,389
speaker Diarization is a process that
determines who's speaking at a given

44
00:02:43,389 --> 00:02:47,824
point in time, and this is essential
in a. In a multi-speaker setting, such

45
00:02:47,824 --> 00:02:51,864
as podcasts and panel discussions,
the traditional systems, they often

46
00:02:51,864 --> 00:02:56,435
struggle with accuracy because, they're
often based on, identifying based on

47
00:02:56,435 --> 00:03:00,114
the, based on the sounds, but not the
context and the conversation history.

48
00:03:00,605 --> 00:03:05,785
but the proposed AI part approach, aims
to reduce the diarization, error, rate

49
00:03:05,815 --> 00:03:08,065
to improve the speaker identification.

50
00:03:08,674 --> 00:03:14,635
our system also creates, dynamic speaker
profiles and metrics showing, speakers

51
00:03:14,635 --> 00:03:16,434
bio on their speaking frequency.

52
00:03:16,434 --> 00:03:20,804
This means a listener can actually look
at, all the topics that the user has, that

53
00:03:20,804 --> 00:03:25,635
the speaker has spoken and, they could
jump to a particular segment without.

54
00:03:25,819 --> 00:03:29,709
and skip through all the sections that
are not, that, that do not interest them.

55
00:03:30,609 --> 00:03:35,429
The user could say, query with
simple things like watch all segments

56
00:03:35,429 --> 00:03:40,549
where Jill spoke, or take me to
a, segment where John is speaking.

57
00:03:41,269 --> 00:03:46,399
So these are some examples, and here
is an example of how a traditional

58
00:03:46,399 --> 00:03:47,989
audio-based system looks like with.

59
00:03:48,059 --> 00:03:52,979
multiple speakers discussing in your,
in the top part of the screen where,

60
00:03:53,069 --> 00:03:58,059
you know they're introducing themself,
talking about inflation, mortgage,

61
00:03:58,149 --> 00:03:59,859
job market, and so on and so forth.

62
00:04:00,549 --> 00:04:03,729
once the speaker diarization
happens, you would be able to get the

63
00:04:03,729 --> 00:04:07,449
timestamps or the speaker segments
as you could see in this example

64
00:04:07,449 --> 00:04:09,069
with, in the bottom of your screen.

65
00:04:09,439 --> 00:04:14,279
With, Imani speaking, zero to two
minutes and Jill speaking on different

66
00:04:14,279 --> 00:04:17,339
segments at two minutes, 10 minutes,
20 minutes, and so on and so forth.

67
00:04:17,789 --> 00:04:22,509
And, at the end of the Diarization
process, you expect, a response to be

68
00:04:22,509 --> 00:04:24,269
written with this, with this values.

69
00:04:24,769 --> 00:04:29,359
So topic segmentation helps organize
audio content into meaningful segments.

70
00:04:29,409 --> 00:04:30,194
our system applies.

71
00:04:30,724 --> 00:04:35,765
NLP techniques like cosign similarity
or term frequency inverse document T,

72
00:04:35,765 --> 00:04:41,255
tf, IDF, to detect the topic, boundaries
and group related content together.

73
00:04:41,614 --> 00:04:45,755
For example, in a two hour long
podcast, if you wanna listen to only

74
00:04:45,755 --> 00:04:49,924
discussions about inflation, you
should be able to instantly jump

75
00:04:49,924 --> 00:04:53,075
to the relevant sections instead of
skimming through the entire episode.

76
00:04:53,575 --> 00:04:57,415
Our multimodal search interface
allows user to search for content,

77
00:04:57,465 --> 00:04:58,905
using text or voice queries.

78
00:04:59,235 --> 00:05:03,985
So the AI retrieves, answers, based on
context instead of relying on generic

79
00:05:03,985 --> 00:05:09,365
keyword matches, for example, a listener
can ask a very, generic question.

80
00:05:09,395 --> 00:05:13,155
Things like what's lottery
starts on, rising prices.

81
00:05:13,485 --> 00:05:17,265
The question doesn't necessarily tell
you that this is about inflation, but the

82
00:05:17,265 --> 00:05:23,545
system, since it's using, NLP and ai, it
should be able to map it, and get to the.

83
00:05:23,600 --> 00:05:27,020
relevant sections of the video
that, that match with this criteria.

84
00:05:27,690 --> 00:05:32,880
another query could be, did anyone in the
panel talk about interest rates or did

85
00:05:32,880 --> 00:05:35,580
Jill express hope for a better economy?

86
00:05:35,980 --> 00:05:38,830
and the system, if you see, these are
all generic queries and the system

87
00:05:38,830 --> 00:05:42,280
should be robust enough to handle
and give you the segments, as needed.

88
00:05:42,780 --> 00:05:47,000
To further enhance engagement, our
system includes, dynamic annotations

89
00:05:47,000 --> 00:05:51,230
with, key points appearing during
playback, follow up links so

90
00:05:51,230 --> 00:05:54,650
that the user can explore related
content without searching manually.

91
00:05:55,180 --> 00:05:58,930
integrated note taking, listeners
can add timestamp notes for future

92
00:05:58,930 --> 00:06:03,160
references, and this whole thing
transforms the passive learning

93
00:06:03,160 --> 00:06:05,410
into a very interactive experience.

94
00:06:05,920 --> 00:06:12,210
So the system also tries to, automatically
index based on various criteria, including

95
00:06:12,570 --> 00:06:17,880
speakers, topics, timestamps, so users can
reduce structured segments efficiently.

96
00:06:17,970 --> 00:06:23,000
For example, educators can use this for
organizing their lecture recordings,

97
00:06:23,050 --> 00:06:26,480
picking, picking topics from various
videos and making them easier for the

98
00:06:26,480 --> 00:06:28,460
students to find specific discussions.

99
00:06:28,960 --> 00:06:31,090
User feedback is another crucial layer.

100
00:06:31,150 --> 00:06:35,490
Our system integrates a rating system
to evaluate, segment relevance.

101
00:06:35,490 --> 00:06:39,930
As you might know, as you might
wanna know how a certain segment

102
00:06:39,930 --> 00:06:43,560
in a video is received and not base
your opinion on the entire video or

103
00:06:43,560 --> 00:06:45,030
the feedback for the entire video.

104
00:06:45,390 --> 00:06:48,690
It happens all the time where people
might like certain portions of the

105
00:06:48,690 --> 00:06:50,220
video, but not the entire video.

106
00:06:50,675 --> 00:06:55,145
the users would be able to rate segments
of the video on not just the entire video.

107
00:06:55,145 --> 00:07:00,360
This helps you, make much more,
data driven decisions or sentiment

108
00:07:00,360 --> 00:07:04,330
driven decisions, a common section
and which is pretty standard.

109
00:07:04,390 --> 00:07:07,990
And also an analytics dashboard
for content creators to understand

110
00:07:07,990 --> 00:07:09,220
their audience preferences.

111
00:07:09,720 --> 00:07:13,350
Now let's dive into the technical
details of this, that powers the

112
00:07:13,350 --> 00:07:15,310
audio base or, navigation system.

113
00:07:15,740 --> 00:07:20,310
this system essentially consists of,
four layers, input layer, which converts

114
00:07:20,340 --> 00:07:24,990
a raw audio into structured text, with
timestamp, processing layer, which

115
00:07:24,990 --> 00:07:30,300
uses the AI driven speaker diarization
to identify speaker and speakers

116
00:07:30,300 --> 00:07:32,160
and topic segmentation to break.

117
00:07:32,400 --> 00:07:34,500
Content into meaningful sections.

118
00:07:34,990 --> 00:07:39,930
indexing layer, it stores the structured
metadata for quick search and retrieval,

119
00:07:40,270 --> 00:07:44,310
interaction layer, which, or the
feedback layer, which will enable

120
00:07:44,310 --> 00:07:48,490
search and playback functionality
using, multimodal input such as text

121
00:07:48,490 --> 00:07:53,800
queries, voice commands, and contextual
recommendations, as well as LLC users to.

122
00:07:54,125 --> 00:07:58,235
provide, detailed feedback on,
digging through various aspects

123
00:07:58,235 --> 00:08:01,655
of a video, which will help the
content recommend, content creators.

124
00:08:02,155 --> 00:08:05,355
alright, let's talk a little
bit about the, the input layer.

125
00:08:05,405 --> 00:08:09,695
here, the audio processing is the first
step in our pipeline where we convert

126
00:08:09,695 --> 00:08:13,905
the raw audio into structured text
using, a speech transcription model,

127
00:08:13,905 --> 00:08:16,915
open AI whisper, or, Google Gemini.

128
00:08:16,965 --> 00:08:18,145
there are quite a few other ones.

129
00:08:18,595 --> 00:08:19,705
Now, why is this important?

130
00:08:19,705 --> 00:08:22,675
audio files by themself are
not very useful for search,

131
00:08:22,675 --> 00:08:23,905
so they don't have structure.

132
00:08:24,265 --> 00:08:28,765
So if we have timestamp for accurate
playback, we would be able to build

133
00:08:28,765 --> 00:08:33,965
high quality UI where the users, based
on these APIs, the users could actually

134
00:08:33,965 --> 00:08:38,045
go to a particular portion of a video,
highlighting, pro providing high

135
00:08:38,045 --> 00:08:40,115
quality speech to text transcription.

136
00:08:40,575 --> 00:08:42,315
will definitely help.

137
00:08:42,365 --> 00:08:47,665
if you have the transcription
accurately, accurately, laid out as

138
00:08:47,665 --> 00:08:52,055
well as the transcription playback
is, is the right playback, it

139
00:08:52,055 --> 00:08:53,555
matches with your, with the input.

140
00:08:53,625 --> 00:08:57,085
audio speed, whisper,
tends to perform better.

141
00:08:57,085 --> 00:08:59,275
At least that's the
experience that I have.

142
00:08:59,325 --> 00:09:00,315
it gives you the exact.

143
00:09:01,230 --> 00:09:05,750
timestamps as well as the, as well as
the exact time, playback speed so that

144
00:09:05,750 --> 00:09:10,230
if you're building experiences, you
are accurately landing on the right

145
00:09:10,380 --> 00:09:12,460
point, when you search for a keyword.

146
00:09:13,160 --> 00:09:13,610
so.

147
00:09:14,440 --> 00:09:15,850
let's check out the code a little bit.

148
00:09:15,880 --> 00:09:20,710
we send the audio file to whisper
API and it returns a structured text.

149
00:09:21,170 --> 00:09:23,720
the, this is just a pseudo code example.

150
00:09:23,930 --> 00:09:27,700
The response include, word level
timestamps, which means we can align,

151
00:09:27,750 --> 00:09:29,730
spoken words to actual audio moments.

152
00:09:30,180 --> 00:09:33,450
And the output also has things
like, an entire transcript.

153
00:09:33,560 --> 00:09:38,640
Plus metadata, like duration, detected
language, word timing, and all of these

154
00:09:38,640 --> 00:09:40,410
would be used in the, in the next layers.

155
00:09:40,950 --> 00:09:42,360
And why is this important?

156
00:09:42,360 --> 00:09:46,800
Because if a user searches for a phrase,
we should be able to jump straight

157
00:09:46,800 --> 00:09:48,330
to the exact moment it was spoken.

158
00:09:48,870 --> 00:09:52,820
And, and these step is essential
for the, for the next layers.

159
00:09:53,320 --> 00:09:56,920
let's talk about Speaker Diarization,
which is another step in the,

160
00:09:57,020 --> 00:10:03,150
processing layer and how we are making
it better using the, NLPs and LLMs.

161
00:10:03,400 --> 00:10:08,030
normally Speaker Diarization is
done with, caustic, models that just

162
00:10:08,030 --> 00:10:11,780
try to figure out who's speaking
based on voice characteristics.

163
00:10:11,780 --> 00:10:13,820
But, sometimes people.

164
00:10:14,130 --> 00:10:18,970
multiple people sound similar or the
audio quality, isn't great all the time.

165
00:10:19,390 --> 00:10:22,390
So here's what we do instead,
we start with the transcription.

166
00:10:22,460 --> 00:10:26,410
we get the words, timestamps, and the
entire, metadata that we just discussed.

167
00:10:27,010 --> 00:10:29,590
We chunk the transcript into segments.

168
00:10:29,650 --> 00:10:33,060
We assume that if there's, the
system makes some, calculated

169
00:10:33,060 --> 00:10:34,890
assumptions, based on pauses.

170
00:10:35,370 --> 00:10:39,530
instead of, relying on the voice
characteristics, the system also,

171
00:10:39,580 --> 00:10:44,680
analyzes the, conversation history
or, what's being spoken, based on

172
00:10:44,680 --> 00:10:46,240
the context and things like that.

173
00:10:46,790 --> 00:10:50,610
and this will give you, a structured
JSON that tells you who spoke when, an

174
00:10:50,610 --> 00:10:55,210
example of a query, in this layer would
look something like, if you want to

175
00:10:55,210 --> 00:11:00,610
use that, bill, UI for these APIs who
spoke in each segment of this podcast.

176
00:11:00,610 --> 00:11:03,400
This could be a simple query
and that should give you a list

177
00:11:03,400 --> 00:11:05,570
of, segments for each speaker.

178
00:11:06,070 --> 00:11:09,770
And here's a simple pseudocode
example on, how this thing works.

179
00:11:09,770 --> 00:11:14,660
So we get the transcript, chunk it,
based on, some calculated guesses.

180
00:11:14,690 --> 00:11:19,130
and, by the LLMs and formatted,
formatted, we call the LLMs with,

181
00:11:19,180 --> 00:11:23,920
our structured prompt and it written
speaker assignments, we pass that

182
00:11:23,920 --> 00:11:28,650
output into a JSON format that's easier
to use and to build any experiences.

183
00:11:29,195 --> 00:11:32,495
so this is great for podcast
meetings, interviews, basically any

184
00:11:32,495 --> 00:11:37,025
conversation where traditional speaker
diarization struggles and it's more

185
00:11:37,025 --> 00:11:41,085
accurate when it, because it actually
understands, more than just the voice.

186
00:11:41,095 --> 00:11:44,805
but the, the context and it does
a lot of analysis on what's being

187
00:11:44,805 --> 00:11:46,515
spoken and what's being discussed.

188
00:11:47,015 --> 00:11:53,165
topic segmentation uses, NLP and LLMs
to divide and classify the topics.

189
00:11:53,465 --> 00:11:58,065
segment them, segment transcripts
using, timestamps and detect,

190
00:11:58,115 --> 00:11:59,885
topic shifts in conversations.

191
00:12:00,155 --> 00:12:05,155
So it can also categorize segments
into themes, and use LLMs to assign

192
00:12:05,185 --> 00:12:09,495
topic labels for audio systems
and developers can build, these,

193
00:12:09,495 --> 00:12:11,685
multifaceted UIs using these APIs.

194
00:12:12,185 --> 00:12:14,465
an example query, of this layer.

195
00:12:14,495 --> 00:12:18,425
how this would be useful is
if you wanna find all sections

196
00:12:18,425 --> 00:12:21,125
using AI ethics, in this podcast.

197
00:12:21,435 --> 00:12:25,455
find all sections, during discussing
AI ethics in this podcast.

198
00:12:25,605 --> 00:12:29,915
And that should be able to get you a
list of items, of all the portions or

199
00:12:29,915 --> 00:12:31,955
sections that talk about this topic.

200
00:12:32,455 --> 00:12:37,615
And this is, once the, once you have
these APIs and the timestamps, the, the

201
00:12:37,615 --> 00:12:41,485
next steps are usually faster because
you're not analyzing the entire video.

202
00:12:41,485 --> 00:12:43,255
You're just, referring
through your metadata.

203
00:12:43,315 --> 00:12:47,025
And you are, you're playing back
based on, a certain timestamp,

204
00:12:47,085 --> 00:12:48,855
based on the criteria that is given.

205
00:12:48,855 --> 00:12:53,595
Either, either it's a speaker that
you wanna land, it based, a speaker.

206
00:12:54,055 --> 00:12:57,795
speaking, a particular topic that you
wanna land on or a topic itself that's

207
00:12:57,795 --> 00:12:59,205
segmented that you want to land on.

208
00:12:59,255 --> 00:13:02,875
so it becomes much faster because you're
just dealing with the metadata and not

209
00:13:02,875 --> 00:13:04,465
analyzing the video again and again.

210
00:13:04,985 --> 00:13:08,525
let's discuss how we handle topic
segmentation, using the transcript

211
00:13:08,525 --> 00:13:10,525
from the, from the input layer.

212
00:13:11,020 --> 00:13:13,150
in a long form audio topics shift.

213
00:13:13,180 --> 00:13:17,170
frequently users want to jump
directly to relevant sections

214
00:13:17,170 --> 00:13:18,700
without scrapping to recordings.

215
00:13:19,030 --> 00:13:23,740
So our solution automates topic detection
and labeling for seamless navigation.

216
00:13:24,130 --> 00:13:27,690
So we chunk the transcript, use the,
output that we received from the

217
00:13:27,690 --> 00:13:32,470
previous layer with word timestamp,
group the words together into, on

218
00:13:32,470 --> 00:13:34,750
regular intervals for adequate context.

219
00:13:35,305 --> 00:13:37,655
compute, text, similar, text similarity.

220
00:13:38,075 --> 00:13:38,915
Convert the segment.

221
00:13:38,915 --> 00:13:43,965
text into TFID effect, calculate
similarity between adjucent

222
00:13:44,015 --> 00:13:45,875
segments to assess the relevance.

223
00:13:46,275 --> 00:13:51,215
detect the topic shifts, assign topics
using, LLMs, send the segmented text

224
00:13:51,215 --> 00:13:57,535
to GPT-4 or, or relevant, models for
descriptive topic labeling and structured

225
00:13:57,535 --> 00:13:59,245
the output for search and playback.

226
00:13:59,795 --> 00:14:00,305
and.

227
00:14:00,330 --> 00:14:03,870
users should be able to quickly search
and jump to any topic that they like.

228
00:14:04,370 --> 00:14:05,210
Indexing layer.

229
00:14:05,210 --> 00:14:08,840
Once we have the structured data from
processing layer, which includes,

230
00:14:08,890 --> 00:14:13,480
sp label, text, and topic segmented
content, we need a fast and scalable

231
00:14:13,480 --> 00:14:14,740
way to search and retrieve it.

232
00:14:15,100 --> 00:14:18,100
And this is often now done on
metadata, so it should be faster.

233
00:14:18,100 --> 00:14:19,660
So how does the indexing layer work?

234
00:14:19,660 --> 00:14:21,460
It stores the segments efficiently.

235
00:14:21,820 --> 00:14:24,100
Each segment is stored in
a database with a topic.

236
00:14:24,135 --> 00:14:28,200
or speaker or start end and, start
date, start time and end time.

237
00:14:28,440 --> 00:14:32,760
And this allows us to map conversation
to structured metadata for easier lookup.

238
00:14:33,240 --> 00:14:38,430
And the next step would be creating
faster, indexes where we index the

239
00:14:38,430 --> 00:14:41,370
data by topic, speaker, and timestamps.

240
00:14:41,725 --> 00:14:46,125
And this enables quick, full text
searches so users can jump to any

241
00:14:46,125 --> 00:14:51,205
sections instantly and retrieving
segments versus, multiple, filters.

242
00:14:51,305 --> 00:14:53,425
retrieving segments,
versus multiple filters.

243
00:14:53,425 --> 00:14:56,725
users can actually search by
different things like topics,

244
00:14:56,725 --> 00:14:58,165
speaker, timestamp, et cetera.

245
00:14:58,585 --> 00:15:02,625
And this is optimized for speed and
scalability and exposing APIs, and it

246
00:15:02,625 --> 00:15:07,805
exposes APIs for, for the clients to
make, build those experiences, with

247
00:15:07,805 --> 00:15:09,355
search, based on different criteria.

248
00:15:09,735 --> 00:15:11,085
And why this is powerful.

249
00:15:11,085 --> 00:15:14,835
This is powerful because users can
find exactly what they need without

250
00:15:14,835 --> 00:15:18,105
scrubbing through long videos, and they
have different criteria to search from.

251
00:15:18,605 --> 00:15:23,275
And, just a high level depiction of how,
different layers of this, indexing layer.

252
00:15:23,275 --> 00:15:26,635
So the input of this layer would be the
data reiterate from the processing layer.

253
00:15:27,020 --> 00:15:29,780
Things like topic, speaker,
certain end times, et cetera.

254
00:15:30,300 --> 00:15:34,290
the output of this layer is expected
to be an index storage and for enabling

255
00:15:34,290 --> 00:15:36,400
faster or, faster search or indexing.

256
00:15:36,730 --> 00:15:42,270
First step would be to save the data, in,
and then, index the data by key attributes

257
00:15:42,270 --> 00:15:44,610
such as topic, speaker, and timestamps.

258
00:15:44,980 --> 00:15:48,240
enable full tech search,
capabilities to handle keyword

259
00:15:48,240 --> 00:15:50,370
queries effectively and support.

260
00:15:50,660 --> 00:15:55,510
Time-based, queries, so that you can
jump to a particular, time in the video.

261
00:15:55,970 --> 00:15:57,350
for optimizing performance.

262
00:15:57,410 --> 00:16:01,790
we use the caching strategies to,
accelerate, frequent queries or

263
00:16:01,855 --> 00:16:05,695
implement ation to manage large
sets of DA data efficiently.

264
00:16:06,075 --> 00:16:10,255
Or, if you want to go advance,
you can build a rag with the, data

265
00:16:10,285 --> 00:16:13,495
to store and retrieve information
from a Vector database for

266
00:16:13,495 --> 00:16:14,995
accurate and relevant results.

267
00:16:15,355 --> 00:16:19,425
And finally, in this layer, it
provides API endpoints for external

268
00:16:19,425 --> 00:16:23,725
facing, fetching externally,
fetching the, index, segments.

269
00:16:23,755 --> 00:16:26,665
And it enables seamless
content navigation.

270
00:16:27,165 --> 00:16:29,565
And let's go over the final
layer, which is the integration,

271
00:16:29,595 --> 00:16:31,335
interaction and the feedback layer.

272
00:16:31,695 --> 00:16:35,925
So this layer, let's say this layer
is very important because if you have,

273
00:16:35,925 --> 00:16:39,745
let's say, a user, you know that, that
is listening to an hour long podcast,

274
00:16:40,165 --> 00:16:43,695
but they would, they have some questions
that they want to answer, they could

275
00:16:43,695 --> 00:16:48,195
simply ask a question that says,
what did John say about, AI ethics?

276
00:16:48,435 --> 00:16:51,835
So the multi-model interaction
for seamless search will, would

277
00:16:51,865 --> 00:16:56,905
essentially, query, based on how you
query, any, any text space system.

278
00:16:57,235 --> 00:17:01,975
And it provides a real time feedback
to improve accuracy as well, because,

279
00:17:02,025 --> 00:17:06,185
you wanna learn as the, as more
users are adopting the system or

280
00:17:06,185 --> 00:17:07,835
adopting, or watching the videos.

281
00:17:08,260 --> 00:17:10,480
AI powered personalization
and recommendations.

282
00:17:10,480 --> 00:17:14,070
So over the time the system
learns, user preferences as well.

283
00:17:14,470 --> 00:17:17,610
and you could make, data
driven, decisions based on it.

284
00:17:18,470 --> 00:17:22,010
And here's a high level exam,
high level depiction of how this

285
00:17:22,010 --> 00:17:23,360
feedback layer would look like.

286
00:17:23,790 --> 00:17:26,790
it takes the user queries,
playback interaction, run ratings

287
00:17:26,790 --> 00:17:30,300
as input and generates enhanced
recommendations using the queries

288
00:17:30,360 --> 00:17:32,850
and indexing from the index layer.

289
00:17:33,240 --> 00:17:37,370
So it also ensures, improved accuracy
and facilitates, learning based on

290
00:17:37,370 --> 00:17:41,120
the feedback as a user interacts
or searches feedback is collected

291
00:17:41,120 --> 00:17:45,290
real time, which will be used to
adjust AI models and generate.

292
00:17:45,640 --> 00:17:48,550
Personalized recommendations and
improved accuracy and precision

293
00:17:48,550 --> 00:17:49,390
over the period of time.

294
00:17:49,810 --> 00:17:53,500
The system can leverage these APIs
to build features to expose, search

295
00:17:53,530 --> 00:17:58,200
and personalized playback, APIs
providing, interactive user experience.

296
00:17:58,250 --> 00:18:02,380
So to summarize our proposed system,
it offers several key benefits.

297
00:18:02,380 --> 00:18:06,590
It, it transforms the passive listening to
an interactive and structured experience

298
00:18:06,590 --> 00:18:10,440
where listeners can effectively find
and engage with, with the audio content.

299
00:18:10,900 --> 00:18:15,090
it, it is efficient, in terms of
topic and speaker classification.

300
00:18:15,480 --> 00:18:18,180
It gives you a real time search
and learning mechanism, and

301
00:18:18,180 --> 00:18:21,990
also a feedback system that is
continuously learning and helps you

302
00:18:21,990 --> 00:18:23,400
with the advanced personalization.

303
00:18:23,940 --> 00:18:27,540
And this framework is scalable because
a system is designed to accommodate

304
00:18:27,540 --> 00:18:31,260
various audio formats, such as podcasts
and webinars, making it versatile and

305
00:18:31,260 --> 00:18:33,030
applicable across different use cases.

306
00:18:33,585 --> 00:18:36,965
Improved accessibility is often,
underlooked, but the system provides

307
00:18:36,965 --> 00:18:40,445
features such as voice queries and
easy navigation aids, supporting

308
00:18:40,445 --> 00:18:45,095
diverse population, user needs, and
ensuring that all the listeners have

309
00:18:45,095 --> 00:18:46,715
the access to the audio contact.

310
00:18:47,215 --> 00:18:51,805
So con in con, in conclusion,
there's this research from Edison

311
00:18:51,805 --> 00:18:57,665
that says more than 45% of the audio
driven platforms, are on demand.

312
00:18:57,705 --> 00:19:02,555
and these are, 45% are on demand
platforms, which, consumers listen

313
00:19:02,555 --> 00:19:06,865
to, including podcasts, meetings,
education, and enterprise applications.

314
00:19:06,865 --> 00:19:07,165
These.

315
00:19:07,395 --> 00:19:08,145
You know these things.

316
00:19:08,145 --> 00:19:12,815
They present an opportunity for
AI powered, navigation systems by

317
00:19:12,815 --> 00:19:17,045
transforming the unstructured speech
into searchable interactive content.

318
00:19:17,105 --> 00:19:21,125
This approach enhances user engagement,
improves accessibility, and drives

319
00:19:21,455 --> 00:19:23,345
intelligent content discovery at scale.

320
00:19:23,845 --> 00:19:25,135
So that's all from me.

321
00:19:25,185 --> 00:19:26,265
thank you for joining.

