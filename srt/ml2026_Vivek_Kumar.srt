1
00:00:00,500 --> 00:00:01,580
Speaker 36: Good afternoon everyone.

2
00:00:02,219 --> 00:00:03,959
Thank you for joining my session.

3
00:00:04,809 --> 00:00:11,260
I will be discussing how machine learning
can be applied to optimize performance

4
00:00:11,350 --> 00:00:13,234
in high volume transaction systems.

5
00:00:13,734 --> 00:00:19,875
So modern enterprise platforms, they
handle millions of concurrent requests.

6
00:00:20,504 --> 00:00:25,015
Maintaining performance, stability,
and efficiency at a scale is

7
00:00:25,015 --> 00:00:26,545
a major engineering challenge.

8
00:00:27,515 --> 00:00:33,825
Machine learning enables system to
predict the demand a scale intelligently

9
00:00:34,665 --> 00:00:37,035
and optimize performance proactively.

10
00:00:37,535 --> 00:00:38,735
This is about myself.

11
00:00:38,765 --> 00:00:40,114
My name is Vive Kuma.

12
00:00:40,955 --> 00:00:45,014
I am a staff developer at Y Tech
Systems Group, which is now Maco.

13
00:00:45,974 --> 00:00:51,324
I have over 16 years of experience
building scalable enterprise

14
00:00:51,324 --> 00:00:56,139
platforms across insurance, finance,
and cloud native environments.

15
00:00:56,904 --> 00:01:02,244
My focus has been on designing
microservice architecture capable of

16
00:01:02,244 --> 00:01:09,054
handling a massive concurrency while
maintaining performance and reliability.

17
00:01:09,774 --> 00:01:14,005
So in this session, I'll share how
machine learning can leverage system

18
00:01:14,094 --> 00:01:17,639
telemetry to predict bottlenecks and.

19
00:01:18,139 --> 00:01:23,119
Infrastructure automatically, when I
say about telemetry is nothing, but

20
00:01:23,119 --> 00:01:29,580
you have your continuous data log about
your CP utilization, all the requests.

21
00:01:30,080 --> 00:01:30,800
Will be used.

22
00:01:31,190 --> 00:01:33,140
We'll take care of those in.

23
00:01:33,140 --> 00:01:35,420
I will explain about
those in extra slides.

24
00:01:36,320 --> 00:01:40,850
So when I talk about the
static rules versus machine

25
00:01:40,850 --> 00:01:42,770
learning, so here is the thing.

26
00:01:43,195 --> 00:01:50,704
Traditional performance optimization
relies on static rules such as a scaling.

27
00:01:50,704 --> 00:01:54,604
When CPU crosses a threshold,
for example, it crosses 70%.

28
00:01:55,264 --> 00:01:58,894
Then some event will be triggered,
and after that we take some

29
00:01:58,894 --> 00:02:00,934
axles, but damage is already done.

30
00:02:01,084 --> 00:02:01,774
Downtime.

31
00:02:01,834 --> 00:02:03,424
Downtime is already there.

32
00:02:03,514 --> 00:02:05,704
Latency is, their system becomes low.

33
00:02:05,704 --> 00:02:06,634
So many problems.

34
00:02:07,134 --> 00:02:11,965
However, these approaches
are reactive by the time.

35
00:02:12,380 --> 00:02:13,580
Scaling happens.

36
00:02:13,580 --> 00:02:19,610
Performance may already be degraded,
but modern distributed systems require

37
00:02:19,730 --> 00:02:25,700
predictive and intelligent scaling
to maintain consistent performance.

38
00:02:26,200 --> 00:02:26,710
Next slide.

39
00:02:27,210 --> 00:02:31,290
So what we are going to learn
today, I will cover four key areas.

40
00:02:31,380 --> 00:02:34,230
First is modern microservice architecture.

41
00:02:34,650 --> 00:02:39,010
Second, how telemetry becomes your
training data for machine learning.

42
00:02:39,389 --> 00:02:39,929
Techniques.

43
00:02:40,429 --> 00:02:41,269
ML driven.

44
00:02:41,269 --> 00:02:46,069
Third is ML driven optimization
strategies, and finally, some real

45
00:02:46,069 --> 00:02:47,694
world implementation examples.

46
00:02:47,864 --> 00:02:48,374
We'll discuss.

47
00:02:48,874 --> 00:02:52,389
So now this is about the
Architecture Foundation.

48
00:02:52,389 --> 00:02:57,769
Modern transaction platforms
uses microservices ed APIs,

49
00:02:58,069 --> 00:02:59,629
Kafka event streaming.

50
00:03:00,169 --> 00:03:03,289
This architecture enables SCA scalability.

51
00:03:03,594 --> 00:03:08,304
But also introduces complexity
in performance optimization.

52
00:03:08,844 --> 00:03:13,684
So nowadays, if you see almost many
projects are using the same design,

53
00:03:13,684 --> 00:03:18,009
like you have spring microservices,
so microservices basically.

54
00:03:18,509 --> 00:03:22,139
We use Spring Boot to implement
microservices, but microservices

55
00:03:22,139 --> 00:03:25,879
is like a type of architecture
where you're moving from monolithic

56
00:03:25,879 --> 00:03:28,629
to microservices monolithic.

57
00:03:28,629 --> 00:03:30,099
You had one big application.

58
00:03:30,099 --> 00:03:33,429
You are breaking those services
in a small pieces, and everyone

59
00:03:33,429 --> 00:03:35,829
has their own database.

60
00:03:36,259 --> 00:03:39,949
And they are communicating with
each other through API gateway.

61
00:03:40,339 --> 00:03:43,339
So what happens here is
your, everything is fast.

62
00:03:43,879 --> 00:03:45,229
You can scale easily.

63
00:03:45,289 --> 00:03:48,079
One service is down, but
it'll not impact the others.

64
00:03:48,829 --> 00:03:53,529
Opposite to monolithic, if something goes
wrong, all complete application down.

65
00:03:54,399 --> 00:03:59,014
And for any deployment there, there
was a. Downtime, which is not there

66
00:03:59,014 --> 00:04:00,444
with this spring boot microservices.

67
00:04:00,944 --> 00:04:05,284
As I said, telemetry is nothing,
but you have your continuous

68
00:04:05,374 --> 00:04:07,314
data CPA, users and all.

69
00:04:07,614 --> 00:04:11,574
So those things are being used
to train your machine learning.

70
00:04:11,574 --> 00:04:14,674
So as it I'll, we'll come to
the next slide where you will

71
00:04:14,674 --> 00:04:19,324
see it, how machine learning you
establish and you give continuous

72
00:04:19,324 --> 00:04:21,484
data, better work in predicting.

73
00:04:21,934 --> 00:04:26,524
So the moment CPUs will high
will increase, it'll predict

74
00:04:26,569 --> 00:04:28,084
it like it is going to crash.

75
00:04:28,084 --> 00:04:32,224
So you can scale, you can add more CPU U
but in meantime, you have a log to verify

76
00:04:32,224 --> 00:04:37,984
the reason and identify the reason, and
you can fix it without any downtime.

77
00:04:38,584 --> 00:04:40,714
So that's what it is.

78
00:04:40,714 --> 00:04:43,744
So telemetry, basically every
system generates telemetry, right?

79
00:04:43,744 --> 00:04:48,574
Including CPS latency, threat
utilization, and request rates.

80
00:04:49,074 --> 00:04:53,874
So machine learning uses this telemetry
to understand your system behavior and

81
00:04:53,874 --> 00:04:56,094
it predicts your future performance.

82
00:04:56,094 --> 00:04:56,814
Based on this.

83
00:04:57,314 --> 00:04:59,894
These are nothing but your
machine learning features.

84
00:05:00,434 --> 00:05:01,514
So important.

85
00:05:01,514 --> 00:05:05,324
Machine learning features include
concurrency, level Q path, transaction

86
00:05:05,824 --> 00:05:07,744
volume, and resource utilization.

87
00:05:08,504 --> 00:05:13,494
These metrics help much ML models
predict system load and performance.

88
00:05:13,994 --> 00:05:18,709
So forecasting load machine
learning enables system to predict.

89
00:05:19,374 --> 00:05:21,324
Load spikes before they occur.

90
00:05:22,044 --> 00:05:26,214
This allows proactive scaling
instead of reactive scaling.

91
00:05:26,514 --> 00:05:29,784
So earlier reactive means something
happened and then you are scaling,

92
00:05:29,784 --> 00:05:33,734
you're adding more CPUs, but now
the moment it spike goes, and as

93
00:05:33,734 --> 00:05:35,414
you said about telemetry, right?

94
00:05:35,414 --> 00:05:38,804
So every system generates telemetry
data, like ccp, utilization,

95
00:05:38,804 --> 00:05:39,824
latency and everything.

96
00:05:40,154 --> 00:05:44,354
So based on that data, ML already
predicts and it forecasts.

97
00:05:44,354 --> 00:05:45,634
So you can avoid, downtime.

98
00:05:46,134 --> 00:05:49,894
These are basically ml or
optimization strategies.

99
00:05:50,404 --> 00:05:55,864
So ML enables intelligent autoscaling,
dynamic caching, and real time

100
00:05:55,864 --> 00:05:57,574
configuration optimization.

101
00:05:58,114 --> 00:06:00,574
So that way it helps you a lot.

102
00:06:00,964 --> 00:06:05,174
Dynamic configuration means in real
time you can, change the tuning of the

103
00:06:05,174 --> 00:06:07,569
connection for timeout values, BA size.

104
00:06:08,484 --> 00:06:10,824
We can retry policies across services.

105
00:06:11,094 --> 00:06:15,894
So machine learning continuously optimizes
parameters based on the current workload

106
00:06:15,894 --> 00:06:18,414
characteristics and performance goals.

107
00:06:18,914 --> 00:06:22,694
You see also, there's adapt,
adaptive auto scaling.

108
00:06:22,694 --> 00:06:27,104
This is what, you can scale based
on the prediction, not after

109
00:06:27,104 --> 00:06:29,054
reaction, not like event happen.

110
00:06:29,054 --> 00:06:30,464
And then you have to scale it.

111
00:06:31,064 --> 00:06:33,739
So these are like three pillars
of machine learning models.

112
00:06:34,239 --> 00:06:38,929
Next slide is about the
performance comparison versus ML

113
00:06:38,929 --> 00:06:40,999
based versus rule based systems.

114
00:06:41,569 --> 00:06:46,609
So you can see the ML driven systems
improve latency, throughput, and

115
00:06:46,609 --> 00:06:49,069
infrastructure efficiency significantly.

116
00:06:49,069 --> 00:06:50,239
For example, you talk about.

117
00:06:50,739 --> 00:06:55,929
73% fewer performance incidents
with proactive optimization.

118
00:06:56,319 --> 00:07:00,789
If you think about efficiency,
28% lower infrastructure cost via

119
00:07:00,789 --> 00:07:03,129
intelligent resolution allocation.

120
00:07:03,339 --> 00:07:08,289
So when I say about you are saving
costs, so how so if high load, you

121
00:07:08,289 --> 00:07:10,659
need more CPU, you need more resources.

122
00:07:10,674 --> 00:07:15,919
But if there is a low request in
the nighttime or oxygen time so you

123
00:07:15,919 --> 00:07:19,279
can, it automatically, it'll remove
those, you can save cost there.

124
00:07:19,779 --> 00:07:20,589
So next slide.

125
00:07:21,199 --> 00:07:23,509
Okay, next slide is all about ML models.

126
00:07:23,839 --> 00:07:27,079
So ML models, different ML
models have different purpose.

127
00:07:27,079 --> 00:07:28,969
Very common one is LSTM.

128
00:07:29,509 --> 00:07:35,689
It predicts load patterns and anomaly
detection, identifies unusual behavior.

129
00:07:35,689 --> 00:07:40,499
Anomaly itself says, something is there
and your behavior is suddenly different.

130
00:07:40,889 --> 00:07:45,119
So this will detect like suddenly
your CPU license is very high.

131
00:07:46,034 --> 00:07:52,484
LSTM says L sst M is long short term
memory and it basically uses your

132
00:07:52,484 --> 00:07:57,394
telemetry and it can tell you the
number more specific things about it.

133
00:07:57,894 --> 00:08:02,274
And a regression model regression
predicts resource requirement.

134
00:08:02,614 --> 00:08:07,174
As I told about you, the telemetries
or telemetries, nothing but the

135
00:08:07,174 --> 00:08:11,414
automatic collection and transmission
of system performance data such

136
00:08:11,414 --> 00:08:16,914
as your CPU uses latency request
metrics of monitoring and analysis.

137
00:08:17,634 --> 00:08:21,799
But here, if you say about
the lstm, so as I told lstm.

138
00:08:22,444 --> 00:08:27,200
Is the LSM regulation is a machine
learning technique that uses long

139
00:08:27,200 --> 00:08:29,540
certain memory neural network.

140
00:08:30,040 --> 00:08:31,870
Continuous numeric values.

141
00:08:32,740 --> 00:08:36,309
So based on your past
sequential or time series data.

142
00:08:36,850 --> 00:08:43,760
So predicting example, predicting future
CP uses future latency request volume

143
00:08:44,310 --> 00:08:46,500
based on historical system metrics.

144
00:08:46,920 --> 00:08:48,720
So this is what LSTM is?

145
00:08:49,515 --> 00:08:49,735
Yes.

146
00:08:50,235 --> 00:08:52,545
So building effective loop.

147
00:08:52,785 --> 00:08:59,205
So what how this loop works is, as I said,
you start with a small, then whatever

148
00:08:59,205 --> 00:09:04,965
your result, whatever your telemetry
data is, you feed that in ML model and.

149
00:09:05,474 --> 00:09:07,814
Using that data, it works better.

150
00:09:08,024 --> 00:09:11,744
So this feedback loop allows
continuous optimization.

151
00:09:12,404 --> 00:09:19,274
Telemetry feeds, ML models which generate
predictions and trigger a scaling axels.

152
00:09:19,694 --> 00:09:24,905
So based on the prediction it, it can
predict, okay CPU users are going high.

153
00:09:25,234 --> 00:09:26,255
So now.

154
00:09:26,855 --> 00:09:29,915
It already predicted based
on the number of requests are

155
00:09:29,915 --> 00:09:31,115
coming or something happening.

156
00:09:31,444 --> 00:09:33,875
So you can easily make decision
about a scaling and you

157
00:09:33,875 --> 00:09:35,345
can scale your application.

158
00:09:35,345 --> 00:09:39,995
You can add more CPU rather than
something already happened, downtime

159
00:09:39,995 --> 00:09:43,625
is already there, and then you are
trying to scale it because number

160
00:09:43,625 --> 00:09:47,665
of users are more, if we talk about
deployment considerations for production.

161
00:09:47,965 --> 00:09:51,455
So production is like obviously
a very critical environment.

162
00:09:51,725 --> 00:09:54,335
So in production systems,
reliability is critical.

163
00:09:54,395 --> 00:09:58,475
ML models must be monitored,
validated, and deployed carefully.

164
00:09:59,135 --> 00:10:04,925
So here you see, deploying ML models
in critical transaction path requires

165
00:10:05,255 --> 00:10:10,755
careful planning and risk mitigation
models must deliver predictions with

166
00:10:10,760 --> 00:10:13,190
consistent latency and high availability.

167
00:10:13,690 --> 00:10:16,690
So ready to optimize with
intelligence if you guys are ready.

168
00:10:17,140 --> 00:10:21,400
So we can, I can give you the
a more real world example.

169
00:10:21,580 --> 00:10:24,040
So machine learning is not
just for the data scientist.

170
00:10:24,670 --> 00:10:26,230
Broader users are there.

171
00:10:26,680 --> 00:10:28,270
So instrument your systems.

172
00:10:28,270 --> 00:10:31,000
Really select models aligned
with your optimization goals.

173
00:10:31,000 --> 00:10:34,450
Deploy with proper safeguards
and hydrate continuously.

174
00:10:34,570 --> 00:10:39,130
As I told you, feed telemetry to the
model and it does the prediction better.

175
00:10:39,130 --> 00:10:40,810
And based on the prediction,
you can do the scaling.

176
00:10:41,310 --> 00:10:43,710
We discuss about, so
what do you understand?

177
00:10:43,920 --> 00:10:48,420
So if I talk about the four things we
understood or we give the priority here is

178
00:10:49,080 --> 00:10:52,045
basically purpose of this presentation is.

179
00:10:52,545 --> 00:10:58,365
Convincing everybody how moving from
reactive to predictive approach is better.

180
00:10:58,755 --> 00:11:03,795
So ml, this transforms performance
optimization from firefighting

181
00:11:03,825 --> 00:11:08,025
to a strategic forecasting
and proactive intervention.

182
00:11:08,355 --> 00:11:08,925
It means.

183
00:11:09,240 --> 00:11:10,650
You, your prediction is better.

184
00:11:10,860 --> 00:11:13,500
You can predict it and you can
scale based on your prediction,

185
00:11:13,500 --> 00:11:14,550
not based on the reaction.

186
00:11:15,240 --> 00:11:16,650
Telemetry is your foundation.

187
00:11:16,650 --> 00:11:20,140
As I said, telemetry is nothing but
your continuous, gathering of your

188
00:11:20,140 --> 00:11:22,810
all CPA users, latency, all this data.

189
00:11:22,810 --> 00:11:26,830
You fit the ML model, and after
that, it predicts better actually.

190
00:11:27,550 --> 00:11:31,930
So measurable business impact ML
enhanced system delivers superior

191
00:11:31,930 --> 00:11:36,490
responsiveness, stability, and resource
efficiency compared to static rules.

192
00:11:37,090 --> 00:11:37,965
So a start is small.

193
00:11:38,465 --> 00:11:41,345
So basically telemetry will
feed and better prediction.

194
00:11:41,645 --> 00:11:45,575
So begin with focused one cases,
establish feedback loops and

195
00:11:45,575 --> 00:11:46,625
expand ML cap capabilities.

196
00:11:47,125 --> 00:11:51,055
So now this is just for, better
understanding because many people

197
00:11:51,055 --> 00:11:53,125
are aware of AWS and Azure.

198
00:11:53,515 --> 00:11:58,285
So these cloud platforms, they already
use ML driven scaling AWS, predictive

199
00:11:58,285 --> 00:12:03,145
water scaling forecasts demand, and
a scale infrastructure automatically.

200
00:12:03,625 --> 00:12:07,495
Azure bernet service dynamically
scales containers based

201
00:12:07,495 --> 00:12:08,790
on the workload telemetry.

202
00:12:09,415 --> 00:12:12,865
These platforms demonstrate
real world ML optimization.

203
00:12:13,365 --> 00:12:14,055
Architecture.

204
00:12:14,475 --> 00:12:16,215
So little bit about this.

205
00:12:16,485 --> 00:12:19,815
So this diagram I will, and
next slide I have a diagram.

206
00:12:19,815 --> 00:12:25,485
So basically this diagram is there,
so it'll basically shows the complete

207
00:12:25,575 --> 00:12:27,955
machine learning optimization pipeline.

208
00:12:28,480 --> 00:12:31,990
So telemetry is collected, which
is your very first screenshot,

209
00:12:31,990 --> 00:12:34,090
CloudWatch or whatever tool you use.

210
00:12:34,840 --> 00:12:36,970
So telemetry is C collected and processed.

211
00:12:37,300 --> 00:12:41,500
ML models generate predictions,
auto scaling system, just infra

212
00:12:41,620 --> 00:12:44,890
structure, which is at the last, you
see the container spot and prior,

213
00:12:45,250 --> 00:12:49,360
second last, if you see already
Kubernetes and all before that LSTM,

214
00:12:49,360 --> 00:12:51,560
that is your machine learning there.

215
00:12:51,560 --> 00:12:53,700
So there they'll predict future.

216
00:12:53,700 --> 00:12:56,040
And based on that you can
scale your infrastructure.

217
00:12:56,540 --> 00:12:59,820
So this is screen whatever I'm saying,
this creates a fully autonomous,

218
00:12:59,820 --> 00:13:04,290
self-optimizing system capable of
maintaining performance at a scale.

219
00:13:04,790 --> 00:13:05,840
So thank you everyone.

220
00:13:05,840 --> 00:13:11,180
Here is my LinkedIn ID and my
Gmail ID as well, any question

221
00:13:11,180 --> 00:13:12,740
we can again talk about it.

222
00:13:13,150 --> 00:13:14,860
So far I can just conclude.

223
00:13:15,250 --> 00:13:18,460
So the takeaway is performance
optimization is shifting from reactive

224
00:13:18,460 --> 00:13:22,420
to productive machine only enables
intelligent scaling, improving

225
00:13:22,470 --> 00:13:24,600
performance, stability, and efficiency.

226
00:13:25,050 --> 00:13:26,580
So thank you for your time.

227
00:13:26,580 --> 00:13:28,320
I would be happy to answer any question.

228
00:13:28,325 --> 00:13:30,600
If you have any question,
you can email me and connect.

