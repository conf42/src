1
00:00:00,500 --> 00:00:04,880
Hello everyone and welcome
to Con 42 Cube Native 2025.

2
00:00:05,630 --> 00:00:12,920
My name is P Shaker and I am delighted to
share my experiences on a topic that sits

3
00:00:12,920 --> 00:00:14,625
at the heart of modern Cloud operations.

4
00:00:15,125 --> 00:00:19,535
On how large language models are
transforming observability and

5
00:00:19,535 --> 00:00:23,255
accelerating resilience in platform
as a service architectures.

6
00:00:23,755 --> 00:00:26,845
I've spent more than 14 years
working across cloud native

7
00:00:26,845 --> 00:00:30,234
technologies, site reliability,
engineering, and software delivery.

8
00:00:30,895 --> 00:00:34,974
Over this time, I have witnessed
how the scale and complexity of

9
00:00:34,974 --> 00:00:38,815
cloud systems have outpaced the
traditional monitoring approaches.

10
00:00:39,315 --> 00:00:40,815
LMS are not just a trend.

11
00:00:41,175 --> 00:00:46,035
They are rapidly reshaping on how
we detect, understand, and recover

12
00:00:46,035 --> 00:00:48,285
from issues in dynamic environments.

13
00:00:49,275 --> 00:00:53,565
Today we will talk through the key
challenges of observability in modern

14
00:00:53,714 --> 00:00:59,955
platform as a service, how LLM driven
innovations are changing the game, and how

15
00:00:59,955 --> 00:01:04,785
providers can deliver observability as a
native service rather than just an add-on.

16
00:01:05,285 --> 00:01:07,625
So let's dive in without
any further delay.

17
00:01:08,125 --> 00:01:12,145
So talking about the challenge
of modern cloud observability.

18
00:01:13,105 --> 00:01:17,664
Observability in today's cloud
world is anything but complex.

19
00:01:17,664 --> 00:01:23,245
Even though the ask might be simple,
with great power comes great complexity.

20
00:01:24,175 --> 00:01:28,495
Hope everyone agrees with me on
this, especially if your mission

21
00:01:28,555 --> 00:01:31,255
is to build resilient platform
as a service architecture.

22
00:01:31,755 --> 00:01:35,714
Organizations are pouring investment
into public cloud platforms to

23
00:01:35,714 --> 00:01:37,695
gain agility and scalability.

24
00:01:38,685 --> 00:01:43,304
Teams are now embracing microservices
and moving workloads onto cloud

25
00:01:43,304 --> 00:01:48,604
native platform as a service offerings
to ship features faster, but with

26
00:01:48,604 --> 00:01:53,845
this evolution also comes with the
steeper price complexity, skyrockets.

27
00:01:54,345 --> 00:01:59,145
Legacy monitoring tools often just
dump vast amounts of raw data,

28
00:01:59,265 --> 00:02:04,455
leaving engineers with dashboards
full of noise, but few actionable

29
00:02:04,455 --> 00:02:07,665
insights when the systems fail.

30
00:02:07,755 --> 00:02:14,570
The gap between an alert and a fix can
keep stretching, slowing down recovery.

31
00:02:14,750 --> 00:02:18,215
In the meantime, in platform
as a service environments.

32
00:02:18,245 --> 00:02:20,735
This complexity is further magnified.

33
00:02:21,620 --> 00:02:25,790
Services are deeply distributed
and dependent on each other

34
00:02:25,850 --> 00:02:27,050
in an unpredictable way.

35
00:02:27,550 --> 00:02:31,420
A single issue can ripple through
an entire ecosystem, right?

36
00:02:31,920 --> 00:02:35,760
And alerts frequently arrive
without enough context for the

37
00:02:35,760 --> 00:02:37,409
engineers to respond quickly.

38
00:02:37,909 --> 00:02:40,339
And let's talk about
the dynamic workloads.

39
00:02:40,909 --> 00:02:45,619
Think about a platform as a
service offering with Kubernetes.

40
00:02:46,119 --> 00:02:50,829
Think auto scaling Kubernetes pods
spinning up and shutting down.

41
00:02:50,829 --> 00:02:54,419
Within seconds, they make the
failure landscape even more harder

42
00:02:54,419 --> 00:03:00,619
to track and resilient and to
identify where the exact root causes.

43
00:03:01,119 --> 00:03:05,379
Now let's take an example of a FinTech
platform as a service, which is handling

44
00:03:05,379 --> 00:03:07,329
a very high volume trades application.

45
00:03:07,829 --> 00:03:15,389
Imagine a sudden spike in traffic hits
and or a single service flatters, and

46
00:03:15,389 --> 00:03:17,849
that causes transaction delays to cascade.

47
00:03:18,349 --> 00:03:24,289
Now your SRA team is flooded with alarms,
but lacks the correlated insights.

48
00:03:24,789 --> 00:03:29,799
Recovery slows down, downtime
grows, and resilience suffers.

49
00:03:30,299 --> 00:03:34,769
I've seen the scenarios play out
repeatedly and they have convinced

50
00:03:34,769 --> 00:03:40,739
me that embedding smarter self
observability directly into the platform

51
00:03:41,399 --> 00:03:45,839
can fundamentally change how fast
we can bounce back from disruption.

52
00:03:46,339 --> 00:03:51,460
So let's deep more into the distributed
platform as a service environments.

53
00:03:52,269 --> 00:03:54,550
Imagine a healthcare
platform as a service.

54
00:03:55,434 --> 00:03:57,834
Managing electronic medical records.

55
00:03:58,794 --> 00:04:02,604
Patient scheduling and clinical
notifications are the core services of it.

56
00:04:03,104 --> 00:04:07,784
Services now must integrate flawlessly
because it depends on the patient.

57
00:04:07,935 --> 00:04:13,305
It directly depends on the patient
experience and even sometimes

58
00:04:13,310 --> 00:04:16,365
safety and other protocols as well.

59
00:04:16,865 --> 00:04:18,515
Seemingly a small update.

60
00:04:19,055 --> 00:04:22,475
To one of the service can suddenly
overload the event streams.

61
00:04:22,565 --> 00:04:29,345
It can choke dependencies and can
also trigger caca cascading failures.

62
00:04:29,845 --> 00:04:32,785
Meanwhile, underlying
infrastructure is also ephemeral.

63
00:04:33,745 --> 00:04:39,805
Container scale in and out, and the IP
tends to change, and all troubleshooting

64
00:04:39,865 --> 00:04:41,605
assumptions are no longer whole.

65
00:04:41,605 --> 00:04:41,695
Good.

66
00:04:42,195 --> 00:04:46,335
So this reality underscores the
urgent need for observability

67
00:04:46,335 --> 00:04:48,785
to evolve platform as a service.

68
00:04:48,815 --> 00:04:54,155
Offerings can no longer treat resilience
as a bolt-on feature or something that

69
00:04:54,155 --> 00:04:56,535
the customers can figure out themselves.

70
00:04:57,435 --> 00:05:00,465
Instead, observability must be integrated.

71
00:05:01,125 --> 00:05:02,775
It should be always on.

72
00:05:03,195 --> 00:05:07,844
It should be context aware and
also be capable of accelerating

73
00:05:07,844 --> 00:05:12,674
detection and recovery the
moment this disruption begins.

74
00:05:13,174 --> 00:05:16,775
Now, with that, let's jump on to
the evolution of observability.

75
00:05:17,275 --> 00:05:21,744
Observability has come a long way,
but it's been a very slow climb.

76
00:05:22,244 --> 00:05:26,924
We started with basic infrastructure
monitoring where CPU graphs.

77
00:05:27,465 --> 00:05:32,355
Memory usage charts and
simple alerts were in place.

78
00:05:33,105 --> 00:05:37,425
These worked well and good when
the systems were monolithic and

79
00:05:37,425 --> 00:05:39,105
failures were easier to isolate.

80
00:05:39,605 --> 00:05:45,365
Then came the three pillars approach
in the DevOps era where metrics,

81
00:05:45,575 --> 00:05:50,945
logs, and traces gave us more
depth, but still required human

82
00:05:50,945 --> 00:05:52,745
correlation and interpretation.

83
00:05:53,245 --> 00:05:57,175
So tools like distributor tracing
and chaos engineering improved

84
00:05:57,235 --> 00:06:02,095
proactive testing, but they
still demanded manual effort.

85
00:06:02,595 --> 00:06:08,865
Now with AI and lms, we are
entering into a new era.

86
00:06:09,365 --> 00:06:14,495
These models can ingest and
interpret massive telemetry streams,

87
00:06:14,945 --> 00:06:18,605
correlate events in real time, and.

88
00:06:18,995 --> 00:06:26,555
Explain what, why, and how an
incident took place rather than

89
00:06:26,585 --> 00:06:28,895
humans tackling this all alone.

90
00:06:29,395 --> 00:06:34,164
So for platform as a service
providers, this locks a new value

91
00:06:34,164 --> 00:06:38,714
proposition, observability as a
built-in int intelligence service.

92
00:06:39,214 --> 00:06:45,604
The platform itself can proactively reduce
downtime and it can guide recovery with

93
00:06:45,604 --> 00:06:48,154
no separate stack for customers to manage.

94
00:06:48,654 --> 00:06:53,694
Now let's take a look at the land LA
large lang language models, which is the

95
00:06:53,694 --> 00:06:58,019
new Paradigm lms break the traditional
rules based approach to monitoring.

96
00:06:58,519 --> 00:07:03,799
Instead of static thresholds or hardcoded
alerts, they understand the context.

97
00:07:04,299 --> 00:07:10,539
They excel at correlating diverse
data sources like metrics, traces

98
00:07:10,749 --> 00:07:16,330
and logs, even deployment histories
to explain why something is

99
00:07:16,330 --> 00:07:18,489
happening and how to resolve it.

100
00:07:18,989 --> 00:07:21,299
Imagine an e-commerce
platform as a service.

101
00:07:21,799 --> 00:07:27,369
During a Black Friday, rush latency
spikes in the checkout service.

102
00:07:27,879 --> 00:07:33,399
A conventional alert tells you only that
the latency is high, but on the other

103
00:07:33,399 --> 00:07:39,969
hand, an LLM can however, dig deeper,
linking a recent deployment, a sudden

104
00:07:39,969 --> 00:07:44,799
memory spike in one of the service and
an abnormal error pattern in another.

105
00:07:45,299 --> 00:07:50,519
Then surface a clear, actionable
remediation to roll back or scale up.

106
00:07:51,019 --> 00:07:54,049
So in this case, the recovery
time shrinks dramatically.

107
00:07:54,549 --> 00:07:59,219
This agility also enables platform
as a service vendors to often

108
00:08:00,059 --> 00:08:04,629
offer tenant specific LLM powered
insight right out of the box.

109
00:08:05,129 --> 00:08:08,399
Customers don't need to build
or fine tune their own models.

110
00:08:09,209 --> 00:08:12,419
Resilience becomes a part
of the platform itself.

111
00:08:12,919 --> 00:08:17,209
Now let's take a deeper look into
the key capabilities that an LLM

112
00:08:17,209 --> 00:08:19,249
enhanced observability model can offer.

113
00:08:19,749 --> 00:08:26,134
So they can offer automated root cause
analysis, correlating telemetry across

114
00:08:26,134 --> 00:08:30,834
layers to rapidly pinpoint why a failure
occurred, a remediation guidance.

115
00:08:31,689 --> 00:08:36,879
We are suggesting or even executing
recovery steps based on learned

116
00:08:36,909 --> 00:08:42,629
patterns and continuously improving
as it observes your workloads over

117
00:08:42,629 --> 00:08:48,979
time by adaptive learning and comes
in and it can also decide when to

118
00:08:48,979 --> 00:08:51,409
escalate to humans versus self.

119
00:08:51,409 --> 00:08:55,609
Ha cutting the manual delays
with intelligent automation.

120
00:08:56,109 --> 00:08:57,159
Picture a logistics.

121
00:08:57,659 --> 00:09:02,764
A company hit by a routing outage
and LLM not only flags the anomaly,

122
00:09:03,304 --> 00:09:08,374
but it can also help to analyze
the patterns and propose or even

123
00:09:08,374 --> 00:09:10,984
auto execute rerouting strategies.

124
00:09:11,824 --> 00:09:16,234
A recovery that could have taken
hearts might now just take minutes.

125
00:09:17,224 --> 00:09:22,894
So let's take a look at how this can be
implemented in an implementation approach.

126
00:09:23,394 --> 00:09:29,544
So we, this can be implemented in
the following ways, like where you

127
00:09:29,544 --> 00:09:32,844
can start by integrating your alerts.

128
00:09:33,654 --> 00:09:39,174
You can feed your LMS with existing
monitoring signals and enrich them with

129
00:09:39,234 --> 00:09:45,054
context, so alerts become more meaningful
narratives rather than just being noise.

130
00:09:45,554 --> 00:09:53,294
And the LLM models can also be trained
based on specific domains, so it can

131
00:09:53,294 --> 00:09:59,234
be fine tuned based on your platform's,
telemetry, or deployment histories and

132
00:09:59,234 --> 00:10:02,234
failure patterns to improve accuracy.

133
00:10:02,734 --> 00:10:07,084
And it can also do predictive
insights where moving away from

134
00:10:07,084 --> 00:10:08,854
reactive alerts to forecasts.

135
00:10:09,354 --> 00:10:13,014
And also catching failing
nodes or bottlenecks before

136
00:10:13,014 --> 00:10:14,484
they impact the tenants.

137
00:10:14,984 --> 00:10:20,054
You can also give engineers a chat
like interface to ask why this latency

138
00:10:20,054 --> 00:10:25,694
is spiking and get a clear context
rich answer with a conversational

139
00:10:25,694 --> 00:10:29,874
interface and you can also correlate.

140
00:10:30,579 --> 00:10:37,629
Or link seemingly unrelated signals
like API errors, pod churns, or

141
00:10:37,659 --> 00:10:42,279
config changes to stop cascading
failures early with deep correlation.

142
00:10:42,779 --> 00:10:47,069
I have read articles about this
that in a media streaming platform

143
00:10:47,069 --> 00:10:52,849
as a service environment where LLM
connected playback reaches to recent

144
00:10:52,849 --> 00:10:57,799
API throttling helping engineers to fix
the issue in minutes instead of house.

145
00:10:58,299 --> 00:11:02,109
Now after the implementation,
the important part is what

146
00:11:02,859 --> 00:11:04,029
is the actual outcome?

147
00:11:04,029 --> 00:11:05,709
What could be the benefits of it?

148
00:11:06,549 --> 00:11:13,029
So before an LLM integration, the
meantime to resolution was taking

149
00:11:13,029 --> 00:11:19,419
long alert, fat fatigue was real, and
the recovery relied heavily on expert

150
00:11:19,419 --> 00:11:21,609
engineers piecing the clothes together.

151
00:11:22,109 --> 00:11:28,709
And after LLM adoption resolution
times drop down dramatically, in

152
00:11:28,709 --> 00:11:31,259
some cases to under even 10 minutes.

153
00:11:31,859 --> 00:11:39,264
And where it also had a positive impact
on less false positives due to which,

154
00:11:39,334 --> 00:11:44,149
like the product productivity increased
and the overall reliability improved.

155
00:11:44,649 --> 00:11:49,799
By baking this into a platform as a
service, managed service providers

156
00:11:49,799 --> 00:11:55,169
can now standardize resilience
gains across all tenants, not just

157
00:11:55,169 --> 00:11:57,269
those with the light SRE teams.

158
00:11:57,769 --> 00:12:00,769
Now, let's take a look at a
technical implementation framework

159
00:12:00,769 --> 00:12:06,549
work that can outline the steps of
what can be done to implement this.

160
00:12:07,049 --> 00:12:09,794
So start with your data
ingestion and normalization.

161
00:12:10,294 --> 00:12:16,534
Where you can pull your telemetry from,
metrics, logs, traces, and even CICD

162
00:12:16,534 --> 00:12:25,594
events, clean and sanitize the data
with a rich metadata, and also choose

163
00:12:25,594 --> 00:12:31,054
models and fine tune for platform
specific workloads, for example, like

164
00:12:31,054 --> 00:12:36,604
scaling patterns or Kubernetes churn,
and ensure low latency inference.

165
00:12:37,104 --> 00:12:43,944
Where you can optimize the LLM and
you can also do insightful insights

166
00:12:43,944 --> 00:12:49,634
on delivery via where tailored outputs
for each persona, instant remediation.

167
00:12:49,634 --> 00:12:56,279
Hence for sre, where summarized health and
trends for developers or even managers.

168
00:12:56,779 --> 00:13:01,969
And in the most important part is the
technical tool chain integration where

169
00:13:01,999 --> 00:13:09,379
you can expose the results via API
dashboards or even platform consoles that

170
00:13:09,379 --> 00:13:11,839
the users can consume insights natively.

171
00:13:12,339 --> 00:13:17,469
And let's talk about a more important
topic, which is the evaluation methodology

172
00:13:18,459 --> 00:13:23,259
when introducing LLM Driven observability
into platform as a service architecture.

173
00:13:23,724 --> 00:13:27,204
It is always essential to
have a structured evaluation

174
00:13:27,234 --> 00:13:29,785
approach to prove its value.

175
00:13:29,785 --> 00:13:34,635
Before wide adoption, a practical
evaluation offers can offering can

176
00:13:34,635 --> 00:13:36,915
include four complementary methods.

177
00:13:37,415 --> 00:13:42,995
One is controlled experimentation
where you stand up a test or a staging

178
00:13:42,995 --> 00:13:45,845
environment that mimic production.

179
00:13:46,345 --> 00:13:51,325
In a multi-tenant Kubernetes cluster
or like a service mesh and real, where

180
00:13:51,505 --> 00:13:56,295
real telemetry pipelines are present,
and you can inject these failures such

181
00:13:56,295 --> 00:14:02,565
as like PO crashes or cascading service
degradation, or even traffic surges.

182
00:14:03,555 --> 00:14:07,335
Now you can compare the performance
of your existing observability stack

183
00:14:07,395 --> 00:14:13,215
with the LM announced system under the
same controlled events to even get a

184
00:14:13,215 --> 00:14:15,675
more clear or reproducible results.

185
00:14:16,175 --> 00:14:21,485
And once you have that in place, next is
to do a quantitative analysis where you

186
00:14:21,515 --> 00:14:26,985
collect hard metrics like mean time to
detection and mean time to resolution.

187
00:14:27,945 --> 00:14:34,480
And also the false positive rates
to actually understand the accuracy

188
00:14:34,480 --> 00:14:36,490
of the root cause detection.

189
00:14:37,435 --> 00:14:42,925
And you should also monitor how
quickly the system detects the trigger

190
00:14:43,015 --> 00:14:47,905
and correlates the signals together
and guides or executes remediation.

191
00:14:48,405 --> 00:14:52,485
A well instrument, a well
instrumented test often shows

192
00:14:52,575 --> 00:14:55,175
significant reduction in MTTR.

193
00:14:55,230 --> 00:14:59,100
That is the mean time to reduction
when ellam insights are added.

194
00:14:59,600 --> 00:15:02,060
Next comes the qualitative analysis.

195
00:15:02,105 --> 00:15:08,185
Where you engage the SRE and the DevOps
teams who work directly with the new

196
00:15:08,185 --> 00:15:15,565
system, you can use structured feedback or
sessions or surveys to capture changes in

197
00:15:15,565 --> 00:15:18,265
alert clarity or even the cognitive load.

198
00:15:18,970 --> 00:15:21,280
And trust in recommendations.

199
00:15:21,780 --> 00:15:27,420
This human-centered lens highlights
whether the AI explanation are

200
00:15:27,420 --> 00:15:31,025
actionable and understandable,
which is critical for adoption.

201
00:15:31,525 --> 00:15:36,895
Then comes the statistical validation
where you apply the statistical testing,

202
00:15:37,195 --> 00:15:44,545
example, T tests, or you can do a AB
testing one with the traditional model and

203
00:15:44,545 --> 00:15:46,645
the other one with the LLM enhanced model.

204
00:15:47,125 --> 00:15:52,855
And also can do effect size calculations
across multiple incidents to ensure

205
00:15:53,185 --> 00:15:55,465
observed improvements aren't just random.

206
00:15:55,965 --> 00:16:01,935
And this step transforms raw results into
credible data backed evidence so that the

207
00:16:01,935 --> 00:16:09,045
new approach truly accelerates resilience
by combining controlled experimentation,

208
00:16:09,435 --> 00:16:12,495
quantitative KPIs, and human feedback.

209
00:16:12,995 --> 00:16:17,195
The platform as a service providers
can confidently validate that an

210
00:16:17,200 --> 00:16:22,685
LLM powered observability delivers
measurable resilience gains before

211
00:16:22,685 --> 00:16:24,515
scaling it as a core service.

212
00:16:25,015 --> 00:16:29,710
So another important feature that we
need to talk about is the privacy,

213
00:16:29,710 --> 00:16:32,080
security, and ethical considerations.

214
00:16:32,995 --> 00:16:38,095
Once evaluation is complete, more
focus should be given to privacy,

215
00:16:38,125 --> 00:16:40,485
security, and ethical implementation.

216
00:16:41,265 --> 00:16:42,885
Trust us as essential, right?

217
00:16:43,385 --> 00:16:48,125
So when we talk about privacy, collect
only what is actually necessary,

218
00:16:48,725 --> 00:16:54,245
anonymize it wherever possible and
designed the data pipelines to comply

219
00:16:54,485 --> 00:16:57,185
with regulations like HIPAA or GDPR.

220
00:16:57,685 --> 00:17:02,515
And in terms of security guard, again,
guard the model against misuse or

221
00:17:02,515 --> 00:17:05,395
data hallucinations or data leakage.

222
00:17:05,895 --> 00:17:09,870
Keep the critical remediation steps human
verified, wherever it is appropriate.

223
00:17:10,370 --> 00:17:13,190
And the most important part is ethical.

224
00:17:13,460 --> 00:17:18,770
As we all know, like AI and LLM has
its own governance, and most of the

225
00:17:18,770 --> 00:17:23,590
systems specifically in healthcare,
government, or financial finance,

226
00:17:23,980 --> 00:17:28,860
they have their own regulations
on how AI and L LMS can be used.

227
00:17:29,360 --> 00:17:33,950
So in this case, mitigate bias in
model training and ensure that the

228
00:17:33,950 --> 00:17:39,400
outputs are explainable so engineers
can trust the recommendation by

229
00:17:39,400 --> 00:17:41,170
addressing these from the start.

230
00:17:41,410 --> 00:17:46,435
A platform as a service provider
can confidently offer observability

231
00:17:46,435 --> 00:17:50,200
as a service without compromising
compliance or user trust.

232
00:17:50,700 --> 00:17:53,825
So let's talk about the future
directions on where we are headed.

233
00:17:54,325 --> 00:17:59,005
So looking forward, we are heading
towards a fully self-healing platform as

234
00:17:59,005 --> 00:18:07,365
a service capability except multi-model
input inputs like logs, metrics, traces,

235
00:18:07,365 --> 00:18:10,370
video or voice to enrich the diagnostics.

236
00:18:10,870 --> 00:18:16,370
And industry specific models will
give domain tuned resilience like

237
00:18:16,370 --> 00:18:20,660
for example, healthcare versus
FinTech or versus gaming, et cetera.

238
00:18:21,160 --> 00:18:23,740
And let's talk about federated learning.

239
00:18:23,800 --> 00:18:29,410
We will allow sharing intelligence across
tenants without leaking sensitive data.

240
00:18:30,280 --> 00:18:35,170
And let's talk about edge aware
observability where computational is

241
00:18:35,220 --> 00:18:40,800
computational aspects and reachability
is limited, where it can adopt to a

242
00:18:40,800 --> 00:18:46,200
hybrid model and a very lightweight
LLM based model that can serve

243
00:18:46,780 --> 00:18:48,419
edge aware observability as well.

244
00:18:48,919 --> 00:18:53,230
In terms of resilience, it'll
become proactive and predictive by

245
00:18:53,230 --> 00:18:56,040
default and no longer a reaction.

246
00:18:56,169 --> 00:18:59,879
But it should be an intrinsic
platform capability.

247
00:19:00,379 --> 00:19:03,919
Now let's talk about a practical
implementation guideline.

248
00:19:04,419 --> 00:19:07,639
So for those planning to
implement the first step is to

249
00:19:07,639 --> 00:19:08,884
prepare your infrastructure.

250
00:19:09,724 --> 00:19:17,464
Always ensure that you have enough comu,
compute and memory for LLM inference

251
00:19:17,464 --> 00:19:20,814
workloads and strategize on your data.

252
00:19:21,684 --> 00:19:27,894
So you need to prioritize clean labeled
telemetry, sanitize the data in such

253
00:19:27,894 --> 00:19:33,384
a way that can be fed into the model
as a baseline in vector databases.

254
00:19:34,314 --> 00:19:38,984
So that the model can seamlessly
respond back by looking at

255
00:19:38,984 --> 00:19:41,264
the semantics of your data.

256
00:19:41,764 --> 00:19:46,204
Next is training your teams
whereby upskilling the ops

257
00:19:46,204 --> 00:19:47,854
team on a driven workflows.

258
00:19:48,354 --> 00:19:53,944
And before you get started, you need
once you have a baseline ready, the

259
00:19:53,944 --> 00:19:59,394
next step is to pilot by starting
small one service or a failure

260
00:19:59,394 --> 00:20:02,064
mode, and then expand gradually.

261
00:20:02,564 --> 00:20:06,805
You can also minimize dep disruption
with well-planned change management.

262
00:20:07,100 --> 00:20:12,590
With smooth integration and also
keep measuring gains and refining

263
00:20:12,590 --> 00:20:14,689
automation with continuous monitoring.

264
00:20:15,189 --> 00:20:20,620
So these steps help a platform as a
service vendors embed LLM Observability

265
00:20:20,620 --> 00:20:23,229
without detailing existing operations.

266
00:20:23,729 --> 00:20:30,379
So now let's wrap To wrap up,
let's talk about like how LMS are

267
00:20:30,379 --> 00:20:32,120
not just improving observability.

268
00:20:32,945 --> 00:20:37,504
They are redefining
resilience at a larger scale.

269
00:20:38,225 --> 00:20:42,155
By embedding these capabilities
directly into platform as a service

270
00:20:42,155 --> 00:20:48,545
offerings, we can now dramatically
cut down downtime, reduce operational

271
00:20:48,545 --> 00:20:55,264
burden, and can also deliver reliable
self-healing platforms to every tenant.

272
00:20:55,764 --> 00:20:57,235
Thank you for joining with me today.

273
00:20:57,504 --> 00:20:59,159
I would love to continue the conversation.

274
00:21:00,084 --> 00:21:05,865
Feel free to connect with me on
LinkedIn if you are exploring or

275
00:21:05,865 --> 00:21:07,215
implementing LLM Driven Observability.

276
00:21:07,715 --> 00:21:08,104
Thank you.

277
00:21:08,604 --> 00:21:08,725
I.

