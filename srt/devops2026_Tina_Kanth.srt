1
00:00:00,930 --> 00:00:01,410
Speaker 45: Hi everyone.

2
00:00:02,090 --> 00:00:06,810
Today we are getting into the
real time finops intelligence.

3
00:00:07,020 --> 00:00:14,790
We all know that AI space is being rogue
in terms of innovations in the latest

4
00:00:15,120 --> 00:00:22,270
years and financial services is no,
different from any other industries,

5
00:00:22,270 --> 00:00:30,490
from being impacted in a positive way
and enhancing the capabilities especially

6
00:00:30,490 --> 00:00:36,819
the ops part and all as same as the
developer productivity side of it.

7
00:00:37,209 --> 00:00:39,905
So today, so I'm Tina.

8
00:00:40,509 --> 00:00:47,260
I work for Microsoft and I work in a team
where we, drive the financial engine, the

9
00:00:47,260 --> 00:00:49,630
trillion dollar platform for Microsoft.

10
00:00:50,200 --> 00:00:55,240
So without further you, let's
jump into the topic and.

11
00:00:55,740 --> 00:00:57,690
Let's talk about the challenges first.

12
00:00:57,690 --> 00:01:00,960
So what happens in the financial platform?

13
00:01:00,960 --> 00:01:04,800
So we're in the realm
of a revolution, right?

14
00:01:05,129 --> 00:01:07,410
And what was the previous one?

15
00:01:07,470 --> 00:01:09,330
It was the cloud services.

16
00:01:09,570 --> 00:01:13,560
So most of the financial platforms
which are risk award, wherein

17
00:01:14,340 --> 00:01:19,740
on-premise servers, which were
moved to cloud services and

18
00:01:20,340 --> 00:01:22,229
most of the companies that as.

19
00:01:22,635 --> 00:01:28,215
That has happened and still there
are like some places here and there,

20
00:01:28,215 --> 00:01:30,585
which is, which are still catching up.

21
00:01:30,585 --> 00:01:36,975
But how for being in cloud services
and in distributed world, brings

22
00:01:36,975 --> 00:01:39,465
its own advantages and challenges.

23
00:01:39,765 --> 00:01:41,295
So what do we do here?

24
00:01:41,295 --> 00:01:46,335
So we have millions of events
coming and we have a high

25
00:01:46,335 --> 00:01:47,925
throughput streaming system.

26
00:01:47,925 --> 00:01:53,335
So most of the systems are either
microservices spark engines,

27
00:01:53,335 --> 00:01:55,135
and they're being like store.

28
00:01:55,435 --> 00:02:03,025
There are storages, like cosmos or reds
in between, storages in Azure or AWS,

29
00:02:03,055 --> 00:02:09,175
which are cloud platforms, which we
have today, and we drive observability

30
00:02:09,295 --> 00:02:11,635
on top of it, which is very critical.

31
00:02:11,665 --> 00:02:15,805
Monitoring and observability are
very critical to her running the

32
00:02:15,805 --> 00:02:19,765
financial systems without outages.

33
00:02:20,300 --> 00:02:22,255
And what do we have here?

34
00:02:22,495 --> 00:02:25,484
So we have transactions.

35
00:02:25,875 --> 00:02:27,195
Which are high throughput.

36
00:02:27,584 --> 00:02:29,054
We have logs coming in.

37
00:02:29,054 --> 00:02:32,754
We track some metrics and there are.

38
00:02:33,254 --> 00:02:33,884
Schema.

39
00:02:33,884 --> 00:02:38,594
So when you say schemas, those are
like very domain dependent, right?

40
00:02:38,834 --> 00:02:46,484
So if, suppose if I am a team which
browses the revenue or recognizes

41
00:02:46,484 --> 00:02:53,089
the revenue of the company, then I.
As a team would be interested in few

42
00:02:53,089 --> 00:02:58,739
fields, whereas the finance or the
marketing would have would be looking

43
00:02:58,739 --> 00:03:02,939
into some other fields as well,
together with some of the consolidated

44
00:03:02,939 --> 00:03:06,019
fields are populated from one team.

45
00:03:06,259 --> 00:03:14,519
So those differ, but what remains the same
as the challenges of less outages high

46
00:03:14,519 --> 00:03:17,849
availability and having minimum latency.

47
00:03:17,849 --> 00:03:24,299
And also the auditability when it
comes to the tools we implement.

48
00:03:24,629 --> 00:03:27,749
So right now we have
automations, we have tools.

49
00:03:28,079 --> 00:03:35,669
So we are at a juncture of finding,
okay, what AI can do, how can we leverage

50
00:03:35,669 --> 00:03:42,259
AI more yeah, this is what sort of, we
talked about, like what we are processing.

51
00:03:42,259 --> 00:03:44,689
So what do we have in hand?

52
00:03:44,989 --> 00:03:51,179
So today, all these systems, whether
the transactions the metrics and

53
00:03:51,179 --> 00:03:55,219
the application locks all are
in different tooling systems.

54
00:03:55,219 --> 00:04:00,709
So we don't have a consolidated
place at DAB or an SRE should go.

55
00:04:01,384 --> 00:04:08,044
To each individual systems and trace the
path of a particular transaction when

56
00:04:08,044 --> 00:04:13,564
it comes to an outages or an error or
why didn't process some transactions.

57
00:04:13,564 --> 00:04:16,354
That's the most common
questions we question we get.

58
00:04:16,924 --> 00:04:20,639
So these are the challenges and.

59
00:04:21,429 --> 00:04:25,959
Yes, we do have ai, we do have co-pilots.

60
00:04:26,259 --> 00:04:32,639
We have authenticated co-pilots integrated
to our systems with some exceptions

61
00:04:32,639 --> 00:04:34,799
like, high risk financial platforms.

62
00:04:34,799 --> 00:04:43,779
Still, we are not fully not every fully
enabling the AI access, but, with with

63
00:04:43,839 --> 00:04:51,089
our own guardrails, Microsoft has AI
systems enabled and if we fully run on AI

64
00:04:51,089 --> 00:04:58,029
platforms with adequate security keeping
security out of the context for a while.

65
00:04:58,029 --> 00:05:03,459
So given that you know we are working
in a secure environment, what are

66
00:05:03,669 --> 00:05:06,129
why the ad hoc prompting fails?

67
00:05:06,369 --> 00:05:12,399
The most important or something as
important a as prompt is the context.

68
00:05:12,609 --> 00:05:19,339
So context engineering is an enter, is
a field by itself and which wouldn't

69
00:05:19,339 --> 00:05:22,279
suffice the stock to cover even.

70
00:05:22,329 --> 00:05:24,609
So we can't even scratch the surface.

71
00:05:25,539 --> 00:05:28,689
Prompt engineering, context
engineering or the mix of both.

72
00:05:29,109 --> 00:05:34,939
But what we carry with us, or what we
keep in mind is that prompt engineering

73
00:05:34,939 --> 00:05:37,369
and context engineering go hand in hand.

74
00:05:37,639 --> 00:05:41,119
One is not a substitute of
other, so I could be a master

75
00:05:41,209 --> 00:05:42,799
of writing the best prompts.

76
00:05:43,129 --> 00:05:48,259
I would've learned everything, what a
clouds on it system or a Gemini system.

77
00:05:49,099 --> 00:05:55,049
Gemini model can work through the
prompts and give a adequate maybe

78
00:05:55,049 --> 00:05:57,299
I can even fine tune my model.

79
00:05:57,869 --> 00:06:03,179
But then if we miss the context,
then it is the prompts are failing.

80
00:06:03,539 --> 00:06:08,759
And why why do you think ad
hoc prompting doesn't suffice?

81
00:06:09,089 --> 00:06:11,729
It's because I'll be a dev one.

82
00:06:11,819 --> 00:06:17,280
I. Write certain type of certain
prompt, and I may not have all the

83
00:06:17,309 --> 00:06:23,909
context on how I should troubleshoot
it or how what I am really looking at.

84
00:06:23,999 --> 00:06:28,809
So maybe I am dev working in a
different team or sister team

85
00:06:28,809 --> 00:06:30,849
or my expertise somewhere else.

86
00:06:31,179 --> 00:06:35,139
And Dev too comes in and writes
the same prompt, but they give

87
00:06:35,139 --> 00:06:39,639
all the necessary context and the
moral give as the right output.

88
00:06:39,689 --> 00:06:43,559
This is more of a
standardization, I would say.

89
00:06:43,619 --> 00:06:50,479
So that calls for the standardization of
using the AI models across not only in

90
00:06:50,580 --> 00:06:52,290
observability space or the DevOps space.

91
00:06:52,290 --> 00:06:53,370
It is everywhere.

92
00:06:53,550 --> 00:07:00,259
So suppose I'm working in a mi in a
microservice and I'm developing those,

93
00:07:00,359 --> 00:07:06,110
i'm developing some logic around like how
I process this process, a set of records,

94
00:07:06,200 --> 00:07:11,210
a set of transactions, or what is the
input how the output should look like.

95
00:07:11,930 --> 00:07:15,940
I would be working and then, moving
forward to another another one and

96
00:07:15,940 --> 00:07:18,400
the one and keep going, but whoever.

97
00:07:19,115 --> 00:07:23,495
Comes in later and makes changes
to that system, which still have to

98
00:07:23,495 --> 00:07:25,715
adhere to the fundamental design of it.

99
00:07:26,224 --> 00:07:31,335
So some of those just just tangent,
tangential to this is, would be like

100
00:07:31,425 --> 00:07:37,665
in developer productivity at GitHub's
SPECT as being phenomenal in setting

101
00:07:37,665 --> 00:07:42,015
up the guard release for the project
and not letting the developers

102
00:07:42,015 --> 00:07:44,385
violate from the design requirements.

103
00:07:44,885 --> 00:07:50,045
And coming back to our topic, so
missing the context when we get an Inca.

104
00:07:50,165 --> 00:07:55,805
So we know that all these AI models
are probabilistic with the same prompt.

105
00:07:56,165 --> 00:07:59,555
There are times where you
get different outputs.

106
00:07:59,915 --> 00:08:04,995
That could be the context window,
that could be how we write the prompt

107
00:08:05,055 --> 00:08:07,335
or like how we write the prompt.

108
00:08:08,040 --> 00:08:14,020
In the sense that, what is the order in
which like we analyze a certain scenario.

109
00:08:14,560 --> 00:08:19,870
With probabilistic model, inconsistent
output is something which should be,

110
00:08:19,920 --> 00:08:26,010
expected at this point, and I know
many of us have gone through the paper

111
00:08:26,070 --> 00:08:31,190
paper published recently about the
hallucinations of the model and how we

112
00:08:31,190 --> 00:08:33,920
can, how that is something unavoidable.

113
00:08:34,245 --> 00:08:34,535
Yeah.

114
00:08:35,035 --> 00:08:41,015
Again, that was something tangential
to our topic, but I think with

115
00:08:41,015 --> 00:08:43,025
the advent of ai, we all have.

116
00:08:43,365 --> 00:08:50,165
Been experiencing, consuming huge
amount of knowledge and catching up

117
00:08:50,165 --> 00:08:51,905
with development every single day.

118
00:08:51,905 --> 00:08:53,735
So we are, should be used to it.

119
00:08:53,825 --> 00:09:00,825
I know all of you are, so I. Yeah,
so this will so moving on to third

120
00:09:00,825 --> 00:09:05,025
and fourth, like this will result
in a poor level of auditability.

121
00:09:05,025 --> 00:09:09,795
So how I'm going to say or
determine the efficiency of that

122
00:09:10,035 --> 00:09:12,125
platform, which I implemented.

123
00:09:12,335 --> 00:09:20,045
So when ad hoc prompting does all of this
will risk cause poor auditability as well,

124
00:09:20,075 --> 00:09:23,245
and in it'll cause a latency overhead.

125
00:09:23,745 --> 00:09:26,895
Now, what is the solution we should do?

126
00:09:26,955 --> 00:09:28,965
Template driven, prompt engineering.

127
00:09:29,175 --> 00:09:34,875
Like I said, this is very similar
to what GitHub's Spec Kit does in

128
00:09:34,925 --> 00:09:38,495
terms of developer productivity,
in terms of software development.

129
00:09:39,095 --> 00:09:46,115
Why then we do have, we are having this
talk here on template driven prompt

130
00:09:46,115 --> 00:09:48,060
engineering for DevOps as assets.

131
00:09:48,950 --> 00:09:52,220
That is because of the
complexity which we spoke about.

132
00:09:52,220 --> 00:09:58,910
Like I have microservices, I have
spark engines running, I have logs

133
00:09:58,910 --> 00:10:01,190
and sto I have data and Cosmos.

134
00:10:01,970 --> 00:10:03,770
How do I integrate all this?

135
00:10:03,770 --> 00:10:07,840
So in case of a software development,
that's not the environment which a

136
00:10:07,840 --> 00:10:12,310
software development happens, but
DevOps happens like this, right?

137
00:10:12,670 --> 00:10:14,020
So we should have the.

138
00:10:14,520 --> 00:10:20,150
The models should be validated, the
designs, the sequence diagrams, the

139
00:10:20,150 --> 00:10:25,510
umls, everything should be integrated
in case of a software deployment.

140
00:10:25,870 --> 00:10:30,930
But in case of, an infrastructure
and full blown application.

141
00:10:31,110 --> 00:10:34,560
We have several components, like
what I stated comes into play.

142
00:10:34,950 --> 00:10:37,980
That is where there
should be more templates.

143
00:10:37,980 --> 00:10:42,620
That's a concept where we can
use LLM into real automation

144
00:10:42,680 --> 00:10:44,930
of financial event streams.

145
00:10:45,490 --> 00:10:49,180
High throughput, high
viability, minimum latency.

146
00:10:49,180 --> 00:10:53,115
All of this could be insured
by moder templates where we.

147
00:10:53,615 --> 00:10:59,355
As engineers, we work on the templates
on what needs to be done and what

148
00:10:59,355 --> 00:11:04,245
sort of prompts and what context are
relevant in a particular scenario.

149
00:11:04,245 --> 00:11:04,335
And.

150
00:11:04,835 --> 00:11:10,565
We also implement the policy constraints,
like what the model should not do.

151
00:11:11,015 --> 00:11:16,395
These become very critical in case
of like production data, especially

152
00:11:16,395 --> 00:11:20,325
you know, when you are dealing with
financial data and machine actionable.

153
00:11:20,625 --> 00:11:25,425
So this is what something going
into the multi-agent system where

154
00:11:25,515 --> 00:11:30,075
the output of one system should
be actionable to actionable and.

155
00:11:30,575 --> 00:11:34,535
Would, should be in a format which
is Fable as input to the next

156
00:11:34,535 --> 00:11:37,625
system for another agent to work on.

157
00:11:38,405 --> 00:11:44,765
Now there are the core framework, so
like we can create template libraries.

158
00:11:44,765 --> 00:11:50,105
And another advantage of doing all
this is that we can use source control

159
00:11:50,345 --> 00:11:52,940
and have it standardized right?

160
00:11:52,940 --> 00:11:58,350
Then any change would go through
reviewers and approvers, which

161
00:11:58,440 --> 00:12:05,520
would help the entire team or the
organization to standardize the process.

162
00:12:06,120 --> 00:12:09,710
And the benefit data layer, which
have all this schema registry.

163
00:12:09,860 --> 00:12:14,690
So this comes into play when we
have to handle the, like anomaly

164
00:12:14,690 --> 00:12:18,920
detection in a financial data
or like any type of transaction.

165
00:12:19,130 --> 00:12:23,390
So that is where the metadata IRK
comes in and the policy engine.

166
00:12:23,390 --> 00:12:28,490
So where you can write rules and
you will have, we have the output

167
00:12:28,490 --> 00:12:30,560
validator to validate the output.

168
00:12:30,590 --> 00:12:34,520
And also this is where the
output validator stage is where

169
00:12:34,520 --> 00:12:36,080
the dashboarding comes to play.

170
00:12:36,130 --> 00:12:38,710
Why won't XX allow a dashboard?

171
00:12:38,990 --> 00:12:44,040
Where the the model efficiency
is being visualized, right?

172
00:12:44,540 --> 00:12:47,990
So the template anatomy what
happens in the template?

173
00:12:47,990 --> 00:12:49,820
What do we have in the template?

174
00:12:50,120 --> 00:12:54,860
We should have the let me touch upon
the operational knowledge first.

175
00:12:55,220 --> 00:12:57,830
Operational knowledge and
the policy constraints first

176
00:12:57,830 --> 00:12:59,150
before coming to the event.

177
00:12:59,150 --> 00:13:04,190
Context and schema reference though, what
constitutes the operational knowledge.

178
00:13:04,190 --> 00:13:07,580
So when do we see an A log?

179
00:13:07,640 --> 00:13:08,660
A log message?

180
00:13:08,900 --> 00:13:10,790
What should be the next action?

181
00:13:11,000 --> 00:13:18,765
So from the log message, what feels a
relevant for the agent for AI system to.

182
00:13:19,425 --> 00:13:23,565
Take the next step or what feels
determined, the next action.

183
00:13:23,985 --> 00:13:31,715
So this operational knowledge lie lies in
the head of many engineers or SRE today.

184
00:13:31,955 --> 00:13:39,035
So this is AI platforms and prompt
engineering especially is a great

185
00:13:39,035 --> 00:13:43,265
way to democratize the knowledge
between within an organization.

186
00:13:43,505 --> 00:13:46,710
So once it is written out there by an SME.

187
00:13:47,175 --> 00:13:48,405
Anyone can use.

188
00:13:48,405 --> 00:13:54,595
So if, I suppose if I'm a junior
engineer going into a troubleshooting

189
00:13:54,595 --> 00:13:59,255
how I do or I, what steps I take for
the troubleshooting, except for the

190
00:13:59,255 --> 00:14:04,835
base basics, it could be way different
than how a senior engineer would

191
00:14:04,835 --> 00:14:06,790
troubleshoot it and get to their results.

192
00:14:07,175 --> 00:14:10,235
So what constitutes
the difference is that.

193
00:14:10,870 --> 00:14:15,170
Subject matter expertise,
their experience and such.

194
00:14:15,230 --> 00:14:19,040
So this is a great equalizer,
great leveler platform.

195
00:14:19,370 --> 00:14:25,190
So the operational knowledge that, and
this is somewhere like the executives

196
00:14:25,190 --> 00:14:31,460
of the ARC comes to play and what the
leadership thinks about AI platform and

197
00:14:31,460 --> 00:14:37,240
how do we enable everyone to be to be
most productive and do more with less.

198
00:14:37,740 --> 00:14:41,505
And the policy constraints, and
we should have an output format

199
00:14:42,005 --> 00:14:46,355
where we determine what should be
or how the output should look like.

200
00:14:46,855 --> 00:14:49,500
So from the templates to the
decision, so when the event.

201
00:14:50,350 --> 00:14:52,250
Ingestion happens.

202
00:14:52,250 --> 00:14:57,740
So we ingest events, which are like
financial events coming into our system.

203
00:14:58,070 --> 00:15:00,440
That is where the
templates come into play.

204
00:15:00,650 --> 00:15:07,160
We can route it using semantic kernel,
which could be a Microsoft agent

205
00:15:07,730 --> 00:15:14,800
framework where, and we can write plugins
where plugins and the agent can route.

206
00:15:15,430 --> 00:15:20,350
The request to several plugins, so
that is the software part of it, the

207
00:15:20,350 --> 00:15:23,700
middleware part, and the templates.

208
00:15:24,255 --> 00:15:27,105
Are the prompts and what do they act upon?

209
00:15:27,135 --> 00:15:29,295
They act upon the logs.

210
00:15:29,475 --> 00:15:31,425
And what is the corpus?

211
00:15:31,425 --> 00:15:36,375
The corpus will have all the docs
that designs the code, everything

212
00:15:36,535 --> 00:15:40,555
embedded and vectorized and
stored in a vector database.

213
00:15:40,855 --> 00:15:46,265
So that, and then we can deploy an
LLM model to process the process the

214
00:15:46,265 --> 00:15:49,475
request or to process the logs, right?

215
00:15:49,685 --> 00:15:51,935
In between it happens.

216
00:15:51,935 --> 00:15:55,095
The that it, what happens
is the context injection.

217
00:15:55,365 --> 00:15:59,675
So the context injection to
a great part would be a part

218
00:15:59,675 --> 00:16:01,745
of the agent framework too.

219
00:16:02,015 --> 00:16:07,185
And we can write the software
to route route on the context

220
00:16:07,185 --> 00:16:08,700
selection of the model.

221
00:16:09,200 --> 00:16:11,150
Of the end framework, I would say.

222
00:16:11,480 --> 00:16:16,090
And then we do a, we can do
necessary validations and

223
00:16:16,280 --> 00:16:17,900
come up with an action item.

224
00:16:18,400 --> 00:16:20,440
So what would be the action item?

225
00:16:20,490 --> 00:16:22,410
The process doesn't end here, right?

226
00:16:22,410 --> 00:16:28,500
The action item could be like, okay, I
have to replay these orders, or I have

227
00:16:28,560 --> 00:16:32,100
to, I have to delete some duplicates.

228
00:16:32,100 --> 00:16:38,240
So there should be an action or which
can be automated enough or enough to,

229
00:16:38,870 --> 00:16:43,520
enough for the agent to perform by
itself or go through an approval process.

230
00:16:44,240 --> 00:16:46,340
So what do we gain here?

231
00:16:46,430 --> 00:16:50,390
So these are production scale, like
we know with the advent of cloud

232
00:16:50,390 --> 00:16:56,300
itself, millions of transactions or
millions processing has been like

233
00:16:56,400 --> 00:17:01,610
like a very common phenomenon with the
capabilities of distributed computing.

234
00:17:01,610 --> 00:17:06,765
So we get the same benefits here
and we get reduced latencies.

235
00:17:07,095 --> 00:17:12,665
And once you build a framework, all
you do always is to build on top of it.

236
00:17:13,115 --> 00:17:17,095
And, what we need to do is only
to update the models when the

237
00:17:17,095 --> 00:17:19,045
new models are in the market.

238
00:17:19,105 --> 00:17:26,925
And by delivering, by using a well
balanced combination of context

239
00:17:26,925 --> 00:17:29,265
engineering and prompt engineering.

240
00:17:29,595 --> 00:17:34,905
We are not, the org organization
is not investing in LS two.

241
00:17:35,405 --> 00:17:37,235
That would be just reinventing the wheels.

242
00:17:37,235 --> 00:17:40,965
We have best performing LLMs right
out in the market and they are all.

243
00:17:41,570 --> 00:17:43,100
We have them out of box.

244
00:17:43,100 --> 00:17:49,610
So all we do is to tweak
it to our own requirements.

245
00:17:50,110 --> 00:17:57,010
So this, for the platform teams,
the most critical component are

246
00:17:57,280 --> 00:18:01,420
critical function of platform
teams is to maintain the systems at

247
00:18:01,420 --> 00:18:04,210
high availability, which AI comes.

248
00:18:05,125 --> 00:18:06,025
Pretty handy.

249
00:18:06,085 --> 00:18:11,665
These tools comes pretty handy and they
have proved to be phenomenal invoices.

250
00:18:11,885 --> 00:18:14,355
And we can automate.

251
00:18:14,505 --> 00:18:19,535
So adding the context and having
the prompts, creating this

252
00:18:19,535 --> 00:18:23,645
automation workflows will give
us more control on what we do.

253
00:18:23,645 --> 00:18:28,805
Not let the model just go hair
and do whatever it wants to,

254
00:18:28,805 --> 00:18:30,755
or whatever it thinks is right.

255
00:18:30,965 --> 00:18:37,235
Rather that the SREs and the platform
teams have a better grasp or better grip,

256
00:18:37,235 --> 00:18:40,025
better control of what these systems do.

257
00:18:40,525 --> 00:18:41,995
What are the considerations?

258
00:18:41,995 --> 00:18:43,825
So where do we start here?

259
00:18:44,125 --> 00:18:49,065
So we can start with a
most common problem, right?

260
00:18:49,455 --> 00:18:52,575
So this is where we go
with the 80 20 rule.

261
00:18:52,575 --> 00:18:59,935
Like 20% of the problems we
solve, we gave an 80% gain, right?

262
00:19:00,295 --> 00:19:05,915
We pick some use cases which are like
very frequent and are at the same

263
00:19:05,915 --> 00:19:11,725
time causing a lot of tile to the DRIs
and integrated through the system.

264
00:19:11,725 --> 00:19:16,375
We connected to the Observative
platforms and at the same time

265
00:19:16,375 --> 00:19:19,640
implement back mechanisms until we know.

266
00:19:20,365 --> 00:19:26,085
The performance and the performance
evaluation is fully done, and we

267
00:19:26,085 --> 00:19:31,515
need to, it's best to have, establish
a feedback loop for improvement.

268
00:19:31,785 --> 00:19:34,785
That could be like ad hoc also, right?

269
00:19:34,835 --> 00:19:38,285
That need not be integrated
at the very beginning.

270
00:19:38,565 --> 00:19:44,385
We can, get the feedbacks, validate get
the results, validate, take the feedback.

271
00:19:44,595 --> 00:19:47,675
And that could be like
one time processing.

272
00:19:47,675 --> 00:19:53,135
So in case of the vector databases, like
people like engineers, the teams are

273
00:19:53,135 --> 00:19:55,985
continuously adding more documentation.

274
00:19:55,985 --> 00:19:59,615
So how do we, how do the model
becomes stale if it works on

275
00:19:59,615 --> 00:20:01,535
the previous data and some of.

276
00:20:02,195 --> 00:20:04,115
Changes could be like critical, right?

277
00:20:04,325 --> 00:20:05,855
So how do we change that?

278
00:20:05,855 --> 00:20:10,355
So there should be a pipeline,
a separate pipeline to populate

279
00:20:10,355 --> 00:20:14,065
and, do the vectorization and
store it and update the database.

280
00:20:14,365 --> 00:20:19,400
So those are the those will be
like a feedback loops which can

281
00:20:19,470 --> 00:20:23,630
sustain the sustainably improve
the performance of the model.

282
00:20:24,590 --> 00:20:29,510
Like we said, we should have version MA
to maximize the template effectiveness.

283
00:20:29,840 --> 00:20:37,570
We are like teams with 40 or 45
engineers working and in an application.

284
00:20:37,570 --> 00:20:43,945
So version control comes, becomes
very important and we should have a

285
00:20:43,945 --> 00:20:49,390
modular composition of the templates
rather than, write in our one big file.

286
00:20:50,055 --> 00:20:50,685
Everything.

287
00:20:50,835 --> 00:20:56,375
So the advantage of prompts is that like
that, again, that's like a separate, that

288
00:20:56,375 --> 00:20:58,115
could be a separate session in itself.

289
00:20:58,165 --> 00:21:03,525
We modularize the prompts and at
the same time and later we can

290
00:21:03,525 --> 00:21:08,105
refer the prompt files so that
the module can check for the file.

291
00:21:08,105 --> 00:21:09,695
So that is something which we touch.

292
00:21:09,695 --> 00:21:15,505
Bay touch touched up on a bit on the agent
while talking about the agentic framework.

293
00:21:15,605 --> 00:21:17,735
Now extending the framework.

294
00:21:17,945 --> 00:21:23,215
So we have lot of cost gains,
especially lot of man hours would

295
00:21:23,215 --> 00:21:28,645
be saved by automating these
workflows and we can implement the

296
00:21:28,645 --> 00:21:31,905
risk evaluations for our platforms.

297
00:21:31,905 --> 00:21:37,790
I'm sure you all would have heard about
architected frameworks, AWS and Azure have

298
00:21:38,060 --> 00:21:40,460
their well architected frameworks and we.

299
00:21:40,960 --> 00:21:47,950
Many teams leverage the, those frameworks
and have agent model evaluate their

300
00:21:47,950 --> 00:21:54,180
applications to for the agent to
validate the security requirements,

301
00:21:54,180 --> 00:21:58,830
the performance efficiency, and
cost effectiveness of those systems,

302
00:21:58,830 --> 00:22:02,010
and it just churns out the results.

303
00:22:02,510 --> 00:22:08,620
So those automations have like huge value
monetarily as well as efficiency wise.

304
00:22:08,980 --> 00:22:13,930
So yeah, that's what the compliance,
something which we attached upon.

305
00:22:13,930 --> 00:22:19,120
And security capacity planning, so the
resource forecasting and scaling recomme.

306
00:22:19,620 --> 00:22:22,560
Also, this model can turn out.

307
00:22:22,980 --> 00:22:28,440
So incident triage is one of the
main areas where many teams have,

308
00:22:28,530 --> 00:22:31,920
are highly leveraging DevOps for ai.

309
00:22:32,420 --> 00:22:34,130
So what are the key takeaways?

310
00:22:34,420 --> 00:22:39,550
Use your templates, modelize your
templates, and put the context in

311
00:22:39,550 --> 00:22:44,680
the right place and make sure you
audit the performance of the model.

312
00:22:45,040 --> 00:22:47,570
With that, I'll end talk here.

313
00:22:47,630 --> 00:22:51,340
It was great to have the
opportunity to interact with.

314
00:22:51,885 --> 00:22:55,575
Everyone of you and we all
look forward to what's next?

315
00:22:55,635 --> 00:22:59,725
What's the next major wave in the ai.

316
00:22:59,725 --> 00:23:01,975
I am also like braising for that.

317
00:23:02,305 --> 00:23:03,565
Thank you everyone.

318
00:23:04,065 --> 00:23:04,335
Bye.

