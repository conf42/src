1
00:00:00,570 --> 00:00:01,320
Hello everyone.

2
00:00:02,100 --> 00:00:07,170
I'm Nikita Kamath and I'm thrilled
to be here today to talk about real

3
00:00:07,170 --> 00:00:12,090
time notification systems, a critical
component in today's digital world.

4
00:00:12,590 --> 00:00:16,640
These systems are responsible
for delivering adults messages

5
00:00:16,759 --> 00:00:18,710
or updates to users instantly.

6
00:00:19,310 --> 00:00:24,350
Whether it's a push notification,
a SMS or an email, they play a huge

7
00:00:24,350 --> 00:00:26,810
role in user engagement and retention.

8
00:00:27,680 --> 00:00:30,950
When done right, they feel
seamless and intuitive.

9
00:00:31,760 --> 00:00:34,340
When they fail, users
will notice immediately.

10
00:00:34,840 --> 00:00:40,360
Today we'll explore how to architect
these systems to scale globally, stay

11
00:00:40,360 --> 00:00:45,190
reliable under pressure, and deliver
messages that matter at the right time.

12
00:00:45,690 --> 00:00:51,360
So let's talk about the first thing that's
most important in these large scale, real

13
00:00:51,360 --> 00:00:54,660
time notification system and that scale.

14
00:00:55,160 --> 00:00:59,065
Or rather, let's call it the
scale challenge, because before

15
00:00:59,065 --> 00:01:01,405
we optimize, we have to survive.

16
00:01:02,395 --> 00:01:07,915
Platforms like Facebook handle about
8 billion notifications every day.

17
00:01:08,455 --> 00:01:11,695
That's nearly a hundred thousand
notifications per second.

18
00:01:12,445 --> 00:01:15,685
And these systems are expected
to deliver with latencies of

19
00:01:15,685 --> 00:01:17,815
less than 75 milliseconds.

20
00:01:18,775 --> 00:01:21,745
Do you know that's less than
a, how much a human can blink?

21
00:01:22,245 --> 00:01:25,455
On top of that, usage
patterns can be unpredictable.

22
00:01:26,205 --> 00:01:31,455
Think major news events, celebrity live
streams, or e-commerce, flash sales.

23
00:01:31,905 --> 00:01:37,065
These triggers, traffic spikes
like, like 300% over the baseline.

24
00:01:37,815 --> 00:01:41,835
Architectures need to handle these,
this level of scale gracefully,

25
00:01:42,375 --> 00:01:47,475
which means being both horizontally
scalable and resilient in real time.

26
00:01:47,975 --> 00:01:54,125
So let's talk about the first thing
that's, architecture, which drives

27
00:01:54,125 --> 00:01:56,855
these real time notification systems.

28
00:01:57,355 --> 00:01:59,965
200 such scale and complexity.

29
00:02:00,415 --> 00:02:04,285
We move away from monoliths and
adopt a microservice architecture.

30
00:02:05,244 --> 00:02:09,355
Each service is small, focused,
and independently deployable.

31
00:02:10,165 --> 00:02:12,954
One service handles user preferences.

32
00:02:13,225 --> 00:02:17,905
The other manages device tokens, a
third one probably queues messages,

33
00:02:18,175 --> 00:02:20,005
and a fourth one actually sends them.

34
00:02:20,505 --> 00:02:26,295
This lets teams deploy and scale
just the services under stress, like

35
00:02:26,295 --> 00:02:30,765
delivering service, like delivery
services during a big campaign without

36
00:02:30,765 --> 00:02:34,025
impacting the rest of the system hidden.

37
00:02:34,460 --> 00:02:38,510
Savings there on cogs, you
only scale where you need

38
00:02:38,510 --> 00:02:40,160
to and not the whole system.

39
00:02:40,660 --> 00:02:43,360
Another huge benefit is
technology diversity.

40
00:02:44,260 --> 00:02:50,800
You might use no JS for low ancy delivery
services, Python for analytics, and go

41
00:02:50,800 --> 00:02:53,410
for performance critical components.

42
00:02:54,100 --> 00:02:59,415
This modularity also further announces
fault tolerance if one service fails.

43
00:02:59,995 --> 00:03:04,105
It doesn't just bring down the
whole system, it's limited, and

44
00:03:04,135 --> 00:03:07,975
just that feature, that part
functionality is what is impacted.

45
00:03:08,475 --> 00:03:13,265
Moving on, the next thing we would talk
about is event driven architecture,

46
00:03:13,835 --> 00:03:16,565
often tightly coupled with services.

47
00:03:17,065 --> 00:03:20,695
Microsoft has shine actually when
paired with event driven architecture.

48
00:03:21,385 --> 00:03:26,095
In this model, services emit events
instead of calling each other.

49
00:03:26,545 --> 00:03:30,805
They're asynchronous, meaning
producers no longer wait for consumers.

50
00:03:31,315 --> 00:03:37,225
For example, when a user likes a photo,
say on Facebook or Instagram, the app for

51
00:03:37,225 --> 00:03:42,745
emit an event, the notification system in
the background listens to this event and

52
00:03:42,745 --> 00:03:44,575
triggers the appropriate notification.

53
00:03:45,400 --> 00:03:51,430
All where the user is totally believes
and the user's experience as such is

54
00:03:51,430 --> 00:03:53,470
absolutely not impacted or affected.

55
00:03:54,370 --> 00:04:00,220
This pattern causes natural deconflict,
so services can evolve independently.

56
00:04:00,940 --> 00:04:05,855
It also introduces buffer zones that
absorb traffic spikes without passing

57
00:04:05,995 --> 00:04:07,810
on back pressure to the whole system.

58
00:04:08,310 --> 00:04:11,399
It's resilient, scalable
patterns that can.

59
00:04:11,900 --> 00:04:15,950
10,000 to a hundred thousand
messages per second with sub

60
00:04:15,950 --> 00:04:17,930
millisecond internal latency.

61
00:04:18,650 --> 00:04:19,640
Isn't that fascinating?

62
00:04:20,140 --> 00:04:23,530
What can event driven architecture
not really function without,

63
00:04:24,030 --> 00:04:25,170
for folks who, you guessed it.

64
00:04:25,350 --> 00:04:26,190
You guessed it, right?

65
00:04:26,789 --> 00:04:27,840
A message broker.

66
00:04:28,340 --> 00:04:31,999
Message broker is basically a glue
that holds the whole system together.

67
00:04:32,900 --> 00:04:37,789
Two of the most popular options that we
have on the market are Kafka and Rabbit

68
00:04:37,820 --> 00:04:40,849
mq, and they serve different purposes.

69
00:04:41,330 --> 00:04:45,830
Kafka is ideal for high throughput
streaming, capable of handling

70
00:04:45,830 --> 00:04:47,659
millions of events per second.

71
00:04:48,499 --> 00:04:53,330
It's optimized for bulk data
fault tolerant and built to scale.

72
00:04:54,169 --> 00:04:58,880
On the other hand, RabbitMQ is more
feature rich in terms of routing.

73
00:04:59,840 --> 00:05:01,640
It supports conditional delivery.

74
00:05:02,119 --> 00:05:04,849
Topic based routing and priority queues.

75
00:05:05,419 --> 00:05:09,619
This is perfect when you need to
deliver different types of messages

76
00:05:09,619 --> 00:05:12,409
to different devices or user segments.

77
00:05:12,890 --> 00:05:18,889
Many real time systems use both Kafka
for ingest events and rapid MQ for

78
00:05:18,889 --> 00:05:20,694
routing and personalization issue.

79
00:05:21,194 --> 00:05:26,425
We discussed earlier that two distributed
systems drive on two specific Cornerstone.

80
00:05:26,785 --> 00:05:31,435
One being scale, which we just discussed,
and the second being resiliency.

81
00:05:32,065 --> 00:05:37,075
So in the next slide, we are gonna
be talking about resiliency patterns.

82
00:05:37,575 --> 00:05:41,745
Let's take a closer look at like
resiliency, because in large scale

83
00:05:41,745 --> 00:05:46,935
distributed systems failure is not just
an, it's imminent, it's a certainty

84
00:05:46,935 --> 00:05:48,735
that's gonna happen someday or the other.

85
00:05:49,575 --> 00:05:53,025
This system consists of dozens
or even hundreds of services.

86
00:05:53,865 --> 00:05:55,515
Communicating over the network.

87
00:05:55,755 --> 00:06:00,885
This means a failure can cascade
fast if not handed properly.

88
00:06:01,245 --> 00:06:06,015
A small issue like a slow red,
instant can ca, can literally

89
00:06:06,015 --> 00:06:07,755
snowball into a full outage.

90
00:06:08,255 --> 00:06:11,375
There are different ways how we
can literally stay resilient.

91
00:06:12,125 --> 00:06:16,085
Let's start off with the first one
on the slide that's circuit breakers.

92
00:06:16,715 --> 00:06:20,045
A circuit breaker has nothing but
a fuse in your electrical system.

93
00:06:20,545 --> 00:06:25,525
If a downtime service starts failing,
say 50% of the requests of a push

94
00:06:25,525 --> 00:06:30,285
certificate provider are timing out
the circuit breaker, the circuit opens

95
00:06:30,555 --> 00:06:32,415
and stops sending traffic temporarily.

96
00:06:33,165 --> 00:06:37,965
This prevents your system from wasting
resources on a failed component and gives

97
00:06:37,965 --> 00:06:44,100
the troubled service some time to recover
after a. The ser, your service will again

98
00:06:44,100 --> 00:06:48,690
test the waters and resume only if the
downstream service is healthy back again.

99
00:06:49,190 --> 00:06:51,290
Next, we are gonna talk about our Balkans.

100
00:06:51,790 --> 00:06:53,110
let's think of a ship here.

101
00:06:53,920 --> 00:06:58,210
It has com compartments
or Balkans if one floods.

102
00:06:59,155 --> 00:07:03,475
We just hope and play that the ship
doesn't go down right or it doesn't sink.

103
00:07:04,135 --> 00:07:09,655
Similarly, in the concept or in the world
of software, it primarily means isolating

104
00:07:09,655 --> 00:07:12,685
components, so failures can be contained.

105
00:07:13,345 --> 00:07:19,135
For example, if an SMS delivery fails
due to a third party provider, it

106
00:07:19,135 --> 00:07:21,115
shouldn't affect push notifications.

107
00:07:21,745 --> 00:07:26,905
Each channel operates in its own resource
pool, separate threads, separate queues.

108
00:07:27,550 --> 00:07:32,950
Sometimes even separate services, so
when there is isolation, you at least

109
00:07:32,950 --> 00:07:34,420
your whole system is not failing.

110
00:07:34,420 --> 00:07:35,170
Catastrophically.

111
00:07:35,670 --> 00:07:37,440
Let's talk about time management here.

112
00:07:38,250 --> 00:07:41,580
A slow service can be more
dangerous than a failed one.

113
00:07:42,360 --> 00:07:47,160
It consumes threads, memory, and
blocks the upstream components.

114
00:07:47,670 --> 00:07:52,530
That's why we use strict timeouts
with exponential back off if a

115
00:07:52,530 --> 00:07:55,230
downstream calls takes more than.

116
00:07:55,890 --> 00:07:58,410
Say your timeout is set
to 200 milliseconds.

117
00:07:59,370 --> 00:08:00,720
We just abandon it.

118
00:08:00,720 --> 00:08:06,630
We don't wait endlessly, and with
exponential back off, each retry is spaced

119
00:08:06,630 --> 00:08:10,950
out to reduce load and increase the chance
of recovery for the downstream system.

120
00:08:11,450 --> 00:08:16,819
That brings us, we spoke about
exponential back off, but wait, how,

121
00:08:16,819 --> 00:08:18,230
where does it come into a picture?

122
00:08:18,280 --> 00:08:21,010
That brings us to the next
topic, which is retry policy.

123
00:08:21,985 --> 00:08:26,455
Not all failures are permanent
transient failures like a network

124
00:08:26,455 --> 00:08:31,135
hiccup can often be resolved by
retrying, but we must be smart.

125
00:08:32,005 --> 00:08:36,414
Use exponential backup to avoid hammering
the downstream service repeatedly.

126
00:08:36,914 --> 00:08:41,864
Add jitters or randomness to spread
out retrials over a period of time and

127
00:08:41,864 --> 00:08:46,214
avoid synchronized spikes a problem
also known as the thundering herd.

128
00:08:46,714 --> 00:08:51,574
These resiliency patterns together
reduce the blast radius of a failure

129
00:08:52,025 --> 00:08:56,944
and let the system degrade gracefully
instead of catastrophically.

130
00:08:57,635 --> 00:08:58,594
Is it a great thing?

131
00:08:58,594 --> 00:09:03,575
No, but at least it's more contained
and handled better than the whole

132
00:09:03,580 --> 00:09:05,285
system going down in like a J.

133
00:09:05,785 --> 00:09:09,135
Coming to our next topic,
sharding and partitioning.

134
00:09:10,094 --> 00:09:14,834
Sharding is how we scale horizontally
and keep the performance consistent.

135
00:09:15,405 --> 00:09:18,525
Even as we add users,
region, and functionality.

136
00:09:19,425 --> 00:09:21,914
there is not a one size
fits all solution here.

137
00:09:22,454 --> 00:09:26,175
So let's take a moment and explore
the different sharding strategies.

138
00:09:26,654 --> 00:09:31,229
Starting off with user based,
shared by user ID to ensure

139
00:09:31,319 --> 00:09:32,759
ordering and consistency.

140
00:09:33,449 --> 00:09:38,550
If all notifications for a user are
handed by the same partition, we can

141
00:09:38,550 --> 00:09:40,230
guarantee that they arrive in order.

142
00:09:40,650 --> 00:09:43,949
This is a critical experience,
especially for messaging,

143
00:09:43,980 --> 00:09:48,300
security alerts or sequential
delivery where ordering matters.

144
00:09:48,900 --> 00:09:51,390
The next thing is the geographic charter.

145
00:09:52,199 --> 00:09:56,969
Be by user ID to ensure
ordering and consistency.

146
00:09:57,540 --> 00:10:01,469
If all notifications for
a user are the partition.

147
00:10:01,969 --> 00:10:04,999
We actually ensure that
they arrive in order.

148
00:10:05,540 --> 00:10:09,050
Now, what is geographic
sharding come into picture here?

149
00:10:09,349 --> 00:10:12,920
The closer your data and the
services are to the user, the

150
00:10:12,920 --> 00:10:14,959
faster you can deliver sharding.

151
00:10:15,459 --> 00:10:21,160
We route users in Europe to a EU based
cluster and us to a US based cluster.

152
00:10:21,639 --> 00:10:25,119
This reduces round trip time,
especially important for push

153
00:10:25,119 --> 00:10:29,769
notifications and mobile devices
where latency is visible to users.

154
00:10:30,189 --> 00:10:35,589
So with the combination of where
we need user, where order matters,

155
00:10:35,920 --> 00:10:38,469
and we also latency matters too.

156
00:10:38,739 --> 00:10:42,430
It's a combination where geographic and
user based sharding comes into picture.

157
00:10:42,459 --> 00:10:45,369
The next thing is time-based charting.

158
00:10:46,089 --> 00:10:49,660
This separates hot data, like
recent notifications from

159
00:10:49,660 --> 00:10:51,999
cold data archive messages.

160
00:10:52,329 --> 00:10:56,019
It improves performance and
simplifies data lifecycle management.

161
00:10:56,800 --> 00:11:01,900
For instance, messages from like
today are placed in fast access

162
00:11:01,900 --> 00:11:06,729
partition, while older messages are
placed in cheaper, slower storage.

163
00:11:07,229 --> 00:11:08,954
What is functional charting?

164
00:11:09,435 --> 00:11:12,614
We often separate different
types of notification.

165
00:11:13,064 --> 00:11:17,685
A high priority alert, like a suspicious
login, doesn't go through the same

166
00:11:17,685 --> 00:11:19,694
pipeline as a promotional banner.

167
00:11:20,535 --> 00:11:23,954
This allows for tailored
delivery strategies, failure,

168
00:11:24,254 --> 00:11:28,004
isolation, and even different
compliance requirements prototype.

169
00:11:28,634 --> 00:11:32,535
By combining these strategies like
we just discussed a little before, we

170
00:11:32,535 --> 00:11:37,604
can scale to millions of users while
keeping latencies under 15 milliseconds.

171
00:11:38,104 --> 00:11:41,569
That brings us to our next
topic, which is adaptive rate.

172
00:11:41,889 --> 00:11:45,395
Limiting rate limiting is traffic
control for your services.

173
00:11:45,844 --> 00:11:51,814
It ensures that no component is over and
that critical traffic gets through first.

174
00:11:52,295 --> 00:11:53,855
But in modern systems.

175
00:11:54,455 --> 00:11:56,375
Static limits just don't work.

176
00:11:56,495 --> 00:12:01,175
The system needs to adapt to these
limits in real time, monitor,

177
00:12:01,325 --> 00:12:05,135
load, system load, and calculate
these capacities dynamically.

178
00:12:05,705 --> 00:12:09,395
So let's go through this phase,
like a brainstorming session, right?

179
00:12:09,395 --> 00:12:12,065
What do we need to have
adapting rate limiting sessions?

180
00:12:12,845 --> 00:12:15,875
Firstly, we need to know
and monitor the system load.

181
00:12:16,365 --> 00:12:19,849
That's where the observability of
our services comes into picture.

182
00:12:20,449 --> 00:12:24,020
We continuously track these health
metrics, whether it's the CPU, our

183
00:12:24,020 --> 00:12:29,360
memory, our request, latencies Q Depth,
and also the downstream success rates.

184
00:12:30,140 --> 00:12:36,079
This gives us a live pulse on how the
system is performing, or what we call in

185
00:12:36,079 --> 00:12:38,599
software like cost, quality of service.

186
00:12:39,319 --> 00:12:43,430
This enables us to calculate
the capacity dynamically.

187
00:12:44,090 --> 00:12:48,530
Based on current and historical
data, we estimate safe throughputs

188
00:12:48,530 --> 00:12:50,060
using predictive models.

189
00:12:50,630 --> 00:12:55,730
If a service has been degrading
around 70% of CPU U historically,

190
00:12:56,120 --> 00:12:57,920
we set that as our soft ceiling.

191
00:12:58,640 --> 00:13:00,650
We also monitor bus patterns.

192
00:13:01,340 --> 00:13:06,740
For example, say that every Friday
at 6:00 PM we see a spike or a peak.

193
00:13:07,580 --> 00:13:09,920
Then we can adjust our models proactively.

194
00:13:10,420 --> 00:13:15,580
This like this kind of helps us adjust
and we have the data, we have our

195
00:13:15,580 --> 00:13:20,140
capacities, and now we can actually
talk about what we do or how we

196
00:13:20,140 --> 00:13:22,060
adjust rate limits when loads spike.

197
00:13:22,560 --> 00:13:25,805
One of the ways we can do this
is adjust in ingress rates.

198
00:13:26,305 --> 00:13:30,895
How many new events can our
service except per second, we can

199
00:13:30,895 --> 00:13:32,365
modulate or titrate this data.

200
00:13:33,235 --> 00:13:36,475
Second, we can also work
on our prioritization.

201
00:13:37,015 --> 00:13:39,145
Give more preference to urgent traffic.

202
00:13:39,595 --> 00:13:42,960
For example, two FA
Codes, alerts, throttling.

203
00:13:43,900 --> 00:13:48,610
Before or further the matter even drop
low priority events at a given time,

204
00:13:48,610 --> 00:13:50,560
like newspaper, less letter things.

205
00:13:51,160 --> 00:13:55,060
If your service is not healthy,
then you actually prioritize, and in

206
00:13:55,060 --> 00:13:59,710
that low prioritize, higher priority
notifications in the load times so

207
00:13:59,710 --> 00:14:04,600
that your system is actually doing
what it essentially needs do, deliver

208
00:14:04,600 --> 00:14:06,280
what's highest priority at that.

209
00:14:06,780 --> 00:14:12,150
This ensure survival system survival and
graceful degradation again, and not just

210
00:14:12,150 --> 00:14:17,640
an abroad failure where, you might end up
delivering a new newsletter notification

211
00:14:17,850 --> 00:14:22,490
and not really, delivering a notification
from the bank, which is of, speech log.

212
00:14:23,150 --> 00:14:27,320
So that's where stuff like prioritization
and throttling help comes handy.

213
00:14:27,820 --> 00:14:31,630
So now we have gone through a whole
cycle of how we have adjusted our rate

214
00:14:31,630 --> 00:14:35,740
limits, but what's the most important
thing after all, this is feedback loop.

215
00:14:36,490 --> 00:14:41,800
We implement a closed loop feedback system
that refines our rate limiting decisions

216
00:14:42,310 --> 00:14:45,040
continuously based on outcome metrics.

217
00:14:45,350 --> 00:14:48,395
what were our vitri rates,
what were the latencies?

218
00:14:48,475 --> 00:14:50,455
Or one of the event drop counts.

219
00:14:50,965 --> 00:14:56,485
This reduces overload incidents
by up to 85% and keeps the system

220
00:14:56,485 --> 00:14:58,765
utilization at peak efficiency.

221
00:14:59,265 --> 00:15:05,935
We have gone through a gamut of different
architectures, strategies, techniques,

222
00:15:06,595 --> 00:15:10,825
which brings us to our next highly
important topic, which is caching.

223
00:15:11,695 --> 00:15:13,375
So in a distributor system.

224
00:15:13,430 --> 00:15:17,300
We talk about ma multilevel
caching strategy.

225
00:15:17,900 --> 00:15:22,400
Caching is what allows our system
to meet performance goals of sub

226
00:15:22,610 --> 00:15:24,650
millisecond latencies or deliveries.

227
00:15:24,740 --> 00:15:29,570
In this case, even when we are
handling billions of events, but.

228
00:15:30,005 --> 00:15:32,855
One cache is not enough when
you're talking about the scale

229
00:15:32,855 --> 00:15:35,885
of billions of notifications
or message deliveries, right?

230
00:15:36,215 --> 00:15:39,575
So that's where multi-level caching
hierarchy comes into picture.

231
00:15:40,235 --> 00:15:44,855
So first we start off with L one, which
is application level memory cache.

232
00:15:45,355 --> 00:15:50,365
This is like an inpro memory stored
directly on your service runtime.

233
00:15:50,865 --> 00:15:56,355
More, you might have heard of terms
like in memory objects or LRU caches.

234
00:15:56,925 --> 00:16:02,355
It provides sub microsecond access time,
perfect for extra extremely hard data.

235
00:16:03,015 --> 00:16:08,025
Things like notification templates, flags,
or user personalized reference toggles.

236
00:16:08,775 --> 00:16:09,825
But it's limited.

237
00:16:09,915 --> 00:16:13,180
It's a limited till you don't have
unlimited storage on that and.

238
00:16:13,680 --> 00:16:16,830
It's local to your service
runtime and cannot be stored

239
00:16:16,830 --> 00:16:19,010
across, shared across instances.

240
00:16:19,460 --> 00:16:23,210
That's where our L two cache or
distributed cache comes into picture.

241
00:16:23,710 --> 00:16:26,530
Here we could use technologies
like reddish or memc.

242
00:16:27,030 --> 00:16:30,720
This cache is shared
across nodes and supports.

243
00:16:30,720 --> 00:16:34,880
Fast read writes in say,
one to five milliseconds.

244
00:16:35,390 --> 00:16:37,460
We store data like device tokens.

245
00:16:37,475 --> 00:16:42,125
Rate limiting metadata or frequently
access personalization info.

246
00:16:42,725 --> 00:16:48,905
We also set that TTL broadly known as
time to live per, and it's done per key

247
00:16:49,355 --> 00:16:51,575
to balance, freshness and efficiency.

248
00:16:52,355 --> 00:16:57,125
TTLs can be dynamic, for example,
shorter for volatile data and

249
00:16:57,125 --> 00:17:00,305
longer for templates, things
that don't train so often.

250
00:17:00,805 --> 00:17:06,175
Moving on from L one and L two, we move to
L three, which is more persistent storage.

251
00:17:06,579 --> 00:17:07,540
That's our database.

252
00:17:08,040 --> 00:17:12,900
You could use something like a
Postgre, SQL, cosmos, DB Cassandra, and

253
00:17:12,900 --> 00:17:14,579
multiple other options on the market.

254
00:17:15,389 --> 00:17:18,629
Access to these systems are
generally taking between 10 to

255
00:17:18,629 --> 00:17:22,530
a hundred milliseconds, so we
use them only as a fallback.

256
00:17:22,889 --> 00:17:25,859
Only when the data missing is
missing in the L one and L two

257
00:17:25,859 --> 00:17:27,420
cache, that's the upper layers.

258
00:17:27,920 --> 00:17:32,840
The database holds the source of
truth, but our caching ensures that it.

259
00:17:33,425 --> 00:17:36,125
It's, it only serves a
fraction of the traffic.

260
00:17:36,995 --> 00:17:40,355
Remember that our latency and
efficiency numbers will only be

261
00:17:40,355 --> 00:17:44,345
met when most of the traffic is
served from L one and L two Caius.

262
00:17:45,125 --> 00:17:50,345
With this approach, we offload 95% of
the reads from the db, dramatically

263
00:17:50,525 --> 00:17:52,775
improving system performance and scale it.

264
00:17:53,275 --> 00:17:56,395
We've spoken about the
first part of efficiency.

265
00:17:57,310 --> 00:18:01,690
How scale and how motivation systems
can actually handle those facets.

266
00:18:02,170 --> 00:18:05,920
Now the second thing that we spoke
about, especially pertaining to

267
00:18:05,920 --> 00:18:09,040
notification systems, is user engagement.

268
00:18:09,430 --> 00:18:13,330
Or how do we actually optimize
user engagement or retention tube?

269
00:18:14,020 --> 00:18:16,600
So it's not just about
sending notifications, right?

270
00:18:16,600 --> 00:18:19,570
It's about sending the right
message at the right time.

271
00:18:20,070 --> 00:18:22,260
So what's important here is timing.

272
00:18:23,250 --> 00:18:27,750
ML models can analyze user behavior and
identify windows of high engagement.

273
00:18:28,050 --> 00:18:33,510
For example, sending messages for
lunch apps during, like for food apps

274
00:18:33,510 --> 00:18:35,879
during lunchtime, nothing like it.

275
00:18:36,480 --> 00:18:38,159
Second thing is personalization.

276
00:18:38,639 --> 00:18:44,280
We tailor the content based on user
interest, location, and recent actions.

277
00:18:44,880 --> 00:18:48,810
This increases click through rates
in instantly and dramatically.

278
00:18:49,440 --> 00:18:52,860
For example, you might have noticed
that sometimes if you have a NTO

279
00:18:53,640 --> 00:18:57,270
gift card in your wallet and you
are around a n some store, you see

280
00:18:57,270 --> 00:18:59,370
a pop popup from your Apple wallet.

281
00:19:00,030 --> 00:19:05,100
That's because you ubiquitously checks
that you are in the area and goes ahead

282
00:19:05,100 --> 00:19:07,050
and pops that user notification for you.

283
00:19:07,550 --> 00:19:10,190
Another thing to keep in
mind is frequency management.

284
00:19:10,700 --> 00:19:14,120
We dynamically adjust how
often we notify each user.

285
00:19:14,720 --> 00:19:18,290
If someone isn't engaging, we
slow down to avoid fatigue.

286
00:19:18,790 --> 00:19:22,120
Brings us to our next topic, which
is cross channel coordination.

287
00:19:22,750 --> 00:19:26,830
If a push call unseen for a long
time, we do follow up with an email,

288
00:19:27,520 --> 00:19:30,700
making sure that we don't keep
bombarding the user, of course.

289
00:19:31,200 --> 00:19:33,810
So having discussed all these topics.

290
00:19:34,140 --> 00:19:37,890
What are our key takeaways
from this presentation?

291
00:19:37,950 --> 00:19:39,870
Let's summarize the key ideas here.

292
00:19:40,560 --> 00:19:42,660
Starting off with scale.

293
00:19:42,660 --> 00:19:43,020
Again.

294
00:19:44,010 --> 00:19:49,020
Designed for extreme scale,
built for five to 10 times of the

295
00:19:49,020 --> 00:19:54,560
expected peak using distributed
microservices, built and resilience.

296
00:19:55,160 --> 00:19:59,390
Use circuit breakers, retries,
and isolation to protect

297
00:19:59,420 --> 00:20:01,850
against catastrophic failures.

298
00:20:02,675 --> 00:20:04,175
Optimize for latency.

299
00:20:04,865 --> 00:20:09,875
Use caching and sharding to keep
delivery times under 15 milliseconds.

300
00:20:10,375 --> 00:20:12,475
Lastly, maximize engagement.

301
00:20:12,975 --> 00:20:16,485
Obtain delivery by timing,
content frequency, and channel

302
00:20:16,985 --> 00:20:21,210
realtime notification systems
aren't just technical challenges.

303
00:20:21,640 --> 00:20:25,325
They're critical to a user
experience and business success.

304
00:20:25,825 --> 00:20:26,515
Thank you.

305
00:20:27,040 --> 00:20:27,340
For.

306
00:20:27,340 --> 00:20:29,830
So thank you so much for all your time.

307
00:20:30,580 --> 00:20:36,050
I hope you had a practical look into
how we build and operate large scale,

308
00:20:36,050 --> 00:20:38,270
real time notification systems.

309
00:20:38,690 --> 00:20:42,260
If you are working on something
similar or just wanna geek out on

310
00:20:42,260 --> 00:20:44,660
distributed systems, I'd love to connect.

311
00:20:45,440 --> 00:20:48,470
Please feel free to reach out to
me after the session or online.

312
00:20:48,950 --> 00:20:49,940
Thanks again and.

