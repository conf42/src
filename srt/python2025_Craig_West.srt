1
00:00:00,200 --> 00:00:03,380
Hello, everyone, and welcome
to the implementing agentic AI

2
00:00:03,540 --> 00:00:05,479
solutions in Python from scratch.

3
00:00:06,090 --> 00:00:09,560
Here is the repo, and you
have full access to it.

4
00:00:09,589 --> 00:00:11,110
We have all the code samples.

5
00:00:11,550 --> 00:00:14,350
I'm using my notes here, which
I'll have access to, as well as an

6
00:00:14,360 --> 00:00:16,410
HTML version, if you so need it.

7
00:00:16,910 --> 00:00:17,720
So who am I?

8
00:00:18,000 --> 00:00:19,950
I'm one of us, a regular Pythonista.

9
00:00:20,400 --> 00:00:24,249
I was in tech in the early 2000s
as a business information architect

10
00:00:24,250 --> 00:00:28,259
and certified Microsoft SQL
Server DBA for about four years.

11
00:00:28,289 --> 00:00:31,694
And then I returned in 2017
via WordPress and JavaScript.

12
00:00:31,795 --> 00:00:32,364
Frameworks.

13
00:00:32,735 --> 00:00:35,974
Moving to Python and
machine learning in 2021.

14
00:00:36,544 --> 00:00:39,815
Currently, I'm working on a project,
AI powered knowledge systems, building

15
00:00:39,824 --> 00:00:42,125
a book framework similar to my Python.

16
00:00:42,769 --> 00:00:45,290
test full stack, which is
here at pytestcookbook.

17
00:00:45,339 --> 00:00:45,670
com.

18
00:00:46,110 --> 00:00:50,949
I've also got some useful notes on
Django full stack testing, and here

19
00:00:50,950 --> 00:00:52,920
is my main project at the moment.

20
00:00:53,809 --> 00:00:56,370
I'm based in Brighton in the
UK, down on the south coast,

21
00:00:56,370 --> 00:00:57,760
and here is our lovely beach.

22
00:00:58,509 --> 00:01:01,319
And I'm a volunteer coach at Cobar.

23
00:01:01,319 --> 00:01:02,490
io, which I find very rewarding.

24
00:01:02,490 --> 00:01:03,970
We meet every two weeks.

25
00:01:04,970 --> 00:01:08,310
And I've just got myself a
new Red Fox Labrador pup, Leo,

26
00:01:08,430 --> 00:01:09,460
much earlier than planned.

27
00:01:10,205 --> 00:01:15,515
And locally, we have a red fox that
is quite tame, and seems to check

28
00:01:15,525 --> 00:01:18,495
Leo out, and they both stare at
each other, wondering who's who.

29
00:01:18,995 --> 00:01:22,795
My first computer was in 1979, and
it was a paper tape reader, with

30
00:01:22,795 --> 00:01:27,325
a teletype printer for output, and
cut and paste was cut and paste.

31
00:01:27,825 --> 00:01:29,705
So what are AI agents?

32
00:01:30,205 --> 00:01:35,505
The word agent is a subject of much
discussion in academic circles, but

33
00:01:35,565 --> 00:01:40,325
if we're looking at AI agents, we can
see that Pydantic has its version, a

34
00:01:40,325 --> 00:01:44,885
primary interface for interacting with
LLMs, and an Anthropic also defines

35
00:01:44,885 --> 00:01:49,095
workflow and agents, and Hugging Face
says AI agents are programs where

36
00:01:49,105 --> 00:01:51,125
LLM outputs control to workflow.

37
00:01:51,625 --> 00:01:54,325
What we're going to do is we're going
to look at examples of code to see what

38
00:01:54,325 --> 00:01:56,735
AI agents are and what they can do.

39
00:01:57,235 --> 00:02:02,444
Now, for example, if we look at this
link here, we will see The range of

40
00:02:02,504 --> 00:02:04,364
AI agents that are being created.

41
00:02:04,364 --> 00:02:11,314
There's 46 character categories, 825, all
different versions in different areas,

42
00:02:11,604 --> 00:02:15,744
so it can be quite overwhelming to know
which framework to use and what they are.

43
00:02:16,244 --> 00:02:18,014
What's the aim of my talk?

44
00:02:18,484 --> 00:02:23,494
Well, my talk is to a chain to
achieve that, to demystify AI agents

45
00:02:23,494 --> 00:02:27,064
and AI programming because it can
seem like it's another different

46
00:02:27,064 --> 00:02:28,954
world of development for us, ISTs.

47
00:02:29,454 --> 00:02:35,014
And what I'd like to propose is what if AI
agents adjust Python code with a REST API

48
00:02:35,104 --> 00:02:37,334
call, admittedly to a very magical API?

49
00:02:37,834 --> 00:02:40,944
Then we could use day to day Python
design patterns to handle the

50
00:02:40,944 --> 00:02:45,304
responses we get back from these
API calls to the AI, to the LLMs.

51
00:02:45,944 --> 00:02:50,684
And so the main focus of this talk is
to demystify and simplify, and not to

52
00:02:50,684 --> 00:02:53,254
focus on actual real world applications.

53
00:02:54,114 --> 00:02:57,434
And with that in mind, we don't need to
fully graph the code this time round.

54
00:02:57,734 --> 00:02:58,964
It may take a few minutes.

55
00:02:59,574 --> 00:03:03,834
iterations to fully grasp it, so
perhaps to look at the high level view

56
00:03:04,114 --> 00:03:08,904
and to see how it is different from
just regular Python and to realize

57
00:03:08,904 --> 00:03:16,964
that actually AI agents are Python
code with REST API calls to an LLM,

58
00:03:17,404 --> 00:03:19,244
admittedly a very magical REST API.

59
00:03:19,744 --> 00:03:22,424
In fact, what I'd like to propose
is that actually there's no real

60
00:03:22,444 --> 00:03:25,619
difference between doing our regular
day to day Python and Python code.

61
00:03:26,119 --> 00:03:29,619
It is very much like a mouse
that we turn around 180 degrees.

62
00:03:29,979 --> 00:03:34,419
It's still the same actions, up,
down, left, right, but in a different

63
00:03:34,419 --> 00:03:35,709
way, in a different paradigm.

64
00:03:36,069 --> 00:03:39,099
And that can be a little bit tricky
to get to grips with initially.

65
00:03:39,599 --> 00:03:45,869
And in this regard, there are three areas
that I consider to be part of agentic AI.

66
00:03:46,694 --> 00:03:49,884
First, it seems that we can almost
create on the client side the

67
00:03:50,194 --> 00:03:53,884
endpoint, the roots, that we would
normally build on the server side.

68
00:03:54,384 --> 00:03:58,094
We also use natural and human
language, in my case English, to

69
00:03:58,094 --> 00:04:00,114
create code, very much like pseudocode.

70
00:04:00,614 --> 00:04:04,044
And thirdly, we give a sense of
autonomy, in a sense that the LLM

71
00:04:04,044 --> 00:04:05,514
can direct the flow of the app.

72
00:04:05,974 --> 00:04:08,674
And that could be within
bounds that we have created.

73
00:04:08,944 --> 00:04:11,764
But basically, the next step
can be determined by the

74
00:04:11,764 --> 00:04:13,484
LLM, which our app will take.

75
00:04:13,984 --> 00:04:17,634
So before we go into some code examples,
why don't we just refresh ourselves

76
00:04:17,664 --> 00:04:23,064
on what a REST API is before we start
using any library implementations by

77
00:04:23,074 --> 00:04:28,844
using just the requests library so that
we can see that how we actually do a

78
00:04:28,874 --> 00:04:34,784
POST request to our LLM rather than
using, say, an OpenAI library that hides

79
00:04:34,784 --> 00:04:36,704
the implementation of the requests.

80
00:04:37,554 --> 00:04:41,064
So we have our model, we have our
endpoint, and it's worth noting that we

81
00:04:41,064 --> 00:04:45,834
only have one endpoint, one root, and
we'll see why this is apparent later on.

82
00:04:46,334 --> 00:04:50,024
And we will need to send our
token, our API key, in the headers.

83
00:04:50,464 --> 00:04:54,204
We will send our payload, for example,
our model, a list of messages, whether

84
00:04:54,204 --> 00:04:55,754
we're streaming, the temperature.

85
00:04:56,434 --> 00:04:59,354
And here, with requests,
what we can do is send POST.

86
00:04:59,919 --> 00:05:04,619
With all these details to get a JSON
response and to bear in mind, the

87
00:05:04,619 --> 00:05:08,379
request is a string of characters and
doesn't contain any objects or other

88
00:05:08,379 --> 00:05:13,349
data types, and basically it's a JSON
dumps or if we were in JavaScript,

89
00:05:13,349 --> 00:05:15,044
it would be a JSON stringify.

90
00:05:15,284 --> 00:05:18,394
Where all that information
is created as a string.

91
00:05:18,894 --> 00:05:22,554
So what we'll do now is we'll look
at our very first file, 01, to

92
00:05:22,554 --> 00:05:27,814
see this in action and to see the
basics of an agent application.

93
00:05:28,314 --> 00:05:32,874
We're now in our very first file, 01, and
the first thing we're just going to do is

94
00:05:32,874 --> 00:05:38,754
load in our imports of OS JSON requests,
and to read our keys from the env file.

95
00:05:38,954 --> 00:05:41,744
I'm going to keep mine secret here,
but you have a copy here as env.

96
00:05:42,674 --> 00:05:43,204
sample.

97
00:05:43,724 --> 00:05:46,169
Just paste your Open AI key here.

98
00:05:46,669 --> 00:05:51,799
If we come back, what we can
then do is load in our AI key.

99
00:05:52,184 --> 00:05:54,764
And just check that we have
it, which we see we have here.

100
00:05:55,594 --> 00:06:00,074
We're going to select our model,
which in this case is GPT 40mini.

101
00:06:00,304 --> 00:06:01,624
And we just check that it's here.

102
00:06:02,124 --> 00:06:06,214
And for the demonstration, we're going
to create our own class to show how

103
00:06:06,214 --> 00:06:09,694
we can do a request to the endpoint.

104
00:06:10,164 --> 00:06:13,174
Later on, we'll use one of
the libraries from OpenAI.

105
00:06:13,584 --> 00:06:16,134
But we're just going to see in
a very raw form actually how

106
00:06:16,134 --> 00:06:18,029
we would create our own class.

107
00:06:18,729 --> 00:06:21,449
request to an LLM
directly to its endpoint.

108
00:06:21,949 --> 00:06:24,509
So we're going to be making a
POST request to this endpoint.

109
00:06:25,229 --> 00:06:29,309
And what we're going to do in this
class is use this one single endpoint,

110
00:06:29,329 --> 00:06:30,789
and there's only ever one endpoint.

111
00:06:30,829 --> 00:06:34,339
It's not that we have different routes
for different tasks that we want to do.

112
00:06:35,149 --> 00:06:38,719
We have the temperature, which
is a hyperparameter that kind of

113
00:06:38,719 --> 00:06:40,359
takes into account the probability.

114
00:06:40,619 --> 00:06:44,829
Zero means it's very strict and
is as deterministic as possible.

115
00:06:45,209 --> 00:06:50,129
If we want to be more creative in
generating text or images, we would

116
00:06:50,129 --> 00:06:52,289
vary the temperature to, say, 0.

117
00:06:52,289 --> 00:06:55,099
5, 1, etc. The range is between 0 and 2.

118
00:06:55,819 --> 00:06:56,969
We have our system prompt.

119
00:06:57,099 --> 00:06:57,899
We'll get into that.

120
00:06:57,969 --> 00:06:58,989
Our API key.

121
00:06:59,409 --> 00:07:02,559
And here we have our headers
so that we get authentication.

122
00:07:02,904 --> 00:07:04,164
Giving it the content type.

123
00:07:04,644 --> 00:07:06,014
This is our request.

124
00:07:06,674 --> 00:07:11,544
And when we want to generate some text, we
pass in a prompt, our query, our request.

125
00:07:12,494 --> 00:07:17,974
We pass in the payload of the model, the
messages, the system prompt, our prompt.

126
00:07:18,154 --> 00:07:18,974
We'll get into that.

127
00:07:19,359 --> 00:07:22,089
Whether we're streaming, in this
case false, the temperature.

128
00:07:22,849 --> 00:07:27,419
And here, we make our requests
to that endpoint with headers

129
00:07:27,629 --> 00:07:29,329
and also with the payload.

130
00:07:29,619 --> 00:07:33,219
And we can put in here for
future reference, URL equals.

131
00:07:33,499 --> 00:07:34,449
So that's a little bit clearer.

132
00:07:34,949 --> 00:07:37,499
So now that we've got that
class, let's create an instance.

133
00:07:37,579 --> 00:07:39,089
We pass in the model we want.

134
00:07:39,709 --> 00:07:43,049
We pass in the system prompt,
which is like, gives the character

135
00:07:43,649 --> 00:07:45,709
or the personality or the role.

136
00:07:46,084 --> 00:07:47,314
of our agent.

137
00:07:47,814 --> 00:07:50,704
And we're saying in this case, you
give concise answers to questions

138
00:07:50,704 --> 00:07:52,494
with no more than 100 characters.

139
00:07:52,994 --> 00:07:56,084
We can get into more complex
system prompts later, and that's

140
00:07:56,094 --> 00:07:57,914
the sort of prompt engineering.

141
00:07:58,414 --> 00:08:02,094
And we can then make a request
to say what is Pydantic.

142
00:08:02,594 --> 00:08:07,714
And if we print the response that we get
back originally, it is a JSON, stringified

143
00:08:07,714 --> 00:08:10,244
JSON object of all this information.

144
00:08:10,864 --> 00:08:14,244
But if we drill down through
choices, message, and content,

145
00:08:14,524 --> 00:08:15,979
we will end up with the Pydantic.

146
00:08:16,679 --> 00:08:20,109
response that we want by Dantic
is a data validation and settings

147
00:08:20,109 --> 00:08:21,329
management library from Python.

148
00:08:21,329 --> 00:08:24,279
And you can see how it's
respecting the 100 characters.

149
00:08:24,779 --> 00:08:28,919
So this is our base and we can see
that if we were not using any of the AI

150
00:08:28,959 --> 00:08:33,749
libraries but merely using our Python
library of requests and env, that this is

151
00:08:33,759 --> 00:08:39,609
how we would send a POST request to the
LLM with all the details that it needs.

152
00:08:39,999 --> 00:08:41,039
what we would get back.

153
00:08:41,119 --> 00:08:44,949
And what we can do is we can just
hide this and then show the response.

154
00:08:45,449 --> 00:08:49,569
We can see it's quite complicated
in quite detail, but it's choices

155
00:08:49,639 --> 00:08:51,149
is where the answers come back.

156
00:08:51,499 --> 00:08:52,959
We want the very first one.

157
00:08:53,349 --> 00:08:55,289
And as we work through, we see message.

158
00:08:55,489 --> 00:09:01,039
We want to look for the content
where the answer to our query lies.

159
00:09:01,539 --> 00:09:03,769
So this is our base template.

160
00:09:04,214 --> 00:09:07,494
We're going to replace it with
OpenAI library for our calls.

161
00:09:07,994 --> 00:09:11,104
Let's now move on to
the second file, O2 API.

162
00:09:11,604 --> 00:09:14,514
And what we're going to do is we're
going to get a joke back, but we're

163
00:09:14,514 --> 00:09:17,974
also going to make a little bit of
prompt engineering to get the LLM to

164
00:09:17,974 --> 00:09:20,194
rate the joke and give us the next step.

165
00:09:21,184 --> 00:09:23,414
So let's go into that to
see an example of that.

166
00:09:23,414 --> 00:09:27,784
So if we go into O2 API, We
again load in our usual imports.

167
00:09:27,834 --> 00:09:32,224
I'm using rich to colorize
the console output.

168
00:09:32,724 --> 00:09:35,804
And we know we can make a
request to some random joke API.

169
00:09:36,534 --> 00:09:41,064
To get a joke here, using the
request library, and we get the joke.

170
00:09:41,884 --> 00:09:42,774
We won't go into that.

171
00:09:43,274 --> 00:09:45,484
We will load in our open API key.

172
00:09:45,534 --> 00:09:46,824
We'll get our model as usual.

173
00:09:47,274 --> 00:09:52,204
And what we'll do is we'll then
set up a more elaborate, system.

174
00:09:52,704 --> 00:09:54,914
We're going to start with
our basic system message.

175
00:09:55,114 --> 00:09:57,804
You are an assistant that
is great at telling jokes.

176
00:09:58,684 --> 00:10:00,694
But we want to get something
a little bit more advanced.

177
00:10:01,314 --> 00:10:05,484
And it's almost as if we were sending to
the endpoint a new endpoint that would

178
00:10:05,494 --> 00:10:07,504
do something totally different for us.

179
00:10:07,504 --> 00:10:12,464
But what we will find is actually that
we will do this on the client side.

180
00:10:12,964 --> 00:10:13,264
So.

181
00:10:13,689 --> 00:10:16,839
Let's just add this extra prompt here,
and I'm just going to call it prompt

182
00:10:16,839 --> 00:10:21,629
engineering because it's separate from
the system message, totally customizable,

183
00:10:21,649 --> 00:10:23,399
doesn't have to be prompt engineering.

184
00:10:24,029 --> 00:10:26,599
And we're giving it a set of
instructions, and we could almost

185
00:10:26,619 --> 00:10:27,859
consider this to be pseudocode.

186
00:10:28,359 --> 00:10:31,069
A joke worthy of publishing is
a joke that has a rating of 8.

187
00:10:31,119 --> 00:10:32,519
5 or 10 or above.

188
00:10:32,989 --> 00:10:35,739
If the joke is worthy of
publishing, also includes the

189
00:10:35,749 --> 00:10:37,339
next step, whether to publish it.

190
00:10:37,859 --> 00:10:41,139
Otherwise, tell us what the next
step would be, which we should retry.

191
00:10:42,019 --> 00:10:43,399
We give it an example.

192
00:10:43,774 --> 00:10:44,944
of what we're going to get.

193
00:10:45,444 --> 00:10:49,934
So this is called, one shot prompting
or multi shot prompting where

194
00:10:49,934 --> 00:10:51,544
you give one or more examples.

195
00:10:52,294 --> 00:10:55,484
And basically we're giving it an example
of what we would like to get back.

196
00:10:55,974 --> 00:10:58,054
Please supply the response
in the following format.

197
00:10:58,119 --> 00:11:00,189
It's in JSON format here.

198
00:11:00,609 --> 00:11:04,619
And to actually help it, let's actually,
although it works, to be more specific,

199
00:11:04,639 --> 00:11:06,649
we'll say the following JSON format.

200
00:11:07,149 --> 00:11:08,869
Because we want to have
clear instructions.

201
00:11:09,069 --> 00:11:13,409
And here we see we get the setup, the
punchline, we ask it to give us a rating,

202
00:11:13,649 --> 00:11:17,699
and we also then tell us that based
on the rating and whether it deems it

203
00:11:17,699 --> 00:11:22,759
worthy of publishing, to either send the
next step, that we pass on higher up in

204
00:11:22,759 --> 00:11:25,569
the app chain to publish or to retry.

205
00:11:26,289 --> 00:11:29,549
We're also giving it some further
instructions to remove all backticks,

206
00:11:29,949 --> 00:11:31,879
any unnecessary characters.

207
00:11:32,259 --> 00:11:35,959
Once again, I did say JSON format here,
and using capitals can be a useful

208
00:11:35,959 --> 00:11:37,899
way to emphasize things for the LLM.

209
00:11:38,399 --> 00:11:41,399
And also we're saying that if we
do a retry, do not repeat the joke,

210
00:11:41,629 --> 00:11:42,809
and we can even say thank you.

211
00:11:43,699 --> 00:11:45,669
So this is our pseudocode.

212
00:11:46,464 --> 00:11:49,804
This is almost a new endpoint that
we would like to have where we

213
00:11:49,804 --> 00:11:53,614
could send a request and it will
process something different rather

214
00:11:53,614 --> 00:11:55,324
than just returning us a joke.

215
00:11:55,924 --> 00:12:00,474
But what we do is on the client side, we
send that code up to that one endpoint.

216
00:12:01,164 --> 00:12:03,724
And that's the mouse being
at 180 degrees different.

217
00:12:04,114 --> 00:12:06,334
It's like we are now
creating our own code.

218
00:12:07,024 --> 00:12:09,354
REST endpoint R route here.

219
00:12:10,064 --> 00:12:13,804
So we just add those two together
so that it's now the system message.

220
00:12:14,534 --> 00:12:17,404
And we also have our user prompt,
which is going to tell a light hearted

221
00:12:17,404 --> 00:12:19,364
joke for an audience of Pythonistas.

222
00:12:19,864 --> 00:12:23,634
And when we send up to the ALM, we want
to send a list of all these messages,

223
00:12:23,634 --> 00:12:25,374
and the convention is we have prompts.

224
00:12:25,664 --> 00:12:29,354
We have a role for system, and
its content is the system message.

225
00:12:29,584 --> 00:12:32,444
We have a role for user, and
the content is the user prompt.

226
00:12:33,054 --> 00:12:36,284
We can also have role of the AI
assistant, because we may want

227
00:12:36,334 --> 00:12:37,714
to filter out the messages.

228
00:12:37,714 --> 00:12:41,434
And this is the practice with these
LLMs, is that we have a system message,

229
00:12:41,474 --> 00:12:46,224
user message, and an AI message, which
is the response back from the LLM.

230
00:12:46,724 --> 00:12:51,704
So we complete as before, but
this time we are using the OpenAI.

231
00:12:52,414 --> 00:12:57,184
If we scroll up to the top, we're using
from OpenAI, one of its libraries.

232
00:12:57,404 --> 00:13:03,049
So we can just create the client
here, An instance of OpenAI.

233
00:13:03,589 --> 00:13:07,949
And therefore, when we send the request,
we don't need to be as detailed.

234
00:13:08,059 --> 00:13:09,699
We can use a convenience method.

235
00:13:09,869 --> 00:13:10,299
Client.

236
00:13:10,539 --> 00:13:10,829
chat.

237
00:13:11,059 --> 00:13:11,819
completions.

238
00:13:11,839 --> 00:13:12,289
create.

239
00:13:12,709 --> 00:13:13,909
We send the model we want.

240
00:13:14,099 --> 00:13:16,739
We send all our messages
in, which is these prompts.

241
00:13:17,359 --> 00:13:19,919
And when we get it back,
we will get a response.

242
00:13:19,999 --> 00:13:22,929
Again, it's in the choices, it's
in the message, in the content,

243
00:13:23,309 --> 00:13:24,499
and we can display it here.

244
00:13:25,269 --> 00:13:31,179
So we now get a JSON object here,
where it has the setup, the punchline,

245
00:13:32,154 --> 00:13:36,214
The rating and what it advises
to do next in the flow, which

246
00:13:36,244 --> 00:13:38,494
we publish as opposed to retry.

247
00:13:38,994 --> 00:13:42,654
Now, in our app, we might have a
state object that holds all of this

248
00:13:42,654 --> 00:13:47,014
information because this is we've
made the REST API call to the LLM.

249
00:13:47,524 --> 00:13:50,264
But what we do next is day to day Python.

250
00:13:50,544 --> 00:13:56,374
It's any system design, PubSub,
ActorModel, FiniteStateMachine.

251
00:13:56,934 --> 00:13:59,814
We've now been given
exactly what to do next.

252
00:14:00,164 --> 00:14:04,524
Now we can pass that over to another
agent, or we can act upon it ourselves.

253
00:14:05,424 --> 00:14:07,964
And what I'm going to
do here is I run it out.

254
00:14:08,274 --> 00:14:10,034
I've extracted it out nicely.

255
00:14:10,534 --> 00:14:11,264
JSON data.

256
00:14:11,764 --> 00:14:16,194
And when I look at it, I can then
basically see that if the result next

257
00:14:16,274 --> 00:14:22,444
equals publish, for example, I can
load it into our state object, and

258
00:14:22,444 --> 00:14:26,474
I could then go on to publish it,
to send a message to another part

259
00:14:26,484 --> 00:14:28,244
of our program or to another agent.

260
00:14:28,734 --> 00:14:31,644
The main thing is we're kind of
getting almost like an event driven

261
00:14:31,754 --> 00:14:36,494
application, but we've been asking
the LLM to decide the next step.

262
00:14:36,884 --> 00:14:40,374
And that's the autonomy, because
it could have come back with retry.

263
00:14:40,874 --> 00:14:43,924
And in which case, the flow of the
program would go in a different direction.

264
00:14:44,424 --> 00:14:47,164
And so what I've just done
here, I've extracted out the

265
00:14:47,164 --> 00:14:48,714
next step, which is publish.

266
00:14:49,244 --> 00:14:55,234
So this is how our app would have its
flow and its direction directed by the

267
00:14:55,234 --> 00:14:57,744
LLM, as opposed to us imperatively.

268
00:14:58,244 --> 00:15:02,004
So to recap with this, the
main thing is we're structuring

269
00:15:02,004 --> 00:15:03,334
what we would like to get back.

270
00:15:03,364 --> 00:15:06,564
Rather than just getting a joke,
we're setting back an endpoint.

271
00:15:06,564 --> 00:15:07,824
We're setting back instructions.

272
00:15:07,874 --> 00:15:09,154
This is our pseudocode.

273
00:15:09,644 --> 00:15:14,334
This is our REST endpoint that we
send from the client up to the server

274
00:15:14,444 --> 00:15:18,554
along with our payload, and then we
get the response back as we want.

275
00:15:18,634 --> 00:15:21,324
And we're going to see various
different forms of these prompts.

276
00:15:21,654 --> 00:15:25,304
They could be more advanced and
more structured as we go along.

277
00:15:25,804 --> 00:15:30,554
So when we get that back, we can then
basically decide what we want to do next.

278
00:15:31,234 --> 00:15:35,804
If we're having many agents, we
may keep track where each agent is.

279
00:15:36,794 --> 00:15:38,104
So this was O2 API.

280
00:15:39,054 --> 00:15:44,914
An initial start into prompt engineering
and how we use the client side REST API

281
00:15:45,144 --> 00:15:51,034
coding in natural language, enabling
autonomy to take place in our AI agent.

282
00:15:51,534 --> 00:15:55,584
We've seen the two out of the three
steps of the AI reverse process,

283
00:15:55,584 --> 00:15:59,114
where we kind of created on the
client side our route, our endpoint,

284
00:15:59,614 --> 00:16:01,134
and the use of natural language.

285
00:16:01,544 --> 00:16:03,274
And we briefly looked at autonomy.

286
00:16:03,274 --> 00:16:06,694
So how do we handle this
autonomy of the flow of an API?

287
00:16:07,304 --> 00:16:08,341
AI agent app.

288
00:16:08,341 --> 00:16:11,944
Well, we asked in our LLM to
give us not just a rating,

289
00:16:11,974 --> 00:16:13,394
but to give us that next step.

290
00:16:13,704 --> 00:16:15,254
And in this case, it was published.

291
00:16:15,754 --> 00:16:20,164
So what we'd like to do now is to begin
to see how we can handle this in our app.

292
00:16:21,124 --> 00:16:24,284
And we're going to go on to our next
example, which is a sort of an idea is

293
00:16:24,284 --> 00:16:29,224
leading into the idea of an FAQ or sort
of the router pattern, a sort of if else.

294
00:16:29,724 --> 00:16:33,449
And what I like about the FAQ pattern,
which we're going to see now, is

295
00:16:33,509 --> 00:16:38,959
the fact that we can introduce
a little bit of AI into our app.

296
00:16:39,399 --> 00:16:41,219
So let's have a look at
this example of OA03FAQ.

297
00:16:41,719 --> 00:16:44,799
We're going to see an example of sort
of retrieval augmented generation.

298
00:16:44,799 --> 00:16:49,349
Now, we're not querying documents,
but RAG is basically supplementing our

299
00:16:49,349 --> 00:16:53,499
query with additional information that
we haven't fine tuned our model with

300
00:16:53,729 --> 00:16:56,589
or that's important for our LLM query.

301
00:16:57,439 --> 00:16:59,439
So what we're going to do is
we're going to give it a list of

302
00:16:59,449 --> 00:17:02,369
frequently asked questions and
have a little chatbot experience.

303
00:17:02,369 --> 00:17:06,359
So And this is going to pave way
for the next file, which will

304
00:17:06,369 --> 00:17:07,779
be a sort of a router pattern.

305
00:17:08,279 --> 00:17:14,519
So once again, we load in our imports, we
get our key, we get a utility client from

306
00:17:14,519 --> 00:17:19,349
the OpenAI library to ease our connection
to the LLM, which is picking our model.

307
00:17:19,849 --> 00:17:23,339
And we're going to create now a function
that has some history, that has all

308
00:17:23,339 --> 00:17:26,599
the previous messages, that has the
system message and the user prompt.

309
00:17:27,409 --> 00:17:30,779
And the history is important because it
gives us a record of what went on before,

310
00:17:31,109 --> 00:17:32,999
because every request is stateless.

311
00:17:33,499 --> 00:17:35,729
It's like the LLM is seeing
it for the first time.

312
00:17:36,169 --> 00:17:40,159
So we need to pass the history
back with it so it has context

313
00:17:40,519 --> 00:17:42,969
and a certain degree of memory.

314
00:17:43,469 --> 00:17:47,119
So in our function chat, We're
having our role, our system.

315
00:17:47,239 --> 00:17:50,389
We're adding in the history of all
the previous messages that we have

316
00:17:50,389 --> 00:17:52,769
in our chatbot and our user message.

317
00:17:53,199 --> 00:17:55,749
We can print out the history,
print out the messages.

318
00:17:56,509 --> 00:18:00,339
We're using the stream option
for our chatbot, which we'll see

319
00:18:00,339 --> 00:18:03,949
in a minute, and we're going to
then chunk out our responses.

320
00:18:04,429 --> 00:18:09,224
So this is where we start to build
more context into our chatbot.

321
00:18:10,014 --> 00:18:10,494
agent.

322
00:18:11,124 --> 00:18:13,414
We're saying it's a helpful
assistant for a shoe store.

323
00:18:13,754 --> 00:18:17,654
And if a user asks a question, please
be as helpful as possible and as

324
00:18:17,654 --> 00:18:19,444
courteous and professional manner.

325
00:18:19,944 --> 00:18:21,974
You are provided with the
following facts to help you.

326
00:18:22,044 --> 00:18:23,904
Please be verbose and suggestive.

327
00:18:24,114 --> 00:18:26,764
So now I'm changing like the
character and the nature of the

328
00:18:26,764 --> 00:18:29,124
prompt, adding in a little bit
more verbosity and suggestiveness.

329
00:18:29,624 --> 00:18:33,374
So here is a list of just some
basic facts about our shop.

330
00:18:34,074 --> 00:18:36,544
And bear in mind that this could
be retrieved from the database.

331
00:18:36,544 --> 00:18:39,524
This could be a result of
selecting from the Options from

332
00:18:39,524 --> 00:18:41,424
a form for further information.

333
00:18:41,914 --> 00:18:45,454
But we can see here is if we
just pass some extra content,

334
00:18:45,464 --> 00:18:46,684
some retrieved content.

335
00:18:46,894 --> 00:18:50,114
Admittedly, it's already in the file,
but it could have been retrieved from

336
00:18:50,114 --> 00:18:55,144
a database or from some other source,
and we join it onto the system message.

337
00:18:55,644 --> 00:19:01,254
We can now use Gradio to set up a little
chatbot interface to see how this works.

338
00:19:01,754 --> 00:19:05,314
And if we look at the code while this
is working, it hasn't taken a lot to

339
00:19:05,314 --> 00:19:10,014
introduce a fairly sophisticated little
chatbot based on very limited information.

340
00:19:10,024 --> 00:19:12,814
We could very much increase
this greatly in our app.

341
00:19:13,794 --> 00:19:14,924
So as that runs through.

342
00:19:15,424 --> 00:19:16,094
Almost there.

343
00:19:16,434 --> 00:19:17,234
Come back down.

344
00:19:17,734 --> 00:19:19,834
We have our Gradio interface,

345
00:19:20,334 --> 00:19:21,754
and let's just open that up.

346
00:19:22,004 --> 00:19:23,184
In fact, I'll put it in.

347
00:19:23,684 --> 00:19:24,234
There we go.

348
00:19:24,414 --> 00:19:25,324
Here's our chatbot.

349
00:19:25,434 --> 00:19:26,234
Type message.

350
00:19:26,744 --> 00:19:28,374
Now, it's quite simple.

351
00:19:28,374 --> 00:19:32,204
I could just say Sunday, and if we
look at that, it's coming through.

352
00:19:32,544 --> 00:19:33,464
Thank you for your inquiry.

353
00:19:33,544 --> 00:19:34,934
Our store is picked up.

354
00:19:34,934 --> 00:19:35,744
It's about time.

355
00:19:35,824 --> 00:19:37,414
It's Monday to Friday 9 to 5.

356
00:19:37,524 --> 00:19:38,974
Unfortunately, we're closed on Sundays.

357
00:19:39,354 --> 00:19:41,554
Notice how it's quite
verbose and suggestive.

358
00:19:41,584 --> 00:19:42,994
We look forward to welcoming you soon.

359
00:19:43,434 --> 00:19:46,744
If we come back to our code, we
can see that here are the facts.

360
00:19:46,944 --> 00:19:47,974
We don't have very many.

361
00:19:48,404 --> 00:19:50,364
This could be a much
more complex document.

362
00:19:50,534 --> 00:19:52,114
It could be an MD markdown file.

363
00:19:52,114 --> 00:19:53,604
It could be driven from the database.

364
00:19:54,104 --> 00:19:56,854
So if we come back and
let's have another example.

365
00:19:57,159 --> 00:19:58,639
I can say green belts.

366
00:19:59,139 --> 00:19:59,999
Thank you for reaching out.

367
00:20:00,039 --> 00:20:01,009
We don't do that.

368
00:20:01,289 --> 00:20:02,339
Gives us the address.

369
00:20:02,569 --> 00:20:04,709
It says that basically
exclusively in shoes.

370
00:20:05,499 --> 00:20:11,559
So this little example here of basically
having an agentic AI with a minimum

371
00:20:11,559 --> 00:20:16,409
amount of rag, a minimum amount of extra
context can produce a nice small little

372
00:20:16,449 --> 00:20:23,509
app in your, small little AI app in your
Python app without having to be fully AI.

373
00:20:24,369 --> 00:20:26,359
This is what I call a
bit of AI programming.

374
00:20:26,859 --> 00:20:30,039
Now, this is quite an interesting
pattern, because in the next one, it's

375
00:20:30,039 --> 00:20:33,599
the agent router, and it's something
that happened when I was at CodeBar.

376
00:20:34,069 --> 00:20:36,909
somebody asked that they
would like to get a job in AI.

377
00:20:37,199 --> 00:20:37,819
They were doing Python.

378
00:20:38,319 --> 00:20:41,439
And I said to them, Do they have
an AI department where they work?

379
00:20:41,459 --> 00:20:42,039
And they said, No.

380
00:20:42,349 --> 00:20:43,279
And I said, What do they do?

381
00:20:43,319 --> 00:20:44,389
They said they were insurance.

382
00:20:44,389 --> 00:20:45,169
I said, What do you do?

383
00:20:45,719 --> 00:20:49,199
And they said they they don't write
the reports that they're there to go

384
00:20:49,229 --> 00:20:52,409
to person that when somebody wants
a report, they know which one it

385
00:20:52,529 --> 00:20:54,249
is, and they can run it for them.

386
00:20:54,779 --> 00:20:55,439
I thought, Brilliant.

387
00:20:55,509 --> 00:20:56,109
You can do that.

388
00:20:56,609 --> 00:20:57,769
And there were concerns.

389
00:20:57,969 --> 00:20:59,969
They said, put me out of a job.

390
00:21:00,179 --> 00:21:03,219
I said, yes, but you'll then be
the head of the AI department.

391
00:21:03,979 --> 00:21:05,419
And so what we're going to
do is we're going to have a

392
00:21:05,419 --> 00:21:07,529
little variation on this FAQ.

393
00:21:07,529 --> 00:21:08,779
It's a similar type of thing.

394
00:21:09,159 --> 00:21:14,669
We're loading in all the usual imports and
setting ourself up the same chat message.

395
00:21:15,169 --> 00:21:20,219
And a useful tip is caps and italics
and even markdown in one's prompts

396
00:21:20,609 --> 00:21:22,849
actually have an impact with the LLM.

397
00:21:23,009 --> 00:21:25,719
It's being trained on so many
of these that it begins to

398
00:21:25,719 --> 00:21:27,279
recognize the importance.

399
00:21:27,939 --> 00:21:30,389
And I'm just basically
setting up a report agent.

400
00:21:30,439 --> 00:21:32,189
I'm saying you're a
report selection agent.

401
00:21:32,199 --> 00:21:35,159
You're very good at returning the best
report to answer a user's question.

402
00:21:35,859 --> 00:21:39,429
For example, if a user wants a joke,
you reply with, and I'm just using

403
00:21:39,429 --> 00:21:41,479
this format for demonstration purposes.

404
00:21:41,689 --> 00:21:43,019
This will make it nice and bold.

405
00:21:43,279 --> 00:21:45,319
The tool they need is the get joke report.

406
00:21:45,959 --> 00:21:49,229
If they want total sales, the tool
or report they need will be the

407
00:21:49,229 --> 00:21:51,419
get sales would be the best report.

408
00:21:52,199 --> 00:21:56,039
So I made a list of reports here for
whether use the get weather for hotel,

409
00:21:56,039 --> 00:22:01,619
the hotel booking, very much like we
did in the last FAQ, adding them all in.

410
00:22:02,309 --> 00:22:05,609
And if I run all of that, we
will see now that we can actually

411
00:22:05,609 --> 00:22:09,199
have a report selection agent
that will get the right report.

412
00:22:09,639 --> 00:22:12,729
And if we combine that with
information like the date range or

413
00:22:12,729 --> 00:22:16,389
any other properties, we could even
run the report for them and send it.

414
00:22:16,754 --> 00:22:18,184
All through agents.

415
00:22:18,684 --> 00:22:19,674
So let's check.

416
00:22:19,704 --> 00:22:20,844
We've got this one working.

417
00:22:21,344 --> 00:22:21,754
Lovely.

418
00:22:22,004 --> 00:22:22,504
Okay.

419
00:22:22,744 --> 00:22:25,834
So say I want to take a plane and
notice I didn't use the word plane.

420
00:22:25,834 --> 00:22:26,544
I use flight.

421
00:22:26,874 --> 00:22:27,654
Plane to Rome.

422
00:22:28,234 --> 00:22:29,334
What report should I get?

423
00:22:29,834 --> 00:22:30,494
Get the flight.

424
00:22:30,994 --> 00:22:36,094
Plane to Rome and Auto, let's
just check the typos there,

425
00:22:36,394 --> 00:22:40,064
to Rome, and auto to Paris.

426
00:22:40,564 --> 00:22:42,144
It comes back with those two reports.

427
00:22:42,754 --> 00:22:46,304
So straight away, with that, if we had
the information, shall we say, sent

428
00:22:46,304 --> 00:22:50,294
along with it through a form selection,
we could then get the right report.

429
00:22:51,204 --> 00:22:54,554
And so this is an example of a
sort of router that actually,

430
00:22:54,554 --> 00:22:55,844
what do we want to do next?

431
00:22:56,344 --> 00:23:01,964
Through a very simple addition of
some context to our system message,

432
00:23:02,464 --> 00:23:07,074
I'm going to return to this slide a
number of times because it's easy when

433
00:23:07,084 --> 00:23:10,864
we're going through these different
patterns and function callings to lose

434
00:23:10,874 --> 00:23:13,494
sight of what is the essence of an A.

435
00:23:13,524 --> 00:23:18,604
I. agent and A. I. agents are python
code with A. P. I. request to L.

436
00:23:18,604 --> 00:23:23,294
L. M. s. We can only pass a string
in the A. P. I. request and in that

437
00:23:23,294 --> 00:23:26,134
string we create a job description.

438
00:23:26,634 --> 00:23:31,234
And this could be what the role is, what
they do, these are the tools you have,

439
00:23:31,454 --> 00:23:35,264
here is the data to work on, this is what
we want returned, and in what format.

440
00:23:36,204 --> 00:23:38,254
This is prompt or flow engineering.

441
00:23:39,254 --> 00:23:43,814
How we make use of this in terms of
design patterns is then day to day Python.

442
00:23:44,314 --> 00:23:50,414
Fundamentally, it is a function with
inputs, LLM magic, and some output

443
00:23:50,444 --> 00:23:52,434
returned in the form that we want.

444
00:23:52,934 --> 00:23:55,814
And we'll return to this slide
as we proceed through some of

445
00:23:55,814 --> 00:23:58,234
the more involved examples.

446
00:23:58,734 --> 00:24:01,954
Now we've seen how an
agent can make decisions.

447
00:24:02,354 --> 00:24:05,764
About the next step, the autonomy,
how we've created our sort of

448
00:24:05,894 --> 00:24:10,644
client side API, and we use natural
language, but an agent may need tools.

449
00:24:10,644 --> 00:24:11,824
It may need to do anything.

450
00:24:11,824 --> 00:24:14,874
It may need to make a
request to the Internet.

451
00:24:15,174 --> 00:24:19,364
It may need to call upon a function
that we have in our code base to

452
00:24:19,844 --> 00:24:22,174
calculate something and use that it.

453
00:24:23,164 --> 00:24:24,644
as part of its response.

454
00:24:25,144 --> 00:24:28,074
So what we're going to do in O5 is
we're going to look at not how we

455
00:24:28,074 --> 00:24:31,804
just define tools, but also how an
agent can decide which one to use.

456
00:24:32,264 --> 00:24:35,554
Now, usually it's better to have an
agent to just one single thing, but

457
00:24:35,554 --> 00:24:38,764
sometimes we might have an agent that
might need to make a decision for a

458
00:24:38,764 --> 00:24:41,464
particular task, which tool to use.

459
00:24:41,624 --> 00:24:46,344
So we may need to determine Which
tool to use in the agent and

460
00:24:46,514 --> 00:24:49,434
basically, what happens is we're
constantly adding new messages.

461
00:24:49,464 --> 00:24:51,424
Now, where does those functions run?

462
00:24:51,924 --> 00:24:55,464
What's going to happen is when
we've given the prompt, which we're

463
00:24:55,464 --> 00:24:57,824
going to have a look at now in 05.

464
00:24:58,324 --> 00:25:01,954
What we're going to do
is do the standard setup.

465
00:25:02,674 --> 00:25:05,934
But now when we come to our tool
prompt, we're saying you're in a system

466
00:25:05,934 --> 00:25:09,594
that is very good at determining what
tool to use to solve a certain query.

467
00:25:10,094 --> 00:25:14,474
And our AI programming is we give
it in descriptive form, which

468
00:25:14,474 --> 00:25:17,384
is saying we're using Markdown
here to emphasize this is tools.

469
00:25:17,584 --> 00:25:18,674
We have two tools.

470
00:25:19,004 --> 00:25:21,954
We're describing our calculator
tool that does basic arithmetic.

471
00:25:22,734 --> 00:25:24,294
And it responds in JSON.

472
00:25:24,474 --> 00:25:28,734
And we give it an example of the JSON
format, of that, when we pick the

473
00:25:28,734 --> 00:25:33,514
particular tool called Calculator,
the next signal will be to use the

474
00:25:33,804 --> 00:25:38,054
doCalculation function, and, for
example, what argument should be passed.

475
00:25:38,224 --> 00:25:41,094
This is an example of one scenario.

476
00:25:41,854 --> 00:25:43,914
We have a second tool, which
is a joke tool, something

477
00:25:43,914 --> 00:25:45,614
totally different, JSON format.

478
00:25:46,524 --> 00:25:49,744
And this is what the result
the tool would look like.

479
00:25:50,244 --> 00:25:52,214
So, for example, we can ask it a question.

480
00:25:52,214 --> 00:25:53,564
What is 10 times 9?

481
00:25:54,064 --> 00:25:57,234
And what we're going to do is when
we send that out, it will actually

482
00:25:57,234 --> 00:26:01,664
determine that the tool it needs, the
response it's sending back to us is

483
00:26:01,664 --> 00:26:05,984
the calculator, the do calculation,
and the arguments that are needed.

484
00:26:06,484 --> 00:26:12,374
We can strip all of that out and
basically then say if the do next was do

485
00:26:12,414 --> 00:26:15,154
calculation, we can run those functions.

486
00:26:15,709 --> 00:26:19,069
If it was perhaps, for example,
to do the joke, we might do an

487
00:26:19,069 --> 00:26:21,199
internet request to get a joke.

488
00:26:21,699 --> 00:26:25,959
So let's just go back
to the flow of messages.

489
00:26:26,439 --> 00:26:28,149
We're doing all the usual messages.

490
00:26:28,259 --> 00:26:31,639
But when the API, when the LLM
decides that it needs a particular

491
00:26:31,639 --> 00:26:37,099
tool, what it does is it sends back
the tool signature and the arguments

492
00:26:37,099 --> 00:26:39,269
that it's extracted from the query.

493
00:26:39,769 --> 00:26:46,009
We then on our own computer box, not the
LLM, run that function, get the result.

494
00:26:46,569 --> 00:26:50,479
And add that back to the list of
messages and send it to the LLM that

495
00:26:50,479 --> 00:26:52,109
does the next step, for example.

496
00:26:52,609 --> 00:26:55,339
Now, that particular example is one
where we're going to see later where

497
00:26:55,339 --> 00:27:00,159
we're going to use planning, where it's
thinking, making an action, getting a

498
00:27:00,159 --> 00:27:01,929
result, putting it back in the loop.

499
00:27:02,709 --> 00:27:08,349
This particular example here in 05 tool
is just the first step of showing how

500
00:27:08,369 --> 00:27:10,449
it can determine which tool to use.

501
00:27:11,249 --> 00:27:12,609
So I'm just going to run everything.

502
00:27:13,339 --> 00:27:16,109
And as you see, we load in
our imports, we get our key.

503
00:27:16,614 --> 00:27:17,594
We get our messages.

504
00:27:18,084 --> 00:27:21,464
It's added into the system
message at the very end here.

505
00:27:21,964 --> 00:27:22,474
Our code.

506
00:27:22,694 --> 00:27:23,784
This is our endpoint.

507
00:27:24,529 --> 00:27:27,659
On the client side, in
natural human language.

508
00:27:28,289 --> 00:27:31,959
And it's like a very clear description you
give to somebody when they join a company.

509
00:27:32,059 --> 00:27:33,419
This is how you do your job.

510
00:27:33,729 --> 00:27:36,549
The more detail, the clearer
you can be, the better.

511
00:27:37,049 --> 00:27:39,559
So, for example, we
asked, what is 10 times 9?

512
00:27:40,289 --> 00:27:41,679
We've added the messages in.

513
00:27:42,049 --> 00:27:44,639
We've got back this response
that it's determined the tool

514
00:27:44,669 --> 00:27:46,139
it needs is the calculator tool.

515
00:27:46,489 --> 00:27:48,289
It knows what to do next.

516
00:27:48,289 --> 00:27:50,769
It's the do calculation,
and it knows the arguments.

517
00:27:50,939 --> 00:27:56,009
10 and 2, 10 and 9, and the
operation is multiplication.

518
00:27:56,299 --> 00:27:57,529
What is 10 times 9?

519
00:27:57,929 --> 00:28:00,359
Notice how it's picked up
times and multiplication.

520
00:28:00,859 --> 00:28:04,899
What we can then do is strip that all
out, and if we come back down here, in

521
00:28:04,899 --> 00:28:09,139
this particular agent, we can actually
run some code, or we could pass this on

522
00:28:09,139 --> 00:28:13,259
to another agent, or run it through a
loop again, which we'll see later on.

523
00:28:13,579 --> 00:28:18,939
So in this example, because it knows that
doNext says doCalculation, it extracts all

524
00:28:18,939 --> 00:28:24,519
the information, the tool, the arguments,
and then basically just carries out these

525
00:28:24,519 --> 00:28:26,339
functions to produce the answer back.

526
00:28:26,959 --> 00:28:31,319
So if we do that again, but with
some different numbers, 102 times 3,

527
00:28:31,819 --> 00:28:34,439
and we run it all, and we see
it going back through to clear.

528
00:28:34,939 --> 00:28:39,799
It's now picked out arguments
102 and 3, operation multiply.

529
00:28:40,139 --> 00:28:41,099
Let's do add.

530
00:28:41,459 --> 00:28:44,509
What is 102's plus 3?

531
00:28:44,619 --> 00:28:45,259
Let's run that.

532
00:28:45,659 --> 00:28:48,579
We can see that it's
picking out the arguments.

533
00:28:48,839 --> 00:28:50,269
It's knowing which tool it needs.

534
00:28:50,344 --> 00:28:52,624
1 0 2 3 and addition.

535
00:28:53,094 --> 00:28:57,274
And when we come back down to the
answer, it produces the answer 1 0 5.

536
00:28:57,774 --> 00:29:00,304
Let's see if it picks up if
it needs the different tool.

537
00:29:00,324 --> 00:29:02,584
So, for example, here, I'll do that.

538
00:29:03,084 --> 00:29:03,694
Tell me a joke.

539
00:29:03,784 --> 00:29:05,684
I'm doing this at a builders conference.

540
00:29:05,844 --> 00:29:06,674
Let's run it all.

541
00:29:07,174 --> 00:29:09,684
So now it should determine
that what is the right tool.

542
00:29:09,684 --> 00:29:10,824
It's the do joke tool.

543
00:29:10,834 --> 00:29:11,764
It's a different tool.

544
00:29:12,264 --> 00:29:12,917
There it are.

545
00:29:12,917 --> 00:29:13,803
The joke, do joke.

546
00:29:13,803 --> 00:29:16,134
The audience has picked
up one of the arguments.

547
00:29:16,134 --> 00:29:17,464
It doesn't run here.

548
00:29:18,064 --> 00:29:21,904
Because the do next is do joke, it
just goes off to the internet and gets

549
00:29:21,904 --> 00:29:23,484
a joke, and then we see the answer.

550
00:29:23,984 --> 00:29:28,164
So you may be thinking they all seem
a little bit the same, the router,

551
00:29:28,194 --> 00:29:30,394
the tool use, the basic query.

552
00:29:30,394 --> 00:29:33,374
And I suppose they are because
they're just function calls.

553
00:29:33,424 --> 00:29:36,364
At the end of the day, we're just
doing Python functions, getting

554
00:29:36,364 --> 00:29:39,974
a response back from an API, then
doing something with that response.

555
00:29:40,334 --> 00:29:44,244
The difference now is that we create
our endpoint, we create our route on the

556
00:29:44,244 --> 00:29:46,594
client side, ship up that pseudocode.

557
00:29:47,344 --> 00:29:50,864
ship up the kind of query,
and we get the answer back.

558
00:29:51,214 --> 00:29:55,094
And we also enable a certain level
of autonomy because we can ask the

559
00:29:55,094 --> 00:29:59,194
LLM what it should do next based
on the prompt that we sent it.

560
00:29:59,694 --> 00:30:02,884
So this is what is called tool
use, or it's just function calling.

561
00:30:02,974 --> 00:30:05,974
And it's just a mechanism of
how we do the function calling.

562
00:30:06,564 --> 00:30:09,884
And once again, the function
takes place on our box.

563
00:30:10,304 --> 00:30:12,694
We don't run it on the LLM's box.

564
00:30:12,974 --> 00:30:17,524
And when we get the result of that
information, we pass it back to the LLM.

565
00:30:17,544 --> 00:30:22,824
And we're going to see that when we come
on to the what's called the reason act

566
00:30:23,064 --> 00:30:26,474
react type pattern for planning agent.

567
00:30:26,644 --> 00:30:32,134
So we've seen a few little pieces of
how we can create simple AI agents,

568
00:30:32,194 --> 00:30:33,714
and some can be quite powerful.

569
00:30:33,714 --> 00:30:37,489
Your utilities, like we saw in the
frequently asked questions or the report

570
00:30:37,489 --> 00:30:43,034
selector and What we're going to do now is
actually look at the four main patterns.

571
00:30:43,254 --> 00:30:48,794
Andrew Ng, in his lecture, listed here,
talked about the four main patterns.

572
00:30:48,794 --> 00:30:53,554
Reflection, where the LLM kind
of re examines its own work.

573
00:30:53,654 --> 00:30:57,744
It sends it back to itself, but
with a critique to say, make a

574
00:30:57,744 --> 00:30:59,734
critique of what I've just sent you.

575
00:31:00,284 --> 00:31:02,284
Tool use, we've seen an example of that.

576
00:31:03,084 --> 00:31:08,054
planning, where it comes up with a
plan to execute a multi step goal,

577
00:31:08,804 --> 00:31:10,664
and also multi agent collaboration.

578
00:31:11,164 --> 00:31:13,764
So we've seen a number of these
examples, and what we're going to

579
00:31:13,764 --> 00:31:18,084
do now is we're going to look at the
reflection pattern, the tool pattern,

580
00:31:18,244 --> 00:31:20,134
planning, and the multi agent pattern.

581
00:31:20,634 --> 00:31:26,134
So let's go into the code to look at
the reflection pattern to start with.

582
00:31:26,634 --> 00:31:28,784
We're now going to look
at the reflection pattern.

583
00:31:29,694 --> 00:31:32,194
And let us not forget in
essence what we're doing.

584
00:31:32,654 --> 00:31:36,694
We're just creating a big string
job description that we send.

585
00:31:36,974 --> 00:31:38,724
We get some response.

586
00:31:39,004 --> 00:31:43,474
We may append that to a new request
or start a new request from fresh.

587
00:31:44,134 --> 00:31:47,204
But in this reflection pattern, what we
do is we generate a response with our

588
00:31:47,214 --> 00:31:53,834
first query, then add this content to
the request in a second query where we've

589
00:31:53,834 --> 00:31:58,114
asked it to do something, and in this case
to have a critique and further refinement.

590
00:31:58,974 --> 00:32:01,664
So in some sense, the first
request, it can be considered

591
00:32:01,664 --> 00:32:03,324
as actually almost like RAG.

592
00:32:03,749 --> 00:32:08,119
That we're generating some
content to add to a new query,

593
00:32:08,629 --> 00:32:12,919
augmenting it with a new set of
instructions and getting a response.

594
00:32:13,279 --> 00:32:15,669
And what we're going to do in this one
is we're going to ask it to generate

595
00:32:15,669 --> 00:32:20,489
some Python code, and then we're going to
ask for it to critique it and make some

596
00:32:20,529 --> 00:32:23,059
adjustments to produce a final response.

597
00:32:23,059 --> 00:32:28,739
So we use our usual standard opening,
getting the key, setting the models.

598
00:32:29,499 --> 00:32:32,779
And we can see here that we're
setting the very first system

599
00:32:32,789 --> 00:32:37,099
message, first role, as a Python
program tasked with generating code.

600
00:32:37,859 --> 00:32:39,669
And we're generating our chat history.

601
00:32:39,709 --> 00:32:44,139
We append our system content,
the job description, as it were.

602
00:32:44,639 --> 00:32:50,429
We then add to that, list, the
user query, which in this case is

603
00:32:50,429 --> 00:32:54,129
generate a Python implementation of
requesting an API with request library.

604
00:32:54,629 --> 00:32:56,489
And we then send that to the LLM

605
00:32:56,989 --> 00:32:58,779
to get our response back.

606
00:32:59,559 --> 00:33:01,219
And here we get our response.

607
00:33:01,719 --> 00:33:05,979
And before we do that in for the next
step, we also add what we got back.

608
00:33:06,529 --> 00:33:09,449
So we can see we get the
response from the LLM.

609
00:33:10,319 --> 00:33:14,259
It's giving us some sample
code with an explanation.

610
00:33:14,759 --> 00:33:16,189
Now we want to reflect on that.

611
00:33:16,259 --> 00:33:19,509
We want to send it back again and
have a little critique or refinement.

612
00:33:20,009 --> 00:33:23,869
So in our chat history, we add
in now another system message,

613
00:33:24,429 --> 00:33:27,089
and we're saying you're an
experienced and talented Pythonista.

614
00:33:27,329 --> 00:33:31,269
You're tasked with generating critique
and recommendations for the user's code.

615
00:33:31,769 --> 00:33:35,929
All of these messages are attached
together and sent, because we

616
00:33:35,929 --> 00:33:39,539
must remember that the request is
stateless, so it needs to know what

617
00:33:39,539 --> 00:33:43,289
went on before, so we must parse in
that history of the conversation.

618
00:33:43,789 --> 00:33:47,829
And we then get our critique
back and display it.

619
00:33:48,754 --> 00:33:50,014
And here is the output.

620
00:33:50,034 --> 00:33:54,274
Your code demonstrates a solid
approach going through it all, all

621
00:33:54,774 --> 00:33:55,394
the way down.

622
00:33:55,894 --> 00:34:00,734
And then we add this again, this
critique to our chat history, send it

623
00:34:00,814 --> 00:34:02,794
all in again to get a final summary.

624
00:34:03,294 --> 00:34:05,644
And here's our final
summary in form of an essay.

625
00:34:06,214 --> 00:34:10,874
You scroll down, and here is
our final output as requested.

626
00:34:11,374 --> 00:34:16,454
Now, of course, what we do and how many
times we go through it is up to us, but

627
00:34:16,454 --> 00:34:21,659
the pattern is literally just Making
requests, getting a response, taking that

628
00:34:21,659 --> 00:34:27,869
response, adding it back into our chat
history, adding in a new prompt to do

629
00:34:27,869 --> 00:34:29,449
something with that and get a response.

630
00:34:29,949 --> 00:34:34,459
It's one function after another
with inputs producing outputs get

631
00:34:34,459 --> 00:34:37,559
put into the next function as an
input that produces an output.

632
00:34:38,409 --> 00:34:39,969
So this is a reflection pattern.

633
00:34:40,469 --> 00:34:42,609
And as we scroll down,
we see the final answer.

634
00:34:43,269 --> 00:34:44,349
Key improvements.

635
00:34:45,179 --> 00:34:48,509
And, of course, we can make this a
class and make the input come through a

636
00:34:48,509 --> 00:34:50,249
certain form field or through a chat bot.

637
00:34:51,009 --> 00:34:54,299
And once again, if we go up to the
very top, we can remember, in essence,

638
00:34:54,999 --> 00:34:56,269
this is really what we're doing.

639
00:34:56,279 --> 00:34:57,349
We're making a function.

640
00:34:57,349 --> 00:35:00,799
We're passing in inputs, getting
an output and really chaining it

641
00:35:01,299 --> 00:35:03,029
from one request to the other.

642
00:35:03,569 --> 00:35:08,689
In this particular reflection pattern,
we're going to look at using planning

643
00:35:08,699 --> 00:35:11,059
or using reflection and tool calling.

644
00:35:11,659 --> 00:35:14,809
Once again, let us remind ourselves
that we're looking in this talk to see

645
00:35:14,979 --> 00:35:19,529
what AI agents are in terms of their
simplicity before we move into frameworks.

646
00:35:20,029 --> 00:35:24,999
And what we're going to do now is use a
sort of pattern of react, reason and act.

647
00:35:25,499 --> 00:35:30,099
And essentially, we pass the output of
each step as an input to the next request

648
00:35:30,099 --> 00:35:31,899
to an LLM, like we did in Reflection.

649
00:35:32,299 --> 00:35:35,559
But we're going to add in some tool
calling, and we're also going to be adding

650
00:35:35,559 --> 00:35:40,549
in some effectively routing, because the
agent will determine which tool to use.

651
00:35:41,139 --> 00:35:42,149
And how to use it.

652
00:35:42,649 --> 00:35:48,039
We'll do this with the 20 planning agent
with loop dot py But we will also do it

653
00:35:48,059 --> 00:35:52,559
with the notebook where we will do this
Looping manually so we can see how it

654
00:35:52,559 --> 00:35:58,099
works So let's just go to the python file
and we can see that we run in everything

655
00:35:58,719 --> 00:36:04,269
As usual and we're creating an agent class
where we're setting the client the system

656
00:36:04,279 --> 00:36:07,169
role We're using the under core method.

657
00:36:07,309 --> 00:36:11,339
We're using an execute function where we
can just invoke the llm to get a response

658
00:36:11,339 --> 00:36:19,029
You And what we want to do in this example
is calculate the total price for an item.

659
00:36:19,529 --> 00:36:22,099
And the two tools we have
are calculate the total that

660
00:36:22,129 --> 00:36:24,139
given a price adds on the VAT.

661
00:36:25,009 --> 00:36:29,999
We also have another tool, function,
get product price, that for a given

662
00:36:30,039 --> 00:36:32,609
argument gets the price of the product.

663
00:36:33,194 --> 00:36:36,044
And if we scroll down, we will
see these two functions here.

664
00:36:36,044 --> 00:36:38,034
CalculateTotal, GetProductPrice.

665
00:36:38,534 --> 00:36:42,974
So let's look at our system prompt, and
we're telling it how we want it to work.

666
00:36:43,364 --> 00:36:48,614
We want it to think, take an action,
get an observation, and repeat the loop.

667
00:36:49,114 --> 00:36:52,934
So we give examples of what the tools are.

668
00:36:53,929 --> 00:36:58,339
What the kind of response we'd like
to get back for both the calculate

669
00:36:58,399 --> 00:37:01,309
price, total and get product price.

670
00:37:01,879 --> 00:37:04,029
And we're also given an example session.

671
00:37:04,139 --> 00:37:06,499
So what's going to happen is a user
is going to say, what is the total

672
00:37:06,679 --> 00:37:08,209
cost of a bike, including that?

673
00:37:09,029 --> 00:37:11,789
We want the AI response
to be in this format.

674
00:37:12,189 --> 00:37:14,829
Thought, I need to find
the cost of a bike.

675
00:37:15,219 --> 00:37:18,659
We're pipe delimbing it so we can
get the function name and the price.

676
00:37:19,029 --> 00:37:22,699
arguments, but it's going to be an
action type as opposed to an answer.

677
00:37:23,169 --> 00:37:26,719
We're going to get the tool call
and we're going to get the argument.

678
00:37:27,219 --> 00:37:31,519
We will get the response back of an
observation of the actual return of

679
00:37:31,519 --> 00:37:35,409
that function, which we will then
use as an input to the next query

680
00:37:35,719 --> 00:37:39,359
where it now needs to calculate
the total price including the VAT.

681
00:37:39,859 --> 00:37:40,659
It's an action.

682
00:37:41,034 --> 00:37:44,764
It knows to use the calculate total,
and it has an argument passed into it.

683
00:37:45,204 --> 00:37:48,844
This is just a sample example so it
can see the format of what it needs

684
00:37:48,844 --> 00:37:53,474
to do, and that we will always be
passing in the result of our LLM

685
00:37:53,524 --> 00:37:56,454
course as observation pipe 240.

686
00:37:56,824 --> 00:37:58,164
That's what it can expect.

687
00:37:58,664 --> 00:38:01,174
Then we tell it that if you
have the answer, print out an

688
00:38:01,244 --> 00:38:02,644
answer for us in this form.

689
00:38:03,144 --> 00:38:05,974
So let's just run this just to see
what it will actually look like.

690
00:38:06,384 --> 00:38:07,494
And if we scroll down.

691
00:38:07,779 --> 00:38:09,039
We can see we've got a loop.

692
00:38:09,079 --> 00:38:11,849
We're not going to get into each
line of the code, but basically

693
00:38:11,899 --> 00:38:13,299
it's going to be looping around.

694
00:38:13,799 --> 00:38:16,089
If it's an action, it
will run that function.

695
00:38:16,589 --> 00:38:19,749
Get a value and pass it back
through to be used again.

696
00:38:20,369 --> 00:38:24,569
If it determines it has an answer, it'll
print the answer and exit the loop.

697
00:38:25,259 --> 00:38:26,989
So, for example, we've got three here.

698
00:38:26,989 --> 00:38:29,069
One is the cost of a
bike, a TV, and a laptop.

699
00:38:29,569 --> 00:38:31,759
So, let's run that and see what happens.

700
00:38:32,259 --> 00:38:33,279
It's starting the loop.

701
00:38:33,289 --> 00:38:34,389
It has a thought.

702
00:38:34,499 --> 00:38:36,009
It has an observation.

703
00:38:36,619 --> 00:38:37,629
It gets passed in.

704
00:38:38,249 --> 00:38:41,079
And each time we get the result here,
but we can see the summary answers.

705
00:38:42,034 --> 00:38:44,764
But if we look at a particular loop
starting the loop, it has the thought,

706
00:38:44,794 --> 00:38:46,374
I need to find the cost of a TV.

707
00:38:47,104 --> 00:38:48,974
We've extracted out the action.

708
00:38:49,474 --> 00:38:51,834
We've extracted out that the
function call is going to be get

709
00:38:51,834 --> 00:38:53,924
product price and the parameter.

710
00:38:54,424 --> 00:38:56,094
We get an observation of 200.

711
00:38:56,494 --> 00:39:00,264
That now gets VAT back in, and it
now knows it needs to calculate

712
00:39:00,404 --> 00:39:01,574
the total, including the VAT.

713
00:39:01,974 --> 00:39:05,584
It's going to use a calculateTotal
with the 200 that we passed in.

714
00:39:05,954 --> 00:39:08,464
The observation is returned back as 240.

715
00:39:08,854 --> 00:39:11,494
That gets put into the
loop to get the answer.

716
00:39:11,994 --> 00:39:17,054
Now, if you notice, we've missed one
here, and we have the answer here.

717
00:39:17,524 --> 00:39:21,934
And that's the nature sometimes of
LLMs, that things do actually go

718
00:39:21,934 --> 00:39:24,474
wrong because it's probabilistic.

719
00:39:25,034 --> 00:39:27,594
But we get the first one,
the price of the bike is 120.

720
00:39:28,094 --> 00:39:32,224
We do get the result, 240, but we
don't get it printed out in the answer.

721
00:39:32,984 --> 00:39:34,754
And if we were to run this again, come

722
00:39:35,254 --> 00:39:39,534
back in here and open this up,
and if we run it again, we'll

723
00:39:39,574 --> 00:39:40,754
probably see a different answer.

724
00:39:41,254 --> 00:39:46,714
Starting the loop, 120, answer
found, the price of the bike is 120.

725
00:39:47,214 --> 00:39:50,304
Starting again, we found
the price of the TV is 240.

726
00:39:50,804 --> 00:39:54,454
And now we've got the price of the
laptop, 360, and we get all three answers.

727
00:39:55,024 --> 00:39:57,884
So it can show that we can't
take things for granted because

728
00:39:57,884 --> 00:39:59,539
the Things will be missed out.

729
00:39:59,589 --> 00:40:02,269
It is not 100 percent deterministic.

730
00:40:02,589 --> 00:40:06,269
So we need to put in some checks and
balances if we were using these results.

731
00:40:06,759 --> 00:40:10,039
But this is an example of
actually how we can loop through.

732
00:40:10,839 --> 00:40:13,869
So what we're going to do now is
actually go to the notebook version

733
00:40:14,009 --> 00:40:15,769
and see how would we do this manually.

734
00:40:15,799 --> 00:40:16,869
What's actually going on?

735
00:40:17,369 --> 00:40:24,049
So We use the same setup, use the
same system prompt, same functions.

736
00:40:24,689 --> 00:40:28,049
But what we do now is we call an
instance of this planning agent with

737
00:40:28,049 --> 00:40:29,589
the client and the system prompt.

738
00:40:30,089 --> 00:40:33,409
We set up an instance of it and we
pass through the first question.

739
00:40:33,729 --> 00:40:35,789
What is the cost of a
laptop, including the VAT?

740
00:40:36,569 --> 00:40:40,089
When we run that, we get back
the result, which is thought.

741
00:40:40,949 --> 00:40:45,289
That we can now extract out by
splitting on the pipe exactly whether

742
00:40:45,289 --> 00:40:48,549
it's an action, what the function
name is, and what the parameter

743
00:40:48,549 --> 00:40:50,049
name is, what the argument is.

744
00:40:50,509 --> 00:40:53,409
And as you can see, when we do
that, we get the function, we

745
00:40:53,409 --> 00:40:56,259
need to run the getProductPrice,
and we need to do it on a laptop.

746
00:40:56,809 --> 00:40:58,659
So manually, this is what we would do.

747
00:40:59,069 --> 00:41:02,929
We'd say, if the next function is, which
we've just determined here by extracting

748
00:41:02,949 --> 00:41:06,499
out this item here in the output.

749
00:41:07,419 --> 00:41:11,069
If it's get product price, run
get product price with that value.

750
00:41:11,839 --> 00:41:14,889
The argument that we got is
the third, fourth item here.

751
00:41:15,389 --> 00:41:19,769
So we get 300, and we said in our prompt
that we would send it back in in the form

752
00:41:19,769 --> 00:41:21,749
of observation, pipe, and the result.

753
00:41:22,219 --> 00:41:24,529
So this is what we're going
to send as the next prompt.

754
00:41:24,919 --> 00:41:26,449
So we run it with our agent.

755
00:41:26,889 --> 00:41:28,229
Next prompt, the result.

756
00:41:28,599 --> 00:41:32,099
It now determines it still needs
another action, but it's the calculate

757
00:41:32,129 --> 00:41:37,219
total and it needs the price of
300 to be calculated with VAT.

758
00:41:37,809 --> 00:41:41,399
So once again, we strip out
the function and the parameter.

759
00:41:41,399 --> 00:41:44,579
We need to do calculate total
and the argument of 300.

760
00:41:45,079 --> 00:41:46,929
And once again, we run this manually.

761
00:41:46,989 --> 00:41:48,209
You know, it's to calculate total.

762
00:41:48,519 --> 00:41:51,429
And the next arc, there are
different ways that we could

763
00:41:51,429 --> 00:41:53,419
run that using, an eval method.

764
00:41:53,639 --> 00:41:56,869
But for simplicity, we're just going
to run it manually is calculate total.

765
00:41:57,219 --> 00:41:58,739
We get the answer 360.

766
00:41:59,179 --> 00:42:03,279
Once again, we're going to send that back
in, in the form of observation result.

767
00:42:03,389 --> 00:42:05,309
That's what we specified
in the system prompt.

768
00:42:05,809 --> 00:42:08,069
So we're sending this
back in as the next query.

769
00:42:08,249 --> 00:42:09,339
That's the next prompt.

770
00:42:09,919 --> 00:42:13,819
Pass it in to the agent, the
next prompt of observation 360.

771
00:42:14,079 --> 00:42:14,799
Print the result.

772
00:42:15,269 --> 00:42:17,849
Now that it's determined it
has an answer, it prints it

773
00:42:17,849 --> 00:42:19,219
out in the format that we want.

774
00:42:19,799 --> 00:42:19,969
Answer.

775
00:42:19,999 --> 00:42:22,349
The price of the laptop,
including that, is 360.

776
00:42:22,849 --> 00:42:26,329
So if we go back up again and
basically change it, and now let's

777
00:42:26,329 --> 00:42:29,819
work out for the price of the TV
and see exactly the same again.

778
00:42:29,819 --> 00:42:31,479
We're going running through.

779
00:42:31,479 --> 00:42:31,819
We

780
00:42:32,319 --> 00:42:35,259
now know the action is get
product price, but it's for a TV.

781
00:42:35,759 --> 00:42:36,719
It runs the function.

782
00:42:36,719 --> 00:42:38,379
It gets the price that it's 200.

783
00:42:39,049 --> 00:42:42,179
We send it back in and
saying the observation from

784
00:42:42,179 --> 00:42:43,689
the previous action is 200.

785
00:42:44,189 --> 00:42:48,029
It then extracts what it needs, and now
it's starting to calculate the total

786
00:42:48,209 --> 00:42:52,339
with the VAT, 200, extracts again, runs.

787
00:42:52,669 --> 00:42:57,509
We then run the function with the
extracted argument, we get 240.

788
00:42:58,089 --> 00:43:01,449
We now send this observation
back in the form that we said we

789
00:43:01,459 --> 00:43:03,489
would, observation pipe result.

790
00:43:04,004 --> 00:43:09,264
You send this into the next query, goes
into here, you print the result, answer

791
00:43:09,264 --> 00:43:11,414
the price of the TV, including VAT is 240.

792
00:43:11,994 --> 00:43:15,404
Now, there are many ways you can
refactor this and use eval without

793
00:43:15,424 --> 00:43:18,144
actually having to specify in a
step the actual function name.

794
00:43:18,704 --> 00:43:20,944
That's effectively what
we do when we do our loop.

795
00:43:21,294 --> 00:43:26,664
We basically extract all that information
and execute appropriately in our loop.

796
00:43:27,164 --> 00:43:30,814
So I hope this kind of shows that although
we can have many different patterns,

797
00:43:31,314 --> 00:43:36,444
Essentially, it's basically involving
reflection, sending back a previous output

798
00:43:36,604 --> 00:43:43,244
as an input, use of tool call, and also
the use of planning of breaking up a task

799
00:43:43,474 --> 00:43:45,624
into separate steps that it executes.

800
00:43:46,124 --> 00:43:49,144
So they all sort of overlap
and are interconnected.

801
00:43:49,574 --> 00:43:54,604
And I hope that this actually explains how
AI agents work and how if we look at where

802
00:43:54,614 --> 00:44:00,594
is the AI in all of this, literally it is
just in the execute function to the LLM.

803
00:44:01,464 --> 00:44:05,374
The rest of the AI part is just
where we invoke the instance of

804
00:44:05,374 --> 00:44:08,874
the agent with the next prompt
that produces all these results.

805
00:44:09,274 --> 00:44:12,564
The rest is we're just stripping
with day to day Python, sending

806
00:44:12,564 --> 00:44:18,424
it back in to the loop, and then
running again another AI API request.

807
00:44:18,924 --> 00:44:20,994
Well, we've just been
through quite a lot of code.

808
00:44:21,494 --> 00:44:24,114
It's something really to digest offline.

809
00:44:24,614 --> 00:44:28,034
As I've said before, it's taken me quite
a while just to kind of work through

810
00:44:28,034 --> 00:44:31,214
and really get it clear in my own mind
to be able to explain it to people.

811
00:44:31,604 --> 00:44:35,684
So if you don't feel it's kind of
sunk in, that's not a problem at all.

812
00:44:35,754 --> 00:44:37,034
That's probably to be expected.

813
00:44:37,704 --> 00:44:41,044
And that's why I've given you the repo
and all the code examples here so you

814
00:44:41,044 --> 00:44:44,714
can work through them, chop it up,
change them around to get understand it.

815
00:44:45,214 --> 00:44:48,274
So we saw the sort of four main,
we've seen sort of reflection, we've

816
00:44:48,274 --> 00:44:51,604
seen tool use, we've seen a sort
of planning where we're sort of

817
00:44:51,604 --> 00:44:55,464
basically having to break a problem
down into steps and work through them.

818
00:44:56,204 --> 00:44:58,694
We haven't really seen
multi agent collaboration.

819
00:44:59,194 --> 00:45:03,974
And the reason for this is basically
it's a design pattern of how one

820
00:45:03,974 --> 00:45:08,034
wants to use what one gets back
from each agent with another agent.

821
00:45:08,534 --> 00:45:12,554
Is it the active pattern, the
pub sub, finite state machine, as

822
00:45:12,554 --> 00:45:14,524
in, for example, like Landgraf?

823
00:45:15,024 --> 00:45:20,124
And what I'd like to talk about
in closing is the libraries and

824
00:45:20,124 --> 00:45:21,384
frameworks that are available.

825
00:45:21,564 --> 00:45:24,344
And I like to think of libraries
as sort of frameworks without

826
00:45:24,344 --> 00:45:25,334
the framework, as it were.

827
00:45:25,684 --> 00:45:28,794
Lots of convenience, tools,
simplicity, that we can build

828
00:45:28,804 --> 00:45:30,034
the overall design pattern.

829
00:45:30,944 --> 00:45:35,144
And Pydantic AI is a new one that's
come towards the end of 2024.

830
00:45:35,484 --> 00:45:37,984
Pydantic is well known
in the Python community.

831
00:45:38,514 --> 00:45:40,344
And as you can see, structured output.

832
00:45:40,969 --> 00:45:45,329
It's very important because we want to
pass structured output from one agent

833
00:45:45,369 --> 00:45:47,559
or from one part of our app to the next.

834
00:45:48,059 --> 00:45:52,099
HuggingFace SmallAgents is another
very small, lightweight framework

835
00:45:52,809 --> 00:45:57,789
library that they implement so that
one can create one's own overall

836
00:45:57,799 --> 00:45:58,869
framework or design pattern.

837
00:45:59,369 --> 00:46:02,549
Then we see many different
crews and swarms and frameworks.

838
00:46:03,229 --> 00:46:08,149
And these are basically design patterns
for, essentially, intercommunication

839
00:46:08,179 --> 00:46:10,689
between bits of code, our AI agents.

840
00:46:11,299 --> 00:46:14,769
And we saw before in the AI agents
directory, how many different

841
00:46:14,769 --> 00:46:16,649
frameworks there are around there.

842
00:46:17,089 --> 00:46:20,699
As you can see, if we start with the
simplicity as we have been doing, we

843
00:46:20,699 --> 00:46:23,049
might refactor it, build our own library.

844
00:46:23,374 --> 00:46:27,024
And then realize actually other people
could benefit from them, customize

845
00:46:27,024 --> 00:46:28,554
it to make it more easy to use.

846
00:46:28,934 --> 00:46:33,314
And then we add another framework to
the AI agents directory, for example.

847
00:46:33,814 --> 00:46:37,254
So in terms of frameworks, there's
LLAMA Index, LLANG Chain, LLANG Graph,

848
00:46:37,254 --> 00:46:40,454
AutoGen, Crew AI, and then very many more.

849
00:46:40,804 --> 00:46:42,444
All very good, all very useful.

850
00:46:42,794 --> 00:46:46,264
And hopefully, having gone through
the simplicity of these AI agents,

851
00:46:46,904 --> 00:46:50,234
not only could you maybe just design
your own mini framework, But when

852
00:46:50,234 --> 00:46:53,484
you come to use these frameworks, you
will actually understand what it is

853
00:46:53,484 --> 00:46:54,964
they're doing and how they're doing it.

854
00:46:55,464 --> 00:47:00,424
So, in summary, I hope AI agents have
been demystified and helped us understand

855
00:47:00,424 --> 00:47:03,854
what they can do, enabling us either
to build our own frameworks or use

856
00:47:03,854 --> 00:47:07,904
existing ones, with a deeper appreciation
and understanding of how they work.

857
00:47:08,404 --> 00:47:12,294
And Anthropic has, in their
blog, has come up with this,

858
00:47:12,334 --> 00:47:14,894
when and when not to use agents.

859
00:47:15,554 --> 00:47:18,294
And I'll just read it out loud because
it sums it up better than I could.

860
00:47:19,029 --> 00:47:22,559
When building applications with
LLMs, we recommend finding the

861
00:47:22,559 --> 00:47:26,399
simplest solution possible, and only
increasing complexity when needed.

862
00:47:26,899 --> 00:47:29,189
This might mean not building
agentic systems at all.

863
00:47:29,989 --> 00:47:32,899
Agentic systems often trade
latency and cost for better task

864
00:47:32,939 --> 00:47:35,919
performance, and you should consider
when this trade off makes sense.

865
00:47:36,799 --> 00:47:40,629
When more complexity is warranted,
workflows offer predictability and

866
00:47:40,629 --> 00:47:44,909
consistency for well defined tasks,
whereas agents are the better options

867
00:47:44,909 --> 00:47:48,539
when flexibility and model driven
decision making are needed at scale.

868
00:47:49,339 --> 00:47:54,429
For many applications, however, optimizing
single LLM calls with retrieval and

869
00:47:54,429 --> 00:47:56,489
in context examples is usually enough.

870
00:47:56,989 --> 00:48:00,579
Thank you very much for letting me
present this topic on AI agents,

871
00:48:01,139 --> 00:48:03,339
their simplicity and their power.

872
00:48:04,069 --> 00:48:05,109
My name is Craig West.

873
00:48:05,379 --> 00:48:06,199
Thank you very much.

