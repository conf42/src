1
00:00:00,500 --> 00:00:01,100
Hello everyone.

2
00:00:01,150 --> 00:00:03,070
My name is Anna Patricia Revu.

3
00:00:03,130 --> 00:00:04,270
I'm alumni of n it.

4
00:00:04,770 --> 00:00:10,919
I graduated in year 2003 and I
have total 22 years of software

5
00:00:10,919 --> 00:00:12,389
experience in IT industry.

6
00:00:13,349 --> 00:00:17,279
Overall, I have been working on different
technologies all my career, starting

7
00:00:17,279 --> 00:00:21,095
from Java G to middleware technologies,
and now to the cloud applications.

8
00:00:21,885 --> 00:00:27,375
Today we will be, and yeah, thanks
to go 40 two.com for giving me

9
00:00:27,375 --> 00:00:31,385
this opportunity to talk about
self-healing data pipelines, right?

10
00:00:31,985 --> 00:00:35,865
So what is the problem statement
which we are trying to solve?

11
00:00:36,135 --> 00:00:37,875
So what is the growing challenge we have?

12
00:00:38,595 --> 00:00:41,115
Over the period of time earlier,
back in days, we used to

13
00:00:41,115 --> 00:00:42,645
integrate applications, right?

14
00:00:42,645 --> 00:00:44,595
So there was a data transformation data.

15
00:00:45,405 --> 00:00:48,825
Data integration happening different
applications, but of late in

16
00:00:48,825 --> 00:00:53,550
this AI world now we see that,
there is sudden influx of data.

17
00:00:53,890 --> 00:00:55,149
Data is the key now, right?

18
00:00:55,239 --> 00:00:56,949
All stakeholders are looking for data.

19
00:00:57,289 --> 00:00:59,029
They want to build visualizations.

20
00:00:59,029 --> 00:01:00,799
They want to do some sort of analytics.

21
00:01:00,799 --> 00:01:03,949
There are some machine learning
models, which you want to.

22
00:01:04,279 --> 00:01:05,779
Train on those particular data sets.

23
00:01:06,050 --> 00:01:11,570
So there's certain influx of data from
all the corners and data engineers build

24
00:01:11,570 --> 00:01:13,789
data pipelines day in and day out, right?

25
00:01:14,200 --> 00:01:15,760
In, in a very high magnitude.

26
00:01:16,310 --> 00:01:20,230
The problem here is if something
gets broken in the pipeline, then

27
00:01:20,230 --> 00:01:25,190
how do we make sure that, the
pipeline is back working properly.

28
00:01:25,595 --> 00:01:29,675
Because there is a downside of it
because it affects our stakeholders

29
00:01:29,675 --> 00:01:31,955
to take key business decisions.

30
00:01:32,075 --> 00:01:36,534
So to address this we traditionally,
what we have done is we always

31
00:01:36,534 --> 00:01:38,304
used a rule based systems.

32
00:01:38,304 --> 00:01:38,664
Correct.

33
00:01:38,904 --> 00:01:41,124
So we say that, there
is a rule saying that.

34
00:01:41,649 --> 00:01:46,649
If the CPU percentage is this much
higher, then add a new node or, stop

35
00:01:46,649 --> 00:01:48,719
the server or restart the server.

36
00:01:49,210 --> 00:01:51,760
If it a system is hanged,
then restart the server.

37
00:01:52,309 --> 00:01:56,509
If there is a network failure, then
you know, do something else, right?

38
00:01:56,869 --> 00:02:00,289
And throw an alert or send a
email alert to the users, right?

39
00:02:00,319 --> 00:02:04,259
So this is what a traditional
rule-based system does and it

40
00:02:04,259 --> 00:02:05,819
is basically static in nature.

41
00:02:06,240 --> 00:02:08,940
Main critical pain points, what
we have seen over the period of

42
00:02:08,940 --> 00:02:11,009
time is the schema drift, right?

43
00:02:11,729 --> 00:02:17,149
And in a typical data pipeline, obviously,
just to oversimplify it, you have a

44
00:02:17,149 --> 00:02:19,969
system source and a system target.

45
00:02:20,179 --> 00:02:23,249
You want to, get the data from
source to the target system.

46
00:02:23,789 --> 00:02:28,469
Now you built your data pipelines
on particular schema, and then

47
00:02:28,469 --> 00:02:29,909
suddenly over the period of time.

48
00:02:30,464 --> 00:02:33,224
The schema has changed
for some reason, right?

49
00:02:33,494 --> 00:02:37,274
Obviously, your pipeline, your ETL
transformations, whatever you've written,

50
00:02:37,544 --> 00:02:42,544
might break because of the schema drift,
and suddenly the data pipeline is broken,

51
00:02:42,904 --> 00:02:46,774
and then you get an alert, and then
you go and fix, try to fix it, and then

52
00:02:46,774 --> 00:02:50,794
rewrite the whole, rewrite that whole
transformation with the new schema.

53
00:02:51,154 --> 00:02:52,954
This is called the schema drift, right?

54
00:02:53,194 --> 00:02:54,304
So this are the pain points.

55
00:02:54,304 --> 00:02:57,874
So basically it, again, brings
down your system and then there

56
00:02:57,874 --> 00:02:59,934
is a loss of business hours there.

57
00:03:00,434 --> 00:03:02,474
Then another problem
is resource contention.

58
00:03:02,634 --> 00:03:04,564
Different in entire pipeline.

59
00:03:04,564 --> 00:03:09,864
You might have your airflow ds working
in the background, glue jobs, working

60
00:03:09,864 --> 00:03:13,644
in the background or different virtual
machines working in the background.

61
00:03:14,144 --> 00:03:19,004
To process the data and due to sudden
influx of data, the resource is not

62
00:03:19,004 --> 00:03:20,804
capable to handle that data, right?

63
00:03:21,464 --> 00:03:22,664
And the system goes down.

64
00:03:23,054 --> 00:03:27,104
So that can be one of the pain points,
and that happens quite frequently, right?

65
00:03:27,434 --> 00:03:32,824
Even sometimes your data warehouse tools
may not be able to handle the queries.

66
00:03:32,834 --> 00:03:34,184
The ETL transformations.

67
00:03:34,529 --> 00:03:39,749
Inserts and insertion data into the tables
or, data visualization queries coming

68
00:03:40,019 --> 00:03:41,889
and, hitting to the database server.

69
00:03:42,399 --> 00:03:44,259
There can be so many issues there.

70
00:03:44,309 --> 00:03:47,759
And then there are system
anomalies, obviously, right?

71
00:03:47,929 --> 00:03:50,869
There are unseen errors, there
is network failures happening.

72
00:03:51,349 --> 00:03:56,149
So these are the typical critical
pain points of a typical data

73
00:03:56,179 --> 00:03:58,069
pipeline set up, as in today.

74
00:03:58,639 --> 00:04:01,669
The limitations we already
have discussed, right?

75
00:04:01,669 --> 00:04:04,369
The limitations of current
temperatures is it is very rigid.

76
00:04:04,759 --> 00:04:09,889
You define a rule, you say if the
CPU percentage is this much higher,

77
00:04:10,159 --> 00:04:12,979
then you do something to fix it.

78
00:04:13,219 --> 00:04:13,579
Correct?

79
00:04:13,969 --> 00:04:19,519
And you if there is certain
deflection, it, there is no rule

80
00:04:19,549 --> 00:04:22,519
for that, and then you are not able
to solve that particular problem.

81
00:04:23,089 --> 00:04:25,909
Another problem is unable to adopt
a new failure patterns, right?

82
00:04:25,959 --> 00:04:30,619
There, there will be constantly new, new
failures happening and you might not have

83
00:04:30,619 --> 00:04:33,319
written rules to take care of that, right?

84
00:04:33,819 --> 00:04:37,449
Obviously, high maintenance, because there
is a manual intervention required, there

85
00:04:37,449 --> 00:04:42,859
is a downtime required and to do the rule
updates and obviously poor performance

86
00:04:42,859 --> 00:04:44,659
in some complex scenarios, right?

87
00:04:45,349 --> 00:04:48,889
We enterprise scale data
operations demand more intelligent

88
00:04:48,889 --> 00:04:50,599
and adapt solutions, right?

89
00:04:51,019 --> 00:04:54,289
And that's why there's a
new paradigm shift, which is

90
00:04:54,289 --> 00:04:55,699
like autonomous self-healing.

91
00:04:55,999 --> 00:05:01,579
So what if the pipelines self
heal, self-heal themselves, right?

92
00:05:02,159 --> 00:05:04,659
That's a thought which
which we should think on.

93
00:05:05,169 --> 00:05:11,769
So what the paradigm shift is moving
from reactive to proactive, right?

94
00:05:12,009 --> 00:05:14,839
And basically a policy driven framework.

95
00:05:15,269 --> 00:05:17,354
And then through intelligent
prompt engineering.

96
00:05:18,044 --> 00:05:21,254
We can actually implement this
autonomous self-healing, right?

97
00:05:21,644 --> 00:05:24,104
So there are two, three
concepts which you may use here.

98
00:05:24,134 --> 00:05:28,604
One is definitely called as
a reinforcement learning.

99
00:05:29,024 --> 00:05:32,544
You can use machine learning
for, checking anomalies.

100
00:05:33,034 --> 00:05:37,739
You can have reinforcement learning
and you have these agents which can

101
00:05:37,739 --> 00:05:41,779
actually think of it like you are, twins.

102
00:05:42,479 --> 00:05:46,319
Who can actually go there and then fix
the problems automatically for you.

103
00:05:47,049 --> 00:05:49,179
So that is what this
RL agent is all about.

104
00:05:49,749 --> 00:05:51,699
And then you can do prompt engineering.

105
00:05:51,749 --> 00:05:56,419
You can get this data, vectorize
it, and then you can use

106
00:05:56,419 --> 00:05:59,089
any agents, agent ai right?

107
00:05:59,569 --> 00:06:05,489
And then, develop some flows and then
query the database in the backend.

108
00:06:06,389 --> 00:06:10,959
And it cannot, obviously, it'll give
you the information of what actually

109
00:06:10,959 --> 00:06:12,609
went wrong into the systems, right?

110
00:06:13,239 --> 00:06:16,539
So how these RL agents learn
this resilience, right?

111
00:06:17,109 --> 00:06:21,849
So basically this RL agents or
reinforcement learning agents, they

112
00:06:21,849 --> 00:06:23,799
observe this pipeline telemetry data.

113
00:06:23,799 --> 00:06:29,169
So what we do is we gather the data of
all the systems in that entire pipeline.

114
00:06:29,409 --> 00:06:34,174
So the pipeline can be your, orchestration
tools let's say I'm just taking

115
00:06:34,174 --> 00:06:36,624
example of Apache Airflow, right?

116
00:06:36,834 --> 00:06:42,554
Or a glue job in AWS or a Lambda
function in AWS or any, virtual

117
00:06:42,554 --> 00:06:45,134
machine, how it is performing, right?

118
00:06:45,374 --> 00:06:49,484
So those telemetry data you
can, and then you have your data

119
00:06:49,604 --> 00:06:51,674
leaks and your data warehouses.

120
00:06:52,184 --> 00:06:55,604
So whatever systems are there
into your entire pipeline.

121
00:06:56,099 --> 00:06:59,334
You can actually collect them,
collect the metrics data, the

122
00:06:59,334 --> 00:07:00,669
tele telemetry data, right?

123
00:07:00,909 --> 00:07:03,669
So what is the resource utilization?

124
00:07:03,699 --> 00:07:07,019
What is the, error reads and
what is their system health?

125
00:07:07,049 --> 00:07:11,174
You can capture all this information
and repeatedly through trial and error.

126
00:07:12,134 --> 00:07:17,944
Try to discover the best possible
way to handle that error scenario

127
00:07:17,974 --> 00:07:19,084
or that failure scenario.

128
00:07:19,504 --> 00:07:23,514
This is what a typical
RR agent would do, right?

129
00:07:23,724 --> 00:07:27,894
The agent basically learns dynamically
at just the resource allocation.

130
00:07:28,464 --> 00:07:32,424
The it can reconfigure the ETL
workflows, apply corrective actions.

131
00:07:32,424 --> 00:07:35,955
If it's, if it has to it can do
schema mapping in case of schema.

132
00:07:36,455 --> 00:07:40,745
It can do some intelligent retries
like exponential backup, something

133
00:07:40,745 --> 00:07:45,635
like that, and then put some adaptive
back pressure before failures cascade.

134
00:07:45,845 --> 00:07:49,275
So this is how this typical
RL agent would would work.

135
00:07:50,145 --> 00:07:55,354
The framework architecture for this
is, definitely important thing is

136
00:07:55,744 --> 00:07:59,465
you need to collect the telemetry
data of your system, right?

137
00:07:59,705 --> 00:08:03,155
So historically, whatever telemetry
data is there, you can collect.

138
00:08:03,465 --> 00:08:06,525
In AWS, if you talk, you will
have your CloudWatch metrics

139
00:08:06,525 --> 00:08:10,565
and all that stuff, and then can
continuously stream these metrics.

140
00:08:10,625 --> 00:08:15,095
And then using simulative fall data,
you can develop a comprehensive learning

141
00:08:15,305 --> 00:08:17,165
on top of this particular data, right?

142
00:08:17,435 --> 00:08:21,995
And then once you get this particular data
set, then you can do your reinforcement

143
00:08:21,995 --> 00:08:23,585
learning agent training, right?

144
00:08:23,585 --> 00:08:24,425
Continuously.

145
00:08:24,725 --> 00:08:26,945
These agents will be
looking at the metrics data.

146
00:08:27,320 --> 00:08:31,290
And, it'll identify the patterns,
it'll identify the failure scenarios

147
00:08:31,290 --> 00:08:34,570
and it'll try to fix it through
autonomous response, right?

148
00:08:34,570 --> 00:08:35,340
Then real time.

149
00:08:35,650 --> 00:08:39,970
These agents can take a corrective actions
including resource allocation, right?

150
00:08:40,070 --> 00:08:45,600
Spin of another node or drop one node,
do some sort of workflow adjustments.

151
00:08:45,600 --> 00:08:46,740
It can do all that stuff.

152
00:08:46,740 --> 00:08:49,890
So this is a typical
framework architecture.

153
00:08:50,470 --> 00:08:54,290
We already talked about just now about
the prompt engineering for intelligent,

154
00:08:54,290 --> 00:08:55,970
prompt engineering for pipelines, right?

155
00:08:56,930 --> 00:09:02,200
First thing first is you can actually
go and then put a prompt onto the system

156
00:09:02,200 --> 00:09:04,160
saying anomaly detection prompts, right?

157
00:09:04,220 --> 00:09:10,600
Using your natural language query, you can
say, Hey tell me like from past 24 hours

158
00:09:10,730 --> 00:09:14,210
what is the CPU utilization of my system?

159
00:09:14,720 --> 00:09:20,910
Or how many error this airflow DAG has
been throwing from past couple of hours.

160
00:09:21,150 --> 00:09:24,970
And then it can it can give you
an anomaly based upon that, right?

161
00:09:24,970 --> 00:09:27,810
So you don't need to go to a
dashboard and, click on it.

162
00:09:27,870 --> 00:09:31,290
You can just do some intelligent prompt
engineering, and then you can get

163
00:09:31,320 --> 00:09:35,390
that information at your fingertips
after you get that information.

164
00:09:35,420 --> 00:09:36,415
And then you can also ask.

165
00:09:37,210 --> 00:09:40,340
Hey, okay these many
machines have been failed.

166
00:09:40,700 --> 00:09:42,530
Can you tell me what is the root cause?

167
00:09:42,590 --> 00:09:48,230
And that prompt can actually trigger
some agent, which can actually try

168
00:09:48,230 --> 00:09:52,880
to figure out, it would go and read
the logs that traces and metrics

169
00:09:53,150 --> 00:09:56,000
and pinpoint exactly what happened.

170
00:09:56,310 --> 00:10:01,170
With respect to the co and it'll give
you contextual explanations on based upon

171
00:10:01,600 --> 00:10:03,520
your anomaly detection prompts, right?

172
00:10:04,300 --> 00:10:07,640
And then you can also say, okay,
you can instruct it because

173
00:10:07,700 --> 00:10:09,410
now you know what is a problem.

174
00:10:09,770 --> 00:10:12,580
And then being human, you
have that intelligence power.

175
00:10:12,970 --> 00:10:15,040
You can just say, okay, go.

176
00:10:15,040 --> 00:10:19,260
And, you go and spin off another
instance EC2 instance, for example,

177
00:10:19,320 --> 00:10:22,960
or spin off another node or spin
off another worker node in Redshift

178
00:10:22,960 --> 00:10:23,890
or something like that, right?

179
00:10:24,400 --> 00:10:29,450
And the agent can go back and
actually do that task for you.

180
00:10:29,570 --> 00:10:33,745
So definitely this prompt
engineering is an added advantage.

181
00:10:33,965 --> 00:10:35,420
Now in this today's AI world.

182
00:10:35,420 --> 00:10:39,570
We can take use of
marvelous, amazing LLMs.

183
00:10:40,380 --> 00:10:45,720
And we can put LLM, we can, vectorize
the data in the database and you

184
00:10:45,720 --> 00:10:50,090
can build your quick agent flows
on top of it and you can actually

185
00:10:50,090 --> 00:10:51,650
automate this entire process.

186
00:10:51,750 --> 00:10:52,470
And then.

187
00:10:52,570 --> 00:10:52,870
Yeah.

188
00:10:53,170 --> 00:10:57,280
A typical process flow of a data
pipeline would look like this, right?

189
00:10:57,320 --> 00:10:59,570
I'm just taking example
of Oracle Database.

190
00:10:59,630 --> 00:11:01,670
Think of this as an
external source, right?

191
00:11:02,270 --> 00:11:05,830
This is not part of the
the part of your pipeline.

192
00:11:06,250 --> 00:11:10,420
And then you have DMS data
management service in a ws, right?

193
00:11:10,750 --> 00:11:14,050
It constantly pulls the
data from the database.

194
00:11:14,150 --> 00:11:19,070
Using CDC technology and then it
has the schema information and

195
00:11:19,070 --> 00:11:21,380
then the airflow deck kicks in.

196
00:11:21,460 --> 00:11:23,805
And then it basically does the ETL.

197
00:11:24,425 --> 00:11:27,455
The pre-processing and post processing
of that particular data set.

198
00:11:27,485 --> 00:11:31,265
You can have blue jobs as well, and
then you put monitoring layer on

199
00:11:31,265 --> 00:11:35,805
top of all these all these nodes
here in a typical data pipeline.

200
00:11:36,255 --> 00:11:39,940
After this, it'll be going to the
Redshift and, data lakes or data

201
00:11:40,185 --> 00:11:43,305
warehouses I'm just using, it can
be Snowflake, anything, right?

202
00:11:43,805 --> 00:11:47,045
And then typically there will
be monitoring layer on top of

203
00:11:47,045 --> 00:11:50,345
all these applications wherein
it'll be collecting the data.

204
00:11:50,765 --> 00:11:55,385
And, then there will be an anomaly
detection layer, which is you can use

205
00:11:55,385 --> 00:11:59,875
a typical machine learning algorithm,
like isolation forest algorithm to

206
00:12:00,095 --> 00:12:04,685
do an anomaly detection continuously
on the data or the monitoring data.

207
00:12:05,405 --> 00:12:10,655
And if you find any problem,
then automatically RL agents will

208
00:12:10,685 --> 00:12:14,585
kick in and do the self feeling
actions like restarting the server.

209
00:12:14,930 --> 00:12:17,690
And then giving the
feedback, like what exactly?

210
00:12:18,190 --> 00:12:19,990
Once that our religion does it work?

211
00:12:20,630 --> 00:12:20,960
It can.

212
00:12:21,460 --> 00:12:25,230
Give a feedback that, oh, the system has
been started and it has been working fine.

213
00:12:25,230 --> 00:12:26,610
Our system is not working fine.

214
00:12:26,610 --> 00:12:28,140
Still it does having some trouble.

215
00:12:28,470 --> 00:12:31,410
So this is a continuous loop,
which it'll go into, right?

216
00:12:31,410 --> 00:12:33,510
So this is the call a
feedback loop, right?

217
00:12:33,990 --> 00:12:38,930
So this is a typical flow of RL agent
based self-healing data pipelines.

218
00:12:39,590 --> 00:12:42,430
So what are the layers, which
will be used to implement this?

219
00:12:43,090 --> 00:12:48,220
One thing is the monitoring layer, like as
I said, continuously track the pipeline,

220
00:12:48,220 --> 00:12:53,290
health resource usage, data quality
across DMS, entire monitoring layer.

221
00:12:53,320 --> 00:12:56,810
I'm just taking an example just
for you to understand what are the

222
00:12:56,810 --> 00:13:00,885
different components in data pipeline
In AWS setup, it for Azure and for

223
00:13:01,275 --> 00:13:05,130
GCP or for your proprietary data
pipelines, it can be different.

224
00:13:05,630 --> 00:13:07,610
And then you have this analysis layer.

225
00:13:07,700 --> 00:13:10,070
You can use machine learning algorithms.

226
00:13:10,070 --> 00:13:11,180
As I told isolation.

227
00:13:11,180 --> 00:13:12,620
Forest algorithms really great.

228
00:13:12,650 --> 00:13:17,350
It can actually figure out
what exactly that anomaly is.

229
00:13:17,470 --> 00:13:21,400
What is a resource bottlenecks, what
are the data inconsist inconsistencies,

230
00:13:21,730 --> 00:13:23,110
and it can trigger an alarm.

231
00:13:23,610 --> 00:13:27,450
When alarm is triggered, there is another
layer, which will be that RN agent layer.

232
00:13:27,960 --> 00:13:32,220
Which will evaluate the pipeline, state
and select optimal recovery actions.

233
00:13:32,220 --> 00:13:37,530
Example is rescheduling the jobs or
scaling the glue resources or remapping

234
00:13:37,530 --> 00:13:40,020
the schemas or retrying the DMS loads.

235
00:13:40,520 --> 00:13:44,480
And then you have your execution
there wherein automate automation

236
00:13:44,480 --> 00:13:48,170
of engines implement recovery
actions across AWS services.

237
00:13:48,220 --> 00:13:52,570
So it'll be executing those distance what
your RL agent has been taking, right?

238
00:13:52,840 --> 00:13:55,290
Minimizing the downtime or
manual intervention, right?

239
00:13:55,320 --> 00:13:56,010
Be required.

240
00:13:56,040 --> 00:13:57,840
If not, it's not fixing.

241
00:13:58,590 --> 00:14:00,870
Then there's a feedback
layer, then aggregates it.

242
00:14:00,870 --> 00:14:03,760
Basically, e executes the action.

243
00:14:03,760 --> 00:14:04,970
And then it also sees for.

244
00:14:05,515 --> 00:14:09,295
Whether that action has been
properly implemented and it is

245
00:14:09,295 --> 00:14:10,765
giving the desire results or not.

246
00:14:11,275 --> 00:14:15,175
And this is for our agents for
its continuous improvement, right?

247
00:14:15,675 --> 00:14:20,445
So we talked about this
whole, reinforcement learning

248
00:14:20,695 --> 00:14:22,225
machine learning and all.

249
00:14:22,225 --> 00:14:24,895
I just wanted to take an opportunity
for the people who doesn't

250
00:14:24,955 --> 00:14:28,645
understand reinforcement learning
and probably they understand machine

251
00:14:28,645 --> 00:14:30,505
learning and AI and all that stuff.

252
00:14:30,985 --> 00:14:34,225
It's kind of machine learning,
but little bit different in the

253
00:14:34,225 --> 00:14:37,125
sense like, in machine learning
you have independent variables.

254
00:14:37,125 --> 00:14:41,235
You have X and X variables, like
you know all the attributes.

255
00:14:41,505 --> 00:14:44,625
And then you have a dependent variable,
which is why, which is a target variable.

256
00:14:45,015 --> 00:14:49,605
And your job is to predict what
the target variable will be, right?

257
00:14:49,965 --> 00:14:55,875
But in reinforcement learning that X,
which we're talking about the independent

258
00:14:55,875 --> 00:14:58,225
variables think of it as a state.

259
00:14:58,645 --> 00:15:01,755
Right that entire row, you
can think of it as a state.

260
00:15:02,255 --> 00:15:02,585
Sorry.

261
00:15:03,085 --> 00:15:06,535
And that state consists
of this whole metrics.

262
00:15:06,565 --> 00:15:13,045
Like for example, what, like for example,
my, it'll say schema mismatch, right?

263
00:15:13,485 --> 00:15:17,235
What action can be taken to
solve that particular problem?

264
00:15:17,745 --> 00:15:19,355
Or what is the metrics there?

265
00:15:19,535 --> 00:15:21,155
Is there a network latency.

266
00:15:21,250 --> 00:15:28,085
Is it a, the mismatch of the mismatch of
the attributes those kind of, data sets?

267
00:15:28,135 --> 00:15:32,675
There will be there like it can be, the
columns can be error type, what stage

268
00:15:32,675 --> 00:15:35,135
it is, what is the latency, right?

269
00:15:35,475 --> 00:15:37,925
These can be it's attributes state at.

270
00:15:38,425 --> 00:15:40,495
And then there will be one more.

271
00:15:40,595 --> 00:15:44,165
You can think of this as state temple,
state action, reward, and next state

272
00:15:44,255 --> 00:15:46,355
you can think of as a tap, right?

273
00:15:46,625 --> 00:15:47,255
In Python.

274
00:15:47,735 --> 00:15:50,795
So action is like on this
particular state, right?

275
00:15:50,825 --> 00:15:54,675
The state is like there's a timeout
in the transformation, right?

276
00:15:55,005 --> 00:15:56,715
So the system is down.

277
00:15:57,150 --> 00:15:59,370
So what action would we like to do?

278
00:15:59,460 --> 00:16:00,600
Probably want to read, right?

279
00:16:00,930 --> 00:16:04,590
So that is action, which you'll be taking
example, restarting that task, right?

280
00:16:05,250 --> 00:16:11,580
And then once the state you saw, you took
an action, then there is a reward system.

281
00:16:11,790 --> 00:16:16,160
The reward system is basically
how good a job you did, right?

282
00:16:16,550 --> 00:16:21,650
And think of that your professor is
giving you extra marks for actually

283
00:16:21,650 --> 00:16:24,650
doing a good job and is punishing
you for not doing any good job.

284
00:16:24,700 --> 00:16:29,440
So this is very critical, and I'll explain
you in a bit why reward is very important.

285
00:16:29,770 --> 00:16:31,240
And then there is a next state.

286
00:16:31,270 --> 00:16:36,340
So once you take a state, you took an
action and you get a reward, obviously

287
00:16:36,370 --> 00:16:37,870
it'll transition to a next state.

288
00:16:37,870 --> 00:16:40,235
So this whole, quadruple double.

289
00:16:40,890 --> 00:16:44,110
Is the key of reinforcement learning.

290
00:16:44,140 --> 00:16:47,150
The, this is the data structure
which it'll be using, right?

291
00:16:47,300 --> 00:16:49,190
I just give an example of the data, right?

292
00:16:49,280 --> 00:16:53,750
So the state can be having error type
action, taken stage, and latency, right?

293
00:16:53,960 --> 00:16:57,500
So this, these are the attributes
which will be going into the

294
00:16:57,560 --> 00:17:02,090
reinforcement learning models,
and then you specify the rewards.

295
00:17:02,420 --> 00:17:07,110
If you say that, if the next state is
healthy and you do did this, so you

296
00:17:07,110 --> 00:17:11,580
say, plus one, if the next state is
not healthy, you probably punish it.

297
00:17:11,640 --> 00:17:15,550
Or yeah you basically will say that,
you are not done a good job and

298
00:17:15,550 --> 00:17:17,050
probably give a minus one to it.

299
00:17:17,300 --> 00:17:20,390
Same way there's a time where
transformation is the action

300
00:17:20,390 --> 00:17:22,670
taken, and latency is two seconds.

301
00:17:23,120 --> 00:17:24,890
And action taken is retried.

302
00:17:25,310 --> 00:17:26,930
The reward is zero, right?

303
00:17:26,960 --> 00:17:29,990
You are not doing it again because
of timeout it time out again.

304
00:17:30,350 --> 00:17:35,270
So you are basically punishing to
the action you have taken, right?

305
00:17:35,990 --> 00:17:39,310
This is, these values are
nothing but called a Q values.

306
00:17:39,310 --> 00:17:43,000
And this is what is drives
everything in reinforcement learning.

307
00:17:43,000 --> 00:17:45,010
It basically looks at that skew values.

308
00:17:45,280 --> 00:17:47,140
It means maintains executable.

309
00:17:47,515 --> 00:17:51,745
And for each and every action, it
actually looks at the particular Q

310
00:17:51,745 --> 00:17:53,935
table and applies the rewards, right?

311
00:17:54,655 --> 00:17:56,095
And it's a continuous process.

312
00:17:56,605 --> 00:18:01,605
The goal of any reinforcement learning
program, or the agent is given a

313
00:18:01,605 --> 00:18:05,385
state, what action should I take
to maximize my long term reward?

314
00:18:05,985 --> 00:18:11,505
So the more the rewards, the more the
reinforcement learning agents understand

315
00:18:11,505 --> 00:18:14,135
that, if I do this way, I'll be rewarded.

316
00:18:14,415 --> 00:18:18,285
Rewarded and then this is the right
way to solve particular problem.

317
00:18:18,465 --> 00:18:21,065
Right now, what are the key capabilities?

318
00:18:21,125 --> 00:18:22,595
It's like real time monitoring.

319
00:18:22,595 --> 00:18:24,845
Continuous pipeline
performance will be tracked.

320
00:18:25,355 --> 00:18:28,085
Your dynamic resource
management can be done right.

321
00:18:28,085 --> 00:18:31,985
Intelligently can reallocate computational
resources based on current demand.

322
00:18:32,695 --> 00:18:35,455
ETL workflow optimizations
can be done right.

323
00:18:36,105 --> 00:18:40,395
If the ETL take is taking a lot
of time or it is not working up

324
00:18:40,395 --> 00:18:45,715
to the mark, then probably your
ETL processes can be adjusted.

325
00:18:46,405 --> 00:18:49,495
It can even do schema
remapping, how schema remapping.

326
00:18:49,525 --> 00:18:55,795
It can crawl over the, it can
crawl over the source schema and

327
00:18:55,795 --> 00:18:57,055
figure out what the schema is.

328
00:18:57,055 --> 00:19:02,205
Then applied those using, those,
those schemas back to your ETL

329
00:19:02,415 --> 00:19:04,935
using some LMS in the background.

330
00:19:04,935 --> 00:19:09,255
And then it does automatically schema
deployment and automatically, sorry, it

331
00:19:09,255 --> 00:19:13,905
automatically remaps the schema and then
through CICD it automatically deploys

332
00:19:13,905 --> 00:19:15,495
that particular solution into the server.

333
00:19:15,495 --> 00:19:16,605
So how cool It's right.

334
00:19:17,175 --> 00:19:21,255
So those, these are the things which
agent will be doing once it figures

335
00:19:21,255 --> 00:19:24,865
out that this is a problem statement,
advanced response mechanisms.

336
00:19:24,865 --> 00:19:28,045
Also, they're like targeted, retry,
exponential, back off, right?

337
00:19:28,045 --> 00:19:29,575
So what is exponential back off?

338
00:19:30,035 --> 00:19:32,585
You want to retry, but you
are continuously retrying.

339
00:19:32,645 --> 00:19:36,695
After every 30 seconds, you're not doing
good because you're blocking the whole,

340
00:19:36,965 --> 00:19:39,315
you're choking the whole pipeline because.

341
00:19:40,095 --> 00:19:44,145
Obviously the target system is not
up and you're still making a call

342
00:19:44,445 --> 00:19:46,155
and you're bombarding the request.

343
00:19:46,155 --> 00:19:47,955
So you're joking with your request, right?

344
00:19:48,735 --> 00:19:52,845
So ideal thing is you do have
exponential back of right, 30 seconds.

345
00:19:52,845 --> 00:19:54,195
Now you did maybe after two.

346
00:19:54,255 --> 00:19:56,835
After two minutes you try,
maybe after 10 minutes you try.

347
00:19:57,195 --> 00:20:01,185
So this way you are not choking
that entire network, right?

348
00:20:01,275 --> 00:20:02,794
With your a request to the server.

349
00:20:03,375 --> 00:20:07,004
Dynamic flow control can be also done to
prevent system overload, which is like

350
00:20:07,004 --> 00:20:10,834
a throttling of your data ingestions,
which is called adaptive back pressure.

351
00:20:11,435 --> 00:20:14,955
You can do resource reallocation,
a automatically like automatic

352
00:20:14,955 --> 00:20:18,345
scaling, and then you can do
some workflow adjustments, right?

353
00:20:18,465 --> 00:20:20,975
Like a deployment you can.

354
00:20:21,499 --> 00:20:24,619
Use Kubernetes based or, ECS.

355
00:20:24,679 --> 00:20:25,999
It's your wish.

356
00:20:26,079 --> 00:20:30,409
And you can categorize this particular
application and then you can deploy

357
00:20:30,409 --> 00:20:32,360
it at any cloud based application.

358
00:20:32,360 --> 00:20:33,699
Not a big problem there.

359
00:20:34,169 --> 00:20:34,799
Evaluation.

360
00:20:34,799 --> 00:20:36,839
How do we evaluate your
methodology, right?

361
00:20:36,839 --> 00:20:40,309
Testing conditions, the best thing
is, first of all, you need to figure

362
00:20:40,309 --> 00:20:44,185
out how to get the data, gather
the data, and simulate that whole.

363
00:20:44,894 --> 00:20:47,594
Error scenarios, good
scenarios, bad scenarios.

364
00:20:47,774 --> 00:20:51,914
Probably you should create another
environment and with simulated

365
00:20:51,914 --> 00:20:57,644
data, you can, you, you can create
these situations and then evaluate

366
00:20:57,914 --> 00:21:02,374
whether you are RL agents are
working appropriately or not, right?

367
00:21:02,884 --> 00:21:03,124
Yeah.

368
00:21:03,624 --> 00:21:07,284
These are the key metrics like we
have seen over the period of time.

369
00:21:07,284 --> 00:21:11,184
By doing this, use response
period three x, your service

370
00:21:11,184 --> 00:21:14,154
level objectives are like 94%.

371
00:21:14,334 --> 00:21:21,014
It's improved, and meantime to recovery is
like drastically reduced system benefits.

372
00:21:21,014 --> 00:21:21,764
Obviously.

373
00:21:21,764 --> 00:21:25,314
It is we are moving from reactive to
proactive data pipeline management

374
00:21:25,314 --> 00:21:27,384
with the learning optimized strategies.

375
00:21:27,894 --> 00:21:31,494
Autonomous self-management,
less human intervention and

376
00:21:31,494 --> 00:21:33,024
enterprise scale capability, right?

377
00:21:33,024 --> 00:21:37,564
It can scale to any sort of magnitude
of data pipelines you have set up.

378
00:21:38,064 --> 00:21:39,744
It is continuous policy volution.

379
00:21:39,744 --> 00:21:43,554
At the end of the day, RL agents
come up with a policy, right?

380
00:21:43,644 --> 00:21:45,444
And it never stops.

381
00:21:45,924 --> 00:21:49,614
If a new pattern comes up again,
it reads it and it tries to

382
00:21:49,614 --> 00:21:50,784
solve that particular problem.

383
00:21:50,804 --> 00:21:52,754
Th this never stops.

384
00:21:52,754 --> 00:21:55,994
So it has this continuous
policy evaluation strategy.

385
00:21:56,774 --> 00:22:00,164
If you want to build yourself failing
pipeline, what need to do, right?

386
00:22:00,224 --> 00:22:04,724
Like basically deploy comprehensive
observability across your pipeline.

387
00:22:05,214 --> 00:22:10,914
Stack tric logs, traces which should
be used to, feed that RL agent.

388
00:22:11,304 --> 00:22:13,884
You can create a simulation
environment, right?

389
00:22:14,574 --> 00:22:17,994
Build a framework that mirrors
production, as I told you, and then.

390
00:22:18,324 --> 00:22:21,344
Define recovery actions like
you need to know that if this

391
00:22:21,344 --> 00:22:23,054
is a problem, this can happen.

392
00:22:23,054 --> 00:22:24,584
So you need to have your actions.

393
00:22:24,584 --> 00:22:25,904
What actions should be taken?

394
00:22:25,904 --> 00:22:29,894
Because again, this has to be
put into that reward system.

395
00:22:29,894 --> 00:22:31,904
Remember, that's why, right?

396
00:22:32,174 --> 00:22:36,464
And deploy andrate slowly
go, don't go big bang, right?

397
00:22:36,524 --> 00:22:41,984
Probably you just try to solve one problem
at a time and then add one more problem,

398
00:22:42,134 --> 00:22:46,274
and then add one more problem now rather
than going with a big bank approach.

399
00:22:46,619 --> 00:22:49,289
Probably you're just looking for
ski mantra for the time being, or

400
00:22:49,289 --> 00:22:53,069
probably you're looking for resource
contention and then you can add for

401
00:22:53,069 --> 00:22:54,869
network latency and stuff like that.

402
00:22:54,919 --> 00:22:58,279
So what is the future intelligent
and adaptive integration?

403
00:22:58,309 --> 00:23:01,699
So surfing pipelines represent
a fundamental shift in how we

404
00:23:01,699 --> 00:23:03,049
architect data infrastructure.

405
00:23:03,739 --> 00:23:08,539
By combining RL with prompt driven
intelligence, we create systems

406
00:23:08,569 --> 00:23:10,849
that don't just fall gracefully.

407
00:23:11,329 --> 00:23:12,444
They learn, adapt, and evolve.

408
00:23:13,044 --> 00:23:16,764
So basically we are shifting
from reactive troubleshooting to

409
00:23:16,764 --> 00:23:19,044
proactive policy driven resolution.

410
00:23:19,854 --> 00:23:20,544
And yeah.

411
00:23:20,664 --> 00:23:21,654
Thank you so much.

412
00:23:21,704 --> 00:23:29,094
Thanks for listening to my thanks for
listening to the presentation and I'm you

413
00:23:29,094 --> 00:23:33,264
can reach out to me on LinkedIn if you
have any questions or anything about it.

414
00:23:33,624 --> 00:23:34,494
Thank you so much.

