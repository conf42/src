1
00:00:02,520 --> 00:00:04,260
Hi, welcome.

2
00:00:05,310 --> 00:00:07,700
Today I'm here to present.

3
00:00:07,750 --> 00:00:12,260
Our Yay is Transforming enterprise
Data Pipelines with performance

4
00:00:12,570 --> 00:00:17,430
efficiency, up to 70 to 85%.

5
00:00:18,180 --> 00:00:22,500
AI innovation, the data
engineering workflow.

6
00:00:23,500 --> 00:00:28,670
In the context of context today world
motivational intelligence is having

7
00:00:28,670 --> 00:00:34,540
a major impact on enterprise data
pipelines and the data engineering

8
00:00:34,540 --> 00:00:40,795
transformations are happening across the
entire data lifecycle from processing.

9
00:00:41,795 --> 00:00:48,915
Myself, I'm with 15 years of experience
in data engineering with a focus on

10
00:00:48,965 --> 00:00:51,125
cloud technologies and automation.

11
00:00:51,815 --> 00:00:57,005
I consistently pioneered innovative
solutions so that bridge technology gaps

12
00:00:57,005 --> 00:01:00,105
and deliver measurable business impact.

13
00:01:01,965 --> 00:01:08,275
Also implementation of data
solutions that changed how

14
00:01:10,975 --> 00:01:12,445
large scale data.

15
00:01:14,590 --> 00:01:18,100
So back to the topic of the
enterprise data pipelines.

16
00:01:18,850 --> 00:01:24,510
Yeah, it can detect the data patterns and
automate the connections between different

17
00:01:24,510 --> 00:01:31,650
systems such as cloud, on-prem, APIs, et
cetera, drastically reducing the manual.

18
00:01:31,745 --> 00:01:32,565
And also it's

19
00:01:34,855 --> 00:01:35,205
clean.

20
00:01:36,815 --> 00:01:43,510
Ations and we can implement the
faster and smarter a within enhanced

21
00:01:43,510 --> 00:01:45,510
data governance and security.

22
00:01:46,140 --> 00:01:51,490
So thereby giving the result in
the real world measurement top two

23
00:01:51,790 --> 00:01:55,080
85% gain in terms of efficiency.

24
00:01:56,080 --> 00:02:00,670
What is the data challenge
we have currently, right?

25
00:02:01,310 --> 00:02:08,250
On a daily basis we are generating
2.5 country of data with this

26
00:02:08,250 --> 00:02:13,475
amount of data under the different
challenges in handling the large volume

27
00:02:14,475 --> 00:02:15,405
efficiency gain.

28
00:02:16,405 --> 00:02:17,815
Human intervention.

29
00:02:19,825 --> 00:02:21,965
We'll discuss more on this topic.

30
00:02:21,965 --> 00:02:28,305
So the, if we delve into what kind of
challenges we're facing in terms of data.

31
00:02:28,755 --> 00:02:34,765
So the first one is, the data silo and
fragmentation it is often spread across

32
00:02:34,855 --> 00:02:41,905
multiple systems, making it difficult
to create a unified view, integrate and

33
00:02:42,355 --> 00:02:47,945
process efficiently across the different
data sets because why It's a problem.

34
00:02:48,095 --> 00:02:52,565
Data engineers spend huge amounts
of time just locating, extracting,

35
00:02:53,075 --> 00:02:54,695
and normalizing the data.

36
00:02:56,495 --> 00:02:59,765
The second, I would like to
highlight the data quality, right?

37
00:02:59,945 --> 00:03:02,655
Most of the times it seems to be poor.

38
00:03:04,015 --> 00:03:07,979
So the organizations struggled with
the missing or incomplete data,

39
00:03:07,979 --> 00:03:14,459
such as the duplicates, inconsistent
formatting and also the sche.

40
00:03:15,459 --> 00:03:16,209
Impacts

41
00:03:18,819 --> 00:03:19,039
the

42
00:03:21,534 --> 00:03:22,104
data

43
00:03:23,104 --> 00:03:23,704
decisions.

44
00:03:23,704 --> 00:03:26,164
We cannot make it more efficient, right?

45
00:03:26,164 --> 00:03:27,394
In terms of business.

46
00:03:29,740 --> 00:03:34,899
And also the complex orchestration
in terms of the transformation when

47
00:03:34,899 --> 00:03:42,399
we do ETL or e LT jobs across the
dependencies, schedules, and environments.

48
00:03:43,089 --> 00:03:49,995
Coordination most of times seems to
be tricky, so failures can cascade.

49
00:03:50,000 --> 00:03:53,089
And the next one would be
the performance bottlenecks.

50
00:03:55,070 --> 00:03:59,419
As data volume grow pipelines
need to scale horizontally.

51
00:04:00,140 --> 00:04:02,089
Many legacy systems.

52
00:04:04,070 --> 00:04:07,079
Most of the times it's poorly optimized.

53
00:04:07,190 --> 00:04:08,540
The pipelines can't keep up.

54
00:04:10,700 --> 00:04:16,430
And the last one could be, data
governance and compliance, right?

55
00:04:16,910 --> 00:04:20,820
Keeping up with the data
policies such as GDPR, hipaa.

56
00:04:20,820 --> 00:04:22,800
CCPA is hard.

57
00:04:23,520 --> 00:04:28,470
So the challenges include tracking
the lineage, the hardback, world based

58
00:04:28,470 --> 00:04:32,270
access control, and the classification
of the data in terms of sensitiveness.

59
00:04:33,270 --> 00:04:35,940
So these are the different
challenges, right?

60
00:04:36,540 --> 00:04:38,220
We facing.

61
00:04:40,035 --> 00:04:47,955
Under solution how can we find it like in
this slide automated feature engineering.

62
00:04:48,765 --> 00:04:52,365
So what is about, how it can
address the data challenge?

63
00:04:52,415 --> 00:04:57,545
So this is an AI driven, game changer
in the data science and machine learning

64
00:04:57,545 --> 00:05:03,245
pipeline because it uses machine
learning, heuristics and domain knowledge

65
00:05:03,635 --> 00:05:10,765
to automatically generate, select,
and optimize the features from raw

66
00:05:10,765 --> 00:05:14,045
data without much traditional way of.

67
00:05:15,045 --> 00:05:19,905
Future engineering, what it's all about,
it's a process of transforming the

68
00:05:19,905 --> 00:05:22,875
raw data into the meaningful inputs.

69
00:05:22,935 --> 00:05:26,985
When I say inputs, it's more of a
features for a machine learning model.

70
00:05:27,585 --> 00:05:33,135
Traditionally, this is manual time
consuming and highly dependent on

71
00:05:34,515 --> 00:05:37,680
the expertise knowledge of domain.

72
00:05:39,180 --> 00:05:40,440
So now comes.

73
00:05:42,195 --> 00:05:49,095
Yay driven automated feature engineering
flips that by using algorithms to

74
00:05:49,095 --> 00:05:55,605
analyze data types and its relationships,
generating the new features such as

75
00:05:55,605 --> 00:05:59,640
ratios, time aggregates, encodings,

76
00:06:01,875 --> 00:06:08,445
rank, and select the best
features, continuously optimize

77
00:06:08,595 --> 00:06:10,635
features for model performance.

78
00:06:11,190 --> 00:06:17,340
It's often powered by the famous
tools like feature tools by alter

79
00:06:20,130 --> 00:06:28,110
maker autopilot, Google Auto, DataRobot,
to name few, and one of the key

80
00:06:28,110 --> 00:06:34,400
techniques being employed in this
engineering in a more automated way.

81
00:06:35,495 --> 00:06:38,525
The number one is a
deep feature synthesize.

82
00:06:39,735 --> 00:06:45,135
DFS automatically builds the
features from relational dataset.

83
00:06:46,185 --> 00:06:52,425
And the next is recursive transformation
creates the layered and higher level

84
00:06:52,455 --> 00:06:58,815
features, embedding and encoding,
which is a technique used to

85
00:06:58,815 --> 00:07:02,595
convert categorical or textual data.

86
00:07:03,595 --> 00:07:07,740
Vector what the algorithm
can be used in the selection

87
00:07:09,840 --> 00:07:15,150
using a correlation method,
mutual information or values

88
00:07:15,150 --> 00:07:17,730
to drop low value features.

89
00:07:18,730 --> 00:07:23,080
Moving on to the next
slide, which talks about.

90
00:07:24,400 --> 00:07:25,690
Data cleansing, right?

91
00:07:25,750 --> 00:07:30,910
How it can be employed in an intelligent
way compared to the traditional methods,

92
00:07:31,630 --> 00:07:37,340
which will give us the improvements in
terms of detecting the, in the data.

93
00:07:37,880 --> 00:07:42,970
And also we can implement the correction
of data in a more automated fashion

94
00:07:43,660 --> 00:07:45,880
and how to recognize the patterns.

95
00:07:46,880 --> 00:07:50,060
So it's a data cleansing,
intelligent data.

96
00:07:50,390 --> 00:07:56,060
Cleaning is more of is a use of ai,
machine learning and advanced automation.

97
00:07:56,160 --> 00:08:00,540
To clean, validate and enrich
or enhance the data are more

98
00:08:00,540 --> 00:08:05,220
precisely and efficiently
compared to traditional methods.

99
00:08:07,680 --> 00:08:13,530
Relying on static rules or manual
review, intelligent cleansing

100
00:08:13,740 --> 00:08:21,450
adapts to the patterns, context, and
domain specific logic to ensure high

101
00:08:21,450 --> 00:08:29,010
quality usable data for the different
purposes, such as analytics reporting

102
00:08:29,790 --> 00:08:31,175
on obviously the machine learning.

103
00:08:32,565 --> 00:08:36,485
So now comes the question when we say the
data cleansing, what makes it intelligent?

104
00:08:37,445 --> 00:08:41,705
Traditional data cleansing involves
manual scripts of pixel logic, like

105
00:08:41,705 --> 00:08:45,395
removing the duplicates, filling
in null values, and standardizing

106
00:08:45,395 --> 00:08:48,575
format to name few on the other.

107
00:08:48,665 --> 00:08:56,375
And intelligent data cleansing
uses a that learn and adapt.

108
00:08:57,375 --> 00:09:03,015
We came across in the data cleansing, most
predominant one is the missing values.

109
00:09:03,675 --> 00:09:09,435
So out intelligence can help right
predicts the missing values using the

110
00:09:09,435 --> 00:09:13,845
ML models, such K or regression models.

111
00:09:15,255 --> 00:09:18,615
And the next one would
be outlier detection.

112
00:09:19,815 --> 00:09:26,025
Using statistical or yay models to flag
the anomalies, not just thresholds,

113
00:09:28,785 --> 00:09:31,335
then the data normalization comes in.

114
00:09:32,385 --> 00:09:37,035
Or can we intelligently cleanse
the data with respect to them

115
00:09:37,695 --> 00:09:40,665
data, an normalization by
learning the patterns, right?

116
00:09:40,965 --> 00:09:45,255
Intake to dates,
addresses, et cetera, and.

117
00:09:47,910 --> 00:09:56,400
And the entity resolution using fuzzy
matching or the NLP to merge duplicates

118
00:09:56,490 --> 00:10:01,830
such as you can say IBM COP versus
International Business machines.

119
00:10:03,060 --> 00:10:08,250
And data enrichment plays
an important role, right?

120
00:10:09,250 --> 00:10:14,050
Attributes by putting from external
sources like business databases

121
00:10:14,050 --> 00:10:22,210
or geolocation we and improve the
reaching the data in an efficient way.

122
00:10:23,210 --> 00:10:24,920
So what would be the next step?

123
00:10:25,020 --> 00:10:27,400
When we successfully cleanse the data?

124
00:10:27,790 --> 00:10:31,420
It's gonna be an interesting
topic for all us, right?

125
00:10:32,500 --> 00:10:36,850
The entire, how we integrate, right?

126
00:10:37,030 --> 00:10:37,570
Using the.

127
00:10:40,300 --> 00:10:45,940
So it is a practice of combining machine
learning systems with DevOps principles

128
00:10:46,480 --> 00:10:52,060
to streamline the lifecycle, right
development, deployment, monitoring,

129
00:10:52,540 --> 00:10:56,320
and governance of models in production.

130
00:10:58,825 --> 00:11:04,555
It off as a glue between data science
and engineering, turning experimental

131
00:11:04,555 --> 00:11:11,065
models into a robust, scalable, and
continuously improving business systems.

132
00:11:11,875 --> 00:11:12,415
So what?

133
00:11:12,415 --> 00:11:14,455
It integrates the lifecycle.

134
00:11:14,575 --> 00:11:22,200
We'll see Ops connects multiple workflows
such as data ingestion, versioning,

135
00:11:22,970 --> 00:11:25,675
model training, and experimentation.

136
00:11:28,105 --> 00:11:28,795
And validation.

137
00:11:31,195 --> 00:11:33,115
And the next one would be the deployment.

138
00:11:33,445 --> 00:11:40,195
CA CDL model monitoring performance, drift

139
00:11:42,385 --> 00:11:44,365
retraining, and the feedback loops.

140
00:11:44,545 --> 00:11:49,620
So these are all the, some workflows
where we can integrate, right?

141
00:11:50,080 --> 00:11:53,075
The would be CD pipelines
for machine learning.

142
00:11:53,825 --> 00:11:53,975
Yeah.

143
00:11:53,975 --> 00:11:55,445
What is the purpose of it?

144
00:11:55,505 --> 00:12:00,215
It can automate model testing,
validation, and deployments,

145
00:12:02,345 --> 00:12:07,055
and the second component would be
model registry, so that can tracks,

146
00:12:07,055 --> 00:12:11,525
versions, metadata, and lineage
of the different trained models.

147
00:12:12,260 --> 00:12:16,250
And the third component would be
feature store, which can be a central

148
00:12:16,700 --> 00:12:21,440
for consistent reusable features across
multiple trainings and inference.

149
00:12:21,980 --> 00:12:26,540
Automated retraining triggers
based on data drift or model decay

150
00:12:28,610 --> 00:12:35,790
monitoring tools and can be used for
checking the currency latency drift.

151
00:12:36,790 --> 00:12:42,560
These are the different components of the
integration of a machine learnings model.

152
00:12:43,560 --> 00:12:46,925
Moving on to predictive
quality monitoring.

153
00:12:47,925 --> 00:12:50,535
Monitor which is critical.

154
00:12:52,345 --> 00:12:53,065
Business.

155
00:12:53,425 --> 00:12:59,140
So statistical modeling and sensor
data to detect patterns forecast

156
00:12:59,140 --> 00:13:04,060
the deviations and identify quality
risks before they lead to defects or

157
00:13:04,060 --> 00:13:07,660
failure in product or the process.

158
00:13:07,710 --> 00:13:09,535
So what would be some of.

159
00:13:11,325 --> 00:13:15,165
We can see reduced defects
can be one of the benefit.

160
00:13:15,525 --> 00:13:18,735
So we can catch the issues
before final product is made.

161
00:13:19,065 --> 00:13:20,925
A lower cost of quality.

162
00:13:21,025 --> 00:13:26,905
We can prevent the scrap rework on
warranty claims and the real time

163
00:13:26,905 --> 00:13:31,705
decision making, which is really
helpful to fix the issues in process

164
00:13:32,125 --> 00:13:35,515
rather than postproduction and also.

165
00:13:38,365 --> 00:13:43,105
Which in turn will maximize the
output while keeping the I standards.

166
00:13:44,155 --> 00:13:50,185
And if there is a issue even in
production, the benefit would be having

167
00:13:50,185 --> 00:13:55,855
a predictive monitoring would be the
root cause insights that we can identify

168
00:13:55,855 --> 00:14:02,185
key variables affecting the quality and
thereby improve upon it in production

169
00:14:02,725 --> 00:14:04,555
regulatory compliance, which is.

170
00:14:05,555 --> 00:14:10,295
Organization whether it's a
retail, manufacturing, finance, or

171
00:14:10,295 --> 00:14:13,085
healthcare, the is sensitive, right?

172
00:14:13,385 --> 00:14:18,095
So father, it'll have an impact
Mentioning the consistent quality

173
00:14:18,095 --> 00:14:23,300
for audits and certifications,
having a monitoring method, which is.

174
00:14:24,300 --> 00:14:26,250
We can say as an example, right?

175
00:14:26,590 --> 00:14:30,255
The predicting health or medical
condition, sepsis in real time.

176
00:14:30,555 --> 00:14:37,025
Yeah, hospital users and EA model
trained on vitals plus labs data.

177
00:14:38,405 --> 00:14:43,685
For example, we can say all rate WBC
count ate levels, so system predicts

178
00:14:43,985 --> 00:14:49,925
sepsis, risk others before the
symptoms actually show in the body.

179
00:14:50,975 --> 00:14:57,305
Since alerts to the car teams, they can do
or provide early antibiotics and fluids,

180
00:14:59,945 --> 00:15:03,965
which in turn can reduce the
mortality rate by 20 to 30%.

181
00:15:04,745 --> 00:15:05,470
That's cool, huh?

182
00:15:06,470 --> 00:15:10,485
Moving on to data enrichment.

183
00:15:10,695 --> 00:15:12,825
How can we drive.

184
00:15:13,825 --> 00:15:20,015
So it uses intelligent algorithms
right to analyze existing data, detect

185
00:15:20,015 --> 00:15:24,395
patterns undocumented with additional
attributes, either from external

186
00:15:24,395 --> 00:15:30,625
sources, derived insights or automated
predictions to increase its quality and.

187
00:15:31,625 --> 00:15:36,095
We can quickly see the different
types of data enrichment, right?

188
00:15:36,155 --> 00:15:40,855
So first is raw data from the
legacy systems unstructured content

189
00:15:42,625 --> 00:15:46,920
areed from the disparate enterprise
sources, and and enhance the insights

190
00:15:47,155 --> 00:15:50,075
which can deliver 56% greater.

191
00:15:51,075 --> 00:15:53,715
So these are the different types, right?

192
00:15:54,115 --> 00:15:58,515
The one popular one would be the
texture enrichment using the natural

193
00:15:58,515 --> 00:16:04,275
language processing, NLP, what it does
basically extract the such structured

194
00:16:04,275 --> 00:16:06,285
data from unstructured text, right?

195
00:16:06,645 --> 00:16:10,335
So the example would be pulling
the job titles and skills.

196
00:16:11,335 --> 00:16:15,865
And image or video enrichment,
that is one of the typical type.

197
00:16:17,125 --> 00:16:22,175
What the AI is doing is uses the
computer vision to label a classify

198
00:16:22,175 --> 00:16:27,335
the media, and the best example would
be tagging the product photos with

199
00:16:27,335 --> 00:16:29,195
the categories, colors, and objects.

200
00:16:30,695 --> 00:16:32,705
And the next one would be geospatial.

201
00:16:34,540 --> 00:16:39,495
We can it, it basically what it
does is adding the location based

202
00:16:39,495 --> 00:16:41,745
data from GPS or address fields.

203
00:16:42,305 --> 00:16:45,755
The typical example would be
attach weather, region risk,

204
00:16:45,755 --> 00:16:47,465
or store proximity data.

205
00:16:48,335 --> 00:16:51,705
So this is how yeah, is powering right.

206
00:16:52,705 --> 00:16:56,275
So it's a smart system that
manages the history and the

207
00:16:56,275 --> 00:16:58,505
evolution of digital artifacts.

208
00:16:58,535 --> 00:17:05,355
We can say such as code data, models,
documents, everything requires

209
00:17:05,405 --> 00:17:06,845
the version controlling, right?

210
00:17:07,204 --> 00:17:10,740
But how can we attribute
using yay to automate?

211
00:17:12,379 --> 00:17:18,139
Detect changes, suggest actions
and optimize collaborations across

212
00:17:18,409 --> 00:17:20,240
multiple teams and pipelines.

213
00:17:21,500 --> 00:17:26,139
So what are the few benefits of
having a intelligent version control?

214
00:17:26,770 --> 00:17:32,999
So smaller collaboration, faster
conflict resolution, and to

215
00:17:32,999 --> 00:17:34,564
detect the semantic conflicts.

216
00:17:34,934 --> 00:17:41,764
And suggest the resolutions
intelligently also improves auditability.

217
00:17:41,794 --> 00:17:47,164
Better traceability automatically
logs the model data and code lineage

218
00:17:47,944 --> 00:17:52,364
vital for the regulated industries
such as finance and healthcare.

219
00:17:54,194 --> 00:17:54,914
Also, this.

220
00:17:56,129 --> 00:18:00,719
Version controlling in an intelligent way
improves the models in data management,

221
00:18:01,169 --> 00:18:07,699
like how changes in data sets can be
tracked quickly and efficiently, or

222
00:18:07,699 --> 00:18:13,159
the features impact the model accuracy
and also effectively supports the

223
00:18:13,159 --> 00:18:18,919
rollback to known good configuration
if any issue occurs reproducibility.

224
00:18:21,139 --> 00:18:25,999
The full experiment, tracking
and rerunning the pipelines with

225
00:18:25,999 --> 00:18:28,429
the exact inputs and parameters,

226
00:18:30,559 --> 00:18:34,189
and also with all these benefits, right?

227
00:18:34,379 --> 00:18:36,120
It can save the time.

228
00:18:36,480 --> 00:18:43,530
And also automation in an efficient way
like intelligent merging, auto, tagging

229
00:18:43,530 --> 00:18:46,050
our versions, change summaries, et cetera.

230
00:18:47,054 --> 00:18:50,930
So what are the different
popular tools available now in

231
00:18:50,930 --> 00:18:52,560
enabling the intelligent version?

232
00:18:52,560 --> 00:18:55,965
Control would be DVC for
the data on thel versioning.

233
00:18:56,275 --> 00:18:56,920
YAML flow.

234
00:18:56,920 --> 00:19:01,860
Is there GitHub copilot is the
AI systems Smart Code versioning.

235
00:19:02,940 --> 00:19:03,270
Yep.

236
00:19:03,540 --> 00:19:05,730
This is all about version control

237
00:19:08,400 --> 00:19:11,230
and we can see about, case studies, right?

238
00:19:11,230 --> 00:19:14,240
Which are implementing these mechanisms.

239
00:19:14,780 --> 00:19:22,520
So yeah, one of the financial services
company that they have booster analyzed

240
00:19:22,520 --> 00:19:27,580
the data analyst, the productivity by
37%, enabling the deeper market insights.

241
00:19:28,780 --> 00:19:32,420
So by tailoring the intelligence
automatically delivered to

242
00:19:32,420 --> 00:19:37,500
each team member based on their
specific role and also the

243
00:19:38,010 --> 00:19:40,650
historical pattern of interactions.

244
00:19:41,730 --> 00:19:48,155
And one of the healthcare provider
automated successfully could say 73% of

245
00:19:48,435 --> 00:19:53,360
critical classifications while maintaining
the strict compliance standards.

246
00:19:56,030 --> 00:20:02,500
And also they can see that one intelligent
classification and masking technologies

247
00:20:03,310 --> 00:20:08,149
help them to complete the sensitive
data on the protection of patient

248
00:20:08,149 --> 00:20:12,350
information and PHI and one of the.

249
00:20:14,735 --> 00:20:19,024
These algorithms and mission
language process, the I value

250
00:20:19,024 --> 00:20:25,564
market data to be presented 2.8
times faster, creating significant

251
00:20:25,564 --> 00:20:27,784
advantage over the competitors.

252
00:20:28,564 --> 00:20:33,574
So this time sensitive, revenue
driven insights deliver the ahead

253
00:20:33,574 --> 00:20:38,034
of their competitors in the industry
directly impacting their results.

254
00:20:38,934 --> 00:20:41,379
Quarterly they can see.

255
00:20:42,379 --> 00:20:46,699
So these industry leaders in the
different segmentation I've seen

256
00:20:46,699 --> 00:20:51,120
their data operations through
the strategic implementation of

257
00:20:51,219 --> 00:20:53,020
these different AI techniques.

258
00:20:53,070 --> 00:21:00,525
We talked about there are transformative
results included dramatically.

259
00:21:01,815 --> 00:21:02,745
Acceleration.

260
00:21:02,795 --> 00:21:05,585
And also reduce the operational costs.

261
00:21:05,705 --> 00:21:09,955
And particularly for the finance and
healthcare businesses or industry,

262
00:21:09,955 --> 00:21:16,945
they could see an enhanced compliance
regulatory across them industry.

263
00:21:17,945 --> 00:21:21,985
So this conversational yay
assistant in a day to day life.

264
00:21:22,195 --> 00:21:25,495
Everyone we'll be coming
across with the AI assistant.

265
00:21:25,495 --> 00:21:28,345
It's a virtual agent powered by yay.

266
00:21:28,375 --> 00:21:32,395
That interacts with the users like
us using the natural language.

267
00:21:32,754 --> 00:21:35,875
Basically text our voice, which automates.

268
00:21:37,805 --> 00:21:42,740
And provides informations and enables a
self service across the digital channels.

269
00:21:43,400 --> 00:21:49,010
It mimics human conversation patterns
and often supports multi conversations,

270
00:21:49,340 --> 00:21:51,620
context retention and personalization.

271
00:21:52,310 --> 00:21:52,580
Yeah.

272
00:21:53,120 --> 00:21:59,274
And one, are the core technologies
involved in these agents used as a.

273
00:22:00,274 --> 00:22:03,425
Which basically understands
the user intent and entities

274
00:22:03,485 --> 00:22:05,195
natural language generation.

275
00:22:05,705 --> 00:22:08,705
The NLG crafts, meaningful
human-like responses.

276
00:22:09,394 --> 00:22:13,834
Dial management manages a flow of
conversation and machine learning.

277
00:22:13,834 --> 00:22:17,884
Lms, for example, the good
example would be GPT covers

278
00:22:17,884 --> 00:22:19,804
adaptability and reasoning, right?

279
00:22:20,284 --> 00:22:21,674
Speech to text to speech.

280
00:22:21,734 --> 00:22:25,214
Converts a spoken input
output for wise assistance.

281
00:22:27,340 --> 00:22:29,925
The APIs and the integrations
are quite popular.

282
00:22:29,925 --> 00:22:33,945
Connects with the backend
systems, so CRMs and databases.

283
00:22:34,935 --> 00:22:37,664
So this is about a assistant.

284
00:22:37,814 --> 00:22:40,424
How can we interact with them efficiently?

285
00:22:41,424 --> 00:22:41,634
Yeah.

286
00:22:41,694 --> 00:22:44,664
With that it concludes this presentation.

287
00:22:45,664 --> 00:22:48,294
Thank you all for joining this session.

288
00:22:48,984 --> 00:22:49,194
Yeah.

289
00:22:49,224 --> 00:22:51,269
Before we wind off lemme conclude.

290
00:22:51,449 --> 00:22:55,049
With the data engineering how
it transforms, it's not just

291
00:22:55,049 --> 00:22:57,069
about adapting new tools.

292
00:22:57,069 --> 00:23:00,639
It's about modernizing the
workflows, I embracing the

293
00:23:00,639 --> 00:23:03,249
automation and aligning with.

294
00:23:05,109 --> 00:23:06,699
Business goals, right?

295
00:23:06,699 --> 00:23:11,439
To create a scalable, agile,
and intelligent data foundation.

296
00:23:12,159 --> 00:23:17,189
So I can provide the quick roadmap
style to transform these data

297
00:23:17,189 --> 00:23:19,349
engineering practices effectively.

298
00:23:20,369 --> 00:23:25,529
So adopt the modern data stack used
in ingestion, storage processing.

299
00:23:26,529 --> 00:23:31,329
Load, transform, integrate
thel for intelligent workflows.

300
00:23:31,719 --> 00:23:36,929
So which will help for any business
in auto detecting schema drift.

301
00:23:36,929 --> 00:23:42,209
Predict pipeline failures, enrich
data intelligently, implement data

302
00:23:42,209 --> 00:23:47,249
observability, create data pipelines
like production system, monitor the

303
00:23:47,249 --> 00:23:53,279
freshness, accuracy, volume, lineage,
and alert for anomalies or downturn.

304
00:23:54,134 --> 00:23:56,894
Use tools like they're quite popular.

305
00:23:56,894 --> 00:24:01,304
Monte load data band and big
test and version, everything,

306
00:24:02,324 --> 00:24:04,694
which is a critical one, right?

307
00:24:04,724 --> 00:24:07,334
Data testing, data versioning,
and model tracking.

308
00:24:08,174 --> 00:24:15,524
And wherever the automation use possible
can be effectively implemented using

309
00:24:15,524 --> 00:24:17,984
the data ingestion, pipelines, even

310
00:24:20,534 --> 00:24:21,794
data quality checks.

311
00:24:23,294 --> 00:24:31,244
So the goal is to reduce a manual ops and
increase pipeline reliability, and finally

312
00:24:32,054 --> 00:24:40,964
try to foster your DevOps culture, treat
data as a product, not as a byproduct.

313
00:24:41,964 --> 00:24:42,114
Yeah.

314
00:24:42,114 --> 00:24:44,504
With this this session concludes.

