1
00:00:00,500 --> 00:00:01,220
Hello everyone.

2
00:00:01,700 --> 00:00:05,390
I'm am Kris, a technical architect
and lead data engineer at for

3
00:00:05,420 --> 00:00:07,640
with over 16 years of experience.

4
00:00:08,140 --> 00:00:12,610
My research paper is titled De-Mystifying
Modern Data Pipeline Architecture from

5
00:00:12,670 --> 00:00:15,340
regional ETL to Cloud Native framing.

6
00:00:16,120 --> 00:00:20,020
Think back to a time when you had
to wait all night for business

7
00:00:20,020 --> 00:00:24,850
report to process that was of ETL,
but today the digital factory never

8
00:00:24,850 --> 00:00:26,680
sleeps and our data can either.

9
00:00:27,180 --> 00:00:32,009
My career, I've been on the frontline
of this dramatic evolution, and today

10
00:00:32,009 --> 00:00:36,330
I'll share how we are moving from
batch oriented mindset to the world

11
00:00:36,330 --> 00:00:41,850
of cloud native framing and what that
means for how we turn data into value.

12
00:00:42,350 --> 00:00:44,755
Here is a look at what will cover today.

13
00:00:45,445 --> 00:00:47,955
We'll start with evolution of
data pipeline architectures.

14
00:00:48,815 --> 00:00:50,885
Discussing the limitations
of legacy systems.

15
00:00:51,395 --> 00:00:54,995
Then we'll dive into modern
art pattern that emerged.

16
00:00:55,865 --> 00:01:00,395
We look at the two evolution landscape,
examine critical design integration

17
00:01:00,395 --> 00:01:05,975
for building resilient pipelines, and
finally look at emerging brands and

18
00:01:06,005 --> 00:01:07,880
practical migration s for organiz.

19
00:01:08,380 --> 00:01:12,219
The key takeaway of modern data
engineering revolution is the shift

20
00:01:12,219 --> 00:01:14,379
from batch to realtime processing.

21
00:01:14,960 --> 00:01:18,170
The recent approach relied
on schedule batch processing

22
00:01:18,500 --> 00:01:19,970
using centralized systems.

23
00:01:20,390 --> 00:01:25,700
Now, modern reality involve distributed
realtime and cloud native architectures.

24
00:01:26,480 --> 00:01:29,570
Two main factors, drivers
is the business driver.

25
00:01:29,990 --> 00:01:35,420
Has an intense need for immediate
insight and operational intelligence.

26
00:01:36,080 --> 00:01:40,580
The technical driver is a demand
for greater scalability, better

27
00:01:40,580 --> 00:01:46,220
cost efficiency, and overall
flexibility in our data systems.

28
00:01:46,720 --> 00:01:50,529
Traditional EDL limitations,
why change was inevitable.

29
00:01:51,029 --> 00:01:55,309
There is an EL framework facing
significant constraints first batch

30
00:01:55,309 --> 00:02:00,799
crossing in windows, typically third
overnight to avoid disruption operation

31
00:02:00,799 --> 00:02:05,479
systems, which severely limited data
availability when businesses started

32
00:02:05,479 --> 00:02:06,679
demanding real-time monitoring.

33
00:02:07,179 --> 00:02:12,914
Second, the design meant single point
of failure with limited recovery.

34
00:02:13,614 --> 00:02:17,334
Option that often require
completely rep processing the data.

35
00:02:17,724 --> 00:02:22,344
Third, rigid infrastructure meant
performance improvement depending

36
00:02:22,344 --> 00:02:27,324
on expensive hardware upgrades
involving high upfront cost and

37
00:02:27,324 --> 00:02:29,544
complex cap capacity planning.

38
00:02:30,044 --> 00:02:35,864
Fourth systems were built primarily for
structure data and second to handle the

39
00:02:35,864 --> 00:02:40,784
explosion of semi-structure logs and
streaming data for new sources like.

40
00:02:41,159 --> 00:02:43,859
Device, devices and web application.

41
00:02:44,669 --> 00:02:47,759
Finally, they also
suffer from vendor login.

42
00:02:48,149 --> 00:02:50,699
New to proprietary query extensions,

43
00:02:51,199 --> 00:02:55,789
the cloud storage revolution,
decoupling storage from compute,

44
00:02:56,179 --> 00:03:02,419
the introduction of cloud storage,
specifically object storage like Azure

45
00:03:02,419 --> 00:03:04,399
block storage or Google Cloud storage.

46
00:03:04,939 --> 00:03:05,299
What.

47
00:03:05,644 --> 00:03:08,194
Fundamental to enabling
modern data architecture.

48
00:03:08,704 --> 00:03:11,764
The most profound change
was the opening of mechanism

49
00:03:12,514 --> 00:03:14,104
between storage and computing.

50
00:03:14,604 --> 00:03:19,524
Before the loud revolution, storage
and computing were tightly funnel,

51
00:03:20,184 --> 00:03:23,174
inexpensive switch capacity warehouses.

52
00:03:23,444 --> 00:03:26,654
If you needed more crossing
power, you usually had to buy

53
00:03:27,314 --> 00:03:28,784
more storage and vice versa.

54
00:03:29,384 --> 00:03:33,344
After the shift, we gained
access to practically unlimited.

55
00:03:33,794 --> 00:03:39,254
Extremely cost effective object
storage, which can scale independently

56
00:03:39,434 --> 00:03:41,114
of our processing cluster.

57
00:03:41,614 --> 00:03:45,064
This adventure Freedom
delivers several key benefits.

58
00:03:45,274 --> 00:03:47,194
First, the pay as you go.

59
00:03:47,464 --> 00:03:52,714
Principal model shift cost from large
capital expenditures to variable

60
00:03:52,774 --> 00:03:58,114
operation costs, making data scale
accessible to every organization.

61
00:03:58,614 --> 00:03:59,004
Second.

62
00:03:59,394 --> 00:04:01,794
Enable schema on read flexibility.

63
00:04:02,005 --> 00:04:06,284
We can now store raw data in
its native format, applying

64
00:04:06,434 --> 00:04:10,724
transformation later, preserving
native integrity rather than forcing

65
00:04:10,724 --> 00:04:13,544
it into rigid redefined ular front.

66
00:04:14,044 --> 00:04:20,194
Finally, cloud storage offers native
redundancy, durability, and it

67
00:04:20,194 --> 00:04:25,444
inherent needs support all formats
such as semi-structured unstructured.

68
00:04:25,969 --> 00:04:26,959
Which was impossible.

69
00:04:27,529 --> 00:04:29,299
With regional databases,

70
00:04:29,799 --> 00:04:34,659
modern artificial patterns, overweight
as data systems make mature to

71
00:04:34,659 --> 00:04:39,369
handle modern cha challenges, several
key artificial patterns emerge.

72
00:04:39,699 --> 00:04:44,319
These patterns represent different
philosophies for how we secure

73
00:04:44,634 --> 00:04:46,214
and process data on a scale.

74
00:04:46,714 --> 00:04:49,144
Explore the five most influential.

75
00:04:49,624 --> 00:04:51,754
First, the medallion architecture.

76
00:04:52,474 --> 00:04:56,884
The pattern defines transparent,
quality layers, bronze, silver,

77
00:04:56,884 --> 00:05:01,594
and gold for data refinement and
curing, governance and quality.

78
00:05:01,594 --> 00:05:03,964
All are built into the pipeline.

79
00:05:04,464 --> 00:05:05,544
The Lambda architecture.

80
00:05:06,114 --> 00:05:11,934
This is a dual path approach using a batch
layer for complete historical accuracy

81
00:05:11,934 --> 00:05:13,494
and free layer for real diamond sight.

82
00:05:14,184 --> 00:05:17,324
Copper architecture cation of Lambda.

83
00:05:17,924 --> 00:05:23,684
This pattern adopts a steam first model
using a single processing engine and

84
00:05:23,684 --> 00:05:28,154
an even log as a source of truth for
both realtime and historical views.

85
00:05:28,654 --> 00:05:29,844
The lake goes paradigm.

86
00:05:30,819 --> 00:05:36,009
This unified approach brings the
transactional feature and performance

87
00:05:36,009 --> 00:05:38,764
of data warehouse directly onto
the low cost storage of data link.

88
00:05:39,264 --> 00:05:40,494
Finally, data match.

89
00:05:41,184 --> 00:05:45,084
This is a less AL architecture
and more organizational structure,

90
00:05:45,834 --> 00:05:50,559
decentralizing data ownership and reading
data product owned by Nomen teams.

91
00:05:51,059 --> 00:05:53,819
Now I would like to discuss
about the medallion architecture,

92
00:05:54,419 --> 00:05:58,229
the medal architecture model of
Olympic medals organizers, data

93
00:05:58,229 --> 00:06:00,464
requirement into discrete all levels.

94
00:06:00,964 --> 00:06:07,864
Bronze laser layer is where raw data lands
into lands without motivation, ensuring

95
00:06:08,194 --> 00:06:13,354
complete source fidelity, its validation
focuses usually on completeness.

96
00:06:13,624 --> 00:06:14,404
A single layer.

97
00:06:14,884 --> 00:06:17,275
Is that what the foundation here?

98
00:06:17,305 --> 00:06:22,525
Raw data is informed by standardizing
format, fixing inconsistencies, and

99
00:06:22,525 --> 00:06:24,354
applying quality and governance rules.

100
00:06:24,905 --> 00:06:27,650
The goal layer, is a data
shape for business assumption.

101
00:06:28,400 --> 00:06:31,340
It includes purpose build
structures like star S schemas,

102
00:06:31,370 --> 00:06:33,320
and pre aggregated metrics.

103
00:06:33,590 --> 00:06:37,610
The main benefit is that it is
clear quality boundaries and

104
00:06:37,610 --> 00:06:40,630
ensures productive processing.

105
00:06:41,080 --> 00:06:45,309
This makes it ideal for organizations
with strong governance requirement.

106
00:06:45,809 --> 00:06:48,329
Now, I would like to discuss
about the comparison between the

107
00:06:48,329 --> 00:06:49,710
Lambda and the KA architecture.

108
00:06:50,210 --> 00:06:53,859
Lambda and KA architecture represent
different philosophies for handling both.

109
00:06:54,204 --> 00:06:56,334
Historical and realtime data.

110
00:06:57,084 --> 00:07:00,414
The Lambda architecture uses
batch and stream processing path.

111
00:07:01,194 --> 00:07:06,174
The batch path handles comprehensive
historical analysis while the

112
00:07:06,174 --> 00:07:07,764
speed path provides immediate.

113
00:07:08,364 --> 00:07:13,609
If preliminary insights, its main
downside are higher complexity and

114
00:07:13,944 --> 00:07:17,484
the necessity of dual code based
maintenance for transforming logic.

115
00:07:17,984 --> 00:07:23,144
Architecture simplifies this by adopting
a stream processing first approach.

116
00:07:23,714 --> 00:07:28,964
It uses a single app and only event
loss as the definitive system of record

117
00:07:29,924 --> 00:07:35,964
batch processing is re ized as just
naming with an unlimited time window.

118
00:07:36,464 --> 00:07:39,959
This result in simpler maintenance
and unified processing model.

119
00:07:40,459 --> 00:07:43,354
Now I like, I would like to
discuss about the lake cost.

120
00:07:43,854 --> 00:07:47,524
The lake of patterning
represents a powerful conversion.

121
00:07:48,269 --> 00:07:52,949
It addresses the historical fragmentation
of having separate data lakes for

122
00:07:52,949 --> 00:07:55,649
raw storage and aware of analytics.

123
00:07:56,459 --> 00:08:01,889
It augments lake flexibility, low cost
storage with data warehouse performance.

124
00:08:02,389 --> 00:08:06,114
S features include implementing
warehouse capabilities like asset

125
00:08:06,114 --> 00:08:11,504
transactions and schema enforcement
directly on cloud storage objects.

126
00:08:11,834 --> 00:08:16,544
It supports multi workload
support for bi ml and streaming

127
00:08:16,574 --> 00:08:18,074
and provides unified governance.

128
00:08:18,574 --> 00:08:22,574
The business impact is is the elimination
of data lubrication, fragmented

129
00:08:22,754 --> 00:08:25,039
governance and integration headaches.

130
00:08:25,539 --> 00:08:29,169
Now I would like to discuss about the
data match domain oriented approach.

131
00:08:29,669 --> 00:08:34,859
The data match pattern is less about
technology, more about organization shift.

132
00:08:35,359 --> 00:08:39,529
Its core principle is to read data
as a product owned by domain team.

133
00:08:40,029 --> 00:08:46,449
This is designed to solve the scaling
bottleneck present with teams key.

134
00:08:46,949 --> 00:08:51,899
Domain oriented ownership, self
serve data platform, federating

135
00:08:51,899 --> 00:08:56,389
governance and data products with clear
interfaces and quality assurances.

136
00:08:57,259 --> 00:09:01,939
The primary benefit is or, and
scalability and better alignment

137
00:09:01,939 --> 00:09:03,469
with specific business domain.

138
00:09:04,459 --> 00:09:09,049
The primary challenge, however, is that
requires significant organization change.

139
00:09:10,009 --> 00:09:12,379
It's a social technical transformation.

140
00:09:12,879 --> 00:09:15,759
I would like to discuss about
the Tool Evolution timeline.

141
00:09:16,449 --> 00:09:18,499
Let's look at the tool evolution Pipeline.

142
00:09:18,859 --> 00:09:22,719
We can segment this journey into four
areas in nineties and two oh, and

143
00:09:22,719 --> 00:09:27,429
the focus was traditional ET tools
like IBM Data stage and Informatica

144
00:09:27,849 --> 00:09:33,389
visual interfaces and mon monolithic
batch oriented design characteristics.

145
00:09:33,889 --> 00:09:40,359
From 2010 to 2015, distributed
processing framework like Hadoop and in

146
00:09:40,779 --> 00:09:46,354
horizonal scaling and old first approach
between two, 2015 and 2013, we saw

147
00:09:46,354 --> 00:09:50,904
the rise of open source orchestration
tools like Airflow and Perfect.

148
00:09:51,404 --> 00:09:56,324
Alongside early cloud services, these
enabled processing and serverless

149
00:09:56,654 --> 00:10:03,494
executions from 2020 onwards, the
landscape was stream first, unified

150
00:10:03,494 --> 00:10:05,654
processing, and ML integration.

151
00:10:06,074 --> 00:10:12,644
We now see real time capabilities,
integrative interfaces, and specialize

152
00:10:12,644 --> 00:10:14,579
system for virtual learning.

153
00:10:15,079 --> 00:10:16,849
Modern tool categories.

154
00:10:17,419 --> 00:10:22,119
The current landscape has four main
categories of tools, orchestration

155
00:10:22,189 --> 00:10:28,829
framework like Apache, airflow, and
perfect focus on scheduling, depending

156
00:10:28,999 --> 00:10:34,049
scheduling dependencies, and monitoring
the apple from actual data process.

157
00:10:34,549 --> 00:10:41,539
Cloud native services such as AWS Glue
and Azure Data Factory offer serverless

158
00:10:41,569 --> 00:10:46,699
execution and consumption based pricing,
fitting well with variable workloads,

159
00:10:47,359 --> 00:10:52,909
saving platforms like Apache Kafka
and Spark streaming, or essential

160
00:10:53,569 --> 00:10:55,764
for low latency realtime inside.

161
00:10:56,149 --> 00:11:03,559
And finally crossing engines like Apache
Bar Link provide the power of complex.

162
00:11:03,994 --> 00:11:08,694
Distributed computations US selection
criteria was choosing a tool must

163
00:11:08,694 --> 00:11:13,674
extend beyond features to include
team skills, operational requirements,

164
00:11:13,839 --> 00:11:16,089
cost model, and integration needs

165
00:11:16,589 --> 00:11:17,749
critical design consideration.

166
00:11:18,289 --> 00:11:21,974
Moving to modern distributed
system introduces a new set of

167
00:11:21,974 --> 00:11:23,309
challenges and operational necess.

168
00:11:23,809 --> 00:11:29,269
We must address five critical design
areas, data governance, and lineage.

169
00:11:29,899 --> 00:11:35,869
Lack racking, data provenance
across complex environments, quality

170
00:11:35,869 --> 00:11:41,669
validations, shifting from periodic
checks to continuous assurance and

171
00:11:41,729 --> 00:11:45,599
performance optimization, leveraging
distributed techniques like

172
00:11:45,689 --> 00:11:48,094
partitioning, security and compliance.

173
00:11:48,719 --> 00:11:53,129
Implementing data centric production
and access control in indonesia's

174
00:11:53,129 --> 00:11:58,589
challenges, managing the coexistence
of real time and bad workloads,

175
00:11:59,089 --> 00:12:01,729
data governance and distributed
systems in distributed hybrid

176
00:12:01,729 --> 00:12:03,769
and multi-cloud environments.

177
00:12:04,189 --> 00:12:08,104
Maintaining visibility is a significant
c. Governance had evolved from

178
00:12:08,764 --> 00:12:10,534
checkbox to an operational activity.

179
00:12:11,034 --> 00:12:15,384
Our solution must include automated
and linear tracking at multiple levels

180
00:12:15,384 --> 00:12:22,164
from dataset to column and even record
levels of provenance for environment

181
00:12:22,164 --> 00:12:28,169
with in complete instrumentation,
emergent techno techniques include.

182
00:12:28,669 --> 00:12:34,699
Lineage that uses statistical methods
to infer transformation relationships.

183
00:12:35,689 --> 00:12:40,819
The business value is clear, rapid
impact analysis during values, simplified

184
00:12:40,819 --> 00:12:44,749
troubleshooting of inconsistencies
and necessary audit rating,

185
00:12:45,019 --> 00:12:46,879
failure, audit rail for compliance,

186
00:12:47,379 --> 00:12:48,654
quality validation framework.

187
00:12:49,524 --> 00:12:53,884
Data quality assurance must be must
now be an continuous and automated

188
00:12:54,184 --> 00:12:57,394
now, not just periodic and manual.

189
00:12:57,934 --> 00:13:00,694
Modern framework implement
multidimensional validation,

190
00:13:01,414 --> 00:13:02,644
syn and correctness.

191
00:13:03,144 --> 00:13:07,854
Semantic validating contextual
appro appropriateness.

192
00:13:08,354 --> 00:13:13,814
Implementation architecture has evolved
to distributed opponents that exhibit

193
00:13:14,174 --> 00:13:16,454
checks at each transformation boundary.

194
00:13:17,084 --> 00:13:22,204
Streaming and this is achieved to
specialized operations that implement

195
00:13:22,504 --> 00:13:26,679
validation logic directly within
the execution model related in an

196
00:13:27,109 --> 00:13:28,999
pattern in continuous data flow.

197
00:13:29,499 --> 00:13:31,344
Emerging ran serverless data processing.

198
00:13:32,019 --> 00:13:32,979
Looking forward.

199
00:13:33,309 --> 00:13:37,089
Serverless data processing
represents profound paradigm shift.

200
00:13:37,959 --> 00:13:42,909
Key characteristics include elimination
of expressing infrastructure, provisioning

201
00:13:43,479 --> 00:13:47,919
di, dynamic resource allocation, and
a shift to consumption based pricing.

202
00:13:48,419 --> 00:13:51,749
This has a significant design impact,
encouraging architecture, nature,

203
00:13:51,749 --> 00:13:56,684
some of smaller focus crossing unit
with easier boundaries rather than

204
00:13:57,094 --> 00:13:58,484
monolithic transformation jobs.

205
00:13:58,984 --> 00:14:04,704
The benefits are powerful, automatic
scaling, significant cost optimization,

206
00:14:05,064 --> 00:14:07,194
and operational simplicity.

207
00:14:07,694 --> 00:14:11,414
AI ML integration data
pipeline meet machine learning.

208
00:14:12,194 --> 00:14:16,994
The integration of AI and ML has
driven specialized natural innovation.

209
00:14:17,729 --> 00:14:20,819
Features stores have emerged
as digital component.

210
00:14:21,569 --> 00:14:25,499
These are specialized systems that
manage lifecycle of machine learning

211
00:14:25,559 --> 00:14:31,049
features, providing versioning access
control, and ensuring consistency between

212
00:14:31,049 --> 00:14:36,079
training and serving modern service
pipeline, integrated time inference,

213
00:14:36,409 --> 00:14:38,119
delivering the core data pipelines.

214
00:14:38,719 --> 00:14:42,919
A key requirement is point in time
feature accuracy for valid modern

215
00:14:42,919 --> 00:14:44,869
learning and lineage lacking for modern.

216
00:14:45,369 --> 00:14:50,289
The business impact is faster model
deployment, consistent feature engineering

217
00:14:50,589 --> 00:14:57,089
and unified infrastructure supporting both
traditional BI and advanced ML workloads,

218
00:14:57,589 --> 00:14:59,569
data contracts and EMA management.

219
00:14:59,929 --> 00:15:03,894
Formal agreement for data exchange
and data and data ecosystem.

220
00:15:04,394 --> 00:15:07,964
Centralized data contracts and
formalized EMA management are

221
00:15:07,964 --> 00:15:10,424
critical for maintaining sim.

222
00:15:10,924 --> 00:15:15,094
The purpose of these contracts is
to establish explicit agreement

223
00:15:15,094 --> 00:15:17,194
between data producers and consumers.

224
00:15:17,974 --> 00:15:23,404
Components specify data structure, quality
characteristics, and delivery patterns.

225
00:15:23,914 --> 00:15:28,144
The main benefit is enhanced
ability in distributor systems

226
00:15:28,744 --> 00:15:30,484
by setting clear expectations.

227
00:15:30,984 --> 00:15:35,559
Implemented via version schema
registries, and with sophisticated

228
00:15:36,249 --> 00:15:41,999
compatibility, checking that validate
proposed change against historical usage,

229
00:15:42,149 --> 00:15:44,189
prevent breaking downstream consumers

230
00:15:44,689 --> 00:15:48,354
migration strategies, practical
approaches to modernization.

231
00:15:49,324 --> 00:15:52,294
Stable approach, risk,
timeline, and e factors.

232
00:15:53,164 --> 00:15:56,494
Finally, migrating from legacy
architecture is a significant

233
00:15:56,494 --> 00:16:00,994
under undertaking and a whole cell
replacement is really feasible.

234
00:16:01,744 --> 00:16:07,774
We must adopt in incremental strategies,
enable outline a few proven approaches,

235
00:16:08,274 --> 00:16:13,029
patent based modernization, identify
more common crossing patterns.

236
00:16:13,529 --> 00:16:19,169
System and analyze usable modernization
approach for each hybrid execution

237
00:16:19,669 --> 00:16:24,979
uses gateway component to maintain
and sustain interface while

238
00:16:24,979 --> 00:16:27,409
underlying implementation transition.

239
00:16:27,409 --> 00:16:27,979
Gradually.

240
00:16:28,479 --> 00:16:32,229
Specialize connectors allows
modern processing framework to

241
00:16:32,229 --> 00:16:36,504
efficiently integrate with existing
data sources through adapter,

242
00:16:36,804 --> 00:16:39,184
patent, and format bridges, and.

243
00:16:40,164 --> 00:16:44,394
Domain by domain starts with
non-critical businesses domain and

244
00:16:44,394 --> 00:16:48,924
expand progressively this low risk
and low risk phase approaches.

245
00:16:48,924 --> 00:16:54,559
Minimize business disruptions,
sorry, and allow organizations to

246
00:16:54,759 --> 00:16:56,719
demonstrate value incrementally.

247
00:16:57,219 --> 00:16:59,529
Key essential insight for data leaders.

248
00:17:00,029 --> 00:17:01,974
To summarize our journey,
here are the key.

249
00:17:02,969 --> 00:17:04,959
Here are essential
insights for data leaders.

250
00:17:05,859 --> 00:17:09,459
There's no single architecture
that future lies in thoughtfully.

251
00:17:09,819 --> 00:17:15,389
System that aligns with specific business
context, adopt incremental migration to

252
00:17:15,419 --> 00:17:17,879
modernize and minimize risk gradually.

253
00:17:18,379 --> 00:17:22,099
Governance is critical line,
lacking in quality management

254
00:17:22,099 --> 00:17:23,314
or operational necessities.

255
00:17:23,449 --> 00:17:26,929
In distributed enrollments,
real time is standard.

256
00:17:27,169 --> 00:17:30,159
Naming is a becoming
basic business responsive.

257
00:17:30,714 --> 00:17:35,694
Finally remember that organization
change is essential as a technology

258
00:17:36,194 --> 00:17:38,654
recommendations action
items for organizations.

259
00:17:39,154 --> 00:17:44,249
For those looking to embark on this
modern modernization journey, I offer

260
00:17:44,344 --> 00:17:51,094
this actions action items as this current
state inventory or existing architecture

261
00:17:51,574 --> 00:17:52,954
and identify the key pain point.

262
00:17:53,454 --> 00:17:59,544
Event target state, use artificial
patterns like medallion or Lake Earth that

263
00:17:59,544 --> 00:18:01,974
align with your strategy business needs.

264
00:18:02,474 --> 00:18:03,254
Start small.

265
00:18:03,734 --> 00:18:08,714
Begin your migration with noncritical
domains or workload activities.

266
00:18:09,214 --> 00:18:13,404
Invest in governance, implement
lineage tracking and quality

267
00:18:13,404 --> 00:18:15,324
framework early in the process.

268
00:18:15,824 --> 00:18:16,724
Build skills.

269
00:18:17,224 --> 00:18:20,709
Develop the necessary cloud
native and streaming processing

270
00:18:20,739 --> 00:18:22,179
bilities within your teams.

271
00:18:22,679 --> 00:18:24,439
And with this I conclude thank you.

272
00:18:24,939 --> 00:18:30,069
This transformation is ongoing and
by embracing cloud native principles

273
00:18:30,759 --> 00:18:34,959
and thoughtful design or nations
can build the resilient, adaptable

274
00:18:35,049 --> 00:18:38,169
data pipeline necessary for future
of operational intelligence.

275
00:18:38,669 --> 00:18:41,399
I note and thank you for
giving me this opportunity.

276
00:18:41,729 --> 00:18:42,239
Thank you.

