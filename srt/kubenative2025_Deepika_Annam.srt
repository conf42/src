1
00:00:00,500 --> 00:00:02,030
Hello, good morning.

2
00:00:02,660 --> 00:00:06,920
I am de Kana a senior data engineer
with experience in cloud native

3
00:00:06,920 --> 00:00:09,260
infrastructure and data platforms.

4
00:00:09,710 --> 00:00:14,180
My motivation for this research
came from seeing firsthand how cloud

5
00:00:14,180 --> 00:00:19,009
workloads are growing more dynamic
and complex, especially in large scale

6
00:00:19,009 --> 00:00:20,564
environments like retail and e-commerce.

7
00:00:21,064 --> 00:00:25,234
I have worked on projects where
traditional Kubernetes resource management

8
00:00:25,294 --> 00:00:29,824
simply couldn't keep up with unpredictable
traffic and changing business needs.

9
00:00:30,574 --> 00:00:34,925
This inspired me to look for
smarter and AI driven solutions.

10
00:00:35,524 --> 00:00:36,815
Let me share a quick story.

11
00:00:37,354 --> 00:00:40,774
In one of our retail systems, we
experienced a certain spike in

12
00:00:40,774 --> 00:00:43,024
online traffic during a flash sale.

13
00:00:43,024 --> 00:00:47,074
Our Kubernetes cluster was
set up with Static Horizon and

14
00:00:47,074 --> 00:00:49,295
report auto scheduler thresholds.

15
00:00:49,795 --> 00:00:52,045
The static policies
couldn't react fast enough.

16
00:00:52,555 --> 00:00:56,755
Some services were well provisioned
based in cloud resources, while

17
00:00:57,055 --> 00:01:00,985
others were under provision
causing slowdowns and even outages.

18
00:01:01,675 --> 00:01:05,455
Manual intervention was needed to
adjust configurations, which isn't

19
00:01:05,485 --> 00:01:09,475
capable, scalable, or reliable
in fast moving environments.

20
00:01:09,745 --> 00:01:13,015
This is highlighted the limitations
of static resource management.

21
00:01:13,735 --> 00:01:17,635
Today I'll show you how deep
reinforcement learning can help

22
00:01:17,995 --> 00:01:22,075
Kubernetes clusters learn and
adapt in real time, making smarter

23
00:01:22,075 --> 00:01:23,935
decisions about resource allocation.

24
00:01:24,445 --> 00:01:29,215
We'll see how AI can recognize patterns
in workload behavior, optimize for

25
00:01:29,215 --> 00:01:34,080
cost and performance, and reduce the
need for manual tuning, et cetera.

26
00:01:34,460 --> 00:01:38,005
By the end of this talk, you'll
understand how intelligent automation

27
00:01:38,005 --> 00:01:41,755
can futureproof your cloud infrastructure
and unlock new efficiencies.

28
00:01:42,580 --> 00:01:44,280
Let's proceed today.

29
00:01:44,340 --> 00:01:50,460
I outlined my presentation in such a way
that to take you on a journey where we'll

30
00:01:50,460 --> 00:01:54,240
start by understanding the core challenges
in Kubernetes resource management.

31
00:01:54,960 --> 00:01:58,920
Then move to why deep reinforcement
learning is promising solution.

32
00:01:59,280 --> 00:02:04,470
Next, I'll share a research
foundations and specific optimization

33
00:02:04,470 --> 00:02:08,850
areas we targeted, followed by
look at the RL algorithms we

34
00:02:08,850 --> 00:02:10,350
used and how we implement them.

35
00:02:11,160 --> 00:02:14,850
We'll discuss the unique challenges
of multi cluster environments,

36
00:02:15,000 --> 00:02:19,890
the C-N-F-C-F tools to support
our approach and how we validated

37
00:02:19,980 --> 00:02:21,540
our results through benchmarking.

38
00:02:22,350 --> 00:02:27,600
Finally, I will cover production
deployment considerations and

39
00:02:27,600 --> 00:02:31,695
outline practical next steps
so you can see how these ideas

40
00:02:31,695 --> 00:02:33,735
translate into real world impact.

41
00:02:34,485 --> 00:02:38,205
While we dive into some
technical details like algorithms

42
00:02:38,205 --> 00:02:39,525
and integration strategies.

43
00:02:40,080 --> 00:02:43,200
I'll make sure to make you
better understanding on that.

44
00:02:43,680 --> 00:02:48,090
Whether you are a data scientist, DevOps
engineer, or a cloud architect, you'll

45
00:02:48,090 --> 00:02:52,890
live with both a deeper understanding of
the technology and concrete ideas where

46
00:02:52,890 --> 00:02:55,230
you can apply in your own environments.

47
00:02:56,190 --> 00:02:58,140
Let's discuss about the challenges.

48
00:02:58,640 --> 00:02:59,840
Let's imagine a scenario.

49
00:03:00,470 --> 00:03:04,090
Your company launches a new
product and suddenly thousands

50
00:03:04,090 --> 00:03:05,560
of user floods your website.

51
00:03:06,340 --> 00:03:09,280
The Kubernetes cluster configured
with static resource limits

52
00:03:09,280 --> 00:03:11,080
can't scale up fast enough.

53
00:03:11,740 --> 00:03:15,520
Some services crash due to lack
of resources while others are

54
00:03:15,520 --> 00:03:18,880
running with excess capacity
leading to wasted cloud spend.

55
00:03:19,630 --> 00:03:21,610
This is inges a theoretical problem.

56
00:03:22,000 --> 00:03:25,090
It's something many organizations
face during peak events, sale

57
00:03:25,590 --> 00:03:26,850
or unexpected viral moments.

58
00:03:27,350 --> 00:03:31,790
When these pipes happen, DevOps team are
forced to jump in and manually adjust

59
00:03:31,790 --> 00:03:35,360
configurations often late at night
and during critical business hours.

60
00:03:36,170 --> 00:03:39,770
This firefighting takes time away
from innovation and strategic work

61
00:03:39,800 --> 00:03:41,360
and increases the risk of human error.

62
00:03:42,050 --> 00:03:45,650
Manual tuning is not only
stressful, but also unsustainable.

63
00:03:45,890 --> 00:03:49,610
As system grow more complex and
workloads become unpredictable.

64
00:03:50,110 --> 00:03:53,230
These challenges are widespread,
and they're exactly what we

65
00:03:53,230 --> 00:03:57,310
are aiming to solve with this
intelligent and adaptive solutions.

66
00:03:57,810 --> 00:04:01,860
Now, why deep reinforcement
learning traditional orders curling

67
00:04:01,860 --> 00:04:05,670
in Kubernetes relies on fixed
thresholds and reactive policies?

68
00:04:06,210 --> 00:04:09,810
When a metric crosses a certain
timeline, the system responds, but

69
00:04:09,810 --> 00:04:13,235
it doesn't anticipate or leave from
past, learn from past behavior.

70
00:04:13,735 --> 00:04:17,425
In contrast, deep reinforcement
learning enables Kubernetes to

71
00:04:17,425 --> 00:04:19,375
continuously learn from its environment.

72
00:04:19,735 --> 00:04:22,945
Our agents observe the pattern,
predict future needs, and

73
00:04:22,945 --> 00:04:24,865
proactively adjust resources.

74
00:04:25,555 --> 00:04:29,665
They shift from reactive to adaptive
management, means cluster can handle

75
00:04:29,875 --> 00:04:33,510
unpredictable workloads more efficiently
with less manual intervention.

76
00:04:34,010 --> 00:04:38,684
RL algorithms excel at uncovering
complex relationships in data.

77
00:04:38,999 --> 00:04:43,349
Patterns that are often too subtle are
multidimensional for humans to spot.

78
00:04:43,739 --> 00:04:47,909
For example, a L can detect
cyclical traffic spikes, seasonal

79
00:04:47,939 --> 00:04:50,909
trends, or correlations between
different microservices that

80
00:04:50,909 --> 00:04:52,319
static rules would overlook.

81
00:04:52,919 --> 00:04:56,969
By learning from historical data and
realtime feedback, AL can optimize

82
00:04:56,969 --> 00:05:00,779
resource allocations in a ways that
manual tuning simply can't achieve it.

83
00:05:01,279 --> 00:05:05,299
Reinforcement learning has already
revolutionized fields like robotics,

84
00:05:05,389 --> 00:05:08,719
where agents learn to navigate
complex environments and gaming.

85
00:05:09,169 --> 00:05:15,079
Where AI has mastered games like Go
and StarCraft, these successes show

86
00:05:15,139 --> 00:05:19,249
RLS power to solve dynamic, high
dimensional problems just like those

87
00:05:19,249 --> 00:05:20,959
we face in cloud infrastructure.

88
00:05:21,649 --> 00:05:25,789
By bringing R to Kubernetes, we
are leveraging proven AI techniques

89
00:05:25,789 --> 00:05:27,529
to make our clusters smart.

90
00:05:28,099 --> 00:05:30,139
More resilient and more cost effective.

91
00:05:30,639 --> 00:05:33,129
Now, let's discuss on
some research foundations.

92
00:05:33,399 --> 00:05:36,669
Our approach was inspired by several
pioneering studies in the field

93
00:05:36,669 --> 00:05:39,849
of cloud resource optimization
using reinforcement learning.

94
00:05:40,179 --> 00:05:44,889
For example, research from leading
universities and tech companies has

95
00:05:44,889 --> 00:05:48,969
shown RL can outperform traditional
auto scaling in dynamic environments.

96
00:05:49,854 --> 00:05:53,544
Benchmarks from these studies
demonstrated significant improvements

97
00:05:53,544 --> 00:05:58,949
in CP utilization and response times,
which motivated us to explore RL

98
00:05:58,949 --> 00:06:00,714
for Kubernetes workload management.

99
00:06:01,644 --> 00:06:05,214
We also looked at open source RL
frameworks and their application

100
00:06:05,214 --> 00:06:07,944
in real world cloud scenarios
to guide our methodology.

101
00:06:08,814 --> 00:06:12,029
One of the most powerful
aspects of RL is its ability to

102
00:06:12,029 --> 00:06:13,554
continuously learn and adapt.

103
00:06:14,094 --> 00:06:18,684
Unlike static policies, RL agents evolve
as workloads and infrastructure change.

104
00:06:19,224 --> 00:06:22,854
This means your Kubernetes
cluster can stay optimized even

105
00:06:22,854 --> 00:06:24,414
as new applications are deployed.

106
00:06:24,714 --> 00:06:27,984
Traffic patterns shipped
or hardware is upgraded.

107
00:06:28,484 --> 00:06:32,384
Continuous learning helps future proof
your infrastructure and reducing the need

108
00:06:32,384 --> 00:06:36,974
for frequent manual reconfiguration and
keeping your systems resilient to change.

109
00:06:37,574 --> 00:06:41,269
During our experiments, we found that
RL driven clusters not only improve.

110
00:06:41,894 --> 00:06:45,944
Resource utilization, but also reduce the
number of scaling events, making resource

111
00:06:45,944 --> 00:06:47,834
management more stable and predictable.

112
00:06:48,554 --> 00:06:48,794
One.

113
00:06:48,794 --> 00:06:52,844
A surprising result was the ability
of RL agents to anticipate traffic

114
00:06:52,844 --> 00:06:54,484
surges before they happened.

115
00:06:54,844 --> 00:06:57,874
Thanks to the pattern recognition
in from historical data.

116
00:06:58,374 --> 00:07:03,244
In literature, there are cases where
RL reduced cloud costs by up to 40%

117
00:07:03,244 --> 00:07:06,574
compared to traditional autoscaling
highlighting the real world impact

118
00:07:06,604 --> 00:07:08,224
of intelligent optimization.

119
00:07:08,724 --> 00:07:11,574
So what are the core optimization areas?

120
00:07:11,814 --> 00:07:14,424
Let's break down the core
optimization areas where machine

121
00:07:14,424 --> 00:07:15,894
learning can make a real impact.

122
00:07:16,394 --> 00:07:17,684
First POD scheduling.

123
00:07:17,954 --> 00:07:21,764
For example, ML can predict which
notes will have the right resource

124
00:07:21,764 --> 00:07:23,834
available and place pods accordingly.

125
00:07:23,984 --> 00:07:27,974
Reducing bottlenecks and improving
performance resource allocation.

126
00:07:28,064 --> 00:07:32,144
Instead of fixed to CPU and memory
assignments, ML models can dynamically

127
00:07:32,144 --> 00:07:36,194
adjust resource based on predicted
demand preventing both W and under

128
00:07:36,194 --> 00:07:38,384
provisioning traffic routing.

129
00:07:38,774 --> 00:07:42,794
In a service, mesh Amil can optimize
routing decisions in real time,

130
00:07:42,824 --> 00:07:46,454
sending requests to the healthiest
or most cost effective endpoints.

131
00:07:47,024 --> 00:07:51,674
Multi cluster orchestration, Amil
can help distribute workloads across

132
00:07:51,674 --> 00:07:55,964
clusters in different regions or clouds,
balancing load, and minimizing late

133
00:07:55,994 --> 00:07:58,964
density by automating complex decisions.

134
00:07:59,024 --> 00:08:03,344
ML driven optimization reduces the need
for manual tuning and constant oversight.

135
00:08:03,844 --> 00:08:07,474
This means DevOps team can focus
on strategic tasks rather than

136
00:08:07,474 --> 00:08:09,064
fire fright, resource issues.

137
00:08:09,754 --> 00:08:13,084
Efficiency improves because
resources are allocated precisely

138
00:08:13,144 --> 00:08:14,494
when and where they are needed.

139
00:08:14,554 --> 00:08:19,864
Minimizing, wasted and maximizing
performance, real-time feedback is

140
00:08:19,864 --> 00:08:22,084
crucial for effective optimization.

141
00:08:22,984 --> 00:08:25,864
ML models rely on continuous
monitoring of cluster metrics

142
00:08:25,864 --> 00:08:27,034
and application performance.

143
00:08:27,874 --> 00:08:32,734
This feedback loop allows the system to
quickly adapt to changing conditions such

144
00:08:32,734 --> 00:08:35,104
as traffic spikes or hardware failures.

145
00:08:35,764 --> 00:08:41,344
Ultimately, realtime feedback ensures that
optimization decision remain in relevant

146
00:08:41,374 --> 00:08:43,534
and effective even as workloads evolve.

147
00:08:44,034 --> 00:08:47,060
Let's see how deep Q networks
for containers orchestration.

148
00:08:47,560 --> 00:08:52,104
DQ networks or dqn are particularly
effective for problems where decisions are

149
00:08:52,104 --> 00:08:57,174
discreet, such as choosing which node to
place a PO or selecting a resource tire.

150
00:08:57,894 --> 00:09:00,744
In Kubernetes, many resource
management actions are not

151
00:09:00,744 --> 00:09:03,624
continuous, but involve clear choice.

152
00:09:04,074 --> 00:09:07,339
DQN can evaluate the current state
of the cluster and select the best

153
00:09:07,399 --> 00:09:09,139
action from a set of possibilities.

154
00:09:09,639 --> 00:09:13,479
This makes DQNA natural fit
for orchestration, container

155
00:09:13,479 --> 00:09:16,359
placement, and scaling decisions
in a dynamic environment.

156
00:09:17,259 --> 00:09:20,679
One of the biggest challenges in
applying DQN to Kubernetes was

157
00:09:20,709 --> 00:09:24,069
designing a reward function that
truly reflects the business goals.

158
00:09:24,399 --> 00:09:28,809
For example, we had to balance
performance, cost, and reliability.

159
00:09:29,409 --> 00:09:33,039
If the reward only focused on
CP utilization, it might ignore

160
00:09:33,099 --> 00:09:34,839
response time and efficiency.

161
00:09:35,299 --> 00:09:36,259
Cost efficiency.

162
00:09:37,189 --> 00:09:41,299
We experimented with multi object to
reward functions and found that waiting

163
00:09:41,299 --> 00:09:46,359
different metrics appropriately was key
to achieve the decide outcome experience.

164
00:09:46,359 --> 00:09:48,129
Relay is a technique, sorry.

165
00:09:48,189 --> 00:09:53,169
Experience replay is a technique where
the RL agent stores past experiences and

166
00:09:53,169 --> 00:09:55,119
samples then randomly during training.

167
00:09:55,619 --> 00:09:59,489
This helps break the correlation
between co experiences making

168
00:09:59,489 --> 00:10:01,349
learning more stable and robust.

169
00:10:01,709 --> 00:10:04,679
Especially in dynamic environments
like Kubernetes, where

170
00:10:04,679 --> 00:10:06,299
conditions can change rapidly.

171
00:10:07,019 --> 00:10:10,529
By revisiting a diverse set of
scenarios, the agent learns more

172
00:10:10,529 --> 00:10:14,399
generalizable policies and avoids
over fitting to the recent events.

173
00:10:14,899 --> 00:10:19,709
Proximal policy optimization in
action PPO, it is also known for its

174
00:10:19,709 --> 00:10:23,489
stability during training, which is
crucial when making continuous resource

175
00:10:23,489 --> 00:10:25,319
allocation decisions in Kubernetes.

176
00:10:25,819 --> 00:10:29,659
Unlike algorithms that can make
abrupt changes, PPO uses a clip

177
00:10:29,719 --> 00:10:33,889
objective function to ensure updates
to the policy are gradual and safe.

178
00:10:34,519 --> 00:10:39,199
This stability means resource adjustments
like CPU and memory allocation are smooth

179
00:10:39,199 --> 00:10:43,669
and predictable, reducing the risk of
performance spikes or resource starvation.

180
00:10:44,299 --> 00:10:48,439
It enables regular benchmarking
and AB testing to ensure the RL

181
00:10:48,439 --> 00:10:51,439
agents decisions are actually
improving the cluster perform.

182
00:10:52,429 --> 00:10:56,029
We track key metrics such as
application response time, resource

183
00:10:56,029 --> 00:10:59,839
utilization, and cost efficiency
before and after policy changes.

184
00:11:00,709 --> 00:11:04,879
Regular benchmarking and AB testing
help us validate that the agent is

185
00:11:04,879 --> 00:11:08,029
learning effectively and not just
over fitting to the recent data.

186
00:11:08,839 --> 00:11:12,859
Safe exploration is critical in production
environments and our agent must try

187
00:11:12,859 --> 00:11:16,464
new actions to learn, but reckless
experimentation can disrupt services.

188
00:11:16,964 --> 00:11:21,644
We use techniques like reward, shaping,
circuit breakers and canary DI deployments

189
00:11:21,674 --> 00:11:23,804
to limit the impact of risky decisions.

190
00:11:24,314 --> 00:11:27,164
This ensures that learning
continues without compromising

191
00:11:27,164 --> 00:11:29,264
reliability or user experience.

192
00:11:29,764 --> 00:11:32,674
Soft factor critic for multi
objective optimization.

193
00:11:33,174 --> 00:11:38,064
SAC is a state of art, deep reinforcement
learning algorithm designed for

194
00:11:38,064 --> 00:11:39,954
environments where actions are continuous.

195
00:11:40,749 --> 00:11:44,829
It is especially valued for its ability
to balance exploration and exploitation,

196
00:11:45,549 --> 00:11:49,899
making it highly effective for complex
real world optimization problems like

197
00:11:50,469 --> 00:11:55,449
Bernet, workload management, entropy,
regularization ease, A technique used in

198
00:11:55,449 --> 00:12:00,579
SAC to encourage the RL agent to explore
a wider range of actions rather than

199
00:12:00,579 --> 00:12:02,679
settling too quickly on a single strategy.

200
00:12:03,519 --> 00:12:06,969
In cloud environments, workloads
and resource demands can change

201
00:12:06,969 --> 00:12:08,559
rapidly and unpredictability.

202
00:12:09,059 --> 00:12:12,959
Exploration helps the agent discover
new, potentially better resource

203
00:12:12,959 --> 00:12:17,459
allocation strategies that static
or overlay conservative approaches

204
00:12:17,459 --> 00:12:22,199
might miss by maintaining a balance
between exploitation and exploration.

205
00:12:22,319 --> 00:12:26,579
SAC ensures the system remains
adapt, adaptable, and can

206
00:12:26,579 --> 00:12:28,199
respond to novel situations.

207
00:12:28,829 --> 00:12:33,359
One of S's strength is its ability to
handle multiple objectives simultaneously.

208
00:12:33,859 --> 00:12:37,579
In Kubernetes, we often need to
optimize for cost, performance,

209
00:12:37,639 --> 00:12:39,679
and reliability All at once.

210
00:12:40,369 --> 00:12:44,869
SSE allows us to define a reward function
that incorporates these different goals.

211
00:12:44,959 --> 00:12:49,759
So the agent learns to make decisions
that doesn't, that don't just maximize

212
00:12:49,849 --> 00:12:51,889
one metric at the expense of others.

213
00:12:52,549 --> 00:12:56,839
This holistic approach leads to smarter
resource management, where clusters

214
00:12:56,839 --> 00:13:01,099
run efficiently, applications stay
responsive and cause TER control.

215
00:13:01,599 --> 00:13:05,409
In our experiments, we found that
SAC was particularly effective in

216
00:13:05,409 --> 00:13:09,339
scenarios with highly variable workloads
and complex resource requirements.

217
00:13:09,579 --> 00:13:14,049
For example, during a multi-service
deployment with fluctuating traffic,

218
00:13:14,259 --> 00:13:18,489
SAC consistently achieved lower response
times and better cost efficiency.

219
00:13:18,489 --> 00:13:24,819
Compared to DQN and PPO, its ability to
adapt to continuous change and optimize.

220
00:13:25,209 --> 00:13:28,599
Across multiple objectives made
it the best choice for our most

221
00:13:28,599 --> 00:13:31,029
demanding cloud native applications.

222
00:13:31,529 --> 00:13:34,879
Integration with Kubernetes
controllers integration.

223
00:13:34,909 --> 00:13:37,819
Integrating reinforcement
learning models with Kubernetes.

224
00:13:37,819 --> 00:13:40,279
Controllers present several challenges.

225
00:13:40,519 --> 00:13:42,204
One major issue is latency.

226
00:13:43,009 --> 00:13:47,809
RL agents needs timely data to make
decisions, but delays in metric collection

227
00:13:47,809 --> 00:13:49,939
or a PA calls can impact performance.

228
00:13:49,984 --> 00:13:52,564
API Compatibility is another hurdle.

229
00:13:53,549 --> 00:13:57,664
Kubernetes APIs evolve and custom
controllers must stay up to date to

230
00:13:57,664 --> 00:14:01,564
ensure smooth communication between
the RL agent and the cluster.

231
00:14:02,434 --> 00:14:06,934
We also had to consider security
and access controls, making sure RL

232
00:14:06,934 --> 00:14:12,754
agents only perform safe authorized
actions within the cluster Custom

233
00:14:12,754 --> 00:14:17,434
Controllers act as translators
between the RL models and Kubernetes.

234
00:14:18,244 --> 00:14:22,504
They colored cluster metrics, feed them
to RL agent, and then convert the agent's

235
00:14:22,504 --> 00:14:24,664
recommended actions into Kubernetes.

236
00:14:24,664 --> 00:14:29,554
API calls this architecture allows
us to leverage the intelligence of

237
00:14:29,554 --> 00:14:33,604
RL while maintaining compatibility
with existing Kubernetes workflows.

238
00:14:34,414 --> 00:14:38,224
Controllers also handle error
shaking and rollback procedures,

239
00:14:38,404 --> 00:14:42,844
ensuring that any action taken by
the RL agent is safe and reversible.

240
00:14:43,819 --> 00:14:48,709
Monitoring oral driven actions is
critical for both safety and transparency.

241
00:14:49,309 --> 00:14:53,479
We learned that deep integration with
existing observability tools like

242
00:14:53,719 --> 00:14:58,579
promises and Grafana helps track the
impact of oral decisions in real time.

243
00:14:59,359 --> 00:14:59,809
Debugging.

244
00:14:59,809 --> 00:15:04,129
Oral agents can be tricky, especially
when decisions seem counter initiative.

245
00:15:04,699 --> 00:15:08,719
We found that logging details, state
information and reward calculations

246
00:15:08,719 --> 00:15:10,819
were essential for diagnosing issues.

247
00:15:11,419 --> 00:15:11,989
Regular audits.

248
00:15:12,649 --> 00:15:16,789
And automated alerts for unusual
behavior helped us catch problems

249
00:15:16,849 --> 00:15:18,739
early and maintain trust in the system.

250
00:15:19,239 --> 00:15:22,509
Let's discuss about some multi
cluster environment challenges.

251
00:15:23,259 --> 00:15:26,409
Synchronizing state across
multiple Kubernetes clusters

252
00:15:26,409 --> 00:15:27,849
is a complex challenge.

253
00:15:28,389 --> 00:15:32,319
Each cluster may be running in a
different region or different hardware,

254
00:15:32,409 --> 00:15:34,059
or even in different cloud providers.

255
00:15:34,884 --> 00:15:38,034
Ensuring that all clusters have
a consistent view of workloads,

256
00:15:38,094 --> 00:15:40,434
resource usage, and policies request.

257
00:15:40,434 --> 00:15:42,594
Robust coordination mechanisms.

258
00:15:43,314 --> 00:15:47,574
We found that network partitions, version
mismatches and a synchronous updates

259
00:15:47,574 --> 00:15:49,674
can easily lead to inconsistencies.

260
00:15:49,734 --> 00:15:53,184
Making reliable
synchronization a top priority.

261
00:15:54,174 --> 00:15:57,414
Latency is a critical factor
in multi cluster environments.

262
00:15:57,849 --> 00:16:02,769
Decision about workload placement or
scaling must account for the time it

263
00:16:02,769 --> 00:16:04,839
takes to communicate between the clusters.

264
00:16:05,319 --> 00:16:08,799
High latency can delay resource
adjustments leading to suboptimal

265
00:16:08,799 --> 00:16:11,529
performance are even service disruptions.

266
00:16:12,099 --> 00:16:15,639
Our approach includes latency
awareness, algorithms that factor in

267
00:16:15,639 --> 00:16:19,749
network delays, helping ensure timely
and effective resource management

268
00:16:19,809 --> 00:16:21,549
across distributed clusters.

269
00:16:22,089 --> 00:16:25,269
Federated learning offers a
promising solution for scaling

270
00:16:25,269 --> 00:16:27,014
reinforcement learning across clusters.

271
00:16:27,514 --> 00:16:31,594
Instead of sharing raw data, each
cluster train its own oral agent

272
00:16:31,744 --> 00:16:34,324
locally and only shares model updates.

273
00:16:34,865 --> 00:16:38,854
This approach preserves data
privacy and security as sensitive

274
00:16:38,854 --> 00:16:40,834
information never leaves the cluster.

275
00:16:41,464 --> 00:16:46,044
Federated learning also enables
collaborative optimization, allowing

276
00:16:46,044 --> 00:16:49,949
clusters to benefit from shared
insights while maintaining autonomy

277
00:16:50,154 --> 00:16:52,524
and compliance with data regulations.

278
00:16:53,024 --> 00:16:56,294
So implementation of
framework with CNCF tools.

279
00:16:56,794 --> 00:16:58,024
CNCF tools.

280
00:16:58,774 --> 00:17:02,734
CLOUDNATIVE Computing Foundation
tools are open source projects and

281
00:17:02,734 --> 00:17:07,294
technologies hosted by Cloud Native
Compute Foundation, designed to

282
00:17:07,294 --> 00:17:11,494
support cloud native infrastructure,
application development, and operations.

283
00:17:12,484 --> 00:17:17,464
These tools help organizations build,
deploy, and manage scalable, resilient,

284
00:17:17,464 --> 00:17:19,354
and observable cloud native systems.

285
00:17:19,654 --> 00:17:23,944
I have listed below some
common CNFC tools promises.

286
00:17:24,064 --> 00:17:28,084
It is essential for RL training
because it provides a comprehensive

287
00:17:28,084 --> 00:17:32,974
and real time view of cluster
metrics, CPU memory network uses,

288
00:17:33,034 --> 00:17:34,804
and customer application metrics.

289
00:17:35,464 --> 00:17:38,914
These rich data streams allow
the RL agents to make informed

290
00:17:38,914 --> 00:17:42,034
decisions and continuously learn
from the actual state of cluster.

291
00:17:42,859 --> 00:17:47,270
Without accurate and granular metrics,
RL models will be flying blind, unable

292
00:17:47,270 --> 00:17:51,500
to adapt to changing workloads or
optimize resource allocation effectively.

293
00:17:52,000 --> 00:17:56,470
Istio as a service mesh gives us
fine grain control over traffic

294
00:17:56,470 --> 00:17:58,330
routing between microservices.

295
00:17:59,080 --> 00:18:03,190
By integrating RL with TO, we can
dynamically adjust routing policies

296
00:18:03,220 --> 00:18:05,530
based on real-time performance feedback.

297
00:18:06,100 --> 00:18:09,280
Sending requests to the healthiest
endpoints are balancing load

298
00:18:09,280 --> 00:18:10,865
to optimize latency and cost.

299
00:18:11,365 --> 00:18:16,405
This level of automation helps maintain
high availability and responsiveness even

300
00:18:16,405 --> 00:18:18,835
as traffic patterns shift unexpectedly.

301
00:18:19,335 --> 00:18:23,415
Helm, it simplifies and
standardizes the deployment of RL

302
00:18:23,415 --> 00:18:27,495
optimized workloads by packaging
configurations into reusable charts.

303
00:18:28,185 --> 00:18:31,575
This reduces manual errors,
speeds up rollouts, and ensures

304
00:18:31,575 --> 00:18:33,615
consistency across environments.

305
00:18:34,125 --> 00:18:37,275
Whether you are deploying to a
single cluster or multiple region.

306
00:18:37,335 --> 00:18:41,825
With Herm updates and rollbacks are
straightforward, making it easier to

307
00:18:41,825 --> 00:18:46,355
manage complex deployments and maintain
reliability as you iterate on oral models.

308
00:18:46,855 --> 00:18:49,524
Let's talk about the performance
benchmarking strategy.

309
00:18:50,245 --> 00:18:54,774
Benchmarking is essential to
objectively measure the impact

310
00:18:54,774 --> 00:18:56,425
of R driven optimization.

311
00:18:56,875 --> 00:19:00,564
By comparing cluster performance
before and after oral deployment,

312
00:19:00,895 --> 00:19:04,675
we can quantify improvements and
identify areas for further tuning.

313
00:19:05,175 --> 00:19:09,584
It is important to establish a baseline
using traditional auto-scaling methods

314
00:19:09,645 --> 00:19:14,415
so we have a clear reference point
for evaluating our ALS effectiveness.

315
00:19:14,985 --> 00:19:19,004
Consistent benchmarking also helps
ensure that changes are beneficial

316
00:19:19,064 --> 00:19:20,804
and don't introduce new issues.

317
00:19:21,405 --> 00:19:26,625
In our RL optimist clusters achieved a
30% reduction in application response

318
00:19:26,625 --> 00:19:28,395
time compared to static auto scaling.

319
00:19:29,145 --> 00:19:34,185
We also observed higher CP utilization
and fewer unnecessary scaling evens.

320
00:19:34,665 --> 00:19:38,865
Which translated into better resource
efficiency and lower cloud costs.

321
00:19:39,585 --> 00:19:42,615
These concrete results demonstrate
the real world value of

322
00:19:42,645 --> 00:19:44,565
intelligent workload optimization.

323
00:19:45,375 --> 00:19:47,955
Benchmarking isn't just about numbers.

324
00:19:48,645 --> 00:19:50,115
It's about building trust.

325
00:19:50,475 --> 00:19:54,524
Stakeholders need to see clear,
repeatable evidence that RL

326
00:19:54,524 --> 00:19:56,235
automation is safe and effective.

327
00:19:56,735 --> 00:20:00,784
Transparent reporting of performance
metrics, REASSURES teams that the

328
00:20:00,784 --> 00:20:04,685
system is making smart decisions
and not compromising reliability.

329
00:20:05,185 --> 00:20:07,885
Regular benchmarking and
sharing results with the team.

330
00:20:08,185 --> 00:20:12,895
Foster confidence in adopting R driven
solutions for critical workloads.

331
00:20:13,395 --> 00:20:17,385
Now let's consider some production
deployment con considerations

332
00:20:17,715 --> 00:20:20,700
as we move from research and
benchmarking to real world deployment.

333
00:20:21,314 --> 00:20:23,534
Safety and reliability become paramount.

334
00:20:24,165 --> 00:20:29,354
First, we implement safety mechanisms
like circuit breakers, where these

335
00:20:29,354 --> 00:20:33,705
prevent RL agents from making
destructive or risky decisions

336
00:20:34,215 --> 00:20:36,135
during training or model updates.

337
00:20:36,495 --> 00:20:39,675
If the agent starts to behave
unexpectedly, the circuit breaker

338
00:20:39,824 --> 00:20:42,794
Hals its actions protecting
the cluster from instability.

339
00:20:43,695 --> 00:20:46,455
Next, we use a gradual rollout strategy.

340
00:20:47,205 --> 00:20:50,925
Instead of deploying new RL policies
cluster where we start with Canary,

341
00:20:50,925 --> 00:20:54,855
Dell deployments rolling out changes
to a small subset of workloads.

342
00:20:55,365 --> 00:20:59,175
If performance degrades, automated
rollback mechanisms quickly revert

343
00:20:59,175 --> 00:21:00,764
to the previous stable policy.

344
00:21:00,794 --> 00:21:04,335
Minimizing risk model
versioning is also critical.

345
00:21:04,784 --> 00:21:08,534
We maintain a comprehensive versioning
and AB testing framework allows

346
00:21:08,534 --> 00:21:11,865
us to compare the performance of
different RL models side by side.

347
00:21:12,600 --> 00:21:16,650
This ensures that only the best performing
models are promoted to production.

348
00:21:17,190 --> 00:21:20,070
Finally, observability
integration is essential.

349
00:21:20,760 --> 00:21:24,210
We deeply integrate with existing
monitoring tasks such as promises and

350
00:21:24,210 --> 00:21:28,860
Grafana to audit RL agent decisions
and model performance in real time.

351
00:21:29,340 --> 00:21:34,410
This transparency built trust and allows
for rapid troubleshooting if issue arises.

352
00:21:35,070 --> 00:21:39,360
In summary, by combining safety
mechanisms, gradual rollout, robust

353
00:21:39,360 --> 00:21:41,250
versioning, and strong observability.

354
00:21:41,625 --> 00:21:45,645
We ensure that RL driven optimization
is not only effective, but also

355
00:21:45,645 --> 00:21:49,095
safe and reliable for production,
kubernetes and environments.

356
00:21:49,595 --> 00:21:50,195
Next steps.

357
00:21:50,225 --> 00:21:52,835
What are the next steps for
implementing RL Optimiz?

358
00:21:52,835 --> 00:21:53,170
Optimiz Kubernetes?

359
00:21:53,670 --> 00:21:58,230
When adopting RL driven optimization,
it's wise to begin with pilot projects

360
00:21:58,230 --> 00:22:02,870
targeting noncritical workloads,
and this approach allows you to

361
00:22:02,870 --> 00:22:06,200
validate the benefits and iron
out any issues before scaling up.

362
00:22:06,700 --> 00:22:12,460
Pilot project help teams gain hand-on
experience with AL tools and workflows,

363
00:22:12,670 --> 00:22:14,350
building confidence in the technology.

364
00:22:15,040 --> 00:22:19,480
By starting small, you minimize risk
and ensure that any expect unexpected

365
00:22:19,480 --> 00:22:22,990
challenges can be addressed without
impacting core business operations.

366
00:22:23,800 --> 00:22:27,910
Before deploying AL solutions, it's
crucial to establish baseline metrics

367
00:22:27,910 --> 00:22:32,950
for your current system, such as resource
utilization, response times, and cost.

368
00:22:33,925 --> 00:22:37,915
These baselines provides a reference point
for measuring improvements and help you

369
00:22:37,915 --> 00:22:40,585
set realistic goals for RL optimization.

370
00:22:41,085 --> 00:22:44,294
Tracking metrics before and
after implementation, ensure that

371
00:22:44,294 --> 00:22:47,715
changes are data driven, and that
you can clearly demonstrate the

372
00:22:47,715 --> 00:22:49,574
value of RL two stakeholders.

373
00:22:50,074 --> 00:22:53,464
The Kubernetes and RL communicates
are vibrant and collaborative.

374
00:22:53,964 --> 00:22:57,654
Sharing your learnings, challenges and
successes can accelerate progress for

375
00:22:57,654 --> 00:23:01,664
everyone contributing to open source
a frameworks or Kubernetes tools.

376
00:23:02,354 --> 00:23:05,414
Not only help others, but also
brings valuable feedback and

377
00:23:05,414 --> 00:23:06,599
innovation to your own projects.

378
00:23:07,099 --> 00:23:09,739
Community collaboration
fosters best practices.

379
00:23:10,444 --> 00:23:13,894
Drives new features and helps
build a more robust ecosystem for

380
00:23:13,894 --> 00:23:15,694
intelligent workload optimization.

381
00:23:16,194 --> 00:23:20,309
So what are the key takeaways
from this to wrap up?

382
00:23:20,639 --> 00:23:23,699
RL Optimist Kubernetes
delivers three major benefits.

383
00:23:24,479 --> 00:23:28,349
Efficiency resources are used
more effectively, reducing

384
00:23:28,529 --> 00:23:29,819
waste and lowering costs.

385
00:23:29,939 --> 00:23:31,589
The second one is adaptability.

386
00:23:31,979 --> 00:23:35,219
The system learns and responds
to changing workloads, keeping

387
00:23:35,249 --> 00:23:39,209
applications running smoothly, even
during unexpected spikes or dips.

388
00:23:39,959 --> 00:23:42,049
Third, it's a future proofing rls.

389
00:23:42,049 --> 00:23:44,959
Continuous learning means your
infrastructure can evolve with your

390
00:23:44,959 --> 00:23:48,019
business needs, new technologies,
and shifting user demands.

391
00:23:48,739 --> 00:23:50,419
So successful adoption.

392
00:23:50,659 --> 00:23:53,929
Adoption of RL in Kubernetes
isn't a one step process.

393
00:23:54,319 --> 00:23:58,129
It's best approached in phases, starting
with pilot projects, establishing

394
00:23:58,129 --> 00:24:01,939
baseline metrics, and gradually
expanding to production workloads.

395
00:24:02,689 --> 00:24:07,489
This phased strategy helps manage
risk, build team expertise, and

396
00:24:07,489 --> 00:24:11,599
ensure that each step delivers
measurable value before scaling up.

397
00:24:12,139 --> 00:24:16,399
Ultimately, our optimist, Kubernetes
is more than just a technical upgrade.

398
00:24:16,969 --> 00:24:18,199
It's a strategic advantage.

399
00:24:18,799 --> 00:24:22,759
It empowers organization to be more
agile, resilient, and cost effective in

400
00:24:22,759 --> 00:24:24,949
a rapidly changing digital landscape.

401
00:24:25,639 --> 00:24:29,209
By embracing intelligent
automation, you position your

402
00:24:29,239 --> 00:24:33,169
infrastructure and your business for
long-term success and innovation.

403
00:24:33,889 --> 00:24:36,049
With this, I conclude my presentation.

404
00:24:36,649 --> 00:24:37,219
Thank you.

