1
00:00:00,500 --> 00:00:03,470
Speaker 5: Hello everyone and
thank you for joining this session.

2
00:00:04,310 --> 00:00:07,879
My name is Anci DeMelo and I
work for Ford Motor Company.

3
00:00:08,379 --> 00:00:13,209
My main focus is on taking machine
learning from lab into reliable

4
00:00:13,209 --> 00:00:15,579
production system at Enterprise Scale.

5
00:00:16,450 --> 00:00:20,229
Today I want to talk about a
journey Many of you are on.

6
00:00:20,729 --> 00:00:25,109
Moving from experimentation to
enterprise by building scalable,

7
00:00:26,009 --> 00:00:29,459
observable ML ops systems on GCP.

8
00:00:29,960 --> 00:00:33,950
Over the past few years, we
have all seen the same pattern.

9
00:00:34,940 --> 00:00:40,285
Teams can train impressive models
and notebooks or in isolated POC.

10
00:00:41,150 --> 00:00:45,800
But when it comes time to run
those models reliably in production

11
00:00:46,220 --> 00:00:51,230
24 by seven under real business
constraint, things get much harder.

12
00:00:52,220 --> 00:00:56,840
The promise of ML driven
insights often collide with harsh

13
00:00:56,840 --> 00:00:58,990
operational realities in this talk.

14
00:00:59,875 --> 00:01:05,155
I will walk through the key production ml,
challenges that block enterprise value,

15
00:01:05,155 --> 00:01:13,085
a practical end-to-end ML ops framework
on Google Cloud, how we use managed

16
00:01:13,085 --> 00:01:20,454
CCP services like verex, ai, data flow,
cloud run cloud monitoring, and BigQuery.

17
00:01:20,954 --> 00:01:27,044
Deployment patterns, observability
foundation and governance practices

18
00:01:27,254 --> 00:01:30,464
that make ML systems reliable at scale.

19
00:01:30,964 --> 00:01:34,294
My goal is that you leave
with a mental blueprint.

20
00:01:34,744 --> 00:01:39,844
You can adapt in your organization,
whether you are just starting with

21
00:01:39,844 --> 00:01:44,224
ML ops or trying to standardize
around a more robust pattern.

22
00:01:44,724 --> 00:01:46,524
The production ML challenge.

23
00:01:47,024 --> 00:01:48,644
Let's start with the reality.

24
00:01:48,649 --> 00:01:49,679
Many of us live in.

25
00:01:50,179 --> 00:01:51,829
Production ML is hard.

26
00:01:52,609 --> 00:01:57,320
On this slide, I am highlighting
the core problem moving machine

27
00:01:57,320 --> 00:02:02,419
learning from experimentation into
reliable, large scale production

28
00:02:03,289 --> 00:02:07,729
remains one of the most critical
challenges organization face today.

29
00:02:08,229 --> 00:02:12,549
We can build a great model, but
the infrastructure is complex.

30
00:02:13,090 --> 00:02:14,440
Tooling is fragmented.

31
00:02:15,220 --> 00:02:16,960
Models degrade silently.

32
00:02:17,739 --> 00:02:20,679
Governance requirements
keep getting stricter.

33
00:02:21,579 --> 00:02:23,499
I like to think of it this way.

34
00:02:24,189 --> 00:02:27,969
In the lab, the model
is a star in production.

35
00:02:28,089 --> 00:02:29,320
The system is a star.

36
00:02:30,130 --> 00:02:35,260
You are no longer optimizing just
for the model accuracy, but for

37
00:02:35,260 --> 00:02:38,859
availability, latency, cause.

38
00:02:39,534 --> 00:02:44,334
Auditability and the ability
to evolve safely over time.

39
00:02:44,834 --> 00:02:48,554
If we do not address this
operational realities, the

40
00:02:48,554 --> 00:02:50,265
business will not feel sustained.

41
00:02:50,765 --> 00:02:54,995
And the value for the ML initiatives,
no matter how good your offline

42
00:02:54,995 --> 00:02:58,075
matrix look, will not be sufficed.

43
00:02:59,035 --> 00:03:01,975
So let's break down the main barriers.

44
00:03:02,475 --> 00:03:03,524
Core barriers.

45
00:03:04,024 --> 00:03:09,025
On this slide, four core barriers that
repeatedly show up when enterprise try

46
00:03:09,025 --> 00:03:11,575
to operationalize machine learning.

47
00:03:12,445 --> 00:03:14,605
First, infrastructure complexity.

48
00:03:15,355 --> 00:03:20,485
We are managing distributed system,
orchestrating workflows and maintaining

49
00:03:20,545 --> 00:03:22,950
compute resources across development.

50
00:03:23,650 --> 00:03:29,860
And production environments, it is easy to
end up with bespoke clusters, handcrafted

51
00:03:29,860 --> 00:03:35,050
pipelines, and snowflake deployments
that are fragile and hard to debug.

52
00:03:35,550 --> 00:03:37,350
Second fragmented tooling.

53
00:03:38,130 --> 00:03:44,095
Many teams are stitching together
different tools for training, deployment,

54
00:03:44,225 --> 00:03:50,270
monitoring, and governance, often with
every weak integration between them.

55
00:03:50,770 --> 00:03:55,600
The result is duplicated work,
inconsistent practices and system

56
00:03:55,960 --> 00:03:57,340
that are difficult to observe.

57
00:03:57,550 --> 00:03:58,030
End to end.

58
00:03:58,530 --> 00:04:06,120
Third, model degradation data, drift
concept drift, and feature queue silently.

59
00:04:06,145 --> 00:04:09,025
Ero, model performance over time.

60
00:04:09,525 --> 00:04:15,615
You might ship a model that looks
greatly initially, but six months

61
00:04:15,615 --> 00:04:21,315
later, your input distribution labels
and your user behavior has shifted

62
00:04:21,795 --> 00:04:24,495
and your business KPI start to suffer.

63
00:04:25,395 --> 00:04:27,525
Finally, governance requirements.

64
00:04:28,095 --> 00:04:34,155
Most enterprise must meet compliance
mandates, ensure auditability and

65
00:04:34,245 --> 00:04:38,385
implement human oversight while
still maintaining enough operational

66
00:04:38,385 --> 00:04:40,605
velocity to ship improvements.

67
00:04:41,325 --> 00:04:47,355
Regulations around privacy, fairness,
and safety are only increasing,

68
00:04:47,685 --> 00:04:52,800
and our ML system must be able to
prove what they did when and why.

69
00:04:53,300 --> 00:04:57,110
The framework I will present is
designed specifically to address these

70
00:04:57,110 --> 00:05:02,770
four barriers in a cohesive way, a
production grade ML ops framework on GCP.

71
00:05:03,760 --> 00:05:06,040
Now, let's look at the
high level framework.

72
00:05:06,970 --> 00:05:10,575
This talks cohesive
end-to-end system built on.

73
00:05:11,364 --> 00:05:15,234
GCP platform and addresses
this challenges heads on.

74
00:05:16,044 --> 00:05:20,424
The central idea here is instead of
building everything from scratch,

75
00:05:20,814 --> 00:05:26,004
we compose managed services into our
integrated architecture that gives

76
00:05:26,004 --> 00:05:31,904
us scalability, observability, and
governance out of the box at a high level.

77
00:05:31,964 --> 00:05:37,034
The framework covers data ingestion
and the feature pipelines, model

78
00:05:37,034 --> 00:05:39,074
training and experiment management.

79
00:05:39,900 --> 00:05:44,520
Deployment patterns for batch,
real time and streaming workloads.

80
00:05:45,270 --> 00:05:49,439
Production observability for
both models and infrastructure.

81
00:05:49,939 --> 00:05:55,570
CICD, driven model deployment, governance,
compliance, and human oversight.

82
00:05:56,070 --> 00:06:01,920
Each piece is designed to work with
others so we can standardize patterns

83
00:06:02,010 --> 00:06:05,219
and reduce one-off bespoke solutions.

84
00:06:05,719 --> 00:06:10,099
In the next few slides, I will walk
through the GCP service stack and

85
00:06:10,099 --> 00:06:12,020
how it supports this framework,

86
00:06:12,520 --> 00:06:14,229
GCP managed Service stack.

87
00:06:15,039 --> 00:06:21,280
Here you see the core GCP services
we use as building blocks TEX ai.

88
00:06:21,820 --> 00:06:28,539
This is our unified ML platform for
training, deploying, and managing models.

89
00:06:29,530 --> 00:06:34,870
It gives us built-in experiment
tracking and pipeline orchestration,

90
00:06:35,289 --> 00:06:41,910
so data scientists can focus on model
logic rather than plumbing data flow.

91
00:06:42,360 --> 00:06:46,290
This is a managed service for
stream and batch processing.

92
00:06:47,040 --> 00:06:51,469
We use it for real time
feature computation and for

93
00:06:51,469 --> 00:06:53,210
orchestrating data pipelines.

94
00:06:53,750 --> 00:06:58,969
Which is critical when you, your models
depend on fresh, consistent features.

95
00:06:59,469 --> 00:07:06,250
Cloud run, this is serverless compute
for containerized model serving with

96
00:07:06,789 --> 00:07:10,030
automatic scaling and pay per use pricing.

97
00:07:10,810 --> 00:07:18,045
It lets us deploy inference services
that scale up under load and scale down.

98
00:07:18,660 --> 00:07:24,570
When I did without managing servers
cloud monitoring ring, this is our

99
00:07:24,570 --> 00:07:29,880
observability platform for matrix
logs, traces, and custom model

100
00:07:30,090 --> 00:07:32,100
health indicators across the system.

101
00:07:32,600 --> 00:07:37,250
It is a backbone for our
dashboard and alerts BigQuery.

102
00:07:37,670 --> 00:07:42,325
This is our serverless data warehouse
used for feature engineering.

103
00:07:43,130 --> 00:07:47,210
Model training data and prediction
logging at petabyte scale.

104
00:07:47,710 --> 00:07:52,090
It is also where we do a lot
of offline analysis of model

105
00:07:52,090 --> 00:07:53,560
performance and the drift.

106
00:07:54,060 --> 00:07:59,520
By standardizing on this service
stack, we can give teams a clear,

107
00:07:59,670 --> 00:08:05,095
opinionated path, you know where
to put data, where to train, where

108
00:08:05,095 --> 00:08:07,044
to deploy, and where to monitor.

109
00:08:07,945 --> 00:08:09,174
This is the clarity.

110
00:08:09,674 --> 00:08:14,224
That reduces the friction and
the acceleration, adoption,

111
00:08:14,724 --> 00:08:19,334
deployment patterns, architectural
trade off, selecting the right approach.

112
00:08:19,949 --> 00:08:22,889
Different workload demand,
different deployment strategies.

113
00:08:23,099 --> 00:08:26,669
And this slide illustrates the
key patterns and their trade-off.

114
00:08:27,169 --> 00:08:29,479
Real time inference priorities.

115
00:08:29,719 --> 00:08:34,519
Subsecond latency for immediate
decision making, fraud detection,

116
00:08:34,519 --> 00:08:36,829
recommendation, personalized pricing.

117
00:08:37,489 --> 00:08:40,579
Here we care deeply about
latency, percentiles

118
00:08:40,999 --> 00:08:42,799
availability, and auto scaling.

119
00:08:42,799 --> 00:08:46,569
Behavior Batch processing
optimizes throughout.

120
00:08:47,019 --> 00:08:54,669
For the offline scoring, nightly,
nightly risk scores, weekly

121
00:08:54,699 --> 00:08:59,589
churn prediction, monthly flow
forecasting latency is less critical.

122
00:09:00,089 --> 00:09:04,589
But efficiency and reliability
at high volume matter a lot.

123
00:09:05,519 --> 00:09:10,349
Streaming patterns set in between
balancing real-time responsiveness

124
00:09:10,829 --> 00:09:12,329
with continuous analysis.

125
00:09:12,719 --> 00:09:16,124
For example, continuously
updating risk scores at events.

126
00:09:16,624 --> 00:09:21,519
As events arrive, your choice depends
on business constraints such as latency

127
00:09:21,519 --> 00:09:26,889
requirements, prediction, volume, cost
tolerance, and operational complexity.

128
00:09:27,819 --> 00:09:32,709
Each pattern uses GCP services
slightly differently, but because

129
00:09:32,709 --> 00:09:37,649
we keep the same stack, we can
reuse governance, observability, and

130
00:09:37,649 --> 00:09:39,809
deployment practices across all of them.

131
00:09:40,769 --> 00:09:44,699
Let me make this more concrete
with a real automotive example,

132
00:09:45,299 --> 00:09:47,369
a vehicle oil change prognostics.

133
00:09:47,869 --> 00:09:53,179
Traditionally, we change engine
oil based on our mileage or time.

134
00:09:53,974 --> 00:09:56,914
Like every 5,000 miles or six months.

135
00:09:57,394 --> 00:10:03,514
That rule is simple, but it ignores how
the vehicle is actually used in reality.

136
00:10:03,544 --> 00:10:05,164
Stop and go city.

137
00:10:05,164 --> 00:10:10,744
Driving frequent short trips or towing
a trailer can degrade oil much faster.

138
00:10:11,134 --> 00:10:12,784
Then high highway cruising.

139
00:10:13,284 --> 00:10:19,104
With an ML driven prognostic system,
we stream telemetry from the vehicle.

140
00:10:19,944 --> 00:10:25,314
Engine temperature profiles, RPM
patterns, trip duration, load

141
00:10:25,314 --> 00:10:29,844
condition, driving environment, and
historical maintenance outcomes.

142
00:10:30,624 --> 00:10:35,814
A model learns to predict the
remaining useful life of the oil for

143
00:10:35,814 --> 00:10:40,254
each specific vehicle, rather than
applying one size fits all rule.

144
00:10:40,754 --> 00:10:43,874
On Google Cloud that looks like this.

145
00:10:43,874 --> 00:10:48,554
Data flow, ingest and aggregate
streaming sensor data.

146
00:10:49,514 --> 00:10:52,664
BigQuery stores historical
trips and maintenance records.

147
00:10:53,264 --> 00:10:57,464
Vertex AI trains models to
predict optimal oil change.

148
00:10:57,464 --> 00:11:01,834
Window and Google Cloud
run host the real time.

149
00:11:02,334 --> 00:11:07,284
Inference service that responds
when a driver starts a trip.

150
00:11:08,124 --> 00:11:13,674
Cloud monitoring tracks both system
health and model outputs, so we can

151
00:11:13,674 --> 00:11:18,444
see, for example, how many vehicles
are being flagged as change oil

152
00:11:18,444 --> 00:11:24,089
soon, and how this correlates with
actual shop visits and engine health.

153
00:11:24,719 --> 00:11:28,854
This is a great illustration of
why deployment patterns matter.

154
00:11:29,514 --> 00:11:30,714
We are streaming.

155
00:11:31,214 --> 00:11:36,554
Data source, realtime recommendation
to the driver and batch retraining jobs

156
00:11:36,974 --> 00:11:42,044
that periodically refresh the model
as new maintenance outcomes arrive

157
00:11:42,544 --> 00:11:44,524
Production observability foundation.

158
00:11:45,244 --> 00:11:48,694
Now I want to zoom into
observability, which is absolutely

159
00:11:48,694 --> 00:11:51,894
critical for production without
comprehensive observability.

160
00:11:52,394 --> 00:11:55,934
Production machine learning
systems operate in darkness.

161
00:11:56,084 --> 00:12:00,734
You might see business KPI degrading
and not know if the cause is

162
00:12:00,734 --> 00:12:04,784
data drift, a bug in the feature
generation or infrastructure

163
00:12:04,784 --> 00:12:06,914
issue, or change in user behavior.

164
00:12:07,414 --> 00:12:12,004
A robust observability strategy
combines model level telemetry and

165
00:12:12,004 --> 00:12:13,684
infrastructure level telemetry.

166
00:12:14,584 --> 00:12:19,774
On the model side, we monitor things like
input distribution, output distribution,

167
00:12:20,014 --> 00:12:25,469
and performance against ground truth when
labels arrive on the infrastructure side.

168
00:12:25,984 --> 00:12:32,104
We monitor uptime, error rates, latency,
percentiles, and auto-scaling behavior.

169
00:12:33,034 --> 00:12:38,284
The goal is complete visibility into
system health and performance, so we

170
00:12:38,284 --> 00:12:44,394
can detect degradation early and respond
quickly, ideally before customers notice.

171
00:12:44,394 --> 00:12:47,939
In the next two slide, I will
separate this two layers.

172
00:12:48,179 --> 00:12:51,709
Model monitoring and infrastructure
telemetry model monitoring.

173
00:12:52,209 --> 00:12:53,889
Detecting model degradation.

174
00:12:54,849 --> 00:12:59,349
On this slide, we look at model
monitoring techniques we rely on in

175
00:12:59,349 --> 00:13:02,589
production first, data drift detection.

176
00:13:03,519 --> 00:13:09,324
We run statistical test comparing incoming
feature distribution against a training

177
00:13:09,724 --> 00:13:14,844
baseline to identify population shifts
if the distribution of the key feature.

178
00:13:15,579 --> 00:13:16,839
Shift significantly.

179
00:13:17,439 --> 00:13:22,869
We know our model may be operating
out of regime in trained, and it was

180
00:13:22,869 --> 00:13:26,159
trained on second feature skew analysis.

181
00:13:26,659 --> 00:13:30,739
We monitor discrepancies between
training and serving feature values.

182
00:13:31,099 --> 00:13:36,974
This helps catch pipeline inconsistencies
such as transformation that that was

183
00:13:36,974 --> 00:13:42,734
applied offline but not online, or a bug
introduced in the new feature pipeline.

184
00:13:43,234 --> 00:13:45,444
Third prediction, drift tracking.

185
00:13:46,074 --> 00:13:52,074
We detect shifts in model output
distribution that may signal changing

186
00:13:52,074 --> 00:13:54,654
behavior patterns or model stateness.

187
00:13:55,254 --> 00:14:02,424
For example, if a classifies rate
suddenly changes or regresses output

188
00:14:02,634 --> 00:14:08,159
range, compresses or explodes, that
is a signal worth investigating.

189
00:14:08,659 --> 00:14:12,199
Finally, performance
degradation monitoring.

190
00:14:13,189 --> 00:14:20,089
When label become available, we track
matrix like accuracy, precision recall,

191
00:14:20,239 --> 00:14:23,089
and business KPIs against ground truth.

192
00:14:24,019 --> 00:14:28,424
This closes the loop and connects
model behavior to actual outcome.

193
00:14:28,924 --> 00:14:33,954
All of this signal feed into dashboard
and alerts so the team can respond

194
00:14:33,954 --> 00:14:38,664
systematically rather than relying
on ad HO checks or user complaints.

195
00:14:39,504 --> 00:14:43,944
If we go back to vehicle oil
change, prognostics example, model

196
00:14:43,944 --> 00:14:45,954
monitoring becomes very tangible.

197
00:14:46,454 --> 00:14:52,604
We monitor data drift by watching how the
distribution of driving patterns changes

198
00:14:52,634 --> 00:14:55,844
over seasons and or between the region.

199
00:14:56,594 --> 00:15:01,784
We track prediction drift by looking
at the proportion of vehicles

200
00:15:01,784 --> 00:15:04,389
flagged for early oil change.

201
00:15:05,049 --> 00:15:09,824
If that certainly spikes in one
fleet, it might signal a data

202
00:15:09,824 --> 00:15:12,704
issue or model that has gone still.

203
00:15:13,204 --> 00:15:18,754
When vehicles come in for services, we
compare the predicted remaining oil life

204
00:15:18,904 --> 00:15:23,914
with actual inspection results and feed
that label data back into our performance

205
00:15:23,914 --> 00:15:25,954
dashboard and retraining pipelines.

206
00:15:26,554 --> 00:15:30,484
This kind of feedback loop is what
keeps a production prognostic system

207
00:15:30,964 --> 00:15:35,584
trustworthy over time instead of
slowly degrading into a more expensive

208
00:15:35,584 --> 00:15:36,604
version of fixed mileage route.

209
00:15:37,104 --> 00:15:40,904
Infrastructure, elementary and
system health model monitoring

210
00:15:40,904 --> 00:15:42,404
is only half the story.

211
00:15:42,854 --> 00:15:46,624
The other half is infrastructure
level, observability surfaces,

212
00:15:47,524 --> 00:15:52,024
operational issues that degrade user
experience regardless of model quality.

213
00:15:52,524 --> 00:15:58,254
Using cloud monitoring, we watch matrix
such as target availability, uptime,

214
00:15:58,434 --> 00:16:05,064
error rates, and service health across all
inference endpoints, latency thresholds,

215
00:16:05,564 --> 00:16:10,689
response times to ensure we meet SLAs
for real-time workloads, auto-scaling

216
00:16:11,109 --> 00:16:17,489
efficiency, scaling behavior resources,
utilization, and cost optimization.

217
00:16:17,989 --> 00:16:18,949
Opportunities.

218
00:16:19,449 --> 00:16:28,059
For example, a spike in P 95 latency
might indicate cold start network issues

219
00:16:28,179 --> 00:16:30,549
or contention in shared dependency.

220
00:16:31,049 --> 00:16:34,649
By looking at both model and
infrastructure telemetry,

221
00:16:35,129 --> 00:16:36,719
we can quickly distinguish.

222
00:16:36,899 --> 00:16:37,439
Model is wrong.

223
00:16:38,294 --> 00:16:43,154
From system is slow and route
incidents to the right owners.

224
00:16:44,054 --> 00:16:48,164
We saw the value of this setup clearly
in oil life, prognostics incident.

225
00:16:48,664 --> 00:16:53,014
Our operational dashboard suddenly
showed that in one region the share

226
00:16:53,044 --> 00:17:00,424
of vehicles flagged for change of
oil soon had almost doubled while

227
00:17:00,424 --> 00:17:02,524
infrastructure metrics stayed healthy.

228
00:17:03,024 --> 00:17:07,344
The daily evaluation pipeline with
joins prediction with service outcomes

229
00:17:07,764 --> 00:17:12,234
confirmed the model had become
overly conservative there without

230
00:17:12,234 --> 00:17:14,334
improving engine health results.

231
00:17:15,054 --> 00:17:19,944
Drift plots revealed that driving
patterns had changed due to many

232
00:17:19,944 --> 00:17:24,019
new short urban trips that were
underrepresented in the training data.

233
00:17:24,519 --> 00:17:30,100
We raised an incident, retrained the
model on the new data and rolled it

234
00:17:30,100 --> 00:17:36,699
out via our CICD pipeline watching
the dashboard as early change.

235
00:17:37,449 --> 00:17:41,414
Recommendation dropped back to normal
levels while outcome stayed stable.

236
00:17:41,914 --> 00:17:43,649
CICD Driven model deployment.

237
00:17:44,149 --> 00:17:49,189
Next, let's talk about how we ship
models to production safely and

238
00:17:49,189 --> 00:17:54,100
reputably production machine learning
requires the same rigor as software

239
00:17:54,100 --> 00:17:59,979
engineering, versioning, and automated
testing, progressive rollouts and

240
00:17:59,979 --> 00:18:02,080
quick rollout back capabilities.

241
00:18:02,560 --> 00:18:08,409
A mature CICD pipeline ensures every
model deployment is reproducible,

242
00:18:09,070 --> 00:18:11,110
auditable, and reversible.

243
00:18:11,610 --> 00:18:14,879
The deployment pipeline
architecture on this slide shows

244
00:18:14,939 --> 00:18:16,709
a progressive rollout strategy.

245
00:18:17,099 --> 00:18:23,459
A typical flow looks like this,
training validation model, fit checks,

246
00:18:23,579 --> 00:18:27,479
and cross validation to ensure the
candidate model is competitive.

247
00:18:27,979 --> 00:18:33,110
Automated testing, unit test,
integration test, and performance test

248
00:18:33,169 --> 00:18:35,179
on the model and the serving code.

249
00:18:35,679 --> 00:18:42,070
Canary deployment, releasing a small
slice of traffic, often around 5%,

250
00:18:42,219 --> 00:18:48,959
and intensive monitoring, progressive
low rollouts, expanding to five first,

251
00:18:49,019 --> 00:18:54,719
then 25, then 50, and then a hundred of
traffic if health checks remain green.

252
00:18:55,219 --> 00:18:59,269
Monitoring and rollback,
if we detect degradation.

253
00:18:59,929 --> 00:19:01,009
Automated rules.

254
00:19:01,429 --> 00:19:07,849
Or on-call engineers trigger a
fast rollback to previous version.

255
00:19:08,349 --> 00:19:13,209
This progressive deployment strategy
minimizes risk for exposing new models

256
00:19:14,049 --> 00:19:19,239
to increasing traffic volume while
continuously monitoring performance.

257
00:19:19,764 --> 00:19:24,219
It allows team to move quickly
without gambling the entire

258
00:19:24,279 --> 00:19:26,439
user experience on each release.

259
00:19:26,939 --> 00:19:29,699
Governance, retraining
and human oversight.

260
00:19:30,629 --> 00:19:33,179
Finally, we have
governance and compliance.

261
00:19:33,509 --> 00:19:37,889
Which often make or break ML
initiatives in regulated environments.

262
00:19:38,609 --> 00:19:44,159
Automated retraining pipelines keep
models fresh as data distribution evolve.

263
00:19:45,029 --> 00:19:49,619
However, regulated environments
require human inner loop approval

264
00:19:50,249 --> 00:19:54,749
gates to satisfy compliance mandates
and manage operational risk.

265
00:19:55,589 --> 00:20:00,209
In the framework shown here, we
see scheduled retraining triggers.

266
00:20:00,569 --> 00:20:06,779
An automated validation against
holdout dataset for low risk

267
00:20:07,049 --> 00:20:10,319
updates that pass all checks.

268
00:20:11,129 --> 00:20:14,434
We can auto approve and deploy
through our CI I CD pipeline.

269
00:20:14,934 --> 00:20:18,744
For high risk changes, same
models impacting credit decision

270
00:20:19,194 --> 00:20:21,204
or safety critical systems.

271
00:20:21,564 --> 00:20:27,204
We route approvals to domain expert who
review performance fairness matrix and the

272
00:20:27,204 --> 00:20:30,534
documentation before promoting the model

273
00:20:31,034 --> 00:20:36,699
Governance controls, auditability
versioning, and rollback model versioning.

274
00:20:37,199 --> 00:20:42,839
Immutable artifacts with semantic
versions linked to training, data code

275
00:20:42,929 --> 00:20:45,569
and hyper parameters to reproducibility.

276
00:20:46,069 --> 00:20:48,954
Governance controls, auditability
versioning, and rollback

277
00:20:49,299 --> 00:20:50,859
governance controls support.

278
00:20:50,979 --> 00:20:52,929
This processes model versioning.

279
00:20:53,429 --> 00:20:58,469
Immutable artifacts for semantic version
linked to training, data code and

280
00:20:58,469 --> 00:21:00,929
hyper parameters for reproducibility.

281
00:21:01,679 --> 00:21:05,969
Audit logging, comprehensive
logs, capturing who deployed,

282
00:21:06,209 --> 00:21:11,189
which model, when with what
approval, and what the outcome was.

283
00:21:12,119 --> 00:21:17,849
Instant rollback, one click rollback to
previous model version with automated

284
00:21:17,849 --> 00:21:20,459
traffic shifting and health verification.

285
00:21:20,959 --> 00:21:27,820
Policy and enforcement automated checks to
ensure models meet fairness, privacy, and

286
00:21:27,820 --> 00:21:29,949
security requirements before deployment.

287
00:21:30,760 --> 00:21:36,760
This control ensure we cannot only
operate at scale, but also answer

288
00:21:36,909 --> 00:21:42,189
tough questions for regulators,
auditors, and internal stakeholders.

289
00:21:42,689 --> 00:21:43,620
Let me close by zooming back out.

290
00:21:44,120 --> 00:21:49,939
This framework provides a comprehensive
battle tested approach to building

291
00:21:50,149 --> 00:21:55,850
reliable, maintainable, and observable
machine learning system on Google Cloud.

292
00:21:56,510 --> 00:22:02,685
By combining managed services with robust
observability governance and deployment

293
00:22:02,685 --> 00:22:07,754
practices, organization can finally
bridge the gap between experimental

294
00:22:07,814 --> 00:22:11,175
ML and reliable enterprise production.

295
00:22:12,060 --> 00:22:16,380
If you are starting on this journey,
my suggestion is do not try to

296
00:22:16,380 --> 00:22:18,270
be perfect at everything at once.

297
00:22:18,900 --> 00:22:25,140
Pick one or two high value use cases,
standardize on a small ated service stack,

298
00:22:25,410 --> 00:22:33,060
and invest early in observability and CICD
as those patents prove successful, scale

299
00:22:33,060 --> 00:22:36,930
them out across more teams and domains.

300
00:22:37,430 --> 00:22:41,120
Thank you so much for your time and
attention today, and thanks for the

301
00:22:41,120 --> 00:22:43,055
conference for giving me this chance.

