1
00:00:01,000 --> 00:00:01,600
Hi everyone.

2
00:00:01,960 --> 00:00:04,270
Good morning, good afternoon,
good evening, wherever the

3
00:00:04,270 --> 00:00:05,410
location that you're joining.

4
00:00:05,410 --> 00:00:11,410
In the session, we are gonna talk about
how can we get a single pane of view of

5
00:00:11,440 --> 00:00:16,090
all the telemetry data that collected
in the enterprise digital landscape.

6
00:00:16,540 --> 00:00:21,110
How can we get a unified view of
all the entire IT landscape so that.

7
00:00:21,529 --> 00:00:25,279
We can easily, quickly detect the
problems and we don't have to switch

8
00:00:25,279 --> 00:00:29,510
between the tools and see what is
affecting the performance and stability

9
00:00:29,565 --> 00:00:34,554
of an IT systems so that we can
spend more time in innovating rather

10
00:00:34,554 --> 00:00:36,385
than finding where the problem is.

11
00:00:36,894 --> 00:00:39,495
So in this session, we are
gonna talk about open source

12
00:00:39,495 --> 00:00:42,944
tool, like Open Telemetry and
also using the Grafana stack.

13
00:00:42,944 --> 00:00:44,864
How can we achieve this unified already?

14
00:00:45,764 --> 00:00:46,064
Okay.

15
00:00:46,064 --> 00:00:46,664
Let's dive into.

16
00:00:46,910 --> 00:00:49,030
What are these unified
help can we achieve?

17
00:00:50,810 --> 00:00:51,950
Quick introduction about me.

18
00:00:52,130 --> 00:00:57,010
I am, I'm a co-founder of Gohar Tech,
where we provide SRE as a service.

19
00:00:57,669 --> 00:01:01,699
I also have our 20 years of IT
experience where performed various roles,

20
00:01:01,699 --> 00:01:05,389
architecture consulting, performance
engineering, set up SRE landscape

21
00:01:05,720 --> 00:01:07,710
at SRE team for big enterprises.

22
00:01:08,365 --> 00:01:12,285
Also we've got the patent in risk
management, risk assessment with respect

23
00:01:12,285 --> 00:01:13,965
to reliability and change management.

24
00:01:14,505 --> 00:01:16,135
Also, one more patent filed.

25
00:01:16,915 --> 00:01:20,885
I'm also an active speaker in the
Chaos Con, so also contribute to

26
00:01:20,885 --> 00:01:22,315
open source community as well.

27
00:01:24,405 --> 00:01:24,705
Okay.

28
00:01:24,805 --> 00:01:27,595
Let's directly get into the
evaluation of observability.

29
00:01:27,805 --> 00:01:31,725
So we talk about monitoring in the
earlier days, but now we started

30
00:01:31,725 --> 00:01:32,925
talking about observability.

31
00:01:33,495 --> 00:01:35,475
The first version of observability.

32
00:01:35,535 --> 00:01:36,795
It's like foundation for all.

33
00:01:36,795 --> 00:01:38,460
I. Modern application development.

34
00:01:38,550 --> 00:01:41,410
And this really helps to
understand the system behavior.

35
00:01:41,980 --> 00:01:44,640
When we write a code we have
an infrastructure there are

36
00:01:44,640 --> 00:01:46,020
middleware components in it.

37
00:01:46,260 --> 00:01:50,430
So this really helps us to gain some
visibility into the IT infrastructure

38
00:01:50,470 --> 00:01:51,730
and the application as well.

39
00:01:52,420 --> 00:01:53,800
So what are the characteristics?

40
00:01:53,860 --> 00:01:57,515
And the components like metrics is
one of the key aspect of observability

41
00:01:57,515 --> 00:02:01,715
on 0.0 where metrics is like
more mostly a time series data.

42
00:02:02,020 --> 00:02:08,730
For example, like a latency or throughput
or error or even the backend system or

43
00:02:08,730 --> 00:02:12,690
infrastructure, CPU, resource utilization,
memory disk, and so on and so forth.

44
00:02:14,495 --> 00:02:15,275
Traces.

45
00:02:15,335 --> 00:02:19,205
This really helps us to troubleshoot
any of these latency or error

46
00:02:19,205 --> 00:02:22,275
problems call tree kind of view.

47
00:02:22,335 --> 00:02:26,985
Where, when there, in the run time, when
there is a transaction or an API or task

48
00:02:26,985 --> 00:02:30,315
getting executed, what are the different
methods and the different services?

49
00:02:30,315 --> 00:02:32,334
What are the things being
actually called that?

50
00:02:32,394 --> 00:02:34,825
How much time it took, how
many times being called?

51
00:02:35,059 --> 00:02:37,584
So this really helps to
capture those information.

52
00:02:38,589 --> 00:02:39,429
And logs.

53
00:02:39,519 --> 00:02:42,879
It's a traditional way where both
system level logs or application level

54
00:02:42,879 --> 00:02:47,464
logs, where whatever the information
we wanna share to the analyzers

55
00:02:47,464 --> 00:02:50,885
or whatever the information that
system is spacing at this point.

56
00:02:50,885 --> 00:02:55,175
It could be informational troubleshooting
purposes are error are feral.

57
00:02:56,690 --> 00:02:57,470
The profiling.

58
00:02:57,650 --> 00:03:01,650
This is also getting added as one of the
signals in Absorbability earlier days.

59
00:03:01,980 --> 00:03:05,505
This was not is one of this is only
for applicable for lower environments

60
00:03:05,565 --> 00:03:09,815
or non-production environments where
the sampling is gonna, is very higher.

61
00:03:09,815 --> 00:03:13,235
So it's gonna affect the performance of
the system if we run in production scale.

62
00:03:13,715 --> 00:03:17,485
But as, as soon as that, after the
Google's dapper algorithm came in,

63
00:03:17,935 --> 00:03:20,815
our team actually found a really
how can we do it in a lightweight

64
00:03:20,935 --> 00:03:22,735
and for production scale system.

65
00:03:22,735 --> 00:03:26,935
So this, there's like advanced level
of tracing is this actually provide

66
00:03:26,935 --> 00:03:31,255
minute level of information in terms of
CPO, how the memory blocks are getting

67
00:03:31,255 --> 00:03:32,725
allocated from which line of code.

68
00:03:32,725 --> 00:03:35,485
So this really helps to solve
a lot of memory leak problems.

69
00:03:36,055 --> 00:03:38,965
So in, in a recent observer
teams, we see a continuous

70
00:03:38,965 --> 00:03:40,955
profiling with EVPF coming in.

71
00:03:41,045 --> 00:03:43,454
It captures all the
cannel level statistics.

72
00:03:43,454 --> 00:03:45,795
What are the things getting
executed and really helps to

73
00:03:45,795 --> 00:03:47,444
troubleshoot code level problems.

74
00:03:47,600 --> 00:03:49,784
I. User monitoring.

75
00:03:49,904 --> 00:03:52,774
So just to understand
instead of a synthetic.

76
00:03:52,774 --> 00:03:56,545
So just how the actual users are
facing the how their experiences

77
00:03:56,545 --> 00:03:57,684
are while using the system.

78
00:03:58,194 --> 00:04:01,694
So this really helps to understand
the user behavior and experience.

79
00:04:02,670 --> 00:04:04,319
Application performance management.

80
00:04:04,319 --> 00:04:05,309
This is there for a while.

81
00:04:05,309 --> 00:04:09,059
So which provides capability of
all these other things we saw.

82
00:04:09,449 --> 00:04:12,359
But this also play vital
role in getting the runtime.

83
00:04:12,410 --> 00:04:16,339
Apart from these, getting a runtime
visibility, if the Java application, how

84
00:04:16,339 --> 00:04:19,579
the Java heap utilization is, how many
threads are there, connection, pooling

85
00:04:19,579 --> 00:04:23,480
and all, if it's a goal and application,
how many sub proteins are span, but

86
00:04:23,480 --> 00:04:25,280
how the ang memory is being used.

87
00:04:25,339 --> 00:04:27,530
So it gives more
visibility apart from this.

88
00:04:28,309 --> 00:04:29,929
So now it can.

89
00:04:30,289 --> 00:04:34,810
Think about, we have all the relevant
signals are capabilities and observability

90
00:04:34,810 --> 00:04:37,930
1.0, but are we in this right path?

91
00:04:37,960 --> 00:04:39,820
Are we using observability 1.0?

92
00:04:40,000 --> 00:04:42,760
Let's see, what are the
characteristics of this?

93
00:04:44,350 --> 00:04:46,240
So we talked about all these things, but.

94
00:04:46,659 --> 00:04:50,949
The challenge here is that it's all
siloed tools, so we have to have

95
00:04:50,949 --> 00:04:55,619
multiple tools to measure individual
characteristics like metrics.

96
00:04:55,669 --> 00:04:59,830
We have to use separate tools, for
example, like Prometheus collector

97
00:04:59,830 --> 00:05:03,490
or our specific agent, which collects
inform metrics and so on, so forth.

98
00:05:03,520 --> 00:05:09,349
Traces like open tracing or open telemetry
just to collect our Eger or zipkin

99
00:05:09,380 --> 00:05:14,460
just to collect the traces alone and
inject it to the backend and the logs.

100
00:05:15,250 --> 00:05:18,580
Any framework which actually helps
to gather the logs and events.

101
00:05:19,010 --> 00:05:19,520
Profiling.

102
00:05:19,520 --> 00:05:23,780
And again, separate tools are separate
instrumentation, radio user monitoring,

103
00:05:23,780 --> 00:05:27,890
again, like amp for any JS application
or any other instrumentation or

104
00:05:27,890 --> 00:05:32,300
agentic base to collect the radio user
monitoring and the APMs a as well.

105
00:05:32,630 --> 00:05:34,460
So these are mostly silos.

106
00:05:34,460 --> 00:05:39,210
Some of them are club together
to get the unified view and

107
00:05:39,210 --> 00:05:40,560
mostly it's a reactive approach.

108
00:05:40,560 --> 00:05:44,050
So we usually capture these
after the fact of some of the

109
00:05:44,050 --> 00:05:45,790
incidental issues already started.

110
00:05:46,375 --> 00:05:48,595
And limited and no correlation.

111
00:05:48,625 --> 00:05:51,015
So that's a gap of this
observability on 0.0.

112
00:05:51,015 --> 00:05:54,585
And it leads a lot of manual analysis
because of the cellular tool.

113
00:05:54,585 --> 00:05:57,135
So we have to look multiple
tools to do a correlation and

114
00:05:57,135 --> 00:05:58,575
find out where the problem is.

115
00:05:59,265 --> 00:06:01,455
So now it get into deeper.

116
00:06:02,455 --> 00:06:04,255
Silo data we saw, I saw about it.

117
00:06:04,255 --> 00:06:08,785
It's really data silos really the
hindering of a ho holistic view.

118
00:06:08,785 --> 00:06:11,845
So we cannot get an end-to-end
picture even in the traces

119
00:06:11,845 --> 00:06:15,755
if you're not collecting the
data from across the system.

120
00:06:15,755 --> 00:06:18,915
So it'll still the manual
correlation needs to be done.

121
00:06:20,475 --> 00:06:21,955
And so limited insights.

122
00:06:22,045 --> 00:06:26,775
So if, even if we don't, instrument
any one of the component of the layer.

123
00:06:27,045 --> 00:06:29,355
So we will not get a complete
end-to-end visibility.

124
00:06:29,355 --> 00:06:32,295
So then if there is any problem
happens, it's hard to troubleshoot.

125
00:06:32,325 --> 00:06:33,345
It's a black box.

126
00:06:34,365 --> 00:06:39,745
So that's when the observability 2.0 was
born, where it really helps to elevate

127
00:06:39,745 --> 00:06:41,515
the insights in the digital ecosystem.

128
00:06:41,755 --> 00:06:46,615
So it, when there is a system with like
thousands of microservices really helps

129
00:06:46,615 --> 00:06:51,265
to understand where exactly the problem
is and how to troubleshoot and fix it.

130
00:06:52,475 --> 00:06:55,540
Where it come, whatever the
signals are, what are the

131
00:06:55,540 --> 00:06:57,220
characteristics of observing at 2.0?

132
00:06:57,220 --> 00:07:01,445
But it brings more context to the
traces where it, it gives a, where.

133
00:07:02,555 --> 00:07:05,915
This request is originated from,
so where we'll have a parent-child

134
00:07:05,915 --> 00:07:10,945
relationship and adds more attributes
or dimensions to those executions.

135
00:07:10,945 --> 00:07:14,815
So it really helps to find out if you
wanna get a trace of this particular

136
00:07:14,815 --> 00:07:16,345
user at this particular order.

137
00:07:16,795 --> 00:07:17,335
It'll be easy.

138
00:07:17,425 --> 00:07:20,245
The context really helps to find
out a little down those levels.

139
00:07:21,070 --> 00:07:21,760
And events.

140
00:07:22,300 --> 00:07:27,430
So logs mostly it's like unstructured
unless otherwise we follow some templates.

141
00:07:27,430 --> 00:07:32,890
Like we have access logs, have the Apache
format and but at the backend, most of

142
00:07:32,890 --> 00:07:37,330
the container logs, unless otherwise we
enforce it, it's gonna be like formatted.

143
00:07:37,690 --> 00:07:41,020
So that's when the events really
helps to do a key value pair with

144
00:07:41,020 --> 00:07:42,400
the formative structure of data.

145
00:07:42,790 --> 00:07:45,880
When there is a critical events
or change in the system happening,

146
00:07:45,880 --> 00:07:48,220
really helps to correlate the issues.

147
00:07:49,270 --> 00:07:51,490
Next, next level to the
real user monitoring, the

148
00:07:51,490 --> 00:07:52,810
user experience monitoring.

149
00:07:53,170 --> 00:07:57,940
So this really helps to understand
as soon as we receive the

150
00:07:57,940 --> 00:07:59,350
data, the UI receive the data.

151
00:07:59,380 --> 00:08:04,030
So how much time it takes to enter the
frame, and how much time it takes for each

152
00:08:04,030 --> 00:08:06,890
of those activity that you ui is doing.

153
00:08:06,890 --> 00:08:10,650
So how the, what is the real user
experience is when they do the

154
00:08:10,650 --> 00:08:14,620
action or when, app start time
mac crashes and handle exceptions.

155
00:08:16,150 --> 00:08:17,020
The data stream.

156
00:08:17,110 --> 00:08:22,810
This is another important with the recent
AI ml world where we deal with like

157
00:08:22,960 --> 00:08:27,380
terabytes and petabytes of data and also
with Kafka streams and all with a lot of

158
00:08:27,380 --> 00:08:31,610
events processing through, especially in
the iot kind of systems where how can we

159
00:08:31,670 --> 00:08:33,980
get deep insights and visibility into it.

160
00:08:33,980 --> 00:08:36,310
So this really gives a
separate perspective with that.

161
00:08:37,510 --> 00:08:38,350
Business metrics.

162
00:08:38,350 --> 00:08:43,570
This is another important, so other than
user experience, we also need to measure,

163
00:08:43,570 --> 00:08:47,950
get the business insights to it so we
can make some data driven decisions.

164
00:08:48,280 --> 00:08:50,830
So like how many artists being
made, how like different.

165
00:08:51,745 --> 00:08:55,615
The user is coming and viewing the
product, but what is a conversion rate?

166
00:08:55,615 --> 00:08:59,645
So whether he's gonna do a checkout
and make a price like order $2, how

167
00:08:59,645 --> 00:09:02,845
much time and how much percentage
of users getting converted.

168
00:09:02,845 --> 00:09:04,345
So those kind of critical metrics.

169
00:09:04,345 --> 00:09:07,945
So if there is any dropouts and what
is a reason for it, if there is any

170
00:09:07,945 --> 00:09:09,805
technical reason or is something else.

171
00:09:09,805 --> 00:09:11,695
So this really helps
to gain those insights.

172
00:09:12,625 --> 00:09:14,725
And alerts in the traditional way.

173
00:09:14,725 --> 00:09:20,155
We mostly set a threshold based, which is
not scalable and in a dynamic environment.

174
00:09:20,155 --> 00:09:23,695
So sometimes you tend to miss
some of these gaps, especially

175
00:09:23,695 --> 00:09:24,955
when the business metrics and all.

176
00:09:25,255 --> 00:09:29,155
So that's where the composite alarm
really helps to avoid these alert static

177
00:09:29,155 --> 00:09:31,855
and reduce the alerts to noise ratio.

178
00:09:32,125 --> 00:09:34,645
So really combine those when
there is an incident happen.

179
00:09:34,645 --> 00:09:37,555
So if multiple layers are
failing, so we're gonna not

180
00:09:37,555 --> 00:09:40,885
gonna have multiple alerts, it's
gonna combine and have a single.

181
00:09:41,040 --> 00:09:43,840
Alarm and really helps
to focus the problem.

182
00:09:45,940 --> 00:09:47,590
So with this, what are we gaining?

183
00:09:47,620 --> 00:09:51,310
The characteristics of 2.0 E helps us
to provide a single source of truth.

184
00:09:51,910 --> 00:09:55,870
Obviously, with the context and formatted
events, we can quickly troubleshoot

185
00:09:55,870 --> 00:09:58,000
or filter out only what we need.

186
00:09:58,890 --> 00:10:02,915
And really with the open telemetry
and open standards and the agents,

187
00:10:03,215 --> 00:10:07,445
we really have low code or no code
instrumentation to get the data.

188
00:10:08,075 --> 00:10:10,675
We'll see what what are those
instrumentation types, how to do

189
00:10:10,675 --> 00:10:14,895
it, and also open standards and
interoperability really helps to have

190
00:10:15,090 --> 00:10:17,070
a kind of no vendor locking situation.

191
00:10:17,070 --> 00:10:20,970
And it also, it's a cloud agnostic
where it can, it'll really

192
00:10:20,970 --> 00:10:24,270
support both on-premise or hybrid
or cloud native environments.

193
00:10:26,625 --> 00:10:29,595
And the absorb funnel I just
touched upon a little earlier.

194
00:10:29,595 --> 00:10:31,620
So when there is any.

195
00:10:32,530 --> 00:10:36,490
Digital ecosystem or digital
IT application runs on any

196
00:10:36,490 --> 00:10:38,200
underlying infrastructure platform.

197
00:10:38,200 --> 00:10:41,110
The absorbability really
helps to capture the capacity,

198
00:10:41,110 --> 00:10:42,610
utilization and saturation level.

199
00:10:42,760 --> 00:10:47,090
So this is the very foundation where
all the code gets executed in binaries

200
00:10:47,900 --> 00:10:50,360
and on top of it we'll have all these.

201
00:10:50,905 --> 00:10:55,195
Downstream systems where data re
resides, where we need to capture

202
00:10:55,195 --> 00:10:58,585
durability, consistency, integrity,
and also recovery aspects.

203
00:10:58,585 --> 00:11:03,625
If you want to have a high availability
solution, scalable solution, on top

204
00:11:03,625 --> 00:11:07,790
of it, we'll have a connector layer,
middleware layer, which actually needs to

205
00:11:07,790 --> 00:11:10,490
propagate the context and also provides.

206
00:11:10,815 --> 00:11:13,575
The resilience view, how the health
of the downstream, do we have a right

207
00:11:14,025 --> 00:11:16,425
circuit baker, a fallback retrace pattern.

208
00:11:16,425 --> 00:11:19,545
So this really helps to get that
perspective of it, that health

209
00:11:19,545 --> 00:11:22,995
of the backend, of downstream
systems on top of connector.

210
00:11:22,995 --> 00:11:26,535
Actual origin of these
business logic are a PA calls.

211
00:11:26,955 --> 00:11:31,155
We mostly measure the golden
signals, the latency throughput

212
00:11:31,335 --> 00:11:32,835
at a rate and saturation level.

213
00:11:34,500 --> 00:11:39,490
Real actual a PA consumers from the web
website or an app site to where it'll

214
00:11:39,490 --> 00:11:42,670
have a get the customer journey map.

215
00:11:42,670 --> 00:11:45,545
So what are the different activity
customers are doing so that we

216
00:11:45,545 --> 00:11:49,600
can get real actions happening in
the app in terms of business usage

217
00:11:49,600 --> 00:11:50,950
and also what users are seeing.

218
00:11:51,865 --> 00:11:55,375
And all the, apart from all these
technical stuff, the on top of

219
00:11:55,375 --> 00:11:58,645
it, the funnel, we have a business
metrics where we'll get clear

220
00:11:58,645 --> 00:12:01,975
visibility into the business
functions and what are the outcomes.

221
00:12:02,245 --> 00:12:07,675
So the idea for this funnel is when
there is a. Drop in the business, or

222
00:12:07,675 --> 00:12:12,085
when the user's complaining about some
of the things not working or not working

223
00:12:12,085 --> 00:12:17,185
as expected, then we need to correlate
to any one of these technical layers.

224
00:12:17,185 --> 00:12:18,895
So that is the intention of this funnel.

225
00:12:18,895 --> 00:12:23,275
So to have a mostly composite
metrics or alarms, and also

226
00:12:23,485 --> 00:12:27,445
to gain visibility of business
interruptions due to any technical

227
00:12:27,445 --> 00:12:29,155
failures or technical challenges.

228
00:12:32,005 --> 00:12:35,965
So this will definitely help us
to have a reliable, resilience,

229
00:12:35,965 --> 00:12:37,765
scalable and SE secure platform.

230
00:12:40,555 --> 00:12:43,945
So now let's get into how can we
achieve this single source of truth?

231
00:12:44,000 --> 00:12:49,690
The reference observability architecture,
I would say, where we have the target IT

232
00:12:49,695 --> 00:12:56,685
system where instrument using the no-code,
low-code agents are the SDKs, which.

233
00:12:57,460 --> 00:13:00,980
Send the telemetry signals to the
data collector in the periodic

234
00:13:00,980 --> 00:13:06,870
interval so the data collector will
receive and, converts and sends it

235
00:13:06,870 --> 00:13:10,650
to the data pipeline where this is
where actual data massaging happens.

236
00:13:10,650 --> 00:13:16,020
Where pipeline not only combines data,
add relevant additional context to it

237
00:13:16,070 --> 00:13:19,940
which cluster it is running on, and
additional masking if there is any

238
00:13:19,940 --> 00:13:21,755
sensitive PI information is there.

239
00:13:21,755 --> 00:13:22,670
We wanna mask it.

240
00:13:23,070 --> 00:13:27,780
And convert it into any other format where
the data lake accepts any common format.

241
00:13:27,840 --> 00:13:29,400
Like it doesn't accept key value there.

242
00:13:29,400 --> 00:13:31,230
It's a column, store column or store.

243
00:13:31,230 --> 00:13:32,280
It's a time series store.

244
00:13:32,760 --> 00:13:36,470
If you wanna split that or if you
want to convert that, it'll also

245
00:13:36,470 --> 00:13:37,910
happen during, in the pipeline site.

246
00:13:38,785 --> 00:13:43,625
Once the data getting converted and it's
ready to store, then it'll be pumped

247
00:13:43,625 --> 00:13:45,665
into enterprise telemetry data lake.

248
00:13:46,085 --> 00:13:49,615
So this is where all the
telemetry signals come and stored

249
00:13:49,615 --> 00:13:51,175
in a single source of truth.

250
00:13:51,205 --> 00:13:55,075
That's where we'll not have a different
perspective, different time zones all

251
00:13:55,075 --> 00:13:58,615
these problems that we usually face
Earlier, we'll have all the signals

252
00:13:58,615 --> 00:14:02,835
like metrics, traces, logs, events
profiling, even profiling data,

253
00:14:03,195 --> 00:14:06,885
and also all the front end business
insights data, user experience data.

254
00:14:07,515 --> 00:14:13,815
So this really helps us to visualize
so we can get a single pane of view.

255
00:14:14,205 --> 00:14:18,255
Not only that, but also helps
us to create a composite alarms.

256
00:14:18,355 --> 00:14:21,665
If there is any standard
DVR anomaly detected.

257
00:14:21,665 --> 00:14:24,380
Really it helps us to correlate
where the issue happens.

258
00:14:25,220 --> 00:14:29,570
And also really helps to run those
synthetics to make sure we try

259
00:14:29,570 --> 00:14:31,760
to capture if even user sees it.

260
00:14:31,760 --> 00:14:36,110
So if there any chance to capture
it earlier and also helps us to

261
00:14:36,110 --> 00:14:39,560
set the, define the SLO thresholds
and also helps to track those SLOs.

262
00:14:40,580 --> 00:14:43,010
And not only that, it also helps to.

263
00:14:43,695 --> 00:14:47,505
Make data recommendations, or since the
data is in a single place, it really

264
00:14:47,505 --> 00:14:52,665
helps to run these ML R Foundation
models to understand the data and

265
00:14:52,665 --> 00:14:54,585
really helps to solve the problems.

266
00:14:56,115 --> 00:14:58,055
Now let's talk about open telemetry.

267
00:14:58,185 --> 00:15:01,545
It's a open standard and it's
is, as everyone knows, a second

268
00:15:01,905 --> 00:15:05,985
project in CNCF, how does it really
help us to get the unified view?

269
00:15:07,505 --> 00:15:12,625
So this is cloud NATO and really helps
to avoid any vendor lacking situation.

270
00:15:12,625 --> 00:15:14,105
So once, we're all instrumented.

271
00:15:14,195 --> 00:15:16,775
So wherever we wanna store
the data really supports it.

272
00:15:17,405 --> 00:15:20,525
And it supports 20 different languages.

273
00:15:20,575 --> 00:15:22,450
For all the four different signals.

274
00:15:22,450 --> 00:15:26,320
They provide what is the later state
of individual programming languages.

275
00:15:28,165 --> 00:15:32,955
And instrumentation is where we, it's
really key where there are, we start out

276
00:15:32,955 --> 00:15:34,485
low code and no-code instrumentation.

277
00:15:34,485 --> 00:15:38,555
So open telemetry supports
most of the language commonly

278
00:15:38,555 --> 00:15:39,725
used languages like Golan.

279
00:15:40,440 --> 00:15:43,050
Java our python with auto instrumentation.

280
00:15:43,500 --> 00:15:47,960
But one challenge is auto instrumentation
really provides us all the coverage

281
00:15:47,960 --> 00:15:52,720
for all the HGTP modules or any
other GRPC based any calls that

282
00:15:52,770 --> 00:15:54,610
it makes between the se services.

283
00:15:54,610 --> 00:15:58,680
So it really helps to auto capture these
information like metrics, cases, and logs.

284
00:15:59,010 --> 00:16:03,190
But but if you want to have a
business insights or business metrics.

285
00:16:03,225 --> 00:16:04,155
Our business events.

286
00:16:04,425 --> 00:16:08,805
So that's where we have to touch a
code a little bit, add an SDK way

287
00:16:08,805 --> 00:16:12,855
of having things get some custom
metrics added or trace and the spans.

288
00:16:12,915 --> 00:16:16,185
So then we need to have
additional lines of code in there.

289
00:16:16,545 --> 00:16:19,915
So otherwise it really supports
the low code just simply through a

290
00:16:19,915 --> 00:16:23,735
couple of lines of configurations
and bring the binary into a model,

291
00:16:23,735 --> 00:16:25,325
and it automatically captures this.

292
00:16:26,470 --> 00:16:29,560
Typically it's like an agent
based instrumentation, how these

293
00:16:29,560 --> 00:16:31,420
traditional a PM tools work.

294
00:16:31,760 --> 00:16:34,940
But the code is really going to
give more granular level details

295
00:16:34,940 --> 00:16:36,140
and additional information.

296
00:16:39,110 --> 00:16:42,560
So once we instrument, the next step
is to collect and correlating the data.

297
00:16:43,070 --> 00:16:47,300
So this Open Telemetry agents really
collect the telemetry and send it

298
00:16:47,300 --> 00:16:49,145
to the Open Telemetry collectors.

299
00:16:49,175 --> 00:16:54,805
Where it actually helps to I'll process
the data and store it into a data lake.

300
00:16:54,835 --> 00:16:58,495
Then the correlation really happens
by leveraging all the metrics.

301
00:16:58,495 --> 00:17:03,755
So when the, when there is a high latency
span absorbed, then it can really give a

302
00:17:03,755 --> 00:17:08,065
correlation of what is a. Actual service
resource utilization at that time.

303
00:17:08,115 --> 00:17:10,545
What was the services
latencies during that time?

304
00:17:10,545 --> 00:17:14,055
Is it only for this particular request
or it was al already performing poor?

305
00:17:14,055 --> 00:17:15,045
What was the error rate?

306
00:17:15,375 --> 00:17:19,275
So it really helps us to give the
contextualize view for a single

307
00:17:19,275 --> 00:17:21,975
pane and really provide unified
view of the system behavior.

308
00:17:22,575 --> 00:17:26,205
And with the help of this context
of, with the custom attributes, we

309
00:17:26,205 --> 00:17:30,465
can even really filter the traces of
spans specific to that particular.

310
00:17:30,520 --> 00:17:31,480
Customer attribute.

311
00:17:31,480 --> 00:17:34,930
Maybe it's a customer email or
a customer id or order ID or

312
00:17:34,930 --> 00:17:37,000
whatever the business attribute is.

313
00:17:38,860 --> 00:17:43,060
And once we have all these data coming in,
the correlation, and so it really helps to

314
00:17:43,750 --> 00:17:45,610
visualize the traces, metrics, and logs.

315
00:17:45,610 --> 00:17:46,690
And Grafana.

316
00:17:46,690 --> 00:17:48,730
Grafana is a wonderful visualization tool.

317
00:17:48,730 --> 00:17:51,300
Lot of the prebuilt dashboards are there.

318
00:17:51,300 --> 00:17:55,800
So we really provide us a single
pane view of all traces, metrics,

319
00:17:55,800 --> 00:17:57,090
logs, and profiles as well.

320
00:17:58,080 --> 00:18:02,070
So it, it also give an interactive
exploration where if you want

321
00:18:02,070 --> 00:18:04,860
to drill down further so you
see a specific anomaly there.

322
00:18:04,860 --> 00:18:06,840
So it really helps us
to further drill down.

323
00:18:06,840 --> 00:18:11,280
It provide hyperlinks to other
views and dashboard and also

324
00:18:11,400 --> 00:18:12,930
helps to build a custom dashboard.

325
00:18:12,930 --> 00:18:16,700
There are plenty of dashboards
available in JSON format, so we don't

326
00:18:16,700 --> 00:18:20,420
have to build new one, especially
for tech common technologies like.

327
00:18:20,920 --> 00:18:24,675
Kafka or Kubernetes or any
cloud specific services.

328
00:18:24,675 --> 00:18:27,285
So a lot of things are out
there, especially for Linex.

329
00:18:27,465 --> 00:18:31,535
I've seen this very big dashboard
which covers entire Linex stack.

330
00:18:31,585 --> 00:18:32,875
So really useful.

331
00:18:32,935 --> 00:18:37,645
So it's ever growing so that it
definitely provide a contextual,

332
00:18:37,645 --> 00:18:38,995
unified, single pane view.

333
00:18:40,855 --> 00:18:42,325
And Grafana also growing.

334
00:18:42,325 --> 00:18:44,605
They've had a lot of other stacks as well.

335
00:18:45,130 --> 00:18:50,485
So this is the complete RAF absorbability
stack where it collects through the agents

336
00:18:50,485 --> 00:18:54,985
through like a Prometheus exporter for any
metrics or open telemetry or affluent bit.

337
00:18:55,405 --> 00:18:58,365
It collects into the
open telemetry collector.

338
00:18:58,365 --> 00:19:01,975
They own open telemetry collector
and also stores in Tinder.

339
00:19:03,060 --> 00:19:06,810
LGTM stack are low key for logs,
Grafana for visualization, tempo

340
00:19:06,810 --> 00:19:10,260
for traces and MiiR for metrics.

341
00:19:10,500 --> 00:19:14,340
And also it provides a continuous
profiling through pyro scope.

342
00:19:15,120 --> 00:19:20,400
So once we have all these data stored
in these data lakes be, definitely

343
00:19:20,940 --> 00:19:22,410
will have a complete visibility.

344
00:19:22,410 --> 00:19:25,620
So we'll have application
observability, which front ends,

345
00:19:25,620 --> 00:19:27,780
and also calculate the service map.

346
00:19:27,780 --> 00:19:30,590
So provide service map views
and through the traces.

347
00:19:31,275 --> 00:19:35,715
Also give EVP of perspective for how
the profiling happens and what are the

348
00:19:35,715 --> 00:19:40,245
hotspots there, and provide infras,
absorbability to fall Kubernetes from

349
00:19:40,245 --> 00:19:44,205
the server VMs or databases, or any
cloud native integrations as well.

350
00:19:44,745 --> 00:19:48,615
And it really helps to have a incident
management in terms of providing

351
00:19:48,645 --> 00:19:53,025
alerting capability on-call support
and for incident tracking and also

352
00:19:53,025 --> 00:19:54,435
even the service level objectives.

353
00:19:54,840 --> 00:19:59,190
So apart from this to shift left and
really the case, it's really helps us

354
00:19:59,190 --> 00:20:02,600
to simulate the load in terms of backend
as well as for the browser testing.

355
00:20:03,260 --> 00:20:08,270
So definitely it's a unified single
place to go for this observability stack.

356
00:20:08,540 --> 00:20:12,416
And it also integrates with a lot of
other big data sources where once the

357
00:20:12,600 --> 00:20:16,380
collection layer there, if you don't wanna
manage these data, we can really ship to

358
00:20:16,380 --> 00:20:18,600
all the commercial products out there.

359
00:20:19,600 --> 00:20:22,630
So now the empowering alerting
and incident response.

360
00:20:22,630 --> 00:20:26,420
So these alerting is very key
there for timely incident response.

361
00:20:26,420 --> 00:20:30,230
So this really helps us to, the
composite alerts really helps us to

362
00:20:30,230 --> 00:20:36,170
have a rules based and multiple data
sources combining into a single alerting

363
00:20:36,170 --> 00:20:40,710
system so that it really helps us
to avoid these alerts to nice ratio.

364
00:20:41,430 --> 00:20:45,420
And also really helps us to automate
root cause analysis to the integrations

365
00:20:45,420 --> 00:20:50,275
with web hooks really helps us to
trigger a workflow where it can go

366
00:20:50,275 --> 00:20:53,845
and look at, based on the knowledge
graphs and the service maps to find

367
00:20:53,845 --> 00:20:58,305
out in the ecosystem or in the chain
of events, what exactly gone wrong

368
00:20:58,355 --> 00:21:03,935
what is anomalies or correlation, and
provided the RCA as soon as possible.

369
00:21:05,945 --> 00:21:09,635
So with these Absorbability 2.0,
what is the future of Absorbability?

370
00:21:09,695 --> 00:21:15,125
So definitely to accelerate the
innovation where the time to detect

371
00:21:15,185 --> 00:21:19,265
and time to isolate and time to
recover is gonna be shorter and shorter

372
00:21:19,385 --> 00:21:23,405
With these tools, definitely really
helps to accelerate the innovation.

373
00:21:24,035 --> 00:21:27,755
The unified absorbability, definitely
the single source of truth is going to

374
00:21:27,755 --> 00:21:30,695
empower to make the data driven decisions.

375
00:21:31,985 --> 00:21:33,275
Intelligent automation.

376
00:21:33,275 --> 00:21:36,875
This really with the evaluation
of a Agentic frameworks and a and

377
00:21:36,875 --> 00:21:40,115
mission learning, really helps to
detect the anomaly and trigger a

378
00:21:40,115 --> 00:21:44,925
workflow for quick Auto RCA and
even do the runbook automation, or

379
00:21:45,555 --> 00:21:47,355
having it self-healing system built.

380
00:21:47,415 --> 00:21:49,245
It auto recovers itself.

381
00:21:50,460 --> 00:21:53,550
And also let's not forget
about the security aspects.

382
00:21:53,550 --> 00:21:58,820
So it really helps to analyze those audit
trails and events, to have a security

383
00:21:58,820 --> 00:22:03,620
threads and vulnerabilities by detecting
the behavior and pattern analysis and

384
00:22:03,620 --> 00:22:07,760
really helps us to find, find any security
threads and vulnerabilities there.

385
00:22:10,385 --> 00:22:12,465
That hope you've learned something today.

386
00:22:12,885 --> 00:22:16,245
Thanks so much for joining us and
if you have any follow up questions

387
00:22:16,245 --> 00:22:19,815
and feel, Brad, please connect me
over LinkedIn and thank you so much

388
00:22:19,815 --> 00:22:21,375
and have a good rest of your day.

