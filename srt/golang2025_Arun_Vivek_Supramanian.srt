1
00:00:00,120 --> 00:00:00,870
Hello everyone.

2
00:00:01,050 --> 00:00:06,330
I'm Arun Subramanian and I'm honored to
be speaking here at Con 42 Golan 2025.

3
00:00:06,870 --> 00:00:11,310
The title of today's stock is Data Mesh
Unleashed, the Scalable Architecture

4
00:00:11,460 --> 00:00:13,860
for a Driven Enterprise Intelligence.

5
00:00:14,340 --> 00:00:19,350
We are going to explore how data mesh
reshapes traditional thinking about data

6
00:00:19,350 --> 00:00:25,350
platforms, especially in the context of
AI and rapidly scaling organizations.

7
00:00:25,890 --> 00:00:28,020
This isn't just a
technical transformation.

8
00:00:28,425 --> 00:00:34,065
And it's an organizational one, and it's
redefining how teams deliver trusted,

9
00:00:34,275 --> 00:00:36,405
scalable, and usable data products.

10
00:00:37,065 --> 00:00:38,475
Let's start with the basics.

11
00:00:38,535 --> 00:00:40,185
What exactly is data mesh?

12
00:00:40,605 --> 00:00:44,115
In short, it's a decentralized
approach to data architecture.

13
00:00:44,835 --> 00:00:49,695
Instead of managing all data in
one central warehouse or lake data,

14
00:00:49,695 --> 00:00:53,505
miss distributes responsibility
to the teams that generate

15
00:00:53,655 --> 00:00:56,355
and use the data, the domains.

16
00:00:57,285 --> 00:01:02,985
This means product marketing,
sales, or finance streams can

17
00:01:03,015 --> 00:01:09,524
own, manage, and serve their data
directly using Shad platform tools.

18
00:01:09,945 --> 00:01:10,664
Why do this?

19
00:01:10,875 --> 00:01:13,755
Because centralized models don't scale.

20
00:01:14,235 --> 00:01:18,465
They create bottlenecks, long lead
times, and poor alignment between

21
00:01:18,465 --> 00:01:20,445
data producers and consumers.

22
00:01:21,045 --> 00:01:22,755
Data mesh flips the model.

23
00:01:23,414 --> 00:01:25,035
Empowering domain teams.

24
00:01:25,515 --> 00:01:32,085
While still maintaining interoperability
through shared standards and governance.

25
00:01:32,585 --> 00:01:37,115
Now ness is based on four core principles.

26
00:01:37,615 --> 00:01:42,204
Each of these addresses a key limitation
of traditional and centralized data

27
00:01:42,204 --> 00:01:44,994
systems, domain rated data ownership.

28
00:01:45,474 --> 00:01:46,314
This principle.

29
00:01:46,914 --> 00:01:50,214
Place a responsibility for
data in the hands of the teams

30
00:01:50,244 --> 00:01:52,044
that generate and use it.

31
00:01:52,554 --> 00:01:56,994
For example, the payments
team in a FinTech company

32
00:01:57,264 --> 00:01:59,064
owns all transactional data.

33
00:01:59,564 --> 00:02:04,604
They know the context like settlement
delays, transaction reversals,

34
00:02:04,844 --> 00:02:09,195
and fraud patterns for better
than a centralized data team.

35
00:02:09,764 --> 00:02:11,505
What this ownership means.

36
00:02:11,895 --> 00:02:16,965
Faster decisions, more relevant
data models, and quicker itration

37
00:02:17,465 --> 00:02:19,565
self-serve data infrastructure.

38
00:02:20,105 --> 00:02:26,345
Just like DevOps built internal platforms
to help engineers ship code faster.

39
00:02:26,765 --> 00:02:31,619
Be built data platforms that
abstract complexity, ingestions

40
00:02:31,915 --> 00:02:35,545
storage, transformation,
cataloging, and monitoring.

41
00:02:36,505 --> 00:02:36,834
Consider.

42
00:02:37,795 --> 00:02:42,084
A marketing team that wants to
expose campaign performance.

43
00:02:42,535 --> 00:02:46,905
Instead of submitting a ticket to a
data engineering, they use internal

44
00:02:46,905 --> 00:02:52,184
tooling to publish a data product by
a declarative config or pipeline ui.

45
00:02:52,684 --> 00:02:55,704
All self-serve federated governance.

46
00:02:55,974 --> 00:03:02,269
This principle ensures standardization
without centralization every domain,

47
00:03:02,289 --> 00:03:04,644
others to a common metadata model.

48
00:03:05,064 --> 00:03:11,214
Lineage tracking, access control and
regulatory compliance, but enforcement

49
00:03:11,214 --> 00:03:13,494
happens automatically through tooling.

50
00:03:14,304 --> 00:03:16,884
Think of GitHubs for data.

51
00:03:17,384 --> 00:03:22,334
Policies, live in code and deviations,
trigger alerts or block deployment.

52
00:03:22,834 --> 00:03:23,914
Data is a product.

53
00:03:24,424 --> 00:03:27,515
Teams don't just expose raw data.

54
00:03:28,174 --> 00:03:30,844
They deliver curated, documented.

55
00:03:31,234 --> 00:03:33,304
Version and reliable data products.

56
00:03:33,904 --> 00:03:39,154
Let's say the sales team publishes
a monthly sales performance dataset.

57
00:03:39,934 --> 00:03:44,074
It should have metadata, freshness
indicators, contact info for

58
00:03:44,074 --> 00:03:46,144
the data steward and alerts.

59
00:03:46,324 --> 00:03:50,284
If the pipeline fails, it's treated
like a production grade a PA.

60
00:03:50,784 --> 00:03:55,914
Now let's talk about how data
warehouse is different from data mesh.

61
00:03:56,574 --> 00:04:01,105
Traditional data varis offer a
single source of truth, structure,

62
00:04:01,614 --> 00:04:03,204
centralized and controlled.

63
00:04:03,444 --> 00:04:05,784
But let's look at a real world challenge.

64
00:04:06,415 --> 00:04:10,404
A healthcare company wants to
add real time lab test results

65
00:04:10,404 --> 00:04:11,844
into their B dashboards.

66
00:04:12,295 --> 00:04:16,375
In a varis model, this means
coordination between data engineers,

67
00:04:16,794 --> 00:04:18,385
analyst, and business teams.

68
00:04:18,684 --> 00:04:24,564
Sometimes taking weeks schema changes
can impact dozens of downstream jobs.

69
00:04:25,510 --> 00:04:30,640
In contrast in a data mesh model,
the clinical lab domain wants the

70
00:04:30,640 --> 00:04:35,709
real time data stream, the expose
curated version data product that

71
00:04:36,159 --> 00:04:41,650
other domains like pash, patient care
or billing can consume immediately.

72
00:04:42,159 --> 00:04:45,130
The sales or analytics
teams don't have to wait.

73
00:04:45,520 --> 00:04:50,020
They use the product via
defined APIs or SQL interfaces.

74
00:04:50,620 --> 00:04:54,069
This removes bottlenecks and
enables parallel innovation.

75
00:04:54,505 --> 00:04:59,515
While a warehouse scales compute you
mess scales, ownership and speed.

76
00:05:00,015 --> 00:05:03,645
Now compare data lakes with data mesh.

77
00:05:04,145 --> 00:05:04,864
Data lakes.

78
00:05:05,074 --> 00:05:07,354
Were a step forward from rigid warehouses.

79
00:05:07,864 --> 00:05:12,844
They let us to store raw, semi-structured
and unstructured data cheaply.

80
00:05:13,205 --> 00:05:17,194
Great for ML use cases and
future proofing, but the

81
00:05:17,194 --> 00:05:18,544
flexibility came at a cost.

82
00:05:19,044 --> 00:05:20,185
Ownership was unclear.

83
00:05:20,544 --> 00:05:25,525
Documentation was often missing, and it
became hard to find or trust anything.

84
00:05:26,065 --> 00:05:31,015
A common example, a data scientist
wants customer feedback, data

85
00:05:31,015 --> 00:05:32,455
force sentiment analysis.

86
00:05:32,815 --> 00:05:36,115
In the lake, there are five data
sets named customer reviews.

87
00:05:36,294 --> 00:05:40,330
Feedback, dump recommends
version two, et cetera.

88
00:05:40,710 --> 00:05:42,715
No documentation, no guarantees.

89
00:05:43,375 --> 00:05:45,044
Result days lost.

90
00:05:45,075 --> 00:05:46,844
Exploring under label data.

91
00:05:47,820 --> 00:05:50,880
Data me fixes this by
assigning clear ownership.

92
00:05:51,030 --> 00:05:55,289
For example, the customer experience
domain wants all feedback data.

93
00:05:55,979 --> 00:06:00,120
Their data product is named
documented ENT and searchable.

94
00:06:00,120 --> 00:06:04,620
Via metadata catalog, consumers
can explore lineage, understand

95
00:06:04,620 --> 00:06:07,560
transformations, and raise
issues to the warning team.

96
00:06:08,130 --> 00:06:13,620
So while a lake store's data
mis prioritizes it, adding

97
00:06:13,620 --> 00:06:15,840
quality, traceability, and trust.

98
00:06:16,340 --> 00:06:19,580
Let's talk about key
benefits of data mesh.

99
00:06:19,910 --> 00:06:21,650
So why are teams making the shift?

100
00:06:22,310 --> 00:06:25,460
Let's look at tangible
benefits, grounded in examples.

101
00:06:25,850 --> 00:06:29,350
Scalability, I. At a large retail
company, the centralized data team had

102
00:06:29,350 --> 00:06:32,710
a backlog of over 150 feature request.

103
00:06:33,220 --> 00:06:36,910
With the data mesh, the teams
like logistics, inventory,

104
00:06:36,970 --> 00:06:40,720
and promotions now publish and
manage their own data products.

105
00:06:41,200 --> 00:06:45,820
The backlog shrinks and velocity
increases, not because we hired more

106
00:06:45,820 --> 00:06:51,125
engineers, but because we distribute our
responsibility, reduced it dependency.

107
00:06:51,570 --> 00:06:55,380
Your global telecom company reported
a 70% drop in ticket volume to

108
00:06:55,380 --> 00:06:56,940
the central data engineering team.

109
00:06:57,240 --> 00:07:01,680
After adopting self-serve
infrastructure, product teams manage

110
00:07:02,180 --> 00:07:06,740
build their pipelines using internal
platforms, reducing handoffs and

111
00:07:07,280 --> 00:07:12,780
accelerating delivery, a faster
decision making in logistic startup.

112
00:07:13,200 --> 00:07:16,590
The operational teams own
shipments tracking data.

113
00:07:16,860 --> 00:07:20,160
Because it's in their control,
they can expose near real-time

114
00:07:20,520 --> 00:07:24,990
metrics delays, EDA variance
without going through central teams.

115
00:07:25,290 --> 00:07:29,610
This leads to instant feedback
loops, daily optimization,

116
00:07:29,760 --> 00:07:30,870
and reduce delivery time.

117
00:07:31,680 --> 00:07:36,450
And ML teams often spend 60 to
80% of their time wrangling data.

118
00:07:36,900 --> 00:07:40,800
Data mesh reduces this by
delivering ready to use reliable.

119
00:07:41,175 --> 00:07:42,795
Well-documented data products.

120
00:07:43,185 --> 00:07:49,005
For example, a fraud detection model can
plug into a domain own stream of payment

121
00:07:49,005 --> 00:07:55,185
events with consistent schemas rather than
scrapping logs or waiting for ET jobs.

122
00:07:55,685 --> 00:08:02,015
Now, enabling technologies for database,
so let's, implementing a database at scale

123
00:08:02,075 --> 00:08:04,295
requires a robust technical foundation.

124
00:08:04,745 --> 00:08:07,475
Here are some of the key
enabling technologies.

125
00:08:07,895 --> 00:08:09,005
Apache Iceberg.

126
00:08:09,215 --> 00:08:14,135
An OpenTable format that brings
AIP compliance, schema evolution,

127
00:08:14,405 --> 00:08:15,965
time travel to big data.

128
00:08:16,025 --> 00:08:20,135
For example, if the product catalog
domain updates, a product name

129
00:08:20,315 --> 00:08:25,325
iceberg ensures consumers can time
travel and compare changes supporting

130
00:08:25,325 --> 00:08:27,275
reproducibility in ML models.

131
00:08:27,905 --> 00:08:31,085
Data Lake tightly integrated
with spark ecosystem.

132
00:08:31,265 --> 00:08:36,455
Delta Lake provides transaction
integrity over data lakes, for instance.

133
00:08:36,935 --> 00:08:41,525
Your finance team using Delta can
perform streaming and batch processing

134
00:08:41,615 --> 00:08:46,955
over the same table while enforcing
schema consistency, snowflake with

135
00:08:46,955 --> 00:08:51,215
separation of compute and storage and
its powerful data sharing capabilities.

136
00:08:51,515 --> 00:08:55,385
Snowflakes allows domain teams
to share data product across

137
00:08:55,385 --> 00:08:57,515
departments and regions securely.

138
00:08:58,055 --> 00:09:02,405
A global marketing team can
publish campaign data once and

139
00:09:02,405 --> 00:09:05,585
teams in other countries can
query it without duplication.

140
00:09:06,085 --> 00:09:07,465
AWS S3 offer.

141
00:09:07,465 --> 00:09:12,505
The foundation layer S3 is used for
durable, scalable object storage.

142
00:09:12,835 --> 00:09:18,925
It works with iceberg or delta to serve
the as the backend for domain owned

143
00:09:19,015 --> 00:09:25,645
data products with versioning and IEM
based access, it enforce governance

144
00:09:25,855 --> 00:09:27,505
policies at the object levels.

145
00:09:28,420 --> 00:09:34,030
And some of the tools like Airflow,
Dexter, and Prefect Orchestrate

146
00:09:34,030 --> 00:09:39,520
pipelines while Data Hub Open
Metadata and Amon provide metadata

147
00:09:39,520 --> 00:09:42,550
management, lineage and discoverability.

148
00:09:43,090 --> 00:09:49,000
For example, a data scientist can look up
a data product, see its owner's freshness,

149
00:09:49,180 --> 00:09:51,340
lineage, and press it before using it.

150
00:09:51,840 --> 00:09:54,630
Now let's talk about the
real world implementation.

151
00:09:55,005 --> 00:09:57,065
With examples four.

152
00:09:57,485 --> 00:10:03,215
Zando as one of the Europe's largest
e-commerce platforms, zando embraced data

153
00:10:03,215 --> 00:10:05,705
mesh to decentralized data ownership.

154
00:10:06,215 --> 00:10:10,895
Each domain team now wants its data
products and publishes them with

155
00:10:10,925 --> 00:10:13,355
standardized metadata and SLAs.

156
00:10:13,655 --> 00:10:17,645
Your product manager in logistics can
access inventory productions without

157
00:10:17,645 --> 00:10:19,235
having to wait for a central team.

158
00:10:20,075 --> 00:10:21,095
JP Morgan Chase.

159
00:10:21,590 --> 00:10:26,750
In highly regulated environments like
finance, federated governance is critical.

160
00:10:27,080 --> 00:10:33,190
JP Morgan used policy as code and
lineage enforcement to decentralized

161
00:10:33,370 --> 00:10:34,870
while maintaining compliance.

162
00:10:35,140 --> 00:10:40,695
The risk analysis team can safely build
models on credit transaction data with

163
00:10:40,935 --> 00:10:47,075
trust in its prominence and control
Netflix while not branding a data mesh.

164
00:10:47,465 --> 00:10:50,285
Netflix follows the principles closely.

165
00:10:50,795 --> 00:10:55,385
Teams publish data through APIs and
internal tooling, and other teams can

166
00:10:55,385 --> 00:10:57,965
subscribe, transform, and build insights.

167
00:10:58,265 --> 00:11:02,975
Their internal data marketplace allows
personalization and content teams to

168
00:11:02,975 --> 00:11:05,015
shared behavioral insights in real time.

169
00:11:05,515 --> 00:11:08,935
Now, let's talk about, the
business impact the data mesh

170
00:11:09,235 --> 00:11:11,125
creates to the organizations.

171
00:11:11,860 --> 00:11:16,450
40% increase in team autonomy
at a large online marketplace.

172
00:11:16,570 --> 00:11:21,340
Domain teams build and deploy their
own products, improving responsiveness

173
00:11:21,340 --> 00:11:27,100
to change, 35% improvement in
data quality at a FinTech startup.

174
00:11:27,100 --> 00:11:31,990
Domain ownership led to faster road
cost analysis and issue resolution.

175
00:11:32,590 --> 00:11:34,600
50% increase in collaboration.

176
00:11:34,900 --> 00:11:38,590
Cross-functional teams at a
healthcare provider now co-design

177
00:11:38,590 --> 00:11:43,570
data contracts, ensuring shared
understanding, 60% reduction in

178
00:11:43,720 --> 00:11:46,240
time to insight in a SaaS company.

179
00:11:46,300 --> 00:11:49,660
The product analytics team ready
use dashboard refresh latency

180
00:11:49,900 --> 00:11:53,080
from hearts to minutes by
owning their own event pipeline.

181
00:11:53,800 --> 00:11:56,110
These are just metrics they represent.

182
00:11:56,110 --> 00:11:59,590
Faster feedback loops, better
trust, and tighter integrations.

183
00:11:59,905 --> 00:12:02,875
Between data producers and consumers.

184
00:12:03,375 --> 00:12:07,275
Now let's talk about some of the
challenges in implementing data mesh.

185
00:12:07,875 --> 00:12:12,255
While data mesh is powerful, it comes
with challenges like cultural change.

186
00:12:12,375 --> 00:12:14,115
Not every team is ready to own data.

187
00:12:14,655 --> 00:12:18,645
Teams must shift from data
is someone else jobs too.

188
00:12:19,605 --> 00:12:22,725
We won and serve this,
for example, HRT Morning.

189
00:12:22,725 --> 00:12:26,805
Their recruiting funnel data
may need upskilling to support

190
00:12:26,805 --> 00:12:28,485
schema changes and documentation.

191
00:12:29,115 --> 00:12:30,225
Platform maturity.

192
00:12:30,885 --> 00:12:34,365
Without strong tooling, the
burden on domain increases.

193
00:12:34,545 --> 00:12:39,015
A company that launched mesh too
early without governance templates

194
00:12:39,285 --> 00:12:42,165
saw schema drift and broken pipelines.

195
00:12:42,735 --> 00:12:47,535
Security and compliance, distributed
ownership expands the attack surface.

196
00:12:47,625 --> 00:12:48,765
Role-based access.

197
00:12:48,885 --> 00:12:52,240
Audit logging, PII,
tagging must be automated.

198
00:12:52,770 --> 00:12:57,690
For example, a healthcare organization
must enforce HIPAA policies even

199
00:12:57,690 --> 00:13:02,630
as domain teams manage patient's
data in clear communication,

200
00:13:02,900 --> 00:13:05,839
training and incentives are needed.

201
00:13:06,260 --> 00:13:10,910
Teams need to know the why and
how of the new model, and be

202
00:13:10,910 --> 00:13:13,219
given the tools to succeed.

203
00:13:13,719 --> 00:13:19,269
Now, let's talk about some of the
best practices for adopting data mass.

204
00:13:20,109 --> 00:13:22,149
Organizational readiness assessment.

205
00:13:22,869 --> 00:13:27,699
You evaluate whether your team align
naturally by domain and if they have the

206
00:13:27,699 --> 00:13:30,039
skills and leadership support to own data.

207
00:13:30,539 --> 00:13:32,249
And start with the pilot domain.

208
00:13:32,849 --> 00:13:36,259
Pick a domain with a clear
value case and motivated team.

209
00:13:36,709 --> 00:13:40,009
Example, your marketing team that
wants to optimize campaign spend

210
00:13:40,369 --> 00:13:45,529
using real time engagement data build
data play platform as a product.

211
00:13:45,919 --> 00:13:48,529
Create golden parts for ingestion.

212
00:13:48,769 --> 00:13:53,539
Documentation, validation and
publishing include observability

213
00:13:53,599 --> 00:13:59,329
and alerting by default, and
also education and enablement.

214
00:13:59,689 --> 00:14:04,549
Offer training and product thinking, data
quality, security, and platform tools.

215
00:14:05,089 --> 00:14:06,379
Celebrate wins.

216
00:14:06,619 --> 00:14:09,919
For example, your domain team,
saving time through automation

217
00:14:10,699 --> 00:14:12,679
and keep it governance in check.

218
00:14:13,354 --> 00:14:17,374
Start with minimal viable policies
and evolve based on feedback.

219
00:14:17,914 --> 00:14:22,234
Use tooling to enforce standardized
without creating manual overhead.

220
00:14:22,734 --> 00:14:26,274
Now let's talk about the future
of enterprise data management.

221
00:14:26,774 --> 00:14:31,424
So we know the future of enterprise
data is gonna be decentralized,

222
00:14:31,724 --> 00:14:34,844
governed, and real time with data mesh.

223
00:14:34,934 --> 00:14:38,114
We are not just improving
performance, we are rethinking the.

224
00:14:38,804 --> 00:14:43,934
The data flows in an organization as
a, and AI and real time personalization

225
00:14:43,934 --> 00:14:48,464
becoming standard, an organization
needs to scale not just storage, but

226
00:14:48,464 --> 00:14:50,684
trust, discoverability and velocity.

227
00:14:51,164 --> 00:14:56,204
Imagine an enterprise where product
teams publish features and marketing

228
00:14:56,204 --> 00:15:00,944
launches, campaigns and fraud systems,
train new models, all powered with

229
00:15:00,944 --> 00:15:02,924
the same interconnected, trusted.

230
00:15:03,269 --> 00:15:10,019
Mesh of data products, that's where we are
headed, and data mesh provides the roadmap

231
00:15:10,519 --> 00:15:11,629
and thank you.

