1
00:00:00,500 --> 00:00:01,130
Hello everyone.

2
00:00:01,250 --> 00:00:02,030
My name is Qatar.

3
00:00:02,420 --> 00:00:05,930
Today I wanna show you how we can
build healthcare AI systems that

4
00:00:05,930 --> 00:00:07,610
are both powerful and private.

5
00:00:08,060 --> 00:00:11,990
My promise to you is that this will
not be just theory or passwords.

6
00:00:12,410 --> 00:00:15,650
By the time we are done, you'll
have a clear picture of how

7
00:00:15,680 --> 00:00:17,689
blockchain federated learning.

8
00:00:18,020 --> 00:00:21,830
And modern privacy engineering can
be combined to make production great

9
00:00:21,860 --> 00:00:24,200
platforms that regulate us trust.

10
00:00:24,505 --> 00:00:24,894
Deploy.

11
00:00:25,345 --> 00:00:28,555
Developers can easily deploy
and patients can rely on.

12
00:00:29,034 --> 00:00:33,084
So I'll walk you through some real
technical issues and give you the

13
00:00:33,084 --> 00:00:36,595
same insights I've developed from
research and independent study.

14
00:00:37,015 --> 00:00:40,915
Think of this, talk less as a
presentation and more as a workshop

15
00:00:41,245 --> 00:00:42,620
where we are looking under the hood to.

16
00:00:43,500 --> 00:00:46,860
Since I mentioned examples,
everything I share in this session

17
00:00:46,860 --> 00:00:49,170
represents my own views and learning.

18
00:00:49,650 --> 00:00:52,800
Nothing here is from any of
my current or past employers.

19
00:00:53,160 --> 00:00:57,559
The slides are based on public references
such as OAS Top 20 open Academic

20
00:00:57,559 --> 00:00:59,870
Research and my IEEE and MIT work.

21
00:01:00,469 --> 00:01:05,000
This is a personal presentation effort
where I share insights and practices I've

22
00:01:05,000 --> 00:01:08,870
developed through study and experience
to compare notes with the community.

23
00:01:09,370 --> 00:01:15,400
All right, so with that, let's
dive deep into slide one.

24
00:01:16,060 --> 00:01:20,410
The healthcare AI privacy paradox,
AI and healthcare is full of promise.

25
00:01:20,410 --> 00:01:24,490
As we all know, whether it's imaging
drug discovery, or personalized

26
00:01:24,490 --> 00:01:28,809
treatments, these models get better
as their training data gets richer.

27
00:01:29,079 --> 00:01:30,849
But here lies the paradox, right?

28
00:01:31,179 --> 00:01:35,409
The richest data sets are locked behind
regulations and competitive boundaries.

29
00:01:35,934 --> 00:01:40,824
HIPAA and GDPR prevent direct
sharing and even anonymization often

30
00:01:40,824 --> 00:01:46,194
fails because modern models can
reconstruct or infer sensitive details.

31
00:01:46,674 --> 00:01:51,804
I've started cases where supposedly
anonymized DICOM scans or clinical notes.

32
00:01:52,109 --> 00:01:55,649
Could still be linked back to
patients through auxiliary data.

33
00:01:55,649 --> 00:02:00,839
Of course centralizing data may look
convenient, but it creates single breach

34
00:02:00,839 --> 00:02:02,820
point and invites inference attacks.

35
00:02:03,389 --> 00:02:05,849
The way forward is not
to move data at all.

36
00:02:06,349 --> 00:02:10,429
Instead, we need to bring computation
and proofs to where the data

37
00:02:10,429 --> 00:02:14,689
lives while guaranteeing to all
stakeholders that their privacy and

38
00:02:14,689 --> 00:02:16,849
compliance obligations remain intact.

39
00:02:17,349 --> 00:02:19,569
Let's look at this problem even further.

40
00:02:20,199 --> 00:02:23,439
I call it the distributed
nature of healthcare data.

41
00:02:23,769 --> 00:02:27,339
So healthcare data is not just
distributed, but it is fragmented

42
00:02:27,339 --> 00:02:32,619
across incompatible systems, EHRs,
or electronic health records.

43
00:02:32,679 --> 00:02:35,379
They sit inside of Epic
or Cerner as an example.

44
00:02:36,169 --> 00:02:39,019
Imaging probably managed by pax.

45
00:02:39,029 --> 00:02:42,149
Genomic data sets live in research silos.

46
00:02:42,509 --> 00:02:44,759
Clinical trials sit with pharma.

47
00:02:45,299 --> 00:02:49,229
Each uses its own schema
vocabulary and security protocols.

48
00:02:49,679 --> 00:02:54,749
I've seen many technical reviews described
this as a data integration problem, but in

49
00:02:54,749 --> 00:02:59,399
reality, to me, it's also a compliance and
trust problem as a security professional.

50
00:03:00,069 --> 00:03:04,509
The mistake is often to build massive
central extractors to normalize

51
00:03:04,509 --> 00:03:08,169
everything, and outcome is brittle
pipelines and exposure risks.

52
00:03:08,749 --> 00:03:13,489
A better pattern is to build thin,
standardized adapters that run locally.

53
00:03:13,489 --> 00:03:18,889
Inside each institution, these
adapters translate into local formats.

54
00:03:19,369 --> 00:03:23,809
The minimal features required for
training enforce local policy and

55
00:03:23,809 --> 00:03:25,339
generate cryptographic groups.

56
00:03:25,699 --> 00:03:31,579
So by tolerating heterogeneity
instead of erasing it, we can

57
00:03:31,579 --> 00:03:35,899
actually create a system resilient
to schema drift, and local governance

58
00:03:35,899 --> 00:03:40,539
differences, right with that, right
with the problem outlined clearly.

59
00:03:40,984 --> 00:03:44,044
Let me bring in blockchain
as a trust foundation.

60
00:03:44,614 --> 00:03:48,784
When I say blockchain, I don't mean
public chains or cryptocurrencies.

61
00:03:49,064 --> 00:03:53,204
A localized version of it, which
most people call it a permission

62
00:03:53,204 --> 00:03:56,984
ledge that acts as a shared source
of truth between institutions.

63
00:03:57,384 --> 00:03:57,414
Okay.

64
00:03:58,284 --> 00:04:00,174
No PHI ever touches this chain.

65
00:04:00,264 --> 00:04:03,654
By the way, since we're talking about
a chain, since we're talking about

66
00:04:03,654 --> 00:04:08,284
institutions with an S you wanna make
sure that no PHI ever touches this chain.

67
00:04:08,524 --> 00:04:10,264
Then what are we gonna use blockchain for?

68
00:04:10,594 --> 00:04:15,634
What goes on the chain are trust
events, which site trained when under

69
00:04:15,664 --> 00:04:21,534
which, the consent policy allows with
what differential privacy budget,

70
00:04:21,594 --> 00:04:24,054
and what model lineage was used.

71
00:04:24,474 --> 00:04:29,574
So when auditors ask me to prove that
a model was never trained on disallowed

72
00:04:29,574 --> 00:04:35,064
cohorts, the ledger provides tamper
evident evidence or tamper proof evidence.

73
00:04:35,334 --> 00:04:35,664
Call it.

74
00:04:36,234 --> 00:04:39,594
The ledger is the bridge where
compliance meets technology.

75
00:04:40,044 --> 00:04:41,389
This requires discipline design.

76
00:04:42,209 --> 00:04:46,739
Only store hashes and references, rotate
keys with hardware backed identities

77
00:04:46,859 --> 00:04:52,229
and ensure that smart contracts encode
regulatory gates like consent versions

78
00:04:52,229 --> 00:04:54,779
or privacy budgets when done correctly.

79
00:04:55,289 --> 00:04:56,219
Blockchain is not a hype.

80
00:04:56,639 --> 00:04:58,079
It is a compliance backbone.

81
00:04:58,619 --> 00:05:01,769
It's not an extra expense,
but a long term saving.

82
00:05:02,159 --> 00:05:03,089
Let's look at.

83
00:05:03,589 --> 00:05:06,769
And the next piece of it,
which is federated learning.

84
00:05:07,369 --> 00:05:10,369
Now, federated learning turns the
traditional model on its head.

85
00:05:10,699 --> 00:05:11,359
What does that mean?

86
00:05:11,779 --> 00:05:16,399
Instead of exporting data to external
model, we export the model to the data.

87
00:05:17,029 --> 00:05:19,729
Hospitals train locally
and send updates back.

88
00:05:19,789 --> 00:05:20,689
But here's the problem.

89
00:05:21,409 --> 00:05:23,334
Knife, federal learning, nicks.

90
00:05:23,949 --> 00:05:25,869
You know what a learning leak is.

91
00:05:26,309 --> 00:05:28,319
Most people call it knife federated.

92
00:05:28,319 --> 00:05:31,259
Learning leaks also is, if that's
the word that you're familiar with.

93
00:05:31,769 --> 00:05:35,129
Raw gradients can reveal
training examples.

94
00:05:35,189 --> 00:05:38,969
Malicious clients can poison
the model or plant factors.

95
00:05:39,689 --> 00:05:43,289
I've seen and I've seen of
course in, in test environments.

96
00:05:43,324 --> 00:05:47,324
Mostly since this is all new I've seen
demonstrations in academic research

97
00:05:47,324 --> 00:05:51,974
where attackers reconstructed sensitive
features just from update streams.

98
00:05:52,534 --> 00:05:57,184
The right implementation requires
secure aggregation, differential

99
00:05:57,184 --> 00:06:00,484
privacy at the edge, and robust
aggregation at the server.

100
00:06:01,174 --> 00:06:04,744
It also means validating and
attesting the training binaries.

101
00:06:04,744 --> 00:06:07,564
So we know each participant
is running the approved code.

102
00:06:08,074 --> 00:06:09,634
So without these measures.

103
00:06:10,109 --> 00:06:13,289
Federated learning is a false
sense of security with them.

104
00:06:13,289 --> 00:06:16,019
It becomes a practical way
to unlock collaboration.

105
00:06:16,889 --> 00:06:21,779
I also talk about the architecture a
little bit and not just stick to theory.

106
00:06:21,779 --> 00:06:23,939
Like we all agreed in
the first slide, right?

107
00:06:24,179 --> 00:06:27,389
Like a real feder federated
platform must honors.

108
00:06:27,809 --> 00:06:33,199
So inside each institution's
boundary okay there, because

109
00:06:33,199 --> 00:06:35,119
it remain there remains PHI.

110
00:06:35,209 --> 00:06:39,139
There remains consent checks
and local computation outside

111
00:06:39,139 --> 00:06:43,759
the boundary or RY orchestration
policies and cryptographic proofs.

112
00:06:44,449 --> 00:06:47,569
I would say abstraction layers
allow each side to connect

113
00:06:47,569 --> 00:06:49,309
despite of different stacks.

114
00:06:49,489 --> 00:06:53,239
So multi-tenancy is crucial because
the same hospital may participate in

115
00:06:53,239 --> 00:06:55,369
several collaborations simultaneously.

116
00:06:56,029 --> 00:06:59,659
Workflow orchestration must
respect real world constraints.

117
00:06:59,989 --> 00:07:01,639
Hospitals have maintenance windows.

118
00:07:01,659 --> 00:07:04,809
Bandwidths and bottlenecks
and competing schedules.

119
00:07:05,199 --> 00:07:07,179
Think of it like an SRE system.

120
00:07:07,329 --> 00:07:14,109
We need retries in the important jobs,
metrics and back pressure controls,

121
00:07:14,139 --> 00:07:19,049
which is an important, this is where a,
proof of concept either scales or fails

122
00:07:19,349 --> 00:07:24,059
by addressing not just algorithms, but
the operational realities of healthcare.

123
00:07:24,269 --> 00:07:25,789
It let's see.

124
00:07:26,289 --> 00:07:29,319
Let's talk about blockchain
infrastructure for healthcare here.

125
00:07:29,619 --> 00:07:31,989
Infrastructure, design
choices matter, right?

126
00:07:32,089 --> 00:07:36,949
And as most of the security professionals
would tell you that security should

127
00:07:36,949 --> 00:07:41,599
always begin at the time of the
design when things are layer up.

128
00:07:42,099 --> 00:07:45,224
Anything that is done during
implementation and not

129
00:07:45,224 --> 00:07:47,174
during design is expensive.

130
00:07:47,674 --> 00:07:49,354
And counterproductive, of course.

131
00:07:49,424 --> 00:07:52,904
So consensus mechanisms
should fit healthcare's needs.

132
00:07:52,994 --> 00:07:58,184
We usually have tens of participants,
so you know, PBFT style consensus

133
00:07:58,184 --> 00:08:00,774
with, strong IL is ideal, right?

134
00:08:01,274 --> 00:08:04,544
We're talking about blockchain
terminologies now, and smart contracts

135
00:08:04,544 --> 00:08:06,164
like should enforce policies.

136
00:08:06,869 --> 00:08:09,359
Such as no model update is accepted.

137
00:08:09,359 --> 00:08:14,729
If a site's differential, privacy
budget is exhausted, block structures

138
00:08:14,729 --> 00:08:16,169
must strike a balance, right?

139
00:08:16,169 --> 00:08:20,819
So enough transparency to support
audits, but not so much metadata

140
00:08:20,849 --> 00:08:22,529
that it can be de anonymized.

141
00:08:23,099 --> 00:08:25,439
So it's always about the balance, right?

142
00:08:25,439 --> 00:08:30,089
You log enough to make sure that
you have a context in place.

143
00:08:30,124 --> 00:08:34,114
To track what is being changed, but
then you also don't start logging

144
00:08:34,114 --> 00:08:36,484
PHI into it, so it's similar.

145
00:08:36,484 --> 00:08:40,754
And blockchain isn't really different
from that perspective, but hospitals

146
00:08:40,754 --> 00:08:44,604
operate with firewalls, intermittent
connections and strict change controls.

147
00:08:44,814 --> 00:08:46,884
I've seen projects
described in research lit.

148
00:08:47,199 --> 00:08:52,119
Literature fail because they always
assumed on connectivity or open

149
00:08:52,119 --> 00:08:55,749
networking, the right topology,
tolerates intermittent links,

150
00:08:55,809 --> 00:08:59,859
encrypts channels, and achieve leisure
state for the long retention cycles

151
00:09:00,099 --> 00:09:02,019
that healthcare regulators demand.

152
00:09:02,719 --> 00:09:03,499
Let's bring in.

153
00:09:03,754 --> 00:09:07,114
Privacy engineering
integration to this equation.

154
00:09:07,114 --> 00:09:12,044
Now, as we saw the problems of blockchain
we also saw how federation would come

155
00:09:12,044 --> 00:09:16,844
into picture and then how all of this
pieces in together in a integration

156
00:09:16,844 --> 00:09:18,174
of sorts, with privacy engineer.

157
00:09:18,384 --> 00:09:19,644
So privacy technology is not.

158
00:09:20,579 --> 00:09:22,979
One silver bullet, but toolbox rather.

159
00:09:23,469 --> 00:09:28,879
Why I say that is because differentially
a private training gives us provable

160
00:09:28,879 --> 00:09:31,219
limits on what leaks from models.

161
00:09:31,439 --> 00:09:36,089
Secure aggregation ensures that the
server never sees individual updates.

162
00:09:36,780 --> 00:09:40,530
And again, a ZTA right, which
also converts in privacy to.

163
00:09:40,894 --> 00:09:44,134
Zero knowledge proofs, if you have
heard, that can demonstrate that

164
00:09:44,134 --> 00:09:49,624
a site respected a consent rule
without revealing raw counts, call

165
00:09:49,624 --> 00:09:52,324
it the homomorphic encryption.

166
00:09:52,974 --> 00:09:56,844
So homo market encryption allows
computations directly on cipher text,

167
00:09:57,145 --> 00:09:59,334
though it is still expensive, right?

168
00:09:59,604 --> 00:10:03,145
But secure multi-party
computation enables collaborative

169
00:10:03,145 --> 00:10:04,915
statistics without input sharing.

170
00:10:05,625 --> 00:10:10,255
So I have a lot of examples and
you can easily find, companies and

171
00:10:10,270 --> 00:10:12,505
vendors that are specializing in this.

172
00:10:12,685 --> 00:10:15,475
And I see that it's not
that expensive with.

173
00:10:15,805 --> 00:10:18,265
The amount of time that is,
that it's being in the market.

174
00:10:18,785 --> 00:10:22,715
The art I think is knowing
when to use which tool.

175
00:10:22,935 --> 00:10:24,105
That keeps your cost down.

176
00:10:24,105 --> 00:10:27,855
For example, I often recommended
starting with a DP plus secure

177
00:10:27,855 --> 00:10:32,925
aggregation for most workflows and
layering in zero knowledge proofs where.

178
00:10:33,305 --> 00:10:37,545
Regulators or the security condition
in itself, or the risk demands higher

179
00:10:37,545 --> 00:10:41,695
assurances, let's say if it is a PHI
workflow versus a payments workflow

180
00:10:41,875 --> 00:10:47,425
versus a regular PII workflow versus
an anonymized data workflow, right?

181
00:10:47,475 --> 00:10:49,545
Where, which control should be applied?

182
00:10:49,995 --> 00:10:51,795
Risk-based control is what I call it.

183
00:10:52,155 --> 00:10:55,275
So what matters is tracking
and enforcing privacy budgets.

184
00:10:55,875 --> 00:11:00,345
Exposing them in dashboards and ensuring
that cryptographically try to training

185
00:11:00,405 --> 00:11:05,265
receipts without these practices, privacy
promises become marketing lines with them.

186
00:11:05,715 --> 00:11:08,325
They just become enforceable guarantees.

187
00:11:08,375 --> 00:11:11,505
And since we touched on that part,
I would like to move straight

188
00:11:11,505 --> 00:11:13,245
into regulatory compliances.

189
00:11:13,910 --> 00:11:14,960
DevOps integration now.

190
00:11:15,430 --> 00:11:19,030
So since we looked at the privacy
integration already, I wanna quickly

191
00:11:19,030 --> 00:11:23,620
move to the complete holistic whole
picture which talks about the DevOps

192
00:11:23,620 --> 00:11:27,930
integration and how regulatory
compliance at such affects it.

193
00:11:28,200 --> 00:11:32,190
So too often compliance is
bottled on at the end, and that

194
00:11:32,190 --> 00:11:33,240
doesn't works in healthcare.

195
00:11:34,080 --> 00:11:34,260
Why?

196
00:11:34,260 --> 00:11:37,230
I say, compliance must be
baked into DevOps, just like.

197
00:11:37,575 --> 00:11:40,425
How we discussed about Secure by Designer.

198
00:11:40,555 --> 00:11:43,225
You input security, right?

199
00:11:43,225 --> 00:11:43,420
When you.

200
00:11:44,080 --> 00:11:45,370
Crafter design, right?

201
00:11:45,610 --> 00:11:49,600
Every code change and every model
training run must be automatically

202
00:11:49,600 --> 00:11:51,910
checked for HIPAA and GDPR safeguards.

203
00:11:52,720 --> 00:11:54,310
So scrubbing is one example.

204
00:11:54,740 --> 00:11:58,730
And it's not the scrubbing that
happens when the data is stored.

205
00:11:58,995 --> 00:12:03,225
It's actually a middleware that has
come into the picture, a new idea

206
00:12:03,225 --> 00:12:08,715
into the architecture that scrubs
data on both sides, even when the

207
00:12:08,715 --> 00:12:13,275
input is captured, as well as when the
output is supposed to be displayed,

208
00:12:13,575 --> 00:12:15,285
whether it's for a service or a human.

209
00:12:15,315 --> 00:12:16,695
Doesn't matter, right?

210
00:12:17,565 --> 00:12:22,035
So the pipeline itself should enforce
differential privacy budgets, generate

211
00:12:22,035 --> 00:12:25,635
documentation, and fail the build if
something is out of bounds, right?

212
00:12:25,635 --> 00:12:29,955
So model cards and lineage should
be produced automatically, not

213
00:12:29,955 --> 00:12:31,845
written months later for operators.

214
00:12:32,775 --> 00:12:36,945
So in my academic and independent
research projects have insisted that

215
00:12:36,945 --> 00:12:42,790
if a training run produces an artifact
that violates policy, the run fails.

216
00:12:43,475 --> 00:12:44,345
Period, right?

217
00:12:44,885 --> 00:12:50,075
This discipline that by the time you're
in front of an auditor, your evidence

218
00:12:50,075 --> 00:12:54,555
package is already generated from
immutable logs and receipts, so this is

219
00:12:54,555 --> 00:12:59,115
what I call about compliance becoming a
byproduct of engineering, not a blocker.

220
00:12:59,460 --> 00:13:03,720
Now this might just work well if
you're just a single hospital Right.

221
00:13:03,720 --> 00:13:08,130
Or a single institution, but
when the scale grows, right?

222
00:13:08,190 --> 00:13:11,980
And when you're talking about
multinational healthcare institutions

223
00:13:11,980 --> 00:13:15,040
or, even like something that is truly.

224
00:13:15,605 --> 00:13:19,995
National which is present throughout the
country or a major part of the country.

225
00:13:20,395 --> 00:13:24,385
Then you face different set of
scaling challenges and I call them

226
00:13:24,435 --> 00:13:26,535
multi-institutional scaling challenges.

227
00:13:26,865 --> 00:13:30,165
When you work up across
multiple hospitals, the hardest

228
00:13:30,165 --> 00:13:31,575
problems are not technical.

229
00:13:32,115 --> 00:13:34,200
They're human and organizational.

230
00:13:34,220 --> 00:13:37,850
So just like any other industry some
institutions have data science teams,

231
00:13:38,180 --> 00:13:40,130
others barely have it staff, right?

232
00:13:40,340 --> 00:13:44,000
Some have GPU clusters, other can
only contribute via managed notes.

233
00:13:44,380 --> 00:13:47,440
Scheduling conflicts,
cultural differences, and data

234
00:13:47,440 --> 00:13:48,460
quality issues are the norm.

235
00:13:48,725 --> 00:13:53,765
So research pilots have collapsed because
smaller hospitals felt excluded, or

236
00:13:53,765 --> 00:13:57,995
because validation threshold weren't
shared transparently, the solution

237
00:13:57,995 --> 00:14:00,845
is to design for unevenness, right?

238
00:14:01,055 --> 00:14:06,575
You can't just expect a perfect response
for a system that is designed right.

239
00:14:07,285 --> 00:14:12,535
What I mean by that is provide turnkey
edge no for resource for sites, right?

240
00:14:12,835 --> 00:14:17,785
Validate schemas per site to catch data
drift and create incentive structure

241
00:14:17,785 --> 00:14:19,615
so every participant sees benefit.

242
00:14:19,885 --> 00:14:23,245
If you don't solve the human and
governance side, no amount of

243
00:14:23,245 --> 00:14:25,345
cryptography will save your rollout.

244
00:14:25,945 --> 00:14:29,905
And with that, of course the emphasis
should be that, how good this is working.

245
00:14:30,155 --> 00:14:31,415
And since I mentioned that.

246
00:14:32,135 --> 00:14:33,545
The next slide is very important.

247
00:14:33,785 --> 00:14:35,465
Platform monitoring and observability.

248
00:14:35,855 --> 00:14:40,925
Observability in this context means
you're seeing enough to ensure safety

249
00:14:41,195 --> 00:14:43,205
without exposing sensitive details.

250
00:14:43,745 --> 00:14:47,885
So you can't log raw patient
patches or stream identifier.

251
00:14:47,915 --> 00:14:53,265
Instead, you need privacy preserving
telemetry, noise metrics, RA

252
00:14:53,265 --> 00:14:55,185
sweeps, aggregate statistics.

253
00:14:55,545 --> 00:14:58,245
At the same time, you must
detect adversarial behavior.

254
00:14:58,620 --> 00:15:02,670
That means monitoring update
magnitudes to catch poisoning

255
00:15:02,670 --> 00:15:08,060
attempts tracking distribution swift
shifts to spot drift and enforcing

256
00:15:08,150 --> 00:15:09,860
consent policies in real time.

257
00:15:10,340 --> 00:15:15,460
So an example would be an alert
must trigger automated quarantine

258
00:15:15,460 --> 00:15:17,110
actions, for example, right?

259
00:15:17,160 --> 00:15:19,125
If isolation of a site that sends.

260
00:15:19,995 --> 00:15:21,285
Am LS updates, right?

261
00:15:21,285 --> 00:15:24,405
Because human triage at
two M isn't enough good.

262
00:15:24,405 --> 00:15:28,875
Monitoring here is both about patient
safety and system integrity, right?

263
00:15:29,505 --> 00:15:32,145
So I think we discussed a lot of things.

264
00:15:32,195 --> 00:15:34,905
We started with, the basic concepts.

265
00:15:34,905 --> 00:15:37,815
We saw the challenges and we saw how.

266
00:15:38,045 --> 00:15:42,165
The three concepts of,
blockchain federation and smart

267
00:15:42,165 --> 00:15:44,415
integration can help things here.

268
00:15:44,895 --> 00:15:48,525
So I just to map it out clearly,
I wanna make sure that we see the

269
00:15:48,525 --> 00:15:51,135
implementation roadmap also, right?

270
00:15:51,425 --> 00:15:53,945
How do we implement it
is always the question.

271
00:15:53,975 --> 00:15:57,965
And since we are not just gonna
do this as an academic study.

272
00:15:58,465 --> 00:15:59,935
Try to turn it into a workshop.

273
00:16:00,325 --> 00:16:02,815
Rolling this out is not a single launch.

274
00:16:02,875 --> 00:16:08,085
I think that's the most important
insight that one needs to understand.

275
00:16:08,085 --> 00:16:13,275
When you're working with a multi scaled
healthcare platform, it's a phase journey.

276
00:16:13,305 --> 00:16:17,025
Phase one establishes the foundation,
which is a permission ledger.

277
00:16:17,865 --> 00:16:21,225
Basic privacy adapters
and pilot integrations.

278
00:16:21,735 --> 00:16:25,695
That's pretty simple, is how
I would set up phase one.

279
00:16:26,355 --> 00:16:29,115
Phase two pilot's, a
non-critical workflow.

280
00:16:29,205 --> 00:16:31,965
You say a readmission
protection with limited risk.

281
00:16:32,505 --> 00:16:35,835
That's when everybody's happy, you've
done all your security checks, your

282
00:16:35,835 --> 00:16:37,665
privacy checks, your compliance check.

283
00:16:37,925 --> 00:16:38,705
Legal's happy.

284
00:16:39,155 --> 00:16:42,365
Phase three expands to more sites
and more robust application.

285
00:16:42,845 --> 00:16:45,545
Then you do your stress testing
with governance and scaling.

286
00:16:45,695 --> 00:16:50,405
And then comes phase four, right where you
move to high stakes clinical applications.

287
00:16:50,405 --> 00:16:51,080
With PHI.

288
00:16:51,390 --> 00:16:56,445
With PII, right with PCI, with full
monitoring and regulatory sign off

289
00:16:56,895 --> 00:17:00,195
at each stage, success means not
just technical performance, but.

290
00:17:00,725 --> 00:17:04,845
Evidences, receipts, compliance
artifacts, or clinical validation.

291
00:17:05,295 --> 00:17:09,895
By treating this as a stage deployment
with clear gates, we build confidence and

292
00:17:09,895 --> 00:17:11,905
reduce the risk of catastrophic failures.

293
00:17:12,355 --> 00:17:15,865
So even though as security you are just
a reviewer, you're just a responder

294
00:17:15,865 --> 00:17:17,365
to something that is given to you.

295
00:17:17,785 --> 00:17:22,265
As I said, you when these initial
meetings happen, when kickoffs happen

296
00:17:22,265 --> 00:17:24,345
for such projects this is exactly.

297
00:17:24,750 --> 00:17:28,230
The success metric or the success
criteria that you should share, that

298
00:17:28,260 --> 00:17:29,970
this needs to be a phased approach.

299
00:17:30,000 --> 00:17:33,840
And that's when security can,
support this project to the maximum.

300
00:17:34,440 --> 00:17:40,360
Finally, like I said, as an industry
again, this is a, an honest attempt that I

301
00:17:40,360 --> 00:17:45,540
want to share some of these best practices
that, again I've learned throughout

302
00:17:45,540 --> 00:17:49,610
the years through my research, through
my study, through my affiliations with

303
00:17:49,610 --> 00:17:51,910
academic institutions and through my.

304
00:17:52,490 --> 00:17:56,360
And the real point of this is to
transform healthcare through privacy.

305
00:17:56,660 --> 00:17:58,520
So privacy is for healthcare, right?

306
00:17:58,620 --> 00:18:01,840
You cannot have the horse
before the card sort of thing.

307
00:18:01,840 --> 00:18:05,620
So the real point is privacy
and progress are not opposites.

308
00:18:05,950 --> 00:18:07,870
Privacy is what unlocks progress.

309
00:18:07,875 --> 00:18:08,560
As they say.

310
00:18:08,740 --> 00:18:09,940
Privacy lets you.

311
00:18:10,570 --> 00:18:13,000
Work and play in a connected
environment, right?

312
00:18:13,000 --> 00:18:17,200
So without trust, institutions
won't share without collaboration.

313
00:18:17,650 --> 00:18:23,500
AI models would stagnate by embedding
serenity proof and compliance

314
00:18:23,500 --> 00:18:24,880
into the fabric of a system.

315
00:18:25,150 --> 00:18:29,920
We create a platform where hospitals,
researchers, and pharma can work together.

316
00:18:30,910 --> 00:18:33,820
We stop asking people to trust
our intentions and start showing

317
00:18:33,820 --> 00:18:35,290
them cryptographic evidence.

318
00:18:35,350 --> 00:18:37,720
And blockchain is definitely the answer.

319
00:18:38,150 --> 00:18:38,930
The result is not.

320
00:18:39,600 --> 00:18:44,110
Better ai but faster breakthroughs,
safer clinical tools, and of

321
00:18:44,110 --> 00:18:45,970
course stronger patient trust.

322
00:18:46,420 --> 00:18:48,790
That is what I mean when
I say privacy first.

323
00:18:48,940 --> 00:18:51,520
AI patient trust is all I mean by that.

324
00:18:52,180 --> 00:18:54,850
Thank you for, spending
this session with me.

325
00:18:55,210 --> 00:18:58,150
If you take away one idea, let it be this.

326
00:18:58,675 --> 00:19:03,895
I would say in healthcare ai, more
computation improves not data.

327
00:19:04,555 --> 00:19:09,895
That simple shift preserves patient
trust, satisfy, satisfies, regulators,

328
00:19:10,135 --> 00:19:12,235
and accelerates medical innovation.

329
00:19:12,715 --> 00:19:17,185
I'd be glad to dive deeper into specific
controls, governance and rollout patterns.

330
00:19:17,465 --> 00:19:21,615
During q and a. First has my
email address and I would love to.

331
00:19:22,155 --> 00:19:26,445
Get comments, feedback questions
from the folks from the industry.

332
00:19:27,015 --> 00:19:29,145
I thank you and have a
good rest of the day.

