1
00:00:00,930 --> 00:00:02,009
Hello everyone.

2
00:00:02,009 --> 00:00:05,820
Good morning, good afternoon, and
good evening to wherever you are.

3
00:00:05,880 --> 00:00:10,680
I am really excited and thrilled to
be speaking in front of you today.

4
00:00:10,730 --> 00:00:14,240
Today I'm going to be talking
about unlocking observability

5
00:00:14,240 --> 00:00:16,880
with react and no J Applications.

6
00:00:17,509 --> 00:00:19,610
My name is Moi Ani.

7
00:00:19,670 --> 00:00:25,520
I am currently a staff software
engineer for Twilio and I've been

8
00:00:25,520 --> 00:00:27,200
with them for almost four years.

9
00:00:27,890 --> 00:00:33,010
I'm part of the phone numbers
product and what my team is basically

10
00:00:33,010 --> 00:00:38,350
responsible for is procuring numbers
and all of its management so we

11
00:00:38,350 --> 00:00:45,410
can enable our cross channels like
email messaging and also drive other

12
00:00:45,450 --> 00:00:47,040
communication products as well.

13
00:00:49,320 --> 00:00:51,980
But for today we'll be diving
into a critical piece of.

14
00:00:53,100 --> 00:00:57,030
Full stack observability specifically
for applications built with React

15
00:00:57,030 --> 00:01:02,809
on the front end and no js on the
backend in today's digital landscape.

16
00:01:03,200 --> 00:01:07,749
Understanding how these two key parts
of our application work together

17
00:01:08,170 --> 00:01:10,420
has become more important than ever.

18
00:01:10,690 --> 00:01:13,300
So without further ado, let's dive in.

19
00:01:14,300 --> 00:01:17,820
So quickly going through the agenda
today first I'll be talking a little

20
00:01:17,820 --> 00:01:22,860
bit about why do I think there is
a need for a unified view, and then

21
00:01:22,860 --> 00:01:27,399
I will talk about what does this
unified full stack observability mean.

22
00:01:28,070 --> 00:01:31,460
Then next we will look at how
can we correlate the data that

23
00:01:31,520 --> 00:01:34,860
we receive on front end with
the backend for observability.

24
00:01:36,310 --> 00:01:41,110
Some case studies tools, best
practices, and then we'll conclude

25
00:01:41,110 --> 00:01:42,970
with some some of our findings.

26
00:01:43,970 --> 00:01:46,760
Our applications are no longer monolith.

27
00:01:47,330 --> 00:01:52,080
We have sophisticated user interfaces
built with React, interacting with

28
00:01:52,080 --> 00:01:54,360
backend APIs, powered by node js.

29
00:01:55,274 --> 00:02:00,195
This complexity makes it challenging
to understand the root case, root

30
00:02:00,195 --> 00:02:04,994
cause of issues when we treat the front
end and backend as isolated systems.

31
00:02:05,774 --> 00:02:09,165
Traditional monitoring approaches
which look at these in silos,

32
00:02:09,555 --> 00:02:12,584
often fail to capture the crucial
interactions between them.

33
00:02:13,334 --> 00:02:16,605
This is where full track
observability comes in, offering

34
00:02:16,605 --> 00:02:19,065
a way to see the entire picture.

35
00:02:19,644 --> 00:02:23,625
One example that I really like to use it,
think of it like, trying to understand

36
00:02:23,625 --> 00:02:27,945
a play only by watching the actors on
stage or just by reading the script.

37
00:02:28,454 --> 00:02:30,554
You need both to grasp the full narrative.

38
00:02:30,975 --> 00:02:36,045
Similarly, we need to see both the
user experience and the server's

39
00:02:36,045 --> 00:02:40,035
operations to truly understand
the state of our applications.

40
00:02:41,035 --> 00:02:45,675
So what exactly is unified
full stack observability for

41
00:02:45,705 --> 00:02:47,565
react and no J applications?

42
00:02:48,225 --> 00:02:53,025
It's about having a single comprehensive
view of everything that's happening, right

43
00:02:53,025 --> 00:02:58,395
from the user's interaction on the React
interface to the server side logic, and

44
00:02:58,395 --> 00:03:00,345
then data handling at the data layer.

45
00:03:00,885 --> 00:03:05,145
This approach recognizes that the front
end and backend are deeply connected.

46
00:03:05,565 --> 00:03:07,935
And their performance
would impact each other.

47
00:03:09,045 --> 00:03:14,415
To achieve this, we basically need
tools that can automatically link data

48
00:03:14,625 --> 00:03:19,425
from both sides, giving us a complete
understanding without any manual effort.

49
00:03:20,145 --> 00:03:23,115
This is a significant step up
from traditional monitoring

50
00:03:23,475 --> 00:03:25,065
with the front end and backend.

51
00:03:25,095 --> 00:03:28,575
Teams would look at these
things in isolation.

52
00:03:28,575 --> 00:03:32,775
They will have their own dashboards, and
then they will try to connect the dots.

53
00:03:32,835 --> 00:03:32,865
Okay.

54
00:03:33,585 --> 00:03:35,415
Imagine a slow loading page.

55
00:03:35,865 --> 00:03:39,465
Is it the React code
rendering, the network request?

56
00:03:39,855 --> 00:03:43,575
The No J server processing
or a database query?

57
00:03:44,205 --> 00:03:48,825
Unified observability should help
us answer this exact situation.

58
00:03:49,334 --> 00:03:53,505
It moves us beyond from just
knowing what is happening, to

59
00:03:53,505 --> 00:03:58,155
understanding where in the system it
is happening and why it is happening.

60
00:03:59,155 --> 00:04:02,985
So let's talk about why should
we strive for this unified,

61
00:04:02,985 --> 00:04:04,605
integrated observability?

62
00:04:05,415 --> 00:04:07,245
The benefits are substantial.

63
00:04:07,605 --> 00:04:12,945
First, when issues would arise, we can
pinpoint by looking at the dashboard

64
00:04:13,555 --> 00:04:17,215
the cause much faster because we are
looking at a complete picture, right?

65
00:04:17,605 --> 00:04:21,311
So it'll cause a significant
or reduced MTTR or MTTD.

66
00:04:22,060 --> 00:04:25,690
Secondly, it fosters better collaboration
between the front end and backend

67
00:04:25,690 --> 00:04:29,740
teams as they would have a shared,
collected view of the system's health.

68
00:04:30,350 --> 00:04:35,930
Thirdly, we can gain deeper insights
into the performance and then use it to

69
00:04:35,980 --> 00:04:38,020
prepare for ongoing releases as well.

70
00:04:39,910 --> 00:04:41,965
This will also help us in
identifying the bottlenecks.

71
00:04:42,670 --> 00:04:47,740
That might span across a front end
backend, or it could also be something

72
00:04:47,740 --> 00:04:49,810
between front end and backend.

73
00:04:50,500 --> 00:04:54,920
This data then allows us to make informed
decisions about how we can optimize

74
00:04:54,920 --> 00:04:59,680
our applications and also allocate
these resources more effectively.

75
00:05:00,289 --> 00:05:03,530
Finally we can use anomaly detection.

76
00:05:03,920 --> 00:05:08,229
Across our entire stack which
can, which will often help us to

77
00:05:08,229 --> 00:05:12,130
catch potential problems before
they can even affect our users.

78
00:05:14,080 --> 00:05:19,270
Ultimately as we've discussed this would
lead to a much better user engagement.

79
00:05:19,840 --> 00:05:23,730
To the point I would like to say
happiness for our users and customers.

80
00:05:24,250 --> 00:05:26,050
More efficient development cycles.

81
00:05:26,455 --> 00:05:30,115
At a more stable, more efficient,
and a more reliable application.

82
00:05:30,795 --> 00:05:36,075
Think about also the cost savings
from reduced downtime and the

83
00:05:36,075 --> 00:05:40,474
increased user satisfaction from
consistently performing application.

84
00:05:41,474 --> 00:05:47,594
So next, the interesting part is how
do we correlate the data coming from

85
00:05:47,594 --> 00:05:50,204
the front end and the backend, right?

86
00:05:50,204 --> 00:05:54,514
Because as we just discussed, we want
our whatever action is happening from

87
00:05:54,514 --> 00:05:57,784
front end, we want to be able to click
it down from backend to the data.

88
00:05:58,634 --> 00:06:00,654
There are several
techniques to achieve this.

89
00:06:01,254 --> 00:06:02,825
One is trace propagation.

90
00:06:03,799 --> 00:06:09,430
Where a unique ID is generated and
whatever user action happens it's

91
00:06:09,430 --> 00:06:11,110
basically assigned a unique id.

92
00:06:11,590 --> 00:06:14,469
And that unique ID is carried
through the backend requests.

93
00:06:15,080 --> 00:06:20,705
We do have some standards for that, like
trace context and adders and like Ry

94
00:06:20,710 --> 00:06:23,020
Trace or Trace Parent facilitate this.

95
00:06:23,690 --> 00:06:28,280
Another thing we can do is we can use
correlation IDs or session IDs to link

96
00:06:28,280 --> 00:06:33,240
these user activities and embed these
IDs and our logs allows, will allow

97
00:06:33,240 --> 00:06:35,490
us to trace a re request journey.

98
00:06:36,180 --> 00:06:40,440
And importantly, many observability
platforms integrate real user

99
00:06:40,440 --> 00:06:41,730
monitoring on the front end.

100
00:06:41,790 --> 00:06:46,150
With a PM on the backend application
prog performance monitoring

101
00:06:46,610 --> 00:06:47,930
automating this correlation.

102
00:06:48,615 --> 00:06:52,035
Open Telemetry provides a vendor
neutral way to implement this

103
00:06:52,085 --> 00:06:53,225
context propagation as well.

104
00:06:54,135 --> 00:06:59,345
As you can see I've added a
request with get host transparent

105
00:06:59,405 --> 00:07:02,285
and Trace Parent and trace State.

106
00:07:04,365 --> 00:07:07,275
This ensures that when a user clicks
on the button on the front end, we

107
00:07:07,275 --> 00:07:10,575
can follow that specific action as
it travels through the entire system.

108
00:07:11,295 --> 00:07:12,705
If we do not have it.

109
00:07:13,335 --> 00:07:18,505
You are basically looking at fragmented
pieces of information, and as

110
00:07:18,505 --> 00:07:22,285
discussed earlier, this definitely
makes debugging very difficult.

111
00:07:23,285 --> 00:07:27,440
Now to effectively monitor our
react and node JS applications,

112
00:07:27,560 --> 00:07:31,975
we need to target track specific
KPIs or key performance indicators.

113
00:07:33,030 --> 00:07:36,570
So I've added some key performance
indicators in this table.

114
00:07:36,620 --> 00:07:38,690
Let's go through them one, one by one.

115
00:07:38,690 --> 00:07:42,750
And I think they are some
of the most important ones.

116
00:07:43,380 --> 00:07:46,860
For example, users perceive
latency tells us how quickly

117
00:07:47,205 --> 00:07:48,975
or how fast the application is.

118
00:07:49,455 --> 00:07:52,935
Error rates across both front end
and back end are basically telling

119
00:07:52,935 --> 00:07:54,585
us how stable the application is.

120
00:07:55,195 --> 00:07:59,975
What kind of errors can also be monitored,
are being experienced by the users.

121
00:08:00,395 --> 00:08:04,595
Then we have end-to-end request
latency, which basically shows us the

122
00:08:04,595 --> 00:08:06,545
total time a given request is taking.

123
00:08:08,255 --> 00:08:11,625
Then we have throughput,
which is basically how much

124
00:08:11,625 --> 00:08:12,945
our application can handle.

125
00:08:13,695 --> 00:08:18,105
Then our resource utilization helps
us identify our potential bottlenecks.

126
00:08:18,945 --> 00:08:25,905
You can figure out which part of our
resource chain is getting slow or acting

127
00:08:25,905 --> 00:08:27,825
up, and we can address this accordingly.

128
00:08:28,395 --> 00:08:32,625
Then user engagement metrics is
connecting performance to user

129
00:08:32,625 --> 00:08:34,995
behavior and core pep vitals

130
00:08:37,125 --> 00:08:41,155
are crucial for understanding the
front end user experience, which

131
00:08:41,155 --> 00:08:42,865
is often influenced by the backend.

132
00:08:43,565 --> 00:08:48,305
Tracking these KPIs together like
in a dashboard, a setup will give

133
00:08:48,305 --> 00:08:52,115
us a comprehensive understanding
of our application's health, both

134
00:08:52,115 --> 00:08:53,525
on the front end and backend.

135
00:08:54,185 --> 00:09:00,305
And these KPIs in a way could, the way
I see it as that they would basically be

136
00:09:00,305 --> 00:09:05,795
starting conversation starters for front
end backend teams to discuss performance.

137
00:09:06,335 --> 00:09:08,795
By monitoring these, my
metrics holistically.

138
00:09:09,305 --> 00:09:12,905
We can understand how changes in
one part of the stack can affect

139
00:09:12,905 --> 00:09:18,715
the other, and ultimately the user
experience and the user engagement.

140
00:09:19,715 --> 00:09:24,905
So as we discussed, distributed tracing
is a powerful technique for understanding

141
00:09:24,905 --> 00:09:27,335
how requests flow through the application.

142
00:09:27,950 --> 00:09:31,610
It involves breaking down each
request into a series of operations.

143
00:09:32,090 --> 00:09:33,410
That's called a span.

144
00:09:35,300 --> 00:09:39,930
And a series of a series
of spans form a trace.

145
00:09:40,590 --> 00:09:45,600
Now the key here, as I mentioned, was
the key here is context propagation,

146
00:09:45,960 --> 00:09:50,400
ensuring that each span, whether it's on
the front end or the backend, is linked

147
00:09:50,400 --> 00:09:53,035
together using the unique trace id.

148
00:09:53,790 --> 00:09:57,210
This allows us to visualize
the entire journey of a request

149
00:09:57,630 --> 00:10:02,280
and pinpoint exactly where any
slowdowns or errors are happening.

150
00:10:03,270 --> 00:10:07,930
Telemetry provides a standard
approach for this to implement this.

151
00:10:07,930 --> 00:10:13,340
And tools like Yeager and Zipkin
allow us to visualize these traces.

152
00:10:13,910 --> 00:10:18,650
Now let's imagine a slow, API call
distributed tracing would allow us to see.

153
00:10:19,415 --> 00:10:23,465
If the delay is in the front end, making
the request, the backend receiving

154
00:10:23,465 --> 00:10:28,535
it, or a specific function within
the backend, a database query, or it

155
00:10:28,535 --> 00:10:32,725
could be like a external service call,
which is taking a really long time.

156
00:10:33,385 --> 00:10:37,825
This level of detail is invaluable
for very efficient debugging.

157
00:10:38,825 --> 00:10:41,225
Now let's look at some
real world scenarios.

158
00:10:41,765 --> 00:10:45,695
Imagine if there was a sudden increase
in your errors on the React front

159
00:10:45,695 --> 00:10:48,335
end with unified observability.

160
00:10:48,815 --> 00:10:52,715
You would be able to see that this might,
let's say, be coinciding with an increased

161
00:10:52,715 --> 00:10:54,575
latency in your specific backend.

162
00:10:54,605 --> 00:10:59,795
API call immediately pointing to the
backend as a source of problem, or

163
00:10:59,795 --> 00:11:03,005
perhaps you notice that users are taking
a particular path on the front end.

164
00:11:03,650 --> 00:11:06,440
That leads to high CPU usage
on the backend service.

165
00:11:06,440 --> 00:11:09,540
This could be a specific
endpoint or a specific route.

166
00:11:11,160 --> 00:11:15,400
By seeing both these sites you can
optimize that the specific user flow.

167
00:11:16,210 --> 00:11:20,470
For instance, if a user has a payment
failure, a trace might show you the

168
00:11:20,470 --> 00:11:26,680
front end request, the call to your
back backend, and then a timeout when

169
00:11:26,680 --> 00:11:29,590
your backend is trying to communicate
with a third party payment gateway.

170
00:11:29,635 --> 00:11:29,665
Okay.

171
00:11:30,355 --> 00:11:35,305
Similarly, slow loading of product images
on the front end might be traced back

172
00:11:35,305 --> 00:11:39,895
to a slow API response from your node
JS server fetching that image data.

173
00:11:40,525 --> 00:11:44,905
Now, for those using the MON Stack
observability solutions often

174
00:11:44,905 --> 00:11:48,135
provide specific integrations to
monitor all of these components.

175
00:11:48,135 --> 00:11:52,545
Together, these examples highlight
how unified observability

176
00:11:52,935 --> 00:11:56,895
transforms, debugging from
guesswork to a very precise science.

177
00:11:57,435 --> 00:12:01,425
By connecting this user behavior with
backend performance, we can quickly

178
00:12:01,485 --> 00:12:06,315
understand the impact of technical
issues on the user experience without

179
00:12:06,315 --> 00:12:12,145
losing that critical time during
an incident, and obviously saving

180
00:12:12,145 --> 00:12:13,675
a lot on the revenue side as well.

181
00:12:14,675 --> 00:12:15,245
Okay.

182
00:12:15,245 --> 00:12:20,285
Let's look at some of the tooling
and products available to us for

183
00:12:20,365 --> 00:12:22,795
implementing this unified observability.

184
00:12:23,275 --> 00:12:25,765
Again, it's a very
crowded space right now.

185
00:12:26,225 --> 00:12:30,805
You have platforms like Datadog and
New Relic, which offer comprehensive

186
00:12:30,805 --> 00:12:33,705
solutions for both React and node js.

187
00:12:34,285 --> 00:12:36,925
Allowing for a seamless
correlation of data.

188
00:12:37,615 --> 00:12:42,055
We have Honeycomb, which focuses
on providing a unified way to

189
00:12:42,625 --> 00:12:44,625
analyze all of our telemetry.

190
00:12:45,705 --> 00:12:47,865
Then we have our Dynatrace ai.

191
00:12:48,525 --> 00:12:54,565
They they basically help us and detect and
analyze issues across the stack as well.

192
00:12:55,165 --> 00:13:00,325
Then you have Elastic Observability and
Grafana Cloud providing robust platforms

193
00:13:00,325 --> 00:13:02,905
for unifying and visualizing data.

194
00:13:03,605 --> 00:13:07,175
Observe specifically focuses on
connecting the front end user

195
00:13:07,175 --> 00:13:10,235
experience with backend troubleshooting.

196
00:13:10,865 --> 00:13:15,125
We have open observe, which is
an open source alternative which

197
00:13:15,125 --> 00:13:16,715
has an open source alternative.

198
00:13:16,745 --> 00:13:22,180
And then underpinning most of these
solutions is open telemetry, which

199
00:13:22,180 --> 00:13:26,190
provides a standardized way to
instrument our all our applications.

200
00:13:26,850 --> 00:13:30,660
Myself for Twilio, we are considering
Open Elementary right now.

201
00:13:31,260 --> 00:13:34,040
Again, I think it's there's
no one size fits all.

202
00:13:34,500 --> 00:13:37,440
Choosing the right tool
depends on your specific needs.

203
00:13:37,920 --> 00:13:42,450
What is the scale and where the,
what your exact requirements

204
00:13:42,510 --> 00:13:44,370
are based on your stack.

205
00:13:44,950 --> 00:13:48,490
Most of these companies offer free
trials, so I would recommend also

206
00:13:48,490 --> 00:13:52,020
exploring doing a few POCs and
taking feedback with the team.

207
00:13:52,635 --> 00:13:57,795
To figure out which we which one fits
your team's work for the best and

208
00:13:57,795 --> 00:13:59,955
provides the insights that you guys need.

209
00:14:00,955 --> 00:14:01,555
Awesome.

210
00:14:01,555 --> 00:14:03,955
So let's talk about
some best practices now.

211
00:14:04,005 --> 00:14:08,955
To truly leverage Unified Observability,
we need to adopt some best practices,

212
00:14:09,525 --> 00:14:11,925
creating dashboards that show key metrics.

213
00:14:12,675 --> 00:14:16,575
And traces from both our front end
and not just backend in one place.

214
00:14:17,055 --> 00:14:19,875
I think it's very essential
for a holistic high level view.

215
00:14:20,535 --> 00:14:24,135
We should set up alerts that
trigger based on correlated data.

216
00:14:24,945 --> 00:14:28,875
For example alerting us only when we
see both us spike in front end errors

217
00:14:28,935 --> 00:14:30,855
and an increase in backend latency.

218
00:14:31,755 --> 00:14:32,595
Then comes

219
00:14:32,675 --> 00:14:34,445
then comes anomaly detection.

220
00:14:34,835 --> 00:14:38,045
Which can help us catch issues
we didn't even know to look for.

221
00:14:38,575 --> 00:14:43,435
Consistent tagging of our telemetry
data also makes it easy to filter and

222
00:14:45,830 --> 00:14:47,150
correlate information.

223
00:14:47,840 --> 00:14:52,400
Defining clear observability goals
and service level objectives helps us

224
00:14:52,400 --> 00:14:57,090
understand what good looks like and
also improves our monitoring setup.

225
00:14:57,630 --> 00:15:02,120
Finally, it is crucial for, to involve
both our front end and backend teams in

226
00:15:02,120 --> 00:15:07,370
defining what we monitor and how we are
alerted, ensuring everyone has a shared

227
00:15:07,370 --> 00:15:14,740
understanding of what good looks like and
making us a very strong, cohesive unit.

228
00:15:15,440 --> 00:15:19,850
One more thing to remember is we
are not just collecting data, but

229
00:15:19,850 --> 00:15:21,440
we need to collect the right data.

230
00:15:21,620 --> 00:15:24,800
And then how do we turn that
data into actionable insights?

231
00:15:25,355 --> 00:15:28,865
Only then we can improve our
application and the user experience.

232
00:15:29,435 --> 00:15:33,365
Regularly review your dashboards
and alerts to ensure they're

233
00:15:33,365 --> 00:15:35,345
still relevant and provide value.

234
00:15:36,345 --> 00:15:40,155
To conclude this talk, embracing
Unified Full Stack Observability

235
00:15:40,155 --> 00:15:43,965
for our React and node applications
offer significant advantages.

236
00:15:44,565 --> 00:15:48,825
It allows us to resolve issues
more quickly, gain deeper insight

237
00:15:48,825 --> 00:15:50,745
into our application performance.

238
00:15:51,390 --> 00:15:53,890
And also allow allow us to resolve

239
00:15:56,590 --> 00:15:58,900
and proactively address
potential problems.

240
00:15:59,440 --> 00:16:04,000
By moving away from siloed monitoring,
we can achieve a much more comprehensive

241
00:16:04,000 --> 00:16:05,590
understanding of our systems.

242
00:16:06,100 --> 00:16:10,630
With the help of modern tools and
open standards like Open Telemetry,

243
00:16:10,630 --> 00:16:14,710
this approach is becoming increasingly
accessible and essential for

244
00:16:14,710 --> 00:16:16,690
building and maintaining reliable.

245
00:16:17,200 --> 00:16:22,060
High performing applications that
deliver excellent user experiences.

246
00:16:24,820 --> 00:16:30,190
To finish, I would like to say
that it, we have the right tools.

247
00:16:30,220 --> 00:16:33,045
It's just that how we use them

248
00:16:35,380 --> 00:16:38,440
to understand and manage these
complex systems efficiently.

249
00:16:38,980 --> 00:16:42,100
Unified, full stack observative
is not just a trend.

250
00:16:42,550 --> 00:16:46,030
But I think it's a fundamental shift
towards building more resilient

251
00:16:46,090 --> 00:16:48,040
and user-centric applications.

252
00:16:48,850 --> 00:16:52,990
I want to thank all of you
for listening to this talk.

253
00:16:53,040 --> 00:16:55,950
I had a really great time sharing
all these insights with you.

254
00:16:56,460 --> 00:17:01,020
If you have more questions, please
don't hesitate to reach out to me and

255
00:17:01,020 --> 00:17:02,790
I look forward to connecting with you.

256
00:17:03,240 --> 00:17:03,630
Bye-bye.

