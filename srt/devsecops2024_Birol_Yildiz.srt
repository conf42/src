1
00:00:00,030 --> 00:00:05,360
Hi, welcome to my talk, manage
alert overload with AI ops.

2
00:00:06,090 --> 00:00:09,130
In this talk, you will learn how
you can leverage AI and large

3
00:00:09,160 --> 00:00:12,950
language models to solve one of the
biggest problems when being on call

4
00:00:13,499 --> 00:00:15,530
alert, flooding and alert fatigue.

5
00:00:16,379 --> 00:00:19,429
My name is Birol and I am the
CEO and co founder of iAlert.

6
00:00:19,929 --> 00:00:23,379
iAlert is a software company based
in Germany and provides an incident

7
00:00:23,419 --> 00:00:28,709
response platform and covers everything
during the incident response lifecycle,

8
00:00:29,209 --> 00:00:32,649
Preparing for incidents, responding
to incidents, communicating incidents,

9
00:00:32,669 --> 00:00:34,049
and also learning from incidents.

10
00:00:34,599 --> 00:00:39,809
Some of the capabilities that we have
are sophisticated alerting, managing

11
00:00:39,819 --> 00:00:44,699
on call and schedules and overrides
in iAlert, and also communicating

12
00:00:44,709 --> 00:00:46,599
incidents through status pages.

13
00:00:47,539 --> 00:00:53,249
Our solution is used by small and
large companies alike across the world,

14
00:00:53,809 --> 00:00:59,629
and DevOps and ITOps and many service
providers use iAlert to, Improve the

15
00:00:59,629 --> 00:01:03,799
operational efficiency, efficiency,
respond faster to incidents and

16
00:01:03,849 --> 00:01:07,339
provide a better service and better
service uptime to, to their customers

17
00:01:07,839 --> 00:01:09,269
before we dig into the talk.

18
00:01:09,299 --> 00:01:12,749
Let's first start with the why,
what, why is it important, to

19
00:01:12,759 --> 00:01:17,599
effectively manage alert overload
leads to, high MTTA and high MTTR.

20
00:01:18,334 --> 00:01:22,894
Makes you, makes you less efficient
and, increases the stress, right?

21
00:01:23,154 --> 00:01:25,274
And in a situation where, you cannot.

22
00:01:25,969 --> 00:01:29,409
You don't need additional stress
because we're talking about, situations

23
00:01:29,419 --> 00:01:33,819
where you're probably experiencing a
major incident and being paged at 3 a.

24
00:01:33,819 --> 00:01:33,949
m.

25
00:01:33,949 --> 00:01:34,529
in the morning.

26
00:01:35,059 --> 00:01:39,500
And the last thing you need is additional
threat, additional stress through a like

27
00:01:39,500 --> 00:01:43,769
mismanaged alert configuration where
you're being flooded with, with alerts.

28
00:01:43,770 --> 00:01:46,929
And in this webinar, I'm going to
show you how you can effectively

29
00:01:46,929 --> 00:01:51,919
manage alert overload using something
we call intelligent alert grouping.

30
00:01:52,419 --> 00:01:56,719
And to set the stage, I'd like to show
the results of one of our customers,

31
00:01:57,579 --> 00:02:03,059
only after two weeks, that particular
customer was able to reduce alert volume

32
00:02:03,119 --> 00:02:09,999
by 93 percent and also improve their
response time by more than four hours.

33
00:02:10,899 --> 00:02:17,189
And as every minute you save in response
time is a minute improvement in your MTTR.

34
00:02:18,179 --> 00:02:22,709
And every minute in improved MTTR,
leads to a better, like a better

35
00:02:22,709 --> 00:02:25,149
uptime and an uptime is like service.

36
00:02:25,179 --> 00:02:29,229
Uptime is probably one of the
North, North star metrics that

37
00:02:29,239 --> 00:02:31,669
every, SRE engineer, cares about.

38
00:02:32,169 --> 00:02:32,519
All right.

39
00:02:32,609 --> 00:02:36,929
Before I actually start talking about
intelligent alert grouping, I would

40
00:02:36,929 --> 00:02:39,479
like to show you other use cases.

41
00:02:39,879 --> 00:02:44,259
Where we already, have been successfully
leveraging AI and these are use cases.

42
00:02:44,259 --> 00:02:49,009
And we usually like to like cluster things
around the incident response life cycle.

43
00:02:49,519 --> 00:02:53,399
And, one, one use case that we've
implemented probably like six

44
00:02:53,399 --> 00:02:58,949
months ago or so, is, using AI
assistance for on call scheduling.

45
00:02:59,664 --> 00:03:03,104
This is a simple chat interface where
you can just lay out your requirements

46
00:03:03,454 --> 00:03:08,204
and then, the AI assistant will walk you
through the, like the schedule creation

47
00:03:08,204 --> 00:03:12,294
process or not walk you through, but
just get all the input that's necessary.

48
00:03:12,344 --> 00:03:13,764
Let's say you have a complex schedule.

49
00:03:13,764 --> 00:03:15,304
You want to create a
follow the sun schedule.

50
00:03:15,664 --> 00:03:15,924
You can just.

51
00:03:16,424 --> 00:03:18,034
in, in like using natural language.

52
00:03:18,324 --> 00:03:21,004
So that's one area where
we've been using, AI.

53
00:03:21,934 --> 00:03:25,504
Another area, probably the most
obvious one is, when you want

54
00:03:25,504 --> 00:03:30,184
to communicate incidents to your
customers, maybe on a status page.

55
00:03:30,714 --> 00:03:34,514
And, when you want to communicate
an incident, first and foremost, you

56
00:03:34,514 --> 00:03:35,694
don't want to spend too much time.

57
00:03:36,159 --> 00:03:40,549
on communicating incidents because
what you actually want is reduce the

58
00:03:40,549 --> 00:03:42,399
business impact and solve the problem.

59
00:03:42,449 --> 00:03:46,199
but of course, communicating incidents
also helps you, like focusing on the

60
00:03:46,199 --> 00:03:49,839
problem because then you have like
less people asking what's the status

61
00:03:49,849 --> 00:03:51,279
or less customer support requests.

62
00:03:51,899 --> 00:03:56,674
And we've embedded, like AI and large
language models into our software where

63
00:03:56,674 --> 00:04:01,054
you can just provide a few words and
it'll create the entire incident for you.

64
00:04:01,574 --> 00:04:05,234
Use a message in a tone that's
appropriate for the audience

65
00:04:05,244 --> 00:04:08,484
when you want to communicate
incidents and make it very easy.

66
00:04:09,254 --> 00:04:13,514
And, another area where we're using AI
is, in the post, like post production.

67
00:04:14,034 --> 00:04:17,704
post incident phase, which is, which
are, the creation of post mortems.

68
00:04:18,364 --> 00:04:21,914
So post mortems, they are, they
are commonly used across tech teams

69
00:04:22,334 --> 00:04:26,784
where they see an incident as an
opportunity and try to learn from them.

70
00:04:27,344 --> 00:04:30,884
And a post mortem document is
essentially a document that lays out

71
00:04:31,624 --> 00:04:34,984
what happened, the incident timeline,
what actions were taken, what was the

72
00:04:34,984 --> 00:04:38,974
business impact, what was the root
cause and what are we going to do in

73
00:04:38,974 --> 00:04:40,584
the future to avoid this incident.

74
00:04:40,614 --> 00:04:41,224
In other words.

75
00:04:41,429 --> 00:04:43,399
How can we learn from this incident?

76
00:04:43,899 --> 00:04:46,409
And if you ever happen to be
in a major incident, you know

77
00:04:46,409 --> 00:04:49,859
that, first of all, there's lots
of data that's being collected.

78
00:04:50,539 --> 00:04:53,759
One type of data is machine
generated, like monitoring

79
00:04:53,759 --> 00:04:55,159
tools, sending constant updates.

80
00:04:55,259 --> 00:04:59,089
maybe you have like other automation
tools that do things, then, you,

81
00:04:59,089 --> 00:05:01,604
you usually collaborate in a,
in a dedicated, environment.

82
00:05:01,934 --> 00:05:02,974
channel chat channel.

83
00:05:03,334 --> 00:05:06,334
what's what we usually
would call a war room.

84
00:05:06,734 --> 00:05:10,514
and then, you collaborate on this
incident and exchange a lot of

85
00:05:10,514 --> 00:05:14,824
information, coordinate tasks and
really, collaborate on the incident.

86
00:05:15,324 --> 00:05:20,939
And these, like all these data, They
contain lots of information, which,

87
00:05:21,019 --> 00:05:24,029
which are usually the foundation
for your postmortem document.

88
00:05:24,809 --> 00:05:28,879
Now you can either go ahead and,
recreate the timeline yourself and

89
00:05:28,889 --> 00:05:33,459
whatever was set, whatever actions
we're taking, or, And, In this area, it

90
00:05:33,469 --> 00:05:38,319
almost feels irresponsible not to use,
not to leverage AI, have an AI, like

91
00:05:38,329 --> 00:05:42,379
scan through all the data, scan through
your chat history, look at all the

92
00:05:42,379 --> 00:05:46,289
machine generated data and reconstruct
the incident timeline, what actions,

93
00:05:46,329 --> 00:05:50,599
were taken, what was the root cause of
the incident, based on the chat, chat

94
00:05:50,609 --> 00:05:55,009
messages and really give you an 80 percent
version of your post mortem documents.

95
00:05:55,009 --> 00:05:59,239
So you can spend the remaining 20
percent on, like the most crucial

96
00:05:59,239 --> 00:06:02,419
phase, which is like learning
from this incident and discussing

97
00:06:02,419 --> 00:06:04,349
with your team how to get better.

98
00:06:04,369 --> 00:06:08,309
And we think that's an area, that's,
it's meant to be, meant to be

99
00:06:08,309 --> 00:06:10,119
improved, with, with the help of AI.

100
00:06:10,619 --> 00:06:14,519
But today we're not going to talk
about, These incident stages, we're

101
00:06:14,539 --> 00:06:16,409
going to talk about the response phase.

102
00:06:17,019 --> 00:06:19,289
And in particular, we're going to
talk about how you can leverage

103
00:06:19,299 --> 00:06:23,169
AI to, to reduce alert fatigue
and manage alert overload.

104
00:06:23,669 --> 00:06:25,329
This time I'm going to
do something different.

105
00:06:25,329 --> 00:06:30,069
So usually I first like layout what
were, like how we did it, and, talk about

106
00:06:30,069 --> 00:06:34,319
some theoretical concepts, how we've
implemented, intelligent AI grouping.

107
00:06:34,969 --> 00:06:37,319
But this time I would like
to like first show you.

108
00:06:37,954 --> 00:06:39,084
AI grouping and action.

109
00:06:39,484 --> 00:06:42,864
And then, which is probably
the most interesting phase.

110
00:06:42,864 --> 00:06:44,294
and you just see how it works.

111
00:06:45,154 --> 00:06:48,324
And then, with the hope that
it's like clearer when I try to

112
00:06:48,324 --> 00:06:50,854
explain how we did it, because
what the end result was, right?

113
00:06:51,704 --> 00:06:55,654
So let's start, with a small demo
of, intelligent alert grouping.

114
00:06:56,154 --> 00:06:59,524
but before I start, let's
quickly define what we mean

115
00:06:59,524 --> 00:07:01,574
by intelligent alert grouping.

116
00:07:02,014 --> 00:07:07,094
So intelligent alert grouping is,
the process of identifying alerts

117
00:07:07,104 --> 00:07:11,404
that refer to the same issue and
consolidating them into one alert.

118
00:07:12,404 --> 00:07:17,414
and this reduces noise and prevents,
alert fatigue, prevents you from being

119
00:07:17,424 --> 00:07:21,244
overwhelmed to have looked, having
to look at multiple alerts and seeing

120
00:07:21,244 --> 00:07:25,224
whether this is a new issue or whether
it refers to the same issue that you

121
00:07:25,224 --> 00:07:26,634
were just paged a few minutes ago.

122
00:07:26,690 --> 00:07:28,110
so let's get into, the demo.

123
00:07:28,220 --> 00:07:31,630
So this is, I learned, right
now there are zero alerts.

124
00:07:32,100 --> 00:07:35,620
And, what I'm going to do now, I'm
going to create a few sample alerts

125
00:07:35,890 --> 00:07:39,400
and see how they behave and, and alert
grouping, is turned on right now.

126
00:07:39,850 --> 00:07:44,260
So let's switch to Postman and I'm
using Postman here just for this demo,

127
00:07:44,260 --> 00:07:45,980
but, this can be any monitoring tool.

128
00:07:45,980 --> 00:07:50,390
So we are, agnostic when it comes to
monitoring, we have 100 plus integrations.

129
00:07:51,030 --> 00:07:55,230
but for the sake of this demo and to make
this a little bit more, smooth and not

130
00:07:55,240 --> 00:07:59,680
having to jump between, tools, I'm going
to use postman and trigger a few alerts.

131
00:08:00,080 --> 00:08:01,590
And the first alert is about.

132
00:08:01,965 --> 00:08:05,235
A coffee machine being, like
being down, in an office.

133
00:08:05,595 --> 00:08:10,505
So the alert says coffee machine
on floor two needs a cafe in boost

134
00:08:10,535 --> 00:08:15,685
maintenance, so let's send this alert
to alert and of course, what we'd expect

135
00:08:15,695 --> 00:08:18,675
is that a new alert is created, right?

136
00:08:19,035 --> 00:08:22,925
That's exactly what happens because
it's the first alert and there's

137
00:08:22,925 --> 00:08:25,435
nothing to group and let's now move on.

138
00:08:25,935 --> 00:08:30,215
create another alert regarding the same
problem, but use a different wording.

139
00:08:30,375 --> 00:08:34,385
alert coffee machine on floor
two is taking a coffee break.

140
00:08:35,035 --> 00:08:38,425
So it's still the coffee machine
that's not working properly, but

141
00:08:38,445 --> 00:08:40,025
we've rephrased it a little bit.

142
00:08:40,525 --> 00:08:41,845
So let's send it to iAlert.

143
00:08:42,345 --> 00:08:47,315
And as you can see, there still is only
one alert and there's a small indicator

144
00:08:47,405 --> 00:08:50,745
that's, that's animating here that
says, there are already two alerts that

145
00:08:50,745 --> 00:08:52,515
have been aggregated into this alert.

146
00:08:52,595 --> 00:08:55,885
And if we would look into this alert,
we'd see which alerts have been

147
00:08:55,885 --> 00:09:01,785
aggregated, but as you can see, although
the two alerts are not, are only

148
00:09:01,785 --> 00:09:05,725
somewhat similar on a textual level,
the AI is able to understand that.

149
00:09:05,725 --> 00:09:06,025
Okay.

150
00:09:06,330 --> 00:09:08,300
These refer actually to the same problem.

151
00:09:08,800 --> 00:09:11,090
And, let's move on with another example.

152
00:09:11,590 --> 00:09:14,880
let's try to be funny and say, instead
of talking about coffee, we're going

153
00:09:14,880 --> 00:09:19,310
to talk about Java, which also means,
coffee, no Java from the Java machine,

154
00:09:19,320 --> 00:09:21,250
floor two coffee maker is down.

155
00:09:21,750 --> 00:09:23,240
So let's send this to I alert.

156
00:09:23,740 --> 00:09:28,700
And again, here would expect no new alert
to be created because it's the same.

157
00:09:29,235 --> 00:09:30,675
Coffee machine on floor two.

158
00:09:31,015 --> 00:09:32,635
So no need to create a new, new alert.

159
00:09:32,725 --> 00:09:35,955
And as you can see here,
the indicator, is now three.

160
00:09:36,175 --> 00:09:39,705
So there are already three events that
were aggregated with these alerts.

161
00:09:40,205 --> 00:09:42,985
Alright, let's provide a counter example.

162
00:09:43,475 --> 00:09:47,005
Let's say something else
on floor two is broken.

163
00:09:47,015 --> 00:09:49,055
The water cooler on floor two is broken.

164
00:09:49,985 --> 00:09:51,055
Let's send this to iAlert.

165
00:09:51,555 --> 00:09:56,405
And here we expect that a new alert
is created because water cooler and

166
00:09:56,405 --> 00:09:59,285
coffee machine are different things,
although they are on the same floor.

167
00:09:59,855 --> 00:10:03,755
So here we can see, even though, like
there is, there are some elements

168
00:10:03,755 --> 00:10:07,500
that are different, the same on a
textual level, but the underlying

169
00:10:07,500 --> 00:10:10,420
semantics are different because one
talks about coffee machines and the

170
00:10:10,420 --> 00:10:11,930
other one talks about water coolers.

171
00:10:12,220 --> 00:10:14,600
So that's why a new
alert, has been created.

172
00:10:15,090 --> 00:10:17,910
And then, and the reason why this
is important, like even if you

173
00:10:17,910 --> 00:10:21,220
are like, you might not even be
the recipient of the second alert.

174
00:10:21,540 --> 00:10:23,610
This might be a completely
different team, right?

175
00:10:23,860 --> 00:10:25,940
And you don't want, to aggregate an alert.

176
00:10:26,440 --> 00:10:28,240
that wasn't intended for your team.

177
00:10:28,820 --> 00:10:31,250
So otherwise, like the team
might never know that there

178
00:10:31,250 --> 00:10:32,390
is something is not working.

179
00:10:33,040 --> 00:10:36,500
and yeah, and the alert, which is the
worst case goes completely unnoticed.

180
00:10:36,530 --> 00:10:39,490
And, you're informed by your customers
that something's not working.

181
00:10:39,990 --> 00:10:41,950
No, let's, move on with another example.

182
00:10:42,450 --> 00:10:46,430
Probably you're thinking, okay,
these are good examples and

183
00:10:46,440 --> 00:10:47,600
they are easy to understand.

184
00:10:48,100 --> 00:10:50,280
But they don't exist in,
in the world of tech.

185
00:10:50,320 --> 00:10:53,580
Like my alerts are not about
coffee machines or water coolers.

186
00:10:53,620 --> 00:10:56,970
My alerts are about like hardware,
about, Kubernetes clusters, about

187
00:10:56,990 --> 00:10:59,400
pods, not being, like being terminated.

188
00:10:59,450 --> 00:11:01,540
CPU is running high, so on and so forth.

189
00:11:02,150 --> 00:11:02,450
All right.

190
00:11:02,450 --> 00:11:02,890
Fair enough.

191
00:11:02,890 --> 00:11:06,110
So let's use a more technical alerts.

192
00:11:06,850 --> 00:11:08,830
One that says the hard drive is full.

193
00:11:09,330 --> 00:11:09,710
So let's.

194
00:11:10,120 --> 00:11:14,530
Let's send this to iAlert and as
expected, a new alert is being created.

195
00:11:14,530 --> 00:11:15,430
The hard drive is full.

196
00:11:15,930 --> 00:11:20,590
And let's rephrase the same
problem with different words.

197
00:11:20,680 --> 00:11:26,010
this time we're not going to reuse any of
the words that was in the previous alert.

198
00:11:26,260 --> 00:11:31,510
So the alert is the storage capacity
has reached the limit of 90%.

199
00:11:31,510 --> 00:11:33,940
As you can see, there are
no common words between.

200
00:11:34,365 --> 00:11:40,245
The hard drive is full and the storage
capacity has reached the limit of 90%.

201
00:11:40,745 --> 00:11:42,155
I'm going to send this alert

202
00:11:42,655 --> 00:11:43,895
and quickly refresh.

203
00:11:43,915 --> 00:11:48,875
And as you can see, Both alerts
have been aggregated into one,

204
00:11:48,895 --> 00:11:50,735
so no new alert was created.

205
00:11:51,425 --> 00:11:56,385
So this, I think this example shows
perfectly what we mean by, like when

206
00:11:57,025 --> 00:11:59,315
the semantics of an alert is captured.

207
00:11:59,645 --> 00:12:02,935
and we're not only looking at the,
textual similarity between alerts.

208
00:12:03,275 --> 00:12:07,915
And here, you can see, that's right now
alert grouping is still in progress.

209
00:12:07,945 --> 00:12:10,005
the grouping window is set
to five minutes, I believe.

210
00:12:10,255 --> 00:12:14,345
So if there are no new alerts, the
grouping will end in a few minutes.

211
00:12:15,245 --> 00:12:18,495
And you can also see like the
entire payload of, of the events

212
00:12:18,495 --> 00:12:22,695
that came in, you still might
be skeptical because, these are.

213
00:12:23,670 --> 00:12:24,670
natural sentences.

214
00:12:24,670 --> 00:12:27,800
Like these are maybe not
representative for real alerts.

215
00:12:27,800 --> 00:12:30,920
You might be thinking my
alerts are like way bigger.

216
00:12:30,950 --> 00:12:34,940
They contain lots of more,
a lot of, more information.

217
00:12:35,470 --> 00:12:38,070
And they might be even a
little like cryptic in nature.

218
00:12:38,120 --> 00:12:38,970
So let's try that.

219
00:12:39,110 --> 00:12:39,970
So let's try.

220
00:12:40,080 --> 00:12:44,520
And what I'm about to do now
is, I am going to, use alerts.

221
00:12:45,020 --> 00:12:48,460
Actually live alerts from,
from a production system.

222
00:12:49,310 --> 00:12:54,830
And these were alerts where we know that,
there were instances where there was.

223
00:12:55,330 --> 00:12:56,720
Many others were being created.

224
00:12:56,720 --> 00:13:00,370
So there was, there clearly was
a case of alert flooding, which

225
00:13:00,370 --> 00:13:03,050
means like in a short period of
time, like maybe 10 minutes, 15

226
00:13:03,050 --> 00:13:05,070
minutes, lots of alerts were created.

227
00:13:05,070 --> 00:13:07,250
And I think in this example,
it's even five minutes or so.

228
00:13:08,050 --> 00:13:13,310
And, what I first, what I'm first
going to do is I am going to, disable

229
00:13:14,200 --> 00:13:16,060
intelligent alert grouping first.

230
00:13:17,060 --> 00:13:19,640
So we see first the effect of, okay.

231
00:13:19,695 --> 00:13:21,485
alerts being created without grouping.

232
00:13:21,525 --> 00:13:23,945
And then we'll look at
that at the same alert.

233
00:13:24,045 --> 00:13:27,825
We will recreate the same alerts
with grouping enabled for this.

234
00:13:27,865 --> 00:13:31,805
I have configured this, this, runner
in, in Postman, and that will,

235
00:13:31,805 --> 00:13:36,805
create 31, like requests, alert,
sent 31 events to our events API.

236
00:13:37,145 --> 00:13:41,645
And these were, these are actually
alerts from, like from, from Prometheus.

237
00:13:41,965 --> 00:13:46,145
So again, real, real alerts that were
generated in a short period of time.

238
00:13:46,145 --> 00:13:47,065
I'm going to run them here.

239
00:13:47,125 --> 00:13:48,335
although there were like.

240
00:13:48,765 --> 00:13:52,295
The generation they took, on production,
the alerts were generated over a

241
00:13:52,295 --> 00:13:53,545
time period of five minutes or so.

242
00:13:53,925 --> 00:13:56,635
I'm going to run them like
immediately with 50 milliseconds

243
00:13:56,635 --> 00:13:57,855
delay so we don't have to wait.

244
00:13:58,805 --> 00:13:59,815
So let's run this.

245
00:14:00,315 --> 00:14:01,145
So this is done.

246
00:14:01,535 --> 00:14:03,825
31 events have been sent to iAlert.

247
00:14:04,675 --> 00:14:06,605
Let's switch to the alerts, overview.

248
00:14:07,585 --> 00:14:12,615
And as you can see here, we
have literally 31 new alerts.

249
00:14:12,625 --> 00:14:13,955
So none of them were grouped.

250
00:14:14,570 --> 00:14:18,930
and this is exactly the way how they
also were created in production.

251
00:14:19,430 --> 00:14:22,700
And some of them like at first
sight, they, they look similar, like

252
00:14:22,730 --> 00:14:26,580
things like black box, probe HTTP
failure, on different instances.

253
00:14:26,580 --> 00:14:28,540
Some of them were on,
on, on the same instance.

254
00:14:29,140 --> 00:14:32,805
so yeah, and then some of them are
clearly, like other issues, for

255
00:14:32,805 --> 00:14:36,735
example, there is an alert regarding
a RabbitMQ node that's down.

256
00:14:37,235 --> 00:14:41,025
So let's now try the same example
with alert grouping enabled.

257
00:14:41,425 --> 00:14:44,755
For this I'm going to, modify
the alert source settings in

258
00:14:44,785 --> 00:14:47,055
iAlert and enable alert grouping.

259
00:14:47,555 --> 00:14:49,955
And there is a default grouping
window, which like, doesn't matter

260
00:14:49,955 --> 00:14:51,435
for our, purposes right now.

261
00:14:51,435 --> 00:14:52,435
So it's five minutes.

262
00:14:52,875 --> 00:14:55,145
so it will all, like the alert
grouping will always happen

263
00:14:55,455 --> 00:14:56,785
during the selected, window.

264
00:14:57,565 --> 00:14:59,155
And there is another important parameter.

265
00:14:59,165 --> 00:15:00,755
It's the similarity threshold.

266
00:15:00,755 --> 00:15:05,525
And I'll be like talking later in the
presentation, what exactly just this

267
00:15:05,665 --> 00:15:10,132
threshold means, but here to make
it, a little bit more To show you the

268
00:15:10,132 --> 00:15:11,672
effect of offsetting the threshold.

269
00:15:11,672 --> 00:15:15,972
We have a small preview When the
threshold is it's a number between

270
00:15:15,972 --> 00:15:21,702
0 and 1 and if it's closer to 1 then
fewer alerts are grouped and if it's

271
00:15:21,702 --> 00:15:26,512
like very low then almost all the
alerts are grouped and we're gonna

272
00:15:26,512 --> 00:15:28,502
leave it at the default F of 0.

273
00:15:29,352 --> 00:15:34,592
75 and here like you have an
idea based on the past alerts

274
00:15:35,302 --> 00:15:37,072
How this grouping will affect?

275
00:15:37,742 --> 00:15:39,492
like future alerts Okay.

276
00:15:39,502 --> 00:15:44,882
So let's save this and now let's
run the same, runner again.

277
00:15:45,762 --> 00:15:48,762
So we are again, creating 31 alerts,

278
00:15:49,262 --> 00:15:53,072
but this time I forgot, I actually
forgot to, to resolve the previous

279
00:15:53,092 --> 00:15:54,612
alerts, but it doesn't matter.

280
00:15:55,022 --> 00:15:57,242
So if we go to the alert overview here.

281
00:15:58,122 --> 00:16:01,752
We can now see, previously
we had 31, we have now, no,

282
00:16:01,792 --> 00:16:04,502
previously we had 30 yes, 31.

283
00:16:04,512 --> 00:16:06,361
Now we have, 34.

284
00:16:06,362 --> 00:16:09,662
That means only five new
alerts have been created.

285
00:16:09,662 --> 00:16:14,042
And you can see here, there are many
alerts that have been, aggregated.

286
00:16:14,542 --> 00:16:18,422
For example, these black box
probe, HTTP failure alerts.

287
00:16:19,062 --> 00:16:23,962
They have been aggregated, because there
are like many alerts that, that, that are,

288
00:16:23,972 --> 00:16:26,392
like semantically similar to this alert.

289
00:16:27,322 --> 00:16:30,552
And, other alerts to the
contrary are not, aggregated.

290
00:16:31,052 --> 00:16:33,382
For example, this hardware
service status is not healthy.

291
00:16:34,342 --> 00:16:35,632
you know what I'm going to do?

292
00:16:35,662 --> 00:16:37,482
I'm quickly going to resolve everything.

293
00:16:37,532 --> 00:16:38,742
So we get a clean slate.

294
00:16:38,742 --> 00:16:42,932
So it's clear, it's clearer how many
alerts you would end up with, when you

295
00:16:42,932 --> 00:16:45,262
have intelligent alert grouping enabled.

296
00:16:45,852 --> 00:16:47,002
So let's do this again.

297
00:16:47,502 --> 00:16:48,272
So it was empty.

298
00:16:48,382 --> 00:16:51,742
Now we have submitted
again, 31 new alerts.

299
00:16:52,422 --> 00:16:57,512
And instead of having 51 alerts,
we now have 1, 2, alerts.

300
00:16:58,402 --> 00:16:59,672
as you can see, these.

301
00:17:00,052 --> 00:17:03,772
Also work with like real,
alerts, production, alerts.

302
00:17:04,482 --> 00:17:07,862
And, yeah, it's capable of, of
understanding like in this case,

303
00:17:07,862 --> 00:17:12,512
for example, it's probably because,
the black box probe HTTP failure,

304
00:17:12,552 --> 00:17:15,752
it's probably because there were
like completely different instances.

305
00:17:16,402 --> 00:17:19,907
and it actually looks at the entire
payload, and, I'm going to explain

306
00:17:19,907 --> 00:17:25,957
later, what pre processing is involved,
before we, like we, we, we use AI to

307
00:17:25,957 --> 00:17:28,647
actually, to do the de duplication.

308
00:17:29,147 --> 00:17:32,177
But yeah, but this problem, for
example, it's like, it's clearly,

309
00:17:32,327 --> 00:17:33,417
it's completely separate problem.

310
00:17:33,427 --> 00:17:37,487
Hardware service status is not
healthy, is also a separate problem.

311
00:17:37,987 --> 00:17:40,037
And, yeah, and this, like some.

312
00:17:40,387 --> 00:17:44,417
firewall outside subnet,
running out of IPs is also a

313
00:17:44,447 --> 00:17:45,437
completely different problem.

314
00:17:46,297 --> 00:17:48,137
yeah, so that was the demo.

315
00:17:48,257 --> 00:17:49,827
now let's move back to the slides.

316
00:17:49,847 --> 00:17:56,017
And I would like to walk you through,
how you can build the exact, same

317
00:17:56,017 --> 00:18:01,677
behavior into either your alerting
system, or maybe even like you use it,

318
00:18:01,717 --> 00:18:03,657
somewhere else, with a similar use case.

319
00:18:04,157 --> 00:18:04,487
All right.

320
00:18:04,587 --> 00:18:09,887
first I'm going to talk about a few
concepts that are required to understand,

321
00:18:09,927 --> 00:18:13,817
to understand, the overall, the
overall, process, how we've built this.

322
00:18:14,317 --> 00:18:15,837
Let's start with vector embeddings.

323
00:18:16,047 --> 00:18:19,697
Vector embeddings, they are, a
vector embedding is a mathematical

324
00:18:19,697 --> 00:18:22,317
representation of any kind of data.

325
00:18:22,367 --> 00:18:23,847
in a high dimensional space.

326
00:18:24,607 --> 00:18:29,837
So where each vector represents a
specific kind of data, could be a word,

327
00:18:29,837 --> 00:18:31,957
could be an entire sentence or an image.

328
00:18:32,597 --> 00:18:36,727
and this piece of data is
essentially transformed into a

329
00:18:36,727 --> 00:18:42,626
vector and a vector, um, is a
point in a high dimensional space.

330
00:18:43,216 --> 00:18:48,316
And these, and these vectors or points
in this, in this space, they capture the

331
00:18:48,316 --> 00:18:50,256
semantic relationship, to each other.

332
00:18:50,256 --> 00:18:56,026
So the closer two vectors are, and we
measure this by, using distance functions,

333
00:18:56,026 --> 00:18:59,956
mathematical distance functions, and
the closer two vectors are in that

334
00:18:59,956 --> 00:19:05,476
space, the more likely is it that they
refer to the same underlying concept.

335
00:19:05,976 --> 00:19:11,336
And these embeddings are, used almost
anywhere in, in, AI applications.

336
00:19:12,016 --> 00:19:17,286
When you, for example, use chat
GPT, your prompts are transformed

337
00:19:17,316 --> 00:19:20,146
into a series of numbers first.

338
00:19:20,876 --> 00:19:26,166
And similarly, we will also transform
alerts into a series of numbers.

339
00:19:26,666 --> 00:19:30,346
An embedding model is a special type
of pre trained machine learning model

340
00:19:30,356 --> 00:19:34,476
that learns to represent complex
data, such as words, sentences,

341
00:19:34,946 --> 00:19:37,836
images, in a lower dimensional space.

342
00:19:38,696 --> 00:19:42,166
so this is actually where,
like most of the magic happens.

343
00:19:42,586 --> 00:19:47,496
like we use these pre trained
models, to transfer, to transform

344
00:19:47,536 --> 00:19:49,776
alerts into, into these vectors.

345
00:19:50,616 --> 00:19:55,366
And once, And once we have everything
in place, we can, we can set up our,

346
00:19:55,366 --> 00:19:58,566
like alert deduplication pipeline,
which consists of, three steps.

347
00:19:59,106 --> 00:20:02,046
The first step is, we
pre process the alerts.

348
00:20:02,946 --> 00:20:06,796
And the second step is we,
turn these alerts into vectors

349
00:20:06,906 --> 00:20:07,986
using an embedding model.

350
00:20:08,176 --> 00:20:12,156
And then we apply the de duplication
logic, which I'll get to, in a second.

351
00:20:12,656 --> 00:20:15,616
So let's first start with
pre processing the alerts.

352
00:20:16,046 --> 00:20:20,056
Pre processing involves like
normalization and cleaning.

353
00:20:20,556 --> 00:20:24,586
During normalization, we, just make
sure that all the alerts that we have,

354
00:20:24,956 --> 00:20:27,166
they, they follow a common, format.

355
00:20:27,466 --> 00:20:28,466
And for us as a.

356
00:20:29,061 --> 00:20:33,191
platform that sits on top of like many
observability and monitoring tools.

357
00:20:33,231 --> 00:20:35,001
This is more or less already the case.

358
00:20:35,471 --> 00:20:41,581
So we integrate with a hundred plus tools
and we turn all these alerts into a,

359
00:20:41,581 --> 00:20:46,571
into a common format and in the cleaning
phase, we're going to remove everything

360
00:20:46,811 --> 00:20:49,301
that's not required for the task at hands.

361
00:20:49,351 --> 00:20:52,221
Like for example, if your
alerts have like unique IDs.

362
00:20:52,721 --> 00:20:55,941
The unique IDs, they don't play a role
when you want to answer the question.

363
00:20:55,941 --> 00:20:58,181
Is this a are two alerts, the same or not?

364
00:20:58,191 --> 00:21:01,711
Because the IDs will differ
anyway, or we're also going to

365
00:21:01,711 --> 00:21:04,621
remove any, syntactical elements.

366
00:21:04,651 --> 00:21:09,841
for example, if, alerts are represented
in JSON, we're going to like, remove all

367
00:21:09,841 --> 00:21:15,211
of the JSON, elements, and reduce the
alerts to the essentially plain text.

368
00:21:15,651 --> 00:21:22,271
This will not only reduce costs or reduce
tokens, which are the primary cost driver

369
00:21:22,281 --> 00:21:24,691
when, using and developing LLM apps.

370
00:21:25,391 --> 00:21:28,901
but it will again, just remove
everything that, you wouldn't

371
00:21:28,901 --> 00:21:31,131
consider, for deduplication.

372
00:21:31,631 --> 00:21:36,151
And then we, vectorize these plain text
documents, using an embedding model.

373
00:21:36,791 --> 00:21:42,541
And in our case, we use a, like a self
hosted embedding model, but you could use,

374
00:21:42,621 --> 00:21:47,241
one of those general purpose, embedding
models provided by open AI, for example.

375
00:21:47,541 --> 00:21:50,821
So they have, API APIs for their,
their text embedding models.

376
00:21:51,191 --> 00:21:52,721
And you can, and you could use those.

377
00:21:53,191 --> 00:21:54,541
The reason why we're using.

378
00:21:54,951 --> 00:21:58,811
a self hosted model is because
for us, it's, this is a high

379
00:21:58,951 --> 00:22:00,251
throughput use case, right?

380
00:22:00,251 --> 00:22:04,121
So we're processing millions of events
on any given day, and we don't want to

381
00:22:04,121 --> 00:22:08,531
introduce an external dependency where
we have to make an external HTTP call.

382
00:22:08,811 --> 00:22:11,901
just simply for performance
and like stability reasons.

383
00:22:12,401 --> 00:22:14,721
therefore, we've, we've
selected one of those.

384
00:22:14,861 --> 00:22:18,491
embedding models that are
available on a hugging face.

385
00:22:19,111 --> 00:22:24,181
And, it's in this case, it's a general
purpose model that was trained on a wide

386
00:22:24,321 --> 00:22:29,801
variety of text, like including the common
data sets such as Wikipedia, but also,

387
00:22:30,141 --> 00:22:34,481
more were, data from the stack overflow
was used for example, and that's why

388
00:22:34,691 --> 00:22:40,561
that, the general purpose model is also
capable of capturing, technical content.

389
00:22:41,061 --> 00:22:44,071
And then, once we have, once we
transformed the alerts into vectors,

390
00:22:44,081 --> 00:22:45,391
we store them in a vector DB.

391
00:22:45,391 --> 00:22:50,461
this is of course optional, but storing
them in a vector DB, is more efficient

392
00:22:50,461 --> 00:22:55,081
because vector databases are optimized
for, like storing and more importantly,

393
00:22:55,091 --> 00:22:57,481
managing vectors and querying vectors.

394
00:22:57,481 --> 00:23:02,126
And one operation we're going
to use is, is calculating The

395
00:23:02,126 --> 00:23:07,126
distance between vectors, which will
represent our de duplication logic.

396
00:23:07,946 --> 00:23:09,376
we have an incoming alert flow.

397
00:23:09,886 --> 00:23:13,136
Every alert is vectorized
the vector database.

398
00:23:13,396 --> 00:23:17,196
And for each incoming new alerts,
we're going to look at, we're going

399
00:23:17,196 --> 00:23:19,216
to query our vector database and see.

400
00:23:19,761 --> 00:23:24,121
Within a time window of let's say
five minutes, order any vectors

401
00:23:24,221 --> 00:23:27,431
that are close to the one that
we're looking at right now.

402
00:23:27,901 --> 00:23:31,831
And this closeness is measured,
through a, like similarity, function.

403
00:23:31,841 --> 00:23:35,861
It could be like any similarity function
that measures distance between, vectors.

404
00:23:36,161 --> 00:23:38,071
For example, the cosine, similarity.

405
00:23:38,571 --> 00:23:41,811
And, and then we set a threshold
for this distance metric.

406
00:23:42,256 --> 00:23:46,616
Returns a number between zero and
one, meaning vectors are, essentially

407
00:23:46,616 --> 00:23:50,116
the same, are very close to each
other and zero, is they are very

408
00:23:50,116 --> 00:23:51,506
far, far away from each other.

409
00:23:52,006 --> 00:23:56,826
The main advantage of this approach
is that we're capturing the underlying

410
00:23:56,826 --> 00:23:59,046
semantics, of, of an alert instead of.

411
00:23:59,491 --> 00:24:01,921
Looking at their textual similarity.

412
00:24:02,231 --> 00:24:06,061
And you can even use this if you're
monitoring, your service on like on

413
00:24:06,071 --> 00:24:10,991
multiple levels, you can use, intelligent
alert grouping across, multiple,

414
00:24:11,041 --> 00:24:15,251
monitoring and observability tools,
things that you have to consider, are of

415
00:24:15,251 --> 00:24:20,071
course, like selecting a model that's,
appropriate for, like your domain.

416
00:24:20,611 --> 00:24:24,991
and then you just, Making sure that
it works as expected, because these

417
00:24:24,991 --> 00:24:26,231
models, they can behave differently.

418
00:24:26,231 --> 00:24:30,471
For example, if, if they weren't
trained on, on non English data,

419
00:24:30,511 --> 00:24:34,271
they might behave differently if your
alerts are not in English, for example.

420
00:24:34,771 --> 00:24:37,811
And of course, like looking at
the thresholds, playing with the

421
00:24:37,811 --> 00:24:41,511
threshold because the threshold
itself, That doesn't mean anything.

422
00:24:41,511 --> 00:24:43,581
So a threshold of, for example, 0.

423
00:24:43,761 --> 00:24:50,001
75 could be perfectly fine for, like
one specific, area, but for another,

424
00:24:50,041 --> 00:24:52,721
you might want it to have in, in, in 0.

425
00:24:52,721 --> 00:24:56,951
9, and I showed you one way to do
this is, just, testing the threshold

426
00:24:56,971 --> 00:25:01,941
on past alerts, where, we use the
slider and simultaneously updated the

427
00:25:01,941 --> 00:25:04,251
grouping, based on the past alerts.

428
00:25:04,751 --> 00:25:08,121
And then we, last but not
least, we do also provide, or

429
00:25:08,161 --> 00:25:10,041
collect feedback on all alerts.

430
00:25:10,431 --> 00:25:13,461
so it's a simple thumbs up,
thumbs down, where you can, say,

431
00:25:13,461 --> 00:25:15,971
okay, this was correctly grouped,
or this was, falsely grouped.

432
00:25:16,311 --> 00:25:19,911
and we make these metrics, transparent
and based on these metrics, you

433
00:25:19,911 --> 00:25:22,411
can further fine tune, the model.

434
00:25:22,911 --> 00:25:23,331
All right.

435
00:25:23,381 --> 00:25:24,881
thank you for, listening.

436
00:25:24,981 --> 00:25:31,801
we, we do have, a dedicated guide, for AI
and incident management, which also covers

437
00:25:31,801 --> 00:25:35,731
the other use case that I like, briefly
hinted, in the beginning of this talk.

438
00:25:36,211 --> 00:25:39,056
Feel free to scan the QR code
and download, download the guide.

439
00:25:39,361 --> 00:25:41,741
these guides are also
available, on guides.

440
00:25:42,351 --> 00:25:42,481
eiler.

441
00:25:42,541 --> 00:25:46,471
com, in an HTML version where you
don't have to download a PDF, but

442
00:25:46,471 --> 00:25:49,971
if you're more, comfortable with
PDFs, just scan the QR code and

443
00:25:50,031 --> 00:25:52,251
download the, the PDF guide for you.

444
00:25:52,751 --> 00:25:55,351
I'll be, this is an online conference,
as I'll be around, if you have any

445
00:25:55,351 --> 00:25:58,341
questions, feel free to drop your
question, in the Slack channel.

446
00:25:58,851 --> 00:25:59,331
Thank you.

