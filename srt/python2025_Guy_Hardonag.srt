1
00:00:00,009 --> 00:00:03,979
Hey everyone, I'm Guy Adornock,
founding engineer at Reverse, the

2
00:00:03,979 --> 00:00:08,459
company behind LakeFS, an open source
platform for managing data at scale.

3
00:00:09,280 --> 00:00:14,059
I spent the past few years working
on LakeFS, helping teams version and

4
00:00:14,059 --> 00:00:15,750
manage their data more efficiently.

5
00:00:16,369 --> 00:00:21,169
In this talk, we'll explore how
data workflows have shifted from

6
00:00:21,169 --> 00:00:22,959
running locally to the cloud.

7
00:00:23,349 --> 00:00:27,200
We'll look at Python libraries
like Pandas and TensorFlow and

8
00:00:27,200 --> 00:00:28,920
how they handle cloud storage.

9
00:00:29,720 --> 00:00:35,209
A key part of this transition is FSSpec,
a Python library that makes working

10
00:00:35,209 --> 00:00:37,919
with different storage systems seamless.

11
00:00:38,340 --> 00:00:42,639
We'll see how it helps both
users accessing object stores

12
00:00:43,050 --> 00:00:46,080
and storage providers looking
to improve their compatibility.

13
00:00:46,580 --> 00:00:51,340
Finally, we'll dive into how FSSpec
can be implemented for an object

14
00:00:51,340 --> 00:00:53,280
store using Lake Office as an example.

15
00:00:53,980 --> 00:00:57,190
Let's start by looking
at the local environment.

16
00:00:57,690 --> 00:01:00,140
working locally is simple and convenient.

17
00:01:00,350 --> 00:01:01,060
And here's why.

18
00:01:02,040 --> 00:01:04,640
of all, built in library support.

19
00:01:05,530 --> 00:01:11,600
Popular Python libraries like Pandas,
TensorFlow, and PyArrow have always

20
00:01:11,620 --> 00:01:13,920
worked seamlessly with local files.

21
00:01:14,810 --> 00:01:17,860
Second, standard file handling.

22
00:01:18,500 --> 00:01:22,890
Python's built in file interface
makes it easy to work with files

23
00:01:22,890 --> 00:01:27,899
using familiar methods like read
and write and setup required.

24
00:01:28,089 --> 00:01:30,769
Just point, point to a file and it works.

25
00:01:31,049 --> 00:01:32,539
No extra configuration needed.

26
00:01:33,084 --> 00:01:37,334
As we see in these examples, working
locally is simple and straightforward.

27
00:01:37,944 --> 00:01:43,334
Whether we're using pandas to read a
csv or tensorflow to load a model, we

28
00:01:43,334 --> 00:01:45,994
just point to a file path and it works.

29
00:01:45,994 --> 00:01:48,314
No extra steps, no setup required.

30
00:01:48,924 --> 00:01:54,894
However, while working locally
is easy, it does have its limits.

31
00:01:55,774 --> 00:02:00,184
As data grows and project scales, those
limitations become more noticeable.

32
00:02:00,684 --> 00:02:04,574
cloud offers some clear advantages
in overcoming these challenges.

33
00:02:05,174 --> 00:02:08,644
Let's look at the key
benefits of moving to cloud.

34
00:02:09,104 --> 00:02:10,024
Redundancy.

35
00:02:10,564 --> 00:02:15,164
Cloud storage keeps your data backed
up and protected from local failures.

36
00:02:16,164 --> 00:02:16,934
Scalability.

37
00:02:17,734 --> 00:02:21,534
You can handle massive data sets
without worrying about running

38
00:02:21,534 --> 00:02:23,494
out of storage or compute power.

39
00:02:24,274 --> 00:02:25,134
Collaboration.

40
00:02:25,714 --> 00:02:30,524
Teams can easily share and access data
without needing to pass around files.

41
00:02:30,814 --> 00:02:31,444
Security.

42
00:02:32,154 --> 00:02:36,414
Cloud providers offer strong security
measures, often better than what's

43
00:02:36,494 --> 00:02:38,214
available on personal machines.

44
00:02:38,834 --> 00:02:40,884
And accessibility.

45
00:02:41,444 --> 00:02:44,924
Your data is available from
anywhere, making it easy to work

46
00:02:44,954 --> 00:02:46,954
across devices and locations.

47
00:02:47,454 --> 00:02:50,684
So moving to the cloud unlocks
new possibilities, but it

48
00:02:50,854 --> 00:02:52,324
also brings in new challenges.

49
00:02:52,834 --> 00:02:54,064
Let's take a closer look.

50
00:02:54,814 --> 00:02:59,434
At what this transition looks like
transitioning to the cloud poses

51
00:02:59,434 --> 00:03:03,044
challenges, especially when it
comes to using familiar libraries.

52
00:03:03,674 --> 00:03:07,064
How can we bridge the gap between
favorite tools and cloud storage?

53
00:03:08,054 --> 00:03:12,754
One approach is to manually sync data
between the local and the cloud storage.

54
00:03:13,024 --> 00:03:16,554
For instance, use AWS CLI or
border three and download and

55
00:03:16,554 --> 00:03:17,784
upload the data all the time.

56
00:03:18,204 --> 00:03:20,889
So while this approach
can get the job done.

57
00:03:21,389 --> 00:03:25,779
It's not the most practical, and it
becomes error prone, cumbersome, and

58
00:03:25,789 --> 00:03:27,789
hard to maintain as things change.

59
00:03:28,509 --> 00:03:32,009
Another approach is using
library specific connectors.

60
00:03:32,229 --> 00:03:35,759
Some libraries, like Pandas
and TensorFlow, offer built in

61
00:03:35,759 --> 00:03:39,129
support for certain cloud storage
providers, making integration easier.

62
00:03:39,519 --> 00:03:40,990
At first, this sounds great.

63
00:03:40,990 --> 00:03:45,399
It allows for seamless access
to cloud data without extra

64
00:03:45,399 --> 00:03:46,789
steps, But there are tradeoffs.

65
00:03:47,489 --> 00:03:51,679
Not every object store is supported,
meaning if you're using a less common

66
00:03:51,709 --> 00:03:53,449
provider, you might be out of luck.

67
00:03:53,949 --> 00:03:58,529
library also needs to maintain
its own implementation, which

68
00:03:58,589 --> 00:03:59,789
leads to duplicate effort.

69
00:04:00,289 --> 00:04:06,319
And even when support exists, way
each library handles configuration,

70
00:04:07,119 --> 00:04:11,619
dependencies, and even basic
operations, things inconsistent.

71
00:04:12,119 --> 00:04:15,659
while this approach can work,
it's not a one size fits all.

72
00:04:16,159 --> 00:04:18,849
we need something more
flexible and standardized.

73
00:04:19,149 --> 00:04:20,759
That's where FSpec in.

74
00:04:21,389 --> 00:04:25,509
The creators of FSpec saw the need
for a unified way to interact with

75
00:04:25,509 --> 00:04:28,309
file systems, both local and remote.

76
00:04:28,749 --> 00:04:33,459
Instead of each library implementing
its own cloud integration, FSpec

77
00:04:33,499 --> 00:04:36,639
provides a standard interface
that simplifies the process.

78
00:04:36,639 --> 00:04:36,949
FSpec.

79
00:04:36,950 --> 00:04:42,244
com It provides a unified interface,
local and remote file systems behave the

80
00:04:42,244 --> 00:04:49,384
same, multiple backend support, access a
variety of storage systems via the same

81
00:04:49,384 --> 00:04:54,854
interface, of integration, libraries
and tools can leverage FSPEC without

82
00:04:54,864 --> 00:05:01,784
worrying about backend specific details,
and enhanced capabilities, features like

83
00:05:01,844 --> 00:05:08,124
caching, transactions, and concurrency
come built in improving efficiency.

84
00:05:09,089 --> 00:05:13,689
Instead of every library re implementing
these best practices with FSpec,

85
00:05:14,169 --> 00:05:18,329
we only need a single, optimized
implementation that handles them all.

86
00:05:18,629 --> 00:05:21,069
Once it's in place, everyone can use it.

87
00:05:21,569 --> 00:05:25,789
using FSpec, we get the
flexibility of cloud storage,

88
00:05:26,019 --> 00:05:27,589
without giving up the simplicity.

89
00:05:28,199 --> 00:05:33,099
Of working locally for the FS spec
audience, we have the end user library

90
00:05:33,099 --> 00:05:35,649
implements and the backend providers.

91
00:05:36,029 --> 00:05:41,289
So FS spec allows you to interact
with cloud storage just as

92
00:05:41,289 --> 00:05:42,879
you would with local files.

93
00:05:42,939 --> 00:05:47,549
For example, with Fs Specs,
S3 implementation, S3 FS,

94
00:05:48,049 --> 00:05:49,609
can read a file from S3.

95
00:05:49,669 --> 00:05:54,369
Just like a local file, moving to any
other provider will only require changing

96
00:05:54,369 --> 00:05:57,039
the F initialization and the paths.

97
00:05:58,009 --> 00:05:59,299
as you can see in the example.

98
00:05:59,799 --> 00:06:01,899
FSpec simplifies the process in two ways.

99
00:06:02,604 --> 00:06:03,924
Explicit configuration.

100
00:06:04,104 --> 00:06:08,074
You can manually provide the
necessary configuration when

101
00:06:08,354 --> 00:06:09,114
initializing the file system.

102
00:06:09,674 --> 00:06:12,404
This gives you control over
how the connection is made.

103
00:06:12,904 --> 00:06:13,704
The other option?

104
00:06:14,254 --> 00:06:15,784
No initialization needed at all.

105
00:06:16,194 --> 00:06:19,434
In many cases, you don't need
to initialize the file system.

106
00:06:19,934 --> 00:06:25,464
With just an open function call, FSPEC
automatically handles the connection,

107
00:06:25,894 --> 00:06:28,864
pulling the required configuration
directly from your environment.

108
00:06:29,369 --> 00:06:30,999
Just like AWS CLI does.

109
00:06:31,999 --> 00:06:35,369
this means you don't have to worry
about the underlying setup at all.

110
00:06:36,079 --> 00:06:37,149
Library developers.

111
00:06:38,029 --> 00:06:42,919
Some libraries integrate FSPEC directly,
allowing seamless cloud storage

112
00:06:42,919 --> 00:06:44,539
access without additional setup.

113
00:06:44,959 --> 00:06:47,799
For example, Pandas uses
FSpec under the hood.

114
00:06:48,149 --> 00:06:52,169
This means that any storage
backend implementing FSpec

115
00:06:52,579 --> 00:06:54,619
automatically works with Pandas.

116
00:06:55,119 --> 00:06:59,099
beyond just using FSpec, you
can extend it by creating a new

117
00:06:59,109 --> 00:07:00,709
backend for a storage system.

118
00:07:01,669 --> 00:07:02,729
take a real world example.

119
00:07:03,209 --> 00:07:06,999
as I mentioned earlier, I work at
Tervoz, the company behind LakeFS.

120
00:07:07,499 --> 00:07:13,904
And one of the users from the community,
recognized the value of integrating LakeFS

121
00:07:13,905 --> 00:07:19,144
with FSpec and decided to contribute
LakeFS FSpec to make that happen.

122
00:07:19,644 --> 00:07:25,614
while LakeFS itself isn't an object
store, it adds data versioning on top

123
00:07:25,614 --> 00:07:27,794
of one and can be accessed like one.

124
00:07:28,294 --> 00:07:33,284
So we'll showcase the LakeFS FSpec
implementation and demonstrate

125
00:07:33,314 --> 00:07:38,304
how it enables compatibility
with Pandas, PyArrow, TensorFlow,

126
00:07:38,374 --> 00:07:39,804
and many other libraries.

127
00:07:40,304 --> 00:07:45,784
before we dive into LakeFS
FSpec, let's take a step back.

128
00:07:46,109 --> 00:07:50,029
and talk about what is LakeFS
and why is it so useful.

129
00:07:50,519 --> 00:07:54,649
LakeFS is an open source project that
sits on top of your object store and

130
00:07:54,649 --> 00:08:00,029
provides Git like capabilities such as
commit, merge, revert, tag, and so on.

131
00:08:00,529 --> 00:08:04,919
we take a look at your current data
ecosystem, working with LakeFS is

132
00:08:04,929 --> 00:08:07,529
almost identical to working with S3.

133
00:08:08,029 --> 00:08:11,484
When accessing the data, the only
difference will be That now in

134
00:08:11,484 --> 00:08:14,994
addition to the path, you need to
reference the branch you are working.

135
00:08:15,494 --> 00:08:22,474
Lake Office is accessible by UI,
a CLI called Lake CTL, and clients

136
00:08:22,474 --> 00:08:24,634
in Python, Java, and many more.

137
00:08:24,944 --> 00:08:27,334
Let's take a high level
look at how it works.

138
00:08:27,889 --> 00:08:30,739
Mainly to emphasize that there
is no need in copying data.

139
00:08:31,239 --> 00:08:34,429
commit is a list of pointers to
objects in your object store.

140
00:08:34,929 --> 00:08:37,249
we edit one file and commit it.

141
00:08:37,749 --> 00:08:41,699
you can see, the next commit will
point to all the unchanged data

142
00:08:41,699 --> 00:08:48,309
from before, but instead of pointing
to 790, we now point to 214.

143
00:08:48,889 --> 00:08:53,784
if we were working on a data set of maps
that is a few terabytes, And we want to

144
00:08:53,784 --> 00:08:59,694
test our code, all we need to do is create
a new branch, and we get a dev environment

145
00:08:59,854 --> 00:09:02,454
in no time, and no duplication of data.

146
00:09:02,954 --> 00:09:08,624
Or, if we trained our model a few months
ago, and we had some change in our code,

147
00:09:09,124 --> 00:09:15,554
can just go back in time, check out
from a commit back then, and check our

148
00:09:15,614 --> 00:09:18,734
updated model on the exact same data.

149
00:09:19,299 --> 00:09:25,579
Now, let's see what would be needed in
order to implement FSpec for LakeFS.

150
00:09:26,079 --> 00:09:30,899
In order to implement FSpec,
all we need to do is implement

151
00:09:31,579 --> 00:09:33,759
the class AbstractFileSystem.

152
00:09:34,279 --> 00:09:39,299
The class AbstractFileSystem provides
a template of methods that a potential

153
00:09:39,299 --> 00:09:43,884
implementation should supply, as
well as default implementations of

154
00:09:43,884 --> 00:09:44,809
functionalities that depend on these.

155
00:09:45,309 --> 00:09:51,439
Methods that could be implemented are
marked with not implement error or pass.

156
00:09:52,089 --> 00:09:56,909
Pass for directory operations that
might not be required for some backends

157
00:09:56,910 --> 00:09:58,129
where directories are emulated.

158
00:09:58,889 --> 00:10:02,049
Note that not all of the
methods are emulated.

159
00:10:02,050 --> 00:10:10,249
For example, some implementations
may be read only, in which things

160
00:10:10,249 --> 00:10:18,039
like pipe, put, touch, rm, and so
on can be left as not implemented.

161
00:10:18,539 --> 00:10:23,189
might also choose to return some read only
exception or raise a permission error.

162
00:10:23,689 --> 00:10:30,379
your backend supports async operations,
can implement async file system, which

163
00:10:30,389 --> 00:10:36,579
extends abstract file systems, and that
way you offer asynchronous capabilities.

164
00:10:37,449 --> 00:10:43,449
To register a new backend with FSpec,
you can use entry point and setup tools.

165
00:10:43,949 --> 00:10:48,864
example, to add a new file
system protocol, myfs, You'd

166
00:10:48,894 --> 00:10:51,524
add these to your setup UI.

167
00:10:52,134 --> 00:10:57,984
Alternatively, you can register it
manually or submit it submitting

168
00:10:57,984 --> 00:11:02,734
a PR to the FSpec Repository to
include in the known implementations.

169
00:11:03,234 --> 00:11:06,474
LakeFS has an FSpec implementation
called lakefs fspec.

170
00:11:07,189 --> 00:11:10,099
That was contributed by the
community, specifically by a

171
00:11:10,099 --> 00:11:11,209
cool company called Material.

172
00:11:11,209 --> 00:11:11,629
ai.

173
00:11:12,599 --> 00:11:17,229
won't go into the full implementation,
but aside from optimization and caching,

174
00:11:17,329 --> 00:11:19,189
most of it is pretty straightforward.

175
00:11:19,569 --> 00:11:22,719
LS would call LakeFS LS using the SDK.

176
00:11:23,039 --> 00:11:25,859
Copy will call LakeFS copy, and so on.

177
00:11:26,579 --> 00:11:31,009
One particularly interesting aspect
is how they handle transactions

178
00:11:31,109 --> 00:11:32,769
using ephemeral branches.

179
00:11:32,974 --> 00:11:36,164
When a transaction starts, an
ephemeral branch is created.

180
00:11:36,664 --> 00:11:41,224
Once the transaction is committed,
we do a regular LakeFS commit and

181
00:11:41,234 --> 00:11:43,294
merge it back using LakeFS merge.

182
00:11:43,794 --> 00:11:48,194
What's really cool here is how
naturally LakeFS and FSpec fit together.

183
00:11:48,969 --> 00:11:52,999
FSpec defines a file system interface,
and LakeFS, with its commit based

184
00:11:52,999 --> 00:11:58,139
model, happens to provide an elegant
way to support atomic transactions.

185
00:11:58,939 --> 00:12:02,029
It's one of those cases where
the pieces just align perfectly.

186
00:12:02,679 --> 00:12:07,449
By using ephemeral branches, every
transaction starts in isolation,

187
00:12:07,649 --> 00:12:09,599
allowing changes to be done safely.

188
00:12:10,579 --> 00:12:15,649
when everything is ready, we can merge,
making the process atomic and reliable.

189
00:12:16,484 --> 00:12:20,694
This means users get the benefits
of versioning and consistency

190
00:12:21,294 --> 00:12:23,534
without any extra complexity.

191
00:12:24,034 --> 00:12:25,484
let's see FSpec in action.

192
00:12:25,984 --> 00:12:27,424
so let's see the slide.

193
00:12:28,204 --> 00:12:30,354
the first thing we'll do is go to LakeFS.

194
00:12:31,244 --> 00:12:32,394
We have a cloud installation.

195
00:12:33,094 --> 00:12:34,494
T Reverse organization.

196
00:12:34,504 --> 00:12:35,634
We will need to log in.

197
00:12:36,249 --> 00:12:37,549
Here's my Google Workspace.

198
00:12:37,550 --> 00:12:41,639
I already logged into Google
Workspace, so it'll just take me there.

199
00:12:42,169 --> 00:12:44,609
And we will create our new repository.

200
00:12:44,639 --> 00:12:46,699
So you can see here a
bunch of repositories.

201
00:12:46,849 --> 00:12:48,189
Let's create our own.

202
00:12:48,359 --> 00:12:50,969
And call it fspec example.

203
00:12:50,970 --> 00:12:55,139
We need to put, provide
a storage namespace.

204
00:12:55,249 --> 00:13:00,009
This is basically place where
the data will exist in S3.

205
00:13:00,509 --> 00:13:04,879
As I said, we don't save the data is in
our underlying storage, in this case,

206
00:13:05,089 --> 00:13:07,579
it's S3, and we will add a sample data.

207
00:13:08,079 --> 00:13:13,819
Okay, so this is our repository,
we have our sample data

208
00:13:13,859 --> 00:13:15,049
here, and there's the lakes.

209
00:13:15,299 --> 00:13:19,649
k file, we could take a look
at it, and, yeah, is this.

210
00:13:20,149 --> 00:13:27,524
now, let's go back to pandas, and,
use, Fspec and LakeFS implementation

211
00:13:27,604 --> 00:13:28,744
in order to read the data.

212
00:13:29,704 --> 00:13:39,974
the first thing we'll do is import
pandas as pd and create a datatime.

213
00:13:40,454 --> 00:13:40,684
ipv8.

214
00:13:40,774 --> 00:13:41,074
get.

215
00:13:41,164 --> 00:13:47,579
Let's go back and take the URI
for the object and use it here.

216
00:13:48,079 --> 00:13:48,619
take a look.

217
00:13:49,119 --> 00:13:49,589
That's it.

218
00:13:49,869 --> 00:13:56,569
We basically used Pandas, and Pandas
uses FSpec, and FSpec has a LakeFS

219
00:13:56,779 --> 00:14:02,939
implementation, so once we gave
the LakeFS schema, it all works.

220
00:14:03,439 --> 00:14:06,769
let's take a look on a
transaction would look like.

221
00:14:06,819 --> 00:14:08,659
let's say we want to make a few changes.

222
00:14:09,449 --> 00:14:10,699
We want to create a branch.

223
00:14:11,409 --> 00:14:15,109
Do our changes and eventually
merge them all back in.

224
00:14:15,619 --> 00:14:28,509
So first of all, we would need to use
LakeFS FSpec so it LakeFSspec and we will

225
00:14:28,529 --> 00:14:31,739
input the LakeFS file system initiate it.

226
00:14:32,579 --> 00:14:36,229
we don't need to give anything because
it will read the configuration from our,

227
00:14:36,489 --> 00:14:38,479
in this case, the Lake CTL YAML file.

228
00:14:39,469 --> 00:14:41,299
And now let's do a transaction.

229
00:14:41,929 --> 00:14:42,339
fsnc.

230
00:14:42,339 --> 00:14:42,929
transaction.

231
00:14:43,429 --> 00:14:50,039
We provide the repository
we're working on, fspec.

232
00:14:50,219 --> 00:14:52,469
example, and branch.

233
00:14:52,969 --> 00:14:54,389
Let's see what this did.

234
00:14:54,909 --> 00:14:55,409
It's closed.

235
00:14:55,879 --> 00:15:00,299
we started a transaction, and
now let's read the data frame.

236
00:15:00,799 --> 00:15:08,199
it's basically this, but instead of that,
we will use the transactionSuppositor.

237
00:15:08,199 --> 00:15:13,199
Instead of branch, we will
use the transaction, sorry,

238
00:15:13,479 --> 00:15:13,519
the transactionBranchId.

239
00:15:13,519 --> 00:15:18,194
Mmkay.

240
00:15:18,694 --> 00:15:22,714
now, let's filter out
the lakes from Canada.

241
00:15:23,214 --> 00:15:23,614
Okay, what's it?

242
00:15:23,864 --> 00:15:24,544
Dixie.

243
00:15:25,044 --> 00:15:30,264
And once we've filtered out the lakes
from Canada, Yeah, this looks Okay.

244
00:15:30,764 --> 00:15:31,664
Yeah, this looks good.

245
00:15:31,764 --> 00:15:34,754
we take the Canada lakes, write
them to pocket, write them on the

246
00:15:34,774 --> 00:15:38,704
repository, on the branch, call it
Canada lakes, dot pocket, and that's it.

247
00:15:38,994 --> 00:15:39,904
And now we can commit.

248
00:15:40,404 --> 00:15:40,664
tx.

249
00:15:41,164 --> 00:15:43,754
commit, and we add Canada Lakes.

250
00:15:44,324 --> 00:15:47,284
Now let's do the same thing for Germany.

251
00:15:47,784 --> 00:15:51,624
we'll take Germany Lakes, will be Germany.

252
00:15:51,974 --> 00:15:59,944
And here we'll call it
johnX and then johnX.

253
00:16:00,444 --> 00:16:04,914
once we run this, if everything
is correct, we will read the data

254
00:16:04,914 --> 00:16:06,734
from, we will create a new branch.

255
00:16:07,469 --> 00:16:13,409
as a transaction, read the data from
our ephemeral branch, out the lakes from

256
00:16:13,409 --> 00:16:16,929
Canada, it in to our branch, commit it.

257
00:16:16,929 --> 00:16:20,899
We will do the same thing from,
for Germany, commit that as well.

258
00:16:20,969 --> 00:16:24,539
And once the transaction is
over, it will merge all the data

259
00:16:24,609 --> 00:16:26,929
in one atomic action into main.

260
00:16:27,429 --> 00:16:29,399
see, let's move here.

261
00:16:30,079 --> 00:16:32,759
Let's look if we have any
branches, if we're lucky.

262
00:16:32,799 --> 00:16:33,039
Yeah.

263
00:16:33,079 --> 00:16:34,609
We could see the transaction branch.

264
00:16:35,109 --> 00:16:39,269
So as you can see, the transaction
branch has two commits.

265
00:16:40,119 --> 00:16:43,619
if I refresh, we won't have it anymore
because it was an ephemeral branch

266
00:16:44,019 --> 00:16:46,559
and it was deleted let's go back.

267
00:16:46,789 --> 00:16:47,109
Yeah.

268
00:16:47,289 --> 00:16:48,549
As I said, it was deleted.

269
00:16:48,819 --> 00:16:53,149
Now we have only one branch and as
we could see, we have the data here.

270
00:16:53,649 --> 00:16:56,169
we could also take a
look and a look at it.

271
00:16:56,169 --> 00:16:56,909
Of course.

272
00:16:57,409 --> 00:16:57,699
Yeah.

273
00:16:57,699 --> 00:16:58,969
So I took Canada lakes here.

274
00:16:59,469 --> 00:16:59,869
You get it.

275
00:17:00,369 --> 00:17:00,799
okay.

276
00:17:01,469 --> 00:17:03,849
we could also see when
we inserted the data.

277
00:17:03,919 --> 00:17:07,309
So we could do a blame and at the commit.

278
00:17:07,919 --> 00:17:09,499
So this is the ad Germany lakes.

279
00:17:09,499 --> 00:17:11,869
We could see what was added here.

280
00:17:12,739 --> 00:17:13,839
See the changes.

281
00:17:14,369 --> 00:17:15,469
We could see the parents.

282
00:17:15,969 --> 00:17:17,659
one before was at Canada lakes.

283
00:17:18,159 --> 00:17:23,819
also look at all comments and
we could see The last commit was

284
00:17:23,849 --> 00:17:28,319
basically a merge commit that merges
the transaction branch into main.

285
00:17:28,819 --> 00:17:29,999
yeah, that's about it.

286
00:17:30,499 --> 00:17:35,359
to sum it up, we've explored the evolution
from working with local files to handling

287
00:17:35,359 --> 00:17:40,859
data in the cloud, and how tools like
FSpec bridge the gap, providing a unified

288
00:17:40,859 --> 00:17:42,509
interface for working with local files.

289
00:17:42,799 --> 00:17:48,009
Various storage systems by
implementing FSpec for LakeFS.

290
00:17:48,979 --> 00:17:53,209
We've seen how it enables seamless
integration with Python libraries

291
00:17:53,209 --> 00:17:54,729
like Pandas and TensorFlow.

292
00:17:55,689 --> 00:17:59,519
LakeFS FSpec, contributed by
the community, not only makes

293
00:17:59,519 --> 00:18:03,799
LakeFS more accessible, but also
introduces powerful transaction

294
00:18:03,799 --> 00:18:05,589
support using ephemeral branches.

295
00:18:06,084 --> 00:18:09,394
ensuring atomic and reliable operations.

296
00:18:10,094 --> 00:18:13,984
As data continues to scale,
solutions like LakeFS and FSPEC

297
00:18:14,214 --> 00:18:18,494
help teams work efficiently, the
flexibility of cloud storage with

298
00:18:18,494 --> 00:18:20,294
the simplicity of local workflows.

299
00:18:20,794 --> 00:18:25,034
If you're looking to bring version
control and transactional consistency

300
00:18:25,074 --> 00:18:30,604
to your data workflows, LakeFS with
FSPEC is a great tool to explore.

301
00:18:31,234 --> 00:18:31,744
Thank you.

302
00:18:32,744 --> 00:18:34,144
Happy to take any questions.

