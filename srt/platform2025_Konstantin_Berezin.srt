1
00:00:00,500 --> 00:00:02,510
So guys, let's get started.

2
00:00:03,230 --> 00:00:05,990
As I said before, Lux is
a live blow of our system.

3
00:00:06,260 --> 00:00:08,060
They can record everything.

4
00:00:08,090 --> 00:00:12,260
What just be once, it can be good
thing, bad thing, routine, and unusual.

5
00:00:12,320 --> 00:00:16,700
As loggers has different levels
like warning, error and stuff.

6
00:00:17,060 --> 00:00:20,480
But when you running a modern platform,
you generate not just hundreds, but

7
00:00:20,480 --> 00:00:22,260
millions of low entries every single day.

8
00:00:22,760 --> 00:00:29,030
So we gotta separate available signals
like em, emerging outage or a security

9
00:00:29,030 --> 00:00:31,820
incident from all the background noise.

10
00:00:32,300 --> 00:00:34,820
That's where embedded contact
agents really come in.

11
00:00:35,470 --> 00:00:38,560
The challenge of logs, think
about the scale of the problem.

12
00:00:38,830 --> 00:00:42,700
In the microservices based architecture,
each service produces logs.

13
00:00:43,060 --> 00:00:45,250
Multiply that by dozens of services.

14
00:00:45,595 --> 00:00:47,545
Multi, multiple environment.

15
00:00:47,575 --> 00:00:52,525
It can be like staging,
production, QA as well.

16
00:00:52,525 --> 00:00:56,815
We have different countries
they locate in different regions

17
00:00:57,175 --> 00:00:59,725
like Europe, Asia, and us.

18
00:01:00,294 --> 00:01:07,895
And as I said before, it we, since we use
microservices their amount can be like.

19
00:01:08,795 --> 00:01:10,295
Thousands at least.

20
00:01:10,505 --> 00:01:15,365
For example, when it comes to find
company, like as I know, Netflix

21
00:01:15,365 --> 00:01:19,145
has 700 at least just let's imagine
how many of them has Google.

22
00:01:19,715 --> 00:01:22,775
So most of those logs are boring things.

23
00:01:22,775 --> 00:01:28,055
Like for an example, connection
open or user disconnected, or

24
00:01:28,055 --> 00:01:30,095
some web socket is open and stuff.

25
00:01:30,185 --> 00:01:32,930
User logged in, cache kit and stuff.

26
00:01:33,020 --> 00:01:33,830
It can be a lot.

27
00:01:34,250 --> 00:01:37,190
But hidden on those streams are
the needles and the hashtag.

28
00:01:37,835 --> 00:01:42,515
The early signs of something going
wrong, maybe just request as timing.

29
00:01:42,575 --> 00:01:45,755
More than usual, maybe login
failures are increasing.

30
00:01:45,755 --> 00:01:52,055
For a specific country, probably some
database chart is slower than the rest.

31
00:01:52,835 --> 00:01:57,725
It's not really something important,
but when we have huge amount of

32
00:01:57,725 --> 00:02:03,065
them is getting the problem, so
we got recognized when it's okay

33
00:02:03,215 --> 00:02:04,930
and when finding them is a noise.

34
00:02:05,430 --> 00:02:06,540
Is a real challenge.

35
00:02:07,040 --> 00:02:10,460
And why is it's really hard?

36
00:02:10,940 --> 00:02:13,580
Local analysis is hard for free reasons.

37
00:02:13,970 --> 00:02:16,520
As I said before, we have huge volume.

38
00:02:16,880 --> 00:02:20,670
You simply cannot you cannot have humans
reading through millions of entries.

39
00:02:20,910 --> 00:02:25,410
Even automated dashboards
can be overwhelming and.

40
00:02:25,860 --> 00:02:30,510
Heterogeneity, just let's imagine
it's really frequent ation and one

41
00:02:30,510 --> 00:02:36,340
team is using some strict structure
using js o with some specific keys,

42
00:02:36,580 --> 00:02:41,290
but some other just using simple
string and it's creating the case.

43
00:02:41,710 --> 00:02:43,335
And the third, the lack of context.

44
00:02:44,110 --> 00:02:47,860
The, a single error message means
nothing without knowing what

45
00:02:47,860 --> 00:02:49,330
cells are happening in the system.

46
00:02:49,630 --> 00:02:54,220
Timeout may be fine, maybe it's
happening frequently, and team know

47
00:02:54,220 --> 00:02:58,540
what's happening, but when it happens
during checkout your e-commerce system

48
00:02:58,540 --> 00:03:00,580
on a Black Friday, it's a huge problem.

49
00:03:01,060 --> 00:03:04,030
So traditional tools
don't give us the context.

50
00:03:04,090 --> 00:03:08,500
They just tell us accurate, and
it's up to us what we gonna do.

51
00:03:09,310 --> 00:03:13,120
And we have, by the way, some
classical approaches what people use.

52
00:03:13,945 --> 00:03:16,555
The first one, first hold based rules.

53
00:03:16,645 --> 00:03:20,875
For example, when we have more than
a hundred errors in a minute, we

54
00:03:20,875 --> 00:03:22,855
got sent an alert to the slack.

55
00:03:23,260 --> 00:03:26,470
Or any other messenger
to check it on our graph.

56
00:03:26,470 --> 00:03:31,600
And so it's simple and easy to
implement, but it often creates

57
00:03:31,600 --> 00:03:33,250
alerts when nothing is truly wrong.

58
00:03:33,400 --> 00:03:35,440
And sometimes it misses subtle issues.

59
00:03:35,740 --> 00:03:38,320
For example, when developer is
doing something important and he

60
00:03:38,320 --> 00:03:41,650
gotta change, he gotta change his
context and check what's happening.

61
00:03:41,710 --> 00:03:42,940
It's getting problematic.

62
00:03:43,735 --> 00:03:47,965
The second statistical methods
using moving averages re

63
00:03:48,145 --> 00:03:49,645
that scores to reactivations.

64
00:03:50,215 --> 00:03:54,175
These are useful for metrics like
latency or throughput, but they don't

65
00:03:54,235 --> 00:03:56,425
understand logs in their semantic sense.

66
00:03:56,515 --> 00:03:57,665
And of course the third one.

67
00:03:58,240 --> 00:04:03,310
Basic ML models like isolation forest,
or clustering these three logs as a data

68
00:04:03,310 --> 00:04:05,500
points and try to identify outliers.

69
00:04:05,800 --> 00:04:10,330
It's promising, but the models often
break down when new types of logs appear,

70
00:04:10,570 --> 00:04:14,830
which is actually when we need them most
because we learn on the logs that we saw

71
00:04:14,830 --> 00:04:18,600
before and why classical approaches fail.

72
00:04:19,110 --> 00:04:21,960
So why do this method
fail in the practice?

73
00:04:22,140 --> 00:04:24,390
They create too many false positives.

74
00:04:25,095 --> 00:04:29,865
If every little deviation triggers an
alert, engineers stop paying attention.

75
00:04:30,225 --> 00:04:34,565
We we've, we had all the situation,
the pages goes off, you look

76
00:04:34,715 --> 00:04:35,975
and it's something harmless.

77
00:04:36,245 --> 00:04:37,475
They like business context.

78
00:04:37,505 --> 00:04:38,615
As I said before.

79
00:04:38,855 --> 00:04:43,415
They gotta change their context and
have poor generalization When the system

80
00:04:43,415 --> 00:04:47,675
encounter something new, classical models
simply say, I don't know what is that?

81
00:04:47,915 --> 00:04:50,795
Which either means they
ignore it or they overreact.

82
00:04:51,095 --> 00:04:53,660
In short, this approach
has flu us with noise.

83
00:04:54,260 --> 00:04:56,840
While missing a signals
we really care about.

84
00:04:57,830 --> 00:05:01,760
And let's enter our
embedded context agents.

85
00:05:02,300 --> 00:05:03,440
What is alternative?

86
00:05:03,920 --> 00:05:06,230
This is where embedded
context really come in.

87
00:05:06,770 --> 00:05:10,770
Instead of positive, and
passive rules or static models.

88
00:05:10,950 --> 00:05:15,510
We embedded small intelligent components
directly into Observ JT Pipeline.

89
00:05:16,320 --> 00:05:17,250
These agents are.

90
00:05:18,030 --> 00:05:22,169
They embedded, they live inside our
system, analyzing logs in real time.

91
00:05:22,780 --> 00:05:24,130
The second one, contextual.

92
00:05:24,520 --> 00:05:27,670
They add business and
system context to every log.

93
00:05:28,030 --> 00:05:32,740
And autonomous agents, they don't
just look at the data, they act alert,

94
00:05:32,800 --> 00:05:35,290
correlate, even fix small problems.

95
00:05:35,469 --> 00:05:38,230
In this case, we don't need to to
change the context of developer.

96
00:05:38,560 --> 00:05:43,430
It's like moving from passive CCTV
camera to an active security guard.

97
00:05:43,930 --> 00:05:46,240
And what does an agent do?

98
00:05:46,900 --> 00:05:51,730
So step one, it ingest logs in real time.

99
00:05:52,000 --> 00:05:54,940
Step two, it enriches them with context.

100
00:05:55,240 --> 00:05:59,290
That means attaching metadata, like
which service, which deployment,

101
00:05:59,350 --> 00:06:00,475
which user, which region.

102
00:06:01,120 --> 00:06:05,980
And step three, it analyzes this
enriched logs using ML models,

103
00:06:06,280 --> 00:06:08,170
embeddings, or pattern recognition.

104
00:06:08,290 --> 00:06:09,430
And step four.

105
00:06:09,865 --> 00:06:14,435
It acts data might mean raising
an hour cluster, an anomaly

106
00:06:14,435 --> 00:06:18,125
with previous incident, or even
recommending an automated mitigation.

107
00:06:18,335 --> 00:06:21,245
It's not about just seeing data,
it's about understanding it

108
00:06:21,515 --> 00:06:23,495
and taking the right next step.

109
00:06:23,995 --> 00:06:27,085
So to make it more clear,
let's use an analogy.

110
00:06:27,685 --> 00:06:34,165
For example, let's consider the mall where
we have security cards, and in the mall we

111
00:06:34,195 --> 00:06:37,405
have cameras that can observe everywhere.

112
00:06:38,335 --> 00:06:42,895
And it can check that people walking
shops, opening deliveries happening.

113
00:06:43,285 --> 00:06:46,075
That's useful, but it
cannot be interpreted.

114
00:06:46,345 --> 00:06:49,165
So for that, we need some
guy who will check everything

115
00:06:49,165 --> 00:06:50,425
and says, what's happening?

116
00:06:50,875 --> 00:06:54,745
The guard doesn't just watch, they
interpret if someone is walking around

117
00:06:54,745 --> 00:06:59,395
the night in a closed mall, that's
suspicious and he need to react.

118
00:06:59,795 --> 00:07:02,795
But the same person walking on
a Black Friday during a sale.

119
00:07:03,425 --> 00:07:04,685
Nobody will pay attention.

120
00:07:04,985 --> 00:07:10,145
There's different context, makes logs
without agents are just raw video, but

121
00:07:10,145 --> 00:07:12,725
logs with agents are actionable insights.

122
00:07:13,565 --> 00:07:17,195
So let's talk a little
bit about architecture.

123
00:07:17,745 --> 00:07:22,525
Initially it'll be something
simple, but later we will deep dive.

124
00:07:23,515 --> 00:07:28,435
Regarding system architecture, how
it looks like logs are ingested.

125
00:07:28,435 --> 00:07:32,845
For tools, it can be like fluid
D logs test or open telemetry.

126
00:07:33,425 --> 00:07:34,534
There are a lot of solutions.

127
00:07:35,315 --> 00:07:41,075
So they go through contextual enrichment,
really, agents at metadata, like

128
00:07:41,075 --> 00:07:44,255
user id, region deployment details.

129
00:07:44,555 --> 00:07:46,385
And the next one is detection core.

130
00:07:46,955 --> 00:07:52,044
It can be all the things that we discussed
before, like classical approaches, but

131
00:07:52,044 --> 00:07:56,840
as well it can be ML models that analyze
the sequence of logs and look for ena.

132
00:07:57,549 --> 00:07:59,499
And finally the action layer.

133
00:07:59,739 --> 00:08:03,909
This is where alerts are raised,
dashboards updated in some cases at

134
00:08:03,909 --> 00:08:05,380
automated response or trigger it.

135
00:08:05,859 --> 00:08:09,370
It's a pipeline in jest and
reach, analyze and connect.

136
00:08:10,030 --> 00:08:12,760
So in the next slide, we'll take a look.

137
00:08:12,789 --> 00:08:17,850
Some primitive architecture, a
lot of details are omitted but

138
00:08:17,910 --> 00:08:20,580
some of them I will just consider.

139
00:08:21,080 --> 00:08:23,420
So let's imagine this pipeline.

140
00:08:24,020 --> 00:08:26,240
We have a few things.

141
00:08:26,850 --> 00:08:31,160
The architecture on the screen illustrates
full end to end and a malo detection.

142
00:08:31,220 --> 00:08:35,900
That system processes logs from multiple
sources and enriches, then individual

143
00:08:35,900 --> 00:08:40,010
information, applies machine learning,
and finally roads to result in the

144
00:08:40,010 --> 00:08:43,050
different destinations and for visibility.

145
00:08:43,050 --> 00:08:44,040
Action automation.

146
00:08:44,820 --> 00:08:48,000
The first one is, as I
said before, log sources.

147
00:08:48,340 --> 00:08:51,530
The pipeline begins with
a diverse resources.

148
00:08:52,085 --> 00:08:56,855
It can be some application
logs regarding APIs, like air

149
00:08:56,855 --> 00:08:59,775
performance transactions as well.

150
00:08:59,775 --> 00:09:03,825
It can be some infrastructure
logs like service containers,

151
00:09:03,855 --> 00:09:05,445
cloud resources and stuff.

152
00:09:06,335 --> 00:09:09,535
And the third thing is, security thing.

153
00:09:10,105 --> 00:09:15,775
For example, when we have like
assess attempts or Audi trials or

154
00:09:15,775 --> 00:09:19,825
firewall events, and this represent
a whole telemetry of the system.

155
00:09:20,515 --> 00:09:23,125
The second step, this
is a local collector.

156
00:09:23,395 --> 00:09:27,295
It can be, for example, open
telemetry or fluent geo and stuff.

157
00:09:27,745 --> 00:09:31,805
A log collector normalizes and
ships the logs in real time.

158
00:09:32,145 --> 00:09:35,205
Fluent tea, for example, responsible
for collecting log files.

159
00:09:35,585 --> 00:09:38,795
Parsing them and applying
lightweight transformations.

160
00:09:39,345 --> 00:09:42,975
It gives us when we have different
format to standard standardizes

161
00:09:43,585 --> 00:09:47,305
this ensure consistent structure
and reduces noise before ingestion.

162
00:09:48,085 --> 00:09:50,545
The next step is message bus.

163
00:09:50,945 --> 00:09:54,695
Guys that we have billions of logs
every day and we need to send them

164
00:09:54,695 --> 00:09:56,795
somehow without breaking the system.

165
00:09:57,185 --> 00:10:01,265
So the solution here can be
some message bus, and in my

166
00:10:01,265 --> 00:10:02,645
opinion, the best one is Kafka.

167
00:10:03,215 --> 00:10:07,115
When we normalize the logs,
then we push them to Kafka.

168
00:10:07,115 --> 00:10:10,685
Message Bus Kafka provides
durable, scalable, high throughput

169
00:10:10,685 --> 00:10:14,645
streaming, and decouples produces
local collector from consumers.

170
00:10:14,645 --> 00:10:19,655
Analysis engine enables replay
and breath pressure handling in

171
00:10:19,700 --> 00:10:20,860
case of downstream slowdowns.

172
00:10:21,360 --> 00:10:26,100
And the next step is embedded
contact station and the core sheets.

173
00:10:26,100 --> 00:10:29,610
The embedded context agent, which
orchestrate normal detection.

174
00:10:29,970 --> 00:10:31,800
It consists three main layers.

175
00:10:31,860 --> 00:10:34,050
The first one, contact enrichment ends.

176
00:10:34,110 --> 00:10:39,780
It can be like metadata session info, user
device contact, and the stuff helps models

177
00:10:39,780 --> 00:10:41,370
understand the story behind raw evens.

178
00:10:41,870 --> 00:10:43,040
ML models is ation.

179
00:10:43,040 --> 00:10:48,980
Forest detect outliers in high dimension
data and out encoder reconstruct

180
00:10:48,980 --> 00:10:51,380
normal patterns, flag deviations.

181
00:10:51,650 --> 00:10:55,220
But by the way here may be
used as well LLM based models.

182
00:10:55,640 --> 00:10:57,170
It can be a little bit problematic.

183
00:10:57,170 --> 00:11:01,160
And later in this presentation I will
explain why, but it provides semantic

184
00:11:01,210 --> 00:11:04,600
semantic interpretation, animal
reasoning, and correlation across logs.

185
00:11:05,500 --> 00:11:08,440
So decision layer, aggregate
result from multiple models.

186
00:11:09,385 --> 00:11:15,235
And applies rules for an logic to
classify, lys, determine d severity

187
00:11:15,235 --> 00:11:16,855
type and the required response.

188
00:11:17,515 --> 00:11:20,245
Of course, the next step is
backend services, where we need to

189
00:11:20,245 --> 00:11:22,145
orchestrate everything, what we got.

190
00:11:22,480 --> 00:11:26,570
In the previous layer this component
act as a hub, that roads classified ano

191
00:11:26,570 --> 00:11:28,250
analyst into different destinations.

192
00:11:28,640 --> 00:11:34,680
It can be, for example, some dashboards
like Grafana, Kibana and stuff.

193
00:11:35,190 --> 00:11:39,425
And for example, wanting understand
that this is something urgent we can

194
00:11:39,845 --> 00:11:43,065
notify by Slack and other messages.

195
00:11:43,565 --> 00:11:49,475
As well, we need to keep this data somehow
to, to use it again, and for this purpose.

196
00:11:49,475 --> 00:11:53,015
We have a databases like Postgres,
Radis and elastic search and stuff.

197
00:11:53,825 --> 00:11:56,205
And for example, we can react on it.

198
00:11:56,940 --> 00:11:57,780
At the moment.

199
00:11:57,870 --> 00:12:01,350
And for that we can use like
Kubernetes auto mitigation.

200
00:12:01,790 --> 00:12:05,150
It triggers workflow, for example,
scaling ports, a flow that will

201
00:12:05,150 --> 00:12:07,250
detected or blocking an IP.

202
00:12:07,370 --> 00:12:11,440
If instructions suspected
reduces MTTR mean to recovery

203
00:12:11,440 --> 00:12:12,850
to eliminate manual steps.

204
00:12:13,510 --> 00:12:16,780
Or some other actions can be
done on the programming language.

205
00:12:17,560 --> 00:12:23,120
And the last step is end users that
get this information and can react.

206
00:12:23,780 --> 00:12:28,190
So this architecture provides resilience,
scalable, and intelligent animal

207
00:12:28,190 --> 00:12:32,270
detection pipeline, local collection and
normalization, and ensure clean data.

208
00:12:32,765 --> 00:12:36,485
Kafka guarantee scalable streaming
and reply and EBITDA engine

209
00:12:36,485 --> 00:12:41,255
combined context, ML and decisions
and output ensure both human

210
00:12:41,255 --> 00:12:43,335
visibility and automated remediation.

211
00:12:43,605 --> 00:12:47,745
It balances the real time detection with
historical analysis and automated action,

212
00:12:47,775 --> 00:12:52,485
making it suitable for modern cloud
native and security crucial environments.

213
00:12:53,145 --> 00:12:56,835
So now let's take a look on
detection Core ML models.

214
00:12:57,335 --> 00:12:59,645
Detection core uses a
combination of techniques.

215
00:13:00,020 --> 00:13:03,860
For example, out encoders, compress
and reconstruct lock patterns.

216
00:13:04,100 --> 00:13:05,570
Plug in anything that doesn't fit.

217
00:13:06,170 --> 00:13:08,720
Isolation forest are great
for identifying outliers.

218
00:13:09,110 --> 00:13:14,450
Log bird brings transformer based s and
capturing the semantic meanings of logs.

219
00:13:15,150 --> 00:13:19,900
For example, we have as well deep lock
use LSTM to model sequence of log events.

220
00:13:20,050 --> 00:13:23,320
And by combining these agents
can handle both structured and un

221
00:13:23,320 --> 00:13:26,290
structured logs, as well as a new
pattern we've never seen before.

222
00:13:26,790 --> 00:13:28,980
Let's talk about
multi-agent collaboration.

223
00:13:29,160 --> 00:13:32,940
Another important point, we don't rely
on a simple agent to do everything.

224
00:13:33,240 --> 00:13:35,100
Instead, we use specialized agents.

225
00:13:35,130 --> 00:13:38,910
One focuses on network traffic,
another on authentication pattern,

226
00:13:39,150 --> 00:13:40,710
another on performance metrics.

227
00:13:41,010 --> 00:13:45,450
They, each of them act as a domain
expert, but they can of course,

228
00:13:45,450 --> 00:13:49,300
collaborate with shared bus, can
be usually kaf or a similar system.

229
00:13:49,630 --> 00:13:52,750
And together they can form a
system of small, specialized.

230
00:13:53,075 --> 00:13:53,675
Experts.

231
00:13:54,365 --> 00:13:57,425
For example, let's take a
look on different agents.

232
00:13:58,145 --> 00:14:00,065
We have three main Skinners.

233
00:14:00,245 --> 00:14:06,355
The first one, for example, when we
got some problem like 500 TTP, when

234
00:14:06,355 --> 00:14:11,275
we did some order, the agent compares
this with MBX of non neuro classes.

235
00:14:11,755 --> 00:14:12,355
Not much.

236
00:14:12,595 --> 00:14:15,295
It's in new, so it raises anomaly flack.

237
00:14:15,595 --> 00:14:20,125
This gets an attention of DevOps before
an error spreads without the agent.

238
00:14:20,125 --> 00:14:23,725
This error might just be borrowed in
a thousand of line of logs and gone

239
00:14:23,725 --> 00:14:25,375
notice until customers complain.

240
00:14:25,875 --> 00:14:32,690
The next scenarios when we have,
for example suspicious logs like the

241
00:14:32,690 --> 00:14:36,920
authentication agent notices a spike in
the login attempts from region, we don't.

242
00:14:37,610 --> 00:14:39,710
Really, you usually see the activity.

243
00:14:40,230 --> 00:14:42,240
We need to enrich a metadata.

244
00:14:42,540 --> 00:14:47,640
For example, we see same API
range, same data frame or timeframe

245
00:14:48,060 --> 00:14:50,250
and hundreds of flights logins.

246
00:14:50,730 --> 00:14:53,580
It's not just technical error,
it's potential and tech.

247
00:14:53,670 --> 00:14:58,260
And by flagging it early, we can prevent
account takeover of data breaches.

248
00:14:58,830 --> 00:15:03,120
And the last scenario that I would
consider is early performance warning.

249
00:15:03,420 --> 00:15:05,340
For example, sometimes the
system interest looks fine.

250
00:15:06,015 --> 00:15:11,415
Like we don't have problem with RAM or
CPU and latency Dashboards are green, but

251
00:15:11,445 --> 00:15:16,455
agents are noticing more and more looks
like slow query detected individually.

252
00:15:16,515 --> 00:15:19,395
It doesn't trigger an
alert, but trend is obvious.

253
00:15:19,665 --> 00:15:23,265
The performance agent raises
an normally alert, something is

254
00:15:23,265 --> 00:15:28,505
degrading and engineers can fix
it before it becomes an outage.

255
00:15:28,655 --> 00:15:31,205
This is power of context and
sequence based detection.

256
00:15:31,705 --> 00:15:35,455
So let's take a look more about ML models.

257
00:15:35,485 --> 00:15:37,975
For example, deep log we got in 2017.

258
00:15:38,455 --> 00:15:41,335
It's a pioneer sequence
based on ly detection.

259
00:15:41,395 --> 00:15:44,675
So it's a kind of recent
technologies special log brought.

260
00:15:44,675 --> 00:15:47,170
It's 2021, applied to transform to logs.

261
00:15:47,795 --> 00:15:51,815
Log AI from Microsoft as an
open source library for log

262
00:15:51,815 --> 00:15:53,195
par and antibody detection.

263
00:15:53,540 --> 00:15:55,970
And as well there is a dent
free, for example, that helps

264
00:15:55,970 --> 00:15:57,470
build streaming log templates.

265
00:15:57,830 --> 00:15:59,870
This is a building blocks and agent.

266
00:15:59,870 --> 00:16:05,390
Combine them with domain specific
context, so I didn't say something before.

267
00:16:05,390 --> 00:16:10,270
Some about LLM, like charge
GPT where it can be fit.

268
00:16:10,950 --> 00:16:15,280
This is actually a very good
thing for example, analyzing.

269
00:16:15,940 --> 00:16:20,780
Patterns and human friendly language
that makes a great tool for engineers

270
00:16:20,780 --> 00:16:26,330
as well day to day activity, but it
has some limitations, unfortunately.

271
00:16:26,830 --> 00:16:28,780
For example, it's not real time.

272
00:16:29,190 --> 00:16:30,420
Do you know how it's happening?

273
00:16:30,450 --> 00:16:34,325
We are doing some kind of request and we
got a wait until we got the full request.

274
00:16:34,825 --> 00:16:36,955
Yeah, it doesn't
continuously monitor logs.

275
00:16:36,985 --> 00:16:38,485
It only works when you ask.

276
00:16:39,085 --> 00:16:42,805
And it's really hard to scale 2
million of logs line per minute.

277
00:16:43,105 --> 00:16:46,255
You would spend a fortune trying
to process all logs with this way.

278
00:16:46,405 --> 00:16:48,985
And finally, there are
issues of privacy and cost.

279
00:16:49,045 --> 00:16:53,875
As guys, it costs a lot, many
logs, contain sensitive data, send

280
00:16:53,925 --> 00:16:56,680
sending everything to large models,
not always safe and affordable.

281
00:16:57,645 --> 00:17:01,965
Of course we can use like local
models, but still it's not like a

282
00:17:02,055 --> 00:17:07,035
thing that can resolve everything,
now let's compare a little bit chat.

283
00:17:07,095 --> 00:17:10,690
GPT or LLM, but l ML agents
that we discussed before.

284
00:17:11,380 --> 00:17:15,830
The reality is following LLM and
ML agents serve different purposes.

285
00:17:16,040 --> 00:17:19,580
For example, agents, they can be
real time, they can be scalable,

286
00:17:19,580 --> 00:17:21,410
they can be domain specific, cheaper.

287
00:17:22,040 --> 00:17:26,690
But Chad GPT is a great explanation
of course, but still it's not

288
00:17:26,690 --> 00:17:28,610
suitable for billions of re requests.

289
00:17:29,600 --> 00:17:33,740
But the best thing we can
use, like a hybrid approach, I

290
00:17:33,740 --> 00:17:35,270
think this is the best setup.

291
00:17:35,900 --> 00:17:39,170
Agents continuously scan and
detect analyst in real time.

292
00:17:39,260 --> 00:17:43,300
They raises alerts, feed dashboards,
and trigger automations with

293
00:17:43,300 --> 00:17:45,010
something serious is found.

294
00:17:45,400 --> 00:17:49,900
But when we're talking about LLM, it
helping engineers quickly understand

295
00:17:49,900 --> 00:17:53,230
what's happening, summarizing patterns
and suggesting possible root causes.

296
00:17:53,730 --> 00:17:56,520
I guess this approach will
give us the best results.

297
00:17:57,020 --> 00:18:02,480
So let's take a look a little bit
benefits from embedded context agents.

298
00:18:02,600 --> 00:18:05,025
What are the benefits of
using cabinet context agents?

299
00:18:05,525 --> 00:18:05,975
Easy.

300
00:18:06,305 --> 00:18:09,245
They reduce noise by
filtering out false positives.

301
00:18:09,515 --> 00:18:14,285
They operate in real time, giving
you early detection and extensible.

302
00:18:14,315 --> 00:18:16,715
You can add new engines for new domains.

303
00:18:17,105 --> 00:18:21,985
They integrate seamlessly with the
DevOps workflows and CICD pipelines and

304
00:18:21,985 --> 00:18:23,905
bring intelligent closers to the data.

305
00:18:24,405 --> 00:18:25,245
Let's take a look.

306
00:18:25,245 --> 00:18:26,020
Some challenges.

307
00:18:26,520 --> 00:18:28,140
But let's be honest, it's not easy.

308
00:18:28,200 --> 00:18:33,540
There are challenges like training models,
especially when it comes to the LLM based.

309
00:18:33,840 --> 00:18:34,860
It can be expensive.

310
00:18:35,310 --> 00:18:37,620
Context must always be on up to date.

311
00:18:37,860 --> 00:18:40,840
New deployments, new
services, snowflakes confis.

312
00:18:41,340 --> 00:18:45,000
Explainability is hard in if
an agent says this, an anomaly

313
00:18:45,030 --> 00:18:46,530
engineer want to know why?

314
00:18:46,710 --> 00:18:48,810
And agent coordination is tricky.

315
00:18:48,900 --> 00:18:52,320
Multiple agents might
overlap or even conflict.

316
00:18:52,590 --> 00:18:53,370
This is serious.

317
00:18:53,370 --> 00:18:58,940
While we still solve as a community, let's
take a look a bit the future Directions.

318
00:18:59,660 --> 00:19:01,850
First rack, four locks.

319
00:19:01,910 --> 00:19:05,185
Combining retrieval of similar
incidents with generative models.

320
00:19:06,080 --> 00:19:11,180
The second self-healing systems agents
that don't just detect animals but also

321
00:19:11,180 --> 00:19:16,250
fix simple issues an automatically, as
I said before, for example, with the

322
00:19:16,250 --> 00:19:20,180
Kubernetes that can kill the ports,
but it's still not something huge.

323
00:19:20,510 --> 00:19:24,770
And the third federated anomaly detection
agent across cluster, sharing their

324
00:19:24,770 --> 00:19:26,810
knowledge to improve detection globally.

325
00:19:27,170 --> 00:19:30,500
This is where the field is going, and it's
very exciting time to be working on it.

326
00:19:31,000 --> 00:19:32,760
Let's take a look a little bit.

327
00:19:32,760 --> 00:19:36,500
The use cases, for example, when
I was talking about DevOps for

328
00:19:36,500 --> 00:19:38,210
DevOps, agents are game changing.

329
00:19:38,270 --> 00:19:41,300
Imagine a deployment pipeline
as a new version roll out.

330
00:19:41,330 --> 00:19:43,370
Agents watch the logs in the real time.

331
00:19:43,730 --> 00:19:47,210
If unusual error spike beyond
what expected during the world

332
00:19:47,640 --> 00:19:50,340
they adjusted automatically fly
or even roll back the deployment.

333
00:19:50,490 --> 00:19:53,310
This reduces downtime
and makes releases safer.

334
00:19:53,880 --> 00:19:55,210
And for example.

335
00:19:55,710 --> 00:19:58,050
It can be used as well for security.

336
00:19:58,480 --> 00:20:00,460
Agent provide another layer of defense.

337
00:20:00,490 --> 00:20:03,940
They monitor login pattern,
user behavior and error errors.

338
00:20:04,730 --> 00:20:09,560
If they detect an ly, say a sudden burst
of failed logins or unusual, API calls

339
00:20:09,560 --> 00:20:14,000
from a specific region, they raise an
alert by coaching this, by catching

340
00:20:14,000 --> 00:20:17,990
this early, you can prevent incident
before they escalate into breaches.

341
00:20:18,490 --> 00:20:20,230
So let's me conclude.

342
00:20:20,650 --> 00:20:22,450
Logs by themselves are just noise.

343
00:20:22,945 --> 00:20:26,185
But by the embedded context, agents
logs are transformed with the

344
00:20:26,185 --> 00:20:28,495
meaningful and actionable signals.

345
00:20:28,885 --> 00:20:33,325
These agents bring context, intelligence
and autonomy into log analysis.

346
00:20:33,445 --> 00:20:37,165
And for example, if we combine it
with like systems like Charge GPT

347
00:20:37,165 --> 00:20:41,905
or other LLM, they empower engineers
to detect, understand, and respond

348
00:20:41,905 --> 00:20:43,945
to problems faster than ever before.

349
00:20:44,515 --> 00:20:47,695
And most likely, this is the
future of platform engineering.

350
00:20:47,785 --> 00:20:51,055
Smarter, more proactive,
more adaptive systems.

351
00:20:51,625 --> 00:20:53,160
So thank you all for your attention.

