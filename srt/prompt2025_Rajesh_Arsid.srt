1
00:00:00,300 --> 00:00:01,080
Hello everyone.

2
00:00:01,080 --> 00:00:01,950
My name is Raje.

3
00:00:02,490 --> 00:00:04,440
I'm principal engineer at Synopsis.

4
00:00:04,710 --> 00:00:08,430
My presentation is about how ultra
ethernet and UL link accelerate

5
00:00:08,430 --> 00:00:10,110
token to token performance.

6
00:00:10,610 --> 00:00:14,239
In my presentation, I'll be covering
on AI infrastructure bottleneck

7
00:00:14,239 --> 00:00:18,049
need for enhanced interconnect
technology, UL link Ultra ethernet,

8
00:00:18,049 --> 00:00:19,580
and finally the conclusion.

9
00:00:20,080 --> 00:00:24,220
AI infrastructure bottleneck in
the present AI infrastructure.

10
00:00:24,220 --> 00:00:26,709
There is exponential growth
in the compute demand.

11
00:00:26,769 --> 00:00:30,909
Despite parallelization, the training time
of model have rise from week to months.

12
00:00:31,360 --> 00:00:34,720
If you look into the picture on the
right hand side, model parameters are

13
00:00:34,720 --> 00:00:36,519
doubling every three to four months.

14
00:00:37,150 --> 00:00:41,349
Multi-dimensional systems are getting
stretched, like network bandwidth,

15
00:00:41,410 --> 00:00:46,480
network latency, memory, bandwidth,
memory capacity, and then compute.

16
00:00:46,980 --> 00:00:50,190
Also the design complexities
are increasing memory bandwidth,

17
00:00:50,219 --> 00:00:51,330
interconnect bandwidth.

18
00:00:51,690 --> 00:00:55,800
Now if you look into the extreme right
side, we have a DDR standard, HBM

19
00:00:55,800 --> 00:01:01,259
standard PCA standard and DD day to day
standard are exponentially increasing.

20
00:01:01,319 --> 00:01:05,920
Therefore, we need AI infrastructure
with enhanced interconnect technology

21
00:01:05,920 --> 00:01:07,630
to meet current and future demand.

22
00:01:08,130 --> 00:01:10,139
Need for enhanced interconnect technology.

23
00:01:10,469 --> 00:01:14,009
In this slide, let try to understand
why GPU performance is important.

24
00:01:14,009 --> 00:01:19,079
If you consider AI ML lifecycle in the
picture, to build a model, we need to

25
00:01:19,079 --> 00:01:23,219
prep the data first, we need to prep
the data, build a model, train it,

26
00:01:23,429 --> 00:01:25,559
test it, and then fine tune the model.

27
00:01:26,100 --> 00:01:29,729
So there is a continuous feedback
provided to fine tune the model,

28
00:01:29,759 --> 00:01:33,719
and this loop continues during
which data is split and fed.

29
00:01:34,090 --> 00:01:39,100
To multiple GPUs and sometimes
multiple machines at larger scale.

30
00:01:39,520 --> 00:01:43,300
Therefore, GPU performance influences
the timeline of deep learning.

31
00:01:43,800 --> 00:01:47,610
UL Link Scale Up UL Link is
an open source interconnect

32
00:01:47,610 --> 00:01:49,230
technology developed to scale up.

33
00:01:49,699 --> 00:01:51,499
Accelerators for AI workload.

34
00:01:51,919 --> 00:01:54,350
If you look into the
picture here, we have a pod.

35
00:01:54,350 --> 00:01:56,179
This is also called as a cluster.

36
00:01:56,270 --> 00:01:59,330
We have a racks in them
stacked vertically.

37
00:01:59,359 --> 00:02:00,889
Each rack will have a GPU.

38
00:02:01,249 --> 00:02:05,749
All these GPUs are interconnected
to get together through a UL link.

39
00:02:06,249 --> 00:02:09,849
So what are we trying to achieve here
is we are trying to connect all the GPUs

40
00:02:09,849 --> 00:02:12,640
together to create one big giant GPU.

41
00:02:13,140 --> 00:02:15,989
To enable memory sharing and
synchronization between the

42
00:02:15,989 --> 00:02:20,579
accelerators so that there is a direct
load store and automat operations

43
00:02:20,579 --> 00:02:22,349
enabling between the accelerators.

44
00:02:23,279 --> 00:02:26,459
UL Link can connect up to a
hundred to thousands of GPUs

45
00:02:26,459 --> 00:02:29,119
together ultra ethernet scale out.

46
00:02:29,619 --> 00:02:33,489
So Ultra Ethernet is an open source,
high performance networking technology

47
00:02:33,489 --> 00:02:37,690
developed by ultra ethernet consortium
to offload AI workload and HPC.

48
00:02:38,109 --> 00:02:40,899
If you look into the picture on the
right hand side, we all, we have

49
00:02:40,899 --> 00:02:46,480
already discussed about UL Link
now talking about ultra ethernet,

50
00:02:46,510 --> 00:02:48,130
which is highlighted in the local.

51
00:02:48,130 --> 00:02:48,385
These are.

52
00:02:48,985 --> 00:02:52,495
Connecting all the clusters
together, which is called scale out.

53
00:02:53,425 --> 00:02:57,115
This establishes a high bandwidth,
multi-path open, standard,

54
00:02:57,115 --> 00:02:59,095
highly configurable interface.

55
00:02:59,245 --> 00:03:03,805
This is very much important for AI
clustering and also ALTA ethernet

56
00:03:03,805 --> 00:03:10,135
stack introduces new transport layer
with enhanced congestion control

57
00:03:10,435 --> 00:03:12,985
and then enhanced RDMA capabilities.

58
00:03:13,945 --> 00:03:17,785
Here we are talking about interconnecting
millions of GPUs together.

59
00:03:18,285 --> 00:03:20,595
Conclusion Token to token performance.

60
00:03:20,894 --> 00:03:25,635
UL Link is used to connect two aspirators
together so that there is a memory

61
00:03:25,635 --> 00:03:27,674
synchronization happening between them.

62
00:03:28,394 --> 00:03:33,140
This scenario is optimized for AI
workload because of rapid token passing.

63
00:03:33,640 --> 00:03:37,660
Ultra ethernet establishes low
latency, high throughput for rapid

64
00:03:37,660 --> 00:03:40,060
token exchange and scalability.

65
00:03:40,560 --> 00:03:43,290
Thank you for taking time
and watching my presentation.

