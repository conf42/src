1
00:00:00,780 --> 00:00:01,500
Hi everyone.

2
00:00:01,680 --> 00:00:05,340
My name is ELA Mo and I'm happy
to be presenting here at the

3
00:00:05,340 --> 00:00:07,380
JavaScript conference 2025.

4
00:00:07,800 --> 00:00:10,770
I wanted to start with a quick
introduction about myself.

5
00:00:11,010 --> 00:00:15,210
I currently work for the Wells Fargo Bank
and I have around 14 years of hands-on

6
00:00:15,210 --> 00:00:17,850
experience in advanced data analytics.

7
00:00:17,860 --> 00:00:22,360
Credit risk strategy and predictive
modeling across financial services,

8
00:00:22,390 --> 00:00:24,550
marketing, and customer intelligence.

9
00:00:24,910 --> 00:00:28,720
I also have extensive experience in
developing machine learning driven

10
00:00:28,720 --> 00:00:33,430
fraud detection strategies, segmentation
models and campaign optimization

11
00:00:33,430 --> 00:00:36,010
frameworks that help improve ROI.

12
00:00:36,555 --> 00:00:40,485
Reduce risk exposure and
enhance operational efficiency.

13
00:00:40,754 --> 00:00:45,975
The topic I'm planning to present today
is client side, NLP, JavaScript, text

14
00:00:45,975 --> 00:00:48,915
Analytics for user Behavior Intelligence.

15
00:00:49,394 --> 00:00:53,285
I'm actually pretty excited today
to be talking about how we are

16
00:00:53,285 --> 00:00:58,595
using client side natural language
processing, NLP, with JavaScript to

17
00:00:58,595 --> 00:01:02,789
understand user behavior in real time
without sending data to a server.

18
00:01:03,644 --> 00:01:07,934
This talk is about rethinking
how we do text analytics, moving

19
00:01:07,934 --> 00:01:13,844
intelligence closer to the user inside
the browser where privacy, latency

20
00:01:13,874 --> 00:01:16,065
and interact with all matter most.

21
00:01:16,395 --> 00:01:21,044
I'll walk you through why this shift
is important, the architecture we used,

22
00:01:21,104 --> 00:01:26,605
the tools and techniques involved, and
that what kind of insights it unlocks

23
00:01:26,605 --> 00:01:29,035
for product teams and developers.

24
00:01:29,535 --> 00:01:32,054
Let's jump to the next slide.

25
00:01:32,414 --> 00:01:35,234
This slide talks about
the hidden data problem.

26
00:01:35,534 --> 00:01:37,094
Let's start with the challenge.

27
00:01:37,335 --> 00:01:41,629
Traditional analytic tools do a
great job of tracking what users do.

28
00:01:42,329 --> 00:01:47,969
Clicks, scrolls and time on each page,
but not why they behave that way.

29
00:01:48,329 --> 00:01:52,349
We are capturing millions of user
interactions, but missing the deeper

30
00:01:52,349 --> 00:01:54,689
layer of intent, hidden in their language.

31
00:01:55,079 --> 00:02:00,479
Every form, submission, chat
message, search query, and feedback

32
00:02:00,479 --> 00:02:02,969
comment contains behavioral signals.

33
00:02:03,464 --> 00:02:08,144
Which are emotion, intent,
confusion, satisfaction.

34
00:02:08,324 --> 00:02:12,914
For example, a user might click
contact support, but the message

35
00:02:12,914 --> 00:02:17,265
they write reveals frustration,
confusion, or even shown intent.

36
00:02:18,164 --> 00:02:22,904
Those insights are hidden inside
unstructured text, and historically

37
00:02:22,904 --> 00:02:27,464
we have ignored them because crossing
language was expensive and required

38
00:02:27,464 --> 00:02:32,195
server side pipelines, that's
the problem we set out to solve.

39
00:02:32,695 --> 00:02:34,555
Moving on to the next slide.

40
00:02:34,915 --> 00:02:38,665
So this slide talks about
why process NLP client side.

41
00:02:38,935 --> 00:02:41,125
So why bring NLP to the client?

42
00:02:41,395 --> 00:02:46,855
There are three main drivers, privacy,
realtime feedback, and efficiency.

43
00:02:47,755 --> 00:02:51,055
We can start talking about privacy
in a world of data regulations

44
00:02:51,865 --> 00:02:53,760
and user trust concerns.

45
00:02:54,204 --> 00:02:57,295
Sending raw text to a server is risky.

46
00:02:57,624 --> 00:03:01,255
Client said, NLP means the
text never leaves the browser.

47
00:03:01,494 --> 00:03:03,505
No data leakage, no compliance headaches.

48
00:03:04,005 --> 00:03:06,465
The next one is real time feedback.

49
00:03:06,734 --> 00:03:10,215
Processing happens
instantly as the user types.

50
00:03:10,504 --> 00:03:15,244
We can detect sentiment, hesitation,
or frustration and adapt the

51
00:03:15,244 --> 00:03:18,124
interference interface in a real time.

52
00:03:18,435 --> 00:03:22,710
Maybe offer help highlight CTA
or are just tone dynamically.

53
00:03:23,555 --> 00:03:25,415
The next one is efficiency.

54
00:03:25,805 --> 00:03:30,515
By offloading computations to the
user's device, we drastically reduce

55
00:03:30,515 --> 00:03:33,215
server costs and network latency.

56
00:03:33,635 --> 00:03:40,355
With GPO acceleration in browsers, users
essentially bring their own compute so we

57
00:03:40,355 --> 00:03:42,785
get privacy, immediacy, and scalability.

58
00:03:43,285 --> 00:03:45,205
The Holy Trinity of modern analytics.

59
00:03:45,505 --> 00:03:47,455
Moving on to the next slide.

60
00:03:47,775 --> 00:03:51,375
This slide talks about the
JavaScript NLP toolkit.

61
00:03:51,644 --> 00:03:55,035
Now let's talk about the tools
that make all this possible.

62
00:03:55,875 --> 00:03:58,185
First, natural and compromise.

63
00:03:58,484 --> 00:04:03,644
These are lightweight NLP Libraries
for tokenization, part of speech

64
00:04:03,644 --> 00:04:08,625
tagging and entity recognition
that run directly in the browser

65
00:04:08,894 --> 00:04:10,785
with zero external dependencies.

66
00:04:11,109 --> 00:04:14,140
Second TensorFlows js.

67
00:04:14,320 --> 00:04:19,029
This brings the power of pre-trained
machine learning models to JavaScript

68
00:04:19,269 --> 00:04:22,570
using WebGL for GPU acceleration.

69
00:04:22,879 --> 00:04:26,359
We use it for sentiment scoring
and text classification.

70
00:04:26,629 --> 00:04:33,799
And third web workers and index DB web
workers keep heavy NLP computations off

71
00:04:33,799 --> 00:04:36,859
the main thread, so the UI never freezes.

72
00:04:37,189 --> 00:04:43,219
While Index DB stores results locally
for offline access or historical trend

73
00:04:43,219 --> 00:04:49,159
analysis, together, these tools let
us replicate what used to require a

74
00:04:49,159 --> 00:04:51,734
Python backend all within the browser.

75
00:04:51,984 --> 00:04:53,965
Moving on to the next slide.

76
00:04:54,395 --> 00:04:57,125
This slide gives the
architecture overview.

77
00:04:57,424 --> 00:04:59,854
Here's a high level view
of the architecture.

78
00:05:00,184 --> 00:05:05,375
The design goal was to isolate NLP
processing from user interaction

79
00:05:05,705 --> 00:05:07,775
to keep everything responsive.

80
00:05:08,224 --> 00:05:13,715
Web workers handle all the model loading
and text analysis in the background.

81
00:05:14,375 --> 00:05:19,685
Service workers support offline
capabilities, caching models and data,

82
00:05:19,685 --> 00:05:21,599
so it still works without connectivity.

83
00:05:22,249 --> 00:05:28,159
Index DB serves as local storage for
processed analytics, letting us analyze

84
00:05:28,159 --> 00:05:31,219
behavior over time, even across sessions.

85
00:05:31,460 --> 00:05:37,159
And finally, results are funneled back
to a centralized state management layer,

86
00:05:37,699 --> 00:05:44,059
think Redux or context API, which updates
the react components in real time.

87
00:05:44,559 --> 00:05:49,899
This architecture mimics distributed
computing, but at the browser level,

88
00:05:50,139 --> 00:05:52,959
light modular and event driven.

89
00:05:53,259 --> 00:05:55,059
Moving on to the next slide.

90
00:05:55,409 --> 00:05:58,919
This slide talks about building
realtime sentiment analysis.

91
00:05:59,219 --> 00:06:03,869
So how does realtime sentiment
analysis actually work?

92
00:06:04,079 --> 00:06:05,279
Here's a process.

93
00:06:05,429 --> 00:06:10,949
The first step, we load a pre-trained
tensor flow dot js model at App startup,

94
00:06:11,309 --> 00:06:13,294
usually a lightweight transformer.

95
00:06:13,659 --> 00:06:17,529
Or LSTM variant optimized for client use.

96
00:06:17,949 --> 00:06:24,009
Next, when a user types into a chat
feedback box or a form, we capture the

97
00:06:24,009 --> 00:06:30,279
input events, but we use debouncing, so
we are not analyzing every keystroke.

98
00:06:30,639 --> 00:06:36,639
The next step is the text gets
tokenized and vectorized into numerical.

99
00:06:36,714 --> 00:06:41,814
Representations and that's passed
to the model for in inference,

100
00:06:42,054 --> 00:06:44,304
which outputs a sentiment score.

101
00:06:44,814 --> 00:06:52,224
Finally, we visualize that score right in
the UI may be a color indicator emoji or

102
00:06:52,314 --> 00:06:55,524
sentiment garge that updates as they type.

103
00:06:56,004 --> 00:07:02,244
This gives immediate emotional feedback
and provides continuous sentiment streams

104
00:07:02,394 --> 00:07:04,824
that pro product teams can analyze.

105
00:07:05,114 --> 00:07:07,304
Moving on to the next slide.

106
00:07:07,614 --> 00:07:10,794
This slide talks about user
segments hidden in text.

107
00:07:11,184 --> 00:07:15,834
Once we have enough text, data
patterns start to emerge that

108
00:07:15,834 --> 00:07:18,204
reveal distinct user segments.

109
00:07:18,869 --> 00:07:22,049
There are different types of
user segments hidden in text.

110
00:07:22,479 --> 00:07:24,069
First one is hesitant users.

111
00:07:24,399 --> 00:07:26,919
They use language full of uncertainty.

112
00:07:26,979 --> 00:07:29,979
Words like maybe I think, or questions.

113
00:07:30,279 --> 00:07:31,869
They're undecided and need reassurance.

114
00:07:32,119 --> 00:07:35,929
The next set of segments
are competitive switchers.

115
00:07:36,259 --> 00:07:40,069
These users explicitly mention
other products or pricing.

116
00:07:40,399 --> 00:07:44,689
They're evaluating options and can
be won over with targeted offers.

117
00:07:45,029 --> 00:07:47,309
The next segment is high intent prospects.

118
00:07:47,549 --> 00:07:52,169
They use decisive action oriented
language, like ready to buy,

119
00:07:52,199 --> 00:07:54,689
need integration, start tomorrow.

120
00:07:54,899 --> 00:07:56,489
These are your hottest leads.

121
00:07:57,389 --> 00:08:01,659
The next one is at-risk users
negative sentiment spikes

122
00:08:01,909 --> 00:08:04,309
complaints or cancellation keywords.

123
00:08:04,579 --> 00:08:08,929
These users signal churn risk
early by categorizing users.

124
00:08:08,929 --> 00:08:12,439
This way we can align
messaging and support to each

125
00:08:12,439 --> 00:08:13,609
behavior type in real time.

126
00:08:13,889 --> 00:08:15,449
Moving on to the next slide.

127
00:08:15,809 --> 00:08:21,900
This slide talks about topic modeling
with JavaScript beyond user sentiment.

128
00:08:22,140 --> 00:08:25,439
We also want to understand
what people are talking about.

129
00:08:25,740 --> 00:08:28,110
That's where topic modeling comes in.

130
00:08:28,510 --> 00:08:34,190
We use JavaScript implementations
of algorithms like key means or

131
00:08:34,190 --> 00:08:38,960
hierarchical clustering to group
similar messages without any labels.

132
00:08:39,320 --> 00:08:45,680
Each text snippet is vectorized
using T-F-I-D-F or word embeddings

133
00:08:45,680 --> 00:08:51,440
and dimensionality reduction helps
us visualize them in clusters, say

134
00:08:51,440 --> 00:08:53,720
billing issues, feature requests.

135
00:08:54,175 --> 00:08:55,824
Or performance feedback.

136
00:08:56,095 --> 00:09:00,714
What's powerful here is that this
runs dynamically as new messages

137
00:09:00,714 --> 00:09:03,475
arrive, clusters update automatically.

138
00:09:03,714 --> 00:09:08,035
It's like having live market research
running inside your product interface.

139
00:09:08,324 --> 00:09:10,094
Moving on to the next slide.

140
00:09:10,594 --> 00:09:13,324
This slide talks about
practical implementation

141
00:09:13,685 --> 00:09:15,514
text, pre-processing pipeline.

142
00:09:15,754 --> 00:09:20,854
Before any model can make sense of
text, we need to clean and structure it.

143
00:09:21,185 --> 00:09:23,944
Our pre-processing
pipeline looks like this.

144
00:09:24,135 --> 00:09:27,584
Normalize, tokenize,
vectorize, and analyze.

145
00:09:28,150 --> 00:09:33,069
When we talk about normalize, it basically
converts everything to lowercase,

146
00:09:33,280 --> 00:09:39,310
strip punctuation and trim white
space tokenize, split text into words,

147
00:09:39,670 --> 00:09:45,430
remove common stop words like the and
is, and apply stemming or limitation.

148
00:09:45,974 --> 00:09:52,574
A vectorize convert words into numerical
vectors using TF IDF, or embeddings.

149
00:09:53,084 --> 00:09:58,574
Analyze feed vectors into NLP models
for classification or clustering.

150
00:09:58,964 --> 00:10:05,234
Each stage is modular and usable, which
means developers can swap algorithms or

151
00:10:05,234 --> 00:10:08,019
add domain specific steps very easily.

152
00:10:08,519 --> 00:10:09,989
Moving on to the next slide.

153
00:10:10,240 --> 00:10:13,990
This slide talks about building
responsive react components.

154
00:10:14,360 --> 00:10:18,800
A big part of making this user
friendly is how we display results.

155
00:10:19,030 --> 00:10:24,280
We built reusable react components
that visualize sentiment

156
00:10:24,370 --> 00:10:26,230
and topics in real time.

157
00:10:26,540 --> 00:10:30,560
Custom hooks manage background
communication with web workers,

158
00:10:30,830 --> 00:10:33,140
so react only updates when.

159
00:10:33,400 --> 00:10:34,930
Fresh data arrives.

160
00:10:35,270 --> 00:10:40,040
We use context providers to
share NLP results across the app.

161
00:10:40,330 --> 00:10:44,800
So sentiment data in one component
can influence recommendations

162
00:10:44,800 --> 00:10:47,530
or UI color schemes elsewhere.

163
00:10:48,515 --> 00:10:53,975
For vis visualization, we integrated
D three Js perfect for generating

164
00:10:53,975 --> 00:10:58,535
dynamic sentiment timelines,
word clouds, and cluster graphs.

165
00:10:58,845 --> 00:11:03,945
The focus is on smooth interactivity,
loading states, transitions, and

166
00:11:03,945 --> 00:11:07,215
animations that make analytics
feel integrated, not intrus.

167
00:11:07,475 --> 00:11:08,765
Moving on to the next slide.

168
00:11:09,105 --> 00:11:12,885
This slide talks about feature
engineering text to metrics.

169
00:11:13,155 --> 00:11:17,355
Once we process the text, the
next step is to turn linguistic

170
00:11:17,355 --> 00:11:19,575
insights into actionable metrics.

171
00:11:19,875 --> 00:11:24,570
We do this in four layers, linguistic
features, behavioral metrics.

172
00:11:25,145 --> 00:11:27,665
Aggregate profiles and actionable scores.

173
00:11:28,125 --> 00:11:34,725
Linguistic features, so this is basically
sentiment, PO polarity tone, readability,

174
00:11:34,785 --> 00:11:41,325
entity density and emotional intensity,
behavioral metrics, how often users

175
00:11:41,325 --> 00:11:46,305
write, how quickly they respond, and
whether their tone changes over time.

176
00:11:46,935 --> 00:11:52,425
Aggregate profiles, combining text and
behavior into holistic user fingerprints

177
00:11:52,455 --> 00:11:54,255
that capture engagement style.

178
00:11:54,925 --> 00:11:58,745
Actionable scores things like
satisfaction index, churn,

179
00:11:58,745 --> 00:12:01,595
probability or intent confidence.

180
00:12:01,985 --> 00:12:07,745
This allows us to move from raw language
to measurable behavioral in intelligence.

181
00:12:08,635 --> 00:12:09,985
Moving on to the next slide.

182
00:12:10,265 --> 00:12:13,445
This slide talks about performance
optimization strategies.

183
00:12:13,805 --> 00:12:18,875
Client side NLP is powerful, but
we can be resource intensive.

184
00:12:19,275 --> 00:12:24,015
We use several optimization
tactics like lazy loading, batch

185
00:12:24,015 --> 00:12:26,595
processing, debouncing and caching.

186
00:12:26,985 --> 00:12:28,815
We only for lazy loading.

187
00:12:28,815 --> 00:12:33,645
We only download heavy tensor flow
models when they're actually needed.

188
00:12:34,095 --> 00:12:38,505
Keeping the initial bundle, small
batch processing, instead of

189
00:12:38,505 --> 00:12:41,025
analyzing text one snippet at a time.

190
00:12:41,085 --> 00:12:45,255
We queue and process in batches
maximizing GPU throughput.

191
00:12:45,755 --> 00:12:46,565
Debouncing.

192
00:12:46,805 --> 00:12:49,775
We avoid reprocessing partial sentences.

193
00:12:50,015 --> 00:12:53,495
Analysis only happens after
the user pauses typing.

194
00:12:54,165 --> 00:12:58,635
Caching results are stored in
indexed DB using hashtag keys.

195
00:12:58,875 --> 00:13:01,545
So repeated text isn't reanalyzed.

196
00:13:01,785 --> 00:13:05,655
These techniques keep the
experience seamless, lightweight

197
00:13:05,655 --> 00:13:10,305
on both CPU and memory while
still providing instant insights.

198
00:13:10,785 --> 00:13:12,720
Moving on to the next slide.

199
00:13:13,020 --> 00:13:17,330
This slide talks about data privacy
and security concentrations.

200
00:13:17,570 --> 00:13:19,850
Privacy isn't an afterthought.

201
00:13:20,180 --> 00:13:21,860
It's a design principle.

202
00:13:22,070 --> 00:13:23,840
Everything happens locally.

203
00:13:24,080 --> 00:13:26,060
User text never leaves the device.

204
00:13:26,855 --> 00:13:32,495
Before analysis users give
explicit consent through an opt-in

205
00:13:32,495 --> 00:13:36,785
prompt that explains exactly
what's being analyzed and stored.

206
00:13:37,145 --> 00:13:42,460
We allow users to view, export, or delete
their stored analytics data at any time.

207
00:13:43,280 --> 00:13:50,330
Sensitive text in Index DB is encrypted
and inco in incognito or private browsing

208
00:13:50,330 --> 00:13:53,120
mode analytics are automatically disabled.

209
00:13:53,510 --> 00:13:57,380
This approach shows that
advanced AI doesn't have to

210
00:13:57,380 --> 00:13:59,420
come at the cost of user trust.

211
00:13:59,670 --> 00:14:00,810
We're going to the next slide.

212
00:14:01,180 --> 00:14:03,640
This slide talks about
real world applications.

213
00:14:03,880 --> 00:14:08,230
Let's talk about where this approach
adds value in production environments.

214
00:14:08,620 --> 00:14:11,020
Some examples are
support, chat enhancement.

215
00:14:11,640 --> 00:14:14,730
Search intent analysis,
feedback classification.

216
00:14:15,030 --> 00:14:19,060
For support chart enhancement,
we can detect frustration or

217
00:14:19,090 --> 00:14:23,380
urgency automatically and route
high priority messages to live

218
00:14:23,380 --> 00:14:25,690
agents search intent analysis.

219
00:14:25,750 --> 00:14:30,790
This classifies search terms to
identify what users are truly looking

220
00:14:30,790 --> 00:14:33,100
for and where your content gaps are.

221
00:14:33,520 --> 00:14:37,420
Feedback, classification,
this automatically tags user

222
00:14:37,420 --> 00:14:39,340
feedback into categories like.

223
00:14:39,690 --> 00:14:41,730
Bugs, features and praise.

224
00:14:41,910 --> 00:14:47,100
Saving hours of manual sorting because
all this happens at the client side.

225
00:14:47,310 --> 00:14:52,590
It's scalable across millions of users
without additional backend load, and

226
00:14:52,590 --> 00:14:55,080
every user's data remains private.

227
00:14:55,330 --> 00:14:57,160
The, this concludes the presentation.

228
00:14:57,380 --> 00:14:59,420
Thank you for your time and attention.

229
00:14:59,670 --> 00:15:04,350
Clients at NLP represents a big
shift from centralized data analysis

230
00:15:04,650 --> 00:15:10,020
to edge intelligence where users
browsers become mini analytics engines.

231
00:15:10,320 --> 00:15:11,070
It's faster.

232
00:15:11,370 --> 00:15:13,500
Safer and far more personal.

233
00:15:13,980 --> 00:15:20,470
As developers, we now ha have
the tools to combine NLP web APIs

234
00:15:20,500 --> 00:15:25,210
and frontend frameworks to create
experiences that truly understand users.

235
00:15:25,210 --> 00:15:28,300
In context, I would like to
discuss how you might apply

236
00:15:28,300 --> 00:15:32,560
these ideas in your own products,
whether it's improving engagement.

237
00:15:32,815 --> 00:15:35,335
Personalization or user trust.

238
00:15:35,585 --> 00:15:40,415
I would like to thank thank the
conference for giving me this opportunity

239
00:15:40,595 --> 00:15:42,215
and I hope you all have a good day.

240
00:15:42,395 --> 00:15:42,875
Thank you.

