1
00:00:00,110 --> 00:00:00,740
Hello, everyone.

2
00:00:00,840 --> 00:00:05,050
My name is Mithun Panda and I specialize
in technology, data and AI helping

3
00:00:05,560 --> 00:00:09,220
Fortune 500 companies solve complex
business and technical challenges.

4
00:00:09,990 --> 00:00:13,320
Today I'll be talking about how to
build scalable AI and data solutions

5
00:00:13,330 --> 00:00:14,959
using cloud native architecture.

6
00:00:15,459 --> 00:00:19,990
Now, imagine this, AI systems on that
scale effortlessly, large data pipelines

7
00:00:20,020 --> 00:00:24,610
that adapt in real time and businesses
that continuously innovate faster

8
00:00:25,070 --> 00:00:30,829
than ever and without worrying, about
infrastructure management over it.

9
00:00:31,189 --> 00:00:33,799
And that's the really true power
of cloud native technology.

10
00:00:34,049 --> 00:00:38,709
So let's dive in and explore how
it is transforming the way we AI.

11
00:00:39,209 --> 00:00:43,279
Now, when you look at the trends, what
is happening in the industry, we are

12
00:00:43,299 --> 00:00:45,289
always obsessed about value creations.

13
00:00:45,299 --> 00:00:49,654
So when we think of digital or technology
or AI transformations, and we always

14
00:00:49,674 --> 00:00:50,974
focus on business value creations.

15
00:00:51,474 --> 00:00:54,904
Now, cloud adoption has definitely
helped organizations accelerate

16
00:00:54,914 --> 00:00:56,154
on the business value side of it.

17
00:00:56,504 --> 00:00:59,034
However, to make this happen,
we must get a few things right?

18
00:00:59,714 --> 00:01:01,384
The foundation has to be correct.

19
00:01:01,424 --> 00:01:05,134
for example, there are certain
six key elements that I have

20
00:01:05,134 --> 00:01:08,284
mentioned here, starting from
building foundation for analytics.

21
00:01:08,514 --> 00:01:13,824
you foundational platform, which
will enable your use cases, which

22
00:01:13,824 --> 00:01:15,364
will help you accelerate use cases.

23
00:01:16,024 --> 00:01:18,864
The second element is the cloud native
architecture, which is very important

24
00:01:18,864 --> 00:01:20,004
and the cloud native architectures.

25
00:01:20,004 --> 00:01:23,844
And we are moving away from on premise
to cloud native, which will help us and

26
00:01:23,854 --> 00:01:28,054
achieve the scaling up or down based on
the demand, achieving the fault tolerance

27
00:01:28,054 --> 00:01:30,704
side of it with high availability
and self healing mechanism, right?

28
00:01:31,164 --> 00:01:32,754
The third is a rapid value creations.

29
00:01:32,804 --> 00:01:36,744
Now to achieve the rapid
value creations, we have to.

30
00:01:37,179 --> 00:01:40,419
understand the cost side of it as well,
and this is financial operation side of

31
00:01:40,539 --> 00:01:45,089
ops side of it, or the cost s how we can
make sure that, and we can process the

32
00:01:45,089 --> 00:01:50,549
large data and AI workloads, while, making
sure that we are cost efficient, right?

33
00:01:50,939 --> 00:01:52,679
And then scaling of the
resources seamlessly.

34
00:01:52,679 --> 00:01:55,974
that is also very important because,
we, we'll be, deploying a lot of AI

35
00:01:55,974 --> 00:01:59,964
use cases, we'll be building a lot
of models and AI models and this

36
00:01:59,964 --> 00:02:02,574
is really necessary that no, we can
scale up the resources seamlessly.

37
00:02:03,074 --> 00:02:05,914
As because we are data driven, we
are in a data driven journey and

38
00:02:05,914 --> 00:02:09,534
making sure that we have the data is
governed properly, it is accessible,

39
00:02:09,564 --> 00:02:12,134
it's reliable, and it's available.

40
00:02:12,154 --> 00:02:12,994
That is really important.

41
00:02:13,004 --> 00:02:15,404
These are the three key
data enablers we must have.

42
00:02:15,454 --> 00:02:16,644
We must have to get this right.

43
00:02:17,024 --> 00:02:19,234
And then finally, future proof
digital advantage, right?

44
00:02:19,234 --> 00:02:23,644
We are moving away from on prem to
cloud and now from cloud to multi cloud.

45
00:02:24,014 --> 00:02:26,194
Again, I'm not saying that
on premise will die here.

46
00:02:26,754 --> 00:02:31,099
But definitely we will Live in a
world, where we'll have, architecture

47
00:02:31,179 --> 00:02:34,489
and aware, which will support very
diverse kind of infrastructure.

48
00:02:34,489 --> 00:02:37,709
So we have multi cloud, you have SAS
based applications when you have on

49
00:02:37,749 --> 00:02:42,979
prem applications going forward to
understand bit on the cloud native

50
00:02:42,979 --> 00:02:47,749
side of it is very foundational on the
what cloud native means, which is to

51
00:02:47,769 --> 00:02:50,839
design, which is to build and which
is to run applications specifically

52
00:02:50,839 --> 00:02:52,739
optimized for cloud applications.

53
00:02:52,964 --> 00:02:54,454
The cloud environments, right?

54
00:02:54,454 --> 00:02:58,974
So instead of traditional on premise
infrastructures, and we, in cloud native

55
00:02:58,974 --> 00:03:02,714
applications is leverage the true power
off the cloud capabilities such as

56
00:03:02,714 --> 00:03:06,994
scalability, resilience, automations and
flexibility, so on and so forth, right?

57
00:03:07,024 --> 00:03:10,854
So when you look at the scalability,
what exactly scalability means is this

58
00:03:10,854 --> 00:03:16,024
is where the cloud has the ability to
adjust resources based on demand, right?

59
00:03:16,024 --> 00:03:19,064
for example, Netflix, it
scales is infrastructure.

60
00:03:19,484 --> 00:03:22,574
During peak hours, and scales
down during peak hours.

61
00:03:22,575 --> 00:03:26,121
it's very intelligent and this is how
they scale, their recommendation systems

62
00:03:26,121 --> 00:03:27,881
and their streaming system as well.

63
00:03:28,081 --> 00:03:32,221
When you look at the resilience,
what is very simple, right?

64
00:03:32,271 --> 00:03:35,181
If something happens, some failure
happens and make sure that your system

65
00:03:35,181 --> 00:03:38,811
is so much resilient that It can
quickly recover from failures and the

66
00:03:38,811 --> 00:03:42,441
automations really, we cannot separate
automations without DevOps or CI ICD

67
00:03:42,941 --> 00:03:45,731
and part of it is the infrastructure
management, which is infrastructure as a

68
00:03:45,731 --> 00:03:49,211
code, such as, infrastructure, which is
very important to automate deployments.

69
00:03:49,591 --> 00:03:52,611
and finally the flexibility and
flexibility is really super, super

70
00:03:52,611 --> 00:03:54,711
important here because we are.

71
00:03:55,211 --> 00:03:59,301
need to we need to provide to make sure
that okay, this is interoperability across

72
00:03:59,301 --> 00:04:02,271
various cloud providers as because we
are going to the multi cloud journey.

73
00:04:02,631 --> 00:04:06,841
now all if I sum it up on the cloud
native architecture side, it leverages

74
00:04:06,871 --> 00:04:12,491
the microservices, the container,
serverless computing, the DevOps CI

75
00:04:12,491 --> 00:04:15,791
CD and orchestration tools to make
sure that it helps us achieve the

76
00:04:15,791 --> 00:04:19,991
lower cost or achieve cost efficiency,
innovate faster and optimized best.

77
00:04:20,001 --> 00:04:20,061
Thank you.

78
00:04:20,486 --> 00:04:20,956
AI performance.

79
00:04:20,966 --> 00:04:25,186
Now to just to deep, deep dive
into just more and just to take a

80
00:04:25,226 --> 00:04:27,956
thousand foot view on each of these
elements, which I just talked about

81
00:04:28,246 --> 00:04:29,696
what exactly mean microservices.

82
00:04:29,746 --> 00:04:33,676
Microservices are nothing but
applications which are broken down

83
00:04:33,686 --> 00:04:38,536
into smaller, independent and modular
services that communicate by APIs.

84
00:04:38,596 --> 00:04:39,486
This is very straightforward.

85
00:04:39,486 --> 00:04:40,846
Microservices.

86
00:04:41,126 --> 00:04:42,196
Then we have the containers.

87
00:04:42,196 --> 00:04:42,976
What are containers?

88
00:04:43,746 --> 00:04:47,956
Is nothing but a lightweight
or which is portable, right?

89
00:04:48,186 --> 00:04:51,716
Environments for running applications
seamlessly across different cloud

90
00:04:51,956 --> 00:04:54,966
setups and then container orchestration
such as Kubernetes, which is very

91
00:04:54,966 --> 00:04:56,626
popular to manage, scale and deploy.

92
00:04:57,226 --> 00:05:00,496
and containerized applications or
automate containerized applications,

93
00:05:00,856 --> 00:05:02,856
server serverless computing.

94
00:05:03,636 --> 00:05:07,146
And this is where we do not need
to manage the cloud infrastructure,

95
00:05:07,336 --> 00:05:10,006
such as AWS Lamb Lambda or NGCP.

96
00:05:10,006 --> 00:05:11,746
You'll have Google Cloud
functions in Azure.

97
00:05:11,746 --> 00:05:12,736
You have Azure functions.

98
00:05:12,976 --> 00:05:14,611
These are the kind of example
of serverless computing.

99
00:05:14,611 --> 00:05:15,406
And this is really.

100
00:05:15,756 --> 00:05:19,376
it helps us without really looking
or scratching our head on managing

101
00:05:19,376 --> 00:05:20,706
the cloud infrastructure side of it.

102
00:05:20,926 --> 00:05:23,796
And then DevOps and CICD, I already
mentioned the CICD side of it,

103
00:05:23,886 --> 00:05:26,746
which is DevOps side of it for
the continuous integrations and

104
00:05:26,746 --> 00:05:30,546
continuous deployment pipelines for
faster deployment and innovations.

105
00:05:30,906 --> 00:05:34,776
Now look, now we are building a lot
of generative AI models as well.

106
00:05:34,786 --> 00:05:39,256
So, CI, CD has now been
extended to CI, CD, CE.

107
00:05:39,266 --> 00:05:44,336
So what that means is we are also
continuously evaluating the models, right?

108
00:05:44,386 --> 00:05:46,096
so that's why I know
this is CI, CD and CE.

109
00:05:46,126 --> 00:05:49,276
So just continuous integrations,
continuous, development and

110
00:05:49,286 --> 00:05:50,856
continuous, evaluations.

111
00:05:51,356 --> 00:05:51,986
Moving on.

112
00:05:52,366 --> 00:05:55,066
again, this is a deep dive on each
of these components that I mentioned,

113
00:05:55,426 --> 00:05:59,506
looking at how we, how the microservice
and the container principles help us

114
00:05:59,506 --> 00:06:01,066
enable the scalability and resilience.

115
00:06:01,076 --> 00:06:04,216
one is the quick, three quick things
that I would like to point out here.

116
00:06:04,426 --> 00:06:08,456
Han look at the data and AI solutions and
to scaling operate is, independent scaling

117
00:06:08,576 --> 00:06:12,596
look, so scale scaling of AI inference
separately from data ingestions, right?

118
00:06:12,596 --> 00:06:15,446
When you build AI models,
basically you have.

119
00:06:15,996 --> 00:06:19,636
to separate the front end side of it, the
back end side of it, the data ingestion

120
00:06:19,636 --> 00:06:24,126
side of it, the feature engineering side
of it, your AI model development side of

121
00:06:24,176 --> 00:06:26,886
it, which is again feature engineering
and the model and the evaluation side of

122
00:06:26,886 --> 00:06:28,646
it, yeah, inference piece of it, right?

123
00:06:28,876 --> 00:06:31,086
and this is where the
containerization is going to help you.

124
00:06:31,966 --> 00:06:35,026
And fault isolation, so if one
service goes down, then making

125
00:06:35,026 --> 00:06:36,186
sure that another is operational.

126
00:06:36,226 --> 00:06:38,336
So it helps us build the
resilience side of it.

127
00:06:38,776 --> 00:06:41,866
And then the faster deployments,
making sure that, you are creating a

128
00:06:41,866 --> 00:06:43,656
modular, your architecture is modular.

129
00:06:44,066 --> 00:06:44,406
and.

130
00:06:44,786 --> 00:06:48,066
And which is really, which is going to
help you in deployment, deploy faster.

131
00:06:48,666 --> 00:06:50,736
Now on the right hand side, I've
just given one example here.

132
00:06:50,736 --> 00:06:53,316
There's tons of examples in
the industry that we have seen.

133
00:06:53,326 --> 00:06:56,826
In fact, in, in your organizations,
you might already be doing that, right?

134
00:06:57,186 --> 00:06:59,506
Leveraging microservices and
Kubernetes based architecture.

135
00:06:59,526 --> 00:07:04,386
How it really help us scale, in, in
building our AI applications or the data.

136
00:07:04,601 --> 00:07:06,831
Driven applications or
data centric applications.

137
00:07:07,101 --> 00:07:11,341
So for example, Spotify uses microservices
in the Kubernetes to scale its AI

138
00:07:11,341 --> 00:07:12,821
powered music recommendation engine.

139
00:07:13,141 --> 00:07:15,561
Netflix is another great example, right?

140
00:07:15,591 --> 00:07:19,711
You know where the deployment
happens, every 10 seconds,

141
00:07:19,711 --> 00:07:21,021
I would say or 11 seconds.

142
00:07:21,521 --> 00:07:25,811
And they leverage Microsoft Kubernetes
modular and, Amazon is another one,

143
00:07:25,841 --> 00:07:27,181
which is the early adopter, right?

144
00:07:27,551 --> 00:07:30,961
I really don't need to talk more about
containerizations, but looking into

145
00:07:31,491 --> 00:07:33,381
our AI specific one basically help.

146
00:07:33,411 --> 00:07:37,561
It helps us the consistency, the
managing consistency across environments.

147
00:07:37,651 --> 00:07:39,661
and then when you look at the
Kubernetes, which is the container

148
00:07:39,661 --> 00:07:44,071
orchestration platform, which has helps
us scaling, and the orchestration and

149
00:07:44,071 --> 00:07:46,831
the selfing of AI workloads, moving on.

150
00:07:47,131 --> 00:07:48,261
The serverless computing.

151
00:07:48,261 --> 00:07:52,451
Now serverless computing is another
really important piece when we

152
00:07:52,731 --> 00:07:54,431
manage our data and AI workloads.

153
00:07:54,791 --> 00:07:58,441
It helps us execute a code without
managing infrastructure, right?

154
00:07:58,771 --> 00:08:03,301
Now in, in generative AI solutions and
when we build generative AI model or

155
00:08:03,311 --> 00:08:07,871
large language model, applications,
GPU as a service or inferences are

156
00:08:07,921 --> 00:08:09,221
two really critical things here.

157
00:08:09,451 --> 00:08:14,226
And this is where You know, making sure
that our infrastructure has the capacity,

158
00:08:14,856 --> 00:08:19,976
our infrastructure is efficient enough to
make sure that we can build a model, we

159
00:08:19,976 --> 00:08:23,726
can deploy the model and we can achieve
the low latency through the inference.

160
00:08:23,776 --> 00:08:27,526
And this is where the serverless
GPUs are very important now.

161
00:08:27,546 --> 00:08:31,866
Inference as a service, you can leverage,
definitely leverage GPU, but also you

162
00:08:31,866 --> 00:08:36,376
have LPUs such as Grok, which provides
a faster inference as a service.

163
00:08:36,876 --> 00:08:37,176
now.

164
00:08:37,951 --> 00:08:41,091
What are the benefits that we achieve
out of, leveraging serverless computing?

165
00:08:41,111 --> 00:08:42,141
One is the auto scaling, right?

166
00:08:42,201 --> 00:08:45,081
it allocates dynamically, the
resource based on the usage, and

167
00:08:45,421 --> 00:08:48,091
then cost efficiency is again,
pay only for execution time

168
00:08:48,091 --> 00:08:50,081
and a faster deployment really.

169
00:08:50,081 --> 00:08:53,461
And you are not, it is eliminating the
overhead of managing the infrastructure.

170
00:08:53,621 --> 00:08:56,561
So this is, these are the kind of three
key benefits that there are so many,

171
00:08:56,591 --> 00:09:01,151
but for a developer and for us, senior
executive or the decision makers, these

172
00:09:01,151 --> 00:09:02,551
are the tangible benefits that we.

173
00:09:02,821 --> 00:09:05,671
immediately seen once we start
leveraging serverless computing.

174
00:09:06,471 --> 00:09:07,701
what are the common use cases?

175
00:09:07,701 --> 00:09:10,931
And definitely there are tons of
use cases, but, in, in, in current

176
00:09:10,941 --> 00:09:15,611
scenario and AI powered chatbots can
be, is one of the use cases, AI model

177
00:09:15,631 --> 00:09:19,531
inference, real time data processing
using serverless ETL pipelines, extract

178
00:09:19,631 --> 00:09:22,611
transfer and load or extract load
and transfer, whatever you call it.

179
00:09:22,611 --> 00:09:26,461
these are the kind of use cases, which
help us, managing the data and AI

180
00:09:26,521 --> 00:09:28,081
workflows with on demand resources.

181
00:09:28,896 --> 00:09:32,576
So moving on now, when we look at,
the managing data and AI solutions

182
00:09:32,886 --> 00:09:38,046
are scalable, making sure that it's
scalable, we cannot ignore the storage

183
00:09:38,046 --> 00:09:41,476
side of it and the data management side
of it, as well as MLOps, which is the

184
00:09:41,476 --> 00:09:43,416
extension of DevOps side of things.

185
00:09:44,231 --> 00:09:46,621
Now, when you look at the storage
setup, we definitely data lake is

186
00:09:46,621 --> 00:09:51,511
there, which we leverage in the S3 or
in AWS world or Azure Data Lake Storage

187
00:09:51,511 --> 00:09:56,141
Gen2 or Delta Lake, data warehouse,
such as we have BigQuery, Snowflake,

188
00:09:56,721 --> 00:09:58,181
Synapse, there are a lot of things.

189
00:09:58,181 --> 00:10:01,271
And in generative, we have, we cannot
separate vector database such as VVH.

190
00:10:02,011 --> 00:10:05,151
in Pinecone, there are so many which
helps us store the embeddings for the

191
00:10:05,151 --> 00:10:06,421
generative AI applications, right?

192
00:10:06,791 --> 00:10:09,451
And the data processing tools, again,
there are so many data processing tools

193
00:10:09,451 --> 00:10:12,221
and Spark has been very popular in Google.

194
00:10:12,621 --> 00:10:16,841
GCP, Dataflow, Databricks is one
of the, one of the best in the

195
00:10:16,841 --> 00:10:18,541
market in terms of the adoption.

196
00:10:18,981 --> 00:10:20,971
now in terms of scalability
strategy, how should.

197
00:10:21,141 --> 00:10:22,551
What should be our scalability strategy?

198
00:10:22,551 --> 00:10:28,351
Definitely, we can use the databases, for
the realtime IU workloads or tier storage,

199
00:10:28,401 --> 00:10:30,171
and the lifecycle policies must be there.

200
00:10:30,321 --> 00:10:33,651
And again, it should be surrounded with
the data governance principles and the

201
00:10:33,651 --> 00:10:35,181
best data management best practices.

202
00:10:35,511 --> 00:10:38,651
Now, on the right hand side, if
you look at an ML ops, to automate

203
00:10:38,651 --> 00:10:41,201
and scale AI workflows, really
if you wanna innovate faster.

204
00:10:41,776 --> 00:10:46,416
If you have to achieve the faster time
to market, MLOps is very important.

205
00:10:46,516 --> 00:10:50,706
and this integrates basically the
DevOps best practices, into AI model

206
00:10:50,706 --> 00:10:52,286
development, deployment, and monitoring.

207
00:10:53,146 --> 00:10:54,186
And now what are some key components?

208
00:10:54,196 --> 00:10:57,116
So definitely when you look at the
machine learning pipeline, it starts

209
00:10:57,126 --> 00:10:59,756
from, building your feature engineering.

210
00:11:00,126 --> 00:11:04,036
storing into the feature stores,
building the model, creating the model

211
00:11:04,106 --> 00:11:08,336
versioning, and then deploying and then
monitoring and the bias detections, right?

212
00:11:08,396 --> 00:11:12,456
to make sure that now it is prevented
from, it prevents drift, right?

213
00:11:12,766 --> 00:11:15,486
One of the examples could be, okay,
if you look at the investment banking

214
00:11:15,486 --> 00:11:18,386
side of the retail banking, A lot of
experience on the banking side of it.

215
00:11:18,866 --> 00:11:21,266
It is just MLOps to continuously
update fraud detections

216
00:11:21,286 --> 00:11:22,596
model, is one of the example.

217
00:11:22,596 --> 00:11:24,086
Risk management is another example.

218
00:11:24,346 --> 00:11:28,156
Hyper personalization is another
example, and this is where MLOps has

219
00:11:28,156 --> 00:11:32,896
been really beneficial, managing large
scale AI workloads, and achieving

220
00:11:32,896 --> 00:11:34,096
the automations and the scale.

221
00:11:34,596 --> 00:11:36,626
Moving on, how should we think
of the security and compliance?

222
00:11:36,626 --> 00:11:38,616
Because we really can separate
security and compliance.

223
00:11:38,626 --> 00:11:41,376
And considering the generative AI
adoption that is happening in the

224
00:11:41,376 --> 00:11:45,216
industry and security and compliance
remain really too critical in our blogs.

225
00:11:45,816 --> 00:11:49,676
and I'll just want to make
it very high level here.

226
00:11:49,726 --> 00:11:52,516
And there are four parts when you
think of the security and compliance.

227
00:11:53,146 --> 00:11:55,196
One is the privacy side
of it, data privacy.

228
00:11:55,336 --> 00:11:56,336
The second is the encryptions.

229
00:11:56,386 --> 00:11:59,316
And the third is GTA, which is
zero trust architecture based,

230
00:11:59,386 --> 00:12:00,216
which is super important.

231
00:12:00,286 --> 00:12:01,726
And then API vulnerabilities.

232
00:12:02,446 --> 00:12:05,906
In terms of the data privacy, we
have to follow certain techniques,

233
00:12:05,946 --> 00:12:09,566
such as an implementation,
implementing, differential privacy.

234
00:12:10,536 --> 00:12:14,746
to prevent a data leak, such a data
leak is, and then data masking, the

235
00:12:14,766 --> 00:12:18,496
anonymizations or the pseudonymizations,
making sure that the PII data is not

236
00:12:18,496 --> 00:12:22,361
exposed, making sure that you are
compliant with GDPR, HIPAA compliance

237
00:12:22,361 --> 00:12:26,916
for the healthcare or the CCPA, which
is California Consumer Protection Act.

238
00:12:26,966 --> 00:12:30,086
There are so many other regulations
and are different based on the

239
00:12:30,086 --> 00:12:32,776
jurisdictions or the country,
countrywide regulations that you are in.

240
00:12:33,476 --> 00:12:36,366
When you look at the encryption, so
definitely, Implementing a hardware

241
00:12:36,366 --> 00:12:40,706
security modules, HSMs for key
management, for example, if you want

242
00:12:40,706 --> 00:12:44,676
to look at in a certain tools or the
services that you want to leverage

243
00:12:44,686 --> 00:12:48,186
on to build your generative AI
solutions, making sure that it is a

244
00:12:48,186 --> 00:12:50,156
hardware security model, HSM compliant.

245
00:12:50,166 --> 00:12:50,976
That is very important.

246
00:12:51,306 --> 00:12:55,286
Otherwise, your security office
or the compliance team, they might

247
00:12:55,286 --> 00:12:59,366
not allow to use the service of
the tools that you'll be using.

248
00:12:59,746 --> 00:13:00,736
Zero trust architecture.

249
00:13:00,766 --> 00:13:03,886
Really, this is very simple
here and making sure that.

250
00:13:04,461 --> 00:13:08,001
you have, you enforce least privilege
access, role based access control is super

251
00:13:08,021 --> 00:13:12,641
important here, continuous authentications
with, behavioral analytics, and some

252
00:13:12,651 --> 00:13:16,461
sort of secure enclave, enclave computing
as well, and not besides this, you

253
00:13:16,461 --> 00:13:21,441
also have the MFA, and that is also
very important, OAuth2, for the API

254
00:13:21,441 --> 00:13:24,291
vulnerabilities, so anyway, how to secure
the AI models, Endpoint security, which

255
00:13:24,291 --> 00:13:32,081
is using an O2 or the you, JSON web
token, J JT JW T or mutual TLS or SSL 2.0.

256
00:13:32,581 --> 00:13:33,541
We have application firewall.

257
00:13:33,601 --> 00:13:37,051
Again, this is very important, to,
make sure that it helps us secure API

258
00:13:37,051 --> 00:13:41,291
gateways, to protect AI services, the
endpoints and it's really important

259
00:13:41,291 --> 00:13:44,821
to regularly, Conduct penetration
testing, and your architecture

260
00:13:44,821 --> 00:13:46,301
must be privacy and security fast.

261
00:13:46,341 --> 00:13:49,511
This is really very important,
when we, are in the journey of

262
00:13:49,511 --> 00:13:52,571
building AI solutions in a cloud
native, architecture presence.

263
00:13:52,851 --> 00:13:55,221
Now, how can we optimize
the performance, right?

264
00:13:55,381 --> 00:13:57,091
Making sure that now we are
getting the right speed.

265
00:13:57,091 --> 00:13:58,191
We are getting the right efficiency.

266
00:13:58,191 --> 00:14:00,501
We are getting the right latency
in terms of the inference.

267
00:14:01,251 --> 00:14:02,601
So I've just mentioned four things here.

268
00:14:02,631 --> 00:14:03,671
One is the infrastructure.

269
00:14:03,701 --> 00:14:07,201
You must ensure that your
infrastructure is sized properly, right?

270
00:14:07,511 --> 00:14:11,621
So so that means and how to design an auto
scale AI workloads based on demand, right?

271
00:14:12,111 --> 00:14:15,181
and then next is the model optimization
So what are some techniques that we

272
00:14:15,181 --> 00:14:18,121
can think to improve model training
and the inference speed, right?

273
00:14:18,151 --> 00:14:21,111
And maybe you know quantizations
is one of the approaches to reduce

274
00:14:21,121 --> 00:14:24,451
model size of the memory and while
maintaining accuracy, right the data

275
00:14:24,451 --> 00:14:27,971
locality can the data be stored and
processed in the same cloud region?

276
00:14:28,411 --> 00:14:32,726
And then What are some frameworks that we
can really adopt for the AI accelerations?

277
00:14:32,776 --> 00:14:36,676
not only in terms of the development, but
also in terms of the inference set up it.

278
00:14:36,676 --> 00:14:41,279
So for example, NVIDIA tracked on hogging
face optimum for LLM performance tuning or

279
00:14:41,279 --> 00:14:46,059
Grok, which uses LPU language processing
unit for really faster inference.

280
00:14:46,559 --> 00:14:48,269
Now I have developed one
architecture, which is very

281
00:14:48,269 --> 00:14:49,979
naive, rag based architecture.

282
00:14:49,979 --> 00:14:53,969
Leveraging how we can implement rag
based architecture, leveraging cloud

283
00:14:53,969 --> 00:14:58,894
native capabilities, just to spend, 10
seconds of the quick, 20, 20, 30 seconds

284
00:14:58,894 --> 00:15:00,424
on the rag, what exactly this means.

285
00:15:00,474 --> 00:15:03,444
Look, you have lot of data on
your, in your organizations, and if

286
00:15:03,444 --> 00:15:08,694
you take any, existing LLM or pre
model, it's not in your data, right?

287
00:15:08,984 --> 00:15:11,234
it is trained up to a
certain point in time.

288
00:15:11,734 --> 00:15:16,824
Now, how to ensure that we achieve
accuracy, and how we can make

289
00:15:16,824 --> 00:15:19,694
sure that, okay, this is, we are
getting some expected results,

290
00:15:19,724 --> 00:15:20,914
based on the correct information.

291
00:15:20,914 --> 00:15:24,634
So the real time information, and this is
where the RAG is very much popular because

292
00:15:24,654 --> 00:15:26,454
we really cannot fine tune all the time.

293
00:15:26,954 --> 00:15:29,374
So you have the unstructured
reference data, as you see a lot

294
00:15:29,374 --> 00:15:31,074
of PDF files, word or text files.

295
00:15:31,704 --> 00:15:35,614
You chunk it, and then you convert it
into embedding, leveraging embedding APIs.

296
00:15:35,644 --> 00:15:40,463
You have tons of embedding APIs, OpenAI
embeddings or Hoggingfix embedding.

297
00:15:40,464 --> 00:15:41,424
So there are a lot of embeddings.

298
00:15:41,424 --> 00:15:43,864
And you can just take a look,
the leaderboard, and just

299
00:15:43,924 --> 00:15:45,064
pick one and just move on.

300
00:15:45,444 --> 00:15:48,794
And then you load into vector database
such as Pinecone or Weaviate or you

301
00:15:48,794 --> 00:15:50,234
can also leverage Redis in Azure.

302
00:15:51,064 --> 00:15:52,724
there's this another tool as well.

303
00:15:52,764 --> 00:15:55,594
I forgot, but there are a lot of
vector database, so you really

304
00:15:55,594 --> 00:15:56,494
don't need to worry on that.

305
00:15:57,474 --> 00:15:58,454
So this is the step one.

306
00:15:58,724 --> 00:16:01,234
The step two is the user
interface side of it.

307
00:16:01,234 --> 00:16:04,374
And this is where multiple users
will be sending their queries.

308
00:16:04,454 --> 00:16:06,874
And make sure that you have
the load balancer, such as API

309
00:16:06,904 --> 00:16:08,564
gateways or Nginx load balancer.

310
00:16:08,904 --> 00:16:13,854
It comes to the APIs, basically in a
search into the vector database, right?

311
00:16:14,354 --> 00:16:16,774
The embedding API must be
the same as the embedding API

312
00:16:16,824 --> 00:16:17,934
which was used earlier, right?

313
00:16:18,864 --> 00:16:20,864
And then it will retrieve the top key.

314
00:16:21,364 --> 00:16:24,824
So in the top three or top five
based on your configurations, and

315
00:16:24,824 --> 00:16:27,494
then the query and the top and the
retrieve documents and are passed

316
00:16:27,514 --> 00:16:29,294
to LLM to get you get the answer.

317
00:16:29,464 --> 00:16:33,484
Right now, how do how does it
translate to the cloud native capital?

318
00:16:33,494 --> 00:16:34,764
Again, this is the right nag.

319
00:16:35,034 --> 00:16:36,344
It is not advanced rag.

320
00:16:36,844 --> 00:16:39,874
But when you look at the load balancer
side of it, which is to ensure that

321
00:16:39,884 --> 00:16:41,344
even distributions of request, right?

322
00:16:41,344 --> 00:16:42,084
It's very important.

323
00:16:42,644 --> 00:16:45,554
And we can also use a load balancer
on the LLM side of it as well,

324
00:16:45,694 --> 00:16:48,064
which I have not shown in this
diagram in this architecture.

325
00:16:48,064 --> 00:16:48,374
But.

326
00:16:48,809 --> 00:16:50,069
A load balancer is really critical.

327
00:16:50,069 --> 00:16:54,399
Number one, each of the front end
side of it, the back end, the vector

328
00:16:54,399 --> 00:17:00,159
database, the prompt template, the LLM
services are docker containerized, and

329
00:17:00,159 --> 00:17:03,659
this is really important because we
don't want to create a monolith systems

330
00:17:03,659 --> 00:17:06,059
and each component is containerized.

331
00:17:06,734 --> 00:17:11,174
Now we can also leverage container
orchestrations like Kubernetes to manage

332
00:17:11,224 --> 00:17:15,714
each of these containers that I just
mentioned with each HPA, horizontal pods,

333
00:17:15,714 --> 00:17:19,814
autoscaler capabilities, capability to
ensure services scale up and down, based

334
00:17:19,814 --> 00:17:20,954
on demand, which is really important.

335
00:17:21,484 --> 00:17:24,194
Now making sure that in this architecture,
the security and the privacy, as I

336
00:17:24,254 --> 00:17:26,983
talked earlier, are leveraged and
the best practices are leveraged,

337
00:17:26,983 --> 00:17:30,633
such as Istio for service mesh,
OAuth for API authentications, RBSC

338
00:17:30,633 --> 00:17:32,313
data encryption, so on and so forth.

339
00:17:32,813 --> 00:17:36,264
Now, when you look at the vector database,
it runs as a stateful, Kubernetes

340
00:17:36,264 --> 00:17:37,494
service and with persistent storage.

341
00:17:37,784 --> 00:17:41,834
Now, LLM hosting and LLM inferences
are very important, which is based

342
00:17:41,834 --> 00:17:47,174
on the GPU nodes or optimized
inference service such as Azure ML

343
00:17:47,174 --> 00:17:48,214
or AWS SageMaker, a lot of things.

344
00:17:48,684 --> 00:17:55,544
And then Grafana API latency and ELK
Stark also, you can leverage that.

345
00:17:56,044 --> 00:17:58,044
Now, Where are we moving?

346
00:17:58,044 --> 00:17:59,014
What are some future trends?

347
00:17:59,014 --> 00:18:00,884
and these are the kind of
four things that I mentioned.

348
00:18:00,904 --> 00:18:05,514
One is we are moving towards in a kind
of a, a power cloud automations through

349
00:18:05,644 --> 00:18:07,214
self optimizing cloud architecture.

350
00:18:07,264 --> 00:18:08,604
This is really happening now.

351
00:18:09,144 --> 00:18:11,344
The second is, you know how
we can make sure that no.

352
00:18:12,334 --> 00:18:14,504
We can process AI closer
to the data source, right?

353
00:18:14,544 --> 00:18:15,484
This is really important.

354
00:18:15,794 --> 00:18:20,364
LLM Ops is another really important
piece, as we are trying to optimize

355
00:18:20,404 --> 00:18:24,554
our generative AI workloads,
LLM based workloads and finalize

356
00:18:24,554 --> 00:18:27,504
the quantum computing in AI and
emerging potential of quantum.

357
00:18:27,954 --> 00:18:33,564
Enhanced AI workloads and that is
also another future trend that I see

358
00:18:33,714 --> 00:18:37,814
look, this is a very small quick 15
minutes presentations, but definitely

359
00:18:37,844 --> 00:18:41,304
you know what I would like to see
and what we have seen working with

360
00:18:41,304 --> 00:18:44,744
so many Fortune 500 organizations
in the adoption is really happening.

361
00:18:45,244 --> 00:18:49,764
The leaders in which have already
established certain foundation on

362
00:18:49,764 --> 00:18:52,694
their infrastructures are really
accelerating their business value.

363
00:18:53,414 --> 00:18:57,604
but also the firms, the organizations,
which have started to realize that,

364
00:18:57,604 --> 00:19:01,204
okay, we have to leverage the cloud
native applications to build a

365
00:19:01,204 --> 00:19:03,104
scalable data and AI applications.

366
00:19:03,554 --> 00:19:07,544
they are also definitely investing,
into their capabilities to make sure

367
00:19:07,544 --> 00:19:10,244
that, they will stay ahead of the
curve and they will remain, into

368
00:19:10,244 --> 00:19:11,744
their competitive advantage positions.

369
00:19:12,244 --> 00:19:12,694
That's it.

370
00:19:13,124 --> 00:19:14,064
Thank you very much.

