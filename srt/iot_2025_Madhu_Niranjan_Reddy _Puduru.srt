1
00:00:00,500 --> 00:00:01,160
Hello everyone.

2
00:00:01,460 --> 00:00:01,910
Hey warm.

3
00:00:01,910 --> 00:00:05,240
Welcome to all of you and thank
you for joining the session today.

4
00:00:05,740 --> 00:00:08,670
My name is from Synchronous
Technologies USA.

5
00:00:09,149 --> 00:00:12,959
Today I will be speaking on a topic
that is becoming more crucial in the

6
00:00:12,959 --> 00:00:19,525
modern IO IOT ecosystems, a GA for Smart
Camera I, IOT devices, real time image

7
00:00:19,525 --> 00:00:21,125
processing without cloud dependencies.

8
00:00:21,625 --> 00:00:24,535
Now before we dive in,
let's frame the big picture.

9
00:00:24,655 --> 00:00:29,335
For years, IOD cameras have been heavily
dependent on the cloud, but the current

10
00:00:29,335 --> 00:00:33,565
real world requires something faster,
more private, and more efficient.

11
00:00:34,345 --> 00:00:37,075
And that's what exactly HGA steps in.

12
00:00:37,575 --> 00:00:39,195
Cloud Central bottleneck.

13
00:00:39,915 --> 00:00:44,385
Before we understand why a GA is
so important, we first need to

14
00:00:44,385 --> 00:00:47,775
understand the limitations of the
traditional cloud-based approach.

15
00:00:48,315 --> 00:00:51,315
The side explains the major bottlenecks.

16
00:00:51,560 --> 00:00:55,610
That occur when smart cameras
dependent fully on cloud processing

17
00:00:56,300 --> 00:00:58,190
bandwidth becomes a major problem.

18
00:00:58,489 --> 00:01:02,570
Every camera that streams video to
the cloud consumes network bandwidth.

19
00:01:03,110 --> 00:01:07,970
As soon as you add more cameras, the
bandwidth requirement rises rapidly.

20
00:01:08,300 --> 00:01:09,320
In real world places like.

21
00:01:10,054 --> 00:01:14,610
Construction sites rural areas,
and densely urban neighborhoods.

22
00:01:14,670 --> 00:01:21,119
Network strength is unusually limited,
so when multiple cameras try to

23
00:01:21,119 --> 00:01:26,009
stream raw video at the same time,
the systems become slow and unstable.

24
00:01:26,460 --> 00:01:31,500
We can simply say the network cannot
handle so much continuous video traffic.

25
00:01:32,335 --> 00:01:34,765
Latency is too high for realtime use.

26
00:01:35,305 --> 00:01:38,725
When everything goes to the cloud,
the device must wait for the video

27
00:01:38,725 --> 00:01:43,105
to upload, for the cloud to process
it, and then send the results back.

28
00:01:43,705 --> 00:01:45,235
Even in good conditions.

29
00:01:45,235 --> 00:01:50,035
The delay can be 50 to 500
milliseconds for realtime application.

30
00:01:50,535 --> 00:01:54,615
Like security alerts, industry
automation or safety monitoring,

31
00:01:54,975 --> 00:01:56,715
this delay is unacceptable.

32
00:01:57,285 --> 00:02:01,155
The system must react immediately,
not half a second later.

33
00:02:01,815 --> 00:02:05,715
Privacy and compliance issues,
sending a video to the cloud rises.

34
00:02:05,715 --> 00:02:06,735
Privacy concerns.

35
00:02:06,975 --> 00:02:12,165
Users do not want the personal
life footage uploaded continuously.

36
00:02:12,380 --> 00:02:17,480
Regulations like GDPR and CCPA
make this even more challenging.

37
00:02:17,480 --> 00:02:23,329
A GA solves this by keeping data local
and sending only essential information.

38
00:02:23,840 --> 00:02:28,520
High cloud costs, streaming
video nonstop requires cloud,

39
00:02:28,520 --> 00:02:33,620
compute, storage, and bandwidth,
all of which cost money at scale.

40
00:02:33,680 --> 00:02:38,570
With the dozens or hundreds of
cameras, the cost become unsustainable.

41
00:02:39,095 --> 00:02:42,575
So the traditional cloud
only setup does not scale.

42
00:02:43,265 --> 00:02:47,735
It is too slow, too expensive,
and too depend on strong internet.

43
00:02:48,215 --> 00:02:51,335
This is why industry is
shifting towards edge ai.

44
00:02:51,835 --> 00:02:56,675
Now we understand that the
cloud problems, let's look how

45
00:02:56,855 --> 00:02:58,835
RGA can solve these problems.

46
00:02:59,240 --> 00:03:02,600
This slide explains why
running EA directly on devices.

47
00:03:03,260 --> 00:03:08,895
Such a powerful shift processing
happens directly on device with the RGA.

48
00:03:09,050 --> 00:03:12,140
The camera does not send video
to the cloud for analysis.

49
00:03:12,350 --> 00:03:17,900
Instead, the AI model runs
next to the camera sensor.

50
00:03:18,170 --> 00:03:23,030
This makes detection and decision
making almost instant because

51
00:03:23,090 --> 00:03:24,535
this, there is no network delay.

52
00:03:25,265 --> 00:03:26,855
Huge reduction in bandwidth.

53
00:03:27,335 --> 00:03:31,874
Instead of streaming full video, the
device only sends important events such

54
00:03:31,874 --> 00:03:34,634
as person detected or object found.

55
00:03:35,264 --> 00:03:36,369
This reduces the bandwidth.

56
00:03:37,049 --> 00:03:43,589
Uses by 85%, making the system much more
easier to deploy anywhere, including

57
00:03:43,589 --> 00:03:48,149
low connectivity environments, big
improvements in battery life because

58
00:03:48,179 --> 00:03:52,229
the device uploads for less data and
performs fewer network operations.

59
00:03:52,259 --> 00:03:53,759
Battery life will be more.

60
00:03:54,259 --> 00:03:58,489
In many cases, battle life improves by
four to five times, which is critical

61
00:03:58,489 --> 00:04:00,919
for outdoor cameras and IOT devices.

62
00:04:01,399 --> 00:04:06,739
Under 50 50 milliseconds, latency
enables realtime performance.

63
00:04:07,039 --> 00:04:11,319
The AI runs locally, there is
no wait time for cloud response.

64
00:04:11,679 --> 00:04:16,779
This system can process frames in
under milli 50 seconds, enabling

65
00:04:16,779 --> 00:04:21,639
real-time alerts, tracking and
analysis, better privacy and security.

66
00:04:21,699 --> 00:04:25,389
Since the raw video stays on
the device, user maintains full

67
00:04:25,389 --> 00:04:29,949
privacy, only the final result, not
the video is shared to the cloud.

68
00:04:30,519 --> 00:04:32,619
This is a major advantage.

69
00:04:32,854 --> 00:04:38,074
In smart home systems, public
environment, and regulated industries.

70
00:04:38,314 --> 00:04:40,834
So NGA completely changes the game.

71
00:04:41,164 --> 00:04:44,944
It gives us speed, efficiency,
privacy, and reliability

72
00:04:45,244 --> 00:04:46,895
all directly on the device.

73
00:04:47,284 --> 00:04:52,564
This is why modern iot camera systems
moving away from the cloud dependence

74
00:04:52,564 --> 00:04:54,694
and adapting edge first approach.

75
00:04:55,194 --> 00:05:00,474
Evolution of IOT cameras to understand
why a GI is so important today.

76
00:05:00,534 --> 00:05:05,934
Let's quickly look how IOT camera
systems have evolved over the years.

77
00:05:06,204 --> 00:05:11,754
The slide shows three major stages in
the development, early local processing.

78
00:05:12,254 --> 00:05:14,874
In the beginning, cameras
had very limited hardware.

79
00:05:15,539 --> 00:05:19,789
They could only do simple task
like detection, motion, recording

80
00:05:19,789 --> 00:05:22,099
a video, or sensing alerts.

81
00:05:22,819 --> 00:05:26,210
There was no intelligence,
just a basic device behavior.

82
00:05:26,629 --> 00:05:29,900
These early systems were
reliable but extremely limited.

83
00:05:30,650 --> 00:05:35,059
Cloud dependence, intelligence as
cloud computing become more popular.

84
00:05:35,559 --> 00:05:39,039
Companies started shifting all the
heavy processing to the cloud servers.

85
00:05:39,369 --> 00:05:44,469
This allowed cameras to perform more
advanced tasks such as facial recognition,

86
00:05:44,529 --> 00:05:46,899
objective detection, behavioral analysis.

87
00:05:47,289 --> 00:05:49,689
But this come up with several problems.

88
00:05:50,049 --> 00:05:54,089
Bandwidth usage, cost is high,
increased latency, privacy concern.

89
00:05:54,589 --> 00:05:59,604
This was the phase where people realized
that relaying completely on cloud.

90
00:06:00,104 --> 00:06:01,274
Was not scalable.

91
00:06:02,144 --> 00:06:04,304
The edge intel in intelligence era.

92
00:06:05,084 --> 00:06:07,034
Now we are in the third phase.

93
00:06:07,334 --> 00:06:10,784
Modern embedded hardware has
become more powerful enough to

94
00:06:10,784 --> 00:06:13,334
runa models directly on the device.

95
00:06:13,724 --> 00:06:18,194
Today, smart cameras can do real time
detection, classification, anomaly

96
00:06:18,194 --> 00:06:19,004
detection, low light enhancement.

97
00:06:19,504 --> 00:06:21,004
All on the device itself.

98
00:06:21,664 --> 00:06:25,924
This evolution has brought us to the
point where a GA is not optional.

99
00:06:26,434 --> 00:06:31,854
It's becoming the standard approach
for all IOT camera systems, neural

100
00:06:31,854 --> 00:06:34,014
net network optimization techniques.

101
00:06:34,449 --> 00:06:38,904
The slide outlines the main techniques
we use to make a models small.

102
00:06:39,219 --> 00:06:42,399
Fast and efficient enough
to run on edge devices.

103
00:06:42,849 --> 00:06:44,499
Let's go through the each one.

104
00:06:44,829 --> 00:06:45,879
In the simple terms.

105
00:06:46,479 --> 00:06:51,162
Quantization reduce the precision of
the numbers inside the model from 30

106
00:06:51,162 --> 00:06:57,009
32 bit to eight bit, and helps reduce
the power usage with very little impact

107
00:06:57,009 --> 00:06:59,439
on accuracy knowledge legislation.

108
00:06:59,859 --> 00:07:03,429
This is a training technique where
big model teaches the small model.

109
00:07:03,429 --> 00:07:06,309
The small model learns
how to perform almost as.

110
00:07:06,729 --> 00:07:11,859
As well as the big one, but with a
fraction of size and compute requirements.

111
00:07:12,219 --> 00:07:13,329
Architecture selection.

112
00:07:13,629 --> 00:07:19,209
Here we choose model structures that
work well with on limited hardware

113
00:07:19,779 --> 00:07:24,339
models like mobile net, squeeze
net, or design specifically to run

114
00:07:24,339 --> 00:07:26,054
on mobile devices and IOT devices.

115
00:07:26,829 --> 00:07:31,899
Neural architecture search can also
automatically generate efficient designs.

116
00:07:32,544 --> 00:07:35,214
Pruning removes unnecessary
parts of neural network.

117
00:07:35,514 --> 00:07:39,534
It gets rid of connections that don't
contribute much to the performance

118
00:07:39,954 --> 00:07:43,784
by reducing the model size and
speeding up efficient architectures.

119
00:07:44,084 --> 00:07:47,774
Finally, we adapt to lightweight
model families, such as

120
00:07:47,774 --> 00:07:49,064
mobile net and squeeze net.

121
00:07:49,454 --> 00:07:54,494
These models use depth wise, convolution,
and streamlined layers to maintain good

122
00:07:54,494 --> 00:07:57,224
accuracy while staying extremely small.

123
00:07:57,644 --> 00:07:59,144
All these techniques work.

124
00:07:59,504 --> 00:08:04,184
Together to transform heavy cloud-based
model into an edge friendly one

125
00:08:04,684 --> 00:08:06,754
a GA has station.

126
00:08:07,324 --> 00:08:10,654
Now let's talk about hardware
that makes a GA possible.

127
00:08:11,164 --> 00:08:15,994
This slide explains why specialized
process like TPUs and NPUs

128
00:08:15,994 --> 00:08:17,224
are critical for real time.

129
00:08:17,724 --> 00:08:19,494
AI on small devices.

130
00:08:19,944 --> 00:08:21,864
Why CPUs alone are not enough.

131
00:08:22,194 --> 00:08:25,074
Traditional CPUs are
general purpose chips.

132
00:08:25,494 --> 00:08:30,284
They can do a bit of everything, but
they're not optimized for heavy metrics

133
00:08:30,284 --> 00:08:35,384
calculations that AI models require
Running AI solely on CPU U would

134
00:08:35,744 --> 00:08:37,694
be slow and consume a lot of power.

135
00:08:38,194 --> 00:08:43,659
The role of TPUs and NPUs, this is
where TPUs, tensor processing units and

136
00:08:43,659 --> 00:08:45,639
NPUs neural processing units come in.

137
00:08:46,089 --> 00:08:49,419
These chips are designed
specifically for a workloads.

138
00:08:49,899 --> 00:08:53,739
They contain parallel processing
units, fast metrics, multiplication

139
00:08:53,739 --> 00:08:56,229
engines, optimizing memory pathways.

140
00:08:56,529 --> 00:09:00,799
This makes them 10 to a hundred times
more efficient than CPS for a task.

141
00:09:01,299 --> 00:09:03,189
Example of edge acceleration.

142
00:09:03,219 --> 00:09:07,609
Some examples include Google
Coral, TPU, Qualcomm, hexagon,

143
00:09:07,609 --> 00:09:09,739
DSP, apple Neural Engine.

144
00:09:10,249 --> 00:09:14,839
These accelerates allow iot
cameras to run complex models

145
00:09:14,899 --> 00:09:16,489
while staying energy efficient.

146
00:09:17,119 --> 00:09:21,409
Without these hardware accelerators,
realtime AI on battery powered

147
00:09:21,409 --> 00:09:23,364
cameras simply would be not possible.

148
00:09:23,864 --> 00:09:27,164
This slide presents the full
system architecture used to run

149
00:09:27,224 --> 00:09:29,384
AI efficiently on edge devices.

150
00:09:29,864 --> 00:09:33,584
It's organized into three main
layers that work together.

151
00:09:34,469 --> 00:09:38,819
H-A-L-H-A-L hides the
difference differences between

152
00:09:38,819 --> 00:09:40,439
various hardware platforms.

153
00:09:40,709 --> 00:09:45,479
It provides a consistent interface for
memory sensors and compute functions.

154
00:09:45,839 --> 00:09:50,459
The this allows us to run CMEA
code on different devices without

155
00:09:50,459 --> 00:09:53,789
rerating everything neural
network optimization layer.

156
00:09:54,289 --> 00:09:56,869
This is where model gets
transformed into an.

157
00:09:57,469 --> 00:09:58,489
Edge Ready version.

158
00:09:59,119 --> 00:10:03,739
It goes through quantization pruning,
architecture, selection, and compilation.

159
00:10:04,069 --> 00:10:09,529
The output is small, fast and
efficient model, Tyler, efficient

160
00:10:10,459 --> 00:10:14,419
to the devices capabilities,
real-time processing pipeline.

161
00:10:14,989 --> 00:10:19,009
This pipeline is responsible for
handling the live video stream.

162
00:10:19,189 --> 00:10:23,359
It manages frame capture,
pre-processing, AI interference.

163
00:10:24,079 --> 00:10:29,029
And post crossing this goal is to
maintain smooth real-time performances

164
00:10:29,719 --> 00:10:32,389
no matter how busy the device gets.

165
00:10:32,689 --> 00:10:37,549
The layered architecture ensures the
flexibility, efficiency, and reliability

166
00:10:37,789 --> 00:10:39,764
across wide range of hardware platforms.

167
00:10:40,264 --> 00:10:41,704
Hardware platform support.

168
00:10:42,034 --> 00:10:46,024
This slide shows the different
hardware platforms we support from

169
00:10:46,024 --> 00:10:50,999
very simple devices to advanced edge
processors, interrelated level devices.

170
00:10:51,359 --> 00:10:58,349
These are basic systems with 512 MB two
one GB Ram, such as arm cortex processors,

171
00:10:58,829 --> 00:11:01,464
adrenal broads with external accelerators.

172
00:11:02,334 --> 00:11:06,024
They can handle lightweight
models for basic de detection

173
00:11:06,024 --> 00:11:08,004
tasks, mid-range platforms.

174
00:11:08,424 --> 00:11:12,764
These include devices like Raspberry
PI three and Raspberry Pi four.

175
00:11:13,454 --> 00:11:17,384
There are popular for smart home
and consumer camera applications.

176
00:11:17,884 --> 00:11:21,574
They offer good balance between
cost, performance and power

177
00:11:21,574 --> 00:11:24,364
usage, high-end edge processors.

178
00:11:24,874 --> 00:11:28,834
These platforms are designed
specifically for industrial

179
00:11:28,894 --> 00:11:30,724
or professional applications.

180
00:11:31,024 --> 00:11:35,524
Examples include Google,
coral, TPU, Qualcomm, hexagon,

181
00:11:35,524 --> 00:11:38,144
DSP, Nvidia Jetson Modules.

182
00:11:38,674 --> 00:11:41,664
These systems support large
models, foster interface.

183
00:11:42,494 --> 00:11:44,114
And more complex analytics.

184
00:11:44,594 --> 00:11:49,724
Supporting this wide range of hardware
ensures that our edge AI solutions

185
00:11:49,724 --> 00:11:56,134
can use anywhere from low cost home
devices to advanced industrial systems,

186
00:11:56,634 --> 00:11:58,679
neural network optimization pipeline.

187
00:11:59,489 --> 00:12:01,254
This pipeline is essential because.

188
00:12:01,754 --> 00:12:06,074
It prepares our EA models
to run efficiently on small

189
00:12:06,104 --> 00:12:07,934
resource limited edge devices.

190
00:12:08,294 --> 00:12:13,004
Remember, cloud models are often
large, slow, and power hungry.

191
00:12:13,694 --> 00:12:18,344
Edge devices cannot run such
models directly, so we use a three

192
00:12:18,344 --> 00:12:22,094
stage optimization pipeline to
convert a standard neural network

193
00:12:22,154 --> 00:12:24,554
into an edge optimized version.

194
00:12:25,154 --> 00:12:26,174
Architecture selection.

195
00:12:26,654 --> 00:12:28,604
That is the stage one choosing.

196
00:12:29,184 --> 00:12:33,504
Highly effective architectures
like mobile net, squeeze net,

197
00:12:34,164 --> 00:12:37,584
mobile net V three, optimized fur.

198
00:12:37,684 --> 00:12:41,704
General vision tasks on mobile
or edge hardwares squeeze net,

199
00:12:41,734 --> 00:12:46,684
extremely small footprint, ideal
fur, very constrained environments.

200
00:12:46,954 --> 00:12:51,214
Lightweight encoder decoder
networks with residual connections

201
00:12:51,274 --> 00:12:53,199
achieved two to five MB model sizes.

202
00:12:53,689 --> 00:12:54,879
These models use.

203
00:12:55,674 --> 00:13:01,524
Techniques like depth wise, supportable,
solution con convulsions and efficient

204
00:13:02,004 --> 00:13:06,384
residual blocks, allowing them to
run quickly while keeping accuracy,

205
00:13:06,384 --> 00:13:08,064
high quantization and compression.

206
00:13:08,114 --> 00:13:11,804
Stage two where the model
becomes truly ad differently.

207
00:13:12,194 --> 00:13:17,534
A standard AI model uses 32, put
32 bait floating point weights.

208
00:13:17,624 --> 00:13:20,684
We FP 32, but FP 32 is very heavy.

209
00:13:21,044 --> 00:13:24,434
It takes more memory, more
energy, and more compute.

210
00:13:24,704 --> 00:13:33,249
So we apply quantization converting FP
32 volumes, values to eight bit integer.

211
00:13:33,749 --> 00:13:37,949
This gives four times reduction in
model size, faster interference speed,

212
00:13:38,099 --> 00:13:39,929
much lower power consumption, and.

213
00:13:40,429 --> 00:13:45,199
The accuracy loss is usually
under 2%, which is completely

214
00:13:45,199 --> 00:13:47,149
acceptable for most applications.

215
00:13:47,539 --> 00:13:50,089
Stage three platform specific compilation.

216
00:13:50,509 --> 00:13:55,309
This is where we use edge focused
compilers, such as tensor flow, light

217
00:13:55,639 --> 00:14:00,319
for arm, process edge, TPU, compiler
for Google, coral hexagon, and

218
00:14:00,319 --> 00:14:02,359
then compiler for quantum Qualcomm.

219
00:14:02,359 --> 00:14:07,159
NPUs specialized compilers
for N-P-U-T-P-U acceleration.

220
00:14:07,659 --> 00:14:13,209
During compilation, the system performs
graph op graph optimization, which removes

221
00:14:13,269 --> 00:14:19,809
unnecessary operations operator fusion,
combines multiple operations into one

222
00:14:19,869 --> 00:14:22,779
for speed, memory layout optimization.

223
00:14:22,839 --> 00:14:25,989
This makes sure memory
access or efficient.

224
00:14:26,889 --> 00:14:30,909
The stage ensures that every
part of model runs our most

225
00:14:30,909 --> 00:14:33,219
efficient compute unit available.

226
00:14:33,699 --> 00:14:35,074
C-P-U-N-P-U or TPUs.

227
00:14:35,574 --> 00:14:37,164
Real time processing architecture.

228
00:14:37,584 --> 00:14:44,154
Now let's see how real time processing
pipeline works inside on a GA camera.

229
00:14:44,604 --> 00:14:50,514
This pipeline is designed so the camera
can continuously process videos at

230
00:14:50,514 --> 00:14:55,839
30 plus frames per second, even when
workload changes image acquisition.

231
00:14:56,339 --> 00:15:00,989
The first step is capturing incoming
video frames from the camera sensor.

232
00:15:01,679 --> 00:15:07,409
We used something called as a ring
buffer, which basically means the camera

233
00:15:07,409 --> 00:15:09,839
is always collecting frames in a loop.

234
00:15:10,049 --> 00:15:13,469
This is important because it
prevents the system from dropping

235
00:15:13,469 --> 00:15:15,419
frames if the processor is busy.

236
00:15:15,839 --> 00:15:20,099
So even if there is a temporary
spike in processing, the camera keeps

237
00:15:20,099 --> 00:15:22,429
capturing smoothly pre-processing.

238
00:15:23,329 --> 00:15:26,599
Once a frame is captured, the
next steps is pre-processing.

239
00:15:26,959 --> 00:15:31,159
This include tasks like resizing
the image, adjusting the colors,

240
00:15:31,159 --> 00:15:32,959
and normalizing the pixel values.

241
00:15:33,559 --> 00:15:36,954
These steps must be extremely
fast, ideally under 10,

242
00:15:37,249 --> 00:15:38,599
10 milliseconds per frame.

243
00:15:39,019 --> 00:15:43,009
To achieve these, we use SIMD
acceleration, which allows

244
00:15:43,339 --> 00:15:47,274
multiple pictures to be processed
at once in simple dumps.

245
00:15:47,334 --> 00:15:51,264
Pre-processing is about getting
the image into the exact same.

246
00:15:51,654 --> 00:15:56,534
And format that a model expects
neural network execution.

247
00:15:56,894 --> 00:15:59,264
Next comes the core steps.

248
00:15:59,384 --> 00:16:03,314
Running a model on the frame
on mid range hardwares.

249
00:16:03,434 --> 00:16:08,804
This takes about 30 to 45 milliseconds
depending on the complexity of the model.

250
00:16:09,074 --> 00:16:13,754
This is where the camera detects objects,
identify patterns, recognize the faces.

251
00:16:14,279 --> 00:16:19,499
This model has already been optimized
for our previous pipeline, so it can run

252
00:16:19,499 --> 00:16:25,589
quickly, even with the limited hardware
post processing, making the output useful

253
00:16:25,709 --> 00:16:27,809
after the model gives its predictions.

254
00:16:27,869 --> 00:16:30,059
We performs post processing.

255
00:16:30,779 --> 00:16:35,849
The, this includes tasks like filtering
noise, applying thresholds, drawing

256
00:16:35,849 --> 00:16:41,299
boundary bonding boxes, or converting raw
model outputs into actionable results.

257
00:16:41,839 --> 00:16:46,329
For example, instead of just giving
numbers, the system says person

258
00:16:46,329 --> 00:16:48,879
de person detected confidence.

259
00:16:48,879 --> 00:16:51,039
95% or no anomaly.

260
00:16:51,039 --> 00:16:56,529
Found the steps, prepares the
results for app, user interfaces,

261
00:16:56,709 --> 00:16:59,979
and automated station making systems.

262
00:17:00,789 --> 00:17:05,349
These four steps capturing
pre-processing AI inference.

263
00:17:05,849 --> 00:17:08,999
And post processing work
together continuously to

264
00:17:08,999 --> 00:17:10,739
maintain real time performance.

265
00:17:11,189 --> 00:17:15,989
Even when the camera faces variable
workloads, the architecture ensures smooth

266
00:17:15,989 --> 00:17:20,819
processing without delays are frame drops.

267
00:17:21,479 --> 00:17:26,459
This is what allows smart cameras to
react instantly and operate reliably

268
00:17:26,519 --> 00:17:27,869
without depending on the cloud

269
00:17:28,369 --> 00:17:30,169
energy efficiency optimization.

270
00:17:30,669 --> 00:17:33,789
Now let's talk about energy
efficiency, which is one of the

271
00:17:33,789 --> 00:17:38,529
most important parts of deploying
AI and battery powered iot cameras.

272
00:17:38,889 --> 00:17:41,199
Energy devices cannot
afford to waste power.

273
00:17:41,949 --> 00:17:48,639
And this slide explains how we
reduce energy usage by 60 to 70%

274
00:17:48,639 --> 00:17:50,919
compared to the cloud-based systems.

275
00:17:51,459 --> 00:17:55,169
Computational optimization,
the first area is competition.

276
00:17:55,669 --> 00:18:00,169
Running AI models normally requires
a lot of CPU cycles, especially when

277
00:18:00,169 --> 00:18:02,989
using 32 bit floating point operations.

278
00:18:03,379 --> 00:18:06,769
But on edge devices, this would
drain the battery very quickly.

279
00:18:07,099 --> 00:18:11,629
So we use techniques like
quantization and operator fusion.

280
00:18:12,129 --> 00:18:15,939
Quantization reduce heavy 32 beat
operations to efficient eight.

281
00:18:15,939 --> 00:18:17,774
BEAT operations Operator.

282
00:18:17,949 --> 00:18:21,729
Operator fusion combines multiple
operations into a single step.

283
00:18:22,389 --> 00:18:23,049
Together.

284
00:18:23,409 --> 00:18:30,669
These reduced CPUs is approximately 60 to
70%, and when hardware accelerators like

285
00:18:30,669 --> 00:18:35,979
NPUs or TPUs are available, they offer
four to 10 times better energy efficiency

286
00:18:35,979 --> 00:18:39,129
per interference, memory optimization.

287
00:18:40,119 --> 00:18:41,709
The second area is memory.

288
00:18:42,209 --> 00:18:48,719
Most people think computation users most
power, but in reality, accessing DRAM

289
00:18:48,719 --> 00:18:50,524
actually consumes a hundred to two times.

290
00:18:51,024 --> 00:18:54,624
200 times more energy than
performing a math calculation.

291
00:18:55,104 --> 00:18:59,484
To solve this, we compress models,
use efficient data structures, and

292
00:18:59,484 --> 00:19:05,094
carefully manage memory buffers
so that data is reused instead

293
00:19:05,094 --> 00:19:07,104
of repeatedly loaded from dram.

294
00:19:07,604 --> 00:19:12,704
This greatly cuts down unnecessary memory
access and saves significant energy.

295
00:19:13,204 --> 00:19:15,604
The final area is controlling the cameras.

296
00:19:16,279 --> 00:19:20,269
Other components like sensors,
radios, and supporting hardware.

297
00:19:21,109 --> 00:19:24,619
We don't keep everything running
at full power all the time.

298
00:19:24,799 --> 00:19:27,289
Instead, we use smart scheduling.

299
00:19:27,589 --> 00:19:31,519
The camera sensor reduces
power during low activity.

300
00:19:31,999 --> 00:19:34,809
Wireless radio transmit
data only when needed.

301
00:19:35,649 --> 00:19:38,769
AI interference happens more
frequently when motion is

302
00:19:38,769 --> 00:19:40,929
detected and less often when this.

303
00:19:41,484 --> 00:19:47,214
Seen as ideal by a, by adapting to
real world conditions, the system

304
00:19:47,214 --> 00:19:53,364
saves a large amount of energy while
still delivering accurate results.

305
00:19:53,604 --> 00:19:59,124
So overall, through this competition
optimization, memory efficiency, and

306
00:19:59,124 --> 00:20:03,329
smart peripheral control, we achieve
massive reduction in energy usage.

307
00:20:03,829 --> 00:20:08,509
This is what makes a GA practical
for long running I OT cameras that

308
00:20:08,509 --> 00:20:13,639
need operate reliably for days or
even months without frequent charging

309
00:20:14,139 --> 00:20:15,279
performance evaluation.

310
00:20:15,579 --> 00:20:18,939
Now let's go over the
performance evaluation results.

311
00:20:19,719 --> 00:20:22,539
This slide shows how well we optimize.

312
00:20:22,539 --> 00:20:25,479
The neural network runs across
different hardware platforms.

313
00:20:25,979 --> 00:20:30,029
And it highlights why a GA is
strong replacement for cloud-based

314
00:20:30,029 --> 00:20:32,189
processing memory footprint.

315
00:20:32,689 --> 00:20:33,829
This is the fast metric.

316
00:20:34,329 --> 00:20:38,499
Our optimized model is extremely
lightweight, only around 600 KB

317
00:20:38,529 --> 00:20:43,899
insights, even when we include runtime
buffers and intermediate ssars.

318
00:20:44,289 --> 00:20:49,379
The total memory used is only
four to eight mb to put that.

319
00:20:49,879 --> 00:20:51,019
Into perspective.

320
00:20:51,139 --> 00:20:57,289
A typical edge device today comes with
one GB of ram, so our solution uses

321
00:20:57,379 --> 00:20:59,959
a small fraction of that around 3%.

322
00:21:00,459 --> 00:21:06,759
The small footprint is what allows low
cost boats like Raspberry Pi or entry

323
00:21:07,059 --> 00:21:13,049
level iot chips to run advanced TA without
any memory appraiser, energy consumption.

324
00:21:13,549 --> 00:21:15,499
Next, we look at how much energy.

325
00:21:15,999 --> 00:21:18,399
Each frame requires during pre-processing.

326
00:21:19,089 --> 00:21:23,529
Running AI models continuously
can run batteries quickly, but our

327
00:21:23,529 --> 00:21:28,739
optimization significantly reduce
power use on a regular OM CPU.

328
00:21:28,829 --> 00:21:36,179
Each frame uses 0.5 to 0.8 joles, but
when we use dedicated AI accelerators

329
00:21:36,179 --> 00:21:42,569
like NPUs and TPUs, that number
drops to just a 0.1 to 0.2 jaws per

330
00:21:42,569 --> 00:21:44,909
frame, almost five times improvement.

331
00:21:45,884 --> 00:21:49,874
And if you compare this, a
cloud-based setup, when devices

332
00:21:49,874 --> 00:21:54,674
must stream video continuously, the
energy difference is more dramatic.

333
00:21:55,154 --> 00:22:00,584
Cloud streaming can consume up to
three jts per frame, meaning a GA

334
00:22:00,584 --> 00:22:06,154
is more efficient for long running
deployments, bandwidth savings.

335
00:22:06,394 --> 00:22:11,944
Finally, let's talk about bandwidth when
cameras, process videos on device itself.

336
00:22:12,754 --> 00:22:17,494
The only spend send important
events are small summaries to the

337
00:22:17,494 --> 00:22:19,474
cloud, not full video streams.

338
00:22:20,434 --> 00:22:25,714
Cloud streaming usually needs
around 1.2, 1.8 MVPS per camera.

339
00:22:26,284 --> 00:22:30,364
But with the LGA, we reduce
this number to 52 60.

340
00:22:30,864 --> 00:22:33,114
52, 200 kps.

341
00:22:33,504 --> 00:22:37,584
This is an 85 to 95
reduction in bandwidth usage.

342
00:22:38,034 --> 00:22:43,344
This makes deployments practical, even
in rural areas, construction sites

343
00:22:43,404 --> 00:22:45,534
or places with limited connectivity.

344
00:22:45,669 --> 00:22:49,959
It also lawyers monthly data
costs and improve reliability.

345
00:22:50,469 --> 00:22:54,549
So overall, the performance
evolution clearly shows the

346
00:22:54,549 --> 00:22:56,345
benefits of our a GA approach.

347
00:22:56,845 --> 00:23:03,165
These results confirm that realtime ai,
realtime EA on edge devices is not only

348
00:23:03,165 --> 00:23:08,061
possible it is efficient, scalable,
and ready for real world deployment.

349
00:23:08,561 --> 00:23:11,351
Let's look at some
application case studies.

350
00:23:11,851 --> 00:23:16,861
These case studies show how technology
performs outside the lab in real world.

351
00:23:17,761 --> 00:23:19,981
Environments with real challenges.

352
00:23:20,481 --> 00:23:22,401
Smart Doorbell security.

353
00:23:22,901 --> 00:23:27,341
Our first example is Smart Doorbell
Camera, built on Raspberry Pi three.

354
00:23:27,851 --> 00:23:32,921
Even though this is an entry level
platform, the optimized AI model

355
00:23:32,981 --> 00:23:37,181
performs extreme level by processing
video directly on the device.

356
00:23:37,211 --> 00:23:42,701
We achieved 72% reduction in
bandwidth usage, 4.25 times

357
00:23:42,701 --> 00:23:44,021
improvement in battery life.

358
00:23:44,606 --> 00:23:47,186
Instant on device detection.

359
00:23:47,546 --> 00:23:53,296
Better privacy since raw video
isn't not uploaded in case study

360
00:23:53,296 --> 00:23:56,056
two, industrial defect detection.

361
00:23:56,716 --> 00:24:02,086
This second case study is from an
industrial automation environment.

362
00:24:02,566 --> 00:24:06,861
Factories often use dozens of
cameras to check the defects on

363
00:24:06,891 --> 00:24:08,941
fast moving production lines.

364
00:24:09,901 --> 00:24:14,601
In this prototype, 50 plus smart
cameras were deployed with LGA.

365
00:24:14,751 --> 00:24:20,811
Each device could detect anomalies in
under a hundred milliseconds, which allows

366
00:24:20,811 --> 00:24:23,511
system to catch defects immediately.

367
00:24:23,901 --> 00:24:28,791
We also reduce the data being
transmitted by over 90%, which is

368
00:24:28,791 --> 00:24:32,781
extremely important in factories where
network stability cannot be guaranteed.

369
00:24:33,501 --> 00:24:38,001
This real time response is something
cloud processing simply cannot deliver.

370
00:24:38,976 --> 00:24:43,896
The third example is smart home
security setup with multiple cameras.

371
00:24:44,106 --> 00:24:48,846
In many homes, internet bandwidth
is limited and people also care

372
00:24:48,846 --> 00:24:50,796
deeply about video privacy.

373
00:24:51,186 --> 00:24:55,746
A GA solves both the challenges, the
systems analyze the video locally.

374
00:24:56,106 --> 00:25:00,306
And since only essential alerts to
the cloud, this reduced streaming

375
00:25:00,306 --> 00:25:04,506
cost, protects user privacy
and ensures the system works

376
00:25:04,536 --> 00:25:06,546
even when internet is unstable.

377
00:25:07,086 --> 00:25:12,666
These case studies clearly demonstrate
AGI is practical, scalable, and

378
00:25:13,056 --> 00:25:17,006
ready for different, real different
industries future directions.

379
00:25:17,936 --> 00:25:22,166
Now let's move on to what the
future looks like for a GI Systems.

380
00:25:22,616 --> 00:25:24,536
The technology is evolving rapidly.

381
00:25:24,971 --> 00:25:28,691
And the next steps will make
smarter devices even faster, more

382
00:25:28,691 --> 00:25:30,071
private, and more intelligent.

383
00:25:30,571 --> 00:25:35,431
Federated learning In the future, devices
will be able to learn from new data

384
00:25:35,461 --> 00:25:37,621
without sending dry images to the cloud.

385
00:25:37,951 --> 00:25:42,781
Instead, each device trends locally and
send only model updates, not videos.

386
00:25:43,111 --> 00:25:46,831
This keeps data private while
improving overall accuracy.

387
00:25:47,401 --> 00:25:51,121
This approach is perfect for home
cameras, healthcare devices, and any

388
00:25:51,121 --> 00:25:52,746
systems where privacy is important.

389
00:25:53,671 --> 00:25:56,371
Collaborative edge intelligence.

390
00:25:56,821 --> 00:26:00,811
Another exciting direction is
device to device collaboration.

391
00:26:00,871 --> 00:26:04,831
Imagine multiple cameras in a home
or a factory sharing the workloads.

392
00:26:05,281 --> 00:26:09,751
If a, if one device is busy or
has low battery, another device

393
00:26:09,751 --> 00:26:10,981
can take over the processing.

394
00:26:11,011 --> 00:26:15,811
This creates a distributed intelligence
layer where devices support each other.

395
00:26:16,021 --> 00:26:19,081
It makes the whole system
more reliable and efficient.

396
00:26:19,651 --> 00:26:21,931
Next generation head hardware.

397
00:26:22,431 --> 00:26:27,441
We expect more powerful NPUs,
TPUs, and specialist chiefs with

398
00:26:27,501 --> 00:26:28,761
much higher performance per.

399
00:26:29,616 --> 00:26:33,276
Devices will run larger AI models
without consuming more power.

400
00:26:33,786 --> 00:26:38,495
This enable analytics like multi object
tracking, behavior prediction, and

401
00:26:38,495 --> 00:26:42,926
even on device video summarization,
standardized application framework.

402
00:26:43,436 --> 00:26:46,706
Finally, we expect platforms
that allow developers to build.

403
00:26:47,396 --> 00:26:52,016
Once and deploy anywhere
these, this means yay.

404
00:26:52,016 --> 00:26:55,376
Applications will run across
all types of hardwares from

405
00:26:55,376 --> 00:26:59,516
doorbell cameras to industrial
robots with without major rework.

406
00:27:00,016 --> 00:27:05,926
These advancements will help speed up
adoption and reduce development cost.

407
00:27:06,426 --> 00:27:07,176
Conclusion.

408
00:27:07,676 --> 00:27:10,706
To conclude a GA is not
just an enhancement.

409
00:27:10,885 --> 00:27:15,865
It represents a complete shift in
how smart camera systems operate.

410
00:27:16,285 --> 00:27:20,245
We are moving from cloud centered
intelligence to a world where

411
00:27:20,335 --> 00:27:23,065
intelligence lives directly on the device.

412
00:27:23,565 --> 00:27:24,765
Toward the presentation.

413
00:27:24,765 --> 00:27:29,685
We have seen this approach gives
us major improvements, like

414
00:27:30,015 --> 00:27:31,935
under 50 milliseconds, latency.

415
00:27:32,435 --> 00:27:37,470
For real time processing works on
devices with little less 512 MBM

416
00:27:37,835 --> 00:27:42,425
80% or more, reduction in bandwidth
usage, four to five times improvement

417
00:27:42,425 --> 00:27:46,805
in battery life, high reliability,
even low connectivity environments.

418
00:27:47,305 --> 00:27:53,426
These results show the A GA is practical,
efficient, and highly scalable.

419
00:27:53,926 --> 00:27:56,356
As a final statement, the future is smart.

420
00:27:56,866 --> 00:27:58,121
Camera is edge first.

421
00:27:58,621 --> 00:28:02,701
With optimized AI models, efficient
hardware and intelligent software

422
00:28:02,701 --> 00:28:07,111
architecture, we can unlock fast,
private, and reliable AI ware.

423
00:28:07,611 --> 00:28:07,926
Thank you.

424
00:28:08,226 --> 00:28:10,566
I'm happy to take any technical questions.

