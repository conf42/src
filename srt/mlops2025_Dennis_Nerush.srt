1
00:00:00,500 --> 00:00:04,220
Hello and welcome to my talk Today.

2
00:00:04,430 --> 00:00:10,070
I'll share with you how we at
Elementor have created a fault

3
00:00:10,070 --> 00:00:16,280
tolerant solution on top of our Gen AI
products that serves millions of users.

4
00:00:16,849 --> 00:00:20,570
It wasn't an easy ride, and
today I wanna share with you.

5
00:00:21,125 --> 00:00:25,865
A lot of our, a lot of our journey
and the lessons that we've learned.

6
00:00:26,615 --> 00:00:30,755
But first I'd like to
start up with a real story.

7
00:00:31,505 --> 00:00:36,275
So several months ago we've
launched a new product.

8
00:00:36,695 --> 00:00:37,355
It's called.

9
00:00:37,855 --> 00:00:41,935
The AI site planner, it's
a really cool product.

10
00:00:42,205 --> 00:00:47,665
It helps professional web creators
plan their website that they're

11
00:00:47,665 --> 00:00:48,715
gonna build for their client.

12
00:00:49,255 --> 00:00:54,235
It starts with an agent that conducts
a sort of AI based interview with the

13
00:00:54,235 --> 00:00:58,620
web creator in order to understand
the goals, the purposes, and.

14
00:00:59,495 --> 00:01:03,995
Interesting elements of the website
that's going to be implemented

15
00:01:04,585 --> 00:01:05,985
and developed for the client.

16
00:01:06,495 --> 00:01:12,090
Then it has some layer of site maps
and wire frames to start off with

17
00:01:12,090 --> 00:01:15,150
a really good draft for website.

18
00:01:15,690 --> 00:01:20,130
It's a very cool product and is
supposed to be launched on the

19
00:01:20,130 --> 00:01:23,430
23rd of March this year, however.

20
00:01:23,840 --> 00:01:31,280
Just two days before the launch, our
open AI account, basically our main

21
00:01:31,280 --> 00:01:37,220
vendor that we have been building the
product on top of has locked our account

22
00:01:37,310 --> 00:01:40,160
and we were not able to make any calls.

23
00:01:40,660 --> 00:01:46,030
I bet that many of you right
now think that back then this

24
00:01:46,030 --> 00:01:46,855
is how we would look like.

25
00:01:47,355 --> 00:01:52,965
Running around in hysteric mode,
incident mode, war zone, but actually

26
00:01:53,505 --> 00:01:55,605
that looked much more like this.

27
00:01:56,205 --> 00:02:00,615
Yes, it wasn't pleasant, but due
to the infrastructure that we've

28
00:02:00,615 --> 00:02:06,375
developed, it wasn't that of a big
deal, and we have easily mitigated the

29
00:02:06,380 --> 00:02:08,389
situation and had a successful launch.

30
00:02:08,889 --> 00:02:13,899
So before I I dive even deeper,
I have to start with an apology.

31
00:02:14,440 --> 00:02:20,840
I'm gonna talk briefly about two
companies, Microsoft and OpenAI, and in

32
00:02:20,840 --> 00:02:23,510
this specific talk, I won't be their fan.

33
00:02:23,989 --> 00:02:27,109
And if any of the listeners
are from Microsoft Open ai.

34
00:02:27,755 --> 00:02:28,565
I'm sorry in advance.

35
00:02:29,065 --> 00:02:29,695
Hi everyone.

36
00:02:30,114 --> 00:02:31,075
My name is Dennis.

37
00:02:31,315 --> 00:02:33,174
I lead the AI at elementary.

38
00:02:33,775 --> 00:02:39,095
I've been in this industry for the
last 17 years, seven of which I've

39
00:02:39,095 --> 00:02:42,035
been practicing AI way before.

40
00:02:42,035 --> 00:02:42,965
AI was cool.

41
00:02:43,715 --> 00:02:46,054
I live and breathe this this topic.

42
00:02:46,114 --> 00:02:50,524
I try out the new tools, the
technologies I play around with Volvo.

43
00:02:51,024 --> 00:02:53,394
I live and breathe on this topic.

44
00:02:53,784 --> 00:02:55,014
I have a lot of passion about it.

45
00:02:55,104 --> 00:02:59,754
I play all the different tools, read
the papers, and always try to find new

46
00:02:59,754 --> 00:03:04,494
technologies that can improve my life and
my, and the product that I'm building.

47
00:03:04,994 --> 00:03:05,564
So today.

48
00:03:06,064 --> 00:03:10,535
How we are, how we've developed an
infrastructure, is able to handle 1

49
00:03:10,535 --> 00:03:17,125
million AI requests, how our solution
is resilient to handle failures as

50
00:03:17,125 --> 00:03:23,215
severe as one of our main AI vendors
lock up our account, and what are we

51
00:03:23,215 --> 00:03:24,834
doing in order to continuously improve?

52
00:03:25,525 --> 00:03:25,855
Let's start.

53
00:03:26,355 --> 00:03:27,975
So a few words about elementary.

54
00:03:28,304 --> 00:03:32,924
Elementary is number one web platform
for building websites in the world.

55
00:03:33,554 --> 00:03:38,984
Right now we're around 19 million
websites that were built by elementary

56
00:03:39,704 --> 00:03:42,455
and right now there are almost 19.

57
00:03:42,955 --> 00:03:44,064
So before we dive in

58
00:03:44,564 --> 00:03:46,364
few words about Elementor.

59
00:03:46,605 --> 00:03:50,955
So Elementor is the number one platform
for building websites in the world.

60
00:03:51,465 --> 00:03:55,514
It's being used by professionals,
and there are almost 19 million,

61
00:03:55,905 --> 00:03:57,944
even more than 19 million websites.

62
00:03:58,444 --> 00:04:03,334
It started off as a drag and drop
tool that can help web creators easily

63
00:04:03,334 --> 00:04:06,214
build beautiful web pages and websites.

64
00:04:06,845 --> 00:04:11,524
But in the last two and a half years,
it's also started to add more and more

65
00:04:11,524 --> 00:04:17,644
AI tools, and I'm proud to say that
elementary is one of the pioneers of AI

66
00:04:17,644 --> 00:04:19,654
products for large scale applications.

67
00:04:20,225 --> 00:04:22,714
Because you know what
BC stands for, right?

68
00:04:23,525 --> 00:04:25,385
Before Chief Chat, g pt, of course.

69
00:04:25,924 --> 00:04:31,715
So Cheche, GPT was launched on
November 20, 22nd, and just a

70
00:04:31,715 --> 00:04:33,064
little bit more than four months.

71
00:04:33,845 --> 00:04:37,054
Elementor has launched
our first AI product.

72
00:04:37,835 --> 00:04:40,475
We had several tools right on the start.

73
00:04:40,985 --> 00:04:45,275
We allowed our users to generate texts,
generate content for their websites,

74
00:04:45,664 --> 00:04:51,125
generate and edit images using ai, even
write some sophisticated code using

75
00:04:51,125 --> 00:04:54,095
AI like for animations even before.

76
00:04:54,704 --> 00:04:56,624
Chat, GPT and OpenAI had memory.

77
00:04:56,924 --> 00:05:01,274
We invented what we called the AI context
where users could upload some additional

78
00:05:01,274 --> 00:05:05,534
data about Versa site and the business
and their tone and voice, and we would

79
00:05:05,534 --> 00:05:07,724
incorporate that in any AI requests.

80
00:05:08,264 --> 00:05:12,314
And lastly, we have some building tools
that can predict the next layout and

81
00:05:12,314 --> 00:05:16,544
has help where creators to be more
efficient and more creative in their work.

82
00:05:17,504 --> 00:05:21,884
So since then, we have more than
2 million users experiencing

83
00:05:21,884 --> 00:05:24,465
AI of elementary AI products.

84
00:05:24,734 --> 00:05:29,975
We have more than 40 AI based
features, and we exchange monthly

85
00:05:29,975 --> 00:05:32,135
around 10 billion AI tokens.

86
00:05:32,635 --> 00:05:36,285
So the first topic that I want
to talk to you about today is how

87
00:05:36,285 --> 00:05:41,025
we're handling 1 million daily AI
requests and to understand that

88
00:05:41,025 --> 00:05:42,555
when to go back to the beginning.

89
00:05:42,915 --> 00:05:47,625
'cause when we just started, like
many companies, we started straight.

90
00:05:47,625 --> 00:05:51,495
We've open ai, but for those of
you who are early adopters, might

91
00:05:51,495 --> 00:05:57,975
remember that JGPT 3.5 turbo rate
limit requests per minute was only 60.

92
00:05:58,455 --> 00:05:59,175
Think about this number.

93
00:05:59,835 --> 00:06:04,695
It back in the day, they allowed
just 60 requests in one minute.

94
00:06:05,535 --> 00:06:09,915
Something that is just not
feasible for enterprise software.

95
00:06:10,415 --> 00:06:13,595
And also need to remember that
two and a half years ago, open

96
00:06:13,595 --> 00:06:15,065
ai, wasn't that a big deal?

97
00:06:15,575 --> 00:06:19,175
So we, as a big company, we
wanted to find a vendor that

98
00:06:19,175 --> 00:06:20,735
might be more suitable for us.

99
00:06:21,095 --> 00:06:23,405
This is why we moved to
Microsoft Edge Azure.

100
00:06:23,825 --> 00:06:27,515
But back in the day, there are limits
and support was much better than open ai.

101
00:06:27,515 --> 00:06:30,625
So it worked really good
for almost a year until.

102
00:06:31,150 --> 00:06:36,040
We build elementary copilot, and
similar to GitHub copilot, it's able

103
00:06:36,040 --> 00:06:38,170
to predict the next line of code.

104
00:06:38,830 --> 00:06:42,969
The elementary copilot is able
to predict the next layout.

105
00:06:43,360 --> 00:06:48,040
So let's say you have a title of your page
or the next element elementary copilot

106
00:06:48,070 --> 00:06:53,200
understands whether it's an about page
and automatically suggests a full layout.

107
00:06:53,455 --> 00:07:00,025
With text based from the AI context
that is suitable for this section.

108
00:07:00,354 --> 00:07:04,364
And from there, web creator can just click
on the next section, the next section

109
00:07:04,424 --> 00:07:09,364
and the next section, and basically
build pages much faster to support that.

110
00:07:09,484 --> 00:07:15,574
However, we had to, the Microsoft Azure
rate limits simply were not enough.

111
00:07:16,074 --> 00:07:19,224
Because back in the day, even
though their rate limits were

112
00:07:19,374 --> 00:07:23,764
higher than what Op OpenAI OpenAI
provided there were not enough.

113
00:07:24,444 --> 00:07:26,724
To support the copilot abilities.

114
00:07:26,994 --> 00:07:28,164
Think about it for a second.

115
00:07:28,554 --> 00:07:30,324
We're taking a lot of context.

116
00:07:30,474 --> 00:07:35,374
We are firing events almost every
interaction users is making 'cause we

117
00:07:35,374 --> 00:07:40,064
need to understand when exactly the
user wants to create the next layout.

118
00:07:40,514 --> 00:07:42,434
Sometimes we just moving the mouse around.

119
00:07:42,554 --> 00:07:45,134
Sometimes we are editing some
stuff and sometimes we do

120
00:07:45,134 --> 00:07:46,094
need to create a new block.

121
00:07:46,484 --> 00:07:52,694
So we've been cha firing a lot of
events and besides the rate I thing.

122
00:07:53,624 --> 00:07:58,819
The more we used, the more we adopted
AI abilities the hard it became to

123
00:07:58,819 --> 00:08:03,849
work with Azure, because if, for most
of you who are working with Microsoft

124
00:08:03,879 --> 00:08:08,659
Azure, know that every time a new model
is being released, but open ai, it

125
00:08:08,719 --> 00:08:11,179
becomes available in Azure pretty fast.

126
00:08:11,509 --> 00:08:14,539
However, in Azure is
a context of a region.

127
00:08:14,689 --> 00:08:18,559
It does not exist in open ai, and
it means that not every model.

128
00:08:19,009 --> 00:08:20,629
It's available in every region.

129
00:08:21,139 --> 00:08:24,809
And that becomes a very
complicated thing for developers.

130
00:08:25,229 --> 00:08:29,879
'cause we need to remember that they
have a region and in every region they

131
00:08:29,969 --> 00:08:33,639
need to create a dedicated resource
where they will deploy the model.

132
00:08:34,299 --> 00:08:38,109
And then the quarter, the
rate limit is across all those

133
00:08:38,109 --> 00:08:39,774
deploys within that region.

134
00:08:40,704 --> 00:08:45,094
Developer need to know against what
region and what deploy they're working.

135
00:08:45,604 --> 00:08:48,634
But what happens when we need
to add a new model that's not

136
00:08:48,634 --> 00:08:50,584
available in one one region?

137
00:08:50,884 --> 00:08:54,184
We need to replicate the whole
architecture from the start.

138
00:08:54,604 --> 00:08:58,734
It's very tedious and time
consuming for all the teams.

139
00:08:59,244 --> 00:09:02,724
And in case you need to increase
your rate limit, this is an actual

140
00:09:02,724 --> 00:09:06,144
form from Azure Portal, which
you do that you need to fill in.

141
00:09:06,579 --> 00:09:08,410
Understand what it is you're asking.

142
00:09:08,410 --> 00:09:10,899
What type of filter do you
want and in what region?

143
00:09:11,290 --> 00:09:16,170
And then submit this request and then
get approval for additional requests.

144
00:09:16,670 --> 00:09:19,849
Shortly speaking, that's
very complicated and tough.

145
00:09:20,630 --> 00:09:25,130
So we were looking for something much
easier, both in terms of rate limits

146
00:09:25,370 --> 00:09:29,905
developer experience, and we wanted to
practice and cha and test new models

147
00:09:30,324 --> 00:09:32,125
the minute they, they become available.

148
00:09:32,635 --> 00:09:37,954
So we moved back to open ai that
around that time, their rate limit,

149
00:09:38,285 --> 00:09:40,925
it was actually higher than Azures.

150
00:09:41,839 --> 00:09:44,389
From developer perspective,
there is nothing to compare

151
00:09:44,659 --> 00:09:46,339
in several lines of code.

152
00:09:46,729 --> 00:09:49,309
You already are fully
integrated with open ai.

153
00:09:49,699 --> 00:09:53,449
You don't know what regions and you
don't care, and you can use any model

154
00:09:53,449 --> 00:09:56,969
you want and you can just switch
the model in a matter of seconds.

155
00:09:57,469 --> 00:10:00,109
But that still wasn't enough of a copilot.

156
00:10:00,919 --> 00:10:04,099
It still wasn't enough
of a copilot, so we went.

157
00:10:04,489 --> 00:10:08,880
To the most reliable source,
JGPT to see how can we get even

158
00:10:08,880 --> 00:10:10,500
higher rate limits from OpenAI.

159
00:10:11,430 --> 00:10:13,469
Basically, OpenAI suggests two options.

160
00:10:13,949 --> 00:10:18,829
One, the second one is to apply to a
higher rate rate limits for basically

161
00:10:19,160 --> 00:10:25,370
engage with OpenAI and start an enterprise
tier, but that is actually very expensive.

162
00:10:25,954 --> 00:10:30,375
And it requires us as a company to
commit to a certain spend of money

163
00:10:30,405 --> 00:10:32,175
that we were not ready to commit.

164
00:10:32,175 --> 00:10:37,965
Yet this, and this is why we go, went with
the first approach of having multiple API

165
00:10:37,965 --> 00:10:43,305
keys for multiple organizations and create
some sort of a round Ruben around them

166
00:10:43,965 --> 00:10:49,995
to make sure that we basically have more
available organizations, and therefore

167
00:10:50,055 --> 00:10:51,945
more our rate limit becomes higher.

168
00:10:52,445 --> 00:10:57,395
So let's see how our infrastructure, the
OpenAI multi organization proxy works.

169
00:10:58,085 --> 00:11:02,045
We have, we've created several
organizations within OpenAI and

170
00:11:02,045 --> 00:11:06,455
we've listed them along with their
information in inside our code base.

171
00:11:07,265 --> 00:11:09,965
Then we extended the OpenAI, SDK.

172
00:11:10,265 --> 00:11:12,395
So the developers will be very easy.

173
00:11:12,755 --> 00:11:17,615
To work with this infrastructure
without actually understanding

174
00:11:17,645 --> 00:11:22,630
or knowing or caring about what
organization exactly they work about,

175
00:11:22,810 --> 00:11:24,940
they work with what we're doing.

176
00:11:24,940 --> 00:11:28,600
We're simply called, instead of
calling open AI service and straight

177
00:11:28,600 --> 00:11:33,010
going to check completion, we first
call get SDK, which is our extension

178
00:11:33,370 --> 00:11:34,780
that basically pulls the next.

179
00:11:35,095 --> 00:11:41,035
Or the next organization and that this
is how we can work with multiple and

180
00:11:41,035 --> 00:11:45,295
multiple organizations and extend our
raise limit basically in indefinitely.

181
00:11:45,795 --> 00:11:46,275
That work.

182
00:11:46,545 --> 00:11:48,075
That worked beautifully.

183
00:11:48,575 --> 00:11:51,905
It even helped us with our
next product, the site planner.

184
00:11:52,265 --> 00:11:55,865
But if you remember, as I mentioned
at the beginning, it starts with an

185
00:11:55,865 --> 00:12:00,125
agent that interviews the web creator,
ask different guiding questions to

186
00:12:00,125 --> 00:12:03,485
understand the goal of the websites,
and then it prepares a draft.

187
00:12:03,840 --> 00:12:07,855
Since we're working with professionals,
we first want to create a site map, the

188
00:12:08,215 --> 00:12:12,475
bird's eye view of all the different
pages, their content, the paragraphs,

189
00:12:12,835 --> 00:12:17,065
and the goal of every page and every
paragraph for that website, because

190
00:12:17,065 --> 00:12:20,395
we're doing some things that are
professional for professionals, so we

191
00:12:20,395 --> 00:12:25,405
need to make sure we create a decent
design and only with them we can move.

192
00:12:26,100 --> 00:12:30,840
To the wire frame, to this first
beautiful draft, it will show both

193
00:12:30,840 --> 00:12:34,860
the upgrader and the user, the client,
how the website will look like.

194
00:12:35,340 --> 00:12:39,480
You can see that it happens really
fast and actually in parallel, there

195
00:12:39,480 --> 00:12:44,380
are hundreds of AI requests that
happen here in order to generate all

196
00:12:44,380 --> 00:12:48,310
this contact because both the layout
and the content, and actually the

197
00:12:48,310 --> 00:12:50,590
whole structure is created by ai.

198
00:12:50,980 --> 00:12:53,800
So we had to have really high limits.

199
00:12:54,300 --> 00:12:56,460
So some takeaways first.

200
00:12:56,640 --> 00:12:59,760
Now, if once you start developing
your product and you're getting

201
00:12:59,760 --> 00:13:03,180
into production, you first need
to understand that rate limits

202
00:13:03,180 --> 00:13:04,680
exist and you need to track them.

203
00:13:05,100 --> 00:13:07,650
You need to know your usage
because if you have a product and

204
00:13:07,920 --> 00:13:11,200
you start to expose it to a world
and more and more user using it.

205
00:13:11,985 --> 00:13:12,585
This is awesome.

206
00:13:12,645 --> 00:13:13,215
This is awesome.

207
00:13:13,695 --> 00:13:17,025
You just need to make sure that you're
not getting too close to a rate limit.

208
00:13:17,325 --> 00:13:21,675
Otherwise, it might be very dangerous
and your users might not like your

209
00:13:21,720 --> 00:13:26,280
service not being available, and
in case you are getting closer for

210
00:13:26,280 --> 00:13:29,970
whatever reason, just remember that you
can easily create more organizations

211
00:13:30,600 --> 00:13:32,760
and use a shared pool around them.

212
00:13:33,260 --> 00:13:33,530
All right.

213
00:13:33,970 --> 00:13:36,755
The second topic would
be how we're building.

214
00:13:37,615 --> 00:13:39,325
A resilient AI solution.

215
00:13:40,025 --> 00:13:43,745
So we, we understood how can,
how could we handle the load

216
00:13:44,165 --> 00:13:45,395
and have handled the rate limit.

217
00:13:45,845 --> 00:13:50,465
But rate limit and load is not
only the case, and I would quote

218
00:13:50,465 --> 00:13:54,235
one of the best engineers, Mike
Tyson, that everyone has a plan.

219
00:13:54,625 --> 00:13:55,645
Tva, he get punched in the mouth.

220
00:13:56,425 --> 00:13:57,715
So you would expect.

221
00:13:57,990 --> 00:14:02,310
This is how if you're working with
multiple ai, open AI organizations,

222
00:14:02,700 --> 00:14:08,020
this is how you, your usage and your
performance would look like similar

223
00:14:08,020 --> 00:14:10,690
because what's the difference between
one organization and the second one

224
00:14:10,695 --> 00:14:12,640
and the, and a different organization?

225
00:14:13,090 --> 00:14:16,150
Let's assume that all of
them are under the same tier.

226
00:14:16,600 --> 00:14:19,330
You would assume that this is
how the chart should look like.

227
00:14:20,300 --> 00:14:23,450
However, this is not exactly the case.

228
00:14:23,810 --> 00:14:28,730
If you actually track duration,
error rate, any other performance

229
00:14:28,730 --> 00:14:32,420
metrics, it would look more of this.

230
00:14:32,900 --> 00:14:37,640
You would see that, yes, most of the time
they have the same response, the same

231
00:14:37,640 --> 00:14:40,020
duration of requests, the same error rate.

232
00:14:40,530 --> 00:14:45,840
Sometimes they diverge and one
organization becomes much slower or

233
00:14:45,840 --> 00:14:48,780
even return failures while others don't.

234
00:14:49,280 --> 00:14:55,820
And if you actually look on the status
pages of Tropic Open ai, any other

235
00:14:55,850 --> 00:14:59,660
AI provider which you would like,
you would see that it's not as green.

236
00:15:00,260 --> 00:15:05,090
As you would expect, and by the way, you
don't have to have major outages like

237
00:15:05,090 --> 00:15:07,460
the red plus spots, even the yellow ones.

238
00:15:07,700 --> 00:15:11,240
That means that some of the
functionality is not working or some

239
00:15:11,240 --> 00:15:13,610
over some delays or some timeouts.

240
00:15:13,940 --> 00:15:16,160
It's something that will,
your product will suffer.

241
00:15:16,400 --> 00:15:20,540
Need to see how you can manage
this, especially than not.

242
00:15:20,540 --> 00:15:25,070
All organizations usually in
open ai, suffer in the same

243
00:15:25,070 --> 00:15:26,760
way during those outages.

244
00:15:27,260 --> 00:15:31,910
As we know, this is how what happens,
our users actually expect that our

245
00:15:31,910 --> 00:15:34,100
system will be a hundred percent uptime.

246
00:15:34,370 --> 00:15:37,250
They don't care about open ai if
they don't care about outages and

247
00:15:37,250 --> 00:15:38,730
they don't care about the rate limit.

248
00:15:39,300 --> 00:15:43,200
They want the product that we're using,
even the product that we've purchased,

249
00:15:43,200 --> 00:15:45,570
for example, to work all the time.

250
00:15:46,425 --> 00:15:50,325
So in this scenario where we see we
have two different organizations and

251
00:15:50,415 --> 00:15:55,165
one of them is divergent becoming
worse while the green one still or

252
00:15:55,195 --> 00:15:56,575
operating in a sufficient level.

253
00:15:57,145 --> 00:16:01,345
What we would want is for our system
not to use the yellow organization,

254
00:16:01,705 --> 00:16:04,015
but to actually use only the green one.

255
00:16:04,825 --> 00:16:09,775
And this is how we've had an additional
layer for our multi-organ solution

256
00:16:10,195 --> 00:16:11,815
in order to make it more resilient.

257
00:16:12,580 --> 00:16:15,850
So let's say we have a client
request, it goes to our infra

258
00:16:16,030 --> 00:16:20,800
that has several organizations,
and let's say that one of those

259
00:16:20,800 --> 00:16:23,470
organizations hit a certain threshold.

260
00:16:24,100 --> 00:16:28,700
For example, it it hit their own rate
limit or for some reason it returns

261
00:16:28,700 --> 00:16:32,920
500 error code or any other request
within a certain time timeframe.

262
00:16:32,980 --> 00:16:35,800
And we see that it's not
something that just happens once.

263
00:16:36,100 --> 00:16:37,240
It happens three times.

264
00:16:37,840 --> 00:16:42,940
So what our solution is able to do is
to take this organization and remove it

265
00:16:43,180 --> 00:16:48,300
from the total pool of the organizations
market as a, let's say, sick organization

266
00:16:48,690 --> 00:16:50,400
and work only with the healthy one.

267
00:16:51,060 --> 00:16:56,100
In this scenario of the client will
never experience the failures that

268
00:16:56,100 --> 00:16:57,990
happen only in this organization.

269
00:16:58,440 --> 00:17:01,950
'cause if we would keep it there,
that means out of four organizations.

270
00:17:02,535 --> 00:17:05,535
Every one out of four
requests will get an error.

271
00:17:05,565 --> 00:17:06,345
And we don't want that.

272
00:17:06,465 --> 00:17:09,435
We wanna have as much
successful requests as possible.

273
00:17:10,275 --> 00:17:15,835
And only after a while we'll try to
reconnect to this organization again.

274
00:17:16,435 --> 00:17:21,035
And in case it works fine, we can
keep it and restore to normal.

275
00:17:21,575 --> 00:17:24,065
So this pattern is
called a circuit breaker.

276
00:17:24,665 --> 00:17:27,755
It's not something new, it's
something that actually comes

277
00:17:27,755 --> 00:17:28,805
from the electricity world.

278
00:17:29,315 --> 00:17:33,425
But basically what we've done, we've
extended the previous infrastructure

279
00:17:33,425 --> 00:17:36,365
solution by adding two additional parts.

280
00:17:36,695 --> 00:17:38,795
One is the Redis cache.

281
00:17:38,800 --> 00:17:43,200
We needed a distributed cache because as
many companies we work in Kubernetes, we

282
00:17:43,200 --> 00:17:47,225
have different pods and we could, we don't
want to make, and what we don't wanna.

283
00:17:47,725 --> 00:17:51,085
Let any, every pod suffer
from the same threshold.

284
00:17:51,415 --> 00:17:55,135
'cause if we know that organization
two, for example, is supposed to

285
00:17:55,135 --> 00:17:58,555
be out, we don't want it to heat a
threshold in one pod, and then the

286
00:17:58,555 --> 00:18:02,400
second pod, and then the third pod
want to heat it certain threshold and

287
00:18:02,400 --> 00:18:03,930
then be removed from all the pods.

288
00:18:04,410 --> 00:18:08,460
So the whole management of the
organization, cycles of actual

289
00:18:08,460 --> 00:18:10,470
pool is being managed inside.

290
00:18:10,470 --> 00:18:14,900
Ve And the circuit breaker
basically is the management system.

291
00:18:15,260 --> 00:18:20,240
That operates on top of all organization
and decides which organization should

292
00:18:20,240 --> 00:18:24,710
be removed from the pool of the health
organizations and when it should be

293
00:18:24,770 --> 00:18:26,900
returned and that shouldn't be returned.

294
00:18:27,400 --> 00:18:31,000
We connected it to our slack,
so we actually can see.

295
00:18:31,060 --> 00:18:35,800
It's very nice because think about all
the time, all those times where you had

296
00:18:35,800 --> 00:18:40,430
a certain resource, it wasn't behaving
in a good way and the developer had to.

297
00:18:41,270 --> 00:18:45,710
Manually deploy some code, do some
changes in order to remove it, and

298
00:18:45,710 --> 00:18:50,810
then set a reminder to the next
day to make bring it back here.

299
00:18:50,870 --> 00:18:52,460
Everything is done automatically.

300
00:18:52,700 --> 00:18:57,650
And when we wake up, we just see this
site of a log, this sort of a log in our

301
00:18:57,650 --> 00:19:00,050
slack where we hit a certain threshold.

302
00:19:00,450 --> 00:19:05,255
The circuit break is open, it removes a
certain organization and after a while.

303
00:19:05,540 --> 00:19:09,510
It returns it and no
engineer had to do anything.

304
00:19:09,900 --> 00:19:12,060
It just works in a very resilient way.

305
00:19:12,560 --> 00:19:15,680
So let's go back to the story
that I started this session.

306
00:19:15,810 --> 00:19:20,635
So two days before the global launch
of the site planner open AI locked our

307
00:19:20,635 --> 00:19:24,885
account and it means we couldn't pay for
additional tokens for additional credits,

308
00:19:25,275 --> 00:19:27,075
and we didn't have enough credits.

309
00:19:27,100 --> 00:19:30,480
So the system could not work with open ai.

310
00:19:30,980 --> 00:19:31,880
This is how it looked like.

311
00:19:31,880 --> 00:19:36,260
We wanted to top, top up the
balance, but for some reason we

312
00:19:36,260 --> 00:19:38,540
got this exception, this error.

313
00:19:38,570 --> 00:19:42,110
And obviously when we tried to contact
someone from support, no one answered

314
00:19:42,470 --> 00:19:46,940
and we just didn't have the time to
wait for others, so we had to act.

315
00:19:47,440 --> 00:19:51,070
And this is why we, from the
start, we knew that we didn't want

316
00:19:51,070 --> 00:19:52,450
to be in a vendor lock position.

317
00:19:52,990 --> 00:19:58,780
We didn't wanna be in a position where in,
in such case, if one of those AI vendors

318
00:19:59,200 --> 00:20:01,390
will fail, our product will not work.

319
00:20:01,750 --> 00:20:03,760
This is why we've started from day one.

320
00:20:03,850 --> 00:20:04,840
Also working with.

321
00:20:05,340 --> 00:20:06,630
Now there are some differences.

322
00:20:06,900 --> 00:20:12,150
So with immigration from OpenAI to cloud
is not as seamless as you would think

323
00:20:12,600 --> 00:20:16,820
because there are some functionalities
that OpenAI support supports that cloud

324
00:20:16,850 --> 00:20:22,310
doesn't like structured output, for
example, and defense validations and,

325
00:20:22,390 --> 00:20:24,010
the system prompts are not the same.

326
00:20:24,310 --> 00:20:28,395
You can have a prompt and then
use it in open AI and in cloud.

327
00:20:29,145 --> 00:20:32,175
In using the same system prompt,
you'll get different results.

328
00:20:32,595 --> 00:20:36,945
And by definition it's two different
models, so you had to adjust the

329
00:20:36,945 --> 00:20:40,465
system prompts a bit depending on the
provider, which you're working with.

330
00:20:40,965 --> 00:20:46,525
But this is, and although this sounds
so complicated, this is something.

331
00:20:46,930 --> 00:20:48,430
That is necessary.

332
00:20:48,760 --> 00:20:49,895
'cause if you're building a product.

333
00:20:50,590 --> 00:20:51,880
For millions of users.

334
00:20:52,360 --> 00:20:57,625
You cannot rely on any vendor that
for any reason will not be available.

335
00:20:57,625 --> 00:20:59,855
It might be out for whatever reason.

336
00:21:00,335 --> 00:21:03,480
And you just don't want to be
in a position saying to a users

337
00:21:03,480 --> 00:21:04,980
that your product is not working.

338
00:21:05,010 --> 00:21:06,585
'cause open AI is not working you.

339
00:21:06,780 --> 00:21:08,400
You gotta have a fallback.

340
00:21:08,700 --> 00:21:12,170
In our case, we have automation,
so we wanted to have a, a

341
00:21:12,440 --> 00:21:14,240
fallback in just one click.

342
00:21:14,900 --> 00:21:18,525
So we, so what it means that
we adjusted our prompts or

343
00:21:18,525 --> 00:21:19,935
system prompts to the provider.

344
00:21:20,145 --> 00:21:24,675
We knew exactly if we were moving from
open AI to cloud the whole system.

345
00:21:25,155 --> 00:21:26,145
Works the same.

346
00:21:26,595 --> 00:21:30,795
What changes is just with, instead
of calling the open ai, SDK, we're

347
00:21:30,795 --> 00:21:35,625
calling the SDK, which in this case
is Claude, and it then touches the

348
00:21:35,625 --> 00:21:39,885
correct system prompts and additional
instruction to make the responses

349
00:21:39,885 --> 00:21:42,465
be as close to open AI as possible.

350
00:21:42,885 --> 00:21:48,285
Yes, it's not ideal, but it's much
better than being out totally.

351
00:21:48,785 --> 00:21:50,735
So right now we actually have this.

352
00:21:51,275 --> 00:21:53,975
So this is not something that happens
automatically because it's a very rare

353
00:21:53,975 --> 00:21:58,985
scenario, but we do have a single key
in our system that once we change,

354
00:21:58,985 --> 00:22:04,055
it moves the entire system to work
against cloud and not against open ai.

355
00:22:04,295 --> 00:22:06,335
And then reverting it is exactly the same.

356
00:22:06,335 --> 00:22:11,485
And we are right now working on open
sourcing this framework for everyone.

357
00:22:11,755 --> 00:22:12,475
So stay tuned.

358
00:22:12,975 --> 00:22:16,845
Alright, takeaways from BI building
and being a resilient AI solution.

359
00:22:17,715 --> 00:22:22,635
Everyone need to remember, and it doesn't,
it has nothing to do with ai, but failure

360
00:22:22,755 --> 00:22:24,885
in software development is inevitable.

361
00:22:24,990 --> 00:22:30,520
It doesn't matter of if, it's a matter
of when Using a proven design partner,

362
00:22:30,610 --> 00:22:35,080
like a circuit breaker is a great
solution for being full tolerant.

363
00:22:35,580 --> 00:22:38,070
I think that, again, it really
depends on the scenario.

364
00:22:38,070 --> 00:22:41,970
It really depends on the level of support
and service that you wanna provide

365
00:22:41,970 --> 00:22:44,490
for your clients and the necessity.

366
00:22:44,490 --> 00:22:46,920
If it's a product that's
working once a day, it's fine.

367
00:22:47,130 --> 00:22:50,910
But if you working globally and you
wanna provide your user with sufficient

368
00:22:51,210 --> 00:22:56,880
coverage for the entire time, then I
would consider an additional vendor

369
00:22:56,970 --> 00:22:58,560
and doing whatever is necessary.

370
00:22:59,130 --> 00:23:01,490
To make sure it's easy
to switch between them.

371
00:23:01,880 --> 00:23:06,230
It's easy to switch between open AI to
cloud, to grok, to whatever vendor you

372
00:23:06,230 --> 00:23:11,540
want and without adding additional code or
changing stuff in the middle of the night.

373
00:23:12,040 --> 00:23:16,060
And another tip, it's not written
here, but it's to test, test.

374
00:23:16,450 --> 00:23:17,980
That means that it's one way.

375
00:23:18,340 --> 00:23:20,830
It's one thing to just
build it and it's ready.

376
00:23:21,250 --> 00:23:24,215
But every now and then practice.

377
00:23:24,715 --> 00:23:28,945
Try to change the provider and see
that everything is work, but the tests

378
00:23:29,005 --> 00:23:34,370
are hitting the limits that the most
important scenarios are passing and

379
00:23:34,430 --> 00:23:38,870
the system is operational because what
happens usually with fallbacks, the being

380
00:23:38,870 --> 00:23:42,140
left behind, and then it's very then.

381
00:23:43,090 --> 00:23:47,400
There is a difference between what
the main provider, how the system

382
00:23:47,400 --> 00:23:51,360
is working with the main provider
versus the fallback that's usually

383
00:23:51,360 --> 00:23:53,430
not as good as the main provider.

384
00:23:53,930 --> 00:23:58,040
Alright, the last subject that I want to
talk to you about today is how we create,

385
00:23:58,070 --> 00:24:00,650
how AI solutions need to continuously.

386
00:24:01,150 --> 00:24:06,940
So we, earlier we said that this is what
our users expect with our system to be

387
00:24:06,940 --> 00:24:09,160
a hundred percent up and always work.

388
00:24:09,460 --> 00:24:12,340
But actually this is what we expect.

389
00:24:12,610 --> 00:24:17,410
They expect that not only if the system
is up, they expect that everything

390
00:24:17,410 --> 00:24:21,380
works great to where to what they expect
from the system and how we use it.

391
00:24:21,880 --> 00:24:25,840
But how we even know that
we are doing a good job.

392
00:24:25,840 --> 00:24:28,840
How do we know that our system
is doing what our users want?

393
00:24:29,650 --> 00:24:32,990
So I bet many of you right
now are thinking about evolves

394
00:24:32,990 --> 00:24:34,220
and evaluation in general.

395
00:24:34,680 --> 00:24:35,520
And you're correct.

396
00:24:36,030 --> 00:24:38,970
And the way to understand if,
and the AI system is working

397
00:24:38,970 --> 00:24:40,200
correctly is by evaluation.

398
00:24:40,680 --> 00:24:45,840
And when we're talking about tax
generations, it's pretty straightforward.

399
00:24:46,260 --> 00:24:50,490
For example, back in the day
we started to evaluate diff our

400
00:24:50,490 --> 00:24:52,600
different results in the text area.

401
00:24:52,990 --> 00:24:58,180
So when we have our users wanted
to change their text of a button

402
00:24:58,180 --> 00:25:00,820
widget, for example, we suddenly saw.

403
00:25:01,210 --> 00:25:05,470
That the AI was generating
huge responses back.

404
00:25:06,100 --> 00:25:09,570
You can see here it's a
paragraph right on the button.

405
00:25:09,660 --> 00:25:12,750
It makes no sense that this type
of text should be on the button.

406
00:25:13,350 --> 00:25:17,615
So once we understood that those
scenarios exist, we created, we had

407
00:25:18,065 --> 00:25:24,425
evaluations to ensure that texts that
are being, that AI should generate four.

408
00:25:24,795 --> 00:25:29,445
Button widget, for example, would be
different in length than a text for a

409
00:25:29,445 --> 00:25:32,235
heading and a text for a full paragraph.

410
00:25:32,985 --> 00:25:35,715
But for texts it's pretty straightforward.

411
00:25:36,255 --> 00:25:40,195
You put text in LLM provides
text out and that's it.

412
00:25:41,005 --> 00:25:43,615
But how do you evaluate images?

413
00:25:44,125 --> 00:25:47,860
'cause it's not only that a user asks for
image of a dog and where you see the dog.

414
00:25:48,625 --> 00:25:53,605
Does the dog look okay as it
has four legs and just one head?

415
00:25:54,145 --> 00:25:58,345
Is it actually walking or is it running
where not only the functionality

416
00:25:58,435 --> 00:26:01,765
that's supposed to work, but also
a certain amount of taste, right?

417
00:26:02,095 --> 00:26:04,805
'cause some, someone would, might
look on a picture and say it's

418
00:26:04,805 --> 00:26:08,105
good picture, but someone else is,
was envisioning something else.

419
00:26:08,495 --> 00:26:14,255
So it's hard to evaluate images and it's
much harder to evaluate a whole structure

420
00:26:14,315 --> 00:26:16,535
of a website, of a page, because.

421
00:26:16,970 --> 00:26:20,360
You would, a user can ask for a
about me page and will have seven

422
00:26:20,360 --> 00:26:25,500
different sections, but the user
might have a different idea of how

423
00:26:25,500 --> 00:26:26,790
this page should be structured.

424
00:26:27,090 --> 00:26:30,600
So how should we evaluate
this type of interaction?

425
00:26:31,350 --> 00:26:36,550
So in order to do that, we didn't we
had to re move away from the traditional

426
00:26:36,550 --> 00:26:38,540
evil of financial evaluations.

427
00:26:38,630 --> 00:26:40,550
And we've defined a success metric.

428
00:26:41,030 --> 00:26:42,440
We called it insert rate.

429
00:26:43,100 --> 00:26:47,750
Basically, for many of our features,
the user had to enter a prompt,

430
00:26:48,200 --> 00:26:52,770
then see a preview of a certain a
preview of a certain result, and

431
00:26:52,770 --> 00:26:55,080
only if the user would use the image.

432
00:26:55,440 --> 00:26:59,760
We would mark that this specific
interaction was successful.

433
00:27:00,120 --> 00:27:03,715
If, for example, we would click on
generate again, or even close the page.

434
00:27:04,430 --> 00:27:08,630
We would know that this interaction
ended with an empty result and we

435
00:27:08,630 --> 00:27:11,090
would mark it as inserted false.

436
00:27:11,630 --> 00:27:17,780
Then we would keep all of this data in a
dedicated database where we would see the

437
00:27:17,780 --> 00:27:23,950
user actual input, the enhanced prompt
that, as AI engineers we are enhancing

438
00:27:23,950 --> 00:27:26,420
prompts and providing additional context.

439
00:27:26,420 --> 00:27:29,720
But we wanted to list
everything in a very clear way.

440
00:27:30,515 --> 00:27:34,235
Then we had the full prompt with the
system prompts and the additional, and

441
00:27:34,235 --> 00:27:40,455
basically everything that eventually goes
to the LM, the result that gets gets back.

442
00:27:40,955 --> 00:27:44,675
And the indication, whether it
was inserted or not, all this

443
00:27:44,675 --> 00:27:48,215
information was stored for every
interaction inside our database.

444
00:27:48,715 --> 00:27:50,015
On top of that, what we started.

445
00:27:50,855 --> 00:27:53,675
We started with just manually
going over the results.

446
00:27:53,735 --> 00:27:57,065
We would fetch the information of
a day, of the week of the month.

447
00:27:57,665 --> 00:28:01,745
We started first with just trying
to understand what's wrong with

448
00:28:01,745 --> 00:28:06,575
the ones that were in, not inserted
when we asked the GPT for help.

449
00:28:06,575 --> 00:28:11,785
We would add provide this information in
an anonymized way into check GPT and ask

450
00:28:11,785 --> 00:28:14,795
it to see if it sees any similarities.

451
00:28:15,270 --> 00:28:18,090
And eventually develop
our own clustering job.

452
00:28:18,810 --> 00:28:23,730
We even have an article right here,
have a QR code, but I invite you to scan

453
00:28:23,790 --> 00:28:29,300
and read how we've developed basically
offline jobs that are a, that are AI

454
00:28:29,300 --> 00:28:31,760
powered that go through this data.

455
00:28:32,170 --> 00:28:38,815
Create different clusters and find
anomalies, find those requests that

456
00:28:38,815 --> 00:28:41,275
have some common ground and they failed.

457
00:28:41,815 --> 00:28:45,190
A great example that I love to
share is what we've noticed.

458
00:28:46,065 --> 00:28:53,155
That we've noticed that by we, our AI
job noticed that several image generation

459
00:28:53,155 --> 00:28:55,435
requests were not being inserted.

460
00:28:55,825 --> 00:28:58,855
And the AI actually once analyzed that.

461
00:28:59,215 --> 00:29:04,285
It's suggests that all of those
requests that had transparent

462
00:29:04,285 --> 00:29:08,575
background actually got rejected,
and the reason was so simple.

463
00:29:09,445 --> 00:29:13,945
The AI model that we were working on
the image generation was not, didn't

464
00:29:13,945 --> 00:29:18,955
support the transparent backgrounds, and
therefore all the images always returned

465
00:29:18,955 --> 00:29:22,490
with, with not transparent background,
with a full background, and users

466
00:29:22,580 --> 00:29:24,200
didn't want that, so we rejected it.

467
00:29:24,860 --> 00:29:29,450
So the AI actually here, once scanning
the information, clustered them correctly,

468
00:29:29,720 --> 00:29:33,785
understood the reason, and even suggested,
suggested the reason for this failure.

469
00:29:34,245 --> 00:29:35,895
The solution, by the way, was very simple.

470
00:29:35,955 --> 00:29:37,725
No, we didn't change payment provider.

471
00:29:37,725 --> 00:29:41,925
We just added a hint that once we
saw in the input that the user was

472
00:29:41,925 --> 00:29:45,855
typing something about transparent,
we also automatically hinted and

473
00:29:45,855 --> 00:29:48,675
say, Hey, we're not supporting
transparent images at the moment.

474
00:29:49,275 --> 00:29:49,905
And that's it.

475
00:29:50,205 --> 00:29:53,955
We aligned expectations with the
users and they were much happier.

476
00:29:54,915 --> 00:29:58,155
Here you can see that every, basically
every week we would get automatic

477
00:29:58,155 --> 00:30:03,355
responses in Slack that would get,
all those clustering informations with

478
00:30:03,355 --> 00:30:07,395
different suggestions and different
insights of fa potential failure reasons.

479
00:30:07,895 --> 00:30:11,255
And yeah, as I mentioned, it found real
issues with transparent background that

480
00:30:11,255 --> 00:30:14,945
I just mentioned, the text length of
the buttons that I mentioned before.

481
00:30:15,245 --> 00:30:19,895
Even that our Japanese understanding
was not as good as we thought it was.

482
00:30:20,495 --> 00:30:24,845
And yeah, many more examples that
manually, especially for large scale

483
00:30:24,845 --> 00:30:26,825
application, it's just not visible.

484
00:30:26,945 --> 00:30:29,225
So you have to get some
sort of automation.

485
00:30:29,615 --> 00:30:29,945
On.

486
00:30:30,095 --> 00:30:33,455
So first you would want to persist
with the data, with all the details,

487
00:30:33,845 --> 00:30:37,120
get some sort of information to scan
this data on a repetitive basis,

488
00:30:37,510 --> 00:30:40,750
and use Ally AI to get insights.

489
00:30:41,250 --> 00:30:42,870
Yeah, this is exactly the takeaways.

490
00:30:42,870 --> 00:30:47,130
Persisted data, define a success
metric, measure a success metric, and

491
00:30:47,130 --> 00:30:49,350
then pick a way to evaluate success.

492
00:30:49,410 --> 00:30:51,540
Always start manually,
always start simple.

493
00:30:51,540 --> 00:30:56,250
And only once you fine tune the metric
and the data and everything works, then

494
00:30:56,250 --> 00:30:58,350
you can move to a more automatic stuff.

495
00:30:58,710 --> 00:31:01,260
And also you leverage AI for your needs.

496
00:31:02,040 --> 00:31:04,200
To summarize this session first.

497
00:31:04,695 --> 00:31:09,435
Not every system needs to be fully
resilient and support huge rate

498
00:31:09,435 --> 00:31:15,165
limits, identify first the need for
scalability, then design the solution.

499
00:31:15,495 --> 00:31:19,605
So if it's rate limit, we've talked
about adding additional organizations,

500
00:31:19,875 --> 00:31:23,235
maybe changing to a different model,
then a different provider that has

501
00:31:23,295 --> 00:31:28,360
a larger context window or a larger
rate limit or, and but in many cases.

502
00:31:29,085 --> 00:31:33,015
It'll be just adding more resources
and then controlling them in a healthy

503
00:31:33,075 --> 00:31:35,405
in a healthy and a resilient layer.

504
00:31:35,725 --> 00:31:39,415
Using something like a circuit breaker
to identify one, once a certain

505
00:31:39,415 --> 00:31:43,615
organization isn't good enough, move it
away and work only with the good ones

506
00:31:44,515 --> 00:31:49,585
and eventually establish a continuous
improvement mechanism will help your

507
00:31:49,585 --> 00:31:52,435
product and your AI solution be better.

508
00:31:52,935 --> 00:31:55,515
So I hope you, you enjoyed this session.

509
00:31:55,665 --> 00:31:58,335
It was a pleasure
speaking to review today.

510
00:31:59,155 --> 00:32:02,095
May the AI be review right here.

511
00:32:02,125 --> 00:32:07,225
You can scan this barcode and see,
get the presentation for this talk

512
00:32:07,524 --> 00:32:10,885
along with additional presentations
that I've been shared in the past.

513
00:32:11,155 --> 00:32:15,024
With different recordings and different
materials, different block posts that

514
00:32:15,024 --> 00:32:19,095
I mentioned in this talk and in other
talks, it's everything the same place.

515
00:32:19,095 --> 00:32:21,535
So it's easy for you to
easy for you to check.

516
00:32:21,805 --> 00:32:23,665
And also it has my contact details there.

517
00:32:23,665 --> 00:32:26,965
So if you want to chat more
about ai, feel free, curious.

