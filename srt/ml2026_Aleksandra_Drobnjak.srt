1
00:00:00,240 --> 00:00:00,870
Speaker: Hi everyone.

2
00:00:01,290 --> 00:00:07,350
Welcome to my session, minimum
Viable, A IMS ISO 42 0 0 1 Required

3
00:00:07,350 --> 00:00:09,600
Documentation Artifacts for beginners.

4
00:00:10,140 --> 00:00:13,800
And thank you to the com 42
organizers for having me here today.

5
00:00:14,220 --> 00:00:17,100
I'm really glad to be part
of this lops focused edition.

6
00:00:18,030 --> 00:00:24,900
My session is built around how I saw IC 42
0 0 1 is actually implemented and audited,

7
00:00:25,440 --> 00:00:27,480
not just how it's written in the standard.

8
00:00:28,245 --> 00:00:32,894
On whether an artificial intelligence
management system truly exists in practice

9
00:00:33,254 --> 00:00:37,535
and whether that can be proved with
quality evidence Today, when you hear

10
00:00:37,535 --> 00:00:43,144
me say minimum viable, A IMS know I'm
talking about the minimum set of required

11
00:00:43,144 --> 00:00:48,695
documentation artifacts let auditors
expect to review, to conclude that an

12
00:00:48,695 --> 00:00:53,794
organization is managing AI systems
in a controlled and repeatable way.

13
00:00:54,560 --> 00:00:58,220
This is not about creating the
perfect documentation, it's about

14
00:00:58,220 --> 00:01:03,290
creating clarity around business risk,
ownership change, and accountability,

15
00:01:03,919 --> 00:01:05,600
meaning auditable clarity.

16
00:01:06,020 --> 00:01:10,130
So AI systems can scale without
crumbling as individuals change.

17
00:01:10,910 --> 00:01:12,919
And that's what I'll
walk you through today.

18
00:01:13,100 --> 00:01:14,509
So let's get started.

19
00:01:15,009 --> 00:01:19,630
A bit of context on why I'm talking on
this topic from this particular angle.

20
00:01:20,365 --> 00:01:24,115
So my background is a combination
of hands-on security, engineering,

21
00:01:24,595 --> 00:01:26,125
and formal audit training.

22
00:01:26,964 --> 00:01:31,585
I spent over 10 years across various
engineering roles, including system and

23
00:01:31,585 --> 00:01:36,115
solution engineering, multi-cloud, and
securing containerized environments.

24
00:01:36,655 --> 00:01:40,914
Along the way, I worked extensively
with vulnerability management, threat

25
00:01:40,914 --> 00:01:45,505
detection, monitoring, incident
response, and what's most relevant

26
00:01:45,505 --> 00:01:47,695
for us today here, compliance.

27
00:01:47,919 --> 00:01:52,809
Over time, a large part of my security
engineering role has involved supporting

28
00:01:52,809 --> 00:01:58,449
internal and external audits, preparing
reports, gathering evidence, and

29
00:01:58,449 --> 00:02:03,219
coordinating improvement efforts for
management, self-assessment of controls,

30
00:02:03,820 --> 00:02:06,399
as well as I saw and SOC two audits.

31
00:02:06,899 --> 00:02:10,164
From a security engineering
perspective, there's also very

32
00:02:10,164 --> 00:02:11,574
practical connection to lops.

33
00:02:12,074 --> 00:02:17,864
For example, the Cloud Security Alliance
outlines key stakeholders across the ML

34
00:02:17,864 --> 00:02:23,024
ops lifecycle, and security engineers
are explicitly named as one of them.

35
00:02:23,774 --> 00:02:24,734
Among others.

36
00:02:25,064 --> 00:02:30,494
We're expected to plan and act on threat
modeling, data protection, access control,

37
00:02:30,854 --> 00:02:37,869
and incident response For ML systems,
I'm also trained as an ISO 2 7 0 0 1.

38
00:02:38,624 --> 00:02:45,584
And I saw 42 0 0 1 lead auditor
through Mastermind Assurance and I'm a

39
00:02:45,584 --> 00:02:47,804
certified information systems auditor.

40
00:02:48,044 --> 00:02:50,264
CSA exam Passer through.

41
00:02:50,764 --> 00:02:54,994
I worked with GRC platforms
like to operationalize auditor

42
00:02:54,994 --> 00:02:59,764
evidence as well as with Microsoft
Purview data protection solution.

43
00:03:00,544 --> 00:03:05,074
Outside of day-to-day work, I'm
nowadays active in the community

44
00:03:05,134 --> 00:03:07,024
as an AWS community builder.

45
00:03:07,819 --> 00:03:12,619
A woman in cybersecurity business
mentor and hack the box meetup host.

46
00:03:13,159 --> 00:03:16,640
I also enjoy publishing blog
posts and practical walkthroughs

47
00:03:16,640 --> 00:03:19,989
on my YouTube channel, which is
the large image on this slide.

48
00:03:20,799 --> 00:03:24,339
So ultimately, this talk comes
directly from the intersection

49
00:03:24,399 --> 00:03:29,079
of all of this making systems
operable, defensible, and auditable.

50
00:03:29,874 --> 00:03:33,954
I'm here as someone who's had to make
governance work in practice without

51
00:03:34,044 --> 00:03:38,154
unnecessarily slowing teams down,
while focusing on prevention, early

52
00:03:38,154 --> 00:03:44,094
detection, and using tactical pauses to
reassess and stabilize so the business

53
00:03:44,094 --> 00:03:46,404
comes out stronger and more resilient.

54
00:03:46,974 --> 00:03:48,954
And that's the mindset I bring today.

55
00:03:49,454 --> 00:03:53,474
Now let's briefly level set on
what an artificial intelligence

56
00:03:53,744 --> 00:03:56,564
management system A IMS actually is.

57
00:03:57,104 --> 00:04:02,054
And what I saw, IC 42 0
0 1 brings to the table.

58
00:04:02,954 --> 00:04:05,534
I saw IC 42 0 0 1.

59
00:04:05,714 --> 00:04:11,744
Moving forward, I short it to, I saw
42 0 0 1 is a voluntary certifiable

60
00:04:11,864 --> 00:04:16,064
international standard, published in 2023.

61
00:04:16,564 --> 00:04:21,094
And it's the first one focused
specifically on managing AI systems

62
00:04:21,394 --> 00:04:22,954
at the organizational level.

63
00:04:23,914 --> 00:04:26,494
It doesn't prescribe models or tooling.

64
00:04:27,154 --> 00:04:31,324
Instead, it shifts the focus
away from individual models and

65
00:04:31,324 --> 00:04:36,694
toward how AI systems are owned,
monitored, and improved over time.

66
00:04:37,194 --> 00:04:41,994
It takes an organizational view
of AI focusing on how systems are

67
00:04:41,994 --> 00:04:46,614
governed across their life cycle
from experimentation and deployment

68
00:04:46,674 --> 00:04:52,530
to use monitoring and continuous
improvement Structurally, ISO 42 0

69
00:04:52,530 --> 00:04:55,424
0 1 contains 10 clauses in total.

70
00:04:56,414 --> 00:05:00,369
This clauses four through 10
defining the auditable requirements.

71
00:05:01,319 --> 00:05:06,419
The documentation artifacts I'll
walk through are derived from the

72
00:05:06,419 --> 00:05:12,059
auditable clauses four through 10,
and all clauses are supported by

73
00:05:12,119 --> 00:05:19,049
NXA, which outlines 38 controls
total across nine control objectives.

74
00:05:19,889 --> 00:05:24,379
Importantly, not all controls are
mandatory organizations select

75
00:05:24,379 --> 00:05:29,389
applicable controls based on their
risk profile, scope and context.

76
00:05:29,889 --> 00:05:36,699
Standard follows the plan, do check ACT
PDCA cycle, which means it's designed to

77
00:05:36,699 --> 00:05:42,849
mesh well with other management system
standards, especially is O 27 0 0 1.

78
00:05:43,239 --> 00:05:45,549
Information Security Management System.

79
00:05:46,059 --> 00:05:46,900
ISMS.

80
00:05:47,829 --> 00:05:50,944
Auditors are not looking for
perfection or automation everywhere.

81
00:05:51,215 --> 00:05:55,905
They're looking for consistency,
intent, and evidence that controls

82
00:05:55,924 --> 00:05:57,549
are applied relative to risk.

83
00:05:58,049 --> 00:06:02,489
ASSO standards commonly describe
maturity, progressing from incomplete

84
00:06:02,909 --> 00:06:09,539
to performed, managed, established,
predictable, and eventually optimized.

85
00:06:10,169 --> 00:06:14,609
The goal is not to jump to the highest
level immediately, but to demonstrate

86
00:06:14,609 --> 00:06:16,335
controlled progress over time.

87
00:06:16,335 --> 00:06:22,229
ASSO 42 0 0 1 also serves as
a standalone within a broader

88
00:06:22,229 --> 00:06:25,020
ecosystem of AI focused standards.

89
00:06:25,679 --> 00:06:32,339
For example, supporting standard ISO
2 2 9 8 9 defines AI terminology and

90
00:06:32,339 --> 00:06:37,829
roles such as providers, developers,
and suppliers, which is why ISO

91
00:06:37,829 --> 00:06:43,859
4 2 0 0 1 is relevant even if you
consume ai, rather than building it

92
00:06:44,789 --> 00:06:50,974
and supporting standard ISO 42 0 0 5
addresses AI system impact assessments.

93
00:06:51,474 --> 00:06:54,474
From a certification
perspective, what it follow?

94
00:06:54,474 --> 00:06:58,314
A clear chain international
accreditation forum.

95
00:06:58,944 --> 00:07:04,164
Two accreditation bodies,
two certification bodies,

96
00:07:04,764 --> 00:07:06,924
two oddity organizations.

97
00:07:07,104 --> 00:07:12,294
So I have two abs to CCBs, to Oddity.

98
00:07:12,794 --> 00:07:16,004
On I have searched search
the global database for

99
00:07:16,004 --> 00:07:18,164
accredited ISO certifications.

100
00:07:18,644 --> 00:07:22,634
You can look up whether an organization
certificate is still valid.

101
00:07:23,134 --> 00:07:27,814
Certification typically involves passing
a stage one documentation readiness audit.

102
00:07:28,684 --> 00:07:33,604
Out by stage two, control effectiveness
audit, and then maintaining and

103
00:07:33,604 --> 00:07:37,774
improving the system through
annual surveillance audits within

104
00:07:37,864 --> 00:07:39,874
a three year certification cycle.

105
00:07:40,444 --> 00:07:47,104
Finally, I saw 42 0 0 1 exists alongside
other major AI governance frameworks

106
00:07:47,524 --> 00:07:53,584
to highlight a few USA's NIST AI Risk
Management framework, which provides

107
00:07:53,584 --> 00:07:56,584
voluntary guidance but is not certifiable.

108
00:07:57,084 --> 00:08:03,534
Use AI Act introduces mandatory legal
applications for certain AI systems,

109
00:08:04,034 --> 00:08:10,304
and I saw 42 0 0 1 here serves
organizations globally to proactively

110
00:08:10,544 --> 00:08:16,184
and voluntarily demonstrate AI readiness
through independent verification.

111
00:08:16,684 --> 00:08:19,984
And this is increasingly relevant
because people are experimenting

112
00:08:19,984 --> 00:08:22,264
with ML and AI everywhere.

113
00:08:23,090 --> 00:08:27,469
While intentioned enthusiasts can
unintentionally create shadow AI

114
00:08:27,950 --> 00:08:34,959
using corporate data on corporate
devices, ISO 42 0 0 1 provides a

115
00:08:34,959 --> 00:08:38,934
structured way to bring that activity
back into visibility and control.

116
00:08:39,434 --> 00:08:40,635
One final note here.

117
00:08:41,025 --> 00:08:45,375
For copyright reasons, I'm not displaying
the standard itself as it must be

118
00:08:45,375 --> 00:08:47,355
purchased from the official ISO store.

119
00:08:47,925 --> 00:08:53,084
Instead, I'll focus on explaining why the
required documentation artifacts exist and

120
00:08:53,084 --> 00:08:55,694
how they connect to real world challenges

121
00:08:56,194 --> 00:08:58,670
and why these matters right now.

122
00:08:59,600 --> 00:09:02,480
Recent industry data
shows a clear pattern.

123
00:09:02,980 --> 00:09:07,449
A majority of organizations that
experienced breaches either don't

124
00:09:07,449 --> 00:09:12,690
have an AI policy at all or are
still in the process of defining one.

125
00:09:13,590 --> 00:09:19,110
Even among the organizations that do have
policies, only a small fraction regularly

126
00:09:19,140 --> 00:09:25,020
audit for and sanctioned or shadow
AI use and the impact is measurable.

127
00:09:25,620 --> 00:09:29,550
One in five organizations has
reported a bridge linked to shadow ai.

128
00:09:30,050 --> 00:09:33,950
And when these kinds of incidents
happen, the damage can be larger.

129
00:09:34,790 --> 00:09:40,130
Incident involving shadow AI tend
to explore more sensitive data and

130
00:09:40,190 --> 00:09:44,480
intellectual property, and they
come with significantly higher

131
00:09:44,480 --> 00:09:49,460
remediation and reputation costs
and traditional security incidents.

132
00:09:49,960 --> 00:09:54,579
At the same time, we're seeing a
rise in public cases related to

133
00:09:54,579 --> 00:09:57,939
unfair or discriminatory AI outcomes.

134
00:09:58,630 --> 00:10:04,720
The AI discrimination case tracker,
those legal cases where AI systems

135
00:10:04,840 --> 00:10:10,870
are relevant to discrimination bias,
or equality issues, particularly in

136
00:10:10,870 --> 00:10:17,949
areas like hiring credit decisioning,
and even arrests caused by inaccurate

137
00:10:17,949 --> 00:10:19,840
facial recognition systems.

138
00:10:20,340 --> 00:10:24,540
These cases rarely come down
to a single bad line of code.

139
00:10:25,320 --> 00:10:31,110
Instead, they expose gaps in
oversight testing, monitoring, and

140
00:10:31,110 --> 00:10:33,870
accountability across the AI lifecycle.

141
00:10:34,680 --> 00:10:37,140
This is one of the key problems I saw.

142
00:10:37,140 --> 00:10:40,860
42 0 0 1 is designed to address.

143
00:10:41,430 --> 00:10:46,255
It doesn't exist to slow innovation
or block experimentation,

144
00:10:47,220 --> 00:10:51,930
and it doesn't guarantee that
incidents can't and won't happen.

145
00:10:52,635 --> 00:10:58,605
What it does provide is operational
clarity that strategically prioritizes the

146
00:10:58,605 --> 00:11:05,324
response when things go wrong and gives
stakeholders confidence that AI systems

147
00:11:05,355 --> 00:11:12,255
are being managed maturely over time, that
translates into more efficient operations

148
00:11:12,645 --> 00:11:14,709
and stronger trust from customers.

149
00:11:15,209 --> 00:11:21,540
So 42 0 0 1 focuses on how organizations
govern and respond to AI risks in

150
00:11:21,540 --> 00:11:27,239
practice, and it also works alongside
technical frameworks like OS Machine

151
00:11:27,239 --> 00:11:33,869
Learning Security Top 10 and MI Atlas,
which help teams understand what can

152
00:11:33,869 --> 00:11:39,119
go wrong at the technical level, such
as data poisoning, input manipulation,

153
00:11:39,209 --> 00:11:41,489
and AI ML supply chain attacks.

154
00:11:41,989 --> 00:11:44,630
And there is an upside
beyond the risk reduction.

155
00:11:45,229 --> 00:11:50,329
Organizations that implement structured
AI governance report tangible benefits.

156
00:11:50,900 --> 00:11:55,819
The so 42 0 0 1 batch on product
marketing pages draws attention,

157
00:11:56,540 --> 00:12:01,670
increases credibility in the eyes of
prospects, shortens pre-sales and due

158
00:12:01,670 --> 00:12:06,260
diligence questionnaires, and help
move conversations toward monetary

159
00:12:06,260 --> 00:12:08,469
investment quicker, as an example.

160
00:12:09,099 --> 00:12:13,869
Several AI powered services you
may likely use today operate within

161
00:12:13,869 --> 00:12:15,639
certified management Systems.

162
00:12:16,359 --> 00:12:22,569
As an AWS Community builder, I would
highlight ISO 42 0 0 1 certified business

163
00:12:22,569 --> 00:12:29,379
units behind services like Amazon
Tech Extract, which is ML service that

164
00:12:29,379 --> 00:12:31,899
extracts data from scanned documents.

165
00:12:31,899 --> 00:12:35,829
Then Amazon transcribe for speech to text.

166
00:12:36,489 --> 00:12:42,069
Amazon Bedrock for building gen AI
applications and Amazon Q Business

167
00:12:42,189 --> 00:12:44,289
as an enterprise AI assistant.

168
00:12:45,129 --> 00:12:51,399
As a small site quest, you can access this
ISO certificate through pre-service a WS

169
00:12:51,399 --> 00:12:58,355
artifact, grab the certification number
and then verify yourself on IF search.

170
00:12:58,855 --> 00:13:01,709
It takes just a couple of
minutes, and it's a great way to

171
00:13:01,709 --> 00:13:03,390
see how this works in practice.

172
00:13:04,185 --> 00:13:08,055
Feel free to reach out to me on LinkedIn
afterward and tell me how it went.

173
00:13:08,145 --> 00:13:11,204
I'm genuinely curious
who actually tries it.

174
00:13:12,074 --> 00:13:15,915
Ultimately, the longstanding
case for standardization still

175
00:13:15,915 --> 00:13:18,615
applies to AI management systems.

176
00:13:19,425 --> 00:13:24,405
As AI systems scale, the cost of
not knowing, not measuring and

177
00:13:24,405 --> 00:13:29,535
not improving grows faster than
the cost of practic planning and

178
00:13:29,535 --> 00:13:31,515
putting the right controls in place.

179
00:13:32,015 --> 00:13:37,385
Now zooming into the artifacts in lead
auditor training, the requirements

180
00:13:37,385 --> 00:13:42,665
of ISO 42 0 0 1 are often distilled
into required activities and

181
00:13:42,665 --> 00:13:44,855
required documentation artifacts.

182
00:13:45,515 --> 00:13:49,895
Today I'm focusing on the documentation
side, and you can think of these

183
00:13:49,895 --> 00:13:56,495
16 items on this slide as 16 points
where clarity is required across the

184
00:13:56,555 --> 00:13:59,945
auditable clauses four through 10, four.

185
00:14:00,740 --> 00:14:05,450
Context of the organization,
plus five focused on leadership,

186
00:14:06,050 --> 00:14:08,120
plus six on planning.

187
00:14:08,630 --> 00:14:17,360
Seven, support eight operations, nine on
performance evaluation, and 10 focused on

188
00:14:17,360 --> 00:14:20,900
improvement or C-L-P-O-P as an acronym.

189
00:14:21,710 --> 00:14:26,030
So context leadership planning,
support operations, performance

190
00:14:26,030 --> 00:14:28,580
evaluation improvement, CLP.

191
00:14:28,595 --> 00:14:34,490
So P. Each one exists because without it,
something important tends to fall apart.

192
00:14:34,990 --> 00:14:38,290
As I go through them, I want to
show full documents today since

193
00:14:38,290 --> 00:14:40,030
they can get quite lengthy.

194
00:14:40,660 --> 00:14:45,310
Instead, I'll highlight their significance
from an audit perspective and connect

195
00:14:45,310 --> 00:14:47,410
each one to the real world problems.

196
00:14:47,410 --> 00:14:52,795
Teams run into one AI
ms. Scope, clause 4.3.

197
00:14:53,604 --> 00:14:56,635
Defining scope answers
are very simple questions.

198
00:14:57,234 --> 00:14:58,854
What exactly are we going?

199
00:14:59,354 --> 00:15:03,854
This artifact exists for one reason
to make sure everyone understands

200
00:15:03,854 --> 00:15:08,834
which AI systems are governed,
which are not, who is responsible

201
00:15:08,834 --> 00:15:10,214
before something goes wrong.

202
00:15:10,714 --> 00:15:14,644
This artifact depends heavily
on two things, organizational

203
00:15:14,644 --> 00:15:18,185
context and AI use cases.

204
00:15:18,454 --> 00:15:23,829
This specific organization identified
without a dose scope becomes guesswork.

205
00:15:24,784 --> 00:15:29,554
In ML Ops, this problem usually appears
the moment experimentation gets popular.

206
00:15:30,094 --> 00:15:35,374
A model starts as an notebook, someone
connects it to a pipeline, and suddenly

207
00:15:35,434 --> 00:15:40,114
it's influencing final workflows
before anyone has defined whether

208
00:15:40,114 --> 00:15:41,854
it belongs inside governance or not.

209
00:15:42,354 --> 00:15:46,854
Very public example of this kind of
situation involves engineers based

210
00:15:47,094 --> 00:15:51,774
proprietary material into unmanaged
AI tools of their personal choice.

211
00:15:52,284 --> 00:15:56,414
During daily work, rarely, let's
say there is malicious intent.

212
00:15:57,194 --> 00:16:02,114
Often it's just an attempt to speed
up repetitive tasks or analyze

213
00:16:02,114 --> 00:16:06,659
large spreadsheets with AI so it
doesn't look like an tech at all.

214
00:16:07,159 --> 00:16:10,879
Organizations need to clearly
define which AI tools are

215
00:16:10,879 --> 00:16:13,159
approved for corporate workflows.

216
00:16:13,609 --> 00:16:16,999
What data can be used
where experimentation,

217
00:16:17,059 --> 00:16:18,799
ads and production begins?

218
00:16:19,309 --> 00:16:23,239
And what happens when acceptable
use rules aren't followed?

219
00:16:23,779 --> 00:16:27,109
'cause without that clarity,
governance only exists on paper.

220
00:16:27,609 --> 00:16:33,399
Very often individual unmanaged AI
accounts come with default data control

221
00:16:33,399 --> 00:16:39,189
settings that allow user content to be
used for model improvement, sometimes

222
00:16:39,189 --> 00:16:41,829
phrased as improve the model for everyone.

223
00:16:42,329 --> 00:16:46,410
These settings may not align with
corporate governance expectations

224
00:16:46,739 --> 00:16:50,459
if they're used on business
devices or with company data.

225
00:16:50,959 --> 00:16:54,409
That's why many organizations
prefer centrally managed enterprise

226
00:16:54,409 --> 00:17:00,649
plans or tools like Chat, GPT,
copilot, cloud, and others.

227
00:17:01,489 --> 00:17:05,359
These enterprise plans typically
include stronger governance features,

228
00:17:05,869 --> 00:17:10,129
role-based access, audit logging,
custom data retention policies.

229
00:17:10,629 --> 00:17:15,789
Content safety, guardrails and
centralized administration, making it

230
00:17:15,789 --> 00:17:20,169
much easier to align with corporate
security and regulatory requirements

231
00:17:20,679 --> 00:17:22,869
without a clearly defined scope.

232
00:17:23,349 --> 00:17:27,009
Teams naturally drift toward
whichever tool feels most

233
00:17:27,009 --> 00:17:29,949
convenient, even if it's default.

234
00:17:29,949 --> 00:17:33,279
Governance model doesn't
match organizational goals.

235
00:17:33,779 --> 00:17:38,099
In many companies, acceptable
use guidelines, even recommend

236
00:17:38,129 --> 00:17:42,359
avoiding unmanaged personal
accounts on corporate equipment.

237
00:17:42,839 --> 00:17:47,399
So it's always worth checking with
internal points of contact before

238
00:17:47,399 --> 00:17:49,589
self choosing an AI tool for work.

239
00:17:50,519 --> 00:17:54,959
We've also seen real infrastructure
issues where lab environments were exposed

240
00:17:54,959 --> 00:18:00,719
externally or experimental tooling and
adapt reachable from production networks.

241
00:18:01,574 --> 00:18:03,674
These weren't sophisticated AI attacks.

242
00:18:04,124 --> 00:18:08,924
They were ordinary infrastructure
mistakes applied to very powerful systems.

243
00:18:09,644 --> 00:18:16,004
This configuration exposure drift and
unowned pipelines are still more common

244
00:18:16,004 --> 00:18:18,734
than advanced adversarial techniques.

245
00:18:19,604 --> 00:18:24,189
From an audit perspective, this is
exactly where the scope artifact helps.

246
00:18:25,169 --> 00:18:30,179
Speed forces explicit scoping of
which ML tools may run in production,

247
00:18:30,599 --> 00:18:35,849
and which environments require
authentication, logging and approvals.

248
00:18:36,749 --> 00:18:42,119
Exposures involving platforms like Jupyter
Cube Flow or ML Flow are still common.

249
00:18:42,539 --> 00:18:46,889
And sometimes a CV appears where
everyone internally says, oh,

250
00:18:46,949 --> 00:18:48,449
this clearly wasn't checked.

251
00:18:48,929 --> 00:18:51,179
I can see how this was
supposed to stay in the lab.

252
00:18:51,599 --> 00:18:55,529
Clear scope of boundaries help
prevent these situations from

253
00:18:55,529 --> 00:18:57,054
quietly reaching production.

254
00:18:57,554 --> 00:19:02,714
Defining an AI policy Sounds abstract
at first, but in practice it can

255
00:19:02,714 --> 00:19:08,054
answer a very close to home question,
what approach and what high level

256
00:19:08,054 --> 00:19:13,484
rules does this specific business
support when working with AI systems

257
00:19:14,294 --> 00:19:18,284
without a clear policy, every team
ends up inventing its own workflow.

258
00:19:18,914 --> 00:19:20,504
One team retrains weekly.

259
00:19:20,894 --> 00:19:25,004
Another deploys directly from
notebooks, and someone else exposes

260
00:19:25,004 --> 00:19:29,954
an endpoint with default settings
because nothing explicitly said not to.

261
00:19:30,454 --> 00:19:34,744
From a governance perspective, this
inconsistency becomes risk very quickly.

262
00:19:35,244 --> 00:19:40,044
This artifact depends heavily on
demonstrated leadership commitment

263
00:19:40,224 --> 00:19:41,874
and organizational strategy.

264
00:19:42,864 --> 00:19:46,764
The policy is commonly approved
by executive management because

265
00:19:46,764 --> 00:19:51,804
it defines the organization's
stance on the responsible AI and

266
00:19:51,894 --> 00:19:53,844
how risk is handled at scale.

267
00:19:54,834 --> 00:19:59,034
We see stricter AI policies
frequently introduced just after

268
00:19:59,034 --> 00:20:03,999
incidents involving sensitive data
exposure or regulatory pressure.

269
00:20:04,499 --> 00:20:07,319
The technology itself is
really the sole issue.

270
00:20:07,994 --> 00:20:12,974
Gaps in governance, oversight and
communication are often at the center.

271
00:20:13,844 --> 00:20:18,044
Policy helps establish shared
expectations across teams and

272
00:20:18,044 --> 00:20:22,639
demonstrates accountability, which
regulators increasingly expect,

273
00:20:23,009 --> 00:20:28,759
especially on their frameworks like GDPR,
focusing on personal data protection.

274
00:20:29,684 --> 00:20:34,184
From an MOPS perspective, this artifact
actually makes engineering easier.

275
00:20:34,814 --> 00:20:38,504
It can give you the authority to
say, these tools are approved here.

276
00:20:38,984 --> 00:20:43,334
These defaults are forbidden, and these
models require additional approvals.

277
00:20:43,934 --> 00:20:48,464
Engineers already know insecure
defaults are dangerous, but without

278
00:20:48,524 --> 00:20:52,724
policy backing, they often lack
the mandate to enforce changes.

279
00:20:53,224 --> 00:20:59,944
We are now in clause six, planning,
CLPP planning more precisely.

280
00:20:59,944 --> 00:21:01,924
Clause 6 1 1.

281
00:21:02,224 --> 00:21:04,954
Planning for risks and opportunities.

282
00:21:05,644 --> 00:21:12,154
The question here is what actions will we
take once we know specific A risks exist?

283
00:21:13,114 --> 00:21:14,074
We know this pattern.

284
00:21:14,134 --> 00:21:19,264
Risks get discussed, even documented,
but nothing changes operationally.

285
00:21:20,014 --> 00:21:20,944
Nothing changes.

286
00:21:20,974 --> 00:21:25,774
If nothing changes, nothing turns
into backlog, work on the board

287
00:21:25,864 --> 00:21:31,294
or real controls because nobody
translated them into engineering tasks.

288
00:21:32,044 --> 00:21:37,594
This artifact exists to drive that
planning and translation into action.

289
00:21:38,509 --> 00:21:43,279
Each risk or opportunity must have
an owner, a decision, and a concrete

290
00:21:43,279 --> 00:21:49,369
next step, such as implementing bias
reviews or developing a resource plan

291
00:21:49,459 --> 00:21:54,969
to address insufficient resources
for A IMS activities In practice,

292
00:21:55,419 --> 00:22:00,039
this enables proactive risk handling
and demonstrates that risks are

293
00:22:00,039 --> 00:22:02,139
acted upon, not just identified.

294
00:22:02,639 --> 00:22:07,289
Before teams can control AI risks,
they need a governance structure that

295
00:22:07,289 --> 00:22:13,889
actually works across leadership support,
resources, training, and clear ownership.

296
00:22:14,309 --> 00:22:17,219
And this document tracks
planning for exactly that.

297
00:22:18,059 --> 00:22:22,350
It also reminds us that governance
isn't only about preventing problems.

298
00:22:22,709 --> 00:22:26,054
Organizations use this artifact
to pursue opportunities as well.

299
00:22:26,894 --> 00:22:32,384
Plans for improving reputation,
accelerating market entry, or reducing

300
00:22:32,384 --> 00:22:36,975
client led queries and questionnaires
by demonstrating mature AI governance.

301
00:22:37,364 --> 00:22:45,614
But just through is O 42 0 0 1
6 1 2 a risk assessment process.

302
00:22:46,604 --> 00:22:52,485
Risk assessment in ISO 42 0 0 1 is
fundamental for identifying and evaluating

303
00:22:52,485 --> 00:22:55,455
risks associated with AI systems.

304
00:22:56,174 --> 00:23:00,764
These risks may include harm
to individuals, bias, lack of

305
00:23:00,764 --> 00:23:06,764
transparency, safety issues,
misuse, or unintended public impact.

306
00:23:07,334 --> 00:23:10,934
The assessment allows the
organization to prioritize risks

307
00:23:11,054 --> 00:23:13,455
based on likelihood and impact.

308
00:23:13,955 --> 00:23:19,130
Results of risk assessments directly
inform risk treatment decisions,

309
00:23:19,850 --> 00:23:22,325
and the selection of NXA controls.

310
00:23:22,825 --> 00:23:26,545
Without a proper risk assessment,
the effectiveness of the A

311
00:23:26,545 --> 00:23:28,795
IMS cannot be demonstrated.

312
00:23:29,455 --> 00:23:31,615
But this artifact answers is simple.

313
00:23:32,275 --> 00:23:37,945
How do we evaluate AI risk consistently
before models move forward?

314
00:23:38,445 --> 00:23:45,135
This process depends on two foundations,
a clearly defined scope and agreed

315
00:23:45,135 --> 00:23:47,355
methodology for risk criteria.

316
00:23:48,120 --> 00:23:51,800
Without those risk decisions
easily become subjective.

317
00:23:52,520 --> 00:23:55,730
One team treats a model
change as a low risk.

318
00:23:56,120 --> 00:24:00,380
Another blocks deployment entirely,
and suddenly governance feels

319
00:24:00,380 --> 00:24:02,420
inconsistent rather than helpful.

320
00:24:02,920 --> 00:24:07,750
A repeatable risk assessment
process changes that it introduces

321
00:24:07,750 --> 00:24:10,180
a common language impact.

322
00:24:10,570 --> 00:24:12,520
Likelihood stakeholder review.

323
00:24:13,030 --> 00:24:17,890
Some model updates are evaluated
using the same logic across pipelines.

324
00:24:18,390 --> 00:24:21,720
The purpose here isn't to
predict every possible outcome.

325
00:24:22,410 --> 00:24:30,150
A risks evolve as models are retrained,
reused, or deployed in new contexts.

326
00:24:30,690 --> 00:24:36,055
What auditors want to see the structured
way to evaluate risk before promotion, not

327
00:24:36,055 --> 00:24:38,455
only after an initial reaches production.

328
00:24:38,955 --> 00:24:43,995
This becomes even more important because
model artifacts aren't just data.

329
00:24:44,595 --> 00:24:50,595
They behave like executable components,
serialization formats, dependencies, and

330
00:24:50,595 --> 00:24:57,315
retraining workflows introduce risks that
traditional teams might overlook when

331
00:24:57,315 --> 00:25:03,825
teams treat promotion as just another
commit vulnerabilities, language toxicity,

332
00:25:03,825 --> 00:25:06,615
and bias can slip through unnoticed.

333
00:25:07,115 --> 00:25:12,995
6 1 3 artifact covers AI risk treatment
and statement of applicability.

334
00:25:13,495 --> 00:25:18,865
What you're seeing on the right is a small
example from an SOA specifically control A

335
00:25:18,865 --> 00:25:23,370
55, which looks at societal impact of ai.

336
00:25:23,870 --> 00:25:27,770
And when people hear that phrase
societal impact, it could sound

337
00:25:27,770 --> 00:25:31,490
abstract, but in practice it
becomes very concrete, very quickly.

338
00:25:32,150 --> 00:25:35,000
First, it implies an environmental impact.

339
00:25:35,660 --> 00:25:39,140
Modern models don't
just live in notebooks.

340
00:25:39,380 --> 00:25:43,490
They run on infrastructure,
consume energy, and create

341
00:25:43,490 --> 00:25:45,140
a real carbon footprint.

342
00:25:45,860 --> 00:25:48,995
Organizations are starting to
track that lifecycle impact,

343
00:25:49,445 --> 00:25:51,560
especially as going green.

344
00:25:51,710 --> 00:25:55,610
Sustainability, environmental,
social, and governance.

345
00:25:55,700 --> 00:25:58,190
ESG, regulations and reporting.

346
00:25:58,190 --> 00:26:00,440
Standard expectations evolve.

347
00:26:00,940 --> 00:26:03,700
Second implied economic impact.

348
00:26:04,180 --> 00:26:08,020
This isn't only about the job
displacement, it's also about who

349
00:26:08,020 --> 00:26:12,970
gains access to new tools, who gets
included, who excluded, and how

350
00:26:12,970 --> 00:26:17,620
organizations support reskilling or
responsible automation decisions.

351
00:26:18,160 --> 00:26:20,770
Third, governance and recourse.

352
00:26:21,580 --> 00:26:22,960
A system makes a decision.

353
00:26:23,230 --> 00:26:26,170
Can it be truly understood or challenged?

354
00:26:26,670 --> 00:26:31,800
Explainability and traceability are no
longer just technical, nice-to-haves.

355
00:26:32,190 --> 00:26:34,350
They're becoming core audit evidence.

356
00:26:35,070 --> 00:26:37,080
Then there's health and wellbeing.

357
00:26:37,740 --> 00:26:43,950
That includes not just safety issues,
but subtle harms like stress caused by

358
00:26:44,010 --> 00:26:49,710
automated decisions or biased outcomes
that could affect access to services.

359
00:26:50,210 --> 00:26:52,760
Finally, cultural cohesion and values.

360
00:26:53,270 --> 00:26:59,540
ISO 42 0 0 1 brings fairness, diversity
and rights into scope in a way that

361
00:26:59,540 --> 00:27:02,450
can be audited, not just discussed.

362
00:27:02,950 --> 00:27:06,185
Plus six two, a objectives
and plans to achieve them.

363
00:27:07,135 --> 00:27:11,875
Very often this section actually
lives inside the broader AI policy

364
00:27:12,385 --> 00:27:16,885
because it translates leadership
intent into something teams can

365
00:27:16,885 --> 00:27:18,985
track and improve over time.

366
00:27:19,855 --> 00:27:25,850
The core question, this artifact answer
is what targets are we aiming at with

367
00:27:26,050 --> 00:27:28,850
our controls and ML ops practices?

368
00:27:29,725 --> 00:27:36,745
In many organizations, monitoring exists,
dashboards exist, alerts exist, but just

369
00:27:36,745 --> 00:27:43,045
in case they are undocumented or not
disseminated and openly discussed among

370
00:27:43,075 --> 00:27:49,060
personnel within A IMS scope, it can
remain unclear what success look like.

371
00:27:49,560 --> 00:27:56,670
So 42 0 0 1 requires objectives to align
with the AI policy and where practical to

372
00:27:56,670 --> 00:28:01,500
be smart, meaning specific, measurable,
achievable, relevant, and timely.

373
00:28:02,250 --> 00:28:07,650
This means things like reducing
model related incidents, detecting

374
00:28:07,650 --> 00:28:13,020
drift faster, or ensuring secure
patching timelines for infrastructure,

375
00:28:13,290 --> 00:28:15,330
supporting ML business units.

376
00:28:15,840 --> 00:28:18,005
This also creates alignment across levels.

377
00:28:18,505 --> 00:28:22,735
Stop management sets direction
and teams translate that into

378
00:28:22,735 --> 00:28:27,505
operational goals, tied to pipelines,
deployments, logging, and monitoring.

379
00:28:28,345 --> 00:28:32,035
From an audit perspective, this
artifact is powerful because it

380
00:28:32,035 --> 00:28:33,715
converts intention into proof.

381
00:28:34,465 --> 00:28:38,875
Instead of seeing, we care about
responsible ai, that is our objective.

382
00:28:39,595 --> 00:28:42,685
You must show measurable
target and the review cycle.

383
00:28:43,315 --> 00:28:47,185
That's how continual improvement
becomes concrete and visible over time.

384
00:28:47,685 --> 00:28:51,795
We are in clause seven Now
support and more precisely 7.2

385
00:28:52,695 --> 00:28:54,885
artifact on personnel competency.

386
00:28:55,545 --> 00:28:59,805
Personnel competency is one of those
artifacts that could sound administrative

387
00:28:59,835 --> 00:29:04,485
and preachy, but in practice it
directly addresses the risk at the

388
00:29:04,485 --> 00:29:07,275
human layer, the core idea is simple.

389
00:29:07,785 --> 00:29:11,865
People operating AI systems
need competence that matches

390
00:29:11,865 --> 00:29:13,635
the risk they influence.

391
00:29:14,295 --> 00:29:18,165
That does not mean everyone
becomes a security expert.

392
00:29:18,375 --> 00:29:20,025
It means role appropriate.

393
00:29:20,025 --> 00:29:23,055
Awareness is intentional and documented.

394
00:29:23,925 --> 00:29:27,945
This art effect depends on clearly
defined roles and responsibilities

395
00:29:28,485 --> 00:29:30,975
and understanding of training needs.

396
00:29:31,635 --> 00:29:36,135
Competency can span areas like
mops, engineering, bias awareness,

397
00:29:36,435 --> 00:29:41,505
data protection, and AI incident
handling, tailored to how each

398
00:29:41,505 --> 00:29:43,400
role interacts with AI systems.

399
00:29:43,900 --> 00:29:50,800
From an ISO 42 0 0 1 audit perspective,
what matters is evidence such as training

400
00:29:50,800 --> 00:29:53,830
records, certifications, or workshops.

401
00:29:54,190 --> 00:29:55,480
The goal isn't perfection.

402
00:29:55,540 --> 00:30:01,780
It's showing that competence is planned,
tracked and aligned with AI lifecycle

403
00:30:02,650 --> 00:30:08,880
and roles responsibilities, 7.5
artifact on all documented information

404
00:30:08,880 --> 00:30:10,290
for the specific organization.

405
00:30:10,790 --> 00:30:11,390
Can answer.

406
00:30:11,390 --> 00:30:16,940
What documentation do we actually need
to operate our AI systems safely beyond

407
00:30:17,000 --> 00:30:19,220
what the standard explicitly lists?

408
00:30:19,880 --> 00:30:23,720
Clause 7.5 gives
organizations flexibility.

409
00:30:24,260 --> 00:30:29,300
It recognizes that real lops
environments generate critical

410
00:30:29,300 --> 00:30:33,200
knowledge that doesn't always fit
into predefined standard templates.

411
00:30:33,700 --> 00:30:38,380
Things like model release notes,
prompt updates, vendor change

412
00:30:38,380 --> 00:30:40,930
tracking, or pipeline hardening.

413
00:30:40,930 --> 00:30:46,840
Steps often start informally
laying in Slack dms across emails,

414
00:30:46,900 --> 00:30:49,540
or kept just in someone's brain.

415
00:30:50,290 --> 00:30:54,160
The risk is that when documentation
leaves only in tribal knowledge,

416
00:30:54,910 --> 00:30:57,220
businesses tend to lose track.

417
00:30:57,925 --> 00:31:03,535
Reproducibility suffers and after
staff changes, nobody knows why

418
00:31:03,535 --> 00:31:05,425
a model behaves the way it does.

419
00:31:05,925 --> 00:31:12,135
This artifact legitimizes documenting
what teams already rely on making

420
00:31:12,135 --> 00:31:16,035
sure critical operational knowledge
survives beyond individuals.

421
00:31:16,535 --> 00:31:21,230
Auditors will look for evidence that
the organization recognizes what the

422
00:31:21,230 --> 00:31:25,850
documentation is necessary for its
own maturity level and AI complexity.

423
00:31:26,525 --> 00:31:31,105
And that those documents support
consistent operation was eight

424
00:31:31,105 --> 00:31:36,565
operations and 8.1 more specific
on operational documentation.

425
00:31:37,525 --> 00:31:41,335
This artifact exists because the
planning we had in clause six

426
00:31:41,815 --> 00:31:43,915
doesn't prevent incidents by itself.

427
00:31:44,095 --> 00:31:45,145
Execution does.

428
00:31:45,895 --> 00:31:49,525
Teams may define change,
management steps, approval

429
00:31:49,525 --> 00:31:52,285
workflows, or rollback procedures.

430
00:31:52,930 --> 00:31:58,360
But if those steps are undocumented
and executed consistently, unexpected

431
00:31:58,420 --> 00:32:04,180
delays can happen and recovery may be
costlier and take longer than expected.

432
00:32:04,680 --> 00:32:10,530
Clause 8.1 in operations in general seems
to ensure that activities like change

433
00:32:10,530 --> 00:32:15,990
management testing, impact assessments,
and rollback procedures are not just

434
00:32:15,990 --> 00:32:20,310
defined, but actually operationalized
and carried out as planned.

435
00:32:20,810 --> 00:32:25,220
Auditors look for evidence that operations
are repeatable and that recovery

436
00:32:25,220 --> 00:32:28,130
actions are prepared before deployment.

437
00:32:28,630 --> 00:32:35,260
Now, a risk register artifact without a
risk register, ML risks tend to scatter

438
00:32:35,260 --> 00:32:38,020
over time and teams lose visibility.

439
00:32:38,830 --> 00:32:42,400
The same issues get rediscovered
repeatedly because there is no

440
00:32:42,670 --> 00:32:44,545
ownership over the full risk picture.

441
00:32:45,045 --> 00:32:49,845
The A risk register creates a single
source of truth and only evaluated.

442
00:32:49,845 --> 00:32:51,465
The risks are recorded here.

443
00:32:51,855 --> 00:32:54,825
After going through the
risk assessment process.

444
00:32:55,635 --> 00:32:58,305
A common MOPS example is data drift.

445
00:32:58,905 --> 00:33:03,675
Teams might notice performance dropping,
but without a central register.

446
00:33:03,915 --> 00:33:07,455
Nobody tracks whether mitigation
actually worked or who is

447
00:33:07,455 --> 00:33:09,465
responsible in the next quarter.

448
00:33:09,965 --> 00:33:13,685
Risk register solves that by
turning individual observations

449
00:33:13,985 --> 00:33:15,755
into organizational memory.

450
00:33:16,255 --> 00:33:20,275
Auditors want to see that risks
are not only identified, but

451
00:33:20,515 --> 00:33:22,765
actively managed over time.

452
00:33:23,265 --> 00:33:27,345
8.3 specific plans for a risk treatment.

453
00:33:27,845 --> 00:33:33,275
Once a risk is identified, how do we make
sure something actually happens next?

454
00:33:33,755 --> 00:33:38,405
And what happens to it is
transferred mitigated or accepted.

455
00:33:38,905 --> 00:33:43,255
Many are good at identifying risks
and even planning mitigations.

456
00:33:43,675 --> 00:33:46,990
What follow throughs is where
things often break down.

457
00:33:47,905 --> 00:33:52,735
An AI risk treatment plan
focuses on that exact pain point.

458
00:33:53,125 --> 00:33:58,825
It links directly to the risk register and
the statement of applicability defining

459
00:33:58,825 --> 00:34:03,985
which controls are applied, who owns
them, and when progress will be reviewed.

460
00:34:04,485 --> 00:34:08,130
8.4 AI system impact assessment artifact.

461
00:34:08,630 --> 00:34:13,820
Models can perform exactly as designed,
but their broader real world impact

462
00:34:13,910 --> 00:34:16,100
isn't always evaluated holistically.

463
00:34:16,700 --> 00:34:23,270
Recommendation engines, decision support
tools or automation workflows may

464
00:34:23,420 --> 00:34:29,480
reduce social, ethical, or operational
consequences that only appear once

465
00:34:29,480 --> 00:34:31,945
systems scale in real world environments.

466
00:34:32,445 --> 00:34:36,945
This artifact exists to make those
effects visible early, and it

467
00:34:36,945 --> 00:34:42,195
complements risk assessments by
focusing on outcomes and potential harm.

468
00:34:42,695 --> 00:34:48,605
Documents intended use affected
stakeholders and known limitations

469
00:34:49,055 --> 00:34:54,515
while also encouraging teams to think
about wider areas like environmental

470
00:34:54,515 --> 00:35:00,485
footprint, economic impact,
explainability, and long-term wellbeing.

471
00:35:00,985 --> 00:35:04,435
Auditors here want to see
that organizations considered

472
00:35:04,525 --> 00:35:08,185
impact deliberately and
documented their reasoning.

473
00:35:08,685 --> 00:35:15,225
Clause nine, performance evaluation
now and 9.1 on results of monitoring

474
00:35:15,315 --> 00:35:17,115
and measurement activities.

475
00:35:17,615 --> 00:35:22,295
Organizations Start by defining what
needs to be monitored, usually through

476
00:35:22,385 --> 00:35:28,540
clear key performance indicators, such as
model accuracy, response time, fairness

477
00:35:28,715 --> 00:35:31,265
indicators, or compliance metrics.

478
00:35:31,925 --> 00:35:36,965
Just as important is defining how the
data is collected, how often is it

479
00:35:36,965 --> 00:35:42,695
reviewed, and who is responsible for
analyzing it Without clear ownership

480
00:35:42,965 --> 00:35:48,305
and a defined review, cadence monitoring
becomes passive data collection

481
00:35:48,635 --> 00:35:50,675
rather than effective governance.

482
00:35:51,635 --> 00:35:57,635
Once data is collected, clause 9.1
requires analysis and evaluation.

483
00:35:58,370 --> 00:36:04,370
Teams compare actual performance
against objectives, identify trends

484
00:36:04,460 --> 00:36:09,980
or emerging risks, and feed those
insights back into decision making.

485
00:36:10,730 --> 00:36:14,240
This creates a continuous
feedback loop in which monitoring

486
00:36:14,360 --> 00:36:16,010
drives tangible improvement.

487
00:36:16,510 --> 00:36:18,665
Auditors also look for
documented evidence.

488
00:36:19,540 --> 00:36:24,490
Monitoring results, analysis,
outputs and performance reports

489
00:36:24,490 --> 00:36:29,290
need to be retained, version
controlled, and accessible for review.

490
00:36:30,070 --> 00:36:35,260
The goal is not to create more dashboards
or more reporting that is not actionable.

491
00:36:35,650 --> 00:36:40,150
It's to show that monitoring leads
to informed action and measurable

492
00:36:40,150 --> 00:36:42,400
progress across the AI life cycle.

493
00:36:42,900 --> 00:36:45,210
9.2 internal audit.

494
00:36:46,095 --> 00:36:50,085
As pipelines evolve,
responsibilities shift people move

495
00:36:50,085 --> 00:36:52,545
on or grow into new positions.

496
00:36:52,965 --> 00:36:57,795
Retraining schedules shift and
controls can get bypassed temporarily,

497
00:36:58,575 --> 00:37:02,175
and over time, the documented
process no longer matches reality.

498
00:37:02,675 --> 00:37:06,850
The gap is what internal audits
are designed to service early.

499
00:37:07,350 --> 00:37:13,350
Artifact depends on all implemented A IMS
documentation and operational records.

500
00:37:13,920 --> 00:37:18,060
And auditors look for evidence that
controls are reviewed periodically.

501
00:37:18,570 --> 00:37:24,390
Findings are tracked, and improvement
actions follow internal audits.

502
00:37:24,480 --> 00:37:26,970
Act as an early warning system.

503
00:37:27,480 --> 00:37:31,830
They identify weaknesses before
customers, external auditors

504
00:37:32,190 --> 00:37:33,630
or attackers expose them.

505
00:37:34,395 --> 00:37:39,405
The audit results, support management
review, and corrective actions.

506
00:37:40,395 --> 00:37:44,475
Internal audits are essential for
maintaining the effectiveness of the a

507
00:37:44,475 --> 00:37:50,055
MS over time, and they also prepared the
organization for external certification

508
00:37:50,115 --> 00:37:53,315
audits 9.3 Management Review.

509
00:37:53,815 --> 00:37:58,915
Management review records demonstrate
that monitoring results, internal

510
00:37:59,005 --> 00:38:04,615
audit findings and AI objectives
are actively reviewed by leadership.

511
00:38:05,575 --> 00:38:09,175
These discussions lead to
decisions about priorities,

512
00:38:09,505 --> 00:38:12,085
resources, and strategic direction.

513
00:38:12,865 --> 00:38:18,680
This is how AI governance moves from
reactive to strategic and intentional.

514
00:38:19,180 --> 00:38:25,180
Management carries responsibility for
ensuring the A IMS achieves its intended

515
00:38:25,210 --> 00:38:31,300
outcomes, which means understanding
trends like drift incidents, recurring

516
00:38:31,300 --> 00:38:37,600
findings or performance changes, and
deciding what needs to adapt Next.

517
00:38:38,470 --> 00:38:42,040
Management review results also
feeds directly into corrective

518
00:38:42,100 --> 00:38:44,710
actions and continual improvement.

519
00:38:45,385 --> 00:38:50,635
They ensure AI governance stays aligned
with business clause rather than

520
00:38:50,725 --> 00:38:53,965
operating as a siloed compliance exercise.

521
00:38:54,465 --> 00:39:01,275
And the last documentation artifact from
me today from clause improvement 10.2

522
00:39:01,695 --> 00:39:07,275
nonconformities and corrective actions
a nonconformity simply means something

523
00:39:07,275 --> 00:39:12,405
did not happen the way that was planned
and documented, or the management system.

524
00:39:13,290 --> 00:39:15,150
Maybe a control was skipped.

525
00:39:15,750 --> 00:39:21,120
Monitoring results were not
reviewed or deployment moved forward

526
00:39:21,330 --> 00:39:23,640
without the required approval step.

527
00:39:24,300 --> 00:39:28,140
The critical part is not the
mistake itself, it's how the

528
00:39:28,140 --> 00:39:29,970
organization responds to it.

529
00:39:30,960 --> 00:39:36,840
Clause 10.2 focuses on structured
response and it closes the loop

530
00:39:36,900 --> 00:39:38,700
of the plan door check act cycle.

531
00:39:39,200 --> 00:39:43,670
When a non-conformity is identified,
the auditor documents the gap

532
00:39:44,090 --> 00:39:45,830
and the supporting evidence.

533
00:39:46,490 --> 00:39:49,430
A mature organization
then takes ownership.

534
00:39:49,910 --> 00:39:56,330
It analyzes the root cause, evaluates the
impact, and defines corrective actions.

535
00:39:56,570 --> 00:39:58,490
So the issue does not repeat.

536
00:39:58,990 --> 00:40:04,300
Not following up on the agreed
upon timelines for fixing or fixing

537
00:40:04,300 --> 00:40:09,280
symptoms that appear on the surface
without addressing the root cause.

538
00:40:09,730 --> 00:40:15,100
Tends to lead to repeated failures,
and in some cases may be interpreted

539
00:40:15,100 --> 00:40:16,930
as organizational negligence.

540
00:40:17,770 --> 00:40:22,335
You'll often hear auditors distinguish
between minor and major nonconformities.

541
00:40:23,170 --> 00:40:29,410
A minor one is typically an isolated gap,
such as a missing approval record, while a

542
00:40:29,410 --> 00:40:34,180
major nonconformity signals a significant
breakdown in the management system.

543
00:40:34,720 --> 00:40:39,880
For example, missing processes or an
entire clause not being implemented

544
00:40:39,880 --> 00:40:45,310
as expected or repeated failures
that suggest systemic negligence.

545
00:40:45,810 --> 00:40:51,360
Feature findings can delay certification
or even suspend an existing certificate

546
00:40:51,780 --> 00:40:54,270
until corrective actions are verified.

547
00:40:54,770 --> 00:40:58,850
Ultimately, auditors expect to
see ownership timelines and prove

548
00:40:58,850 --> 00:41:00,470
that actions were implemented.

549
00:41:01,085 --> 00:41:05,825
If corrective actions from a previous
audit are ignored, the issue may

550
00:41:05,825 --> 00:41:11,615
escalate into a major nonconformity
signaling, a deeper breakdown in the

551
00:41:11,615 --> 00:41:16,145
management system, and potentially
impacting certification status.

552
00:41:17,080 --> 00:41:17,300
The.

553
00:41:17,800 --> 00:41:21,850
And with that, we wrapped up all 16
required documentation artifacts.

554
00:41:22,300 --> 00:41:26,110
Something I'd highlight as a
key takeaway right now is this.

555
00:41:26,560 --> 00:41:31,570
When organizations start implementing
this minimal viable set of A IMS

556
00:41:31,570 --> 00:41:35,950
document artifacts, they tend to see
disproportionate benefits right away.

557
00:41:35,950 --> 00:41:40,830
Knowledge and information gaps
are brought to light, so effective

558
00:41:40,830 --> 00:41:43,260
discussions and actions are sparked.

559
00:41:44,220 --> 00:41:48,240
Boundaries between experimentation
and production becomes visible.

560
00:41:48,750 --> 00:41:53,340
Releases become safer because fewer
assumptions are left undocumented,

561
00:41:54,000 --> 00:41:58,980
and when something goes wrong, the
response is faster because at any

562
00:41:58,980 --> 00:42:01,620
time, they are not stakeholders.

563
00:42:01,620 --> 00:42:07,320
Know the documented information already
exists, where to find it and how to

564
00:42:07,320 --> 00:42:09,990
conveniently follow the outlined steps.

565
00:42:10,490 --> 00:42:14,210
Another important lesson is
that advanced models rarely

566
00:42:14,210 --> 00:42:16,220
cause problems by themselves.

567
00:42:17,120 --> 00:42:23,240
Most issues are governance, failures,
missing ownership, missing traceability,

568
00:42:23,690 --> 00:42:25,730
and missing feedback loops.

569
00:42:26,450 --> 00:42:32,420
Problems emerge when assumptions go
unchallenged, and no one can answer with

570
00:42:32,420 --> 00:42:38,210
certainly simple questions like, who
approved the change, what data was used?

571
00:42:38,780 --> 00:42:41,840
Whether the system is
still behaving as intended.

572
00:42:42,620 --> 00:42:48,140
A third lesson is that ISO 42 0 0 1
works best when integrated into existing

573
00:42:48,140 --> 00:42:54,950
practices, not after the fact and
siloed as a checkbox exercise, adapting

574
00:42:54,950 --> 00:42:59,665
existing processes across product
planning, risk management, audits

575
00:42:59,720 --> 00:43:06,350
and security reviews, and extended to
cover A IML specific risks works best.

576
00:43:07,100 --> 00:43:11,750
One thing that comes up repeatedly is
the importance of people and leadership.

577
00:43:12,620 --> 00:43:16,850
Training matters not just for
engineers, but also for product

578
00:43:16,850 --> 00:43:19,790
owners, sales decision makers.

579
00:43:20,330 --> 00:43:26,420
Executive sponsorship of ISO 42 0
0 1 certification efforts is often

580
00:43:26,420 --> 00:43:31,550
the difference between AI governance
existing on paper and actually being

581
00:43:31,550 --> 00:43:36,620
followed in practice with evident
and positive behavior changes for

582
00:43:36,620 --> 00:43:38,390
the organization and its people.

583
00:43:38,890 --> 00:43:42,640
There's also a very practical
fact worth stating clearly

584
00:43:43,480 --> 00:43:46,120
certification is almost never free.

585
00:43:46,930 --> 00:43:48,190
It requires time.

586
00:43:48,835 --> 00:43:53,725
Attention and budget, and it
carries other opportunity costs.

587
00:43:54,205 --> 00:43:59,545
That's why organizations that succeed
tend to take a risk-based approach.

588
00:43:59,875 --> 00:44:06,505
Prioritizing controls proportionate to
this specific business impact instead

589
00:44:06,505 --> 00:44:10,075
of trying to boil the ocean all at once.

590
00:44:10,575 --> 00:44:12,630
Finally, the strongest takeaway is this.

591
00:44:13,485 --> 00:44:20,355
Organizations that approach ISO 42
0 0 1 as a strategic investment, not

592
00:44:20,355 --> 00:44:25,995
just a compliance exercise, serial
returns, and these include improved

593
00:44:25,995 --> 00:44:32,445
customer trust, better internal
clarity, smoother audits, and more

594
00:44:32,445 --> 00:44:34,980
confidence when scaling AI systems.

595
00:44:35,480 --> 00:44:38,660
I'd like to finish by thanking you
all for being here and also the

596
00:44:38,660 --> 00:44:43,460
organizers for the Warm welcome as
well as the sponsors and everyone else

597
00:44:43,460 --> 00:44:45,860
supporting this event behind the scenes.

598
00:44:46,490 --> 00:44:50,150
I welcome your feedback and I'd be
glad to continue the conversation.

599
00:44:50,660 --> 00:44:54,260
Feel free to connect with me on LinkedIn,
and hopefully I'll have the opportunity

600
00:44:54,260 --> 00:44:59,570
to share more insights with you at
the future edition of the conference.

601
00:45:00,170 --> 00:45:02,330
Greetings from me until next time.

