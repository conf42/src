1
00:00:00,750 --> 00:00:04,260
Hello, my name is Emmanuel Bakari,
and today I'll be taking you through

2
00:00:04,260 --> 00:00:06,150
exploring the math of thresholds.

3
00:00:06,690 --> 00:00:09,450
Thresholds are bounds,
upper and lower bounds.

4
00:00:09,570 --> 00:00:14,690
And I'll be showing methods using
numerical methods which is in

5
00:00:14,690 --> 00:00:19,220
mathematics, iterative methods for
predicting where something should be.

6
00:00:19,720 --> 00:00:20,315
So where am I?

7
00:00:20,470 --> 00:00:21,460
My name is Ivano Ery.

8
00:00:21,550 --> 00:00:22,420
People call me back, man.

9
00:00:23,275 --> 00:00:24,385
I'm a senior developer engineer at Twilio.

10
00:00:24,985 --> 00:00:27,985
I also on decide build
solutions at Baseline EQ Cloud.

11
00:00:28,044 --> 00:00:32,064
One of them is Course Craft where we
use some of these methods for things

12
00:00:32,064 --> 00:00:36,115
like right sizing request and limiting
Kubernetes automatic right sizing.

13
00:00:36,325 --> 00:00:37,765
So you can check out the product there.

14
00:00:37,765 --> 00:00:39,805
Cost craft to baseline
HQ Cloud if you like it.

15
00:00:40,645 --> 00:00:41,695
What is the threshold?

16
00:00:42,255 --> 00:00:46,275
A threshold is a boundary at certain
time that defines a minimal maximum.

17
00:00:46,335 --> 00:00:48,434
So it can either be an upper
bound or a lower bound.

18
00:00:48,985 --> 00:00:50,155
Metrics define.

19
00:00:51,070 --> 00:00:53,560
The values that you then use, right?

20
00:00:53,620 --> 00:00:56,140
To check whether threshold
exceeds or not at the time.

21
00:00:56,140 --> 00:00:59,620
T And then if it reaches for
assessing period, usually

22
00:00:59,800 --> 00:01:01,060
that's when you can take action.

23
00:01:01,540 --> 00:01:04,235
In this case I'm showing what is
an anomaly band, which is a open

24
00:01:04,235 --> 00:01:06,005
and lower bound maxima, right?

25
00:01:06,155 --> 00:01:09,755
This is a scatter up plot, but in this,
that the values are drawn out, right?

26
00:01:09,755 --> 00:01:11,315
So this is a predictive model.

27
00:01:12,075 --> 00:01:15,135
So as you may notice, no
taches are the same, right?

28
00:01:15,135 --> 00:01:17,420
That is very different to what you
would've defined as a threshold.

29
00:01:18,160 --> 00:01:20,950
And that's because the one we are
aware of is static thresholds, right?

30
00:01:20,980 --> 00:01:23,110
Which is just a classic line
through the noise, right?

31
00:01:23,530 --> 00:01:25,150
They're fixed, they're easy to estimate.

32
00:01:25,540 --> 00:01:26,740
They're also easy to diagnose.

33
00:01:26,740 --> 00:01:30,190
Like you can look at it straight up
and know that find this is not there,

34
00:01:30,190 --> 00:01:31,780
but they're also prone to noise, right?

35
00:01:31,780 --> 00:01:32,560
They can fail.

36
00:01:33,020 --> 00:01:34,010
They don't adjust well.

37
00:01:34,130 --> 00:01:38,510
So the situations of your system, things
like organic growth, E to C, TC, and

38
00:01:38,510 --> 00:01:39,890
they're also single variants, right?

39
00:01:39,890 --> 00:01:43,760
You can either monitor CPU or
memory, but in dynamic thresholds,

40
00:01:43,760 --> 00:01:45,770
you can use CPU on memory to say.

41
00:01:46,270 --> 00:01:47,350
To define, okay, fine.

42
00:01:47,350 --> 00:01:48,640
We need to scale the system.

43
00:01:48,790 --> 00:01:49,030
Right?

44
00:01:49,450 --> 00:01:54,310
So autoscaling models usually employ
dynamic thresholds and dynamic thresholds.

45
00:01:54,310 --> 00:01:57,220
In this case, they change
with the metric observations.

46
00:01:57,730 --> 00:01:59,410
So they're boundary based, right?

47
00:01:59,410 --> 00:02:01,480
So that can be upper or lower, right?

48
00:02:01,480 --> 00:02:04,755
In the case of the anomaly bands I just
showed you they're always calls home.

49
00:02:04,755 --> 00:02:08,320
So they rely on past and present data
and they are feedback loop driven.

50
00:02:09,220 --> 00:02:11,470
That's basically what the
causal sense means, right?

51
00:02:11,470 --> 00:02:13,420
It's control systems, that kind of stuff.

52
00:02:13,480 --> 00:02:17,020
So they're motivated, they adapt
well, they're complex to align, right?

53
00:02:17,025 --> 00:02:19,780
You can't basically say this
is where it's gonna be, right?

54
00:02:20,210 --> 00:02:21,650
They need more, you need at the start.

55
00:02:21,710 --> 00:02:24,140
'cause you also have to understand
how your system is modeled and,

56
00:02:24,140 --> 00:02:26,450
but they're less pro to false
positives and a result of that.

57
00:02:27,290 --> 00:02:28,820
So definitions are done.

58
00:02:28,880 --> 00:02:30,890
So what's the process like
for static thresholds?

59
00:02:30,890 --> 00:02:32,450
So start off with a static threshold.

60
00:02:32,450 --> 00:02:34,280
You have a bunch of data points, right?

61
00:02:35,120 --> 00:02:37,310
So you need to filter out
for out layers, right?

62
00:02:37,670 --> 00:02:38,600
To start off, right?

63
00:02:38,750 --> 00:02:41,750
Clean up the data, that kind of
pre-processing if you want to.

64
00:02:41,840 --> 00:02:45,550
It's not particularly a required
process, but you can do so with

65
00:02:45,550 --> 00:02:47,500
things like I QRS and Z course, right?

66
00:02:47,500 --> 00:02:49,000
Just to see just how the world spread out.

67
00:02:49,000 --> 00:02:50,110
Your data is right.

68
00:02:50,110 --> 00:02:53,470
Standard deviation also comes in
here, but that's on the Z course.

69
00:02:53,600 --> 00:02:56,180
The second thing you do after that
is that you define your aggregate

70
00:02:56,270 --> 00:02:59,390
statistics, which is like all
these data points that are present,

71
00:02:59,390 --> 00:03:00,920
how do we make it to one value?

72
00:03:01,500 --> 00:03:03,060
And you can do that with present ours.

73
00:03:03,120 --> 00:03:05,460
You can also do that with a
linear aggregate like in me.

74
00:03:06,000 --> 00:03:10,580
Which is just an average or in this
there's geometric kind of means, but in

75
00:03:10,580 --> 00:03:13,200
this case, I mean a normal linear mean.

76
00:03:13,800 --> 00:03:17,640
So the once you've done all of that,
then you can reduce your mean error.

77
00:03:18,150 --> 00:03:20,970
I'm using something like the law of large
numbers, which I'm explaining a bit,

78
00:03:21,390 --> 00:03:25,410
but this is like the entire process for
static thresholds with numerical methods.

79
00:03:25,980 --> 00:03:28,440
So why do we need to filter
for out the years is because

80
00:03:28,440 --> 00:03:29,495
static thresholds don't adapt.

81
00:03:30,025 --> 00:03:32,935
So that means that from the start,
you need to be sure that is the normal

82
00:03:32,935 --> 00:03:37,045
band of your system that isn't really
influenced by those art layers, right?

83
00:03:37,050 --> 00:03:38,965
So that's the skewness
of your distribution.

84
00:03:38,965 --> 00:03:42,415
The ketosis is reasonably fair, right?

85
00:03:42,715 --> 00:03:44,215
That's that be core distribution.

86
00:03:44,845 --> 00:03:47,484
Art layer filtering basically
guarantees that you reduce the case of

87
00:03:47,515 --> 00:03:49,135
a false negative or a false positive.

88
00:03:49,195 --> 00:03:50,274
I can keep it going from there.

89
00:03:50,774 --> 00:03:54,285
Now in Takota Ranges, as we've spoken
about before for that filtering

90
00:03:54,285 --> 00:03:55,875
process is very simple, right?

91
00:03:55,875 --> 00:03:58,155
And I only recommend this, and
then you can then move into

92
00:03:58,155 --> 00:04:00,465
more extreme methods, right?

93
00:04:00,465 --> 00:04:01,605
But this is very straightforward.

94
00:04:02,205 --> 00:04:06,705
The idea is that you take what the
first and third quarter would be, right?

95
00:04:07,005 --> 00:04:10,335
You subtract them, which is the range,
part of the inter quarter range, and

96
00:04:10,335 --> 00:04:13,845
then you basically define the minimum
maximum using the first quarter, minus

97
00:04:13,845 --> 00:04:15,855
1.5 times that inter quarter range.

98
00:04:16,500 --> 00:04:19,890
And then the third quarter plus
1.5 times the inter quarter range.

99
00:04:20,280 --> 00:04:24,469
So it helps you like, stabilize
those numbers before you then go into

100
00:04:24,950 --> 00:04:26,930
predicting you know, where they should be.

101
00:04:27,620 --> 00:04:31,419
Now, the law of large numbers as you
have seen me mentioning, is basically a

102
00:04:31,419 --> 00:04:35,679
lot that the average of a lot of results
will converge towards the value that

103
00:04:35,679 --> 00:04:38,200
should be the result of the experiment.

104
00:04:38,710 --> 00:04:42,039
So if you had a lot of data points,
say spread out and say you took

105
00:04:42,039 --> 00:04:43,929
the P 99 of the first five minutes.

106
00:04:44,320 --> 00:04:47,140
Then on the next five minutes, then
on the next five minutes in that like

107
00:04:47,140 --> 00:04:50,919
rolling window fashion, you might get
one in the first one, one in the second

108
00:04:50,919 --> 00:04:53,919
one, two in the third one, one in the
fourth one, one in the fifth one, right?

109
00:04:54,039 --> 00:04:55,150
That's who is the outlier.

110
00:04:55,690 --> 00:04:59,280
But because of the fact that you have
a lot of samples broken down, if you

111
00:04:59,280 --> 00:05:04,260
got the average of everything, it would
converge below two and towards one.

112
00:05:04,679 --> 00:05:08,669
And with the more samples you add, the
larger the probability that it tends

113
00:05:08,669 --> 00:05:10,409
towards one, which is your ideal mean.

114
00:05:11,054 --> 00:05:12,705
So that's the idea of
the law of large numbers.

115
00:05:12,705 --> 00:05:14,294
It reduces the error case.

116
00:05:14,835 --> 00:05:18,405
So the way that you start it off
is that you define a trend for

117
00:05:18,405 --> 00:05:19,965
what a system should behave like.

118
00:05:19,965 --> 00:05:22,695
In this case is a CR job that
runs three times every minute.

119
00:05:23,234 --> 00:05:24,674
You can fail every time.

120
00:05:24,674 --> 00:05:26,325
It can pass once, it can pass twice.

121
00:05:26,325 --> 00:05:27,284
So it can pass all the time.

122
00:05:28,215 --> 00:05:32,085
You take those numbers and then you
define what would be a logic table, right?

123
00:05:32,085 --> 00:05:33,635
With those sets of currencies.

124
00:05:33,635 --> 00:05:36,215
Can it fail all the time or
can it pass all the time?

125
00:05:37,115 --> 00:05:40,445
And in that order, and then you basically
throw it into what that trend formula

126
00:05:40,445 --> 00:05:42,425
would be because it's a static threshold.

127
00:05:42,425 --> 00:05:43,805
If it's predictive, you can do this.

128
00:05:43,805 --> 00:05:48,005
If it's non predictive, then you can
basically go about that P 99 case I

129
00:05:48,005 --> 00:05:50,915
mentioned of just sampling by time, right?

130
00:05:50,915 --> 00:05:54,515
In this case, I can already know
what the value would be at that time,

131
00:05:54,515 --> 00:05:58,240
and I can use that then to estimate
what that, trend would be like.

132
00:05:59,190 --> 00:06:01,410
And then just generate like
my own data set for it.

133
00:06:01,950 --> 00:06:05,400
So yeah, so you run multiple
IT versions of that, right?

134
00:06:05,400 --> 00:06:10,320
Using percentiles or, you can use an
aggregate function as we've already

135
00:06:10,320 --> 00:06:12,990
spoken about, mean max or a percentile.

136
00:06:13,080 --> 00:06:16,710
And then as you begin to then average
that trend, you will get towards

137
00:06:16,710 --> 00:06:18,330
the value that meets that threshold.

138
00:06:18,780 --> 00:06:22,710
You can also guarantee the convergence
using standard deviation, right?

139
00:06:22,710 --> 00:06:24,900
So in this case, this is like a
sample of what that looks like

140
00:06:24,900 --> 00:06:26,290
from like when I do one and 10.

141
00:06:27,150 --> 00:06:28,640
Iterations of, each running.

142
00:06:28,640 --> 00:06:32,150
So one in the sense that there's only one
crown jump running and 10 in the sense

143
00:06:32,150 --> 00:06:33,650
that there are 10 crown jobs running.

144
00:06:33,710 --> 00:06:37,790
Those three iterative steps every
minute, either health checks or

145
00:06:37,790 --> 00:06:39,110
whatever you use to define it.

146
00:06:39,560 --> 00:06:42,170
And then this is like what that
random occurrence would look like

147
00:06:42,170 --> 00:06:45,050
if I six a DP 90 or the P 99.999.

148
00:06:45,680 --> 00:06:48,320
You notice the P 90 has
higher aviation, right?

149
00:06:48,320 --> 00:06:51,650
So this would be nice if your system
say, had a lot of spikes, right?

150
00:06:51,650 --> 00:06:52,970
And it was very like jagged.

151
00:06:53,465 --> 00:06:56,915
And this P 99.999 works in a
sense that it's very predictive.

152
00:06:56,915 --> 00:07:00,335
It's always available and you want
to have very like strict SLOs.

153
00:07:00,755 --> 00:07:04,295
So the standard deviation will obviously
capture this, right where you can see

154
00:07:04,295 --> 00:07:06,335
standard division for P 90 is quite high.

155
00:07:06,935 --> 00:07:09,245
And as you then add more ations,
you'll notice it goes down.

156
00:07:09,305 --> 00:07:12,155
So this also then means that we're
attending towards that convergence.

157
00:07:12,215 --> 00:07:15,455
So this is where the low large
numbers then plays a good part

158
00:07:15,455 --> 00:07:17,045
in running experiments like this.

159
00:07:17,525 --> 00:07:21,155
'cause you can be able to then
define a way to then aggregate.

160
00:07:21,875 --> 00:07:23,465
That value towards getting the threshold.

161
00:07:23,555 --> 00:07:26,615
And also then analyze whether
like you are getting much closer

162
00:07:26,615 --> 00:07:31,235
towards the actual mean that you
want rather than it's increasing.

163
00:07:31,235 --> 00:07:33,365
And in that case, you would see
the standard deviation go up.

164
00:07:33,905 --> 00:07:36,455
Like in this case where you can
see a hundred, 200, it decreases

165
00:07:37,085 --> 00:07:38,885
two 50 with 10 increases again.

166
00:07:38,885 --> 00:07:42,890
So with duration, P 99.999
obviously has more variants, but

167
00:07:43,415 --> 00:07:45,485
the standard deviation is quite low.

168
00:07:45,945 --> 00:07:46,195
So you can.

169
00:07:46,800 --> 00:07:50,700
Obviously use that to judge
based on your expectations.

170
00:07:50,700 --> 00:07:51,060
So yeah.

171
00:07:51,090 --> 00:07:53,640
But yeah, here's a graph of
what the experiment looks like.

172
00:07:54,090 --> 00:07:57,420
You notice that the P 90
threshold is close enough at

173
00:07:57,420 --> 00:07:59,820
102 hundred iterations, right?

174
00:07:59,820 --> 00:08:02,970
And it obviously then, shows like
what that range of values would be.

175
00:08:03,390 --> 00:08:05,820
So it can take all this data and
then be able to define what that

176
00:08:05,820 --> 00:08:07,470
threshold value on comp should be like.

177
00:08:07,560 --> 00:08:09,030
We know what are heuristic estimate.

178
00:08:09,530 --> 00:08:10,250
Would fit into.

179
00:08:10,520 --> 00:08:13,520
And then this is basically how you
apply the low, large numbers, right?

180
00:08:14,240 --> 00:08:18,080
And yeah, you can basically then
define that actic threshold.

181
00:08:18,170 --> 00:08:19,970
So that's basically what
the low, large numbers is.

182
00:08:20,000 --> 00:08:24,290
It's a simple method for approximate
static threshold to predefined behavior.

183
00:08:24,350 --> 00:08:27,711
Even if it's not predefined, you
can obviously then procedure 99

184
00:08:27,716 --> 00:08:29,210
to aggregate for that timeline.

185
00:08:30,020 --> 00:08:33,099
It allows you to get clear methods
on where that value should be.

186
00:08:33,850 --> 00:08:35,230
At any given point in time.

187
00:08:35,350 --> 00:08:38,350
And then you can obviously then define
it based on that your aggregate function,

188
00:08:38,350 --> 00:08:41,050
whether it's a sum, whether it's a
percentile, whether it's an average,

189
00:08:41,890 --> 00:08:43,840
and then convergence across multiple.

190
00:08:43,840 --> 00:08:47,290
The top points implies the
limits that captures where that

191
00:08:47,290 --> 00:08:50,655
threshold should be, and so you
don't then have to guess, right?

192
00:08:50,980 --> 00:08:52,630
You've already proven that out, right?

193
00:08:52,630 --> 00:08:56,620
Over a very large period of time with
multiple iterations, experiments.

194
00:08:57,130 --> 00:08:57,790
And checks.

195
00:08:58,150 --> 00:09:00,610
So yeah, static thresholds
are very easy, right?

196
00:09:00,790 --> 00:09:02,080
What about dynamic thresholds?

197
00:09:02,080 --> 00:09:02,890
How do we get there?

198
00:09:03,490 --> 00:09:08,200
Now, dynamic thresholds in platforms
is, these are is usually called anomaly

199
00:09:08,200 --> 00:09:10,450
detection, but it's the same thing, right?

200
00:09:10,450 --> 00:09:14,140
So you want to basically capture
whether the behavior system is

201
00:09:14,560 --> 00:09:18,580
outside, what a predictive model
would say it should be in, right?

202
00:09:18,580 --> 00:09:20,290
So again, dynamic, right?

203
00:09:20,290 --> 00:09:21,310
It's not fixed.

204
00:09:21,790 --> 00:09:24,070
So the idea is that you can build them on.

205
00:09:24,340 --> 00:09:25,630
Supervised models, right?

206
00:09:25,630 --> 00:09:28,900
So in this case you get into
machine learning or tical models,

207
00:09:28,900 --> 00:09:33,010
so things like logistical regression
falls in here you have a priority

208
00:09:33,070 --> 00:09:36,610
algorithms, things like clustering,
unsupervised learning, and also core

209
00:09:36,610 --> 00:09:38,200
fitting, which is just Aris, right?

210
00:09:38,380 --> 00:09:41,590
You just basically say, this is how I
feel the strength should be, and then

211
00:09:41,590 --> 00:09:45,310
I can then find values that match it,
and then I can then use that to then

212
00:09:45,310 --> 00:09:50,530
judge the fit against whatever assign
disorder behavior that system exhibits.

213
00:09:51,325 --> 00:09:54,565
So I'll talk about curve fitting
'cause it's the one that I've

214
00:09:54,565 --> 00:09:56,155
done and I can share examples of.

215
00:09:56,215 --> 00:09:59,335
But they are, but all the other
methods I'll talk about them in a bit.

216
00:09:59,725 --> 00:10:06,145
So the basics to curve fitting is that
you have a, like a series that exhibits

217
00:10:06,145 --> 00:10:11,035
like, some or wave like wave form, like
fashion, and then you're trying to then

218
00:10:11,035 --> 00:10:13,694
find what's trend would fit on it, right?

219
00:10:13,694 --> 00:10:17,664
You can say it's a sine wave, a hand wave,
a cross wave, like whichever way you want

220
00:10:17,664 --> 00:10:22,314
to express that wave form, if, even if
it's exponential to the exact same way.

221
00:10:22,914 --> 00:10:26,334
So you start off like this, you
basically define, say perfect.

222
00:10:26,334 --> 00:10:29,394
In this case I've generat, I've
generated random data showing

223
00:10:29,394 --> 00:10:30,504
you our sales in millions.

224
00:10:30,834 --> 00:10:32,189
It goes up and down across the hour.

225
00:10:33,004 --> 00:10:37,474
All of this is basically random,
randomly distributed, but it's still

226
00:10:37,474 --> 00:10:38,765
follows as any sort of fashion.

227
00:10:39,334 --> 00:10:44,354
Then we then find a waveform
that would fit it, right?

228
00:10:44,354 --> 00:10:46,875
And here you can see what the normal
waveform would be, and here you can see

229
00:10:46,875 --> 00:10:48,704
what the fits the curve would fit for.

230
00:10:48,704 --> 00:10:52,214
It would be like I, since that
division is reasonably low, and

231
00:10:52,214 --> 00:10:55,665
then the fit is around 70%, right?

232
00:10:55,665 --> 00:10:57,314
So how do we get here?

233
00:10:57,584 --> 00:10:59,324
The idea is that remember that?

234
00:11:00,194 --> 00:11:01,305
Use case that we defined.

235
00:11:01,334 --> 00:11:04,275
So we obviously didn't have to estimate
whether the fit is good or not.

236
00:11:04,364 --> 00:11:07,425
You can use the cheese square, good
fit or standard deviation, which

237
00:11:07,484 --> 00:11:08,984
is used in cheese square, good fit.

238
00:11:09,764 --> 00:11:13,155
And then cheese square good fit is
better because it also takes into

239
00:11:13,155 --> 00:11:16,514
consideration the number of samples as
opposed to standard deviation where I

240
00:11:16,514 --> 00:11:20,594
just checking the range of deviation
from those, of the samples from the mean.

241
00:11:21,194 --> 00:11:27,849
So you get there by taking A, B, and
C. So A can be the amplitude B can.

242
00:11:28,489 --> 00:11:32,420
B, I guess the phase multiply
and then CS the constant, right?

243
00:11:32,420 --> 00:11:36,739
So you take all of those together, you
then iterate across different values of

244
00:11:36,739 --> 00:11:40,129
A, B, and C to see like which one fits
better, and then you then basically

245
00:11:40,129 --> 00:11:41,509
check what the error there would be.

246
00:11:41,719 --> 00:11:44,629
Or in this case, you can
use the FIT formula, right?

247
00:11:44,629 --> 00:11:47,869
So then judge like how well it actually
fits the curve that you defined.

248
00:11:48,469 --> 00:11:52,699
It's the, for core fitting, the
V variables are bounded, right?

249
00:11:52,699 --> 00:11:55,819
You can bound it based on, say,
the amplitude of the curve, right?

250
00:11:56,059 --> 00:11:57,409
Like how high does it go.

251
00:11:57,889 --> 00:12:00,499
And then you can obviously then
use that to then find, okay, fine.

252
00:12:00,499 --> 00:12:03,949
If a look like it's tend towards the
fits, then you can then change B and

253
00:12:03,949 --> 00:12:05,659
C to see whether fits better or not.

254
00:12:06,149 --> 00:12:10,059
So that would be that regressive the
defense model for error correction.

255
00:12:10,569 --> 00:12:13,269
So yeah, it's very simple,
it's very straightforward.

256
00:12:13,549 --> 00:12:16,549
There also are methods for
doing, dynamic thresholds, right?

257
00:12:16,549 --> 00:12:19,729
Based on that machine learning phase
of either you're doing anomaly bands,

258
00:12:20,269 --> 00:12:23,899
or in this case you're just having
one part trying to, approximate

259
00:12:24,019 --> 00:12:25,249
what that threshold should be.

260
00:12:25,949 --> 00:12:27,780
You can use logistic regression methods.

261
00:12:27,780 --> 00:12:31,759
K Ns, you can use naive phase scaler
vector machines also fitting in here.

262
00:12:31,819 --> 00:12:35,790
And you also have so these ones you
would've classified data, they would

263
00:12:35,790 --> 00:12:37,920
label properly, everything else.

264
00:12:37,920 --> 00:12:38,939
So this is more supervised.

265
00:12:39,624 --> 00:12:41,844
The case where you have like
unsupervised methods, right?

266
00:12:41,844 --> 00:12:43,464
So you can use isolation forest.

267
00:12:43,774 --> 00:12:46,834
The case that I showed before of those
are normally band with the open and

268
00:12:46,834 --> 00:12:49,015
lu with the open lower fits, right?

269
00:12:49,015 --> 00:12:51,485
You can obviously then
use density based cans.

270
00:12:51,485 --> 00:12:55,735
This case, this example, use them
PCA for filtering, alignments

271
00:12:55,795 --> 00:12:56,755
and all those sort bits.

272
00:12:57,145 --> 00:12:59,885
So that this is also an
example where you can do.

273
00:13:00,755 --> 00:13:05,795
Anomaly detection on random scatter,
upload data with, unsupervised learning

274
00:13:06,125 --> 00:13:09,445
methods like density based scans and PCAs.

275
00:13:10,025 --> 00:13:10,325
Yeah.

276
00:13:10,815 --> 00:13:16,085
The summary of all of this is that to
basically use numerical methods, you can

277
00:13:16,085 --> 00:13:19,415
identify the business opportunities like
the kids that I've mentioned with the

278
00:13:19,415 --> 00:13:24,935
dynamic autoscaling or say in the non,
technical sense where you're just trying

279
00:13:24,935 --> 00:13:28,205
to see whether user orders have increased
so that you can scale operations.

280
00:13:28,775 --> 00:13:32,735
All these things are more, you can use
them to basically predict what that value

281
00:13:32,735 --> 00:13:34,805
should be if it lies outside effects.

282
00:13:34,865 --> 00:13:38,445
And then you can obviously
then, take action over time.

283
00:13:38,925 --> 00:13:42,045
Organic growth and, pass data
will influence these models.

284
00:13:42,045 --> 00:13:45,285
So they would just auto fit over
time, but at least now you can

285
00:13:45,285 --> 00:13:48,545
capture, those trends without having
to get paged every minute or two.

286
00:13:49,125 --> 00:13:50,085
Yeah, confirm with that.

287
00:13:50,085 --> 00:13:51,585
You need dynamic or static threshold.

288
00:13:51,870 --> 00:13:53,520
Determine what the noise step should be.

289
00:13:53,970 --> 00:13:56,670
You can use cut up plots, you can
use clustering methods, you can

290
00:13:56,670 --> 00:13:58,440
use iqr, you can use these scores.

291
00:13:58,440 --> 00:14:01,020
There's various ways to actually
check whether your dataset is

292
00:14:01,470 --> 00:14:02,910
fit for these methods or not.

293
00:14:03,720 --> 00:14:06,330
And then from there, just iterate
until the error is minimized.

294
00:14:06,370 --> 00:14:09,250
A lot of these things are iterative
methods, multicolor in that.

295
00:14:09,880 --> 00:14:11,650
And, whichever we choose to define them.

296
00:14:11,650 --> 00:14:12,730
These are just methods.

297
00:14:12,730 --> 00:14:15,550
They don't particularly define the
approach, but it's a good way to

298
00:14:15,550 --> 00:14:19,150
think about thresholds and statistics
when you're modeling, monitoring

299
00:14:19,150 --> 00:14:23,250
systems or just trying to improve
the way that you iterate everywhere.

300
00:14:23,830 --> 00:14:25,060
Yeah, thank you all.

301
00:14:25,160 --> 00:14:27,110
Again, you don't need
to use all these guys.

302
00:14:27,110 --> 00:14:31,160
Sometimes eyeballing might be
worth all of these, but if your

303
00:14:31,220 --> 00:14:33,170
systems behave in very weird ways.

304
00:14:33,995 --> 00:14:36,965
It might be worth the deal to with
statistics to save you months of headache.

305
00:14:37,685 --> 00:14:38,345
Thank you all.

306
00:14:38,495 --> 00:14:40,295
Go forward and explore thresholds.

307
00:14:40,505 --> 00:14:40,715
Thank you.

