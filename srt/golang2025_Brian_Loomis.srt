1
00:00:00,000 --> 00:00:03,720
Hello, my name's Brian and I'll be
speaking with you today about four

2
00:00:03,720 --> 00:00:07,920
quality attributes involved in building
enterprise level distributed solutions.

3
00:00:08,640 --> 00:00:13,260
Those are availability, resilience,
reliability to a lesser degree.

4
00:00:13,260 --> 00:00:16,040
Scalability, do you have
a distributed system?

5
00:00:16,190 --> 00:00:19,520
Will you bang if you have multiple
applications in your solution,

6
00:00:19,520 --> 00:00:22,580
which operate across multiple
networked environments at once?

7
00:00:22,610 --> 00:00:27,170
For example, if you have a browser on
a user's laptop, that opens up a user

8
00:00:27,170 --> 00:00:32,150
experience on a remote server and that
connects to a separate SaaS, API in the

9
00:00:32,150 --> 00:00:36,199
cloud, which may call other systems,
which live in other clouds or route,

10
00:00:36,199 --> 00:00:39,529
may be back to a specific data center
operated by another organization.

11
00:00:40,030 --> 00:00:45,100
I'll show you a few examples of how we're
going to deal with building resilient

12
00:00:45,100 --> 00:00:49,320
cloud native platforms, including
our latest DevOps offering from chef.

13
00:00:49,820 --> 00:00:52,940
Each of these quality attributes can
be designed into your system to meet

14
00:00:52,940 --> 00:00:56,270
your specific technical goals, possibly
building up to offering a service

15
00:00:56,270 --> 00:00:57,860
level agreement to your customers.

16
00:00:58,640 --> 00:01:02,000
First, we'll dive into the theory, talk
a little bit about tools we have in the

17
00:01:02,000 --> 00:01:05,570
open source GoLine community, and show
a couple worked examples, which should

18
00:01:05,570 --> 00:01:09,260
give you a practical way to plan and
test your own distributed applications.

19
00:01:09,740 --> 00:01:12,860
This talk does stay at a high level
and does not contain any proprietary

20
00:01:12,860 --> 00:01:14,510
information from any of the examples.

21
00:01:15,010 --> 00:01:17,890
Rest assured, they're real world
examples that are systems that

22
00:01:17,890 --> 00:01:19,000
are out in production today.

23
00:01:19,960 --> 00:01:22,360
I'll also show a few diagrams
as we go through this.

24
00:01:22,390 --> 00:01:25,330
Some are my own, but where it's a
picture that I've borrowed from someone

25
00:01:25,330 --> 00:01:29,260
else, I've cited the publicly available
source in the slide notes, let's

26
00:01:29,260 --> 00:01:30,970
take a look at why this is important.

27
00:01:31,470 --> 00:01:32,250
So we start with this.

28
00:01:32,255 --> 00:01:35,710
This is a pretty well, versed,
Picture set of points, that talk

29
00:01:35,710 --> 00:01:38,740
about distributed computing that kind
of made the rounds and everything.

30
00:01:38,860 --> 00:01:41,830
And this is a set of assumptions
we might start a project with.

31
00:01:41,880 --> 00:01:45,930
and we generally know from experience
that as we get out to a production

32
00:01:45,930 --> 00:01:49,530
grade system, a lot of these things
tend not to hold true, right?

33
00:01:50,370 --> 00:01:53,970
These reflect the lessons of systems
engineering, the engineering of

34
00:01:53,970 --> 00:01:57,480
multiple systems as opposed to single
self-contained systems or maybe

35
00:01:57,480 --> 00:02:00,120
even MVPs over the last few decades.

36
00:02:00,750 --> 00:02:04,260
In software, we often start with
that core system, an MVP, a proof of

37
00:02:04,260 --> 00:02:09,590
concept, and then if we're successful,
we grow, number of users, increased

38
00:02:09,590 --> 00:02:13,310
functionality, possibly broadening
out to additional tools, to different

39
00:02:13,310 --> 00:02:18,560
integrations, and then we start seeing
failure modes as that system grows, which

40
00:02:18,560 --> 00:02:20,870
are both analyzable and preventable.

41
00:02:21,370 --> 00:02:25,510
The key is to get design in there
at the right point in this path.

42
00:02:26,200 --> 00:02:30,400
Hopefully the quality attribute aspects
are designed are considered upfront, but

43
00:02:30,400 --> 00:02:34,300
the techniques we see today will certainly
be able to be added later as needed.

44
00:02:34,450 --> 00:02:37,120
Just a little bit higher
cost to implement and test.

45
00:02:37,630 --> 00:02:40,180
The other lesson we've learned
is that there are many levels

46
00:02:40,180 --> 00:02:44,230
of quality from good enough to
supporting higher performance levels.

47
00:02:44,560 --> 00:02:48,400
Maybe we phrase this as a service
level agreement or SLA, which is

48
00:02:48,490 --> 00:02:53,560
effectively a more specific quality
contract with the customer, both what

49
00:02:53,560 --> 00:02:57,550
the customer may see as value and
expect in the solution, and what an

50
00:02:57,550 --> 00:02:59,980
engineering team has confidence in read.

51
00:03:00,130 --> 00:03:04,420
We've tested it and can provide a
guarantee during normal operating

52
00:03:04,420 --> 00:03:07,175
conditions, specifically to
talk to some of these points.

53
00:03:07,175 --> 00:03:11,250
Distributed systems face challenges in
the fact that the network connection

54
00:03:11,250 --> 00:03:14,250
between different subsystems
is fundamentally unreliable.

55
00:03:14,750 --> 00:03:18,020
In timing some operations taking
longer than others to complete or

56
00:03:18,020 --> 00:03:22,460
identify an error condition, other
actors may interfere with our systems.

57
00:03:22,520 --> 00:03:24,320
Maybe we call this secure operations.

58
00:03:24,620 --> 00:03:28,190
There may be changes to the network
topology itself, which our applications

59
00:03:28,190 --> 00:03:33,355
will need know about and looking at,
number eight, the conception that

60
00:03:33,355 --> 00:03:34,880
all networks are exactly the same.

61
00:03:35,380 --> 00:03:38,230
Each of these aspects focuses on
the networking and integration

62
00:03:38,230 --> 00:03:39,430
side between subsystems.

63
00:03:39,430 --> 00:03:43,450
And this is one of three major classes
of failures which may affect our system.

64
00:03:44,290 --> 00:03:49,060
The other two, apart from networking
being hardware failure, probably CPU

65
00:03:49,060 --> 00:03:50,990
or some, memory or something like that.

66
00:03:51,050 --> 00:03:54,110
And then the last one being
storage and persistence failures.

67
00:03:54,110 --> 00:03:57,890
When we talk about writing things
to disc in persisting state, we'll

68
00:03:57,890 --> 00:04:01,130
see all these as we analyze our
system for likely types of failures.

69
00:04:01,250 --> 00:04:04,100
In short, there are lots of things that
can go wrong as we think about how our

70
00:04:04,100 --> 00:04:08,150
subsystems work together, and we'll rely
on systems thinking to look at which are

71
00:04:08,150 --> 00:04:11,030
most likely and how we mitigate the risks.

72
00:04:11,960 --> 00:04:15,830
That typically mitigating risks is
reducing the impact of them if they

73
00:04:15,830 --> 00:04:19,460
do occur, or reducing the likelihood
that they'll actually occur at all.

74
00:04:19,730 --> 00:04:23,720
And we're gonna do this through both
design and implementation, right?

75
00:04:23,900 --> 00:04:26,900
So we're gonna start from the
architecture side and we're going to

76
00:04:27,320 --> 00:04:32,120
add elements to our design that help
make it more reliable, robust over time.

77
00:04:32,620 --> 00:04:36,310
Now, at the end, we'll probably make
sure that we continually test for changes

78
00:04:36,310 --> 00:04:38,140
in operating the system as well, right?

79
00:04:38,140 --> 00:04:42,310
So we're gonna make sure that those
improvements that we've made show up again

80
00:04:42,310 --> 00:04:47,080
in the system and that we cer see certain
types of failures that are either masked

81
00:04:47,350 --> 00:04:50,590
we can deal with or don't occur at all.

82
00:04:51,090 --> 00:04:53,580
If you're thinking this sounds a
little bit like failure modes, effects

83
00:04:53,580 --> 00:04:58,050
analysis, FMEA, you'd be correct in
how we identify possible failures.

84
00:04:58,200 --> 00:05:01,740
We'll show some examples of how this
process is not so daunting, and you

85
00:05:01,740 --> 00:05:03,450
can apply this on your system as well.

86
00:05:03,990 --> 00:05:05,280
Let's take a look at what's coming up.

87
00:05:05,780 --> 00:05:08,975
We will talk about topics in this
order here, serve as a rough agenda.

88
00:05:09,635 --> 00:05:11,705
We'll talk about failures
and some definitions of our

89
00:05:11,705 --> 00:05:13,385
quality attributes in SLAs.

90
00:05:13,625 --> 00:05:16,805
We'll dive a little bit into patterns
both on the infrastructure side and

91
00:05:16,805 --> 00:05:18,125
the application development side.

92
00:05:18,455 --> 00:05:21,845
It's important to note this is a shared
responsibility and that both pieces

93
00:05:21,845 --> 00:05:26,285
of this puzzle of our design, of our
architecture make a successful system.

94
00:05:26,345 --> 00:05:28,865
We're gonna make changes on the
infrastructure side and we're

95
00:05:28,865 --> 00:05:32,315
gonna make changes inside the
software itself in the code.

96
00:05:33,035 --> 00:05:37,355
That's gonna make, give us a well
designed platform and software

97
00:05:37,355 --> 00:05:38,825
controls that go on top of that.

98
00:05:39,425 --> 00:05:41,885
I'll show a simple example and
then we'll build up to a couple

99
00:05:41,885 --> 00:05:45,335
more complex ones as we go, as we
show applying these techniques.

100
00:05:45,965 --> 00:05:48,965
Then we're gonna go into Golang
specifically and take a look at some of

101
00:05:48,965 --> 00:05:51,935
the tools that are available for us to
do so we don't have to write our own.

102
00:05:52,385 --> 00:05:54,845
We'll follow up with a little
bit on testing and then some

103
00:05:54,845 --> 00:05:57,695
final summary, summary slide.

104
00:05:58,475 --> 00:06:02,015
Each slide has references on the specific
topics described, and then there's some.

105
00:06:02,515 --> 00:06:04,795
Overview references on
the very last slide.

106
00:06:05,065 --> 00:06:09,415
So on the slide notes for each
slide, you'll see more specifics

107
00:06:09,415 --> 00:06:10,675
on what that slide covers.

108
00:06:11,175 --> 00:06:14,985
Now I'll talk a little bit about
why we're actually here and why

109
00:06:14,985 --> 00:06:16,845
I'm talking about this today.

110
00:06:16,845 --> 00:06:22,275
And it really starts with our journey here
at Chef and we came up with the idea of

111
00:06:22,275 --> 00:06:26,715
a brand new DevOps solution, which gives
fundamental capabilities of monitoring

112
00:06:26,715 --> 00:06:30,825
a customer's infrastructure assets, do
some things that our previous systems

113
00:06:30,825 --> 00:06:34,215
had done in terms of infrastructure,
configuration management compliance,

114
00:06:34,455 --> 00:06:36,465
typical DevOps sort of work processes.

115
00:06:36,825 --> 00:06:39,945
And then extending this into a general
purpose job system, which could work

116
00:06:39,945 --> 00:06:44,865
across customer data centers on-prem,
multiple clouds, that they may have assets

117
00:06:44,865 --> 00:06:49,545
in desktops, and which we could also
bring to market as a software as a service

118
00:06:49,545 --> 00:06:51,885
offering for a large number of customers.

119
00:06:52,065 --> 00:06:54,345
So multi-tenant type offering here.

120
00:06:55,125 --> 00:06:59,055
This system started as an MVP over a
year ago, and we launched our first

121
00:06:59,055 --> 00:07:01,695
version AWS last month of SaaS solution.

122
00:07:02,415 --> 00:07:06,135
As we talk to more customers, probably
similar to your journey, we had lots of

123
00:07:06,135 --> 00:07:08,265
questions about these quality attributes.

124
00:07:08,625 --> 00:07:12,015
What could the individual customer
expect in terms of system performance?

125
00:07:12,135 --> 00:07:13,755
How often could they do jobs?

126
00:07:13,845 --> 00:07:16,095
How many assets could they
include in these jobs?

127
00:07:16,215 --> 00:07:17,625
What if the system went down?

128
00:07:18,015 --> 00:07:20,595
What if the network went
down between us and them?

129
00:07:20,955 --> 00:07:23,415
What would happen to jobs in
flight or ones which had not been

130
00:07:23,415 --> 00:07:24,705
delivered, but an outage habit?

131
00:07:25,125 --> 00:07:30,765
Could chef offer a service level agreement
if these are things that your system is

132
00:07:30,975 --> 00:07:32,595
trying to answer questions on as well?

133
00:07:32,805 --> 00:07:34,065
This is probably the talk for you.

134
00:07:34,565 --> 00:07:38,195
We've learned a lot about how our
system performs over the last year

135
00:07:38,225 --> 00:07:40,895
and we realize this is really a
continuous improvement problem.

136
00:07:40,895 --> 00:07:44,435
So a lot of the things we're gonna
talk about here, though, they are

137
00:07:44,435 --> 00:07:48,425
analysis that we've done on our system
and we've learned from other systems

138
00:07:48,425 --> 00:07:50,825
that other architects have brought us.

139
00:07:50,945 --> 00:07:51,965
it shows a lot of the.

140
00:07:52,445 --> 00:07:56,765
Way of thinking about these
and how to inject design into

141
00:07:56,825 --> 00:07:58,475
the application as it grows.

142
00:07:58,975 --> 00:08:02,125
It's easier to state goals than it
is to prove a system will not fill.

143
00:08:02,545 --> 00:08:06,675
A lot of times a more approachable way
for an engineer to deal with a system

144
00:08:06,675 --> 00:08:10,545
and improve quality attributes is to
plan the organic development of features.

145
00:08:10,605 --> 00:08:13,635
In other words, making a
process your own right.

146
00:08:14,055 --> 00:08:17,270
So you have to have kind of a checklist
or a set of tasks that you're gonna

147
00:08:17,270 --> 00:08:21,140
execute probably over and over again
that fit within your development process.

148
00:08:21,590 --> 00:08:24,380
These steps are just one way
that we've listed out to build

149
00:08:24,380 --> 00:08:27,230
availability and reliability into
your distributed application.

150
00:08:27,770 --> 00:08:31,160
This design process works for both
large systems and smaller systems

151
00:08:31,160 --> 00:08:32,750
with less stringent requirements.

152
00:08:33,250 --> 00:08:38,050
So fundamentally, just go through these,
start with your system diagram and what

153
00:08:38,050 --> 00:08:40,060
your business process description is.

154
00:08:40,390 --> 00:08:41,920
Figure out what your goals are.

155
00:08:42,370 --> 00:08:45,190
We'll show how that can be expressed
as a service level agreement or

156
00:08:45,190 --> 00:08:48,490
maybe quantitative measures that
you can take in your system.

157
00:08:49,270 --> 00:08:53,320
And then go through a brainstorming
exercise that we'll call FMEA

158
00:08:53,800 --> 00:08:56,440
to understand what failure
modes might be present.

159
00:08:57,280 --> 00:09:00,250
Then you're gonna go through
looking for design changes.

160
00:09:00,400 --> 00:09:04,510
We're gonna back this with particular
patterns that we know from previous

161
00:09:04,510 --> 00:09:07,780
experiences that have worked, and we'll
list a few of those as we go along here.

162
00:09:08,140 --> 00:09:12,490
And then finally, after you've implemented
them and deployed your solution test and

163
00:09:12,490 --> 00:09:16,450
make sure that you're meeting your goals,
from an operations perspective, you're

164
00:09:16,450 --> 00:09:18,610
gonna continue monitoring your solution.

165
00:09:18,880 --> 00:09:22,940
You're going to automate your,
playbooks or your standard

166
00:09:22,940 --> 00:09:24,620
operating procedures to recover.

167
00:09:25,100 --> 00:09:25,790
That's great.

168
00:09:26,630 --> 00:09:28,940
But the important thing for the
development team, and certainly an

169
00:09:28,940 --> 00:09:33,080
infrastructure team supporting a
solution, is that you're gonna learn

170
00:09:33,080 --> 00:09:34,640
from operations as well, right?

171
00:09:34,640 --> 00:09:37,460
So when you have a root cause
analysis after an outage.

172
00:09:38,150 --> 00:09:40,460
Turn that around and make sure
that turns into the design

173
00:09:40,460 --> 00:09:41,480
cycle at the very beginning.

174
00:09:41,480 --> 00:09:46,300
Again, the image on the right is one of
an uptime page for a set of services.

175
00:09:46,300 --> 00:09:49,720
We've probably seen a lot of these,
and really this is our end goal

176
00:09:49,720 --> 00:09:53,280
for our efforts is to let our
customers know if the services up

177
00:09:53,280 --> 00:09:56,400
or down, hopefully minimizing the
down and we have all green here.

178
00:09:56,880 --> 00:10:00,280
And if it does go down, what
can we communicate to them

179
00:10:00,340 --> 00:10:01,510
about when it'll be back?

180
00:10:02,010 --> 00:10:06,120
The key with large systems is really
that it's an automation game, right?

181
00:10:06,120 --> 00:10:09,570
So we can't rely on manual
processes as much when we get

182
00:10:09,570 --> 00:10:11,310
to larger and larger installs.

183
00:10:11,790 --> 00:10:15,390
more customers, more feature sets,
more distributed type operations.

184
00:10:15,660 --> 00:10:19,260
And so we're gonna have to bake this
all into the system design itself.

185
00:10:19,830 --> 00:10:23,550
We can't rely on the old days of, oh,
we'll just go reboot the server and

186
00:10:23,550 --> 00:10:26,870
you'll be back, we'll send an email out
to customers or something like that.

187
00:10:27,380 --> 00:10:31,350
when you get to large scale solutions,
that's not as, either helpful or

188
00:10:31,350 --> 00:10:36,760
promising to customers or as, easy
to do because you have large data

189
00:10:36,760 --> 00:10:38,080
backups that you have to restore.

190
00:10:38,080 --> 00:10:40,450
Potentially that might
take a long time, right?

191
00:10:40,450 --> 00:10:43,670
Might have a sustained outage, just
due to the scale of the system.

192
00:10:44,510 --> 00:10:47,630
Now, this observability and self-service
is really what we're striving for.

193
00:10:47,630 --> 00:10:51,140
So you're gonna see a lot of
our techniques here rely on how

194
00:10:51,140 --> 00:10:52,460
do you bake it into automation?

195
00:10:52,460 --> 00:10:55,160
How do you take metrics
actively on the system?

196
00:10:55,520 --> 00:10:59,510
How do you feed those back into
certain types of behaviors that

197
00:10:59,510 --> 00:11:00,800
the system can adapt around?

198
00:11:00,800 --> 00:11:01,580
Failures?

199
00:11:02,080 --> 00:11:05,710
Next we're gonna jump into a little bit
on the common definitions of these quality

200
00:11:05,710 --> 00:11:09,400
attributes and how we might describe them
to others in the business or as customers.

201
00:11:09,460 --> 00:11:10,540
We've talked about SLAs.

202
00:11:10,810 --> 00:11:14,560
Let's jump into a couple of these sorts
of things and talk about availability

203
00:11:15,060 --> 00:11:16,170
as a first definition.

204
00:11:17,100 --> 00:11:18,450
We're gonna take a look at.

205
00:11:18,885 --> 00:11:20,775
One of our quality
attributes availability.

206
00:11:21,405 --> 00:11:24,735
This is the ability of a service to be
able to take requests for a percentage

207
00:11:24,735 --> 00:11:26,385
of a defined operating period.

208
00:11:27,075 --> 00:11:28,845
Okay, so this is the front door, right?

209
00:11:29,025 --> 00:11:30,615
It's open to submitting requests.

210
00:11:30,615 --> 00:11:34,335
The system seems up and running to
the user for a percentage of time,

211
00:11:34,935 --> 00:11:39,825
even though those results may delay by
internal errors, failure conditions.

212
00:11:40,545 --> 00:11:42,885
We have the table on the left
that shows how this percentage is

213
00:11:42,885 --> 00:11:46,845
determined, defined in terms of
outages as seen by the customer, right?

214
00:11:47,205 --> 00:11:51,825
Just different types of systems and usage
by different customers may influence what

215
00:11:51,825 --> 00:11:53,235
the system availability is built for.

216
00:11:53,715 --> 00:11:57,825
Not all systems need five nines,
and many systems are not even tested

217
00:11:57,825 --> 00:11:59,565
for three nines or four nines.

218
00:12:00,285 --> 00:12:04,215
For example, voice software like
Zoom or something like that, may

219
00:12:04,215 --> 00:12:06,705
want five or six nines, right?

220
00:12:06,705 --> 00:12:12,065
It may really want to have very accurate,
packet transmission so that you don't

221
00:12:12,065 --> 00:12:16,385
have a break in the signal quality,
whereas email or a non-mission critical

222
00:12:16,385 --> 00:12:21,335
application, maybe your time card tool
mailing, you need three nights, right?

223
00:12:21,905 --> 00:12:24,065
And maybe some days you
wish it were or less.

224
00:12:24,325 --> 00:12:26,965
on the right side panel is a
description of some hypothetical

225
00:12:27,025 --> 00:12:28,375
impacts of this availability.

226
00:12:28,705 --> 00:12:33,545
This is from Mark Richard's, blog and
an article, which is in the slide notes.

227
00:12:33,545 --> 00:12:37,085
I mentioned that before, but gives you
an idea of what do, what does, what do

228
00:12:37,085 --> 00:12:41,485
different nines actually mean in real
life here right now, you could get lucky

229
00:12:41,545 --> 00:12:45,295
if your system is simple enough, or you
do not actually measure the availability.

230
00:12:45,535 --> 00:12:49,525
Your system may have a particular
built in high availability already.

231
00:12:50,025 --> 00:12:52,425
There's a lot of cases though, this
isn't enough to give a customer

232
00:12:52,425 --> 00:12:54,135
a formal service level agreement.

233
00:12:54,750 --> 00:12:58,940
Or a contract, which says we're
going to hit this, or there are

234
00:12:58,940 --> 00:13:00,950
some, repercussions afterwards.

235
00:13:01,000 --> 00:13:04,990
There are, however, design changes we can
make to get to a desired availability,

236
00:13:05,050 --> 00:13:08,470
both in terms of making sure that the
theoretical redundancy in the system

237
00:13:08,470 --> 00:13:12,760
supports such a statement, and by
masking failures, maybe caching data

238
00:13:12,880 --> 00:13:16,030
to make sure that a client application
doesn't notice that a backend service is

239
00:13:16,030 --> 00:13:18,220
temporarily down or intermittently down.

240
00:13:18,720 --> 00:13:22,020
Now, there is obviously a balancing
act that goes on here, right?

241
00:13:22,020 --> 00:13:26,350
So software and infrastructure
improvements here tend have, they

242
00:13:26,350 --> 00:13:30,160
may drive higher availability, but
they have an increased cost, and

243
00:13:30,160 --> 00:13:34,300
not all customers may want or want
to pay for something like that.

244
00:13:34,300 --> 00:13:35,680
That would be in an SLA, right?

245
00:13:35,680 --> 00:13:40,420
So if an SLA is appropriate to you,
if a particular availability number

246
00:13:40,420 --> 00:13:43,750
is appropriate to you, question,
what's the value statement here?

247
00:13:43,780 --> 00:13:47,710
Are we building this just because we
think it needs to be this, or are we

248
00:13:47,710 --> 00:13:49,480
building this because our customers.

249
00:13:49,840 --> 00:13:54,420
Really have a need for it to be this
note that not all components have to

250
00:13:54,420 --> 00:13:57,570
be running, and data does not have
to be consistent up to the minute

251
00:13:58,200 --> 00:13:59,940
only the operations can be started.

252
00:14:00,870 --> 00:14:03,660
This also talks a little bit about
how little time you actually have to

253
00:14:03,660 --> 00:14:05,370
return to operations after a failure.

254
00:14:05,670 --> 00:14:10,230
Manual processes can propagate you to two
nines, but you're gonna need automated

255
00:14:10,230 --> 00:14:13,920
recovery and monitoring at multiple
points in the system to get to four nines.

256
00:14:14,700 --> 00:14:17,730
The calculation shown at the bottom, which
can be easily implemented and calculated

257
00:14:17,730 --> 00:14:21,390
in a running system through tracking
health checks and telemetry metrics.

258
00:14:21,890 --> 00:14:24,920
Two additional definitions we see
a lot are a service level agreement

259
00:14:24,980 --> 00:14:26,810
and a service level objective.

260
00:14:27,260 --> 00:14:33,140
So typically an SLA is an overall
SaaS service availability target, and

261
00:14:33,590 --> 00:14:37,220
probably has a disaster recovery plan
based on reliability and recovery.

262
00:14:38,090 --> 00:14:41,105
Office 365 is listed in the
notes as a good reference for

263
00:14:41,105 --> 00:14:42,170
how to build one of these.

264
00:14:42,320 --> 00:14:44,780
It's typically a contractual.

265
00:14:45,065 --> 00:14:49,145
Thing between the service
provider and the customer, right?

266
00:14:49,145 --> 00:14:52,625
So it says what happens if
a service level is not met?

267
00:14:53,045 --> 00:14:55,595
A service level objective, on
the other hand, builds up to an

268
00:14:55,595 --> 00:14:57,905
SLA often is internal, right?

269
00:14:57,905 --> 00:15:01,595
Where we have multiple dependent
services with different operators.

270
00:15:01,835 --> 00:15:06,215
Each operator may have their own
objective, which supports the overall SLA.

271
00:15:06,455 --> 00:15:10,655
For example, an IT department may
maintain data center hardware that the

272
00:15:10,655 --> 00:15:12,395
SaaS product team is deployed upon.

273
00:15:13,325 --> 00:15:17,855
So they would have an SLO to that
SaaS product, SLA or a security

274
00:15:17,855 --> 00:15:19,805
team may operate active directory.

275
00:15:19,805 --> 00:15:24,655
The IDENT identity system, which an
application may need to have a certain

276
00:15:24,655 --> 00:15:26,755
uptime to meet its overall SLA.

277
00:15:27,255 --> 00:15:30,105
Let's take a look at another
definition here as well.

278
00:15:30,105 --> 00:15:31,115
I. Oh, sorry.

279
00:15:31,115 --> 00:15:35,495
Another, example, this is one from
a sample retail store, so we'll

280
00:15:35,495 --> 00:15:37,175
see this in our example here.

281
00:15:37,425 --> 00:15:40,935
imagine that you can do e-commerce
orders into a store and you can

282
00:15:40,935 --> 00:15:44,355
also walk into the store and order
things and get delivery of them.

283
00:15:44,685 --> 00:15:48,520
And so this would be si similar
for a branch office or any

284
00:15:48,520 --> 00:15:50,210
other, distributed environment.

285
00:15:50,690 --> 00:15:54,170
The first statement in red text is
really a highly caveated objective,

286
00:15:54,170 --> 00:15:55,640
and it's probably unreachable.

287
00:15:56,120 --> 00:15:59,060
It's an availability statement, but
states a hundred percent uptime,

288
00:15:59,120 --> 00:16:01,010
meaning no outer can be tolerated.

289
00:16:02,000 --> 00:16:05,210
Hence the caveats for any
bad incidents or whatever.

290
00:16:05,360 --> 00:16:08,390
A flood or anything like that
would invalidate the statement.

291
00:16:09,080 --> 00:16:12,200
The second statement is really
a description of five nines

292
00:16:12,200 --> 00:16:13,490
for a specific subsystem.

293
00:16:13,990 --> 00:16:16,780
This might be more appropriate as an SLO.

294
00:16:17,280 --> 00:16:21,900
And this is for the payment system which
lives on top of a platform and connects

295
00:16:21,900 --> 00:16:25,960
to external banks that we're communicating
with, as one end of an integration.

296
00:16:26,050 --> 00:16:29,740
We see this a lot in microservices
architecture where one API may call

297
00:16:29,740 --> 00:16:33,450
another, in this case it's in a
different environment altogether.

298
00:16:34,200 --> 00:16:39,120
And we have to assume then that both sides
of the integration since one is, has a

299
00:16:39,120 --> 00:16:44,460
dependency on the other, are at that five
nines level or are marked explicitly out

300
00:16:44,460 --> 00:16:46,440
of scope for an availability statement.

301
00:16:46,940 --> 00:16:49,970
We've seen a lot at how we look
at the customer experience through

302
00:16:49,970 --> 00:16:53,960
availability here, how the system can
take inputs to initiate functional flows.

303
00:16:54,410 --> 00:16:57,980
Let's take a look at resilience and
reliability next, and that's gonna be

304
00:16:58,490 --> 00:17:02,420
the ability of the system to correctly
process until either successful

305
00:17:02,420 --> 00:17:06,020
completion or a known error condition
that can be reflected back to the

306
00:17:06,020 --> 00:17:11,240
user, but to go all the way through
and mask any failures that might occur.

307
00:17:11,740 --> 00:17:14,710
When we talk about resilience, we're
really talking about reliability.

308
00:17:14,860 --> 00:17:19,390
The ability of the system to respond to
error conditions appropriately, reflecting

309
00:17:19,390 --> 00:17:23,560
back to the user what happened, and to
continue processing subsequent requests

310
00:17:23,650 --> 00:17:27,520
without a sustained outage or a failure
across the entire business process.

311
00:17:28,120 --> 00:17:32,200
To do this, we need to make sure that the
system can detect failures, maybe with

312
00:17:32,200 --> 00:17:34,330
error handling or infrastructure failover.

313
00:17:34,830 --> 00:17:37,560
Optionally report that failure to
a person to take corrective action,

314
00:17:37,920 --> 00:17:41,855
proceed to determine the type of
failure and therefore what recovery plan

315
00:17:42,060 --> 00:17:45,420
gonna be put in place, what automated
recovery plans gonna be put in place.

316
00:17:45,810 --> 00:17:48,330
Since failures can happen, lots
of different levels of the system,

317
00:17:48,630 --> 00:17:53,160
the responses which ideally are
automated, will span both hardware

318
00:17:53,250 --> 00:17:55,200
and platform reliability fixes.

319
00:17:55,650 --> 00:17:58,350
And then you're also gonna have
software components as well.

320
00:17:58,560 --> 00:18:00,030
You'll see these in the examples.

321
00:18:00,530 --> 00:18:04,110
The last one here we're gonna talk about,
after resilience is really reliability,

322
00:18:04,110 --> 00:18:06,390
which is much more quantitative.

323
00:18:06,900 --> 00:18:10,890
Reliability is the percentage of time
which a system or subsystem performs

324
00:18:10,920 --> 00:18:15,690
reliably and is normally applied to
a complete business process, such as

325
00:18:15,690 --> 00:18:20,640
purchasing a process, purchasing a product
as an end-to-end workflow, as opposed

326
00:18:20,640 --> 00:18:23,730
to availability, which is the amount
of uptime for submitting a request.

327
00:18:23,760 --> 00:18:28,410
Reliability is the amount of time the
system can perform entire operations,

328
00:18:28,590 --> 00:18:30,540
the ability to complete the operations.

329
00:18:30,870 --> 00:18:34,590
It's usually measured by two metrics,
meantime to failure and meantime between

330
00:18:34,590 --> 00:18:38,390
failure for software systems, usually we
combine this into a single measurement

331
00:18:38,390 --> 00:18:43,870
because MTTF really is a fundamentally
a hardware type, measurement there.

332
00:18:44,470 --> 00:18:47,680
And then you can, and you can see
there meantime between failures

333
00:18:47,680 --> 00:18:50,680
is something that we can actually
measure on the software side where

334
00:18:50,680 --> 00:18:52,030
we have outages and things like that.

335
00:18:52,530 --> 00:18:56,490
If the system becomes unable to
process, that is it suffers an outage.

336
00:18:57,300 --> 00:19:01,410
We're going to define two particular
terms related to the recovery.

337
00:19:01,830 --> 00:19:05,340
We're gonna talk about a recovery point
objective and a recovery time objective.

338
00:19:05,820 --> 00:19:10,740
The RPO is effectively how much data is
lost between the time when the system

339
00:19:10,740 --> 00:19:14,190
goes down and when it's recovered
or declared, normally functioning.

340
00:19:14,190 --> 00:19:20,070
Again, the recovery time objective is
how long that recovery takes, right?

341
00:19:20,310 --> 00:19:24,720
So not how much data is lost, that's RPO,
but how long it's going to take to get

342
00:19:24,720 --> 00:19:26,040
the system back up and running again.

343
00:19:26,540 --> 00:19:29,570
For example, a system which is backed
up once a day and has a failure in the

344
00:19:29,570 --> 00:19:35,090
middle of a Wednesday, has an RPO of
up to one day the last backup and an

345
00:19:35,090 --> 00:19:39,290
RTO, we'll say maybe five minutes that
it takes to reboot the system and apply

346
00:19:39,290 --> 00:19:41,060
the latest database backup, right?

347
00:19:41,060 --> 00:19:42,050
So these two can differ.

348
00:19:42,680 --> 00:19:45,320
We haven't talked about scalability
yet, which is really a whole nother

349
00:19:45,320 --> 00:19:49,160
topic, but this is the third leg
of our stool reliability that.

350
00:19:49,660 --> 00:19:49,930
Sorry.

351
00:19:49,930 --> 00:19:53,110
Reliability, availability, and
scalability together form that

352
00:19:53,110 --> 00:19:57,820
three-legged stool, which is usually
tested under a number of concurrent

353
00:19:57,820 --> 00:20:02,050
operations or load that the system can
take on without showing failure signs.

354
00:20:02,530 --> 00:20:05,200
We define this as the normal
operating range, right?

355
00:20:05,200 --> 00:20:08,830
So what we want to do is build our
SLA around what we've tested to.

356
00:20:09,330 --> 00:20:13,950
Now when we have complex environments,
we can look at reliability and

357
00:20:13,950 --> 00:20:20,190
availability, and figure out what their
theoretical number might be, right?

358
00:20:20,250 --> 00:20:23,700
And so maybe this is, for either
availability or reliability.

359
00:20:24,000 --> 00:20:27,510
We have to multiply the
subsystem components which have

360
00:20:27,510 --> 00:20:28,620
dependencies on each other.

361
00:20:28,960 --> 00:20:31,510
in series this gets worse, right?

362
00:20:31,510 --> 00:20:36,040
So a five nines in series with a
five nines is less than five nines.

363
00:20:36,460 --> 00:20:38,815
if it's in parallel, then
sometimes that can get better.

364
00:20:39,335 --> 00:20:43,085
So if we have redundant components
and only one of them goes down, or

365
00:20:43,085 --> 00:20:46,775
if we assume that the failure mode is
that at most one of them will go down

366
00:20:46,775 --> 00:20:51,785
at a time, then we may have better
availability or better reliability.

367
00:20:51,785 --> 00:20:54,095
'cause we have two components
that could take up the load.

368
00:20:54,965 --> 00:20:57,815
Usually we assume that software at
the top of the stack is a hundred

369
00:20:57,815 --> 00:21:01,775
percent reliable and then the
things below it are less reliable.

370
00:21:02,275 --> 00:21:05,285
So we're gonna use like a stack
diagram like this, show that chain

371
00:21:05,285 --> 00:21:08,705
of dependencies from the software
layer at the very top on down

372
00:21:08,705 --> 00:21:10,905
through, things in the physical world.

373
00:21:11,005 --> 00:21:11,425
Okay.

374
00:21:12,145 --> 00:21:15,235
In the diagram shown we have two
physical nodes, for instance,

375
00:21:15,235 --> 00:21:16,885
side by side on the platform.

376
00:21:16,915 --> 00:21:18,505
And I wish I could
point these out on here.

377
00:21:18,505 --> 00:21:21,805
So you see node A and node B
that have a big 99.9 on them.

378
00:21:22,255 --> 00:21:24,355
That's two elements in parallel, right?

379
00:21:24,355 --> 00:21:25,915
So you have some redundancy in there.

380
00:21:25,915 --> 00:21:28,165
So that's how we show that
on this kind of diagram here.

381
00:21:28,665 --> 00:21:31,815
The availability of a component
may allow for some degraded mode

382
00:21:31,815 --> 00:21:35,085
of operations as well, which may
count it as still available, right?

383
00:21:35,115 --> 00:21:40,215
For instance, order taking may not be
affected, but backend billing is delayed.

384
00:21:40,665 --> 00:21:42,405
You still may be a
hundred percent available.

385
00:21:42,905 --> 00:21:47,525
If there are redundant components with
failover required, say an active passive

386
00:21:47,525 --> 00:21:52,175
database, then remember that we're gonna
have an RPO and an RTO involved, right?

387
00:21:52,205 --> 00:21:54,125
Which is gonna impact our uptime, right?

388
00:21:54,125 --> 00:21:57,755
So if we can't recover the system
within five minutes, we can't get to

389
00:21:57,755 --> 00:22:01,685
five nights, we're gonna have an outage
longer than that period would allow.

390
00:22:02,185 --> 00:22:04,705
If the redundant components are
operating active, say in a load

391
00:22:04,705 --> 00:22:07,825
balance service, then a partial
outage typically does not decrease the

392
00:22:07,825 --> 00:22:12,085
availability unless the load is entirely
out of our normal operating range.

393
00:22:12,325 --> 00:22:17,995
So if you have a side A and a side B for
instance, and side A goes down, everything

394
00:22:18,115 --> 00:22:23,125
transitions over to side B. If that load
is higher than what side B is able to

395
00:22:23,125 --> 00:22:26,815
take, then we may have a cascading outage.

396
00:22:27,315 --> 00:22:29,835
In this diagram, the color
blue represents software.

397
00:22:29,835 --> 00:22:32,385
The application green is the
infrastructure platform, and

398
00:22:32,385 --> 00:22:34,215
yellow is external integrations.

399
00:22:34,575 --> 00:22:37,395
And this is for, again, for the retail
scenario that we'll see in a bit.

400
00:22:37,875 --> 00:22:39,855
Our sample calculation is pretty simple.

401
00:22:39,975 --> 00:22:43,665
Reliability of the entire system
is gonna be the platform pieces,

402
00:22:44,055 --> 00:22:48,645
the software pieces, times,
maybe some external integrations,

403
00:22:48,795 --> 00:22:50,835
times, maybe some physical pieces.

404
00:22:50,835 --> 00:22:54,855
The payment devices, lemme
see on the yellow side there.

405
00:22:55,365 --> 00:22:59,445
database reliability that's in there
times also your container hardware.

406
00:23:00,015 --> 00:23:02,715
And so dependencies in series
are multiplied together.

407
00:23:03,045 --> 00:23:07,605
And so two elements at one,
at 80%, one at 85% will give

408
00:23:07,605 --> 00:23:09,855
you a 68% reliability, right?

409
00:23:09,855 --> 00:23:11,115
So we multiply them together.

410
00:23:11,115 --> 00:23:15,135
If they're in series, meaning there's
a direct dependency one-to-one, and

411
00:23:15,135 --> 00:23:20,295
elements in parallel where you have
replication or duplicate, redundant,

412
00:23:20,355 --> 00:23:26,295
maybe hardware or containers, you get,
a normal parallel equation for that.

413
00:23:26,295 --> 00:23:26,685
Sorry.

414
00:23:26,815 --> 00:23:27,955
it's in the notes here.

415
00:23:28,225 --> 00:23:33,865
but if you have five elements at 85%
availability, that might give a total

416
00:23:33,865 --> 00:23:37,305
reli, total availability at 99.992%.

417
00:23:37,305 --> 00:23:39,615
So you can see it gets much higher
if you have redundancy in there.

418
00:23:40,115 --> 00:23:40,505
Okay.

419
00:23:41,005 --> 00:23:43,165
Let's go on and we'll talk a
little bit about how we're gonna

420
00:23:43,165 --> 00:23:44,695
brainstorm about failures here.

421
00:23:45,235 --> 00:23:48,115
Here we're basically going
to use a technique called

422
00:23:48,115 --> 00:23:49,825
failure modes effects analysis.

423
00:23:49,915 --> 00:23:53,245
We're gonna go through each layer of the
stack and identify sources of failure.

424
00:23:53,845 --> 00:23:57,065
We can just scattershot them and go,
what could possibly happen at this layer.

425
00:23:57,125 --> 00:24:00,695
Look at our stack diagram or a
system diagram from before, and

426
00:24:00,695 --> 00:24:01,865
then we can just list them out.

427
00:24:01,925 --> 00:24:07,105
Now, one recommendation I would say
here is that before you get to solving

428
00:24:07,105 --> 00:24:10,825
this, before you get to understanding
how you might reduce the impact

429
00:24:10,825 --> 00:24:13,435
or changing design, list them out.

430
00:24:13,435 --> 00:24:15,385
But we want to prioritize
them by likelihood.

431
00:24:15,655 --> 00:24:16,705
Do that second.

432
00:24:16,975 --> 00:24:21,085
And then also as you're going
through FMEA, you might look at it

433
00:24:21,085 --> 00:24:23,575
by the OSI seven layer model, right?

434
00:24:23,575 --> 00:24:26,935
What happens at the application layer,
what happens at the container layer,

435
00:24:26,935 --> 00:24:31,105
virtualization layer, what happens at the
network layer, and so on and so forth.

436
00:24:31,585 --> 00:24:34,435
So on the next two slides, we're
gonna see a complete result of this.

437
00:24:35,095 --> 00:24:38,365
Again, for the retail example here,
I know this is a lot of stuff.

438
00:24:38,365 --> 00:24:40,915
You may wanna come back to
these to look at what these are.

439
00:24:41,245 --> 00:24:46,925
But basically we're going through a
tabletop, failure mode analysis, coming

440
00:24:46,925 --> 00:24:50,015
up with a bunch of failures that might
happen, and then you can see we added

441
00:24:50,015 --> 00:24:53,915
a couple columns to the right where
after we did the initial listing on the

442
00:24:53,915 --> 00:24:58,235
left side, we're adding things that we
think might be able to avoid this type

443
00:24:58,235 --> 00:25:01,985
of failure or things that we might be
able to add to the design to remediate.

444
00:25:02,485 --> 00:25:05,275
Couple obvious ones that we
learned from on this one.

445
00:25:05,275 --> 00:25:10,525
There's actually a good case study of
Square going down in 2017 and Chick-fil-A

446
00:25:10,525 --> 00:25:12,715
in 2021 that I also put in the notes here.

447
00:25:13,215 --> 00:25:15,315
This is the second slide
of our failure analysis.

448
00:25:15,375 --> 00:25:18,155
This is just the, just showing
you the complete, analysis here

449
00:25:18,425 --> 00:25:20,045
of what we thought could go wrong.

450
00:25:20,545 --> 00:25:22,495
There are a number of solutions
for dealing with risks.

451
00:25:22,495 --> 00:25:25,405
For example, when we have a dependency
on a separate cloud dependent service,

452
00:25:25,645 --> 00:25:29,305
we may have an offload, offloaded
login to a third party provider.

453
00:25:29,845 --> 00:25:31,885
We may exclude that from
our calculation, right?

454
00:25:31,885 --> 00:25:35,395
And we may go, Hey, that provider doesn't
give us a service level agreement, or

455
00:25:35,395 --> 00:25:40,735
does, and then we can include that as a
dependency in our own calculation, right?

456
00:25:41,185 --> 00:25:45,985
Or we may just go, we can monitor it and
add maybe redundant paths to it and, be

457
00:25:45,985 --> 00:25:49,495
able to maybe pat around it a little bit.

458
00:25:49,945 --> 00:25:53,515
in the case of a database or network
link, we might add redundancy again.

459
00:25:53,565 --> 00:25:56,655
or we may have a contract for a
different service provider to give us

460
00:25:56,655 --> 00:25:59,625
insurance or a spare capacity to jump in.

461
00:25:59,625 --> 00:26:03,375
In case of failure for long
running services, we may implement

462
00:26:03,375 --> 00:26:07,185
checkpointing or store and forward
to save the request to disc before

463
00:26:07,185 --> 00:26:08,535
starting a long running workflow.

464
00:26:09,035 --> 00:26:12,395
If you're the primary service provider,
the one with the SLA to the customer.

465
00:26:12,950 --> 00:26:15,980
Usually we can't do risk
transference to a third party.

466
00:26:15,980 --> 00:26:20,630
So in our case of a login service
that someone else is maintaining,

467
00:26:20,810 --> 00:26:24,670
we may have to accept that risk and
bear it within ourselves as maybe

468
00:26:24,670 --> 00:26:28,090
a service level objective that we'd
like to monitor with that provider.

469
00:26:28,520 --> 00:26:31,280
we may not be able to pass
that along to the customer.

470
00:26:31,780 --> 00:26:32,200
Okay.

471
00:26:32,530 --> 00:26:36,460
Now we're gonna take a look at how
we're going to improve our designs, what

472
00:26:36,460 --> 00:26:41,190
patterns we're gonna use when we get
to looking at our top, failure modes.

473
00:26:41,670 --> 00:26:43,620
What are we gonna apply
to change that, right?

474
00:26:44,120 --> 00:26:45,830
Sometimes these are prior examples.

475
00:26:45,830 --> 00:26:49,010
They might be codified in a common
practice or a new module that we

476
00:26:49,010 --> 00:26:51,380
can adapt into our own solution.

477
00:26:51,710 --> 00:26:54,710
We might use it to improve either
availability and reliability.

478
00:26:54,710 --> 00:26:58,370
I. Fundamentally, the system needs
to auto automatically identify a

479
00:26:58,370 --> 00:27:01,760
failure in another component and adapt
around it to stay in a healthy state.

480
00:27:02,390 --> 00:27:05,240
And that healthy state is either
being able to, in the case of

481
00:27:05,240 --> 00:27:08,930
availability, accept new requests,
or in the case of reliability,

482
00:27:09,020 --> 00:27:12,530
continue processing a workflow while
it goes along and it encounters

483
00:27:12,530 --> 00:27:13,730
something that it wasn't expected.

484
00:27:14,230 --> 00:27:17,260
We may use different approaches depending
on the layer of the application we

485
00:27:17,260 --> 00:27:19,160
need to, improve the performance in.

486
00:27:19,490 --> 00:27:22,820
And so we're gonna see on the next
couple slides, we'll see a set of

487
00:27:22,850 --> 00:27:25,730
infrastructure patterns and we'll
see a set of software patterns.

488
00:27:26,230 --> 00:27:30,250
So for the infrastructure folks out
there, this is really talking about the

489
00:27:30,250 --> 00:27:32,770
infrastructure patterns that we rely on.

490
00:27:33,550 --> 00:27:36,520
Also, remember, it's easier
to use the providers.

491
00:27:36,880 --> 00:27:40,240
Redundancy mechanisms because they'll
work well within the platform.

492
00:27:40,540 --> 00:27:44,740
So for backup and restore, the same
solution can be done for redundancy.

493
00:27:44,740 --> 00:27:49,990
So if you have an RDS instance, for
instance, within AWS, use that rather

494
00:27:49,990 --> 00:27:53,050
than rolling in your own infrastructure
as a service implementation.

495
00:27:53,800 --> 00:27:56,950
On the lower left panel, I've listed many
of the options, which an infrastructure

496
00:27:56,950 --> 00:27:58,930
architect or SRE could help implement.

497
00:27:59,470 --> 00:28:02,800
The only one's probably lesser
known of these is the geo pattern,

498
00:28:02,800 --> 00:28:06,730
which is described in the speaker's
notes as one that Microsoft.

499
00:28:06,820 --> 00:28:11,020
It came up with where services are
located in multiple regions globally,

500
00:28:11,020 --> 00:28:15,640
and any one of which can serve any given
customer possibly raving to a region.

501
00:28:15,640 --> 00:28:19,600
Specific backend well being
a good solution for something

502
00:28:19,600 --> 00:28:21,070
as big as Office 365.

503
00:28:21,100 --> 00:28:25,060
It like all infrastructure solutions,
has a cost associated with it, and

504
00:28:25,060 --> 00:28:28,390
some of the infrastructure solutions
provide security benefits as well,

505
00:28:28,450 --> 00:28:33,130
such as a web application firewall,
which can throttle or restrict traffic

506
00:28:33,130 --> 00:28:37,420
from bad actors, as well as rates
of traffic from actual customers.

507
00:28:37,750 --> 00:28:40,360
So some of these can have
two different usages in them.

508
00:28:41,290 --> 00:28:44,650
Most of these are designed for
service interactions, not inside

509
00:28:44,680 --> 00:28:45,820
the same cluster environment.

510
00:28:45,820 --> 00:28:49,360
Since the likelihood of network failure is
low inside of something like Kubernetes,

511
00:28:49,570 --> 00:28:53,770
the same cluster, it's more likely
that you're gonna see a failure between

512
00:28:53,770 --> 00:28:57,400
environments where you're reaching out
from one cloud environment to another.

513
00:28:57,900 --> 00:28:59,520
Let's take a look at software once step.

514
00:28:59,970 --> 00:29:02,670
Reliability, resiliency,
availability and scalability is

515
00:29:02,670 --> 00:29:04,380
a single pattern for scalability.

516
00:29:04,410 --> 00:29:08,250
Consistent across all the services,
rarely is a good idea to mix and match

517
00:29:08,250 --> 00:29:11,250
methods with some services doing one
thing and others on a different pattern.

518
00:29:11,250 --> 00:29:15,470
So when we come to software ones,
we might have a set of services that

519
00:29:15,470 --> 00:29:20,450
do crud operations and then we might
imagine one's doing CQRS as well.

520
00:29:20,900 --> 00:29:24,530
Typically, mixing and matching these
makes it very difficult to put in the

521
00:29:24,530 --> 00:29:30,170
type of scalability and availability
changes that we want because we have

522
00:29:30,170 --> 00:29:31,730
to deal with two different patterns.

523
00:29:32,230 --> 00:29:35,140
if you're doing database
transactions, also remember that

524
00:29:35,140 --> 00:29:37,840
your backend may be constraining
your software a little bit as well.

525
00:29:38,020 --> 00:29:43,570
So try and put true database transactions
in a single centralized service and

526
00:29:43,630 --> 00:29:46,900
try and avoid using a transaction
broker if you're having to span a

527
00:29:46,900 --> 00:29:49,180
transaction across multiple services.

528
00:29:49,680 --> 00:29:52,500
Finally, think about what your
recovery should be if something

529
00:29:52,500 --> 00:29:54,270
fails and microservices.

530
00:29:55,215 --> 00:29:57,795
Being restarted, what actually happens?

531
00:29:57,795 --> 00:30:00,075
Then we can call this item potency.

532
00:30:00,105 --> 00:30:04,945
So this is the ability to of
how your service is going to

533
00:30:05,335 --> 00:30:09,835
perform an operation twice in a
row with the same effect, right?

534
00:30:10,075 --> 00:30:14,665
So doing a read operation is inherently
item potent, assuming the back data

535
00:30:14,665 --> 00:30:19,035
doesn't change, but retrying a payment
would not be Ida potent, right?

536
00:30:19,035 --> 00:30:21,915
Because if you ran it twice, the
person might be charged twice.

537
00:30:22,185 --> 00:30:25,125
So you'll have to add some things
like maybe a transaction reference

538
00:30:25,125 --> 00:30:28,965
IED that says the second time the
operation gets requested for the

539
00:30:28,965 --> 00:30:32,565
same purchase, the processor knows
that it was submitted previously and

540
00:30:32,565 --> 00:30:34,755
just says, Hey, I already did this.

541
00:30:34,815 --> 00:30:36,795
I'm not going to do this a second time.

542
00:30:37,665 --> 00:30:40,695
That's also why there's a spinner on a
lot of checkout buttons when you see it

543
00:30:40,695 --> 00:30:42,075
in shopping carts and things like that.

544
00:30:42,075 --> 00:30:46,425
It graze out the screen and takes away
the ability for you to, as a user, click

545
00:30:46,425 --> 00:30:47,745
on it a couple different times, right?

546
00:30:48,245 --> 00:30:52,615
For our DevOps example, as well, you're
gonna see that, we want to fail safely.

547
00:30:52,615 --> 00:30:56,605
And since we're a job based system
against infrastructure, we wanna make

548
00:30:56,605 --> 00:31:01,725
sure that jobs in flight, if they get
put in during the system outage, we have

549
00:31:01,725 --> 00:31:05,595
deterministic behavior on how that's
going to happen and how to restart them.

550
00:31:06,435 --> 00:31:10,185
And we probably are not going to
reintroduce those jobs on recovery

551
00:31:10,515 --> 00:31:15,135
so that we have basically item potent
behavior, which is failing safe.

552
00:31:15,765 --> 00:31:19,095
the lower left panel indicates a list in
order of common consideration from top

553
00:31:19,095 --> 00:31:22,935
to bottom of software techniques that
we typically add to an implementation

554
00:31:22,935 --> 00:31:24,135
to increase its reliability.

555
00:31:24,525 --> 00:31:26,025
I'm not gonna go through all of these.

556
00:31:26,075 --> 00:31:29,615
obviously the first ones goes
without being said, but yes,

557
00:31:29,675 --> 00:31:31,085
let the user retry, right?

558
00:31:31,085 --> 00:31:34,565
That's always an option unless you
have a business requirement that

559
00:31:34,565 --> 00:31:36,845
says that's not available, right?

560
00:31:37,185 --> 00:31:38,655
or not an okay fallback.

561
00:31:39,155 --> 00:31:40,895
Just cancel it out and
then start over again.

562
00:31:41,315 --> 00:31:44,045
now when you get into Software
Ones, we will go, we'll start

563
00:31:44,045 --> 00:31:47,135
with error handling first because
it's the easiest to standardize.

564
00:31:47,195 --> 00:31:52,085
And often one of the reasons that
Go Programs fail is really just

565
00:31:52,085 --> 00:31:53,735
having an uncut panic exception.

566
00:31:54,395 --> 00:31:58,035
So now there are a lot, this list
is based on a lot of the ones

567
00:31:58,035 --> 00:31:59,415
that are cataloged at Awesome.

568
00:31:59,415 --> 00:31:59,805
Go.

569
00:31:59,805 --> 00:32:03,015
So if you're familiar with that site,
the link is again, in the slide notes.

570
00:32:03,405 --> 00:32:06,495
but many of these we then go down
the list and we go, okay, if we

571
00:32:06,495 --> 00:32:09,105
can't do that, can't do number
two, can we do number three?

572
00:32:09,135 --> 00:32:10,125
Can we do number four?

573
00:32:10,305 --> 00:32:13,455
And so we're gonna get to more and more
complicated ones as we go down the list.

574
00:32:13,955 --> 00:32:18,415
Some of the ones that, we call out here,
we use a term called Store and Forward.

575
00:32:18,835 --> 00:32:22,135
we don't really call this out too
often as a pattern, but basically

576
00:32:22,135 --> 00:32:25,375
it's the idea that an API is gonna
write a request of immediately to

577
00:32:25,375 --> 00:32:27,475
DISC before performing a long running.

578
00:32:27,850 --> 00:32:31,300
Or error prone operation so that
it can restart quickly if a failure

579
00:32:31,300 --> 00:32:35,800
is detected later on and it revert
back to the original request and

580
00:32:35,800 --> 00:32:39,460
go, okay, if I can retry it, I at
least have all that information.

581
00:32:39,910 --> 00:32:44,380
And I've kept my availability for
taking that request, even though my

582
00:32:44,380 --> 00:32:48,570
reliability means I'm gonna have to take
a different path for stateless services.

583
00:32:48,570 --> 00:32:51,630
Remember to invalidate the cash if
something happens to the back end.

584
00:32:51,780 --> 00:32:54,690
a lot of times we think of this when
we're building microservices, but we

585
00:32:54,690 --> 00:32:57,300
include it here to be complete backoff.

586
00:32:57,300 --> 00:33:01,080
And retry is a strategy for intermittent
network loading issues, right?

587
00:33:01,410 --> 00:33:03,450
Circuit breakers described
in a lot of places.

588
00:33:03,450 --> 00:33:06,440
Microsoft, Martin Fowler,
are common references there.

589
00:33:06,890 --> 00:33:11,150
circuit Breaker basically helps manage
or throttle down the requests giving.

590
00:33:11,525 --> 00:33:15,935
Given to a down service until such time
as the issue is detected is cleared.

591
00:33:15,935 --> 00:33:19,475
So we don't wanna, if we detect a
service is down on request number one,

592
00:33:19,475 --> 00:33:21,995
we don't wanna throw a hundred more
requests at that same service 'cause

593
00:33:21,995 --> 00:33:23,405
we'll probably get the same answer.

594
00:33:23,495 --> 00:33:26,585
And a lot of times we'll have an
external signal that will tell us when

595
00:33:26,585 --> 00:33:29,015
that down service comes back online.

596
00:33:29,515 --> 00:33:32,455
Sometimes we talk about doing
asynchronous web services or

597
00:33:32,455 --> 00:33:34,255
event driven services, eventing.

598
00:33:34,255 --> 00:33:38,215
Maybe those tend to be a little bit
more complicated to actually tell when a

599
00:33:38,215 --> 00:33:42,465
failure's occurred because we have a long
pole or some sort of socket that might be

600
00:33:42,465 --> 00:33:46,305
open for a long time and you don't know
if the other end went down or if it's just

601
00:33:46,305 --> 00:33:48,525
taking a long time processing something.

602
00:33:49,215 --> 00:33:54,105
And then the last couple on here, bulkhead
and fallbacks, which really limit the

603
00:33:54,105 --> 00:33:58,785
impact of a failure by maybe isolating
one workflow from another, or having

604
00:33:58,785 --> 00:34:02,775
specific undo events and being able to.

605
00:34:03,510 --> 00:34:06,480
We might call this a compensating
transaction or something like

606
00:34:06,480 --> 00:34:11,455
that lets us roll back at least
partially through a workflow if we

607
00:34:11,455 --> 00:34:12,805
can't roll back the entire thing.

608
00:34:13,645 --> 00:34:16,675
So let's take a look at some, look at a
module that brings a lot of this together.

609
00:34:17,175 --> 00:34:20,735
I'm gonna start with, probably the one
that most people would recognize is,

610
00:34:21,105 --> 00:34:23,534
the Netflix one from his called Histrix.

611
00:34:23,725 --> 00:34:25,795
this was a port from Java originally.

612
00:34:26,104 --> 00:34:28,804
and it's a good thing that there aren't
too many Netflix sized deployments

613
00:34:28,804 --> 00:34:32,824
because this is a very complex technical
implementation to add into any system.

614
00:34:33,245 --> 00:34:35,964
And, so I'd say, you can
look at this as a model.

615
00:34:36,025 --> 00:34:39,955
You may take parts of it, or you may
find some of the parts other people

616
00:34:39,955 --> 00:34:43,375
have taken on and made standalone
that you could pick up one at a time.

617
00:34:44,305 --> 00:34:47,455
This one does provide a lot of
availability, reliability, and recovery.

618
00:34:48,324 --> 00:34:52,344
Techniques in it and allows you
to add it into your code base.

619
00:34:52,464 --> 00:34:56,274
And also the added benefit of
providing monitoring, so when it

620
00:34:56,274 --> 00:35:01,104
detects a failure has occurred in a
dependent service, it alerts, right?

621
00:35:01,104 --> 00:35:03,924
It can take it to Prometheus or
it can take it out to telemetry.

622
00:35:04,424 --> 00:35:07,154
Now, the techniques included here
show a wide variety of solutions

623
00:35:07,154 --> 00:35:09,914
that can be used singly or in
coordination with others, right?

624
00:35:09,914 --> 00:35:12,825
So you don't have to use all the things
on that previous list in the software

625
00:35:12,825 --> 00:35:16,814
list to make sure that a singular
failure, single failure does not cause

626
00:35:16,814 --> 00:35:18,705
other dependent services to stop working.

627
00:35:19,185 --> 00:35:23,714
we may be able to get by in certain
cases with certain of those solutions.

628
00:35:23,714 --> 00:35:25,334
We don't actually have to
use all of them, right?

629
00:35:25,334 --> 00:35:29,475
So we might use a circuit
breaker, but only with retry.

630
00:35:29,535 --> 00:35:33,495
And we don't have to use fallback and
we don't have to use bulkhead, right?

631
00:35:33,495 --> 00:35:36,194
So we don't have to use
everything everywhere, right?

632
00:35:36,194 --> 00:35:38,865
Just think from the architectural
perspective, where do you

633
00:35:38,865 --> 00:35:40,424
wanna apply which technique?

634
00:35:40,924 --> 00:35:43,325
To wrap up the patterns description,
it's important to remember that

635
00:35:43,325 --> 00:35:44,524
we have to monitor for failure.

636
00:35:44,524 --> 00:35:47,124
So this is something that
districts gave, gives us, it

637
00:35:47,124 --> 00:35:48,384
happens at multiple levels, right?

638
00:35:48,384 --> 00:35:50,725
We're gonna track hardware
performance, we're gonna track

639
00:35:50,725 --> 00:35:52,464
traffic analysis for latency.

640
00:35:52,774 --> 00:35:54,514
we're gonna see how the
network is performing, right?

641
00:35:54,514 --> 00:35:54,814
For those.

642
00:35:55,414 --> 00:35:57,064
Different lengths of service calls.

643
00:35:57,424 --> 00:36:01,234
Is that within, is it normal for it to
come back after a second or is it, should

644
00:36:01,234 --> 00:36:02,704
it be in the a hundred millisecond range?

645
00:36:03,214 --> 00:36:05,704
And we also will have business
telemetry and metrics, right?

646
00:36:05,704 --> 00:36:07,414
So open telemetry or something like that.

647
00:36:07,924 --> 00:36:09,664
This goes a lot beyond logging, right?

648
00:36:09,664 --> 00:36:13,234
So a lot of times our first pass at this
is just to throw out a log of all the

649
00:36:13,264 --> 00:36:18,034
different things that are going on and
sequencing 'em by time of occurrence.

650
00:36:18,334 --> 00:36:21,724
Realize when you get to multiple different
customers, multiple different flows

651
00:36:21,724 --> 00:36:25,384
through the application, we're gonna
need things like correlation, IDs and

652
00:36:25,384 --> 00:36:29,314
timestamps to start looking through
those logs to actually get the events

653
00:36:29,314 --> 00:36:34,144
out of them and see causality and see
where failures are actually occurring.

654
00:36:34,444 --> 00:36:39,574
It may happen on one particular customer's
job because of the way they specified that

655
00:36:39,574 --> 00:36:43,924
job, or it may be because the load that
customer's putting on it is affecting.

656
00:36:44,424 --> 00:36:45,324
Another customer.

657
00:36:45,624 --> 00:36:47,164
So we'll have to sift through it.

658
00:36:47,164 --> 00:36:51,344
And logging is often not the best
way just to, look at how we're

659
00:36:51,344 --> 00:36:52,814
going to find failures here, right?

660
00:36:53,264 --> 00:36:55,634
It's kinda looking for a needle
in a haystack when something goes

661
00:36:55,634 --> 00:36:58,844
wrong, sometimes it goes very
wrong and we want to have metrics

662
00:36:58,844 --> 00:36:59,954
to abstract that a little bit.

663
00:37:00,644 --> 00:37:02,924
Now we're gonna take a look at
three systems and see if we can put

664
00:37:02,924 --> 00:37:04,184
this into practice a little bit.

665
00:37:04,684 --> 00:37:08,824
So our first example here is just
a simple two-step microservice,

666
00:37:08,824 --> 00:37:10,174
one calling the other, right?

667
00:37:10,624 --> 00:37:14,164
the two APIs may not be in the single
environment, and we have command line

668
00:37:14,164 --> 00:37:18,034
activating the whole thing, maybe user on
their laptop, which calls the first API.

669
00:37:18,124 --> 00:37:18,154
Okay.

670
00:37:18,654 --> 00:37:22,124
Now the second API is tied to a
database, so we're gonna call it a

671
00:37:22,124 --> 00:37:24,164
crud layer over there at number three.

672
00:37:24,464 --> 00:37:28,294
Maybe this is a simple to-do application
or something like that, that, is

673
00:37:28,474 --> 00:37:30,904
fairly self-contained right now.

674
00:37:30,994 --> 00:37:34,084
AWS in this case, if that's where
we're hosting, may give us part of

675
00:37:34,084 --> 00:37:37,624
an SLA as a platform, so that we can
build on top of that in each of the

676
00:37:37,624 --> 00:37:39,274
two environments where the APIs live.

677
00:37:39,814 --> 00:37:44,074
We may also have RDS or something like
that for the database that gives us

678
00:37:44,074 --> 00:37:46,474
redundancy and maybe recoverability.

679
00:37:47,014 --> 00:37:48,164
And so this is pretty helpful.

680
00:37:48,164 --> 00:37:52,034
We can compose the simple case out
of pretty handy infrastructure parts.

681
00:37:52,034 --> 00:37:55,364
Right now, we probably measure
availability at the first rest.

682
00:37:55,364 --> 00:37:58,214
API, we're gonna put that
little yellow star on there.

683
00:37:58,244 --> 00:37:59,414
That's the front door.

684
00:37:59,474 --> 00:38:01,094
Remember that is our uptime.

685
00:38:01,934 --> 00:38:06,584
And then we're gonna measure the overall
reliability by the ability to persist

686
00:38:06,584 --> 00:38:07,964
all the way through the database.

687
00:38:08,054 --> 00:38:10,214
That's the orange star
in this case, right?

688
00:38:10,514 --> 00:38:13,454
So not a lot of techniques we're
probably gonna need on the software side

689
00:38:13,454 --> 00:38:14,834
due to the simplicity of the system.

690
00:38:15,334 --> 00:38:17,254
We probably consider the
command line outta scope.

691
00:38:17,824 --> 00:38:21,694
And if the customer environment can't
reach AWS for instance, we're not

692
00:38:21,694 --> 00:38:23,404
gonna include it in our uptime, right?

693
00:38:23,434 --> 00:38:25,834
Uptime is, you have to be able
to get it to our front door.

694
00:38:26,334 --> 00:38:29,454
Now to increase uptime, we might
have load balancing on that first.

695
00:38:29,454 --> 00:38:31,674
API we might cache some results.

696
00:38:32,004 --> 00:38:34,554
that might come from that
second API or something like

697
00:38:34,554 --> 00:38:35,784
that over at API number one.

698
00:38:36,174 --> 00:38:40,234
and then, our availability would
be based on the customer being

699
00:38:40,234 --> 00:38:43,504
able to create requests and maybe
view status of their requests.

700
00:38:43,804 --> 00:38:48,574
So all, everything in that rest API number
one, reliability might add a couple more.

701
00:38:48,664 --> 00:38:50,944
We might add a redundant database, right?

702
00:38:51,124 --> 00:38:55,334
Provided by our hoster or otherwise,
we might load balance API number two,

703
00:38:55,754 --> 00:38:59,834
we might provide a circuit breaker
in API number one to store requests

704
00:38:59,834 --> 00:39:03,954
locally and, back off on sending
stuff to API two if it detects that

705
00:39:03,954 --> 00:39:06,204
it's, if it's going down right.

706
00:39:06,704 --> 00:39:09,884
And then we might have some
way to tell the API number one.

707
00:39:10,064 --> 00:39:13,954
Okay, it's okay to turn traffic
back on to API number two.

708
00:39:14,584 --> 00:39:14,854
Okay.

709
00:39:15,604 --> 00:39:18,124
Now we're certainly gonna probably
put in place monitoring as well

710
00:39:18,124 --> 00:39:21,904
and maybe continuous testing with
something like synthetic transactions.

711
00:39:21,904 --> 00:39:26,014
So we send through no
operation type transactions.

712
00:39:26,164 --> 00:39:29,104
We pretend we're the user and
we send something through maybe

713
00:39:29,104 --> 00:39:31,294
a read operation that goes all
the way through the database.

714
00:39:31,294 --> 00:39:34,144
And returns to status doesn't
affect anything, but that's what

715
00:39:34,144 --> 00:39:35,524
we'll call a synthetic transaction.

716
00:39:35,524 --> 00:39:39,304
So we can actually test while the
system's running, what's going on.

717
00:39:40,054 --> 00:39:43,174
Now we're gonna take a look at a
more complex scenario in retail here

718
00:39:43,384 --> 00:39:47,484
that deals with e-commerce and maybe
an in-store, set of operations.

719
00:39:47,984 --> 00:39:50,954
This is effectively multi-channel
sales, if you've ever heard of that.

720
00:39:51,004 --> 00:39:54,244
the customer can order it either
in a brick and mortar shop or

721
00:39:54,244 --> 00:39:56,144
online with maybe pickup in store.

722
00:39:57,099 --> 00:39:59,269
of course nowadays we expect
delivery to home as well.

723
00:39:59,359 --> 00:40:00,529
But, we'll leave that aside.

724
00:40:01,129 --> 00:40:03,349
The order starts two business processes.

725
00:40:03,449 --> 00:40:08,999
so when we do our online order at number
one, the starts a financial transaction,

726
00:40:08,999 --> 00:40:12,539
a pre-authorization maybe against a
credit card that goes to the bank, right?

727
00:40:12,539 --> 00:40:16,619
And says, Hey, does the customer
have the ability to pay for goods?

728
00:40:17,099 --> 00:40:19,949
And then we're gonna send that
order down into the store.

729
00:40:20,009 --> 00:40:21,479
So that's step number three there.

730
00:40:22,139 --> 00:40:26,279
And then in store, the customer's
going to do what we call a

731
00:40:26,279 --> 00:40:27,659
completing the transaction.

732
00:40:27,659 --> 00:40:32,349
They may have to, sign that they
picked up the, goods, and then that

733
00:40:32,349 --> 00:40:36,729
notice goes back to the bank to say, go
ahead and charge them $17 and 5 cents.

734
00:40:37,119 --> 00:40:41,409
And then the customer walks out with
the goods in step number five there.

735
00:40:41,589 --> 00:40:44,889
Now, they may short this, and
actually, instead of step number one,

736
00:40:44,889 --> 00:40:46,269
they may start at step number five.

737
00:40:46,269 --> 00:40:47,439
They may walk into the store.

738
00:40:47,874 --> 00:40:51,654
An order right then and there, in
which case those two transactions back

739
00:40:51,654 --> 00:40:53,064
to the bank probably get collapsed.

740
00:40:53,274 --> 00:40:56,664
But the point here we're showing is
that bank is in a different environment.

741
00:40:57,024 --> 00:41:00,564
The e-commerce is in an environment, and
the in-store system is in an environment.

742
00:41:00,924 --> 00:41:04,824
So we're probably going to measure
availability at both the online

743
00:41:04,824 --> 00:41:07,914
ordering site and in store, right?

744
00:41:07,914 --> 00:41:11,154
So if the customer can do either
one of them, then that's good.

745
00:41:11,654 --> 00:41:15,134
For this more complex set of
environments, we actually wanna

746
00:41:15,134 --> 00:41:16,784
measure reliability end to end.

747
00:41:16,874 --> 00:41:20,054
That is the system supports both
customer delivery of goods within a

748
00:41:20,054 --> 00:41:24,134
certain amount of time after ordering
and the financial transaction is

749
00:41:24,134 --> 00:41:25,634
correct, a hundred percent the time.

750
00:41:26,134 --> 00:41:27,789
So obviously we used a lot of different.

751
00:41:28,459 --> 00:41:31,759
Solutions from both the software
side and the infras side.

752
00:41:31,829 --> 00:41:36,839
in this example here, this particular
system, so many of the techniques were

753
00:41:36,839 --> 00:41:41,009
used from previous two slides there on
the patterns, but we primarily focused

754
00:41:41,009 --> 00:41:42,959
on reliability over availability.

755
00:41:42,989 --> 00:41:48,769
So failovers of clusters and e-commerce,
failover, data redundancy and workflow

756
00:41:48,769 --> 00:41:51,109
restarts in store, are they a priority?

757
00:41:52,009 --> 00:41:54,949
Next we're gonna take a look
at an example from DevOps.

758
00:41:55,449 --> 00:42:00,909
So this is a simplified diagram of
a DevOps system like Chefs Cloud 360

759
00:42:00,909 --> 00:42:04,859
offering that I mentioned we started about
a month ago, where one of the services is

760
00:42:04,859 --> 00:42:09,869
to provide a generic job runner capable
of running scripts or specific skills for

761
00:42:09,869 --> 00:42:14,729
multiple customers from the cloud hus,
cloud hoster to the customer's own data

762
00:42:14,729 --> 00:42:16,769
center servers and other cloud providers.

763
00:42:17,069 --> 00:42:24,734
So the customer will start a. Job
at number one, that will go into our

764
00:42:24,734 --> 00:42:29,744
service environment, number two, and
then come back to their managed assets

765
00:42:29,744 --> 00:42:32,534
over in number four and number five.

766
00:42:33,034 --> 00:42:37,114
An end user or operator can automate
or interact with the initial rest API,

767
00:42:37,114 --> 00:42:41,914
number two, to enroll their servers or
nodes in the system with a command line

768
00:42:42,124 --> 00:42:47,104
that's Mark number one, or a browser
based experience as they authenticate the

769
00:42:47,104 --> 00:42:51,304
API forwards the request actually to an
open ID service that we don't control the

770
00:42:51,304 --> 00:42:53,464
customer's chosenness to lock them in.

771
00:42:54,244 --> 00:42:58,714
These API services have internal
databases, queues, and file

772
00:42:58,714 --> 00:43:02,934
storage inside the platform to
be able to perform operations.

773
00:43:02,934 --> 00:43:06,834
I've shown those as basically
a Kubernetes logo and a light

774
00:43:06,834 --> 00:43:09,474
blue Azure queue icon there.

775
00:43:09,804 --> 00:43:13,904
In terms of ours, once the operator
enrolls the node under management, they

776
00:43:13,904 --> 00:43:16,184
call it second API in AWS to submit a job.

777
00:43:16,199 --> 00:43:19,559
We'll call this number two
and also the yellow star.

778
00:43:19,559 --> 00:43:21,149
We're measuring availability, right?

779
00:43:21,149 --> 00:43:24,299
Because this job submission is one
of the critical things that we want

780
00:43:24,299 --> 00:43:25,589
to have up and running all the time.

781
00:43:26,089 --> 00:43:29,229
These API services are in
Kubernetes nodes, different usage

782
00:43:29,229 --> 00:43:30,669
than what the customer sees.

783
00:43:30,969 --> 00:43:33,099
we're using the word node in a
couple different senses here.

784
00:43:33,479 --> 00:43:37,679
in our case it is as a service provider,
we're using multiple nodes inside of

785
00:43:37,679 --> 00:43:40,499
AWS in a couple availability zones.

786
00:43:40,999 --> 00:43:43,219
We'll see other infrastructure
design on the next slide.

787
00:43:43,219 --> 00:43:44,659
So let's continue with our flow here.

788
00:43:45,229 --> 00:43:49,339
Our primary API services may call
an outpost or local set of similar

789
00:43:49,339 --> 00:43:53,959
API services that live across
the bridge mark number four, in

790
00:43:53,959 --> 00:43:55,159
the customer's own data center.

791
00:43:55,189 --> 00:43:57,649
In order to forward a job
to maybe a local queue.

792
00:43:58,399 --> 00:44:00,979
We actually call this a connector
in our architecture, but let's

793
00:44:01,339 --> 00:44:04,489
consider it really a bridge right
now so that we forward that.

794
00:44:05,179 --> 00:44:09,379
From the job submission from number
two, over to number four into the

795
00:44:09,379 --> 00:44:11,029
customer's data center to execute.

796
00:44:11,449 --> 00:44:14,149
Realize the customer may have multiple
of data centers on their side.

797
00:44:14,649 --> 00:44:19,689
Now the agent and server number five,
our real target where we wanna run the

798
00:44:19,689 --> 00:44:22,869
job may actually pick it up directly
from number two or number four.

799
00:44:22,929 --> 00:44:23,619
Doesn't really matter.

800
00:44:24,119 --> 00:44:28,109
The chef agent lives on this
server typically, or it leaves

801
00:44:28,109 --> 00:44:29,399
lives where the job will run.

802
00:44:29,489 --> 00:44:32,759
This is mark number five on the
diagram, and typically this is

803
00:44:32,759 --> 00:44:35,549
one of the customer servers where
they want something to happen.

804
00:44:35,679 --> 00:44:39,369
maybe a patch to be applied or maybe
a restart or something like that.

805
00:44:40,119 --> 00:44:43,299
It may also run remotely
through to a second server

806
00:44:43,809 --> 00:44:45,069
that we call this target mode.

807
00:44:45,279 --> 00:44:48,399
So the agent may live on something
like number five, but it may

808
00:44:48,399 --> 00:44:49,839
reach out to another server.

809
00:44:50,274 --> 00:44:52,494
That for some reason we
can't put an agent on.

810
00:44:52,584 --> 00:44:54,744
Maybe it's a router,
maybe it's an appliance.

811
00:44:55,584 --> 00:45:00,564
The job goes then from the chef, API
service to that outpost, API service in

812
00:45:00,564 --> 00:45:05,514
the customer environment to an agent or
maybe a proxy for remote targeting to

813
00:45:05,604 --> 00:45:11,004
effectively where that destination of
the application or service job will run.

814
00:45:11,754 --> 00:45:15,084
The final part of this workflow, which
we wanna make as reliable as possible is

815
00:45:15,084 --> 00:45:18,774
the agent reporting back what happened,
the completed state back through

816
00:45:18,774 --> 00:45:23,394
the same chain, through the outpost,
backup to AWS and the APIs and maybe

817
00:45:23,394 --> 00:45:25,434
populating reports in a web application.

818
00:45:25,464 --> 00:45:29,484
We show this as number six on here
and also where the yellow star, where

819
00:45:29,484 --> 00:45:34,494
our reliability metric of successfully
completed jobs without failures.

820
00:45:34,704 --> 00:45:39,324
Or if there was a failure, we can identify
what it was and have the user resubmit.

821
00:45:39,744 --> 00:45:41,064
that can be measured there.

822
00:45:41,564 --> 00:45:43,394
We've designed our overall reliability.

823
00:45:43,724 --> 00:45:47,204
Based on a lot of AWS primitive
for infrastructure and also

824
00:45:47,204 --> 00:45:48,434
application techniques.

825
00:45:48,704 --> 00:45:54,294
And we do exclude from our, service
level at this point, customer responsible

826
00:45:54,294 --> 00:46:00,054
items such as, how outposts are running,
how their SSO is running and their

827
00:46:00,054 --> 00:46:02,304
target server reliability, right?

828
00:46:02,304 --> 00:46:04,914
The actual server that's
running the job may have failure

829
00:46:04,914 --> 00:46:05,994
modes and things like that.

830
00:46:06,354 --> 00:46:08,364
All we can do is monitor for those.

831
00:46:08,364 --> 00:46:11,934
And so when we say a service
level, we're really giving the

832
00:46:11,934 --> 00:46:13,554
customer a service level objective.

833
00:46:13,554 --> 00:46:19,584
Because our system is so tightly dependent
on their reliability for most of it,

834
00:46:20,034 --> 00:46:23,934
we can certify our reliability in the
fact that it'll identify that an error

835
00:46:23,934 --> 00:46:29,994
occurred, but that error may have occurred
in an asset that the customer controls or.

836
00:46:30,729 --> 00:46:34,929
Maybe in the case of single sign-on
that they're operating on our behalf.

837
00:46:35,229 --> 00:46:38,559
And so we don't have an
actual SLA at this point.

838
00:46:38,559 --> 00:46:42,429
And so that makes the reliability
calculations a little bit more, manageable

839
00:46:42,429 --> 00:46:45,219
when we can exclude some of those
things and say that will, that isn't

840
00:46:45,219 --> 00:46:47,229
included in our reliability calculations.

841
00:46:47,729 --> 00:46:50,969
So we call these customer
responsible or cr typically.

842
00:46:51,469 --> 00:46:55,339
The other deployment is if a customer
deployed this system entirely on

843
00:46:55,339 --> 00:46:58,729
premises, we're not really gonna
cover that here, but many of the

844
00:46:58,729 --> 00:47:00,649
infrastructure parallels exist.

845
00:47:00,649 --> 00:47:06,739
So if we have A-W-S-R-D-S services
there, we can have a multi-node, a

846
00:47:06,739 --> 00:47:09,749
redundant database, on-prem as well.

847
00:47:09,959 --> 00:47:14,689
And so we typically do infrastructure
components in a purely on-prem

848
00:47:14,689 --> 00:47:18,679
deployment that doesn't use
our SaaS part of the solution.

849
00:47:19,639 --> 00:47:22,584
They will have equivalent
components on the infrastructure

850
00:47:22,584 --> 00:47:24,534
side as what we would have in AWS.

851
00:47:24,864 --> 00:47:28,644
So we'll show in the next slide
really some of the software techniques

852
00:47:28,644 --> 00:47:31,854
that we've prioritized and how we've
built up the production system.

853
00:47:31,854 --> 00:47:36,674
So from a basic, MVP of sending a job
through the system, we go, there are a

854
00:47:36,674 --> 00:47:38,234
lot of places that failures could occur.

855
00:47:38,474 --> 00:47:42,194
So what have we done to change
the system and improve it?

856
00:47:42,834 --> 00:47:46,254
go through these, in, in the order that
we present 'em here, but they're really

857
00:47:46,254 --> 00:47:50,604
from the front door on through to how
do we monitor things in production.

858
00:47:50,604 --> 00:47:53,484
So we're still adding changes,
improving the availability and

859
00:47:53,484 --> 00:47:56,964
reliability of the applications that
make up this latest DevOps offering.

860
00:47:57,354 --> 00:47:59,304
But this is just a
summary of what they are.

861
00:47:59,804 --> 00:48:04,994
one thing you'll notice here is that even
our diagram, if it's, only a few different

862
00:48:04,994 --> 00:48:08,264
forwards between different services,
but it does span different environments,

863
00:48:08,534 --> 00:48:12,404
we're bringing a lot of different,
both software and infrastructure.

864
00:48:13,154 --> 00:48:15,074
Patterns to bear on this, right?

865
00:48:15,674 --> 00:48:18,224
the specifics don't really matter
too much unless you're building

866
00:48:18,224 --> 00:48:19,544
exactly the same system here.

867
00:48:19,794 --> 00:48:22,644
but I'll start talk you through
the process of how we brainstormed,

868
00:48:22,884 --> 00:48:26,244
identified failure points and put
in place these techniques to improve

869
00:48:26,244 --> 00:48:27,384
the availability and reliability.

870
00:48:27,384 --> 00:48:30,534
Right now, number one is
all about availability.

871
00:48:30,714 --> 00:48:34,584
It's making sure the customer, DNS
and the job submission API is highly

872
00:48:34,584 --> 00:48:37,824
available, especially when combined
with number five there, right?

873
00:48:37,824 --> 00:48:41,904
So we take in the request, we immediately
write it to disc, and then we start

874
00:48:41,904 --> 00:48:43,854
the process of running the job, right?

875
00:48:43,854 --> 00:48:46,924
So we can always fail back to,
what was the actual request if

876
00:48:46,924 --> 00:48:49,684
something happens down the line
that we need to recover from.

877
00:48:50,314 --> 00:48:54,004
Number two is about redundancy for the
API calls through the gateway scaling

878
00:48:54,004 --> 00:48:58,924
horizontally within the cluster, and the
ability to spread load across multiple

879
00:48:58,924 --> 00:49:00,964
clusters, potentially right to geo.

880
00:49:01,744 --> 00:49:03,064
Diversify if you were.

881
00:49:03,904 --> 00:49:06,184
Number three is about using
redundant services to the cloud

882
00:49:06,184 --> 00:49:09,454
provider, making sure we're staying
on the pattern that AWS gives us.

883
00:49:09,504 --> 00:49:14,004
we do have some interaction with Azure
as well, so using those primitives

884
00:49:14,004 --> 00:49:17,574
that are there and then monitoring
for external service failures.

885
00:49:18,354 --> 00:49:23,034
Number four is important deals with
the agent that's sitting next to, or

886
00:49:23,034 --> 00:49:27,534
actually on a customer server, it has to
be zero trust, but it also needs to let

887
00:49:27,534 --> 00:49:29,274
the APIs know what its status is, right?

888
00:49:29,274 --> 00:49:30,354
We have to be able to monitor it.

889
00:49:30,354 --> 00:49:34,284
We have to know that it's
still up and responsive.

890
00:49:34,614 --> 00:49:38,424
We use multiple techniques here to make
sure the agent can reliably execute jobs.

891
00:49:38,634 --> 00:49:41,934
One of the interesting ones here is
that we have a heartbeat that is on a

892
00:49:41,934 --> 00:49:46,524
regular basis that tells our APIs on
the server side, Hey, I'm still here.

893
00:49:46,794 --> 00:49:47,634
Hey, I'm still here.

894
00:49:47,814 --> 00:49:50,244
hey, I just did that particular
job you asked me to do.

895
00:49:50,634 --> 00:49:54,814
And so we have a heartbeat status that
always comes back in addition to maybe

896
00:49:54,814 --> 00:49:57,194
jobs specific, artifacts and status.

897
00:49:57,584 --> 00:50:01,814
And then number five is really preventing
failures by immediately writing

898
00:50:01,874 --> 00:50:03,554
request to disc before executing them.

899
00:50:04,394 --> 00:50:05,144
Kinda makes sense.

900
00:50:05,324 --> 00:50:07,724
Number six and seven
really process items here.

901
00:50:07,724 --> 00:50:11,624
So again, work with your operations
team to make sure you're monitoring

902
00:50:11,624 --> 00:50:15,794
the right stuff and that you're
able to do root cause analysis

903
00:50:15,884 --> 00:50:17,054
of any failures that may happen.

904
00:50:17,894 --> 00:50:20,324
Let's talk next about components in Go.

905
00:50:20,824 --> 00:50:23,704
And this is the language that we've picked
for building all of our services in.

906
00:50:24,204 --> 00:50:27,474
So in Go, most of us will probably
know we don't have the easy solution

907
00:50:27,474 --> 00:50:31,644
of something the language is already
picked for us, nor a single simple,

908
00:50:31,644 --> 00:50:35,524
well-known battle tested module that,
maybe other languages might have.

909
00:50:35,874 --> 00:50:37,864
and look at, in t net languages.

910
00:50:37,864 --> 00:50:39,934
We have something called
Poly that's a library.

911
00:50:40,384 --> 00:50:44,014
we don't really have that here
except maybe the implementation

912
00:50:44,014 --> 00:50:45,064
that Netflix came up with.

913
00:50:45,574 --> 00:50:47,764
What we have in Go is
really the community, right?

914
00:50:47,824 --> 00:50:50,614
You can go out to the Awesome Go
website and you can get 20 different

915
00:50:50,614 --> 00:50:52,084
recommendations on stuff, right?

916
00:50:52,084 --> 00:50:55,234
So you'll see lots of different
options out there, some of which I've

917
00:50:55,234 --> 00:51:00,724
tried to list in the most functional
to the least support and meaning.

918
00:51:00,804 --> 00:51:01,824
we know some of these out.

919
00:51:01,824 --> 00:51:05,994
There are single developer that may
not have the time to maintain them, but

920
00:51:06,024 --> 00:51:10,224
did a good code prototype or something
like that, that we may want to take

921
00:51:10,224 --> 00:51:12,384
on ourselves and maintain, right?

922
00:51:12,384 --> 00:51:15,934
Going forward, at the top of
the list we have, has lots of

923
00:51:15,934 --> 00:51:17,464
functionality, adds observability.

924
00:51:17,964 --> 00:51:21,144
And is probably the most complex of
any of the solutions out there, right?

925
00:51:21,414 --> 00:51:24,294
Maybe too big of a hammer,
but is our gold standard.

926
00:51:24,294 --> 00:51:27,354
So if we were looking for somewhere
to go look for a reference,

927
00:51:27,434 --> 00:51:28,784
architecture, that would probably be it.

928
00:51:29,534 --> 00:51:32,024
The next few are pretty well
supported and broadly functional.

929
00:51:32,304 --> 00:51:35,364
multiple patterns described on the
software slide are implemented.

930
00:51:35,424 --> 00:51:38,304
We've got circuit breaker, we've
got retries, we've bulkheads.

931
00:51:38,644 --> 00:51:40,714
so you could pick and choose
some of these components if you

932
00:51:40,714 --> 00:51:41,944
want something lighter weight.

933
00:51:42,254 --> 00:51:46,214
I tried to go with ones that had a
lot of either community forks had

934
00:51:46,214 --> 00:51:51,354
UpToDate maintenance or had obviously,
a lot of people were following and

935
00:51:51,354 --> 00:51:52,824
using in their own implementations.

936
00:51:52,824 --> 00:51:56,694
So going with the idea that
popularity is it's usability.

937
00:51:57,194 --> 00:51:57,614
Okay.

938
00:51:58,274 --> 00:52:00,434
With each of these sort of
check and see how well it

939
00:52:00,434 --> 00:52:02,024
integrates into your use of go.

940
00:52:02,054 --> 00:52:05,444
A lot of times there's a medium or goal
or something on how to use this example.

941
00:52:05,804 --> 00:52:08,624
some of them work better with
the go language, some use

942
00:52:08,684 --> 00:52:10,364
goroutines more than others.

943
00:52:10,364 --> 00:52:15,834
Some use go, syntax and semantics a
little bit more natively than others.

944
00:52:15,914 --> 00:52:17,894
pick the one that works
best for your team, right?

945
00:52:17,894 --> 00:52:20,024
Because we're probably going to be
putting these in a few different

946
00:52:20,024 --> 00:52:21,524
places in our code, right?

947
00:52:22,424 --> 00:52:26,324
also, as you get through looking at these
particular techniques, ask if you need

948
00:52:26,324 --> 00:52:28,934
all this complexity in your application,
go back to that value statement.

949
00:52:28,934 --> 00:52:31,394
Why am I building high
availability in here?

950
00:52:31,724 --> 00:52:35,204
Why am I building higher
reliability to this?

951
00:52:35,669 --> 00:52:39,394
Not all applications need all
the techniques applied, right?

952
00:52:39,634 --> 00:52:43,024
All these techniques when applied could
probably make a really good system.

953
00:52:43,364 --> 00:52:48,084
but it takes time, it takes effort,
it takes, your, cost to get this

954
00:52:48,084 --> 00:52:49,764
out the door and into your system.

955
00:52:50,739 --> 00:52:53,889
If your customer is demanding an SLA,
figure out what parts of that SLA are

956
00:52:53,889 --> 00:52:57,669
most important and figure out which
components support that the best.

957
00:52:58,169 --> 00:53:02,009
And then the code sample off to the side
is a sample, circuit breaker over there

958
00:53:02,009 --> 00:53:03,329
so you can see what that looks like.

959
00:53:03,829 --> 00:53:08,064
Last little note here, for distributed
systems at scale plan on having

960
00:53:08,104 --> 00:53:09,439
a regular test program, right?

961
00:53:09,709 --> 00:53:10,939
You can't do this automated.

962
00:53:10,999 --> 00:53:14,479
You have to have people that are running
tests, new tests, different types of

963
00:53:14,479 --> 00:53:19,799
tests, pick some test tools that the test
teams are familiar with, and of course,

964
00:53:19,859 --> 00:53:22,709
standard guidance of don't have the
developer tests their own stuff, right?

965
00:53:23,049 --> 00:53:27,199
they're only going to be looking for,
things that they conceptualize, right?

966
00:53:27,199 --> 00:53:30,409
A tester's gonna find things
that a developer probably

967
00:53:30,409 --> 00:53:31,369
never thought about, right?

968
00:53:31,449 --> 00:53:34,879
which is what we see, when
our system faces first contact

969
00:53:34,879 --> 00:53:36,199
with the, customer, right?

970
00:53:36,709 --> 00:53:39,289
Run tests and pre-production of
sample data and configuration.

971
00:53:39,349 --> 00:53:42,169
Run with multiple tenants,
run with different roles, run

972
00:53:42,169 --> 00:53:45,559
with malformed inputs, on each
release of the application.

973
00:53:45,649 --> 00:53:49,009
That's a good thing for the development
team to take on as integration test

974
00:53:49,159 --> 00:53:51,349
and then run it in production as well.

975
00:53:51,349 --> 00:53:54,599
If you have an operations
team, or a dedicated test tube,

976
00:53:55,379 --> 00:53:56,819
you can use canary tenants.

977
00:53:56,859 --> 00:54:01,359
I like this because with microservices, we
could put out there, a hundred, commercial

978
00:54:01,359 --> 00:54:05,439
customers in, in there and they may have
different load profiles, but I as, as long

979
00:54:05,439 --> 00:54:10,419
as I have a tenant that is mine that I can
just see all the way through the system,

980
00:54:10,419 --> 00:54:14,339
kinda like those dynamic, tests that
we talked about earlier, the synthetic

981
00:54:14,339 --> 00:54:18,419
ones, you can test a transaction going
all the way through to make sure all the

982
00:54:18,419 --> 00:54:22,379
components are up and running and then you
can report on your availability that way.

983
00:54:22,429 --> 00:54:22,999
Okay.

984
00:54:23,049 --> 00:54:26,509
we use a tool called K six to do a
lot of our, testing, but I've listed

985
00:54:26,509 --> 00:54:27,889
a few other ones here as well.

986
00:54:28,189 --> 00:54:29,599
We do not use fail points.

987
00:54:29,599 --> 00:54:30,979
That looks really promising.

988
00:54:31,059 --> 00:54:32,199
I would say look at that one.

989
00:54:32,589 --> 00:54:36,159
And then one of the really
interesting tools out here is Chaos

990
00:54:36,159 --> 00:54:40,989
Monkey, which injects failures
into your actually running system.

991
00:54:41,489 --> 00:54:45,594
Finally, I'm gonna recommend that you have
some mechanism to report on availability

992
00:54:45,594 --> 00:54:47,214
and reliability of your services.

993
00:54:47,694 --> 00:54:52,074
Many teams don't have a big enough
staff to have dedicated SRAs or

994
00:54:52,074 --> 00:54:55,824
operations teams, but if you design in
your metrics from the very beginning,

995
00:54:56,214 --> 00:54:59,544
anyone can dive in and see what's
going on and identify issues quickly.

996
00:55:00,174 --> 00:55:04,044
Even if you don't have an SLA, tracking
these metrics from the very beginning

997
00:55:04,344 --> 00:55:08,754
leads to being able to build fixes
into your application, which eventually

998
00:55:08,754 --> 00:55:12,834
gives you better customer satisfaction,
making these measurements visible.

999
00:55:12,894 --> 00:55:14,664
we use something like Grafana.

1000
00:55:14,664 --> 00:55:19,084
We use Prometheus, common tools that'll
help show us the business events that

1001
00:55:19,084 --> 00:55:23,494
are going on, show us the load on the
system, show us the performance of the

1002
00:55:23,494 --> 00:55:26,644
different components in there, show
us latencies and things like that,

1003
00:55:27,364 --> 00:55:29,194
showing that to the operations team.

1004
00:55:29,194 --> 00:55:33,294
And the development team then helps
both in an outage situation and then

1005
00:55:33,294 --> 00:55:37,104
afterwards to see how the system's
performing and you can brainstorm

1006
00:55:37,104 --> 00:55:38,514
collectively on improvements.

1007
00:55:39,014 --> 00:55:41,594
There are really four things I'd
like you to take away from this talk.

1008
00:55:42,094 --> 00:55:44,824
First, that availability and reliability.

1009
00:55:45,574 --> 00:55:48,634
Require both software and
infrastructure patterns.

1010
00:55:48,724 --> 00:55:52,084
You're gonna borrow from both camps,
and the combination of those two is

1011
00:55:52,084 --> 00:55:55,114
what's really gonna get you to the
numbers that you want as your goal.

1012
00:55:55,614 --> 00:55:58,284
Second, look to examples
from other systems.

1013
00:55:58,784 --> 00:56:00,224
There's best practice out there.

1014
00:56:00,224 --> 00:56:03,794
There are larger systems than the one
you're building that just shows that

1015
00:56:03,794 --> 00:56:05,924
the path has been paved ahead of time.

1016
00:56:06,254 --> 00:56:09,494
This common practice gets built
into application templates over

1017
00:56:09,494 --> 00:56:13,574
time and eventually becomes the kind
of the easy way to do this, right?

1018
00:56:13,574 --> 00:56:18,764
You don't have to build your own model
from scratch every time, and this is

1019
00:56:18,764 --> 00:56:22,394
certainly something I think AI is going to
help with over the years, is it'll be able

1020
00:56:22,394 --> 00:56:25,484
to look at all these different patterns
and then go, this is what you should

1021
00:56:25,484 --> 00:56:27,734
actually use until that point in time.

1022
00:56:28,124 --> 00:56:32,084
Use your experience or your knowledge
from other systems as a guide.

1023
00:56:32,584 --> 00:56:34,864
Third, do as much as you need.

1024
00:56:35,734 --> 00:56:36,484
Don't do anymore.

1025
00:56:36,484 --> 00:56:37,384
Don't do any less.

1026
00:56:37,714 --> 00:56:38,164
This is.

1027
00:56:38,614 --> 00:56:42,574
A balancing act of cost and
schedule against technical.

1028
00:56:43,084 --> 00:56:44,914
I call it chrome plating
or over engineering.

1029
00:56:45,754 --> 00:56:49,924
Use your architect's experience here
to know how far you should go with

1030
00:56:49,924 --> 00:56:51,154
each of these techniques, right?

1031
00:56:51,214 --> 00:56:52,204
You don't need all of them.

1032
00:56:52,624 --> 00:56:56,764
You may need only certain ones right
now, you may need other ones over time.

1033
00:56:57,754 --> 00:57:02,644
Finally, number four, don't overlook
testing and qu quantitative analysis.

1034
00:57:02,854 --> 00:57:06,274
It's the only way to be sure before
you advertise an SLA to customers.

1035
00:57:06,774 --> 00:57:09,174
Now we've got a wrap up slide
here that shows some of the high

1036
00:57:09,174 --> 00:57:10,614
level resources for this talk.

1037
00:57:11,364 --> 00:57:13,584
We mentioned poly for net briefly.

1038
00:57:13,704 --> 00:57:18,364
His, from Netflix is a good go model
to look at cast monkey for testing.

1039
00:57:18,944 --> 00:57:20,024
we mentioned a lot of other things.

1040
00:57:20,024 --> 00:57:23,964
Those will be in the slide notes as
we go, as you pull those down and look

1041
00:57:23,964 --> 00:57:25,584
through them on particular topics.

1042
00:57:25,924 --> 00:57:29,794
FMEA, Particular other testing
tools, different pattern

1043
00:57:29,794 --> 00:57:30,964
references and things like that.

1044
00:57:31,834 --> 00:57:34,234
Of course, you can look through
cloud providers best practices.

1045
00:57:34,294 --> 00:57:38,254
both AWS and Azure have good well
architected programs that sort of

1046
00:57:38,254 --> 00:57:42,544
give us a list of, these are the
prescriptive things that if we were

1047
00:57:42,544 --> 00:57:45,424
to give you a workshop and they
can, this is what you should look

1048
00:57:45,424 --> 00:57:49,734
at for a large SaaS application
that's going global or is in lots of

1049
00:57:49,734 --> 00:57:51,144
different, networking environments.

1050
00:57:51,534 --> 00:57:55,134
Uber has a really good compilation
here of notes on the SRE role.

1051
00:57:55,384 --> 00:57:58,834
if you're ever wonder what does
DevOps do, where did DevOps come from?

1052
00:57:59,344 --> 00:58:02,914
it came from SRE Site Reliability
Engineering and came a

1053
00:58:02,914 --> 00:58:04,264
little closer to development.

1054
00:58:04,594 --> 00:58:08,204
But, they talk a lot about how to
operationalize systems and how to put in

1055
00:58:08,204 --> 00:58:10,154
place good discipline on these practices.

1056
00:58:10,664 --> 00:58:13,364
And finally, if you're interested in
the latest from Chef, I put our product

1057
00:58:13,364 --> 00:58:14,534
documentation at the bottom here.

1058
00:58:15,034 --> 00:58:15,604
As a thank you.

1059
00:58:15,604 --> 00:58:17,579
I'll put a little bit of a pitch here.

1060
00:58:17,579 --> 00:58:20,819
I'd like to thank the engineers that
obviously published before on this topic.

1061
00:58:21,239 --> 00:58:25,589
the history goes way, way back over
multiple decades in terms of building

1062
00:58:26,009 --> 00:58:27,749
available and reliable systems.

1063
00:58:27,999 --> 00:58:32,079
obviously the teams at Poly and
Netflix who've bottled up a lot of this

1064
00:58:32,139 --> 00:58:35,199
into tools, modules that we can use.

1065
00:58:35,709 --> 00:58:38,349
Thank you, of course to the open
source engineers who maintain all

1066
00:58:38,349 --> 00:58:39,939
the tools that we talked about here.

1067
00:58:40,779 --> 00:58:44,259
A lot of teams at AWS and Microsoft
and other cloud providers that

1068
00:58:44,259 --> 00:58:46,029
brought us lessons learned from that.

1069
00:58:46,139 --> 00:58:49,589
and especially on the infrastructure
set of best practices there.

1070
00:58:49,929 --> 00:58:54,799
they were really the first ones to see
global scale and a lot of these things, as

1071
00:58:55,039 --> 00:58:58,519
organizations moved out of their own data
centers into shared hosting facilities.

1072
00:58:58,939 --> 00:59:02,029
We obviously learned from our mentors
and then enables each of us to take

1073
00:59:02,029 --> 00:59:03,949
on increasingly scalable solutions.

1074
00:59:04,339 --> 00:59:06,889
thank you also to you, the audience
for listening to this talk.

1075
00:59:06,889 --> 00:59:10,129
Without you, there wouldn't be
any next application to build.

1076
00:59:10,559 --> 00:59:13,769
I hope this has caused you to look at
your applications, inspires you to talk

1077
00:59:13,769 --> 00:59:15,539
to your teams about quality regularly.

1078
00:59:16,139 --> 00:59:19,019
Please reach out to me on LinkedIn if
you have any questions or feedback.

1079
00:59:19,409 --> 00:59:22,319
We wish you a very helpful
and valuable conference.

1080
00:59:22,319 --> 00:59:24,899
42 from Brian at Chef.

1081
00:59:25,289 --> 00:59:25,529
Goodbye.

