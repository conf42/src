1
00:00:00,360 --> 00:00:01,080
Hello everyone.

2
00:00:01,260 --> 00:00:02,490
I'm Aaron Subramanian.

3
00:00:02,820 --> 00:00:06,060
It's a pleasure to be here at Conti
two to talk about one of the most

4
00:00:06,060 --> 00:00:08,430
urgent and timely topics in tech today.

5
00:00:08,820 --> 00:00:13,380
Ethical data engineering as AI systems
increasingly influence our daily

6
00:00:13,380 --> 00:00:18,210
lives, the decisions we make as data
engineers have deep societal impacts.

7
00:00:18,600 --> 00:00:22,650
My talk today will explore how
we can design and operate data

8
00:00:22,650 --> 00:00:27,240
systems that respect privacy,
eliminate bias, and uphold fairness.

9
00:00:27,765 --> 00:00:31,875
We are going to explore real world
case studies and dive into practical

10
00:00:31,875 --> 00:00:35,925
techniques with the goal of leaving
you with tools and insights you

11
00:00:35,925 --> 00:00:37,485
can bring back to your teams.

12
00:00:37,725 --> 00:00:43,394
And think about this as a blueprint
for ethical, scalable data systems

13
00:00:43,425 --> 00:00:45,675
that actually work in practice.

14
00:00:47,415 --> 00:00:48,705
We are living in an era.

15
00:00:49,860 --> 00:00:56,490
Of exponential data growth by 2025, we
expect to hit 175 terabytes globally.

16
00:00:57,030 --> 00:00:59,280
That's a fivefold increase since 2018.

17
00:00:59,460 --> 00:01:04,830
But more data doesn't just mean better
insights, it also means greater risk.

18
00:01:05,640 --> 00:01:09,270
Take for example, Facebook's
Cambridge Analytica scandal.

19
00:01:09,660 --> 00:01:14,070
Millions of users personal data was
harvested and used to influence selections

20
00:01:14,430 --> 00:01:16,230
That wasn't just a model failure.

21
00:01:16,605 --> 00:01:19,575
It was a data engineering
failure, a breakdown in

22
00:01:19,575 --> 00:01:22,245
governance, consent, and ethics.

23
00:01:22,785 --> 00:01:25,425
As engineers, our
responsibilities are expanding.

24
00:01:25,815 --> 00:01:29,505
It's no longer acceptable to
say I just build the pipeline.

25
00:01:30,015 --> 00:01:32,565
We are also responsible for
understanding the societal

26
00:01:32,565 --> 00:01:34,695
implications of the systems we create.

27
00:01:36,915 --> 00:01:41,655
So nearly half of engineering teams
are working with generative AI

28
00:01:41,955 --> 00:01:44,490
reporting privacy issues specifically.

29
00:01:45,330 --> 00:01:49,619
That model can output sensitive
information or identifying information.

30
00:01:49,800 --> 00:01:54,179
This isn't just a technical problem,
it's an ethical and legal one.

31
00:01:54,690 --> 00:01:59,699
For instance, a chat bot trained
on internal company data once

32
00:01:59,699 --> 00:02:02,280
leaked social security numbers.

33
00:02:02,399 --> 00:02:07,440
When prompted cleverly, we can't rely on
black box models to do the right thing.

34
00:02:07,830 --> 00:02:10,049
We must enforce privacy at the data level.

35
00:02:10,680 --> 00:02:13,470
This is where the techniques
like differential privacy.

36
00:02:13,875 --> 00:02:15,825
Federated learning and careful data.

37
00:02:15,825 --> 00:02:16,845
Labeling comment

38
00:02:18,975 --> 00:02:22,155
bias in AI is not limited to model design.

39
00:02:22,304 --> 00:02:26,655
It can creep in at every stage of
the data pipeline, from collections

40
00:02:26,834 --> 00:02:28,575
to pre-processing, to labeling.

41
00:02:29,325 --> 00:02:30,825
Let me give you a powerful example.

42
00:02:30,825 --> 00:02:35,954
The recruiting algorithm trained
on resumes ended up penalizing

43
00:02:35,984 --> 00:02:41,295
applications that include the word
women's asin, women's just captain.

44
00:02:41,834 --> 00:02:42,195
Why?

45
00:02:42,375 --> 00:02:47,535
Because the training data reflected past
hiring PA patterns that favored men.

46
00:02:48,165 --> 00:02:52,484
This shows how deeply bias
can bake, be baked into data.

47
00:02:53,054 --> 00:02:58,484
We must actively design processes
that detect and correct such

48
00:02:58,484 --> 00:03:00,495
issues before they each production.

49
00:03:03,165 --> 00:03:09,165
Let's look at two main strategy strategies
for privacy preserving techniques.

50
00:03:09,885 --> 00:03:15,015
One being like differential privacy,
which offers mathematical guarantees

51
00:03:15,045 --> 00:03:16,485
by adding noise to outputs.

52
00:03:16,515 --> 00:03:22,330
For example, apple collects usage
statistics from your phone using epsilon.

53
00:03:23,864 --> 00:03:25,184
Values between two and four.

54
00:03:25,695 --> 00:03:31,424
This adds enough noise to mask individual
data while preserving aggregated trends.

55
00:03:31,904 --> 00:03:36,915
Meanwhile, Google's wrapper
systems uses values as low as 0.3

56
00:03:36,915 --> 00:03:38,954
to protect search and usage data.

57
00:03:39,434 --> 00:03:41,475
These are real world scalable systems.

58
00:03:42,179 --> 00:03:47,010
On the other side, data anonymization
techniques like K anonymity,

59
00:03:47,160 --> 00:03:53,160
ensures that any individual data is
indistinguishable from at K minus one.

60
00:03:53,190 --> 00:03:53,760
Others.

61
00:03:54,359 --> 00:03:59,040
Combining these approaches, anonymization
plus differential privacy, offers

62
00:03:59,070 --> 00:04:01,470
much stronger privacy protections.

63
00:04:03,780 --> 00:04:09,450
So now let's talk about how are we
gonna address algorithmic bias and

64
00:04:09,450 --> 00:04:11,309
how do we fight it systematically?

65
00:04:11,670 --> 00:04:17,399
First, treat fairness checks as an ongoing
process, one time audit, third work.

66
00:04:17,700 --> 00:04:20,429
Second, apply intersectional analysis.

67
00:04:20,490 --> 00:04:25,409
For example, if credit scoring
model might appear fair by gender

68
00:04:25,830 --> 00:04:31,679
or race independently, but still
disadvantage black women due to the

69
00:04:31,679 --> 00:04:33,390
intersection of race and gender.

70
00:04:34,080 --> 00:04:38,820
Only 29% of the organizations
currently test for this third use

71
00:04:38,820 --> 00:04:41,340
automated tools to flag imbalances.

72
00:04:41,804 --> 00:04:47,505
In feature correlations and model
outputs in healthcare general fairness,

73
00:04:47,505 --> 00:04:52,125
metrics missed clinical relevant
biases in 47% of applications.

74
00:04:52,755 --> 00:04:54,674
Domain specific understanding is key.

75
00:04:55,995 --> 00:04:59,174
Now let's look at a
case study of eCommerce.

76
00:05:00,345 --> 00:05:05,505
Let's explore on this specific to
recommendation engine for which was

77
00:05:05,534 --> 00:05:07,934
performing worse for female users.

78
00:05:08,775 --> 00:05:13,605
On average, women received suggestions
that were 18% less aligned with

79
00:05:13,605 --> 00:05:15,195
their preferences than men.

80
00:05:15,825 --> 00:05:19,485
After digging in, they realized
the training data was skewed

81
00:05:19,485 --> 00:05:21,284
towards male purchase behavior.

82
00:05:21,855 --> 00:05:26,385
They correct this using stratified
sampling, fairness aware, feature

83
00:05:26,385 --> 00:05:28,245
engineering, and model retraining.

84
00:05:29,025 --> 00:05:32,684
The disparity was already reduced
significantly, and customer satisfaction

85
00:05:32,684 --> 00:05:34,049
scores improved across the board.

86
00:05:34,800 --> 00:05:38,610
This illustrates how we even
seemingly neutral systems like

87
00:05:38,610 --> 00:05:44,130
recommendation engine scandal reflect,
reinforce bias if we are in careful

88
00:05:46,380 --> 00:05:46,650
in.

89
00:05:47,220 --> 00:05:49,380
Let's go through the
case study of healthcare.

90
00:05:50,789 --> 00:05:54,090
Arguably the most sensitive
domain for data ethics.

91
00:05:54,690 --> 00:05:59,310
Predictive analytics model designed
to forecast hospital readmissions was

92
00:05:59,340 --> 00:06:02,220
underperforming for non-white patients.

93
00:06:02,490 --> 00:06:03,120
The root cause.

94
00:06:03,450 --> 00:06:07,530
Under representation in the training
data, the team redesigned their data

95
00:06:07,530 --> 00:06:11,730
collection pipeline implemented targeted
outreach and stratified sampling.

96
00:06:11,760 --> 00:06:17,220
Non-white representation
rose from 24.7% to 49%.

97
00:06:18,750 --> 00:06:22,500
They then applied federated
learning to protect patients'

98
00:06:22,500 --> 00:06:25,435
privacy across five institutions.

99
00:06:26,069 --> 00:06:30,630
And used synthetic data generation
by gans to supplement rare subgroups.

100
00:06:31,110 --> 00:06:35,159
These changes cut
performance disparity by 68%.

101
00:06:35,610 --> 00:06:38,159
That's not just fair,
it's clinically better.

102
00:06:39,209 --> 00:06:41,880
Now let's talk about ethical gates.

103
00:06:42,929 --> 00:06:43,890
How do we implement them?

104
00:06:44,730 --> 00:06:46,020
Ethics is in test separate.

105
00:06:46,020 --> 00:06:46,319
Separate step.

106
00:06:46,345 --> 00:06:49,470
It needs to be ba baked into
every stage of the pipeline.

107
00:06:49,980 --> 00:06:52,829
Think of it as a four
gates collection gate.

108
00:06:53,760 --> 00:06:55,169
Are we collecting data ethically?

109
00:06:55,680 --> 00:06:59,970
With consent and appropriate
representation processing it or

110
00:06:59,970 --> 00:07:04,560
our transformation introducing
bias output gate, are the outputs

111
00:07:04,590 --> 00:07:07,440
privacy, safe and fair monitoring it?

112
00:07:07,740 --> 00:07:11,789
Are we continuously evaluating
deployed systems for emerging risk?

113
00:07:12,630 --> 00:07:16,980
This ethics by design model merits
privacy by design, and helps prevent

114
00:07:17,880 --> 00:07:19,770
ethical failures beyond they occur.

115
00:07:20,770 --> 00:07:23,409
Now documentation and transparency.

116
00:07:24,429 --> 00:07:30,489
These are like two important things that
we need to talk about when related to

117
00:07:30,789 --> 00:07:32,859
bridging the ethics and accountability.

118
00:07:33,580 --> 00:07:37,599
When we log data lineage from original
source through transformations, we

119
00:07:37,599 --> 00:07:41,474
create a transparent system, but we
must go beyond technical details.

120
00:07:41,980 --> 00:07:46,570
What tradeoffs were made, what biases
were discovered, what limitations remain.

121
00:07:47,020 --> 00:07:51,730
For example, if your dataset excludes
users under 18 due to constant rules,

122
00:07:52,030 --> 00:07:57,030
note that's a limitation that could
affect general accessibility, tailor

123
00:07:57,030 --> 00:07:59,100
documentation for your audience.

124
00:07:59,310 --> 00:08:01,410
Technical teams want reproducibility.

125
00:08:01,980 --> 00:08:03,930
Regulators want compliance.

126
00:08:04,230 --> 00:08:05,820
End users want clarity,

127
00:08:08,430 --> 00:08:10,525
and let's talk about.

128
00:08:11,670 --> 00:08:13,470
Interdisciplinary collaboration.

129
00:08:14,820 --> 00:08:17,310
Ethical systems are
built by diverse teams.

130
00:08:17,670 --> 00:08:23,970
Engineers bring technical rigor, ethicist
translate principles to policies.

131
00:08:24,600 --> 00:08:27,150
Domain experts provide contextual nuances.

132
00:08:27,270 --> 00:08:31,740
For instance, in an education app,
collaboration with teachers revealed

133
00:08:31,740 --> 00:08:36,870
that an A tutor was reinforcing
gender stereotypes in math problems.

134
00:08:37,260 --> 00:08:39,750
This wouldn't have been
caught by engineers alone.

135
00:08:40,454 --> 00:08:43,664
End users, especially from
vulnerable communities, must

136
00:08:43,664 --> 00:08:45,015
also have a seat at the table.

137
00:08:45,645 --> 00:08:51,375
They can flag issues other misses
because they live with the consequences.

138
00:08:54,254 --> 00:09:00,795
And let's talk about how are we plan
to automate this whole ethical checks?

139
00:09:01,004 --> 00:09:05,295
Because manual checks are not
scalable, we need an automated

140
00:09:05,295 --> 00:09:07,670
system that accesses beneficiaries.

141
00:09:08,430 --> 00:09:12,329
Does the system produce net
positive impact non maleficence?

142
00:09:12,750 --> 00:09:16,319
Does it avoid harm, autonomy,
or users are in control?

143
00:09:16,920 --> 00:09:17,490
Justice.

144
00:09:17,879 --> 00:09:20,280
Are outcomes fair across groups?

145
00:09:21,540 --> 00:09:22,349
Applicability?

146
00:09:22,560 --> 00:09:24,359
Can we explain decisions?

147
00:09:25,290 --> 00:09:31,650
Tools like IBMA, fairness 360, and
Microsoft's offer promising capabilities,

148
00:09:31,710 --> 00:09:33,180
but they're not silver bullets.

149
00:09:33,660 --> 00:09:37,440
We must ensure they are aligned with
real world values and validated in

150
00:09:37,440 --> 00:09:40,470
production settings, not just test beats.

151
00:09:42,839 --> 00:09:45,690
Now let's talk about regulations.

152
00:09:46,689 --> 00:09:48,219
Regulations like GDPR.

153
00:09:48,609 --> 00:09:53,589
HIPAA and UA Act are forcing companies
to treat ethics as a compliance issue.

154
00:09:54,189 --> 00:09:55,869
But the best companies go further.

155
00:09:56,259 --> 00:10:00,040
They embed governance by design,
build pipelines that are flexible

156
00:10:00,040 --> 00:10:02,050
enough to adapt to future rules.

157
00:10:02,709 --> 00:10:05,800
They maintain audit trails that
demonstrate what decisions were

158
00:10:05,800 --> 00:10:10,899
made and why, and most importantly,
they foster a culture where ethics

159
00:10:10,899 --> 00:10:15,879
is part of engineering, not an
afterthought imposed by legal teams.

160
00:10:16,990 --> 00:10:21,130
To wrap up, ethical data engineering
is about more than avoiding risk.

161
00:10:21,310 --> 00:10:22,150
It's about trust.

162
00:10:22,689 --> 00:10:27,160
It's about designing systems that are
not just technically sound, but socially

163
00:10:27,160 --> 00:10:32,800
responsible with privacy, preserving
tools, fairness frameworks, rigorous

164
00:10:32,800 --> 00:10:37,120
documentation and interdisciplinary
collaboration, we can build data

165
00:10:37,120 --> 00:10:43,030
infrastructure that ends and deserves
the trust of users and regulators alike.

166
00:10:44,619 --> 00:10:45,579
Thank you for your time.

167
00:10:45,670 --> 00:10:48,670
I look forward to your
questions and conversations.

