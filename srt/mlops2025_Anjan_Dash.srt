1
00:00:00,955 --> 00:00:01,245
Okay.

2
00:00:01,745 --> 00:00:02,015
Okay.

3
00:00:02,225 --> 00:00:03,034
Hello everyone.

4
00:00:03,284 --> 00:00:08,775
My name is Angen Das and I studied at
Indian Institute of Management and I'm

5
00:00:08,775 --> 00:00:13,155
currently a technical leader at Meta.

6
00:00:13,815 --> 00:00:23,205
So here I work in ads ranking
systems where we use big ML models

7
00:00:23,595 --> 00:00:25,485
to find the right kind of ads.

8
00:00:26,294 --> 00:00:32,144
That can be shown to users on
Facebook, Instagram, et cetera.

9
00:00:32,644 --> 00:00:39,964
So today I'm here to talk about
lops at massive scale, how massive

10
00:00:40,464 --> 00:00:44,874
trillion parameter large models, right?

11
00:00:45,394 --> 00:00:49,444
And we'll see what are the
operational challenges that we

12
00:00:49,444 --> 00:00:52,270
get while training this model.

13
00:00:53,075 --> 00:00:55,625
These models, these really
large language models.

14
00:00:55,775 --> 00:00:57,825
Okay let's dive right in.

15
00:00:58,325 --> 00:01:03,095
In agenda, the first item, which
we have, is to how to deal with the

16
00:01:03,095 --> 00:01:08,899
scale and what are the challenges
while dealing with this really large.

17
00:01:09,574 --> 00:01:15,035
Machine learning models training
them, operationalizing them, serving

18
00:01:15,035 --> 00:01:17,255
them, doing the inference, et cetera.

19
00:01:17,335 --> 00:01:18,715
So we'll talk about that.

20
00:01:18,905 --> 00:01:23,135
The second point we'll talk about
is that what are the infrastructure

21
00:01:23,165 --> 00:01:28,795
components required to build
these kind of models, right?

22
00:01:29,295 --> 00:01:31,035
And serve these kind of models.

23
00:01:31,665 --> 00:01:34,825
The next is pipeline
orchestration automation.

24
00:01:35,095 --> 00:01:39,805
We'll talk about different frameworks
and techniques for managing this

25
00:01:39,805 --> 00:01:44,615
really long training cycles which can
be weeks or sometimes months, right?

26
00:01:45,115 --> 00:01:48,565
Then we'll talk about the
monitoring and failure recovery.

27
00:01:48,845 --> 00:01:52,715
These train, these models are getting
trained for a really long time or weeks.

28
00:01:52,715 --> 00:01:57,815
And then what are the metrics that we
should be tracking because otherwise

29
00:01:58,115 --> 00:01:59,915
we are just flying blind, right?

30
00:02:00,655 --> 00:02:05,165
And also so we'll be tracking
how stable the models are when

31
00:02:05,165 --> 00:02:06,275
the training is happening.

32
00:02:06,635 --> 00:02:10,775
And if some error happens, the
loss should not be, catastrophic.

33
00:02:11,390 --> 00:02:11,720
Okay.

34
00:02:11,880 --> 00:02:14,820
The next is resource
optimization and cost management.

35
00:02:15,090 --> 00:02:21,250
So we'll talk about different
strategies, how to utilize the

36
00:02:21,280 --> 00:02:26,230
maximum of these hardwares so that
we can bring down the cost, right?

37
00:02:27,130 --> 00:02:27,459
Okay.

38
00:02:27,959 --> 00:02:30,839
So the first thing is the skill challenge.

39
00:02:31,409 --> 00:02:32,550
The scale is very high.

40
00:02:32,550 --> 00:02:35,744
And this is a really challenge
because we are talking about

41
00:02:35,934 --> 00:02:38,154
trillion parameter models, right?

42
00:02:38,654 --> 00:02:43,244
So the first challenge that we see
is, I think we already touched Job on,

43
00:02:43,244 --> 00:02:47,624
is that really long training cycles.

44
00:02:48,094 --> 00:02:51,054
Because of these extended
training cycles, it takes.

45
00:02:51,554 --> 00:02:56,669
A really long time to actually find
out whether anything worked or not.

46
00:02:56,749 --> 00:03:00,829
So you have data, you train and train
for weeks, and then you got a model.

47
00:03:00,889 --> 00:03:02,239
Model is good or not.

48
00:03:02,749 --> 00:03:05,119
You only find out after weeks, right?

49
00:03:05,839 --> 00:03:06,109
Okay.

50
00:03:06,229 --> 00:03:10,529
So next is numerous distributor,
GPU working in synchronized.

51
00:03:10,844 --> 00:03:12,284
And parallel manner, right?

52
00:03:12,614 --> 00:03:14,384
So you need a literally lot of GPUs.

53
00:03:14,384 --> 00:03:17,684
So we are talking about thousands and
thousands of GPUs are required, and

54
00:03:17,684 --> 00:03:20,114
these GPUs have to be synchronized.

55
00:03:20,114 --> 00:03:24,484
Otherwise, you'll not be able
to get a model trained, right?

56
00:03:24,754 --> 00:03:26,554
So that's a second challenge.

57
00:03:27,124 --> 00:03:30,364
The third challenge is cost, because this.

58
00:03:30,639 --> 00:03:35,799
Training is happening on a data center,
which have a lot of GPUs and GPUs.

59
00:03:35,799 --> 00:03:38,169
Cost is massive power cost is massive.

60
00:03:38,529 --> 00:03:42,529
And on top of that, people who
know technical expertise on

61
00:03:42,529 --> 00:03:46,739
these things these people's cost
a lot to the companies, right?

62
00:03:47,609 --> 00:03:49,469
So that is a substantial training cost.

63
00:03:49,469 --> 00:03:52,709
So the second is massive
amount of training data, right?

64
00:03:52,919 --> 00:03:56,269
So you have very big, cluster.

65
00:03:56,269 --> 00:03:59,119
And then, you have a very big
model you want to train on,

66
00:03:59,149 --> 00:04:00,529
but what is the data, right?

67
00:04:00,679 --> 00:04:05,119
So we need really massive data
to be fed into this really

68
00:04:05,149 --> 00:04:07,699
hungry, right data hungry models.

69
00:04:07,759 --> 00:04:10,199
And so that is the next challenge.

70
00:04:10,499 --> 00:04:14,659
And then, the last challenge is
considerable volumes of model checkpoints.

71
00:04:14,839 --> 00:04:19,359
So you have really big trillion,
trillion parameter, large model.

72
00:04:19,569 --> 00:04:25,299
Now, if you want to save a snapshot,
like you train for say, one hour

73
00:04:25,299 --> 00:04:29,259
and you want to save a snapshot,
okay, what are my weights right

74
00:04:29,259 --> 00:04:31,450
now that trained weights right now?

75
00:04:31,599 --> 00:04:36,150
So you have to save the whole, whole
model and saving that basically means

76
00:04:36,150 --> 00:04:37,590
that, you need that much memory.

77
00:04:38,100 --> 00:04:42,650
So these are the biggest challenges of,
of training these big models, right?

78
00:04:43,470 --> 00:04:46,840
Let's take look at, take a look
at financial realities, right?

79
00:04:46,840 --> 00:04:49,990
So what are the financial
cost involved here?

80
00:04:50,560 --> 00:04:51,400
We talked about.

81
00:04:51,855 --> 00:04:53,445
That it takes a lot of time.

82
00:04:53,625 --> 00:04:57,045
And then the weekly training
cost is super, super high, right?

83
00:04:57,075 --> 00:04:57,915
That is first thing.

84
00:04:58,305 --> 00:05:00,945
The second thing is cost per failed hour.

85
00:05:00,975 --> 00:05:02,835
If it fails for some reason, right?

86
00:05:02,835 --> 00:05:04,575
The training is happening and it fails.

87
00:05:04,815 --> 00:05:07,065
Then then because of the.

88
00:05:07,515 --> 00:05:09,075
Because of a failure.

89
00:05:09,295 --> 00:05:12,775
Your cost is very high because
your GPUs are sitting idle.

90
00:05:13,025 --> 00:05:18,755
You have already invested a lot of power
in running the GPUs and that is gone.

91
00:05:18,815 --> 00:05:21,635
You might have to go back
to a last checkpoint.

92
00:05:21,685 --> 00:05:24,115
Because so any kind of downtime.

93
00:05:24,615 --> 00:05:26,235
Basically cost a lot, right?

94
00:05:26,775 --> 00:05:28,365
Then there is potential waste, right?

95
00:05:28,545 --> 00:05:30,885
So potential waste means
the data is not coming.

96
00:05:30,885 --> 00:05:34,605
That's why your training is not
happening, or your power went

97
00:05:34,605 --> 00:05:36,645
out or power's not enough, right?

98
00:05:36,924 --> 00:05:39,804
So that is always chances of waste, right?

99
00:05:39,804 --> 00:05:42,564
So we have to make sure that
we minimize the waste, right?

100
00:05:43,354 --> 00:05:43,684
Yeah.

101
00:05:43,835 --> 00:05:47,044
Third is the last one is
basically trading duration.

102
00:05:47,254 --> 00:05:48,214
And since.

103
00:05:48,639 --> 00:05:53,429
This training is going on for a really
long time, then, you have to have right

104
00:05:53,459 --> 00:05:57,239
strategy to minimize cost here because
it's going to take a really long time

105
00:05:57,629 --> 00:05:59,519
and really lot of money because of that.

106
00:05:59,729 --> 00:05:59,939
Okay.

107
00:06:00,489 --> 00:06:01,749
Okay, moving on.

108
00:06:02,249 --> 00:06:05,849
Now let's take a look
at the second agenda.

109
00:06:05,849 --> 00:06:10,289
Basically we have is what are the
infrastructure components required?

110
00:06:10,964 --> 00:06:15,674
To build this and operationalize
these large language models.

111
00:06:16,514 --> 00:06:18,224
The first is very obvious.

112
00:06:18,224 --> 00:06:22,304
You need a distributed training
infrastructure because these

113
00:06:22,304 --> 00:06:23,534
are really large models.

114
00:06:23,534 --> 00:06:27,194
They cannot fit in one machine, and
you want a really big cluster to.

115
00:06:27,694 --> 00:06:31,754
Basically load these models and
then train these models, right?

116
00:06:31,844 --> 00:06:35,084
That is first, and then there
since they're distributed.

117
00:06:35,084 --> 00:06:38,894
That's why you need a right very
high bandwidth connection between

118
00:06:38,894 --> 00:06:43,214
GPUs, between clusters, so that
you can actually effectively train.

119
00:06:43,274 --> 00:06:46,669
Otherwise, you'll lose
lot of time to only data.

120
00:06:47,169 --> 00:06:51,319
And on top of that, additionally
you can have custom kernels.

121
00:06:51,844 --> 00:06:57,559
Kernels are nothing but programs that
run on the single set of instructions

122
00:06:57,589 --> 00:07:00,079
that run on multiple machines, right?

123
00:07:00,269 --> 00:07:04,679
And that way it helps to
train the model efficiently.

124
00:07:05,179 --> 00:07:10,229
But to harness the hardware power
you can have custom kernels.

125
00:07:11,024 --> 00:07:16,414
And they can really harness the hardware
specific, acceleration capabilities

126
00:07:16,474 --> 00:07:17,819
that are provided by these hardwares.

127
00:07:18,319 --> 00:07:18,649
Okay.

128
00:07:18,869 --> 00:07:20,639
So the next is data pipeline system.

129
00:07:20,919 --> 00:07:24,729
You have your distributed training
infras already, but you also

130
00:07:24,729 --> 00:07:26,899
need and those are data hungry.

131
00:07:27,199 --> 00:07:33,349
That's why you need a data pipeline system
to feed data continuously into that.

132
00:07:33,429 --> 00:07:35,499
So that's a data pipeline
system we're talking about.

133
00:07:35,529 --> 00:07:35,859
Okay.

134
00:07:36,669 --> 00:07:40,459
The third the third is
orchestration frameworks.

135
00:07:40,639 --> 00:07:44,269
So you need, different kind of
frameworks, which are aware of the

136
00:07:44,269 --> 00:07:46,619
GPU and which you can schedule right.

137
00:07:46,619 --> 00:07:48,599
Kind of jobs and right
kind of hardware, right?

138
00:07:48,839 --> 00:07:50,339
And orchestration is very important.

139
00:07:50,769 --> 00:07:53,329
The last one is monitoring
and observability.

140
00:07:53,719 --> 00:07:55,250
This company is very important.

141
00:07:55,760 --> 00:07:59,699
So in monitoring and observability
we talk about what kind of

142
00:07:59,699 --> 00:08:00,845
metrics that you want to track.

143
00:08:01,214 --> 00:08:01,244
Okay.

144
00:08:01,814 --> 00:08:06,645
And if those metrics are going,
haywire going up, down and you have

145
00:08:06,645 --> 00:08:09,045
right kind of alerts are set up right?

146
00:08:09,285 --> 00:08:14,295
So that what you have is, as the
training is progressing, you are know,

147
00:08:14,295 --> 00:08:19,215
what's going on because without the
metrics, you are basically flying blind.

148
00:08:19,755 --> 00:08:20,055
Okay.

149
00:08:20,835 --> 00:08:21,225
Okay.

150
00:08:21,315 --> 00:08:26,075
So let's move on and take a deeper look
at distributed training frameworks.

151
00:08:26,565 --> 00:08:29,935
We talk about four components
distributor training, component

152
00:08:29,965 --> 00:08:34,655
distributor training frameworks, and
then we talk about data pipelines.

153
00:08:34,835 --> 00:08:39,505
Then we talked about automation and
frameworks, and then monitoring, right?

154
00:08:39,655 --> 00:08:41,875
So the first thing, we talk about
distributor training frameworks now.

155
00:08:42,435 --> 00:08:45,915
We have, we are here, we are
showing two distributor frameworks.

156
00:08:45,915 --> 00:08:47,415
One is Megatron by Nvidia.

157
00:08:48,010 --> 00:08:50,110
The other is deep speed
by Microsoft, right?

158
00:08:50,590 --> 00:08:56,210
So in Megatron we have tensor parallelism
for efficient layer distribution.

159
00:08:56,420 --> 00:08:58,880
So any neural network has lot of layers.

160
00:08:59,250 --> 00:09:03,150
So with tensor parallelism, we can
basically make it more efficient.

161
00:09:04,020 --> 00:09:05,820
Then the next one is sequence parallelism.

162
00:09:06,690 --> 00:09:07,890
What is sequence bism?

163
00:09:07,950 --> 00:09:09,480
So let's understand what is sequence.

164
00:09:10,295 --> 00:09:14,640
You send a set of send a prompt
some text to a large language

165
00:09:14,640 --> 00:09:15,780
model that is a sequence, right?

166
00:09:16,110 --> 00:09:22,280
So input sequence can be really
large and that has a very

167
00:09:22,520 --> 00:09:25,670
big memory footprint, right?

168
00:09:25,740 --> 00:09:28,170
And this is in for sequence,
we can chunk it out.

169
00:09:28,670 --> 00:09:32,420
And assign a chunk to
A-G-P-U-A specific GPU, right?

170
00:09:32,780 --> 00:09:36,110
So that way you can manage
that memory footprint, right?

171
00:09:36,170 --> 00:09:37,880
So that is what a sequence parallelism is.

172
00:09:38,180 --> 00:09:43,230
And then we can basically megaton
is optimized to Nvidia GPUs, right?

173
00:09:43,500 --> 00:09:47,990
Since we have some GPU here, a 100 H
100, but it can be more different kind

174
00:09:47,990 --> 00:09:51,580
of GPUs also similarly deep speed.

175
00:09:52,080 --> 00:09:56,640
Have something called zero optimizer,
which kind of me optimizes memory.

176
00:09:57,000 --> 00:10:01,040
You have pipeline parallelism or
for balance computation, you have

177
00:10:01,040 --> 00:10:05,030
some offloading techniques for
CPU and VME memory extensions.

178
00:10:05,300 --> 00:10:07,220
So these are different kind of techniques.

179
00:10:07,220 --> 00:10:12,170
These distributed frameworks used
to make the training faster, right?

180
00:10:12,690 --> 00:10:16,100
This modern, massive
scale parameter training.

181
00:10:16,475 --> 00:10:20,165
Combines multiple parallelism
strategies simultaneously.

182
00:10:20,555 --> 00:10:23,645
So you can have tensor parallelism,
you can pipeline parallelism,

183
00:10:23,645 --> 00:10:24,845
you can data parallelism.

184
00:10:24,845 --> 00:10:29,815
It's a sequence parallelism all working
together to together to basically

185
00:10:29,815 --> 00:10:31,195
train the model more efficiently.

186
00:10:31,585 --> 00:10:31,915
Okay.

187
00:10:32,905 --> 00:10:33,265
Okay.

188
00:10:33,445 --> 00:10:35,785
So next let's talk about
the pipeline organization.

189
00:10:35,785 --> 00:10:38,135
So we talk about distributed
components, distributed

190
00:10:38,135 --> 00:10:39,455
architecture, distributed frameworks.

191
00:10:39,785 --> 00:10:42,065
And then we talk about
pipeline organization, right?

192
00:10:42,245 --> 00:10:46,705
So pipeline organization, we talk
about what that the first is, first

193
00:10:46,705 --> 00:10:48,415
step is data preparation, right?

194
00:10:48,415 --> 00:10:50,725
So without data you don't
have a model, right?

195
00:10:51,295 --> 00:10:56,965
So you will prepare the data and then
feed that data to this data hungry

196
00:10:57,715 --> 00:11:02,145
training clusters where we are training
the, training this models, right?

197
00:11:02,865 --> 00:11:05,805
So in training, so the next
step is training orchestration.

198
00:11:05,805 --> 00:11:09,435
You are doing this, orchestrating
this training on, various

199
00:11:09,555 --> 00:11:11,965
big cheap U clusters, right?

200
00:11:12,025 --> 00:11:15,385
And after that you have to have a
model and you have to evaluate the

201
00:11:15,385 --> 00:11:17,125
model and see what is good or bad.

202
00:11:17,455 --> 00:11:20,035
And if it's good, then you
have to register the model.

203
00:11:20,740 --> 00:11:25,690
You have to, basically, you have to
save the model version it, register

204
00:11:25,690 --> 00:11:28,800
it in a way that you know, you can
basically, you have thousands of versions

205
00:11:28,800 --> 00:11:30,210
and you can find the right version.

206
00:11:30,450 --> 00:11:33,450
Which version is working correctly,
it was good or bad, and all that.

207
00:11:33,630 --> 00:11:39,960
So the good model checkpoint or snapshot,
basically will be store in model registry.

208
00:11:40,460 --> 00:11:43,670
So all these components, all these
com pipeline orchestration is very

209
00:11:43,670 --> 00:11:49,440
important to get this whole thing running
smoothly for a long duration of time.

210
00:11:49,440 --> 00:11:51,840
Like we are talking about
weeks or sometimes months.

211
00:11:51,940 --> 00:11:52,150
Okay.

212
00:11:52,650 --> 00:11:53,220
Moving on.

213
00:11:53,370 --> 00:11:56,370
The third is automated
recovery strategies.

214
00:11:56,400 --> 00:12:01,205
What kind of so here we'll talk about
what kind of failures can happen and

215
00:12:01,205 --> 00:12:05,250
what can we do if a failure happens,
so a failure detection, right?

216
00:12:05,295 --> 00:12:06,765
So the first is failure detection.

217
00:12:06,765 --> 00:12:10,405
So it'll be continuously
monitoring for harder failures

218
00:12:11,035 --> 00:12:12,865
without gradient explosions.

219
00:12:12,955 --> 00:12:15,895
Basically your training and you
have forward path and backward

220
00:12:15,895 --> 00:12:20,035
path, and then you are computing
this gradient and a gradient spike.

221
00:12:20,485 --> 00:12:24,595
So you have to basically notify for
that, and you also have to notify

222
00:12:24,595 --> 00:12:26,035
about data corruption issues.

223
00:12:26,335 --> 00:12:27,115
So that's very important.

224
00:12:27,615 --> 00:12:29,505
The next point is checkpoint management.

225
00:12:29,505 --> 00:12:34,185
We talked about, which we touched
upon it a little bit earlier that you

226
00:12:34,185 --> 00:12:35,690
have, like your training is going on.

227
00:12:36,280 --> 00:12:37,180
For two weeks.

228
00:12:37,240 --> 00:12:40,445
It's not that you will see,
you will be having a output

229
00:12:40,445 --> 00:12:41,525
only after two weeks, right?

230
00:12:41,825 --> 00:12:45,335
You're getting an output after
every little training you do, and

231
00:12:45,345 --> 00:12:48,635
basically keep track of what is the
checkpoint, what is the saving the

232
00:12:48,635 --> 00:12:50,895
checkpoint as a training progresses.

233
00:12:50,995 --> 00:12:54,955
It should have a right kind of
strategy that if something fails,

234
00:12:55,375 --> 00:12:59,305
which checkpoint I should look at, and
how many checkpoints I can fall back.

235
00:12:59,815 --> 00:13:00,085
Two.

236
00:13:00,185 --> 00:13:03,485
The third one is, the next
one is resource reallocation.

237
00:13:03,995 --> 00:13:05,135
Dynamic provisioning.

238
00:13:05,135 --> 00:13:07,685
And cluster reconfiguration happens here.

239
00:13:08,205 --> 00:13:11,595
Your resources allocated for
some training, but then you can

240
00:13:11,775 --> 00:13:13,455
want to reallocate it, right?

241
00:13:13,455 --> 00:13:15,975
So you can be, you should
be able to do that here.

242
00:13:16,735 --> 00:13:18,235
Then the training re redemption.

243
00:13:18,535 --> 00:13:19,480
So if something fails.

244
00:13:19,980 --> 00:13:22,260
Which the training should
resume automatically.

245
00:13:22,350 --> 00:13:25,550
It's not you have to manually
go and fix something.

246
00:13:25,660 --> 00:13:27,550
It should be automatically doing.

247
00:13:27,730 --> 00:13:32,770
So robust recovery automation is
very important and mission critical.

248
00:13:32,950 --> 00:13:33,160
Okay.

249
00:13:34,000 --> 00:13:34,540
Moving on.

250
00:13:35,040 --> 00:13:36,565
What kind of monitoring are we doing?

251
00:13:37,065 --> 00:13:38,295
We talked about metrics.

252
00:13:38,355 --> 00:13:40,605
So what kind of metrics
are we tracking here?

253
00:13:41,025 --> 00:13:44,505
So the first metric is system
metrics, and the next is training

254
00:13:44,505 --> 00:13:47,625
metrics, because your system metric
is basically hardware metrics.

255
00:13:47,895 --> 00:13:53,995
So G utilization, network
bandwidth disc io power.

256
00:13:54,085 --> 00:13:55,165
So these are hardware.

257
00:13:55,490 --> 00:13:56,660
Hardware and power, right?

258
00:13:56,900 --> 00:13:59,540
So these are system metrics,
but you are training it.

259
00:13:59,540 --> 00:14:01,460
So there are training
methods, training a model.

260
00:14:01,520 --> 00:14:02,930
That's why they have a training metrics.

261
00:14:02,930 --> 00:14:05,930
So you have loss, you have a
gradient, you have a learning rate

262
00:14:05,930 --> 00:14:09,080
and a parameter, denomination,
parameters, how they're doing, right?

263
00:14:09,080 --> 00:14:10,400
Evolution of parameters.

264
00:14:10,670 --> 00:14:11,780
So these are the metrics.

265
00:14:12,200 --> 00:14:16,490
And then for all the system metrics
and training metrics, you have

266
00:14:16,490 --> 00:14:17,780
to have some kind of alerting.

267
00:14:17,840 --> 00:14:23,150
So if something is not right
or not tending right, then you

268
00:14:23,150 --> 00:14:24,290
have to have an alert system.

269
00:14:24,550 --> 00:14:30,700
Anomaly detection algorithms can basically
notify folks whenever there is an anomaly.

270
00:14:31,090 --> 00:14:33,370
And then you can have
predictive failure analysis.

271
00:14:33,670 --> 00:14:35,290
You have the data, now you can do some.

272
00:14:36,070 --> 00:14:39,820
Put analysts on that, then you
have to have, you have notify

273
00:14:39,850 --> 00:14:40,990
multiple channels, right?

274
00:14:41,110 --> 00:14:43,090
It's not like only a phone rings.

275
00:14:43,090 --> 00:14:46,750
You have a phone, you have Slack,
you have IM messages, email, right?

276
00:14:47,110 --> 00:14:48,820
Call, everything you can do, right?

277
00:14:49,390 --> 00:14:50,800
And automated escalation policy.

278
00:14:50,880 --> 00:14:55,680
If you, if somebody doesn't pick up the
phone and fix it, then, keep on escorting.

279
00:14:56,180 --> 00:14:56,480
Okay.

280
00:14:56,540 --> 00:14:58,340
So that is real time monitoring systems.

281
00:14:58,610 --> 00:15:01,100
Let's move on to some
advanced monitoring systems.

282
00:15:01,100 --> 00:15:03,530
And early warning systems, right?

283
00:15:03,530 --> 00:15:05,360
So those are not early.

284
00:15:05,360 --> 00:15:07,460
So these are earlier,
early rise systems, right?

285
00:15:07,760 --> 00:15:11,450
The gradient explosion signals, so
we talked about this gradient, right?

286
00:15:11,450 --> 00:15:15,710
So gradient is the way to
adjust the weights, right?

287
00:15:15,860 --> 00:15:17,360
So you are turning the model.

288
00:15:17,510 --> 00:15:20,960
Turning the model is nothing, but,
adjusting the weights of the parameters

289
00:15:21,380 --> 00:15:25,850
and whenever the gradient becomes, shoots
up or shoots down, shoots up really high.

290
00:15:26,240 --> 00:15:28,730
Then it will just mess
up the training, right?

291
00:15:28,730 --> 00:15:31,775
The stability of the training
will go sideways and go south.

292
00:15:32,555 --> 00:15:35,465
Monitoring for certain
spikes is super important.

293
00:15:35,965 --> 00:15:38,695
Activation distribution ships, right?

294
00:15:39,055 --> 00:15:43,750
So tracking layer output distribution
statistics for unexpected pattern changes.

295
00:15:43,885 --> 00:15:47,395
So if you are basically tracking every
activation and there's like super high

296
00:15:47,575 --> 00:15:52,075
change on that, then also it should
be basically, taking some action here.

297
00:15:52,575 --> 00:15:54,015
Validation and trained ence.

298
00:15:54,165 --> 00:15:59,145
So you have trained the model a lot and
then you have validated it that, okay, the

299
00:15:59,205 --> 00:16:00,945
trained model is actually performing here.

300
00:16:00,995 --> 00:16:01,505
Or not.

301
00:16:01,865 --> 00:16:06,650
If the difference between train and
the test, train and validation is

302
00:16:06,875 --> 00:16:11,735
really high, then something is wrong
and we need to be notified for that.

303
00:16:12,235 --> 00:16:15,645
Similarly, learning data and loss
correlation, monitoring for unexpected

304
00:16:15,645 --> 00:16:20,295
loss behavior during learning rate changes
that also needs to be tackled, right?

305
00:16:20,795 --> 00:16:23,315
So early duration basically
enables intervention before

306
00:16:23,315 --> 00:16:24,875
catastrophic failure happens, right?

307
00:16:24,875 --> 00:16:28,175
So you have some ation happening
that if something is going on,

308
00:16:28,175 --> 00:16:29,885
so you can intervene, right?

309
00:16:30,125 --> 00:16:31,895
You can see a temporary
learning rate reduction.

310
00:16:31,895 --> 00:16:35,765
You can do, you can say selective
gradient clipping adjustments.

311
00:16:35,945 --> 00:16:38,015
The grad is going high, so
you can clip it a little bit.

312
00:16:38,435 --> 00:16:40,895
You can do emergency checkpoint,
emergency check pointing.

313
00:16:40,895 --> 00:16:42,185
Things are going bad.

314
00:16:42,485 --> 00:16:46,655
So then that's why you can checkpoint
quickly emergency so that you can save

315
00:16:46,655 --> 00:16:51,345
a good snapshot and a snapshot of the
weights, and then, train from there.

316
00:16:51,825 --> 00:16:54,345
Or you can do some automatic hyper tuning.

317
00:16:54,525 --> 00:16:59,735
So now let's talk about
computational efficient optimization.

318
00:16:59,735 --> 00:17:01,895
So we're talking we
touched up on this, right?

319
00:17:02,045 --> 00:17:05,325
So you can do hardware acceleration
and those kind of things.

320
00:17:05,545 --> 00:17:08,065
But here we have some optimization
techniques we talk about.

321
00:17:08,365 --> 00:17:11,335
So the first optimization technique
is mixed precision training.

322
00:17:12,065 --> 00:17:16,145
Use using f floating point
16 bit instead of 32 bit.

323
00:17:16,475 --> 00:17:21,185
You can basically reduce the memory
footprints and you can also increase

324
00:17:21,185 --> 00:17:22,595
the throughput of the system.

325
00:17:22,955 --> 00:17:29,725
Basically, number of bits is less so
computation is less and memory is less.

326
00:17:29,740 --> 00:17:35,050
That, so the, so overall, the
throughput of the system improves.

327
00:17:35,550 --> 00:17:37,470
So next is activation Checkpointing.

328
00:17:37,485 --> 00:17:44,520
So activation checkpointing is
basically a mechanism to, to trade.

329
00:17:44,925 --> 00:17:49,085
Memory for compute, when the
training is happening, you are

330
00:17:49,085 --> 00:17:52,775
having, creating activations and
then saving those activations.

331
00:17:52,925 --> 00:17:55,625
And in the backward pass you
can use it in the backward back

332
00:17:55,625 --> 00:17:56,705
propagation, you can use it.

333
00:17:57,125 --> 00:18:01,105
Instead of that, you are
basically computing it on the

334
00:18:01,105 --> 00:18:02,875
fly during the back propagation.

335
00:18:02,875 --> 00:18:04,945
So that is basically is
active reject pointing.

336
00:18:05,395 --> 00:18:05,425
Okay.

337
00:18:05,935 --> 00:18:06,835
The flash attention.

338
00:18:06,835 --> 00:18:11,185
So these are, this is a special technique
which basically makes the attention.

339
00:18:11,685 --> 00:18:13,475
Computation is really faster.

340
00:18:13,535 --> 00:18:14,855
So that is flash attention.

341
00:18:15,635 --> 00:18:19,855
And with a lower memory footprint,
they can be really helpful in speeding

342
00:18:19,855 --> 00:18:21,805
up your machine learning model train.

343
00:18:22,285 --> 00:18:22,555
Okay?

344
00:18:23,515 --> 00:18:27,235
So with all this computational
efficiency optimizations you'll get

345
00:18:27,235 --> 00:18:29,215
faster training cycles and lower costs.

346
00:18:29,875 --> 00:18:33,935
Okay moving on to resource
optimization strategies.

347
00:18:34,435 --> 00:18:39,630
So sometimes the resource so this
approach is basically dynamic

348
00:18:39,630 --> 00:18:41,760
resource allocation strategy, right?

349
00:18:42,120 --> 00:18:45,500
Your resource needs to change
without within the training.

350
00:18:45,560 --> 00:18:47,960
Throughout the training, your
resource might change, right?

351
00:18:48,200 --> 00:18:49,370
Resource requirement will change.

352
00:18:49,700 --> 00:18:52,070
So initial phases you'll
need a really high memory.

353
00:18:52,490 --> 00:18:56,090
When the middle phases, you'll
need more compute, less memory.

354
00:18:56,460 --> 00:19:01,920
Or stable memory and final phases,
you might basically reduce passage.

355
00:19:02,420 --> 00:19:05,870
So that's why in the final
pages know resource requirement

356
00:19:05,870 --> 00:19:06,770
might be different, right?

357
00:19:07,010 --> 00:19:09,320
So there are graphs which
basically show that you know

358
00:19:09,560 --> 00:19:11,510
how things are changing, right?

359
00:19:11,970 --> 00:19:18,260
Early phases here, then phase, then later,
then scaling down and refinement, right?

360
00:19:18,710 --> 00:19:20,900
So in all these phases.

361
00:19:21,400 --> 00:19:23,290
Different kind of resource
requirement is there.

362
00:19:23,290 --> 00:19:28,320
So if we can, manage that,
that will improve overall cost

363
00:19:28,320 --> 00:19:29,850
and the resource requirement.

364
00:19:30,630 --> 00:19:30,870
Okay.

365
00:19:31,320 --> 00:19:31,620
Okay.

366
00:19:32,110 --> 00:19:32,350
Okay.

367
00:19:32,350 --> 00:19:34,000
The next is model deploy permanent.

368
00:19:34,500 --> 00:19:36,600
So you have the whole thing ready.

369
00:19:37,110 --> 00:19:41,310
You have trained four weeks and you
have the, finally you have the model,

370
00:19:41,460 --> 00:19:43,170
which can be used in production.

371
00:19:44,130 --> 00:19:48,370
But to use a model in production,
you need to do a lot of things.

372
00:19:48,640 --> 00:19:56,010
The first is serving these para big
models will require big memory, right?

373
00:19:56,610 --> 00:20:02,310
So that means you need a substantial
GPUs to basically just serve request

374
00:20:02,670 --> 00:20:04,760
to, for predictions on these models.

375
00:20:04,860 --> 00:20:06,570
Next is inference latency.

376
00:20:07,280 --> 00:20:11,270
Yeah, you have a model, but then
when the request comes in to

377
00:20:11,270 --> 00:20:16,220
predict it, you have user's expert,
not super responsive responsible,

378
00:20:16,220 --> 00:20:17,660
basically super fast responsive.

379
00:20:18,170 --> 00:20:23,250
But if the inference latency is very high
because because you have less GPUs or

380
00:20:23,250 --> 00:20:26,640
something like that, or your bandwidth
is not good, then it'll not work.

381
00:20:27,150 --> 00:20:29,130
So for that, we have.

382
00:20:29,945 --> 00:20:33,095
Inference, la inference we have to
be very careful about it, right?

383
00:20:33,395 --> 00:20:37,685
So inference latency basically
increases or basically increases when

384
00:20:37,685 --> 00:20:39,905
the sequence length increases, right?

385
00:20:40,115 --> 00:20:43,115
When the number of inputs,
sequences is very large.

386
00:20:43,325 --> 00:20:48,005
Then this attention layers that we have
in large, longer models or transformer

387
00:20:48,005 --> 00:20:52,115
layers that we have, that competition
skill will basically increase a lot.

388
00:20:52,615 --> 00:20:53,785
Throughput optimizations.

389
00:20:53,905 --> 00:20:57,715
So you can do some batching strategies
and balance latency versus throughput.

390
00:20:57,715 --> 00:20:58,735
So you can do some batching.

391
00:20:58,765 --> 00:20:59,785
So basically increase it.

392
00:21:00,355 --> 00:21:04,405
So key diploma strategies that we need to
make sure that first extensor parallelism

393
00:21:04,435 --> 00:21:06,565
across multiple GPUs within a node.

394
00:21:07,085 --> 00:21:11,345
So within a node, basically you have
multiple GPUs, and then you can do

395
00:21:11,345 --> 00:21:12,665
Tencent pallets in there, right?

396
00:21:12,665 --> 00:21:13,415
That is the first thing.

397
00:21:13,835 --> 00:21:15,935
Second is quantization basically use.

398
00:21:16,520 --> 00:21:21,020
Reduced memory footprint because you
are using lower precision, 14 point

399
00:21:21,020 --> 00:21:24,810
numbers and quantizing and lower
precision lower precision, 14 point

400
00:21:24,810 --> 00:21:26,520
numbers if you use, it'll be good.

401
00:21:26,910 --> 00:21:29,610
And then KV cache optimization
for efficient context handling.

402
00:21:29,610 --> 00:21:31,170
So you can use some caching there.

403
00:21:31,590 --> 00:21:36,060
And then specialized high performance
hardware you can always use to, and the

404
00:21:36,060 --> 00:21:38,080
speed of the performance of your systems.

405
00:21:38,200 --> 00:21:40,620
Okay, so let's come do key takeaways here.

406
00:21:40,920 --> 00:21:46,730
So for ML ops to work for you, you have to
have infrastructure first approach, right?

407
00:21:47,130 --> 00:21:48,120
At a massive scale.

408
00:21:48,180 --> 00:21:50,700
This ML Ops is not an afterthought.

409
00:21:50,730 --> 00:21:54,030
It has to be critical foundation
that makes training possible.

410
00:21:54,060 --> 00:21:56,280
So you have to invest
in distributor systems.

411
00:21:56,280 --> 00:21:59,550
You have to invest in distributor
system expertise in terms of

412
00:21:59,550 --> 00:22:01,080
people who know what they're doing.

413
00:22:01,180 --> 00:22:03,220
Next is automate everything Manual.

414
00:22:03,220 --> 00:22:06,760
Manual intervention during extended
training runs is impractical.

415
00:22:07,375 --> 00:22:10,495
And build a comprehensive, you
can't just do that manually.

416
00:22:10,545 --> 00:22:13,845
So you have to, comprehensive
automation, we talk about monitoring.

417
00:22:13,845 --> 00:22:16,845
We, you have a recovery,
you have optimization.

418
00:22:16,845 --> 00:22:18,015
Everything has to be automated.

419
00:22:18,115 --> 00:22:19,765
So then multi-level monitoring.

420
00:22:19,855 --> 00:22:24,025
So you have to have integrate hardware
level monitoring, like system level

421
00:22:24,025 --> 00:22:26,005
monitoring and alerting is there.

422
00:22:26,275 --> 00:22:30,835
Then for similarly, you have the training
level metrics and training level alerting.

423
00:22:31,135 --> 00:22:33,505
So multi-level monitoring
and alerting has to be there.

424
00:22:33,535 --> 00:22:37,445
Okay last but not the
least is cost, right?

425
00:22:37,715 --> 00:22:39,305
So cost is a first class method.

426
00:22:39,895 --> 00:22:46,015
At this scale, at basically a trillion
parameter, large language models, right

427
00:22:46,315 --> 00:22:50,545
at that scale, ML ops efficiency directly
impacts substantial learning cost, right?

428
00:22:51,205 --> 00:22:56,185
So you can reduce the running cost
very easily if you improve your mops.

429
00:22:56,685 --> 00:23:00,705
So that's why you have to track
it, optimize it, report it, and

430
00:23:00,705 --> 00:23:02,145
keep on improving it, right?

431
00:23:02,455 --> 00:23:07,165
And this has to be a si a similar
goal as model performance.

432
00:23:07,165 --> 00:23:11,365
So model you train is performing
better that you track, but I

433
00:23:11,365 --> 00:23:12,565
would tracking the cost also.

434
00:23:13,105 --> 00:23:18,655
Okay, so that's the key takeaway and
with that I would like to say thank

435
00:23:18,655 --> 00:23:21,325
you very much and have a nice day.

