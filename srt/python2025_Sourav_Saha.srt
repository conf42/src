1
00:00:00,220 --> 00:00:00,970
Hello, everyone.

2
00:00:01,740 --> 00:00:02,920
Welcome to my talk.

3
00:00:03,090 --> 00:00:07,760
Today I'll be talking about a data
frame library named fire ducks that

4
00:00:07,790 --> 00:00:09,949
we are working at the research lab.

5
00:00:10,520 --> 00:00:16,460
So we will explore, the motivation
behind developing the fighters by stating

6
00:00:16,460 --> 00:00:19,120
some of the key challenges with pandas.

7
00:00:19,505 --> 00:00:22,585
And of course we'll be talking about
the tips and tricks and the best

8
00:00:22,585 --> 00:00:26,305
practices when dealing with large
scale data processing in pandas What

9
00:00:26,325 --> 00:00:30,495
fireducks is and what it is offering
the optimization strategy available

10
00:00:30,495 --> 00:00:35,175
with it With some live demo and the
resources available on fireducks

11
00:00:36,095 --> 00:00:38,765
so Let's have a quick introduction.

12
00:00:38,765 --> 00:00:39,865
My name is Saurav.

13
00:00:40,255 --> 00:00:43,645
I'm currently doing in the I'm
currently working as a resource

14
00:00:43,645 --> 00:00:45,415
engineer at NSE Corporation.

15
00:00:45,745 --> 00:00:49,395
So I have been associated with
NSE over the last 11 years where

16
00:00:49,395 --> 00:00:52,785
I'm doing different work in
the field of high performance

17
00:00:52,785 --> 00:00:57,114
computing, distributed processing,
AI machine learning and desktop.

18
00:00:57,115 --> 00:01:00,875
So we have a long history of working
with the vector supercomputer.

19
00:01:01,290 --> 00:01:06,919
where we start with S6 series of
supercomputers and, work with various

20
00:01:06,920 --> 00:01:11,070
legacy applications from tsunami
prediction to earthquake simulations

21
00:01:11,570 --> 00:01:13,000
on this vector architecture.

22
00:01:13,070 --> 00:01:20,489
my leader at my team, Stacy Sarkar, first
thought that we including our expertise in

23
00:01:20,490 --> 00:01:24,410
the field of HPC and the compiler and this
kind of distributed processing related

24
00:01:24,650 --> 00:01:28,585
tasks, How about creating something
that will use the compiler technology

25
00:01:28,585 --> 00:01:32,735
because we have a keen interest in
compiler And at the same time do something

26
00:01:32,735 --> 00:01:36,415
with the python for the data science
community So that is the time when you

27
00:01:36,415 --> 00:01:40,865
thought okay data scientist often face
challenges With the pandas computation.

28
00:01:40,875 --> 00:01:46,765
So let's create some library That will
look completely same as pandas But

29
00:01:46,795 --> 00:01:51,175
internally it will have a compiler
and do a lot of magic kind of stuff

30
00:01:51,470 --> 00:01:56,690
Such that a programmer, a pandas
programmer can experience much faster,

31
00:01:57,070 --> 00:01:59,054
data processing when using firetrucks.

32
00:01:59,345 --> 00:02:04,025
So please stay with me till the end of
this presentation where I'll be talking

33
00:02:04,025 --> 00:02:09,235
about various best practices I will give
you walkthrough with the finders the

34
00:02:09,235 --> 00:02:13,655
offering the motivation the Comparison
with other data frame libraries that is

35
00:02:13,655 --> 00:02:18,315
available and of course the end of the end
I'll have some frequently asked questions

36
00:02:18,565 --> 00:02:20,165
because this is something online.

37
00:02:20,225 --> 00:02:24,845
So and Of course if you have any
questions, you can get in touch with

38
00:02:24,865 --> 00:02:28,935
me over the linkedin or other social
media i'll be happy to collaborate

39
00:02:28,995 --> 00:02:30,705
and help you in whatever matter I can.

40
00:02:31,345 --> 00:02:34,555
Yes So this is the flow.

41
00:02:34,605 --> 00:02:39,375
I believe all of you can relate
what we do as a data scientist, we

42
00:02:39,415 --> 00:02:43,775
collect data from various resources
and the data in the raw format is

43
00:02:43,775 --> 00:02:47,425
not resilient digestible by the AI
algorithm, AI machine learning algorithm.

44
00:02:47,425 --> 00:02:51,675
So what we do is we clean the data,
make it more meaningful so that we

45
00:02:51,705 --> 00:02:55,254
can get a better features out of
it and we can do a better model.

46
00:02:55,335 --> 00:02:57,225
So better the data is
the better the model.

47
00:02:57,235 --> 00:02:59,745
That is some kind of old story
that we have been learned about.

48
00:03:00,525 --> 00:03:05,115
But the important thing for this entire
cycle to understand that, although

49
00:03:05,155 --> 00:03:09,145
AI machine learning is taking time
and deploying model is also taking

50
00:03:09,165 --> 00:03:13,095
time, but the most time consuming
part is the creation of the data as

51
00:03:13,095 --> 00:03:14,595
per the research from the anaconda.

52
00:03:14,945 --> 00:03:18,385
It says that the data loading, cleaning,
and the visualization is something

53
00:03:18,385 --> 00:03:21,625
that occupies almost 75 percent
of the effort of a data scientist.

54
00:03:22,025 --> 00:03:25,505
So if you have a lot of questions in
mind, if you want to extract a lot of

55
00:03:25,505 --> 00:03:30,740
many features, You need to spend a lot
of time in during the first a few months.

56
00:03:30,740 --> 00:03:36,030
I would say for doing the efficient
data exploration So that is why

57
00:03:36,120 --> 00:03:40,940
various researches has been happening
in this field Because this is one of

58
00:03:40,940 --> 00:03:45,710
the most important areas where we need
optimization Now with these ladies,

59
00:03:45,720 --> 00:03:47,759
let me have something about pandas.

60
00:03:47,759 --> 00:03:50,669
I believe all of you are from
the data science background So

61
00:03:50,669 --> 00:03:53,169
you must have Work with pandas.

62
00:03:53,399 --> 00:03:57,099
So it is one of the most popular
libraries even today It is having the

63
00:03:57,099 --> 00:04:02,789
second most download after numpy But
it comes with some kind of problems.

64
00:04:02,929 --> 00:04:07,319
The very common one is it's not
multi threaded So if you have a good

65
00:04:07,329 --> 00:04:10,839
system with many core available,
you cannot be able to leverage

66
00:04:10,839 --> 00:04:13,469
that so Your program will be slow.

67
00:04:13,469 --> 00:04:16,739
You have to wait for a longer time
to get your result if you are working

68
00:04:16,739 --> 00:04:21,424
with large data So So longer waiting
times, slow execution, if you're running

69
00:04:21,424 --> 00:04:26,054
this on the cloud side, a higher cloud
cost, and of course, longer execution

70
00:04:26,054 --> 00:04:27,894
will activate a lot of carbon dioxide.

71
00:04:28,144 --> 00:04:31,914
So these is, these are something
that is associated with bundles.

72
00:04:32,214 --> 00:04:35,264
So it is ideally not
ideal for the large scale.

73
00:04:35,514 --> 00:04:37,194
scale exhibition, right?

74
00:04:37,534 --> 00:04:42,684
So now Although these are the challenges
related with pandas But the key challenge

75
00:04:42,744 --> 00:04:48,754
is like there are various way of writing
the same programming pandas So if we

76
00:04:48,784 --> 00:04:53,544
ourselves can identify the best approaches
the best practices and take care of that

77
00:04:53,544 --> 00:04:58,384
when writing our program It's fine enough
because by default pandas doesn't offer

78
00:04:58,384 --> 00:05:03,854
you optimization as it is eager in nature
so whatever you Ask it to do it will do it

79
00:05:03,864 --> 00:05:09,684
for you So if you have written a good code
unoptimized one yet, you are at good stage

80
00:05:10,004 --> 00:05:14,084
But if your program has performance issue
You may need to pay a lot of performance

81
00:05:14,094 --> 00:05:19,084
cost when your data goes in size or
your program goes in complexity So we're

82
00:05:19,084 --> 00:05:22,904
talking about some crucial performance
issue that involves in a pandas program

83
00:05:22,964 --> 00:05:28,284
and how to take care of that And what
are the automation possibility for those

84
00:05:28,524 --> 00:05:30,674
cases using fireducks and other library?

85
00:05:31,174 --> 00:05:36,204
So to going forward let us understand
one by one the best practices and

86
00:05:36,214 --> 00:05:41,124
challenges associated with data
large scale data analysis program So

87
00:05:41,124 --> 00:05:43,334
with that let us have a first quiz.

88
00:05:43,474 --> 00:05:44,974
can you identify?

89
00:05:45,294 --> 00:05:49,124
Which one is a better code whether
the left one or the right one?

90
00:05:50,024 --> 00:05:53,894
yeah since i'm doing this online, so
definitely cannot take the response

91
00:05:53,894 --> 00:05:58,754
from you But when I asked this kind of
question in one of the event last time,

92
00:05:58,754 --> 00:06:04,444
so I believe I had Mostly around 90
percent of them answered the rightmost

93
00:06:04,524 --> 00:06:09,864
one is the best performing query Because
of many reasons so as you can see this

94
00:06:09,894 --> 00:06:14,864
is written in chain and that is ideally
the best way of writing This kind of

95
00:06:15,264 --> 00:06:17,244
query in pandas or pandas like library.

96
00:06:17,464 --> 00:06:22,554
Why Let's have let's understand
that with some kind of Illustration.

97
00:06:22,834 --> 00:06:27,134
So let's say this is the data that I have
loaded from file Of course, you can see

98
00:06:27,134 --> 00:06:30,534
there are some duplicates once I remove
the duplicate This will be the data my

99
00:06:30,534 --> 00:06:34,794
data might be reduced to its half Then
I will shorting the result by b column

100
00:06:34,804 --> 00:06:39,794
selecting, the top two of the result and
that's it Now, let's assume some kind of

101
00:06:39,834 --> 00:06:41,874
memory footprint associated with the data.

102
00:06:41,954 --> 00:06:46,054
Let's say my data is 16 gb You After
the removing duplicate it will be

103
00:06:46,054 --> 00:06:50,924
8gb, 8gb and some other gb, so at
the end of the thing it will be 32gb.

104
00:06:51,599 --> 00:06:56,719
But till the time the foo is alive
all these references like df2 t3 are

105
00:06:56,729 --> 00:07:02,799
alive because pandas not pandas python
cannot Make them ready for the garbage

106
00:07:02,799 --> 00:07:06,229
collection because they have the
reference alive in them alive, right?

107
00:07:06,599 --> 00:07:10,689
So that is the challenge with not
writing zend expression what happens

108
00:07:10,689 --> 00:07:15,519
when you go for the zend expression
It will be like, the data is loaded.

109
00:07:15,619 --> 00:07:20,584
We remove the duplicates But now when
we'll be applying the shorting at that

110
00:07:20,584 --> 00:07:25,954
moment, I only need the input from the
drop duplicate side So I don't need

111
00:07:25,954 --> 00:07:31,834
this data anymore So this data will be
out of reference and python can make it

112
00:07:31,834 --> 00:07:34,204
garbage collected based on It's neat.

113
00:07:34,564 --> 00:07:38,364
So the same way when I will perform
the top two, I don't need the

114
00:07:38,374 --> 00:07:41,864
duplicated result drop duplicates
result I only need the shorter result.

115
00:07:42,154 --> 00:07:47,044
So this way at the end of my program I
can remove the intermediate data that

116
00:07:47,044 --> 00:07:52,764
is there that is generated during my
query of the long expression So and that

117
00:07:52,764 --> 00:07:55,984
is definitely going to help you when
you'll be dealing with large scale data.

118
00:07:55,984 --> 00:07:59,604
I'll be showing this kind of thing
with example in my upcoming slides.

119
00:07:59,854 --> 00:08:04,304
So that is one of the best practices
that write your expression in chained

120
00:08:04,304 --> 00:08:08,624
expression and it is possible in
pandas, of course it's possible.

121
00:08:08,844 --> 00:08:12,874
So as much as possible try to go
for the chained expression such

122
00:08:12,874 --> 00:08:15,754
that you can avoid this kind of
memory and performance issues.

123
00:08:16,334 --> 00:08:18,154
Now let's consider the second one.

124
00:08:18,494 --> 00:08:19,424
So here is a quiz.

125
00:08:19,924 --> 00:08:23,454
Where it is trying to solve the same
problem it is trying to find the

126
00:08:23,494 --> 00:08:28,894
top five of a value based on the
shorting from the b column, right?

127
00:08:29,394 --> 00:08:33,974
Now, can you guess which one is a
better code if you give the if you're

128
00:08:33,974 --> 00:08:37,494
about to write the code whether you'll
go for the top one or you'll go for

129
00:08:37,504 --> 00:08:42,174
the bottom one yeah, so when I asked
this kind of question in a recent

130
00:08:42,174 --> 00:08:47,664
event, so almost I have mixed answer
So 40 to 50 percent answer that the

131
00:08:47,674 --> 00:08:52,804
top one is the good one And 50 to 60
percent answer that the bottom one is

132
00:08:52,834 --> 00:08:57,744
the good one So with the majority the
answer is of course the bottom one.

133
00:08:58,054 --> 00:09:02,804
So if you yourself, thought it is
the bottom one Give a pat on your

134
00:09:02,804 --> 00:09:05,734
back Now, let's understand why it is.

135
00:09:05,734 --> 00:09:11,479
So what happens ideally your data may
have many columns in relative, right?

136
00:09:11,479 --> 00:09:16,644
Not only a and b if you are going to
perform the shorting on the entire

137
00:09:16,644 --> 00:09:21,494
data, what will happen, not only the
target columns A and B, rest of the

138
00:09:21,494 --> 00:09:23,674
columns like C to J will get shorted.

139
00:09:24,119 --> 00:09:27,979
And that will never be used in your final
result because you're only interested

140
00:09:28,299 --> 00:09:31,759
To have the shorted value from the a
column after the result is shorted by

141
00:09:31,789 --> 00:09:37,939
b So the c to g column will get shorted
based on the b columns It will occupy the

142
00:09:37,939 --> 00:09:40,059
memory, but it will no longer be needed.

143
00:09:40,059 --> 00:09:44,659
So it is waste of competition time and
memory So what you can do you can create

144
00:09:44,659 --> 00:09:49,179
a view of the data of my interest that
i'm interested in a and b column only So

145
00:09:49,179 --> 00:09:51,389
why not just create a view of that data?

146
00:09:51,389 --> 00:09:56,479
You Perform the same steps do the
same thing and this will definitely

147
00:09:56,479 --> 00:09:59,679
going to reduce a lot of competition
calls because it doesn't matter How

148
00:09:59,679 --> 00:10:01,249
many column you will data may have?

149
00:10:01,609 --> 00:10:02,839
Because you also understand.

150
00:10:02,849 --> 00:10:07,419
I am interested in a and b so let's
not bother about rest of the column

151
00:10:07,519 --> 00:10:11,449
Let's create a view and work on that
This kind of optimization is something

152
00:10:11,449 --> 00:10:15,319
called projection pushdown and it is very
important that we should take care when

153
00:10:15,319 --> 00:10:19,929
dealing with large scale data analysis
Now, let's consider another example You

154
00:10:20,884 --> 00:10:25,949
So before going to the quiz, let me show
what it is trying to do For example, I am

155
00:10:25,949 --> 00:10:29,849
an, I have an employee table in a country
table and I want to perform some kind

156
00:10:29,849 --> 00:10:35,439
of merging to have a larger joint result
and see who are the employee, who are the

157
00:10:35,439 --> 00:10:40,869
male employees from which country, like
how many, like country wise count of the

158
00:10:40,869 --> 00:10:43,199
male employee is something of my problem.

159
00:10:43,529 --> 00:10:47,809
So how it can be done, one thing that
I can just join these two tables.

160
00:10:48,634 --> 00:10:53,804
Then I can filter all the male category
from the result in data and I can

161
00:10:53,824 --> 00:10:59,124
perform a group by count So by looking
at this illustration, can you identify

162
00:10:59,154 --> 00:11:00,844
what is the performance bottleneck?

163
00:11:01,344 --> 00:11:08,034
Yes The issue is with the expensive
merge operation So although we are

164
00:11:08,064 --> 00:11:12,944
only interested in the male category
employee What we did is we performed the

165
00:11:13,044 --> 00:11:15,564
merging on the entire employee table.

166
00:11:15,684 --> 00:11:17,614
We could instead do what?

167
00:11:17,994 --> 00:11:23,154
We could first filter only the male
category from my employee table and

168
00:11:23,154 --> 00:11:25,254
then we can do the merging operation.

169
00:11:25,594 --> 00:11:32,424
Followed by the group by count It will
do a lot of benefit when you have like

170
00:11:32,434 --> 00:11:37,364
kind of 50 50 percent of the employee
ratio Or you have very less amount of mail

171
00:11:37,364 --> 00:11:42,444
Like employees because it can reduce your
data by to a great extent and the merge

172
00:11:42,474 --> 00:11:47,344
will not take much time in that case So
this is something called predicate push

173
00:11:47,374 --> 00:11:50,529
down In terms of compiler technology.

174
00:11:50,559 --> 00:11:54,639
So again, this is one of the very
important optimization strategy We

175
00:11:54,639 --> 00:11:59,659
should ideally take care of that And
we'll see with some demo that how

176
00:11:59,679 --> 00:12:02,609
this kind of strategy is important
when dealing with large scale

177
00:12:02,609 --> 00:12:04,879
data analysis in upcoming slides.

178
00:12:05,269 --> 00:12:09,769
So now let's put these two in,
some kind of, like rule of thumb.

179
00:12:10,159 --> 00:12:13,979
So again, in order to understand
that the importance of such execution

180
00:12:14,019 --> 00:12:18,449
order, let's take this example where
it is trying to perform shorting

181
00:12:18,459 --> 00:12:21,919
by the E column, do the filtration
based on some conditional B column.

182
00:12:22,619 --> 00:12:24,639
Select the E column and get the top two.

183
00:12:25,409 --> 00:12:29,619
Now, in order to illustrate that,
let's consider this is my data and

184
00:12:29,639 --> 00:12:34,079
if I short the data with this color
code, I will have the shorted result

185
00:12:34,139 --> 00:12:36,209
from the yellow, red, green, and blue.

186
00:12:36,259 --> 00:12:39,909
this is my data after I shorted,
something like yellow, red, green,

187
00:12:39,909 --> 00:12:41,149
and blue will be the result.

188
00:12:41,469 --> 00:12:44,699
Let's consider b equals to 1 for
the darker shade and b equals

189
00:12:44,699 --> 00:12:46,799
to 1, 2 for the lighter shade.

190
00:12:46,814 --> 00:12:51,336
if I filter, I'll only have the lighter
shaded part, then I will select the

191
00:12:51,336 --> 00:12:53,586
only E column and find the top two.

192
00:12:54,086 --> 00:13:00,616
Now, if we carefully notice this
entire data flow, can you identify

193
00:13:00,636 --> 00:13:02,476
what is wrong in this journey?

194
00:13:03,396 --> 00:13:04,816
Of course you can, right?

195
00:13:05,126 --> 00:13:10,636
you can say that although I am
interested in only A, B and E columns.

196
00:13:11,111 --> 00:13:16,651
I have already involved rest of the
columns like c and d that is there

197
00:13:16,651 --> 00:13:20,961
in my data that have been used But
never referenced so that is ideally

198
00:13:20,961 --> 00:13:26,411
a waste of membrane competition time
So instead what we can do is we can

199
00:13:26,431 --> 00:13:28,791
first identify that for this query.

200
00:13:28,831 --> 00:13:33,851
I only did a b and e column so why
not just create a view of a b e column

201
00:13:33,851 --> 00:13:38,461
such that we can reduce my scope of the
data first in the Horizontal direction

202
00:13:38,461 --> 00:13:41,176
by applying pushdown projection Okay.

203
00:13:41,666 --> 00:13:42,586
projection poster.

204
00:13:43,086 --> 00:13:47,656
Then I can see that, okay, this
is my data and, but instead,

205
00:13:47,796 --> 00:13:49,426
why to short this entire data?

206
00:13:49,426 --> 00:13:53,656
Because I'm only interested in
the lighter shaded part, right?

207
00:13:53,846 --> 00:13:58,796
So what I can do, I can, for, before
going to do shorting, I can reduce the

208
00:13:58,796 --> 00:14:00,806
data further by applying the filter.

209
00:14:01,381 --> 00:14:06,141
Once the filtration is done, I can have
my further reduced data and this data

210
00:14:06,171 --> 00:14:10,011
is ready for the shorting because that
is ideally part of the data that I'm

211
00:14:10,011 --> 00:14:11,941
interested to perform the shorting on.

212
00:14:12,281 --> 00:14:15,371
Now apply the shorting, find the
E column and find the top two.

213
00:14:16,141 --> 00:14:21,721
This is the way by applying kind of
projection pushdown You can ideally

214
00:14:21,721 --> 00:14:26,071
optimize the data flow and this is
a very important execution order.

215
00:14:26,461 --> 00:14:29,211
Following that in your large
scale data analysis, you can

216
00:14:29,211 --> 00:14:32,271
definitely make sure a lot of it.

217
00:14:32,561 --> 00:14:34,691
Benefit from the execution
time and the memory.

218
00:14:35,011 --> 00:14:40,311
So we'll understand this with some Demo
with some example real example in upcoming

219
00:14:40,311 --> 00:14:47,081
slides Now with that, let's understand
what fire ducks is and why to explore this

220
00:14:47,081 --> 00:14:52,461
library So the first thing, the fireducks
is a DataFrame library, high performance

221
00:14:52,461 --> 00:14:54,681
compiler accelerated DataFrame library.

222
00:14:55,101 --> 00:14:56,511
why it is called so?

223
00:14:56,601 --> 00:15:00,561
Basically, the ducks is, it's just a name
of the animal because all the DataFrame

224
00:15:00,591 --> 00:15:04,521
libraries, they're like the pandas,
boulders, they are named after an animal.

225
00:15:04,521 --> 00:15:07,101
So we just thought of, first
of all, we thought of DAX.

226
00:15:07,731 --> 00:15:11,511
Some data frame accelerator, but
dx is already a name that time.

227
00:15:11,511 --> 00:15:15,511
So we thought of naming it as a ducks
So it doesn't have any link with the

228
00:15:15,511 --> 00:15:20,881
duck db for information But the fire
has a meaning fire is from the flexible

229
00:15:20,921 --> 00:15:25,181
ir engine What is ir is something
called intermediary representation.

230
00:15:25,191 --> 00:15:31,211
That is the backbone of the fire dogs
jit compilation So what fire does do is?

231
00:15:31,656 --> 00:15:37,086
It's lazy execution not like pandas like
you ask and it do instead you ask and

232
00:15:37,106 --> 00:15:41,236
it creates some instruction for example
you ask it to do shorting instead of

233
00:15:41,246 --> 00:15:45,906
shorting it will just create okay you
ask me to do short on this data on this

234
00:15:45,916 --> 00:15:50,526
column perfect then you ask it to do
please filter based on this condition

235
00:15:50,546 --> 00:15:54,731
it will just create an instruction
Without any calculation or, like

236
00:15:54,731 --> 00:15:58,331
computations, just create an instruction
followed by a projection instruction

237
00:15:58,641 --> 00:16:00,431
and followed by a slicing instruction.

238
00:16:00,751 --> 00:16:02,461
Just some instruction will be generated.

239
00:16:02,631 --> 00:16:05,921
So if you measure the execution time
for this part, you will see that it's

240
00:16:05,921 --> 00:16:11,021
executed blink of the eyes because it does
nothing in, but creating the instruction.

241
00:16:11,451 --> 00:16:15,001
Now you can tell me like when
this instruction will be executed.

242
00:16:15,561 --> 00:16:19,211
these are the instruction that
will be automatically triggered by

243
00:16:19,221 --> 00:16:21,041
some of your action on the result.

244
00:16:21,181 --> 00:16:25,271
For example, when you want the result
to be printed, when you want the

245
00:16:25,271 --> 00:16:30,241
result to be dumped in a file by two
csv2 parquet kind of operation, when

246
00:16:30,241 --> 00:16:34,001
you apply some kind of aggregation on
this result, for example, finding some

247
00:16:34,021 --> 00:16:35,871
maximum minimum value or something.

248
00:16:36,151 --> 00:16:40,501
So that is the time by when these kind
of expression will be automatically

249
00:16:40,531 --> 00:16:42,061
triggered and it will be executed.

250
00:16:42,521 --> 00:16:47,071
With some compilation related optimization
now, let's say like I want to print

251
00:16:47,081 --> 00:16:51,511
the result So that is the time the
compiler will be activated and it

252
00:16:51,511 --> 00:16:55,821
will try to figure out the instruction
associated with the result And what are

253
00:16:55,821 --> 00:16:57,561
the instruction it will say that okay.

254
00:16:57,561 --> 00:17:02,701
I want to find the top two based
on e column So it based on the

255
00:17:02,701 --> 00:17:07,106
filtration b column based on the
shorting on the a column This

256
00:17:07,156 --> 00:17:09,266
is how the compiler understand.

257
00:17:09,266 --> 00:17:15,126
Okay, I have this data, but I'm only
interested in a b and e column So it will

258
00:17:15,206 --> 00:17:20,976
add a new instruction To project only
the target columns of a b and e before

259
00:17:20,976 --> 00:17:24,516
doing the further operation Such that
it can reduce the data in the horizontal

260
00:17:24,516 --> 00:17:27,726
iteration first Then it will figure okay.

261
00:17:27,726 --> 00:17:31,186
You want to do short then you want
to do filter That means you're only

262
00:17:31,186 --> 00:17:35,286
interested to perform shorting on a
selective set of rows it can do some

263
00:17:35,496 --> 00:17:37,216
interchanges of the instruction.

264
00:17:37,536 --> 00:17:41,116
It can do the filter first, then
it can do the short and rest of

265
00:17:41,116 --> 00:17:42,836
the operation will remain as it is.

266
00:17:42,936 --> 00:17:46,536
This way the compiler will
optimize the generated IR.

267
00:17:46,866 --> 00:17:51,506
Once the optimized IR is generated, it can
be translated to another pandas version.

268
00:17:51,506 --> 00:17:54,886
For example, if you consider it's a
pandas program, it can be something like.

269
00:17:55,176 --> 00:17:59,456
Project a b e filter based on this
condition perform the shorting

270
00:17:59,466 --> 00:18:04,286
project the column and take the top
two Now this can be a pandas program.

271
00:18:04,286 --> 00:18:09,236
This can be a c Api call this can be
any other library call of your interest

272
00:18:09,586 --> 00:18:13,586
because this ir is quite flexible It
can be translated to any other data

273
00:18:13,586 --> 00:18:17,876
frame library or any other method call
for your reference I just ported it

274
00:18:17,926 --> 00:18:19,606
to pandas to understand it better.

275
00:18:19,616 --> 00:18:25,056
So that is why write one query You
Use this design and it can execute

276
00:18:25,076 --> 00:18:29,456
anywhere of your target choice So we
have developed this multi core kernel

277
00:18:29,556 --> 00:18:33,946
in c where we use the multi threaded
and other high performance related stuff

278
00:18:33,966 --> 00:18:38,476
like effective utilization of the caches
the vectorization and other related

279
00:18:38,476 --> 00:18:43,811
stuff such that not only, the results,
but also it can do much faster when

280
00:18:43,811 --> 00:18:48,501
just comparing the kernel operations,
like the only filter, only join, only

281
00:18:48,501 --> 00:18:52,131
group by itself will be faster because
of the careful optimization, careful

282
00:18:52,131 --> 00:18:53,471
implementation of the algorithm.

283
00:18:54,031 --> 00:18:57,701
So again, I'll be talking about these
in details in the benchmarking slides.

284
00:18:58,201 --> 00:19:02,761
So now why you should be understanding
or you should be interested in exploring

285
00:19:02,761 --> 00:19:07,471
this library because the first thing
Pandas doesn't offer you optimization, but

286
00:19:07,501 --> 00:19:12,361
firetax is lazy So because of the lazy it
can do just in time optimization for you

287
00:19:12,731 --> 00:19:17,074
If you have a good system with multiple
core available it can distribute it can

288
00:19:17,254 --> 00:19:20,914
parallelize the workload not distribute
parallelize the workload among the

289
00:19:20,924 --> 00:19:26,864
multiple threads So that will result much
faster data analysis If you work in the

290
00:19:26,864 --> 00:19:31,704
cloud you will experience less cloud, cost
and of course, it will run much faster So

291
00:19:31,734 --> 00:19:36,704
it will attribute to less carbon dioxide,
but the most important thing is if pandas,

292
00:19:36,754 --> 00:19:38,534
that's fine You don't need to understand.

293
00:19:38,864 --> 00:19:42,684
You don't need to learn a new library
new data frame library So when the

294
00:19:42,694 --> 00:19:46,484
technology like ai and these kind
of llm things is progressing so far

295
00:19:46,744 --> 00:19:50,384
It's better to learn a new technology
than to learn a new library, right?

296
00:19:50,619 --> 00:19:55,239
So just you know pandas because we
all of us usually start from pandas So

297
00:19:55,249 --> 00:19:58,849
knowing pandas is sufficient, but the
challenges associated with pandas can

298
00:19:58,849 --> 00:20:02,889
be addressed by firedesk automatically
Now the question is does it handle

299
00:20:02,969 --> 00:20:04,479
any kind of pandas application?

300
00:20:04,619 --> 00:20:09,249
Yes, it does handle any kind of pandas
application not only pandas application

301
00:20:09,249 --> 00:20:13,479
but also the library that expect you
to provide the pandas data for example

302
00:20:13,489 --> 00:20:18,319
seaborn matplotlib, scikit learn,
imbalance learn, category encoders all

303
00:20:18,329 --> 00:20:21,989
the library that expect the you to input
the pandas data frame You can provide

304
00:20:21,989 --> 00:20:26,299
a firedash data frame and it can work
seamlessly with those as well So no extra

305
00:20:26,299 --> 00:20:30,479
learning no code modification You can
experience much better performance benefit

306
00:20:30,709 --> 00:20:32,709
when switching from pandas to firetax.

307
00:20:33,019 --> 00:20:36,499
Now, let's understand some
kind of demo with this.

308
00:20:36,799 --> 00:20:41,649
So here in this example, I am using
some kind of Bitcoin data of the

309
00:20:41,659 --> 00:20:45,579
last three to five years and it is
trying to perform some moving average.

310
00:20:45,919 --> 00:20:49,789
Just load the data creating a window and
perform the average, perform the mean

311
00:20:50,089 --> 00:20:52,019
average calculation and plot the result.

312
00:20:52,544 --> 00:20:55,244
So the code is same in both
the left and right side.

313
00:20:55,244 --> 00:20:58,874
The only difference is in the import
So left side is important pandas and

314
00:20:58,874 --> 00:21:00,504
the right side is importing firedex.

315
00:21:00,504 --> 00:21:00,734
pandas.

316
00:21:00,784 --> 00:21:04,454
That is the only change But if you
see in the execution time, you can

317
00:21:04,454 --> 00:21:06,534
see that pandas takes around 4.

318
00:21:06,535 --> 00:21:10,144
06 whereas firedex can complete
it Within 275 milliseconds.

319
00:21:10,144 --> 00:21:15,089
So this simple code No modification
you can experience 15 times build up.

320
00:21:15,419 --> 00:21:18,939
So that is some kind of quick demo
I'll be talking about another demo

321
00:21:18,959 --> 00:21:23,779
another thing in upcoming slides
for sure Okay, so with that let me

322
00:21:23,789 --> 00:21:26,079
explain you the usage of firedug.

323
00:21:26,089 --> 00:21:30,439
So first of all Firedux is currently
available for Linux only, so if

324
00:21:30,439 --> 00:21:34,159
you're from the Windows or Mac
users, so at this moment, probably

325
00:21:34,419 --> 00:21:37,548
you can consider using Google Colab
or the platform that support Linux.

326
00:21:37,548 --> 00:21:40,999
For Windows, definitely you can try
WSL because it can work with WSL.

327
00:21:41,779 --> 00:21:44,939
And the supported Pythons are from 3.

328
00:21:44,940 --> 00:21:45,247
9 to 3.

329
00:21:45,247 --> 00:21:49,589
12. So if you can satisfy these two
conditions, you can install it using pip

330
00:21:49,929 --> 00:21:54,379
and you can use instead of import pandas,
import firedux pandas and that will

331
00:21:54,379 --> 00:21:59,279
be sufficient to execute your existing
program using firedux and optimize it.

332
00:21:59,779 --> 00:22:02,379
Now, how about zero code modification?

333
00:22:02,749 --> 00:22:07,649
if you are using program that is you
importing pandas and it is importing

334
00:22:07,679 --> 00:22:12,109
other modules All those module might
in turn importing pandas as well.

335
00:22:12,369 --> 00:22:15,029
So do you need to replace
all this import by yourself?

336
00:22:15,309 --> 00:22:18,154
No There is monkey passing thing.

337
00:22:18,154 --> 00:22:22,444
So when executing the program using
the python command, just pass hyphen

338
00:22:22,444 --> 00:22:24,204
option followed by firetus pandas.

339
00:22:24,514 --> 00:22:28,114
It can automatically replace all
the pandas with firetus pandas.

340
00:22:28,444 --> 00:22:32,244
And it can do the optimization for
you without any code modification.

341
00:22:32,564 --> 00:22:35,604
For notebook like platform,
the same thing, your rest

342
00:22:35,604 --> 00:22:36,784
of the notebook can be same.

343
00:22:37,024 --> 00:22:40,464
Just on top of import pandas, you
can add this extension module.

344
00:22:40,789 --> 00:22:44,469
And it can automatically replace the
pandas with fireducks pandas and you

345
00:22:44,469 --> 00:22:48,379
can experience the benefit out of it
So now let's understand, what is the

346
00:22:48,399 --> 00:22:52,159
challenge with seamless integration
with pandas like, Of course wire ducks

347
00:22:52,159 --> 00:22:55,029
is something that i'm talking about But
there are other high performance pandas

348
00:22:55,609 --> 00:23:00,209
alternatives as well, right like duckdb,
polars, dusk These are the library that

349
00:23:00,269 --> 00:23:04,149
try to achieve the same kind of problem
like because pandas is slow So they

350
00:23:04,159 --> 00:23:08,514
have their own version of optimization
that can address the slow performance

351
00:23:08,514 --> 00:23:12,829
of pandas But the major challenge comes
with the compatibility with pandas

352
00:23:13,129 --> 00:23:18,209
like, if you want to use those library
you have to learn that library first if

353
00:23:18,229 --> 00:23:22,989
because Either they are not compatible
or they are not fully compatible.

354
00:23:23,269 --> 00:23:26,829
So you need to understand the
library first and sometime it

355
00:23:26,829 --> 00:23:31,344
may happen that pandas because
pandas is full of features, right?

356
00:23:31,634 --> 00:23:33,804
But those library might
not offer that feature.

357
00:23:33,804 --> 00:23:37,164
So you need to convert, the
library to pandas using from

358
00:23:37,264 --> 00:23:38,834
pandas into pandas mechanism.

359
00:23:39,234 --> 00:23:43,474
And, when, comparing the result in the
performance, once your complete program

360
00:23:43,474 --> 00:23:47,044
can be migrated, you can compare the
performances, or taste the results.

361
00:23:47,054 --> 00:23:49,084
So that are the challenges
associated with it.

362
00:23:49,084 --> 00:23:54,064
So whether you want to take that
effort, For that cost optimization.

363
00:23:54,074 --> 00:23:58,394
So there is always a cost performance
thing in your mind so yeah, you have

364
00:23:58,484 --> 00:24:03,874
options like modern does vex, but
they are ideally for the multi Node

365
00:24:03,874 --> 00:24:07,164
environment there is something called
polars that is for the single node

366
00:24:07,164 --> 00:24:11,664
environment that is super fast But it
is not much compatible with pandas.

367
00:24:11,664 --> 00:24:14,394
So if you want to use folders probably
you need to understand the library

368
00:24:14,394 --> 00:24:19,204
in the very first place that is where
fireducks can help you like being it

369
00:24:19,224 --> 00:24:23,384
highly compatible with pandas No code
modification, no node learning associated

370
00:24:23,384 --> 00:24:27,724
with it And you can experience the
single node speed up If you're working

371
00:24:27,724 --> 00:24:31,944
on the data and that can be fit into
memory, you can work with firetrucks.

372
00:24:32,384 --> 00:24:34,304
Of course, it provides optimization.

373
00:24:34,354 --> 00:24:37,784
it may happen that although you
have a bigger data, but you're not

374
00:24:37,784 --> 00:24:39,504
interested in every part of the data.

375
00:24:39,504 --> 00:24:43,144
it can do the optimization for you
and it can just selectively load the

376
00:24:43,154 --> 00:24:47,074
target columns and rows so that it
can do a lot of justice with your,

377
00:24:47,504 --> 00:24:51,229
like limited set of resources, the
memory and the number of cores.

378
00:24:51,729 --> 00:24:55,739
let's see like how seamless
integration is possible with pandas.

379
00:24:55,749 --> 00:24:57,619
For example, this is a demo notebook.

380
00:24:57,619 --> 00:25:00,059
You can consider the actual
notebook from this link.

381
00:25:00,449 --> 00:25:02,659
But let's understand
what it's trying to do.

382
00:25:02,939 --> 00:25:06,969
It is trying to load parquet data
from InnoIC parking violation.

383
00:25:07,304 --> 00:25:11,284
After the data loading is done, it is
trying to, perform a query where it is

384
00:25:11,324 --> 00:25:14,744
trying to see, which parking violation
is the most commonly committed by

385
00:25:14,764 --> 00:25:17,914
vehicles from various United States.

386
00:25:18,124 --> 00:25:21,034
because of, in order to execute
the query, it does some group by

387
00:25:21,034 --> 00:25:22,439
head and shorting kind of stuff.

388
00:25:22,829 --> 00:25:26,369
So the basic code is
entirely written in pandas.

389
00:25:26,379 --> 00:25:30,879
So there is nothing that you can identify
new So everything is written in pandas.

390
00:25:30,899 --> 00:25:34,509
Now simply I executed the same
program using python followed by

391
00:25:34,519 --> 00:25:39,819
the program name So you can see
that the data loading take around 2.

392
00:25:39,819 --> 00:25:40,859
4 and the query one takes around 2.

393
00:25:40,879 --> 00:25:43,199
8 seconds Overall, it takes around 5.

394
00:25:43,199 --> 00:25:48,859
3 seconds But in order to execute the same
program with fire dogs, no code change

395
00:25:49,169 --> 00:25:55,314
just add hyphenium fire dust and pandas
And it can be done much faster like almost

396
00:25:55,334 --> 00:25:59,494
eight times can eight times faster but
the data loading as well as the query

397
00:25:59,494 --> 00:26:03,474
processing can be done much faster when
we're switching to Pandas to firetrucks

398
00:26:03,764 --> 00:26:05,674
just by adding a program option.

399
00:26:05,684 --> 00:26:06,344
Nothing else.

400
00:26:06,724 --> 00:26:11,324
So that is how it is very like Easy
to integrate once you download it.

401
00:26:11,324 --> 00:26:14,544
It can be integrated to any pandas
application and you can experience

402
00:26:14,544 --> 00:26:19,564
this better So definitely you can try
the notebook for Further exploration

403
00:26:20,174 --> 00:26:24,504
So now let's consider the optimizing
features that is available in FireDogs.

404
00:26:24,514 --> 00:26:30,534
So this is the design of FireDogs like
It looks like a pandas program because we

405
00:26:30,594 --> 00:26:36,154
mock all the api that is written All the
user interface in a pandas like interface

406
00:26:36,404 --> 00:26:40,424
But down the line everything is written
in c once the instruction is optimized

407
00:26:40,454 --> 00:26:44,594
by the compiler So what happens is pandas
is like you write the program and it

408
00:26:44,594 --> 00:26:49,814
execute it right after you click But in
case of Firedux, as I explained that it

409
00:26:49,814 --> 00:26:54,454
will create some kind of instructions and
those instructions will be optimized by

410
00:26:54,454 --> 00:26:59,324
the compiler and the optimized instruction
will be converted by a system call and

411
00:26:59,324 --> 00:27:03,094
then it will be executed by the multiple
code that is available in your system.

412
00:27:03,094 --> 00:27:06,214
So all the available code in your
system will be effectively used.

413
00:27:06,764 --> 00:27:09,504
the system cache will be
effectively used, the vectorization

414
00:27:09,504 --> 00:27:10,534
will be effectively used.

415
00:27:10,734 --> 00:27:14,694
So because of this you can experience
much faster speedup when using firetux.

416
00:27:15,114 --> 00:27:17,224
So now what is the offering
from these two layers?

417
00:27:17,234 --> 00:27:20,594
So first of all there is a compiler,
so definitely you can expect compiler

418
00:27:20,594 --> 00:27:24,434
specific optimization like common
sub expression elimination, date

419
00:27:24,434 --> 00:27:25,994
code elimination kind of stuff.

420
00:27:26,364 --> 00:27:30,029
So you can expect domain
specific optimization like get

421
00:27:30,049 --> 00:27:33,199
pushed down a projection post
on that I talked about so far.

422
00:27:33,519 --> 00:27:36,719
You can expect some pandas
specific tuning here as well.

423
00:27:36,719 --> 00:27:38,739
So I'll be talking about
it in upcoming slides.

424
00:27:39,249 --> 00:27:42,639
Now, what about the second
most layer that is the backend?

425
00:27:42,649 --> 00:27:44,759
The backend is multithreaded first of all.

426
00:27:44,759 --> 00:27:48,924
So If you have a very good, system with
multiple threads available, you can

427
00:27:49,214 --> 00:27:51,444
experience much, throughput out of that.

428
00:27:51,854 --> 00:27:55,164
The, it can take good, use of
the memory because it is backed

429
00:27:55,164 --> 00:27:56,644
by Apache Parallel Memory.

430
00:27:56,664 --> 00:28:00,339
The major data structure is backed by
Apache Arrow, so you can have Compact

431
00:28:00,339 --> 00:28:04,839
usage of the memory as well and all
the kernels like join, group by filter,

432
00:28:04,849 --> 00:28:08,899
drop in, everything is written from
scratch by our own patented algorithms.

433
00:28:09,179 --> 00:28:13,909
So this kernel operation itself is much
faster when comparing to pandas or other

434
00:28:13,909 --> 00:28:15,999
high performance pandas alternatives.

435
00:28:16,329 --> 00:28:18,549
We'll be experiencing it
in upcoming slide for sure.

436
00:28:19,049 --> 00:28:22,899
So yes, so let's understand what
is compiler specific optimization.

437
00:28:22,949 --> 00:28:27,149
I took this example from one of the Kaggle
notebook that I recently noticed like you

438
00:28:27,149 --> 00:28:33,309
can see that here df time is a column of
string type So since it is string type

439
00:28:33,649 --> 00:28:37,539
and the pop the person wants to perform
some group by based on the year and

440
00:28:37,539 --> 00:28:39,959
month he wants to find the average sales.

441
00:28:39,959 --> 00:28:44,699
So what he does it he performed the string
column to date time In order to extract

442
00:28:44,749 --> 00:28:49,769
the year field and the month field And do
the stuff but as you can understand the

443
00:28:49,799 --> 00:28:53,809
two data itself is a complex operation
because you try to parse a string to

444
00:28:53,839 --> 00:28:58,439
Convert it to datetime So if you do
the same operation the same data more

445
00:28:58,439 --> 00:29:04,429
than once it is going to cost you when
your data is putting in size So ideally

446
00:29:04,429 --> 00:29:08,704
what is better is You can just compute
it once put it in some placeholder and

447
00:29:08,714 --> 00:29:13,224
use it wherever you need it So this is
something a basic programming know how

448
00:29:13,564 --> 00:29:17,984
But because find x uses a compiler and the
compiler can identify this kind of common

449
00:29:17,984 --> 00:29:20,874
expression So well, I already have do it.

450
00:29:21,104 --> 00:29:25,574
So so because compiler will create
an instruction for that Again, it

451
00:29:25,574 --> 00:29:29,179
will create an instruction for this
And then it will say, I already

452
00:29:29,179 --> 00:29:30,389
have created the instruction.

453
00:29:30,389 --> 00:29:31,659
So let me use that.

454
00:29:32,149 --> 00:29:35,449
And wherever I, wherever it is
referred, instead of creating

455
00:29:35,449 --> 00:29:38,939
a new instruction, this kind of
optimization possible by FireDogs.

456
00:29:38,969 --> 00:29:42,799
So this kind of, CAC, Common
Sub Explanatory Elimination

457
00:29:42,829 --> 00:29:46,129
Optimization Benefit, you can
definitely get from FireDogs.

458
00:29:46,679 --> 00:29:48,849
Another thing is something
called Dead Code Elimination.

459
00:29:48,849 --> 00:29:51,199
So for example, this
is a piece of function.

460
00:29:51,199 --> 00:29:57,064
So where it is trying to merge x. Table
with the y table And after merging it

461
00:29:57,064 --> 00:30:01,034
is trying to do the shorting operation
after shorting It is doing the group

462
00:30:01,044 --> 00:30:05,674
by but the group by using the merged
variable not the shorted variable, right?

463
00:30:06,014 --> 00:30:11,974
So in the entire function This is the line
that is used but have never referenced So

464
00:30:11,974 --> 00:30:16,334
when you use pandas like eager execution
model, it will be executed because the

465
00:30:16,334 --> 00:30:18,564
shorting is you ask it to short, right?

466
00:30:18,874 --> 00:30:20,284
But fired us can identify.

467
00:30:20,354 --> 00:30:23,094
Okay, you asked me to do That is fine.

468
00:30:23,134 --> 00:30:25,964
I create an instruction,
but you never used it.

469
00:30:26,094 --> 00:30:29,364
So I will not execute the instruction
I will eliminate that date code

470
00:30:29,364 --> 00:30:32,154
out of my execution, graph, right?

471
00:30:32,364 --> 00:30:36,164
So this kind of date code elimination
is possible when you switch to firetux,

472
00:30:36,604 --> 00:30:40,684
from pandas if you want to explore more,
so you can definitely take, read the

473
00:30:40,684 --> 00:30:44,864
article that explored about this kind of
compiler specific optimization details.

474
00:30:45,744 --> 00:30:49,844
yes, now let's talk about some,
domain specific optimization that

475
00:30:49,844 --> 00:30:53,354
we discussed so far like projection
pushdown and predicate pushdown.

476
00:30:53,354 --> 00:31:00,941
for that, I have taken one sample query
from TPCS benchmark, the query, Q3 10

477
00:31:00,991 --> 00:31:02,871
unshipped orders with the highest value.

478
00:31:03,371 --> 00:31:05,101
Ideally it was written in SQL.

479
00:31:05,131 --> 00:31:09,701
I first let's convert it to pandas and see
what it does So if you carefully notice

480
00:31:09,701 --> 00:31:14,331
the query It doesn't perform any kind of
best practices that we understand like

481
00:31:14,331 --> 00:31:18,071
kind of reduction the data in the row
direction the column direction It doesn't

482
00:31:18,071 --> 00:31:22,351
do anything as such it load the entire
data from three tables customer orders

483
00:31:22,351 --> 00:31:27,586
and line item After loading the data it
can Just join them to create a bigger

484
00:31:27,636 --> 00:31:32,746
table such that it can perform Rest of
the operation like filtration and group

485
00:31:32,756 --> 00:31:37,536
by rest of the stuff as per the demand of
the query So when we execute this query

486
00:31:37,776 --> 00:31:44,461
using python using pandas, that pandas
took around 203 seconds and the memory

487
00:31:44,461 --> 00:31:50,331
consumption was around 60 gb for the scale
factor 10 but when we execute the same

488
00:31:50,331 --> 00:31:56,441
program using fire dogs just by adding
this Plug in, it can be finished within 4.

489
00:31:56,442 --> 00:31:59,081
24 seconds and the memory
consumption was around 3.

490
00:31:59,081 --> 00:32:04,681
3 gb Yes, because fire although we
have written it without consideration

491
00:32:04,711 --> 00:32:08,941
of the optimization Fighters
compiler will do it for you.

492
00:32:08,941 --> 00:32:13,111
It can figure out All those stuff like the
reduction of the data in the horizontal

493
00:32:13,111 --> 00:32:17,831
direction and the vertical direction
and apply it automatically, because of

494
00:32:17,831 --> 00:32:22,481
which you can explain much, much speed
up, from this kind of program that

495
00:32:22,481 --> 00:32:24,091
doesn't take care of the optimization.

496
00:32:24,471 --> 00:32:27,756
In order to understand that, let's
manually optimize this program.

497
00:32:28,046 --> 00:32:31,096
So what is the best practices?

498
00:32:31,126 --> 00:32:35,906
First of all, the practice is to instead
of operating on the entire data, reduce

499
00:32:35,906 --> 00:32:38,016
the data in the columnar direction, right?

500
00:32:38,016 --> 00:32:39,346
In the vertical direction.

501
00:32:39,666 --> 00:32:44,346
So by carefully observing the query,
you can see that we only need these

502
00:32:44,346 --> 00:32:45,936
two columns from the customer table.

503
00:32:45,956 --> 00:32:48,556
Although there are eight
columns, we only need these

504
00:32:48,556 --> 00:32:50,016
four columns from the line item.

505
00:32:50,016 --> 00:32:52,226
Although there are 16 columns,
we only need these four

506
00:32:52,226 --> 00:32:53,706
columns from the order table.

507
00:32:53,996 --> 00:32:55,426
Although there are nine columns.

508
00:32:55,736 --> 00:32:59,886
So instead of loading the entire data, I
will only load these selective columns.

509
00:33:00,586 --> 00:33:04,766
Then I only need some particular
rows from these tables because

510
00:33:04,846 --> 00:33:05,986
of the filter operation.

511
00:33:06,256 --> 00:33:10,276
So I will filter all the target
table based on the given condition.

512
00:33:10,556 --> 00:33:14,346
Then I will just join the tables
to the group by an aggregate

513
00:33:14,346 --> 00:33:15,456
and the shorting operation.

514
00:33:15,896 --> 00:33:21,230
Now when I execute the same optimized
query in pandas, it becomes 13 seconds.

515
00:33:21,230 --> 00:33:25,186
So 203 seconds can be optimized
to 13 seconds with this manual

516
00:33:25,186 --> 00:33:28,561
optimization and the memory can
also be reduced from 60 GB to 5.

517
00:33:28,561 --> 00:33:29,966
5 GB.

518
00:33:30,381 --> 00:33:36,601
Even when you use pandas effectively But
when you see the fire ducks, even though

519
00:33:36,601 --> 00:33:42,861
you try q3 and opt q3 Manual optimization
is present or it is absent doesn't matter

520
00:33:42,901 --> 00:33:47,401
It can execute with the same speed and
with the same memory command Because if

521
00:33:47,401 --> 00:33:51,511
you do a panel optimization fair enough
when you cannot do it firedust can do

522
00:33:51,511 --> 00:33:56,891
it for you So you can rely such optimize
rely for such optimization on fire ducks

523
00:33:57,211 --> 00:34:01,581
definitely Now let's understand one of
the parameter tuning that is possible

524
00:34:01,581 --> 00:34:03,821
in FireDogs, not FireDogs actually.

525
00:34:04,011 --> 00:34:07,011
First of all, what is the parameter
tuning that you can do in Pandas, then

526
00:34:07,041 --> 00:34:08,511
how FireDogs can take care of that.

527
00:34:08,911 --> 00:34:13,631
So in order to understand that, let's
consider that this query where it is

528
00:34:13,641 --> 00:34:18,631
trying to perform, Group by on the
department column of the employee table

529
00:34:18,661 --> 00:34:23,681
followed by that it is calculating the
average salary and shorting the Salary

530
00:34:23,681 --> 00:34:25,541
based on the descending order, right?

531
00:34:25,901 --> 00:34:30,581
So what will happen it will create the
group So it group admin group finance

532
00:34:30,581 --> 00:34:34,891
group corporate and sales group and it
will perform the average of each group So

533
00:34:34,891 --> 00:34:40,161
that is ideally the group by operations
But there is a hidden cost associated

534
00:34:40,161 --> 00:34:46,241
with it You And the cost is the group
by is by default has a parameter called

535
00:34:46,241 --> 00:34:53,621
short and the value is true But the
meaning of the short parameter is It is

536
00:34:53,631 --> 00:34:58,161
trying to short the result of the group
by based on the key column here The key

537
00:34:58,161 --> 00:35:02,411
is the department so it will not only
compute the group by aggregation, but

538
00:35:02,411 --> 00:35:07,311
also short the result by the key column
So admin will come first followed by

539
00:35:07,311 --> 00:35:13,711
corporate finance id and sales shorted
by the department but In your query,

540
00:35:13,721 --> 00:35:15,681
you don't need this shorting to happen.

541
00:35:15,701 --> 00:35:19,271
Instead, you want the result to
be shorted by the salary with the

542
00:35:19,271 --> 00:35:21,051
value column in descending order.

543
00:35:21,451 --> 00:35:24,951
So again, because you ask the
pandas to do so it will do it.

544
00:35:25,191 --> 00:35:28,861
It will short the result by the
descending order when it will

545
00:35:28,861 --> 00:35:30,541
encounter the short values.

546
00:35:31,031 --> 00:35:36,341
what you can expect that this is the
step that is taking place because of

547
00:35:36,341 --> 00:35:42,571
this default parameter of group by but
this is the redundant step it can be

548
00:35:42,571 --> 00:35:46,281
avoided, Even if we don't perform this
step, it doesn't impact my result.

549
00:35:46,651 --> 00:35:47,961
So this is a redundant step.

550
00:35:47,961 --> 00:35:49,051
How we can avoid it?

551
00:35:49,101 --> 00:35:53,131
We can simply impute the short equals
to false parameter for this case.

552
00:35:53,431 --> 00:35:56,721
And if we apply this, Pandas
will skip that step and we'll

553
00:35:56,751 --> 00:35:57,971
get much better performance.

554
00:35:58,726 --> 00:36:03,446
Now, this data have very less cardinality,
only 5 groups, but when you have a

555
00:36:03,456 --> 00:36:07,096
very high cardinality in your data,
many groups are present, you can

556
00:36:07,106 --> 00:36:11,316
expect that this will give you much
more performance in terms of speedup.

557
00:36:11,506 --> 00:36:17,036
Like when I try with 100 million samples
with high cardinality, I expect that,

558
00:36:17,396 --> 00:36:20,746
without short equals to false, it
was taking 50 seconds and with short

559
00:36:20,746 --> 00:36:22,276
equals to false, it took 30 seconds.

560
00:36:22,356 --> 00:36:28,096
the short itself has, A lot of like
almost like more than 50 percent

561
00:36:28,096 --> 00:36:30,996
contribution for this query So
this is something called parameter

562
00:36:31,366 --> 00:36:37,286
tuning and because of the compiler
in firedux it can detect that After

563
00:36:37,336 --> 00:36:38,996
groupby you want to short the result.

564
00:36:39,536 --> 00:36:44,926
So the default shorting in the groupby is
not needed And firedux can automatically

565
00:36:44,936 --> 00:36:50,621
impute such parameter by making it false
such that you can avoid this kind of

566
00:36:50,681 --> 00:36:54,491
performance cost which is hidden in pandas
So that is again one of the offering when

567
00:36:54,491 --> 00:37:00,131
you will be using fire tax Now with that,
let me talk about some benchmarks So

568
00:37:00,161 --> 00:37:04,351
this is a very popular benchmark called
deb benchmark where they have written

569
00:37:04,461 --> 00:37:10,276
several around 10 queries of different
complex City of the data with different

570
00:37:10,316 --> 00:37:14,986
cardinality and size for the join and
group operations So when you see the

571
00:37:15,026 --> 00:37:18,756
comparison table, of course pandas is
slow So that is why it is on from the very

572
00:37:18,756 --> 00:37:23,446
bottom But other hyperlink alternative of
pandas like colors dark divi and all you

573
00:37:23,446 --> 00:37:27,936
can see the products is something That
can outperform all the other library as

574
00:37:27,936 --> 00:37:32,841
well and it is something around the top So
now let's, if we talk about some general

575
00:37:32,841 --> 00:37:37,671
overview of this popular library, so I
can have this kind of image at my mind.

576
00:37:37,671 --> 00:37:41,391
So in terms of pandas compatibility and
single node performance, if we compare

577
00:37:41,391 --> 00:37:48,421
this library, so we can see duckdb is a
library that duckdb and polars Are the

578
00:37:48,421 --> 00:37:53,471
library that can work better on single
node performance Whereas park, dusk,

579
00:37:53,471 --> 00:37:57,351
modin are the library that works better
on the multi node computation environment

580
00:37:57,391 --> 00:38:01,401
because they're target for the multi
node In terms of pandas compatibility

581
00:38:01,801 --> 00:38:06,681
duckdb has very less compatible because
it is for the sql api where polars is

582
00:38:06,681 --> 00:38:11,931
not that compatible Although it somewhat
look alike interface with pandas But

583
00:38:11,931 --> 00:38:16,101
firedogs and modding, these are the
library that are fully compatible

584
00:38:16,101 --> 00:38:17,801
with, like pandas kind of thing.

585
00:38:17,811 --> 00:38:20,341
And of course, Rapids has its own QDF.

586
00:38:21,171 --> 00:38:24,421
That is for GPU, although that
is another, different story.

587
00:38:24,421 --> 00:38:29,911
But Rapids is again compatible with
pandas, providing good performance.

588
00:38:29,931 --> 00:38:33,621
But since it is for the GPU platform, so
it is something different than all these.

589
00:38:33,906 --> 00:38:35,776
at this moment.

590
00:38:36,066 --> 00:38:39,236
All the Polars has its
own GPU version available.

591
00:38:39,236 --> 00:38:42,136
And so for the FireDUX that
we are currently working on.

592
00:38:42,636 --> 00:38:46,566
Now, if we talk about the TPC H that
is one of the popular benchmark.

593
00:38:46,576 --> 00:38:53,146
In this demo, I have used query three
only, but actually there is 22 22

594
00:38:53,146 --> 00:38:56,606
queries for against Pandas and other
high performance alternative like

595
00:38:56,606 --> 00:39:02,616
Polars and Modin, we can see that
Modin was working not that good, like

596
00:39:02,616 --> 00:39:04,226
somewhat similar to Pandas, but older.

597
00:39:04,346 --> 00:39:05,696
Was quite faster.

598
00:39:05,696 --> 00:39:09,266
It was around 57 times
faster when comparing, when

599
00:39:09,266 --> 00:39:11,246
performing the processing stuff.

600
00:39:11,516 --> 00:39:17,206
But fire desks can outperform the
average around 1 25 times, so it can

601
00:39:17,206 --> 00:39:21,646
work much faster without even modifying
a piece of existing partner code.

602
00:39:22,146 --> 00:39:26,916
and if you compare the scalability of the
library in terms of TPCs framework also.

603
00:39:27,186 --> 00:39:30,846
We can see that, the polars, duck,
db, fire, ducks are the library that

604
00:39:30,886 --> 00:39:35,386
offer multithreading, where pandas
works same, even though we increase the

605
00:39:35,386 --> 00:39:39,576
number of cores, there is no visible
change in terms of performance in

606
00:39:39,576 --> 00:39:44,506
pandas, even if we include the I O or
exclude the I O. But, in case of duck

607
00:39:44,506 --> 00:39:51,526
db and fireducks, that shows somewhat
good, scalability when we increase the

608
00:39:51,816 --> 00:39:54,056
number of threads, during the execution.

609
00:39:54,396 --> 00:39:58,386
Polars is again, doing scalability, but
to some, after some extent, the polars,

610
00:39:59,036 --> 00:40:03,536
doesn't show that much of scalability
when, at least for the TPC benchmark.

611
00:40:03,826 --> 00:40:06,256
till 8 cores, it was
fine, but after 8 cores.

612
00:40:06,511 --> 00:40:12,561
The scalability was something that was
not quite good when we compared TPC H.

613
00:40:13,061 --> 00:40:15,171
so here are some resources on FireDUX.

614
00:40:15,181 --> 00:40:19,291
If you want to know more about it, explore
the articles written in, written for

615
00:40:19,291 --> 00:40:23,581
FireDUX and other libraries, we can check
out this, Website that is there we can

616
00:40:23,581 --> 00:40:28,361
follow on us on twitter where we publish
the information related to articles

617
00:40:28,361 --> 00:40:32,091
the new development the new release
etc if you find any issue and want to

618
00:40:32,151 --> 00:40:37,421
report back to us you can refer to the
github page you can definitely connect

619
00:40:37,421 --> 00:40:41,341
us over the slack channel we'd be happy
to welcome you there and collaborate

620
00:40:41,381 --> 00:40:46,971
directly with you if you have any
questions so with that let me conclude my

621
00:40:47,041 --> 00:40:52,616
talk thank you once again for listening
to me And I hope you enjoyed the talk.

622
00:40:52,666 --> 00:40:56,846
the key insight that I talked about in
best practices and how fire ducks can help

623
00:40:56,846 --> 00:41:00,316
you to improve your journey with pandas.

624
00:41:00,536 --> 00:41:04,696
So enjoy green computing
and explore fire ducks.

625
00:41:04,996 --> 00:41:08,296
If you have any questions, I understand
that this is something online.

626
00:41:08,296 --> 00:41:12,441
So probably I cannot make it interactive
this time, but if you have any questions,

627
00:41:12,441 --> 00:41:16,781
you can connect me over the LinkedIn or
any other prefer social media network,

628
00:41:16,781 --> 00:41:19,911
or you can definitely get in touch
with the Slack channel and other media.

629
00:41:20,251 --> 00:41:25,121
I'd be happy to help you in optimizing
whatever problems you may have with

630
00:41:25,181 --> 00:41:30,111
pandas and discuss some kind of
collaboration if you are interested in it.

631
00:41:30,691 --> 00:41:31,101
Yes.

632
00:41:31,141 --> 00:41:33,411
With that, I'm concluding my talk.

633
00:41:33,581 --> 00:41:36,681
Thank you very much once again
for listening to me till this.

634
00:41:37,551 --> 00:41:40,991
till this end, I hope you enjoy
rest of the presentation as well,

635
00:41:41,461 --> 00:41:43,371
happy collaboration, happy learning.

636
00:41:43,511 --> 00:41:44,221
Thank you so much.

