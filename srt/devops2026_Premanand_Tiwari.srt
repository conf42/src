1
00:00:00,500 --> 00:00:04,820
Speaker 31: Hi everyone, this is
Prima an from Amazon and I'm an

2
00:00:04,820 --> 00:00:09,869
implied scientist at Amazon who works
in developing forecasting models

3
00:00:09,959 --> 00:00:15,099
for capacity planning, inventory
planning and some other few things.

4
00:00:15,849 --> 00:00:20,799
Today we are going to talk about
agent cost and autonomous AI system

5
00:00:20,979 --> 00:00:26,710
that I have built that enables our
planners, especially who works in

6
00:00:26,710 --> 00:00:34,120
capacity planning and analysts to in
to use autonomous AI system to create

7
00:00:34,120 --> 00:00:36,250
forecasting models and deploy them.

8
00:00:36,750 --> 00:00:39,390
And we'll talk about forecasting at scale.

9
00:00:39,820 --> 00:00:43,900
Specifically for DevOps the
DevOps forecasting challenge

10
00:00:43,900 --> 00:00:48,280
there is in traditional op there
are some traditional obstacles.

11
00:00:48,339 --> 00:00:52,239
Capacity planning requires usually
multiple data science expertise.

12
00:00:52,970 --> 00:00:58,339
Data scientists, planners, operational
engineers and the, these kind of

13
00:00:58,339 --> 00:00:59,989
people are usually very expensive.

14
00:00:59,989 --> 00:01:06,459
How to get the get and get hired and to
maintain their cost related to the models.

15
00:01:07,389 --> 00:01:09,129
And then there is mo manual model.

16
00:01:09,690 --> 00:01:13,980
Management consumes valuable time
usually, for example, if you have to.

17
00:01:14,505 --> 00:01:18,735
Retune since the accuracy is going bad,
then you have to go back in, do analysis.

18
00:01:18,735 --> 00:01:21,795
If there's different types of data
that has been added, then you have

19
00:01:21,795 --> 00:01:23,925
to go back in and do manual analysis.

20
00:01:24,615 --> 00:01:27,125
So these are some of the
challenges that we fail.

21
00:01:27,665 --> 00:01:32,745
We see this static approach fail, that
evolving for evolving infrastructures.

22
00:01:32,745 --> 00:01:37,615
For example you are adding more
cube clusters or more containers,

23
00:01:37,885 --> 00:01:39,895
or more resistors, container resist.

24
00:01:40,750 --> 00:01:43,300
Then you have to make sure
that everything is in the line.

25
00:01:43,800 --> 00:01:49,649
Performance monitoring, lack predictive
capabilities, meaning when you are

26
00:01:49,800 --> 00:01:54,929
monitoring the performance of your
stack, you do not know what is

27
00:01:54,929 --> 00:01:59,929
going to happen day from today, day
from day after tomorrow, and so on.

28
00:02:00,229 --> 00:02:04,639
So you have to make a system ready
to take care of these things.

29
00:02:05,139 --> 00:02:07,809
And there is like real operational impact.

30
00:02:07,989 --> 00:02:11,469
DevOps team faces constant
pressure to predict resources.

31
00:02:11,469 --> 00:02:14,939
For example, how much compute they
would need to run applications,

32
00:02:14,969 --> 00:02:19,579
especially if there are multiple
spikes they are anticipating.

33
00:02:19,579 --> 00:02:22,329
For example, if there's you are
working in a company where you are

34
00:02:22,329 --> 00:02:26,769
selling something in a holiday season,
more so your traffic will be higher.

35
00:02:27,324 --> 00:02:28,794
So preventing that.

36
00:02:29,604 --> 00:02:34,604
And then making sure that if you
predict your capacities in real time,

37
00:02:34,604 --> 00:02:36,254
you are able to scale up very quickly.

38
00:02:37,034 --> 00:02:41,384
And and basically cost optimization
related to infrastructure as well.

39
00:02:41,744 --> 00:02:46,744
You can maintain and you can maintain
very complex ML systems, like how

40
00:02:46,744 --> 00:02:50,464
much you compute need, how much
you need compute for inference.

41
00:02:50,914 --> 00:02:52,654
Where you wanna save your data at.

42
00:02:53,154 --> 00:02:57,394
So we are proposing a solution an
autonomous AI forecasting system

43
00:02:57,394 --> 00:03:01,364
that you can plug into the database
where you have all your data and

44
00:03:01,364 --> 00:03:05,654
it can learn different patterns and
do time series forecasting for you.

45
00:03:06,154 --> 00:03:11,584
Agent cast, it's a fully autonomous AI
agent, agentic based approach where you

46
00:03:11,584 --> 00:03:13,804
don't really need any human intervention.

47
00:03:14,314 --> 00:03:20,325
You and just plug in the agent with the
data that you're getting in real time.

48
00:03:20,385 --> 00:03:23,564
It could be streaming data, it could be
batch data, and it will learn from it.

49
00:03:23,984 --> 00:03:28,304
And it is generate forecasting
based on the patterns that it sees.

50
00:03:28,804 --> 00:03:33,844
It's DevOps native, so technically
you can just plug in and it'll do

51
00:03:34,504 --> 00:03:37,054
all like predictive planning for you.

52
00:03:37,669 --> 00:03:44,939
And it's continuously adaptive means as
soon as you see system is getting more

53
00:03:44,939 --> 00:03:50,219
intermittent data or more sparse data,
it'll adapt to it and you can scale it

54
00:03:50,219 --> 00:03:52,529
to adapt to different type of capacities.

55
00:03:52,529 --> 00:03:57,389
For example, if you are writing more data
to some drive, it can predict disc usage

56
00:03:57,389 --> 00:04:00,269
and memory usage and apps and whatnot.

57
00:04:00,769 --> 00:04:01,169
So it's com.

58
00:04:01,669 --> 00:04:02,689
Continuously adaptive.

59
00:04:03,439 --> 00:04:05,389
It's a comprehensive model toolkit.

60
00:04:05,419 --> 00:04:09,569
It has bunch of machine learning
models, deep neural network models.

61
00:04:09,929 --> 00:04:13,379
For example, we use temporal
fusion transformers.

62
00:04:13,379 --> 00:04:14,539
We use nbs.

63
00:04:14,539 --> 00:04:19,229
We use LSTM and GR use and
transformer based models.

64
00:04:19,559 --> 00:04:23,279
So it has whole suite of models
because we don't know what kind

65
00:04:23,279 --> 00:04:24,114
of data we are gonna get in.

66
00:04:24,764 --> 00:04:27,854
So agent decides what is the
best approach to take based on

67
00:04:27,854 --> 00:04:28,934
the data that you are getting.

68
00:04:29,874 --> 00:04:34,044
There are machine learning
approaches such as XG Boost and

69
00:04:34,144 --> 00:04:37,394
for example if you have more d.

70
00:04:38,349 --> 00:04:40,449
You have more independent variables.

71
00:04:40,519 --> 00:04:43,339
Xeg boost kind of model works
better where you wanna do a very

72
00:04:43,339 --> 00:04:48,559
simple forecast, Rema or automa, A
simple hold winters can work better.

73
00:04:48,559 --> 00:04:51,829
So it has statistical model machine
learning models and deep learning

74
00:04:51,829 --> 00:04:57,019
models, and it picks up based on the
data and performance on the data to

75
00:04:57,019 --> 00:04:59,239
see what is the best model to use.

76
00:04:59,739 --> 00:05:04,539
Intelligent adaptation in action means
it has context aware intelligence.

77
00:05:05,424 --> 00:05:10,404
Agent doesn't really rely on pre-trained
models because it looks at your data

78
00:05:10,434 --> 00:05:14,004
runs model to do training, hyper
parameter tuning and everything else

79
00:05:14,124 --> 00:05:16,734
finds the best loss metric for you.

80
00:05:16,734 --> 00:05:21,234
For example, are you looking for
me, RSME, or other kind of metric?

81
00:05:21,384 --> 00:05:24,964
If you have your custom metric that
you wanna look at then it looks

82
00:05:24,964 --> 00:05:30,934
at that, it discovers patterns
in your application usage or it.

83
00:05:31,799 --> 00:05:36,719
It discover patterns in spikes and
it figures out what's noisy, what's

84
00:05:36,719 --> 00:05:41,979
actual trend, and based on that, it
does forecasting and it does it with

85
00:05:42,039 --> 00:05:44,329
pretty pretty good precision as well.

86
00:05:44,829 --> 00:05:47,829
It can do multiple
forecast horizons as well.

87
00:05:47,829 --> 00:05:51,489
For example, if you're planning one
month out or six months out, you

88
00:05:51,489 --> 00:05:54,649
just have to change the forecast
horizon for the agent and it will.

89
00:05:55,329 --> 00:05:56,649
It'll take care of it.

90
00:05:56,709 --> 00:06:00,269
Figuring out what is the best look
back period for this kind of forecast.

91
00:06:01,139 --> 00:06:03,149
And it also does probabilistic forecast.

92
00:06:03,149 --> 00:06:06,449
So instead of doing 0.5 forecast
where you're just getting one

93
00:06:06,449 --> 00:06:12,689
point at a time in time, it does
your P 10, P 50, P 70, and P 90.

94
00:06:12,689 --> 00:06:16,689
So it just gets you a
probabilistic distribution rather

95
00:06:16,689 --> 00:06:18,669
than just a point forecast.

96
00:06:19,169 --> 00:06:24,039
The reason it is also like DevOps
centric is because when you're doing

97
00:06:24,039 --> 00:06:29,509
probabilistic approach, you can also see
how much variance you are expecting in

98
00:06:29,509 --> 00:06:36,569
your demand or what other things you need
in the cycle to adjust your forecast.

99
00:06:36,569 --> 00:06:39,759
So it gives you different
distribution metrics.

100
00:06:40,209 --> 00:06:42,369
Different level of deciles actually.

101
00:06:43,209 --> 00:06:46,179
So it eliminates also like
manual hyper parameter.

102
00:06:46,179 --> 00:06:48,969
Like you don't have to provide
a grid search manually and

103
00:06:48,969 --> 00:06:50,229
run through different models.

104
00:06:50,229 --> 00:06:53,619
It based on the data decides
a grid search for you.

105
00:06:54,099 --> 00:06:59,559
And it has probabilistic methods
within the hyper parameter tuning

106
00:06:59,649 --> 00:07:05,569
that finds the best hyper parameter
tuning based on each iteration.

107
00:07:06,069 --> 00:07:09,824
And it is very seamless to integrate
into your continuous integration

108
00:07:09,824 --> 00:07:12,064
and continuous deployment pipelines.

109
00:07:12,064 --> 00:07:18,914
You just have to upload the code of agent
and plug in to the current workflow.

110
00:07:18,914 --> 00:07:23,505
And it just creates the bells and
everything else to run the models.

111
00:07:24,005 --> 00:07:26,905
It is find seasonal variances on its own.

112
00:07:27,055 --> 00:07:28,735
What's trend, what's noise?

113
00:07:28,785 --> 00:07:33,525
What's long range dependencies
and provides forecast based

114
00:07:33,525 --> 00:07:36,365
on your infrastructure needs.

115
00:07:36,865 --> 00:07:39,780
It has enterprise grade
reliability because.

116
00:07:40,610 --> 00:07:43,190
You really don't need
any manual intervention.

117
00:07:43,190 --> 00:07:48,440
It has its autonomous operational
capabilities, so it's running on its own.

118
00:07:48,440 --> 00:07:53,200
It's performing daily or
weekly iterations of training.

119
00:07:53,230 --> 00:07:58,360
It checks the wave in real time or
me, or RMS in real time, like how the

120
00:07:58,360 --> 00:08:03,880
forecasted and based on that, if it is
deviation from what is the acceptable

121
00:08:03,880 --> 00:08:07,050
norm of forecast, it like creates creates.

122
00:08:07,760 --> 00:08:12,980
New training system where it retrains
hyper parameter optimization in,

123
00:08:12,980 --> 00:08:17,520
then it like creates the final model
that is then used for inference.

124
00:08:18,020 --> 00:08:20,090
So that's the autonomous operation.

125
00:08:20,490 --> 00:08:26,310
As I said, it constantly looks for what's
the need of cap capacities, how much

126
00:08:26,310 --> 00:08:28,020
hardware you need, how much info you need.

127
00:08:28,545 --> 00:08:32,265
It does that continuous monitoring
and also keeps running different

128
00:08:32,265 --> 00:08:36,675
kind of error metrics to figure out
that the forecast is not deviating,

129
00:08:37,485 --> 00:08:39,355
it's a production ready agent.

130
00:08:39,355 --> 00:08:42,925
You just have to plug into your
current workflow and it works fine.

131
00:08:43,425 --> 00:08:47,755
Versatile forecasting horizon it
can do high frequency monitoring

132
00:08:48,065 --> 00:08:51,635
real time infrastructure metric
with subnet granularity for.

133
00:08:52,280 --> 00:08:54,140
Immediate operational insights.

134
00:08:54,140 --> 00:08:57,080
For example, if you are running
a cube cluster where you're

135
00:08:57,080 --> 00:09:01,570
running multiple parts, you are
up scaling to multiple parts.

136
00:09:01,570 --> 00:09:05,900
It can identify that spike
in the time series trend.

137
00:09:05,900 --> 00:09:11,760
And based on that, it can forecast
and tell how much whether, how long

138
00:09:11,760 --> 00:09:14,610
and how much you need your info to be.

139
00:09:14,970 --> 00:09:16,210
To be to be up.

140
00:09:16,270 --> 00:09:21,250
And one of the reason, most of these
services on cloud have auto-scaling.

141
00:09:21,700 --> 00:09:25,350
Versus what is the need of this is this
kind of forecasting system is like, auto

142
00:09:25,350 --> 00:09:27,390
scaling is typically very expensive.

143
00:09:27,390 --> 00:09:31,290
You can enable auto scaling and
that might work well for you, but

144
00:09:31,290 --> 00:09:33,900
at the end, when you get the final
bill, it's gonna be very expensive.

145
00:09:33,900 --> 00:09:38,100
So usage of this kind of
system where you are uploading.

146
00:09:38,440 --> 00:09:44,140
Forecast from this agent into your
yam l configurations to get the

147
00:09:44,220 --> 00:09:49,350
infrastructure scaling up or down
Is price wise beneficial and it

148
00:09:49,560 --> 00:09:51,540
reduce cost related to your infra?

149
00:09:52,040 --> 00:09:54,550
It also just like longer
medium term planning.

150
00:09:54,550 --> 00:09:59,770
So like it can do how many, cPUs
or GPUs you need based on the

151
00:09:59,770 --> 00:10:01,210
kind of task you're performing.

152
00:10:01,300 --> 00:10:05,550
Whether it is a app based system you
are running on, or you are running

153
00:10:05,550 --> 00:10:11,160
a ML based system, it can do both
long range capacity, like it can do

154
00:10:11,190 --> 00:10:15,240
monthly to quarterly, up to three
year forecast easily based on the

155
00:10:15,240 --> 00:10:17,550
pattern that it has seen historically.

156
00:10:18,050 --> 00:10:20,570
There are multiple
components to agent cast.

157
00:10:20,640 --> 00:10:22,380
First is automatic discovery.

158
00:10:22,875 --> 00:10:27,015
Based on the time series data that
it reads from the data sources

159
00:10:27,505 --> 00:10:31,485
it's does what is the best kind
of model for the time series data.

160
00:10:31,485 --> 00:10:36,795
If you have a problem where you have
multiple independent variables and

161
00:10:37,005 --> 00:10:42,075
medium or short term horizon forecast
ML based system like XGBoost or linear

162
00:10:42,075 --> 00:10:44,295
model might work best where you have.

163
00:10:44,655 --> 00:10:48,105
A really long range forecasting
need a temporal fusion

164
00:10:48,105 --> 00:10:50,625
transformer or a DAR based model.

165
00:10:50,625 --> 00:10:55,365
That might work because it can easily
detect dependencies in the long range.

166
00:10:55,465 --> 00:11:00,565
Horizon long range time series
model evaluation is based on

167
00:11:00,595 --> 00:11:02,095
you running multiple models.

168
00:11:02,095 --> 00:11:06,055
You are checking mean absolute
error, may mean absolute percentage

169
00:11:06,115 --> 00:11:07,945
error, rms, a bunch of other metrics.

170
00:11:07,945 --> 00:11:12,925
You can also define custom metrics for
evaluation, and it finds the best model.

171
00:11:13,345 --> 00:11:17,155
For it and a lot of time it can
also do ensembling of models.

172
00:11:17,155 --> 00:11:21,385
Like maybe just not one model works
best for you, but combination of

173
00:11:21,385 --> 00:11:23,005
two or three model works for you.

174
00:11:23,215 --> 00:11:27,665
It can do ensembling as well and give you
the best results ensuring there is no over

175
00:11:27,665 --> 00:11:29,665
referring to your time series forecasting.

176
00:11:30,165 --> 00:11:34,335
It has intelligent selection
based on the analysis performed.

177
00:11:34,365 --> 00:11:35,025
It can.

178
00:11:35,525 --> 00:11:40,355
Find the best performance metric,
whether RMSE is the best way or

179
00:11:40,355 --> 00:11:43,065
some other like MA is the best.

180
00:11:43,175 --> 00:11:45,725
If you want to capture more
variance between your time series

181
00:11:45,725 --> 00:11:50,445
forecast, you'll use RMSE, which
captures roots squared route.

182
00:11:50,475 --> 00:11:55,695
If you want to do like a point wise
forecast, MA is more absolute, or ma

183
00:11:55,725 --> 00:12:00,805
is more absolute, it can figure it
out that autonomous deployment is, it

184
00:12:00,805 --> 00:12:04,675
deploys these model into production
environment, whatever it finds, whether

185
00:12:04,675 --> 00:12:06,745
it is single model or ensemble model.

186
00:12:07,015 --> 00:12:11,005
And it also performs continuous
evaluation of the model.

187
00:12:11,035 --> 00:12:13,715
Based on that, it can
do continuous evolution.

188
00:12:13,715 --> 00:12:17,615
If you need to rerun hyper
parameter tuning that can be added.

189
00:12:17,615 --> 00:12:22,085
If the nature of data changes, it can
rerun the whole suite of models to find

190
00:12:22,085 --> 00:12:23,570
what is the best model for the newer data.

191
00:12:24,070 --> 00:12:29,270
It has real DevOps application from
infrastructure monitoring how your

192
00:12:29,420 --> 00:12:32,690
CPU usage or GPU usage will look like.

193
00:12:32,690 --> 00:12:37,280
Memory usage will look like if
you're expecting more spike.

194
00:12:37,340 --> 00:12:42,630
It can predict how much more you need
based on more cost optimized manner

195
00:12:42,690 --> 00:12:44,640
rather than just doing auto scales.

196
00:12:45,390 --> 00:12:49,550
It can also be if you have network higher.

197
00:12:49,595 --> 00:12:56,845
T traffic network usage it can enable
potential bottlenecks failure, and

198
00:12:56,845 --> 00:13:01,935
it can give you the right forecast
for your infrastructure so that you

199
00:13:01,935 --> 00:13:03,855
don't run into potential bottlenecks.

200
00:13:04,355 --> 00:13:09,405
Capacity planning it's long term,
so like you can tell within one

201
00:13:09,405 --> 00:13:11,085
year or two year horizon, how many.

202
00:13:11,820 --> 00:13:15,270
What your infrastructure need
would be for different components

203
00:13:15,300 --> 00:13:16,500
of your infrastructure.

204
00:13:17,130 --> 00:13:19,530
And it can also do like
predictive maintenance.

205
00:13:19,530 --> 00:13:25,200
So if you are seeing degradation
over time, it can do re rerun of

206
00:13:25,200 --> 00:13:28,680
model, figuring out what's the
best model for the latest data.

207
00:13:29,180 --> 00:13:31,310
It's built for DevOps scale.

208
00:13:31,310 --> 00:13:34,520
So you can do different kind
of automated versioning.

209
00:13:34,520 --> 00:13:36,350
You can do rollback capabilities.

210
00:13:36,450 --> 00:13:38,640
If you are not liking them
all, you can roll back.

211
00:13:38,640 --> 00:13:40,810
You can disable the inference system.

212
00:13:41,290 --> 00:13:45,280
There is AB testing bill so you can
deploy in one system and leave one

213
00:13:45,280 --> 00:13:48,460
system without the deployment and see
which system is performing better.

214
00:13:48,940 --> 00:13:51,720
So you can perform all
kind of ab testing as well.

215
00:13:52,220 --> 00:13:57,530
It can do performance monitoring, as
I said, like it can do wave monitoring

216
00:13:57,530 --> 00:14:00,770
over time of the real time forecast in.

217
00:14:01,290 --> 00:14:06,060
In the system, and it can tell whether, if
you're degrading and it can automatically

218
00:14:06,480 --> 00:14:13,580
decide whether it needs to retrain the
model or find newer model it's scalable.

219
00:14:13,670 --> 00:14:17,175
So your architecture is
like very distributed.

220
00:14:17,225 --> 00:14:21,275
Everything from individual microservice
to enterprise wise deployment can be done.

221
00:14:21,335 --> 00:14:25,265
Whether you're running on a simple
application or multi microservice

222
00:14:25,265 --> 00:14:29,045
kind of application, it can be
plugged into both the systems.

223
00:14:29,545 --> 00:14:34,985
The autonomous advantage is more like you
don't really need any human intervention,

224
00:14:35,345 --> 00:14:40,295
so you don't need to hire a data
scientist or a statistician to run and

225
00:14:40,355 --> 00:14:42,395
retrain and figure out what's the best.

226
00:14:42,975 --> 00:14:44,565
Best model for your system.

227
00:14:44,565 --> 00:14:45,735
It can do on its own.

228
00:14:46,065 --> 00:14:51,125
It has its own probabilistic brain
that that it uses to make decisions.

229
00:14:51,625 --> 00:14:53,695
It can reduce operational overhead.

230
00:14:53,695 --> 00:14:58,395
So rather than, somebody telling, Hey,
we don't really like this model to

231
00:14:58,395 --> 00:15:02,435
the statistician or data scientist,
it can decide all those things on own.

232
00:15:02,435 --> 00:15:04,325
So like communication over churn.

233
00:15:04,385 --> 00:15:05,885
It is reduced.

234
00:15:06,035 --> 00:15:08,495
That's reducing the operational overhead.

235
00:15:08,995 --> 00:15:09,385
Faster.

236
00:15:09,385 --> 00:15:13,225
Time to value means as soon as
you deploy the model, it starts

237
00:15:13,225 --> 00:15:17,165
generating forecast from quickly
like plugging into your system.

238
00:15:17,165 --> 00:15:23,365
And that helps you get reduced time
from very starting point to getting

239
00:15:23,465 --> 00:15:29,400
the forecast that is finally being used
to make decisions of your DevOps team.

240
00:15:29,900 --> 00:15:33,320
What you'll see today is like
autonomous workflow demonstration.

241
00:15:33,320 --> 00:15:38,760
Watch, agent cast discover, evaluate, and
deploy models without human intervention.

242
00:15:39,180 --> 00:15:42,450
And this is the expectation of the agent.

243
00:15:43,350 --> 00:15:47,340
Some of the examples involve
forecasting, CPU, utilization, memory

244
00:15:47,340 --> 00:15:49,830
consumption, network traffic patterns.

245
00:15:50,280 --> 00:15:52,410
Do you, are you planning to see Spike?

246
00:15:53,010 --> 00:15:58,450
So things like if you're expecting higher
traffic load balancer scale up, scale

247
00:15:58,450 --> 00:16:02,960
down and then the integration walkthrough
into the current into the current.

248
00:16:03,535 --> 00:16:07,575
DevOps pipeline you can transform
your predictive operations

249
00:16:07,705 --> 00:16:09,865
with using the agent cast.

250
00:16:10,365 --> 00:16:15,095
The future of DevOps forecasting
is enabling based on AI agents that

251
00:16:15,095 --> 00:16:19,605
can perform autonomous forecasting
without human intervention.

252
00:16:19,605 --> 00:16:22,655
So reducing human decision making.

253
00:16:22,655 --> 00:16:26,495
Human workflows and it
needs to work on its own.

254
00:16:27,125 --> 00:16:28,445
Figure out bottlenecks.

255
00:16:28,445 --> 00:16:33,515
Figure out what kind of utilization
are you or capacity are you expecting

256
00:16:33,515 --> 00:16:35,405
short term, medium term, and long term.

257
00:16:36,095 --> 00:16:40,685
And also do predictive maintenance of
the workflow, which is to understand

258
00:16:40,715 --> 00:16:44,385
if you need to scale up, scale down.

259
00:16:44,435 --> 00:16:50,075
It looks at also the vape and
map and RMC, what based on the

260
00:16:50,075 --> 00:16:52,355
type of data you have to, to.

261
00:16:53,090 --> 00:16:58,360
To produce the best solution
for your DevOps scaling issues.

262
00:16:59,110 --> 00:17:03,650
So yeah, scale, infrastructure, auto
automation across your infrastructure,

263
00:17:03,650 --> 00:17:09,410
not just like on-prem, but it can
also do cloud and any other hybrid

264
00:17:09,690 --> 00:17:11,040
approach that you might have.

265
00:17:11,850 --> 00:17:13,200
This is agent gas.

266
00:17:13,200 --> 00:17:15,270
Thank you very much for joining in.

267
00:17:15,690 --> 00:17:20,840
And I'm honored to present my AI agent,
autonomous AI agent for forecasting

268
00:17:21,440 --> 00:17:22,910
to this elite group of people.

269
00:17:22,910 --> 00:17:23,900
Thank you very much.

270
00:17:24,805 --> 00:17:25,095
Okay.

