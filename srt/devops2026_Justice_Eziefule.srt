1
00:00:00,500 --> 00:00:05,689
Speaker 16: Hi everyone, my name is
Justice and welcome to my TED Talk.

2
00:00:06,189 --> 00:00:11,500
I'm the co-founder and CT EO of Meta
Stable Labs, and we are building the very

3
00:00:11,500 --> 00:00:14,740
first margin layer to prediction markets.

4
00:00:15,400 --> 00:00:19,809
Before this, I worked as a senior
software engineer at Pay Stack and for

5
00:00:19,809 --> 00:00:23,859
Context, pay Stack is like the biggest
payments company in the whole of Africa.

6
00:00:23,919 --> 00:00:27,399
And just a few years back,
it was acquired by Stripe.

7
00:00:27,899 --> 00:00:33,839
So at Pay Stack, I spent years building
and operating systems that processed

8
00:00:33,839 --> 00:00:37,890
hundreds of millions of dollars
in transaction every single month.

9
00:00:38,369 --> 00:00:42,819
So it's safe to say that I've been
on both sides of the story, building

10
00:00:42,819 --> 00:00:47,260
features that look great in staging
and then watching production teach

11
00:00:47,264 --> 00:00:48,745
you lessons you didn't ask for.

12
00:00:49,719 --> 00:00:55,329
So today I'm going to talk about what
actually breaks in high traffic systems.

13
00:00:56,004 --> 00:00:59,814
Why it breaks and the technical
habits that keep things

14
00:00:59,814 --> 00:01:01,135
running when things go wrong.

15
00:01:01,635 --> 00:01:04,304
And I'll try my best not to bore you guys.

16
00:01:04,394 --> 00:01:06,934
And and I'll try not to
make this a shopping list.

17
00:01:07,264 --> 00:01:09,424
Actually, I'm not going to
make it a shopping list.

18
00:01:09,724 --> 00:01:13,694
So I'm going to talk about failure
mechanisms be because those

19
00:01:13,754 --> 00:01:19,129
are like the same whether you
are on AWS or GCP or whatever.

20
00:01:19,629 --> 00:01:22,179
So the promise of reliability.

21
00:01:22,749 --> 00:01:25,149
So let's start with a simple truth.

22
00:01:26,079 --> 00:01:29,529
Most outages don't begin with chaos.

23
00:01:29,859 --> 00:01:31,179
They begin quietly.

24
00:01:31,569 --> 00:01:37,629
And the problem is teams or
engineers usually look for chaos.

25
00:01:37,959 --> 00:01:43,689
They expect an outage to say stuff
like service went down, or 500 errors

26
00:01:43,689 --> 00:01:45,999
everywhere, or their terminal, all in red.

27
00:01:46,824 --> 00:01:51,894
But real incidents at
scale start slowly, right?

28
00:01:51,894 --> 00:01:59,095
So they often look like requests you
succeed, but on a slower pace, your

29
00:01:59,095 --> 00:02:02,424
queues slowly grow and time out, creep up.

30
00:02:02,924 --> 00:02:08,085
And while all these things are happening,
your metrics still look okay, right?

31
00:02:08,085 --> 00:02:10,210
And then suddenly everything collapses.

32
00:02:11,024 --> 00:02:15,734
So the real question isn't how
do we prevent all these failures?

33
00:02:16,334 --> 00:02:21,315
The real question is how do we keep
the system stable while it's failing?

34
00:02:21,815 --> 00:02:26,695
So let me say tell you guys
like a realistic example.

35
00:02:27,195 --> 00:02:29,445
Let's assume is a normal
day at work, right?

36
00:02:29,535 --> 00:02:31,155
Traffic increases.

37
00:02:31,425 --> 00:02:34,545
Maybe you guys launched a new feature.

38
00:02:35,265 --> 00:02:39,165
Maybe it could be a marketing
campaign, whatever it was.

39
00:02:40,125 --> 00:02:43,365
So everything looks fine
for the first 20 minutes.

40
00:02:43,935 --> 00:02:46,575
Then one dependency starts slowing down.

41
00:02:46,905 --> 00:02:50,925
So it could be Postgres on the
heavier read or right operations.

42
00:02:51,315 --> 00:02:53,505
It could be your red is getting hot.

43
00:02:54,075 --> 00:02:59,055
It might even be a third party pro, a
third party provider with limiting you

44
00:02:59,385 --> 00:03:01,065
because of all this whole new traffic.

45
00:03:01,845 --> 00:03:06,734
You get one API request, for instance,
it calls service A calls your db,

46
00:03:06,824 --> 00:03:10,934
which is proposed grace, and then
after that it calls service B.

47
00:03:11,204 --> 00:03:13,484
Service B calls another service.

48
00:03:13,984 --> 00:03:19,474
Now, if one dependency, if one of
these service gets slower, every

49
00:03:19,474 --> 00:03:24,904
other 10 is affected, so your
average query time grows from.

50
00:03:25,339 --> 00:03:28,789
A 15 milliseconds to
about 150 milliseconds.

51
00:03:29,209 --> 00:03:35,159
So everything still works, don't get
me wrong, and it won't feel like a

52
00:03:35,159 --> 00:03:41,289
disaster yet until you actually do
the mats in modern systems one user

53
00:03:41,289 --> 00:03:43,959
action can trigger multiple calls.

54
00:03:44,169 --> 00:03:48,519
Example, if a user loads the
dashboard on the front end.

55
00:03:49,029 --> 00:03:54,129
And it calls the backend, which
in turn calls six internal tools.

56
00:03:54,129 --> 00:04:00,849
For instance, each of these tools or
service hits the db, and if one starts

57
00:04:00,849 --> 00:04:02,559
slowing down, it affects everything.

58
00:04:03,039 --> 00:04:08,709
So if you have, let's assume you
make six calls and each has about

59
00:04:08,769 --> 00:04:14,339
100 milliseconds cell latency, you
find out that your one will request.

60
00:04:15,224 --> 00:04:18,434
Becomes 600 milliseconds very quickly.

61
00:04:19,094 --> 00:04:23,594
And believe me when I say this,
several latency is the real killer.

62
00:04:24,104 --> 00:04:26,444
Most teams look at average latency.

63
00:04:27,044 --> 00:04:29,864
Average is going to light
to you at scale, right?

64
00:04:29,864 --> 00:04:31,544
Average high stereo pen.

65
00:04:31,994 --> 00:04:40,695
So if P 50 is 100 milliseconds, for
example, but P 99 is five seconds, users

66
00:04:40,695 --> 00:04:42,659
are going to feel that five seconds not.

67
00:04:43,574 --> 00:04:45,014
The 100 milliseconds.

68
00:04:45,434 --> 00:04:51,494
So if you do enough requests, someone
is always living in that P 99.

69
00:04:51,994 --> 00:04:55,444
Now the system becomes increasingly slow.

70
00:04:56,014 --> 00:05:01,704
Developers then those what
feels reasonable, the add ries.

71
00:05:02,124 --> 00:05:07,689
So what happens is the clients
retries, the service retries, the SEK

72
00:05:07,969 --> 00:05:10,224
retries, everything attempts to retry.

73
00:05:10,794 --> 00:05:15,714
Now, instead of making one request,
you are making two or three.

74
00:05:16,704 --> 00:05:22,434
And here is the trap retries don't
reduce loads, retries, multiply,

75
00:05:22,434 --> 00:05:24,984
load at the worst possible moment.

76
00:05:25,344 --> 00:05:27,354
And this is called a retry storm.

77
00:05:28,104 --> 00:05:32,724
So it turns slow dependency
into a dependency death spiral.

78
00:05:33,025 --> 00:05:39,344
And this is how a quiet incident
becomes a full outage nest.

79
00:05:39,465 --> 00:05:44,730
You hit the nest failure mode, which
I call connection, poor exhaustion.

80
00:05:45,390 --> 00:05:50,719
If postgre grows sorry, if
postgre queries are slower.

81
00:05:51,219 --> 00:05:57,819
That means connections stay longer and
your pool quickly grows to 50 connections.

82
00:05:58,319 --> 00:06:04,669
Now on normal conditions, request complete
fast and connections return quickly.

83
00:06:05,150 --> 00:06:08,239
But now queries take about 10 x longer.

84
00:06:08,749 --> 00:06:12,049
So those 50 connections
are basically stuck.

85
00:06:12,650 --> 00:06:16,039
Now you have incoming requests, right?

86
00:06:16,400 --> 00:06:18,259
But then they keep piling up.

87
00:06:18,754 --> 00:06:22,775
Because they're waiting
for the last co connection.

88
00:06:23,275 --> 00:06:29,124
So eventually your service still looks
up, but it's effect effectively frozen.

89
00:06:29,275 --> 00:06:30,265
Nothing works.

90
00:06:30,835 --> 00:06:36,145
Your CPU might look fine, your
memory might even look fine, but your

91
00:06:36,145 --> 00:06:38,515
system is essentially now a zombie.

92
00:06:39,015 --> 00:06:42,520
So at this point, timeout starts firing.

93
00:06:43,485 --> 00:06:46,605
And this is where many
systems are poorly designed.

94
00:06:47,115 --> 00:06:52,635
Example of a bad timeout
behavior like could look like.

95
00:06:52,735 --> 00:06:54,235
It doesn't time out at all.

96
00:06:54,865 --> 00:07:00,805
So we request Hank forever or
timeout takes too long so you wait

97
00:07:00,805 --> 00:07:04,105
30 seconds and block everything else.

98
00:07:04,675 --> 00:07:08,245
Or it could also be that timeouts
don't cancel work at all.

99
00:07:09,145 --> 00:07:11,365
And this last one is huge.

100
00:07:11,995 --> 00:07:16,585
If you time out the request, but
the expensive query keeps running.

101
00:07:17,035 --> 00:07:23,095
You have built a systems, a system that
bonds resources, doing work nobody needs.

102
00:07:23,965 --> 00:07:27,625
So now users we try manually, right?

103
00:07:27,955 --> 00:07:30,525
Clients also retry the services.

104
00:07:30,525 --> 00:07:32,535
Also try to retry as well.

105
00:07:33,035 --> 00:07:35,975
But all these retries are being called.

106
00:07:36,475 --> 00:07:41,635
The DP instead is still doing work
for already canceled requests.

107
00:07:42,535 --> 00:07:43,735
You get the points right?

108
00:07:44,425 --> 00:07:46,405
So this is how systems die.

109
00:07:46,405 --> 00:07:47,155
Politely.

110
00:07:47,785 --> 00:07:52,175
No explosions just slowly ation slowly.

111
00:07:52,265 --> 00:07:53,230
The whole system dies.

112
00:07:53,730 --> 00:07:56,790
So when this is happening,
let's be very clear.

113
00:07:57,330 --> 00:08:00,870
Your goal during an incident
is not to do a root.

114
00:08:01,620 --> 00:08:04,350
Analysis to find out what's happening.

115
00:08:05,010 --> 00:08:10,200
Your goal first is to stabilize
the system and protect us users.

116
00:08:10,890 --> 00:08:16,000
So you want the system to shed load
degrade grace, fully stop doing

117
00:08:16,000 --> 00:08:18,550
unnecessary work and become stable.

118
00:08:19,480 --> 00:08:22,540
Only then do you try to find
out what happened, right?

119
00:08:23,260 --> 00:08:25,360
So how do we actually do this?

120
00:08:25,360 --> 00:08:28,835
I'm going to walk you guys through
some steps to step pattern the.

121
00:08:29,580 --> 00:08:30,870
I use at work.

122
00:08:31,370 --> 00:08:34,460
The first step is to limit concurrency.

123
00:08:35,390 --> 00:08:40,100
High traffic systems fail when
they accept infinite work.

124
00:08:40,850 --> 00:08:46,310
So one of the most un undirected
stability tools is this.

125
00:08:46,910 --> 00:08:50,150
We only process any
number of these at a time.

126
00:08:50,650 --> 00:08:51,190
Simple.

127
00:08:52,135 --> 00:08:54,385
Make sure to limit concurrent requests.

128
00:08:54,385 --> 00:09:00,305
Pine points, you can limit concurrent
DB queries and other stuff as well.

129
00:09:00,805 --> 00:09:03,835
So this is basically
called a back pressure.

130
00:09:04,165 --> 00:09:08,995
It stops the system from melting
itself, so it's better to return

131
00:09:09,325 --> 00:09:15,060
a 4, 2, 9, 2, many request, or
even a quick, please try again.

132
00:09:15,955 --> 00:09:20,665
Done to accept everything and
collapse because when you collapse,

133
00:09:20,725 --> 00:09:22,375
everyone just soft suffers.

134
00:09:22,875 --> 00:09:26,655
The next step is timeout,
but done properly.

135
00:09:27,155 --> 00:09:32,984
A good rule for this is timeout should
be short, consistent, and layered.

136
00:09:33,524 --> 00:09:35,354
Client time, sorry.

137
00:09:36,074 --> 00:09:39,645
Client timeout might be
two to three seconds.

138
00:09:40,020 --> 00:09:47,260
For a user request service to service
timeout might be 500 milliseconds to maybe

139
00:09:47,410 --> 00:09:49,930
1.5 seconds depending on the endpoint.

140
00:09:50,690 --> 00:09:54,530
And here is the most
important key timeout.

141
00:09:55,190 --> 00:09:57,410
Most cancel work.

142
00:09:58,160 --> 00:10:02,570
If a request times out, stop
doing any expensive work for it.

143
00:10:03,020 --> 00:10:05,600
Cancel or pending DB queries.

144
00:10:06,000 --> 00:10:08,340
Stop downstream costs, basically stop.

145
00:10:08,685 --> 00:10:12,405
Everything for that particular re request.

146
00:10:13,005 --> 00:10:16,005
So this one habit prevents
so many death spirals.

147
00:10:16,505 --> 00:10:17,975
Now circuit breakers.

148
00:10:18,665 --> 00:10:22,535
A circuit breaker means if a
dependency is failing or slow,

149
00:10:22,925 --> 00:10:24,485
stop calling it for a bit.

150
00:10:25,415 --> 00:10:30,065
This prevents your service from
spending all these resources hammering

151
00:10:30,065 --> 00:10:31,505
something that won't recover.

152
00:10:31,985 --> 00:10:34,775
It also creates like re recovery space.

153
00:10:35,120 --> 00:10:37,850
So we have some space to
actually recover, right?

154
00:10:38,750 --> 00:10:43,910
If you do this right, you prevent
cascading failure and protect

155
00:10:43,910 --> 00:10:47,300
your own service from being
dragged down by sick dependency.

156
00:10:47,800 --> 00:10:55,240
Now, the parts that suppress mature
systems from fragile ones is graceful

157
00:10:55,300 --> 00:10:59,530
degradation during such scenarios.

158
00:11:00,175 --> 00:11:02,935
Some features should
turn off automatically.

159
00:11:03,205 --> 00:11:05,605
Not everything is equally important.

160
00:11:06,145 --> 00:11:12,885
Example, in a payment infrastructure, we
all agree that authentication is critical.

161
00:11:13,455 --> 00:11:19,545
Placing an order is also critical,
but something like a recommended item

162
00:11:19,545 --> 00:11:21,615
widget somewhere is probably not.

163
00:11:22,455 --> 00:11:26,660
So knowing this, you should
build the system to degrade.

164
00:11:27,370 --> 00:11:33,940
You could self cashed data or return
partial results or even turn off

165
00:11:33,940 --> 00:11:36,930
non essential stuff in payments.

166
00:11:36,930 --> 00:11:42,930
For instance, if your analytics pipeline
is slow, you do not block payments.

167
00:11:43,430 --> 00:11:46,810
Instead, you degrade worth
failing, not this transaction.

168
00:11:47,310 --> 00:11:50,190
The next step is load shedding.

169
00:11:50,850 --> 00:11:53,460
This is being intentional about failure.

170
00:11:54,180 --> 00:11:59,300
You can shed load by rejecting
request early, dropping non-critical

171
00:11:59,300 --> 00:12:03,030
background task sampling logs
instead of logging everything.

172
00:12:03,530 --> 00:12:07,615
A line I always repeat all the
time is, it's better to fail

173
00:12:07,615 --> 00:12:10,285
fast than to fail slowly because.

174
00:12:10,785 --> 00:12:14,085
Slow failure destroys your system fast.

175
00:12:14,085 --> 00:12:15,375
Failure keeps you alive.

176
00:12:15,875 --> 00:12:20,515
And this essentially completes the
list of things I feel you need to

177
00:12:20,515 --> 00:12:24,450
do next is things to watch out for.

178
00:12:24,950 --> 00:12:33,170
So most teams have monitoring, but in
incidents, they don't know what matters

179
00:12:33,925 --> 00:12:35,850
in the high traffic system, for instance.

180
00:12:36,350 --> 00:12:39,940
You need to track four 10
like your life depends on it.

181
00:12:40,630 --> 00:12:44,530
Latency, traffic, errors, and saturation.

182
00:12:45,010 --> 00:12:50,260
And for latency, you need to
track percentiles, not averages.

183
00:12:50,770 --> 00:12:58,410
So you want to see P 50, P 95, P 99,
and for each of these, you need to

184
00:12:58,410 --> 00:13:00,270
see the error rates, the Q depth.

185
00:13:00,780 --> 00:13:07,590
The DB connection usage, the trade
usage, the downstream core latency,

186
00:13:07,590 --> 00:13:14,220
and so on, because incidents often show
up first as saturation, not errors.

187
00:13:14,720 --> 00:13:17,480
And here is how quiet incidents show up.

188
00:13:17,980 --> 00:13:20,560
So let's assume P 50 is stable.

189
00:13:21,250 --> 00:13:24,900
P 95 creeps up a little bit.

190
00:13:25,400 --> 00:13:28,040
But then P 99 spikes, right?

191
00:13:28,540 --> 00:13:31,840
So what happens is Q Depth slowly grows.

192
00:13:32,110 --> 00:13:35,380
DB connections approach maximum limits.

193
00:13:35,910 --> 00:13:38,430
Our retries also increase as well.

194
00:13:39,030 --> 00:13:41,190
Then suddenly everything breaks.

195
00:13:41,690 --> 00:13:47,390
And when this happens, the right
alerts shouldn't be is the CPU high.

196
00:13:48,080 --> 00:13:50,180
The right alerts should be.

197
00:13:50,680 --> 00:13:53,230
What's the bond rate of P 99?

198
00:13:53,470 --> 00:13:55,300
What is the Q growth rates?

199
00:13:55,540 --> 00:13:57,160
What is the error budget bond?

200
00:13:57,190 --> 00:13:59,230
What's the DB connection pool like?

201
00:13:59,230 --> 00:14:00,910
Is it nearing exhaustion?

202
00:14:01,600 --> 00:14:07,930
Marshall teams are lot on trends,
not bond rates, not raw thresholds.

203
00:14:08,430 --> 00:14:12,420
So when things are failing,
loss alone won't save you.

204
00:14:12,840 --> 00:14:17,730
You need to take one request
and ask where did this go?

205
00:14:18,230 --> 00:14:20,990
And I call that a
distributor tracing, right?

206
00:14:21,830 --> 00:14:24,930
Even if you have, even if you
don't have food tracing you

207
00:14:24,930 --> 00:14:27,420
should do the basic things, right?

208
00:14:27,420 --> 00:14:33,150
So you should know where you should check
the, for correlation IDs, structural

209
00:14:33,150 --> 00:14:36,100
logs consistent timing and so on.

210
00:14:36,730 --> 00:14:38,855
Then you can see, so in.

211
00:14:39,745 --> 00:14:43,765
Instead of having like normal logs
everywhere, you could actually

212
00:14:43,765 --> 00:14:47,425
see how long your API call took.

213
00:14:47,575 --> 00:14:54,025
How long did it take service A to
get a response from the db, right?

214
00:14:54,535 --> 00:14:59,490
Where exactly did your query get
stuck and so on, and this is how

215
00:14:59,515 --> 00:15:02,275
you move from panic to diagnosis.

216
00:15:02,775 --> 00:15:05,235
So let me say something controversial.

217
00:15:05,625 --> 00:15:09,735
Many outages are not
prolonged by bad code.

218
00:15:10,185 --> 00:15:12,435
They are prolonged by messy response.

219
00:15:13,275 --> 00:15:21,345
I've seen in incidents where up to five
engineers are deploying fixes, right?

220
00:15:21,405 --> 00:15:22,485
Nobody owns it.

221
00:15:22,965 --> 00:15:24,525
Nobody knows what change.

222
00:15:24,795 --> 00:15:27,975
Every fix introduces a new variable.

223
00:15:28,905 --> 00:15:32,445
So yes, a simple step by step playbook.

224
00:15:32,445 --> 00:15:33,855
I feel like we work.

225
00:15:33,915 --> 00:15:35,205
I know we work.

226
00:15:35,705 --> 00:15:42,815
The first one is name an incident lead,
so someone needs to be the coordinator.

227
00:15:43,385 --> 00:15:45,185
That's very important.

228
00:15:45,575 --> 00:15:50,855
The second one is freeze non-essential
changes and stop random deployments.

229
00:15:51,380 --> 00:15:53,900
Totally stabilize the system.

230
00:15:54,080 --> 00:15:58,600
You can do this by re reducing
loads, disabling some features

231
00:15:59,000 --> 00:16:00,830
stop retries and so on.

232
00:16:01,400 --> 00:16:07,400
The fourth one, so you can
attempt to restore safe services.

233
00:16:07,700 --> 00:16:11,570
You can do this by rolling
back to the previous working

234
00:16:11,810 --> 00:16:14,150
fashion or working deployment.

235
00:16:14,660 --> 00:16:18,440
You can reduce the blast
reduce, for instance.

236
00:16:18,940 --> 00:16:24,940
And the fifth one is only after stability
do you investigate the root cause.

237
00:16:25,440 --> 00:16:29,250
The goal is to make incidents
boring, not exciting.

238
00:16:29,750 --> 00:16:35,690
If you want reliability, you
don't start during the outage.

239
00:16:36,080 --> 00:16:38,300
You start before the outage.

240
00:16:38,510 --> 00:16:40,550
You design with failure in mind.

241
00:16:40,940 --> 00:16:42,290
That means in dependency.

242
00:16:42,790 --> 00:16:50,800
So when client clients re retry,
your system must not double charge or

243
00:16:50,800 --> 00:16:52,750
double create, or even double execute.

244
00:16:53,530 --> 00:16:57,070
And this means you should have
queues for expensive work.

245
00:16:57,370 --> 00:17:01,650
Don't do heavy work in line in
the same re requests, right?

246
00:17:02,640 --> 00:17:08,700
It also means bulkhead one failing
part shouldn't sink the entire ship.

247
00:17:09,075 --> 00:17:12,465
Separate pools and resources
for critical parts.

248
00:17:13,095 --> 00:17:18,735
It also means with limiting you need
to implement a with limit to prevent

249
00:17:18,735 --> 00:17:21,075
your system from abuse and spikes.

250
00:17:21,575 --> 00:17:24,155
And I'm going to end with this.

251
00:17:24,785 --> 00:17:29,525
High traffic systems don't break
because engineers are careless.

252
00:17:30,035 --> 00:17:34,505
They break because complexity
compounds faster than intuition.

253
00:17:35,345 --> 00:17:39,515
And the most dangerous
incidents are the pirate ones.

254
00:17:39,855 --> 00:17:47,745
The slow dependency, the creeping
cues, the retry storm, and so on, until

255
00:17:47,775 --> 00:17:49,925
production becomes a fight, right?

256
00:17:50,585 --> 00:17:54,785
If you want your system to
survive at scale, you must

257
00:17:54,785 --> 00:17:57,155
put hard limits at on work.

258
00:17:57,605 --> 00:17:59,585
Use timeouts that cancel.

259
00:18:00,125 --> 00:18:06,605
Add circuit breakers degrade gracefully,
she load intentionally and so on

260
00:18:07,105 --> 00:18:11,515
because production will eventually
test every assumption you've made.

261
00:18:12,055 --> 00:18:14,425
And the goal isn't to avoid that test.

262
00:18:14,635 --> 00:18:18,235
The goal is to build systems
that take it, that can take

263
00:18:18,235 --> 00:18:20,395
it and recover without drama.

264
00:18:21,295 --> 00:18:24,405
And this is the end of my presentation.

265
00:18:24,905 --> 00:18:26,765
Thank you for listening.

266
00:18:26,765 --> 00:18:30,245
You can reach out to me on
LinkedIn if you have any questions.

267
00:18:30,695 --> 00:18:30,935
Thank you.

