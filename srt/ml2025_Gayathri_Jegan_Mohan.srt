1
00:00:02,820 --> 00:00:03,540
Hello everyone.

2
00:00:03,600 --> 00:00:06,870
Welcome to the Con 42
Machine Learning Conference.

3
00:00:07,240 --> 00:00:08,590
I am Ga Mohan.

4
00:00:08,590 --> 00:00:13,180
Today I'm gonna talk about bringing AI
to the edge, how ML machine learning

5
00:00:13,180 --> 00:00:15,460
models are covering the future of iot.

6
00:00:15,820 --> 00:00:16,720
So that's my topic.

7
00:00:16,720 --> 00:00:18,009
So let's get started.

8
00:00:18,640 --> 00:00:23,690
So the first thing I wanna talk
about is what is h AI mean?

9
00:00:24,125 --> 00:00:27,305
Because in iot, usually the machine
learning models are deployed in

10
00:00:27,305 --> 00:00:29,255
the cloud rather than in the edge.

11
00:00:29,675 --> 00:00:34,685
And but with Edge ai, what happens is it's
a little bit different because you have

12
00:00:34,685 --> 00:00:40,085
the capability to deploy models locally
on the edge device and reduce the latency.

13
00:00:40,415 --> 00:00:45,115
So usually what happens is if you see here
in the Edge device, in the H AI scenario,

14
00:00:45,415 --> 00:00:48,175
so the devices are connected to a local.

15
00:00:48,725 --> 00:00:53,675
Pre-trained model and which can do some
inferencing, and that pre-processing

16
00:00:53,675 --> 00:00:55,475
is done in the edge itself.

17
00:00:55,865 --> 00:01:00,875
And then any result that is there, it
goes directly to the user interface.

18
00:01:01,295 --> 00:01:04,445
But in the case of the usual scenario,
the cloud scenario, what happens?

19
00:01:04,445 --> 00:01:08,025
It goes one level step first,
the it goes into the cloud, to

20
00:01:08,025 --> 00:01:09,615
the do to do the inferencing.

21
00:01:09,615 --> 00:01:13,275
And once the inferencing results are
out, then the user gets notified.

22
00:01:13,650 --> 00:01:18,449
But in the case of Edge ai, you are
reducing multiple levels of hopping here.

23
00:01:18,749 --> 00:01:23,920
So everything is done at the edge itself,
and this helps in real time tracking.

24
00:01:23,920 --> 00:01:28,330
For example, if you wanna do traffic light
control system, or if you wanna understand

25
00:01:28,330 --> 00:01:33,340
if your appliances are broken, all these
informations can be immediately available.

26
00:01:33,340 --> 00:01:37,030
And you can track also the real
time status of your devices.

27
00:01:37,780 --> 00:01:42,340
So this also actually solves
the privacy issues being once

28
00:01:42,340 --> 00:01:43,900
you send the data to the cloud.

29
00:01:43,900 --> 00:01:47,470
So it can be geo geographically
moved from one location to another.

30
00:01:47,740 --> 00:01:52,180
So you can avoid that by keeping the
model inferencing very close to the

31
00:01:52,180 --> 00:01:53,980
source where it is getting generated.

32
00:01:54,520 --> 00:01:56,890
So that is all about H ai.

33
00:01:57,130 --> 00:02:01,930
So now let's see about, now
let's see about what it means to.

34
00:02:03,065 --> 00:02:05,615
Actually seen in an actual scenario.

35
00:02:05,855 --> 00:02:08,765
So for example, the first
one is a surveillance camera.

36
00:02:08,855 --> 00:02:14,714
So I was talking about if you want to
create create inferencing in the place

37
00:02:14,714 --> 00:02:20,204
where the cameras are located, for example
you want to track any anomaly in the

38
00:02:20,204 --> 00:02:26,034
outputs so you can keep influencing models
right close to the source where the data

39
00:02:26,034 --> 00:02:27,564
is generated, which is in the case of.

40
00:02:28,624 --> 00:02:32,184
So you can detect if any anomalies
happening in the same way.

41
00:02:32,184 --> 00:02:36,754
You can do that in the manufacturing
robot, so you can real time track without

42
00:02:36,754 --> 00:02:38,404
sending these images to the cloud.

43
00:02:38,884 --> 00:02:42,304
And then there is a smart speaker
in your home, which recognizes your

44
00:02:42,304 --> 00:02:46,504
voice locally instead of sending the,
sending your voice and your prompt

45
00:02:46,844 --> 00:02:48,694
to the cloud for further processing.

46
00:02:49,054 --> 00:02:52,354
So instead, you keep everything
in the source for faster response

47
00:02:52,354 --> 00:02:53,854
and also for privacy concerns.

48
00:02:54,409 --> 00:02:57,339
So these are some of the
examples that you can see today.

49
00:02:57,700 --> 00:03:03,969
And the next one is about why
edge is better than cloud.

50
00:03:04,149 --> 00:03:06,879
So now we are moving the
AI inferencing to the edge.

51
00:03:06,879 --> 00:03:08,859
So is it how better it is?

52
00:03:09,129 --> 00:03:12,219
So you can see that in the
cloud, the inferencing happens

53
00:03:12,279 --> 00:03:13,329
in the cloud, in the edge.

54
00:03:13,329 --> 00:03:14,439
It happens in the.

55
00:03:14,779 --> 00:03:15,649
Device itself.

56
00:03:15,649 --> 00:03:16,850
So what happens?

57
00:03:17,179 --> 00:03:18,950
What is a big advantage of this?

58
00:03:18,980 --> 00:03:22,399
So the first advantage is latency, as
we have seen, because you are getting

59
00:03:22,760 --> 00:03:27,039
the real time response as the data is
generated, and then the inferencing is

60
00:03:27,039 --> 00:03:29,829
done, and also it reduces the bandwidth.

61
00:03:29,829 --> 00:03:33,429
So bandwidth being so you are not
sending a lot of data to the cloud.

62
00:03:33,429 --> 00:03:36,619
So there's minimal data that is
being up that is being processed

63
00:03:36,619 --> 00:03:38,200
as as well as sent to the cloud.

64
00:03:38,605 --> 00:03:41,555
Now comes security, which
we already touched upon.

65
00:03:41,555 --> 00:03:46,115
So you are reducing the number of
hops that happens for the data.

66
00:03:46,385 --> 00:03:51,435
So in this case, in this scenario, what
happens is you are creating a much more

67
00:03:51,435 --> 00:03:56,595
secure solution because you're, you are
not moving the data around most often.

68
00:03:57,085 --> 00:04:00,145
And also it's very reliable
because it operates with even

69
00:04:00,195 --> 00:04:03,705
no internet connectivity as well
as limited internet connection.

70
00:04:04,125 --> 00:04:06,935
That way you can create
reliable solutions.

71
00:04:07,445 --> 00:04:10,955
And the last but not least
advantage of that is the energy.

72
00:04:10,955 --> 00:04:16,305
So because you are not spending that much
compute com compute in the cloud, you're

73
00:04:16,305 --> 00:04:18,015
basically reducing the energy as well.

74
00:04:18,495 --> 00:04:23,865
So this is some difference between
Edge AI and then the cloud ai.

75
00:04:24,885 --> 00:04:30,895
So the biggest change in the paradigm
shift for IOT today is 75% of the data

76
00:04:30,895 --> 00:04:35,605
today as we speak for this year, is
actually processed at the edge, which

77
00:04:35,605 --> 00:04:38,305
is revolutionizing the AI powered iot.

78
00:04:38,635 --> 00:04:43,355
This is changing the way we approach the
iot solutions because you are doing a

79
00:04:43,355 --> 00:04:48,125
lot more than what you can a lot more at
the edge than you were able to do before.

80
00:04:48,860 --> 00:04:52,990
But doing this actually ha poses its
own challenges in the background.

81
00:04:52,990 --> 00:04:57,105
So the first challenge is limited
computing resources because.

82
00:04:57,720 --> 00:05:01,110
Now you're limited to, now you're
doing more than what you used to do.

83
00:05:01,110 --> 00:05:06,010
So you have usually these devices used to
be resource constrained devices, right?

84
00:05:06,430 --> 00:05:10,841
So now you have low processing power
compared to those powerful GPUs and

85
00:05:10,845 --> 00:05:11,950
TPUs that are running in the cloud.

86
00:05:12,430 --> 00:05:15,730
And then you also have limited
memory and power, memory and

87
00:05:15,730 --> 00:05:18,460
storage for the AI models to use.

88
00:05:18,880 --> 00:05:21,760
Because there are so much going on, right?

89
00:05:21,760 --> 00:05:25,900
And the devices are already resource
constrained and the power supply.

90
00:05:25,900 --> 00:05:28,240
So mostly these devices
are battery operated.

91
00:05:28,240 --> 00:05:32,720
So because of that, there is a
constraint in the power as well.

92
00:05:33,260 --> 00:05:38,515
And you cannot run actually complex
models like a full fledged resident

93
00:05:38,545 --> 00:05:40,165
one 50 and stuff like that, because.

94
00:05:40,540 --> 00:05:45,180
You don't have that much resources
in the case of compute, storage

95
00:05:45,460 --> 00:05:46,960
processing power, et cetera.

96
00:05:47,530 --> 00:05:52,690
And also the module deployment and update
becomes a challenge because it's very hard

97
00:05:52,690 --> 00:05:57,270
to push all these updates to the we might
have 10,000 devices in the edge, right?

98
00:05:57,270 --> 00:06:00,930
So it's very hard to go and,
update each and every device for

99
00:06:00,930 --> 00:06:02,400
a new model when it is available.

100
00:06:02,805 --> 00:06:06,335
And also you have the risk of
not matching the version that you

101
00:06:06,335 --> 00:06:08,015
are actually, you actually need.

102
00:06:08,505 --> 00:06:13,165
This also I mean we have already seen
that it actually helps with security, but

103
00:06:13,165 --> 00:06:16,735
also it also possesses another security
in the other side of the spectrum.

104
00:06:17,125 --> 00:06:20,795
So the security issues being
the devices are actually mostly

105
00:06:20,795 --> 00:06:22,655
open to the public network.

106
00:06:23,100 --> 00:06:26,705
So you you are prone to malware
attacks, and also you need

107
00:06:26,705 --> 00:06:28,355
a secure boot in the device.

108
00:06:28,625 --> 00:06:33,285
And also you need to use lightweight
standard protocols, which which

109
00:06:33,645 --> 00:06:35,815
provides security from the start.

110
00:06:36,185 --> 00:06:41,875
So other than that it becomes a huge
prone to threats and attacks even though

111
00:06:41,875 --> 00:06:45,265
it is secure in its way that you're
not doing in the cloud, but there is of

112
00:06:45,265 --> 00:06:47,995
course security risk in the edge itself.

113
00:06:48,865 --> 00:06:50,065
So there are other challenges.

114
00:06:50,065 --> 00:06:52,615
Let's go into the second
part of the challenge.

115
00:06:52,615 --> 00:06:54,115
The second part of the challenge being.

116
00:06:54,715 --> 00:06:55,675
Connectivity issues.

117
00:06:55,675 --> 00:06:59,595
So connectivity issues, as I said
before these devices are usually

118
00:06:59,595 --> 00:07:01,095
deployed in remote services.

119
00:07:01,095 --> 00:07:05,445
Sometimes they don't have any internet in
them, so it is very hard to sync to the

120
00:07:05,445 --> 00:07:09,255
cloud even after getting some inferencing
data and then you wanna store it in the

121
00:07:09,255 --> 00:07:11,725
cloud or, use it for later purposes.

122
00:07:11,775 --> 00:07:16,415
For deploying the cloud for using
the cloud for post-processing.

123
00:07:16,910 --> 00:07:19,520
So it's very, it's
becomes a hard challenge.

124
00:07:19,760 --> 00:07:23,150
And the next challenge is model
optimization and compression.

125
00:07:23,510 --> 00:07:26,570
So you have to ensure that
you are creating a model which

126
00:07:26,570 --> 00:07:28,550
is suitable for the edge.

127
00:07:28,790 --> 00:07:30,140
So it has to be lightweight.

128
00:07:30,470 --> 00:07:34,640
So the accuracy might be compromised
because you're not gonna get the full

129
00:07:34,640 --> 00:07:39,490
blown result of the model because it's
very tailored to the scenario at the edge.

130
00:07:39,970 --> 00:07:43,350
And in the case of observability,
it's very hard to do because.

131
00:07:43,740 --> 00:07:46,690
You have to monitor
performance and failures.

132
00:07:46,690 --> 00:07:48,520
You can do only simple logs and tools.

133
00:07:48,520 --> 00:07:53,260
You cannot do complex telemetry data
like open Telemetry, Yeager, and

134
00:07:53,260 --> 00:07:58,270
all that stuff because it's just not
possible with the resources in hand.

135
00:07:58,810 --> 00:08:02,290
And there's also diversity problem,
Harvard diversity problems.

136
00:08:02,290 --> 00:08:03,855
So there are so many devices.

137
00:08:04,560 --> 00:08:07,140
So many different vendors are there.

138
00:08:07,140 --> 00:08:10,290
So everyone has their own OS protocols.

139
00:08:10,630 --> 00:08:14,320
So it becomes a challenge to
aggravate those devices and ensure

140
00:08:14,380 --> 00:08:18,460
that you're able to run those models
everywhere in a uniform fashion.

141
00:08:18,760 --> 00:08:21,190
So these are one of the
challenges that we have.

142
00:08:21,610 --> 00:08:26,490
Now let's go into the nitty
gritty details of best practices.

143
00:08:26,755 --> 00:08:29,395
So even though we have some
challenges, but there is a way

144
00:08:29,395 --> 00:08:34,055
that we can actually make it better
by using some of these practices.

145
00:08:34,105 --> 00:08:37,165
So there are four practices
that I'm gonna cover today.

146
00:08:37,435 --> 00:08:40,255
So the first is deploying
ML models at scale.

147
00:08:40,765 --> 00:08:45,665
How we can do that so that we have 10,000
devices and the devices keep increasing.

148
00:08:45,665 --> 00:08:48,875
How do we make sure that the models
are getting deployed to every device?

149
00:08:49,535 --> 00:08:51,905
And the second one is
updating models on the edge.

150
00:08:51,905 --> 00:08:52,745
How do you do it?

151
00:08:52,805 --> 00:08:55,965
If there is a new version of the
model that is available how we

152
00:08:55,965 --> 00:08:59,145
can ensure that you are following
the best practice for that.

153
00:08:59,445 --> 00:09:02,465
And also you use model compression
techniques, which will reduce the

154
00:09:02,465 --> 00:09:06,635
model size and then ensure that it
is used for the scenario in hand.

155
00:09:07,025 --> 00:09:09,965
And also last but not the
least, using AI pipelines.

156
00:09:10,055 --> 00:09:13,215
The cloud is not in the not completely
out of the picture, but you can

157
00:09:13,215 --> 00:09:17,535
still use them for your other
purposes, which will get into it.

158
00:09:18,345 --> 00:09:20,865
So the first practice, which
I wanna cover, is like how to

159
00:09:20,865 --> 00:09:22,755
deploy models at this edge.

160
00:09:22,755 --> 00:09:25,545
Since we have so many devices,
thousands and thousand devices,

161
00:09:25,545 --> 00:09:29,605
it becomes very critical to ensure
that you have model portability.

162
00:09:29,860 --> 00:09:33,580
So you wanna ensure that you're
using a runtime in the device, which

163
00:09:33,580 --> 00:09:35,480
is portable for all the models.

164
00:09:35,790 --> 00:09:39,690
For example, use formats like Onyx
or TensorFlow Light that works across

165
00:09:39,690 --> 00:09:44,150
different hardware because we wanna ensure
that why if it is working in one hardware

166
00:09:44,520 --> 00:09:48,120
and if it's a different vendor in a
different hardware, it should work, right?

167
00:09:48,150 --> 00:09:51,725
Because then there is a
model incompatibility.

168
00:09:52,115 --> 00:09:54,785
So that's why you should use
a model that is portable.

169
00:09:55,130 --> 00:09:59,940
And also you should also use hardware
abstractions like target accelerators

170
00:09:59,940 --> 00:10:06,780
Nvidia Jetson or in Intel Vid or Arm
NPUs because these devices are designed

171
00:10:06,780 --> 00:10:09,660
in a way to use unified run times.

172
00:10:09,690 --> 00:10:14,370
And you can use these unified runtime
to deploy any model of your choice,

173
00:10:14,670 --> 00:10:16,800
which is targeted specifically for H ai.

174
00:10:17,745 --> 00:10:22,605
And the last best practice to deploy
models at scale is containerization.

175
00:10:22,965 --> 00:10:28,545
So once you package all the dependencies
in a container, you can use Docker to

176
00:10:28,545 --> 00:10:33,405
run it so that you have a consistent
execution across different platforms

177
00:10:33,615 --> 00:10:38,415
so that in that way you have a
steady way to deploy these models.

178
00:10:38,755 --> 00:10:43,075
So you package the models inside
the containers as well so that you

179
00:10:43,075 --> 00:10:47,465
ha you, you're expecting something
in the same consistent result in

180
00:10:47,465 --> 00:10:49,145
every device that you're deploying.

181
00:10:49,475 --> 00:10:54,355
So this is one of the best practices
to follow first when you're deploying

182
00:10:54,685 --> 00:10:57,595
so many devices for I iot H Solutions.

183
00:10:57,985 --> 00:11:01,435
So the next solutions for
updating the models, for example.

184
00:11:01,815 --> 00:11:05,425
If you have a new model that is
coming up after you have pre-trained

185
00:11:05,425 --> 00:11:09,325
them with the data, that historic
data that you have, now you need to

186
00:11:09,325 --> 00:11:11,855
understand how to update them, right?

187
00:11:11,855 --> 00:11:14,695
Because frequent updates are
required required in order

188
00:11:14,695 --> 00:11:16,105
to improve the efficiency.

189
00:11:16,420 --> 00:11:20,230
As well as, if there is any vulnerability,
you wanna ensure that is also taken care.

190
00:11:20,560 --> 00:11:23,560
So you need to adapt to an environment
that is constantly shifting.

191
00:11:23,710 --> 00:11:27,620
So in order to do that, some of the
best practices to do is over the

192
00:11:27,620 --> 00:11:29,600
air updates with version control.

193
00:11:29,600 --> 00:11:34,925
So that is basically sell sending to
multiple multiple devices over the

194
00:11:35,165 --> 00:11:36,605
air, which means over the internet.

195
00:11:36,965 --> 00:11:39,365
And then there is another
practice called AB testing.

196
00:11:39,695 --> 00:11:44,915
So you first test with one model and
deploy them and see how it works,

197
00:11:45,125 --> 00:11:48,725
and then you roll out the rest of
them for the rest of the devices.

198
00:11:49,145 --> 00:11:52,565
And there is also another scenario
you can use, which is digital twin.

199
00:11:52,565 --> 00:11:57,005
So digital twin is basically, you
mimic the same physical device

200
00:11:57,005 --> 00:12:01,495
to a digital environment and see
that if you put the new model into

201
00:12:01,495 --> 00:12:03,145
the digital twin, what happens?

202
00:12:03,145 --> 00:12:06,485
And if everything is the
everything is going fine, then

203
00:12:06,485 --> 00:12:08,165
you can go ahead and update that.

204
00:12:09,170 --> 00:12:12,470
So this one is about how
to update the models.

205
00:12:12,710 --> 00:12:18,390
So the third practice is how to use models
that uses the compression techniques.

206
00:12:18,690 --> 00:12:23,100
So since Edge AI is going to use
a different model than the usual

207
00:12:23,370 --> 00:12:27,470
model that we use, so we need to
understand what kind of compression

208
00:12:27,470 --> 00:12:29,350
techniques that goes to the model.

209
00:12:29,680 --> 00:12:31,810
So the first one is the quantization.

210
00:12:31,810 --> 00:12:36,690
So quantization you can think
of if you have a cloud, a beefy

211
00:12:36,690 --> 00:12:40,970
system, you would use a 34 bit
machine or a 64 bit machine, right?

212
00:12:41,270 --> 00:12:47,480
So now you are no longer in such a luxury
en environment, you're in a very small,

213
00:12:47,510 --> 00:12:49,880
constrained devices like a raspberry pie.

214
00:12:50,150 --> 00:12:52,250
So now you have to run
the model in there, right?

215
00:12:52,250 --> 00:12:56,180
So you need to compress your
model to eight bit in teacher

216
00:12:56,480 --> 00:12:59,450
so that it's smaller size and it
can also do faster inferencing.

217
00:13:01,025 --> 00:13:03,305
And also another technique
that you can do is pruning.

218
00:13:03,365 --> 00:13:06,815
So what you do is you remove
the insignificant fats or

219
00:13:06,845 --> 00:13:08,135
the neurons from the model.

220
00:13:08,465 --> 00:13:12,185
So this way what happens is it speeds
up the computation as well as gives

221
00:13:12,185 --> 00:13:13,955
you inferencing the cells very quickly.

222
00:13:14,435 --> 00:13:17,765
And then the next, the third one
is like knowledge distillation.

223
00:13:17,765 --> 00:13:22,555
So basically it's a concept of dialing
down the model to a student model so

224
00:13:22,555 --> 00:13:24,475
that it does the basic operations.

225
00:13:25,330 --> 00:13:25,900
Correctly.

226
00:13:26,230 --> 00:13:31,120
And then you try to learn to make the
teacher model, which is the larger model.

227
00:13:31,510 --> 00:13:36,720
So in this way you get also higher
efficiency with good accuracy as well.

228
00:13:37,050 --> 00:13:40,780
So these are some of the techniques
that are deployed in order to support

229
00:13:40,840 --> 00:13:43,480
the model for the edge AI scenario.

230
00:13:44,020 --> 00:13:46,510
So the last but not the
least practice is the.

231
00:13:47,845 --> 00:13:52,435
Cloud managed AI pipelines so you're
not taking cloud entirely out of the

232
00:13:52,435 --> 00:13:57,805
picture because rather you would try to
augment both cloud and edge together.

233
00:13:58,045 --> 00:14:03,565
So rather than running the, managing
the life cycles of all these systems

234
00:14:03,565 --> 00:14:08,125
manually, you can do it using
cloud based ML operations, which

235
00:14:08,125 --> 00:14:09,535
are machine learning operations.

236
00:14:09,745 --> 00:14:13,765
So these pipelines can actually
train and retrain on the cloud

237
00:14:13,765 --> 00:14:15,010
and then update the dataset.

238
00:14:15,550 --> 00:14:19,660
What I mean by that is like you're
doing behind the scene work, which is

239
00:14:19,660 --> 00:14:25,190
doing all the training and, making the
model better so that you can, you don't

240
00:14:25,190 --> 00:14:26,930
have to pre-train them anymore, right?

241
00:14:26,930 --> 00:14:29,960
So the CICD pipelines are
there to automate the testing

242
00:14:29,960 --> 00:14:31,100
as well as the deployment.

243
00:14:31,630 --> 00:14:34,810
And also you can do telemetry
collection from edge to continuously

244
00:14:34,810 --> 00:14:35,950
improve the models because.

245
00:14:36,275 --> 00:14:40,385
Some basic telemetry is still okay
because you would need that to understand

246
00:14:40,385 --> 00:14:44,075
what's going on in the edge device
and then use that information to train

247
00:14:44,075 --> 00:14:47,785
your model better and then use it for
the, different version, which is much

248
00:14:47,785 --> 00:14:48,955
better than the previous version.

249
00:14:49,595 --> 00:14:50,675
These are some of the examples.

250
00:14:50,675 --> 00:14:54,855
Azure, ml AWS SageMaker always,
they have their own solutions.

251
00:14:55,380 --> 00:15:00,910
For AI pipelines so that you can integrate
and continuously deploy AI solutions

252
00:15:01,010 --> 00:15:03,185
in for the real world application.

253
00:15:03,785 --> 00:15:07,535
So now let's see some of the actual
real world application where it

254
00:15:07,535 --> 00:15:10,895
is actually using some of the
principles we just talked about.

255
00:15:10,895 --> 00:15:12,815
So first is the deploying example.

256
00:15:13,235 --> 00:15:18,925
So Walmart actually uses, for deploying
models at scale, how what they did is

257
00:15:18,925 --> 00:15:22,255
basically they deployed thousands of
cameras with AI models in their retail

258
00:15:22,825 --> 00:15:24,835
stores to monitor inventory levels.

259
00:15:25,165 --> 00:15:30,535
Also understand where each order, each
item is placed, and also custom behavior.

260
00:15:30,535 --> 00:15:34,325
There is any theft or if there is
any anomalies that is happening.

261
00:15:34,665 --> 00:15:39,375
So how they used it is they used compact
edge servers like Nvidia Jetson and also

262
00:15:39,375 --> 00:15:44,415
they used computer vision models in which
is optimized just for edge scenario.

263
00:15:44,895 --> 00:15:45,585
And they use tens.

264
00:15:46,020 --> 00:15:50,075
Tens is it is a simpler
version of the user 10 flow.

265
00:15:50,525 --> 00:15:52,475
So now we have the result.

266
00:15:52,480 --> 00:15:55,475
So result is basically you have
reduced stockouts and you have

267
00:15:55,475 --> 00:15:57,545
optimized the restocking schedules.

268
00:15:57,905 --> 00:16:01,445
You understand how to improve the
efficiency, operational efficiency

269
00:16:01,445 --> 00:16:03,145
across different locations.

270
00:16:03,685 --> 00:16:07,855
So this is for deploying at scale
for, the first best practice.

271
00:16:08,185 --> 00:16:11,005
So now let's go into the second
best practice, which is updating the

272
00:16:11,005 --> 00:16:15,435
versions over the ensuring that you're
versioning it, and you are delivering

273
00:16:15,435 --> 00:16:17,515
the right model to the right devices.

274
00:16:17,515 --> 00:16:20,875
So Tesla actually did the
over the air updates model.

275
00:16:21,145 --> 00:16:21,985
Model updates.

276
00:16:22,285 --> 00:16:24,840
So what they did is they
routinely push AI updates.

277
00:16:25,340 --> 00:16:28,970
For example, the autopilot vision
and the driving behavior models,

278
00:16:29,270 --> 00:16:30,830
they go to the cars globally.

279
00:16:31,190 --> 00:16:35,270
So how they did it was they
used the secure OTA pipelines to

280
00:16:35,270 --> 00:16:38,240
validate and deploy the module
with rollback capabilities.

281
00:16:38,570 --> 00:16:43,170
So they have ensured that for each model
that they are incrementing, they made

282
00:16:43,170 --> 00:16:45,130
the minor major or minor improvements.

283
00:16:45,580 --> 00:16:49,920
Without the customers going to
the store and making an update.

284
00:16:49,920 --> 00:16:53,790
So they basically, they did it over
the air, which means that the customer

285
00:16:53,790 --> 00:16:59,360
received the models just over the internet
and then it is much easier to deploy and

286
00:16:59,360 --> 00:17:04,370
reduces the time and efficiency of the
customer as well as the service centers.

287
00:17:04,370 --> 00:17:08,870
So this was, this is a very good
example of real time scenario

288
00:17:08,870 --> 00:17:10,700
of Tesla cars getting updated.

289
00:17:11,255 --> 00:17:15,005
So the last, oh no, the la
the model compression, right?

290
00:17:15,245 --> 00:17:18,425
So Google actually did the model
compression for their mobile

291
00:17:18,425 --> 00:17:20,105
net on their Android devices.

292
00:17:20,505 --> 00:17:24,795
So what they did was they did the
mobile net family, which is basically

293
00:17:24,795 --> 00:17:29,445
just targeted for a lightweight
mobile inferencing scenario.

294
00:17:29,685 --> 00:17:33,645
So you can do image classification in
with the compute power that you have

295
00:17:33,645 --> 00:17:36,675
for the mobile or object detection.

296
00:17:36,960 --> 00:17:41,220
So how they did it, basically they
used quantitation as well as pruning

297
00:17:41,220 --> 00:17:45,870
techniques, which we talked about to
reduce the model size and also use the

298
00:17:45,870 --> 00:17:50,980
that to retain the accuracy in which they
wanna detect or image or object detection.

299
00:17:51,310 --> 00:17:53,020
So the result was real time.

300
00:17:53,020 --> 00:17:56,460
You were able to get AI
features in your phone instantly

301
00:17:56,460 --> 00:17:57,930
without the use of internet.

302
00:17:58,285 --> 00:18:02,695
So you can just even in Europe plane,
you can do all the image detection

303
00:18:02,695 --> 00:18:05,175
and object detection pretty easily.

304
00:18:06,045 --> 00:18:07,605
So this is an example.

305
00:18:07,605 --> 00:18:09,625
Very good example for model compression.

306
00:18:09,625 --> 00:18:13,165
So let's go to the last example, which
is the cloud managed AI pipeline.

307
00:18:13,435 --> 00:18:16,925
So Siemen actually predicted
they did predicted.

308
00:18:17,555 --> 00:18:19,175
Maintenance in the factories.

309
00:18:19,175 --> 00:18:25,035
So Simons, what they did was they
deployed the AI across factory flows.

310
00:18:25,335 --> 00:18:29,745
So whenever the machines were down, or
before even getting to that state, they

311
00:18:29,745 --> 00:18:34,605
were able to predict it and they used
Azure iot as well as ML pipelines to

312
00:18:34,605 --> 00:18:39,285
train models in the cloud and also deploy
the different versions that are needed.

313
00:18:39,595 --> 00:18:42,925
By this way, they reduce the downtime
because what happens is whenever

314
00:18:42,925 --> 00:18:46,905
the the machines go on maintenance
mode, it takes forever to come back

315
00:18:46,905 --> 00:18:51,175
up and it creates a lot of downtime,
cost inconvenience to the customer.

316
00:18:51,175 --> 00:18:56,095
So it, they reduced all that and
the machine downtime was reduced

317
00:18:56,095 --> 00:18:57,955
to 30%, which is pretty nice.

318
00:18:58,225 --> 00:19:02,455
So now they're also using the cloud
to continuously retain models.

319
00:19:02,555 --> 00:19:05,615
For all the edge collected
for the edge scenarios.

320
00:19:05,825 --> 00:19:09,215
So they were able to collect the
telemetry from the edge and use that

321
00:19:09,215 --> 00:19:14,075
in the cloud and use the pipelines to
make this seamless so that they use

322
00:19:14,075 --> 00:19:16,965
both the edge in the cloud scenario.

323
00:19:17,235 --> 00:19:22,295
So this way they were able to reduce
the maintenance as well as make sure

324
00:19:22,295 --> 00:19:26,645
that they are getting real time updates
of their devices using the AI models.

325
00:19:27,140 --> 00:19:31,460
So these are some of the best practices
and we saw some real time use cases.

326
00:19:31,910 --> 00:19:36,890
So that's pretty much what I had and the
how AI is actually re revolutionizing

327
00:19:36,920 --> 00:19:39,170
iot in the edge space is amazing.

328
00:19:39,470 --> 00:19:44,400
And hopefully this gave you some
insights into the into the background

329
00:19:44,400 --> 00:19:47,640
of IOT and its solution and what
is being followed right now.

330
00:19:48,000 --> 00:19:48,930
Thank you so much.

