1
00:00:00,190 --> 00:00:04,939
Hey everyone, I'm Amar and I'm thrilled
to welcome you to this session.

2
00:00:05,290 --> 00:00:09,179
as a senior manager of software
engineering at Lowes, I have had the

3
00:00:09,180 --> 00:00:14,490
privilege of, leading multiple teams, that
design and deliver, platforms, that are

4
00:00:14,490 --> 00:00:17,109
capable of, scaling, to millions of users.

5
00:00:17,159 --> 00:00:20,520
but today, I'm going to dive
into something even more

6
00:00:20,520 --> 00:00:21,970
exciting and transformative.

7
00:00:22,800 --> 00:00:27,760
how to build a resilient, real time
analytics pipeline, a topic, that's

8
00:00:27,970 --> 00:00:32,440
redefining the way the business operates
in our, fast paced, digital world.

9
00:00:32,940 --> 00:00:34,190
Think about this, right?

10
00:00:34,490 --> 00:00:38,290
What if, you could process over
a million events per second while

11
00:00:38,310 --> 00:00:40,590
maintaining near perfect uptime?

12
00:00:41,420 --> 00:00:46,710
imagine, the competitive edge, that the,
that your business brings, whether it is

13
00:00:46,710 --> 00:00:51,860
delivering, lightning fast personalized
recommendations or, enabling real

14
00:00:51,860 --> 00:00:55,700
time decision making or probably for
that matter, proactively addressing,

15
00:00:55,820 --> 00:00:57,720
customer needs, before even they arise.

16
00:00:58,220 --> 00:01:02,960
In this session, I'm going to uncover
the cutting edge architectures

17
00:01:02,990 --> 00:01:07,000
and then strategies that some
of the leading companies are

18
00:01:07,010 --> 00:01:09,350
using to achieve these goals.

19
00:01:09,680 --> 00:01:13,250
I'll share real time and
real world success stories,

20
00:01:13,580 --> 00:01:14,584
walk you through the process.

21
00:01:14,915 --> 00:01:18,625
the technical, challenges and the
technical design and probably the

22
00:01:18,655 --> 00:01:24,115
solution and provide a roadmap to help you
implement, these systems in your projects.

23
00:01:24,665 --> 00:01:28,685
whether you are here to supercharge your
technical expertise, gathering actionable

24
00:01:28,685 --> 00:01:33,205
insights to tackle your current challenges
or simply explore, the incredible

25
00:01:33,205 --> 00:01:36,885
possibilities in real time analytics,
absolutely are in the right place.

26
00:01:37,105 --> 00:01:38,435
Let's, go on.

27
00:01:38,935 --> 00:01:43,194
so, before, getting on to the
subsequent, slides, I would like to

28
00:01:43,194 --> 00:01:46,894
say that, you could connect me on
LinkedIn at, Amarnath Imadisetty.

29
00:01:47,394 --> 00:01:50,734
so, let's get on to
the subsequent, slides.

30
00:01:51,234 --> 00:01:54,364
Let's try to understand what's
the customer expectations.

31
00:01:55,084 --> 00:02:00,824
So, as I said, in today's digital era,
80, 82 percent of the customers do expect

32
00:02:00,904 --> 00:02:03,534
instant and personalized responses.

33
00:02:03,654 --> 00:02:06,274
this has an absolute,
competitive advantages.

34
00:02:06,334 --> 00:02:08,924
Organizations who are
using real time analytics.

35
00:02:09,424 --> 00:02:14,344
experience a 26 percent boost
in customer satisfaction matrix.

36
00:02:14,414 --> 00:02:16,904
And also this led to 3.

37
00:02:16,944 --> 00:02:20,194
2 times higher customer lifetime value.

38
00:02:20,694 --> 00:02:25,874
So let's get on to, the core
benefits of, the real time analytics.

39
00:02:26,714 --> 00:02:29,074
so real time analytics has a cap.

40
00:02:29,094 --> 00:02:34,304
I think when we talk about real time
analytics, the primary thing that to look

41
00:02:34,304 --> 00:02:38,484
after is, the processing speed or the
processing power at which we process it.

42
00:02:39,334 --> 00:02:43,044
real time analytics, Has to
handle, a huge amount of data.

43
00:02:43,064 --> 00:02:46,304
So probably I'm talking 1
million events per second, right?

44
00:02:46,824 --> 00:02:52,594
And also, the instant personalization
is something that, is embedded as part

45
00:02:52,594 --> 00:02:54,164
of the real time analytics, right?

46
00:02:54,604 --> 00:02:58,784
so it allows business to tailor customer
experiences in a blink of an eye.

47
00:02:58,999 --> 00:02:59,319
Right.

48
00:02:59,939 --> 00:03:03,049
and probably that also boils
down to the business agility.

49
00:03:03,169 --> 00:03:07,649
when I talk about business agility,
companies using real time analytics can,

50
00:03:07,679 --> 00:03:09,469
adapt to changes within milliseconds.

51
00:03:10,269 --> 00:03:14,919
Let's understand, the, real time,
Scenarios where the companies are using.

52
00:03:15,499 --> 00:03:19,629
imagine a large e commerce site, and I
hope most of you or all of you might have

53
00:03:19,629 --> 00:03:21,509
shopped somewhere at that time at Amazon.

54
00:03:21,509 --> 00:03:23,399
I'm taking Amazon as an example here.

55
00:03:24,159 --> 00:03:26,729
imagine the number of
transactions that Amazon do get.

56
00:03:26,869 --> 00:03:31,189
when a customer clicks on a product,
adds it to their cart, or searches for

57
00:03:31,189 --> 00:03:35,509
something, or, view the cart, or maybe
making a payment or search for the deals.

58
00:03:36,069 --> 00:03:39,159
So, All this data has to be
collected instantaneously.

59
00:03:39,659 --> 00:03:43,889
With the power of real time processing,
the system can track millions of these

60
00:03:44,279 --> 00:03:50,369
events happening at the same time, and
across the globe at the same time, right?

61
00:03:50,369 --> 00:03:53,289
Ensuring a smooth and fast
experience for every user.

62
00:03:53,794 --> 00:03:57,904
let's assume, if you have ever been to
Amazon and then probably, if you are

63
00:03:58,024 --> 00:04:03,494
shopping for say, a refrigerator and the
subsequent browse history will, let you

64
00:04:03,514 --> 00:04:07,294
capture this refrigerator section and then
start recommending the products for you.

65
00:04:07,444 --> 00:04:07,664
Right.

66
00:04:08,174 --> 00:04:10,774
even, when it comes to instant
personalization, right?

67
00:04:11,034 --> 00:04:14,894
Some other, classic example, most of us
might have watched Netflix or Amazon.

68
00:04:15,354 --> 00:04:19,474
for instance, when we, when we open
Amazon, the platform quickly analyzes

69
00:04:19,554 --> 00:04:24,009
the past viewing history and suggests
the shows or movies that, We might like,

70
00:04:24,719 --> 00:04:28,969
similarly an online shopping that I was
just talking about Amazon, as we just

71
00:04:28,979 --> 00:04:33,009
browse through the product, we will all
noticed personalized recommendations

72
00:04:33,039 --> 00:04:38,069
based on, our preferences, such as people
who bought this also bought this all

73
00:04:38,109 --> 00:04:39,989
made possible by real time analytics.

74
00:04:40,829 --> 00:04:44,519
when it comes to business agility,
as I was talking about, this.

75
00:04:45,124 --> 00:04:49,734
Empowers the business to adopt
or, to the situation in sub

76
00:04:49,734 --> 00:04:52,034
milliseconds to milliseconds.

77
00:04:52,034 --> 00:04:55,844
For example, let's take ride sharing
apps like Uber or Lyft, right?

78
00:04:55,954 --> 00:05:00,894
they do adjust prices dynamically based
on the demand, traffic conditions, and

79
00:05:00,894 --> 00:05:02,964
number of, available drivers, right?

80
00:05:03,694 --> 00:05:07,839
when demand, spikes, during the
rush hour, or probably a special

81
00:05:07,839 --> 00:05:09,399
event or an event that's happening.

82
00:05:09,949 --> 00:05:14,149
the system quickly increases, prices to
encourage more drivers to get on the road.

83
00:05:14,159 --> 00:05:16,969
This kind of fast decision
making help business, stay,

84
00:05:17,439 --> 00:05:19,719
competitive and responsive, right?

85
00:05:19,769 --> 00:05:23,939
these benefits show how real time
analytics is just not a technical

86
00:05:24,079 --> 00:05:28,929
capability, but a game changer in creating
better, faster, and more personalized

87
00:05:28,979 --> 00:05:29,959
experiences for other customers.

88
00:05:30,459 --> 00:05:30,659
Right.

89
00:05:30,709 --> 00:05:35,159
As I just talked through, quickly,
so this slide talks about, how

90
00:05:35,179 --> 00:05:36,889
the major, retailers are doing it.

91
00:05:36,949 --> 00:05:37,169
And then.

92
00:05:37,989 --> 00:05:41,919
how the travel industry, does
the, dynamic pricing adjustment.

93
00:05:42,309 --> 00:05:43,359
Even it's the same thing.

94
00:05:43,419 --> 00:05:48,359
if you are, planning to, look for,
a flight ticket, so as more and more

95
00:05:48,359 --> 00:05:51,639
searches are more and more, clicks
through come in or more and more

96
00:05:51,659 --> 00:05:56,359
demand comes in, the price of the
flight, the, the ticket price at the

97
00:05:56,359 --> 00:05:58,359
flight gets increased or decreased.

98
00:05:59,139 --> 00:06:05,529
So, This empowers the business to
do a lot many things in instant

99
00:06:05,959 --> 00:06:10,369
personalization, probably the business
agility is one of the primary thing.

100
00:06:10,869 --> 00:06:16,619
So again, when it comes to the
bigger aspect of the real time

101
00:06:16,619 --> 00:06:20,979
analytics, I think we can certainly
help, this helps us in anticipating

102
00:06:20,979 --> 00:06:22,989
the required customer needs, right?

103
00:06:23,489 --> 00:06:26,479
So this gives what
exactly a customer needs.

104
00:06:26,519 --> 00:06:31,589
So let's assume, if we go to, Starbucks
and then they call your name as soon

105
00:06:31,589 --> 00:06:35,599
as they see you, and then probably tell
what is required, and then probably

106
00:06:35,599 --> 00:06:40,239
what goes along with you, you would
start looking to be a more associated

107
00:06:40,239 --> 00:06:45,549
customer with that particular store or
the, with that particular company, right?

108
00:06:45,809 --> 00:06:48,839
And when we talk about churn
reduction, I think probably the

109
00:06:48,839 --> 00:06:52,639
friction, that it creates, it
helps in proactively addressing the

110
00:06:52,649 --> 00:06:54,279
issues before even they escalate.

111
00:06:54,779 --> 00:06:58,194
So, this obviously helps
in, improving the, complete,

112
00:06:58,324 --> 00:06:59,914
customer, engagement journey.

113
00:07:00,464 --> 00:07:05,444
so let's deep dive into how do we design
and then, what are all the things,

114
00:07:05,494 --> 00:07:10,724
that are, required in order to build
the, the real time analytics pipeline.

115
00:07:10,924 --> 00:07:14,674
So primarily I'll just, go high level
and then start talking everything at,

116
00:07:14,684 --> 00:07:19,364
So, data collection is a place where
you start collecting, the data, right?

117
00:07:19,784 --> 00:07:23,404
as, as I was talking about, unless
you have a data to act upon, you would

118
00:07:23,404 --> 00:07:27,734
not be able to predict or you would
not be able to act upon it, right?

119
00:07:27,874 --> 00:07:30,884
The first step in the process is to
collect the data and then collect it.

120
00:07:30,884 --> 00:07:31,934
Process the data.

121
00:07:31,934 --> 00:07:35,324
When I talk about process, the
processing, the data, probably it

122
00:07:35,324 --> 00:07:39,444
could be cleansing the data or it could
be on the lines of izing the data,

123
00:07:39,924 --> 00:07:42,084
making the value insights out of it.

124
00:07:42,084 --> 00:07:45,864
And then we, obviously we need a
data storage to store the data.

125
00:07:46,524 --> 00:07:46,864
and then.

126
00:07:47,334 --> 00:07:53,354
The delivery, the data that is
being, synthesized, needs to be, put

127
00:07:53,354 --> 00:07:57,374
in a business acumen or a business
format where there can be some

128
00:07:57,374 --> 00:08:00,374
actionable insights around it, which
is the data delivery component.

129
00:08:00,794 --> 00:08:05,084
And all boils down to your monitoring
and resilience, where unless we

130
00:08:05,084 --> 00:08:09,574
have, and monitoring and resilience
of the overall application in place.

131
00:08:10,034 --> 00:08:14,354
the reliability, stability, the accuracy
of the data will not be accurate.

132
00:08:14,364 --> 00:08:18,454
So monitoring plays a bigger role
in ensuring that the data quality,

133
00:08:18,454 --> 00:08:22,784
data governance, data stability, data
accuracy, comes down into the picture.

134
00:08:23,284 --> 00:08:27,444
So let's, deep dive into, the first
section of data source, right?

135
00:08:27,494 --> 00:08:31,384
So when I talk about, data collection,
so there are two aspects of it.

136
00:08:31,484 --> 00:08:32,559
what is the data source?

137
00:08:32,719 --> 00:08:34,869
And then, what do we capture, right?

138
00:08:35,479 --> 00:08:39,949
So if you are taking an example of a
data source, this is as I was talking

139
00:08:39,949 --> 00:08:43,299
about, this is starting point of
any real time analytics pipeline,

140
00:08:43,799 --> 00:08:45,269
where the data originates, right?

141
00:08:45,839 --> 00:08:50,209
So probably data sources are the
platforms or probably the systems or

142
00:08:50,309 --> 00:08:54,639
probably devices generating information
that needs to be analyzed, right?

143
00:08:54,819 --> 00:08:59,259
so these are most of the systems that
we do use almost daily, in our, life.

144
00:08:59,279 --> 00:09:02,649
So probably like most of us
will be using websites, right?

145
00:09:02,649 --> 00:09:05,979
so what do we capture and
where do we capture, right?

146
00:09:06,029 --> 00:09:10,149
So in terms of the places of the
availability, it comes down to,

147
00:09:10,199 --> 00:09:11,589
some of these data sources, right?

148
00:09:11,989 --> 00:09:16,449
So websites where we do capture user
activities, like what the user has,

149
00:09:16,499 --> 00:09:20,209
viewed, or clicked, or probably like
what the user has submitted, right?

150
00:09:20,209 --> 00:09:20,804
Okay.

151
00:09:21,174 --> 00:09:23,254
And, the second aspect is mobile apps.

152
00:09:23,254 --> 00:09:26,904
I think mobile apps helps us in
getting user interactions like

153
00:09:26,934 --> 00:09:31,064
app navigations, probably product
searches and, purchases, right?

154
00:09:31,934 --> 00:09:35,814
I would, this, in the last, couple of
years, I think, or maybe I would say in

155
00:09:35,814 --> 00:09:41,254
the last, 10 to 15 years, IOT devices
has become a part of our life, right?

156
00:09:41,314 --> 00:09:45,364
So it could be on home sensors, probably
the wearables or maybe industrial

157
00:09:45,364 --> 00:09:50,634
equipment that continuously generate
the data, telemetry data, right?

158
00:09:51,424 --> 00:09:56,424
For the application health or
monitoring the activity around it.

159
00:09:56,924 --> 00:10:02,704
And so, at times we also need to parse
through the logs, the server that generate

160
00:10:02,704 --> 00:10:04,584
logs or probably application logs.

161
00:10:04,989 --> 00:10:08,779
that track the system performance,
errors, and maybe the user

162
00:10:08,779 --> 00:10:10,399
interactions too, right?

163
00:10:10,539 --> 00:10:13,109
And we all are equipped with social media.

164
00:10:13,949 --> 00:10:18,659
So, I think real time opinions of
the customers matter a lot and then

165
00:10:18,659 --> 00:10:21,419
the trends that are happening and
the mentions from the platforms

166
00:10:21,439 --> 00:10:23,149
like Twitter, Meta, or Instagram.

167
00:10:23,149 --> 00:10:30,854
And it comes down to some of this payment
system where the transactional data that

168
00:10:30,854 --> 00:10:35,634
records the real time purchases, refunds,
or the subscription renewals that we have.

169
00:10:36,314 --> 00:10:40,774
So, there's numerous places the
data sources can is available.

170
00:10:41,064 --> 00:10:44,974
so these are all the places that,
that are widely used to derive some

171
00:10:44,974 --> 00:10:46,544
kind of a real time analytics, right?

172
00:10:47,504 --> 00:10:49,104
yes, the data is available.

173
00:10:49,124 --> 00:10:50,524
Now, how do we capture it?

174
00:10:50,944 --> 00:10:51,174
Right?

175
00:10:51,794 --> 00:10:56,714
so as I talked about data in a, in
a real time, you, you have to fairly

176
00:10:56,834 --> 00:11:00,134
understand, or we all have to fairly
understand, it's a continuous mechanism.

177
00:11:00,134 --> 00:11:01,134
It's not a bad job.

178
00:11:01,634 --> 00:11:05,124
As soon as an event happens,
you are acting upon it or the

179
00:11:05,124 --> 00:11:06,684
system is acting upon it, right?

180
00:11:07,184 --> 00:11:13,144
So data is collected continuously
and transmitted in real time, right?

181
00:11:14,064 --> 00:11:20,094
Some of these things can be achieved
by using technologies like APIs or

182
00:11:20,094 --> 00:11:23,534
probably like the event trackers
or data streaming frameworks.

183
00:11:24,034 --> 00:11:28,444
Each, data source, that I just talked
about, contributes to unique insights,

184
00:11:28,504 --> 00:11:31,944
that helps business better understand
their operations and customers, right?

185
00:11:32,524 --> 00:11:35,154
probably let's, take an
example of healthcare.

186
00:11:35,204 --> 00:11:40,014
maybe, they might not be as interested,
on the payment systems compared to,

187
00:11:40,114 --> 00:11:43,164
the, website or the transactions
that they are dealing with.

188
00:11:43,664 --> 00:11:46,144
When it comes to the retail, I
think they might be more interested

189
00:11:46,144 --> 00:11:49,394
on social media to see how the
guest purchases are happening.

190
00:11:49,404 --> 00:11:52,694
And then probably, the websites
at which they are browsing or the

191
00:11:52,714 --> 00:11:55,964
competitive, sales that are happening,
they need all that information.

192
00:11:56,494 --> 00:12:00,484
So depends on the business,
at which we are in, probably,

193
00:12:00,534 --> 00:12:01,784
the source, various matters.

194
00:12:02,284 --> 00:12:07,759
Um, so some examples I can certainly
talk, through as a, real time example.

195
00:12:07,759 --> 00:12:11,389
I think as we just talked about a
mission, every time a user, searches

196
00:12:11,389 --> 00:12:14,249
for a product, clicks on an item.

197
00:12:14,409 --> 00:12:16,699
So, and then add an item to the cart.

198
00:12:17,334 --> 00:12:22,304
and then probably go to the payment page
and then complete a purchase, right?

199
00:12:23,174 --> 00:12:28,674
So in this whole journey, so the
data flows continuously, into the,

200
00:12:28,724 --> 00:12:33,164
real time, analytics, allowing the
system to act immediately, right?

201
00:12:33,434 --> 00:12:35,749
As soon as, this process is happening.

202
00:12:35,759 --> 00:12:40,559
We will start providing the
recommendations and then track inventory

203
00:12:40,919 --> 00:12:45,549
and then analyze customer preferences
for the future recommendations.

204
00:12:46,019 --> 00:12:50,819
and say simple example, for instance,
if the user, was looking for the

205
00:12:50,819 --> 00:12:52,449
wireless heads, headphones, right?

206
00:12:52,949 --> 00:12:57,199
So, this particular real time analytics
helps you to collect, I mean, collects

207
00:12:57,209 --> 00:13:01,709
this input and then it immediately
processes it to suggest what's a

208
00:13:01,709 --> 00:13:06,759
related product or specific brands
or accessories associated with it.

209
00:13:07,599 --> 00:13:13,344
So sometimes, I think in these days, I
think most of these larger, companies

210
00:13:13,344 --> 00:13:18,904
do have, the mobile app based and
then, the, web based application.

211
00:13:18,934 --> 00:13:25,094
So, sometimes we have to understand, the
ty in the session between the mobile app.

212
00:13:25,744 --> 00:13:27,704
And, the web is highly required.

213
00:13:27,774 --> 00:13:32,104
Let's assume that, we went to Amazon app
and then tried searching for a product.

214
00:13:32,314 --> 00:13:35,484
We wanted to show the recommendations
when we go to Amazon web as well.

215
00:13:36,084 --> 00:13:37,554
So in the real time example.

216
00:13:37,724 --> 00:13:43,684
So this, when we start capturing the data,
understanding, the business, operation

217
00:13:43,744 --> 00:13:48,594
and business agility and business need,
the data will be captured and the data

218
00:13:48,594 --> 00:13:49,964
will be transmitted to the system.

219
00:13:50,514 --> 00:13:51,144
so.

220
00:13:51,454 --> 00:13:55,214
So, the primary step, in the
complete, data processing is,

221
00:13:55,264 --> 00:13:58,114
I mean, there's a bread and
butter of the application, right?

222
00:13:58,484 --> 00:13:59,354
We have data.

223
00:13:59,454 --> 00:14:03,784
If you cannot make the use of the data,
it does not make a lot of sense to

224
00:14:03,784 --> 00:14:05,934
derive, the actionable inserts out of it.

225
00:14:06,574 --> 00:14:09,834
so in this particular step, the raw
data is being collected, from the

226
00:14:09,834 --> 00:14:11,424
different sources as I talked about.

227
00:14:11,814 --> 00:14:17,274
analyzed, transformed, cleansed,
cleaned, into something useful.

228
00:14:17,774 --> 00:14:24,594
So this step makes the data to be
more understandable and actionable

229
00:14:24,714 --> 00:14:25,994
for real time data making.

230
00:14:26,624 --> 00:14:31,924
let's assume an example of, we
wanted to do what a customer expects.

231
00:14:31,974 --> 00:14:35,534
Unless the customer has a continuity
pattern, we would not be able to

232
00:14:36,489 --> 00:14:38,379
derive the real time insights.

233
00:14:38,699 --> 00:14:43,049
So if I am putting in a real time
example, let us assume that, we wanted

234
00:14:43,049 --> 00:14:47,889
to understand, what does Amazon make,
in terms of revenue tomorrow, right?

235
00:14:48,449 --> 00:14:53,139
Unless we have the historical data,
which is continuous, which is actionable,

236
00:14:53,149 --> 00:14:57,729
which makes the average, which makes the
mean, which makes some statistical data

237
00:14:57,759 --> 00:15:01,894
around, you know, the historical data,
we would not be able to act upon it.

238
00:15:02,134 --> 00:15:06,314
So, as soon as we start on the
day one, we might not be able to

239
00:15:06,354 --> 00:15:08,284
derive lot many insights around it.

240
00:15:08,794 --> 00:15:14,679
Yes, we can do some geo based and few
other attributes, but A continuity or

241
00:15:14,819 --> 00:15:19,729
a, a, historical approach is always
required in order to make this,

242
00:15:19,839 --> 00:15:21,289
real time decision making, right?

243
00:15:21,719 --> 00:15:25,329
during this step, we will understand
whether this data can be actionable

244
00:15:25,329 --> 00:15:27,019
or this data will not be actionable.

245
00:15:27,844 --> 00:15:28,024
Right.

246
00:15:28,794 --> 00:15:32,804
so, what exactly happens,
during data processing, right?

247
00:15:32,864 --> 00:15:37,454
So when the data enters into our
system, it, it might be messy or

248
00:15:37,504 --> 00:15:40,344
probably pretty unstructured, right.

249
00:15:40,344 --> 00:15:43,944
Because as I said, we can get
the data from website, sorry,

250
00:15:43,944 --> 00:15:47,484
mobile apps, and then probably
some payment systems, IOT devices.

251
00:15:47,484 --> 00:15:47,524
Yeah.

252
00:15:48,429 --> 00:15:51,039
we have numerous, data sources
that we are getting it from.

253
00:15:51,329 --> 00:15:52,729
Each has its own data structure.

254
00:15:52,759 --> 00:15:54,469
Sometimes it might be too much of data.

255
00:15:54,609 --> 00:15:57,239
Sometimes it is too less of a data, right?

256
00:15:57,269 --> 00:16:02,019
When it comes to processing, it primarily
involves, data cleaning, as I said,

257
00:16:02,429 --> 00:16:08,159
during this, phase, we will try to, fix,
the errors, probably, duplicated data.

258
00:16:08,259 --> 00:16:13,289
And, and we also check whether the data
that we have received is complete or not.

259
00:16:14,289 --> 00:16:20,309
say, like, simple example, if, we we get,
an identical purchase from two sources,

260
00:16:20,419 --> 00:16:22,369
the system has to keep only one, right?

261
00:16:22,889 --> 00:16:26,209
yes, there are times where we might
need the duplicated data for some

262
00:16:26,299 --> 00:16:30,279
actionable insights, but many a times,
when we talk about, making some of these

263
00:16:30,299 --> 00:16:34,479
real time decision making, the data
duplication is, can be avoided, right?

264
00:16:35,009 --> 00:16:39,729
and data transformation, typically
transform, into the format that,

265
00:16:39,939 --> 00:16:44,129
is required in the subsequent steps
around, for data visualization

266
00:16:44,129 --> 00:16:45,659
or maybe a data delivery aspect.

267
00:16:46,209 --> 00:16:51,599
so a simple example was, I think we,
as I talked about, like the clickstream

268
00:16:51,629 --> 00:16:55,349
data, the user data that is being
captured on the website can be translated

269
00:16:55,349 --> 00:16:59,189
into, something like, what's the most
clicked product in the last 10 minutes.

270
00:16:59,689 --> 00:17:02,669
And at this point of time, it also
involves something like analysis and

271
00:17:02,669 --> 00:17:08,149
insights, as I just talked about, is
this data can be used or not, right?

272
00:17:08,229 --> 00:17:11,439
something like we can use some kind
of an algorithm or probably like some

273
00:17:11,439 --> 00:17:15,139
kind of a machine learning models to
generate insights around it, right?

274
00:17:15,604 --> 00:17:19,954
Predicting which products a
user might, mostly like to

275
00:17:19,954 --> 00:17:21,584
buy based on, their behavior.

276
00:17:22,584 --> 00:17:25,284
so, we talked about what
are the steps involved.

277
00:17:25,284 --> 00:17:27,964
Just I'm trying to run down data
cleaning, data transformation,

278
00:17:27,964 --> 00:17:29,604
and probably analysis and insight.

279
00:17:29,734 --> 00:17:33,124
So, what are the industry
tools, looks like, right?

280
00:17:33,184 --> 00:17:37,760
So, I think when it comes to
data streaming, without any say,

281
00:17:37,760 --> 00:17:41,474
I think Apache Flink or Apache
Spark are pioneers in this.

282
00:17:41,659 --> 00:17:44,659
system when it comes to open
source, framework, right?

283
00:17:44,729 --> 00:17:48,379
So it can handle large
amounts of data in real time.

284
00:17:48,419 --> 00:17:52,299
I have, tested myself and
I'm very well, aware of it.

285
00:17:52,369 --> 00:17:56,519
And, there are times where, I have, seen
a use cases where, we had to process

286
00:17:56,519 --> 00:18:00,389
more than million records, per second,
and then I did not have any problem.

287
00:18:00,619 --> 00:18:00,769
Right.

288
00:18:01,589 --> 00:18:06,959
and, when it comes to AWS, AWS Lambda,
I think, it has the capability to

289
00:18:07,449 --> 00:18:11,169
execute, small tasks, quickly, as
and when new data comes in, right.

290
00:18:11,249 --> 00:18:15,939
It will be quite helpful when it comes to,
the delta variations that we talk about.

291
00:18:16,439 --> 00:18:21,759
Um, and another thing, Google Dataflow,
I think, it process, streams of data,

292
00:18:21,939 --> 00:18:24,929
from various sources and transforms
them, similar to a full fledged

293
00:18:24,959 --> 00:18:26,829
product that's available from, Google.

294
00:18:27,719 --> 00:18:31,979
Um, and as I talked about,
why is this important, right?

295
00:18:32,109 --> 00:18:37,269
without data processing, the raw data
that is being collected is too much and

296
00:18:37,289 --> 00:18:39,209
too kinetic to understand for reuse.

297
00:18:39,259 --> 00:18:45,289
unless, we process it, organize it and
analyze the data, to extract meaningful

298
00:18:45,339 --> 00:18:49,519
patterns and trends, it will not be
considered for, the actionable insights.

299
00:18:50,019 --> 00:18:53,829
So, again, some real time example
that I can talk about, right?

300
00:18:53,869 --> 00:18:57,559
we just talked about a user searched
for a product and within milliseconds,

301
00:18:57,609 --> 00:19:01,359
the system process this data to update
the personalized recommendation.

302
00:19:02,209 --> 00:19:06,319
As simple as that, when we try to buy
a washer, the subsequent recommendation

303
00:19:06,329 --> 00:19:11,019
should be more on the dryer, but not like,
say, some toilet or something, right?

304
00:19:11,429 --> 00:19:14,739
So, it also analyzes how many
searches are happening at the same

305
00:19:14,769 --> 00:19:19,799
time, to adjust inventory and price,
probably recommendations, et cetera.

306
00:19:20,639 --> 00:19:24,849
so, like, in the case of, the Uber ride
or Lyft ride that we talked about or

307
00:19:24,909 --> 00:19:29,529
the ride sharing, any of the popular
ride sharing apps, the app process the

308
00:19:29,529 --> 00:19:33,669
data like what's the rider's location,
what's the driver availability, how does

309
00:19:33,669 --> 00:19:38,949
the traffic look, look like and what
is the, event process, any subsequent

310
00:19:38,959 --> 00:19:44,809
events are happening, the traffic volume,
climatic conditions and probably and,

311
00:19:44,909 --> 00:19:46,699
define the estimated phase, right?

312
00:19:47,129 --> 00:19:53,309
So it has to process all of it, make
a reasonable insight, and, in majority

313
00:19:53,309 --> 00:19:57,869
of the cases, we see that, this will
help, the process happens in real time,

314
00:19:58,329 --> 00:20:03,849
ensuring, we get, pretty accurate, ETAs
and probably, the pricing as well, right?

315
00:20:04,659 --> 00:20:08,799
and when it comes to, some of these
financial institute, institutions where,

316
00:20:08,899 --> 00:20:13,839
During banking and fraud detection
companies, when a transaction occurs, the

317
00:20:13,869 --> 00:20:19,619
data processing checks for, say, probably
particular patterns to detect a fraud.

318
00:20:20,119 --> 00:20:23,479
if you go, this is where it happens
and I think, Many of us would have

319
00:20:23,479 --> 00:20:27,999
encountered if we have used some of our
credit card in a shop that we have not

320
00:20:28,009 --> 00:20:34,069
been or probably tagged for an unusual
spending, it blocks then and there.

321
00:20:34,709 --> 00:20:40,679
So at this point of time, the system
detected our user pattern, the location

322
00:20:40,679 --> 00:20:44,189
at which it happened, and the type
of transaction that is happening

323
00:20:44,199 --> 00:20:45,889
and the amount at which it happened.

324
00:20:46,329 --> 00:20:50,859
And probably it also has to churn through
what is the limits that we have kept in.

325
00:20:51,354 --> 00:20:57,914
So, this has absolutely empowered, like,
there's too many numerous examples, right?

326
00:20:57,984 --> 00:21:03,814
This has empowered, the banking and,
many industries to act on the real time,

327
00:21:04,754 --> 00:21:06,254
which has a great potential, right?

328
00:21:06,324 --> 00:21:11,454
So the data processing, plays a huge role
to understand what are all the attributes

329
00:21:11,454 --> 00:21:15,374
and how do I translate the data, right?

330
00:21:15,874 --> 00:21:19,934
So again, when it comes to data storage,
I think pretty much it is, the storage,

331
00:21:20,034 --> 00:21:23,184
where the processed data is saved, right?

332
00:21:23,604 --> 00:21:28,074
so it can be accessed, quickly or in
near real time as and when needed.

333
00:21:28,554 --> 00:21:32,974
Again, every section of this is quite
crucial, Pat, because it ensures the

334
00:21:32,974 --> 00:21:38,844
system has a reli and it has to be the
fault tolerant, HA, high availability,

335
00:21:38,854 --> 00:21:44,094
ZEO located because, it's a reliable
place where the data is available to

336
00:21:44,124 --> 00:21:47,724
act upon and probably retrieve the
infor information without any delays.

337
00:21:48,224 --> 00:21:48,534
Right.

338
00:21:48,584 --> 00:21:52,084
what happens exactly at the,
data storage, situation?

339
00:21:52,784 --> 00:21:57,084
once the data has been processed, it
needs to be stored, in a way that, it

340
00:21:57,374 --> 00:21:59,424
supports or empowers real time access.

341
00:21:59,679 --> 00:21:59,859
Right.

342
00:22:00,389 --> 00:22:05,399
data must be accessible, instantly
and then probably for any, analytics

343
00:22:05,399 --> 00:22:07,149
and maybe the decision making.

344
00:22:07,219 --> 00:22:07,479
Right.

345
00:22:08,109 --> 00:22:12,999
And as I was talking about, you understand
the volume at which it comes like, so

346
00:22:13,049 --> 00:22:16,889
like if you take a credit card example,
I just talked about, there are numerous

347
00:22:16,899 --> 00:22:20,409
transactions, numerous ways, numerous
credit cards, numerous locations happens.

348
00:22:21,249 --> 00:22:26,809
so the storage has to be able to
handle, extremely high volumes of data.

349
00:22:27,019 --> 00:22:27,319
Right.

350
00:22:27,729 --> 00:22:31,749
so it can, retrieve fastly,
it can act upon fastly, right?

351
00:22:32,279 --> 00:22:38,229
And as, as it boils down to, unless
you have a reliable data storage, it

352
00:22:38,289 --> 00:22:40,664
has to be stored, in a secure, manner.

353
00:22:40,664 --> 00:22:43,244
When I talk about secure
manner, I'm not talking about.

354
00:22:43,564 --> 00:22:46,134
PCI, HIPAA and all depends on
the industry and depends on

355
00:22:46,134 --> 00:22:47,394
the data that we are storing.

356
00:22:47,394 --> 00:22:51,494
This all boils down when I talk about
security, it has to be, HA, high

357
00:22:51,494 --> 00:22:55,644
availability, multiple zones, and then
probably it should have, data duplication.

358
00:22:55,644 --> 00:22:58,704
And some of this should happen in
case of, one data center on one

359
00:22:58,704 --> 00:23:02,164
system goes down, you have a big
backup data available to act upon.

360
00:23:02,714 --> 00:23:06,714
when it comes to, data storage,
typically talking about in, real time

361
00:23:06,714 --> 00:23:10,119
analytics, some of these classic,
things are, in memory stories.

362
00:23:10,559 --> 00:23:15,529
so, So data is stored, in memory,
like for a quick action, right?

363
00:23:15,529 --> 00:23:21,439
Let's, assume that an user was, trying
to, act, say, trying to add to cart.

364
00:23:21,479 --> 00:23:25,149
And then, the user was playing, in
that pace for a good amount of time.

365
00:23:25,679 --> 00:23:29,559
So at that point of time, we can
immediately act on that event and

366
00:23:29,559 --> 00:23:30,959
start throwing a promotion on it.

367
00:23:31,159 --> 00:23:33,729
If, if, if that's what the
business was looking for.

368
00:23:33,969 --> 00:23:34,179
Right.

369
00:23:34,579 --> 00:23:39,709
So, I think some of these classic,
in, in storage, or in memory,

370
00:23:39,759 --> 00:23:44,929
things for ultra fast access where,
it has a TTL based and all, Redis

371
00:23:44,929 --> 00:23:46,799
and memcache plays a bigger role.

372
00:23:47,169 --> 00:23:51,839
I think mostly, the, Redis and
Memcached is primarily used, to

373
00:23:51,839 --> 00:23:56,309
pull recommendations from the cache
data to respond instantly, right?

374
00:23:56,819 --> 00:24:00,719
because, As soon as we, get
onto the website, we want the

375
00:24:00,719 --> 00:24:02,309
quicker recommendations, right?

376
00:24:02,809 --> 00:24:07,189
So, and, when it comes to, another
set of, databases, I think NoSQL

377
00:24:07,189 --> 00:24:08,879
is another popular database.

378
00:24:09,289 --> 00:24:11,969
I think this is primarily
required for high scale and

379
00:24:11,969 --> 00:24:13,884
probably large scale data storage.

380
00:24:14,434 --> 00:24:17,764
where, we might have to
make, too many, conditions to

381
00:24:17,764 --> 00:24:20,214
derive, to the insert, right?

382
00:24:20,734 --> 00:24:26,394
say, I think Mongo and Cassandra
and DynamoDB, some of these are

383
00:24:26,424 --> 00:24:28,364
pretty popular NoSQL databases.

384
00:24:29,034 --> 00:24:32,834
a ride sharing app like, Uber,
probably, store the trip details.

385
00:24:33,024 --> 00:24:33,324
Right.

386
00:24:33,404 --> 00:24:38,194
that's what we see whenever we go to the
ride sharing app and, and, probably it

387
00:24:38,194 --> 00:24:43,264
also show the driver availability for a
quick, updates and probably retrieval.

388
00:24:43,804 --> 00:24:49,114
so, and, in, in, like with the, machine
learning and AI in boom, I think time

389
00:24:49,114 --> 00:24:51,654
series, predictions are, used, heavily.

390
00:24:51,734 --> 00:24:56,024
so, and one of the popular, I
think for real time, analytics is,

391
00:24:56,074 --> 00:24:58,054
using the time series databases.

392
00:24:58,054 --> 00:25:02,494
so, ideal, for the, data that
is, Changing over a time, right?

393
00:25:02,494 --> 00:25:06,099
When we talk about your, xaxis,
Xaxis derives your time, and then

394
00:25:06,099 --> 00:25:08,449
why access is your, attribute data.

395
00:25:08,779 --> 00:25:12,319
So let's assume that as I just
talked, about like one of the example,

396
00:25:12,389 --> 00:25:14,089
What does my company make tomorrow?

397
00:25:14,089 --> 00:25:16,189
What does my company
make day after tomorrow?

398
00:25:16,189 --> 00:25:18,649
What does my company make,
after three days, right?

399
00:25:18,669 --> 00:25:22,089
When I take about what does my company
make, it could be many attributes, right?

400
00:25:22,389 --> 00:25:23,709
What does my sales look like?

401
00:25:23,709 --> 00:25:25,079
What does my orders look like?

402
00:25:25,089 --> 00:25:26,829
Or maybe what does my store make it?

403
00:25:27,119 --> 00:25:31,289
So based on this data, many, Things
can be done, in terms of, I would

404
00:25:31,299 --> 00:25:33,749
be able to, project my inventory.

405
00:25:33,749 --> 00:25:37,009
I would be able to project the
volume of the data that is coming in.

406
00:25:37,679 --> 00:25:41,609
And I would be able to project the,
people that are entering into the store.

407
00:25:41,609 --> 00:25:44,499
So I will be able to equip
well in advance, right?

408
00:25:45,184 --> 00:25:50,004
so, some of these time series,
databases are, Influx is one of

409
00:25:50,044 --> 00:25:53,924
the popular DB and then, we have
TimescaleDB and then Druid is also

410
00:25:53,944 --> 00:25:56,694
another DB that we all refer to, right?

411
00:25:57,114 --> 00:26:01,374
and, so some of these, monitoring,
systems like Prometheus, use,

412
00:26:01,454 --> 00:26:06,114
time series gb, to store, server,
performance data, in real time.

413
00:26:06,854 --> 00:26:10,639
and, and another, thing when
it talks about, the real time

414
00:26:10,879 --> 00:26:14,569
analytics, say sometimes we wanted
to store the data at a larger level.

415
00:26:15,069 --> 00:26:17,699
and then, historical
data that I talked about.

416
00:26:17,749 --> 00:26:18,439
so in order for the.

417
00:26:18,939 --> 00:26:21,089
machine learning models
to accurately work.

418
00:26:21,139 --> 00:26:26,079
Sometimes we wanted to give a
lot of input data to accommodate

419
00:26:26,079 --> 00:26:29,909
seasonal, to accommodate climatic
situation, to accommodate how

420
00:26:29,929 --> 00:26:32,059
the journey of the system is.

421
00:26:32,109 --> 00:26:34,309
So we need large, data storages, right?

422
00:26:34,339 --> 00:26:36,959
At that point of time,
we, jump into, data lakes.

423
00:26:37,009 --> 00:26:40,669
so, this is where, either we can
store the raw data or processed

424
00:26:40,749 --> 00:26:45,649
data, if you wanted to do, Large data
processing or large data volumes at

425
00:26:45,649 --> 00:26:48,509
the, uh, in the future case, right?

426
00:26:48,909 --> 00:26:54,659
So Amazon S3, probably and Hadoop,
HDFS are, the, some of these, data

427
00:26:54,659 --> 00:26:56,009
lakes that are in the market, right?

428
00:26:56,884 --> 00:27:01,744
so, if you take, Netflix as an example,
it stores, probably the user data

429
00:27:01,744 --> 00:27:06,254
in, data lakes for later analysis,
such as, finding what's the long term

430
00:27:06,254 --> 00:27:07,894
viewing trends looks like, right?

431
00:27:08,444 --> 00:27:14,829
so what, when we talk about all
these, four, different styles of,

432
00:27:14,879 --> 00:27:20,159
data storage systems like in memory
or could be NoSQL or could be time

433
00:27:20,159 --> 00:27:21,819
series or could be a data lake.

434
00:27:21,819 --> 00:27:23,274
What are the key features?

435
00:27:23,774 --> 00:27:26,374
that we need to look after a
data storage system, right?

436
00:27:26,964 --> 00:27:29,014
it should have an absolute low latency.

437
00:27:29,194 --> 00:27:32,674
It does not matter what kind
of a DB that we use, right?

438
00:27:33,224 --> 00:27:37,434
it should have a fast read and
probably write, speeds as well

439
00:27:37,604 --> 00:27:38,984
for the real time accesses.

440
00:27:39,894 --> 00:27:46,974
and scalability ability to handle
the growing volume of traffic or

441
00:27:46,974 --> 00:27:52,354
growing data volume without slowing
down, on the picture and, fault

442
00:27:52,364 --> 00:27:53,704
tolerance, as I said earlier.

443
00:27:53,714 --> 00:27:57,224
so there is initially started
off, the data storage should

444
00:27:57,234 --> 00:27:58,954
not lose any data, right?

445
00:27:59,424 --> 00:28:01,874
Even if there is a
hardware failure, right?

446
00:28:01,874 --> 00:28:08,364
With all this it also boils down to
how flexible to access it, right?

447
00:28:08,874 --> 00:28:13,314
it should, do we need a structured data,
semi structured data or unstructured data?

448
00:28:13,814 --> 00:28:18,984
So, when we think about the data
storage, fault tolerancy, low latency,

449
00:28:19,044 --> 00:28:23,414
and then probably scalability and then
accessibility, plays a bigger role, right?

450
00:28:23,894 --> 00:28:28,669
So, this is what my, reading,
through some of how Amazon inventory

451
00:28:28,669 --> 00:28:30,159
data is being stored, right?

452
00:28:30,829 --> 00:28:35,629
so in, so the real time inventory,
data has been, like the, when we

453
00:28:35,629 --> 00:28:40,099
talk about an inventory, the item
inventory, is being stored in, DynamoDB.

454
00:28:40,249 --> 00:28:44,149
Amazon's DynamoDB, and then it
ensures, accurate, stock updates,

455
00:28:44,249 --> 00:28:46,029
for millions of products, right?

456
00:28:46,119 --> 00:28:49,999
When an item is purchased, the
system instantly updates, inventory

457
00:28:49,999 --> 00:28:54,279
level, so the customer only
sees what is in stock, right?

458
00:28:54,759 --> 00:28:58,609
so when it comes to, probably, Netflix
streaming recommendations, right?

459
00:28:59,429 --> 00:29:02,779
It stores data on, user viewing habits.

460
00:29:03,259 --> 00:29:06,849
Like, what are the last movies or the
shows that have been seen and what's

461
00:29:06,849 --> 00:29:12,279
the preferences in, in a distributed
DB like Cassandra, I mean, the data

462
00:29:12,279 --> 00:29:15,839
is being accessed in real time to
suggest, the movies or the shows

463
00:29:15,909 --> 00:29:18,179
are tailored to each user, right?

464
00:29:18,739 --> 00:29:23,199
IoT sensors, so smart homes, I think
most of us are equipped with smart

465
00:29:23,199 --> 00:29:27,079
homes in one or other way these
days, smart thermostats and cameras.

466
00:29:27,079 --> 00:29:31,869
send, DB, like InfluxDB, right?

467
00:29:32,469 --> 00:29:35,089
this has been stored and
processed to send alerts and

468
00:29:35,089 --> 00:29:36,729
then adjust, things in real time.

469
00:29:36,759 --> 00:29:41,169
I think, some of these thermostats
also are equipped to, store all these

470
00:29:41,169 --> 00:29:44,879
in near real time, especially some
of these, thermostats where it looks

471
00:29:44,879 --> 00:29:48,239
after the temperature and then it
tries to adjust what are we using,

472
00:29:48,239 --> 00:29:51,489
how are we using, echo things and then
it adjusts the temperatures, right.

473
00:29:52,039 --> 00:29:56,839
so, as we talked about, all these
aspects of, data collection and data

474
00:29:56,839 --> 00:30:01,589
processing, as much as every component is
important, data storage plays a huge role.

475
00:30:01,879 --> 00:30:06,369
This is, the place where we have equipped
or we have stored all this, data, right?

476
00:30:06,969 --> 00:30:11,404
without the efficient, data storage,
I think, even the best of the best

477
00:30:11,434 --> 00:30:15,284
real time processing systems that we
have built in terms of collecting the

478
00:30:15,284 --> 00:30:20,084
data, in terms of processing, it would
absolutely fail, because it would not

479
00:30:20,124 --> 00:30:25,384
be available, when required and then,
high latency would, frustrate the users,

480
00:30:25,864 --> 00:30:30,614
because most of the time when we open
some of these, tools for recommendations,

481
00:30:31,084 --> 00:30:33,334
the users do expect instant responses.

482
00:30:33,964 --> 00:30:38,084
and then the other challenge would
also be if the data is being lost,

483
00:30:38,144 --> 00:30:43,214
we would not be able to make accurate
reliability or, we cannot make accurate

484
00:30:43,234 --> 00:30:45,194
insights out of it, in the long term.

485
00:30:45,694 --> 00:30:45,954
Right.

486
00:30:46,534 --> 00:30:50,564
and, as we talked about a lot of
things about data storage, yes, we

487
00:30:50,574 --> 00:30:51,894
have the data storage available.

488
00:30:52,504 --> 00:30:57,214
How do we make the data storage
available to make some actionable

489
00:30:57,214 --> 00:30:58,584
insights or share it with the users?

490
00:30:58,844 --> 00:30:59,084
Right.

491
00:30:59,754 --> 00:31:03,814
so this is probably the final
step, though I have put monitoring

492
00:31:03,814 --> 00:31:05,204
and resilience as a thing.

493
00:31:05,254 --> 00:31:08,374
I think that should boil down
with every component of it, right?

494
00:31:08,874 --> 00:31:12,454
So when I talk about data delivery,
this is a final step, in the real

495
00:31:12,734 --> 00:31:17,034
time analytics pipeline, where, the
processed insights that is being stored.

496
00:31:17,389 --> 00:31:17,579
Right.

497
00:31:18,069 --> 00:31:20,509
associated with the users
or applications, right?

498
00:31:20,819 --> 00:31:26,449
This is a place where we share it
with the systems, applications, users

499
00:31:26,649 --> 00:31:28,519
to make actionable insights, right?

500
00:31:29,449 --> 00:31:32,949
so, what happens right
during the data delivery?

501
00:31:32,989 --> 00:31:38,904
I think in the simpler terms,
the data is made accessible to

502
00:31:38,904 --> 00:31:41,084
the required destinations, right?

503
00:31:41,564 --> 00:31:43,734
what could be some of these destinations?

504
00:31:43,734 --> 00:31:47,294
You could either throw it in some of
these visualization tools like super

505
00:31:47,294 --> 00:31:52,344
site looker, where, you provide them
insights about how the user journey or

506
00:31:52,424 --> 00:31:56,289
how the user analytics are, for the teams
or, stakeholders to act upon, right?

507
00:31:56,689 --> 00:32:01,609
And, sometimes you have to provide, this
data through, APS and microservices,

508
00:32:01,659 --> 00:32:04,349
as well, to deliver data to act upon.

509
00:32:05,179 --> 00:32:09,829
you might want it to put it in, some of
these, Grafana or some of these systems

510
00:32:09,899 --> 00:32:12,679
where, it does a real time alert, right?

511
00:32:12,749 --> 00:32:16,629
So when you identify a, anomaly,
like in the case of a credit card,

512
00:32:16,999 --> 00:32:21,089
situation, where a fraud transaction
is being detected, you wanted to

513
00:32:21,099 --> 00:32:23,219
alert the user immediately, right?

514
00:32:23,689 --> 00:32:26,869
and sometimes you also wanted to
take some kind of a trigger, right?

515
00:32:26,969 --> 00:32:30,499
where, you wanted to adjust the price
of the, so the, in the, in the previous

516
00:32:30,499 --> 00:32:35,519
case, you alerted it and then you block
the transaction attempts the, in the other

517
00:32:35,539 --> 00:32:41,769
case where you, decided, that there is
an some anomaly or some, event, happening

518
00:32:41,769 --> 00:32:47,889
like in the case of o Uber or Lyft Ride
sharing app, where, we wanted to, update

519
00:32:47,889 --> 00:32:51,779
a price of, the ride, on the fly, right?

520
00:32:51,879 --> 00:32:52,239
so.

521
00:32:52,934 --> 00:32:56,804
How the data is being, how data
is delivered, it, it relies

522
00:32:56,804 --> 00:32:58,314
on, multiple ways, right?

523
00:32:58,364 --> 00:33:01,894
Probably it could be a RESTful API, or
it could be a WebSocket, it could be a

524
00:33:01,894 --> 00:33:06,974
Kafka topic where it misses brokers, or,
there are some real time notifications

525
00:33:06,974 --> 00:33:11,434
or, There are plenty of ways where,
the data is being delivered to, the

526
00:33:11,434 --> 00:33:16,944
systems to act upon, the things, for
visualization or probably, maybe for

527
00:33:16,994 --> 00:33:19,334
notifications or maybe alert triggers.

528
00:33:20,104 --> 00:33:22,704
So what are the types
of, data delivery, right?

529
00:33:22,864 --> 00:33:26,654
I'm, I'm just going to take a minute and
then talk through, types of data delivery

530
00:33:26,654 --> 00:33:28,664
and some real time examples, right?

531
00:33:29,354 --> 00:33:30,934
So a push delivery, right?

532
00:33:30,984 --> 00:33:34,494
sending data proactively
to users or systems, right?

533
00:33:35,034 --> 00:33:39,784
say, a, a notification pops up
on our phone, when a Uber ride

534
00:33:39,784 --> 00:33:41,204
is, about to arrive, right?

535
00:33:42,014 --> 00:33:43,164
Pull delivery, right?

536
00:33:43,234 --> 00:33:47,954
Allowing users of the system, to
request data when required, right?

537
00:33:48,524 --> 00:33:54,194
so, simple example was a stock trading
app lets you refresh the page to

538
00:33:54,194 --> 00:33:55,864
update the market prices, right?

539
00:33:56,744 --> 00:34:01,224
and, These are the ways that, the
delivery happen in, in the case of push

540
00:34:01,224 --> 00:34:03,234
notification, we practically send it.

541
00:34:03,314 --> 00:34:07,564
And then the other case, the data is
available, pull it when required, right?

542
00:34:08,054 --> 00:34:11,534
so again, as we just talked about
what are the key features of data

543
00:34:11,534 --> 00:34:14,864
storage, what are the key features
of, an effective delivery, right?

544
00:34:15,304 --> 00:34:18,884
in this particular case, the low latency
plays again a bigger role because

545
00:34:18,944 --> 00:34:23,304
it has to ensure that there is an,
near instantaneous delivery, right?

546
00:34:23,854 --> 00:34:29,564
and, here the reliability plays a bigger
role because it's not an asynchronous,

547
00:34:29,614 --> 00:34:32,964
request that you make to ensure that,
it's not a fire and forget, right?

548
00:34:33,354 --> 00:34:37,434
There should be a way that it
guarantee guarantees that, the

549
00:34:37,434 --> 00:34:39,254
data is, delivered correctly.

550
00:34:39,284 --> 00:34:43,234
Even during, system failures assume
that if you are on Uber ride, you

551
00:34:43,234 --> 00:34:47,494
are waiting for the Uber car to come
up and then you never have received

552
00:34:47,494 --> 00:34:51,814
a notification, then it's gonna be a
juggle to identify which driver, which

553
00:34:51,814 --> 00:34:53,674
car, what information and not right.

554
00:34:54,174 --> 00:34:56,784
And also it boils down to
the security aspect, right?

555
00:34:56,814 --> 00:35:03,474
It has to ensure that when we send the
data, it protects the data sensitivity and

556
00:35:03,474 --> 00:35:05,824
then data during the data transformation.

557
00:35:07,054 --> 00:35:12,564
so I think, this, brings down to, the
section of, the, how do we design and

558
00:35:12,564 --> 00:35:16,544
what are the popular structure, popular,
tools that are available in the market

559
00:35:16,964 --> 00:35:21,224
and how do we, get to a state where
we design an end to end aspect of it.

560
00:35:21,224 --> 00:35:25,664
again, when it comes to implementing the
real time analytics, we have to assess

561
00:35:25,694 --> 00:35:29,404
what's the current state, what kind of
an infrastructure that is available.

562
00:35:29,664 --> 00:35:32,784
maybe for everyone, Kafka might
not be a feasible solution.

563
00:35:33,284 --> 00:35:37,014
Maybe for everyone, AWS Lambda
might not be a feasible solution.

564
00:35:37,444 --> 00:35:41,994
So depends on where the data is available,
how the data is available, we will

565
00:35:42,014 --> 00:35:46,154
start with understanding the existing
data infrastructure and then derive

566
00:35:46,374 --> 00:35:48,014
what are the possible capabilities.

567
00:35:48,994 --> 00:35:53,124
and, As the data is available, let's
try to understand what's the objective

568
00:35:53,154 --> 00:35:54,654
that we are trying to achieve, right?

569
00:35:54,704 --> 00:35:59,404
When it comes to real time
analytics, is this, a business

570
00:35:59,704 --> 00:36:03,534
driven or can we make a data driven
decision making out of it, right?

571
00:36:04,004 --> 00:36:09,064
so when we start thinking about, the
objective and understand the state at

572
00:36:09,104 --> 00:36:13,044
which the data is available and the
possible options available within the

573
00:36:13,114 --> 00:36:16,909
company or within the system that you
are working on, probably as I just

574
00:36:16,909 --> 00:36:21,299
discussed, discussed about the various
components of the technology pieces, let's

575
00:36:21,309 --> 00:36:27,509
pick the appropriate technologies and,
and again, it all boils down to, the,

576
00:36:27,559 --> 00:36:30,069
how do you deploy and optimize, right?

577
00:36:30,569 --> 00:36:35,469
So, I, as I talked about it, when
you deploy and optimize, right,

578
00:36:35,549 --> 00:36:39,149
the final piece of information,
as soon as you are getting into

579
00:36:39,149 --> 00:36:42,909
production or trying to get into your
production, right, the monitoring and

580
00:36:42,909 --> 00:36:44,739
resilience plays a huge role, right?

581
00:36:44,974 --> 00:36:49,324
I think, this will ensure that the
key components or the key features

582
00:36:49,354 --> 00:36:52,764
that are required for either in
data storage or data processing,

583
00:36:52,764 --> 00:36:55,314
data collection or data delivery.

584
00:36:55,754 --> 00:37:01,024
This piece ensures that the system
operates smoothly and it can

585
00:37:01,034 --> 00:37:03,434
handle the, unexpected issues.

586
00:37:03,664 --> 00:37:07,744
and probably measure the
performance goals like up 99.

587
00:37:07,744 --> 00:37:11,674
99 percent, which is
what I was talking about.

588
00:37:11,724 --> 00:37:15,134
We wanted to ensure that we
do achieve a no, no downtime,

589
00:37:15,144 --> 00:37:16,794
which talks also about 99.

590
00:37:16,834 --> 00:37:18,644
99 percent uptime, right?

591
00:37:19,084 --> 00:37:20,604
So what is monitoring, right?

592
00:37:20,644 --> 00:37:26,434
So primarily monitoring involves
continuously observing the health, and

593
00:37:26,474 --> 00:37:28,394
performance of the system metrics, right?

594
00:37:28,684 --> 00:37:33,144
so this includes probably tracking metrics
like response time, throughput errors,

595
00:37:33,194 --> 00:37:35,144
and, what's the resource utilization.

596
00:37:35,624 --> 00:37:39,404
if something goes wrong, monitoring
systems, trigger alerts for

597
00:37:39,514 --> 00:37:40,644
quick intervention, right?

598
00:37:40,794 --> 00:37:42,134
What is resilience, right?

599
00:37:42,724 --> 00:37:46,814
it's the system ability to
recover from failures, right?

600
00:37:46,914 --> 00:37:51,434
And continue functioning,
without, any, any downtime or

601
00:37:51,484 --> 00:37:53,044
probably the data loss, right?

602
00:37:53,514 --> 00:37:59,034
So this will help to ensure that, the
system that we are building, the pipeline

603
00:37:59,034 --> 00:38:04,774
that we are building, is reliable and
customer satisfied even under the,

604
00:38:04,804 --> 00:38:07,434
high stress or, unexpected events.

605
00:38:07,934 --> 00:38:12,204
So some like say probably when we
talk about, the, like some of the

606
00:38:12,224 --> 00:38:16,274
components of monitoring, we are talking
about checking the CPU memory, disk

607
00:38:16,294 --> 00:38:18,094
usage and network performance, right?

608
00:38:18,724 --> 00:38:22,544
so it's more like ensuring that
the servers don't get overloaded,

609
00:38:22,644 --> 00:38:26,044
on a Black Friday sale, on an e
commerce retail platform, right?

610
00:38:26,664 --> 00:38:30,734
And we also talk about application
performance monitoring, where we talk

611
00:38:30,744 --> 00:38:35,624
about, tracking the behavior of an
application, for latency errors and

612
00:38:35,774 --> 00:38:37,864
probably response times, et cetera, right?

613
00:38:38,384 --> 00:38:42,764
something like, monitoring a, ride
sharing app to ensure that ride requests

614
00:38:42,874 --> 00:38:45,474
are processed in milliseconds, right?

615
00:38:46,254 --> 00:38:50,455
And, so probably like, you, we
wanted to collect the logs to see,

616
00:38:50,505 --> 00:38:52,425
for debugging the errors, right?

617
00:38:52,525 --> 00:38:56,755
and, and when it comes to alerts and
notifications, Grafana, Prometheus,

618
00:38:56,755 --> 00:39:01,065
some of these tools helps to build the
dashboards and then probably raise a

619
00:39:01,065 --> 00:39:03,345
real time alerts, within the system.

620
00:39:03,895 --> 00:39:07,965
in case if the data pipeline crashes,
or maybe the operating, the operations

621
00:39:07,965 --> 00:39:13,045
team, needs to be notified, or the team
that has been responsible to derive

622
00:39:13,065 --> 00:39:17,260
some of this work can be notified
through, Teams, Slack, email, SMS,

623
00:39:17,260 --> 00:39:22,265
PagerDuty, and some of these, alerts
that are available, in the, system.

624
00:39:22,805 --> 00:39:28,285
while we do all of it, we wanted to
ensure that, it can handle failures,

625
00:39:28,335 --> 00:39:32,085
and then probably is the traffic evenly
distributed across servers to prevent

626
00:39:32,095 --> 00:39:36,585
overload, and then automatically
adjust the number of, resources like

627
00:39:36,645 --> 00:39:41,505
auto scaling, in place, and make sure
that is the data copy between multiple

628
00:39:41,515 --> 00:39:43,935
regions or maybe multiple servers.

629
00:39:44,055 --> 00:39:48,895
happening accurately, and, say
probably, redirects the traffic to

630
00:39:48,895 --> 00:39:51,855
the backup, backup system when there
is a primary system fails, right?

631
00:39:52,355 --> 00:39:57,015
So, I mean, we do have plenty
of performance testing tools and

632
00:39:57,015 --> 00:40:00,775
chaos engineering tools to derive,
some of these things, right?

633
00:40:01,185 --> 00:40:05,505
I have seen an article about how
Netflix is, uses chaos engineering

634
00:40:05,505 --> 00:40:10,100
to test, resilience by deliberately,
introducing the failures into the system.

635
00:40:10,100 --> 00:40:12,950
System to ensure it can handle the
real world descriptions, right?

636
00:40:13,670 --> 00:40:18,900
and, and, and most retailers, do
have to thoroughly extremely test the

637
00:40:18,900 --> 00:40:22,550
server performance, to ensure that
it, it's ready for the big sale days,

638
00:40:22,550 --> 00:40:26,960
like Amazon Prime Day or probably
Black Friday, cyber Monday, and some

639
00:40:27,020 --> 00:40:29,060
of these large sale events, right?

640
00:40:29,390 --> 00:40:32,380
So at that point of time, they,
how to ensure that auto-scaling,

641
00:40:32,480 --> 00:40:34,630
can process these, data, right?

642
00:40:35,070 --> 00:40:40,320
So in order to accommodate, in order
to accommodate, the, the real time

643
00:40:40,320 --> 00:40:45,880
data analytics pipeline, as much as
we build these systems, we wanted

644
00:40:45,880 --> 00:40:50,820
to ensure that the monitoring and
resilience is built in every component

645
00:40:50,820 --> 00:40:57,040
of it to handle, the, large amount
of data that's available, right?

646
00:40:57,750 --> 00:41:01,620
so I think what's the sentiment
analysis on this, right?

647
00:41:01,795 --> 00:41:05,305
So when it talks about real time
monitoring, as I just talked about,

648
00:41:05,305 --> 00:41:09,965
you continuously monitor and analyze
customer emotions across media reviews

649
00:41:09,965 --> 00:41:14,975
and support tickets and chat interactions
with AI powered sentiment detection.

650
00:41:15,665 --> 00:41:19,995
And then, Proactively reach out
to understand how the customer, is

651
00:41:19,995 --> 00:41:25,735
feeling and then try to translate
that negative feedback into positive,

652
00:41:26,125 --> 00:41:31,145
experiences by, before the customer
reach, we practically can reach

653
00:41:31,155 --> 00:41:32,665
out to identify what's happening.

654
00:41:33,105 --> 00:41:35,265
And then at the same time, we can also.

655
00:41:35,560 --> 00:41:39,540
Understand is this a trend across
the board, right, to uncover

656
00:41:40,540 --> 00:41:45,040
some of the larger problems
that the application is facing.

657
00:41:45,540 --> 00:41:50,880
So, if we start doing that sentiment
analysis on the customer support side

658
00:41:50,880 --> 00:41:55,770
of things where it is applicable,
so it absolutely helps the customer

659
00:41:55,800 --> 00:42:00,340
support with 67 percent faster
processing requests where there is

660
00:42:00,340 --> 00:42:03,830
a dramatic improvement in response
time to discuss, to customer issues.

661
00:42:04,665 --> 00:42:08,915
And, 90%, it's the first contact
resolution, right, where the ticket

662
00:42:08,915 --> 00:42:12,515
is not being bounced between levels
to, resolve the issue quickly.

663
00:42:13,115 --> 00:42:19,090
and this also helps with, an auto chart,
application where we don't need a physical

664
00:42:19,130 --> 00:42:22,020
presence to, address all those, things.

665
00:42:22,570 --> 00:42:26,570
this is what, the industry and some
of these articles talks about where,

666
00:42:26,630 --> 00:42:29,800
average, ROI, could be, an impressive.

667
00:42:29,800 --> 00:42:32,560
There are companies which
has talked through that.

668
00:42:32,580 --> 00:42:36,640
They have achieved 287%, within
the first year of deployment.

669
00:42:36,670 --> 00:42:42,340
And, And 94 percent of them, the customer,
feedback, talks about the real time

670
00:42:42,340 --> 00:42:47,310
personalization, plays a bigger role, in
terms of maintaining the brand loyalty

671
00:42:47,360 --> 00:42:53,020
and then, so, and it takes on a year,
time to implement all these things,

672
00:42:53,030 --> 00:42:57,600
stabilize the data and understand what
works and what does not work, right.

673
00:42:58,100 --> 00:43:03,890
So, as we, get into end aspect
of the complete real time

674
00:43:03,890 --> 00:43:05,320
data, analytics pipeline.

675
00:43:05,780 --> 00:43:08,080
So, let's take an end
to end example, right?

676
00:43:08,330 --> 00:43:10,890
So, I'm just taking an
Amazon as an example.

677
00:43:10,890 --> 00:43:13,730
A customer searches for a
product, on an e commerce website.

678
00:43:14,280 --> 00:43:20,110
so, so the search event, the event that
the customer has been, searched will be

679
00:43:20,110 --> 00:43:24,150
sent to, this is your data collection
aspect, website, and then the data, right?

680
00:43:24,670 --> 00:43:25,190
The data.

681
00:43:26,045 --> 00:43:28,725
is being sent to a Kafka topic, right?

682
00:43:28,855 --> 00:43:33,795
so data, if you remember the architecture
that we were talking about, data,

683
00:43:33,845 --> 00:43:35,795
source, and then the, data capture.

684
00:43:35,855 --> 00:43:38,775
So data source is your website
where we have captured it.

685
00:43:39,055 --> 00:43:43,665
And then data, Capture is placed
where we have sent it to Kafka, right?

686
00:43:43,745 --> 00:43:49,565
And now, as soon as the user searches
for the product, and then it is

687
00:43:49,585 --> 00:43:55,905
being sent to Kafka for ingestion,
and then we have Apache Flink to

688
00:43:55,905 --> 00:44:01,455
process that data to check what's
the preference and suggested product.

689
00:44:02,370 --> 00:44:06,550
And it is stored in the
DynamoDB, for the quick access.

690
00:44:07,150 --> 00:44:11,110
And then the data delivery talks
about the real time recommendation

691
00:44:11,110 --> 00:44:14,490
that appears, to the customers on
the screen within milliseconds.

692
00:44:15,420 --> 00:44:19,810
And then, Prometheus monitors across
all these pipelines, to ensure that,

693
00:44:19,860 --> 00:44:22,290
the system works in resiliency, right.

694
00:44:23,190 --> 00:44:27,280
so, what does, the real time
analytics, do in this case?

695
00:44:27,340 --> 00:44:27,580
Right.

696
00:44:27,940 --> 00:44:33,910
It helps to stay competitive, in the, in
today's, digital fast-paced era, deliver

697
00:44:34,150 --> 00:44:36,392
personalized, experiences at a scale.

698
00:44:36,950 --> 00:44:41,510
And, absolutely drive, measurable,
results in customer satisfaction,

699
00:44:41,520 --> 00:44:44,440
loyalty and revenue, right?

700
00:44:44,970 --> 00:44:49,330
so I think as we talk about, at the
end of, this, so I think companies,

701
00:44:49,330 --> 00:44:54,790
like Netflix, Amazon, Uber and, many
retailers and many healthcare industries

702
00:44:54,790 --> 00:44:59,580
and many industries, started setting the
gold standards for real time analytics.

703
00:45:00,220 --> 00:45:04,670
by following the design, pattern and
architecture that has been discussed,

704
00:45:04,740 --> 00:45:09,420
I think any business can, transform its
operations and the customer experience.

705
00:45:09,970 --> 00:45:14,120
with that, I would like to,
thank you again for taking this

706
00:45:14,120 --> 00:45:16,700
time to, get onto my, session.

707
00:45:17,460 --> 00:45:20,630
if you have any questions,
please do connect me on LinkedIn,

708
00:45:20,730 --> 00:45:22,580
Amaren, in Madison City.

709
00:45:23,320 --> 00:45:27,110
I would like to connect with you and
start, helping you out as much as I can.

710
00:45:27,740 --> 00:45:28,215
Thank you all.

711
00:45:28,395 --> 00:45:30,205
have a good day and, good evening.

