1
00:00:00,050 --> 00:00:00,790
Hello, everyone.

2
00:00:01,120 --> 00:00:02,570
I hope everyone is doing well.

3
00:00:03,050 --> 00:00:07,090
Welcome and thank you for joining
the session on AI Driven Chaos

4
00:00:07,150 --> 00:00:11,540
Engineering, Automating Resiliency
Testing with Predictive Insights.

5
00:00:12,040 --> 00:00:16,820
Shreeman Yaram, Senior Engineering
Manager in Test at Coupa Software.

6
00:00:17,180 --> 00:00:22,310
I specialize in AI, automation, and
resiliency testing, including focusing

7
00:00:22,310 --> 00:00:24,480
on DevSecOps and security testing.

8
00:00:25,430 --> 00:00:28,660
Over the years, I have worked
with distributed systems.

9
00:00:28,675 --> 00:00:30,813
I'm going to talk about what
is Chaos Engineering and AI

10
00:00:30,813 --> 00:00:32,713
driven testing strategies.

11
00:00:33,503 --> 00:00:38,853
Helping teams to build more
reliable, failure architectures

12
00:00:38,963 --> 00:00:40,433
and infrastructures as well.

13
00:00:40,933 --> 00:00:45,303
focus is on making chaos engineering
smarter in today's session,

14
00:00:46,023 --> 00:00:49,963
instead of just breaking the
system to see how they fail it.

15
00:00:50,953 --> 00:00:53,808
believe AI can help us
to predict the future.

16
00:00:54,128 --> 00:01:00,388
failures, automate chaos testing, and
continuously improve system resiliency.

17
00:01:00,888 --> 00:01:07,038
I'll walk you through how AI can enhance
chaos testing, shifting us from reacting

18
00:01:07,038 --> 00:01:13,708
to reacting the failures, anticipating and
preventing them before they impact users.

19
00:01:14,208 --> 00:01:14,918
Let's dive in.

20
00:01:15,418 --> 00:01:18,618
Introduction to chaos engineering
and most of us would be knowing

21
00:01:18,628 --> 00:01:19,888
what is chaos engineering.

22
00:01:19,888 --> 00:01:25,058
But let us take a moment and let us first
recap and refresh our knowledge on what is

23
00:01:25,108 --> 00:01:27,448
basically chaos engineering is all about.

24
00:01:28,318 --> 00:01:32,518
Chaos engineering is all about
testing system resiliency by

25
00:01:32,558 --> 00:01:34,338
intentionally introducing failures.

26
00:01:34,338 --> 00:01:38,758
We want to intentionally introduce the
failures and see how the system of waiting

27
00:01:38,758 --> 00:01:44,478
for someone, something goes wrong in
production, we do is we try to simulate

28
00:01:44,558 --> 00:01:50,648
real world failures in a controlled
way and see how the system responds.

29
00:01:51,148 --> 00:01:56,218
the purpose we want to identify
the weaknesses before they cause

30
00:01:56,248 --> 00:02:01,888
the actual issues in the production
for our users, for our customers.

31
00:02:02,388 --> 00:02:03,758
Be proactive.

32
00:02:04,743 --> 00:02:10,033
Testing different failure scenarios,
we can fix the problems early while in

33
00:02:10,033 --> 00:02:15,133
the development phase itself and make
our system more reliable rather than

34
00:02:15,143 --> 00:02:16,783
seeing these problems in production.

35
00:02:17,283 --> 00:02:22,813
its core, the key principle, at its
core, engineering, chaos engineering

36
00:02:22,813 --> 00:02:28,493
follows one principle, embrace
failure to build robust system.

37
00:02:28,993 --> 00:02:34,553
Instead of fearing of failures, we can
use them as opportunity to learn, adopt,

38
00:02:35,383 --> 00:02:36,943
and improve the system resiliency.

39
00:02:37,443 --> 00:02:44,793
This approach helps us to move from being
reactive, fixing things when they break,

40
00:02:45,153 --> 00:02:50,633
to being proactive, designing the systems
that can handle failures gracefully.

41
00:02:51,133 --> 00:02:52,703
all about the chaos engineering.

42
00:02:53,623 --> 00:02:58,163
While many of my colleagues, many of the
speakers have already covered what is

43
00:02:58,163 --> 00:03:03,073
kiosk engineering and all sort of things,
it's a, I don't want to stress on that,

44
00:03:03,433 --> 00:03:10,063
but I want to show you how the traditional
kiosk testing is being done and why it's

45
00:03:10,063 --> 00:03:17,483
not enough and why we need to use AI
that could take us into the next level.

46
00:03:17,983 --> 00:03:20,923
Challenges in, in predicting
real time failures.

47
00:03:21,423 --> 00:03:22,923
Unpredictable failures.

48
00:03:23,363 --> 00:03:28,833
Even with all the kiosk testing we
do, unexpected failures still happens,

49
00:03:29,533 --> 00:03:31,663
often at the worst possible times.

50
00:03:32,473 --> 00:03:39,443
Engineers, SREs, DevOps, SecOps teams
work hard to anticipate issues, but

51
00:03:39,483 --> 00:03:43,783
real world systems are unpredictable,
and failures can still slip through.

52
00:03:44,283 --> 00:03:46,313
One big challenge is complexity.

53
00:03:46,923 --> 00:03:48,123
Because why it happens?

54
00:03:48,153 --> 00:03:49,223
Because of the complexity.

55
00:03:50,053 --> 00:03:54,193
Modern systems are made up of
countless services, dependencies,

56
00:03:54,223 --> 00:03:58,843
and external integrations like
AWS, S3s, what, you name it.

57
00:03:58,843 --> 00:04:03,801
In the microservices world, people use a
lot of third party software, third party

58
00:04:03,801 --> 00:04:06,007
systems and they integrate with them.

59
00:04:06,007 --> 00:04:11,743
As the scale predicting every possible
failure mode becomes nearly impossible.

60
00:04:12,243 --> 00:04:18,083
And then there is a cost,
both in time and resources.

61
00:04:18,743 --> 00:04:22,793
Running continuously chaos testing
takes effort, infrastructure,

62
00:04:22,843 --> 00:04:24,513
and careful coordination.

63
00:04:24,903 --> 00:04:29,393
It's not always feasible to simulate
every failure scenario at scale.

64
00:04:29,923 --> 00:04:31,623
It's impossible, basically.

65
00:04:32,123 --> 00:04:35,623
So while chaos engineering is a
great way to uncover weaknesses,

66
00:04:35,713 --> 00:04:37,543
it also has limitations.

67
00:04:38,433 --> 00:04:39,513
The question comes.

68
00:04:40,173 --> 00:04:43,533
How do we get beyond
traditional chaos testing?

69
00:04:44,033 --> 00:04:48,533
How can we make it make this
process smarter and more efficient?

70
00:04:49,033 --> 00:04:51,323
where the AI comes into the picture.

71
00:04:52,313 --> 00:04:59,983
Let's dive in how AI can help predict
and manage failures in a better way.

72
00:05:00,063 --> 00:05:05,013
When we Let's, before we go in deeper
about the idea, let's break down the

73
00:05:05,013 --> 00:05:08,943
difference between the traditional kiosk
engineering and AI driven kiosk testing.

74
00:05:09,443 --> 00:05:15,043
traditional kiosk engineering, the
approach engineer use usually takes is

75
00:05:15,053 --> 00:05:20,675
manually fault injecting into the systems,
which we know, which the engineer knows

76
00:05:20,675 --> 00:05:26,033
or which the team knows, a shutdown in
your server, adding a latency to an API.

77
00:05:26,813 --> 00:05:31,493
or simulating a database outage or
simulating a third party system outage.

78
00:05:31,893 --> 00:05:38,063
These tests are based on what an engineer
or what a team could think of it.

79
00:05:38,563 --> 00:05:44,223
Meaning that we are relaying whole
on a knowledge and past experiences.

80
00:05:44,723 --> 00:05:45,793
see a problem here.

81
00:05:46,793 --> 00:05:50,493
The problem is We are not
covering the real world failures.

82
00:05:50,533 --> 00:05:53,823
We do not know once the system
goes in production, what kind

83
00:05:53,823 --> 00:05:55,573
of a problems we would face.

84
00:05:56,053 --> 00:06:01,743
Real world problems, failures do
not always follow our assumptions.

85
00:06:02,243 --> 00:06:07,933
Because systems are complex and unknown
failure can occur anytime and that

86
00:06:07,933 --> 00:06:09,913
could slip through us, through from us.

87
00:06:10,413 --> 00:06:15,863
Plus, because the, This
approach is most manual.

88
00:06:16,593 --> 00:06:19,613
The scope is often limited, if
you see, because it's all the

89
00:06:19,633 --> 00:06:21,523
engineering team's knowledge.

90
00:06:22,513 --> 00:06:27,213
can only run a set of limited
real world test cases.

91
00:06:27,713 --> 00:06:29,343
after that, the tests are done.

92
00:06:29,693 --> 00:06:34,043
Engineer After the tests are done,
engineers have to manually try

93
00:06:34,043 --> 00:06:38,963
to see the results, which takes
time and leaves a room for human

94
00:06:39,023 --> 00:06:43,193
error, because when we introduce
manually It leaves a human error.

95
00:06:43,693 --> 00:06:45,553
And there is the biggest issue.

96
00:06:45,913 --> 00:06:50,783
This whole process is a reactive
approach and we are identifying

97
00:06:50,783 --> 00:06:55,243
failures after they happen rather than
preventing them in the first place.

98
00:06:55,483 --> 00:07:01,143
we do, but at one level, one
level, but not at a dynamic level.

99
00:07:01,903 --> 00:07:06,063
Now, what else can we use to stop this?

100
00:07:06,748 --> 00:07:11,448
Yeah, here is what my thought process on
doing this and we have started doing it.

101
00:07:12,338 --> 00:07:15,698
Let's understand how we
can use AI driven approach.

102
00:07:16,228 --> 00:07:22,898
Now let's use, AI to help
us to make it better.

103
00:07:23,398 --> 00:07:29,468
of engineers manually introducing
failures, why not AI can simulate

104
00:07:29,888 --> 00:07:32,388
faults based on the real world data.

105
00:07:33,268 --> 00:07:34,898
What do you mean by the real world data?

106
00:07:35,238 --> 00:07:39,868
We will come there, but let us see
what the real world data could help.

107
00:07:40,368 --> 00:07:43,918
is just not following a
predefined test cases.

108
00:07:43,958 --> 00:07:47,648
If you see in the traditional
approach, we go by knowledge.

109
00:07:48,278 --> 00:07:53,598
Here, we want to go adaptive and
dynamically from the real world data,

110
00:07:54,098 --> 00:07:56,558
we want to continuously monitor the log.

111
00:07:56,588 --> 00:07:58,878
The real world data here is logs.

112
00:07:59,418 --> 00:08:04,998
the real production logs, analyze
the traffic patterns, analyze how

113
00:08:05,018 --> 00:08:09,218
the systems are behaving in the
production environment learn from the

114
00:08:09,258 --> 00:08:14,258
past incidents to uncover the failure
patterns, which we are unfamiliar.

115
00:08:14,398 --> 00:08:19,578
The teams are unfamiliar we
might not, be even aware of it.

116
00:08:20,458 --> 00:08:23,658
For example, let's take an
example for this context.

117
00:08:23,698 --> 00:08:27,898
I'm taking an example as a, in this
context as an e commerce platform.

118
00:08:28,398 --> 00:08:34,818
E commerce platform has a suddenly traffic
spike during Black Friday, okay, AI can

119
00:08:34,818 --> 00:08:41,528
observe how the infrastructure behaves
under such loads, detect subtle issues,

120
00:08:42,138 --> 00:08:46,628
like maybe a payment service might be
a DOM or a shipping service may not be

121
00:08:46,878 --> 00:08:53,338
supporting it and dynamically creating
those kiosks test based on those findings.

122
00:08:54,018 --> 00:08:54,588
That means.

123
00:08:55,138 --> 00:08:59,848
These learnings, what we have in the,
in the Black Friday's, things, we

124
00:08:59,848 --> 00:09:09,008
want that data to be converted into a
machine, into a new experiments for kiosk

125
00:09:09,088 --> 00:09:13,598
engineering testing and continuously
integrating into our CI CD process.

126
00:09:14,578 --> 00:09:20,808
This means instead of repeating
the same old kiosk experiments,

127
00:09:21,308 --> 00:09:26,248
AI adopts the test based on what
actually happens in production.

128
00:09:27,118 --> 00:09:27,468
How?

129
00:09:27,778 --> 00:09:28,988
Using a real world data.

130
00:09:29,068 --> 00:09:30,518
What do you mean by real world data?

131
00:09:30,778 --> 00:09:35,018
All the locks, all the system locks,
all the production locks we take and

132
00:09:35,018 --> 00:09:40,108
we use AI to make it an adaptable.

133
00:09:40,608 --> 00:09:43,128
again, it scales across different systems.

134
00:09:43,798 --> 00:09:48,818
We can, since we, we have the production,
We have all the locks running continuously

135
00:09:49,408 --> 00:09:54,468
and automates, we can automate the
entire testing lifecycle from injecting

136
00:09:54,468 --> 00:09:58,508
failures to analyzing results and
even suggesting improvements as well.

137
00:09:59,008 --> 00:10:00,778
Do you think this is the biggest shift?

138
00:10:01,398 --> 00:10:01,998
Yes.

139
00:10:02,298 --> 00:10:04,868
AI moves from reactive to proactive.

140
00:10:05,678 --> 00:10:09,618
Using AI, now we are going from reactive
approach in the traditional one.

141
00:10:10,198 --> 00:10:11,938
to a proactive approach.

142
00:10:12,438 --> 00:10:16,508
Instead of waiting for a failure to
happen, which we do with the traditional

143
00:10:16,508 --> 00:10:19,288
approach as well, where we have a
limited knowledge, where with the

144
00:10:19,288 --> 00:10:22,248
limited knowledge we are thinking
and we are creating the experiment.

145
00:10:23,198 --> 00:10:29,138
In the AI driven approach, we are using
the production logs to manifest and

146
00:10:29,138 --> 00:10:35,368
create dynamic test cases as a part of
our continuous, loop, continuous loop.

147
00:10:35,868 --> 00:10:38,618
So it anticipates risks and tests.

148
00:10:39,173 --> 00:10:41,703
for them before they cause
the real world issues.

149
00:10:42,203 --> 00:10:46,943
bottom line I want to say is traditional
chaos testing is like guessing what

150
00:10:46,943 --> 00:10:49,813
might break and testing for that.

151
00:10:50,783 --> 00:10:56,483
driven chaos is like watching what
actually break in the real world testing

152
00:10:56,503 --> 00:11:02,143
for those patterns dynamically and as
part of our continuous development cycle.

153
00:11:02,643 --> 00:11:05,713
AI makes chaos engineering
smarter in my view.

154
00:11:06,168 --> 00:11:09,408
faster and more effective
and more resilient.

155
00:11:09,658 --> 00:11:12,228
It makes us the system
to be more resilient.

156
00:11:13,018 --> 00:11:17,708
Again, my suggestion is that
we don't, we should not leave

157
00:11:17,708 --> 00:11:19,778
the traditional replacement.

158
00:11:20,048 --> 00:11:22,478
it's not a replacement for
the traditional approach.

159
00:11:23,148 --> 00:11:29,218
We should use AI as an extension for
what we are already doing it, what we

160
00:11:29,218 --> 00:11:30,698
are already doing in the traditional way.

161
00:11:31,198 --> 00:11:34,498
the knowledge base help us
to simulate the scenarios.

162
00:11:34,948 --> 00:11:39,188
However, in the production, once
the system goes into the production,

163
00:11:39,558 --> 00:11:43,728
use the production logs to become
more effective, run continuous

164
00:11:43,778 --> 00:11:45,908
and improve your chaos testing.

165
00:11:45,948 --> 00:11:50,048
And that's the philosophy that,
that says the, testing, continuously

166
00:11:50,048 --> 00:11:51,308
improve your experiments.

167
00:11:51,348 --> 00:11:52,568
That's exactly what we are doing.

168
00:11:52,578 --> 00:11:56,828
But right now we are using
AI with that, with the real

169
00:11:56,848 --> 00:11:58,448
world data and we are using it.

170
00:11:58,948 --> 00:12:03,058
The best approach is, in my view, the
best approach or what I suggest is the

171
00:12:03,058 --> 00:12:06,648
best approach is to combine both methods.

172
00:12:07,338 --> 00:12:12,098
Keep running well designed manual
chaos experiments while using AI

173
00:12:12,098 --> 00:12:13,898
to expand and improve upon them.

174
00:12:14,398 --> 00:12:17,878
understand integrating AI to
predict and automate failures.

175
00:12:18,058 --> 00:12:22,273
Now that we understood AI driven
Chaos engineering is more adaptive.

176
00:12:22,483 --> 00:12:28,133
Let's break down how AI actually
makes this happen, AI for analysis,

177
00:12:28,813 --> 00:12:30,553
learning from historical failures.

178
00:12:30,583 --> 00:12:31,433
We talked about that.

179
00:12:31,493 --> 00:12:33,923
We have a historical data
in the sense like historical

180
00:12:33,933 --> 00:12:35,683
logs from multiple systems.

181
00:12:35,683 --> 00:12:36,643
We have captured that.

182
00:12:37,363 --> 00:12:41,333
The first step is to
analyze the past failures.

183
00:12:42,093 --> 00:12:44,883
The very first step, we want
to analyze the past failures.

184
00:12:45,383 --> 00:12:51,313
can go through historical logs, everything
from system crashes, slowdowns, error

185
00:12:51,313 --> 00:12:57,773
spikes, CPU spikes, memory spikes, you
name it, and identify these patterns which

186
00:12:57,813 --> 00:13:02,893
humans might have missed in their static
experiments, in the traditional approach.

187
00:13:03,583 --> 00:13:09,178
Instead of relying on intuition,
what I would think is, AI can surface

188
00:13:09,208 --> 00:13:14,128
hidden trends in failure data because
we have a large set of data from last

189
00:13:14,138 --> 00:13:20,238
couple of months, last couple of years,
and use the data, go into deeper and

190
00:13:20,258 --> 00:13:25,358
deeper to understand insights, how
the system may behave under a stress.

191
00:13:25,858 --> 00:13:30,765
Again, for example, let's take,
our database, example, our

192
00:13:30,765 --> 00:13:36,008
database frequently, black Friday
afternoon or on a Friday afternoon.

193
00:13:36,508 --> 00:13:38,578
There could be slowdowns on the system.

194
00:13:38,658 --> 00:13:42,208
On the weekends, we may not be using
it, so we are trying to bring it down

195
00:13:42,458 --> 00:13:44,408
because these are external systems.

196
00:13:45,378 --> 00:13:49,618
may not immediately notice this
pattern, but AI after analyzing

197
00:13:49,618 --> 00:13:56,038
months of logs can flag that this is a
coincidence with a weekly traffic surge.

198
00:13:56,968 --> 00:14:00,418
We don't, we know about our
internal system, we might have a

199
00:14:00,418 --> 00:14:02,048
knowledge on our internal system.

200
00:14:02,418 --> 00:14:05,868
But let's say a payment gateway
system we are integrating, that

201
00:14:05,938 --> 00:14:08,508
knowledge may not be having the steam.

202
00:14:08,928 --> 00:14:13,788
And that external teams, let's say Visa
or Mastercard, if you're using those

203
00:14:13,788 --> 00:14:19,078
systems, they want to reduce, because of
their cost improvement plans, they want

204
00:14:19,078 --> 00:14:23,968
to reduce on the weekends, the usage,
they want to have it as low, and they're

205
00:14:24,258 --> 00:14:26,348
tearing down the servers on the weekends.

206
00:14:27,308 --> 00:14:31,238
It's a hypothetical situation,
we are taking an example here.

207
00:14:31,978 --> 00:14:36,868
That pattern we may not be able to know
until and unless we analyze the logs.

208
00:14:37,728 --> 00:14:43,758
that's where we want our systems to
be very proactively acting on it.

209
00:14:43,858 --> 00:14:45,478
And how do we handle this situation?

210
00:14:46,148 --> 00:14:49,668
That's where the beauty of this
idea comes into the picture.

211
00:14:50,168 --> 00:14:54,558
in other words, if a storm is coming,
if we know that storm is coming, we can

212
00:14:55,038 --> 00:15:00,068
Let's prepare ahead of a time instead
of waiting to get it gone the rain.

213
00:15:01,008 --> 00:15:06,088
Similarly, AI warns us about a
potential system failures before they

214
00:15:06,118 --> 00:15:13,128
escalate, giving time, giving a team
a time ahead to enforce those areas.

215
00:15:13,628 --> 00:15:16,878
Automated injections, kiosk experiments.

216
00:15:17,198 --> 00:15:19,908
Now what we want is
smarter kiosk experiments.

217
00:15:19,928 --> 00:15:23,578
So what first we did is AI analysis
and then we have predictive

218
00:15:23,588 --> 00:15:25,298
modeling we want to give and.

219
00:15:25,668 --> 00:15:31,518
forecast what could be failure, and
then we want to generate automatic

220
00:15:31,598 --> 00:15:36,378
injections or experiments in chaos world,
we see it as experiments automatically

221
00:15:36,378 --> 00:15:38,008
generating the chaos experiments.

222
00:15:38,748 --> 00:15:40,978
Now that's the exciting part, right?

223
00:15:40,978 --> 00:15:44,998
So AI does not just
predict it, acts on that.

224
00:15:45,498 --> 00:15:49,768
on the insights that it learned
from analysis and modeling, AI can

225
00:15:49,798 --> 00:15:52,808
automatically help us design and inject.

226
00:15:53,238 --> 00:15:54,618
A targeted failure.

227
00:15:55,228 --> 00:16:00,108
Areas that are more likely to break
based on the external factors need

228
00:16:00,108 --> 00:16:02,718
not to be external internals as
well based on the traffic and all

229
00:16:02,718 --> 00:16:03,978
those things dynamic in nature.

230
00:16:04,698 --> 00:16:09,228
If AI detects that the payment service
is slow during a peak hours or during

231
00:16:09,288 --> 00:16:15,158
weekends, learns on that pattern
and it can generate a controlled

232
00:16:15,158 --> 00:16:20,198
latency into the service before
the real traffic search happens.

233
00:16:20,698 --> 00:16:25,768
This model will allow the engineers to
test fixes before users are affected.

234
00:16:26,208 --> 00:16:29,098
And this is a continuous process,
which we keep on learning.

235
00:16:29,598 --> 00:16:33,558
on, on just to bring everything all
together, what we are trying to do

236
00:16:33,558 --> 00:16:38,138
is AI learns from historical data
instead of relying on assumptions,

237
00:16:38,158 --> 00:16:41,733
which we talked about In, in, in the
slide where we want to, where we are

238
00:16:41,803 --> 00:16:42,733
trying to show the differentiation.

239
00:16:43,233 --> 00:16:47,413
AI predicts future weak points
based on the past data, helping the

240
00:16:47,413 --> 00:16:49,713
teams to stay ahead on the failures.

241
00:16:50,213 --> 00:16:54,603
automates the chaos experiments, making
tests more relevant and effective.

242
00:16:55,103 --> 00:16:59,303
means we are no longer ran, testing
randomly with the static knowledge.

243
00:16:59,738 --> 00:17:02,898
We are running chaos experiment
where they actually matter.

244
00:17:03,028 --> 00:17:03,528
that's the thing.

245
00:17:04,028 --> 00:17:09,008
let's explore how AI driven chaos testing
can be integrated into a real world force.

246
00:17:09,638 --> 00:17:14,918
And again, I just want to focus why
this works uses relatable examples

247
00:17:14,918 --> 00:17:19,258
like database slowdowns, weather
forecasting, payment service issues,

248
00:17:19,848 --> 00:17:24,938
shipping services, many shipping services
have been slowed down or we want to

249
00:17:24,938 --> 00:17:26,818
take alternative shipping mechanisms.

250
00:17:26,858 --> 00:17:27,968
All sorts of things are.

251
00:17:28,503 --> 00:17:31,693
are really important for an
example of an e commerce.

252
00:17:32,193 --> 00:17:37,193
connects AI role in each stage, we are
learning, predicting and acting it.

253
00:17:37,693 --> 00:17:40,653
excitement while, keeping it
practical and easy to flow.

254
00:17:41,153 --> 00:17:45,813
Now, if you see most applications, these
are the popular tools that they use it.

255
00:17:46,013 --> 00:17:46,363
Okay.

256
00:17:46,633 --> 00:17:49,183
And this is what, where
our entry point would be.

257
00:17:49,713 --> 00:17:53,503
So key observability tools and
logs, if you see, to make AI driven

258
00:17:53,503 --> 00:17:54,953
chaos engineering effective, okay.

259
00:17:55,708 --> 00:17:57,878
We need the right observability.

260
00:17:58,378 --> 00:18:02,748
we don't have a proper logging
mechanism, this is very hard to achieve.

261
00:18:03,538 --> 00:18:08,550
Any application For troubleshooting
or debugging, the logs and the

262
00:18:08,550 --> 00:18:10,586
observability tools are very important.

263
00:18:11,266 --> 00:18:16,836
Tools like New Relic, Splunk, Elk,
CloudWatches, AWS CloudWatches.

264
00:18:16,866 --> 00:18:20,946
There are hundreds of observability
tools, commercial, non commercial and all.

265
00:18:21,446 --> 00:18:24,396
New Relic can help us to
monitor application performance.

266
00:18:24,396 --> 00:18:27,516
Splunk can aggregate and analyze the logs.

267
00:18:27,826 --> 00:18:31,146
And AWS Clouds provides deeper
insights into both infrastructure

268
00:18:31,146 --> 00:18:35,156
and application health.

269
00:18:35,886 --> 00:18:40,976
All these tools are individual tools
can help us to collect the comprehensive

270
00:18:41,036 --> 00:18:45,916
data in and then passing it to the
AI model to learn from the real

271
00:18:45,956 --> 00:18:51,576
world system behavior and finally
generate the experiments files.

272
00:18:52,446 --> 00:18:52,696
Okay.

273
00:18:53,446 --> 00:18:57,176
Now, here is what, I would
say, this is a model.

274
00:18:57,836 --> 00:19:00,566
some of these things are,
at a very high level.

275
00:19:00,596 --> 00:19:04,736
If you see from, the steps that,
what I'm trying to do it here

276
00:19:04,736 --> 00:19:07,506
is first initialize S3 clients.

277
00:19:07,626 --> 00:19:08,046
Here.

278
00:19:08,136 --> 00:19:12,286
This could be replaced with your,
Splunk SD case or a New Relic SD

279
00:19:12,286 --> 00:19:15,826
case, which could help us to get
the data from the real world system.

280
00:19:16,096 --> 00:19:19,546
But for our reference, for a just
easy understanding, let's assume

281
00:19:19,546 --> 00:19:24,086
that there is an S3 bucket, which
has all the system log, okay.

282
00:19:24,151 --> 00:19:24,371
Or.

283
00:19:25,136 --> 00:19:29,846
Or a job which will get the data
from this Splunk New Relic, you

284
00:19:29,846 --> 00:19:33,926
name it, and then gets that into
the S3 and then, that's where, so

285
00:19:33,926 --> 00:19:37,796
we are trying, I did a small sample
program to explain you how it works.

286
00:19:37,856 --> 00:19:39,586
It should, it can be used.

287
00:19:40,016 --> 00:19:47,265
So if you see line number, 8 to, 8 to 11
or, 12, say S3 client is equals to Boto.

288
00:19:47,305 --> 00:19:51,475
I'm using here Boto, bucket name
and get, your S3 bucket and logs.

289
00:19:51,715 --> 00:19:51,985
Okay.

290
00:19:52,370 --> 00:19:54,220
Fetch logs file from S3.

291
00:19:54,450 --> 00:19:58,470
So let us go bucket name, log file,
key file name and whatever the date it

292
00:19:58,470 --> 00:20:02,750
is and read the CSV file and based on
the timestamp and all sort of things

293
00:20:02,750 --> 00:20:06,730
you are trying to do a data frames
and Now converts the timestamp column

294
00:20:06,730 --> 00:20:12,280
and into date format for a proper
time based analysis and again think

295
00:20:12,280 --> 00:20:17,840
of S3 as I mentioned, think of S3 as
a jaint warehouse storing system logs.

296
00:20:18,340 --> 00:20:21,680
section fetch can help you to
fetch all the logs and organize

297
00:20:21,680 --> 00:20:22,990
them into a structure format.

298
00:20:23,350 --> 00:20:26,910
Once again, I'm trying to tell, this is
a sample piece, which I'm trying to take

299
00:20:27,660 --> 00:20:33,800
to get an idea of how we can use this
system and, the idea of using the existing

300
00:20:33,800 --> 00:20:35,840
data or the historical data to do it.

301
00:20:36,340 --> 00:20:40,270
There are many ways to do it instead
of S3, instead of Splunk, you can

302
00:20:40,270 --> 00:20:45,040
use a fetching mechanism or you can
write a client separate code block

303
00:20:45,040 --> 00:20:47,980
for this itself or a separate program
which will keep on fetching the

304
00:20:47,980 --> 00:20:50,320
data and passes it to our AI model.

305
00:20:50,330 --> 00:20:51,150
This you can do it.

306
00:20:51,160 --> 00:20:52,980
This is just an example.

307
00:20:53,710 --> 00:20:58,620
Then you define your feature for AI
analysis that as we talked line number 15

308
00:20:58,880 --> 00:21:02,621
to 17 I would not such, 17 would help you.

309
00:21:03,231 --> 00:21:08,051
Which will help you to understand the
key performance indicators, KPIs, like

310
00:21:08,051 --> 00:21:13,821
CPI usage and response time and all
This metrics are like health indicators

311
00:21:13,821 --> 00:21:17,781
for your system, like checking heart
rate, blood pressure and temperature.

312
00:21:17,791 --> 00:21:20,161
And you can use what
other mechanisms as well.

313
00:21:20,331 --> 00:21:22,301
Again, as I said, it's a just one.

314
00:21:22,771 --> 00:21:25,691
And then detecting anomalies with AI.

315
00:21:26,071 --> 00:21:27,541
Use the isolation forest.

316
00:21:27,541 --> 00:21:29,411
I'm using the isolation forest here.

317
00:21:29,411 --> 00:21:29,421
Okay.

318
00:21:29,421 --> 00:21:29,430
Thanks.

319
00:21:30,021 --> 00:21:31,801
and then, detecting the lysis.

320
00:21:31,801 --> 00:21:35,541
If you see line number 20 to 22
is what we are talking about, that

321
00:21:35,921 --> 00:21:40,861
label, on, a normal logs as normal and
lysis log logs as an normal anomaly.

322
00:21:41,441 --> 00:21:46,031
Like for example, if it is info error
debug based on that, you can label it.

323
00:21:46,331 --> 00:21:46,661
Okay.

324
00:21:47,281 --> 00:21:48,521
it's it's oh.

325
00:21:48,581 --> 00:21:51,891
It's If I have to take an example,
it's like airport security.

326
00:21:51,891 --> 00:21:56,291
Most people pass through a normal,
but suspicious behavior anomalies

327
00:21:56,321 --> 00:21:58,221
get flagged for a deeper inspections.

328
00:21:59,091 --> 00:21:59,451
Okay.

329
00:22:00,121 --> 00:22:01,691
Extract anomaly locks.

330
00:22:01,801 --> 00:22:05,661
Now we use the anomaly locks,
filters the data set that only makes

331
00:22:05,711 --> 00:22:09,090
meaningful and flagged anomalies.

332
00:22:09,590 --> 00:22:14,940
then what we try to do is
finally line number 28 to 25,

333
00:22:15,780 --> 00:22:17,455
you are trying to, inject it.

334
00:22:17,955 --> 00:22:19,505
You are injecting the experiments.

335
00:22:19,505 --> 00:22:22,925
That means if you see on the right
side, experiment is injected.

336
00:22:22,955 --> 00:22:27,291
This is AWS FIS experiment I have
taken for a demonstration purpose.

337
00:22:27,291 --> 00:22:31,475
And that could help us to
understand that there is a CPU.

338
00:22:31,835 --> 00:22:35,655
Usage is high and this could
be applied and it's a regular

339
00:22:35,655 --> 00:22:38,375
example I've taken for CPU, but
there could be many things, right?

340
00:22:38,375 --> 00:22:42,905
DDoS attacks were happening and
that is causing much more slower and

341
00:22:42,905 --> 00:22:45,175
that's where the CPU is increasing.

342
00:22:45,265 --> 00:22:47,885
And how do we simulate that DDoS attacks?

343
00:22:47,925 --> 00:22:52,755
Maybe fake up, use a faker library or
anything and fake up the calls so that,

344
00:22:52,805 --> 00:22:54,805
this kind of scenarios you can build up.

345
00:22:55,305 --> 00:22:58,225
finally, what we are trying to do
is we are trying to finally save

346
00:22:58,225 --> 00:22:59,945
the experiment configuration file.

347
00:23:00,465 --> 00:23:04,715
And, finally we are visualizing it
and, that's how we are trying to do.

348
00:23:04,745 --> 00:23:07,215
The summary is fetching the logs from AWS.

349
00:23:07,265 --> 00:23:08,185
That could be anything.

350
00:23:08,675 --> 00:23:12,515
you can use, SDK clients like Splunk
clients and all sorts of things, and then

351
00:23:12,525 --> 00:23:17,535
detecting anomalies, generating the fault
injections experiments, visualizing,

352
00:23:17,585 --> 00:23:19,685
the display detected anomalies.

353
00:23:19,715 --> 00:23:21,565
that's exactly what this
program is trying to do.

354
00:23:22,065 --> 00:23:24,985
This approach automates,
chaos engineering, making

355
00:23:24,985 --> 00:23:26,295
it smarter and proactive.

356
00:23:26,975 --> 00:23:29,375
instead of relaying on
a predefined scenarios.

357
00:23:29,875 --> 00:23:34,295
Now, overall summarizing the benefits
of AI driven chaos engineering.

358
00:23:34,795 --> 00:23:39,145
So basically what we are trying to do
is proactive resiliency, anticipate

359
00:23:39,185 --> 00:23:40,975
and mitigate potential failures.

360
00:23:41,645 --> 00:23:45,405
We are trying to make sure instead
of waiting for failures to happen

361
00:23:46,395 --> 00:23:52,025
reacting, we shift the predicting,
predicting it where it could

362
00:23:52,035 --> 00:23:54,605
happen and we are preventing that.

363
00:23:54,605 --> 00:24:01,905
AI kiosk engineering analyzes past
failures simulates potential issues

364
00:24:02,405 --> 00:24:09,947
experimental files like AWS AI and
AWS FIS or other kiosk toolkits.

365
00:24:10,447 --> 00:24:12,387
This will help us to strengthen.

366
00:24:12,917 --> 00:24:15,797
And continuous streamlining the process.

367
00:24:16,767 --> 00:24:20,527
Overall efficiency, streamline
testing through automation.

368
00:24:21,027 --> 00:24:24,092
you take the previous ones, we
are, the traditional approach.

369
00:24:24,152 --> 00:24:28,472
It is manual and also it's time consuming
to see what's going on, how it, the

370
00:24:28,472 --> 00:24:30,032
application works and all those things.

371
00:24:30,532 --> 00:24:36,892
With the ai, are preparing the
experiments dynamically based on our past

372
00:24:36,892 --> 00:24:42,502
learnings, which also auto, which is like
analyzing the locks, detecting risks.

373
00:24:43,002 --> 00:24:47,842
generating the fault, faults,
dynamical, experimentals, dynamical.

374
00:24:48,342 --> 00:24:52,492
This reduce the needs for constant
human monitoring and allows teams to

375
00:24:52,492 --> 00:24:58,122
focus on improving the system instead
of just reading it, of just testing.

376
00:24:58,622 --> 00:25:01,032
Again, continuous
improvement is another one.

377
00:25:01,222 --> 00:25:06,112
Since we are leveraging AI for insights
for ongoing system enhancements, can

378
00:25:06,112 --> 00:25:09,012
learn from every test and every failure.

379
00:25:09,467 --> 00:25:13,017
Making future predictions more accurate.

380
00:25:13,517 --> 00:25:19,407
Again, instead of static testing, our
resiliency strategy adapts more over the

381
00:25:19,447 --> 00:25:24,987
time, ensuring stronger, more reliable
systems and new challenging arises.

382
00:25:25,487 --> 00:25:30,467
I close out, you can use
different models to predict.

383
00:25:30,817 --> 00:25:36,427
In our example, I used random
forest, but you can use other things

384
00:25:36,437 --> 00:25:39,767
which could be a time series models
which can understand and do it.

385
00:25:40,517 --> 00:25:45,037
You could find many models
available online and search,

386
00:25:45,637 --> 00:25:46,847
Google search could help.

387
00:25:47,657 --> 00:25:52,237
And by close out thoughts, AI
driven chaos engineering is not

388
00:25:52,257 --> 00:25:54,017
just about breaking the things.

389
00:25:54,517 --> 00:25:58,827
It's about learning, improving
and building systems.

390
00:25:59,557 --> 00:26:03,607
that can handle failures
before they even happen.

391
00:26:04,107 --> 00:26:08,537
layer of testing we are adding
from a traditional versus AI.

392
00:26:09,037 --> 00:26:14,517
By combining prediction, automating
and continuous learning, we are

393
00:26:14,587 --> 00:26:19,537
creating a smarter proactive
approach to the resiliency problem.

394
00:26:20,037 --> 00:26:20,867
you for attending.

395
00:26:21,837 --> 00:26:25,367
If you have any, questions,
please feel free to send it to me.

396
00:26:26,167 --> 00:26:26,537
Thank you.

397
00:26:26,877 --> 00:26:27,917
Have a great day guys.

398
00:26:28,137 --> 00:26:28,377
Bye.

