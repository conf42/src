1
00:00:00,500 --> 00:00:01,130
Speaker 42: Hi everyone.

2
00:00:01,789 --> 00:00:05,750
Welcome to Con 42 and logistics,
as well as in federal space.

3
00:00:06,350 --> 00:00:11,030
And the topic for my today's
talk is observability as a.

4
00:00:11,530 --> 00:00:15,930
Platform contract and how can we
implement a zero code instrumentation and

5
00:00:15,959 --> 00:00:18,930
enforce it through S-C-I-C-D pipelines?

6
00:00:19,920 --> 00:00:23,970
We're going to dive into different
problem statements, what exactly the

7
00:00:23,970 --> 00:00:25,134
problem statement we're trying to solve.

8
00:00:25,635 --> 00:00:29,625
What are the various solutions along
the way and how we have solved this

9
00:00:29,625 --> 00:00:34,635
in our organization, which can be
helpful and useful for not only our

10
00:00:34,635 --> 00:00:39,015
organization, but various organizations
and industries along the way.

11
00:00:39,825 --> 00:00:43,875
So with that context, let's dive in.

12
00:00:44,375 --> 00:00:46,325
So before we dive in.

13
00:00:47,270 --> 00:00:52,510
Let's understand what's the state of the
observability today which means we have to

14
00:00:52,510 --> 00:00:57,520
understand what is the problem statement,
what are we trying to solve, and what

15
00:00:57,520 --> 00:01:01,270
are the pain points in the industry
and in various organization today?

16
00:01:02,080 --> 00:01:08,180
So as you see on the screen, maturity
of the state of the observability today.

17
00:01:08,705 --> 00:01:14,315
Lies between these four blocks, and
to give you an, when we talk about

18
00:01:14,345 --> 00:01:19,555
observability various organization have
purchased and licensed various different

19
00:01:19,975 --> 00:01:25,645
observability tools starting from Datadog,
new Relic, Prometheus, Grafana, and

20
00:01:25,675 --> 00:01:29,755
many others, many million other tooling,
different variations of tooling along

21
00:01:29,755 --> 00:01:32,475
the way, which means every single tool.

22
00:01:33,225 --> 00:01:40,935
Would need a manual instrumentation, which
implies if you have to find a certain log

23
00:01:40,935 --> 00:01:47,295
or a trace or a metric, or any alerting
you want to create, it should be coded

24
00:01:47,565 --> 00:01:52,660
somewhere in your solution or in a part
of your application, which means it's

25
00:01:52,700 --> 00:01:54,455
a part of the manual instrumentation.

26
00:01:54,905 --> 00:01:57,065
And if you think from.

27
00:01:57,770 --> 00:02:02,000
At retrofitting this solution into
existing organization or large enterprise

28
00:02:02,000 --> 00:02:07,780
organizations who have millions and
millions of lines of code, it's very

29
00:02:07,780 --> 00:02:12,970
difficult task, a very daunting task,
which means if a part of a code is not

30
00:02:13,000 --> 00:02:16,555
instrumented, or you have not added a
line of code, which then generates a

31
00:02:16,555 --> 00:02:21,640
trace or generates a log, et cetera,
it's a very inconsistent coverage.

32
00:02:21,640 --> 00:02:25,390
So only those part which we
instrumentation has been added.

33
00:02:26,005 --> 00:02:26,964
Would get the coverage.

34
00:02:27,475 --> 00:02:31,154
Everything else becomes a
blind spot, which results into

35
00:02:31,454 --> 00:02:33,105
a very reactive debugging.

36
00:02:33,704 --> 00:02:38,475
When the fire comes or an incident
comes, you become a very reactive way

37
00:02:38,475 --> 00:02:41,825
of looking at things which means safe.

38
00:02:42,575 --> 00:02:45,845
Your instrumentation was already coded.

39
00:02:46,580 --> 00:02:48,710
Some part of it is coded
and some part is not.

40
00:02:48,710 --> 00:02:52,520
So if the issue comes in a very
different part of your stack where it

41
00:02:52,520 --> 00:02:57,440
was not covered, it becomes a reactive
approach of, Hey, let's go fix this.

42
00:02:57,650 --> 00:03:01,750
So it's a very reactive way of looking at
things, which can add a lot of frustration

43
00:03:01,750 --> 00:03:03,430
and a lot of incomplete solution.

44
00:03:03,930 --> 00:03:09,079
And team silos is also big one
which talks about how current

45
00:03:09,079 --> 00:03:10,859
teams platform teams would tell.

46
00:03:11,359 --> 00:03:14,929
Or think from a mindset
perspective, we do not understand

47
00:03:14,929 --> 00:03:16,579
the application codes context.

48
00:03:16,999 --> 00:03:19,879
We do not understand what part
of the code should be covered

49
00:03:19,879 --> 00:03:20,959
and should not be covered.

50
00:03:21,499 --> 00:03:25,239
And from an application team's
perspective, they may say it's a part

51
00:03:25,239 --> 00:03:27,159
of the platform team's responsibility.

52
00:03:27,159 --> 00:03:31,299
So that creates a very siloed approach
towards how we look at observability.

53
00:03:31,659 --> 00:03:36,129
So overall, if you look at these four
pillars, it becomes a very incomplete.

54
00:03:36,534 --> 00:03:40,374
Solution as well as a very
disturbing culture, as well as

55
00:03:40,374 --> 00:03:44,284
very disturbing and frustrating
experience from platform perspective,

56
00:03:44,284 --> 00:03:48,904
from an application perspective
and from a business perspective.

57
00:03:48,904 --> 00:03:52,084
It becomes a very challenging when
it comes to reactive debugging, which

58
00:03:52,084 --> 00:03:53,344
means it's a cost to the company.

59
00:03:53,344 --> 00:03:58,264
So various organizations, even the larger
enterprise, the larger the enterprises.

60
00:03:58,819 --> 00:04:02,089
The bigger the problem, the bigger
the scale of the problem becomes.

61
00:04:02,329 --> 00:04:05,439
So that is the core of the talk
today, and this is the problem

62
00:04:05,439 --> 00:04:09,029
statement, which exists not only one
enterprise, but many enterprises and

63
00:04:09,029 --> 00:04:10,319
how we're going to solve for this.

64
00:04:10,619 --> 00:04:12,369
So let's go into how
we're going to solve it.

65
00:04:12,869 --> 00:04:15,779
So as we talked about the
problem statement, which exist

66
00:04:15,779 --> 00:04:19,529
in various organization and
enterprises, let's now dive into

67
00:04:20,189 --> 00:04:21,419
what does the new model look like?

68
00:04:22,229 --> 00:04:23,520
It's called platform contract.

69
00:04:24,020 --> 00:04:29,270
And the core idea behind this is how
can the platform contract elevate our

70
00:04:29,270 --> 00:04:34,310
observability from an opt-in heroics
to an explicit platform responsibility?

71
00:04:34,910 --> 00:04:39,530
How do we transform it into a
guaranteed, which is provided to

72
00:04:39,650 --> 00:04:43,705
every service in exchange of the
kind of falling paved road, right?

73
00:04:44,405 --> 00:04:48,545
The approach for this is to
make observability as a first

74
00:04:48,545 --> 00:04:51,065
class platform rather than.

75
00:04:51,485 --> 00:04:54,095
An afterthought bolted
into the individual teams.

76
00:04:54,095 --> 00:05:00,215
So it doesn't mean if you have added
a core observability as part of your

77
00:05:00,215 --> 00:05:05,125
stack or a part of your application, you
would still get that observability as a

78
00:05:05,125 --> 00:05:10,255
platform contract, which then results into
a standardized instrumentation, patents.

79
00:05:10,755 --> 00:05:15,885
It results in a consistent signal
collection across various services, so

80
00:05:15,885 --> 00:05:18,495
that as a business, as an organization.

81
00:05:18,915 --> 00:05:23,265
You get visibility into all the
services rather than just some opt-in

82
00:05:23,265 --> 00:05:27,085
services, which had a thought of
observability as a first class citizen.

83
00:05:27,765 --> 00:05:31,635
You also have a measurable reli
requirements, reliability requirements,

84
00:05:31,635 --> 00:05:37,015
which is meeting your SLOs and meeting
your SLAs with your external customer.

85
00:05:37,614 --> 00:05:43,284
And then also there is a federated way of
doing a compliance verification as well.

86
00:05:43,585 --> 00:05:44,014
So these are.

87
00:05:44,574 --> 00:05:47,494
The platform contract which
we're talking about as an

88
00:05:47,494 --> 00:05:49,664
observability first class platform.

89
00:05:50,234 --> 00:05:54,825
And which would then result into
all of these outcomes as well.

90
00:05:55,215 --> 00:06:00,205
So let's also now dive into what
does it means when we talk about

91
00:06:00,205 --> 00:06:01,794
this platform contract in detail.

92
00:06:02,294 --> 00:06:03,495
So as we talked about.

93
00:06:04,020 --> 00:06:08,609
The platform contract, let's now
understand how does it tie together

94
00:06:08,789 --> 00:06:11,180
with the service layer and the contract
layer and guaranteed observability.

95
00:06:11,930 --> 00:06:14,414
So if you look at the
diagram here you would see.

96
00:06:15,159 --> 00:06:20,079
Four core rings where one is platform
layer, another is service layer or

97
00:06:20,079 --> 00:06:23,469
an app layer, and the contract layer,
and then the guaranteed observability.

98
00:06:23,969 --> 00:06:28,559
So if you look at the top two platform
layer as well as the service layer, they

99
00:06:28,559 --> 00:06:31,219
are tied together with the contract layer.

100
00:06:31,700 --> 00:06:35,110
And then if you look at the
service side as a outcome of this

101
00:06:35,110 --> 00:06:36,950
model a service, which is an.

102
00:06:37,525 --> 00:06:42,145
Service or an application would
get an observability as a outcome

103
00:06:42,205 --> 00:06:43,915
of this platform contract model.

104
00:06:44,094 --> 00:06:48,174
So in a nutshell, if you have to
understand this the core idea behind this

105
00:06:48,174 --> 00:06:54,354
is a service would get a comprehensive
observability without writing a single

106
00:06:54,354 --> 00:06:57,205
line of instrumentation code to get.

107
00:06:58,104 --> 00:07:02,824
Observability in terms of the logs,
the traces, the metrics kind of

108
00:07:02,824 --> 00:07:08,755
database call visibility and all
the observability artifacts and the

109
00:07:08,755 --> 00:07:12,385
platform would handle the collection of.

110
00:07:13,000 --> 00:07:16,150
All of these observability
metrics for you, right?

111
00:07:16,630 --> 00:07:21,940
And the contract layer would define
the expectation in terms of what

112
00:07:22,150 --> 00:07:26,230
should be the core expectation in
terms of whether it's S-L-A-S-L-O,

113
00:07:26,650 --> 00:07:29,350
how the platform should behave, or
the service should behave, et cetera.

114
00:07:29,590 --> 00:07:33,219
The contract would define those
expectation with respect to

115
00:07:33,219 --> 00:07:34,960
the observability in this case.

116
00:07:35,289 --> 00:07:35,485
And then.

117
00:07:36,445 --> 00:07:41,755
To make sure we enforce this as a
compliance, A-C-I-C-D, which is continuous

118
00:07:41,755 --> 00:07:43,345
integration and continuous deployment.

119
00:07:43,845 --> 00:07:48,704
Will become very handy for us to make
sure it is it is observed, it is enforced,

120
00:07:48,735 --> 00:07:53,275
and it is compliant so that it becomes a
first class citizen in every single time.

121
00:07:53,515 --> 00:07:55,975
And then we're gonna dive into
a little bit onto zero code

122
00:07:55,975 --> 00:07:56,934
instrumentation architecture.

123
00:07:57,674 --> 00:07:58,214
Next.

124
00:07:58,304 --> 00:07:58,844
So let's see.

125
00:07:58,844 --> 00:08:01,664
What does the zero code
instrumentation architecture look like?

126
00:08:02,164 --> 00:08:06,694
So if we start looking into a zero
code instrumentation architecture,

127
00:08:07,054 --> 00:08:11,460
there are three core pillars or
core components of the zero code

128
00:08:11,460 --> 00:08:16,020
instrumentation, architecture,
automated instrumentation, runtime

129
00:08:16,020 --> 00:08:18,990
side cars, and base container images.

130
00:08:19,500 --> 00:08:20,939
So let's talk about each one by one.

131
00:08:21,390 --> 00:08:23,760
So what does automated
instrumentation mean?

132
00:08:24,369 --> 00:08:29,049
It means we'll have open telemetry
agents and framework configurations,

133
00:08:29,439 --> 00:08:33,969
inject telemetry across languages
and protocols without code changes.

134
00:08:34,509 --> 00:08:40,989
As a developer or an application team or a
platform team, you have to write zero line

135
00:08:40,989 --> 00:08:44,139
of code for this instrumentation to work.

136
00:08:44,139 --> 00:08:45,879
That's why it's called
auto instrumentation.

137
00:08:45,879 --> 00:08:49,349
So there are various different
frameworks, Java Spring Boot Letter,

138
00:08:49,399 --> 00:08:53,599
or any other framework you're using
as a language, whether to come

139
00:08:53,599 --> 00:08:55,399
from java.net or other languages.

140
00:08:55,699 --> 00:09:00,139
So these agent has multi-language
support and they can be injected and

141
00:09:00,139 --> 00:09:03,109
configured according to what is needed.

142
00:09:03,109 --> 00:09:04,579
So that's why it's automated.

143
00:09:04,639 --> 00:09:06,259
In, in, in that way.

144
00:09:06,319 --> 00:09:10,219
You do not have to manually write
any line of code or observability

145
00:09:10,309 --> 00:09:15,349
stack to work, and inside card
pattern would basically capture.

146
00:09:15,849 --> 00:09:18,789
Network traffic, it would
capture database query, it would

147
00:09:19,089 --> 00:09:20,499
capture message interaction.

148
00:09:20,499 --> 00:09:24,169
It would capture your web
traffic and all the component.

149
00:09:24,469 --> 00:09:28,759
And if you look think from a
application standpoint, starting from

150
00:09:28,759 --> 00:09:37,129
a web layer to the middleware to a
database query, you get a visibility.

151
00:09:37,549 --> 00:09:42,679
For your stack end to end, not just
web layer, not just middleware, not

152
00:09:42,679 --> 00:09:45,799
just database queries, but end to end.

153
00:09:45,799 --> 00:09:49,039
And then there is a way to
correlate a web traffic to a

154
00:09:49,039 --> 00:09:50,939
database query to a middleware.

155
00:09:51,119 --> 00:09:55,300
And so you get an end-to-end visibility
with these run time side cards.

156
00:09:55,330 --> 00:09:58,810
And then you can build a lot of
alerting along the way in terms of

157
00:09:58,810 --> 00:10:03,040
those metrics traces, as well as the
logs and from the platform layer.

158
00:10:03,100 --> 00:10:04,870
If you talk about base container images.

159
00:10:05,370 --> 00:10:11,280
You as a platform team would ship a
pre-configured images, which has baked

160
00:10:11,280 --> 00:10:15,840
in observability component, which
ensures the consistent deployment.

161
00:10:16,260 --> 00:10:16,980
What does it mean?

162
00:10:17,670 --> 00:10:24,090
It means your base images, which are being
used for deployment of the service or an

163
00:10:24,090 --> 00:10:26,820
application to the production systems.

164
00:10:27,360 --> 00:10:32,070
It would have observability baked
in, which means we have enabled.

165
00:10:32,610 --> 00:10:36,750
Open telemetry agents, we have
enabled, what ports would it listen to?

166
00:10:36,900 --> 00:10:41,070
We have configured what kind of
metrics, traces, logs would be

167
00:10:41,070 --> 00:10:43,320
enabled disabled based on that.

168
00:10:43,650 --> 00:10:48,890
So that as an application team,
as soon as you create a PR and

169
00:10:48,890 --> 00:10:53,720
merge your PR and the pipeline
fronts and it gets that base image.

170
00:10:54,305 --> 00:10:58,865
Your observability is already baked
in to that system, so that's why it

171
00:10:58,865 --> 00:11:00,575
becomes a zero code instrumentation.

172
00:11:00,755 --> 00:11:05,435
It abstracts away all the information
from the application teams.

173
00:11:05,625 --> 00:11:09,945
And if you look at the diagram
below, your application code goes in,

174
00:11:10,245 --> 00:11:15,135
instrumentation layer comes in terms
of the base container images, and then

175
00:11:15,315 --> 00:11:20,185
that has already baked in information
about your observability components.

176
00:11:20,740 --> 00:11:26,500
Observably backend gets executed, and
then at the end of this infinity loop

177
00:11:26,550 --> 00:11:29,820
this cycle, you can analyze and improve.

178
00:11:30,060 --> 00:11:31,710
How do you want to now configure?

179
00:11:31,710 --> 00:11:35,060
And then you'd start getting the
results as part of your stack.

180
00:11:35,060 --> 00:11:40,200
Whether you have a tool called Datadog,
new Relic, Grafana or any other million

181
00:11:40,200 --> 00:11:45,420
other tooling available, you can start
analyzing and improving those results as a

182
00:11:45,420 --> 00:11:47,340
part of this instrumentation architecture.

183
00:11:47,840 --> 00:11:53,020
As we talked about the zero code
instrumentation framework, now the next

184
00:11:53,020 --> 00:11:58,200
question comes out, how do I support my
application teams or my platform team?

185
00:11:58,470 --> 00:12:01,860
What kind of languages do I
support and how do I, Hey, we

186
00:12:01,860 --> 00:12:04,810
are in A-W-S-G-C-P versus Azure.

187
00:12:05,430 --> 00:12:07,240
How do we get this working?

188
00:12:07,990 --> 00:12:09,490
So good news.

189
00:12:10,040 --> 00:12:13,410
It is available for multi-language
as well as multi-cloud.

190
00:12:13,830 --> 00:12:15,930
And I understand the reality we live in.

191
00:12:15,990 --> 00:12:19,530
We live in a very heterogeneous
environment, which definitely re requires

192
00:12:19,530 --> 00:12:21,620
a strategic planning along the way.

193
00:12:21,680 --> 00:12:24,810
So that's why when I talked about
Opal Telemetry agents, it has

194
00:12:24,810 --> 00:12:31,140
support for automatic instrumentation
starting from Java, Python, node.

195
00:12:31,680 --> 00:12:36,865
Go T net and many more languages
including PHP and others as well.

196
00:12:37,375 --> 00:12:41,595
And when you talk about cloud agnostic
solutioning we have given done a

197
00:12:41,595 --> 00:12:46,305
consistent deployment across AWS
Azure GCP using portable containers

198
00:12:46,605 --> 00:12:47,925
and standardized collectors.

199
00:12:47,925 --> 00:12:52,465
So if you use docker containers and any
of those technologies behind the scene,

200
00:12:53,185 --> 00:12:56,755
it can be deployed to E-K-S-A-K-S-G-K-S.

201
00:12:57,135 --> 00:13:02,435
With the very cloud agnostic approach
and for our organization, we have

202
00:13:02,465 --> 00:13:06,175
instrumented successfully instrumented
200 plus services across three

203
00:13:06,175 --> 00:13:09,595
cloud with 99.2% and more coverage.

204
00:13:09,845 --> 00:13:12,455
Which gives us a good confidence
in terms of multi-language

205
00:13:12,455 --> 00:13:14,135
and the multi-cloud reality.

206
00:13:14,135 --> 00:13:18,035
So it becomes a very widespread
solution, which all the

207
00:13:18,035 --> 00:13:19,545
organization can benefit from.

208
00:13:20,045 --> 00:13:25,145
As we talked about the multi-language
and the multi-cloud support, it's always

209
00:13:25,145 --> 00:13:30,245
important to, it's also important to talk
about the observability contracts and

210
00:13:30,245 --> 00:13:32,375
the first class artifacts it generates.

211
00:13:32,675 --> 00:13:37,455
So if you look at the diagram on the slide
the core component would be defining.

212
00:13:38,130 --> 00:13:42,520
The contract definition, we defined
the signal requirements as a

213
00:13:42,700 --> 00:13:49,000
specification and the validation tool,
and think this from a platform as

214
00:13:49,000 --> 00:13:50,650
well as from the service perspective.

215
00:13:51,150 --> 00:13:55,380
The idea here is we want to have.

216
00:13:55,880 --> 00:13:59,770
A very low noise and high impact signals.

217
00:14:00,010 --> 00:14:06,130
So we want to understand what signal
would define a service to be reliable

218
00:14:06,510 --> 00:14:11,880
up as well as the co deficiency, or if
the system is down, what would that be?

219
00:14:12,030 --> 00:14:15,450
And then from an SLO specification,
it's a service level.

220
00:14:16,120 --> 00:14:17,375
At a service level, we need to define.

221
00:14:17,875 --> 00:14:19,045
What kind of SLAs?

222
00:14:19,075 --> 00:14:24,215
SLOs we want to define which means
whether we want to have a 99.9% uptime,

223
00:14:24,545 --> 00:14:26,495
whether we wanna define P 95 latency.

224
00:14:26,965 --> 00:14:31,565
So all of those becomes a manifest
and pipelines, which is basically

225
00:14:31,565 --> 00:14:33,305
version controlled agreement.

226
00:14:33,305 --> 00:14:36,945
So think of this as think of this as
contract, which sits alongside your

227
00:14:36,945 --> 00:14:41,985
manifest as well as pipeline, and they're
very much version controller agreements.

228
00:14:42,045 --> 00:14:42,515
And then.

229
00:14:43,330 --> 00:14:49,180
As a result of this contract would
transform your vague reliability goal

230
00:14:49,210 --> 00:14:55,570
of, Hey, my website or my service is
slow versus fast into a measurable

231
00:14:55,570 --> 00:14:59,110
and enforced agreement that integrate
directly into your deployment pipeline.

232
00:14:59,110 --> 00:15:05,490
Then you can go with a confidence and
say, my website's P 95 latency is.

233
00:15:05,990 --> 00:15:10,520
Point three millisecond, 0.1
millisecond, so on and so forth.

234
00:15:10,520 --> 00:15:15,530
So rather than this vague goal of
being a fast and slow, you can pinpoint

235
00:15:15,630 --> 00:15:17,585
to that data with the artifact.

236
00:15:17,585 --> 00:15:20,785
So that's the core objective of
these observability contracts.

237
00:15:21,285 --> 00:15:24,505
So as we talked about
observability, contracts we

238
00:15:24,505 --> 00:15:26,465
should also start thinking about.

239
00:15:27,095 --> 00:15:29,635
How should we enforce those through CICD?

240
00:15:30,145 --> 00:15:35,605
So think of contract enforcements
similar to CICD enforcement for

241
00:15:36,025 --> 00:15:44,245
security protocol if, or a testing
protocol, or say for example, if your

242
00:15:44,485 --> 00:15:51,825
repo has a testing framework and the
test coverage is not about 75 to 80%.

243
00:15:52,325 --> 00:15:54,025
We fail the pipeline.

244
00:15:54,525 --> 00:15:58,185
So similarly, these observability
contracts should also be

245
00:15:58,185 --> 00:15:59,565
enforced through CICD.

246
00:15:59,955 --> 00:16:02,715
So take the typical example
of your source code.

247
00:16:02,865 --> 00:16:07,925
Your code is committed, it is pushed
the changes are pushed to the repo.

248
00:16:08,635 --> 00:16:14,105
The contract validation which is the
core component of this where you would

249
00:16:14,105 --> 00:16:16,925
verify schema and the a PA contracts.

250
00:16:17,315 --> 00:16:18,755
The contracts get validated.

251
00:16:19,715 --> 00:16:25,355
You would also do instrumentation
check to ensure the observability hook

252
00:16:25,415 --> 00:16:32,580
exists the pre-baked in image exists,
and then the pipeline moves forward.

253
00:16:33,000 --> 00:16:38,245
So the core element behind
this is if the contract.

254
00:16:38,745 --> 00:16:44,995
Is the validation fails or schema fails,
or contract enforcement or contract

255
00:16:45,085 --> 00:16:47,335
validation fails in the pipeline.

256
00:16:48,265 --> 00:16:51,295
There should be a way to
fail the pipeline similar to.

257
00:16:52,255 --> 00:16:55,885
If the test coverage is not
more than 75% or if there is

258
00:16:55,885 --> 00:16:59,365
a core security vulnerability,
and then the pipeline fails.

259
00:16:59,445 --> 00:17:02,625
Similarly so think from a mental
model perspective, your contract

260
00:17:02,625 --> 00:17:06,545
enforcement through CICD, similar
to how a testing or a security or a

261
00:17:06,545 --> 00:17:10,195
vulnerable bug would be enforced to
the, through the continuous integration,

262
00:17:10,195 --> 00:17:11,515
continuous deployment pipeline.

263
00:17:12,015 --> 00:17:12,555
As we talked about.

264
00:17:13,125 --> 00:17:17,415
Some of the enforcement through
the CICD, the another objective

265
00:17:17,450 --> 00:17:18,925
or a question we would ask is.

266
00:17:19,425 --> 00:17:21,260
What are, what is the data telling us?

267
00:17:21,270 --> 00:17:25,329
What kind of results have we seen with
those changes in our organization?

268
00:17:25,719 --> 00:17:29,630
So the diagram on the slide right
now talks about in the left hand

269
00:17:29,630 --> 00:17:32,870
side and the right hand side around
before and the after of this.

270
00:17:32,870 --> 00:17:33,890
And then the first.

271
00:17:34,265 --> 00:17:37,945
AF talks about the manual and
fragmented approach, and then, and

272
00:17:37,945 --> 00:17:40,825
the right hand side talks about the
automated and the unified approach.

273
00:17:40,825 --> 00:17:46,085
The numbers we have seen is from the
SLO, which was at 23% before, and the

274
00:17:46,085 --> 00:17:48,755
meantime to respond was 45 minute.

275
00:17:49,400 --> 00:17:51,950
Both of those numbers has
significantly improved.

276
00:17:52,100 --> 00:17:56,450
I would say there is a 60 to 70%
improvement on those numbers.

277
00:17:56,960 --> 00:18:02,270
And not only those numbers have improved,
but we have also seen our failed

278
00:18:02,270 --> 00:18:07,520
deployment has gone to 12%, which was
at a very high rate of 30 to 40% before.

279
00:18:07,910 --> 00:18:15,110
And then similarly, manual rollbacks are
78% less now, where we'd have to manually.

280
00:18:15,345 --> 00:18:19,335
Roll back anything, which means we
have now higher confidence in terms

281
00:18:19,335 --> 00:18:25,425
of deploying our services because we
have observability in place for those.

282
00:18:25,635 --> 00:18:29,235
So that observability gives us a
good confidence of deploying with

283
00:18:29,475 --> 00:18:33,015
confidence, less failure rate our.

284
00:18:33,920 --> 00:18:40,070
Time to respond for incident has improved
significantly, and our SLO metrics have

285
00:18:40,070 --> 00:18:44,930
improved drastically, and our customers
are much more happier with those results.

286
00:18:44,945 --> 00:18:48,495
So these are some of the results I wanted
to share so that it gives perspective

287
00:18:48,495 --> 00:18:53,014
on what are the key metrics to look
for when you think from a success

288
00:18:53,014 --> 00:18:57,485
metric or KPIs to define to your
leadership in various organization.

289
00:18:57,485 --> 00:18:58,655
And then how, and.

290
00:18:58,885 --> 00:19:02,245
It would apply to different organization
in a different way, but there is a

291
00:19:02,455 --> 00:19:07,254
costing related to number of resources
allocated to it, number of licensing

292
00:19:07,254 --> 00:19:12,024
allocated to it, and specifically if
you can save developers time in terms

293
00:19:12,024 --> 00:19:14,125
of the failed deployment and rollbacks.

294
00:19:14,959 --> 00:19:21,570
That itself translate into multimillion
dollars every single year, depending on

295
00:19:21,610 --> 00:19:26,410
the different organizations and different
enterprises and the cost saving of that as

296
00:19:26,410 --> 00:19:31,370
the enterprises becomes larger and larger,
the and exponential in that case as well.

297
00:19:31,870 --> 00:19:35,429
As we talked about some of
the results in a brief way.

298
00:19:35,729 --> 00:19:39,679
So let's zoom a little bit around
what does it mean from wake targets

299
00:19:39,679 --> 00:19:40,969
to the measurable agreements?

300
00:19:41,360 --> 00:19:45,030
So as we touch a little bit around
being a web being a slow website

301
00:19:45,030 --> 00:19:49,230
to the fast website or to a slow
service, to a fast service or

302
00:19:49,410 --> 00:19:51,540
somewhat available to available.

303
00:19:51,845 --> 00:19:57,585
In terms of P 95 or P 99 response time,
and then what is our meantime to respond

304
00:19:57,615 --> 00:19:59,755
to a specific service or incident.

305
00:19:59,755 --> 00:20:05,695
So the core idea behind SLOs
is from big target to vegetable

306
00:20:05,695 --> 00:20:07,210
agreements for our customers.

307
00:20:08,105 --> 00:20:12,105
For our stakeholders, for
engineers, for platform to everyone

308
00:20:12,105 --> 00:20:13,425
in the organization, right?

309
00:20:13,845 --> 00:20:16,995
So when we talk about these
numbers, so what we have

310
00:20:16,995 --> 00:20:19,605
achieved is 99.9% availability.

311
00:20:19,605 --> 00:20:22,965
Which basically target encoded
directly in the contracts.

312
00:20:23,235 --> 00:20:23,700
So 22.

313
00:20:24,525 --> 00:20:30,615
50 millisecond latency when it comes to P
99 or P 95 in terms of the time enforced

314
00:20:30,615 --> 00:20:36,795
for the latency SLO, when it comes to the
added budgets, when in the range of 0.1%,

315
00:20:37,215 --> 00:20:41,325
and when it comes to the reduced MTTR
talking about 73% in terms of after the

316
00:20:41,325 --> 00:20:43,575
contact was implementation implemented.

317
00:20:44,085 --> 00:20:48,310
So if you look from a very traditional
versus a contract based approach,

318
00:20:48,920 --> 00:20:52,605
we're talking about manual as
tracking, which means not only.

319
00:20:53,105 --> 00:20:57,935
We have a wastage of time in terms of,
we don't have a way to measure it, but

320
00:20:57,935 --> 00:21:01,715
even measuring it manually wastes more
time for the developers and the teams.

321
00:21:02,435 --> 00:21:06,635
We have a lack of automated SLO
enforcement 80% or more, which is, I think

322
00:21:07,205 --> 00:21:09,635
for us it was like 85% or plus incident.

323
00:21:10,400 --> 00:21:15,350
SLO context in terms of db layer, web
layer, middleware layer, and how does

324
00:21:15,350 --> 00:21:20,300
it translate into incident and how do we
respond back to with a better context.

325
00:21:20,300 --> 00:21:22,670
So that's a traditional
approach, flaw flaws.

326
00:21:23,090 --> 00:21:26,775
And when we talk about contract based,
a hundred percent are deployments.

327
00:21:26,775 --> 00:21:28,635
Are SLO correlated to correlate?

328
00:21:29,389 --> 00:21:32,894
Takes 10 to 20 minutes and to
identify root cause it could be

329
00:21:32,894 --> 00:21:34,244
anywhere between five to 10 minutes.

330
00:21:34,244 --> 00:21:38,924
So that very much contract based numbers,
which gives us a good confidence in terms

331
00:21:38,924 --> 00:21:43,484
of deployment, in terms of coating issue,
in terms of identifying root cause.

332
00:21:43,694 --> 00:21:49,264
So we take away the vagueness from
the equation and convert that into a

333
00:21:49,264 --> 00:21:53,514
measurable agreement when we follow
visibility with the platform contracts.

334
00:21:54,014 --> 00:21:57,314
As we talked about some of the
numbers in the previous slide.

335
00:21:57,475 --> 00:22:01,985
So let's start looking into how does the
release automation integration would work?

336
00:22:02,405 --> 00:22:05,535
What kind of deployment
strategies would be implemented

337
00:22:05,655 --> 00:22:07,495
when it pertains to contracted?

338
00:22:07,975 --> 00:22:09,775
The overall baseline is the following.

339
00:22:10,465 --> 00:22:14,365
So you have a baseline capture,
which means you basically current

340
00:22:14,875 --> 00:22:17,995
record your current SLI performance
before deployment begins.

341
00:22:17,995 --> 00:22:23,665
Canary deployment is usually a good way
to, to look at this because the core

342
00:22:23,725 --> 00:22:27,895
idea behind canary deployment is you
route a very small percentage of your

343
00:22:27,895 --> 00:22:30,415
traffic to a new versions, new version.

344
00:22:30,775 --> 00:22:34,885
So let's say for example, if a service
teams comes back with a new version, you.

345
00:22:35,425 --> 00:22:42,895
You deploy in a canary deployment fashion,
you start from 5% of your traffic split,

346
00:22:43,165 --> 00:22:47,155
or it can be 1%, 2% de depending on
what the organization is comfortable

347
00:22:47,155 --> 00:22:48,685
with or the teams are comfortable with.

348
00:22:49,045 --> 00:22:53,175
But let's say for an example, you
start with a 5% split, and then

349
00:22:53,675 --> 00:22:57,295
based on that 5% split, you can also.

350
00:22:57,910 --> 00:22:59,320
An SLO evaluation.

351
00:22:59,680 --> 00:23:03,640
So compare your canary metrics against
your baseline or your production

352
00:23:03,640 --> 00:23:07,700
live traffic, which is 95% of
your traffic and burn rate policy.

353
00:23:08,030 --> 00:23:15,680
So based on those two metrics, you
would be able to understand what my P 95

354
00:23:15,680 --> 00:23:18,175
latency is, what my metrics look like.

355
00:23:18,415 --> 00:23:19,975
Have we improved our SLO?

356
00:23:20,005 --> 00:23:21,925
Have we not improved our SLO?

357
00:23:21,925 --> 00:23:24,385
So basically testing this feature.

358
00:23:24,885 --> 00:23:32,865
In comparison with your SLO evaluation
and if your numbers look good, you promote

359
00:23:33,315 --> 00:23:38,835
and then increase the traffic to 50% and
then a hundred percent, which is full

360
00:23:38,835 --> 00:23:41,115
rollout, which is a successful release.

361
00:23:41,385 --> 00:23:46,305
And if it does not satisfy your
SLO requirement or degrade your

362
00:23:46,570 --> 00:23:48,305
SLAs and SLOs, we can roll back.

363
00:23:48,695 --> 00:23:53,115
And then only 5% of the test of
the traffic was exposed to it.

364
00:23:53,445 --> 00:23:55,615
You tested it and then defined.

365
00:23:55,615 --> 00:24:00,385
So that not only gives you confidence
in your deployment, but it also

366
00:24:00,385 --> 00:24:05,605
tells you your SLOs and SLAs and
how your website is performing.

367
00:24:05,905 --> 00:24:08,285
So it not only gives you a good.

368
00:24:08,490 --> 00:24:12,250
A way to look at your services.

369
00:24:12,370 --> 00:24:15,950
It also improves your SLOs and
you can make these real time

370
00:24:15,950 --> 00:24:18,180
decisions based on these metrics.

371
00:24:18,230 --> 00:24:19,400
So not only your.

372
00:24:19,990 --> 00:24:26,190
Taking away the vague nature of the
reliability metrics, but also taking

373
00:24:26,280 --> 00:24:30,310
away the vague nature of how you
deploy with confidence so that gives

374
00:24:30,310 --> 00:24:34,180
you a good baseline to deploy with
confidence and to deploy with better

375
00:24:34,180 --> 00:24:36,875
SLOs as well as we looked into.

376
00:24:37,375 --> 00:24:43,195
The deployment patents and the confidence
with the SLO contracts in in, in

377
00:24:43,195 --> 00:24:47,225
connection with community deployment and
how it helps in the confidence of the

378
00:24:47,225 --> 00:24:49,315
deployment with respect to SLOs and SLAs.

379
00:24:49,325 --> 00:24:52,235
Let's also look into accelerated
in incident response.

380
00:24:52,295 --> 00:24:56,405
So a kind of streamlined
resolution before and after, right?

381
00:24:56,765 --> 00:24:59,525
So from a symptom to root
these unified workflows.

382
00:25:00,080 --> 00:25:06,590
Would use existing traces, existing
metrics, which would help us in rapid

383
00:25:06,590 --> 00:25:08,870
identification of problematic changes.

384
00:25:09,230 --> 00:25:15,490
So as we talked about doing a Canadian
leases of 5% or 10% or 1%, depending

385
00:25:15,490 --> 00:25:17,080
on your organization's foot level.

386
00:25:18,040 --> 00:25:21,159
As the new data would
start coming into this.

387
00:25:21,159 --> 00:25:26,110
So we would have a way to
automatically correlate to a specific

388
00:25:26,290 --> 00:25:28,340
deployment through a version tag.

389
00:25:28,670 --> 00:25:32,690
Say a version one is deployed
and version two comes in and

390
00:25:32,690 --> 00:25:35,580
version two has 5% traffic split.

391
00:25:36,180 --> 00:25:42,990
As we connect these different versions,
this degraded SLI would then get tied

392
00:25:42,990 --> 00:25:45,540
to that particular version tag and then.

393
00:25:46,050 --> 00:25:50,130
If you have to do a trace analysis,
a trace, if you have to expand a

394
00:25:50,130 --> 00:25:54,140
little bit is basically multiple
span, collection of multiple span.

395
00:25:54,380 --> 00:25:56,330
So you drill into a specific span.

396
00:25:56,570 --> 00:26:01,040
It would show where was the web
call, where was the middleware call,

397
00:26:01,370 --> 00:26:04,250
where was the database call, and
what, and not only where was the

398
00:26:04,250 --> 00:26:07,590
database call, what was exactly the.

399
00:26:08,570 --> 00:26:13,760
The database query, which ran and
based on advanced observability

400
00:26:13,760 --> 00:26:20,030
tools, you can also connect into
what query ran for what second, and

401
00:26:20,030 --> 00:26:21,770
whether that was indexed or not.

402
00:26:21,830 --> 00:26:25,430
Adding Datadog and many other tools
provide that kind of capabilities

403
00:26:25,430 --> 00:26:28,450
where you can actually enable the
database monitoring, which then.

404
00:26:29,035 --> 00:26:31,245
Talks about and it has a flamed graft.

405
00:26:31,275 --> 00:26:32,325
It has a spam graft.

406
00:26:32,355 --> 00:26:36,165
You can replay many of those and have
a lot of correlation along the way.

407
00:26:36,535 --> 00:26:40,795
Which gives you a very
accelerated incident response.

408
00:26:41,125 --> 00:26:44,525
I have personally been in many
of the situation where we had

409
00:26:44,525 --> 00:26:48,785
an incident and then I would, I
was able to pinpoint directly.

410
00:26:49,715 --> 00:26:53,495
From timestamp perspective, from
database query perspective, as well

411
00:26:53,495 --> 00:26:55,055
as from a web call perspective.

412
00:26:55,445 --> 00:27:00,585
So I was able to take a query, run it
back to an application code, was able

413
00:27:00,585 --> 00:27:05,925
to tie together to what deployment
tag was culprit for that incident.

414
00:27:05,930 --> 00:27:07,545
And then also going back to.

415
00:27:08,115 --> 00:27:11,665
The application code to a to a
GitHub in terms of what exactly

416
00:27:11,665 --> 00:27:14,865
was the application code, which
was responsible for that error.

417
00:27:15,315 --> 00:27:18,885
So giving you that level of
visibility would accelerate the

418
00:27:18,885 --> 00:27:21,045
incident response tremendously.

419
00:27:21,045 --> 00:27:24,075
Would used to take you a hundred
percent of the time, would literally

420
00:27:24,075 --> 00:27:26,410
take you 10 or 15% of the time.

421
00:27:26,775 --> 00:27:29,635
So it's a huge saving in terms
of the incident response and.

422
00:27:29,980 --> 00:27:34,710
Not to mention whenever the incident
happens, 50 or a hundred people

423
00:27:34,710 --> 00:27:40,080
joining the call and taking arts to,
to resolve has a huge cost downside

424
00:27:40,080 --> 00:27:41,580
to it, or potential to it as well.

425
00:27:41,880 --> 00:27:46,730
So it, it gives you that accelerated
incident response in many ways.

426
00:27:46,730 --> 00:27:49,680
So it's a win-win
situation when it comes to.

427
00:27:50,210 --> 00:27:55,580
Your deployment confidence as well as
accelerated incident response as well.

428
00:27:56,080 --> 00:27:58,930
So as we talked about the accelerated,
it's the incident response.

429
00:27:59,000 --> 00:28:02,200
It's very important to talk
about the adoption pattern that

430
00:28:02,500 --> 00:28:07,210
worked for us and can work for
various different organization

431
00:28:07,210 --> 00:28:08,590
and enterprises along the way.

432
00:28:08,640 --> 00:28:12,240
Core components or core pillars you
would see is the pilot expansion and

433
00:28:12,240 --> 00:28:14,800
scale in different implementation phases.

434
00:28:15,230 --> 00:28:19,090
In the pilot phase, I would start
with two or three, which would take

435
00:28:19,300 --> 00:28:23,410
roughly around somewhere between
four to six weeks and expansion

436
00:28:23,590 --> 00:28:28,240
would be 20 to 10 to 20 services,
two to three months of timeframe.

437
00:28:28,240 --> 00:28:34,240
And then as you start measuring the
result of pilot result of your expansion.

438
00:28:34,620 --> 00:28:37,950
You would have early adopters,
you would have some teams which

439
00:28:37,950 --> 00:28:41,190
has done a little bit of exploring
along the way, and you have done

440
00:28:41,190 --> 00:28:42,900
some experimentation along the way.

441
00:28:42,900 --> 00:28:44,310
You have some learnings along the way.

442
00:28:44,310 --> 00:28:46,380
What works for your organization
and what does not work.

443
00:28:46,380 --> 00:28:50,520
Your work, work for your organization and
some constraints to work with in terms

444
00:28:50,520 --> 00:28:55,260
of security, adoption, different teams,
different adoption practices, et cetera.

445
00:28:55,770 --> 00:29:00,270
And then that's the point when you take
these learnings and then scale, which is

446
00:29:00,270 --> 00:29:02,740
an organization wide ongoing, and then.

447
00:29:03,240 --> 00:29:06,810
Start with gradual introduction,
which is start with pilot services,

448
00:29:06,810 --> 00:29:08,730
expand and avoid any roadblocks.

449
00:29:09,090 --> 00:29:13,530
And then when we start to scale,
you also want to start doing

450
00:29:13,530 --> 00:29:14,910
in engineering train training.

451
00:29:14,910 --> 00:29:19,350
So start with educating teams on
SLO through workshops, through

452
00:29:19,350 --> 00:29:22,320
different documentation, through
embedded platform engineers.

453
00:29:22,770 --> 00:29:26,760
What worked for us was an
organization-wide workshop,

454
00:29:26,795 --> 00:29:28,855
so we would engage.

455
00:29:29,250 --> 00:29:34,380
Two to three teams, every workshop
starting from a signup sheet,

456
00:29:34,740 --> 00:29:37,905
we would engage two or three
different teams in a workshop.

457
00:29:38,405 --> 00:29:43,295
And then we would pick up their use
cases, the metrics they're interested

458
00:29:43,295 --> 00:29:47,365
in the traces they're interested in, so
that it becomes interesting enough rather

459
00:29:47,365 --> 00:29:49,465
than a generic training or generic talk.

460
00:29:49,465 --> 00:29:54,295
And it solves their use cases
and problems and service level

461
00:29:54,295 --> 00:29:57,775
metrics, which gives them, which
keeps them engaged and involved.

462
00:29:58,030 --> 00:30:02,780
In the process, and when one team
starts seeing the results and problems

463
00:30:02,780 --> 00:30:07,650
being addressed, that's when the
adoption becomes contagious and it

464
00:30:07,650 --> 00:30:09,580
starts scaling out very rapidly.

465
00:30:09,580 --> 00:30:12,130
So that was one of the lessons which
we learned through the workshops,

466
00:30:12,430 --> 00:30:16,120
keep it very pertaining to two
or three teams, target specific

467
00:30:16,600 --> 00:30:18,430
problem statement for that service.

468
00:30:18,535 --> 00:30:22,745
Team and let it expand from that point.

469
00:30:22,745 --> 00:30:26,165
Have early adopters, champions,
and then start clear, start

470
00:30:26,165 --> 00:30:28,835
creating community along with a
good documentation along the way.

471
00:30:28,835 --> 00:30:34,115
So I think gradual introduction and
training with a specific use cases was

472
00:30:34,115 --> 00:30:37,475
the adoption pattern which works and
then can be very well implemented in

473
00:30:37,905 --> 00:30:39,875
many organizations and enterprises.

474
00:30:40,375 --> 00:30:41,755
As we touched a little bit on around.

475
00:30:42,505 --> 00:30:46,305
The adoption pattern, which
works I wanted to share about the

476
00:30:46,305 --> 00:30:48,675
implementation across various industries.

477
00:30:48,735 --> 00:30:52,675
I, I've been involved in starting
from financial services to

478
00:30:52,705 --> 00:30:54,905
retail as well as in logistics.

479
00:30:55,245 --> 00:31:00,900
In financial services, we have achieved
99.9% of traceability across 2.3

480
00:31:00,900 --> 00:31:03,435
million daily transactions in retail.

481
00:31:03,665 --> 00:31:08,075
Maintain 99.95% uptime
during Black Friday with.

482
00:31:08,900 --> 00:31:11,390
X normal traffic using
added budget policies.

483
00:31:11,660 --> 00:31:15,810
An e-commerce platform used added
budgets to balance the feature velocity

484
00:31:15,810 --> 00:31:17,580
with customer experience during.

485
00:31:18,255 --> 00:31:22,245
Peak shopping periods, we're
talking about 12 x 13 x, 14 x of

486
00:31:22,245 --> 00:31:25,885
the usual traffic into the retail
and then e-commerce space as well.

487
00:31:26,225 --> 00:31:30,585
Even in the logistics and maritime
instrumented four 50 plus microservices

488
00:31:30,885 --> 00:31:35,265
with zero code changes, which
reduced our deployment time by 40%.

489
00:31:35,315 --> 00:31:39,255
So these are various industries I've been
part of where we've implemented this.

490
00:31:39,255 --> 00:31:41,660
So that's why it gives us
a good confidence that.

491
00:31:42,275 --> 00:31:46,245
The adoption pattern works and then,
and the results also come through.

492
00:31:46,245 --> 00:31:48,995
So these are some of the, kind of
the examples and some of the data

493
00:31:49,085 --> 00:31:52,900
that I wanted to share for all of
us as to to use and learn from.

494
00:31:53,400 --> 00:31:56,670
So we talked about adoption pattern,
we talked about some numbers and

495
00:31:56,670 --> 00:31:57,900
some of the things we have done.

496
00:31:57,930 --> 00:32:00,960
The million dollar question
is, how does this blue.

497
00:32:01,350 --> 00:32:03,960
Print work for our success, right?

498
00:32:03,960 --> 00:32:05,265
For platform observability.

499
00:32:05,580 --> 00:32:09,930
So this is, if you look in another diagram
on the right hand side, these are some

500
00:32:09,930 --> 00:32:14,670
of the blueprint or kind of paved road
we should look into, which would give

501
00:32:14,670 --> 00:32:17,830
you, so these core five points, right?

502
00:32:18,649 --> 00:32:19,075
Which would give.

503
00:32:19,760 --> 00:32:23,209
The blueprint for the success in
your organization and navigation

504
00:32:23,209 --> 00:32:26,840
through your organization for platform
observability, zero code instrumentation

505
00:32:27,529 --> 00:32:31,039
deployed zero code instrumentation
with open elementary agents and

506
00:32:31,039 --> 00:32:33,979
base images, defining contracts.

507
00:32:34,009 --> 00:32:37,009
Define your SLOs definition,
your signal requirement.

508
00:32:37,070 --> 00:32:39,949
Work with your application
teams on what are the high.

509
00:32:40,624 --> 00:32:46,114
Signal, low noise signals, which
are critical for the success, bread

510
00:32:46,114 --> 00:32:50,404
and butter of that service, CICD
integration, have a validation.

511
00:32:50,404 --> 00:32:53,384
Gates enforce those gates along the way.

512
00:32:53,934 --> 00:32:54,984
Production monitoring.

513
00:32:55,884 --> 00:33:00,979
Have SLO tracking have added budgets along
the way, which will help to balance out

514
00:33:01,029 --> 00:33:05,799
and for the SLO tracking whether we are on
track or not, and continuous improvement.

515
00:33:06,299 --> 00:33:11,969
Continue to improve, start small,
iterate over it over and over again

516
00:33:12,179 --> 00:33:14,849
until you get to the final destination.

517
00:33:14,849 --> 00:33:19,959
So these are the kind of blueprint of
the success which would one by one.

518
00:33:20,874 --> 00:33:24,514
Day by day, iteration by
iteration can be improved on.

519
00:33:24,964 --> 00:33:28,284
And then these are the core pillars, which
would help us to get to the final state

520
00:33:28,284 --> 00:33:30,114
of platform observability with zero code.

521
00:33:30,614 --> 00:33:35,989
So once we follow those blueprint, what
is the end outcome we're looking at?

522
00:33:36,334 --> 00:33:39,334
So let's look at if we enable
this at organization wide

523
00:33:39,334 --> 00:33:41,524
scale, what are we looking at?

524
00:33:41,584 --> 00:33:44,404
We're looking at a measurable reliability.

525
00:33:45,229 --> 00:33:50,119
We are looking at continuous improvement,
we're looking at safe velocity and

526
00:33:50,119 --> 00:33:52,129
we're also looking at the team autonomy.

527
00:33:52,879 --> 00:33:59,629
And if you start looking back to the
problem statement we started with was.

528
00:34:00,004 --> 00:34:05,374
These four pillars in terms of
there's a team silos that is not

529
00:34:05,674 --> 00:34:10,784
very difficult or easy way to deploy
confidently with automated rollback.

530
00:34:11,294 --> 00:34:15,754
And then how do we make sure our manual
instrumentation where the insufficient

531
00:34:15,754 --> 00:34:16,889
coverage, et cetera, et cetera.

532
00:34:16,994 --> 00:34:20,264
So the laundry list of
problems is a huge to tap into.

533
00:34:20,314 --> 00:34:24,384
So four pillars of coverage,
manual instrumentation, and then.

534
00:34:24,984 --> 00:34:28,034
How do you deploy with confidence
and team autonomy and very

535
00:34:28,034 --> 00:34:29,834
fragmented solution overall.

536
00:34:29,924 --> 00:34:34,064
So this is what gives you that enable,
that will enable the organization wide

537
00:34:34,064 --> 00:34:42,374
scale with 99.2% of service coverage,
73% of faster MTTR from 45 minutes to 12

538
00:34:42,374 --> 00:34:47,209
minutes for your incident or meantime to
respond confidence of your deployment.

539
00:34:47,709 --> 00:34:52,469
85% or more, which is SLO driven releases,
would give you that confidence, which we

540
00:34:52,469 --> 00:34:56,339
talked about during the canary releases
and how we can deploy with confidence

541
00:34:56,669 --> 00:35:01,469
and 40% or more faster delivery,
which means you reduce the manual

542
00:35:01,469 --> 00:35:06,209
validation, you rely on SLOs and you
rely on them service level agreement.

543
00:35:06,839 --> 00:35:10,089
And then we can have a faster
delivery along the way as well.

544
00:35:10,929 --> 00:35:11,149
And.

545
00:35:11,869 --> 00:35:16,189
If you have questions around, I'm
more than happy to discuss this, how

546
00:35:16,189 --> 00:35:20,509
to implement observability contracts
in your organization as well.

547
00:35:21,319 --> 00:35:28,219
And with that, I would like to conclude
my talk here at Conference 42 DevOps 2026.

548
00:35:28,219 --> 00:35:29,719
This is my LinkedIn link.

549
00:35:29,719 --> 00:35:31,909
I would also share that in
the presentation as well.

550
00:35:31,909 --> 00:35:33,649
Happy to connect over the LinkedIn.

551
00:35:34,149 --> 00:35:41,019
Reach out if you have any questions or any
implementation details or would love to

552
00:35:41,019 --> 00:35:43,239
have a discussion around observability.

553
00:35:43,629 --> 00:35:47,379
I'm more than happy to connect,
discuss, and share ideas and even

554
00:35:47,379 --> 00:35:48,539
help on the implementation side as.

555
00:35:48,924 --> 00:35:52,194
So I'll look forward for many
other talks during the conference.

556
00:35:52,194 --> 00:35:53,514
I hope this was helpful.

557
00:35:53,544 --> 00:35:58,584
This was worthwhile your time, and I'm
looking forward for many more conferences

558
00:35:58,584 --> 00:36:02,204
and other panelists to to share idea,
ideas and thoughts along the way.

559
00:36:02,264 --> 00:36:05,924
So that was my talk at Con 42 DevOps 2026.

560
00:36:05,924 --> 00:36:06,704
Thank you so much.

561
00:36:06,974 --> 00:36:08,654
Have a wonderful rest of the conference.

