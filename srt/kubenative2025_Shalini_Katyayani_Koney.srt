1
00:00:00,500 --> 00:00:01,400
Hello everyone.

2
00:00:01,880 --> 00:00:06,950
Today I'm going to share practical
insights on scaling Kafka on Kubernetes

3
00:00:06,950 --> 00:00:12,920
to handle extreme streaming workloads,
specifically how to architect and optimize

4
00:00:12,920 --> 00:00:17,300
for over a hundred thousand messages
per second in production environment.

5
00:00:17,685 --> 00:00:21,405
I'm an independent researcher
focusing on cloud native

6
00:00:21,465 --> 00:00:23,595
architecture and distributed systems.

7
00:00:24,305 --> 00:00:29,075
This presentation draws from real
world implementation experience with

8
00:00:29,075 --> 00:00:31,925
high throughput streaming platforms.

9
00:00:32,425 --> 00:00:34,195
Let's start with the problem space.

10
00:00:34,615 --> 00:00:39,895
Modern containerization application are
generating unprecedented data volumes.

11
00:00:40,130 --> 00:00:44,635
We are talking millions of messages per
second that need real time processing.

12
00:00:45,135 --> 00:00:47,685
The challenge isn't just about raw volume.

13
00:00:48,370 --> 00:00:54,100
We need Kubernetes native streaming
platforms that can scale dynamically

14
00:00:54,190 --> 00:00:59,800
with workload demands while maintaining
stateful streaming workloads with

15
00:00:59,890 --> 00:01:01,900
low latencies and high throughput.

16
00:01:02,650 --> 00:01:07,180
This intersection of stateful
applications and container infrastructure

17
00:01:07,570 --> 00:01:11,320
creates unique architectural
challenges that we will address today.

18
00:01:11,820 --> 00:01:14,580
So why Kafka Kubernetes
in the first place?

19
00:01:15,080 --> 00:01:17,660
Firstly, continued ization.

20
00:01:18,140 --> 00:01:22,700
Kubernetes gives us automatic
scaling, self-healing, and

21
00:01:23,000 --> 00:01:24,950
sophisticated resource management.

22
00:01:24,950 --> 00:01:26,120
For Kafka clusters.

23
00:01:26,690 --> 00:01:30,170
These capabilities are built
into the platform rather than

24
00:01:30,440 --> 00:01:31,820
requiring custom tooling.

25
00:01:32,320 --> 00:01:33,914
Second cloud native benefits.

26
00:01:34,600 --> 00:01:39,910
We get seamless integration with cloud
services, persistent storage solutions,

27
00:01:39,970 --> 00:01:42,309
and service mesh architectures.

28
00:01:42,729 --> 00:01:46,809
This integration reduces operational
complexity significantly.

29
00:01:47,305 --> 00:01:53,395
Third, dynamic scaling horizontal pod
auto scaling lets our Kafka deployment

30
00:01:53,815 --> 00:01:59,515
adapt to traffic patterns automatically
while maintaining streaming performance.

31
00:01:59,845 --> 00:02:03,775
This elasticity is critical
for cost optimization and

32
00:02:03,775 --> 00:02:05,905
handling variable workloads.

33
00:02:06,405 --> 00:02:12,135
The foundation of any successful
Kafka on Kubernetes deployment

34
00:02:12,195 --> 00:02:16,905
rests on three pillars, stateful set
configuration, which is essential.

35
00:02:17,415 --> 00:02:22,395
We deploy Kafka brokers as stateful
sets, ensuring order deployment,

36
00:02:22,905 --> 00:02:27,195
stable network identities, and
position storage attachment.

37
00:02:27,855 --> 00:02:32,115
This is critical because Kafka
brokers are inherently stateful.

38
00:02:32,955 --> 00:02:37,545
Our persistent volume strategy
uses optimized storage classes,

39
00:02:37,755 --> 00:02:44,085
specifically SSD, backed volumes with
high IOPS to meet Kafka's demanding

40
00:02:44,235 --> 00:02:49,665
log segment performance requirements
and low latency access patterns.

41
00:02:50,580 --> 00:02:56,910
Finally, service mesh integration
provides secure interpod communication

42
00:02:56,970 --> 00:03:01,080
with traffic management and
comprehensive observability.

43
00:03:01,770 --> 00:03:07,020
This gives us encrypted communication
and detailed metrics without

44
00:03:07,080 --> 00:03:08,640
modifying the application code.

45
00:03:09,140 --> 00:03:12,230
Resource allocation
requires careful tuning.

46
00:03:12,890 --> 00:03:14,750
Here's what works in production.

47
00:03:15,350 --> 00:03:18,680
CPO requests should be two
to four cores per broker.

48
00:03:19,160 --> 00:03:23,990
This provides this, provide
enough processing power for high

49
00:03:23,990 --> 00:03:25,670
throughput workloads without.

50
00:03:26,105 --> 00:03:29,315
Over provisioning memory
allocation is critical.

51
00:03:29,675 --> 00:03:35,645
We typically configure eight to 16 GB
heap sized optimization per broker.

52
00:03:36,365 --> 00:03:40,415
The exact size depends on your
platform count, partition count,

53
00:03:40,415 --> 00:03:41,595
and message retention policies.

54
00:03:42,095 --> 00:03:45,305
Resource limits are equally
important to prevent noisy

55
00:03:45,305 --> 00:03:47,495
neighbors in multi-tenant clusters.

56
00:03:47,945 --> 00:03:52,685
We use the guaranteed QOS class
for critical Kafka workloads,

57
00:03:52,715 --> 00:03:57,070
ensuring resources are in
preempted under cluster pressure.

58
00:03:57,570 --> 00:04:03,120
Proper resource allocation ensures
consistent performance under varying loads

59
00:04:03,420 --> 00:04:05,700
while preventing resource contention.

60
00:04:06,150 --> 00:04:12,020
This is specifically important in shared
cluster environment Storage is where

61
00:04:12,230 --> 00:04:15,230
many Kafka on Kubernetes deployment fail.

62
00:04:15,710 --> 00:04:20,330
Let me walk through our three part
strategy First storage classes.

63
00:04:21,230 --> 00:04:24,800
We use SSD based storage
classes with high.

64
00:04:25,685 --> 00:04:28,655
IOP is specifically log segments.

65
00:04:29,135 --> 00:04:34,535
Kafka's sequential right patterns
benefit anomalously from fast storage

66
00:04:34,625 --> 00:04:39,665
and the low latency access patterns are
essential for the consumer performance.

67
00:04:40,055 --> 00:04:42,065
Second volume configuration.

68
00:04:42,755 --> 00:04:46,625
Persistent volume claims must be
sized according to your retention

69
00:04:46,625 --> 00:04:48,785
policies and replication factors.

70
00:04:49,355 --> 00:04:54,155
Undersizing here leads to frequent disc
pressure issues and broker instability.

71
00:04:54,655 --> 00:04:59,755
And the third one, the performance
tuning file system optimization and mount

72
00:04:59,755 --> 00:05:02,425
options dramatically impact throughput.

73
00:05:02,785 --> 00:05:07,915
For example, using XFS with specific
mount options can improve right

74
00:05:07,915 --> 00:05:13,255
performance by 30 to 40% compared
to the default configurations.

75
00:05:13,755 --> 00:05:17,175
Network performance is often
an overlooked bottleneck.

76
00:05:17,475 --> 00:05:19,695
We address this through three mechanisms.

77
00:05:20,235 --> 00:05:25,035
Network segmentation uses isolated
network policies for Kafka clusters.

78
00:05:25,515 --> 00:05:31,185
This ensures security without performance
degradation a common issue when

79
00:05:31,185 --> 00:05:32,895
network policies are too restrictive.

80
00:05:33,395 --> 00:05:36,725
Load balancing configuration
is optimized for streaming

81
00:05:36,725 --> 00:05:39,335
workloads with session affinity.

82
00:05:39,635 --> 00:05:43,265
This is important because Kafka
clients benefit from sticky

83
00:05:43,265 --> 00:05:45,545
connections to specific brokers.

84
00:05:46,045 --> 00:05:49,955
Bandwidth optimization involves
network performance tuning for

85
00:05:49,955 --> 00:05:52,145
high throughput message processing.

86
00:05:52,685 --> 00:05:58,955
This includes TCP buffer tuning network,
plugin selection, and sometimes dedicated

87
00:05:58,955 --> 00:06:02,075
network interfaces for Kafka traffic.

88
00:06:02,575 --> 00:06:07,435
Auto scaling stateful applications
like Kafka requires a nuanced approach.

89
00:06:07,945 --> 00:06:12,085
Custom metrics are key standard
CPU and memory metrics.

90
00:06:12,085 --> 00:06:13,255
Don't tell the full story.

91
00:06:13,855 --> 00:06:18,925
We implement HPA based on Kafka
specific metrics like consumer-like

92
00:06:19,105 --> 00:06:20,245
and partition throughput.

93
00:06:20,695 --> 00:06:24,805
These metrics directly indicate
when more capacity is needed.

94
00:06:25,610 --> 00:06:30,110
Our scaling policies use intelligent
algorithms that account for the

95
00:06:30,350 --> 00:06:32,240
stateful nature of the Kafka brokers.

96
00:06:32,930 --> 00:06:35,300
You can't just terminate
brokers instantly.

97
00:06:35,720 --> 00:06:37,910
Partition leadership
needs to be transferred.

98
00:06:38,180 --> 00:06:39,770
Replicas need to be reassigned.

99
00:06:40,280 --> 00:06:43,760
The scaling policy must respect
these operational requirements.

100
00:06:43,760 --> 00:06:49,250
Admins traffic adaption also ensures
dynamic adjustment to varying

101
00:06:49,250 --> 00:06:53,900
message volumes while maintaining
partition balance across the clusters.

102
00:06:54,850 --> 00:06:59,890
Unbalanced partitions can create hotspots
that negate the benefits of scaling.

103
00:07:00,390 --> 00:07:00,680
Okay.

104
00:07:01,180 --> 00:07:02,710
Multi cluster deployment.

105
00:07:02,860 --> 00:07:06,880
Enterprise deployments typically
require multi Kafka clusters.

106
00:07:07,210 --> 00:07:11,920
We manage this through helm charts,
which provide the standardized

107
00:07:11,920 --> 00:07:16,150
deployments across environments
with configurable parameters.

108
00:07:16,750 --> 00:07:21,725
This consistency reduces
operational lenders and enables a

109
00:07:21,725 --> 00:07:22,965
rapid deployment in new regions.

110
00:07:23,465 --> 00:07:28,025
Global distribution implements like
cross region application for disaster

111
00:07:28,025 --> 00:07:33,035
recovery and reduced latency users
in different geographic regions

112
00:07:33,065 --> 00:07:37,705
can consume from the local clusters
while maintaining data consistency,

113
00:07:37,765 --> 00:07:42,415
ease maintained through mirrormaker,
which provides reliable cluster data

114
00:07:42,415 --> 00:07:45,295
synchronization with exactly one somatics.

115
00:07:45,775 --> 00:07:50,245
This is critical for maintaining
data integrity across regions.

116
00:07:50,745 --> 00:07:54,285
Operational excellence requires
treating infrastructure as code.

117
00:07:54,825 --> 00:07:57,375
Our GitHub's workflow
has three components.

118
00:07:57,765 --> 00:08:03,105
Infrastructure as code means all Kafka
cluster configurations are managed through

119
00:08:03,105 --> 00:08:04,810
kit repositories with version control.

120
00:08:05,310 --> 00:08:08,580
Every change is tracked,
reviewed, and audited.

121
00:08:09,240 --> 00:08:13,470
Automated deployments use CICD
pipelines that trigger cluster updates

122
00:08:13,530 --> 00:08:15,300
based on configuration changes.

123
00:08:15,840 --> 00:08:20,040
This eliminates manual deployment
steps and associated errors.

124
00:08:20,880 --> 00:08:23,160
Rollback capabilities are built in.

125
00:08:23,660 --> 00:08:26,690
We implement safe deployment
practices with automated

126
00:08:26,690 --> 00:08:28,730
rollbacks on configuration errors.

127
00:08:29,120 --> 00:08:33,500
If a change causes cluster instability,
we can automatically revert

128
00:08:33,500 --> 00:08:35,690
within minutes rather than ours.

129
00:08:36,190 --> 00:08:38,115
You can't manage what you can't measure.

130
00:08:38,615 --> 00:08:41,914
Our monitoring Slack
stack has three layers.

131
00:08:42,304 --> 00:08:46,745
Prometheus integration, which
export JMX metrics for comprehensive

132
00:08:46,745 --> 00:08:52,745
cluster monitoring, we track broker
health partition metrics, consumer

133
00:08:52,745 --> 00:08:54,935
group lag and resource utilization.

134
00:08:55,435 --> 00:08:59,635
Grafana dashboards provide real
time visualization of throughput,

135
00:08:59,694 --> 00:09:02,125
latency, and resource allocation.

136
00:09:02,785 --> 00:09:05,755
Operators can quickly identify
bottlenecks and trending issues.

137
00:09:06,255 --> 00:09:10,065
Alerting rules, implement proactive
notification for performance

138
00:09:10,065 --> 00:09:12,345
degradation and capacity thresholds.

139
00:09:12,795 --> 00:09:16,065
We alert before users
experience issues, not after.

140
00:09:16,665 --> 00:09:20,685
This includes alerts on consumer
lag, spikes under replicated

141
00:09:20,685 --> 00:09:24,285
partitions, and this space thresholds.

142
00:09:24,785 --> 00:09:27,965
Now let's talk some numbers
in production deployments.

143
00:09:27,965 --> 00:09:31,835
Using these patterns, we
consistently achieve hundred

144
00:09:31,835 --> 00:09:33,275
thousand messages per second.

145
00:09:33,275 --> 00:09:36,755
Sustained throughput with
optimized configurations.

146
00:09:37,115 --> 00:09:38,735
This isn't burst traffic.

147
00:09:38,765 --> 00:09:42,245
This is a continuous processing
under production load.

148
00:09:42,745 --> 00:09:48,295
Less than five millisecond P 99 latency
maintained even under very high loads.

149
00:09:48,775 --> 00:09:54,625
Low latency is very critical for real-time
applications and achieving sub millisecond

150
00:09:54,685 --> 00:10:00,565
P 99 at the scale demonstrates the
effectiveness are optimization strategies.

151
00:10:01,065 --> 00:10:07,695
99%, 99.9% availability With zero
down to deployments, we can perform

152
00:10:07,785 --> 00:10:11,805
rolling upgrades, scale clusters,
and handle load failures without

153
00:10:11,805 --> 00:10:13,785
dropping messages or causing downtime.

154
00:10:14,775 --> 00:10:18,975
These metrics demonstrate the
effectiveness of Cloud Native Kafka,

155
00:10:18,975 --> 00:10:24,255
deployments in production, environment
handling, demanding streaming workloads.

156
00:10:24,945 --> 00:10:25,880
This isn't theoretical.

157
00:10:26,290 --> 00:10:28,240
These are real production numbers.

158
00:10:28,740 --> 00:10:30,810
Disaster recovery isn't optional.

159
00:10:30,870 --> 00:10:32,520
For critical streaming infrastructure.

160
00:10:32,850 --> 00:10:34,740
We implement a layered approach.

161
00:10:35,310 --> 00:10:39,180
Kubernetes operators provide
automated backup restoration

162
00:10:39,180 --> 00:10:40,830
and failover capabilities.

163
00:10:41,400 --> 00:10:46,260
These operators understand Kafka's
operational requirement and can perform

164
00:10:46,770 --> 00:10:49,110
complex recovery procedures automatically.

165
00:10:49,610 --> 00:10:53,990
Cross region backups ensure we can
recover from regional failures.

166
00:10:54,470 --> 00:11:00,350
We maintain topic configuration,
consumer group offsets, and ECLS in

167
00:11:00,350 --> 00:11:03,200
specific regions or in separate regions.

168
00:11:03,680 --> 00:11:07,195
Automated recovery procedures
are tested regularly through

169
00:11:07,195 --> 00:11:09,020
kios engineering practices.

170
00:11:09,500 --> 00:11:14,360
We deliberately fail components to ensure
our recovery mechanisms work as expected.

171
00:11:14,860 --> 00:11:20,530
Data, application and consistency
guarantees ensure that failing

172
00:11:20,530 --> 00:11:24,310
over to backup cluster doesn't
result in data loss or duplication.

173
00:11:25,090 --> 00:11:30,370
This requires careful configuration
of replication factors and

174
00:11:30,370 --> 00:11:31,690
acknowledgement settings.

175
00:11:32,200 --> 00:11:36,280
Together these capabilities
ensure business continuity for

176
00:11:36,280 --> 00:11:37,930
critical streaming applications.

177
00:11:38,430 --> 00:11:43,470
As we wrap up, I want to emphasize
three key points, strategic planning,

178
00:11:44,130 --> 00:11:48,210
proper resource allocation and
storage optimization aren't optional.

179
00:11:48,420 --> 00:11:51,840
They are critical for sustained
high throughput performance.

180
00:11:52,230 --> 00:11:56,130
Many deployments fail because they
underestimate these requirements.

181
00:11:56,520 --> 00:11:58,865
Second, embrace cloud native approaches.

182
00:11:59,365 --> 00:12:02,875
Leverage Kubernetes native
platforms for scaling, monitoring,

183
00:12:02,905 --> 00:12:04,255
and operational excellence.

184
00:12:04,555 --> 00:12:07,704
Don't fight against the
platform, use its capabilities.

185
00:12:08,125 --> 00:12:10,645
Third, be production ready from day one.

186
00:12:11,155 --> 00:12:16,165
Implement comprehensive monitoring,
GitHub workflows and disaster

187
00:12:16,165 --> 00:12:17,334
recovery from the beginning.

188
00:12:17,905 --> 00:12:19,525
These aren't things you add later.

189
00:12:19,555 --> 00:12:21,385
They are foundational requirements.

190
00:12:22,220 --> 00:12:26,660
Building Kafka on Kubernetes at scale
requires careful attention to these

191
00:12:26,660 --> 00:12:31,760
architecture patterns, but the result
is a resilient, scalable streaming

192
00:12:31,760 --> 00:12:36,890
platform that can handle massive
workloads with operational simplicity.

193
00:12:37,390 --> 00:12:39,400
And thank you all for your attention.

194
00:12:39,760 --> 00:12:41,650
I'm happy to take any questions about.

195
00:12:42,040 --> 00:12:46,510
Any aspect of scaling Kaf
Kubernetes from regional specific

196
00:12:46,510 --> 00:12:50,080
configuration recommendations to
troubleshooting any common challenges.

197
00:12:50,500 --> 00:12:53,620
You can connect with me after the
talk if you'd like to discuss your

198
00:12:53,620 --> 00:12:55,810
specific use case in more detail.

