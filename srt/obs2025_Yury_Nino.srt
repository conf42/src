1
00:00:00,500 --> 00:00:01,490
Hi everybody.

2
00:00:01,490 --> 00:00:07,100
Thank you so much for being here and thank
you EH com 42 for accepted my proposal.

3
00:00:07,670 --> 00:00:12,290
The title of my talk is Proactive
Cost Management, detecting Anomaly in

4
00:00:12,290 --> 00:00:14,000
Logs with Time Series Use Analysis.

5
00:00:14,420 --> 00:00:15,590
Since today's.

6
00:00:16,055 --> 00:00:17,345
Cloud native world.

7
00:00:17,945 --> 00:00:20,915
Managing cost proactively
is more crucial than ever.

8
00:00:20,945 --> 00:00:25,744
We will explore how time series
analysis particularly apply to

9
00:00:25,744 --> 00:00:27,605
logs can be useful in this regard.

10
00:00:28,105 --> 00:00:30,535
Nice to meet you here a little about me.

11
00:00:30,535 --> 00:00:31,615
I am Jordan Nino.

12
00:00:31,645 --> 00:00:33,925
I am cloud application engineer at Google.

13
00:00:34,224 --> 00:00:39,655
My patients like in SRE, above scale
engineering and observability, of

14
00:00:39,655 --> 00:00:43,944
course, as well as sharing knowledge,
draw, reading, writing, and teach.

15
00:00:44,334 --> 00:00:48,084
You can connect with me on
LinkedIn, medium or X at jino.

16
00:00:48,355 --> 00:00:51,565
My personal web page is www jino.

17
00:00:52,065 --> 00:00:54,075
How many of you have
been in this situation?

18
00:00:54,575 --> 00:00:56,555
Take one moment to think about this.

19
00:00:57,055 --> 00:01:01,525
This Mimi captures perfectly a common
scenario where DevOps teams are asking

20
00:01:01,525 --> 00:01:03,535
for more budget from their finance team.

21
00:01:03,715 --> 00:01:04,555
That is very common.

22
00:01:05,035 --> 00:01:08,545
This is a consequence of mis
configuring cloud computing needs.

23
00:01:09,045 --> 00:01:10,635
Let me tell you that.

24
00:01:10,635 --> 00:01:12,590
But unfortunately, very common story.

25
00:01:13,095 --> 00:01:14,055
This starts with me.

26
00:01:14,115 --> 00:01:14,505
Indeed.

27
00:01:14,505 --> 00:01:18,705
She's physically similar to me,
an oncology dinner, constantly

28
00:01:18,705 --> 00:01:20,085
battling production issues.

29
00:01:20,385 --> 00:01:24,645
Last week during an incident, I
realized a critical flow there

30
00:01:24,645 --> 00:01:27,795
are in logs to investigate the
root cause of the incident.

31
00:01:28,035 --> 00:01:30,795
You can imagine my
reaction at that moment.

32
00:01:31,295 --> 00:01:35,975
After that to have to say that it will be
impossible to investigate the incident.

33
00:01:35,975 --> 00:01:42,005
I decided to activate all available logs,
activity, data access, and system events,

34
00:01:42,245 --> 00:01:44,705
because that wasn't going to happen.

35
00:01:44,705 --> 00:01:45,125
Me too.

36
00:01:45,215 --> 00:01:45,575
Okay.

37
00:01:46,075 --> 00:01:51,670
And here all was okay, but I
got an unexpected consequence.

38
00:01:51,940 --> 00:01:56,920
700% of increment in the billing
all due of activating cloud.

39
00:01:56,920 --> 00:01:56,980
Do.

40
00:01:57,480 --> 00:02:02,010
This story is a perfect example of
this quote highlighted by Dr. Mad

41
00:02:02,010 --> 00:02:07,200
Doric, the hidden cause of relying on
anomaly only detection and response

42
00:02:07,200 --> 00:02:12,540
systems, which while detecting anomaly
is crucial, a reactive approach

43
00:02:12,540 --> 00:02:16,740
can lead to significant financial
repercussions that are an immediately.

44
00:02:17,240 --> 00:02:19,220
So what is the true cause of in action?

45
00:02:19,265 --> 00:02:23,525
As you can see from this graph, a
lack of proactive cost management

46
00:02:23,525 --> 00:02:25,775
can lead to increases of.

47
00:02:26,275 --> 00:02:31,825
In forecasted costs, like this
example, showing a 700% increase from

48
00:02:31,855 --> 00:02:38,395
April to May beyond direct expenses
in action, results in downtime, in

49
00:02:38,395 --> 00:02:42,460
efficient resource use, security
breachs, and wasted engineering time.

50
00:02:42,920 --> 00:02:47,780
The first one related to lost revenue,
customer churn, and reputational damage.

51
00:02:48,280 --> 00:02:53,240
Inefficient resort use associated
to cloud bills Exploding was

52
00:02:53,300 --> 00:02:58,570
infrastructure security breaches when
we have massive financial penalties,

53
00:02:58,570 --> 00:03:00,490
legal costs, and irreparable harm.

54
00:03:01,420 --> 00:03:05,110
And finally, was that engineering
time refers to our expense

55
00:03:05,110 --> 00:03:06,640
rule shooting reactive problem.

56
00:03:07,140 --> 00:03:12,570
Similar that the sad story with this
context here is what I will cover today.

57
00:03:12,810 --> 00:03:17,940
I will start by discussing cost management
challenges, then explore how logs can be

58
00:03:17,940 --> 00:03:20,940
valuable asset despite their complexities.

59
00:03:21,420 --> 00:03:25,230
I will then dive into machine
learning techniques, but that is an

60
00:03:25,230 --> 00:03:28,695
introduction of the machine learning
techniques at first, of course.

61
00:03:29,270 --> 00:03:33,059
Specifically focusing on time,
serious analysis, and I am going to

62
00:03:33,059 --> 00:03:37,049
explore practical use cases before
opening the floor for your questions.

63
00:03:37,549 --> 00:03:41,420
Implementing proactive cloud
cost, which involves continuously

64
00:03:41,420 --> 00:03:46,009
monitoring, analyzing, and optimizing,
expanding on cloud, it is about

65
00:03:46,009 --> 00:03:48,079
preventing instead of reaction cost.

66
00:03:48,379 --> 00:03:53,119
The central premise here is that if
logs are part of the problem, they

67
00:03:53,119 --> 00:03:55,220
also hold the key to the solution.

68
00:03:55,720 --> 00:03:58,749
In that con in the context
of cloud computing, that

69
00:03:58,749 --> 00:04:00,400
is a challenge environment.

70
00:04:00,799 --> 00:04:04,609
Proactive calls refer to a
strategic approach to managing and

71
00:04:04,609 --> 00:04:08,989
optimizing cloud spending before
it leads to unexpected over runs.

72
00:04:09,470 --> 00:04:13,129
And it is mentioned here, it is
a critical component to maximize

73
00:04:13,129 --> 00:04:17,390
business value from cloud investment
while keeping costs under country.

74
00:04:17,890 --> 00:04:21,339
Here are more benefits from
proactive cause in the cloud context.

75
00:04:21,519 --> 00:04:22,840
An important benefit.

76
00:04:22,840 --> 00:04:28,870
It's study that low part, anticipating and
preventing issues in terms of waiting for

77
00:04:28,870 --> 00:04:31,120
cause spikes or resource waste to occur.

78
00:04:31,990 --> 00:04:37,419
Another benefits include continuous
optimization, predictive analytics

79
00:04:38,200 --> 00:04:43,180
increasing elasticity, choosing
the right pricing models, and

80
00:04:43,620 --> 00:04:44,849
visibility and monitoring.

81
00:04:45,210 --> 00:04:50,820
The first one related to co continuous
optimization is an ongoing process of

82
00:04:50,820 --> 00:04:55,950
refining and improving cloud deployments
to maximize resource utilization and

83
00:04:55,950 --> 00:05:00,960
achieve business outcomes and the lowest
possible price, predictive analytics

84
00:05:00,960 --> 00:05:05,940
that is related to ensure that cloud
resources, for example, virtual machines,

85
00:05:05,970 --> 00:05:09,180
storage databases, are properly sized.

86
00:05:09,220 --> 00:05:10,870
For the actual workloads.

87
00:05:10,900 --> 00:05:16,190
Workloads and increase elasticity,
providing a scaling mechanism to

88
00:05:16,190 --> 00:05:18,590
match resource allocation with demand.

89
00:05:18,800 --> 00:05:22,039
So you only pay for what you use.

90
00:05:22,490 --> 00:05:28,190
Choosing the rightsizing pricing models
matching storage classes, and as I

91
00:05:28,190 --> 00:05:30,020
mentioned, visibility and monitoring.

92
00:05:30,260 --> 00:05:31,880
That is the focus of this talk.

93
00:05:32,380 --> 00:05:37,240
However, analyzing logs in the cloud
presence, unique challenges compared

94
00:05:37,240 --> 00:05:41,050
to the traditional environments, like
on-premise environment, for example.

95
00:05:41,380 --> 00:05:46,480
This is a consequence of distributed
dynamic and often ephemeral nature

96
00:05:46,480 --> 00:05:51,040
of cloud infrastructure, which can
log collection, analysis, and storage

97
00:05:51,040 --> 00:05:53,305
significantly more complex and cost.

98
00:05:53,805 --> 00:05:56,595
And so we circle back to our core idea.

99
00:05:56,865 --> 00:06:00,825
If the problem originates from
logs, the solution too should be

100
00:06:00,825 --> 00:06:03,895
formed within the logs themselves.

101
00:06:04,205 --> 00:06:07,925
The challenge is how to
effectively extract the solution.

102
00:06:08,425 --> 00:06:12,775
With this challenges in mind, the big
question reminds what is the solution?

103
00:06:13,255 --> 00:06:18,705
How do we can use log data into actionable
insights from proactive cost management?

104
00:06:19,205 --> 00:06:22,295
A solution lies in SIE anomaly detection.

105
00:06:22,865 --> 00:06:27,920
That is a solution because we have another
options of solutions in the state of that,

106
00:06:28,460 --> 00:06:31,010
but in this case, this powerful approach.

107
00:06:31,010 --> 00:06:35,540
Combine sophisticated machine learning
techniques with statistical methodologies.

108
00:06:35,540 --> 00:06:40,310
For example, identify insignificant
departures from past data.

109
00:06:40,810 --> 00:06:44,140
Pre establish criteria
to pinpoint anomalies.

110
00:06:44,170 --> 00:06:48,340
These allow us to move vision, simple
thresholds, and detect deviations.

111
00:06:48,840 --> 00:06:50,615
Here what is OMA detection?

112
00:06:50,885 --> 00:06:55,595
It's about identifying patterns that
significantly devi from expected

113
00:06:55,595 --> 00:07:02,285
behavior, finding the normal abnormal
wipe for loss because chias from reactive

114
00:07:02,285 --> 00:07:07,475
fight fighting to proactive production
and give us a early warning system

115
00:07:07,535 --> 00:07:09,915
to catch issues before they escalate.

116
00:07:10,415 --> 00:07:15,095
By using algorithms that can recognize
patterns or anomalies in big data

117
00:07:15,095 --> 00:07:19,805
sets, machine learning provides a more
advanced method of anomaly detection.

118
00:07:20,135 --> 00:07:23,785
The following are important machine
learning methods that frequently

119
00:07:23,785 --> 00:07:25,800
apply in SRE to anomaly detection.

120
00:07:26,300 --> 00:07:30,670
I am going to start with observe
own supervised learning algorithms.

121
00:07:30,940 --> 00:07:34,970
In this case, we don't require
labor label data as and considering

122
00:07:34,970 --> 00:07:39,380
we don't have classes in cloud do
it is well situated for anomaly

123
00:07:39,380 --> 00:07:41,490
detection applications like this case.

124
00:07:41,940 --> 00:07:42,180
Eh.

125
00:07:42,210 --> 00:07:47,340
Techniques like clustering algorithms
and colors fall into this category.

126
00:07:47,610 --> 00:07:51,659
The first ones, eh, clustering algorithms
include algorithms that cluster

127
00:07:51,659 --> 00:07:56,340
data together based on similarity
and recognize outliers as possible.

128
00:07:56,340 --> 00:08:01,140
Anoma and Al colors are models
based on network networks

129
00:08:01,200 --> 00:08:02,669
that can recreate in Punta.

130
00:08:03,169 --> 00:08:07,909
Supervised learning alert algorithms and
other type of machine learning techniques,

131
00:08:08,120 --> 00:08:13,380
on the other hand, require historical data
with label anomalies to train the motor.

132
00:08:13,799 --> 00:08:18,369
This this picture that I make
it think illustrate this concept

133
00:08:18,669 --> 00:08:22,869
classification algorithms include
super vector machines and random form.

134
00:08:23,369 --> 00:08:28,299
And semi-supervised learning
algorithms combine elements of

135
00:08:28,299 --> 00:08:30,489
supervised and unsupervised learning.

136
00:08:30,819 --> 00:08:34,539
And normally this can be identified
by the model, which has been

137
00:08:34,599 --> 00:08:37,119
trained on normal data Installation.

138
00:08:37,119 --> 00:08:41,469
Forest is a three based technique that
separates data into subsets in order

139
00:08:41,529 --> 00:08:43,239
to isolate the normal, the anoma.

140
00:08:43,739 --> 00:08:47,669
Since many SIE measures have a
temporal component that is really

141
00:08:47,669 --> 00:08:50,239
challenge use these techniques.

142
00:08:50,419 --> 00:08:54,769
So th time series analysis techniques
are essential for identifying

143
00:08:54,769 --> 00:08:59,449
abnormalities over time in seasonal
decomposition to final anomalies

144
00:08:59,449 --> 00:09:04,304
time series that is broke down into
seasonal trend and residual components.

145
00:09:04,639 --> 00:09:06,739
I will review more
details in the next slide.

146
00:09:07,239 --> 00:09:10,719
Lemme now talk about how
Google managed time series.

147
00:09:11,079 --> 00:09:16,859
Because Google Cloud offers some
services that provides strategies for for

148
00:09:16,859 --> 00:09:18,749
solving issues related to time series.

149
00:09:19,249 --> 00:09:23,699
When dealing with time series it
is important to understand what.

150
00:09:24,060 --> 00:09:29,129
Most are not stationary, meaning data,
statistical properties change over time.

151
00:09:29,339 --> 00:09:33,829
I think that is that it could be
confused since we are not expert

152
00:09:33,829 --> 00:09:35,089
in machine learning techniques.

153
00:09:35,329 --> 00:09:38,059
But I think that is a good
introduction for the topic.

154
00:09:38,734 --> 00:09:43,844
And the most important for solving the
issue related to predict cost management.

155
00:09:44,144 --> 00:09:49,814
For instance, financial time series often
exhibit a random work with drift behavior.

156
00:09:50,294 --> 00:09:54,524
Similarly, energy production is
hugely influenced by factors like

157
00:09:54,524 --> 00:09:59,704
wind and solar supply leading to
dynamic patterns for solving that.

158
00:09:59,704 --> 00:10:06,064
The sad, the story that I told you I
choose Arimo ARI is the acronym of a air.

159
00:10:06,994 --> 00:10:10,439
Im I air related to outdoor regression.

160
00:10:10,469 --> 00:10:15,060
The model use a dependent relationship
between an observation and some

161
00:10:15,060 --> 00:10:16,859
numbers of lack observations.

162
00:10:17,339 --> 00:10:24,350
I integrated this is the mini is the
first layer in integrated, the use of

163
00:10:24,350 --> 00:10:29,089
different, of raw observations in order
to make this time serious stationary.

164
00:10:29,654 --> 00:10:30,854
And moving an average.

165
00:10:31,685 --> 00:10:36,424
The last letters a model that used
the dependency between an observation

166
00:10:36,424 --> 00:10:40,864
and a residual error from a moving
model applied to observations.

167
00:10:41,364 --> 00:10:44,905
As I mentioned, these time series
models are available in Google Cloud,

168
00:10:44,964 --> 00:10:50,185
particularly through BigQuery ml.
BigQuery ML allow you to create

169
00:10:50,185 --> 00:10:53,844
and execute machine learning models
using a standard SQL queries.

170
00:10:54,114 --> 00:10:59,061
I am going to show a demo in which
you can see this this service and this

171
00:10:59,695 --> 00:11:01,980
feature, including to Arima of course.

172
00:11:02,480 --> 00:11:08,540
Now let me explore some real work examples
and use cases where anomaly detection

173
00:11:08,540 --> 00:11:13,370
with time series analysis can provide
significant value by using algorithms

174
00:11:13,370 --> 00:11:15,710
that can recognize complex patterns.

175
00:11:15,710 --> 00:11:21,410
Machine learning, eh, provides a more
advanced method of anomaly identification.

176
00:11:21,710 --> 00:11:26,000
The following are important machine
learning method that are frequent,

177
00:11:26,030 --> 00:11:30,950
that methods are, that are frequently
applied in SRE two anomaly detection.

178
00:11:31,450 --> 00:11:35,350
Time series analysis and anomaly
detection have broad application

179
00:11:35,350 --> 00:11:36,790
across various industries.

180
00:11:37,150 --> 00:11:43,045
But I choose this one this these
use cases because they are very.

181
00:11:43,895 --> 00:11:47,135
Challenging and they are in the
state of the art of the market.

182
00:11:47,135 --> 00:11:51,475
And re in retail and e-commerce, they are
used for sales and demand forecasting.

183
00:11:51,655 --> 00:11:55,525
Short rate prediction challenges
include forecasting new products

184
00:11:55,525 --> 00:11:57,025
and complex product hierarchies.

185
00:11:57,295 --> 00:12:01,705
Financial services leverage it for
asset management and product sales

186
00:12:01,735 --> 00:12:06,385
forecasting, despite challenges
like nosy data and partially

187
00:12:06,385 --> 00:12:08,755
observable markup decision process.

188
00:12:09,220 --> 00:12:14,430
In manufacturing also use cases
include predictive maintenance and

189
00:12:14,430 --> 00:12:18,750
adaptive controls, often facing poor
data quality and diverse sensor types.

190
00:12:19,140 --> 00:12:23,970
And finally, healthcare where anomaly
detection is used for bed and emergency

191
00:12:23,970 --> 00:12:28,320
occupancy and drug demand forecasting
with data privacy and disparate

192
00:12:28,800 --> 00:12:30,525
resource being key challenges.

193
00:12:31,025 --> 00:12:35,685
This is the architecture of my solution,
which includes an input that is basically

194
00:12:35,685 --> 00:12:40,695
the logs which are sent to BigQuery using
a log router for the time series analysis.

195
00:12:40,725 --> 00:12:45,865
Remember that the proper solution
in Google Cloud to analysis the logs

196
00:12:45,865 --> 00:12:47,820
using time series is BigQuery method.

197
00:12:47,955 --> 00:12:53,430
Since third Party Solutions offer a better
visualization of results I export the

198
00:12:53,430 --> 00:12:58,980
results of the analysis, the term series
analysis to CVS five, which is imported

199
00:12:59,470 --> 00:13:01,900
after that is imported in Google Sheet.

200
00:13:02,480 --> 00:13:04,755
With the aim to visualize the resource.

201
00:13:04,855 --> 00:13:05,785
As you can see here,

202
00:13:06,285 --> 00:13:10,285
let me show, now let me go to the
Google console and show you can

203
00:13:10,285 --> 00:13:14,215
detect an anomaly in cloud logging
using BigQuery at Time Cities.

204
00:13:14,935 --> 00:13:19,275
To do this, we are going to
create a log sync in cloud logging

205
00:13:19,275 --> 00:13:20,835
using the log route option.

206
00:13:21,335 --> 00:13:24,700
In this form, we entered the name
and a description for the syn.

207
00:13:25,200 --> 00:13:31,310
We select a destination, which in this
case can be a bucket in cloud logging,

208
00:13:31,810 --> 00:13:37,525
a data set in BigQuery, abouting
Cloud storage, a topic in pops up,

209
00:13:38,215 --> 00:13:39,850
or a separate Google Cloud project.

210
00:13:40,350 --> 00:13:44,870
For our exercise, we select the
BigQuery data set, which enable us

211
00:13:44,870 --> 00:13:46,610
to create or select the data set.

212
00:13:47,110 --> 00:13:51,160
In this option, we can create the
data set in the local project in a

213
00:13:51,160 --> 00:13:56,530
new one or select one that already
exists, just what we are going to do.

214
00:13:56,650 --> 00:13:57,610
In this case,

215
00:13:58,110 --> 00:13:59,010
we add a filter.

216
00:13:59,510 --> 00:14:03,710
To define the logs that we are
going to send to que, in this

217
00:14:03,710 --> 00:14:05,390
case, we leave option in blank.

218
00:14:05,890 --> 00:14:10,480
And as an optional thing, we can create
an exclusion to determine which log

219
00:14:10,480 --> 00:14:12,940
records will not be sent to the dataset.

220
00:14:13,440 --> 00:14:18,090
With this information, we click on
create Sync and verify this creation.

221
00:14:18,590 --> 00:14:21,725
To do this, we are going to be
query service and we are going

222
00:14:21,725 --> 00:14:23,570
to verify that the table exists.

223
00:14:24,070 --> 00:14:25,625
Indeed the table exists.

224
00:14:25,655 --> 00:14:29,450
Now it is time to create the
model and detect anomaly inqu.

225
00:14:29,950 --> 00:14:36,370
Considering that the lock tables has
several fields with no values that are

226
00:14:36,370 --> 00:14:39,010
not relevant for anomaly detection,

227
00:14:39,510 --> 00:14:43,175
we create the model on a subset
of the data using this square.

228
00:14:43,675 --> 00:14:48,240
As evidence by the query, we are going
using the time stamp of the logs.

229
00:14:48,740 --> 00:14:52,940
Although we truncate E two minutes
because it is the pH level of

230
00:14:52,940 --> 00:14:57,590
granularity that supports time series
based on anomaly detection to have

231
00:14:57,590 --> 00:15:03,910
a sample per unit of time, we group
the logs by date and length, which

232
00:15:03,910 --> 00:15:07,780
in this case transform values to
integer to facilitate estimation.

233
00:15:08,280 --> 00:15:12,300
With this grouping we are, we can
have the associated ER count, which

234
00:15:12,360 --> 00:15:15,300
we will use the model configuration.

235
00:15:15,800 --> 00:15:19,370
What is done, let's use
another query to track anoma.

236
00:15:19,870 --> 00:15:24,730
Now we are going to download the results
to a local file and import them in Google

237
00:15:24,730 --> 00:15:27,490
Sheets to have more visualization options.

238
00:15:27,990 --> 00:15:33,520
With this done, I generate agile in
order to visualize a possible anomaly.

239
00:15:34,140 --> 00:15:38,695
As you can see, the anomalies as here,
there are a lot of logs generated

240
00:15:38,695 --> 00:15:42,325
in these days, which matches with
the behavior in cloud billing.

241
00:15:42,825 --> 00:15:47,325
Having this pattern identified, we can
predict the next increment in the size

242
00:15:47,325 --> 00:15:49,275
of logs and in consequence in the.

