1
00:00:00,500 --> 00:00:03,599
Hi everyone, this is Han Joseph Raach.

2
00:00:04,100 --> 00:00:08,780
Today I'm going to talk about
building high performance AI

3
00:00:08,780 --> 00:00:10,940
literature processing platforms.

4
00:00:11,440 --> 00:00:15,110
So AI has been a buzzword
for past several years.

5
00:00:15,610 --> 00:00:20,089
And let's talk about how AI can
be utilized with respect to the.

6
00:00:20,589 --> 00:00:26,999
Navigating through the complex ecosystem
of the researchers out there, the

7
00:00:26,999 --> 00:00:29,219
research is happening every day.

8
00:00:29,510 --> 00:00:33,920
There are plenty of researchers happening
and the publications out there are like

9
00:00:34,010 --> 00:00:36,170
thousands and thousands of applications.

10
00:00:36,670 --> 00:00:42,189
The challenge is not about publishing
a research it's about keeping it up.

11
00:00:42,689 --> 00:00:48,334
Without a powered platforms, critical
research discoveries, something which

12
00:00:48,334 --> 00:00:53,844
may be phenomenal or maybe utilized
to build on top of that's a vital

13
00:00:53,844 --> 00:00:56,514
component maybe for another research.

14
00:00:57,324 --> 00:01:01,404
So if you wanted to navigate through
that and find out those things, it may

15
00:01:01,404 --> 00:01:06,664
be buried in the exponentially growing
volume of publications for an instance.

16
00:01:07,164 --> 00:01:11,094
10,000 plus biomedical papers
are published every day.

17
00:01:11,594 --> 00:01:16,274
So let's talk about rag
retrieval, augmented generation

18
00:01:16,395 --> 00:01:17,714
as a foundation pattern.

19
00:01:18,674 --> 00:01:19,845
Where do we need rag?

20
00:01:19,924 --> 00:01:26,164
There are lots of LLM power models like
we have Gemini Claude Engine, grok.

21
00:01:26,194 --> 00:01:27,634
There are lots of things out there.

22
00:01:28,519 --> 00:01:31,729
What is the value addition
being done by the rag?

23
00:01:32,229 --> 00:01:35,139
Let's talk about it through these slides.

24
00:01:35,889 --> 00:01:40,330
But the main idea is rag
prevents hallucinations.

25
00:01:40,449 --> 00:01:44,529
When you talk about the research
discovery, we want accuracy.

26
00:01:44,920 --> 00:01:46,899
We want the correct
details to be rendered.

27
00:01:47,439 --> 00:01:49,210
So rag prevents hallucinations.

28
00:01:49,240 --> 00:01:54,610
It's about grounding LLM reasoning
in authoritative references.

29
00:01:55,110 --> 00:01:59,730
Let's talk about the role of
distributed vector database

30
00:01:59,940 --> 00:02:01,590
in there and its architecture.

31
00:02:02,090 --> 00:02:09,200
So unlike keywords, embeddings capture
the context, distributing indexes,

32
00:02:09,260 --> 00:02:12,290
ensure subsecond retrieval at scale.

33
00:02:13,040 --> 00:02:14,945
So we are not only concerned about.

34
00:02:15,445 --> 00:02:21,095
Navigating through, through the
complex ecosystem of researchers, we

35
00:02:21,095 --> 00:02:23,225
also want it to happen efficiently.

36
00:02:23,725 --> 00:02:26,425
So rack distributed, sorry.

37
00:02:26,575 --> 00:02:29,635
Vector database helps with
semantic understanding.

38
00:02:30,135 --> 00:02:34,965
We also have distributed indexing,
so which means that it's sharding

39
00:02:35,025 --> 00:02:38,475
across nodes enables parallel
processing and fault tolerance.

40
00:02:38,975 --> 00:02:44,015
So now we are talking about the
sub-second retrieval optimized nearest

41
00:02:44,105 --> 00:02:50,215
neighbor search algorithms maintain
speed at scale, so we want to happen

42
00:02:50,245 --> 00:02:55,525
this entire research to be navigated
through and get us the right truth of

43
00:02:55,525 --> 00:02:58,345
information in split second timing.

44
00:02:58,375 --> 00:03:01,015
That's what we are, we care about.

45
00:03:01,315 --> 00:03:04,975
We care about the reliability,
accuracy, and also the response time.

46
00:03:05,475 --> 00:03:10,985
Event driven microservice architecture,
elastic scaling during document

47
00:03:11,585 --> 00:03:17,605
search surges, keeps ingestion and
inference smooth benefits of this

48
00:03:17,785 --> 00:03:19,825
independent scaling of components.

49
00:03:19,885 --> 00:03:25,215
Fault isolation, a technology
flexibility, and we can use

50
00:03:25,275 --> 00:03:27,705
messaging system like Kafka.

51
00:03:28,440 --> 00:03:31,320
Stateless services and
even driven workflows.

52
00:03:31,620 --> 00:03:32,340
Workflows.

53
00:03:33,060 --> 00:03:37,080
So it's not about how we
are going to implement it.

54
00:03:37,170 --> 00:03:40,800
We may use the mix and match of
different tools and technologies, but

55
00:03:41,220 --> 00:03:47,810
we need to have an architecture which
scales and helps us with the, with

56
00:03:47,810 --> 00:03:54,310
respect to the, without being worried
about the volume of the transactions.

57
00:03:54,810 --> 00:03:57,300
Efficient LLM Inference pipelines.

58
00:03:57,870 --> 00:04:00,060
So here we are talking
about the techniques like

59
00:04:00,150 --> 00:04:02,490
batching, reuse, and caching.

60
00:04:02,990 --> 00:04:08,300
They help us to cut cost drastically
while enabling instant answer.

61
00:04:08,800 --> 00:04:14,155
So batching means it's combining multiple
request to maximum GPO utilization.

62
00:04:14,265 --> 00:04:17,475
We have lots of GPO
resource, but it's costly.

63
00:04:17,475 --> 00:04:19,335
We have to use it very wisely.

64
00:04:19,635 --> 00:04:24,175
We have to if it is needed and if it is
efficient, we need to campaign multiple

65
00:04:24,175 --> 00:04:30,545
records to maximize GPO utilization
response caching, which means that storing

66
00:04:30,545 --> 00:04:35,715
common queries and responses to avoid the
computations being done repeatedly, the

67
00:04:35,715 --> 00:04:37,965
same computations being done repeatedly.

68
00:04:38,850 --> 00:04:43,070
Model quantity, quantitation,
optimizing model size without

69
00:04:43,130 --> 00:04:44,690
sacrificing their quality.

70
00:04:45,190 --> 00:04:50,570
We talked about the rag to bring
more avoid hallucination and to bring

71
00:04:50,570 --> 00:04:56,130
more accuracy in while navigating
through the document search.

72
00:04:56,180 --> 00:04:59,150
Getting the truth out of the
research publications out there.

73
00:04:59,650 --> 00:05:04,060
It's not about getting the right
information, getting the right

74
00:05:04,060 --> 00:05:06,195
information with split second timing.

75
00:05:06,865 --> 00:05:11,845
It's all also about keeping
up with the research updates.

76
00:05:11,845 --> 00:05:16,225
Like researchers change
quickly there are more and more

77
00:05:16,285 --> 00:05:17,725
research publications out there.

78
00:05:17,725 --> 00:05:21,025
Something might have changed,
something may be outdated and new

79
00:05:21,025 --> 00:05:22,345
information might be coming in.

80
00:05:22,945 --> 00:05:26,305
So we also need to keep up to
date with the research out there.

81
00:05:27,175 --> 00:05:29,755
So real time knowledge
synchronization help with that.

82
00:05:29,755 --> 00:05:32,875
It's all about in involving
the techniques, like

83
00:05:32,875 --> 00:05:34,415
continuous inte ingestions.

84
00:05:34,690 --> 00:05:38,710
Incremental updates, conflict
resolution and knowledge integration

85
00:05:39,210 --> 00:05:40,890
entity extraction at scale.

86
00:05:41,390 --> 00:05:47,030
So we are going to implement
domain specific models to

87
00:05:47,030 --> 00:05:49,100
extract genes, proteins.

88
00:05:49,250 --> 00:05:52,730
Here I'm talking about, sorry, here
I'm talking about the model specific

89
00:05:52,730 --> 00:05:55,315
to bioinformatics or biomedical.

90
00:05:55,815 --> 00:06:00,165
So domain specific models extract genes,
proteins, and diseases, merging them into

91
00:06:00,165 --> 00:06:02,145
knowledge graphs for deeper insights.

92
00:06:02,415 --> 00:06:07,125
Entity types like such as genes and
proteins may have a relationship

93
00:06:07,125 --> 00:06:08,895
like causes and cost to buy.

94
00:06:08,895 --> 00:06:13,695
So those kind of entity types and
relationship types or stored at scale.

95
00:06:14,195 --> 00:06:18,125
So which means we can navigate
easily through these geo entities

96
00:06:18,125 --> 00:06:21,785
and their relationship and find out
the right information, which we need.

97
00:06:22,285 --> 00:06:26,875
And we also need to understand the
horizontal scaling strategies rather

98
00:06:26,875 --> 00:06:28,675
than bigger machines will scale.

99
00:06:29,125 --> 00:06:32,245
We scale horizontally distributing
embeddings and inferences

100
00:06:32,335 --> 00:06:34,285
across G-P-S-G-P-U pools.

101
00:06:35,185 --> 00:06:41,960
So we need to distribute and horizontally
scaling will help us to avoid cost when

102
00:06:41,960 --> 00:06:46,400
it's not needed, and also help us to
meet the demand when it's really needed.

103
00:06:46,900 --> 00:06:51,750
Caching layers and cost optimization
caching ensures the instant results

104
00:06:51,750 --> 00:06:56,430
without invoking a full pipeline,
keeping latency low and cost sustainable.

105
00:06:56,430 --> 00:07:00,160
So we don't need to update everything
every minute, if it is something

106
00:07:00,160 --> 00:07:01,630
frequently access to query.

107
00:07:02,130 --> 00:07:05,310
And we don't need to update
it every day every however,

108
00:07:05,640 --> 00:07:07,080
every 30 minutes or like that.

109
00:07:07,110 --> 00:07:11,160
So we probably might update with the next
incremental update, but until then, we can

110
00:07:11,190 --> 00:07:14,310
retrieve from the cache to save some cost.

111
00:07:14,810 --> 00:07:20,780
So cash with can be with the embeddings
retrieval cash or response cash.

112
00:07:21,280 --> 00:07:23,290
Monitoring and observability.

113
00:07:23,890 --> 00:07:28,570
End-to-end monitoring captures throughput,
latency, and inference Quality.

114
00:07:29,070 --> 00:07:34,140
Observability ensures issues are caught
before they escalate, so we have to

115
00:07:34,140 --> 00:07:38,760
capture performance metrics, throughput
latency, error rates, and results,

116
00:07:38,790 --> 00:07:43,150
utilization, quality metrics, anomaly
detection, and distributed tracing.

117
00:07:43,165 --> 00:07:47,965
To make sure that whatever we
deploy, they work and meet the

118
00:07:47,965 --> 00:07:52,105
demand and help us to get the right
information and right on time.

119
00:07:52,605 --> 00:07:57,345
Reliability and error handling
failures are inevitable in any systems.

120
00:07:57,345 --> 00:08:01,105
So when we talk about the system
design, we also need to talk

121
00:08:01,105 --> 00:08:03,085
about handling the failures.

122
00:08:03,925 --> 00:08:05,665
What matters is resilience.

123
00:08:05,695 --> 00:08:09,475
Graceful degradation ensures
uninterrupted service.

124
00:08:10,390 --> 00:08:15,340
Resilience patterns means circuit
breakers, retry with back off

125
00:08:15,820 --> 00:08:18,490
fallback, recur, redundant components.

126
00:08:19,330 --> 00:08:24,550
So grace will, degradation may
be simpler models as backup cast

127
00:08:24,610 --> 00:08:29,800
responses when life fails, partial
resistance when complete unavailable.

128
00:08:29,975 --> 00:08:31,405
Clear error communication.

129
00:08:31,480 --> 00:08:35,050
So we need to adapt a technique
which suitable for that particular

130
00:08:35,050 --> 00:08:38,140
instance, and make sure that
it's not a complete failure.

131
00:08:38,860 --> 00:08:43,400
While we work on to rectify that
problem that is happening in the

132
00:08:43,400 --> 00:08:48,010
production infrastructure as code
and CACD, this is a common technique

133
00:08:48,010 --> 00:08:50,590
used in the software engineering.

134
00:08:51,040 --> 00:08:54,280
So infrastructure as a code
ensures reproducibility.

135
00:08:54,640 --> 00:08:58,720
CACD pipelines validate changes
and rollouts, updates safely.

136
00:08:59,530 --> 00:09:05,090
Infrastructure definition like a Terraform
Cloud formation, Kubernetes manifest.

137
00:09:05,510 --> 00:09:09,760
Automated testing, we can ingest
embed the automated testings in

138
00:09:09,760 --> 00:09:13,870
the code file to make sure that's
happening along with the pipeline run.

139
00:09:13,870 --> 00:09:16,920
CACD pipeline runs gradual rollout.

140
00:09:17,850 --> 00:09:20,970
Canary deployments and bluegreen
strategies, performance

141
00:09:20,970 --> 00:09:25,620
fallback, continuous monitoring
of the deployed changes.

142
00:09:26,120 --> 00:09:28,400
Containerization and
deployment strategies.

143
00:09:29,180 --> 00:09:35,940
So when we talk about the environment,
the run environment of any code, we also

144
00:09:35,940 --> 00:09:37,740
need to talk about the containerization.

145
00:09:38,730 --> 00:09:43,320
So microservices run in isolated,
continuous orchestrated for scaling,

146
00:09:43,320 --> 00:09:45,090
resilience and seamless update.

147
00:09:45,930 --> 00:09:47,550
So the benefits of container.

148
00:09:47,895 --> 00:09:47,955
It.

149
00:09:48,915 --> 00:09:53,205
It provides a consistent environment,
isolated dependencies, efficient

150
00:09:53,295 --> 00:09:59,285
resource usage, orchestration
features, autoscaling, self-healing,

151
00:09:59,435 --> 00:10:03,445
rolling updates, load balancing,
so deployment strategies.

152
00:10:03,495 --> 00:10:08,415
When we comes to deployment strategies,
we can adapt blue green deployment,

153
00:10:08,415 --> 00:10:12,370
canary releases, feature flags, et cetera.

154
00:10:12,870 --> 00:10:16,560
So we also need to come up with
the benchmarking and draw a line

155
00:10:16,890 --> 00:10:18,480
and do the performance validation.

156
00:10:18,980 --> 00:10:24,320
So benchmarks, they validate design
goals, testing under load failure

157
00:10:24,320 --> 00:10:26,180
scenarios, guides capacity planning.

158
00:10:26,680 --> 00:10:28,905
So we have to define those matrices.

159
00:10:28,905 --> 00:10:32,925
Like latency, like for an
example, it should be less than 50

160
00:10:32,925 --> 00:10:35,725
milliseconds for typical queries.

161
00:10:36,685 --> 00:10:42,415
Throughput how many queries should be
processed per second accuracy how much

162
00:10:42,415 --> 00:10:47,695
accuracy we expect We have to define those
matrics and constantly monitoring that

163
00:10:47,965 --> 00:10:53,935
those matrics are met and we are on track
and we deliver what we intend to deliver.

164
00:10:54,505 --> 00:10:58,585
And uptime system availability,
that's a very crucial part.

165
00:10:59,085 --> 00:11:00,885
Cross domain applicability.

166
00:11:01,385 --> 00:11:07,205
Though I talked about biomedical
research, the same patterns, ex can be

167
00:11:07,205 --> 00:11:13,964
extended to law, finance, insurance,
or any type of knowledge for legal.

168
00:11:13,964 --> 00:11:19,590
For an instance, we can have case laws,
sta statutes and regulatory documentation,

169
00:11:19,650 --> 00:11:25,935
doc documents, finance market reports,
filings and analysis, technical documents,

170
00:11:25,935 --> 00:11:28,635
specifications, manuals, and standards.

171
00:11:29,219 --> 00:11:33,749
Historical archives like manuscripts,
records and cultural artifacts.

172
00:11:33,814 --> 00:11:38,930
In, in, in certain industry we may
have plenty of documents conclusion.

173
00:11:39,430 --> 00:11:41,530
So high performance platforms.

174
00:11:41,530 --> 00:11:45,130
So like you carefully engineering,
pipe engineered pipelines.

175
00:11:45,400 --> 00:11:48,699
This blueprint accelerates
discovery across science industry.

176
00:11:49,199 --> 00:11:52,530
It's not about deploying
an LLM for our need.

177
00:11:52,560 --> 00:11:55,319
It's about engineering
an ecosystem around it.

178
00:11:56,010 --> 00:12:02,119
That's, that's then only we can
achieve the accuracy, the speed,

179
00:12:02,239 --> 00:12:07,669
and also we keep up to date to avoid
outdated information being transmitted

180
00:12:07,669 --> 00:12:09,169
to the critical science community.

181
00:12:10,069 --> 00:12:12,349
The architecture patterns
we have explored provide.

182
00:12:12,769 --> 00:12:16,789
Foundation for building scalable,
reliable, efficient AI literature

183
00:12:16,789 --> 00:12:20,509
processing platforms that can
handle exponential growth of

184
00:12:20,509 --> 00:12:22,009
obligations across domain.

185
00:12:22,509 --> 00:12:24,399
Thank you for this opportunity.

186
00:12:24,609 --> 00:12:30,329
I hope that this particular presentation
would've helped few audiences.

187
00:12:30,649 --> 00:12:32,209
And thank you for this opportunity.

188
00:12:32,239 --> 00:12:32,779
One second.

189
00:12:33,279 --> 00:12:33,609
Thank you.

