1
00:00:00,500 --> 00:00:01,339
Hi everyone.

2
00:00:01,370 --> 00:00:06,129
I'm RA Herra, and today I'm sharing
a real world case study on building

3
00:00:06,520 --> 00:00:11,799
silent cross cloud analytics
pipeline with Azure Databricks

4
00:00:12,029 --> 00:00:14,399
with more than 10 TB daily scale.

5
00:00:14,949 --> 00:00:19,689
You'll hear exactly how we combined
SRE discipline with Delta Lake

6
00:00:19,689 --> 00:00:24,129
optimizations to keep strict SLAs
while making performance predictable

7
00:00:24,459 --> 00:00:26,229
and keeping the costs sane.

8
00:00:26,729 --> 00:00:31,680
For context, I think billions of
events per day, multi-region ingestion

9
00:00:31,740 --> 00:00:35,580
and user facing dashboards that
must stay fresh within minutes.

10
00:00:36,080 --> 00:00:39,350
So here's the flow for
the next 12 to 15 minutes.

11
00:00:39,600 --> 00:00:44,930
So the first I'll discuss the architecture
and where the cross cloud seam are.

12
00:00:45,110 --> 00:00:48,700
And second the reliability
engineering we enforced.

13
00:00:49,060 --> 00:00:53,020
Third, the performance work,
especially Delta Lake and Sea order.

14
00:00:53,350 --> 00:00:56,530
Fourth, our incident
playbook and how we shrank.

15
00:00:56,540 --> 00:00:57,500
MTTR.

16
00:00:57,894 --> 00:01:02,635
And finally a practical
blueprint too that you can adopt.

17
00:01:02,875 --> 00:01:07,375
And I anchor each section with
the concrete thresholds and

18
00:01:07,375 --> 00:01:09,205
examples so you can relate to them.

19
00:01:09,705 --> 00:01:13,995
So our challenge was ad
campaign in telemetry in

20
00:01:13,995 --> 00:01:15,825
double digit terabytes per day.

21
00:01:16,785 --> 00:01:23,295
So with many interdependent e ETLs,
a single upstream blip could cascade.

22
00:01:23,920 --> 00:01:26,020
The business expectations were blunt.

23
00:01:26,320 --> 00:01:31,450
So real time dashboards, historical
trends in minutes, cross cloud consistency

24
00:01:31,450 --> 00:01:35,260
without wasteful duplication and
predictable performance, even when the

25
00:01:35,260 --> 00:01:37,930
load is spikey without blowing the budget.

26
00:01:37,960 --> 00:01:45,250
And we set the working target of P 95,
freshness less than 15 minutes, and P 99,

27
00:01:45,280 --> 00:01:49,810
less than 30 minutes with back pressure
rules when the, these numbers drift.

28
00:01:50,310 --> 00:01:56,650
So we deliberately cross clouded not to
use everything but to pick the best pieces

29
00:01:56,650 --> 00:01:59,590
and define hard contracts at the seams.

30
00:02:00,429 --> 00:02:06,650
Three principles guided US one, data
locality and replication across regions

31
00:02:06,890 --> 00:02:09,840
and providers for HADR and latency.

32
00:02:10,290 --> 00:02:15,590
R-D-R-R-P-O, targeted single digit
minutes and RTO under one hand.

33
00:02:16,550 --> 00:02:19,420
So two standardized APIs and oss.

34
00:02:19,670 --> 00:02:25,130
So Spark or delta are lingo, franca,
and provider perks are hidden.

35
00:02:25,630 --> 00:02:28,210
And the third is unified absorbability.

36
00:02:28,570 --> 00:02:32,560
One place to see metrics,
logs, and traces across clouds.

37
00:02:32,620 --> 00:02:35,840
So detection and response are fast.

38
00:02:36,080 --> 00:02:39,765
Every alert ties to an
explicit SLI and a runbook.

39
00:02:40,265 --> 00:02:43,145
Concretely, our stack looked like this.

40
00:02:43,325 --> 00:02:50,685
So we had Azure Databricks for spar, for
compute paired with Delta Lake for acid

41
00:02:50,685 --> 00:02:57,455
se semantics, observe schema in evolution
and time travel and Azure data factory

42
00:02:57,515 --> 00:03:01,570
orchestrated dependencies, retries,
and monitoring with jitter back offs.

43
00:03:02,240 --> 00:03:06,800
Amazon S3 provided durable
versioning storage, where we needed

44
00:03:06,800 --> 00:03:09,080
it for landing curated zones.

45
00:03:09,620 --> 00:03:10,580
Around those.

46
00:03:10,580 --> 00:03:15,450
We added reliability layer this
where like the circuit breakers, bulk

47
00:03:15,450 --> 00:03:17,340
heads, and the boundary isolation.

48
00:03:17,340 --> 00:03:19,500
So a cloud specific issue is.

49
00:03:20,000 --> 00:03:25,480
On top a performance layer which
had this z order and partitioning

50
00:03:25,540 --> 00:03:30,400
aligned to access, small file,
compaction, dynamic allocation.

51
00:03:30,400 --> 00:03:35,590
And finally, an operational layer run
where we had the run books, automated

52
00:03:35,590 --> 00:03:37,630
remediation and blameless reviews.

53
00:03:38,130 --> 00:03:43,590
Every handoff writes checkpoints, and the
stage can resume without rep reprocessing.

54
00:03:44,090 --> 00:03:47,810
And we ran this like a product using SRE.

55
00:03:48,290 --> 00:03:53,900
So we tracked SLIs on three things, the
freshness from the ingest to the gold

56
00:03:53,900 --> 00:04:00,200
availability P 95, query latency for key
workloads and end-to-end success rate.

57
00:04:01,160 --> 00:04:06,680
We set error budgets that triggered
graceful degradation for reliability

58
00:04:06,680 --> 00:04:13,200
slip, for example, pausing non-critical
enrichments if the seven day rolling

59
00:04:13,200 --> 00:04:15,420
P five freshness exceeded budget.

60
00:04:16,260 --> 00:04:22,130
So post incident, blameless reviews,
drove sim, systematic fixes, and we

61
00:04:22,190 --> 00:04:27,100
attacked toil first, like automating
the top recurring failure classes.

62
00:04:27,100 --> 00:04:29,005
So humans are in babysitting pipeline.

63
00:04:29,505 --> 00:04:33,805
Our goal was to keep auto
resolution above 70%.

64
00:04:34,305 --> 00:04:40,335
To make failure safe, we standardized
four pipelines, patents, checkpointing, so

65
00:04:40,335 --> 00:04:43,565
every stage can resume from known offsets.

66
00:04:43,805 --> 00:04:47,995
Offsets are committed only after
item potent rights succeed.

67
00:04:48,760 --> 00:04:51,630
Circuit breakers at integration points.

68
00:04:51,630 --> 00:04:58,140
So we stop cascades when freshness
drifts and or dependency fail repeatedly.

69
00:04:58,590 --> 00:05:03,970
For instance trip if error rate is
greater than 10% for five minutes,

70
00:05:03,970 --> 00:05:07,875
or if the freshness breaches versus
for three cons consecutive checks.

71
00:05:08,375 --> 00:05:10,595
Exponential back offs with jitters.

72
00:05:10,645 --> 00:05:14,565
Retries do not create
stampedes and random jitters.

73
00:05:14,565 --> 00:05:16,095
Were up to three seconds.

74
00:05:16,425 --> 00:05:18,105
And item put in write offs.

75
00:05:18,155 --> 00:05:19,925
Replace never corrupt state.

76
00:05:20,195 --> 00:05:24,395
All mergers are key and we
avoid side effects on retry.

77
00:05:25,175 --> 00:05:30,355
In short, retrials only work when
your operations are at important.

78
00:05:30,855 --> 00:05:34,085
And then performance was the other half.

79
00:05:34,784 --> 00:05:39,074
Before tuning we had full
scans, big shuffles and spike

80
00:05:39,074 --> 00:05:43,634
latencies, and we aligned storage
to actually access patterns.

81
00:05:43,944 --> 00:05:47,594
Partitioning that prunes
for example by event date.

82
00:05:47,954 --> 00:05:52,474
And the ordering on high
selectivity, keys used together.

83
00:05:52,504 --> 00:05:55,534
Think campaign ID region and device time.

84
00:05:56,359 --> 00:06:01,909
So we kept metadata lean with
optimize and small file compaction.

85
00:06:02,150 --> 00:06:04,939
So spark wasn't drowning in tiny files.

86
00:06:05,869 --> 00:06:11,030
The result, drastic input, output
reduction and complex queries dropping two

87
00:06:11,119 --> 00:06:15,319
minutes or even seconds with stable P 95.

88
00:06:15,679 --> 00:06:21,269
And K hit rates improved and shuffle,
spill decreased meaningfully.

89
00:06:21,769 --> 00:06:24,739
Practically, how did we choose C or keys?

90
00:06:24,839 --> 00:06:29,999
We mined query logs to find the most
frequent multi column filters with

91
00:06:29,999 --> 00:06:33,509
high cardinality and low overlap.

92
00:06:34,199 --> 00:06:41,309
We validated with A or B runs and
representative workloads and revisited

93
00:06:41,309 --> 00:06:46,779
keys quarterly because access patterns
shift with new features and dashboards.

94
00:06:47,589 --> 00:06:52,459
And we also capsize the order frequency
to avoid consistent tree clustering.

95
00:06:52,859 --> 00:06:57,419
So typically weekly for hot
tables, monthly for warm ones.

96
00:06:57,919 --> 00:07:02,119
We also tackled cost without
sacrificing reliability.

97
00:07:02,509 --> 00:07:07,839
We abandoned fixed clusters and
moved to workload aware auto-scaling.

98
00:07:08,379 --> 00:07:13,929
Inputs included priority tire,
incoming volume q, depth SLA,

99
00:07:13,929 --> 00:07:19,409
proximity utilization, and hard
costs guards like max nodes per tire.

100
00:07:19,909 --> 00:07:24,319
That combination kept clusters
right sized cut, ideal burn

101
00:07:24,409 --> 00:07:26,089
and preserved predictability.

102
00:07:26,449 --> 00:07:29,569
The side effect far fewer capacity pages.

103
00:07:29,869 --> 00:07:32,989
Engineers focused on data
quality and features.

104
00:07:33,829 --> 00:07:38,859
When nearing an SLA breach, we
temporarily allele burst scaling,

105
00:07:39,279 --> 00:07:41,544
then decay back to steady state.

106
00:07:42,044 --> 00:07:43,814
Incidents still happen.

107
00:07:43,934 --> 00:07:47,204
And our response lifecycle is five steps.

108
00:07:47,504 --> 00:07:53,634
So we have detection is it is
S-L-A-S-L-I driven, freshness, drift,

109
00:07:53,634 --> 00:07:59,534
and success rates not just the CPU
classification tag scope, and business

110
00:07:59,534 --> 00:08:01,760
criticality and selects the runbook.

111
00:08:02,260 --> 00:08:07,610
Containment trips, breakers diverse
traffic to last known good slices

112
00:08:07,940 --> 00:08:09,455
and shred non-critical loads.

113
00:08:09,955 --> 00:08:17,125
Reation kicks off self-healing re qing
item potent stages with jitter, retries,

114
00:08:17,335 --> 00:08:20,785
compacts, small files if that's the cause.

115
00:08:21,355 --> 00:08:24,525
And restart targeted tasks of whole jobs.

116
00:08:25,025 --> 00:08:30,735
Recovery resumes from checkpoints and
passes a consistency validation gate like

117
00:08:30,735 --> 00:08:33,885
the schema, row counts, and spot queries.

118
00:08:34,125 --> 00:08:36,500
So before we lift any stale banners.

119
00:08:37,000 --> 00:08:38,530
A concrete example.

120
00:08:39,110 --> 00:08:45,300
So an AWS S3 cross Region Replication
Lag Freshness monitors flagged a

121
00:08:45,300 --> 00:08:50,580
timestamp drift beyond budget, and
we paused drown stream joints served

122
00:08:50,580 --> 00:08:53,525
the last consistent slice with a new.

123
00:08:54,120 --> 00:08:59,480
Clear stale data banner and diver
diverted non critical workloads.

124
00:09:00,170 --> 00:09:05,270
When replication recovered, we validated
consistency on B boundary tables

125
00:09:05,480 --> 00:09:09,200
burst, scaled, or clear the backlog.

126
00:09:10,145 --> 00:09:14,945
And return to steady state because
the playbook was predefined and

127
00:09:14,945 --> 00:09:18,005
on call only approved steps.

128
00:09:18,215 --> 00:09:20,465
Everything else was automated.

129
00:09:20,965 --> 00:09:27,105
So the metrics we get about most
was MTTR, first time success

130
00:09:27,105 --> 00:09:29,205
rate, P 95, query latency.

131
00:09:29,705 --> 00:09:36,345
And auto resolution rate and freshness,
data quality, error rate, and overall

132
00:09:36,345 --> 00:09:41,175
system uptime versus a 99.9% target.

133
00:09:41,955 --> 00:09:45,765
Watching these together is
what lets you act proactively.

134
00:09:45,945 --> 00:09:51,795
For example, a rising DQ error rate
pair with stable compute such as

135
00:09:51,825 --> 00:09:57,375
sche drift, not capacity, a widening
P 95 with flat volume, such as.

136
00:09:57,455 --> 00:10:00,305
Partitions queue or small file bloat.

137
00:10:00,805 --> 00:10:05,105
So if you are adopting this
here's a practical blueprint.

138
00:10:05,285 --> 00:10:10,995
So foundation that defined SLIs
and error budgets set up cost cloud

139
00:10:10,995 --> 00:10:16,875
authentication, and build a unified
monitoring fabric and be explicit about

140
00:10:16,875 --> 00:10:22,745
your consistency model when is slightly
stale, acceptable, and for how long?

141
00:10:23,580 --> 00:10:28,950
And then for reliability, put circuit
breakers at every integration scene.

142
00:10:29,130 --> 00:10:34,940
Configure auto-scaling policies with
burst windows, and add checkpoints at

143
00:10:34,940 --> 00:10:42,210
all handoffs and gate promotions to
gold through validation and schemas

144
00:10:42,260 --> 00:10:44,450
row counts and sampling and freshness.

145
00:10:44,950 --> 00:10:49,830
For performance tuned shuffles and
partitions implements that order from

146
00:10:49,830 --> 00:10:55,170
real query patents enable dynamic
allocation and targeted occasion

147
00:10:55,200 --> 00:11:00,880
only where hit rates justify and
schedule small file compaction.

148
00:11:01,115 --> 00:11:07,355
With the thresholds, for example,
compact, when average file size is less

149
00:11:07,355 --> 00:11:15,345
than 1 28 mb. And for operational codify
the runbooks, automate the top three

150
00:11:15,465 --> 00:11:21,315
remediations, hold blameless reviews
with action items and drill cross stream

151
00:11:21,315 --> 00:11:24,165
on two failures modes per quarter.

152
00:11:24,665 --> 00:11:30,965
So my key takeaways are one, at
this scale, SRE is not optional.

153
00:11:31,145 --> 00:11:37,985
It's the backbone two, automate
recovery to slash mttr and on-call ing.

154
00:11:38,600 --> 00:11:42,410
Target more than half of the
incidents to be self healed.

155
00:11:43,010 --> 00:11:46,040
Three, optimize for your access patterns.

156
00:11:46,230 --> 00:11:50,550
So z order and storage, lay out
choices, deliver outside spins.

157
00:11:51,000 --> 00:11:54,270
Verify with query logs, not intuition.

158
00:11:55,200 --> 00:12:00,580
And four cross cloud only works with
intentional contracts at the seams.

159
00:12:00,880 --> 00:12:04,630
That is how you prevent cascades
and keep costs predictable.

160
00:12:05,130 --> 00:12:06,660
So thank you for listening.

161
00:12:06,690 --> 00:12:11,410
And in q and a I'm happy to unpack
how we picked the order keys from

162
00:12:11,410 --> 00:12:18,190
query logs and extract freshness, sli
thresholds we used and why, and or

163
00:12:18,250 --> 00:12:23,090
the conditions that trip or circuit
breakers, including the grace periods

164
00:12:23,090 --> 00:12:25,670
and histories that prevent flapping.

165
00:12:26,300 --> 00:12:27,080
Thank you.

