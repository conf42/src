1
00:00:00,050 --> 00:00:07,109
my name is Rahing Taku and I am working
as a lead software engineer in Salesforce.

2
00:00:08,759 --> 00:00:11,529
I have I wanted to talk about this.

3
00:00:11,584 --> 00:00:14,524
I, how can we scale the the service?

4
00:00:15,899 --> 00:00:20,999
So I do want to talk about the, how
can we enhance the clouds, scalability

5
00:00:21,840 --> 00:00:24,600
and fault always using the sharding.

6
00:00:25,950 --> 00:00:29,010
So before I start, I will give
the little background about this.

7
00:00:29,520 --> 00:00:33,210
So now, nowadays it's very
common practice to deploy the

8
00:00:33,210 --> 00:00:35,290
service as microservices, right?

9
00:00:36,870 --> 00:00:40,230
When we deploy microservices, there are.

10
00:00:43,125 --> 00:00:44,895
Replic of the service which runs.

11
00:00:45,385 --> 00:00:49,735
And it is mainly done because we want
to make sure if there is an issue with

12
00:00:49,765 --> 00:00:53,245
one replica of the service, the other
is still continue to serve the profit.

13
00:00:53,595 --> 00:00:56,035
So my idea is also on the same.

14
00:00:56,375 --> 00:00:58,855
But but it extend the idea.

15
00:00:59,125 --> 00:01:04,735
So instead of having just one instance
of the service, we can actually, the.

16
00:01:05,290 --> 00:01:09,550
Services itself, we can have the
multiple instance of the services

17
00:01:10,630 --> 00:01:16,470
and then each instance of the service
serves the some traffic, right?

18
00:01:17,460 --> 00:01:22,950
And and and using this architecture,
we can actually improve the.

19
00:01:23,550 --> 00:01:28,680
Improve on and actually have the
high availability for our service.

20
00:01:30,840 --> 00:01:31,110
Yeah.

21
00:01:31,290 --> 00:01:36,160
So I will go in deep down on the
different aspect of the aspect of it.

22
00:01:38,185 --> 00:01:40,735
So first thing is that why
sharding matters, right?

23
00:01:40,735 --> 00:01:42,470
So if we do not shard, right?

24
00:01:42,520 --> 00:01:46,200
Then most of the then you have one
single point of failure, right?

25
00:01:46,200 --> 00:01:48,240
Everything goes on to
the same place, right?

26
00:01:49,200 --> 00:01:53,050
Which means that if there is anything
has to go wrong it'll impact all

27
00:01:53,050 --> 00:01:55,519
the all the incoming traffic, right?

28
00:01:57,634 --> 00:02:00,604
And since everything is coming onto
the single place, you cannot have an

29
00:02:00,604 --> 00:02:03,374
intelligent load distribution, right?

30
00:02:04,464 --> 00:02:07,674
And then it affect your
availability, right?

31
00:02:07,734 --> 00:02:10,014
If everything is on one place,
everything has to go down.

32
00:02:10,014 --> 00:02:13,789
It'll affect it'll reduce the
availability of the service.

33
00:02:15,409 --> 00:02:17,239
In my proposed architecture.

34
00:02:17,749 --> 00:02:23,629
So there are I'm introducing a
metadata service and so in my proposed

35
00:02:23,629 --> 00:02:26,699
architecture there are different
instance of the service, right?

36
00:02:26,699 --> 00:02:29,429
So not only one instance, but
different instance of the service.

37
00:02:31,049 --> 00:02:34,589
And now since there are different
instance of the service the request

38
00:02:34,589 --> 00:02:36,119
need to go to the one instance.

39
00:02:37,589 --> 00:02:39,239
Always need to go to the same instance.

40
00:02:39,239 --> 00:02:41,189
So because of that, we
need a metadata service.

41
00:02:41,789 --> 00:02:46,439
The responsibility of the metadata
service is to make sure that for a given

42
00:02:46,979 --> 00:02:52,569
request identified as the same request
using the IP address or the customer

43
00:02:52,629 --> 00:02:57,009
credentials, it always goes onto the
same chart of the service, right?

44
00:02:58,129 --> 00:02:59,929
So that's why we need
the metadata service.

45
00:03:00,869 --> 00:03:08,749
So Met also can be used for, managing
more metadata about the whole, all

46
00:03:08,749 --> 00:03:12,229
the services that participate in
this, and then it knows about the,

47
00:03:12,229 --> 00:03:13,489
all the charts of the services.

48
00:03:13,559 --> 00:03:17,794
And then at the end, then it'll
redirect the service through particular.

49
00:03:18,794 --> 00:03:23,724
Okay, so before, before I go further
deep, I just want to focus one more time.

50
00:03:23,814 --> 00:03:25,524
Like what are the benefits of it?

51
00:03:25,614 --> 00:03:30,564
So since we have now multiple instance
of the service running, it'll improve

52
00:03:30,564 --> 00:03:32,244
the availability of the service.

53
00:03:32,664 --> 00:03:35,814
If one instance is down, it still,
there are, let's say we have five

54
00:03:35,814 --> 00:03:38,634
instance of the service still,
there are four instance running, so

55
00:03:38,634 --> 00:03:43,104
that means the avail availability
is down only by the 20%, right?

56
00:03:44,484 --> 00:03:47,790
And it also helps to make
sure that all the request, the

57
00:03:50,279 --> 00:03:50,629
which.

58
00:03:51,629 --> 00:03:55,169
So for example, we have dive five,
different instance, and then each of

59
00:03:55,169 --> 00:03:57,299
the fraction, like 20% of the customers.

60
00:03:57,299 --> 00:04:01,199
And then if we have a local cache, then
it can serve the customer better, right?

61
00:04:01,249 --> 00:04:05,599
So we have a small instance and
then we know the request from

62
00:04:05,599 --> 00:04:08,829
the customer will always come to,
come on this particular instance.

63
00:04:08,829 --> 00:04:10,759
So then it can so it'll be able to.

64
00:04:13,379 --> 00:04:17,429
And third one is like the, it can
optimize the deployment, right?

65
00:04:17,459 --> 00:04:23,869
So now if you can configure your pipeline
to do Thete deployment and whenever

66
00:04:23,869 --> 00:04:27,259
you are releasing the change and not
release on all the five instance, but

67
00:04:27,259 --> 00:04:32,629
release it on the first one, instance
first, and do all kind validation.

68
00:04:33,629 --> 00:04:36,719
To go into production and then
it's not causing any regression.

69
00:04:36,719 --> 00:04:39,270
Just wanna add more on the benefits.

70
00:04:39,569 --> 00:04:42,840
Since we have multiple instance
of the service, we can actually

71
00:04:42,919 --> 00:04:45,679
have the intelligent routing.

72
00:04:46,044 --> 00:04:46,335
Okay.

73
00:04:47,719 --> 00:04:51,259
If you want to scale up or scale
down one particular instance of the

74
00:04:51,259 --> 00:04:55,280
service to serve a different purpose,
then you can actually scale it up.

75
00:04:55,280 --> 00:04:59,710
And then while you keep the keep the
so for example, let's say you have

76
00:04:59,710 --> 00:05:02,919
five instance of the service and then
one instance which serves the current.

77
00:05:04,325 --> 00:05:07,715
The request for the, let's
say for the current day, okay.

78
00:05:08,075 --> 00:05:11,045
And then the other FI for
instance, they serve the request

79
00:05:11,045 --> 00:05:12,215
from the historic data, right?

80
00:05:12,545 --> 00:05:17,315
So you can actually scale the current
service, which serve the current data.

81
00:05:17,885 --> 00:05:19,955
To be more performant.

82
00:05:20,284 --> 00:05:25,294
So yeah, this is just an example, but the
idea is that if you have different, then

83
00:05:25,294 --> 00:05:30,435
you can actually ma manage them separately
and then you can have the intelligent

84
00:05:30,495 --> 00:05:34,224
load distribution also done for isolation.

85
00:05:36,390 --> 00:05:39,419
For example, let's say we have,
again, going with the same

86
00:05:39,419 --> 00:05:40,710
example, we have five instance.

87
00:05:40,710 --> 00:05:42,780
They are running in different
five availability zone.

88
00:05:43,020 --> 00:05:45,419
Let's say there is an issue with
the vulner availability zone.

89
00:05:46,009 --> 00:05:49,529
It does not impact the other
four instance, which are on the

90
00:05:49,529 --> 00:05:50,789
different availability zone, right?

91
00:05:50,789 --> 00:05:53,099
So you can actually isolate the fault.

92
00:05:53,099 --> 00:05:54,869
You can minimize the blast radius.

93
00:05:57,599 --> 00:06:00,899
Then also if we use it correctly,
we can actually use it for

94
00:06:00,899 --> 00:06:02,339
the cost efficiency also.

95
00:06:02,489 --> 00:06:07,859
Because if we had only one instance
where you have to keep scaling, scale

96
00:06:07,859 --> 00:06:12,699
up that instance all the time in this
scenario, you can actually scale up

97
00:06:12,699 --> 00:06:15,849
or scale down the different instance
at the different point of the time.

98
00:06:15,999 --> 00:06:18,699
So it gives you the flexibility
of managing it better.

99
00:06:21,264 --> 00:06:21,594
Okay.

100
00:06:21,644 --> 00:06:26,394
Now I do want to talk about the,
a little about the implementation.

101
00:06:27,144 --> 00:06:30,924
So we need way to dynamically
provision this instance, right?

102
00:06:31,024 --> 00:06:34,384
So now since we have
more than one instance.

103
00:06:35,329 --> 00:06:40,099
And it is possible for the number
of instance to grow or reduce in the

104
00:06:40,099 --> 00:06:44,909
size, as in when we, as in, when the
the product grow or shrink, right?

105
00:06:44,959 --> 00:06:49,969
So it is possible, it is very
important for us to have a service

106
00:06:50,239 --> 00:06:53,779
which can provision the service
instance dynamically, right?

107
00:06:53,779 --> 00:06:55,969
So it need to be configurable and.

108
00:06:57,544 --> 00:07:00,955
And then there need to be the
configuration, which can be done

109
00:07:00,955 --> 00:07:02,754
separately for the each instance, right?

110
00:07:02,804 --> 00:07:03,624
And yep.

111
00:07:03,894 --> 00:07:10,730
So going next, yeah, we also need to
have, maintain some kind of configuration

112
00:07:10,730 --> 00:07:12,110
for all these instance, right?

113
00:07:12,110 --> 00:07:15,140
Since these are the different
instance, we, I mean it's the same

114
00:07:15,140 --> 00:07:16,280
service, but it's still different.

115
00:07:16,280 --> 00:07:19,100
Instance, there may be some runtime
properties which are different

116
00:07:19,100 --> 00:07:20,180
for the different instance.

117
00:07:20,570 --> 00:07:26,450
And what we want to make sure is
that the are configurable, right?

118
00:07:26,450 --> 00:07:29,540
So for that we need a
configuration service.

119
00:07:30,125 --> 00:07:34,535
And for each instance we configure the
properties same or differently, depend

120
00:07:34,535 --> 00:07:39,805
on the use case and then then used
when we deploy the service instance.

121
00:07:41,485 --> 00:07:41,815
Okay.

122
00:07:42,335 --> 00:07:45,725
So as I told before, like we
also need a metadata service.

123
00:07:45,725 --> 00:07:51,335
The responsibility of the metadata service
is to make sure that we are able to route

124
00:07:51,335 --> 00:07:57,605
the request to the same chart every time
which means that we need to have the

125
00:07:57,935 --> 00:08:02,715
consistent routing, also, I just want
to touch upon the stateful migration.

126
00:08:02,865 --> 00:08:09,375
So if your service is state,
which means that it has it, it to.

127
00:08:11,460 --> 00:08:19,230
Then if you want to move the customer
from the one, if you want to move

128
00:08:19,230 --> 00:08:22,830
the customer from the one instance to
another instance for any reason, right?

129
00:08:22,860 --> 00:08:24,300
There can be multiple reasons.

130
00:08:24,350 --> 00:08:27,585
For example, one of the instances, is
not able to serve any more traffic.

131
00:08:27,585 --> 00:08:31,345
So you don't want you want to distribute
the traffic and move some of the traffic

132
00:08:31,345 --> 00:08:32,870
to the the other instance, right?

133
00:08:33,640 --> 00:08:35,135
Or maybe for some other reasoning.

134
00:08:35,135 --> 00:08:38,445
For example, you have a logical
distribution of the customers,

135
00:08:38,445 --> 00:08:41,085
but then for some reason you
decide to move the one customer.

136
00:08:43,765 --> 00:08:44,935
So this like open to.

137
00:08:45,935 --> 00:08:50,495
But the point is, for any reason,
at any point, if you decide to move

138
00:08:50,495 --> 00:08:52,965
the change the chart for a customer.

139
00:08:53,580 --> 00:08:56,340
If it's in a stateful service,
then we have to take care of the

140
00:08:56,340 --> 00:08:58,125
state for the data migration also.

141
00:08:58,125 --> 00:09:00,075
And we all know data
migration is not easy.

142
00:09:00,125 --> 00:09:04,225
But but I think if it's done correctly,
we can actually have this also.

143
00:09:04,465 --> 00:09:08,575
The point is that we have to consider
the stateful migration when we are to.

144
00:09:09,575 --> 00:09:13,745
Okay, now I would like to talk
about some of the challenges.

145
00:09:13,995 --> 00:09:15,585
One is the deeper complexity, right?

146
00:09:15,635 --> 00:09:18,395
When we want to debug an issue, right?

147
00:09:18,425 --> 00:09:21,645
The first thing we need to know
the routing of the shard happened

148
00:09:21,645 --> 00:09:25,585
correctly then we need to know
which shard the customer belongs to.

149
00:09:25,975 --> 00:09:30,485
And then and then for each search, since
there are different configuration, we

150
00:09:30,485 --> 00:09:34,135
just want to make sure that so it's up
to the complexity for debugging, right?

151
00:09:35,735 --> 00:09:36,755
Migration complexity.

152
00:09:36,755 --> 00:09:39,475
I think this is one of the biggest
challenge the migration complex.

153
00:09:41,380 --> 00:09:43,930
As I explained before, we
have different insights.

154
00:09:43,930 --> 00:09:46,800
We if for any reason we
decide to change the,

155
00:09:49,410 --> 00:09:49,770
we.

156
00:09:50,770 --> 00:09:54,190
Moving the data in the live
production system is not easy.

157
00:09:54,190 --> 00:09:58,960
That's why I recommend if we ever try
to do it consider the dual ingestion.

158
00:09:58,960 --> 00:10:01,720
So you keep ingesting the
data into the old shard.

159
00:10:01,720 --> 00:10:05,320
You keep start to ingest the data into
the new shard, then the data is in sync.

160
00:10:05,320 --> 00:10:08,755
Then you move the customer
from the from the, and.

161
00:10:09,755 --> 00:10:12,875
Being served from the new shot,
then you can think about cleanup

162
00:10:13,085 --> 00:10:14,785
from the from the old chart.

163
00:10:15,445 --> 00:10:21,115
And also I will suggest that if we
keep the retention based cleanup,

164
00:10:21,115 --> 00:10:23,385
that will be nice in that sense.

165
00:10:23,385 --> 00:10:26,495
You don't have to worry about
cleanup of the data, but like when

166
00:10:26,495 --> 00:10:28,875
the data leads to the end of life.

167
00:10:29,875 --> 00:10:32,005
Then third is infrastructure complexity.

168
00:10:32,005 --> 00:10:36,435
So since now we have multiple instance
you need to have a service which

169
00:10:36,435 --> 00:10:40,245
can spin up the spin up the new
instance and take care of it, right?

170
00:10:40,295 --> 00:10:40,715
Yeah.

171
00:10:40,805 --> 00:10:42,870
So this will add up the extra complexity.

172
00:10:44,130 --> 00:10:45,420
The data consistency.

173
00:10:45,420 --> 00:10:50,000
I think that the data consistency is
mainly when we actually moving the

174
00:10:50,000 --> 00:10:51,920
customer from the one shot to another.

175
00:10:52,010 --> 00:10:55,220
So we just want to make sure
that the data is consistent.

176
00:10:56,040 --> 00:10:59,205
Network latency is also in the
same sense that when we are when

177
00:10:59,205 --> 00:11:02,780
we are actually moving the customer
from one shot to another, we are

178
00:11:02,785 --> 00:11:04,085
actually doing the deal, right?

179
00:11:04,115 --> 00:11:08,405
And then that might add up to
the latency for the customer.

180
00:11:10,830 --> 00:11:15,730
I do want to to support some of the
mitigation for the Herbo problem.

181
00:11:16,010 --> 00:11:19,460
The first thing is that if we have
the robust monitoring so for any given

182
00:11:19,460 --> 00:11:24,380
customer, if we know the, like which
chart the customer goes, and then all the

183
00:11:24,380 --> 00:11:26,577
configurations that we can see for the.

184
00:11:28,915 --> 00:11:33,445
And then having a proper dashboard
and alarms to monitor the, monitor,

185
00:11:33,445 --> 00:11:35,900
the each each service instance, right?

186
00:11:35,960 --> 00:11:42,640
This will help to reduce the the debugging
effort to and can be done very fast.

187
00:11:43,450 --> 00:11:46,170
The second time second
is caching strategy.

188
00:11:46,270 --> 00:11:51,495
The caching strategy will help a lot
during the in the metadata service

189
00:11:51,495 --> 00:11:57,045
when we actually wanna, assign when
we want to decide the which chart the

190
00:11:57,045 --> 00:12:01,935
customer should go, I think the caching
strategy will play an important role

191
00:12:01,935 --> 00:12:05,695
because the aiding or removing the chart
is not a frequent operation, right?

192
00:12:05,695 --> 00:12:10,095
So we can very well have a
cache which gets invalidated

193
00:12:10,095 --> 00:12:11,775
whenever there is an a new.

194
00:12:12,775 --> 00:12:14,005
Through cache in here.

195
00:12:14,035 --> 00:12:15,845
And then this will help us.

196
00:12:15,845 --> 00:12:19,965
So this will help us like doing the
routing it without adding up on the

197
00:12:19,965 --> 00:12:24,205
much latency because we'll be doing it
from the cash the deal right system.

198
00:12:24,205 --> 00:12:27,655
So the deal right system is very
important when we want to do the

199
00:12:28,795 --> 00:12:31,705
when we want to move the customer
from the one shot to the another.

200
00:12:31,765 --> 00:12:32,390
I think if.

201
00:12:33,390 --> 00:12:39,550
Make sure that the customer do not see any
downtime during the during this process.

202
00:12:39,550 --> 00:12:43,370
And then the customer even would
know if there was any change

203
00:12:43,370 --> 00:12:45,020
on the back in the backend.

204
00:12:45,120 --> 00:12:45,330
Yeah.

205
00:12:47,690 --> 00:12:54,600
Yeah, so I just want to conclude
on this, the idea is to have the

206
00:12:54,600 --> 00:12:56,100
multiple charts of the service.

207
00:12:57,090 --> 00:13:06,030
And so that each short serve some of the
data or some of the customer, it helps you

208
00:13:06,300 --> 00:13:12,870
increase the availability of the service,
also doing the better load management

209
00:13:13,710 --> 00:13:16,560
and improve on the fault tolerance.

210
00:13:17,040 --> 00:13:17,520
Yes.

211
00:13:17,520 --> 00:13:19,950
So I think, yeah, that's it.

212
00:13:20,160 --> 00:13:22,060
And, thank you very much.

