1
00:00:00,400 --> 00:00:01,160
Hi, everyone.

2
00:00:01,270 --> 00:00:02,420
Welcome to my session.

3
00:00:02,460 --> 00:00:07,860
I am super excited to be the speaker
at Con42 Prom Engineering Conference.

4
00:00:08,220 --> 00:00:12,870
In this talk, you will learn what are the
generative feedback loops and how to use

5
00:00:12,870 --> 00:00:18,080
them to create personalized recommendation
solutions and generate targeted

6
00:00:18,119 --> 00:00:21,220
ads based on real time information.

7
00:00:21,700 --> 00:00:26,840
This session can be specifically useful
for Python developers, data engineers.

8
00:00:27,200 --> 00:00:32,970
or, super base or we relate users looking
for solutions for detecting changes on

9
00:00:33,010 --> 00:00:38,659
primary database like, a super base or
PostgreSQL and streaming these changes and

10
00:00:38,659 --> 00:00:41,059
continuously updating vector databases.

11
00:00:41,499 --> 00:00:47,669
For ai powered applications, let me babur.

12
00:00:47,739 --> 00:00:50,589
I'm developer advocate at glass flow.

13
00:00:50,589 --> 00:00:56,949
I'm also microsoft's mvp For azure ai
services if you have any questions,

14
00:00:57,309 --> 00:01:01,279
please scan the qr code and connect
me on linkedin I will be more than

15
00:01:01,289 --> 00:01:06,549
happy to address all of your questions
Here's what we will be covering today

16
00:01:06,799 --> 00:01:09,019
We'll start with an introduction.

17
00:01:09,059 --> 00:01:14,209
What is a generative feedback loop and
followed by the real world use cases

18
00:01:14,579 --> 00:01:16,849
and real time GFL pipeline architecture.

19
00:01:16,849 --> 00:01:21,859
I'm going to explain and show you, the
components, how to detect changes on

20
00:01:21,859 --> 00:01:27,259
your primary database and streaming these
changes, continuously updating all your,

21
00:01:27,269 --> 00:01:31,979
as a destination vector database, when
you are building AI powered applications.

22
00:01:32,279 --> 00:01:36,359
And you will see also demo of building
a typical pipeline with, technologies

23
00:01:36,359 --> 00:01:41,580
like Glassflow, Superbase, and Vue 8 for
our simple, ERBNB, listing application.

24
00:01:42,110 --> 00:01:48,700
To optimize and personalize these
property listings for a Airbnb, this

25
00:01:48,700 --> 00:01:53,830
will be a live demo of running the
production ready pipeline in 15 minutes.

26
00:01:54,170 --> 00:01:57,480
By the end of this, my session,
you will be able to process

27
00:01:57,510 --> 00:01:59,880
a simple Airbnb listing data.

28
00:02:00,235 --> 00:02:06,115
which is in the super base and enrich it
with AI and store it vector database like

29
00:02:06,235 --> 00:02:11,015
we wait, and you can also search through
all these, and Richard listings in the way

30
00:02:11,015 --> 00:02:13,695
we're using, we weights, graph console.

31
00:02:14,245 --> 00:02:18,545
I'm incredibly happy to have the,
such a knowledgeable, audience

32
00:02:18,585 --> 00:02:21,685
in this session and this webinar.

33
00:02:21,695 --> 00:02:25,595
So let's me start with, by
understanding was a feedback loop.

34
00:02:26,410 --> 00:02:30,400
feedback loop actually relates
to the use of, current, outputs,

35
00:02:30,440 --> 00:02:32,730
to optimize the future, results.

36
00:02:32,910 --> 00:02:37,390
It involves, as you can see in
diagram, utilizing outputs, to

37
00:02:37,390 --> 00:02:42,880
generate better, inputs in the next
stage and feedback loops are critical

38
00:02:42,920 --> 00:02:46,270
components also in, training AI models.

39
00:02:46,560 --> 00:02:52,180
Like when you are give, some AI model
data, a model you can train, but in

40
00:02:52,180 --> 00:03:00,290
real time also user input can impact
on the AI model responses in real time.

41
00:03:00,790 --> 00:03:05,120
Generative feedback loops or
shortly GFL, takes also approach

42
00:03:05,120 --> 00:03:07,089
of a generative AI model.

43
00:03:07,260 --> 00:03:12,220
but it takes a step further by
introducing continuous improvement cycle.

44
00:03:12,280 --> 00:03:18,870
As you can see in the diagram in GFL,
the output generated by the AI, when you

45
00:03:18,870 --> 00:03:24,380
do from engineering, is not just a final
product is a part of ongoing process.

46
00:03:24,700 --> 00:03:28,320
Usually GFL, takes the
results generated from the.

47
00:03:28,820 --> 00:03:34,850
Gen AI or from your language models
like GPT vectorizes them and saves

48
00:03:35,410 --> 00:03:40,030
the result back to the vector database
and you can use this generated

49
00:03:40,040 --> 00:03:44,100
data for future AI processing.

50
00:03:44,455 --> 00:03:49,955
And this output is used to also improve
the future results from the AI models.

51
00:03:49,975 --> 00:03:55,015
In other words, the outputs become
the inputs for the loop cycle.

52
00:03:55,335 --> 00:04:00,365
And AI outputs are analyzed to
optimize training data for the

53
00:04:00,385 --> 00:04:03,295
AI or algorithms, parameters.

54
00:04:03,495 --> 00:04:03,965
The goal.

55
00:04:04,110 --> 00:04:08,240
It's to improve the quality of
the future content generation.

56
00:04:08,520 --> 00:04:13,040
Let's think about AI can generate
a code snippets based on the users

57
00:04:13,050 --> 00:04:19,050
requirements and create a feedback loop
with tests to improve the code over time.

58
00:04:19,550 --> 00:04:22,059
some people always confuses, GFL with GNI.

59
00:04:22,059 --> 00:04:28,639
GNI, we know that it primarily
operates, in a one way direction.

60
00:04:29,239 --> 00:04:33,499
It generates a content based on
the input data you provide for AI.

61
00:04:33,949 --> 00:04:39,729
however, GFL on the other side,
incorporates the feedback loop where AI

62
00:04:39,759 --> 00:04:44,539
solution continues to learn and improves
from the outputs as it's generated.

63
00:04:45,049 --> 00:04:49,669
And GNI, we know that it's typically
used for creating new content or new

64
00:04:49,699 --> 00:04:55,809
data, summarizing existing data, but
GFL is focused on improving AI outputs

65
00:04:56,219 --> 00:05:01,519
So over and over time, so the cycle of
feedback loop learning and GFL solutions

66
00:05:01,589 --> 00:05:07,249
are more complex than the standard GNI
models because they require mechanism

67
00:05:07,609 --> 00:05:13,339
for collecting feedback from the users
or from the human, analyzing it and

68
00:05:13,339 --> 00:05:16,499
adjusting the AI models accordingly.

69
00:05:16,999 --> 00:05:19,059
Let me bring a couple of examples.

70
00:05:19,119 --> 00:05:24,889
A good example of GFL in action can
be the personalized recommendations.

71
00:05:25,164 --> 00:05:28,594
There are AI might
suggest products to user.

72
00:05:28,949 --> 00:05:34,459
Based on the browsing history
or clicks or purchases on an

73
00:05:34,469 --> 00:05:37,129
online store, like a website.

74
00:05:37,449 --> 00:05:41,729
The users further can interact with these
recommendations to provide the feedback

75
00:05:42,029 --> 00:05:44,709
to AI to refine the future suggestions.

76
00:05:44,710 --> 00:05:50,989
In the real estate, another example, in
the real estate industry, for example,

77
00:05:51,519 --> 00:05:54,029
property listings are often updated.

78
00:05:54,364 --> 00:05:58,644
if you want to buy or rent the apartment
with the new information, such as the

79
00:05:58,644 --> 00:06:04,244
price always changes, availability
also changes, and additional features

80
00:06:04,264 --> 00:06:07,884
like, you can add sofa, you can
remove the refrigerator and so on.

81
00:06:08,194 --> 00:06:12,544
And GFL pipeline can automatically
update these descriptions of, let's say,

82
00:06:12,544 --> 00:06:15,994
real estate, listings or information.

83
00:06:16,439 --> 00:06:20,539
By making do, making sure that
they always, car currently

84
00:06:20,539 --> 00:06:22,069
optimized for social engines.

85
00:06:22,069 --> 00:06:27,829
If you open a Google and then search
for, specific apartment, if the apartment

86
00:06:27,949 --> 00:06:34,599
data is changes in real time, you can see
also, this, also reflected on your search.

87
00:06:35,019 --> 00:06:39,219
Other examples, you can apply the GFL
and these two other industries like

88
00:06:39,219 --> 00:06:42,189
a real time job listing optimization.

89
00:06:42,559 --> 00:06:46,719
For example, the platforms
like LinkedIn can use this TFL.

90
00:06:47,209 --> 00:06:51,400
if the AI detects, let's say, someone
searching for remote work or flexible

91
00:06:51,400 --> 00:06:55,940
hours, it can update the relevant
job listings to emphasize these,

92
00:06:56,060 --> 00:07:01,020
expectations or aspects, thereby
like increasing, the visibility of

93
00:07:01,160 --> 00:07:03,150
your job posting to the candidate.

94
00:07:03,535 --> 00:07:08,495
to make it more attractive, for them,
or you can use the same solution GFL,

95
00:07:08,505 --> 00:07:10,835
like to customize travel itineraries.

96
00:07:10,835 --> 00:07:14,125
This is one, this example, I really,
I like, let's say if you use your

97
00:07:14,125 --> 00:07:20,855
frequently search for some new
trips online or cultural experience

98
00:07:20,875 --> 00:07:22,105
in different countries and AI.

99
00:07:22,605 --> 00:07:27,905
Can, create, generate some itineraries to
include, some activities that perfectly

100
00:07:27,965 --> 00:07:33,625
aligned with the preference of the user
and other exams, like a TV shows, you

101
00:07:33,625 --> 00:07:37,945
can like a Netflix can use to create
a personalized viewing experience.

102
00:07:37,945 --> 00:07:39,035
the red Netflix.

103
00:07:39,370 --> 00:07:45,910
You can always see the quite relevant,
movies, based on your, historical,

104
00:07:45,960 --> 00:07:50,550
watch it, like the movies always,
it will find something relevant

105
00:07:50,550 --> 00:07:52,230
to your emotional, experience.

106
00:07:52,540 --> 00:07:58,430
if you want to learn more about the GFL,
I found this article by the author Connor.

107
00:07:58,925 --> 00:08:02,875
He explains very nicely the concept of
Generative Feedback Loops with large

108
00:08:02,875 --> 00:08:08,225
language models like LLMs, how you can
retrieve the information from a vector

109
00:08:08,225 --> 00:08:14,315
database such as WeWeight to prompt
vector databases generative model and then

110
00:08:14,615 --> 00:08:20,655
vectorize and save the results of the AI
generated content back to the database.

111
00:08:21,525 --> 00:08:25,765
let's focus on now GFL
with real time data.

112
00:08:25,765 --> 00:08:25,830
for listening.

113
00:08:26,040 --> 00:08:30,080
as you can see in the diagram, real
time data transformation, and this

114
00:08:30,080 --> 00:08:34,270
transformation are the backbone of
a very effective GFL automation.

115
00:08:34,700 --> 00:08:38,360
it's about detecting, real time data
changes and continuously updating

116
00:08:38,370 --> 00:08:40,770
vector storage on the right side.

117
00:08:41,280 --> 00:08:46,070
In the context of, ERBNB listings, this
means as soon as like new room is listed.

118
00:08:46,840 --> 00:08:51,790
you, the description of, for the
room can be, created by the AI and

119
00:08:51,790 --> 00:08:55,020
vectorizes for a better search.

120
00:08:55,330 --> 00:08:57,919
And then, you can store the data.

121
00:08:57,920 --> 00:09:01,270
It's a vectorized result
in a vector database.

122
00:09:01,510 --> 00:09:07,240
Let's say user searching for a cozy
apartment in Paris using Airbnb, and

123
00:09:07,240 --> 00:09:13,160
he can see all up to date options
for him in the same location.

124
00:09:13,610 --> 00:09:18,295
And the real time transformation also
here, what it does, As you can see,

125
00:09:18,295 --> 00:09:23,515
if you see the yellow square is sort
of pipeline, we are detecting some

126
00:09:23,515 --> 00:09:29,055
changes from APIs, files, databases,
where exactly Airbnb listings, located,

127
00:09:29,195 --> 00:09:35,203
and then say, if new Airbnb listing is
added, we are generating, description

128
00:09:35,513 --> 00:09:38,068
based on the Airbnb listing attributes.

129
00:09:38,538 --> 00:09:43,148
we are calling the open AI or other
models, such as compilations and point.

130
00:09:43,618 --> 00:09:47,858
And then B for the generated
content from the AI, we are

131
00:09:48,058 --> 00:09:49,928
calculating vector embeddings.

132
00:09:50,758 --> 00:09:55,668
And then we are sending a vector embedding
to store somewhere in the vector database.

133
00:09:55,698 --> 00:10:00,318
Why we are storing in the vector
database, because we would like to

134
00:10:00,328 --> 00:10:03,078
build, in the next step, like simple.

135
00:10:03,508 --> 00:10:06,468
And the application, to give the booking.

136
00:10:06,648 --> 00:10:10,858
com experience to the users,
where they can search for.

137
00:10:11,213 --> 00:10:16,513
some, apartments, by, using the human
language and then this human language

138
00:10:16,533 --> 00:10:24,003
can be also converted to the vector
embeddings and we can compare our data

139
00:10:24,023 --> 00:10:26,913
corpus we created in the vector database.

140
00:10:27,303 --> 00:10:32,163
To find the matching the
apartments for the user query.

141
00:10:32,603 --> 00:10:36,243
This is a, everything is happening
in real time, as you can see, in

142
00:10:36,243 --> 00:10:38,203
the previous slide, I showed you.

143
00:10:38,523 --> 00:10:43,373
Data, can change here, maybe
every minute or every five minute.

144
00:10:43,703 --> 00:10:48,153
And this data is captured and in
milliseconds, data will be visible.

145
00:10:48,458 --> 00:10:49,838
In the vector databases.

146
00:10:49,888 --> 00:10:55,048
This is the concept of real
time GFL, how that works.

147
00:10:55,753 --> 00:11:00,733
And now let's focus on, the, some
of the technologies, to build,

148
00:11:00,803 --> 00:11:02,933
how we can build these pipelines.

149
00:11:03,183 --> 00:11:07,113
One of those technologies I'm working
with also, helps you to build real

150
00:11:07,113 --> 00:11:10,463
time data pipelines for, AI use cases.

151
00:11:10,778 --> 00:11:15,538
Like you can build a pipeline
such as a real time GFL using

152
00:11:15,548 --> 00:11:21,008
Glassflow and Glassflow simplifies
also creation process of your real

153
00:11:21,008 --> 00:11:22,538
time data processing pipeline.

154
00:11:22,648 --> 00:11:28,058
As we have seen previously, you may spend
up to 15 minutes to setting up everything.

155
00:11:28,418 --> 00:11:32,918
And your new pipeline is, ready to
run in a production, environment.

156
00:11:33,328 --> 00:11:37,508
and also with Glassware, you can
integrate, with various data sources

157
00:11:37,548 --> 00:11:43,408
such as PostgreSQL, MongoDB, or some
message brokers like a Google PubSub,

158
00:11:44,018 --> 00:11:49,538
message queues like Amazon SQS, and you
can apply transformation in the middle.

159
00:11:49,818 --> 00:11:54,748
Then you can store the results to
the databases or BI analytics tools

160
00:11:54,828 --> 00:11:56,818
or vector storages like Vue Vid.

161
00:11:57,648 --> 00:12:04,608
So this is, how the works, the Glassflow,
why we, decided to build this solution,

162
00:12:04,618 --> 00:12:06,768
especially for Python developers.

163
00:12:07,078 --> 00:12:11,668
we are trying to offer all in one
platform, for, that focuses on the

164
00:12:11,668 --> 00:12:15,450
creation of, easy creation of your
data pipelines for data engineers

165
00:12:15,450 --> 00:12:19,880
and data teams, especially for data
scientists, you don't have to worry

166
00:12:19,880 --> 00:12:22,240
about the infrastructure under the hood.

167
00:12:22,620 --> 00:12:27,071
In other words, it removes the
complexity of the, real time for data

168
00:12:27,071 --> 00:12:32,210
processing pipelines, be it Kafka
plus Flink, you can do everything in

169
00:12:32,210 --> 00:12:34,160
a single serverless infrastructure.

170
00:12:34,720 --> 00:12:38,250
let me explain how that works,
building a pipeline with Glasswell.

171
00:12:38,660 --> 00:12:43,340
let's say you, start with connecting
to your live data sources like

172
00:12:43,360 --> 00:12:47,590
Airbnb listings using built in
integrations, or you can build your

173
00:12:47,590 --> 00:12:49,840
own integration using Python SDK.

174
00:12:50,250 --> 00:12:55,210
Then you start to build your pipeline,
within the local environment like a Gospel

175
00:12:55,210 --> 00:13:01,630
web app, or Using the CLI, if you prefer,
like CLI option, then you implement your

176
00:13:01,900 --> 00:13:07,870
transformation function, which is a very
heart of the transformation in Python.

177
00:13:08,170 --> 00:13:12,760
And you can, after your transformation
function is ready, you can deploy to

178
00:13:13,110 --> 00:13:18,340
serverless execution engine, where
your transformation can scale up to

179
00:13:18,750 --> 00:13:21,400
processing like billions of records.

180
00:13:21,620 --> 00:13:24,980
And you don't have to worry
about the scaling and manually.

181
00:13:25,480 --> 00:13:29,970
And then when the transformation
is ready, you can send the

182
00:13:29,970 --> 00:13:32,335
output event to the transcript.

183
00:13:32,965 --> 00:13:37,715
Internet destinations using the same
integrations or building integrations.

184
00:13:38,155 --> 00:13:41,365
this is, some of the use cases
you can achieve with Glassflow.

185
00:13:41,865 --> 00:13:45,435
You can build your pipeline, for
example, to enrich your data with

186
00:13:45,595 --> 00:13:50,625
predicted future prices, using
AI, and to detect some changes.

187
00:13:51,145 --> 00:13:54,435
In your database and also
sending this data changes by

188
00:13:54,435 --> 00:13:56,465
transforming to their destinations.

189
00:13:56,995 --> 00:14:02,174
You can also build some let's say
realtime clickstream analytics

190
00:14:02,174 --> 00:14:08,215
dashboard to analyze clickstreams
data from your website and sends them

191
00:14:08,215 --> 00:14:11,525
to other downstream applications.

192
00:14:12,025 --> 00:14:15,725
If you want to know more about
Glassflow and Glassflow use cases,

193
00:14:15,725 --> 00:14:19,965
You can scan this QR code, it will
bring you to the GitHub repository.

194
00:14:20,320 --> 00:14:25,450
Where you can try some of our
real world examples and run them

195
00:14:25,450 --> 00:14:28,100
just right from Jupyter notebook.

196
00:14:28,620 --> 00:14:34,330
Now let's come back, switch to our real
time genetic feedback loop automation.

197
00:14:34,830 --> 00:14:39,320
We're going to build as a part of
the session sample pipeline for GFL

198
00:14:39,870 --> 00:14:46,171
to make a feedback loop pipelines,
as you can see, here in the diagram.

199
00:14:47,036 --> 00:14:52,156
We have data source, let's assume
that ERB data always stored to

200
00:14:52,156 --> 00:14:55,396
the SuperBase because while using
SuperBase, SuperBase is open source.

201
00:14:55,426 --> 00:15:00,566
First of all, then, it is another
alternative to Google Firebase, which

202
00:15:00,566 --> 00:15:05,936
works quite nice, especially when
you have a real time data and your

203
00:15:05,936 --> 00:15:07,556
data always changing your database.

204
00:15:07,936 --> 00:15:13,606
Let's say whenever new, ERB listings
added, or you updated existing one.

205
00:15:13,706 --> 00:15:18,886
and Superbase can trigger an event,
to send this change directly to

206
00:15:18,886 --> 00:15:23,876
the Glassflow pipeline, using it is
a webhook, data source connector.

207
00:15:24,306 --> 00:15:29,336
Every change happening, Airbnb listings
can be, actually, send it to the

208
00:15:29,336 --> 00:15:30,956
automatically to the Glassflow pipeline.

209
00:15:30,986 --> 00:15:33,156
And then when you, It reads
the glass of pipeline.

210
00:15:33,646 --> 00:15:38,936
You can do where AI and vectorization
and you can write transformation

211
00:15:38,936 --> 00:15:45,536
function to apply some, AI driven,
solution in a driven insight in Python.

212
00:15:45,946 --> 00:15:47,376
In this case, let's say, AI model.

213
00:15:47,996 --> 00:15:52,276
can call the open AI to enrich
listing and descriptions by

214
00:15:52,276 --> 00:15:55,996
generating, more descriptive,
description for the Airbnb listing.

215
00:15:56,496 --> 00:16:00,416
you can summarize it and then
transform it, all the descriptions.

216
00:16:00,866 --> 00:16:05,386
and then at the same stage will
be vectorize it and converted to

217
00:16:05,386 --> 00:16:09,636
the vectors, format, and it'll be
sent to the vector database like

218
00:16:09,636 --> 00:16:11,466
a , as you can see in diagram.

219
00:16:12,106 --> 00:16:16,966
I will show you, why this approach
and why we are calculating

220
00:16:16,966 --> 00:16:18,946
your vector, embeddings.

221
00:16:18,976 --> 00:16:21,616
And again, like we are
storing a vector database.

222
00:16:22,191 --> 00:16:25,931
you will see in the next slides, I will
give you some of the example queries.

223
00:16:26,191 --> 00:16:30,481
It will give the, more, cleared,
querying option for users.

224
00:16:30,671 --> 00:16:35,261
They don't have to use SQL, if data runs
in the Vector database, they can use

225
00:16:35,261 --> 00:16:38,191
query data using their human language.

226
00:16:38,861 --> 00:16:41,601
let me, explain what
is our sample data set.

227
00:16:41,841 --> 00:16:49,921
To train our AI or the build a pipeline
and we can use like a simple CSV data set.

228
00:16:50,526 --> 00:16:54,616
And we're like, is there some room
listings that say New York city

229
00:16:54,636 --> 00:16:57,836
from Airbnb last year in 2023.

230
00:16:57,836 --> 00:17:03,636
And this dataset includes every, Airbnb
listing attributes, such as a listing

231
00:17:03,636 --> 00:17:10,176
name, host name, location, details, room
type, price, availability, and so on.

232
00:17:10,726 --> 00:17:12,346
once a dataset we have.

233
00:17:12,511 --> 00:17:17,451
Here we can, what we can do by
running this pipeline and saving to

234
00:17:17,461 --> 00:17:22,041
the data to the way with, we can run
typical queries, like we can fetch,

235
00:17:22,211 --> 00:17:27,461
let's say, top five most reviewed
listings in New York or in Brooklyn.

236
00:17:27,951 --> 00:17:32,811
this could be quite useful to identify
popular listings in this area.

237
00:17:32,831 --> 00:17:37,781
Or you can find the listings, in Brooklyn
that, a budget friendly, let's say,

238
00:17:38,171 --> 00:17:43,121
less than 100 US dollars and have a
positive reviews, something like that.

239
00:17:43,601 --> 00:17:48,231
Or you can also search for the queries,
may user might be looking for listings

240
00:17:48,261 --> 00:17:53,721
that are described, like I want have
a, the apartment with a great view.

241
00:17:54,481 --> 00:17:59,361
This query would return the most relevant
results based on the user queries.

242
00:17:59,841 --> 00:18:05,111
And this conversion, from the human
language to the, vector query operation

243
00:18:05,121 --> 00:18:09,851
will be handled automatically by
the vector database, like Vue Wait.

244
00:18:10,431 --> 00:18:15,081
Once you understand the pipeline
and, GFL, let's build this pipeline.

245
00:18:15,686 --> 00:18:16,436
step by step.

246
00:18:17,006 --> 00:18:22,726
Here, how we build our pipeline, like
we start with setting up our vector

247
00:18:22,726 --> 00:18:24,556
storage, which is our final destination.

248
00:18:24,876 --> 00:18:28,636
we create a pipeline with
Glassflow and we set up the super

249
00:18:28,636 --> 00:18:31,606
base with a simple, Airbnb data.

250
00:18:32,116 --> 00:18:34,206
Then, we run the pipeline.

251
00:18:34,266 --> 00:18:40,201
Once data already In Vuelo GraphQL, I
will show you how you can query, Vuelo

252
00:18:40,201 --> 00:18:43,281
weight using it is GraphQL console.

253
00:18:44,021 --> 00:18:48,141
Let me bring your
attention to the demo here.

254
00:18:48,141 --> 00:18:50,281
As you can see the demo, in place.

255
00:18:50,321 --> 00:18:55,281
I will start by, analyzing,
creating first, setting up the Vuelo

256
00:18:55,281 --> 00:19:00,451
weight cloud, you can create, the
first cluster, on Vuelo weight.

257
00:19:00,961 --> 00:19:05,351
Then, once the cluster is up and
running, you can, start to create a

258
00:19:05,351 --> 00:19:08,541
new collection, inside your cluster.

259
00:19:09,196 --> 00:19:15,016
It is called, let's call it Airbnb,
NYC, which is we did in New York.

260
00:19:15,346 --> 00:19:21,066
Then, you choose a vectorized type, like
in my case, I'm using Text two Vector

261
00:19:21,366 --> 00:19:26,131
from the open ai, and you choose a model
like text embedding three small or large.

262
00:19:26,366 --> 00:19:29,516
for me, it's a three small is
enough, and you can keep the rest

263
00:19:29,516 --> 00:19:32,066
of the configuration by default.

264
00:19:32,566 --> 00:19:37,416
So as a next step, and now once I have
the, Vuex site is ready, I'm going to

265
00:19:37,416 --> 00:19:39,076
create the pipeline with Glassflow.

266
00:19:39,516 --> 00:19:41,746
You can sign up for it for free.

267
00:19:41,936 --> 00:19:43,026
You can get a free account.

268
00:19:43,406 --> 00:19:45,676
And when you can create
your first pipeline easily,

269
00:19:45,736 --> 00:19:47,006
let's create a new pipeline.

270
00:19:47,481 --> 00:19:50,701
And then, choose a data
source as a webhook.

271
00:19:50,791 --> 00:19:57,171
Because we are getting data from
Superbase, as the events, sends

272
00:19:57,171 --> 00:19:58,351
events through the webhook.

273
00:19:58,741 --> 00:20:02,711
The third step is defining
your transformation function.

274
00:20:03,121 --> 00:20:05,501
You can define by writing the Python code.

275
00:20:05,621 --> 00:20:09,981
As you can see, I have created already
one simple transformation function.

276
00:20:10,416 --> 00:20:16,606
it, simply, receives, from SuperBase
Airbnb listing data, and, using

277
00:20:16,606 --> 00:20:22,666
the OpenAI, it generates, the
description from the Airbnb attributes.

278
00:20:22,666 --> 00:20:28,406
Then, after AI response, with a generated,
Airbnb listing, description, we're gonna

279
00:20:28,446 --> 00:20:29,776
create, create the vector embeddings.

280
00:20:30,521 --> 00:20:35,141
As you can see, it has a handler
function, which is important

281
00:20:35,141 --> 00:20:37,421
function for, GlassFlow.

282
00:20:37,801 --> 00:20:42,431
It automatically detects, whatever inside
the Lua logic under the handler function.

283
00:20:42,961 --> 00:20:47,051
And then, our transformation
function is more or less ready.

284
00:20:47,161 --> 00:20:52,327
So next step, yes, also we don't
forget to include your, dependencies,

285
00:20:52,327 --> 00:20:56,774
like OpenAI dependency for my
transformation function or Py file.

286
00:20:57,624 --> 00:21:01,764
And next day, we, I'm going to
define data sync operation, where my

287
00:21:01,984 --> 00:21:03,734
transformer data will be send it again.

288
00:21:03,734 --> 00:21:07,674
I'm going to choose a webhook
because out the data I will send

289
00:21:07,674 --> 00:21:13,374
to, we wait, collection that we,
created with you in the before.

290
00:21:13,834 --> 00:21:20,914
So I'm gonna, find first, we
wait cloud, API key, in the URL.

291
00:21:21,549 --> 00:21:27,899
I need a URL for the, my cluster and
just copy past the URL for the cluster.

292
00:21:27,929 --> 00:21:31,159
And then I will bring
the admin key for that.

293
00:21:31,709 --> 00:21:35,659
let me find the admin key
from Vue with console.

294
00:21:35,789 --> 00:21:40,599
and I can give me the define also
content type and like application JSON.

295
00:21:40,639 --> 00:21:41,259
Then.

296
00:21:41,659 --> 00:21:45,929
it's a mandatory to define also,
the API key to make sure that we are

297
00:21:45,929 --> 00:21:48,499
securely connected to V Wave cluster.

298
00:21:48,889 --> 00:21:51,749
Then, we send our data.

299
00:21:52,349 --> 00:21:54,089
from the Glasslow Pipeline.

300
00:21:54,569 --> 00:21:55,069
here we go.

301
00:21:55,129 --> 00:21:58,419
I have the Bureau token
and authentication is done.

302
00:21:58,499 --> 00:22:00,739
I will click on the next step.

303
00:22:01,259 --> 00:22:04,069
And you can see the
overview of your Pipeline.

304
00:22:04,079 --> 00:22:08,589
And when you press to click
Pipeline, your Pipeline is ready

305
00:22:08,589 --> 00:22:10,259
to run in a serverless environment.

306
00:22:10,939 --> 00:22:12,969
The last step is setting up a Superbase.

307
00:22:13,139 --> 00:22:14,819
Make sure that you have
a Superbase account.

308
00:22:15,309 --> 00:22:19,909
Assume that I have, my already
created a simple database, called

309
00:22:20,059 --> 00:22:23,489
it Airbnb, NYC, in super base.

310
00:22:23,489 --> 00:22:26,499
You can see, from I had
some different data sets.

311
00:22:26,499 --> 00:22:33,309
This one is from, 2019 and has some
attributes, to map the Airbnb listings,

312
00:22:33,309 --> 00:22:35,709
like host name, location, room type price.

313
00:22:36,339 --> 00:22:39,979
then, once I have, this data in place.

314
00:22:40,314 --> 00:22:45,224
I can go and navigate to, the
table editor to see, sample data.

315
00:22:46,124 --> 00:22:50,634
As you can see, I have now, existing
sample data, five, listings in place

316
00:22:50,824 --> 00:22:54,244
to give your understanding, like
this is how the data is looks like.

317
00:22:54,774 --> 00:22:58,174
and I'm, I will create a webhook
trigger on Superbase because

318
00:22:58,194 --> 00:23:02,624
Superbase, triggers using the webhook
or the Glassdoor pipeline, right?

319
00:23:03,154 --> 00:23:04,104
let's create our webhook.

320
00:23:04,104 --> 00:23:04,559
Okay.

321
00:23:04,919 --> 00:23:09,929
Like you can call it, you can give it
any name, in my case, maybe ERBMB, a

322
00:23:09,939 --> 00:23:17,059
listing, or ERBMB data change capture,
because it changes, detects, the changes

323
00:23:17,059 --> 00:23:18,689
and sends to the Glasgow pipeline.

324
00:23:19,369 --> 00:23:22,244
And you're reading in the next
step, create, choose a data type.

325
00:23:23,174 --> 00:23:27,254
database, table, which database table
we are, choosing and events like

326
00:23:27,254 --> 00:23:31,394
in sort, we have frequent entries,
should trigger in sort or so on.

327
00:23:31,904 --> 00:23:36,534
this, and also next step is we need
to also define, Glassflow Pipeline

328
00:23:36,534 --> 00:23:42,184
Access Token in the webhook URL,
because when the SIPL base calls the

329
00:23:42,224 --> 00:23:48,164
Glassflow, we need to securely make
this connection using the access token.

330
00:23:48,579 --> 00:23:52,684
I put it in the header of the,
view, triple base web hook,

331
00:23:52,764 --> 00:23:54,174
access token for the glass flow.

332
00:23:54,384 --> 00:23:59,494
Now, as you can see, is our, the
pipeline, for the web hook is ready.

333
00:23:59,944 --> 00:24:03,214
And make sure you need to enable
real time for your database because

334
00:24:03,214 --> 00:24:05,944
it should get real time updates.

335
00:24:06,364 --> 00:24:07,894
Now everything is set up.

336
00:24:08,254 --> 00:24:10,474
Next step I'm gonna.

337
00:24:10,529 --> 00:24:14,949
send, some more sample data to
SuperBase because you remember we

338
00:24:14,959 --> 00:24:17,699
had the only, like five listings.

339
00:24:17,729 --> 00:24:22,579
Let's add maybe 10 or
20 more for, my listing.

340
00:24:22,579 --> 00:24:27,628
As you can see, I have a bunch of them
in the sample data set, and I have also

341
00:24:27,628 --> 00:24:29,357
one Python script called, to populate
the, SuperBase with sample data.

342
00:24:29,357 --> 00:24:31,558
And, I have one Python script called, to
populate the, SuperBase with sample data.

343
00:24:31,558 --> 00:24:33,288
And, I have also one Python
script called, to populate the,

344
00:24:33,288 --> 00:24:33,916
SuperBase with sample data.

345
00:24:34,267 --> 00:24:38,227
it just inserts, rows of
data, in bunch, in batch mode.

346
00:24:38,787 --> 00:24:44,747
We are just, three, simulating, some
incoming or registering your listings.

347
00:24:44,777 --> 00:24:50,357
but in reality, this, service, any service
can call these, to, insert some more data.

348
00:24:50,857 --> 00:24:55,177
And let's run this Python script
and generate some input data.

349
00:24:55,812 --> 00:24:59,632
I'm going to create 20 more rows
for the super base listings.

350
00:25:00,022 --> 00:25:00,422
There we go.

351
00:25:00,462 --> 00:25:01,742
I added 20 more.

352
00:25:01,742 --> 00:25:03,042
Everything's successful.

353
00:25:03,752 --> 00:25:10,982
and now I can switch back to
the super base and check if,

354
00:25:11,062 --> 00:25:12,432
this data already in place.

355
00:25:12,432 --> 00:25:12,822
Yes.

356
00:25:12,862 --> 00:25:17,262
As you can see, the data
is in place and now.

357
00:25:17,602 --> 00:25:22,722
we can go to Glassflow and check, if the
data received by the Glassflow after we

358
00:25:22,722 --> 00:25:24,612
did the insort on SuperBase side, right?

359
00:25:24,612 --> 00:25:26,072
at the real time what's happening.

360
00:25:26,082 --> 00:25:26,542
Yes.

361
00:25:26,642 --> 00:25:27,052
Here we go.

362
00:25:27,532 --> 00:25:30,782
As we insorted, every insort,
detected by SuperBase and sent

363
00:25:30,782 --> 00:25:33,982
to the Glassflow pipeline, which
is where our magic is happening.

364
00:25:34,412 --> 00:25:39,872
And Glassflow is already, in
milliseconds, and sent to the SuperBase.

365
00:25:40,187 --> 00:25:44,437
after this transformation and
I can now, ready to query, and

366
00:25:44,437 --> 00:25:45,967
search for Airbnb listings.

367
00:25:46,477 --> 00:25:48,907
And, the view with the GraphQL console.

368
00:25:49,357 --> 00:25:53,257
I did one query, like to find, five
most reviewed listings in Brooklyn.

369
00:25:53,287 --> 00:25:53,667
Yes.

370
00:25:53,677 --> 00:25:54,097
Here we go.

371
00:25:54,097 --> 00:25:55,497
Here are some listings.

372
00:25:55,697 --> 00:26:00,047
And now let's try, some, query
using, the human language.

373
00:26:00,467 --> 00:26:04,887
For that, you need also, the OpenAI key.

374
00:26:05,437 --> 00:26:12,507
Because, we need to, it's going to use AI
to, use like human interactive searching.

375
00:26:13,027 --> 00:26:15,547
I'm going to pass my open AI API key.

376
00:26:16,077 --> 00:26:21,127
let's say when user, might be asking
like luxury apartment with a nice view,

377
00:26:21,547 --> 00:26:26,747
and as you can see, it found, based
on the human language, relevant, data.

378
00:26:26,847 --> 00:26:32,897
in our, database and the summary is
actually generated, from the summary

379
00:26:32,897 --> 00:26:37,907
generated from the AI generated this
summary, but we had in the beginning,

380
00:26:38,277 --> 00:26:44,147
only the data, about the Airbnb
listings and this generated data

381
00:26:44,147 --> 00:26:50,497
summary can be always, change it and
reach it, in based on the property,

382
00:26:50,547 --> 00:26:52,107
something that's on the property side.

383
00:26:52,137 --> 00:26:56,287
if I change now and find any
property and change the price.

384
00:26:56,687 --> 00:27:00,597
For the property, it will be reflected
immediately on the view weight.

385
00:27:01,107 --> 00:27:08,277
This is how the, real time, continuous
vector embeddings generation.

386
00:27:08,842 --> 00:27:11,732
And, with the database, dating works.

387
00:27:12,302 --> 00:27:16,822
So that was my, demo in summary.

388
00:27:16,852 --> 00:27:20,902
we have, so far presented, concept
of the generative feedback loops.

389
00:27:21,462 --> 00:27:26,637
And this describes, let's say, not only
using the results from the database to

390
00:27:26,637 --> 00:27:32,722
answer, the user's queries, but, we can
also save the result and, once, we create,

391
00:27:32,722 --> 00:27:36,552
save the results, back to the vector
database again for future references.

392
00:27:37,142 --> 00:27:43,802
So this is called, we call it GFL and
real time GFLs, uses real time data, to

393
00:27:43,812 --> 00:27:51,572
receive, like user input and change the AI
output based on the user's interactivity.

394
00:27:51,932 --> 00:27:57,602
So you can always get most relevant and
updated content using a real time GFLs.

395
00:27:58,417 --> 00:28:01,897
So I hope you find the
session interesting.

396
00:28:01,897 --> 00:28:03,727
If you have any questions.

397
00:28:03,777 --> 00:28:07,527
and now we can jump on a current session
or give you questions in a comment,

398
00:28:07,957 --> 00:28:13,047
and, scan score code to find the,
the use case I showed you with GFL.

399
00:28:13,047 --> 00:28:17,047
Also other use cases, do you
want to try out, you can try

400
00:28:17,047 --> 00:28:18,667
out some of other use cases.

401
00:28:18,927 --> 00:28:21,177
Thanks for your attention.

402
00:28:21,537 --> 00:28:23,217
and have a nice day.

