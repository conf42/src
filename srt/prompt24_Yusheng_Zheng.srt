1
00:00:00,559 --> 00:00:01,640
Hi, everyone.

2
00:00:01,650 --> 00:00:03,290
My name is Yushan Zheng.

3
00:00:03,449 --> 00:00:06,079
I'm the founder of the Yunomiya BPF.

4
00:00:06,260 --> 00:00:12,049
Today, I'm excited to share our project,
CodeSurvey, which uses language models

5
00:00:12,049 --> 00:00:14,660
to help us understand large codebases.

6
00:00:15,160 --> 00:00:21,229
First, I will start by talking about what
actually means to understand codebases.

7
00:00:21,270 --> 00:00:25,240
We know large language models can help
us understand code to some extent,

8
00:00:25,380 --> 00:00:30,149
but they often struggle when it comes
to answering questions about really

9
00:00:30,150 --> 00:00:33,039
big code reports like Linus Kernel.

10
00:00:33,665 --> 00:00:38,245
Today, I will go over another approach
to explore git repos or big codebases,

11
00:00:38,305 --> 00:00:45,154
and I will show an example of how
we try to gain insights from kernel.

12
00:00:45,714 --> 00:00:48,354
And I will also discuss
some best practices we've

13
00:00:48,405 --> 00:00:50,614
learned and our next steps.

14
00:00:51,114 --> 00:00:56,304
First, we all know that LLMs can help
us find bugs and write code, so they

15
00:00:56,314 --> 00:00:58,594
do understand code to a certain degree.

16
00:00:59,049 --> 00:01:02,999
But understand code isn't just about
fixing bugs or writing functions.

17
00:01:03,609 --> 00:01:06,609
Sometimes we are trying to
answer more high level questions.

18
00:01:07,179 --> 00:01:09,749
about the code and how it evolves.

19
00:01:10,059 --> 00:01:14,909
For example, you might want to know
how new features impact stability

20
00:01:14,949 --> 00:01:20,629
or performance, or whether there are
stages in a component's life cycle.

21
00:01:21,319 --> 00:01:25,889
We may ask how certain features have
changed over time, which parts of the

22
00:01:25,889 --> 00:01:31,154
code are most likely to have bugs, or
how different features and Components

23
00:01:31,154 --> 00:01:37,340
depending on each other, this high level
questions can help us with things like

24
00:01:37,340 --> 00:01:42,984
software design, bug studies, find root
causes of issues, improve code review

25
00:01:43,004 --> 00:01:45,704
process, and make software more stable.

26
00:01:45,894 --> 00:01:50,864
They can also help us design new
debugging tools or CI tools or

27
00:01:50,874 --> 00:01:55,744
make it easier for newcomers to
understand open source projects.

28
00:01:56,539 --> 00:01:59,509
So why are we using live
language models for this?

29
00:01:59,829 --> 00:02:01,879
traditional methods are limited.

30
00:02:02,609 --> 00:02:07,099
Data analysis and manual code review
are very time consuming, and even then,

31
00:02:07,679 --> 00:02:11,204
they often don't tell the full story.

32
00:02:11,704 --> 00:02:15,384
There are so many insights hidden
in comments, emails, and other

33
00:02:15,384 --> 00:02:16,664
kinds of unstructured data.

34
00:02:16,804 --> 00:02:21,864
This data produced during the software
development could be really valuable,

35
00:02:21,894 --> 00:02:27,014
but if we only rely on traditional
methods, they are often limited by things

36
00:02:27,024 --> 00:02:29,614
like Scala, bias, and subjectivity.

37
00:02:30,114 --> 00:02:35,284
If you just try to apply large language
models into these use cases, they

38
00:02:35,284 --> 00:02:39,134
also face their own challenges when it
comes to understanding large codebases.

39
00:02:40,004 --> 00:02:44,734
For example, within context learning,
we can put snippets of code into the

40
00:02:44,734 --> 00:02:49,394
model's context, but this has strict
limits due to the context length.

41
00:02:49,394 --> 00:02:53,354
We can't load an entire
data report into it.

42
00:02:53,854 --> 00:02:58,024
There is another approach called
REC, where we combine library models

43
00:02:58,024 --> 00:03:01,814
with internal source of knowledge,
but it's mostly rely on accurate

44
00:03:01,824 --> 00:03:06,084
retrieval information, so it's
really only good on small, precise

45
00:03:06,084 --> 00:03:08,394
questions about specific code snippets.

46
00:03:08,894 --> 00:03:12,664
Time toning is a narrow option when
you adapt models with specific data,

47
00:03:12,704 --> 00:03:17,094
but it's costly, time consuming,
and there's a risk it might get too

48
00:03:17,094 --> 00:03:19,044
narrow and miss the big picture.

49
00:03:19,904 --> 00:03:28,524
these approaches are generally not
ideal for answering high level design

50
00:03:28,524 --> 00:03:33,374
questions like spam notification powers
or a cost overload being reported.

51
00:03:34,019 --> 00:03:39,789
Even more, it may have a long history
during the report development.

52
00:03:40,289 --> 00:03:43,219
That's why we are proposing a
new approach we call CodeSurvey.

53
00:03:43,219 --> 00:03:49,129
CodeSurvey is an LRM driven
methodology or tool for analyzing

54
00:03:49,159 --> 00:03:51,659
codebases or JIT reports.

55
00:03:52,039 --> 00:03:57,969
The basic idea to think about how
we measure is that what if we could

56
00:03:58,149 --> 00:04:02,399
ask every developer to take a survey
about everything they contribute

57
00:04:02,899 --> 00:04:04,739
during the development process?

58
00:04:05,659 --> 00:04:10,389
So the idea is we can treat
random models as if they were

59
00:04:10,419 --> 00:04:12,379
human participants in a survey.

60
00:04:12,879 --> 00:04:17,029
Because in a way, software
development is a social activity.

61
00:04:17,509 --> 00:04:21,149
Using this approach, we can take
everything involved in software

62
00:04:21,189 --> 00:04:26,419
development, commits, emails, discussions,
and turn it into a structured dataset.

63
00:04:26,894 --> 00:04:31,444
Then we can run queries, perform
quantitative analysis on development

64
00:04:31,544 --> 00:04:33,954
process, and visualize the results.

65
00:04:34,454 --> 00:04:36,264
Here is how code survey works.

66
00:04:36,334 --> 00:04:41,134
First, we have SBAT and LRM
agents working together to come

67
00:04:41,134 --> 00:04:42,494
up with high level questions.

68
00:04:42,834 --> 00:04:45,434
These questions can help us
figure out what we need to know.

69
00:04:46,004 --> 00:04:51,064
Then, we identify the required data types
and sources to answer these questions.

70
00:04:51,104 --> 00:04:51,834
After left.

71
00:04:51,984 --> 00:04:57,974
We design survey questions
based on the data we collected.

72
00:04:58,264 --> 00:05:02,764
The LLMs then use these questions to
process the data and answer a survey.

73
00:05:03,309 --> 00:05:10,149
After this, we either have human aspects
or more LLM agents and evaluate survey

74
00:05:10,149 --> 00:05:11,919
results to make sure they are accurate.

75
00:05:11,929 --> 00:05:16,619
If needed, we refine the questions or
the survey process based on the feedback.

76
00:05:17,279 --> 00:05:21,909
Finally, we end up with a dataset list
tagged and organized, allowing us to

77
00:05:22,399 --> 00:05:25,079
analyze the data in a structured way.

78
00:05:25,579 --> 00:05:31,749
For example, we create a survey
definition about classified human aspects.

79
00:05:32,449 --> 00:05:36,169
with the main type of this commit
with several choices like bug

80
00:05:36,209 --> 00:05:39,139
fix or new features and so on.

81
00:05:39,469 --> 00:05:43,059
We also provide some context for
a survey so the language model

82
00:05:43,099 --> 00:05:46,639
understands the background, it's
like a description and hint.

83
00:05:47,209 --> 00:05:50,879
These survey definitions are then
turned into forms, like this one.

84
00:05:51,379 --> 00:05:55,639
which include the commit details,
description, and the question we ask.

85
00:05:56,189 --> 00:06:00,329
With this problem, we can process
thousands of commits to generate

86
00:06:00,379 --> 00:06:05,069
a meaningful dataset, or even
hundreds of thousands of commits.

87
00:06:05,569 --> 00:06:09,770
Now, let's look at our case study on
the eBPF subsystem in the Linux kernel.

88
00:06:09,770 --> 00:06:13,432
eBPF is a subsystem that allows
you to run programs in the kernel.

89
00:06:13,432 --> 00:06:15,966
It's like having a virtual
machine in the kernel.

90
00:06:15,966 --> 00:06:20,754
So let's take a look at our case study on
the eBPF subsystem in the Linux kernel.

91
00:06:21,064 --> 00:06:22,784
So why do we pick eBPF?

92
00:06:22,974 --> 00:06:27,144
First, because we know a lot about
it, which means we can ask current

93
00:06:27,154 --> 00:06:32,314
aspects to confirm our finding,
because our company or organization

94
00:06:32,364 --> 00:06:34,194
are mainly doing eBPF ourselves.

95
00:06:34,694 --> 00:06:39,893
Second, eBPF is also a readily
involved subsystem with a lot

96
00:06:39,893 --> 00:06:42,112
of complexity and functionality.

97
00:06:42,112 --> 00:06:49,234
We applied a course survey to analyze
the evolution of eBPF to gain insights

98
00:06:49,244 --> 00:06:50,801
that viewers are interested in.

99
00:06:50,801 --> 00:06:52,224
Here are some of

100
00:06:52,724 --> 00:06:53,894
the initial results.

101
00:06:53,909 --> 00:06:58,075
we observe, for example, we
observe feature changes over time

102
00:06:58,075 --> 00:07:00,075
since the API is event driven.

103
00:07:00,515 --> 00:07:05,075
It can attach to multiple events,
like network events, tracing events,

104
00:07:05,105 --> 00:07:07,325
profiling events, or security hooks.

105
00:07:08,090 --> 00:07:12,020
We could see how these events have
changed over time, which reflects

106
00:07:12,020 --> 00:07:14,260
the evolving use cases for eBPF.

107
00:07:15,050 --> 00:07:20,180
Initially, eBPF was designed for network
related features, but now it has expanded

108
00:07:20,210 --> 00:07:25,960
to include more features like strategy
ops or security management control.

109
00:07:25,960 --> 00:07:29,250
You can see that in the features.

110
00:07:29,750 --> 00:07:34,160
We also look into the interdependency
between features and components.

111
00:07:34,190 --> 00:07:37,720
For example, where new features add
which components need to be modified.

112
00:07:38,220 --> 00:07:42,920
We found that the Cisco interface and
the Leap BPF library are closely related.

113
00:07:43,830 --> 00:07:50,020
Some features require changes to both
of Leap's helpers and kernel functions.

114
00:07:50,785 --> 00:07:54,435
Also tightly coupled with the eBPF
verifier, adding new helpers or

115
00:07:54,435 --> 00:08:01,285
keyframes usually involves updating
the eBPF verifier to add more logic.

116
00:08:02,065 --> 00:08:06,295
When we add runtime features in
eBPF, like a BPF link to console BPF

117
00:08:06,535 --> 00:08:10,995
reader, this often needs to update to
multiple components at the same time.

118
00:08:11,495 --> 00:08:15,335
We also, for example, we also
found that adding new eBPF

119
00:08:15,355 --> 00:08:20,215
instructions often includes more
types of bugs in the eBPF verifier.

120
00:08:20,215 --> 00:08:25,055
In this feature, we compare the total
numbers of bugs in the verifier with the

121
00:08:25,555 --> 00:08:30,225
addition of new instruction features, and
there is a clear overlap between them.

122
00:08:30,725 --> 00:08:34,435
If you are interested in a more
detailed report, it's available

123
00:08:34,445 --> 00:08:36,725
on GitHub and our archive paper.

124
00:08:37,225 --> 00:08:39,795
Now let's talk about some
best practices we've learned.

125
00:08:39,795 --> 00:08:46,255
Using predefined tags or categories
can help us standardize their

126
00:08:46,255 --> 00:08:51,345
response, which then make the
response clear and reduce ambiguity.

127
00:08:51,845 --> 00:08:55,715
We implement workflows for LLM
agents to reveal and refine their

128
00:08:55,795 --> 00:08:58,505
answers, which improves accuracy.

129
00:08:59,005 --> 00:09:03,535
We also know the models to answer,
allow the models to answer and not sure

130
00:09:03,575 --> 00:09:07,115
if they don't have enough information,
which prevents them from giving

131
00:09:07,125 --> 00:09:09,235
misleading answers or hallucinations.

132
00:09:09,735 --> 00:09:13,645
We start with parallel testing
and refine our survey iteratively.

133
00:09:13,945 --> 00:09:17,185
Make sure the questions are
clear and logically consistent.

134
00:09:17,955 --> 00:09:22,265
Finally, we make sure to evaluate
the data for consistency and

135
00:09:22,265 --> 00:09:24,915
catch any contradictory answers.

136
00:09:25,205 --> 00:09:28,095
This approach helps us improve
the readability and accuracy

137
00:09:28,095 --> 00:09:31,155
of the data generated by LRMs.

138
00:09:31,465 --> 00:09:38,055
It can also apply to other, not just
code surveys, like other tagging or tags.

139
00:09:38,555 --> 00:09:41,095
However, code survey also
has some limitations.

140
00:09:41,475 --> 00:09:46,275
For example, LLMs can make mistakes,
so they may sometimes misinterpret

141
00:09:46,275 --> 00:09:51,055
information or hallucinations
by providing incorrect answers.

142
00:09:51,345 --> 00:09:56,115
Human expertise is still sometimes
needed to help with the design of

143
00:09:56,115 --> 00:09:58,915
survey questions and validate results.

144
00:09:59,415 --> 00:10:00,765
Quality of the results.

145
00:10:00,765 --> 00:10:01,740
Depending on the data.

146
00:10:01,920 --> 00:10:07,145
If the data is in or there are un
enough resources, it can leave gaps

147
00:10:07,145 --> 00:10:10,075
in our understanding for next step.

148
00:10:10,135 --> 00:10:15,115
Next step, we plan to automate
our process even more so it

149
00:10:15,115 --> 00:10:16,735
requires less human effort.

150
00:10:16,765 --> 00:10:21,905
We are working on making decentralized
and scalable, and we are also fin

151
00:10:22,565 --> 00:10:28,865
refining the process using more
elements to push efficiency and another.

152
00:10:29,365 --> 00:10:33,655
Another priority is like to building a
robust framework for evaluation, which

153
00:10:33,655 --> 00:10:36,215
makes the validation more reliable.

154
00:10:36,275 --> 00:10:40,862
And finally, we are planning to
apply the CodeSheriff to other

155
00:10:40,862 --> 00:10:48,245
projects like LLVM and Kubernetes
to see how well it's classed.

156
00:10:48,805 --> 00:10:50,015
Thanks for your listening.

157
00:10:50,075 --> 00:10:55,205
You can find our Git report and
archive paper at these links.

