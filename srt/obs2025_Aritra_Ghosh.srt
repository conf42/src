1
00:00:01,080 --> 00:00:01,800
Hi everyone.

2
00:00:01,800 --> 00:00:02,820
I'm Raho.

3
00:00:02,880 --> 00:00:07,380
I'm currently a senior product manager
at Microsoft and I work on cloud

4
00:00:07,380 --> 00:00:09,030
native monitoring and troubleshooting.

5
00:00:09,450 --> 00:00:14,340
I'm excited to present to you today's
talk on shrinking the observability table.

6
00:00:14,799 --> 00:00:19,779
We are gonna discuss several strategies
on how you can optimize your cloud

7
00:00:19,779 --> 00:00:23,970
native monitoring costs while at the
same time maintaining the insights.

8
00:00:25,919 --> 00:00:27,599
So let's jump right in.

9
00:00:28,599 --> 00:00:32,439
Today we are gonna be diving into yeah,
the critical topic of observability costs.

10
00:00:32,439 --> 00:00:36,069
And this is focused on cloud,
native and Cuban environments.

11
00:00:36,339 --> 00:00:39,929
As we explore strategies for
cost effective monitoring, it's

12
00:00:39,929 --> 00:00:44,050
also essential to recognize
the balance between maintaining

13
00:00:44,109 --> 00:00:46,599
visibility and managing expenses.

14
00:00:47,119 --> 00:00:50,029
I'll guide you through some of the
innovative approaches that can help

15
00:00:50,029 --> 00:00:55,009
organizations optimize their observability
practices without sacrificing the

16
00:00:55,009 --> 00:00:57,169
insights they need to operate efficiently.

17
00:00:58,169 --> 00:01:01,924
This is gonna be our agenda, but
today we'll start off with why we

18
00:01:01,924 --> 00:01:04,015
see costs rising in this field.

19
00:01:04,530 --> 00:01:07,830
We'll discuss a framework
on how we can cut the waste.

20
00:01:08,080 --> 00:01:12,130
We are gonna do a deep dive on
some of the specific aspects of

21
00:01:12,130 --> 00:01:15,400
cloud native monitoring, which is
the control plane, the nodes, the

22
00:01:15,400 --> 00:01:16,990
pods, and the network monitoring.

23
00:01:17,400 --> 00:01:21,930
I'll discuss a few tools and dashboards
that can help and we'll end with a few

24
00:01:21,930 --> 00:01:25,890
key takeaways and action plan so that you
can go start acting right away on this.

25
00:01:26,890 --> 00:01:28,470
So first off why am I here?

26
00:01:28,470 --> 00:01:29,940
Why are we talking about this?

27
00:01:30,290 --> 00:01:35,550
Observability can cost up to 40%
of the bill sometimes even more.

28
00:01:35,925 --> 00:01:40,315
And given how dynamic cloud native
environments are, the number of

29
00:01:40,315 --> 00:01:45,175
microservices, the spin up, the spin down,
there's just a lot of data generated.

30
00:01:45,325 --> 00:01:50,755
Sometimes it's even beyond your control,
what data is collected, what is generated.

31
00:01:50,905 --> 00:01:55,524
So it's important to realize the
difference between a regular VM

32
00:01:55,524 --> 00:01:59,124
or another host monitoring versus
cloud native monitoring, where some

33
00:01:59,124 --> 00:02:00,534
things are just out of control.

34
00:02:00,994 --> 00:02:04,714
The second important point is the
fact that there is a lot of data,

35
00:02:04,744 --> 00:02:06,724
but not all the data is useful.

36
00:02:06,974 --> 00:02:10,334
There may not be valuable insights
in all the data that you collect,

37
00:02:10,664 --> 00:02:14,594
so it's important that we build
smart and efficient strategies so

38
00:02:14,594 --> 00:02:17,594
that you can only collect what you
want and get the best value data.

39
00:02:18,429 --> 00:02:23,549
At the bottom I have a sample example of
what the cost breakdown can look like.

40
00:02:23,699 --> 00:02:27,299
Here we have the observability,
the networking, the compute,

41
00:02:27,299 --> 00:02:28,409
and the storage costs.

42
00:02:28,469 --> 00:02:33,259
Of course, this is just a presentative
example but you can see that it can cost

43
00:02:33,259 --> 00:02:37,519
a lot, especially if you're not mindful of
the things that, that you're collecting.

44
00:02:38,519 --> 00:02:40,679
Where are we spending all this money?

45
00:02:41,079 --> 00:02:46,579
There's three or four different categories
where I have seen that enterprises

46
00:02:46,579 --> 00:02:51,509
spend most of the dollars in collecting
observability and monitoring metrics.

47
00:02:51,689 --> 00:02:54,209
The first one, high ity custom metrics.

48
00:02:54,624 --> 00:02:58,954
Most of the tools out there charge
a different fee or a different

49
00:02:58,954 --> 00:03:00,724
pricing for these custom metrics.

50
00:03:00,814 --> 00:03:04,414
They're extremely high cardinality,
which means they have labels like

51
00:03:04,414 --> 00:03:07,144
the user ID and things like that.

52
00:03:07,454 --> 00:03:14,354
The second one is a long log retention,
depending on how if it's 30 days, 90 days

53
00:03:14,424 --> 00:03:16,704
it could spike up the bill pretty badly.

54
00:03:17,049 --> 00:03:18,849
And also verbose logging.

55
00:03:18,849 --> 00:03:24,039
If you have extremely verbose logs like
debug logs and other logs configured,

56
00:03:24,369 --> 00:03:28,779
it's gonna be spending a lot of your money
just collecting Yeah, these verbose logs.

57
00:03:29,179 --> 00:03:33,279
The third thing that I've seen is the
same data gets replicated multiple

58
00:03:33,279 --> 00:03:34,959
times to different destinations.

59
00:03:35,199 --> 00:03:39,079
That could, of course spike a
pay cost to and all when you're

60
00:03:39,079 --> 00:03:42,289
sending data across the network
to different regions or otherwise.

61
00:03:42,589 --> 00:03:46,489
That could also accrue expensive,
both egress and storage fees too.

62
00:03:47,489 --> 00:03:51,209
So now let's get into the
framework that I had promised.

63
00:03:51,499 --> 00:03:56,149
So the first important thing to realize
is we have to collect what matters.

64
00:03:56,149 --> 00:04:00,229
And here the service level
objectives are most important as

65
00:04:00,229 --> 00:04:02,209
compared to all the different logs.

66
00:04:02,699 --> 00:04:05,159
You have to start with the
service level objectives.

67
00:04:05,159 --> 00:04:10,289
That is the main metric which affects
users, and that's what users are seeing.

68
00:04:10,589 --> 00:04:13,109
And then you have to
choose the right signal.

69
00:04:13,439 --> 00:04:16,809
So in terms of the signal
itself and the value metrics

70
00:04:16,809 --> 00:04:18,429
are the most important signal.

71
00:04:18,429 --> 00:04:21,849
They help in alerting the make
sure that the services are up and

72
00:04:21,849 --> 00:04:23,649
running, if not that you're alerted.

73
00:04:24,079 --> 00:04:25,369
The second is traces.

74
00:04:25,879 --> 00:04:28,819
So now you have a flow of
like how the data goes from a

75
00:04:28,819 --> 00:04:31,759
different, from one service to
another, and that helps you debug.

76
00:04:31,819 --> 00:04:36,029
And the last is logs when in a service
you can go and debug what's going wrong.

77
00:04:36,399 --> 00:04:40,779
The third one is you have to tune
your retention and aggregation.

78
00:04:40,779 --> 00:04:44,019
Retention is how long it is stored,
and aggregation is, yeah, how do you

79
00:04:44,019 --> 00:04:48,669
make sense of this data and wherever
possible, consolidate and offload.

80
00:04:48,719 --> 00:04:50,969
Move your data to a cold
storage if it's not useful.

81
00:04:51,329 --> 00:04:55,589
So again, summarizing metrics are the
most important signal and in terms of the

82
00:04:55,589 --> 00:04:57,809
value they provide, followed by traces.

83
00:04:57,809 --> 00:04:58,829
Followed by logs.

84
00:04:59,829 --> 00:05:04,959
So now we'll dive into the important parts
of specifically Kubernetes monitoring.

85
00:05:04,959 --> 00:05:07,929
So like I had mentioned, there's
four different components.

86
00:05:07,979 --> 00:05:11,639
The control plane, the nodes, the
pods, and the network monitoring.

87
00:05:12,239 --> 00:05:17,299
Again when monitoring the control plane,
the important thing is the precision.

88
00:05:17,449 --> 00:05:22,369
Instead of ingesting full logs, we should
focus on managed service metrics such as

89
00:05:22,369 --> 00:05:24,709
the API latency and the health checks.

90
00:05:25,079 --> 00:05:29,099
Sampling or summarizing logs can
help reduce data volume while

91
00:05:29,129 --> 00:05:30,689
providing valuable insights.

92
00:05:31,079 --> 00:05:35,879
And again, alerts should be based on
deviations from the expected behavior and

93
00:05:35,909 --> 00:05:39,689
not on based on static thresholds, which
allows for more responsive monitoring.

94
00:05:40,334 --> 00:05:43,364
It's also important to understand
why control plane insights are

95
00:05:43,364 --> 00:05:46,454
important because they are the
brain of Kubernetes monitoring.

96
00:05:46,454 --> 00:05:49,784
And if the control plane is down,
nothing else can function and you

97
00:05:49,784 --> 00:05:51,464
cannot make any updates to the system.

98
00:05:52,914 --> 00:05:56,544
Below I have some of the examples
like the a p you can see that,

99
00:05:56,544 --> 00:05:58,974
see lag and the throttling rates.

100
00:05:59,974 --> 00:06:01,504
Now let's get to the notes.

101
00:06:03,454 --> 00:06:07,894
For smarter node monitoring, it's
essential to track only the key metrics

102
00:06:07,894 --> 00:06:12,334
that truly reflect the system Health,
such as the CCP usage, memory usage, and

103
00:06:12,334 --> 00:06:17,344
disc io avoid getting bogged down with
poor process metrics unless you're in a

104
00:06:17,344 --> 00:06:18,964
debugging or troubleshooting scenario.

105
00:06:19,384 --> 00:06:23,614
Monitoring node degradation through
heartbeats and node ready transitions

106
00:06:23,824 --> 00:06:27,664
can provide early warnings of
potential issues without overwhelming

107
00:06:27,724 --> 00:06:28,804
your data collection effort.

108
00:06:31,799 --> 00:06:34,499
Yeah, and again like I said, you
do not need to provide, collect

109
00:06:34,539 --> 00:06:38,239
all the metrics you can only
collect when something has changed.

110
00:06:39,469 --> 00:06:43,389
So here we wanna touch upon something
important, which is the adoption

111
00:06:43,389 --> 00:06:45,159
and the use of open telemetry.

112
00:06:45,259 --> 00:06:45,679
Again.

113
00:06:46,039 --> 00:06:49,219
Optimizing metrics with open
telemetry can significantly

114
00:06:49,219 --> 00:06:51,169
enhance our observability strategy.

115
00:06:51,379 --> 00:06:56,719
By using explicit aggregation temporality,
we can better manage the data volume

116
00:06:57,049 --> 00:07:01,719
and you can filter some of the labels
to reduce the cardinality and that,

117
00:07:02,269 --> 00:07:05,739
and limit your custom metrics to
only business critical parts, like I

118
00:07:05,739 --> 00:07:07,779
mentioned, that can accrue a lot of costs.

119
00:07:08,104 --> 00:07:11,434
This approach not only saves
costs, it also helps to improve the

120
00:07:11,434 --> 00:07:12,964
clarity of the data that we collect.

121
00:07:13,964 --> 00:07:18,344
So now we go to the port level
visibility like we discussed

122
00:07:18,394 --> 00:07:19,864
in the cloud native world.

123
00:07:19,984 --> 00:07:23,104
It could be a lot of different
microservices that's spin up and spin

124
00:07:23,104 --> 00:07:28,994
down, and it's important to be optimizing
for the port costs, port monitoring costs.

125
00:07:29,044 --> 00:07:29,944
So first one.

126
00:07:30,269 --> 00:07:33,569
Achieving port level visibility
without the noise, it is a ch

127
00:07:33,599 --> 00:07:35,099
challenge that we can tackle.

128
00:07:35,369 --> 00:07:39,629
And by filtering logs at the source,
we can exclude unnecessary data such

129
00:07:39,629 --> 00:07:41,519
as health checks and static endpoints.

130
00:07:41,989 --> 00:07:45,619
Utilizing labels to correlate
telemetry with services enhances

131
00:07:45,619 --> 00:07:47,419
our ability to analyze performance.

132
00:07:47,719 --> 00:07:52,344
And like I mentioned before, adopting
telemetry or open telemetry for auto

133
00:07:52,344 --> 00:07:55,849
tagging and structured tracing can
streamline this process and also improve,

134
00:07:56,239 --> 00:07:59,989
the last one, the network course.

135
00:08:00,919 --> 00:08:05,119
When it comes to networking observability,
it's important to focus on the big issues.

136
00:08:05,269 --> 00:08:09,259
Tracking connection, failures and GNS
resolution problems, which are pretty

137
00:08:09,259 --> 00:08:13,919
common, can provide critical insights
into the network health using sampling

138
00:08:13,919 --> 00:08:16,719
flow routes such as net flow or IP fix.

139
00:08:17,189 --> 00:08:20,319
You can manage data volume
while still capturing essential

140
00:08:20,319 --> 00:08:22,149
information wherever possible.

141
00:08:22,149 --> 00:08:25,719
Also, adopt the EBPF based
tooling, which can offer more

142
00:08:25,719 --> 00:08:27,549
efficient monitoring solutions.

143
00:08:30,339 --> 00:08:35,239
The last one, which I want to touch upon
is not one of this, but is based on the

144
00:08:35,239 --> 00:08:37,189
third party tooling that, that you use.

145
00:08:37,604 --> 00:08:42,074
Wherever you have your data ensure
that you're not duplicating the

146
00:08:42,074 --> 00:08:46,334
telemetry across different tools
and wherever possible filter at the

147
00:08:46,334 --> 00:08:49,604
source, which is ship filtered logs,
whether you're using crabb crop or

148
00:08:49,604 --> 00:08:54,514
fluent parcels, send them when you're
making an analysis to make an analysis

149
00:08:54,514 --> 00:08:56,884
based on the dollar cost per gv four.

150
00:08:57,209 --> 00:08:58,859
Logs and traces and for metrics.

151
00:08:58,859 --> 00:09:01,049
Some of the cases, it could be
the number of metrics shipped.

152
00:09:01,139 --> 00:09:04,519
So below there's a table
which shows different tools

153
00:09:04,519 --> 00:09:06,179
and what is the cost per gb?

154
00:09:06,629 --> 00:09:08,609
What this analysis helps in understanding.

155
00:09:08,609 --> 00:09:08,789
Yeah.

156
00:09:08,819 --> 00:09:10,079
Which is the tool you should be using.

157
00:09:10,079 --> 00:09:11,189
Where should you be sending what?

158
00:09:12,189 --> 00:09:15,639
I wanna present a quick
case study of a customer.

159
00:09:15,639 --> 00:09:20,569
Again, this is a fictional customer,
no specific customer in mind where

160
00:09:20,569 --> 00:09:25,479
implementing some of the strategies they
were able to reduce 40% of their costs

161
00:09:25,809 --> 00:09:30,969
without losing the important telemetry
that they need for troubleshooting.

162
00:09:32,049 --> 00:09:36,399
So by dropping unnecessary health check
logs and by reducing their velocity

163
00:09:36,399 --> 00:09:40,599
levels, they streamlined their data
collection by transitioning from raw

164
00:09:40,599 --> 00:09:42,759
latency logs to Prometheus system grams.

165
00:09:42,999 --> 00:09:47,319
They had the same essential insights
while moving cold observability

166
00:09:47,319 --> 00:09:51,029
data to low object storage
further reducing their expenses.

167
00:09:51,449 --> 00:09:54,839
It's again, you can see how the reduction
in the logs, metrics and traces.

168
00:09:55,229 --> 00:09:57,419
And how it changed before and after.

169
00:09:57,479 --> 00:09:59,249
The monthly dollar cost
is just an example.

170
00:09:59,249 --> 00:10:00,569
It's not the exact dollar cost.

171
00:10:01,569 --> 00:10:06,339
It's crucial to understand what you're
given up when making changes to your

172
00:10:06,339 --> 00:10:11,259
observability Strategy sampling does
not necessarily mean losing visibility.

173
00:10:11,259 --> 00:10:12,729
It's a strategic reduction.

174
00:10:12,939 --> 00:10:16,029
Shortening retention periods
can be effective for most

175
00:10:16,029 --> 00:10:17,379
active monitoring scenarios.

176
00:10:17,874 --> 00:10:21,894
The keys to balance data fidelity
with the trade offs in both

177
00:10:21,894 --> 00:10:23,304
engineering resources and costs.

178
00:10:23,304 --> 00:10:28,904
Like you can see there is a curve and
at some point you lose the value of

179
00:10:28,934 --> 00:10:30,914
adding more metrics or more telemetry.

180
00:10:31,224 --> 00:10:35,064
It's important to understand
what trade offs you are signing

181
00:10:35,064 --> 00:10:38,635
up for, and then deciding
accordingly how you deal with them.

182
00:10:39,635 --> 00:10:43,445
Like I discussed, now we wanna
discuss how you implement this at

183
00:10:43,445 --> 00:10:45,245
your organization and your team.

184
00:10:45,585 --> 00:10:49,485
First off, you have to have a cost
aware observability dashboard.

185
00:10:49,755 --> 00:10:52,815
You have to visualize this
cost per telemetry type, like

186
00:10:52,815 --> 00:10:53,865
I showed in some of the list.

187
00:10:54,015 --> 00:10:56,655
How much do the logs,
metrics, and traces cost?

188
00:10:57,045 --> 00:11:01,125
The second you have to then
attribute it to the different teams.

189
00:11:01,425 --> 00:11:03,045
That are using this telemetry.

190
00:11:03,225 --> 00:11:07,185
So you have to add the tags for the
specific teams and make sure that they,

191
00:11:07,245 --> 00:11:09,495
those chargebacks are at least visible.

192
00:11:09,885 --> 00:11:13,275
And in many cases, the tools
also integrate with the building

193
00:11:13,275 --> 00:11:16,600
export APIs, which are helpful
in building this dashboard.

194
00:11:17,600 --> 00:11:21,200
The second step is you have to
have an observability budget.

195
00:11:21,510 --> 00:11:25,020
It's a proactive step towards
managing costs effectively.

196
00:11:25,200 --> 00:11:29,730
Setting a cap on expenses per cluster per
month for telemetry is almost essential.

197
00:11:30,150 --> 00:11:34,110
Establishing quota for log volume and
custom metrics can help enforce those

198
00:11:34,110 --> 00:11:37,800
limits and it can utilize tools like
flow and Bit and low key, which can

199
00:11:37,800 --> 00:11:41,235
help in maintaining this constraints and
ensuring compliance within your budget.

200
00:11:42,235 --> 00:11:46,015
Here is a framework for your cost
optimization checklist and how you can go

201
00:11:46,015 --> 00:11:50,655
about implementing some of this so that
you can satisfy your observability budget.

202
00:11:51,095 --> 00:11:54,605
First start by dropping any
unused metrics and logs.

203
00:11:54,605 --> 00:11:55,715
That's a drop phase.

204
00:11:55,985 --> 00:11:58,625
Then you shorten retention
periods wherever possible.

205
00:11:58,625 --> 00:12:02,525
If you do not need the logs beyond
30 days of your organization

206
00:12:02,525 --> 00:12:05,765
doesn't have this kind of a
policy, you don't really need them.

207
00:12:06,110 --> 00:12:09,620
For some of the metrics, you can store
them for longer and for logs you can

208
00:12:09,650 --> 00:12:13,525
store them for shorter filter the
telemetry, the source, like I mentioned,

209
00:12:13,525 --> 00:12:18,195
whether it's pods or any other place
for it, just saves in both the compute

210
00:12:18,195 --> 00:12:23,265
time as well as the storage costs and
consolidate tools wherever possible.

211
00:12:23,625 --> 00:12:28,125
And last but not least, make sure you tune
your alert rules to minimize the noise.

212
00:12:28,125 --> 00:12:30,795
We'll also be talking about
it a little bit more now.

213
00:12:33,500 --> 00:12:38,320
First off, I wanted to point out
something with the high card edge metrics.

214
00:12:38,350 --> 00:12:43,500
So wherever possible avoid storing the
labels like user ID or UU ID in your

215
00:12:43,500 --> 00:12:48,180
metrics and use things like tire region
status code, which are both more useful.

216
00:12:48,420 --> 00:12:49,715
And it saves cost too.

217
00:12:49,960 --> 00:12:53,520
Yeah, you can reduce your series
count, which essentially reduces

218
00:12:53,520 --> 00:12:55,410
your Prometheus storage cost.

219
00:12:57,150 --> 00:12:58,770
Now we'll get to the alerts.

220
00:12:59,770 --> 00:13:00,040
Yeah.

221
00:13:00,070 --> 00:13:04,170
De detoxifying your alert noise
is essential for maintaining

222
00:13:04,200 --> 00:13:05,280
effective monitoring.

223
00:13:05,540 --> 00:13:09,025
Here the funnel approach, which is
moving from raw data to filtered,

224
00:13:09,620 --> 00:13:13,850
to routed to actionable alerts, it
is, can streamline your process.

225
00:13:14,120 --> 00:13:19,490
So first off, focus on alerting only
for SLO violations or customer impact.

226
00:13:19,490 --> 00:13:20,870
So start with your alerts.

227
00:13:21,155 --> 00:13:24,455
At the customer impact, which
will both reduce your fatigue and

228
00:13:24,455 --> 00:13:26,165
improve your mean time to recovery.

229
00:13:26,525 --> 00:13:31,345
And yeah, this strategic approach enhances
the overall observability of effectiveness

230
00:13:31,405 --> 00:13:32,695
of your observability efforts,

231
00:13:33,695 --> 00:13:34,655
like I mentioned.

232
00:13:34,705 --> 00:13:34,885
Yeah.

233
00:13:34,885 --> 00:13:37,585
It's important to understand
that observability doesn't

234
00:13:37,585 --> 00:13:38,725
mean logging everything.

235
00:13:38,755 --> 00:13:41,665
Instead, we focus on capturing
valuable signals that provide

236
00:13:41,665 --> 00:13:43,075
insights of system performance.

237
00:13:43,465 --> 00:13:48,025
We prioritizing quality over quantity,
we can keep the cost manageable while

238
00:13:48,025 --> 00:13:51,955
also gaining the insights necessary
to maintain operational excellence.

239
00:13:52,165 --> 00:13:56,305
Once again, focus on the valuable signals,
not the raw volume of data collected.

240
00:13:56,305 --> 00:13:59,575
Just because I collect a lot
of data doesn't mean I have all

241
00:13:59,575 --> 00:14:00,715
the insights that I will need.

242
00:14:01,415 --> 00:14:05,945
The second thing here is important to
note is the fact that sometimes when you

243
00:14:05,945 --> 00:14:09,785
have a lot of this data, it's not easy
to sift through it or to understand.

244
00:14:09,785 --> 00:14:11,855
It's almost like finding
a needle in a haystack.

245
00:14:12,165 --> 00:14:14,415
So just because you have
it doesn't always help.

246
00:14:15,415 --> 00:14:18,420
Yeah like we discussed, so
the end to end observability

247
00:14:18,420 --> 00:14:19,860
flows look something like this.

248
00:14:19,950 --> 00:14:23,040
You start with ingesting the
data, then you process it and

249
00:14:23,230 --> 00:14:24,820
either aggregate it or filter it.

250
00:14:24,880 --> 00:14:28,780
Then you store it and you alert
on it and then optimize on it.

251
00:14:29,170 --> 00:14:33,250
It's important to tag a dollar
value at each stage to measure the

252
00:14:33,250 --> 00:14:34,990
ROI of doing each of the steps.

253
00:14:35,455 --> 00:14:39,445
And yeah, you can, this is essentially a
loop where you iterate to make sure that

254
00:14:39,445 --> 00:14:42,775
you're within your budget and you're also
getting the most value from your data.

255
00:14:43,775 --> 00:14:48,145
I want to end with some of the recommended
tools that I would suggest you check out.

256
00:14:48,365 --> 00:14:51,635
First off, when it comes to
metrics, Prometheus with Anos

257
00:14:51,635 --> 00:14:55,145
is super anos, as the storage is
super helpful, our cortex as well.

258
00:14:55,525 --> 00:14:58,435
You can explore fluent bit for
log filtering in the source.

259
00:14:58,775 --> 00:15:02,675
Open Telemetry, SDKs and collector
for both traces, metrics and logs.

260
00:15:02,995 --> 00:15:06,535
Grafana dashboards are super
helpful for visualizations.

261
00:15:06,535 --> 00:15:09,745
They're both easy to build and
also there's lots of open source

262
00:15:09,865 --> 00:15:10,945
dashboards that you can use.

263
00:15:10,975 --> 00:15:14,725
And EBPF tools like Cilium PC, they
help for network observability.

264
00:15:15,725 --> 00:15:18,814
So once again, I'd like to
reemphasize what we discussed.

265
00:15:19,064 --> 00:15:21,374
These are some of the
common pitfalls to avoid.

266
00:15:21,879 --> 00:15:23,709
You don't have to log everything.

267
00:15:23,759 --> 00:15:27,049
Make sure that you're sampling or
filtering ensure that you're using

268
00:15:27,049 --> 00:15:30,899
the right retention settings for
your telemetry to not duplicate

269
00:15:30,899 --> 00:15:33,379
telemetry to multiple destinations.

270
00:15:33,689 --> 00:15:36,719
Make sure that you have no
met high coordinative metrics.

271
00:15:36,719 --> 00:15:38,909
Metrics which are have,
which increase your bills.

272
00:15:39,209 --> 00:15:41,509
And also alerting on static thresholds.

273
00:15:41,509 --> 00:15:43,599
This could lead to a lot of alert fatigue.

274
00:15:44,599 --> 00:15:44,929
Yeah.

275
00:15:44,959 --> 00:15:48,649
In closing, remember this mantra,
better signals lead to better,

276
00:15:48,679 --> 00:15:50,719
lower cost and happier engineers.

277
00:15:50,959 --> 00:15:54,109
By focusing on the quality of our
observability data, we can enhance

278
00:15:54,109 --> 00:15:58,129
our operational efficiency while
also managing expenses effectively.

279
00:15:58,369 --> 00:16:01,429
And let's strive for a balance that
supports both our technical needs

280
00:16:01,459 --> 00:16:02,749
and our budgetary constraints.

281
00:16:03,749 --> 00:16:04,589
Thanks everyone.

282
00:16:04,709 --> 00:16:05,819
Hope you found this useful.

