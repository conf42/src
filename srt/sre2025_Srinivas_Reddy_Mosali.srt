1
00:00:00,500 --> 00:00:01,189
Hello everyone.

2
00:00:01,519 --> 00:00:03,290
Thank you for joining
me in this conference.

3
00:00:03,950 --> 00:00:07,340
Today we will discuss on how
FinTech reliability is achieved

4
00:00:07,340 --> 00:00:08,235
through SRE Innovation.

5
00:00:08,735 --> 00:00:13,264
So as organizations accelerate their
AI initiatives, many are discovering

6
00:00:13,264 --> 00:00:17,804
that traditional infrastructure was
not built to handle the high demands

7
00:00:17,804 --> 00:00:19,584
of machine learning operations.

8
00:00:20,064 --> 00:00:23,634
These workloads are highly resource
intensive and are unpredictable.

9
00:00:24,244 --> 00:00:28,594
While Kubernetes has emerged as
a platform of choice for managing

10
00:00:28,594 --> 00:00:32,014
these environments, simply running
the containers is not enough.

11
00:00:32,444 --> 00:00:35,054
The challenge lies in
running them optimally.

12
00:00:35,824 --> 00:00:40,684
What we are seeing is that many teams
are struggling, be it dealing with

13
00:00:40,774 --> 00:00:45,885
over-provisioned resources, unpredictable
costs and performance bottlenecks.

14
00:00:46,185 --> 00:00:51,025
So in this particular session, I would
like to walk you through a set of proven

15
00:00:51,025 --> 00:00:56,685
SRE strategies that are designed to
increase performance, reduce costs, and

16
00:00:56,685 --> 00:01:01,605
significantly improve the reliability
of the AI ML pipelines running in.

17
00:01:02,040 --> 00:01:06,250
Kubernetes these approaches come directly
from the high stakes environments

18
00:01:06,680 --> 00:01:11,360
these where even minor inefficiencies
can lead to a major financial losses.

19
00:01:11,990 --> 00:01:15,720
So together we'll go through the
techniques that have delivered up

20
00:01:15,720 --> 00:01:20,220
to a 60% improvements in resource
utilization and substantially

21
00:01:20,540 --> 00:01:22,480
reductions in incident rates.

22
00:01:22,915 --> 00:01:26,455
All while maintaining the
agility and performance.

23
00:01:26,955 --> 00:01:32,015
Ultimately, this presentation is about
more than just keeping the systems online.

24
00:01:32,315 --> 00:01:36,845
It's about engineering confidently
at scale in an increased AI

25
00:01:36,845 --> 00:01:38,405
driven financial ecosystem.

26
00:01:38,795 --> 00:01:42,865
So let's take a closer look on how
the transformation is happening.

27
00:01:43,365 --> 00:01:43,574
Okay.

28
00:01:43,574 --> 00:01:47,114
When we talk about scaling AI
infrastructure efficiently, GPU

29
00:01:47,114 --> 00:01:51,084
utilization is one of the most
overlooked and most expensive

30
00:01:51,204 --> 00:01:52,644
and has more bottlenecks.

31
00:01:52,854 --> 00:01:58,254
So let's look at how we can dramatically
shift that performance to cost ratio

32
00:01:58,654 --> 00:02:00,874
using smart GPU level strategies.

33
00:02:01,694 --> 00:02:05,204
Let's start with the MIG or the
multi-instance GPU Partitioning.

34
00:02:05,504 --> 00:02:10,274
So this is a feature available in
NVIDIA's 800 and h hundred series.

35
00:02:10,704 --> 00:02:16,354
It allows you to carve up a single GPU
into a multiple isolated instances,

36
00:02:16,714 --> 00:02:20,509
which enables like multiple workloads to
run simultaneously without any issues.

37
00:02:21,364 --> 00:02:26,564
Our real world tests have shown up to a
seven x better utilization especially in

38
00:02:26,564 --> 00:02:28,855
environments with varied inference loads.

39
00:02:29,844 --> 00:02:36,424
Next in in memory efficiency QDA provides
like a low level access to memory

40
00:02:36,424 --> 00:02:39,784
management, allowing us to opti optimize.

41
00:02:39,784 --> 00:02:42,824
Tensor core use and
reduce the fragmentation.

42
00:02:43,124 --> 00:02:48,704
So this can lead to 15 to 30% of
throughput gains particularly for

43
00:02:48,704 --> 00:02:53,004
vision heavy models like ID verification
or biometric fraud detection.

44
00:02:53,504 --> 00:02:55,664
Oh, coming to precision optimization.

45
00:02:55,724 --> 00:03:00,484
In many FinTech companies like ML
workloads, especially during inference.

46
00:03:00,984 --> 00:03:04,224
Full FFP 32 precision
is not always needed.

47
00:03:04,524 --> 00:03:11,724
So by switching the, switching it to FP
16 or even INT eight, organizations can

48
00:03:11,724 --> 00:03:13,974
drastically shrink the memory footprint.

49
00:03:14,454 --> 00:03:16,224
So these techniques are game changers.

50
00:03:16,414 --> 00:03:21,324
They maximize the GPU resource usage
and provide critical improvements in

51
00:03:21,324 --> 00:03:23,884
both performance and cost efficiency.

52
00:03:24,384 --> 00:03:25,644
Moving on to the next slide.

53
00:03:25,644 --> 00:03:29,984
On auto-scaling strategies, as
organizations push to run ML workloads

54
00:03:29,984 --> 00:03:34,264
more efficiently traditional auto-scaling
strategies are not enough, especially

55
00:03:34,264 --> 00:03:36,064
in GPU accelerated environments.

56
00:03:36,334 --> 00:03:41,764
Here we'll go through four techniques
that enable smarter ML strategies.

57
00:03:42,104 --> 00:03:43,634
Which will help with the autoscaling.

58
00:03:44,604 --> 00:03:46,494
First we'll go through
the workload profiling.

59
00:03:46,884 --> 00:03:52,014
So the foundation of autoscaling is
understanding when and how much to scale.

60
00:03:52,374 --> 00:03:56,334
So workload profiling uses the
historical telemetry across both

61
00:03:56,334 --> 00:03:58,434
training and inference cycles.

62
00:03:59,024 --> 00:04:00,824
To uncover the usage patterns.

63
00:04:01,214 --> 00:04:06,384
So this helps define the baseline
resource needs, identify the spikes

64
00:04:06,384 --> 00:04:10,404
during the market, events or frauds,
fraud searches, and ensures that

65
00:04:10,404 --> 00:04:14,544
the auto-scaling decisions are
data informed and are not reactive.

66
00:04:15,044 --> 00:04:17,444
Coming to the custom metrics pipeline.

67
00:04:17,924 --> 00:04:22,434
So if you look at the out of box
Kubernetes like Kubernetes HPA doesn't

68
00:04:22,584 --> 00:04:24,564
know anything about the machine learning.

69
00:04:24,894 --> 00:04:29,644
That's where like the custom metrics
come in using Prometheus adapters.

70
00:04:29,694 --> 00:04:35,694
We expose the ml. Relevant signals like
the Q Depth batch processing duration,

71
00:04:35,694 --> 00:04:38,924
and GPU memory pressure to the autoscaler.

72
00:04:39,284 --> 00:04:45,444
So this enables a precise decision based
on the actual workload characteristics,

73
00:04:45,624 --> 00:04:48,354
not just the CPU load or the pod count.

74
00:04:48,854 --> 00:04:53,144
Coming to predictive scaling for
recurring or seasonal workloads.

75
00:04:53,504 --> 00:04:58,204
Think EOD model model retaining
or the batch scoring jobs.

76
00:04:58,594 --> 00:05:02,554
We can apply the time-based
or ML based scaling triggers.

77
00:05:02,884 --> 00:05:06,454
So predictive models can anticipate
the spikes before they happen.

78
00:05:07,000 --> 00:05:09,969
Enabling warm pools to
be spun up just in time.

79
00:05:10,340 --> 00:05:16,250
This reduces the cold start latency
by up to 85%, especially valuable

80
00:05:16,280 --> 00:05:18,340
for real time FinTech systems.

81
00:05:19,160 --> 00:05:23,760
Coming to the buffer management we
can't forget the cluster resilience.

82
00:05:24,004 --> 00:05:28,464
So the buffer management means
deliberately over provisioning a

83
00:05:28,464 --> 00:05:33,774
small GPU headroom during critical
critical hours like trading sessions

84
00:05:33,774 --> 00:05:36,534
or things like holiday season, right?

85
00:05:36,594 --> 00:05:40,215
Then you can scale them back down
more aggressively after the hours.

86
00:05:40,960 --> 00:05:45,439
This balances the control cost control
with high availability, which is

87
00:05:45,439 --> 00:05:48,019
non-negotiable for financial applications.

88
00:05:48,529 --> 00:05:53,289
So these autoscaling strategies
go well beyond the HPA setup.

89
00:05:53,669 --> 00:05:57,699
When implemented together, they enable
the organizations to sustain peak

90
00:05:57,699 --> 00:06:00,454
model performance with minimal costs.

91
00:06:00,954 --> 00:06:01,164
Okay.

92
00:06:01,164 --> 00:06:05,314
Coming to the multi GPU training
orchestration the biggest bottlenecks

93
00:06:05,314 --> 00:06:11,754
in scaling a machine learning training
right is GPU job orchestrations.

94
00:06:12,034 --> 00:06:14,989
It's not just having
about having more GPUs.

95
00:06:15,594 --> 00:06:17,754
It's about using them in intelligently.

96
00:06:18,114 --> 00:06:21,954
When we orchestrate jobs sufficiently
across the gps, especially in

97
00:06:21,954 --> 00:06:27,164
distributed environments, we unlock,
huge throughput gains in cost efficiency.

98
00:06:27,654 --> 00:06:30,334
The first step is
topology aware scheduling.

99
00:06:30,754 --> 00:06:31,834
Why does this matter?

100
00:06:31,894 --> 00:06:37,834
So when jobs span across multiple
GPUs, especially across nodes network

101
00:06:37,834 --> 00:06:43,194
latency becomes c enemy like we solve
this by using the g Kubernetes node

102
00:06:43,194 --> 00:06:47,994
affinity rules to schedule the jobs on
nodes that are physically close to the

103
00:06:48,354 --> 00:06:50,514
interconnect through high bandwidth.

104
00:06:50,939 --> 00:06:53,769
Links like NV link, or PCIE.

105
00:06:54,319 --> 00:06:57,589
This minimizes the communication
overhead and drastically improves the

106
00:06:57,589 --> 00:06:59,679
performance and distributed training.

107
00:07:00,179 --> 00:07:02,759
Next we move on to distributed
training frameworks.

108
00:07:03,119 --> 00:07:06,959
So framework for frameworks
like TensorFlows multi

109
00:07:06,959 --> 00:07:09,309
worker needs to be tuned.

110
00:07:09,694 --> 00:07:14,064
To our infrastructure, often we
need poor performance because

111
00:07:14,124 --> 00:07:15,804
training scripts are not optimized.

112
00:07:16,454 --> 00:07:21,284
So we address this with Custom
gate template that enable, like

113
00:07:21,344 --> 00:07:25,124
dynamic scaling, reduces the
training time variance, and

114
00:07:25,124 --> 00:07:28,004
it minimizes the communication
bottlenecks between the nodes.

115
00:07:28,459 --> 00:07:32,484
Finally, there's a priority
based preemptive, mechanism.

116
00:07:32,784 --> 00:07:36,229
So in this particular technique if
you look at our shared clusters.

117
00:07:37,004 --> 00:07:42,404
When multiple teams submit the jobs,
it's critical to ensure that high

118
00:07:42,454 --> 00:07:46,624
priority jobs don't wait in line
behind the rule, low priority ones.

119
00:07:47,074 --> 00:07:51,344
So we implement intelligent
queuing mechanisms that evaluate

120
00:07:51,344 --> 00:07:53,054
the job priority and then.

121
00:07:53,119 --> 00:07:55,489
Deadlines and then resource fairness.

122
00:07:55,729 --> 00:08:00,139
It's like air traffic control for
GPUs, like preventing the collisions,

123
00:08:00,139 --> 00:08:03,979
reducing the idle time, and keeping
the system fair and efficient.

124
00:08:04,479 --> 00:08:09,609
The key takeaway is like when you bring
all these three pillars together, the

125
00:08:09,609 --> 00:08:14,519
topology awareness, smart frameworks,
and priority based scheduling you can

126
00:08:14,519 --> 00:08:17,429
cut the training time by 30 to 50%.

127
00:08:17,939 --> 00:08:22,079
That's a massive impact when you are
running the running hundreds or even

128
00:08:22,079 --> 00:08:24,329
thousands of experiments each week.

129
00:08:24,829 --> 00:08:25,159
Okay.

130
00:08:25,159 --> 00:08:27,649
Moving on to the advanced
monitoring and observability.

131
00:08:27,889 --> 00:08:32,819
As machine learning workloads grow in
complexity and scale, the need for robust

132
00:08:32,879 --> 00:08:35,099
observability becomes absolutely critical.

133
00:08:35,649 --> 00:08:39,249
Not just for performance, but
for cost efficiency and op

134
00:08:39,459 --> 00:08:40,779
and operational reliability.

135
00:08:41,319 --> 00:08:41,799
So this.

136
00:08:42,709 --> 00:08:45,829
Here outlines the layered approach.

137
00:08:45,884 --> 00:08:49,414
We take towards building a
comprehensive monitoring stack for

138
00:08:49,414 --> 00:08:51,534
the machine learning infrastructure.

139
00:08:51,834 --> 00:08:55,284
So let's start at the
foundation, the historical data.

140
00:08:55,659 --> 00:08:59,779
So long-term storage of metrics
is essential for capacity

141
00:08:59,779 --> 00:09:01,399
planning and trend analysis.

142
00:09:01,729 --> 00:09:06,209
Whether you are sizing your GPU
clusters for next quarter's workloads,

143
00:09:06,479 --> 00:09:11,009
or preparing for upcoming model
retraining cycles, you need to.

144
00:09:11,819 --> 00:09:14,009
Analyze historical context, right?

145
00:09:14,249 --> 00:09:19,099
So it helps avoid over provisioning
and gives a factual basis

146
00:09:19,159 --> 00:09:20,629
for infrastructure budgeting.

147
00:09:21,349 --> 00:09:24,619
So next we move up to
resource utilization.

148
00:09:24,890 --> 00:09:29,479
So this is where the fine-grained
metrics come in, like the CPU usage,

149
00:09:29,780 --> 00:09:34,000
memory pressure, GPU utilization
at the process level, right?

150
00:09:34,270 --> 00:09:38,080
So this layer is all about visibility
into how your infrastructure.

151
00:09:38,590 --> 00:09:42,520
Is being consumed in real time
without this, you are essentially

152
00:09:42,690 --> 00:09:47,500
like flying blind about That is
we have the performance insights.

153
00:09:48,000 --> 00:09:52,640
So generic metrics like CPU usage
won't tell you how long a training

154
00:09:52,640 --> 00:09:56,700
model takes or when your data
pipeline is getting congested.

155
00:09:56,970 --> 00:10:02,130
So we create a custom dashboard that
reflects that ML specific metrics

156
00:10:02,580 --> 00:10:05,160
such as throughput or training loss.

157
00:10:05,485 --> 00:10:06,745
Over the period of time.

158
00:10:06,745 --> 00:10:08,905
And then the GPU memory fragmentation.

159
00:10:09,375 --> 00:10:13,205
So these these give the engineers
a much cleaner picture of how

160
00:10:13,205 --> 00:10:14,525
their workloads are behaving.

161
00:10:15,455 --> 00:10:20,105
At the top of the pyramid, if you
look at, is the root cause analysis.

162
00:10:20,525 --> 00:10:24,915
So this is the most advanced layer and
arguably the most valuable as well.

163
00:10:25,245 --> 00:10:26,295
So it.

164
00:10:26,595 --> 00:10:30,635
Ties everything together through
distributed tracing and correlation

165
00:10:31,005 --> 00:10:37,195
correlates across the layers, like linking
spikes in the GPU usages to a specific

166
00:10:37,195 --> 00:10:42,635
phase of a model training while something
breaks or slows down, this is how you

167
00:10:42,635 --> 00:10:44,825
rapidly isolate and resolve the issue.

168
00:10:45,815 --> 00:10:48,865
The systems we are talking
are not just about alerting.

169
00:10:49,445 --> 00:10:54,005
They're about creating a narrative
that ML engineers and SREs can follow.

170
00:10:54,395 --> 00:10:59,915
They support both issues at hand and
also work on strategic decision making.

171
00:11:00,575 --> 00:11:05,135
The most successful organizations
like built all four of these layers

172
00:11:05,135 --> 00:11:07,875
into their monitoring approach.

173
00:11:08,325 --> 00:11:10,575
Together they create a holistic feedback.

174
00:11:10,885 --> 00:11:16,385
That helps optimize models and improve
uptime as well as cost controls.

175
00:11:16,885 --> 00:11:22,665
Now let's explore how we can dramatically
improve, data to a reduced latency in the

176
00:11:23,025 --> 00:11:25,425
ML pipelines by optimizing the storage.

177
00:11:25,815 --> 00:11:30,375
So as machine learning workloads
become increasingly data intensive

178
00:11:30,645 --> 00:11:32,865
storage becomes a critical bottleneck.

179
00:11:33,165 --> 00:11:36,810
So this slides outlines like three
key strategies to overcome that.

180
00:11:37,520 --> 00:11:41,280
So if you look at the distributed
file systems we have file systems

181
00:11:41,280 --> 00:11:43,380
like GPFS, which are very distributed.

182
00:11:43,800 --> 00:11:48,000
These are built for high throughput
scenarios and are capable of parallel

183
00:11:48,000 --> 00:11:50,910
data processing across hundreds of nodes.

184
00:11:51,240 --> 00:11:55,830
So they outperform the traditional network
storage solutions by up to eight 10 x.

185
00:11:56,435 --> 00:12:01,015
Right, especially when working with
the small, fragmented file types

186
00:12:01,295 --> 00:12:03,665
which are common in the ML data sets.

187
00:12:04,205 --> 00:12:09,365
The design allows like efficient
aggregation of bandwidth and parallel

188
00:12:09,365 --> 00:12:13,385
read and writes, so which accelerates
the data ingestion during the training.

189
00:12:13,885 --> 00:12:16,985
Next, we deploy the
in-memory caching layers.

190
00:12:17,480 --> 00:12:22,870
Between the persistent storage and
compute resources these caching

191
00:12:22,870 --> 00:12:26,440
layers drastically reduce IO wait
times, especially for frequently

192
00:12:26,440 --> 00:12:28,870
accessed data sets by 65% or more.

193
00:12:29,370 --> 00:12:33,760
They also implement automatic eviction
policies ensuring that the optimal

194
00:12:33,760 --> 00:12:35,950
memory usage without manual intervention.

195
00:12:36,460 --> 00:12:40,850
This is particularly beneficial
in iterative workflows where

196
00:12:40,850 --> 00:12:42,900
the same data sets are are.

197
00:12:43,615 --> 00:12:46,105
Read multiple times
during the model tuning.

198
00:12:46,605 --> 00:12:49,815
Lastly, we have something called
the automated storage tiering.

199
00:12:50,325 --> 00:12:55,605
So this involves moving data between the
hot and cold storage tiers based on the

200
00:12:55,605 --> 00:12:58,305
access, frequency and performance needs.

201
00:12:58,635 --> 00:13:02,455
So this not only ensures
that the performance for the

202
00:13:02,455 --> 00:13:05,155
active data sets, but also.

203
00:13:05,155 --> 00:13:10,010
Achieves like the 40 to 50 percent
of cost reductions by shifting the

204
00:13:10,010 --> 00:13:12,710
inactive data to lower cost storage.

205
00:13:13,210 --> 00:13:16,680
This is done transparently
through a unified namespace.

206
00:13:16,860 --> 00:13:21,930
So engineers don't need to worry about
where the data physically rec resides.

207
00:13:22,500 --> 00:13:28,260
So together with these kind of three
layers, distributed file system, in

208
00:13:28,260 --> 00:13:32,510
memory caching and intelligent tiering
this create a robust, cost efficient

209
00:13:32,540 --> 00:13:35,120
foundation for data driven ML pipelines.

210
00:13:35,690 --> 00:13:39,530
The result is faster training,
lower cost, and smoother operations.

211
00:13:40,030 --> 00:13:44,520
Here we are looking at how we optimize
cloud spend for machine learning

212
00:13:44,520 --> 00:13:46,960
workloads using sport instances.

213
00:13:47,660 --> 00:13:50,030
We start with the
workload classifications.

214
00:13:50,390 --> 00:13:54,525
So we assess the jobs based on
the fall tolerance and run length

215
00:13:54,915 --> 00:13:59,335
to identify which, which can
safely run on the spot Instances.

216
00:13:59,885 --> 00:14:02,105
Next we implement like 4,000.

217
00:14:02,590 --> 00:14:06,020
By adding automated checkpoints
and recovery systems.

218
00:14:06,420 --> 00:14:10,620
So if a spot instance gets reclaimed,
the job can resume smoothly.

219
00:14:11,460 --> 00:14:16,380
Then comes the bidding strategy,
optimization izing, the historical

220
00:14:16,920 --> 00:14:18,780
and real time pricing data.

221
00:14:19,020 --> 00:14:23,010
We just are bidding to maximize the
savings while maintaining the job.

222
00:14:23,010 --> 00:14:23,850
Re reliability.

223
00:14:24,660 --> 00:14:28,450
Lastly, we adopt the
hybrid deployment models.

224
00:14:28,950 --> 00:14:33,870
So dynamic, clearly, like switching
between the spot and on demand instances

225
00:14:33,920 --> 00:14:35,600
based on cost and availability.

226
00:14:35,870 --> 00:14:39,705
So this is one of the important models
which we can deploy to make sure

227
00:14:39,705 --> 00:14:46,105
we enable that the cost savings of
up to 60 to 80% is achieved without

228
00:14:46,165 --> 00:14:48,175
even sacrificing on the performance.

229
00:14:48,675 --> 00:14:52,855
So moving on, let's talk about
optimizing the no pool configurations.

230
00:14:53,325 --> 00:14:57,165
So an area where we can unlock
substantial performance and cost

231
00:14:57,165 --> 00:15:01,305
benefits, especially in machine
learning and compute heavy environments.

232
00:15:01,905 --> 00:15:05,435
So first we start with
heterogeneous hardware segmentation.

233
00:15:05,855 --> 00:15:10,385
So rather than mixing all the GPUs,
GU types into a single nor pool.

234
00:15:11,025 --> 00:15:15,075
We create a specialized nor pools
for different GPUs, like eight

235
00:15:15,075 --> 00:15:16,875
hundreds going to one nor pool.

236
00:15:16,875 --> 00:15:20,175
And then the v hundreds go
to a different nor pool.

237
00:15:20,475 --> 00:15:23,865
So each has a different compute
and the memory characteristics.

238
00:15:24,075 --> 00:15:27,505
So assigning the workloads
without creating the nor pools

239
00:15:27,505 --> 00:15:29,765
may lead to and inefficiencies.

240
00:15:30,395 --> 00:15:36,125
So by using Ts and tolerations, we make
sure workloads are scheduled only to.

241
00:15:36,525 --> 00:15:38,085
Hardware data optimized for.

242
00:15:38,585 --> 00:15:40,835
Next we have resource optimization.

243
00:15:41,315 --> 00:15:46,385
This is about fine tuning the CPU and
the GPU as well as the memory ratios

244
00:15:46,385 --> 00:15:49,130
to match the ml workload profiles.

245
00:15:49,400 --> 00:15:54,220
For example inference might
require more CPU, whereas training

246
00:15:54,225 --> 00:15:56,290
model could need more GPU memory.

247
00:15:56,750 --> 00:15:57,155
So if.

248
00:15:57,590 --> 00:16:02,360
If these ratios are mislead or
miscalculated, we either end up with

249
00:16:02,360 --> 00:16:06,315
bottlenecks or underutilized hardware,
both of which are very costly.

250
00:16:06,815 --> 00:16:11,395
Then there is network topology alignment
where you'll you are running, when you're

251
00:16:11,395 --> 00:16:16,505
running your distributed training network
bandwidth, and latency matters a lot.

252
00:16:17,165 --> 00:16:20,820
So we design a no pools to align with.

253
00:16:21,320 --> 00:16:26,860
With the physical infrastructure, that
means like grouping into a nodes nodes

254
00:16:26,860 --> 00:16:32,370
with the high speed interconnects like
NV link or InfiniBand into a same pool.

255
00:16:33,060 --> 00:16:37,960
This ensures that the data exchanges
between the GPUs in a fast and consistent

256
00:16:37,960 --> 00:16:41,950
manner, which avoids the slowdowns
also during the training process.

257
00:16:42,450 --> 00:16:47,180
Now let's quickly look at how we can
share resources fairly when multiple

258
00:16:47,180 --> 00:16:48,920
teams are working on the same system.

259
00:16:49,870 --> 00:16:53,090
First we use namespace
to keep things organized.

260
00:16:53,330 --> 00:16:57,880
So each team or, type of work like
production versus a non-production

261
00:16:58,190 --> 00:16:59,690
gets its own namespace.

262
00:17:00,140 --> 00:17:04,540
So this way we can set up different
permissions use specific settings

263
00:17:04,540 --> 00:17:08,350
for each environment and keep
the network traffic separate.

264
00:17:08,740 --> 00:17:11,320
Second, we can set up
the resource controls.

265
00:17:11,710 --> 00:17:16,720
That means we limit how GPU memory
and storage each team can use.

266
00:17:17,110 --> 00:17:21,860
This helps, make, making sure like
team doesn't accidentally take

267
00:17:21,910 --> 00:17:24,400
more share than which is allocated.

268
00:17:24,900 --> 00:17:30,360
Then finally we give higher priority to
critical jobs like live production jobs.

269
00:17:30,640 --> 00:17:35,500
At the same time, we make sure like every
team gets at least a minimum resources

270
00:17:35,930 --> 00:17:37,550
which are needed for their operations.

271
00:17:37,850 --> 00:17:42,735
So this avoids the problem of one
team overutilizing the resources while

272
00:17:42,810 --> 00:17:44,280
the other team doesn't get anything.

273
00:17:44,940 --> 00:17:46,160
So the overall this setup.

274
00:17:46,985 --> 00:17:51,965
Keeps fair, reliable, and running the
infrastructure smoothly for everyone.

275
00:17:52,465 --> 00:17:55,940
Here's what an impact these
optimization strategies can have.

276
00:17:56,270 --> 00:17:59,660
So in one of the case studies, it
was observed that companies that

277
00:17:59,660 --> 00:18:04,020
applied this techniques saw an
average of 43% reduction in costs.

278
00:18:04,530 --> 00:18:10,515
That, near that is nearly half of their
infrastructure spending saved just by.

279
00:18:11,050 --> 00:18:13,540
Tuning the things right next.

280
00:18:13,820 --> 00:18:19,140
Training jobs became much faster,
like 2.8 times faster on average.

281
00:18:19,350 --> 00:18:23,870
That means models are trained
and ready in a bayless time which

282
00:18:23,870 --> 00:18:27,890
speeding up the entire development
cycle study also suggests that.

283
00:18:28,640 --> 00:18:35,390
There was a big jump in the GPU
usage, so which is up around 67%.

284
00:18:35,720 --> 00:18:40,880
So instead of having expensive
GPU sitting idle, they're being

285
00:18:40,880 --> 00:18:43,820
used efficiently across a cluster.

286
00:18:44,300 --> 00:18:49,100
And finally, the production models
became more reliable with 94% of the

287
00:18:49,100 --> 00:18:51,550
inherent requests, meeting the SLA goals.

288
00:18:52,120 --> 00:18:55,570
So that's a strong sign of
stable production ready systems.

289
00:18:56,070 --> 00:19:00,130
So this slides gives us a step
by step guide for applying the

290
00:19:00,130 --> 00:19:01,950
strategies we've covered so far.

291
00:19:02,460 --> 00:19:05,850
Step one is to establish
the baseline metrics.

292
00:19:06,120 --> 00:19:10,000
Like before making any changes,
we need to understand how our

293
00:19:10,000 --> 00:19:12,375
resources are being used right now.

294
00:19:12,725 --> 00:19:15,455
That includes tracking
the GPU and the CP usage.

295
00:19:15,920 --> 00:19:18,750
Training times and how much
it's costing up per model.

296
00:19:19,720 --> 00:19:22,840
Then step two is to go for the quick wins.

297
00:19:23,110 --> 00:19:25,260
Like there are changes that are not.

298
00:19:26,010 --> 00:19:30,480
Easy to be implemented and give
immediate value without disruption

299
00:19:30,510 --> 00:19:32,730
disrupting our current setup.

300
00:19:33,420 --> 00:19:37,860
So examples include resizing the resource
limits correctly, and then enabling

301
00:19:38,645 --> 00:19:43,665
simple auto scaling, and then using the
right storage classes for each workloads.

302
00:19:44,445 --> 00:19:47,865
Then step three is to deploy
more advanced optimizations.

303
00:19:48,330 --> 00:19:53,830
Once the basics are in place, we can
start using smarter tactics like custom

304
00:19:53,830 --> 00:19:58,400
autoscaling based on the real time
metrics using spot instances to cut

305
00:19:58,430 --> 00:20:03,530
the costs or even fine tuning the node
placement based on the hardware topology.

306
00:20:04,190 --> 00:20:09,170
And then finally step four is
continuously continuous refinement.

307
00:20:09,500 --> 00:20:10,970
So optimization is not.

308
00:20:11,480 --> 00:20:12,830
One, one time thing.

309
00:20:13,190 --> 00:20:16,910
So it needs to evolve as
workloads and technologies change.

310
00:20:17,090 --> 00:20:21,900
That means doing regular reviews,
setting up alerts for unusual costs,

311
00:20:22,260 --> 00:20:26,860
and making sure our infrastructure
and models are improved over over

312
00:20:26,860 --> 00:20:29,475
time which is very important, right?

313
00:20:29,840 --> 00:20:33,530
So in short, this roadmap helps
balance the quick efficiency gains

314
00:20:33,860 --> 00:20:35,640
with smart long-term planning.

315
00:20:36,140 --> 00:20:39,950
Finally, thank you for joining me in
this session and I hope you have a great

316
00:20:39,950 --> 00:20:41,870
time at this particular conference.

317
00:20:42,170 --> 00:20:42,560
Thank you.

318
00:20:43,060 --> 00:20:43,750
Hello everyone.

319
00:20:44,080 --> 00:20:45,850
Thank you for joining
me in this conference.

320
00:20:46,510 --> 00:20:49,900
Today we will discuss on how
FinTech reliability is achieved

321
00:20:49,900 --> 00:20:50,795
through SRE Innovation.

322
00:20:51,295 --> 00:20:55,825
So as organizations accelerate their
AI initiatives, many are discovering

323
00:20:55,825 --> 00:21:00,365
that traditional infrastructure was
not built to handle the high demands

324
00:21:00,365 --> 00:21:02,145
of machine learning operations.

325
00:21:02,625 --> 00:21:06,195
These workloads are highly resource
intensive and are unpredictable.

326
00:21:06,805 --> 00:21:11,155
While Kubernetes has emerged as
a platform of choice for managing

327
00:21:11,155 --> 00:21:14,575
these environments, simply running
the containers is not enough.

328
00:21:15,005 --> 00:21:17,615
The challenge lies in
running them optimally.

329
00:21:18,385 --> 00:21:23,245
What we are seeing is that many teams
are struggling, be it dealing with

330
00:21:23,335 --> 00:21:28,445
over-provisioned resources, unpredictable
costs and performance bottlenecks.

331
00:21:28,745 --> 00:21:33,585
So in this particular session, I would
like to walk you through a set of proven

332
00:21:33,585 --> 00:21:39,245
SRE strategies that are designed to
increase performance, reduce costs, and

333
00:21:39,245 --> 00:21:44,165
significantly improve the reliability
of the AI ML pipelines running in.

334
00:21:44,600 --> 00:21:48,810
Kubernetes these approaches come directly
from the high stakes environments

335
00:21:49,240 --> 00:21:53,920
these where even minor inefficiencies
can lead to a major financial losses.

336
00:21:54,550 --> 00:21:58,280
So together we'll go through the
techniques that have delivered up

337
00:21:58,280 --> 00:22:02,780
to a 60% improvements in resource
utilization and substantially

338
00:22:03,100 --> 00:22:05,040
reductions in incident rates.

339
00:22:05,475 --> 00:22:09,015
All while maintaining the
agility and performance.

340
00:22:09,515 --> 00:22:14,575
Ultimately, this presentation is about
more than just keeping the systems online.

341
00:22:14,875 --> 00:22:19,405
It's about engineering confidently
at scale in an increased AI

342
00:22:19,405 --> 00:22:20,965
driven financial ecosystem.

343
00:22:21,355 --> 00:22:25,425
So let's take a closer look on how
the transformation is happening.

344
00:22:25,925 --> 00:22:26,135
Okay.

345
00:22:26,135 --> 00:22:29,675
When we talk about scaling AI
infrastructure efficiently, GPU

346
00:22:29,675 --> 00:22:33,645
utilization is one of the most
overlooked and most expensive

347
00:22:33,765 --> 00:22:35,205
and has more bottlenecks.

348
00:22:35,415 --> 00:22:40,815
So let's look at how we can dramatically
shift that performance to cost ratio

349
00:22:41,215 --> 00:22:43,435
using smart GPU level strategies.

350
00:22:44,255 --> 00:22:47,765
Let's start with the MIG or the
multi-instance GPU Partitioning.

351
00:22:48,065 --> 00:22:52,835
So this is a feature available in
NVIDIA's 800 and h hundred series.

352
00:22:53,265 --> 00:22:58,915
It allows you to carve up a single GPU
into a multiple isolated instances,

353
00:22:59,275 --> 00:23:03,070
which enables like multiple workloads to
run simultaneously without any issues.

354
00:23:03,925 --> 00:23:09,125
Our real world tests have shown up to a
seven x better utilization especially in

355
00:23:09,125 --> 00:23:11,415
environments with varied inference loads.

356
00:23:12,405 --> 00:23:18,985
Next in in memory efficiency QDA provides
like a low level access to memory

357
00:23:18,985 --> 00:23:22,345
management, allowing us to opti optimize.

358
00:23:22,345 --> 00:23:25,385
Tensor core use and
reduce the fragmentation.

359
00:23:25,685 --> 00:23:31,265
So this can lead to 15 to 30% of
throughput gains particularly for

360
00:23:31,265 --> 00:23:35,565
vision heavy models like ID verification
or biometric fraud detection.

361
00:23:36,065 --> 00:23:38,225
Oh, coming to precision optimization.

362
00:23:38,285 --> 00:23:43,045
In many FinTech companies like ML
workloads, especially during inference.

363
00:23:43,545 --> 00:23:46,785
Full FFP 32 precision
is not always needed.

364
00:23:47,085 --> 00:23:54,285
So by switching the, switching it to FP
16 or even INT eight, organizations can

365
00:23:54,285 --> 00:23:56,535
drastically shrink the memory footprint.

366
00:23:57,015 --> 00:23:58,785
So these techniques are game changers.

367
00:23:58,975 --> 00:24:03,885
They maximize the GPU resource usage
and provide critical improvements in

368
00:24:03,885 --> 00:24:06,445
both performance and cost efficiency.

369
00:24:06,945 --> 00:24:08,205
Moving on to the next slide.

370
00:24:08,205 --> 00:24:12,545
On auto-scaling strategies, as
organizations push to run ML workloads

371
00:24:12,545 --> 00:24:16,825
more efficiently traditional auto-scaling
strategies are not enough, especially

372
00:24:16,825 --> 00:24:18,625
in GPU accelerated environments.

373
00:24:18,895 --> 00:24:24,325
Here we'll go through four techniques
that enable smarter ML strategies.

374
00:24:24,665 --> 00:24:26,195
Which will help with the autoscaling.

375
00:24:27,165 --> 00:24:29,055
First we'll go through
the workload profiling.

376
00:24:29,445 --> 00:24:34,575
So the foundation of autoscaling is
understanding when and how much to scale.

377
00:24:34,935 --> 00:24:38,895
So workload profiling uses the
historical telemetry across both

378
00:24:38,895 --> 00:24:40,995
training and inference cycles.

379
00:24:41,585 --> 00:24:43,385
To uncover the usage patterns.

380
00:24:43,775 --> 00:24:48,945
So this helps define the baseline
resource needs, identify the spikes

381
00:24:48,945 --> 00:24:52,965
during the market, events or frauds,
fraud searches, and ensures that

382
00:24:52,965 --> 00:24:57,105
the auto-scaling decisions are
data informed and are not reactive.

383
00:24:57,605 --> 00:25:00,005
Coming to the custom metrics pipeline.

384
00:25:00,485 --> 00:25:04,995
So if you look at the out of box
Kubernetes like Kubernetes HPA doesn't

385
00:25:05,145 --> 00:25:07,125
know anything about the machine learning.

386
00:25:07,455 --> 00:25:12,205
That's where like the custom metrics
come in using Prometheus adapters.

387
00:25:12,255 --> 00:25:18,255
We expose the ml. Relevant signals like
the Q Depth batch processing duration,

388
00:25:18,255 --> 00:25:21,485
and GPU memory pressure to the autoscaler.

389
00:25:21,845 --> 00:25:28,005
So this enables a precise decision based
on the actual workload characteristics,

390
00:25:28,185 --> 00:25:30,915
not just the CPU load or the pod count.

391
00:25:31,415 --> 00:25:35,705
Coming to predictive scaling for
recurring or seasonal workloads.

392
00:25:36,065 --> 00:25:40,765
Think EOD model model retaining
or the batch scoring jobs.

393
00:25:41,155 --> 00:25:45,115
We can apply the time-based
or ML based scaling triggers.

394
00:25:45,445 --> 00:25:49,015
So predictive models can anticipate
the spikes before they happen.

395
00:25:49,560 --> 00:25:52,530
Enabling warm pools to
be spun up just in time.

396
00:25:52,900 --> 00:25:58,810
This reduces the cold start latency
by up to 85%, especially valuable

397
00:25:58,840 --> 00:26:00,900
for real time FinTech systems.

398
00:26:01,720 --> 00:26:06,320
Coming to the buffer management we
can't forget the cluster resilience.

399
00:26:06,565 --> 00:26:11,025
So the buffer management means
deliberately over provisioning a

400
00:26:11,025 --> 00:26:16,335
small GPU headroom during critical
critical hours like trading sessions

401
00:26:16,335 --> 00:26:19,095
or things like holiday season, right?

402
00:26:19,155 --> 00:26:22,775
Then you can scale them back down
more aggressively after the hours.

403
00:26:23,520 --> 00:26:28,000
This balances the control cost control
with high availability, which is

404
00:26:28,000 --> 00:26:30,580
non-negotiable for financial applications.

405
00:26:31,090 --> 00:26:35,850
So these autoscaling strategies
go well beyond the HPA setup.

406
00:26:36,230 --> 00:26:40,260
When implemented together, they enable
the organizations to sustain peak

407
00:26:40,260 --> 00:26:43,015
model performance with minimal costs.

408
00:26:43,515 --> 00:26:43,725
Okay.

409
00:26:43,725 --> 00:26:47,875
Coming to the multi GPU training
orchestration the biggest bottlenecks

410
00:26:47,875 --> 00:26:54,315
in scaling a machine learning training
right is GPU job orchestrations.

411
00:26:54,595 --> 00:26:57,550
It's not just having
about having more GPUs.

412
00:26:58,155 --> 00:27:00,315
It's about using them in intelligently.

413
00:27:00,675 --> 00:27:04,515
When we orchestrate jobs sufficiently
across the gps, especially in

414
00:27:04,515 --> 00:27:09,725
distributed environments, we unlock,
huge throughput gains in cost efficiency.

415
00:27:10,215 --> 00:27:12,895
The first step is
topology aware scheduling.

416
00:27:13,315 --> 00:27:14,395
Why does this matter?

417
00:27:14,455 --> 00:27:20,395
So when jobs span across multiple
GPUs, especially across nodes network

418
00:27:20,395 --> 00:27:25,755
latency becomes c enemy like we solve
this by using the g Kubernetes node

419
00:27:25,755 --> 00:27:30,555
affinity rules to schedule the jobs on
nodes that are physically close to the

420
00:27:30,915 --> 00:27:33,075
interconnect through high bandwidth.

421
00:27:33,500 --> 00:27:36,330
Links like NV link, or PCIE.

422
00:27:36,880 --> 00:27:40,150
This minimizes the communication
overhead and drastically improves the

423
00:27:40,150 --> 00:27:42,240
performance and distributed training.

424
00:27:42,740 --> 00:27:45,320
Next we move on to distributed
training frameworks.

425
00:27:45,680 --> 00:27:49,520
So framework for frameworks
like TensorFlows multi

426
00:27:49,520 --> 00:27:51,870
worker needs to be tuned.

427
00:27:52,255 --> 00:27:56,625
To our infrastructure, often we
need poor performance because

428
00:27:56,685 --> 00:27:58,365
training scripts are not optimized.

429
00:27:59,015 --> 00:28:03,845
So we address this with Custom
gate template that enable, like

430
00:28:03,905 --> 00:28:07,685
dynamic scaling, reduces the
training time variance, and

431
00:28:07,685 --> 00:28:10,565
it minimizes the communication
bottlenecks between the nodes.

432
00:28:11,020 --> 00:28:15,045
Finally, there's a priority
based preemptive, mechanism.

433
00:28:15,345 --> 00:28:18,790
So in this particular technique if
you look at our shared clusters.

434
00:28:19,565 --> 00:28:24,965
When multiple teams submit the jobs,
it's critical to ensure that high

435
00:28:25,015 --> 00:28:29,185
priority jobs don't wait in line
behind the rule, low priority ones.

436
00:28:29,635 --> 00:28:33,905
So we implement intelligent
queuing mechanisms that evaluate

437
00:28:33,905 --> 00:28:35,615
the job priority and then.

438
00:28:35,680 --> 00:28:38,050
Deadlines and then resource fairness.

439
00:28:38,290 --> 00:28:42,700
It's like air traffic control for
GPUs, like preventing the collisions,

440
00:28:42,700 --> 00:28:46,540
reducing the idle time, and keeping
the system fair and efficient.

441
00:28:47,040 --> 00:28:52,170
The key takeaway is like when you bring
all these three pillars together, the

442
00:28:52,170 --> 00:28:57,080
topology awareness, smart frameworks,
and priority based scheduling you can

443
00:28:57,080 --> 00:28:59,990
cut the training time by 30 to 50%.

444
00:29:00,500 --> 00:29:04,640
That's a massive impact when you are
running the running hundreds or even

445
00:29:04,640 --> 00:29:06,890
thousands of experiments each week.

446
00:29:07,390 --> 00:29:07,720
Okay.

447
00:29:07,720 --> 00:29:10,210
Moving on to the advanced
monitoring and observability.

448
00:29:10,450 --> 00:29:15,380
As machine learning workloads grow in
complexity and scale, the need for robust

449
00:29:15,440 --> 00:29:17,660
observability becomes absolutely critical.

450
00:29:18,210 --> 00:29:21,810
Not just for performance, but
for cost efficiency and op

451
00:29:22,020 --> 00:29:23,340
and operational reliability.

452
00:29:23,880 --> 00:29:24,360
So this.

453
00:29:25,270 --> 00:29:28,390
Here outlines the layered approach.

454
00:29:28,445 --> 00:29:31,975
We take towards building a
comprehensive monitoring stack for

455
00:29:31,975 --> 00:29:34,095
the machine learning infrastructure.

456
00:29:34,395 --> 00:29:37,845
So let's start at the
foundation, the historical data.

457
00:29:38,220 --> 00:29:42,340
So long-term storage of metrics
is essential for capacity

458
00:29:42,340 --> 00:29:43,960
planning and trend analysis.

459
00:29:44,290 --> 00:29:48,770
Whether you are sizing your GPU
clusters for next quarter's workloads,

460
00:29:49,040 --> 00:29:53,570
or preparing for upcoming model
retraining cycles, you need to.

461
00:29:54,380 --> 00:29:56,570
Analyze historical context, right?

462
00:29:56,810 --> 00:30:01,660
So it helps avoid over provisioning
and gives a factual basis

463
00:30:01,720 --> 00:30:03,190
for infrastructure budgeting.

464
00:30:03,910 --> 00:30:07,180
So next we move up to
resource utilization.

465
00:30:07,450 --> 00:30:12,040
So this is where the fine-grained
metrics come in, like the CPU usage,

466
00:30:12,340 --> 00:30:16,560
memory pressure, GPU utilization
at the process level, right?

467
00:30:16,830 --> 00:30:20,640
So this layer is all about visibility
into how your infrastructure.

468
00:30:21,150 --> 00:30:25,080
Is being consumed in real time
without this, you are essentially

469
00:30:25,250 --> 00:30:30,060
like flying blind about That is
we have the performance insights.

470
00:30:30,560 --> 00:30:35,200
So generic metrics like CPU usage
won't tell you how long a training

471
00:30:35,200 --> 00:30:39,260
model takes or when your data
pipeline is getting congested.

472
00:30:39,530 --> 00:30:44,690
So we create a custom dashboard that
reflects that ML specific metrics

473
00:30:45,140 --> 00:30:47,720
such as throughput or training loss.

474
00:30:48,045 --> 00:30:49,305
Over the period of time.

475
00:30:49,305 --> 00:30:51,465
And then the GPU memory fragmentation.

476
00:30:51,935 --> 00:30:55,765
So these these give the engineers
a much cleaner picture of how

477
00:30:55,765 --> 00:30:57,085
their workloads are behaving.

478
00:30:58,015 --> 00:31:02,665
At the top of the pyramid, if you
look at, is the root cause analysis.

479
00:31:03,085 --> 00:31:07,475
So this is the most advanced layer and
arguably the most valuable as well.

480
00:31:07,805 --> 00:31:08,855
So it.

481
00:31:09,155 --> 00:31:13,195
Ties everything together through
distributed tracing and correlation

482
00:31:13,565 --> 00:31:19,755
correlates across the layers, like linking
spikes in the GPU usages to a specific

483
00:31:19,755 --> 00:31:25,195
phase of a model training while something
breaks or slows down, this is how you

484
00:31:25,195 --> 00:31:27,385
rapidly isolate and resolve the issue.

485
00:31:28,375 --> 00:31:31,425
The systems we are talking
are not just about alerting.

486
00:31:32,005 --> 00:31:36,565
They're about creating a narrative
that ML engineers and SREs can follow.

487
00:31:36,955 --> 00:31:42,475
They support both issues at hand and
also work on strategic decision making.

488
00:31:43,135 --> 00:31:47,695
The most successful organizations
like built all four of these layers

489
00:31:47,695 --> 00:31:50,435
into their monitoring approach.

490
00:31:50,885 --> 00:31:53,135
Together they create a holistic feedback.

491
00:31:53,445 --> 00:31:58,945
That helps optimize models and improve
uptime as well as cost controls.

492
00:31:59,445 --> 00:32:05,225
Now let's explore how we can dramatically
improve, data to a reduced latency in the

493
00:32:05,585 --> 00:32:07,985
ML pipelines by optimizing the storage.

494
00:32:08,375 --> 00:32:12,935
So as machine learning workloads
become increasingly data intensive

495
00:32:13,205 --> 00:32:15,425
storage becomes a critical bottleneck.

496
00:32:15,725 --> 00:32:19,370
So this slides outlines like three
key strategies to overcome that.

497
00:32:20,080 --> 00:32:23,840
So if you look at the distributed
file systems we have file systems

498
00:32:23,840 --> 00:32:25,940
like GPFS, which are very distributed.

499
00:32:26,360 --> 00:32:30,560
These are built for high throughput
scenarios and are capable of parallel

500
00:32:30,560 --> 00:32:33,470
data processing across hundreds of nodes.

501
00:32:33,800 --> 00:32:38,390
So they outperform the traditional network
storage solutions by up to eight 10 x.

502
00:32:38,995 --> 00:32:43,575
Right, especially when working with
the small, fragmented file types

503
00:32:43,855 --> 00:32:46,225
which are common in the ML data sets.

504
00:32:46,765 --> 00:32:51,925
The design allows like efficient
aggregation of bandwidth and parallel

505
00:32:51,925 --> 00:32:55,945
read and writes, so which accelerates
the data ingestion during the training.

506
00:32:56,445 --> 00:32:59,545
Next, we deploy the
in-memory caching layers.

507
00:33:00,040 --> 00:33:05,430
Between the persistent storage and
compute resources these caching

508
00:33:05,430 --> 00:33:09,000
layers drastically reduce IO wait
times, especially for frequently

509
00:33:09,000 --> 00:33:11,430
accessed data sets by 65% or more.

510
00:33:11,930 --> 00:33:16,320
They also implement automatic eviction
policies ensuring that the optimal

511
00:33:16,320 --> 00:33:18,510
memory usage without manual intervention.

512
00:33:19,020 --> 00:33:23,410
This is particularly beneficial
in iterative workflows where

513
00:33:23,410 --> 00:33:25,460
the same data sets are are.

514
00:33:26,175 --> 00:33:28,665
Read multiple times
during the model tuning.

515
00:33:29,165 --> 00:33:32,375
Lastly, we have something called
the automated storage tiering.

516
00:33:32,885 --> 00:33:38,165
So this involves moving data between the
hot and cold storage tiers based on the

517
00:33:38,165 --> 00:33:40,865
access, frequency and performance needs.

518
00:33:41,195 --> 00:33:45,015
So this not only ensures
that the performance for the

519
00:33:45,015 --> 00:33:47,715
active data sets, but also.

520
00:33:47,715 --> 00:33:52,570
Achieves like the 40 to 50 percent
of cost reductions by shifting the

521
00:33:52,570 --> 00:33:55,270
inactive data to lower cost storage.

522
00:33:55,770 --> 00:33:59,240
This is done transparently
through a unified namespace.

523
00:33:59,420 --> 00:34:04,490
So engineers don't need to worry about
where the data physically rec resides.

524
00:34:05,060 --> 00:34:10,820
So together with these kind of three
layers, distributed file system, in

525
00:34:10,820 --> 00:34:15,070
memory caching and intelligent tiering
this create a robust, cost efficient

526
00:34:15,100 --> 00:34:17,680
foundation for data driven ML pipelines.

527
00:34:18,250 --> 00:34:22,090
The result is faster training,
lower cost, and smoother operations.

528
00:34:22,590 --> 00:34:27,080
Here we are looking at how we optimize
cloud spend for machine learning

529
00:34:27,080 --> 00:34:29,520
workloads using sport instances.

530
00:34:30,220 --> 00:34:32,590
We start with the
workload classifications.

531
00:34:32,950 --> 00:34:37,085
So we assess the jobs based on
the fall tolerance and run length

532
00:34:37,475 --> 00:34:41,895
to identify which, which can
safely run on the spot Instances.

533
00:34:42,445 --> 00:34:44,665
Next we implement like 4,000.

534
00:34:45,150 --> 00:34:48,580
By adding automated checkpoints
and recovery systems.

535
00:34:48,980 --> 00:34:53,180
So if a spot instance gets reclaimed,
the job can resume smoothly.

536
00:34:54,020 --> 00:34:58,940
Then comes the bidding strategy,
optimization izing, the historical

537
00:34:59,480 --> 00:35:01,340
and real time pricing data.

538
00:35:01,580 --> 00:35:05,570
We just are bidding to maximize the
savings while maintaining the job.

539
00:35:05,570 --> 00:35:06,410
Re reliability.

540
00:35:07,220 --> 00:35:11,010
Lastly, we adopt the
hybrid deployment models.

541
00:35:11,510 --> 00:35:16,430
So dynamic, clearly, like switching
between the spot and on demand instances

542
00:35:16,480 --> 00:35:18,160
based on cost and availability.

543
00:35:18,430 --> 00:35:22,265
So this is one of the important models
which we can deploy to make sure

544
00:35:22,265 --> 00:35:28,665
we enable that the cost savings of
up to 60 to 80% is achieved without

545
00:35:28,725 --> 00:35:30,735
even sacrificing on the performance.

546
00:35:31,235 --> 00:35:35,415
So moving on, let's talk about
optimizing the no pool configurations.

547
00:35:35,885 --> 00:35:39,725
So an area where we can unlock
substantial performance and cost

548
00:35:39,725 --> 00:35:43,865
benefits, especially in machine
learning and compute heavy environments.

549
00:35:44,465 --> 00:35:47,995
So first we start with
heterogeneous hardware segmentation.

550
00:35:48,415 --> 00:35:52,945
So rather than mixing all the GPUs,
GU types into a single nor pool.

551
00:35:53,585 --> 00:35:57,635
We create a specialized nor pools
for different GPUs, like eight

552
00:35:57,635 --> 00:35:59,435
hundreds going to one nor pool.

553
00:35:59,435 --> 00:36:02,735
And then the v hundreds go
to a different nor pool.

554
00:36:03,035 --> 00:36:06,425
So each has a different compute
and the memory characteristics.

555
00:36:06,635 --> 00:36:10,065
So assigning the workloads
without creating the nor pools

556
00:36:10,065 --> 00:36:12,325
may lead to and inefficiencies.

557
00:36:12,955 --> 00:36:18,685
So by using Ts and tolerations, we make
sure workloads are scheduled only to.

558
00:36:19,085 --> 00:36:20,645
Hardware data optimized for.

559
00:36:21,145 --> 00:36:23,395
Next we have resource optimization.

560
00:36:23,875 --> 00:36:28,945
This is about fine tuning the CPU and
the GPU as well as the memory ratios

561
00:36:28,945 --> 00:36:31,690
to match the ml workload profiles.

562
00:36:31,960 --> 00:36:36,780
For example inference might
require more CPU, whereas training

563
00:36:36,785 --> 00:36:38,850
model could need more GPU memory.

564
00:36:39,310 --> 00:36:39,715
So if.

565
00:36:40,150 --> 00:36:44,920
If these ratios are mislead or
miscalculated, we either end up with

566
00:36:44,920 --> 00:36:48,875
bottlenecks or underutilized hardware,
both of which are very costly.

567
00:36:49,375 --> 00:36:53,955
Then there is network topology alignment
where you'll you are running, when you're

568
00:36:53,955 --> 00:36:59,065
running your distributed training network
bandwidth, and latency matters a lot.

569
00:36:59,725 --> 00:37:03,380
So we design a no pools to align with.

570
00:37:03,880 --> 00:37:09,420
With the physical infrastructure, that
means like grouping into a nodes nodes

571
00:37:09,420 --> 00:37:14,930
with the high speed interconnects like
NV link or InfiniBand into a same pool.

572
00:37:15,620 --> 00:37:20,520
This ensures that the data exchanges
between the GPUs in a fast and consistent

573
00:37:20,520 --> 00:37:24,510
manner, which avoids the slowdowns
also during the training process.

574
00:37:25,010 --> 00:37:29,740
Now let's quickly look at how we can
share resources fairly when multiple

575
00:37:29,740 --> 00:37:31,480
teams are working on the same system.

576
00:37:32,430 --> 00:37:35,650
First we use namespace
to keep things organized.

577
00:37:35,890 --> 00:37:40,440
So each team or, type of work like
production versus a non-production

578
00:37:40,750 --> 00:37:42,250
gets its own namespace.

579
00:37:42,700 --> 00:37:47,100
So this way we can set up different
permissions use specific settings

580
00:37:47,100 --> 00:37:50,910
for each environment and keep
the network traffic separate.

581
00:37:51,300 --> 00:37:53,880
Second, we can set up
the resource controls.

582
00:37:54,270 --> 00:37:59,280
That means we limit how GPU memory
and storage each team can use.

583
00:37:59,670 --> 00:38:04,420
This helps, make, making sure like
team doesn't accidentally take

584
00:38:04,470 --> 00:38:06,960
more share than which is allocated.

585
00:38:07,460 --> 00:38:12,920
Then finally we give higher priority to
critical jobs like live production jobs.

586
00:38:13,200 --> 00:38:18,060
At the same time, we make sure like every
team gets at least a minimum resources

587
00:38:18,490 --> 00:38:20,110
which are needed for their operations.

588
00:38:20,410 --> 00:38:25,295
So this avoids the problem of one
team overutilizing the resources while

589
00:38:25,370 --> 00:38:26,840
the other team doesn't get anything.

590
00:38:27,500 --> 00:38:28,720
So the overall this setup.

591
00:38:29,545 --> 00:38:34,525
Keeps fair, reliable, and running the
infrastructure smoothly for everyone.

592
00:38:35,025 --> 00:38:38,500
Here's what an impact these
optimization strategies can have.

593
00:38:38,830 --> 00:38:42,220
So in one of the case studies, it
was observed that companies that

594
00:38:42,220 --> 00:38:46,580
applied this techniques saw an
average of 43% reduction in costs.

595
00:38:47,090 --> 00:38:53,075
That, near that is nearly half of their
infrastructure spending saved just by.

596
00:38:53,610 --> 00:38:56,100
Tuning the things right next.

597
00:38:56,380 --> 00:39:01,700
Training jobs became much faster,
like 2.8 times faster on average.

598
00:39:01,910 --> 00:39:06,430
That means models are trained
and ready in a bayless time which

599
00:39:06,430 --> 00:39:10,450
speeding up the entire development
cycle study also suggests that.

600
00:39:11,200 --> 00:39:17,950
There was a big jump in the GPU
usage, so which is up around 67%.

601
00:39:18,280 --> 00:39:23,440
So instead of having expensive
GPU sitting idle, they're being

602
00:39:23,440 --> 00:39:26,380
used efficiently across a cluster.

603
00:39:26,860 --> 00:39:31,660
And finally, the production models
became more reliable with 94% of the

604
00:39:31,660 --> 00:39:34,110
inherent requests, meeting the SLA goals.

605
00:39:34,680 --> 00:39:38,130
So that's a strong sign of
stable production ready systems.

606
00:39:38,630 --> 00:39:42,690
So this slides gives us a step
by step guide for applying the

607
00:39:42,690 --> 00:39:44,510
strategies we've covered so far.

608
00:39:45,020 --> 00:39:48,410
Step one is to establish
the baseline metrics.

609
00:39:48,680 --> 00:39:52,560
Like before making any changes,
we need to understand how our

610
00:39:52,560 --> 00:39:54,935
resources are being used right now.

611
00:39:55,285 --> 00:39:58,015
That includes tracking
the GPU and the CP usage.

612
00:39:58,480 --> 00:40:01,310
Training times and how much
it's costing up per model.

613
00:40:02,280 --> 00:40:05,400
Then step two is to go for the quick wins.

614
00:40:05,670 --> 00:40:07,820
Like there are changes that are not.

615
00:40:08,570 --> 00:40:13,040
Easy to be implemented and give
immediate value without disruption

616
00:40:13,070 --> 00:40:15,290
disrupting our current setup.

617
00:40:15,980 --> 00:40:20,420
So examples include resizing the resource
limits correctly, and then enabling

618
00:40:21,205 --> 00:40:26,225
simple auto scaling, and then using the
right storage classes for each workloads.

619
00:40:27,005 --> 00:40:30,425
Then step three is to deploy
more advanced optimizations.

620
00:40:30,890 --> 00:40:36,390
Once the basics are in place, we can
start using smarter tactics like custom

621
00:40:36,390 --> 00:40:40,960
autoscaling based on the real time
metrics using spot instances to cut

622
00:40:40,990 --> 00:40:46,090
the costs or even fine tuning the node
placement based on the hardware topology.

623
00:40:46,750 --> 00:40:51,730
And then finally step four is
continuously continuous refinement.

624
00:40:52,060 --> 00:40:53,530
So optimization is not.

625
00:40:54,040 --> 00:40:55,390
One, one time thing.

626
00:40:55,750 --> 00:40:59,470
So it needs to evolve as
workloads and technologies change.

627
00:40:59,650 --> 00:41:04,460
That means doing regular reviews,
setting up alerts for unusual costs,

628
00:41:04,820 --> 00:41:09,420
and making sure our infrastructure
and models are improved over over

629
00:41:09,420 --> 00:41:12,035
time which is very important, right?

630
00:41:12,400 --> 00:41:16,090
So in short, this roadmap helps
balance the quick efficiency gains

631
00:41:16,420 --> 00:41:18,200
with smart long-term planning.

632
00:41:18,700 --> 00:41:22,510
Finally, thank you for joining me in
this session and I hope you have a great

633
00:41:22,510 --> 00:41:24,430
time at this particular conference.

634
00:41:24,730 --> 00:41:25,120
Thank you.

