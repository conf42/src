1
00:00:00,500 --> 00:00:01,190
Hello everyone.

2
00:00:01,489 --> 00:00:02,239
I'm Manke.

3
00:00:02,420 --> 00:00:06,770
I currently work at Twilio as a director
of engineering, where I lead teams working

4
00:00:06,770 --> 00:00:09,320
on predictive and generative AI products.

5
00:00:09,820 --> 00:00:14,590
I'm excited to talk with you about how we
can enhance performance of large language

6
00:00:14,590 --> 00:00:17,110
models by incorporating contextual data.

7
00:00:17,770 --> 00:00:21,610
We'll also discuss how this approach
moves is beyond traditional pre-training

8
00:00:21,610 --> 00:00:25,960
methods, creating much more powerful
and personalized AI experiences.

9
00:00:26,460 --> 00:00:30,210
So first, let's start by exploring
the strength of large language models.

10
00:00:31,170 --> 00:00:35,010
These models have impressive capabilities
when it comes to generating content.

11
00:00:35,510 --> 00:00:40,309
They excel in conversational inter
interactions, making user interfaces

12
00:00:40,309 --> 00:00:41,944
feel very natural and intuitive.

13
00:00:42,444 --> 00:00:43,664
In fact, this interface.

14
00:00:44,299 --> 00:00:48,319
Has been key to democratizing AI
across different user personas.

15
00:00:48,819 --> 00:00:51,459
A prompt is now valent of a model.

16
00:00:51,959 --> 00:00:55,649
They're also incredibly versatile
and performed remarkably well across

17
00:00:55,649 --> 00:01:00,059
various tasks, including summarizations,
generating quote snippets, crafting

18
00:01:00,059 --> 00:01:04,679
compelling copywriting, and more
recently even advanced reasoning tasks.

19
00:01:05,179 --> 00:01:11,419
Despite these impressive strengths, LLMs
also come with some critical limitations.

20
00:01:11,919 --> 00:01:16,599
For one, they're primarily
tamed on static data sets.

21
00:01:17,019 --> 00:01:20,320
This means the information
they have access to can

22
00:01:20,320 --> 00:01:21,999
become outdated or incomplete.

23
00:01:22,499 --> 00:01:26,429
Additionally, LLM sometimes generate
incorrect or fabricated information,

24
00:01:26,999 --> 00:01:28,954
what we often refer to as hallucinations.

25
00:01:29,454 --> 00:01:35,004
Perhaps most importantly, without
contextual informations, LLM struggle

26
00:01:35,004 --> 00:01:40,374
to deliver genuinely personalized
responses limiting their effectiveness

27
00:01:40,374 --> 00:01:41,844
in many real world scenarios.

28
00:01:42,344 --> 00:01:45,794
Let's look at a practical example of
a support chat bot to illustrate this

29
00:01:45,794 --> 00:01:48,284
issue without appropriate context.

30
00:01:48,404 --> 00:01:53,174
A large language model might
respond inaccurately or irrelevantly

31
00:01:53,174 --> 00:01:56,914
because it doesn't have access to
the necessary information to tailor

32
00:01:56,914 --> 00:02:00,739
its response Effectively, this
limitation underscores the importance

33
00:02:00,739 --> 00:02:02,339
of incorporating contextual data.

34
00:02:02,839 --> 00:02:05,359
The solution to this issue is
generally called retrieval,

35
00:02:05,359 --> 00:02:06,859
augmented Generation, or rag.

36
00:02:07,369 --> 00:02:09,919
The core idea behind RAG
is pretty straightforward.

37
00:02:10,879 --> 00:02:14,329
The idea is to retrieve relevant
contextual information from various data

38
00:02:14,329 --> 00:02:18,859
sources and integrate it into the prompts
we give to our large language models.

39
00:02:19,639 --> 00:02:25,099
This process alone significantly improved
the models responses, making them a lot

40
00:02:25,099 --> 00:02:26,989
more accurate, timely, and personalized.

41
00:02:27,489 --> 00:02:31,359
Rag also enables the
inclusion of current data.

42
00:02:31,739 --> 00:02:36,139
it can help inclusion of
specialized domain knowledge, even

43
00:02:36,139 --> 00:02:39,499
proprietary knowledge basis can be
included as part of the prompts.

44
00:02:39,999 --> 00:02:45,399
And most important in context of
today's discussion is its ability

45
00:02:45,399 --> 00:02:50,559
to add custom, personal, contextual
information into the prompt to drive

46
00:02:50,559 --> 00:02:52,479
hyper-personalized AI interactions.

47
00:02:52,979 --> 00:02:58,439
However, as we've seen this example,
just simply combining LLMs with a very

48
00:02:58,439 --> 00:03:03,809
generic knowledge base isn't enough to
drive depersonalization users expected.

49
00:03:04,619 --> 00:03:05,999
We need something more targeted.

50
00:03:06,209 --> 00:03:10,559
For instance, if you look at this
example, even though the sup, the AI

51
00:03:10,559 --> 00:03:16,199
support agent has access to documents
relating to the return policy.

52
00:03:16,574 --> 00:03:20,474
It doesn't have the personalized
context of the user leading to a

53
00:03:20,474 --> 00:03:22,364
somewhat poor customer experience.

54
00:03:22,864 --> 00:03:27,544
And this brings us to the concept
of customer data platforms or CDPs.

55
00:03:28,044 --> 00:03:31,104
We'll see how CDPs, which are
traditionally known for marketing

56
00:03:31,104 --> 00:03:36,894
use cases, can enable personalization
at scale effectively by providing

57
00:03:37,104 --> 00:03:41,244
rich, personalized data that can
be used directly by AI systems.

58
00:03:41,744 --> 00:03:46,814
And before we dig into more details
on how A CDP can be a powerful source

59
00:03:46,814 --> 00:03:50,804
of contextual information, let's
first understand at a high level what

60
00:03:50,804 --> 00:03:53,174
exactly is a customer data platform.

61
00:03:53,674 --> 00:04:00,544
In simple terms, A CDP is a sophisticated
tool that gathers and organizes

62
00:04:00,544 --> 00:04:04,414
customer data from a variety of
different sources and interactions.

63
00:04:04,789 --> 00:04:08,939
And send it, sends it to,
downstream apps and destinations.

64
00:04:09,439 --> 00:04:13,349
The sources could be your web
app, your chat bot, your, POS

65
00:04:13,349 --> 00:04:15,119
systems or even your warehouse.

66
00:04:15,619 --> 00:04:18,589
And the destination could be the
apps where you want to activate this

67
00:04:18,589 --> 00:04:22,459
data, such as marketing automation
platforms, ad destinations, et cetera.

68
00:04:22,959 --> 00:04:27,669
CDP is increasingly also create
a unified and very comprehensive

69
00:04:27,669 --> 00:04:29,619
view of each individual customer.

70
00:04:29,949 --> 00:04:35,829
enabling businesses to understand the 360
degree view of each customer in real time.

71
00:04:36,329 --> 00:04:39,929
There are also nuances like identity
resolution across different platform

72
00:04:40,619 --> 00:04:43,199
that help build rich user profiles.

73
00:04:43,699 --> 00:04:45,679
Let's see how this would work in action.

74
00:04:46,179 --> 00:04:48,669
So as the user interacts
with different applications.

75
00:04:49,464 --> 00:04:54,234
The user profile gets hydrated,
allowing for deeper personalization for

76
00:04:54,234 --> 00:04:59,814
subsequent interactions, and now in this
example, you can also see how the chat

77
00:04:59,814 --> 00:05:04,434
experience also becomes very seamless
as the profile gets richer and richer.

78
00:05:04,934 --> 00:05:09,079
So what we are seeing is that in
the context of AI applications.

79
00:05:09,754 --> 00:05:13,354
CDP can act as a very powerful
personalization engine.

80
00:05:13,924 --> 00:05:17,824
It allows AI applications to
access relevant and personalized

81
00:05:17,824 --> 00:05:22,504
data effortlessly, and thereby
significantly enhancing the user

82
00:05:22,504 --> 00:05:25,864
experience as we see with the example
that we have been taking so far.

83
00:05:26,364 --> 00:05:28,614
Depending on the context
of the information, all you

84
00:05:28,614 --> 00:05:31,704
need to do is pull relevant
information from the user profile.

85
00:05:32,124 --> 00:05:36,744
Add to the prompt for the large language
models and then you're able to generate

86
00:05:36,744 --> 00:05:38,634
a much more personalized response.

87
00:05:39,134 --> 00:05:43,064
In fact, this is very similar to how
the memory feature in chat GPT works.

88
00:05:43,564 --> 00:05:49,864
So if you have noticed within chat GPT,
you can add a lot of your preferences

89
00:05:49,864 --> 00:05:52,054
or information about yourself as memory.

90
00:05:52,324 --> 00:05:56,944
And what that does it, it enables
the models to maintain context.

91
00:05:57,274 --> 00:06:01,674
Ensuring consistent and relevant
interaction over time, but CDPs can

92
00:06:01,674 --> 00:06:07,734
go a lot further in terms of providing
a more comprehensive context across

93
00:06:07,734 --> 00:06:11,724
different digital touch points, whether
it's an email interaction or an action

94
00:06:11,724 --> 00:06:13,014
you performed on the mobile app.

95
00:06:13,514 --> 00:06:16,694
So with the CDP and with
the cost of inference.

96
00:06:17,279 --> 00:06:20,669
In large language models coming
down dramatically, we'll find that

97
00:06:20,669 --> 00:06:25,889
companies can unlock capabilities
that require personalization at scale.

98
00:06:26,389 --> 00:06:30,619
These would include one-to-one
personalized messaging, dynamically

99
00:06:30,799 --> 00:06:34,939
personalized web experiences, and highly
effective customer support interactions.

100
00:06:35,569 --> 00:06:39,499
Imagine having a customer support
that instantly understands your

101
00:06:39,499 --> 00:06:43,639
context without needing extensive
background information, and that

102
00:06:43,639 --> 00:06:45,559
is exactly what a CDP can deliver.

103
00:06:46,059 --> 00:06:49,119
so far we have learned how A
CDP is sufficient for delivering

104
00:06:49,119 --> 00:06:51,159
personalized experiences.

105
00:06:52,059 --> 00:06:57,609
But now I wanna focus on how A CDP
is also necessary for improving

106
00:06:57,609 --> 00:07:00,219
personalized enterprise applications.

107
00:07:00,719 --> 00:07:05,219
And in particular, I want to emphasize
is why you need a more sophisticated

108
00:07:05,219 --> 00:07:09,959
data infrastructure than just a key value
store to drive this personalization.

109
00:07:10,459 --> 00:07:15,289
The first reason is very simply that
CDPs help break down information silos.

110
00:07:15,789 --> 00:07:22,679
It's possible that customers, make
purchases on different platforms, and

111
00:07:22,679 --> 00:07:26,129
these events are collected at different
touch points by different teams.

112
00:07:26,129 --> 00:07:30,359
Similarly, it's also possible that
one team manages the web app and

113
00:07:30,359 --> 00:07:32,279
another team manages the mobile app.

114
00:07:32,879 --> 00:07:37,109
And the behavior is captured
differently across these two platforms.

115
00:07:37,609 --> 00:07:42,409
Having a unified way of collecting
customer data and unifying that

116
00:07:42,409 --> 00:07:47,269
data into customer profiles is core
to unlocking value from this data.

117
00:07:47,269 --> 00:07:51,069
Using AI with real time data
capture and identity resolution,

118
00:07:51,069 --> 00:07:54,279
you can build a pretty rich customer
profile for each of your users.

119
00:07:54,779 --> 00:07:59,399
And finally, CDPs can also provide
the much needed data governance

120
00:07:59,729 --> 00:08:02,759
for different types of data sets,
depending on their sensitivity.

121
00:08:03,259 --> 00:08:08,509
So now that we have established
the need for personal context and

122
00:08:08,509 --> 00:08:13,129
how CDPs can fulfill that need,
let's focus on different ways of

123
00:08:13,129 --> 00:08:14,809
implementing this with the CDP.

124
00:08:15,309 --> 00:08:18,699
So one potential or a
naive solution would be.

125
00:08:19,164 --> 00:08:24,954
To embed all the available customer
profile information in each LLM prompt.

126
00:08:25,454 --> 00:08:28,784
While this seems very comprehensive in the
sense that you would be getting all the

127
00:08:28,784 --> 00:08:33,674
data that you have about that customer,
it often leads to very large prompts, and

128
00:08:33,674 --> 00:08:38,384
thereby it increases the computational
cost and latency and ultimately makes

129
00:08:38,384 --> 00:08:40,814
the solution pretty impractical at scale.

130
00:08:41,314 --> 00:08:44,374
So depending on how large
your user profile is.

131
00:08:44,884 --> 00:08:49,924
You could have a lot of characteristics
you're storing about your users, a lot

132
00:08:49,924 --> 00:08:53,884
of behaviors you're capturing about
your users, and as a result, this

133
00:08:53,884 --> 00:08:55,774
user profile could be pretty huge.

134
00:08:56,274 --> 00:08:59,454
So now let's look at
another way of doing this.

135
00:08:59,954 --> 00:09:04,424
So another approach of doing this
would involve selectively using only

136
00:09:04,424 --> 00:09:08,564
specific and predetermined customer
traits or events from the user profile.

137
00:09:09,064 --> 00:09:13,224
For instance, if I am talking to
a customer bought and asking for a

138
00:09:13,224 --> 00:09:18,324
restaurant recommendation, if you know
that I have stored, the user's location

139
00:09:18,354 --> 00:09:22,344
as part of the user profile, I can
fetch that and use that to provide more

140
00:09:22,344 --> 00:09:24,305
tailored, restaurant recommendations.

141
00:09:24,805 --> 00:09:30,040
While this helps reduce the prompt size,
it also severely limits the richness

142
00:09:30,040 --> 00:09:31,840
of the personalization experience.

143
00:09:32,340 --> 00:09:37,439
So first of all, it misses any other
relevant information that may be available

144
00:09:37,439 --> 00:09:39,630
as part of the CDP or the user profile.

145
00:09:40,170 --> 00:09:45,600
For instance, it's possible that there's
another team that is through some other

146
00:09:45,600 --> 00:09:48,150
means recording favorite qz for each user.

147
00:09:48,660 --> 00:09:52,640
Now, because as an engineer, while
implementing this feature, I did

148
00:09:52,640 --> 00:09:56,420
not know that this particular
trait existed on the user profile.

149
00:09:56,974 --> 00:10:01,114
I won't have a deliberate way of
including it in my implementation.

150
00:10:01,614 --> 00:10:04,734
The other challenge is it
actually requires you to have this

151
00:10:04,734 --> 00:10:08,364
comprehensive knowledge of what is
stored in the user profile and the

152
00:10:08,364 --> 00:10:11,844
semantic understanding of everything
that exists on the user profile.

153
00:10:12,344 --> 00:10:17,409
And this brings us to our final,
recommendation for implementation.

154
00:10:17,909 --> 00:10:22,379
Is to dynamically retrieve just the
right amount of relevant customer

155
00:10:22,379 --> 00:10:26,669
data based on the specific interaction
or the application context.

156
00:10:27,269 --> 00:10:32,519
And how you do it is you take the
natural language prompt and you calculate

157
00:10:32,519 --> 00:10:37,439
semantic similarity with different data
elements that exist on the user profile

158
00:10:38,039 --> 00:10:44,009
in real time, and fetch that data and
inject that data into the LLM prompt.

159
00:10:44,849 --> 00:10:48,859
So this solves the problem of
having a smaller context that

160
00:10:48,859 --> 00:10:50,569
you're injecting in the LLM prompt.

161
00:10:51,049 --> 00:10:56,599
It also solves the problem that you're
able to use all of the data that is stored

162
00:10:56,599 --> 00:11:02,289
in the user profile without actually
having any prior knowledge of what existed

163
00:11:02,289 --> 00:11:07,779
in the profile because you're doing the
semantic similarity check at runtime.

164
00:11:08,279 --> 00:11:12,479
So this method is pretty efficient,
flexible, scalable, ensures

165
00:11:12,479 --> 00:11:15,749
optimal use of all the data that
you have without compromising

166
00:11:15,749 --> 00:11:17,549
on the personalization quality.

167
00:11:18,049 --> 00:11:21,109
So let's wrap up our discussion
today with a few key takeaways.

168
00:11:21,609 --> 00:11:25,899
First and foremost, whenever you are
building an AI application using large

169
00:11:25,899 --> 00:11:30,759
language models for a lot of use cases,
that actually translates to solving.

170
00:11:31,164 --> 00:11:33,894
A data infrastructure, data
quality, and data governance

171
00:11:33,894 --> 00:11:35,184
problem in the first place.

172
00:11:35,684 --> 00:11:40,534
Secondly, we saw that CDPs
provide a pretty compelling

173
00:11:40,805 --> 00:11:43,894
option as a personalization
engine for large language models.

174
00:11:44,394 --> 00:11:48,234
And finally, we saw how CDPs can
act as long-term memories for large

175
00:11:48,234 --> 00:11:51,924
language models by deriving structured
information and storing them as user

176
00:11:51,924 --> 00:11:56,214
profile and events and making them
available for large language models.

177
00:11:56,544 --> 00:12:00,474
As and when needed for generating
hyper-personalized responses.

178
00:12:00,974 --> 00:12:03,974
Thank you all so much for your
attention and engagement today.

179
00:12:04,064 --> 00:12:08,204
If you'd like to continue this
discussion or have any questions on any

180
00:12:08,204 --> 00:12:12,624
of the topics that we discussed today,
please feel free to reach out to me.

181
00:12:13,164 --> 00:12:13,674
Thanks.

