1
00:00:00,060 --> 00:00:03,380
Hi, my presentation is on DevOps at scale.

2
00:00:03,750 --> 00:00:08,010
we have used a multi agent systems
to automate the build failure in

3
00:00:08,010 --> 00:00:10,150
a very large enterprise setup.

4
00:00:11,030 --> 00:00:17,019
this enterprise has about 315,
000 employees, 100 applications,

5
00:00:17,419 --> 00:00:21,599
and a wide variety of stack as
you see in the table above, right?

6
00:00:22,039 --> 00:00:24,020
We have different deployment models.

7
00:00:24,789 --> 00:00:29,959
Some of it is on prem, some of it is cloud
native, some of it is a hybrid solution,

8
00:00:30,199 --> 00:00:36,359
a variety of programming languages,
frameworks, databases, you know, some of

9
00:00:36,360 --> 00:00:41,899
it is COTS products, some of us, and we
also have different kinds of containers.

10
00:00:41,919 --> 00:00:46,370
So you see the variety, there's
quite a bit of, difference in their

11
00:00:46,450 --> 00:00:47,950
heterogeneity in the character.

12
00:00:48,580 --> 00:00:53,250
Similarly, when you look at When we
actually looked at the system and

13
00:00:53,250 --> 00:00:59,060
studied a bit more about the distribution
of what caused the build failure.

14
00:00:59,169 --> 00:01:04,430
And this we studied using five years
log, which was a goldmine, right?

15
00:01:04,860 --> 00:01:06,490
We realized some things, right?

16
00:01:06,510 --> 00:01:08,540
Some things about the commit size.

17
00:01:09,125 --> 00:01:13,485
certain source lines of code,
if the commits lines is above

18
00:01:13,485 --> 00:01:15,345
that, the failure rate increased.

19
00:01:15,775 --> 00:01:21,435
certain pieces of information was related
to, you know, how these build failures

20
00:01:21,464 --> 00:01:23,235
happen in certain times of the day.

21
00:01:23,245 --> 00:01:25,375
Like weekends, we used
to have more issues.

22
00:01:25,665 --> 00:01:27,695
End of the day, we used
to have more issues.

23
00:01:28,035 --> 00:01:29,465
and there were things of that nature.

24
00:01:29,645 --> 00:01:31,175
Which is what we have listed, right?

25
00:01:31,455 --> 00:01:36,805
Developer experience, the kind of CIC
CD pipeline maturity in each of these

26
00:01:36,805 --> 00:01:41,605
systems, because some of these systems
were old systems, legacy systems, some

27
00:01:41,605 --> 00:01:43,975
of them were relatively new, right?

28
00:01:44,044 --> 00:01:46,755
And the project maturity
level was also different.

29
00:01:47,005 --> 00:01:49,365
So that is why you see this variation.

30
00:01:50,145 --> 00:01:55,195
Now, when we looked at it, we, we were
also curious to understand that given

31
00:01:55,205 --> 00:02:01,365
these bend failures, How does it look
across various internal platforms?

32
00:02:01,375 --> 00:02:06,545
For example, we had the finance platform,
we have healthcare, we have certain

33
00:02:06,545 --> 00:02:12,335
things related to employee engagement, we
have, things like developer productivity.

34
00:02:12,335 --> 00:02:16,125
So when you look at it, the finance
applications in this case, followed

35
00:02:16,135 --> 00:02:18,675
by, had very high build failures.

36
00:02:19,425 --> 00:02:24,835
And also those related to developer
productivity or employee engagement

37
00:02:25,085 --> 00:02:26,595
and human resource, right?

38
00:02:26,795 --> 00:02:31,075
So this is a distribution of the build
failures when we looked at from a domain

39
00:02:31,075 --> 00:02:35,465
perspective, but we also looked at
it from a perspective of tech, right?

40
00:02:35,705 --> 00:02:39,275
So Java, for instance, if you
see has a highest build failure

41
00:02:39,315 --> 00:02:42,375
rate JavaScript and so on.

42
00:02:42,405 --> 00:02:46,945
So this is an interesting fact, but
let's get into the next level of details.

43
00:02:47,815 --> 00:02:48,935
When we looked into the next.

44
00:02:49,305 --> 00:02:50,035
Level of details.

45
00:02:50,585 --> 00:02:50,915
here.

46
00:02:50,915 --> 00:02:53,855
I want to step back and talk
about our DevOps journey, right?

47
00:02:54,135 --> 00:02:57,025
So, as I said, we were very fortunate.

48
00:02:57,185 --> 00:03:02,735
Not everybody has this, but we
had five years worth logs, which

49
00:03:02,735 --> 00:03:05,765
is about 500, 000 build logs.

50
00:03:05,910 --> 00:03:07,070
Over five years.

51
00:03:07,390 --> 00:03:12,270
And, and as I said in my initial
introduction, these were a hundred

52
00:03:12,560 --> 00:03:16,640
enterprise applications, which
were running across 56 countries.

53
00:03:16,850 --> 00:03:18,850
And there were a lot of users, right?

54
00:03:18,880 --> 00:03:23,110
About 300 and 300, 000 users
were a part of the system.

55
00:03:23,360 --> 00:03:28,140
So we took the logs and the first step
is to actually process these logs.

56
00:03:28,200 --> 00:03:30,049
Now, what would you do when you find out?

57
00:03:30,460 --> 00:03:31,430
process these logs.

58
00:03:31,430 --> 00:03:34,570
First and foremost, all
of these were build logs.

59
00:03:34,930 --> 00:03:39,740
We wanted to isolate those logs which
were related to the failures and

60
00:03:39,740 --> 00:03:45,420
we got approximately 10, 000 logs
related to these build failures.

61
00:03:46,280 --> 00:03:46,570
Right.

62
00:03:46,830 --> 00:03:50,730
And then once we got that, we were
curious about the distribution

63
00:03:50,730 --> 00:03:52,050
of the error pattern, right?

64
00:03:52,280 --> 00:03:53,780
What are these errors?

65
00:03:53,810 --> 00:03:55,110
Where are they occurring?

66
00:03:55,330 --> 00:04:01,620
What are these other kinds of metrics
related to MTTR or cost and so on, right?

67
00:04:01,840 --> 00:04:05,490
and also we wanted to
use this, source of data.

68
00:04:05,530 --> 00:04:06,420
We cleaned it up.

69
00:04:06,710 --> 00:04:11,840
We removed all, PII data, and
we used this to also create an

70
00:04:11,850 --> 00:04:16,910
eval dataset, which formed the
basis for our evaluation chassis.

71
00:04:17,400 --> 00:04:19,460
so these are things which
we did with our logs.

72
00:04:19,820 --> 00:04:25,500
The next step which we worked on was
actually to build a agentic system, right?

73
00:04:25,500 --> 00:04:29,460
But the agentic system
did not happen day one.

74
00:04:30,180 --> 00:04:35,530
First, we were thinking that, you
know, why not use the frontier LLMs?

75
00:04:35,915 --> 00:04:38,585
And use that for root cost analysis.

76
00:04:38,595 --> 00:04:40,674
So we began with Azure OpenAI 3.

77
00:04:40,675 --> 00:04:47,915
5 as the first model wherein we
build prompts using these logs.

78
00:04:47,925 --> 00:04:53,085
So when we got this 10k logs, we
actually did a lot of post processing.

79
00:04:53,310 --> 00:04:57,760
Which would help us understand the
domain specifics using which we would

80
00:04:57,770 --> 00:04:59,900
use prompt and prompt engineering, right?

81
00:05:00,190 --> 00:05:04,119
This was the first quick
method, which we employed.

82
00:05:04,499 --> 00:05:08,849
Then we use drag to capture
the domain specific, right?

83
00:05:08,849 --> 00:05:10,929
So then we improved
that a little bit more.

84
00:05:11,359 --> 00:05:16,549
The third thing which we did is that
we realized that soon after, you know,

85
00:05:16,549 --> 00:05:18,609
these models were continuously changing.

86
00:05:18,609 --> 00:05:20,639
So we had, GPT 4.

87
00:05:20,879 --> 00:05:21,399
Right.

88
00:05:21,469 --> 00:05:24,959
And we were also trying with
Claude onthropic, right?

89
00:05:24,999 --> 00:05:30,249
So in this case, we wanted to,
first of all, be able to support

90
00:05:30,489 --> 00:05:31,939
the model swappability, right?

91
00:05:32,069 --> 00:05:36,119
We wanted to make sure that every
time we knew this is a time when

92
00:05:36,119 --> 00:05:39,419
models are perishable, every day
there is a better and better model.

93
00:05:39,864 --> 00:05:44,734
So we wanted to build that infrastructure
which would allow us to swap these models.

94
00:05:45,054 --> 00:05:50,224
And the other outcome of that was every
time you change a model, even within the

95
00:05:50,224 --> 00:05:52,684
same family, the prompts may not work.

96
00:05:52,684 --> 00:05:56,744
So we wanted to have a way in
which these prompts would work.

97
00:05:56,805 --> 00:05:58,444
So those are things we started doing.

98
00:05:59,344 --> 00:06:05,539
The next step, we realized that If
you really look at DevOps, right,

99
00:06:05,659 --> 00:06:09,709
it is a case when there is, we
need a lot of reasoning abilities.

100
00:06:09,739 --> 00:06:13,939
And I will talk about, you know, in a
DevOps situation, how there is a lot of

101
00:06:13,939 --> 00:06:19,979
uncertainties and we do need a reasoning
system, in order to handle it effectively.

102
00:06:20,229 --> 00:06:22,389
So we built a multi agent system.

103
00:06:22,679 --> 00:06:26,599
Both for RCA and also
for issue resolution.

104
00:06:26,939 --> 00:06:32,039
Let's take a step back and talk about,
first question I want to say is, What

105
00:06:32,039 --> 00:06:34,539
is it that you need in an agentic AI?

106
00:06:34,539 --> 00:06:37,559
You do not straight away jump
to an agentic AI because it

107
00:06:37,569 --> 00:06:38,779
is going to cost you more.

108
00:06:39,319 --> 00:06:42,429
Instead of one particular LLM,
you're going to have a set of

109
00:06:42,429 --> 00:06:44,679
LLMs doing many different things.

110
00:06:44,939 --> 00:06:47,359
There were two things I
think we need to be aware of.

111
00:06:47,659 --> 00:06:52,969
One is these cost of these models
is going down year on year, right?

112
00:06:52,989 --> 00:06:54,419
That's the first thing to notice.

113
00:06:54,879 --> 00:06:59,929
And the second thing is all of these
model context length is increasing.

114
00:07:00,219 --> 00:07:06,889
So today enterprises actually can use
frontier LLMs, much more effectively.

115
00:07:06,999 --> 00:07:07,629
That's one.

116
00:07:08,049 --> 00:07:10,469
The second part that's
related to the viability.

117
00:07:10,469 --> 00:07:14,559
The second aspect which it is
related to is a feasibility, right?

118
00:07:14,699 --> 00:07:16,359
So when you look at this graph.

119
00:07:16,609 --> 00:07:19,409
Here are the various
maturity levels, right?

120
00:07:19,769 --> 00:07:25,009
On one axis, it is the generality and
the second aspect is the performance.

121
00:07:25,209 --> 00:07:29,849
So today when we looked at this system,
we were operating in this phase, right?

122
00:07:30,099 --> 00:07:35,754
Wherein we built an agentic LLM, right,
with all the reasoning abilities.

123
00:07:36,254 --> 00:07:41,934
So, that is where, and DevOps as I
said is a partially observable domain,

124
00:07:41,974 --> 00:07:47,554
which means that there is always
this uncertainty and you may have to

125
00:07:47,624 --> 00:07:49,604
take decisions based on uncertainty.

126
00:07:49,864 --> 00:07:53,384
And when you have these kinds of
system where the data is incomplete.

127
00:07:53,684 --> 00:07:56,184
And there's evolving, changes, right?

128
00:07:56,184 --> 00:07:59,434
It could be a threats, it could
be performance issues, which you

129
00:07:59,444 --> 00:08:01,664
hadn't expected, keep coming, right?

130
00:08:01,724 --> 00:08:05,644
You do need advanced reasoning
capabilities to manage these

131
00:08:05,664 --> 00:08:07,384
belief networks, right?

132
00:08:07,434 --> 00:08:11,984
So that is also one of the reasons
which motivated us to move to agent AI.

133
00:08:12,464 --> 00:08:15,344
The third part, what, what
did we land up doing, right?

134
00:08:15,634 --> 00:08:18,639
So when you talk about
an agent system, one is.

135
00:08:18,819 --> 00:08:20,309
As you see on the top, right?

136
00:08:20,309 --> 00:08:24,229
So you do have the traditional
agents, which can actually take the

137
00:08:24,239 --> 00:08:26,439
build failure and do a set of things.

138
00:08:26,439 --> 00:08:31,249
We will double click on what were the
various agents we had, but these agents

139
00:08:31,309 --> 00:08:33,749
must work with traditional tools.

140
00:08:33,759 --> 00:08:36,159
For example, if you think about.

141
00:08:36,974 --> 00:08:39,724
Static code analysis, code smell tools.

142
00:08:39,914 --> 00:08:42,054
You could use a sonar cube, right?

143
00:08:42,094 --> 00:08:45,924
You could use a tool for
providing insights from the

144
00:08:45,924 --> 00:08:48,034
Git commit changes, right?

145
00:08:48,194 --> 00:08:52,594
You could have a JIRA tickets
and conference to find historic

146
00:08:52,594 --> 00:08:54,224
solutions, which people have used.

147
00:08:54,454 --> 00:08:57,184
You could use things for
privacy protection, right?

148
00:08:57,184 --> 00:09:02,735
So you want to take off from these log
anything, which is, you know, has PII.

149
00:09:03,475 --> 00:09:07,635
to make sure that that
information is masked, right?

150
00:09:07,635 --> 00:09:12,585
So you may have to use a variety of
tools, which is what we used, right?

151
00:09:12,835 --> 00:09:16,865
On the top, you have, in this case,
we have four different agents.

152
00:09:17,175 --> 00:09:21,895
Orchestrator is the master agent,
which orchestrates, the other agents.

153
00:09:22,230 --> 00:09:25,560
And if you take the build agent
and I have double clicked, right,

154
00:09:25,560 --> 00:09:26,940
what does a build agent do?

155
00:09:27,240 --> 00:09:32,890
First of all, the build agent actually
analyzes, has many sub agents, right?

156
00:09:32,940 --> 00:09:37,450
In this case, a build agent has a log
analysis agent, which will look at

157
00:09:37,450 --> 00:09:43,550
the logs continuously, monitor the
logs to extract any failures, right?

158
00:09:43,770 --> 00:09:47,710
Then there is a diagnosis agent, which
goes and analyzes this information,

159
00:09:47,710 --> 00:09:52,230
which has been extracted, tries to
classify the information, Or categorize

160
00:09:52,240 --> 00:09:55,560
the type of problem, which is a
performance issue, security issue,

161
00:09:55,850 --> 00:09:57,830
why is it happening, and so on, right.

162
00:09:57,830 --> 00:09:59,070
So it determines that.

163
00:09:59,340 --> 00:10:04,860
The third part is a resolution
agent, which actually uses the log

164
00:10:04,900 --> 00:10:07,250
and the variety of artifacts to say.

165
00:10:07,540 --> 00:10:08,390
Hey, you know what?

166
00:10:08,540 --> 00:10:11,600
This is a kind of resolution
which needs to be taken, right?

167
00:10:11,800 --> 00:10:14,260
And the fourth is a notification agent.

168
00:10:14,460 --> 00:10:19,980
So we build this complete, multi
agent system to address this, right?

169
00:10:20,330 --> 00:10:22,630
Now what were the challenges?

170
00:10:22,850 --> 00:10:24,860
Let's talk about some of the challenges.

171
00:10:25,165 --> 00:10:29,595
we faced while building the system
over this, period in time, right?

172
00:10:29,825 --> 00:10:35,905
First of all was this, today the LLM
really impacts the accuracy, right?

173
00:10:35,955 --> 00:10:39,565
And every day, so the choice
of LLM is very important.

174
00:10:40,400 --> 00:10:45,040
Second is every LLM you change, the
prompts are no more effective here.

175
00:10:45,140 --> 00:10:47,310
If you see, when we use 3.

176
00:10:47,510 --> 00:10:49,530
5, our accuracy was 82%.

177
00:10:49,540 --> 00:10:51,590
Of course, we had other
metrics we measured.

178
00:10:51,990 --> 00:10:57,325
when we moved to four, the accuracy
improved, but when we moved back to GPT 4.

179
00:10:57,325 --> 00:11:01,080
0 mini, which the cost is very
less, our accuracy dropped because

180
00:11:01,080 --> 00:11:02,690
the prompts would no more work.

181
00:11:03,040 --> 00:11:03,230
Right.

182
00:11:03,230 --> 00:11:06,680
So it's very important for us to make
sure that the prompts would work.

183
00:11:07,100 --> 00:11:07,890
The second is.

184
00:11:08,235 --> 00:11:11,015
Human evaluations are not scalable.

185
00:11:11,055 --> 00:11:15,035
You need to have an automated
way for evaluation and that

186
00:11:15,035 --> 00:11:16,785
is very important, right?

187
00:11:16,855 --> 00:11:19,425
And for all of this, reasoning is a key.

188
00:11:20,165 --> 00:11:24,415
You cannot have a system which does
not support reasoning because you're

189
00:11:24,415 --> 00:11:27,105
dealing with a DevOps problem, right?

190
00:11:27,345 --> 00:11:31,215
And Frontier models, remember,
cannot be fine tuned.

191
00:11:31,445 --> 00:11:36,640
So if you really need to fine tune,
you need to create, An SLM, small

192
00:11:36,650 --> 00:11:42,400
language model, where you can train
your, LLM with your custom data, right?

193
00:11:42,650 --> 00:11:43,820
So keep that in mind.

194
00:11:43,980 --> 00:11:49,750
So what we really built is, you know,
while we had the agent architecture,

195
00:11:50,020 --> 00:11:51,930
but we also built certain components.

196
00:11:52,190 --> 00:11:55,250
So first of those is an LLM hub, right?

197
00:11:55,420 --> 00:11:56,860
What does an LLM hub do?

198
00:11:57,090 --> 00:12:00,825
Helps you in selecting It helps
you into swapping these models.

199
00:12:00,825 --> 00:12:04,945
So today you have a model, you change
the model, it helps you to change, right?

200
00:12:05,585 --> 00:12:09,835
Besides this, you could also build your
own, find you in your own model, right?

201
00:12:10,105 --> 00:12:10,995
So it helps you.

202
00:12:11,015 --> 00:12:12,745
That's that's the first purpose.

203
00:12:12,975 --> 00:12:16,085
The second is when you talk about
prompt management, as I said,

204
00:12:16,305 --> 00:12:20,285
you know, here, when I walked you
through, so prompt operations.

205
00:12:20,630 --> 00:12:25,570
Prompt evaluations is very important
wherein you need to make sure the

206
00:12:25,570 --> 00:12:30,300
prompts do not fail with every
change of the underlying LLM.

207
00:12:30,360 --> 00:12:31,280
That is important.

208
00:12:31,520 --> 00:12:35,810
And the finally, the most important
thing, which I think all of you must

209
00:12:35,820 --> 00:12:41,090
have to build trustworthy AI systems
is an evaluation chassis, right?

210
00:12:41,130 --> 00:12:46,810
Evaluation chassis actually does the model
evaluation, In an automatic fashion, and

211
00:12:46,810 --> 00:12:52,690
here we introduce a concept called as
LLM as an evaluator or LLM as a judge.

212
00:12:52,710 --> 00:12:54,190
I will talk about that soon.

213
00:12:54,450 --> 00:13:00,120
And this particular evaluation chassis has
an eval benchmark which we created with

214
00:13:00,120 --> 00:13:03,040
the historic data after pre processing it.

215
00:13:03,370 --> 00:13:07,310
Right, so these are some of the
things we will talk about very soon.

216
00:13:07,390 --> 00:13:10,340
And I have left you some links
which you can always refer.

217
00:13:10,840 --> 00:13:15,800
Okay, having seen this Let's come
to LLM as an evaluator, right?

218
00:13:15,810 --> 00:13:18,870
So on the left hand side, what
you see is, let's suppose that

219
00:13:18,870 --> 00:13:21,330
you have an input prompt, right?

220
00:13:21,800 --> 00:13:26,150
And using one, the first
LLM as a candidate, you get

221
00:13:26,150 --> 00:13:28,000
some kind of output, right?

222
00:13:28,380 --> 00:13:33,445
Now as I spoke earlier, the
human eval, And human evaluation

223
00:13:33,475 --> 00:13:35,325
is not a very scalable way.

224
00:13:35,575 --> 00:13:39,945
So we are looking at alternate
ways in which LLM themselves

225
00:13:39,945 --> 00:13:41,355
can be a judge, right?

226
00:13:41,645 --> 00:13:43,885
So here the second LLM comes into play.

227
00:13:44,245 --> 00:13:46,545
and actually produces an evaluation.

228
00:13:46,795 --> 00:13:49,605
Now this evaluation
has two metrics, right?

229
00:13:49,825 --> 00:13:55,165
The first metric is actually a score
which says that, you know, what

230
00:13:55,165 --> 00:13:59,805
is the, what is the probability
that this, output produced by the

231
00:13:59,815 --> 00:14:02,265
first, LLM is right or wrong, right?

232
00:14:02,615 --> 00:14:04,695
And the second one is
the supporting evidence.

233
00:14:04,695 --> 00:14:05,805
You just don't give a score.

234
00:14:05,845 --> 00:14:08,335
You always give a supporting evidence.

235
00:14:08,645 --> 00:14:13,195
So when you look at it and here I'm
taking, you know, this paper, when

236
00:14:13,195 --> 00:14:15,485
you look at it, it has a benchmark.

237
00:14:15,745 --> 00:14:17,515
It's a very interesting reading, right?

238
00:14:17,795 --> 00:14:22,905
Let's suppose that, first LLM
was used to create a code, right?

239
00:14:23,125 --> 00:14:25,920
And let's suppose that you
had used Claude Sonnet 3.

240
00:14:26,075 --> 00:14:27,285
5, right?

241
00:14:27,485 --> 00:14:28,995
And that is why you have a solver.

242
00:14:29,535 --> 00:14:31,835
Since it's a coding task, right?

243
00:14:32,115 --> 00:14:34,385
And you use Claude Sonnet solver.

244
00:14:34,715 --> 00:14:38,765
Let's suppose that, the accuracy is 88.

245
00:14:38,765 --> 00:14:38,785
1.

246
00:14:39,255 --> 00:14:39,625
Right.

247
00:14:40,295 --> 00:14:46,305
Now, what really happens is that if
you had another Claude Sornet as a

248
00:14:46,315 --> 00:14:52,815
judge, right, to critique the way the
first, first, Claude model solved it,

249
00:14:53,055 --> 00:14:55,135
then, this is a critique agent, right?

250
00:14:55,135 --> 00:14:58,205
Critique is also a judge,
is also an evaluator.

251
00:14:58,565 --> 00:15:02,675
to judge it, then you get a
different score, which is 64.

252
00:15:02,675 --> 00:15:03,260
29.

253
00:15:03,990 --> 00:15:04,230
Right.

254
00:15:04,230 --> 00:15:10,520
So the same LLM when used as a solver
versus worst, when used as a judge

255
00:15:10,770 --> 00:15:12,890
could have different performance.

256
00:15:13,290 --> 00:15:16,950
And this performance also varies
between different kinds of tasks,

257
00:15:17,000 --> 00:15:21,900
knowledge, reasoning, math, coding, you
know, all of them are very different.

258
00:15:22,190 --> 00:15:24,020
Claude is best for coding.

259
00:15:24,440 --> 00:15:27,030
And if you look at the
newer versions of GPD.

260
00:15:27,045 --> 00:15:29,215
4 0 1, right?

261
00:15:29,215 --> 00:15:32,155
They may actually have
much better scores, right?

262
00:15:32,225 --> 00:15:38,055
One of the key observation is that the
ability to judge, to verify the solution

263
00:15:38,215 --> 00:15:43,095
pairs, solution pairs is highly correlated
with its ability to solve the problem.

264
00:15:43,095 --> 00:15:46,965
So if something is good at coding, it
will also be good at judging, right?

265
00:15:47,255 --> 00:15:48,335
That's the key outcome.

266
00:15:49,025 --> 00:15:55,565
Go to the last slide, which is, I would
do a key conclusion and results and, you

267
00:15:55,565 --> 00:16:02,695
know, between Jan 2024 to September 2024
is when we were actually measuring the

268
00:16:02,695 --> 00:16:07,865
impact of the new DevOps with agentic
AI over the hundred applications.

269
00:16:08,275 --> 00:16:11,955
So when we looked at it, the
first metric which came down is

270
00:16:11,955 --> 00:16:15,945
the mean time to resolve, right,
which was average of four hours.

271
00:16:16,160 --> 00:16:17,690
This came down to 2.

272
00:16:17,690 --> 00:16:18,800
2 hours, right?

273
00:16:19,060 --> 00:16:19,910
This is a big deal.

274
00:16:19,910 --> 00:16:24,910
The second thing is, you know,
these problems are never technical.

275
00:16:25,110 --> 00:16:27,020
It is about humans, right?

276
00:16:27,330 --> 00:16:28,790
imagine the DevOps team.

277
00:16:28,940 --> 00:16:30,950
They do a lot of repetitive work.

278
00:16:31,240 --> 00:16:35,210
Sometimes, you know, escalations
come in during, Christmas

279
00:16:35,250 --> 00:16:37,270
times, Thanksgiving times.

280
00:16:37,535 --> 00:16:37,765
Right.

281
00:16:37,775 --> 00:16:42,095
When this in, especially in retail kind
of scenario, there are performance issues

282
00:16:42,105 --> 00:16:46,845
which plague the systems, you know, the
team is always very stressed, right?

283
00:16:46,845 --> 00:16:49,105
They do not know when
a problem will happen.

284
00:16:49,605 --> 00:16:53,905
So in that case, the team
satisfaction went up dramatically.

285
00:16:54,235 --> 00:16:56,685
that is a very important indicator for us.

286
00:16:56,985 --> 00:17:00,085
The third thing is the
quality of resolution, right?

287
00:17:00,565 --> 00:17:05,845
What is the chance that the response or
the resolution you gave indeed worked?

288
00:17:06,335 --> 00:17:12,855
That is, the resolution quality
substantially went up by about 30%, right?

289
00:17:13,345 --> 00:17:15,755
And the last of those is, Cost, right?

290
00:17:16,085 --> 00:17:21,145
So you may think that today
without DevOps, an agentic AI,

291
00:17:21,345 --> 00:17:25,120
let's suppose that you had certain
cost in terms of people, right?

292
00:17:25,625 --> 00:17:32,285
And today, because of having agentic AI,
you change the cost structure, right?

293
00:17:32,315 --> 00:17:34,745
But it becomes an operation expense.

294
00:17:34,995 --> 00:17:36,955
And that came down by 25%.

295
00:17:37,085 --> 00:17:37,295
Right?

296
00:17:37,295 --> 00:17:41,085
This is also because with time, As
I walked you through in the earlier

297
00:17:41,085 --> 00:17:47,085
slides, the cost of these models is
dramatically reducing year on year.

298
00:17:47,535 --> 00:17:51,405
with this, I conclude, thank you so
much for this opportunity to present.

