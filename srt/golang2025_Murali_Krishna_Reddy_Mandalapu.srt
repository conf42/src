1
00:00:00,500 --> 00:00:01,220
Hello everyone.

2
00:00:01,550 --> 00:00:04,160
My name is Murli Kris Manum.

3
00:00:04,760 --> 00:00:09,260
Welcome to the Con 42 Golan Conference.

4
00:00:09,760 --> 00:00:14,670
Today we will discuss about
real-time, a inference at the

5
00:00:14,670 --> 00:00:16,619
edge for self-driving cars.

6
00:00:17,119 --> 00:00:19,729
Self-driving cars represent
one of the most challenging

7
00:00:19,729 --> 00:00:25,279
applications of edge computing and
ai, modern autonomous vehicles.

8
00:00:25,625 --> 00:00:30,395
Must process over a gigabyte of
sensor data per second from high

9
00:00:30,395 --> 00:00:35,705
resolution cameras, lidars, and
radar units to make a split second

10
00:00:35,705 --> 00:00:39,815
decision that ensure passenger safety.

11
00:00:40,315 --> 00:00:45,474
In this presentation, we will explore
how real-time a inference at the

12
00:00:45,474 --> 00:00:50,605
edge enables autonomous vehicles
to function safely and efficiently.

13
00:00:51,474 --> 00:00:52,584
We'll also examine.

14
00:00:52,959 --> 00:00:57,069
Computational challenges, hardware
innovations and optimization

15
00:00:57,069 --> 00:00:59,379
techniques that makes it this possible.

16
00:01:00,279 --> 00:01:00,669
Welcome.

17
00:01:01,539 --> 00:01:03,720
Let's get into the details first.

18
00:01:03,720 --> 00:01:08,730
Let's understand the challenge
autonomous vehicles pose in terms

19
00:01:08,730 --> 00:01:11,760
of the processing requirements.

20
00:01:12,355 --> 00:01:15,320
First, the data that it
generates is massive.

21
00:01:16,070 --> 00:01:20,240
Second, the latency requirements
for safe operation of the.

22
00:01:20,825 --> 00:01:23,975
Vehicle on the road is very strict.

23
00:01:24,545 --> 00:01:28,985
And then three, the variability
of the compute requirements based

24
00:01:28,985 --> 00:01:33,634
on the environment the vehicle
operates in varies significantly

25
00:01:34,134 --> 00:01:35,515
from the data perspective.

26
00:01:35,815 --> 00:01:41,125
Autonomous vehicles generate anywhere
from 1.5, 1.4 terabytes to 19

27
00:01:41,125 --> 00:01:46,914
terabytes of raw data per hour from
the multiple high resolution cameras.

28
00:01:47,830 --> 00:01:52,450
Which operate at 32 60 frames
per second at full HD resolution.

29
00:01:53,020 --> 00:02:00,310
And then lidars, which generates a hundred
thousand to 4.5 million points per frame.

30
00:02:00,700 --> 00:02:06,280
And lidar radar systems
operating at 24 to 77 gigahertz.

31
00:02:06,780 --> 00:02:10,884
With all this amount of data, strict
latency requirements, poses another

32
00:02:10,884 --> 00:02:12,545
challenge, complete perception.

33
00:02:13,160 --> 00:02:17,720
Decision and action pipeline must
execute within a hundred to 300

34
00:02:17,720 --> 00:02:21,260
milliseconds for collagen avoidance.

35
00:02:21,340 --> 00:02:22,300
at high speeds.

36
00:02:22,660 --> 00:02:27,310
Each 10 10 milliseconds of crossing
delay translates to approximately

37
00:02:27,820 --> 00:02:31,810
0.3 to 0.5 meters of extra stopping
distance, which is very crucial

38
00:02:31,810 --> 00:02:33,155
for safe operation of the cars.

39
00:02:33,655 --> 00:02:33,945
Next.

40
00:02:34,705 --> 00:02:37,635
Environmental variability,
computational load.

41
00:02:38,230 --> 00:02:43,390
Can vary by up to 480% between
minimal complexity scenarios,

42
00:02:43,540 --> 00:02:48,160
which are open highways and maximum
complexity environments like

43
00:02:48,310 --> 00:02:53,560
dense urban intersections where
vehicles crisscross humans present,

44
00:02:54,250 --> 00:02:56,200
and many more complex scenarios.

45
00:02:56,700 --> 00:03:00,360
This requires adaptive
computation architectures.

46
00:03:00,860 --> 00:03:05,825
Let's understand how edge computing
evolved for autonomous, cars.

47
00:03:06,325 --> 00:03:10,275
First generation repurposed consumer gpu.

48
00:03:10,775 --> 00:03:15,305
Early autonomous prototypes used
adapted consumer GPUs delivering up

49
00:03:15,305 --> 00:03:20,240
to 12 te terra operations per second
with significant limitations, namely.

50
00:03:20,690 --> 00:03:23,540
High power consumption, up to 300 watts.

51
00:03:24,260 --> 00:03:29,420
This high power post, additional
constraints with respect to the thermal

52
00:03:29,420 --> 00:03:36,350
management, which required liquid cooling
and also data transfer bottlenecks

53
00:03:36,560 --> 00:03:38,545
up to 67% of the processing time.

54
00:03:39,045 --> 00:03:42,710
In the second generation
automotive grad, automotive

55
00:03:42,710 --> 00:03:45,160
grade accelerators were designed.

56
00:03:45,865 --> 00:03:50,965
As a purpose built process with
improved efficiency up to two to four

57
00:03:51,325 --> 00:03:56,175
terra operations per second per watt,
with reduced memory traffic by up to

58
00:03:56,175 --> 00:04:01,725
60% through pruning and compression
techniques, and also supported

59
00:04:01,875 --> 00:04:05,835
reduced patient computing for up
to four x throughput improvement.

60
00:04:06,735 --> 00:04:10,035
Let's get into the next level of
details on all these techniques.

61
00:04:10,535 --> 00:04:13,265
Edge versus com cloud computing trade off.

62
00:04:13,535 --> 00:04:19,835
Let's understand the differences and
the advantages between edge and cloud.

63
00:04:19,840 --> 00:04:24,565
Computings edge computing advantages,
it is near instantaneous processing

64
00:04:25,105 --> 00:04:30,985
with five to 15 milliseconds latency
versus up to 500 milliseconds for cloud.

65
00:04:31,485 --> 00:04:35,715
Maintains functionality during
connectivity interruptions, which may

66
00:04:35,715 --> 00:04:41,855
happen up to 38% of the route reduced
security vulnerabilities up to 50%.

67
00:04:42,355 --> 00:04:44,645
Better privacy protection
by keeping sensitive data

68
00:04:44,650 --> 00:04:46,320
within the vehicle boundaries.

69
00:04:46,820 --> 00:04:50,185
And of course, cloud computing
has its own advantages.

70
00:04:50,455 --> 00:04:53,215
It is two to three others, magnitude
greater computational throughput.

71
00:04:54,190 --> 00:04:57,640
It enables more sophisticated
algorithms with higher accuracy.

72
00:04:58,510 --> 00:05:04,150
It externalizes the power consumption
up to 1500 watts for edge processing.

73
00:05:04,840 --> 00:05:10,720
As the power is delivered from
a grid, there is no limit on

74
00:05:10,720 --> 00:05:12,310
the power that it can consume.

75
00:05:13,030 --> 00:05:17,140
It is ideal for non-safety
critical task, like high definition

76
00:05:17,140 --> 00:05:19,030
map generation, et cetera.

77
00:05:19,530 --> 00:05:23,070
So with that, now let's
get into hybrid approach.

78
00:05:23,570 --> 00:05:27,620
In the cloud processing, high
bandwidth analytics mapping

79
00:05:27,650 --> 00:05:29,120
and fleet learning are used.

80
00:05:29,660 --> 00:05:32,990
This is up to 70, 72% of the task.

81
00:05:33,490 --> 00:05:37,240
The hybrid processing where intelligent
load balancing with seamless task

82
00:05:37,240 --> 00:05:40,300
migration across the platforms is done.

83
00:05:40,800 --> 00:05:43,410
The last one is edge processing.

84
00:05:43,680 --> 00:05:48,800
This is used primarily for mission
critical perception and control systems,

85
00:05:48,800 --> 00:05:54,650
which is up to 82% of the processing
needs in a hybrid, in a autonomous car,

86
00:05:55,640 --> 00:06:00,440
the industry has conversed on a hybrid
architecture that strategically allocates

87
00:06:00,440 --> 00:06:03,890
computational workloads between onboard
systems and cloud infrastructure.

88
00:06:04,760 --> 00:06:06,080
The sophisticated approach.

89
00:06:06,845 --> 00:06:10,655
Delivers almost a hundred percent
functional liability while reducing the

90
00:06:10,655 --> 00:06:16,265
vehicle computational requirements by up
to 45% compared to pure edge solutions.

91
00:06:17,195 --> 00:06:22,265
By leveraging both paradigms in both
paradigms, inherent strengths, which

92
00:06:22,265 --> 00:06:27,305
is immediacy of the edge processing
with scalability of cloud computing.

93
00:06:27,695 --> 00:06:32,045
This hybrid framework creates an
optimal balance of safety, efficiency,

94
00:06:32,345 --> 00:06:35,495
and capitalize while effectively
neutralizing the limitations of

95
00:06:35,585 --> 00:06:38,045
either approach in its isolation.

96
00:06:38,545 --> 00:06:42,700
Let's understanding some, let's
understand some of the issues are

97
00:06:42,740 --> 00:06:46,965
some of the requirements related to
various aspects of autonomous driving.

98
00:06:47,745 --> 00:06:50,205
First, update detection
and classification.

99
00:06:50,705 --> 00:06:54,085
This is done mostly at the
edge performance requirements.

100
00:06:54,655 --> 00:07:01,285
With the state of the art models, we
can achieve up to 87% of precision while

101
00:07:01,285 --> 00:07:05,605
operating within the strict latency
constraints of up to 50 milliseconds

102
00:07:05,605 --> 00:07:07,855
per frame on auto grade processors.

103
00:07:08,355 --> 00:07:11,590
Second multi-stage processing
in this, technique.

104
00:07:12,080 --> 00:07:12,890
In Shell region.

105
00:07:12,890 --> 00:07:17,870
Proposal networks operate at
2230 heads, followed by more

106
00:07:17,870 --> 00:07:19,700
intensive classification networks.

107
00:07:19,850 --> 00:07:24,740
That process only identified region
of interest reducing computational

108
00:07:24,740 --> 00:07:27,350
requirements by up to 75%.

109
00:07:27,850 --> 00:07:31,810
There are, there is one more optimization
technique that is quantization.

110
00:07:32,310 --> 00:07:33,180
Quantization.

111
00:07:33,210 --> 00:07:36,225
If it is full precision, we use 32 bits.

112
00:07:36,930 --> 00:07:41,370
But by quantization reducing the
quantization from 32 bit to eight bit

113
00:07:41,370 --> 00:07:47,730
in T Precision, which reduces the memory
requirements up to 76% and inference

114
00:07:47,730 --> 00:07:53,040
time by up to 3.4 times with accuracy.

115
00:07:53,040 --> 00:07:58,290
Degradation of only up to 1.8
are maybe 2% compared to the full

116
00:07:58,290 --> 00:08:00,755
precision 30 littlebit parameters.

117
00:08:01,255 --> 00:08:05,815
Next, let's understand the lane detection
and project planning compute requirements.

118
00:08:06,685 --> 00:08:11,775
Lane detection con consumes up to 12%
of perception budget processing camera

119
00:08:11,775 --> 00:08:17,805
feeds either in seven 20 P resolution
or 1,080 p resolution to extract the

120
00:08:17,805 --> 00:08:21,975
lane markings within five to eight
centimeters, accuracy at distances

121
00:08:21,975 --> 00:08:24,740
up to 80 meters multimodal fusion.

122
00:08:25,580 --> 00:08:31,910
In this RGB camera data is combined
with lidar reflectivity and rather

123
00:08:31,910 --> 00:08:36,590
returns to maintain accuracy during
adverse weather when visual data

124
00:08:36,590 --> 00:08:39,320
quality degrades almost by 60%.

125
00:08:39,820 --> 00:08:45,315
Third, trajectory planning in this,
in, in this task hardware are, the

126
00:08:45,315 --> 00:08:49,755
compute requirements evaluates 1500
to 3000 candidate trajectories.

127
00:08:50,220 --> 00:08:54,930
Every a hundred milliseconds, which
comprises of almost 25% of the

128
00:08:54,930 --> 00:08:59,480
computational budget for optimizing
the safety, comfort, and efficiency

129
00:08:59,480 --> 00:09:04,340
within the strict time constraints
or latency constraints we have.

130
00:09:04,840 --> 00:09:10,300
Next let's you, let's understand few
of the techniques that are used to

131
00:09:10,300 --> 00:09:13,720
optimize further on the model itself.

132
00:09:14,470 --> 00:09:15,250
Quantization.

133
00:09:15,850 --> 00:09:17,890
Converts high precision that bit floating.

134
00:09:17,890 --> 00:09:22,900
Point to an efficient eight bit
T representation, delivering four

135
00:09:22,900 --> 00:09:26,920
times weight compression and two
times activation Compression with

136
00:09:26,920 --> 00:09:30,760
minimal accuracy loss up to 1.2%.

137
00:09:31,260 --> 00:09:33,030
This is critical for its deployment.

138
00:09:33,150 --> 00:09:38,220
Eight bit operations consume
nine times less energy compared

139
00:09:38,220 --> 00:09:39,990
to floating point calculations.

140
00:09:40,490 --> 00:09:42,620
Next pruning technique.

141
00:09:43,415 --> 00:09:48,995
In this technique, we systematically
eliminate T neural network parameters,

142
00:09:49,355 --> 00:09:56,055
which are up to 70%, that they contribute
negligible to the performance, and then

143
00:09:56,055 --> 00:10:00,795
structured pruning techniques yield up
to 3.8 times computational efficiency

144
00:10:00,795 --> 00:10:03,405
gains while maintaining model integrity.

145
00:10:03,905 --> 00:10:08,675
With this, maybe accuracy drops
only up to 2% with fine tuning.

146
00:10:09,175 --> 00:10:13,345
And then another technique for model
optimization is knowledge distillation.

147
00:10:14,095 --> 00:10:18,895
In this technique, we use larger teach
model, teacher models to guide the

148
00:10:18,895 --> 00:10:22,165
training of compact and student networks.

149
00:10:22,705 --> 00:10:28,525
This technique enables dramatically
smaller models up to 8, 8, 8 times fewer

150
00:10:28,525 --> 00:10:32,995
parameters to capture the essential
calculates of larger architectures.

151
00:10:33,355 --> 00:10:37,885
While sacrificing only up to
3% accuracy, this is ideal for

152
00:10:37,885 --> 00:10:40,765
resource constrained edge devices.

153
00:10:41,265 --> 00:10:46,985
Let's get into some hard, hardware aware
neural architecture, such improvements

154
00:10:47,485 --> 00:10:52,665
automated discovery discovers in
this mode, in this optimization.

155
00:10:52,725 --> 00:10:57,885
It discovers novel network architectures
tailored to specific hardware,

156
00:10:57,945 --> 00:11:05,055
outperforming handcrafted designs by up to
12% inaccuracy, while reducing inference

157
00:11:05,055 --> 00:11:08,655
latency by 2.3 times, which is very good.

158
00:11:09,155 --> 00:11:11,885
Then the second technique
is hardware modeling.

159
00:11:12,385 --> 00:11:18,595
in this technique, we incorporate details,
detailed models of memory access patterns.

160
00:11:19,420 --> 00:11:24,340
Operator execution times and paralyzation
capital capabilities of hardware to

161
00:11:24,340 --> 00:11:30,130
maximize the hardware utilization in this
models are aware of the actual hardware,

162
00:11:30,130 --> 00:11:35,440
underlying hardware and its capabilities
to achieve battery utilization.

163
00:11:35,940 --> 00:11:39,450
Third sensor fusion optimization,
particularly effective for

164
00:11:39,450 --> 00:11:44,165
sensor fusion tasks, reducing re
inference latency by up to 48%.

165
00:11:44,910 --> 00:11:49,580
Compared to the conventional architecture
on auto grade accelerators, fourth

166
00:11:49,850 --> 00:11:54,620
attention mechanisms discovers
efficient attention based architectures

167
00:11:54,890 --> 00:11:58,770
that selectively process high
information reasons, reducing overall

168
00:11:58,800 --> 00:12:01,285
computational requirements by up to 55%.

169
00:12:01,785 --> 00:12:03,255
There is also another.

170
00:12:04,045 --> 00:12:07,605
Emerging trend, which is
neuromorphic computing before

171
00:12:07,605 --> 00:12:09,285
understanding neuromorphic computing.

172
00:12:09,345 --> 00:12:12,785
Let's understand what is
event driven processing?

173
00:12:13,285 --> 00:12:18,775
Event driven processing is instead
of processing data frame by frame,

174
00:12:19,165 --> 00:12:21,205
we process data based on an event.

175
00:12:21,955 --> 00:12:26,485
This reduces power consumption up
to 95% compared to the frame based

176
00:12:26,485 --> 00:12:29,905
approach By allocating resources
only when significant changes occur.

177
00:12:30,405 --> 00:12:33,915
This also helps temporal advantages,
which is microsecond scale.

178
00:12:33,915 --> 00:12:40,575
Temporal resolutions up to 10 microseconds
improves detection latency by 20 to

179
00:12:40,575 --> 00:12:45,915
45 milliseconds for high speed objects
compared to conventional vision systems.

180
00:12:46,905 --> 00:12:51,885
This also helps lighting, adaptability,
maintains consistent detection.

181
00:12:52,380 --> 00:12:57,930
Across elimination ranges from point
0.1, luminance to a hundred thousand

182
00:12:57,930 --> 00:13:02,579
luminance, addressing limitations with
high dynamic range in neuro environments.

183
00:13:03,079 --> 00:13:06,139
With this understanding,
let's get, let's understand.

184
00:13:06,139 --> 00:13:07,759
What is neuromorphic computing?

185
00:13:08,269 --> 00:13:11,569
Neuromorphic computing represent a
fundamental departure from conventional

186
00:13:11,569 --> 00:13:15,829
architectures, drawing the inspiration
from biological neural systems.

187
00:13:16,369 --> 00:13:17,869
These systems integrate.

188
00:13:18,499 --> 00:13:23,300
Processing and memory in artificial
neuron and signup structures that

189
00:13:23,300 --> 00:13:25,339
mimic their biological counterparts.

190
00:13:26,030 --> 00:13:32,689
Prototype neuromorphic processing units
can process five 50 to a hundred million

191
00:13:32,719 --> 00:13:38,269
events per second while consuming
only a hundred to 300 milliwatts of

192
00:13:38,269 --> 00:13:43,280
power representing a two orders of
magnitude improvement in efficiency

193
00:13:43,280 --> 00:13:46,099
compared to GPU based solutions.

194
00:13:46,599 --> 00:13:49,180
Okay, let's summarize the future.

195
00:13:49,719 --> 00:13:51,130
Where are we headed with this?

196
00:13:52,120 --> 00:13:57,030
Distributed AI and continuous
learning are the future distributed

197
00:13:57,060 --> 00:14:03,569
AI architectures maintain 85 to 92% of
the critical functionality, even with

198
00:14:04,079 --> 00:14:06,750
failures in 30% of the processing nodes.

199
00:14:07,250 --> 00:14:11,810
Reduce latency by up to 47%
for complex perception tasks.

200
00:14:11,870 --> 00:14:16,780
Through better paralyzation and reduced
data movement, continuous learning systems

201
00:14:17,530 --> 00:14:20,950
improve detection accuracy by 15 to 20%.

202
00:14:21,040 --> 00:14:25,860
In novel environments not represented
in initial training data, employee

203
00:14:25,860 --> 00:14:29,220
safety, aware, incremental updates,
that limit parameter changes

204
00:14:29,220 --> 00:14:31,290
to only up to 2% per cycle.

205
00:14:31,790 --> 00:14:32,360
Federated learning.

206
00:14:32,860 --> 00:14:37,450
Vehicles continue to col collective
intelligence while transmitting

207
00:14:37,480 --> 00:14:42,520
only a 0.5% of the data required
for central centralized approaches

208
00:14:43,020 --> 00:14:48,870
with this synchronization of every
a hundred to 500 kilometers provides

209
00:14:48,960 --> 00:14:52,760
up to 85% of continuous connectivity
benefits, which is a great one.

210
00:14:53,260 --> 00:14:53,770
Thank you.

211
00:14:54,129 --> 00:14:54,545
With this.

212
00:14:55,045 --> 00:14:57,025
I conclude my presentation.

