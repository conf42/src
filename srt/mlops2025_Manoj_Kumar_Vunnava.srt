1
00:00:00,500 --> 00:00:01,310
Hi everyone.

2
00:00:01,400 --> 00:00:02,420
Thanks for tuning in.

3
00:00:02,480 --> 00:00:07,010
I am Manu Kumar Awa, and I work for
GoDaddy as a senior telecom engineer.

4
00:00:07,609 --> 00:00:12,779
And today we are going to explore how to
scale conversational AI in production.

5
00:00:13,119 --> 00:00:15,339
And this is specially for contact centers.

6
00:00:15,999 --> 00:00:21,600
Now, when I say contact center, you
might think of, long hold times choppy

7
00:00:21,600 --> 00:00:25,800
sounds, and then the, background music
and agents juggling multiple screens.

8
00:00:26,190 --> 00:00:30,580
But actually the behind the screens AI is
quietly transforming these environments.

9
00:00:30,629 --> 00:00:35,819
Sometimes you talk directly to it other
times, it helps an agent responds faster.

10
00:00:36,304 --> 00:00:40,974
So this talk is all about how we
can make those AI system production

11
00:00:40,974 --> 00:00:43,195
ready, reliable, and scalable.

12
00:00:43,695 --> 00:00:45,885
So here's what we will cover.

13
00:00:45,935 --> 00:00:49,754
First thing is the cha, the challenges
unique to the contact center.

14
00:00:49,754 --> 00:00:54,275
Ai and ML ops framework are
tailored to conversational ai.

15
00:00:54,705 --> 00:00:59,355
Some technical details like architecture,
CICD, pipeline and monitoring.

16
00:00:59,975 --> 00:01:02,285
And finally how to measure the success.

17
00:01:02,685 --> 00:01:06,315
And when I say measuring the success,
it's not just with the technical

18
00:01:06,345 --> 00:01:09,115
KPIs but with business outcomes.

19
00:01:09,605 --> 00:01:14,715
So as we go through, I will also share
some stories and particular examples.

20
00:01:14,955 --> 00:01:18,755
So this doesn't feel like a theory,
but something you can imagine in

21
00:01:18,785 --> 00:01:20,865
in action, with contact centers.

22
00:01:20,870 --> 00:01:25,550
A good thing is we all actually experience
this on a day-to-day basis because every

23
00:01:25,550 --> 00:01:30,300
company does have a contact center that,
once in a lifetime, obviously you have

24
00:01:30,300 --> 00:01:32,620
to call them and have to experience it.

25
00:01:33,120 --> 00:01:35,460
So let me start with a story.

26
00:01:35,460 --> 00:01:40,430
So a few years ago, a telecom company,
tried launching a chatbot to handle

27
00:01:40,430 --> 00:01:44,890
customer complaints on day one,
customer asked about a network outage.

28
00:01:45,290 --> 00:01:49,850
The bot had no idea what was going on,
and kept replying with generic message.

29
00:01:49,850 --> 00:01:53,750
Something like, Hey, have you tried your,
restarting your phone, router, or modem?

30
00:01:54,250 --> 00:01:58,770
Messages, customers, obviously were
furious, based on this kind of response.

31
00:01:59,230 --> 00:01:59,800
But why?

32
00:01:59,850 --> 00:02:03,360
Because contact center AI is
unique, uniquely challenging.

33
00:02:03,390 --> 00:02:07,980
So to say, we are take, we are talking
millions of conversations per day.

34
00:02:08,380 --> 00:02:13,395
Unlike an agent who handles 30 calls,
a handles everything, and it's instant.

35
00:02:13,895 --> 00:02:15,935
And second thing is human in the loop.

36
00:02:16,145 --> 00:02:19,805
So basically it has to work
side by side with agents.

37
00:02:20,205 --> 00:02:23,975
Think of it like a co-pilot, not
but not the actual replacement.

38
00:02:24,155 --> 00:02:26,255
And the other thing is cost of errors.

39
00:02:26,735 --> 00:02:30,385
A wrong answer is not just
about one unhappy customer.

40
00:02:30,385 --> 00:02:34,525
It could training complaints
on Twitter or any other social

41
00:02:34,525 --> 00:02:36,175
media platform within ours.

42
00:02:36,175 --> 00:02:38,605
It's wildfire in social media nowadays.

43
00:02:39,369 --> 00:02:42,190
And the next thing is diverse language.

44
00:02:42,459 --> 00:02:45,160
So every industry has its own jargon.

45
00:02:45,170 --> 00:02:49,730
Take it like healthcare, insurance,
retail, banking industry.

46
00:02:50,040 --> 00:02:54,840
You can't rely on one generic model when
it comes to the contact center language.

47
00:02:55,340 --> 00:03:00,290
So the real challenge isn't about
building ai, but it's building AI that

48
00:03:00,290 --> 00:03:03,780
scales adapts and earns customer trust.

49
00:03:04,280 --> 00:03:07,910
So this is where ML ops enter the picture.

50
00:03:08,270 --> 00:03:09,090
So if you think of.

51
00:03:09,855 --> 00:03:14,625
Theater production, the actors on the
stage, but the backstage crew, makes

52
00:03:14,654 --> 00:03:19,304
sure the lights work, the sound is clear,
and they actually run the show on time.

53
00:03:19,545 --> 00:03:21,554
That is how here the ML ops is.

54
00:03:21,885 --> 00:03:24,954
So the ML ops is the
backstage crew for ai.

55
00:03:25,254 --> 00:03:29,844
So basically tens features are
consistently engineered, training

56
00:03:29,844 --> 00:03:34,054
pipelines run smoothly every time
deployments happening safely.

57
00:03:34,054 --> 00:03:35,014
And monitoring.

58
00:03:35,569 --> 00:03:41,869
Catches issues before customers actually
do so without ML Ops AI project end up,

59
00:03:41,899 --> 00:03:46,839
as cool demos that has, that doesn't
actually survive the chaos of production.

60
00:03:47,339 --> 00:03:53,320
So now the features, so think of
features as, what the model pays

61
00:03:53,320 --> 00:03:55,930
attention to for conversational ai.

62
00:03:55,930 --> 00:04:01,339
It's not just the text it's the context
which means, what's being said earlier in

63
00:04:01,339 --> 00:04:07,549
the conversation and how they said it and
the timing, how quickly customer replies,

64
00:04:07,549 --> 00:04:10,219
how often they contact the support, right?

65
00:04:10,729 --> 00:04:12,499
And the third one is the language.

66
00:04:12,904 --> 00:04:17,674
Which is basically sentiment intent
and domain specific keywords.

67
00:04:18,264 --> 00:04:21,084
We manage all this
through a feature store.

68
00:04:21,514 --> 00:04:24,604
If you think of cooking, it's
like a well organized pantry.

69
00:04:24,604 --> 00:04:27,154
You don't want to discover midway
that you know you're running

70
00:04:27,154 --> 00:04:28,654
out of salt or something else.

71
00:04:29,054 --> 00:04:32,684
Similarly, your A model shouldn't
find that a feature is missing

72
00:04:32,684 --> 00:04:34,194
or something that is outdated.

73
00:04:34,759 --> 00:04:38,879
I have seen teams skip this
step and they end up with models

74
00:04:38,879 --> 00:04:42,289
that work fine in lab, but they
actually fall apart in production.

75
00:04:42,789 --> 00:04:44,409
Next comes training.

76
00:04:44,739 --> 00:04:48,549
Training can't be a one time
experiment on someone's laptop.

77
00:04:48,969 --> 00:04:51,644
It has to be robust
repeatable and automated.

78
00:04:52,639 --> 00:04:57,759
Which basically means that, checking
data quality, garbage in, out generating

79
00:04:57,759 --> 00:05:03,869
synthetic data for for rare but important
cases like fraud detection detecting bias.

80
00:05:04,199 --> 00:05:08,519
Imagine a model that works well for
one language, but fails for another.

81
00:05:09,059 --> 00:05:12,539
As an example, maybe it works well
for retail, but when it comes to the

82
00:05:12,539 --> 00:05:17,020
healthcare industry, you have complicated
medicine names or or the formula

83
00:05:17,020 --> 00:05:18,310
names that they have to pronounce.

84
00:05:18,310 --> 00:05:20,229
And system should be able to recognize it.

85
00:05:21,130 --> 00:05:23,439
And the next one is, tracking experiments.

86
00:05:23,859 --> 00:05:27,189
So you know why one version
worked better than the another.

87
00:05:27,280 --> 00:05:29,229
And finally, validation.

88
00:05:29,659 --> 00:05:32,629
We don't just measure
accuracy, but we test how.

89
00:05:33,044 --> 00:05:37,365
How the model, would affect actual
KPIs, like customer satisfaction

90
00:05:37,365 --> 00:05:39,075
scores or resolution time.

91
00:05:39,685 --> 00:05:41,995
Think of it like testing an airplane.

92
00:05:42,535 --> 00:05:44,515
You don't just check if it can fly.

93
00:05:44,905 --> 00:05:48,495
You actually, simulate the
turbulence, emergencies, and

94
00:05:48,495 --> 00:05:50,085
worst case scenarios as well.

95
00:05:50,585 --> 00:05:53,140
AB testing okay, now your model is ready.

96
00:05:53,660 --> 00:05:55,670
Do you unleash it on
all customers at once?

97
00:05:56,510 --> 00:05:57,080
No no way.

98
00:05:57,080 --> 00:05:57,770
We are doing that.

99
00:05:57,800 --> 00:05:59,780
So basically we start small.

100
00:06:00,180 --> 00:06:04,890
So shadow testing, the model
runs silently in the background.

101
00:06:05,190 --> 00:06:09,270
It makes predictions but actually
doesn't affect real customers.

102
00:06:09,700 --> 00:06:13,630
Canary releases a tiny fraction
of traffic, goes to the new

103
00:06:13,630 --> 00:06:15,950
model and then staged rollout.

104
00:06:16,219 --> 00:06:20,270
So gradually increase exposure
as confidence bills and

105
00:06:20,299 --> 00:06:22,075
always a rollback switch.

106
00:06:22,075 --> 00:06:22,195
Ready?

107
00:06:22,695 --> 00:06:26,745
I like to compare this to, teaching
someone to drive first they watch from

108
00:06:26,745 --> 00:06:31,305
the passenger seat and then they drive in
a parking lot, then slowly onto the main

109
00:06:31,305 --> 00:06:35,385
road and freeways, you don't actually
throw them straight onto a freeway, right?

110
00:06:35,445 --> 00:06:36,825
So that is how this is.

111
00:06:37,325 --> 00:06:41,985
And obviously monitoring deployment
isn't the finish line, right?

112
00:06:41,985 --> 00:06:45,795
It's actually the starting point, but
we need to monitor at four levels.

113
00:06:46,125 --> 00:06:49,715
The first one is technical
metrics speed and the error rates.

114
00:06:50,065 --> 00:06:53,635
The next one is data quality,
which means our input's actually

115
00:06:53,635 --> 00:06:54,670
drifting the model or not.

116
00:06:55,450 --> 00:06:57,370
And the next one is performance, right?

117
00:06:57,400 --> 00:07:00,490
Like accuracy, agent overrides, et cetera.

118
00:07:01,000 --> 00:07:02,680
And finally, the business outcome.

119
00:07:02,740 --> 00:07:07,120
Our customers happier,
our issues resolve faster.

120
00:07:07,480 --> 00:07:10,900
So this is like having
multiple dashboards in a plane.

121
00:07:11,210 --> 00:07:14,900
One for altitude, one for fuel,
and another for weather, and

122
00:07:14,900 --> 00:07:16,970
something else you don't rely on.

123
00:07:16,970 --> 00:07:18,280
Just, one metric.

124
00:07:18,780 --> 00:07:22,620
So here's what the overall
architecture looks like.

125
00:07:23,110 --> 00:07:25,810
We ingested data from calls and chats.

126
00:07:26,500 --> 00:07:31,560
We run it through a feature store
on an, and, to enrich it, the model

127
00:07:31,560 --> 00:07:33,390
service predictions in real time.

128
00:07:33,720 --> 00:07:38,039
And then we log everything, feed
it into the monitoring tools and

129
00:07:38,039 --> 00:07:42,130
agency suggestions directly in their
workspace while, they're having

130
00:07:42,130 --> 00:07:43,780
the conversation with the customer.

131
00:07:44,530 --> 00:07:49,390
So the key here is AI is invisible
to the customers when they,

132
00:07:49,540 --> 00:07:51,400
when the experience is faster.

133
00:07:51,760 --> 00:07:54,030
And the service is much smarter.

134
00:07:54,530 --> 00:07:58,010
So scaling requirements
for the CICD, right?

135
00:07:58,250 --> 00:08:04,280
So we treat models like software, so code,
configurations, and the features like gi.

136
00:08:04,770 --> 00:08:06,959
We automate test files on each change.

137
00:08:07,559 --> 00:08:11,920
Models are registered with metadata
deployments are containerized

138
00:08:11,920 --> 00:08:13,810
and they rolled out gradually.

139
00:08:14,080 --> 00:08:17,890
So basically this cuts deployment
time from weeks to hours.

140
00:08:18,230 --> 00:08:22,100
It is a difference between mailing a
letter and sending a WhatsApp message.

141
00:08:22,500 --> 00:08:26,310
That agility allows, business to
adapt in the actual real life.

142
00:08:26,810 --> 00:08:29,480
But even the best pipelines can't stop.

143
00:08:29,480 --> 00:08:31,310
One thing that is data drifting.

144
00:08:31,870 --> 00:08:33,100
Picture this, it's a holiday.

145
00:08:33,100 --> 00:08:33,760
Seasons.

146
00:08:33,760 --> 00:08:36,650
Suddenly everyone is
asking, where's my package?

147
00:08:36,650 --> 00:08:41,190
Or imagine a viral outage million
people calling about the same issue.

148
00:08:41,580 --> 00:08:44,250
So over time, even language evolves.

149
00:08:44,320 --> 00:08:45,250
A year ago.

150
00:08:45,805 --> 00:08:47,695
No one was asking about charge GBT.

151
00:08:47,695 --> 00:08:50,195
Now it's a common
customer, support question.

152
00:08:50,525 --> 00:08:55,085
So if the model isn't updated,
it slowly becomes irrelevant.

153
00:08:55,175 --> 00:08:57,805
That is why, drift detection is essential.

154
00:08:58,105 --> 00:09:02,355
It's a statistical check embedding,
monitoring, and animal alerts.

155
00:09:02,685 --> 00:09:05,445
So it's like having a
doctor check your vitals.

156
00:09:05,925 --> 00:09:10,545
Small shifts can tell you, something
wrong, long before you actually collapse.

157
00:09:11,045 --> 00:09:17,765
And when drift is detected, retraining
shouldn't require a massive manual effort.

158
00:09:18,015 --> 00:09:22,695
It should be automated, so collect
new interactions, verified by the

159
00:09:22,695 --> 00:09:27,465
agents trigger retraining jobs
automatically validate against all

160
00:09:27,465 --> 00:09:32,445
and new scenarios roll out carefully
with monitoring these kind of method.

161
00:09:32,445 --> 00:09:33,375
This closes the loop.

162
00:09:34,060 --> 00:09:36,040
So this is, it's never static.

163
00:09:36,085 --> 00:09:38,320
It act, it all, it
actually always learning.

164
00:09:38,380 --> 00:09:39,880
Just like your agents are.

165
00:09:40,300 --> 00:09:41,940
I think of it like fitness.

166
00:09:41,940 --> 00:09:47,210
So if you don't exercise, your body
drifts, retraining keeps your AI in shape.

167
00:09:47,710 --> 00:09:49,330
So let's wrap this up.

168
00:09:49,380 --> 00:09:53,980
Some of the key features are the
foundation training pipelines.

169
00:09:54,350 --> 00:09:57,740
Progress your rollout, which makes,
basically minimizes the risk.

170
00:09:58,080 --> 00:10:03,130
Monitoring keeps us honest and retraining
ensures the system, keeps evolving.

171
00:10:03,850 --> 00:10:07,060
And the golden rule here is
AI doesn't replace people.

172
00:10:07,060 --> 00:10:10,250
It frees them, and it take
cares of the routine questions.

173
00:10:10,250 --> 00:10:15,210
So agents can focus on empathy,
problem solving, and complex cases.

174
00:10:15,600 --> 00:10:18,960
And when humans and AI work
together, contact center,

175
00:10:18,960 --> 00:10:20,940
shifts from cost center to.

176
00:10:21,295 --> 00:10:22,495
Value creations.

177
00:10:23,405 --> 00:10:26,035
And I really thank you
for watching this session.

178
00:10:26,405 --> 00:10:31,055
Scaling conversational AI is not just
about building models, it's about

179
00:10:31,055 --> 00:10:36,245
putting the right MLO practices in
place so those models stay, reliable,

180
00:10:36,245 --> 00:10:37,985
scalable, and value over time.

181
00:10:38,475 --> 00:10:39,225
When done right.

182
00:10:39,825 --> 00:10:41,655
AI doesn't replace human agents.

183
00:10:41,685 --> 00:10:46,095
It empowers them and it takes
of the REIT tasks so people can

184
00:10:46,095 --> 00:10:50,315
focus on actual empathy, judgment,
and complex problem solving.

185
00:10:51,065 --> 00:10:55,475
I hope this gave you a practical ideas,
you can take back to your own work.

186
00:10:55,805 --> 00:10:58,595
Thanks again for joining and
I will leave you with this.

187
00:10:58,865 --> 00:11:02,355
The future of customer experience
isn't just human or ai.

188
00:11:02,655 --> 00:11:04,335
It's human with ai.

189
00:11:04,845 --> 00:11:05,505
Thank you everyone.

