1
00:00:00,090 --> 00:00:00,780
Hi everyone.

2
00:00:00,780 --> 00:00:05,370
Welcome to our Con 42 observability
session where we will talk about how we

3
00:00:05,370 --> 00:00:09,750
built an observability platform using
fluent bit open search and Prometheus.

4
00:00:10,250 --> 00:00:15,770
My name is Nian Costa, and I work as
an associate technical lead at WS Soto

5
00:00:15,830 --> 00:00:17,479
within the Couri Observability team.

6
00:00:18,110 --> 00:00:22,189
And I'm Isla and I'm a senior software
engineer within co observability.

7
00:00:22,689 --> 00:00:25,750
Before we go into the
observability platform, let me

8
00:00:25,750 --> 00:00:27,400
first explain why we needed that.

9
00:00:28,345 --> 00:00:31,044
Every company nowadays
is a software company.

10
00:00:31,825 --> 00:00:35,004
For them to serve their customers,
they need some sort of software,

11
00:00:35,694 --> 00:00:38,724
and this all starts with the
developer writing some code and

12
00:00:38,724 --> 00:00:40,495
committing it to a code repository.

13
00:00:41,185 --> 00:00:45,205
This code needs to come out as
an in product to their customers.

14
00:00:45,265 --> 00:00:49,405
This could be something like an A
PIA web application or a mobile app.

15
00:00:50,275 --> 00:00:53,335
There are several different things
that the code must go through

16
00:00:53,364 --> 00:00:54,864
for that end result to happen.

17
00:00:55,584 --> 00:01:00,175
This could be security scans,
CICD processes and so on.

18
00:01:00,675 --> 00:01:03,255
To do all of that, a platform is required.

19
00:01:03,255 --> 00:01:07,905
An internal developer platform building
an internal developer platform is

20
00:01:07,905 --> 00:01:09,885
certainly possible for every company.

21
00:01:10,455 --> 00:01:14,235
However, the problem is it takes
a lot of time and expertise.

22
00:01:14,235 --> 00:01:14,475
Expertise.

23
00:01:15,045 --> 00:01:19,695
We at ws, so two, have already
built a platform called Corio, and

24
00:01:19,695 --> 00:01:21,345
we are providing it as a service.

25
00:01:22,305 --> 00:01:27,225
Now the important internal developer
platform needs several different parts,

26
00:01:27,585 --> 00:01:29,385
and observability is one of them.

27
00:01:29,505 --> 00:01:32,865
That is because when you run some
sort of a program, you need to

28
00:01:32,865 --> 00:01:34,695
understand how it is performing.

29
00:01:34,995 --> 00:01:37,755
You need to know whether there are
any problems within it, whether

30
00:01:37,755 --> 00:01:39,195
there are any error, and so on.

31
00:01:39,705 --> 00:01:41,955
And to do that, you need observability.

32
00:01:42,705 --> 00:01:47,685
And both of us are in the co observability
team and we built this observability

33
00:01:47,685 --> 00:01:49,845
platform within our team for corio.

34
00:01:50,445 --> 00:01:54,795
And this talk would be about how we
created that observability platform.

35
00:01:55,295 --> 00:01:59,675
Let's talk a little bit about the history
of this observability side of things.

36
00:02:00,175 --> 00:02:05,425
Initially, WS O2 was heavily
invested in Asia, and our career

37
00:02:05,425 --> 00:02:09,214
platform we were building was heavily
centered around Azure as well.

38
00:02:09,714 --> 00:02:13,704
And for the observability
part, we also use Azure native

39
00:02:13,704 --> 00:02:15,564
solutions like log analytics.

40
00:02:15,774 --> 00:02:17,514
A DX and event up.

41
00:02:18,014 --> 00:02:22,334
So from this simplified architecture
diagram, you can see how our

42
00:02:22,334 --> 00:02:23,714
initial setup looks like.

43
00:02:24,084 --> 00:02:29,274
So we have few services running in our
nodes, and each known has Azure o Ms

44
00:02:29,334 --> 00:02:35,654
agent and this agent extract the logs
and telemetry from the services CPN

45
00:02:35,654 --> 00:02:39,524
Marick and write them to Asia Organics.

46
00:02:40,109 --> 00:02:46,439
And for tracing and metrices, we deployed
Open Telemetry Collector, which to

47
00:02:46,439 --> 00:02:51,919
which all the services publish data
and itself published data to A DX.

48
00:02:52,489 --> 00:02:56,989
And we wrote a simple API,
which queries both Asia and A

49
00:02:56,989 --> 00:02:59,659
DX for both login and metricses.

50
00:03:00,109 --> 00:03:04,269
And these data then published to
our dashboard where we show the.

51
00:03:04,744 --> 00:03:06,004
Logging event metrics view.

52
00:03:06,504 --> 00:03:10,644
However, there were some challenges
after we kept running this for some time.

53
00:03:11,334 --> 00:03:13,794
First one was observability
doesn't come cheap.

54
00:03:13,984 --> 00:03:18,634
Publishing logs, metrics into a cloud
platform and querying them cost a lot.

55
00:03:19,054 --> 00:03:22,144
So we wanted to try and reduce
his cost as much as possible.

56
00:03:22,644 --> 00:03:27,384
Furthermore, although we started with
Azure in the first place, as time went

57
00:03:27,384 --> 00:03:31,434
on, we got requirements from customers
to support other cloud providers.

58
00:03:31,734 --> 00:03:36,114
Some wanted to have their co data
plans that is a part of the COO

59
00:03:36,294 --> 00:03:40,854
platform on AWS, and then there were
requirements to run it on Vulture.

60
00:03:41,244 --> 00:03:46,194
Then we also wanted to support
on-premise deployments as well, so at

61
00:03:46,194 --> 00:03:50,064
the end of the day, we had to support
multiple different cloud platforms.

62
00:03:50,564 --> 00:03:54,284
Also, we wanted to have more control
over this observability stack.

63
00:03:54,704 --> 00:03:58,484
That is because if we have full
control over it, we would be able

64
00:03:58,484 --> 00:04:00,764
to build more features on top of it.

65
00:04:01,264 --> 00:04:04,144
So these were some challenges
that we wanted to address.

66
00:04:04,294 --> 00:04:08,044
And in order to do that, we
decided to create a new platform

67
00:04:08,044 --> 00:04:10,324
using some open source tools.

68
00:04:10,824 --> 00:04:15,924
So with those challenges, we defined
our next generation observability stack.

69
00:04:16,659 --> 00:04:21,229
With two main perspectives
in mind for courier users.

70
00:04:21,469 --> 00:04:26,199
Our primary customers, we wanted
to provide speed that means

71
00:04:26,199 --> 00:04:28,419
drastically reduced log latency.

72
00:04:29,139 --> 00:04:34,319
And we wanted to also provide
smarter tools such as individual

73
00:04:34,319 --> 00:04:37,559
UIs, where like you can see all
their services are interacting.

74
00:04:38,399 --> 00:04:44,939
And then another main key, the
decision we made was we want this

75
00:04:44,939 --> 00:04:47,279
platform to be zero instrumentation.

76
00:04:47,339 --> 00:04:50,789
That means user doesn't have to
do anything, they just deploy

77
00:04:50,969 --> 00:04:52,469
and we take care of the rest.

78
00:04:52,759 --> 00:04:57,689
And on the platform side one of the
main concern we had was the cost, right?

79
00:04:58,079 --> 00:04:59,669
Because observability is not cheap.

80
00:04:59,879 --> 00:05:05,159
We were racking up a lot of infrastructure
cost and we want to say, if we can reduce

81
00:05:05,159 --> 00:05:12,169
this and another, the problem that arose,
as Nisan said, the Cloud Agnos ability.

82
00:05:12,799 --> 00:05:18,689
So we initially, even though we developed
this system to run within Asia there are

83
00:05:18,689 --> 00:05:23,849
new prospect coming our way asking, can we
run this on AWS, can we run this on-prem?

84
00:05:24,449 --> 00:05:28,409
So we wanted to provide the observability
stack for those customers as well.

85
00:05:28,909 --> 00:05:31,649
And within eu, especially EU region.

86
00:05:32,004 --> 00:05:34,734
This data sovereignty is
a really important topic.

87
00:05:34,944 --> 00:05:40,784
For example, if a cluster is running
on the EU region UKO within eu, we

88
00:05:40,784 --> 00:05:45,584
can't export those data to our central
log organic solution running in us.

89
00:05:45,584 --> 00:05:46,634
So we had to run two.

90
00:05:47,414 --> 00:05:53,704
So we wanted to make sure we can further
scope these logs to the data plane

91
00:05:53,704 --> 00:05:57,844
itself, so we comply with this regulation.

92
00:05:58,534 --> 00:06:03,584
And another main thing we want to
do was have control over our logging

93
00:06:03,584 --> 00:06:08,264
sec so we can build and ship features
faster since we have full control.

94
00:06:08,764 --> 00:06:13,644
So we decided to create this new
platform and we decided to use open

95
00:06:13,644 --> 00:06:18,689
search fluent bit and Prometheus as
the main tools for that fluent wheat.

96
00:06:18,744 --> 00:06:21,354
And Prometheus was used
for the log side of things.

97
00:06:22,099 --> 00:06:25,759
In Korea, when users come and
deploy their applications, they get

98
00:06:25,759 --> 00:06:27,919
deployed within a Kubernetes cluster.

99
00:06:28,249 --> 00:06:33,859
So we have fluent bid running as a d set,
collecting logs from all these containers

100
00:06:33,859 --> 00:06:35,689
and publishing them to open search.

101
00:06:36,259 --> 00:06:41,449
Open search was used as a log storage
location, and then we use the open

102
00:06:41,449 --> 00:06:43,849
search operator to manage open search.

103
00:06:44,809 --> 00:06:49,489
Finally, we use logging a PA,
that is our own internal a PA

104
00:06:49,669 --> 00:06:51,049
to do the log fetching part.

105
00:06:51,754 --> 00:06:57,514
When a user goes into the co console, the
UI and makes a request to fetch the logs,

106
00:06:57,664 --> 00:07:02,764
the request comes through some ingress
controllers, API gateway trace, and then

107
00:07:02,764 --> 00:07:07,434
it reaches the logging API would talk
to open search, fetch the required logs,

108
00:07:07,464 --> 00:07:09,594
and send them back to the Korea console.

109
00:07:10,094 --> 00:07:10,394
Okay.

110
00:07:10,544 --> 00:07:15,434
For metrics, we adapted promises which
is a stand industry standard for metrics.

111
00:07:16,169 --> 00:07:18,119
And to manage promises.

112
00:07:18,119 --> 00:07:22,899
We use the promises operator because we
wanted to send updates to promises and

113
00:07:23,139 --> 00:07:25,389
surrounding infrastructure smoothly.

114
00:07:26,379 --> 00:07:32,629
And for long-term data
retention and high reliability.

115
00:07:32,629 --> 00:07:33,379
We use tens.

116
00:07:34,424 --> 00:07:38,359
And if you're familiar with Protus,
Protus works in a pull model rather than

117
00:07:38,449 --> 00:07:41,439
push, as pool targets, we had three.

118
00:07:41,979 --> 00:07:47,909
So the first one was Cube State Metrics
which gathers the Kubernetes context

119
00:07:47,909 --> 00:07:54,839
and export es, for example PO labels pod
status for allocations, recur and limits.

120
00:07:55,289 --> 00:07:58,079
And those get to promises
through Cube State Metrics.

121
00:07:58,679 --> 00:08:01,439
And we had see advisor
running on all Cubelets.

122
00:08:02,009 --> 00:08:06,609
Sea Advers is part of the cube blood
standard cube blood distribution which can

123
00:08:06,609 --> 00:08:14,079
be used to scrape the CPU memory and other
matrices used by the singular container.

124
00:08:14,949 --> 00:08:20,449
And we also used a tool called
Hubble, which is from Celia, which

125
00:08:20,449 --> 00:08:25,799
is a like sub project from Celia,
where we can extract its TTP level.

126
00:08:26,189 --> 00:08:30,369
Basically layer seven and layer
three to four tric without

127
00:08:30,399 --> 00:08:31,719
any user instrumentation.

128
00:08:31,749 --> 00:08:36,369
For example, through Hubbell, we were
able to extract all the Htt p status

129
00:08:36,369 --> 00:08:43,439
codes, latencies and save them within
promises so we can show how the services

130
00:08:43,439 --> 00:08:45,569
behaving if it's a H three DB service.

131
00:08:46,069 --> 00:08:50,399
And and everything, all this
information is stored in Prometheus.

132
00:08:51,239 --> 00:08:55,409
And we have your replicas of PROEs,
which is backed by a Thanos Korea,

133
00:08:56,099 --> 00:09:01,439
which aggregate this data and serve
to our metrics a p, which in turn

134
00:09:01,489 --> 00:09:03,229
feed the data to our dashboards.

135
00:09:03,729 --> 00:09:07,569
That was like the high level architecture
of the logs, PLA, and metrics

136
00:09:07,569 --> 00:09:09,279
platform, the observability platform.

137
00:09:09,639 --> 00:09:13,689
Next, we will do a deep dive into
each of these areas, and I will

138
00:09:13,689 --> 00:09:15,249
start with the log side of things.

139
00:09:15,749 --> 00:09:20,009
Now our journey with open search
and fluent bit actually started

140
00:09:20,009 --> 00:09:23,639
because of a problem that we face
when we were using log analytics.

141
00:09:24,449 --> 00:09:28,409
Now, as I mentioned earlier, when
our customers come and deploy the

142
00:09:28,409 --> 00:09:32,699
applications, they end up as containers
within our Kubernetes cluster.

143
00:09:33,599 --> 00:09:39,359
So the Asia agent that is using that is
collecting these logs, would publish them

144
00:09:39,359 --> 00:09:44,399
to log analytics, and we were querying
log analytics to fetch those logs.

145
00:09:44,909 --> 00:09:49,889
However, there were also log latency
problem, the time it took for the log

146
00:09:49,919 --> 00:09:54,449
entry to be generated by the container,
and the time that it appeared within

147
00:09:54,449 --> 00:09:59,969
log analytics was a little bit too
much as per Azure documentation.

148
00:09:59,999 --> 00:10:04,949
This latency could be anywhere
between 20 seconds and three minutes,

149
00:10:05,234 --> 00:10:09,719
and we were also experiencing
a latency around that number.

150
00:10:09,969 --> 00:10:15,279
So this affected the developer experience
in our internal developer platform because

151
00:10:15,309 --> 00:10:20,589
once customers deploy their applications
within our observability, we, there was a

152
00:10:20,589 --> 00:10:22,539
place where they could look at the logs.

153
00:10:23,139 --> 00:10:28,509
So if a login is generated and they
have to wait a minute or two, that

154
00:10:28,509 --> 00:10:30,609
hindered the developer experience a bit.

155
00:10:31,089 --> 00:10:36,119
So we wanted to try and improve this
by reducing this log latency during

156
00:10:36,119 --> 00:10:38,219
some internal discussions that we had.

157
00:10:38,829 --> 00:10:40,149
We decided on a goal.

158
00:10:40,569 --> 00:10:44,769
We wanted to bring this latency
down to five seconds or less.

159
00:10:45,279 --> 00:10:48,039
That is because we wanted
to give a tail miner safe

160
00:10:48,039 --> 00:10:49,989
experience when it comes to logs.

161
00:10:50,499 --> 00:10:54,819
So someone comes and deploys the
application and once they go into the

162
00:10:54,819 --> 00:10:59,899
runtime logs, they would see logs coming
coming as soon as they're generated.

163
00:11:00,439 --> 00:11:04,909
So we wanted to provide that experience
and we were looking for tools to do that.

164
00:11:05,689 --> 00:11:07,879
We evaluated so many tools.

165
00:11:08,029 --> 00:11:12,589
We evaluated some other observability,
external observability platforms as well.

166
00:11:13,099 --> 00:11:16,699
And finally we decided to go
with Fluent Bit and Open search.

167
00:11:17,659 --> 00:11:22,459
The initial deployment of Fluent
Bit and Open Search showed a log

168
00:11:22,459 --> 00:11:27,619
latency of about two to three
seconds, so that was very good for us.

169
00:11:27,719 --> 00:11:31,649
We were able to achieve our goal through
that, so we decided to go ahead with that.

170
00:11:32,149 --> 00:11:35,929
Now this deployment was a bit
different from the current deployment

171
00:11:35,929 --> 00:11:40,339
that we had because back then
we use open search as a cache.

172
00:11:41,119 --> 00:11:46,399
Whenever logs are generated, we pump
them into open search as well as log

173
00:11:46,399 --> 00:11:51,859
analytics, and we use two different APIs
to fetch data and show them in the uua.

174
00:11:52,549 --> 00:11:56,539
We had a live logs a PA, which
fetched data from open search.

175
00:11:56,854 --> 00:12:00,784
And a historical logs API, which
fetched data from Log Analytics,

176
00:12:01,444 --> 00:12:05,494
open search retained data for
about an hour, an hour and a half.

177
00:12:05,914 --> 00:12:08,824
And log analytics retained
for a longer period than that.

178
00:12:09,814 --> 00:12:15,394
So since open search had a lower latency,
we were able to show the logs quickly

179
00:12:15,664 --> 00:12:21,514
by fetching the that data from the live
logs API at the same time for users

180
00:12:21,514 --> 00:12:23,944
who wanted to look at historical data.

181
00:12:24,284 --> 00:12:29,934
Logs that were about a day or a seven
days old when they set that time range

182
00:12:29,964 --> 00:12:34,344
in the runtime logs wave, the platform
would switch to the historical logs

183
00:12:34,344 --> 00:12:36,804
API and fetch them from log analytics.

184
00:12:37,494 --> 00:12:42,644
So this initial deployment worked
well and we got a good learning

185
00:12:42,644 --> 00:12:46,454
experience with fluent bit and open
search, and we were actually impressed

186
00:12:46,454 --> 00:12:48,794
by its performance and ease of use.

187
00:12:49,294 --> 00:12:53,464
So therefore we decided if you're
going to build a new platform, we could

188
00:12:53,464 --> 00:12:59,044
certainly expand this that is fluent be
10 open search to store all the logs.

189
00:12:59,764 --> 00:13:04,864
So we created the log stack based
on fluent be 10 open search and

190
00:13:04,864 --> 00:13:07,174
we added some other tools as well.

191
00:13:07,894 --> 00:13:11,954
So fluent bit was used to con
collect all the container logs.

192
00:13:11,984 --> 00:13:14,925
As I mentioned earlier,
it Tran has demon sets.

193
00:13:15,224 --> 00:13:18,435
It collected all the logs and
published them into open search.

194
00:13:19,035 --> 00:13:23,324
Open search was the log storage
location and all the querying

195
00:13:23,324 --> 00:13:24,704
and collecting happened here.

196
00:13:25,634 --> 00:13:31,214
When we built this updated observability
platform, we deployed open search

197
00:13:31,214 --> 00:13:33,104
as a distributed deployment.

198
00:13:33,524 --> 00:13:37,634
Instead of just running it as an
all-in-one deployment, we separated

199
00:13:37,634 --> 00:13:42,134
out the master nodes and the data nodes
and used a distributed deployment.

200
00:13:43,109 --> 00:13:46,439
We also use the open search
operator to manage it, and I will

201
00:13:46,439 --> 00:13:47,579
explain why in a little while.

202
00:13:48,079 --> 00:13:52,310
And then we use open search
dashboards for our internal users.

203
00:13:52,849 --> 00:13:57,469
So let's say there is some sort of an
incident or a bug that is reported.

204
00:13:58,040 --> 00:14:02,269
Then what usually happens is our SRE
team needs to access the logs of those

205
00:14:02,269 --> 00:14:04,369
services to understand what is happening.

206
00:14:04,869 --> 00:14:09,669
And with Log Analytics and Azure, we had
the portal to do it, the Azure portal.

207
00:14:10,254 --> 00:14:15,534
But with open search, we decided to
go ahead with the dashboard so our SRA

208
00:14:15,534 --> 00:14:19,644
team could log into the open search
dashboard and access these logs.

209
00:14:20,274 --> 00:14:25,854
Finally, we kept using the Logging API,
which is our internal service, to talk

210
00:14:25,854 --> 00:14:29,904
to open search, fetch the logs, and
return them back to the Korea console.

211
00:14:30,404 --> 00:14:33,104
Now, when it came to the
deployment, we decided to go

212
00:14:33,104 --> 00:14:34,874
with the health-based approach.

213
00:14:35,174 --> 00:14:38,354
What we did was we based our health chart.

214
00:14:38,789 --> 00:14:42,959
On the upstream helm chart, and we
customized it the way that we wanted.

215
00:14:43,619 --> 00:14:48,899
So with the open search operator base
deployment, we had objects like open

216
00:14:48,899 --> 00:14:53,579
search index templates, open search index,
state management policies, and so on.

217
00:14:54,029 --> 00:14:58,229
So we combined all this into a helm
chart with the configurations that

218
00:14:58,229 --> 00:15:04,029
we wanted, and we wrote it to support
different cases where we needed

219
00:15:04,029 --> 00:15:05,619
different customizations as well.

220
00:15:06,119 --> 00:15:10,229
And the reason why we went with the
distributed deployment of open search

221
00:15:10,229 --> 00:15:15,410
was it is better suited to handle
production traffic when we separate out

222
00:15:15,410 --> 00:15:17,390
the master nodes and the data nodes.

223
00:15:17,810 --> 00:15:23,300
All the cluster management activities
are done by the master nodes, and then

224
00:15:23,300 --> 00:15:27,380
things like log ingestion, log querying,
they're handled by the data nodes.

225
00:15:27,680 --> 00:15:32,329
So if there is a burst of logs, if there
is a high amount of traffic for query.

226
00:15:33,200 --> 00:15:37,340
All that would go into the data nodes and
nothing would happen to the master nodes.

227
00:15:37,370 --> 00:15:39,560
They could continue managing the cluster.

228
00:15:40,160 --> 00:15:45,620
So by separating this, we were able
to achieve better availability and not

229
00:15:45,740 --> 00:15:50,820
affect different functionalities because
of something like a spike in logs.

230
00:15:51,320 --> 00:15:55,130
Also, the reason the, one of the main
reasons why we went with the operator

231
00:15:55,130 --> 00:15:57,710
was it made upgrades much easier.

232
00:15:58,324 --> 00:16:01,864
So when it comes to open search, if
you want to do a version upgrade,

233
00:16:02,194 --> 00:16:05,584
there is a particular order where
you need to upgrade the nodes.

234
00:16:05,884 --> 00:16:11,074
If we wanted to do that with multiple
Kubernetes clusters, that would require

235
00:16:11,074 --> 00:16:15,574
a lot of manual intervention because
the thing is with coo, we have this

236
00:16:15,574 --> 00:16:20,344
thing called private data planes,
where we set up private corio data

237
00:16:20,344 --> 00:16:25,114
planes for customers, and those are
dedicated for that customer itself.

238
00:16:25,639 --> 00:16:29,179
So when we get a lot of customers,
that means we will have to manage

239
00:16:29,239 --> 00:16:31,519
upgrades in each of these data planes.

240
00:16:32,029 --> 00:16:37,060
So doing things manually was
error prone, and then it would

241
00:16:37,060 --> 00:16:38,439
take a lot of time as well.

242
00:16:39,040 --> 00:16:43,030
But with the operator, it made
the upgrade process much easier.

243
00:16:43,569 --> 00:16:47,379
We could specify the new version
and the operator would handle

244
00:16:47,379 --> 00:16:49,149
the restart order and everything.

245
00:16:49,824 --> 00:16:53,994
Furthermore, when it comes to managing
the cluster in day-to-day running

246
00:16:53,994 --> 00:16:58,994
scenarios as well, operator takes
care of things like rollout, restarts,

247
00:16:59,264 --> 00:17:02,174
then other index management and so on.

248
00:17:02,774 --> 00:17:08,904
So we went with the operator and
to ensure that there is backups, we

249
00:17:08,904 --> 00:17:11,364
implemented backups at several levels.

250
00:17:12,114 --> 00:17:16,224
The first layer would be backups
within the open search cluster.

251
00:17:16,884 --> 00:17:23,244
In order to do that, we went with primary
shards and replica shards in open search.

252
00:17:23,634 --> 00:17:28,494
So what happens is when a login trace
ingested into open search, there would

253
00:17:28,494 --> 00:17:34,794
be one primary copy and one no more
replica copies Thereafter, piece spread

254
00:17:34,854 --> 00:17:40,164
all these shards into different open
search nodes, and then these nodes were

255
00:17:40,164 --> 00:17:42,414
spread across different failure domains.

256
00:17:42,414 --> 00:17:44,394
Something like availability zones.

257
00:17:44,944 --> 00:17:49,894
As an example, let's say there is an
open search setup where we have one

258
00:17:49,894 --> 00:17:54,874
primary shard, two replica shards,
and three open search data pods.

259
00:17:55,234 --> 00:17:59,614
What happens is one open search
pod would store their primary shard

260
00:18:00,004 --> 00:18:03,934
and the other two pods, they would
store the replica shards, and these

261
00:18:03,939 --> 00:18:07,624
three pods would be spread across
three different availability zones.

262
00:18:08,404 --> 00:18:10,024
The advantage of that is.

263
00:18:10,399 --> 00:18:15,820
If one of the pods go down or even
the PVC that is having the logs

264
00:18:15,820 --> 00:18:18,110
stored in the logs gets deleted.

265
00:18:18,560 --> 00:18:22,460
Once a new pod comes up, it
would be able to replicate data

266
00:18:22,460 --> 00:18:26,300
from the other open search pods
that happens within open search

267
00:18:26,300 --> 00:18:28,610
internally without any intervention.

268
00:18:29,110 --> 00:18:33,580
And by having failure domains like
availability zones, even if an

269
00:18:33,580 --> 00:18:35,560
entire availability zone goes down.

270
00:18:36,025 --> 00:18:39,145
The observability stack
could continue working.

271
00:18:39,445 --> 00:18:44,545
Log ingestion and query could continue
with the remaining pods, so that

272
00:18:44,545 --> 00:18:46,705
was like the first layer of backup.

273
00:18:47,515 --> 00:18:52,405
Then we also enabled open search
snapshots because we could

274
00:18:52,405 --> 00:18:54,475
use that as the last resort.

275
00:18:55,225 --> 00:18:58,735
Let's say we have all these
spots spread across different

276
00:18:58,735 --> 00:19:03,085
availability zones, but then due to
some disaster, we lose everything.

277
00:19:03,610 --> 00:19:09,280
In that case, we are able to restore
logs from the snapshot and the snapshot

278
00:19:09,310 --> 00:19:13,210
would store logs in a different
location than the open search cluster.

279
00:19:13,710 --> 00:19:18,360
By running this platform, we were able
to achieve a lot of cost saving as well.

280
00:19:19,170 --> 00:19:23,850
Instead of using log analytics
and in switching to fluent be

281
00:19:23,850 --> 00:19:25,710
10 open search, we were able to.

282
00:19:25,845 --> 00:19:29,485
St. Save about $1,800 per month.

283
00:19:29,695 --> 00:19:32,635
The log analytics,
ingestion, and storage cost.

284
00:19:33,355 --> 00:19:39,085
And then there was one particular
customer on AWS where they were able

285
00:19:39,085 --> 00:19:45,655
to save about 2,500 USD per day from
their AWS infrastructure course.

286
00:19:46,375 --> 00:19:50,965
Now, there was one incident where the
workloads that were deployed by this

287
00:19:50,965 --> 00:19:53,395
customer started printing some error logs.

288
00:19:53,725 --> 00:20:00,025
And all that was ingested into AWS
CloudWatch logs, and when they started

289
00:20:00,055 --> 00:20:04,795
querying the logs, this added up
querying cost as well, and that's why

290
00:20:04,795 --> 00:20:07,375
their infrastructure bills skyrocketed.

291
00:20:07,915 --> 00:20:12,925
And by moving to a platform like Open
Search, there is no extra querying cost.

292
00:20:13,315 --> 00:20:18,025
There is some cost for storage, but
the querying cost would not be there.

293
00:20:18,550 --> 00:20:22,870
That's why we were able to save such
a lot by using a platform like this.

294
00:20:23,370 --> 00:20:26,280
So there were several lessons
that we learned during this time.

295
00:20:26,810 --> 00:20:31,880
When it comes to a platform where you have
containers publishing different types of

296
00:20:31,880 --> 00:20:37,340
logs, it's better to store the logs in
the raw format that they get generated.

297
00:20:37,790 --> 00:20:41,400
And if someone wants to show
it in different formats, they

298
00:20:41,400 --> 00:20:45,180
could use different internal
services to break it up into.

299
00:20:45,540 --> 00:20:49,890
Different formats and then log
throttling is something that is

300
00:20:49,890 --> 00:20:53,910
useful when fluent, which starts
publishing logs to open search.

301
00:20:54,300 --> 00:20:57,990
There could be cases where the open
search cluster could be overwhelmed.

302
00:20:58,620 --> 00:21:02,820
To prevent something like that
from happening, we could use log

303
00:21:02,820 --> 00:21:06,540
throttling, and we were able to
do that by writing a custom lu

304
00:21:06,630 --> 00:21:08,790
script and adding it to fluent bit.

305
00:21:09,290 --> 00:21:12,470
Health monitoring is also
important in a platform like this.

306
00:21:12,830 --> 00:21:18,030
To do health monitoring, we used an
external tool to check the health

307
00:21:18,030 --> 00:21:23,280
of open search fluent bit, and our
logging, a PA and health monitoring

308
00:21:23,280 --> 00:21:24,990
has to be done at different levels.

309
00:21:24,990 --> 00:21:29,730
You need to see whether all the pods up
and running, whether the CPU usage and

310
00:21:29,730 --> 00:21:31,650
the memory usage of the mall, right?

311
00:21:32,010 --> 00:21:35,430
Whether the disc is reaching
its capacity, so on.

312
00:21:35,700 --> 00:21:37,470
So all that needs to be monitored.

313
00:21:37,970 --> 00:21:41,270
And the storage medium that
is used for open search plays

314
00:21:41,270 --> 00:21:42,650
an important role as well.

315
00:21:43,130 --> 00:21:49,630
There was one deployment that we did with
EFS in AWS and we noticed that it didn't

316
00:21:49,630 --> 00:21:55,000
have the performance that we expected
Thereafter, we switched to EBS and things

317
00:21:55,000 --> 00:21:56,950
started working properly afterwards.

318
00:21:57,400 --> 00:22:00,190
So the storage medium
plays an important role.

319
00:22:00,910 --> 00:22:05,440
Also, when you have a observability stack
like this, you need to look at everything

320
00:22:05,440 --> 00:22:07,090
as a whole when you are debugging.

321
00:22:07,540 --> 00:22:13,370
So there could be cases where fluent bit
bots would not be able to publish logs.

322
00:22:13,760 --> 00:22:17,150
However, the underlying cost could
be something to do with the storage

323
00:22:17,150 --> 00:22:19,070
medium that is used in open search.

324
00:22:19,580 --> 00:22:24,380
So simply by looking at the metrics of
fluent bit, you can't determine whether

325
00:22:24,380 --> 00:22:29,520
you need to increase those metrics
because of a. Usage increase or whether

326
00:22:29,520 --> 00:22:31,290
there's an underlying problem elsewhere.

327
00:22:31,590 --> 00:22:34,140
So that's why you need to
look at everything as a whole.

328
00:22:34,640 --> 00:22:38,080
And then when it comes to fluent
with configurations, there are so

329
00:22:38,080 --> 00:22:41,950
many configurations, and if you
don't configure them properly,

330
00:22:42,310 --> 00:22:44,230
log ingestion could be affected.

331
00:22:44,500 --> 00:22:49,210
So sometime back we were using the
iNOTiFY Watcher in the fluent be Tail

332
00:22:49,210 --> 00:22:54,170
plugin, and there was a problem where it
stopped collecting logs after some time.

333
00:22:54,710 --> 00:22:58,730
And by switching to the file STA watcher,
we were able to fix that problem.

334
00:22:59,270 --> 00:23:03,380
So you need to know the configurations
that you're using in this stacks if you

335
00:23:03,380 --> 00:23:05,720
want to make it run in a smooth manner.

336
00:23:06,220 --> 00:23:10,510
And what on screen right now is a
screenshot of the runtime logs, API.

337
00:23:10,900 --> 00:23:13,720
Here we are showing
logs of the application.

338
00:23:13,990 --> 00:23:18,040
That is the workloads deployed
by the customers and the gateway.

339
00:23:18,355 --> 00:23:21,955
And we are able to do different
types of filtering on that log.

340
00:23:22,345 --> 00:23:26,245
They are able to query using different
strings and all that is covered

341
00:23:26,395 --> 00:23:30,745
using the observability stat we built
using fluent bit and open search.

342
00:23:31,245 --> 00:23:34,050
Okay, now let's switch to metrics
side of our observability.

343
00:23:34,550 --> 00:23:39,420
So for metrics, our high level
we had three high level goals

344
00:23:39,420 --> 00:23:41,460
for, to provide to our users.

345
00:23:42,120 --> 00:23:47,260
One is provide HTP metrics a clear
insight into re request rates, error

346
00:23:47,260 --> 00:23:49,210
rates, latencies on their services.

347
00:23:49,990 --> 00:23:51,970
And the second is usage metrics.

348
00:23:52,000 --> 00:23:56,260
We wanted to show to our customers how
much resources were allocated to them,

349
00:23:56,710 --> 00:24:01,640
how much they are CPU, and memory are
used from those allocated costs usage.

350
00:24:02,570 --> 00:24:06,520
And another very exciting goal we
had was to provide a service tag.

351
00:24:06,850 --> 00:24:11,890
So this is automatically de generated
visualization of their services, how

352
00:24:11,890 --> 00:24:16,070
they interact with each other without
ze doing any user intervention.

353
00:24:16,370 --> 00:24:21,420
A key enabler of this its TTP metrics
and especially the zero instrumentation.

354
00:24:21,420 --> 00:24:27,560
And this beautiful diagram I will
show in a later site is Hubble which

355
00:24:27,560 --> 00:24:29,905
is which leverage cilium and EBPF.

356
00:24:30,405 --> 00:24:34,255
On the diagram, on the left, you
can see how a traditional service

357
00:24:34,255 --> 00:24:38,435
mesh, particular like a steel,
something would work work through.

358
00:24:39,125 --> 00:24:42,935
So every request that comes
to the particular application

359
00:24:42,935 --> 00:24:48,645
goes through a proxy within the
container, no pod itself, right?

360
00:24:48,645 --> 00:24:52,785
So that comes to the pod and then
the request get proxy to the backend.

361
00:24:53,640 --> 00:24:57,210
And during this proxy process,
we can extract telemetric like

362
00:24:57,480 --> 00:25:03,510
error rates and what, and on the
right you can see the EBPF powered

363
00:25:03,560 --> 00:25:05,860
solution cilium team has made.

364
00:25:06,360 --> 00:25:11,160
So in here you can see when a request
comes, it goes to a, when request is

365
00:25:11,160 --> 00:25:16,830
directed at a backend, it first directly
goes to a nvo proxy, which is residing

366
00:25:16,830 --> 00:25:19,620
within that node of the designation pod.

367
00:25:20,120 --> 00:25:24,180
And within this particular NY
instance, they extract the telemetry

368
00:25:24,180 --> 00:25:30,780
data and then they, again with EBPF
that request to the correct backend.

369
00:25:31,370 --> 00:25:35,530
This get rid of lot of the overhead
that will be required to run a

370
00:25:35,530 --> 00:25:39,120
service mesh at this scale to
provide zero instrumentation.

371
00:25:39,720 --> 00:25:43,890
And through Illinois, we are able
to extract red MET resource like

372
00:25:44,040 --> 00:25:50,950
error rates HT P duration high HT P
duration for HGTP and for other layer

373
00:25:51,250 --> 00:25:55,390
two three protocols, we directly
use the data extracted through EBPF.

374
00:25:55,890 --> 00:26:02,500
And so talking about the so that's the
HTB part and talking about the CPN memory.

375
00:26:02,770 --> 00:26:05,050
Usage and fire system and nectar usages.

376
00:26:05,590 --> 00:26:10,550
For that, we use a solution called
Visor which is a standard component

377
00:26:10,550 --> 00:26:13,430
integrate to every on each nod.

378
00:26:14,360 --> 00:26:20,740
So this sea advisor subprocess talk to
the container runtime running within

379
00:26:20,740 --> 00:26:25,120
the node and build up a cache of what
are the containers running within that.

380
00:26:25,675 --> 00:26:31,345
And then it interacts with the C groups
file system of that particular container.

381
00:26:32,005 --> 00:26:36,955
And from those C groups, they read
the values for CPU, memory and other

382
00:26:37,255 --> 00:26:38,785
mixtures that need to be extracted.

383
00:26:39,565 --> 00:26:44,655
And after reading those values c advisor
exposes a me matrix and point which

384
00:26:44,655 --> 00:26:47,735
the Prometheus instrument can script.

385
00:26:48,305 --> 00:26:52,355
So with this, we can, get
detailed resource users for

386
00:26:52,385 --> 00:26:54,095
every container out of the box.

387
00:26:54,595 --> 00:27:01,725
So to enrich our metrics with cube context
for a particularly labels and annotations

388
00:27:02,095 --> 00:27:07,175
we use another solution called Cube State
Metrics, which establish a connection with

389
00:27:07,175 --> 00:27:12,465
the Cube API and provide a realtime up,
up to date snapshot of the cube state.

390
00:27:13,215 --> 00:27:19,515
So this also exposes A-H-T-T-P metrics
endpoint, which promeus can come and read.

391
00:27:20,015 --> 00:27:25,095
And this data get into Prometheus and we
correlate with other information like so

392
00:27:25,095 --> 00:27:30,445
for example, from the C advisor, we can
see the pod name and we correlate that

393
00:27:30,445 --> 00:27:36,290
with the labels extracted through dates so
users can query the CT usage by a label.

394
00:27:36,590 --> 00:27:40,910
For each production rate production,
gate metric systems, reliability

395
00:27:40,910 --> 00:27:45,380
and scalability, as well as
long-term data retention is a key

396
00:27:46,080 --> 00:27:48,930
requirement for this, we use Thanos.

397
00:27:49,230 --> 00:27:55,050
So Thanos is able to integrate with lots
of existing blog storage solutions like

398
00:27:55,140 --> 00:27:58,020
S3, Google Cloud Storage, and Asia Blog.

399
00:27:58,520 --> 00:28:02,450
And also Thanos provides a
load balancing capability for

400
00:28:02,970 --> 00:28:04,930
Prometheus to run in a mode.

401
00:28:05,430 --> 00:28:08,940
If you look at this diagram,
this is a high level architecture

402
00:28:08,940 --> 00:28:10,680
view of how Thanos run.

403
00:28:10,680 --> 00:28:14,730
I won't go through each and
everything, but I just wanted to

404
00:28:14,780 --> 00:28:16,910
show the, highlight this diagram.

405
00:28:17,180 --> 00:28:20,060
So on the bottom of the screen,
you can see the external

406
00:28:20,060 --> 00:28:22,460
promises server as well as this.

407
00:28:23,360 --> 00:28:23,450
Ano.

408
00:28:23,570 --> 00:28:28,270
So these two run in a single pod, and
the Prometheus server talks to all

409
00:28:28,270 --> 00:28:32,970
the pros endpoints and extract the
data and right to the, disk, right?

410
00:28:33,480 --> 00:28:40,850
And this particular disk this file Pros
create and Cyca has a sub component

411
00:28:40,850 --> 00:28:45,920
called Shipper, which read these
files and ship it to a blob storage.

412
00:28:45,920 --> 00:28:46,580
We configure.

413
00:28:47,090 --> 00:28:49,160
So this provide the long term retention.

414
00:28:49,660 --> 00:28:52,230
And for load balancing.

415
00:28:52,730 --> 00:28:57,670
Ano also exposes a new API called
store API which is exposed through

416
00:28:57,670 --> 00:28:59,330
all the anos and ES endpoint.

417
00:28:59,870 --> 00:29:04,610
And when we are, when we deploy the
Thanos query component, as you can see

418
00:29:04,610 --> 00:29:09,840
in the top of the diagram it also have
a store, API, which connect to the

419
00:29:09,840 --> 00:29:11,670
store API running within the sidecar.

420
00:29:12,405 --> 00:29:14,235
And is able to extract data.

421
00:29:14,235 --> 00:29:19,535
So when we run a single query on
Anos query it distributed that query

422
00:29:19,535 --> 00:29:24,015
for both in both of like how many
instances they run and aggregate the

423
00:29:24,015 --> 00:29:28,135
resource and give us a single point
to get the state of the cluster.

424
00:29:28,635 --> 00:29:32,625
And there are other things like channels,
query, frontend for response caching.

425
00:29:32,745 --> 00:29:38,535
We are not currently using those and,
but what we do use is the store gateway.

426
00:29:39,255 --> 00:29:43,845
So store gateway allows us to
again query the data that was

427
00:29:43,845 --> 00:29:45,525
stored within the object storage.

428
00:29:45,855 --> 00:29:50,055
So this is another component and
which get plugged into Tenus Query.

429
00:29:50,055 --> 00:29:54,845
We are the store API and when we
query for data that is not available

430
00:29:54,845 --> 00:29:59,555
within the 10 instances running, it
automatically goes to the store gateway

431
00:29:59,825 --> 00:30:03,665
and it reads the data off of the
drop storage and give us the result.

432
00:30:04,235 --> 00:30:11,145
So this gives us a clean way to
inference metrics and yeah, that's and

433
00:30:11,535 --> 00:30:16,585
all of these complexity translate to
these beautiful design views, right?

434
00:30:16,825 --> 00:30:22,455
So in here you can see the request per
minute, total success, total fail, as

435
00:30:22,455 --> 00:30:25,345
well as latencies and other key metric.

436
00:30:25,585 --> 00:30:27,625
For example you can see the memory usage.

437
00:30:27,985 --> 00:30:29,695
And CP usage as well.

438
00:30:30,005 --> 00:30:31,025
Not just the usage.

439
00:30:31,085 --> 00:30:35,915
We can see the limit and the request
as for, and all these data are

440
00:30:35,915 --> 00:30:40,875
provided by Thanos and Prometheus
in a near real time manner.

441
00:30:41,445 --> 00:30:45,155
So users can easily understand when
there's a spike happening over, if there's

442
00:30:45,155 --> 00:30:47,735
services slowing down, what's going on.

443
00:30:47,915 --> 00:30:51,485
And another key feature here is
when you see a spike and click on

444
00:30:51,485 --> 00:30:54,085
the particular ui, we call the.

445
00:30:54,530 --> 00:30:57,730
Logging API together the related logs.

446
00:30:58,270 --> 00:31:02,480
This provides a ECN scalable way
for users to debug in production.

447
00:31:02,980 --> 00:31:03,250
Okay.

448
00:31:03,280 --> 00:31:06,200
So this is a diagram I was
talking about the service diagram.

449
00:31:06,560 --> 00:31:11,000
This a dynamically generated UI
where, which shows how each of your

450
00:31:11,000 --> 00:31:12,950
components interact with each other.

451
00:31:13,940 --> 00:31:16,370
And from here you can see the.

452
00:31:17,220 --> 00:31:21,380
You can see how the error
rates latencies and latencies

453
00:31:22,160 --> 00:31:24,170
occurs between two components.

454
00:31:24,230 --> 00:31:28,670
These are like isolated component
and particularly, for example,

455
00:31:28,670 --> 00:31:33,010
if one service is failing, we can
easily use this diagram to identify

456
00:31:33,010 --> 00:31:34,480
the root causes of the problem.

457
00:31:34,980 --> 00:31:40,760
And so this is all powered to zero
instrumentation metrics approach.

458
00:31:41,260 --> 00:31:41,560
Okay.

459
00:31:41,620 --> 00:31:46,510
So there were a lot of things we learned
the hard way while developing this system.

460
00:31:47,200 --> 00:31:52,150
So one of the key issues we were facing
as soon as we were started developing

461
00:31:52,150 --> 00:31:57,750
this system was the request count
with a request rate isn't accurate.

462
00:31:57,780 --> 00:32:03,830
So if you send 15 requests, the diagram
may show 13 points 13.8 or something.

463
00:32:04,330 --> 00:32:10,380
So our older system that chooses
open telemetry and event based

464
00:32:10,380 --> 00:32:12,330
metrics were showing the exact count.

465
00:32:12,330 --> 00:32:15,210
We were wondering what
is the reason for this?

466
00:32:15,210 --> 00:32:19,890
And we came across like few
documentation from Prometheus, say,

467
00:32:20,690 --> 00:32:25,250
if you're using proms, you can only
see the summarize and aggregated data.

468
00:32:25,760 --> 00:32:30,620
So if you want exact counts and events
for, especially for, let's say for

469
00:32:30,620 --> 00:32:33,260
billing purposes, you can't use Metrices.

470
00:32:33,260 --> 00:32:35,210
So for that, you need to use logs.

471
00:32:35,710 --> 00:32:38,110
And once we understood
that, everything clicks.

472
00:32:38,890 --> 00:32:43,600
And another thing we learned
while developing was even

473
00:32:44,050 --> 00:32:45,130
told, using a operator.

474
00:32:45,130 --> 00:32:48,880
If there is a operator, use the
operator because the operators,

475
00:32:49,630 --> 00:32:54,290
Kubernetes operators reduce the
complexity of managing stateful sets

476
00:32:54,530 --> 00:32:57,010
and underlying infrastructure for that.

477
00:32:57,490 --> 00:32:58,180
Very easy.

478
00:32:58,300 --> 00:33:03,160
Otherwise, when we are doing upgrades,
it's a multi-day planning step.

479
00:33:03,760 --> 00:33:07,180
Right now what we are doing is we
just bumped the hem shine and the

480
00:33:07,180 --> 00:33:08,680
operator takes out of the rest.

481
00:33:09,180 --> 00:33:15,350
And another issue while we actually
faced during our initial production

482
00:33:15,350 --> 00:33:19,940
deployment was slash metrics API
can screw up the whole thing.

483
00:33:20,630 --> 00:33:26,580
The reason for this was so as I mentioned,
we use huble to collect per metrics

484
00:33:27,070 --> 00:33:33,110
metrics data, and there was a bug in the
hubs, metrics endpoint where sometimes.

485
00:33:33,650 --> 00:33:36,790
The port information is not
cleared from the metrics.

486
00:33:36,790 --> 00:33:40,150
So that means like if the
port get deleted, that metrics

487
00:33:40,150 --> 00:33:41,380
information leaves forever.

488
00:33:41,900 --> 00:33:44,840
This is particularly challenging
when we have a cluster that's

489
00:33:44,840 --> 00:33:48,730
running a lot of CR jobs, which
creates tons of port each minute.

490
00:33:49,660 --> 00:33:54,980
And and turns out this is a issue
with C's port management cycle, and

491
00:33:54,980 --> 00:33:56,750
they had fixed it in a later version.

492
00:33:57,155 --> 00:34:00,705
So once we bump to that newer
version, everything resolved.

493
00:34:01,205 --> 00:34:06,305
And another issue we ran in the early days
of our development of this platform was

494
00:34:06,405 --> 00:34:09,665
we often see the corrupt file corruptions.

495
00:34:09,965 --> 00:34:14,765
So when you're querying in PROEs, if there
are like single block was corrupted and we

496
00:34:14,765 --> 00:34:19,825
are going for a longer trench, the whole
query fail saying this file is unreadable.

497
00:34:20,325 --> 00:34:24,915
And while again browsing the
documentation, we noticed like promises

498
00:34:24,975 --> 00:34:27,705
explicitly mentioned not to use NFS.

499
00:34:28,035 --> 00:34:31,485
So we were using EFS AWS NFS solution.

500
00:34:31,875 --> 00:34:35,905
And since we found out we moved
to blob storage block storage

501
00:34:36,115 --> 00:34:39,905
and all the platforms, and we
didn't had any issues since then.

502
00:34:40,405 --> 00:34:45,655
And another issue we are currently
facing is, some of rates on rate of sums.

503
00:34:46,255 --> 00:34:51,325
So when Proess has a lot of aggregation
functions, especially rate and some,

504
00:34:52,075 --> 00:34:56,995
and the order of the operations
we use matters significantly.

505
00:34:57,445 --> 00:35:01,155
So we had the recording rule
that aggregate sum data and put

506
00:35:01,155 --> 00:35:03,275
the some in our data ER storage.

507
00:35:03,755 --> 00:35:06,925
And when we are taking the
rate of that particular, time

508
00:35:06,925 --> 00:35:08,725
series, we notice some errors.

509
00:35:09,055 --> 00:35:12,295
So while investigating only we
notice this particular issue.

510
00:35:12,545 --> 00:35:20,215
So let's revise the, so let's revise the
cost because we did lot of things, right?

511
00:35:20,755 --> 00:35:24,885
So previously we spent
around close to $1,200.

512
00:35:25,410 --> 00:35:29,730
On a DX and thousand dollars on log
analytics per month, per data plane

513
00:35:30,390 --> 00:35:35,120
to retain this data and serve these to
our customers and for ourself as well.

514
00:35:35,620 --> 00:35:41,140
But with our newer open source stack,
we only pay for compute and storage.

515
00:35:41,860 --> 00:35:47,920
So since we manage everything for
compute, we use around 250 usds.

516
00:35:48,460 --> 00:35:51,670
And for storage it's around
thousand hundred sorry.

517
00:35:51,670 --> 00:35:52,600
A hundred usds.

518
00:35:53,290 --> 00:35:57,950
So this roughly translate to
around 70% overall cost savings.

519
00:35:58,450 --> 00:36:02,810
And as well as this gives
us more flexibility and more

520
00:36:02,900 --> 00:36:05,120
control and better performance.

521
00:36:05,620 --> 00:36:08,530
So that brings us to the end
of our deep dive sections.

522
00:36:08,530 --> 00:36:11,760
And next we'll go through some
summary points about what we

523
00:36:11,760 --> 00:36:13,320
learned when building this platform.

524
00:36:14,250 --> 00:36:18,730
When it comes to cloud providers
things, providers like Azure,

525
00:36:18,790 --> 00:36:22,480
AWS, they have very good support
when it comes to observability.

526
00:36:22,960 --> 00:36:26,800
They have powerful querying
languages, easy to integrate

527
00:36:26,800 --> 00:36:30,580
solutions when you want to connect,
get observative data from something

528
00:36:30,580 --> 00:36:33,380
like a Kubernetes cluster into Azure.

529
00:36:33,800 --> 00:36:38,150
So all this work really well, and,
but the thing is, when you want to

530
00:36:38,150 --> 00:36:40,190
work with multiple crowd providers.

531
00:36:40,550 --> 00:36:43,720
You will have to go through
some overhead to do that.

532
00:36:44,170 --> 00:36:47,500
That is because different cloud
providers have different querying

533
00:36:47,500 --> 00:36:51,790
languages, different APIs,
different deployment mechanisms.

534
00:36:52,120 --> 00:36:57,500
So if you want to use this cloud providers
native solutions and support multiple

535
00:36:57,500 --> 00:37:01,730
vendors, you will have to add support
for each and every cloud provider.

536
00:37:02,270 --> 00:37:06,380
But if you use a common platform where you
use a common set of services everywhere.

537
00:37:07,175 --> 00:37:10,595
It would be very easy to
enable observability for

538
00:37:10,595 --> 00:37:12,185
different cloud providers.

539
00:37:12,685 --> 00:37:15,875
And the other thing is when you
have a stack that you have full

540
00:37:15,875 --> 00:37:20,015
control over, it would be very
easy to add additional features.

541
00:37:20,225 --> 00:37:24,365
So we recently introduced an
alerting feature within coo, and

542
00:37:24,365 --> 00:37:28,985
we built that alerting feature
using the stack that we created.

543
00:37:29,015 --> 00:37:32,525
It is powered using Prometheus
through and big and open search.

544
00:37:33,025 --> 00:37:37,255
And debugging also becomes easier when
you have the same setup everywhere.

545
00:37:37,765 --> 00:37:40,945
Let's say you have different
cloud providers and different

546
00:37:40,945 --> 00:37:43,285
APIs to connect to each of them.

547
00:37:43,735 --> 00:37:47,945
If there is a problem, you need
to figure out where it went wrong.

548
00:37:48,305 --> 00:37:53,075
You need to understand about that API
and the way it's deployed and so on.

549
00:37:53,435 --> 00:37:57,275
But if it's the same setup, debugging
would be similar everywhere.

550
00:37:57,305 --> 00:37:58,925
So debugging becomes easier.

551
00:37:59,675 --> 00:38:04,925
And cost would also reduce when you use
a platform like this, a platform, an

552
00:38:04,925 --> 00:38:07,325
open source platform that you built.

553
00:38:07,805 --> 00:38:10,925
So cost could also be reduced
by doing something like that.

554
00:38:11,425 --> 00:38:16,075
And that brings us to today's session
on how we built this observability

555
00:38:16,075 --> 00:38:18,415
platform using some popular tools.

556
00:38:19,005 --> 00:38:23,085
Thank you all for joining with us
for this session and hope you were

557
00:38:23,085 --> 00:38:24,525
able to learn something from it.

558
00:38:24,795 --> 00:38:25,185
Thank you.

559
00:38:25,275 --> 00:38:25,755
Thank you all.

