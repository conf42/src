1
00:00:00,500 --> 00:00:01,249
Hello everyone.

2
00:00:01,519 --> 00:00:02,719
I am Ajani.

3
00:00:03,219 --> 00:00:07,689
We are here to talk about how to
build an intelligent platform.

4
00:00:08,380 --> 00:00:12,880
Over the years at IBM and at and
t, I've seen what it takes to run

5
00:00:12,880 --> 00:00:16,450
some of the most demanding high
scale platforms in the world.

6
00:00:16,450 --> 00:00:18,280
At IBM, we work.

7
00:00:18,780 --> 00:00:24,930
Multiple enterprise clients building
hybrid cloud platforms at and TI

8
00:00:24,930 --> 00:00:29,070
was part of projects operating at
telecom scale where millions of

9
00:00:29,070 --> 00:00:31,979
customers rely on services 24 by seven.

10
00:00:32,940 --> 00:00:39,709
That experience taught me an important
lesson, so the infrastructure doesn't

11
00:00:39,709 --> 00:00:44,300
just need to scale in size, it also
needs to scale in intelligence.

12
00:00:45,259 --> 00:00:50,149
So that's what we'll talk about
here today, how we can use AI driven

13
00:00:50,149 --> 00:00:55,419
observability and self-healing to make
platforms resilient and future ready.

14
00:00:55,919 --> 00:01:01,349
So picture this, it's 3:00 AM
and it get a call for incident.

15
00:01:01,859 --> 00:01:03,419
So you log into dashboards.

16
00:01:04,034 --> 00:01:09,824
CPU looks fine, memory looks
stable, and you dive into logs.

17
00:01:09,854 --> 00:01:15,574
Millions of lines and traces
missing half of the context.

18
00:01:15,994 --> 00:01:21,834
By the time you correlated the pieces,
the customer impact is already done

19
00:01:22,464 --> 00:01:24,994
and maybe there might be an outage.

20
00:01:25,504 --> 00:01:27,159
So that's the world.

21
00:01:28,099 --> 00:01:28,639
World.

22
00:01:28,939 --> 00:01:31,099
So lots of data, little insight.

23
00:01:31,579 --> 00:01:37,189
So our goal here is to flip that
equation and fear for alarms

24
00:01:37,189 --> 00:01:39,049
and faster root cause detection.

25
00:01:39,350 --> 00:01:44,380
Ideally, no calls at 3:00 AM
So let's look at the agenda.

26
00:01:44,880 --> 00:01:47,429
So to get there, what should we do?

27
00:01:48,620 --> 00:01:57,130
We have to build intelligence intelligent
platforms and use AI where it is essential

28
00:01:57,250 --> 00:02:01,540
to scale and identify problems beforehand.

29
00:02:02,260 --> 00:02:09,259
And how do we architect and implement this
air driven observability in the systems.

30
00:02:09,759 --> 00:02:15,880
Advance anomaly detection and what
goes beyond simple thresholds.

31
00:02:16,380 --> 00:02:22,420
And then we will look at self-healing
infrastructure, which closes the loop.

32
00:02:23,290 --> 00:02:28,180
And finally, an implementation roadmap
with real examples from my work.

33
00:02:29,110 --> 00:02:34,960
And by the end, you'll have a sense
of not just of the why, but also how.

34
00:02:35,460 --> 00:02:38,040
Okay, let's move ahead.

35
00:02:38,820 --> 00:02:44,050
So e evolution of, the platforms, right?

36
00:02:44,650 --> 00:02:46,630
So let's rewind a bit.

37
00:02:46,630 --> 00:02:51,400
At IBMI worked on the enterprise
platform where infrastructure

38
00:02:51,400 --> 00:02:55,930
meant racks of servers, VMware
clusters, or private clouds.

39
00:02:56,620 --> 00:03:00,280
So it was all about uptime,
patching, and careful provisioning.

40
00:03:00,760 --> 00:03:01,750
Fast forward.

41
00:03:01,850 --> 00:03:06,950
The platform supporting millions of API
calls every second for telecom services.

42
00:03:06,950 --> 00:03:11,720
We had to manage real time traffic,
scaling microservices across the hybrid

43
00:03:11,720 --> 00:03:16,190
environments while meeting the strict SLS.

44
00:03:17,350 --> 00:03:21,700
The, this shows the journey of the
platform engineering from managing

45
00:03:21,700 --> 00:03:27,160
the servers to managing the cloud, to
now enabling intelligent platforms.

46
00:03:27,520 --> 00:03:30,790
It's no longer enough to
provision the infrastructure.

47
00:03:30,820 --> 00:03:36,430
The platform itself has to detect,
diagnose, and in some cases heal itself.

48
00:03:36,730 --> 00:03:37,570
That's the future.

49
00:03:38,070 --> 00:03:44,130
So the observability is a foundation
of resilience, but traditional

50
00:03:44,160 --> 00:03:50,700
observability has major flas scale
metrics, logs, traces, work grade

51
00:03:50,880 --> 00:03:53,820
when you are running tens of services.

52
00:03:54,540 --> 00:04:00,090
But when you are running services in
thousands, metrics spike constantly.

53
00:04:00,590 --> 00:04:04,910
So log logs, just looking at
the logs is going to be noisy.

54
00:04:05,000 --> 00:04:07,310
90% of them doesn't matter.

55
00:04:07,820 --> 00:04:09,740
So the traces are fragmented.

56
00:04:10,220 --> 00:04:14,015
So I've seen teams at
and t drowning in alerts.

57
00:04:14,375 --> 00:04:19,445
Each microservices produces alarms,
and most of them are false positives.

58
00:04:19,895 --> 00:04:23,825
So instead of helping
observability becomes distraction.

59
00:04:24,395 --> 00:04:26,555
So the challenge is this.

60
00:04:26,885 --> 00:04:27,635
How do you.

61
00:04:28,355 --> 00:04:30,425
Extract signal from noises.

62
00:04:30,425 --> 00:04:34,625
How do you go from data
overload to real insight?

63
00:04:35,125 --> 00:04:38,425
Okay, that's where AI driven
observability comes in.

64
00:04:38,875 --> 00:04:45,435
So AI is how we bridge the gap, how
it changes the pillars, metrics.

65
00:04:45,935 --> 00:04:47,795
So these are the four pillars.

66
00:04:47,795 --> 00:04:52,295
So the metrics AI looks for
the subtle drifts, seasonality,

67
00:04:52,295 --> 00:04:55,925
and predictive spikes and logs.

68
00:04:56,315 --> 00:05:02,825
NLP models sift through millions
of lines to highlight the unusual

69
00:05:02,825 --> 00:05:04,835
patterns in the error messages.

70
00:05:05,555 --> 00:05:07,685
And then traces the graph.

71
00:05:08,075 --> 00:05:12,125
Graph analysis connects transactions
across the services showing

72
00:05:12,635 --> 00:05:18,335
cause, causality, and not just
correlation instead of C of alerts.

73
00:05:18,575 --> 00:05:20,255
AI gives you context.

74
00:05:20,615 --> 00:05:23,045
It doesn't just tell
you something is wrong.

75
00:05:23,545 --> 00:05:28,645
It suggests what's wrong and where
it started and why it's spreading.

76
00:05:29,455 --> 00:05:36,115
So think of it like moving from RAs
CCTV footage to smart assistant that

77
00:05:36,115 --> 00:05:40,435
points out the suspicion suspicious
activity before it escalates.

78
00:05:40,935 --> 00:05:43,575
Okay, so think about this.

79
00:05:43,845 --> 00:05:44,740
How do we build this?

80
00:05:44,990 --> 00:05:48,385
At and t, our architecture
looks something like this.

81
00:05:49,345 --> 00:05:58,015
So data ingestion, so streaming streams of
APIs, network logs and metrics pipelines,

82
00:05:58,405 --> 00:06:00,865
and then there's a processing layer.

83
00:06:01,365 --> 00:06:06,414
The tools like Kafka, spark
and flunk handling millions of

84
00:06:06,534 --> 00:06:08,994
events are requests per second.

85
00:06:09,474 --> 00:06:14,265
And then there are AI ML models
trained to detect anomalies,

86
00:06:14,265 --> 00:06:16,185
forecast the demand, and.

87
00:06:16,595 --> 00:06:21,785
Correlating the incidents before
they occur and integration

88
00:06:21,845 --> 00:06:23,525
and coming to integration.

89
00:06:23,525 --> 00:06:27,815
The results are pushed back into existing
systems, dashboards, and incident

90
00:06:27,815 --> 00:06:30,995
management, or even CICD pipelines.

91
00:06:31,385 --> 00:06:36,290
So integrating anomaly detection
to our CICD pipelines instead

92
00:06:36,290 --> 00:06:38,255
of deploying the broken builds.

93
00:06:39,065 --> 00:06:40,475
And discovering the problems.

94
00:06:40,475 --> 00:06:42,695
Later, we flagged the regression early.

95
00:06:43,145 --> 00:06:48,395
That meant our saved fewer customer
visible issues and better reliability,

96
00:06:48,895 --> 00:06:52,635
and coming to advanced anomaly detection.

97
00:06:53,135 --> 00:06:56,015
Thresholds are built blunt instruments.

98
00:06:56,255 --> 00:06:58,955
AI gives us more refined tools.

99
00:06:59,405 --> 00:07:04,775
Time series forecast, time series
forecasting, so predict future,

100
00:07:04,984 --> 00:07:06,724
future loads and degradation.

101
00:07:07,224 --> 00:07:13,759
Graph based correlation, NLP on logs, for
example, in telecom signaling services.

102
00:07:14,599 --> 00:07:18,284
So a service might show
tiny latency increase.

103
00:07:18,424 --> 00:07:19,444
Humans would miss it.

104
00:07:20,239 --> 00:07:26,849
An AI model trained in historical outages
can flag it precursor to a major issue.

105
00:07:27,349 --> 00:07:32,509
So that's the difference between
firefighting after the outage

106
00:07:32,509 --> 00:07:34,549
versus preventing it all together

107
00:07:34,799 --> 00:07:36,965
And then comes the root cause analysis.

108
00:07:37,465 --> 00:07:40,585
So root cause analysis is where AI shines.

109
00:07:41,064 --> 00:07:41,874
Traditionally.

110
00:07:41,874 --> 00:07:47,335
Root cause analysis means hours of
bridges on no calls and networking

111
00:07:47,335 --> 00:07:51,324
teams, database teams, application
teams, all sitting together on call

112
00:07:51,324 --> 00:07:55,374
for hours and hours together to
come up to what caused this issue.

113
00:07:55,554 --> 00:07:58,344
AI changes this by mapping dependencies.

114
00:07:58,844 --> 00:08:03,105
And it can highlight one failing
node that triggered thousand

115
00:08:03,105 --> 00:08:09,210
downstream alarms at and t using
AI enhanced dependency mapping.

116
00:08:09,570 --> 00:08:12,060
Cut our RCA time def dramatically.

117
00:08:12,930 --> 00:08:15,720
So from ours, two mere minutes.

118
00:08:16,220 --> 00:08:20,710
Thus the difference between a four
hour outage to a 20 minute hiccup.

119
00:08:21,210 --> 00:08:24,000
So let's talk about self-healing.

120
00:08:24,420 --> 00:08:28,370
Infrastructure detection is great,
but the next step is healing.

121
00:08:28,760 --> 00:08:32,620
Think of infrastructure
is like immune system.

122
00:08:33,120 --> 00:08:33,240
I,

123
00:08:33,740 --> 00:08:34,760
it fights back.

124
00:08:35,030 --> 00:08:36,800
The cycle looks like this.

125
00:08:37,159 --> 00:08:39,530
Detect anomalies, diagnose the.

126
00:08:40,039 --> 00:08:41,210
Problem, root cause.

127
00:08:41,270 --> 00:08:49,030
Decide on remediation and then learn, feed
the results back to the model example,

128
00:08:49,570 --> 00:08:55,540
as an example, automatically restarting
a failure container, scaling APIs during

129
00:08:55,540 --> 00:08:58,180
peak, promotional events like Apple.

130
00:08:58,700 --> 00:09:04,460
Apple release launch, so on and
so forth, and isolating dependency

131
00:09:04,460 --> 00:09:06,500
before taking down the system.

132
00:09:07,310 --> 00:09:11,270
Of course, we can guardrail,
we can add guardrails like

133
00:09:11,270 --> 00:09:14,420
thresholds, approvals, audit logs.

134
00:09:15,320 --> 00:09:17,870
Automation must be
powerful, but also safe.

135
00:09:18,370 --> 00:09:21,190
How do you adapt AI driven observability?

136
00:09:21,490 --> 00:09:23,320
The roadmap usually looks like this.

137
00:09:23,820 --> 00:09:25,740
Assess your observability gaps.

138
00:09:26,520 --> 00:09:27,330
Run a pilot.

139
00:09:27,450 --> 00:09:30,900
Start a small, maybe one critical service.

140
00:09:30,960 --> 00:09:36,680
Start on a critical service, then scale it
across multiple services and environments.

141
00:09:37,220 --> 00:09:41,540
The long term maintenance, continuously
refining the models, learn from the

142
00:09:41,570 --> 00:09:46,450
incidents, and then the key, incremental
key is in the incremental progress.

143
00:09:47,140 --> 00:09:48,160
You don't need to.

144
00:09:48,205 --> 00:09:50,215
I, you don't need a big bang.

145
00:09:50,215 --> 00:09:51,085
Transformation.

146
00:09:51,235 --> 00:09:54,775
Start from where the pain is the greatest.

147
00:09:55,275 --> 00:09:57,065
So where are we heading?

148
00:09:57,875 --> 00:10:00,005
I see three trends.

149
00:10:00,505 --> 00:10:04,974
So reinforcement learning applied
to operations systems that optimize

150
00:10:04,974 --> 00:10:07,045
themselves by trial and feedback.

151
00:10:08,005 --> 00:10:09,474
Safety aware automation.

152
00:10:10,225 --> 00:10:15,115
Balancing autonomy with guardrails
and then industry-wide adaptation

153
00:10:15,145 --> 00:10:21,055
of self-healing by default,
where resilience isn't an add-on,

154
00:10:21,055 --> 00:10:23,125
but a core design principle.

155
00:10:23,545 --> 00:10:26,305
This is the future of
platform engineering.

156
00:10:26,805 --> 00:10:29,264
Okay, let me leave you with this thought.

157
00:10:29,354 --> 00:10:33,884
The future of platform engineering
is not about fighting outages.

158
00:10:33,884 --> 00:10:36,224
It's about building fireproof systems.

159
00:10:36,724 --> 00:10:42,154
AI driven observability, self-healing
infrastructure helps us move from reactive

160
00:10:42,154 --> 00:10:44,374
firefighting to proactive reliability.

161
00:10:44,474 --> 00:10:52,354
It's about giving engineers more time to
innovate and less chasing around the logs.

162
00:10:53,045 --> 00:10:53,615
Thank you.

