1
00:00:00,500 --> 00:00:01,190
Hello everyone.

2
00:00:01,610 --> 00:00:05,780
I am RA principal, Salesforce engineer
and platform architect at ON 24.

3
00:00:06,649 --> 00:00:10,010
I'm excited to be speaking
at Con 42 Prompt in day 2025.

4
00:00:10,790 --> 00:00:13,310
It's a space where we explore
how to make AI systems more

5
00:00:13,310 --> 00:00:15,200
reliable and human aligned.

6
00:00:15,700 --> 00:00:18,124
My talk today is titled
Prompting for Trust.

7
00:00:18,915 --> 00:00:24,444
It's about how we as builders can design
prompts that make large language morals

8
00:00:24,544 --> 00:00:26,884
not just accurate, but trustworthy.

9
00:00:27,384 --> 00:00:30,805
In enterprise AI trust is the
real adoption barrier, and it

10
00:00:30,985 --> 00:00:33,205
starts long before deployment.

11
00:00:33,705 --> 00:00:34,935
Here's what we'll walk through today.

12
00:00:35,415 --> 00:00:39,144
First we'll look at the trust
challenge in enter enterprise ai.

13
00:00:39,685 --> 00:00:43,675
Then we will talk about why
trust starts right at the prompt.

14
00:00:44,065 --> 00:00:49,105
We will move into five core principles
for trust building prompts, some real

15
00:00:49,105 --> 00:00:52,675
world case studies, and we'll finish
with the roadmap and key takeaways

16
00:00:53,035 --> 00:00:55,194
that can apply in own ecosystems.

17
00:00:55,694 --> 00:00:57,434
The trust challenge in enterprise ai.

18
00:00:58,154 --> 00:01:02,594
In many organizations, LMS are
already running chatbots, co-pilots,

19
00:01:02,594 --> 00:01:06,554
and decision supporting tools, but
performance alone doesn't create

20
00:01:06,614 --> 00:01:11,774
adoption if users can't explain how
or why an LLM reached its answer.

21
00:01:12,449 --> 00:01:13,380
They won't act on it.

22
00:01:14,339 --> 00:01:15,300
That's the trust gap.

23
00:01:15,749 --> 00:01:18,329
An invisible friction
that slows AI rollout.

24
00:01:18,829 --> 00:01:23,029
You'll see systems technically capable
but unused because people don't

25
00:01:23,029 --> 00:01:24,559
feel confident in their reasoning.

26
00:01:25,339 --> 00:01:30,709
So the challenge isn't about model
capability, it's about human confidence.

27
00:01:31,209 --> 00:01:33,189
Trust begins at the prompt layer.

28
00:01:33,279 --> 00:01:37,359
That's why we instruct the model,
determine the way we instruct.

29
00:01:37,359 --> 00:01:39,399
The model determines how it behaves.

30
00:01:40,209 --> 00:01:41,589
Three areas matter most.

31
00:01:42,579 --> 00:01:45,999
Prompt design where we control
how the model frames its answers,

32
00:01:46,209 --> 00:01:50,699
sites information and expresses
uncertainty, context, scaffolding

33
00:01:51,509 --> 00:01:53,309
where we give structured content.

34
00:01:53,309 --> 00:01:58,379
So outputs used enterprise language,
and comply with regulations and output

35
00:01:58,379 --> 00:02:02,654
performing where we make reasoning
visible and easy to validate downstream.

36
00:02:03,644 --> 00:02:06,164
So prompt engineering isn't
about better responses.

37
00:02:06,554 --> 00:02:10,755
It's about designing comprehension,
accountability, and alignment into

38
00:02:11,234 --> 00:02:15,464
every interaction, core principles
of trust building prompts.

39
00:02:16,244 --> 00:02:19,484
Now let's talk about the five principles
that make prompts trustworthy.

40
00:02:19,984 --> 00:02:23,230
Confidence tagging, ask
the model to label outputs.

41
00:02:23,230 --> 00:02:26,650
As act now needs review
and seek expert input.

42
00:02:27,150 --> 00:02:32,310
Source of attribution, make the model side
documents or reasoning sources directly.

43
00:02:32,810 --> 00:02:33,980
Adaptive clarification.

44
00:02:34,549 --> 00:02:38,885
Let the model ask for more information
when the user's intent is unclear.

45
00:02:39,385 --> 00:02:42,854
Progressive disclosure show high
level insights first and then

46
00:02:42,854 --> 00:02:44,694
deeper details only when requested.

47
00:02:45,664 --> 00:02:50,294
Audit trail integration embed metadata
into outputs so every decision

48
00:02:50,354 --> 00:02:52,214
can be placed and audited later.

49
00:02:52,714 --> 00:02:53,074
Together.

50
00:02:53,074 --> 00:02:56,015
These principles make the system
transparent, accountable, and

51
00:02:56,015 --> 00:02:57,864
easier to trust confidence.

52
00:02:57,864 --> 00:02:59,184
Drug tagging in action.

53
00:02:59,724 --> 00:03:01,374
Here's what it looks like in practice.

54
00:03:01,974 --> 00:03:08,334
Instead of just outputting data, the model
says defect rate, moderate coined action,

55
00:03:09,054 --> 00:03:10,885
inspect the proportion of the batch.

56
00:03:11,754 --> 00:03:16,164
Now the user instantly understands the
level of certainty and the next step.

57
00:03:17,034 --> 00:03:18,174
It's no longer an answer.

58
00:03:18,204 --> 00:03:20,635
It's guided contextual response.

59
00:03:21,249 --> 00:03:26,170
Conference tagging gives users permission
to act or to validate further with clarity

60
00:03:26,670 --> 00:03:28,650
lining outputs with enterprise context.

61
00:03:29,150 --> 00:03:33,650
One major reason why trust
breakstone is context mismatch.

62
00:03:34,150 --> 00:03:37,420
Gmic elements don't understand
your organization's vocabulary,

63
00:03:37,870 --> 00:03:39,640
regulatory boundaries or thresholds.

64
00:03:40,150 --> 00:03:42,190
So the solution is structured.

65
00:03:42,970 --> 00:03:44,890
We inject domain vocabulary like.

66
00:03:45,295 --> 00:03:47,365
Qualified or material risk.

67
00:03:47,965 --> 00:03:53,185
We embed regulatory S like GDPR
or SOC compliant requirements,

68
00:03:53,905 --> 00:03:56,155
and then we define thresholds.

69
00:03:56,545 --> 00:04:01,040
Specifying, specifying where to
escalate, approve, or send for human.

70
00:04:01,880 --> 00:04:04,730
This ensures that model outputs
are consistent with enterprise

71
00:04:04,730 --> 00:04:07,299
policies and human judgment building.

72
00:04:07,799 --> 00:04:09,840
Building feedback loops into prompts.

73
00:04:10,340 --> 00:04:12,170
Trust isn't something you design once.

74
00:04:12,260 --> 00:04:13,700
It's something you maintain.

75
00:04:14,150 --> 00:04:16,490
That's why Facebook
feedback loops are critical.

76
00:04:17,210 --> 00:04:21,620
When users correct an error response,
that signal can be captured right

77
00:04:21,620 --> 00:04:23,985
into that chat or orchestration layer.

78
00:04:24,485 --> 00:04:28,355
Then we refine prompts, identify
failure models, and then we fine

79
00:04:28,445 --> 00:04:29,975
tune models based on real feedback.

80
00:04:30,965 --> 00:04:34,724
A trustworthy system doesn't just
doesn't just answer it like, here's a

81
00:04:34,724 --> 00:04:36,794
case study lead qualification assistant.

82
00:04:37,294 --> 00:04:38,194
Here's a real example.

83
00:04:38,284 --> 00:04:40,924
Sales reps are ignoring
AI generated lead scores.

84
00:04:41,734 --> 00:04:44,374
The issue wasn't accuracy,
it was explanation.

85
00:04:44,884 --> 00:04:48,184
So we redesigned the prompts
to incur confidence tags.

86
00:04:48,585 --> 00:04:52,244
Source citations and structuring
the reasoning change.

87
00:04:52,744 --> 00:04:58,744
The result is 71% more follow up actions
and 34% improvement in conversations.

88
00:04:59,135 --> 00:05:00,244
Nothing changed in the model.

89
00:05:00,395 --> 00:05:01,234
Only the prompts.

90
00:05:01,734 --> 00:05:04,195
That's the power of trust centered design.

91
00:05:04,695 --> 00:05:07,275
One more case study
summarization pipeline.

92
00:05:07,325 --> 00:05:08,765
It's a legal documentation summary.

93
00:05:08,765 --> 00:05:11,945
Teams refuse to trust AI
Aries without every clause.

94
00:05:12,095 --> 00:05:15,545
We updated prompts to add section
level attribution and highly.

95
00:05:15,910 --> 00:05:19,060
Ambiguous losses or deviations
with standard templates.

96
00:05:19,840 --> 00:05:22,810
Suddenly the review time
dropped drastically and the

97
00:05:22,810 --> 00:05:24,610
user conference rose near 90%.

98
00:05:25,110 --> 00:05:29,905
Once people could see where each statement
came from, trust followed naturally.

99
00:05:30,405 --> 00:05:33,225
This is one more yesterday,
decision support chat bot.

100
00:05:33,725 --> 00:05:36,575
When asked, should we
approve this vendor contract?

101
00:05:36,695 --> 00:05:38,525
The older version answered immediately.

102
00:05:39,125 --> 00:05:42,575
The improved version asked, is
this a new vendor or a renewal?

103
00:05:43,235 --> 00:05:45,595
What's the contract value when it checks?

104
00:05:45,865 --> 00:05:48,625
Spend thresholds and compliance
before recommending approval?

105
00:05:49,045 --> 00:05:53,095
That small shift gathering, missing
context first dramatically increased

106
00:05:53,695 --> 00:05:55,295
user reliability and trust.

107
00:05:55,795 --> 00:05:58,855
Structured prompting techniques
to make this scalable.

108
00:05:58,915 --> 00:06:00,955
We used structured prompting patterns.

109
00:06:01,615 --> 00:06:04,735
Schema driven outputs force
the model to respond in JSON

110
00:06:04,735 --> 00:06:09,355
or it, so systems can validate
automatically, chain automatically.

111
00:06:09,955 --> 00:06:13,675
Chain of thought prompting makes
the models show its reasoning

112
00:06:13,675 --> 00:06:14,725
before giving the final.

113
00:06:15,225 --> 00:06:17,505
And conditional instructions
handle edge cases.

114
00:06:17,505 --> 00:06:20,595
For example, if data is missing,
it'll ask for clarification.

115
00:06:21,585 --> 00:06:24,195
These design patterns create
transparency by default.

116
00:06:24,695 --> 00:06:28,715
This slide shows the transformation
clearly before black box outputs.

117
00:06:29,435 --> 00:06:30,785
No, no visibility.

118
00:06:30,845 --> 00:06:31,830
No confidence signals.

119
00:06:32,170 --> 00:06:37,270
Low user engagement After transparent
designing chains, source attribution,

120
00:06:37,720 --> 00:06:40,190
confidence tagging, and context awareness.

121
00:06:40,980 --> 00:06:42,270
When you treat prompt layer.

122
00:06:42,890 --> 00:06:48,800
As a design service, not a technical
input, you turn alarms from opaque

123
00:06:48,800 --> 00:06:50,930
responders into trusted collaborators.

124
00:06:51,430 --> 00:06:54,629
Now looking at the implementation
roadmap, here's how to get

125
00:06:54,629 --> 00:06:55,949
started in your own environment.

126
00:06:56,699 --> 00:07:03,359
Step one or current prompts, find where
ambiguity or opacity creates trust gaps.

127
00:07:03,859 --> 00:07:08,120
Redesign for transparency, add
tags and verification triggers.

128
00:07:08,620 --> 00:07:12,130
Test with real users, validate
comp comprehension and usability.

129
00:07:12,880 --> 00:07:15,490
And I trade based on feedback.

130
00:07:15,760 --> 00:07:18,340
Keep refining instructions
to close landing lobes.

131
00:07:18,940 --> 00:07:21,700
Trust is not a feature,
it's a continuous process.

132
00:07:22,200 --> 00:07:26,400
So the key takeaways, trust is
designed, you can architect.

133
00:07:26,400 --> 00:07:30,510
Transparency, accountability and
alignment through intentional

134
00:07:30,510 --> 00:07:33,359
prompt design context is critical.

135
00:07:33,630 --> 00:07:34,859
Structured prompting users.

136
00:07:35,359 --> 00:07:37,669
Reflect your enterprise
vocabulary and policies.

137
00:07:38,510 --> 00:07:42,740
Feedback drives improvement the most
trusted systems evolve with user input.

138
00:07:43,460 --> 00:07:46,340
And finally, users trust
what they understand.

139
00:07:46,730 --> 00:07:50,210
When we combine confidence tagging,
attribution, and progressive

140
00:07:50,210 --> 00:07:54,400
disclosure, your AI stocks being a
mystery, becomes a reliable partner.

141
00:07:54,900 --> 00:07:59,039
Thank you for your time and attention
to this Con 42 prompting 3 20 25.

142
00:07:59,669 --> 00:08:03,359
I'm Raku and I hope these ideas
help you design prompt strategies

143
00:08:03,359 --> 00:08:07,650
that earn trust, not just s Feel
free to connect and continue the

144
00:08:07,650 --> 00:08:11,400
discussion on building transparent,
human aligned element systems.

145
00:08:12,209 --> 00:08:12,780
Thank you very much.

