1
00:00:00,500 --> 00:00:02,720
Hey guys, welcome to Con 42.

2
00:00:03,380 --> 00:00:04,850
Thank you for joining this session.

3
00:00:05,410 --> 00:00:09,520
My name is she Reya and I
serve as a senior engineer

4
00:00:09,610 --> 00:00:11,110
engineering manager at Truco.

5
00:00:11,719 --> 00:00:15,320
Over the last several years, I had
the privilege of leading engineering

6
00:00:15,320 --> 00:00:19,790
teams responsible for building and
scaling large distributed systems.

7
00:00:20,229 --> 00:00:24,070
These are the kinds of platforms that
power millions of users and they come

8
00:00:24,070 --> 00:00:26,049
with some unique engineering challenges.

9
00:00:26,980 --> 00:00:31,970
One lesson I've learned is that scaling
isn't just about throwing more servers

10
00:00:31,970 --> 00:00:36,679
at a problem, anyone can provision
more compute or more databases, but

11
00:00:36,679 --> 00:00:41,839
true scale comes from making systems
resilient, adaptive, and importantly

12
00:00:42,199 --> 00:00:44,209
simple for developers to work with.

13
00:00:44,929 --> 00:00:46,894
That's really the heart of today's stock.

14
00:00:47,105 --> 00:00:48,535
We'll explore how caching.

15
00:00:49,035 --> 00:00:52,755
Something often treated as a
minor optimization can actually

16
00:00:52,755 --> 00:00:57,705
be transformed into a self-healing
intelligent infrastructure component.

17
00:00:58,095 --> 00:01:03,464
We'll see how that shift reduces latency,
improves reliability, and enables growth

18
00:01:03,495 --> 00:01:05,625
without overwhelming engineering teams.

19
00:01:06,074 --> 00:01:10,630
So let's begin by looking at the kinds of
challenges that make this shift necessary.

20
00:01:11,130 --> 00:01:15,780
So let's start with the pain points that
almost every large scale system runs into.

21
00:01:16,280 --> 00:01:20,660
As platforms grow, demand
scales faster than most.

22
00:01:20,660 --> 00:01:22,250
Infrastructure can keep up with.

23
00:01:22,750 --> 00:01:27,100
Databases that once seem to rock
solid, begin hitting connection

24
00:01:27,100 --> 00:01:28,405
limits during peak traffic.

25
00:01:28,905 --> 00:01:32,960
APIs that were fast under small
loads suddenly start chiming

26
00:01:32,960 --> 00:01:36,070
out unpredictably and, sure.

27
00:01:36,070 --> 00:01:40,360
Many of you have been in situations
where an engineering team spends days

28
00:01:40,360 --> 00:01:46,690
or even weeks firefighting, rushing to
patch database overloads, or trying to

29
00:01:46,690 --> 00:01:52,110
chase down why latency spikes during a
big traffic event and what's happening.

30
00:01:52,110 --> 00:01:55,380
In those moments, developers
are distracted from building new

31
00:01:55,380 --> 00:01:59,090
features because they're stuck
keeping the lights on tradition.

32
00:01:59,090 --> 00:02:01,250
Traditional caching helps a little.

33
00:02:01,565 --> 00:02:02,824
But it's limited.

34
00:02:03,174 --> 00:02:06,614
They have static tls those
time to leave settings.

35
00:02:06,674 --> 00:02:13,944
Al data either expires too soon leading
to unnecessary re computation, or it stays

36
00:02:13,944 --> 00:02:16,704
around too long serving scale results.

37
00:02:16,974 --> 00:02:21,239
Scaling is usually manually
manual and reactive.

38
00:02:21,694 --> 00:02:25,324
Someone notices load building
up and spins up more nodes

39
00:02:26,074 --> 00:02:27,544
and observable pretty is poor.

40
00:02:28,009 --> 00:02:31,909
Engineers don't have the data
they need until a user complains.

41
00:02:32,459 --> 00:02:36,479
This patterns repeat across companies,
industries, and technologies.

42
00:02:36,959 --> 00:02:40,889
They highlight why we need
caching and infrastructure that

43
00:02:40,889 --> 00:02:42,479
can actually adapt on its own.

44
00:02:42,979 --> 00:02:44,959
So what does a better approach look like?

45
00:02:45,319 --> 00:02:48,259
The vision is to build
systems that are self-healing.

46
00:02:48,649 --> 00:02:52,519
That adapt, recover, and scale
without manual intervention?

47
00:02:52,919 --> 00:02:55,739
There are five guiding
principles I want to highlight.

48
00:02:56,239 --> 00:02:58,729
First one is self-service platforms.

49
00:02:58,829 --> 00:03:02,999
Developers shouldn't need to
be distributed systems experts.

50
00:03:03,339 --> 00:03:08,079
If a product team wants to add
caching to an EPI, it should be

51
00:03:08,079 --> 00:03:14,199
as simple as a config change or a
notation, not VIX of deep engineering.

52
00:03:14,739 --> 00:03:15,309
Second.

53
00:03:15,744 --> 00:03:19,614
Intelligent systems traffic
patterns shift constantly.

54
00:03:20,224 --> 00:03:24,754
Maybe a product launch in Europe
causes traffic to surge at 3:00 AM

55
00:03:24,754 --> 00:03:30,694
Local time systems should notice
adept and rebalance caching strategies

56
00:03:30,694 --> 00:03:33,624
automatically self-healing mechanisms.

57
00:03:33,724 --> 00:03:34,744
Failure happens.

58
00:03:35,114 --> 00:03:37,874
A cash node goes down a replica legs.

59
00:03:37,974 --> 00:03:43,614
A region experiences network issues
instead of engineers waking up at

60
00:03:43,614 --> 00:03:48,744
2:00 AM The system should promote
replicas, reroute traffic, or rebuild

61
00:03:48,954 --> 00:03:51,474
state from logs without human input.

62
00:03:52,244 --> 00:03:54,884
Fourth one is comprehensive observability.

63
00:03:55,604 --> 00:04:01,484
Self-healing doesn't mean blind automation
teams still need reach, visibility

64
00:04:01,694 --> 00:04:04,454
dashboards, metrics and recommendations.

65
00:04:04,784 --> 00:04:06,584
So they understand what's happening.

66
00:04:07,524 --> 00:04:12,804
Fifth one is effortless scaling growth
should not require a proportional

67
00:04:12,804 --> 00:04:14,484
increase in operational effort.

68
00:04:14,964 --> 00:04:19,524
If user traffic doubles, engineers
shouldn't suddenly be working.

69
00:04:19,794 --> 00:04:23,214
Doublers, the system should
handle the growth gracefully.

70
00:04:23,714 --> 00:04:28,244
This vision transforms infrastructure
from something BRI and reactive into

71
00:04:28,244 --> 00:04:29,229
something resilient and adaptive.

72
00:04:29,729 --> 00:04:32,789
Now let's move from
vision into architecture.

73
00:04:33,179 --> 00:04:36,959
How do we actually design a caching
platform that supports this?

74
00:04:37,889 --> 00:04:42,569
A key insight is that caching
works best as a layered strategy.

75
00:04:43,119 --> 00:04:48,519
At the foundation you have database
caches reducing repeated query costs.

76
00:04:49,344 --> 00:04:54,474
About that distributed caches
act as shared key value stores

77
00:04:54,594 --> 00:04:56,394
accessible across services.

78
00:04:56,684 --> 00:05:00,744
So for example, a Redis is an
example of distributed cache.

79
00:05:01,254 --> 00:05:06,744
Then application level caches hold hot
data right where it's needed in memory.

80
00:05:07,324 --> 00:05:12,244
Finally, edge caches deliver
content closest to the user,

81
00:05:12,304 --> 00:05:13,924
improving global performance.

82
00:05:14,424 --> 00:05:16,494
But layering isn't enough by itself.

83
00:05:16,884 --> 00:05:20,514
The design is held together by
four architectural principles.

84
00:05:20,974 --> 00:05:23,344
First one is service mesh integration.

85
00:05:24,004 --> 00:05:28,654
The caching layer integrates with the
service mesh for automatic discovery,

86
00:05:28,954 --> 00:05:31,084
routing, retries, and circuit breaking.

87
00:05:31,454 --> 00:05:35,054
This services degrade
gracefully under stress.

88
00:05:35,664 --> 00:05:37,854
The second is event driven architecture.

89
00:05:38,334 --> 00:05:40,644
Caches are updated by
streams, not polling.

90
00:05:41,144 --> 00:05:45,584
That means a sink, warming,
real time currency, and even

91
00:05:45,584 --> 00:05:48,224
reconstruction state from event log.

92
00:05:48,724 --> 00:05:55,184
Third is directory based ency at a
massive scale invalidating caches.

93
00:05:56,084 --> 00:05:57,914
Cache is reliably is hard.

94
00:05:58,414 --> 00:06:02,359
Directory based currency
protocols help ensures accuracy

95
00:06:02,359 --> 00:06:03,709
across thousands of nodes.

96
00:06:04,349 --> 00:06:09,829
Layered strategy by spreading caching
responsibilities across multiple tiers,

97
00:06:10,009 --> 00:06:12,379
no single layer becomes a bottleneck.

98
00:06:12,879 --> 00:06:18,009
So what's powerful here is that this
principles combine to make caching not

99
00:06:18,009 --> 00:06:22,749
just an optimization layer, but resilient
backbone for platform engineering.

100
00:06:23,249 --> 00:06:25,379
So static rules only go so far.

101
00:06:25,909 --> 00:06:30,819
This is where machine learning steps
in with ML caching becomes adaptive.

102
00:06:31,329 --> 00:06:33,189
So let's look at some of the.

103
00:06:33,984 --> 00:06:34,944
ML techniques.

104
00:06:34,994 --> 00:06:37,304
So first one is predictive cash warming.

105
00:06:37,784 --> 00:06:43,574
Instead of waiting for a request,
ML models anticipate demand base on

106
00:06:43,664 --> 00:06:47,984
historical usage, popularity spikes,
and even time of day patterns.

107
00:06:48,284 --> 00:06:50,204
This reduces cold start delays.

108
00:06:50,604 --> 00:06:56,124
Second is adaptive, TTL management
static expirations brittle, mL

109
00:06:56,244 --> 00:07:01,794
dynamically adjusts TTLs based on
data volatility and excess patterns,

110
00:07:01,944 --> 00:07:04,404
balancing freshness with efficiency.

111
00:07:05,044 --> 00:07:07,444
Third is anomaly detection.

112
00:07:07,764 --> 00:07:11,784
ML identifies the unusual
traffic potential cash poisoning

113
00:07:11,844 --> 00:07:15,924
attempts or performance
degradations before they escalate.

114
00:07:16,484 --> 00:07:17,894
So here the impact is clear.

115
00:07:18,064 --> 00:07:21,574
Caching moves from being
reactive to being proactive.

116
00:07:22,339 --> 00:07:24,109
Predictive and self tuning.

117
00:07:24,609 --> 00:07:28,089
So how do we make this work in practice?

118
00:07:28,279 --> 00:07:31,279
There are three main
implementation patterns.

119
00:07:31,619 --> 00:07:33,749
First is automated pipelines.

120
00:07:34,119 --> 00:07:36,924
Caching infrastructure should
be defined declarative.

121
00:07:37,524 --> 00:07:41,664
Validated through GitHub's workflows
and rolled out with zero downtime.

122
00:07:41,994 --> 00:07:44,844
This ensures consistency and reduces risk.

123
00:07:45,344 --> 00:07:49,774
Second is developer friendly APIs
abstract away the complexity.

124
00:07:49,864 --> 00:07:50,704
Complexity.

125
00:07:50,954 --> 00:07:55,754
Developers should be able to annotate
methods or endpoints and instantly

126
00:07:55,754 --> 00:07:59,834
benefit from caching strategies
without needing to know the internals.

127
00:08:00,154 --> 00:08:02,194
Third is self healing mechanisms.

128
00:08:02,644 --> 00:08:06,484
Systems detect failures within
milliseconds, promote replicas

129
00:08:07,114 --> 00:08:12,754
reroute traffic seamlessly, and
reconstruct lost state from logs.

130
00:08:13,064 --> 00:08:16,275
When these patterns come together,
caching becomes resilient,

131
00:08:16,304 --> 00:08:18,549
consistent, and developer accessible.

132
00:08:19,049 --> 00:08:21,780
Let's talk about
observability and monitoring.

133
00:08:22,159 --> 00:08:24,859
None of this works if we
can't see what's happening.

134
00:08:25,800 --> 00:08:29,370
Observability is what gives
developers confidence.

135
00:08:29,850 --> 00:08:35,430
So if you can build dashboards that
could track cash or cash hit or mh

136
00:08:35,430 --> 00:08:42,630
ratios latency distributions like P
50, P 95, P 99 memory utilization and

137
00:08:43,020 --> 00:08:46,090
eviction rates error rates and timeouts.

138
00:08:46,370 --> 00:08:50,240
But then more than raw metrics,
observability needs insights.

139
00:08:50,360 --> 00:08:56,370
For example detecting that cache hit rates
dropped sharply after the deployment, or

140
00:08:56,430 --> 00:09:01,530
recommending that caching a particular
endpoint could cut latency significantly.

141
00:09:02,130 --> 00:09:06,480
Observability, if you think about
it, observability transforms raw

142
00:09:06,480 --> 00:09:11,340
data into actionable guidance, and
it's what enables true self feeling.

143
00:09:11,840 --> 00:09:14,420
What's the out outcome of all this work?

144
00:09:14,660 --> 00:09:17,960
The impact of self filling
caching is transformative.

145
00:09:18,410 --> 00:09:23,260
It basically provides latency
reduction users experience faster

146
00:09:23,260 --> 00:09:25,240
response times across the board.

147
00:09:25,590 --> 00:09:29,550
Database load reduction query
volume, drop dramatically.

148
00:09:29,690 --> 00:09:32,090
Freeing capacity, third one is high.

149
00:09:32,090 --> 00:09:36,510
Each ratios caches maintain
efficiently even during peak

150
00:09:36,510 --> 00:09:39,100
surges and global consistency.

151
00:09:39,350 --> 00:09:43,700
Geo distributed caches delivered
reliable performance worldwide.

152
00:09:44,100 --> 00:09:49,710
In other words, caching shifts from being
a small optimization to being a strategic

153
00:09:49,710 --> 00:09:53,010
enabler for scale and performance.

154
00:09:53,510 --> 00:09:55,640
So let's zoom in.

155
00:09:56,000 --> 00:09:58,880
How machine learning
actually powers caching.

156
00:09:59,360 --> 00:10:00,980
There are different ML techniques.

157
00:10:01,010 --> 00:10:04,075
And briefly describe
few ml techniques here.

158
00:10:04,465 --> 00:10:06,655
So first one is neural networks.

159
00:10:06,685 --> 00:10:12,205
So neural networks learn patterns in data
access and predict which items to preload.

160
00:10:12,705 --> 00:10:15,225
We could also apply
reinforcement learning.

161
00:10:15,705 --> 00:10:20,715
What it means is it could continuously
refine eviction strategies to

162
00:10:20,715 --> 00:10:24,285
maximize hit ratio, hit ratios.

163
00:10:24,425 --> 00:10:27,535
We could also use
clustering what it means.

164
00:10:27,565 --> 00:10:32,365
So clustering means group workloads
or users for targeted cash warming.

165
00:10:33,135 --> 00:10:37,105
The benefit includes higher hit
ratio than static approaches.

166
00:10:37,675 --> 00:10:40,705
Reduce memory usage via smarter eviction.

167
00:10:41,365 --> 00:10:45,195
Better search prediction during
traffic spikes cost efficiency

168
00:10:45,195 --> 00:10:47,355
through optimized resource use.

169
00:10:48,095 --> 00:10:52,775
Now this refrains, this sort
of, this reframes caching as a

170
00:10:52,775 --> 00:10:56,705
continuous optimization problem
powered by machine learning.

171
00:10:57,205 --> 00:11:01,015
Now let's talk about the edge where
the caching gets even trickier.

172
00:11:01,465 --> 00:11:04,975
Extending caching to the edge
introduces unique challenges.

173
00:11:05,285 --> 00:11:08,465
First one is currency at scale managed.

174
00:11:08,765 --> 00:11:11,615
And this can be managed
with eventual consistency.

175
00:11:11,645 --> 00:11:12,665
Vector locks.

176
00:11:12,965 --> 00:11:15,875
Vector clocks, and read repair mechanisms.

177
00:11:16,195 --> 00:11:20,865
The second challenges limited resources,
edge notes have less capacity.

178
00:11:21,235 --> 00:11:24,665
So dynamic sizing
intelligent eviction and.

179
00:11:25,280 --> 00:11:27,530
Compressions are essential here.

180
00:11:27,920 --> 00:11:33,410
Third one, third challenge that we
have often seen is network partitions.

181
00:11:33,820 --> 00:11:40,050
Edge caches must continue serving
users even when disconnected and

182
00:11:40,050 --> 00:11:42,030
then reconcile when reconnected.

183
00:11:42,450 --> 00:11:46,560
So by solving this caching ensures
performance is strong everywhere,

184
00:11:46,830 --> 00:11:48,990
not just in core data centers.

185
00:11:49,490 --> 00:11:52,280
So technology alone
doesn't guarantee success.

186
00:11:52,700 --> 00:11:53,720
Practices matter.

187
00:11:54,180 --> 00:11:58,050
So best practices include
self-service infrastructure.

188
00:11:58,540 --> 00:12:02,320
What it means is developers can
provision caching resources with

189
00:12:02,320 --> 00:12:07,420
one click run simulations and
configure without deep expertise.

190
00:12:07,965 --> 00:12:09,795
The second is operational excellence.

191
00:12:09,855 --> 00:12:14,685
Automating repetitive work,
applying predictive maintenance

192
00:12:14,775 --> 00:12:16,635
and minimizing to toil.

193
00:12:17,575 --> 00:12:21,415
Together these practices ensure
caching platforms are reliable,

194
00:12:21,475 --> 00:12:23,335
trusted, and developer friendly.

195
00:12:23,835 --> 00:12:28,365
The next slide talks about the
culture cultural transformation.

196
00:12:28,485 --> 00:12:31,785
So culture is the multiplier
that determines success.

197
00:12:32,225 --> 00:12:33,695
Technology alone.

198
00:12:34,085 --> 00:12:38,195
Technology alone won't scale
unless culture evolves with it.

199
00:12:38,695 --> 00:12:39,625
So what does it mean?

200
00:12:40,145 --> 00:12:44,645
First is culture, which empower
developers, give them ownership

201
00:12:44,645 --> 00:12:47,950
of caching strategies with
tools that simplify adoption.

202
00:12:48,685 --> 00:12:50,785
The second is shared responsibility.

203
00:12:51,505 --> 00:12:54,865
Reliability isn't siloed ops task.

204
00:12:55,075 --> 00:12:56,575
It's shared across teams.

205
00:12:56,975 --> 00:12:58,595
The third is data driven culture.

206
00:12:59,165 --> 00:13:02,885
Use metrics and feedback loops
to guide continuous improvement.

207
00:13:03,475 --> 00:13:05,725
The now what is the payoff?

208
00:13:05,725 --> 00:13:10,645
The payoff is faster delivery, lower
cost, and a better user experience.

209
00:13:10,915 --> 00:13:13,705
Culture is what unlocks the
full potential of technology.

210
00:13:14,205 --> 00:13:18,435
So what lessons can we draw
from building systems at scale?

211
00:13:19,125 --> 00:13:23,305
Some of the lessons include
developer experience comes first.

212
00:13:23,615 --> 00:13:27,175
Simplicity and good communication
matter more than feature overload.

213
00:13:27,685 --> 00:13:29,755
Incremental migration is essential.

214
00:13:30,115 --> 00:13:34,015
What it means is support legacy systems
while enabling smooth transitions.

215
00:13:34,665 --> 00:13:37,305
Third is observability is non-negotiable.

216
00:13:37,625 --> 00:13:40,565
Metrics, validate assumptions
and guide decisions.

217
00:13:40,975 --> 00:13:43,225
Third is expect challenges.

218
00:13:43,375 --> 00:13:45,925
Technical hurdles like hotkey.

219
00:13:46,525 --> 00:13:51,235
Technical challenges like hot keys are
solvable, but organizational resistance

220
00:13:51,235 --> 00:13:53,545
and skill gaps require just as much focus.

221
00:13:54,045 --> 00:13:57,255
And these lessons are universal
across large scale systems.

222
00:13:57,755 --> 00:14:00,695
Finally let's look at
where this field is headed.

223
00:14:01,085 --> 00:14:07,655
Near term trends include stronger ML
models, graph ql, caching optimization,

224
00:14:07,835 --> 00:14:09,905
and automated capacity management.

225
00:14:10,385 --> 00:14:10,875
Midterm.

226
00:14:11,660 --> 00:14:14,395
Midterm, we will see innovations
like serverless cache functions.

227
00:14:15,215 --> 00:14:20,845
Realtime cash analytics novel validation
methods and post quantum security

228
00:14:20,845 --> 00:14:25,885
approaches long term, the vision is
AI driven optimization, autonomous

229
00:14:25,885 --> 00:14:30,335
operations and intent base infrastructure
systems that understand what you

230
00:14:30,335 --> 00:14:32,405
want, not just what you configure.

231
00:14:33,055 --> 00:14:34,795
The TRA trajectory is clear.

232
00:14:35,025 --> 00:14:38,115
Caching and infrastructure
are becoming more intelligent,

233
00:14:38,115 --> 00:14:39,765
adaptive, and autonomous.

234
00:14:40,265 --> 00:14:42,005
Let's wrap up with the big picture.

235
00:14:42,305 --> 00:14:45,005
Self hailing caching isn't
just about performance.

236
00:14:45,035 --> 00:14:50,045
It's about reducing latency, lowering
database load, maintaining reliability

237
00:14:50,045 --> 00:14:52,115
globally, and empowering developers.

238
00:14:52,835 --> 00:14:57,995
Caching has evolved from being side
optimization into a strategic foundation

239
00:14:58,325 --> 00:15:00,155
for resilience and scalability.

240
00:15:01,055 --> 00:15:06,125
If there's one takeaway from today, it's
this investing in self-healing cashing.

241
00:15:06,470 --> 00:15:10,670
Unlocks faster systems, happier
developers, and sustainable growth.

242
00:15:11,330 --> 00:15:11,780
Thank you.

