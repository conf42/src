1
00:00:03,420 --> 00:00:04,140
Hi everyone.

2
00:00:04,980 --> 00:00:09,960
Thank you for joining this conference, con
42 conference and for joining my session.

3
00:00:10,590 --> 00:00:14,700
I am er and I'm a developer
advocate at Amazon Web Services.

4
00:00:15,510 --> 00:00:17,400
I will keep my session in.

5
00:00:18,540 --> 00:00:22,830
Less than 30 minutes, I will do a
very high level introduction of vector

6
00:00:22,830 --> 00:00:27,330
embeddings and retrieval augmented
generation, and we'll take a look at how

7
00:00:27,330 --> 00:00:30,060
these two actually compliment each other.

8
00:00:30,960 --> 00:00:34,019
If you have any questions or any
feedback while you're watching this

9
00:00:34,019 --> 00:00:37,260
session, please find me on LinkedIn.

10
00:00:37,290 --> 00:00:40,950
I'm there as er, and I would
love to hear your feedback.

11
00:00:41,489 --> 00:00:43,650
So let's not waste any more time.

12
00:00:43,650 --> 00:00:44,910
Let's get started.

13
00:00:46,080 --> 00:00:49,860
Have you ever wondered how music
applications suggest songs to you,

14
00:00:50,400 --> 00:00:55,320
or how shopping applications suggest
products that perfectly match your taste?

15
00:00:55,950 --> 00:00:59,580
To understand how all of this works,
you would have to dive into the

16
00:00:59,580 --> 00:01:04,800
world of vector databases, and this
is where data isn't just stored as

17
00:01:04,830 --> 00:01:11,039
rows in tables, but it's also mapped
out as geometric points in space.

18
00:01:12,600 --> 00:01:16,710
So now taking a look at a simple
conversation with a large language

19
00:01:16,710 --> 00:01:21,179
model, and throughout this talk
I will be interchangeably using

20
00:01:21,179 --> 00:01:26,880
the words large language model,
LLM Foundation, model based model.

21
00:01:27,240 --> 00:01:31,050
Every time I use any of these, just know
that I'm talking about the same thing.

22
00:01:31,530 --> 00:01:33,150
So looking at a brief conversation.

23
00:01:33,150 --> 00:01:37,140
Let's say you've got an application
that you've built and the user of your

24
00:01:37,140 --> 00:01:39,330
application goes in and they type.

25
00:01:39,660 --> 00:01:44,010
The capital of France is, but
because your application has

26
00:01:44,010 --> 00:01:48,539
generative AI capabilities, there
is completion of the sentence.

27
00:01:48,810 --> 00:01:51,810
The application returns Paris.

28
00:01:52,680 --> 00:01:57,180
So now we have the portion that the
user in inputted, and then we've

29
00:01:57,180 --> 00:02:05,520
got the session or the portion that
AI returned through these LLMs.

30
00:02:06,120 --> 00:02:08,340
So we've got Paris returned by ai.

31
00:02:10,215 --> 00:02:13,215
So how does all of this work?

32
00:02:13,244 --> 00:02:18,734
How did it happen that we got
this return of Paris back to

33
00:02:18,765 --> 00:02:20,685
the front end for the user

34
00:02:23,535 --> 00:02:23,864
first?

35
00:02:23,864 --> 00:02:27,375
Now looking at large language
models, defining them before

36
00:02:27,375 --> 00:02:28,635
we look at how they work.

37
00:02:29,325 --> 00:02:33,495
Large language models that are
known as LLMs as well are very

38
00:02:33,495 --> 00:02:37,875
large deep learning models that are
pre-trained on vast amounts of data.

39
00:02:38,579 --> 00:02:43,529
The underlying transformer is a set
of neural networks that consists

40
00:02:43,559 --> 00:02:48,660
of an encoder and a decoder that
has self attention capabilities.

41
00:02:49,440 --> 00:02:54,209
This means that the encoder and
the decoder extract meanings from a

42
00:02:54,209 --> 00:02:58,859
sequence of text, and they understand
the relationships between words

43
00:02:58,920 --> 00:03:01,589
and phrases that are in this text.

44
00:03:03,885 --> 00:03:12,464
So this means then that we cannot send raw
text directly to models because machine

45
00:03:12,464 --> 00:03:15,464
learning models only understand numbers.

46
00:03:15,554 --> 00:03:20,774
So when that text was typed by the user in
the front end, there would've had to be a

47
00:03:20,774 --> 00:03:25,755
conversion to numbers so that there can be
engagement or interaction with the models.

48
00:03:27,089 --> 00:03:31,469
What we are looking at is a
numerical representation then of

49
00:03:31,469 --> 00:03:33,600
that sentence that we saw earlier on.

50
00:03:35,399 --> 00:03:42,359
Looking at another example, let's consider
the terms coffee and tea in a hypothetical

51
00:03:42,359 --> 00:03:47,159
vocabulary space, these two could be
transformed into numerical vectors.

52
00:03:47,760 --> 00:03:54,049
So if we visualize this in a tree
or in a three dimensional vector

53
00:03:54,049 --> 00:04:00,829
space, coffee might be represented
as a set of numbers and tea might be

54
00:04:00,889 --> 00:04:02,644
represented as another set of numbers.

55
00:04:03,844 --> 00:04:08,044
So these numerical vectors then
carry semantic information.

56
00:04:08,494 --> 00:04:12,724
This indicates that coffee and
tea are conceptually similar to

57
00:04:12,724 --> 00:04:17,794
each other because they have an
association with hot beverages.

58
00:04:18,125 --> 00:04:21,635
This means then that they
likely will be positioned closer

59
00:04:21,635 --> 00:04:23,434
together in the vector space.

60
00:04:25,445 --> 00:04:28,054
Now we are adding another word cappuccino.

61
00:04:28,835 --> 00:04:31,534
And cappuccino could be
something in between.

62
00:04:32,179 --> 00:04:34,429
Cappuccino has some coffee in it.

63
00:04:34,849 --> 00:04:39,739
It might not look like it on my graph,
but it's very likely that cappuccino will

64
00:04:39,739 --> 00:04:44,090
be stored a lot closer to where coffee
is because there's coffee in cappuccino.

65
00:04:45,319 --> 00:04:50,179
The actual positioning of these
words is based on a training process

66
00:04:50,539 --> 00:04:54,349
and is also based on how these
words are used on the large amount

67
00:04:54,349 --> 00:04:56,809
of text that is used for training.

68
00:04:58,294 --> 00:05:03,784
So we can think of these words on paper
as vectors, and one way to look at them is

69
00:05:03,784 --> 00:05:06,395
like looking at them like they are arrows.

70
00:05:06,994 --> 00:05:12,005
All of them starting from one of the
corners and the direction of the arrow.

71
00:05:12,005 --> 00:05:15,125
Then bringing the meaning of the word.

72
00:05:16,445 --> 00:05:20,855
Now I can start to scale or
setting up a scale on the sites

73
00:05:20,855 --> 00:05:25,235
and writing down the distance from
the corner to each of the words.

74
00:05:26,885 --> 00:05:31,565
I can use these numbers to uniquely
identify these words in these two

75
00:05:31,625 --> 00:05:33,875
dimensional space that we have.

76
00:05:34,534 --> 00:05:39,665
And then by doing this, we have
effectively translated words into numbers.

77
00:05:40,145 --> 00:05:45,215
And these words embed some of the
meaning of the original words.

78
00:05:46,625 --> 00:05:49,594
Of course, two dimensions like.

79
00:05:50,525 --> 00:05:53,945
One piece of paper are not
enough to map the complexity

80
00:05:53,945 --> 00:05:55,864
of a vocabulary into numbers.

81
00:05:56,405 --> 00:06:00,965
For this, we need more dimensions
and we need more numbers, and we need

82
00:06:00,965 --> 00:06:03,544
hundreds, if not thousands of them.

83
00:06:06,034 --> 00:06:08,405
So right now, where do we stand?

84
00:06:08,405 --> 00:06:12,905
We've already looked at the text that was
captured by the user in the front end,

85
00:06:13,145 --> 00:06:15,664
and they said the capital of France is.

86
00:06:16,445 --> 00:06:21,034
Then we saw the input context
represented as tokens in the

87
00:06:21,034 --> 00:06:22,895
numbers that we saw earlier on.

88
00:06:23,674 --> 00:06:28,265
And we've had, we've since then gone
and defined what embeddings are, which

89
00:06:28,265 --> 00:06:32,614
are these numerical representations
of how these words are stored.

90
00:06:33,695 --> 00:06:37,174
So we've learned that converting
our data into vectors is the

91
00:06:37,174 --> 00:06:38,645
first thing that we need to do.

92
00:06:40,385 --> 00:06:43,085
So now let's think about this.

93
00:06:43,444 --> 00:06:47,344
Does this mean that LMS then
can answer all of our questions?

94
00:06:48,784 --> 00:06:54,604
So embeddings transform data
into numerical vectors, making

95
00:06:54,604 --> 00:06:56,854
them highly adaptable tools.

96
00:06:57,515 --> 00:07:02,494
They enable us to apply mathematical
operations to assess similarities

97
00:07:02,944 --> 00:07:06,694
or integrate these words into
various machine learning models.

98
00:07:07,715 --> 00:07:08,794
There uses.

99
00:07:09,349 --> 00:07:15,500
Diverse ranging from search and
similarity assessments to categorization

100
00:07:15,560 --> 00:07:20,120
and topic identification, and this
flexibility is great because it then

101
00:07:20,120 --> 00:07:26,270
makes embeddings a fundamental component
in many data-driven applications.

102
00:07:30,530 --> 00:07:30,679
There are.

103
00:07:31,610 --> 00:07:33,800
Different ways to create embeddings.

104
00:07:34,599 --> 00:07:38,659
Just some that we could consider and
we'll look at a demo later on using

105
00:07:38,659 --> 00:07:47,119
bottle three in AWS and you could
also use link chain bottle three.

106
00:07:47,119 --> 00:07:51,539
We can use that with a bedrock client,
which will be looking at later on.

107
00:07:53,124 --> 00:07:54,234
And I talked about a demo.

108
00:07:54,234 --> 00:07:58,044
So we look at a demo on how we
can create embeddings using bot

109
00:07:58,284 --> 00:08:00,474
three with a bedrock client.

110
00:08:01,284 --> 00:08:06,775
And what we are now seeing on
the screen is sample code that

111
00:08:06,775 --> 00:08:08,934
we can use for this and just.

112
00:08:09,189 --> 00:08:10,330
Stepping through it.

113
00:08:10,390 --> 00:08:18,219
We see right at the beginning that we
initialize a session with AWS using

114
00:08:18,219 --> 00:08:24,640
bot three, and we then create a client
for the Bedrock Runtime Service.

115
00:08:25,989 --> 00:08:32,620
Next, we then define a get embedding
function, which takes text as

116
00:08:32,620 --> 00:08:39,069
input and also then utilizes the
Amazon Titan Embeddings model to

117
00:08:39,069 --> 00:08:41,829
transform this text into embeddings.

118
00:08:42,309 --> 00:08:45,490
And once the embedding is
generated, the function then

119
00:08:45,520 --> 00:08:47,890
returns the embedding vector.

120
00:08:48,219 --> 00:08:52,649
We can see there that's
returned as a response.

121
00:09:00,869 --> 00:09:06,299
So now moving on from vector embeddings
and looking at retrieval, augmented

122
00:09:06,299 --> 00:09:10,649
generation and Reg, and looking
at how these two actually talk to

123
00:09:10,649 --> 00:09:14,159
each other or how they relate or
how they compliment each other.

124
00:09:14,639 --> 00:09:16,379
Now looking to define re.

125
00:09:17,399 --> 00:09:22,319
Imagine that you have a database
or you've got a document.

126
00:09:22,829 --> 00:09:28,109
This is in PDF, and this is storing
courses that are available for your

127
00:09:28,109 --> 00:09:30,959
internal staff at your organization.

128
00:09:31,079 --> 00:09:35,489
So staff can just look and see what
kind of training they can have, and this

129
00:09:35,489 --> 00:09:37,319
is all internal to your organization.

130
00:09:38,369 --> 00:09:42,389
So can anyone answer then any
questions that are related

131
00:09:42,659 --> 00:09:44,069
to these internal courses?

132
00:09:45,599 --> 00:09:48,779
Can they answer questions such
as, how many members of staff have

133
00:09:48,779 --> 00:09:50,459
completed a particular course?

134
00:09:51,179 --> 00:09:55,739
How long on average does it take an
employee to complete a course and

135
00:09:55,739 --> 00:09:57,179
which course is the most popular?

136
00:09:58,739 --> 00:10:04,589
And at this point, this is where then
customization of the response from

137
00:10:04,589 --> 00:10:07,199
the L LMS then becomes a necessity

138
00:10:09,479 --> 00:10:11,399
retrieval augmented generation.

139
00:10:11,894 --> 00:10:16,604
Also known as Reg is a process
of optimizing the output

140
00:10:16,634 --> 00:10:18,104
of a large language model.

141
00:10:18,794 --> 00:10:23,414
It does this so that and it does this by
referencing an authoritative knowledge

142
00:10:23,414 --> 00:10:28,694
base that is outside of its training
data sources before generating a

143
00:10:28,694 --> 00:10:31,094
response and returning that to the user.

144
00:10:32,144 --> 00:10:37,484
LMS are trained on vast volumes of data,
and they use billions of parameters to

145
00:10:37,484 --> 00:10:43,934
generate original output for tasks, like
answering questions like translating

146
00:10:43,934 --> 00:10:46,544
languages or completing a sentence.

147
00:10:50,174 --> 00:10:56,159
RE extends this already powerful
capability of LLMs to specific domains

148
00:10:56,459 --> 00:11:02,849
or to specific organizations, internal
knowledge base, such as what courses are

149
00:11:02,849 --> 00:11:05,639
being taken by staff at this organization.

150
00:11:06,059 --> 00:11:10,439
It does all of that without
the need to retrain the LLM.

151
00:11:11,189 --> 00:11:14,549
It's a very cost effective
approach to improving LLM output

152
00:11:14,549 --> 00:11:16,319
so that it remains relevant.

153
00:11:16,964 --> 00:11:20,114
Accurate and useful in various contexts.

154
00:11:21,869 --> 00:11:25,859
So you might feel that, okay,
this is too much of a task.

155
00:11:26,039 --> 00:11:30,539
How do I now, in addition to the LLMs
that I already have, how do I then

156
00:11:30,839 --> 00:11:35,199
bring this authoritative information
or data close to the L LMS so

157
00:11:35,199 --> 00:11:38,259
that I get a customized responses?

158
00:11:38,619 --> 00:11:41,949
So you could be thinking, how do I
manage the multiple data sources?

159
00:11:42,339 --> 00:11:45,339
How do I create vector embeddings
for large volumes of data?

160
00:11:45,649 --> 00:11:50,029
How do I do incremental updates to
vector stores, the coding efforts

161
00:11:50,029 --> 00:11:53,899
that might be involved, the scaling
of this retrieval mechanism and

162
00:11:53,899 --> 00:11:56,059
the orchestration of all of this.

163
00:11:57,430 --> 00:12:04,540
So on AWS we have Amazon Bedrock Knowledge
Bases, and this feature gives foundation

164
00:12:04,540 --> 00:12:09,280
models and agents contextual information
from your private data sources.

165
00:12:09,280 --> 00:12:14,619
For Reg, with or on Amazon Bedrock
knowledge bases, you get fully managed

166
00:12:14,619 --> 00:12:17,319
support for end-to-end workflows.

167
00:12:17,859 --> 00:12:20,410
You get to securely connect
your foundation models.

168
00:12:20,410 --> 00:12:22,360
And your agents to data sources.

169
00:12:22,689 --> 00:12:27,430
You get to easily retrieve relevant
data and augment prompts, and you

170
00:12:27,430 --> 00:12:29,295
get to provide source attribution.

171
00:12:31,574 --> 00:12:35,444
There is, we saw earlier on with the
previous slide, the text generation

172
00:12:35,444 --> 00:12:40,904
workflow portion of engaging with
retrieval augmented generation.

173
00:12:40,904 --> 00:12:43,814
So this, you get to do an Amazon bedrock.

174
00:12:43,814 --> 00:12:49,275
This is where you augment the
prompt before you get the response

175
00:12:49,275 --> 00:12:51,865
that gets sent back to the user.

176
00:12:52,689 --> 00:12:55,329
This is after you've received
the input from the user.

177
00:12:55,360 --> 00:13:03,220
And there's also the API, the
retrieve API that is used to

178
00:13:03,220 --> 00:13:05,170
enrich or get more context.

179
00:13:05,680 --> 00:13:11,290
That also gets to be used to enrich
the prompt or augment the prompt.

180
00:13:11,920 --> 00:13:14,800
All of this is happening within
Amazon Bedrock knowledge basis.

181
00:13:14,800 --> 00:13:17,770
And then ultimately there's
another API retrieve.

182
00:13:18,370 --> 00:13:23,710
And generate API that would then do the
retrieval of the response that then gets

183
00:13:23,710 --> 00:13:26,170
sent back to the user in the front end.

184
00:13:28,060 --> 00:13:33,785
So you can get started with Amazon Bedrock
you can scan the QR code that you see.

185
00:13:34,655 --> 00:13:39,475
And if you wanna find out more or learn
more about large language models, there's

186
00:13:39,475 --> 00:13:41,605
another QR code that you can scan.

187
00:13:41,935 --> 00:13:46,495
And there's also training that I've
seen from one of my colleagues on

188
00:13:46,525 --> 00:13:51,205
multimodal re and embeddings with
Amazon Nova and Bedrock on AWS.

189
00:13:51,565 --> 00:13:55,095
You can consult that course to
learn more and dive deeper into the

190
00:13:55,095 --> 00:13:58,545
topic that I introduced us to today.

191
00:13:59,085 --> 00:14:02,325
And with that, I thank you
very much and thank you for.

192
00:14:03,180 --> 00:14:07,950
Coming to this session or for watching
this session, and I look forward to

193
00:14:07,950 --> 00:14:13,140
hearing from you on LinkedIn about the
feedback that you have regarding this

194
00:14:13,290 --> 00:14:15,060
topic that we just went through today.

195
00:14:15,360 --> 00:14:18,000
Thank you very much and enjoy
the rest of the conference.

196
00:14:18,270 --> 00:14:18,780
Bye-bye.

