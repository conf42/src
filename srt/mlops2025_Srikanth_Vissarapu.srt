1
00:00:00,500 --> 00:00:01,130
Hello everyone.

2
00:00:01,200 --> 00:00:06,090
This is Han with, I work as a software
engineer at Meta for this talk.

3
00:00:06,180 --> 00:00:09,560
I would be covering scalable mLab
pipelines that actually work.

4
00:00:09,620 --> 00:00:14,819
Essentially, what are the, some of the
best practices that can help to move model

5
00:00:14,819 --> 00:00:18,595
development from, whatever model developed
from laptop production essentially.

6
00:00:19,095 --> 00:00:22,214
The first, we'll talk about
the existing challenges.

7
00:00:22,314 --> 00:00:23,874
If you look at the global landscape.

8
00:00:24,564 --> 00:00:28,994
Looks like we can see ML deployments
are growing at a rapid pace,

9
00:00:29,564 --> 00:00:32,774
almost like 40% annually, or
probably more than that right now.

10
00:00:32,894 --> 00:00:35,044
And at success remains extremely low.

11
00:00:35,224 --> 00:00:40,234
So if you look at some of the statistics
we can see, you can see almost like 87% of

12
00:00:40,234 --> 00:00:41,824
the thel projects never reach production.

13
00:00:41,824 --> 00:00:45,439
A lot of them will die down much before
experimentation phase or much before that.

14
00:00:46,279 --> 00:00:50,539
Even for the 13% of them which actually
reach production, we can see that, there

15
00:00:50,539 --> 00:00:54,229
are a lot of operational challenges
and eventually they may not deliver

16
00:00:54,229 --> 00:00:57,539
the success metrics that we want
to see in production environments.

17
00:00:57,539 --> 00:01:01,589
Like it could be challenges around
model drift, infrastructure bot likes,

18
00:01:02,099 --> 00:01:05,354
or even operational complexity of how
we have built the whole pipelines.

19
00:01:06,014 --> 00:01:10,925
So what are we trying to what would we
be covering at the, as part of this talk?

20
00:01:10,984 --> 00:01:13,294
I think this is a brief agenda
of what we would be covering.

21
00:01:13,714 --> 00:01:17,554
First we'll try to look at what is the
existing state of enterprise ops, what

22
00:01:17,554 --> 00:01:18,994
is the current landscape look like?

23
00:01:19,384 --> 00:01:23,974
And we'll talk about, what are the s ml
ops architectures that we can look at

24
00:01:24,044 --> 00:01:28,874
essentially all towards, what are the
efficient model development, deployment,

25
00:01:28,874 --> 00:01:31,994
management companies, monitoring, and
all those different aspects of it.

26
00:01:32,774 --> 00:01:35,854
And the next important aspect is what
are the implementation strategies

27
00:01:35,854 --> 00:01:40,194
that we can implement, mostly into
technical solutions of what are the,

28
00:01:40,434 --> 00:01:45,804
how do we have our right methods in
place for model validation testing ml

29
00:01:45,994 --> 00:01:49,714
ML workflows for seamless M-L-C-S-E-D
deployment pipelines and all that.

30
00:01:50,554 --> 00:01:54,335
Another important aspect is also
like organizational transformation.

31
00:01:54,335 --> 00:01:58,394
It's not only about the tools, it's also
about how do we organizationally function

32
00:01:58,394 --> 00:02:02,654
in such a way that, all, the teams like,
data science engineering and operation

33
00:02:02,654 --> 00:02:06,744
teams work together in collaboration
to make this whole journey a success.

34
00:02:07,244 --> 00:02:11,224
And eventually I think we'll also touch
upon what are the emerging trends that we

35
00:02:11,224 --> 00:02:16,324
can look at, so that we are building these
pipelines or normal ops for the future.

36
00:02:16,434 --> 00:02:19,284
So the first thing looking at,
what is the current state, but

37
00:02:19,284 --> 00:02:22,184
just starting with that, there
are two quadrants we can look at.

38
00:02:22,184 --> 00:02:26,474
One extreme is like purely manual,
low maturity, and other side

39
00:02:26,474 --> 00:02:28,004
is mostly like full automation.

40
00:02:28,004 --> 00:02:28,664
High maturity.

41
00:02:29,154 --> 00:02:30,634
ML lops systems.

42
00:02:31,114 --> 00:02:33,354
Again looking at, the
manual side of the things.

43
00:02:33,354 --> 00:02:35,834
Essentially, if you look at, there
are processes around starting

44
00:02:35,834 --> 00:02:39,454
from left bottom side, there's
like limited pipeline automation.

45
00:02:40,204 --> 00:02:41,644
And from the left off we can see.

46
00:02:41,704 --> 00:02:41,944
Okay.

47
00:02:41,944 --> 00:02:45,684
But these are different different
systems we usually see in,

48
00:02:45,984 --> 00:02:47,589
in, in a production, right?

49
00:02:47,989 --> 00:02:52,369
So mostly it'll start, there it can
be like, some systems can only be ad

50
00:02:52,369 --> 00:02:53,989
hoc experimentation, mostly manually.

51
00:02:53,989 --> 00:02:55,729
We just do some kind of experimentation.

52
00:02:56,139 --> 00:03:00,939
Mostly manual, no automation primarily
engineers or scientists or essentially.

53
00:03:01,274 --> 00:03:04,364
Billing pipelines, testing them,
and ad hoc as the core, right?

54
00:03:04,754 --> 00:03:08,204
And and the other aspect is there
could be minimum automation, limited

55
00:03:08,204 --> 00:03:10,374
pipeline automation for scheduled jobs.

56
00:03:10,374 --> 00:03:13,204
Maybe like a small CR
jobs or stuff like that.

57
00:03:13,284 --> 00:03:17,254
But that's about it mostly, like manual
and low maturity kind of models, right?

58
00:03:17,764 --> 00:03:22,404
If you look at high maturity and the other
spectrum of it the ML lops systems, right?

59
00:03:22,884 --> 00:03:25,314
Team specific ML lops
systems, but it can be like.

60
00:03:25,814 --> 00:03:29,594
Every team or every company has
its own lot of internal tools.

61
00:03:30,094 --> 00:03:33,044
How do we deploy package
package the models, deploy the

62
00:03:33,044 --> 00:03:34,689
models, and and also test them.

63
00:03:34,999 --> 00:03:36,039
All those construct are.

64
00:03:36,539 --> 00:03:40,619
Relatively different in terms of how
companies adopt such methodologies.

65
00:03:40,989 --> 00:03:44,639
So having that consistent or, having
that having that repeatable workflows,

66
00:03:44,689 --> 00:03:48,499
how do we all through this life cycle
of model development, of deployment

67
00:03:48,499 --> 00:03:51,679
and testing and all those aspects,
how do we have that repeatable

68
00:03:51,679 --> 00:03:53,389
workflows is also extremely important.

69
00:03:53,389 --> 00:03:59,129
Having that as a. Is in a, is a maturity
level of any m lops systems in a way.

70
00:03:59,669 --> 00:04:02,759
And the other important aspect,
we can also look at enterprise

71
00:04:03,149 --> 00:04:06,539
lops and a full automation, data
driven where we have metrics to

72
00:04:06,539 --> 00:04:08,129
capture at every stage of model.

73
00:04:08,749 --> 00:04:13,879
Deployment when there is development
or testing and even deployment

74
00:04:14,129 --> 00:04:18,089
we can have lot of testing
baked into at every phase of it.

75
00:04:18,449 --> 00:04:22,069
And eventually looking at the metrics,
we proceed with the model deployment or

76
00:04:22,349 --> 00:04:25,289
roll back or go to different versions
and all those different aspects, which

77
00:04:25,289 --> 00:04:27,154
we will touch upon in a brief file now.

78
00:04:27,654 --> 00:04:31,074
Again, a similar aspect to it, just to
elaborate on what I've been touching

79
00:04:31,074 --> 00:04:34,814
upon before is the first level Zero is
mostly about ad hoc experimentation.

80
00:04:34,814 --> 00:04:36,314
People just start with something new.

81
00:04:36,564 --> 00:04:41,044
And also purely manual with with
no standardized workflows at all,

82
00:04:41,374 --> 00:04:44,684
and fragmented collaboration,
people collaborating is also

83
00:04:44,714 --> 00:04:46,334
more mostly fragmented in a way.

84
00:04:46,334 --> 00:04:49,814
And there is no desired metrics or
success metrics defined and model

85
00:04:49,814 --> 00:04:51,614
performance also lacks a lot, right?

86
00:04:52,074 --> 00:04:55,369
And the next level in terms of
maturity is like pipeline automation.

87
00:04:55,369 --> 00:04:58,159
Now we don't have a lot
of automation in place.

88
00:04:58,159 --> 00:05:00,849
Then we come up with some
kind of automation, the basic

89
00:05:00,849 --> 00:05:02,109
automation we can say, right?

90
00:05:02,429 --> 00:05:06,509
And it even though we made some progress
in terms of pipeline automation there

91
00:05:06,509 --> 00:05:07,979
are still some lacking statements.

92
00:05:07,979 --> 00:05:10,229
Things like, we don't have really
good governance frameworks.

93
00:05:10,509 --> 00:05:12,309
Teams are pretty siloed in a way.

94
00:05:12,619 --> 00:05:16,489
And also the monitoring is not
robust enough to identify model

95
00:05:16,489 --> 00:05:20,079
thrifts or, all kinds of performance
issues that we can see, right?

96
00:05:20,129 --> 00:05:23,249
That, that is a state of level
one we can briefly talk about.

97
00:05:23,639 --> 00:05:27,629
Level two is like a little more
advanced or matured kind of systems

98
00:05:27,629 --> 00:05:29,339
is mostly continuous integration.

99
00:05:29,679 --> 00:05:31,599
We have standardized repeatable pipelines.

100
00:05:31,599 --> 00:05:37,164
You can bake in how you or want to have
a model go through the system to have it.

101
00:05:37,474 --> 00:05:39,874
Seamless or repeatable
pipelines, essentially, right?

102
00:05:40,264 --> 00:05:42,784
And also we have robust version control.

103
00:05:42,924 --> 00:05:46,674
Every model we put a
version to it and all that.

104
00:05:46,774 --> 00:05:47,164
Stuff.

105
00:05:48,034 --> 00:05:51,194
At the same time automated
testing extremely important.

106
00:05:51,244 --> 00:05:54,644
When we deploy a model every
phase having that automated

107
00:05:54,644 --> 00:05:56,174
testing is extremely important.

108
00:05:56,224 --> 00:05:57,464
Ladies has lot of manual effort.

109
00:05:58,209 --> 00:06:02,709
And also proactive having that really
sound monitoring is also extremely

110
00:06:02,709 --> 00:06:06,489
important in when we talk about,
enhancing this lops pipelines.

111
00:06:07,479 --> 00:06:10,299
And the next important thing, I
think this is like a this is like

112
00:06:10,299 --> 00:06:11,999
the we have the best in class.

113
00:06:11,999 --> 00:06:15,799
You have the end-to-end
automation with very, or very

114
00:06:15,799 --> 00:06:17,209
low or no manual effort at all.

115
00:06:17,709 --> 00:06:21,639
And comprehensive governance and
compliance policies established and

116
00:06:21,639 --> 00:06:26,169
also centralized ML lops pipelines
and also advanced operator observed

117
00:06:26,499 --> 00:06:28,689
continuous implementing optimizations.

118
00:06:28,689 --> 00:06:33,999
So this is like a extremely sophisticated
ML lops pipelines, which are built

119
00:06:34,279 --> 00:06:36,019
to look at monitoring aspect of it.

120
00:06:36,019 --> 00:06:40,309
When we deploy during the deployment or
during model deployment testing, we have

121
00:06:40,459 --> 00:06:42,889
metrics baked into the whole pipeline.

122
00:06:43,164 --> 00:06:46,644
And all these checks around governance
and the compliance is also baked

123
00:06:46,644 --> 00:06:48,354
into the whole end, into pipeline.

124
00:06:48,854 --> 00:06:52,614
And let's talk few details about what
are the primary obstacles to ML success.

125
00:06:52,614 --> 00:06:53,414
I think some of the.

126
00:06:53,824 --> 00:06:55,954
Primary, if you look at
some of the charts here.

127
00:06:55,954 --> 00:06:58,534
So I think you can look
at organizational silos.

128
00:06:58,564 --> 00:07:00,154
I think for any success.

129
00:07:00,154 --> 00:07:03,974
I think it's extremely important
for teams to collaborate and work

130
00:07:03,974 --> 00:07:06,324
as a liaison for its own success.

131
00:07:06,324 --> 00:07:08,254
I think that is extremely important.

132
00:07:08,564 --> 00:07:10,754
So at least in some of
the metrics, some of the.

133
00:07:11,254 --> 00:07:15,544
Studies, we can see almost 73% of ML
initiatives directly are linked to

134
00:07:15,544 --> 00:07:20,124
inadequate, failed ML Institute surgeon,
that because of inadequate collaboration

135
00:07:20,124 --> 00:07:24,224
and governance there are other limitations
we can think of in infrastructure is

136
00:07:24,224 --> 00:07:29,854
not as strong enough to is not built for
what what we want to serve for customers

137
00:07:30,154 --> 00:07:31,954
and also not having enough monitoring.

138
00:07:32,339 --> 00:07:35,229
If we don't have monitoring,
we don't know what's going on.

139
00:07:35,229 --> 00:07:38,829
That's, I think, extremely important
to have a robust monitoring.

140
00:07:39,189 --> 00:07:41,759
Again, data quality issues, most similar.

141
00:07:41,759 --> 00:07:43,529
I think garbage in, garbage out.

142
00:07:43,799 --> 00:07:48,869
Having that really quality data to
train on or build those models is also

143
00:07:48,869 --> 00:07:50,579
an extremely important aspect of it.

144
00:07:51,119 --> 00:07:54,059
And also the lack of standardization
without having the standard

145
00:07:54,059 --> 00:07:56,819
tools and technologies and
also the pipelines to deploy.

146
00:07:57,509 --> 00:08:01,319
And models at scale it becomes
extremely important difficult,

147
00:08:01,829 --> 00:08:02,819
and also governance gaps.

148
00:08:02,819 --> 00:08:05,869
Having those governance governance
governance gaps is also extreme.

149
00:08:05,959 --> 00:08:09,679
But, and also looking at some of the
metrics here, considering some of

150
00:08:09,679 --> 00:08:13,834
these links, like we can think about
like 56% of production ML failure.

151
00:08:14,614 --> 00:08:16,624
Our directly linked to
data quality issues.

152
00:08:16,624 --> 00:08:20,824
As I was mentioning, data quality is
extremely important in a production ml,

153
00:08:21,154 --> 00:08:23,074
a ML kind of systems in a way, right?

154
00:08:23,414 --> 00:08:27,794
Similarly for insufficient monitoring,
almost like 38% are linked to insufficient

155
00:08:27,794 --> 00:08:30,434
monitoring, lack of standardization, 45%.

156
00:08:30,434 --> 00:08:32,774
These are like ballpark
numbers that we have gathered.

157
00:08:33,164 --> 00:08:36,284
This is also showing how important
to have these systems in place

158
00:08:36,284 --> 00:08:38,144
so that we are more buildings.

159
00:08:38,564 --> 00:08:42,024
Towards these now obstacles so that
we can build for success in a way.

160
00:08:42,324 --> 00:08:44,239
So that's like what I want
to cover for this slide.

161
00:08:44,739 --> 00:08:47,649
The next important thing is what
are the architectural patterns that

162
00:08:47,649 --> 00:08:50,929
we can put in place we can build a
system which is really robust, right?

163
00:08:50,929 --> 00:08:55,039
The first important thing I want to touch
here is mo modular component architecture.

164
00:08:55,039 --> 00:08:57,349
So the more like decomposable
components, right?

165
00:08:57,679 --> 00:09:01,744
Every component again, the Compass,
compass core construct is component

166
00:09:01,744 --> 00:09:03,244
should be able to iterate on its own.

167
00:09:03,294 --> 00:09:04,464
It can deploy model.

168
00:09:05,034 --> 00:09:08,904
Yeah, not only model anyway, like all
the ML ops supply plan has multiple

169
00:09:08,904 --> 00:09:10,374
steps or multiple components to it.

170
00:09:10,824 --> 00:09:12,684
Every component should be able to.

171
00:09:13,184 --> 00:09:17,884
Be iterated on its own and also
improved all those good stuff, right?

172
00:09:18,334 --> 00:09:20,224
And the other important thing
is centralized feature store.

173
00:09:20,374 --> 00:09:25,324
For any model development pipelines or
model development teams, features are the

174
00:09:25,324 --> 00:09:26,974
most important things, essentially, right?

175
00:09:26,974 --> 00:09:29,344
These are the things that are
baked into the model and used for

176
00:09:29,344 --> 00:09:30,424
predictions and stuff like that.

177
00:09:30,424 --> 00:09:35,944
So having that consistent standardized
feature store is extremely important.

178
00:09:36,074 --> 00:09:39,704
So that also provides, guaranteed
consistency for training

179
00:09:39,704 --> 00:09:40,694
and inference environment.

180
00:09:40,694 --> 00:09:44,414
So that is extremely important
in a model environments.

181
00:09:44,414 --> 00:09:44,684
In a way.

182
00:09:45,404 --> 00:09:49,924
Another important thing is reproducible
training pipelines, again so we really

183
00:09:49,924 --> 00:09:54,159
want to deploy models at scale and
also want to release new models.

184
00:09:54,714 --> 00:09:55,854
As frequently as possible.

185
00:09:55,854 --> 00:09:58,204
So we are giving the best
to our customers, right?

186
00:09:58,254 --> 00:10:03,465
Looking at that as a metric, having that
reproducible training pipeline so that lot

187
00:10:03,465 --> 00:10:06,154
of manual efforts can be can be avoided.

188
00:10:06,425 --> 00:10:10,124
And lot of these things can be, having
that consistent pipelines and also

189
00:10:10,124 --> 00:10:15,005
reproducible would help eventually in
accelerating the development station.

190
00:10:15,979 --> 00:10:19,069
Another important thing is also
having automated validation back then.

191
00:10:19,069 --> 00:10:22,980
So at every stage, having that
model automation in terms of

192
00:10:23,130 --> 00:10:25,755
having those validations at every
step of the phase, extremely

193
00:10:25,755 --> 00:10:27,465
important as we scale our systems.

194
00:10:27,965 --> 00:10:31,334
So let's talk about how do you
build a robust MOPS pipeline?

195
00:10:31,834 --> 00:10:35,734
Having that model lifecycle
deployment, model lifecycle management.

196
00:10:36,244 --> 00:10:39,805
I know essentially, how do we is
also extremely important in terms

197
00:10:39,805 --> 00:10:44,365
of, this helps almost deliver three
x better quality monitoring at

198
00:10:44,365 --> 00:10:48,085
the same time, reduces production
strength by almost a half, right?

199
00:10:48,535 --> 00:10:52,185
So what are the various things
that we usually look at when

200
00:10:52,185 --> 00:10:53,295
we think about model, right?

201
00:10:53,295 --> 00:10:54,555
So we have data engineering.

202
00:10:55,455 --> 00:10:59,165
Which provides all the essential
data points, whether it is like data

203
00:10:59,165 --> 00:11:01,765
required for feature engineering
on all those other aspects of data.

204
00:11:02,305 --> 00:11:05,905
So once we have data, I think the next
important part is like model development.

205
00:11:05,905 --> 00:11:10,825
So the primarily ML engineers or applied
scientist, who looks at, takes this

206
00:11:10,855 --> 00:11:14,405
data and builds some robust pipelines,
where we train ML models to come up

207
00:11:14,405 --> 00:11:17,904
with the robust models for any use case,
essentially, and the next important,

208
00:11:17,909 --> 00:11:22,044
the thing is, like now, once a model is
developed, the important thing is like

209
00:11:22,044 --> 00:11:26,434
we have to validate it, whether it is
working now models are usually trained

210
00:11:26,434 --> 00:11:27,934
on training data set, essentially.

211
00:11:28,144 --> 00:11:31,534
Then we have to validate it how
it is working on the real data

212
00:11:31,534 --> 00:11:32,704
set and production data set.

213
00:11:32,954 --> 00:11:36,454
Are we seeing the right kind of
production predictions or if if,

214
00:11:36,484 --> 00:11:39,425
if it is not performing as good,
it can be various other things.

215
00:11:39,425 --> 00:11:42,704
It can be modeled accuracy kind
of information, or it could be

216
00:11:42,704 --> 00:11:45,004
like performance and production
and stuff like that, right?

217
00:11:45,004 --> 00:11:49,234
So having that validation is at
the step that is extreme important.

218
00:11:49,234 --> 00:11:52,364
And also eventually, once it, it
covers all the validation aspects

219
00:11:52,364 --> 00:11:55,364
of it, and mostly like deployment
of it in production and scaling it.

220
00:11:55,414 --> 00:11:57,574
And once we deploy it in
production, the most important

221
00:11:57,574 --> 00:11:58,984
thing is like monitoring for it.

222
00:11:58,984 --> 00:12:04,244
So for the first thing is that now we
deploy the model and so once we deploy

223
00:12:04,244 --> 00:12:05,774
the model how is the model performing?

224
00:12:05,774 --> 00:12:08,354
How is the predictions look
like for the attachable model?

225
00:12:08,804 --> 00:12:10,484
Are the users that are
using our systems are.

226
00:12:11,254 --> 00:12:15,009
A sentiment is positive or
they're not so much or negative.

227
00:12:15,429 --> 00:12:21,829
Having that feedback signals fed back into
our systems would help us help us improve

228
00:12:21,829 --> 00:12:24,709
our models tremendously in the future.

229
00:12:24,709 --> 00:12:29,189
So having that end-to-end monitoring
and also feedback systems that can be

230
00:12:29,239 --> 00:12:33,109
used for model deployment or development
deployment is extremely important

231
00:12:33,409 --> 00:12:35,209
in building any ml s pipelines.

232
00:12:35,709 --> 00:12:40,059
So the next aspect I want to touch
upon is so technical implementation

233
00:12:40,059 --> 00:12:41,349
of model validation framework.

234
00:12:41,349 --> 00:12:44,439
So how do we validate what are the
things we want to actually validate?

235
00:12:44,889 --> 00:12:49,449
So again from, as I was mentioning,
now we have a model that is developed.

236
00:12:49,509 --> 00:12:53,139
And now we need to think about what
are the things we need to, while we

237
00:12:53,139 --> 00:12:55,629
are building these MLS pipelines,
what are the things, what are the

238
00:12:55,629 --> 00:12:57,129
testing things we need to build, right?

239
00:12:57,219 --> 00:13:00,979
So the first important thing,
government, data, everything

240
00:13:00,979 --> 00:13:02,539
starts with the data and ml, right?

241
00:13:02,539 --> 00:13:07,699
So I think we have how good data
that we have would determine

242
00:13:07,699 --> 00:13:09,019
how good our models are.

243
00:13:09,199 --> 00:13:10,279
Garbage in, garbage out.

244
00:13:10,279 --> 00:13:14,289
So it's extremely important to start
with ml primarily a data validation

245
00:13:14,289 --> 00:13:17,019
and whatever is a use case, we need
to have some kind of a data set.

246
00:13:17,509 --> 00:13:18,139
Having that.

247
00:13:19,009 --> 00:13:22,709
While we are creating this data set,
having that schema enforcement and also

248
00:13:22,709 --> 00:13:25,919
having that in our distribution checks,
drift detection and stuff like that.

249
00:13:25,919 --> 00:13:31,499
So to make sure we have a very clean
data is the first step of any validation.

250
00:13:31,999 --> 00:13:35,790
The next thing we want to think
about is like now model is deployed.

251
00:13:35,790 --> 00:13:37,170
Model is deployed in production.

252
00:13:37,170 --> 00:13:38,910
So the next important
thing is like, how do we.

253
00:13:39,410 --> 00:13:41,329
Measure the accuracy, so how good it is.

254
00:13:41,329 --> 00:13:42,974
Production, performance, production.

255
00:13:43,474 --> 00:13:46,204
And also align with some kind of
a key performance metrics, right?

256
00:13:46,604 --> 00:13:49,324
So that is other aspect
of validation as well.

257
00:13:49,654 --> 00:13:53,924
The other thing is like the, now we talk
about operational things now the other

258
00:13:53,924 --> 00:13:55,724
operational thing is the latency to stick.

259
00:13:55,724 --> 00:14:00,254
So even though the performance or
inaccuracy super good, but the thing is if

260
00:14:00,254 --> 00:14:04,894
the per model predictions are taking too
long, or if it is taking a lot more time,

261
00:14:05,134 --> 00:14:07,174
the customers may not like it, right?

262
00:14:07,174 --> 00:14:10,794
So I think in this world, users
would like to see fast responses.

263
00:14:10,984 --> 00:14:14,284
The other important aspect we need
to validate is like how fast it is

264
00:14:14,284 --> 00:14:18,315
providing all these predictions and,
that are providing the timely predictions

265
00:14:18,315 --> 00:14:19,575
is extremely important as well.

266
00:14:20,325 --> 00:14:23,204
Again, other aspects like it's not
only latency metrics and also you look

267
00:14:23,204 --> 00:14:27,044
at resource utilization, throughput
analysis, all the, all those good stuff.

268
00:14:27,734 --> 00:14:30,855
And the last thing I want to touch
upon is like ethical validation.

269
00:14:30,855 --> 00:14:35,635
Is there any bias that is coming out of
this model that is one side of the things

270
00:14:35,635 --> 00:14:37,625
we need to have a validation as well?

271
00:14:37,675 --> 00:14:40,984
Is the responses more
consistent or is it fair?

272
00:14:41,295 --> 00:14:44,415
Is also an extremely important
metric we need to capture

273
00:14:44,415 --> 00:14:46,425
or, test our models against.

274
00:14:47,025 --> 00:14:48,975
So what are the best
practices here, right?

275
00:14:48,975 --> 00:14:51,435
So having that clear acceptance criteria.

276
00:14:51,935 --> 00:14:55,635
With the with pass or fail threshold
for each validation step is

277
00:14:55,635 --> 00:14:58,665
extremely important while we go
build these production systems.

278
00:14:58,995 --> 00:15:02,175
And also having those integrated
into CCD pipelines, we don't

279
00:15:02,175 --> 00:15:03,375
want to make it manual, right?

280
00:15:03,375 --> 00:15:07,125
So I think we want to build them into
the consistency CD pipelines that can

281
00:15:07,125 --> 00:15:10,205
actually enforce these checks at one time.

282
00:15:10,675 --> 00:15:12,635
And also having that history.

283
00:15:12,995 --> 00:15:16,625
To track how this quality of models
are, how this model deployment and

284
00:15:16,625 --> 00:15:21,655
validation delivery is happening
is also very good indicator for us

285
00:15:21,685 --> 00:15:25,975
for our maturity or on how we are
progressing as ML ops teams, right?

286
00:15:26,475 --> 00:15:30,815
The next thing I want to touch up
on is so technical implementation.

287
00:15:31,355 --> 00:15:33,725
So there are a few aspects
I want to touch upon.

288
00:15:33,775 --> 00:15:38,095
For any CSC pipeline or CSCD
systems, continuous integration,

289
00:15:38,095 --> 00:15:41,695
continuous delivery is an
extremely important aspect of it.

290
00:15:42,195 --> 00:15:45,995
So for in, in case of continuous
integration again, a few

291
00:15:45,995 --> 00:15:47,225
things I want to touch upon.

292
00:15:47,225 --> 00:15:48,370
Model quality validations.

293
00:15:49,055 --> 00:15:53,755
During the integration stuff and also
versioning model artifacts, during

294
00:15:53,845 --> 00:15:57,295
building the models, artifacts,
having this versioning in place

295
00:15:57,295 --> 00:15:58,300
and right method attached to it.

296
00:15:59,065 --> 00:16:01,944
And also dependency environment
management is also baked into

297
00:16:01,944 --> 00:16:03,024
the same process as well.

298
00:16:03,704 --> 00:16:07,454
The other important thing I want to
touch upon is like a continuous delivery.

299
00:16:07,694 --> 00:16:10,965
So the, when we are deploying these
models at scale into any production

300
00:16:10,965 --> 00:16:13,014
environment, usually it's containerized.

301
00:16:13,324 --> 00:16:16,444
There's a lot of other tools that
we usually use, like it could be

302
00:16:16,444 --> 00:16:19,685
Titan Server, various other things
that we use, Docker containers,

303
00:16:19,744 --> 00:16:21,304
various other things that we can use.

304
00:16:21,675 --> 00:16:21,944
But.

305
00:16:22,725 --> 00:16:26,865
This is one thing we need to also
keep in mind how do we build those

306
00:16:26,865 --> 00:16:30,335
consistent con containerization
of model serving framework, right?

307
00:16:30,705 --> 00:16:33,675
The other thing is also
environment specific configurations

308
00:16:33,675 --> 00:16:36,775
and IAS infrastructure as
code deployments as well.

309
00:16:37,275 --> 00:16:39,615
Other important thing is like
canary or blue green deployment.

310
00:16:39,615 --> 00:16:43,215
So when I, when we want to deploy the
model into production, there are various

311
00:16:43,435 --> 00:16:45,115
deployment strategies that we can look at.

312
00:16:45,115 --> 00:16:48,145
For example, if you want to do
something like canary deployment,

313
00:16:48,145 --> 00:16:52,045
it's more in a production systems, we
may expose some percent of production

314
00:16:52,045 --> 00:16:56,035
traffic to this new version of the
model and see how it is performing.

315
00:16:56,035 --> 00:16:59,455
And based on, metrics that is
omitted by this new model, we

316
00:16:59,455 --> 00:17:02,695
can either promote it to higher
percentage of production traffic.

317
00:17:03,185 --> 00:17:06,245
And eventually take a hundred percent
of operating traffic or blue green

318
00:17:06,245 --> 00:17:10,145
deployment is creating a simultaneous
green environment or a blue environment

319
00:17:10,475 --> 00:17:14,790
along with the production environment
and see and pass through some of the

320
00:17:15,150 --> 00:17:17,400
traffic to this new model version.

321
00:17:17,900 --> 00:17:22,690
And see if if the metrics are looking
good, we promote the new version, green

322
00:17:22,690 --> 00:17:24,340
deployment into a production version.

323
00:17:24,340 --> 00:17:28,150
So those are like different methodologies
that we primarily use in case of

324
00:17:28,200 --> 00:17:33,210
microservices, deployment architectures
again once we build and deploy.

325
00:17:33,210 --> 00:17:36,120
The other important thing, as
we have talked about previously,

326
00:17:36,120 --> 00:17:37,500
is also like monitoring, right?

327
00:17:37,830 --> 00:17:39,270
Having this real time performance metric.

328
00:17:39,770 --> 00:17:43,345
And also other aspects like drift
detection and alerts, and also

329
00:17:43,345 --> 00:17:46,255
feature distribution monitoring
is also extremely important.

330
00:17:46,755 --> 00:17:50,565
And all these are, have all these
are correlated also, see how this is

331
00:17:50,565 --> 00:17:51,975
all correlated to business method.

332
00:17:51,975 --> 00:17:53,805
Correlation is also extremely important.

333
00:17:54,105 --> 00:17:57,865
Having that consistent monitoring
gives a better view of how

334
00:17:57,865 --> 00:17:59,005
the system is performing.

335
00:17:59,005 --> 00:18:01,075
What are the things that we
can improve upon as well.

336
00:18:01,735 --> 00:18:02,005
Yeah.

337
00:18:02,035 --> 00:18:05,635
The other thing I want to chop on
here is like a b testing of models.

338
00:18:05,725 --> 00:18:10,335
Again, I think ML models I think this is
a very well known framework that is used

339
00:18:10,335 --> 00:18:13,005
in a ML teams is having that AB testing.

340
00:18:13,005 --> 00:18:17,445
So basically enterprise, with automated
AV testing, we achieve almost 40%

341
00:18:17,445 --> 00:18:22,185
faster model attrition cycles and also
25% higher performance improvements

342
00:18:22,185 --> 00:18:23,805
compared to manual testing approaches.

343
00:18:24,325 --> 00:18:25,885
So what is actually IB testing?

344
00:18:25,885 --> 00:18:29,895
So it's mostly having that when we
have two different versions of the

345
00:18:29,895 --> 00:18:35,805
models exposing a customer to both the
versions of the models and see that user

346
00:18:35,805 --> 00:18:39,295
sentiment, which is performing better in
terms of giving, whether it is like a.

347
00:18:39,705 --> 00:18:43,455
Recommendation systems are any kind of
use case that we're trying to build,

348
00:18:43,785 --> 00:18:47,955
seeing which model is performing better,
and also trying to use those signal to

349
00:18:47,955 --> 00:18:52,215
promote either of them is like a testing
is what we usually call about, right?

350
00:18:52,485 --> 00:18:53,745
There are a few aspects to it.

351
00:18:53,745 --> 00:18:55,245
Again, traffic allocation dynamic.

352
00:18:55,245 --> 00:18:57,255
Clear route is a traffic
to model variance.

353
00:18:57,575 --> 00:18:59,075
Again, it could be configurable.

354
00:18:59,375 --> 00:19:02,645
Similarly, performance
measurements, accurate track across.

355
00:19:03,065 --> 00:19:05,495
Both miss race and technical metrics.

356
00:19:05,825 --> 00:19:09,185
So as I was talking about latency
and all those other things will

357
00:19:09,185 --> 00:19:12,755
come into technical businesses,
like how is a sentiment look like?

358
00:19:12,755 --> 00:19:16,445
Is it performing positive, negative, and
all this user aspect of the things right?

359
00:19:16,895 --> 00:19:19,115
And also other thing is
like statistical analysis.

360
00:19:19,235 --> 00:19:24,405
Just think about again, mostly about
data-driven deployment decisions, right?

361
00:19:24,405 --> 00:19:27,385
Rigorously having that analysis is also.

362
00:19:27,705 --> 00:19:30,225
Extremely useful in such kind of systems.

363
00:19:30,725 --> 00:19:34,415
So the other aspect I want to talk
about here is like now we have

364
00:19:34,485 --> 00:19:36,225
talked about what are the challenges?

365
00:19:36,225 --> 00:19:38,925
What are the things that we can
put in place and what are extremely

366
00:19:38,925 --> 00:19:42,620
important in terms of testing and
some of the testing framework, like

367
00:19:43,190 --> 00:19:44,540
a testing and stuff like that, right?

368
00:19:44,540 --> 00:19:50,310
So now functionally in terms of how
do we build this in such a way that.

369
00:19:50,650 --> 00:19:51,640
It is built for success.

370
00:19:51,640 --> 00:19:53,860
There is like primarily in any ML teams.

371
00:19:54,190 --> 00:19:56,770
We have the data science team
and also ML engineering team.

372
00:19:57,070 --> 00:19:59,320
So two different teams.

373
00:19:59,350 --> 00:20:03,870
And also the collaboration of both
of them comes to a ML ops excellence.

374
00:20:04,240 --> 00:20:07,810
So essentially these are the people
who have mixed knowledge of both data

375
00:20:07,810 --> 00:20:09,940
science side and also engineering.

376
00:20:09,990 --> 00:20:12,920
So these are the this, these are the.

377
00:20:13,070 --> 00:20:17,200
Core collaboration that helps
build some of the the typical

378
00:20:17,200 --> 00:20:18,550
aspects of ML lops pipeline.

379
00:20:18,600 --> 00:20:23,150
So a few things I want to mention here
is like cross-functional, like an ML lops

380
00:20:23,150 --> 00:20:27,200
teams are extremely important in terms
of working cross teams to build those

381
00:20:27,200 --> 00:20:32,170
consistent end to ML products and shared
accountability model in a joint ownership.

382
00:20:32,675 --> 00:20:35,495
It's a joint ownership between
ML performance, operational

383
00:20:35,495 --> 00:20:36,875
health business outcome.

384
00:20:36,875 --> 00:20:40,535
So it's not one team's responsibility
or one person responsibility.

385
00:20:40,535 --> 00:20:41,915
It's a joint responsibility.

386
00:20:42,395 --> 00:20:46,415
And also the other thing is also ml Op
Center of Excellence, like critical team.

387
00:20:46,865 --> 00:20:50,325
Having that best practices governance
framework and also helping them

388
00:20:50,325 --> 00:20:54,415
do that self service ops is an
extremely important thing as well.

389
00:20:54,915 --> 00:20:58,815
Then I want to touch upon few aspects
of cost optimization as well, right?

390
00:20:58,815 --> 00:21:00,975
So I think all these GPO
hardwares are expensive.

391
00:21:01,365 --> 00:21:04,575
For building any ML pipeline, I
think these are few things we need

392
00:21:04,575 --> 00:21:08,350
to keep in mind so that, we are
building in a much more efficient way.

393
00:21:08,700 --> 00:21:13,440
Key cost drivers looking at, compute cost,
as we all know, I think compute hardware

394
00:21:13,890 --> 00:21:16,350
all the GPUs are extremely expensive.

395
00:21:16,760 --> 00:21:19,130
We definitely need to consider that.

396
00:21:19,130 --> 00:21:22,490
And also data storage and
processing, the data pipelines and

397
00:21:22,490 --> 00:21:24,290
our feature stores, ML pipelines.

398
00:21:24,290 --> 00:21:28,700
How do we generate this data is also
the next aspect of, where the cost could

399
00:21:28,700 --> 00:21:33,100
come in and the tools and platforms
ML ops, pipelines and monitoring

400
00:21:33,100 --> 00:21:36,980
systems, and also specialized tools
that help us build this monitoring at

401
00:21:36,980 --> 00:21:38,535
scale is also something we need to.

402
00:21:39,015 --> 00:21:42,415
Keep in mind the other thing is like
operational overhead, like system

403
00:21:42,415 --> 00:21:45,055
maintenance, support incidents
and all those other things, right?

404
00:21:45,105 --> 00:21:48,345
A few things I want to touch
upon in terms of operational

405
00:21:48,345 --> 00:21:51,225
strategies, resources, autoscaling.

406
00:21:51,315 --> 00:21:54,735
I think a lot of provide cloud
providers provide this out of the box.

407
00:21:54,735 --> 00:21:57,895
We should be able to autoscale
the infrastructure to dynamically

408
00:21:57,895 --> 00:21:59,795
match our workloads demands.

409
00:21:59,795 --> 00:22:03,425
If there's the, if the request grow,
we can have those auto automatic

410
00:22:03,425 --> 00:22:06,245
knobs in place so that, we can
get those auto scaling right.

411
00:22:06,675 --> 00:22:07,755
Model efficiency.

412
00:22:08,255 --> 00:22:13,595
These are few ml ml but ml concepts
that we usually use, like model pruning,

413
00:22:13,625 --> 00:22:17,825
quantization distillation to reduce the
footprint and the size of the models,

414
00:22:17,825 --> 00:22:21,655
and also probably get similar or close
to good, similar performance in a way.

415
00:22:22,165 --> 00:22:24,205
And the other thing is
like process automation.

416
00:22:24,235 --> 00:22:27,865
Having that complete end-to-end
automation is extremely important

417
00:22:27,865 --> 00:22:29,455
in terms of our strategies as well.

418
00:22:29,955 --> 00:22:34,185
Enterprises that actually implement
these strategies achieve definitely have

419
00:22:34,305 --> 00:22:38,495
significant cost detections and also
most importantly, visibility, right?

420
00:22:38,495 --> 00:22:42,545
I think where, how much is going on is
also extremely important while we are

421
00:22:42,545 --> 00:22:47,565
building this system so that we know
where to increase our spending and how

422
00:22:47,775 --> 00:22:51,855
we can optimize is also an extremely
important decision that we can look at.

423
00:22:52,355 --> 00:22:55,085
Since we have talked about a lot of
other aspects of it, now I want to

424
00:22:55,085 --> 00:22:56,675
touch upon what is the future, right?

425
00:22:56,675 --> 00:22:59,485
This is an evolving space,
which is rapidly growing.

426
00:22:59,855 --> 00:23:03,155
How do we build systems that can
looking at the future, right?

427
00:23:03,155 --> 00:23:06,125
So I think we are just making sure
that we are building for the future.

428
00:23:06,605 --> 00:23:10,025
So a few things I want to touch
upon is like mops, observability,

429
00:23:10,355 --> 00:23:11,675
gaining that insights.

430
00:23:12,065 --> 00:23:16,585
Into the model behavior as I was speaking,
is having that complete monitoring and

431
00:23:16,595 --> 00:23:19,685
end-to-end metrics is extremely important.

432
00:23:19,895 --> 00:23:22,625
That is one thing which
we need to keep in mind.

433
00:23:22,955 --> 00:23:26,735
And also having that AI governance
implement frameworks for responsibility,

434
00:23:26,915 --> 00:23:30,425
deployment responsibility AI is
extremely important nowadays.

435
00:23:30,475 --> 00:23:34,255
And also having that auditing
bias detection compliance controls

436
00:23:34,255 --> 00:23:35,605
is extremely important as well.

437
00:23:36,325 --> 00:23:37,255
Federative learning.

438
00:23:37,620 --> 00:23:40,710
Again, this is more like having
that models being trained across

439
00:23:41,550 --> 00:23:42,960
decentralized data sources.

440
00:23:43,470 --> 00:23:47,790
Again, extremely important aspect and also
obviously improving privacy and security.

441
00:23:48,690 --> 00:23:52,220
Auto ml is also like autonomous
continuous model improvement.

442
00:23:52,350 --> 00:23:56,560
Again, looking at feature selection,
getting the feedback hyper tuning.

443
00:23:56,560 --> 00:24:00,315
It's like a complete cycle of
how do we use that end to end to

444
00:24:00,315 --> 00:24:01,610
actually improve our models at scale.

445
00:24:02,335 --> 00:24:05,915
Is something that is that is,
that we can keep in mind as well.

446
00:24:06,415 --> 00:24:11,465
Key SI want to talk of briefly
about key takeaways or high level

447
00:24:11,965 --> 00:24:13,585
start with clear go governance.

448
00:24:13,615 --> 00:24:16,695
I think there's a few things like,
establish that robust governance and

449
00:24:16,695 --> 00:24:18,345
motor lifecycle management practices.

450
00:24:19,215 --> 00:24:23,465
Make sure to make sure that we have that
solid foundation for ML instructors.

451
00:24:23,885 --> 00:24:25,475
Build modular architectures.

452
00:24:25,635 --> 00:24:29,205
Making sure that these are decomposable
units, I can build them, deploy

453
00:24:29,205 --> 00:24:31,245
them, scale them independently.

454
00:24:31,665 --> 00:24:33,495
So those are extremely important as well.

455
00:24:34,155 --> 00:24:37,425
Automate ruthlessly everything,
I think extremely important.

456
00:24:37,425 --> 00:24:40,970
Automation is very important as we
scale our infrastructure systems.

457
00:24:41,470 --> 00:24:42,610
Any manual process.

458
00:24:42,610 --> 00:24:45,880
Look for opportunity to
automate, integrate teams.

459
00:24:45,880 --> 00:24:49,525
Again, the teams, if there are silos,
it's extremely important things work in,

460
00:24:49,705 --> 00:24:53,915
is on have that consistent expectations
or collaboration with each other.

461
00:24:54,415 --> 00:24:57,325
Is extremely important in
this kind of systems as well.

462
00:24:57,955 --> 00:25:01,905
Again, looking at some of the statistics
we can see like enterprises with mature

463
00:25:01,905 --> 00:25:07,575
ML ops deploy almost five x faster
and also reduce, achieve 60% higher

464
00:25:07,575 --> 00:25:08,805
model performance and production.

465
00:25:09,255 --> 00:25:10,905
Extremely important statistics there.

466
00:25:10,905 --> 00:25:15,285
I think having these tools and
processes and building these consistent

467
00:25:15,285 --> 00:25:18,525
pipelines can definitely improve.

468
00:25:18,945 --> 00:25:20,775
These systems, ML systems at scale.

469
00:25:20,825 --> 00:25:21,125
Yep.

470
00:25:21,175 --> 00:25:23,995
I think these are all the things
I want to cover for this talk.

471
00:25:24,055 --> 00:25:25,775
Thank you very much for tuning in.

472
00:25:26,360 --> 00:25:27,050
Have a nice day.

