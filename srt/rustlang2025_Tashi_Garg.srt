1
00:00:00,500 --> 00:00:01,484
Hello everyone.

2
00:00:01,575 --> 00:00:02,535
I'm Tashi Kirk.

3
00:00:02,925 --> 00:00:07,305
I am from Juniper Networks and
today we are going to address a

4
00:00:07,305 --> 00:00:12,255
very serious problem in the cloud
infrastructure management, the hidden

5
00:00:12,255 --> 00:00:14,175
limitation of static dashboards.

6
00:00:14,675 --> 00:00:18,935
Dashboards are supposed to be
our eyes into the system, but in

7
00:00:18,935 --> 00:00:22,435
reality so many operate on a delay.

8
00:00:23,125 --> 00:00:26,860
That delay create blind spots,
and those blind spots can cause.

9
00:00:27,720 --> 00:00:31,800
Uptime revenue, trust, a lot of issues.

10
00:00:32,280 --> 00:00:36,959
So in this talk, I will ex,
I'll explain why this happens.

11
00:00:37,110 --> 00:00:40,950
How Rust, help us eliminate those
blind spots with predictable

12
00:00:41,330 --> 00:00:43,070
low latency mon monitoring.

13
00:00:43,315 --> 00:00:46,795
And what happens when the
organization make that shift?

14
00:00:47,295 --> 00:00:49,245
So let's see the agenda for the day.

15
00:00:49,365 --> 00:00:53,265
We will start by looking
at the operational impact

16
00:00:53,265 --> 00:00:54,765
of the static dashboard.

17
00:00:55,425 --> 00:00:59,385
Then I'll explain why Rust
with its concurrency model.

18
00:00:59,385 --> 00:01:03,320
Deterministic performance is ideal
for realtime telemetry processing.

19
00:01:03,980 --> 00:01:08,540
We'll also explore some real word results,
including a large scale case study.

20
00:01:09,040 --> 00:01:13,870
And we'll end with a phase
map for the adoption.

21
00:01:14,370 --> 00:01:19,890
So let's begin with the static the
hidden cost of using static dashboards.

22
00:01:20,520 --> 00:01:25,980
So most static dashboard, they rely
on polling, for example, collecting

23
00:01:25,980 --> 00:01:27,660
data at the fixed interval.

24
00:01:27,660 --> 00:01:30,060
It could be 32nd, it could be one minute.

25
00:01:30,060 --> 00:01:32,510
It depends on the widget.

26
00:01:32,895 --> 00:01:36,285
That we are using the dashboards
that's being deployed.

27
00:01:36,915 --> 00:01:41,295
So now that means that even can happen
well before they appear on the ui.

28
00:01:41,795 --> 00:01:44,375
So that is called the data stillness.

29
00:01:44,975 --> 00:01:48,815
So we, me, we have measured
that across environments.

30
00:01:49,085 --> 00:01:55,245
67% of the network operator reports
detection delays, averaging almost

31
00:01:55,275 --> 00:01:58,185
8.5 minutes from the incident onset.

32
00:01:58,265 --> 00:02:01,774
To the first visibility
to the network operator.

33
00:02:02,634 --> 00:02:06,684
And because most dashboards they
use binary or threshold based health

34
00:02:06,684 --> 00:02:12,505
checks, 73% of the anomalies, like
intermittent packet loss, degraded

35
00:02:12,505 --> 00:02:15,265
latencies, they're never flag at all.

36
00:02:15,684 --> 00:02:20,454
So when we combine staleness with
these missed anomalies, we end up

37
00:02:20,454 --> 00:02:23,154
in a very dangerous blind spots.

38
00:02:23,654 --> 00:02:30,464
So for that, we did they come
this reactive monitoring trap.

39
00:02:31,065 --> 00:02:34,934
So what happens when
these blind spots occurs?

40
00:02:34,994 --> 00:02:38,505
They forces teams into
the reactive pusher.

41
00:02:39,405 --> 00:02:41,355
So reactive pressure is
when you are reacting.

42
00:02:41,855 --> 00:02:45,595
After the act after the incident
occurs, so they respond only when the

43
00:02:45,595 --> 00:02:48,234
issue is already impacting customers.

44
00:02:48,804 --> 00:02:54,605
Statistically reactive monitoring
correlates with 2.8 times more

45
00:02:54,605 --> 00:02:59,714
service disruptions and 7.4
hours of unplanned downtime per

46
00:02:59,714 --> 00:03:01,484
month in the production system.

47
00:03:01,984 --> 00:03:06,854
In contrast to this the event driven
real time dashboards, the kind

48
00:03:06,944 --> 00:03:13,594
that we are talking today, cut that
downtime to 2.1 hours by detecting

49
00:03:13,624 --> 00:03:16,384
issues as they happen to get there.

50
00:03:16,384 --> 00:03:21,424
Though we need a system that can
process massive telemetry volumes

51
00:03:21,424 --> 00:03:26,834
in real time with consistent low
latency, and that's where Rust fits in.

52
00:03:27,334 --> 00:03:29,674
So how does changes everything?

53
00:03:30,604 --> 00:03:35,254
So when we look at the platform
for real time monitoring, two

54
00:03:35,254 --> 00:03:36,934
things are non-negotiable.

55
00:03:37,444 --> 00:03:39,814
One is predictable low latency.

56
00:03:40,549 --> 00:03:42,679
And second is high readability.

57
00:03:42,739 --> 00:03:47,119
Reliability under load and
rust deliver both of them.

58
00:03:47,689 --> 00:03:53,419
So if I keep the points read out the
points, it is zero cost abstraction

59
00:03:53,419 --> 00:03:58,069
that let us push through high throughput
data processing without runtime over.

60
00:03:58,569 --> 00:04:03,489
Memory safety without garbage collection
means there is no unpredictable GC pauses.

61
00:04:03,489 --> 00:04:08,889
When there is a telemetry spike
ownership model and borrower checker

62
00:04:09,669 --> 00:04:13,929
borrow checker that prevents the race
conditions and concurrent workloads.

63
00:04:14,409 --> 00:04:17,709
Compile time guarantees that
eliminates entire classes of

64
00:04:17,739 --> 00:04:20,639
runtime manner like NU D references.

65
00:04:21,614 --> 00:04:26,864
So this combination make Russ uniquely
well suited for latency sensitive

66
00:04:26,864 --> 00:04:29,384
workloads like containers monitoring.

67
00:04:29,884 --> 00:04:33,664
Now the Russ ecosystem, although
the language is very powerful,

68
00:04:33,694 --> 00:04:35,099
but the ecosystem is what?

69
00:04:35,724 --> 00:04:42,174
It makes it completely production ready
to get it to the production stage.

70
00:04:42,624 --> 00:04:49,094
We need maybe Tokyo for asynchronous
runtime, optimized for millions

71
00:04:49,094 --> 00:04:50,774
of concurrent IO bound tasks.

72
00:04:51,174 --> 00:04:53,695
That is perfect for the
telemetry ingestion.

73
00:04:54,099 --> 00:04:59,659
So for zero copy serialization,
decentralization for massive js or proto

74
00:05:00,650 --> 00:05:07,440
or even aro pay payloads act as any low
latency bear framework that can deliver

75
00:05:07,620 --> 00:05:13,679
low live updates to the dashboards or
crossbeam that can have safe fund currency

76
00:05:13,679 --> 00:05:19,469
primitives for multi-core scaling of
these telemetry processing with this type.

77
00:05:20,304 --> 00:05:24,174
We can process millions of
data points per second while

78
00:05:24,294 --> 00:05:26,064
keeping latency under control.

79
00:05:26,564 --> 00:05:32,445
So let's see how this transformational
results the transformational result when

80
00:05:32,534 --> 00:05:34,755
this kind of ecosystem was deployed.

81
00:05:35,255 --> 00:05:39,155
So in a real world deployment
where we are replacing the static

82
00:05:39,155 --> 00:05:43,825
dashboards with the rust powered
event driven system, we saw that

83
00:05:44,275 --> 00:05:47,935
there is almost 63% reduction in MTTR.

84
00:05:48,385 --> 00:05:50,155
So it was from 52 minute.

85
00:05:50,295 --> 00:05:56,625
Down to 19 minutes, there were 37%
improvement for critical incident,

86
00:05:56,925 --> 00:06:02,835
44% fewer operator errors, thanks
to the instant feedback, 31%

87
00:06:02,835 --> 00:06:05,145
less downtime year over year.

88
00:06:05,415 --> 00:06:07,935
And these aren't the
hypothetical improvement.

89
00:06:07,935 --> 00:06:11,745
They come directly from the production
environment, from where the study well,

90
00:06:11,805 --> 00:06:14,355
where these deployments were done.

91
00:06:14,855 --> 00:06:21,235
Going forward let's take a case study
where we have a global financial

92
00:06:21,235 --> 00:06:28,775
service provider with an environment
with almost 12,000 there more than

93
00:06:28,775 --> 00:06:32,795
12,000 instances that are running
across multiple cloud providers.

94
00:06:33,705 --> 00:06:35,295
The problem was faced that.

95
00:06:35,570 --> 00:06:42,170
Almost 11 times average detection
time, 9.4 hours of downtime per month.

96
00:06:42,590 --> 00:06:49,040
So after migrating to a RAs base,
realtime monitoring backend the pro

97
00:06:49,070 --> 00:06:55,230
the process 3.2 million telemetry data
points per second, with less than five

98
00:06:55,230 --> 00:07:01,950
milliseconds of pipeline latency, the
anomalies were detected 65% faster.

99
00:07:02,325 --> 00:07:08,325
The incident detection was reduced
from 11 minutes to 47 seconds,

100
00:07:08,985 --> 00:07:14,445
and the downtime, it was dropped
to almost 2.3 hours per month.

101
00:07:15,075 --> 00:07:19,895
So from 11 minutes sorry, 9.54 minutes.

102
00:07:20,060 --> 00:07:22,790
Two, 2.3 hours per minute per month.

103
00:07:23,300 --> 00:07:24,950
That was a huge difference.

104
00:07:25,250 --> 00:07:28,430
So that's the operational
differences that these real time

105
00:07:28,430 --> 00:07:30,790
make in it's seen in the real time.

106
00:07:31,290 --> 00:07:35,145
So along with Rust, there are
other things that can be done.

107
00:07:35,645 --> 00:07:39,934
There are a lot of emerging trends these
days when we have real time telemetry.

108
00:07:39,934 --> 00:07:43,864
There can be powerful capabilities
that can be added on top of it.

109
00:07:44,344 --> 00:07:47,084
It could be AI driven anomaly detection.

110
00:07:47,124 --> 00:07:53,714
More ML models can be can be added on top
of it that can catch these deviations that

111
00:07:53,714 --> 00:07:56,054
are invisible to those static thresholds.

112
00:07:56,624 --> 00:07:59,594
NLP based query interfaces can be added.

113
00:07:59,919 --> 00:08:05,469
When natural language query interface
is provided, it can translate la

114
00:08:05,499 --> 00:08:11,959
questions in the common language into a
structured telemetry searches, and even

115
00:08:11,959 --> 00:08:17,140
on the ui a more sp more graph based
visualization, more interactive math.

116
00:08:18,045 --> 00:08:24,345
Maybe maps of dependencies that can cut
diagnosis or time over time by over 40%.

117
00:08:24,975 --> 00:08:29,915
So these can be added on top of it,
but the prerequisite, if we consider

118
00:08:29,915 --> 00:08:35,285
is low latency, reliable telemetry
pipeline, which just already enables.

119
00:08:35,785 --> 00:08:40,405
So let's see how we
want to implement this.

120
00:08:40,555 --> 00:08:45,295
So a typical rust power monitoring
stack may include agent layer,

121
00:08:45,655 --> 00:08:50,015
which is lightweight, rust
Aries, maybe less than 15 mbs.

122
00:08:50,760 --> 00:08:52,590
Less than 2% CPO use.

123
00:08:53,150 --> 00:09:00,420
And sending telemetry over GR PCs or web
sockets streaming pipelines that are acing

124
00:09:00,420 --> 00:09:06,800
processing of multi-core parting real time
API layer that can deliver updates to the

125
00:09:06,800 --> 00:09:09,260
dashboard in less than 50 milliseconds.

126
00:09:09,655 --> 00:09:16,595
And then the reactive ui that's where it
is instead of polling it is getting events

127
00:09:16,595 --> 00:09:22,085
from web soc, web sockets, or SSE based
dashboards that can update automatically.

128
00:09:22,585 --> 00:09:25,210
So there, there are three common.

129
00:09:25,210 --> 00:09:31,950
Now while adopting this stack, there
will be three common challenges.

130
00:09:32,610 --> 00:09:33,150
One is.

131
00:09:33,770 --> 00:09:38,630
Integrating with the leg legacy
environment, so that can be solved with

132
00:09:38,630 --> 00:09:45,830
the just FFI or GRPC bridges so that you
don't have to replace everything at once.

133
00:09:46,760 --> 00:09:51,880
The definitely a skill gap that we
can start with one self-contained

134
00:09:51,880 --> 00:09:52,990
component to build confidence.

135
00:09:53,490 --> 00:09:58,320
And then the data volume, the
filter and aggregate or add the

136
00:09:58,320 --> 00:10:00,650
agent before sending data upstream.

137
00:10:01,310 --> 00:10:06,230
So these challenges are real, but
manageable if it is rolled out in stages.

138
00:10:06,730 --> 00:10:12,310
Considering the roadmap, we, it is
always recommended a phased approach.

139
00:10:12,700 --> 00:10:13,870
So in phased approach.

140
00:10:13,950 --> 00:10:19,890
The first month maybe can be for dedicated
for auditing all these blind spot,

141
00:10:20,340 --> 00:10:23,430
defining baseline m TTRs and downtime.

142
00:10:24,110 --> 00:10:25,190
Then couple of months.

143
00:10:25,245 --> 00:10:33,045
To deploy agent in staging, validating
latencies, throughput detection, accuracy.

144
00:10:33,545 --> 00:10:37,835
Post that a quarter maybe in rolling
out to production, integrating

145
00:10:37,835 --> 00:10:39,995
with the incident response tools.

146
00:10:40,505 --> 00:10:45,145
And beyond that it can be dedicated
for all these emerging trends.

147
00:10:45,145 --> 00:10:50,875
Integration like adding ml, anomaly
detection, domain specific visualization.

148
00:10:51,375 --> 00:10:56,865
So adding, doing in a phase
approach will keep the risk low

149
00:10:57,345 --> 00:10:59,715
while proving its value lit early.

150
00:11:00,215 --> 00:11:02,885
The key takeaways is static dashboard.

151
00:11:02,885 --> 00:11:06,515
They create dangerous visibility
caps due to polling delays

152
00:11:06,515 --> 00:11:08,555
and limited anomaly detection.

153
00:11:09,055 --> 00:11:14,865
Rust deterministic performance,
memory, safety concurrency model

154
00:11:14,925 --> 00:11:19,945
make it ideal for building real time
monitoring pipelines in production.

155
00:11:19,945 --> 00:11:24,700
The this translate to faster
detection, fewer errors.

156
00:11:25,490 --> 00:11:28,640
Significant, significantly low downtime.

157
00:11:29,630 --> 00:11:33,980
So Rust Power dashboards aren't
just I would say tech upgrade.

158
00:11:33,980 --> 00:11:36,920
They are more as an operational shift.

159
00:11:37,790 --> 00:11:40,550
They let team move from reacting.

160
00:11:40,600 --> 00:11:45,880
To protecting from, like
firefighting to preventing.

161
00:11:46,360 --> 00:11:50,920
So once we experience this detecting
incident in under a minute, there won't

162
00:11:50,920 --> 00:11:52,870
be no, there won't be any going back.

163
00:11:53,780 --> 00:11:55,280
That's all from my side today.

164
00:11:55,400 --> 00:11:55,970
Thank you.

