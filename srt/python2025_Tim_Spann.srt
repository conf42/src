1
00:00:00,200 --> 00:00:03,490
Hi, Tim Spam here, Senior
Solutions Engineer.

2
00:00:04,080 --> 00:00:06,670
Today, I'm going to be talking
about unlocking your data's

3
00:00:06,720 --> 00:00:09,140
potential utilizing Python.

4
00:00:09,709 --> 00:00:10,530
Let's get to it.

5
00:00:11,200 --> 00:00:18,919
I've been doing data, AI, IoT,
Edge stuff for a number of years

6
00:00:19,029 --> 00:00:20,469
at some different companies.

7
00:00:21,149 --> 00:00:24,169
right now I'm at Snowflake
and it's pretty awesome.

8
00:00:24,179 --> 00:00:28,549
We have some amazing technology,
really easy to use, really

9
00:00:28,549 --> 00:00:30,639
easy to use any kind of data.

10
00:00:31,194 --> 00:00:36,104
And easy to work with, lots of different,
open source technologies that we support.

11
00:00:36,105 --> 00:00:42,734
Apache NiFi, Apache Iceberg, the newest,
very interesting, Apache Polaris,

12
00:00:43,274 --> 00:00:45,644
Streamlit, let you build some cool apps.

13
00:00:46,064 --> 00:00:50,044
And, on the side I'm also doing some
Flink and Kafka, because those are

14
00:00:50,224 --> 00:00:51,964
a lot of fun in the streaming space.

15
00:00:52,824 --> 00:00:53,934
so let's get into it.

16
00:00:54,134 --> 00:00:57,694
We'll do the intro, do an overview,
look at some super friends,

17
00:00:57,724 --> 00:01:00,474
where, what, why, all the things.

18
00:01:01,044 --> 00:01:05,454
When you're going to unlock a lot of data,
you need a team, and that team could be a

19
00:01:05,484 --> 00:01:08,644
combination of people, AI, and technology.

20
00:01:09,164 --> 00:01:12,714
We're going to cover some of
the technologies that intersect

21
00:01:12,724 --> 00:01:16,474
with Python, and let you do
some interesting data stuff.

22
00:01:17,134 --> 00:01:23,934
And that's Snowpark, of course Python,
Polaris, Iceberg, Streamlit, NiFi, and

23
00:01:23,934 --> 00:01:29,884
in the core of it all, Snowflake that
makes things easy, fast, and cheap, and

24
00:01:30,044 --> 00:01:31,634
supports all this great open source.

25
00:01:32,134 --> 00:01:35,754
So let's first look at, the
different types of data out there.

26
00:01:36,184 --> 00:01:37,964
And there's lots of ways to define it.

27
00:01:38,174 --> 00:01:42,574
But we're going to look at the data
that, makes sense to process today.

28
00:01:42,634 --> 00:01:45,934
Structured, semi structured,
and unstructured.

29
00:01:46,634 --> 00:01:48,074
So cats, let's take it away.

30
00:01:48,454 --> 00:01:50,054
First, we'll look at semi structured.

31
00:01:50,564 --> 00:01:51,884
This one, there's a lot of it.

32
00:01:52,384 --> 00:01:53,774
Oh, I have a kitty here.

33
00:01:54,444 --> 00:01:55,674
Maybe he won't come on camera.

34
00:01:55,675 --> 00:01:58,524
Actually, I don't know
if you could see him.

35
00:01:58,824 --> 00:01:59,534
He is a good man.

36
00:02:00,419 --> 00:02:01,569
His name is Mr. Splunk.

37
00:02:02,169 --> 00:02:03,759
Probably wants some kind of snack.

38
00:02:03,879 --> 00:02:05,209
That's what he's into.

39
00:02:05,719 --> 00:02:07,209
so semi structured data.

40
00:02:07,909 --> 00:02:09,989
Very common for open data.

41
00:02:10,639 --> 00:02:12,019
the ones from OpenAQ.

42
00:02:12,019 --> 00:02:13,079
Air quality data.

43
00:02:13,439 --> 00:02:15,949
Things like location, time, sensors.

44
00:02:16,369 --> 00:02:18,109
Lots of different formats out there.

45
00:02:18,219 --> 00:02:21,697
Like Apache Avro.

46
00:02:21,697 --> 00:02:21,933
Apache Parquet.

47
00:02:21,933 --> 00:02:22,169
Apache ORC.

48
00:02:22,169 --> 00:02:25,219
These also can be used
for more structured data.

49
00:02:25,249 --> 00:02:27,479
When we layer on top Iceberg.

50
00:02:28,074 --> 00:02:31,084
JSON and XML, we'll work with, that today.

51
00:02:31,084 --> 00:02:33,394
Today actually we're going to
work with Protobuf, convert

52
00:02:33,394 --> 00:02:34,784
it into JSON and use that.

53
00:02:35,374 --> 00:02:40,174
This is where you'll see things like
hierarchical data, arrays, things that

54
00:02:40,174 --> 00:02:44,624
are, they're they got a structure, but
it's not, easily do in SQL, it's a pain.

55
00:02:44,974 --> 00:02:49,424
You probably want that more structured
to be faster, easier to work with.

56
00:02:49,844 --> 00:02:55,849
Things like logs, key value, and this
is a source of a lot of the raw data

57
00:02:55,879 --> 00:02:58,229
out there, and a good source of data.

58
00:02:59,189 --> 00:03:00,559
We're never going to forget about that.

59
00:03:00,649 --> 00:03:04,759
Unstructured data, in the past
you just put it on the side, maybe

60
00:03:04,759 --> 00:03:08,449
used a specialized, functioning
search engine against it.

61
00:03:09,399 --> 00:03:12,399
didn't do too much with it,
maybe pulled some text out.

62
00:03:12,869 --> 00:03:15,369
We were doing that for the
last couple of years with NiFi.

63
00:03:16,049 --> 00:03:20,759
There's a lot of formats, things
like text, documents, PDF, images,

64
00:03:20,769 --> 00:03:24,099
videos, audio, email, variants.

65
00:03:24,679 --> 00:03:29,519
Now fortunately, with the new tools
out there, using Apache NiFi and

66
00:03:29,519 --> 00:03:34,109
Python, there's a lot of libraries
that let us extract, convert things,

67
00:03:34,439 --> 00:03:40,359
turn a Word document into Markdown,
grab that text, use it for different,

68
00:03:40,439 --> 00:03:42,529
brag approaches, use it for AI.

69
00:03:42,979 --> 00:03:47,369
Doing really good stuff with,
Snowflake Cortex and NiFi here.

70
00:03:47,869 --> 00:03:52,489
Now Structured Data, this we've
been using for a very long time.

71
00:03:53,259 --> 00:03:59,569
And this is commonly stored in things like
Snowflake Tables, Snowflake Hybrid Tables,

72
00:04:00,079 --> 00:04:01,479
it lets you do some interesting stuff.

73
00:04:01,549 --> 00:04:05,309
Iceberg Tables, the standard
relational tables that can be

74
00:04:05,319 --> 00:04:07,139
set in any of your databases.

75
00:04:07,729 --> 00:04:09,289
Postgres is a good example.

76
00:04:09,834 --> 00:04:16,074
Also, CSV and tab or fixed
width files are very structured

77
00:04:16,074 --> 00:04:18,724
as they're not, hierarchical.

78
00:04:18,894 --> 00:04:20,134
There's no arrays in there.

79
00:04:20,764 --> 00:04:21,204
It's flat.

80
00:04:21,204 --> 00:04:23,694
It maps to fields really well.

81
00:04:24,044 --> 00:04:28,264
Of course, in a CSV, you can't
enforce, types or other things,

82
00:04:28,264 --> 00:04:30,104
but it's very structured.

83
00:04:30,604 --> 00:04:32,524
Now, next up, Iceberg.

84
00:04:33,024 --> 00:04:35,114
You haven't used Iceberg yet.

85
00:04:36,029 --> 00:04:37,289
You really should.

86
00:04:37,809 --> 00:04:39,399
It is high performance.

87
00:04:39,489 --> 00:04:40,679
It is open source.

88
00:04:40,729 --> 00:04:44,789
There's a great community there
and it works out really well for

89
00:04:44,799 --> 00:04:47,209
extremely large analytical tables.

90
00:04:47,619 --> 00:04:49,319
SQL support, very strong.

91
00:04:49,689 --> 00:04:54,449
Tons of different engines, which is
nice is this lets you set up things the

92
00:04:54,449 --> 00:04:59,089
way you want and makes data available
for different tools, whether it's

93
00:04:59,089 --> 00:05:01,739
Snowflake, Trino, Flink, Presto, Hive.

94
00:05:02,149 --> 00:05:06,029
Tons of different things can, query
this and run your applications.

95
00:05:06,029 --> 00:05:09,269
Great way to, make data
available for different tools.

96
00:05:09,789 --> 00:05:14,629
I like this as a good way to back
up your data into a data lake.

97
00:05:14,749 --> 00:05:19,739
When you have like your production data
and snowflake, what's nice with iceberg

98
00:05:19,759 --> 00:05:25,519
supports, lots of different data catalogs
and the, API for that is very open.

99
00:05:26,019 --> 00:05:27,779
The two big ones are Polaris.

100
00:05:28,614 --> 00:05:35,414
Which is the shiny new open source one
that's got some interesting Features

101
00:05:35,414 --> 00:05:39,384
there and then been around for a
while Nessie, which has an awesome

102
00:05:39,854 --> 00:05:42,254
Logo, it does some cool git stuff.

103
00:05:42,754 --> 00:05:47,954
They may start getting closer together as
there's a lot of people working in common

104
00:05:47,954 --> 00:05:53,784
on those but Pick which one works best
in your environment What's nice if you're

105
00:05:53,784 --> 00:05:56,819
using Snowflake things you're handles
where you don't have to worry about it.

106
00:05:56,820 --> 00:06:00,469
A couple of features in Iceberg
that are pretty awesome.

107
00:06:01,119 --> 00:06:05,009
support for the full evolution
of your schema so things can

108
00:06:05,019 --> 00:06:08,489
change as data changes over time.

109
00:06:08,549 --> 00:06:09,519
That's the real world.

110
00:06:09,839 --> 00:06:11,009
You get time travels.

111
00:06:11,009 --> 00:06:14,039
You can go back and look at older data.

112
00:06:14,489 --> 00:06:17,459
This is great if you accidentally
deleted something you shouldn't

113
00:06:17,469 --> 00:06:21,202
have and you'll be able to go
back or compare a current thing.

114
00:06:21,202 --> 00:06:24,139
Then you do a load, look at it afterwards.

115
00:06:24,854 --> 00:06:30,384
Partitioning, which used to be super
hard back in the old days of Hive, this

116
00:06:30,384 --> 00:06:35,894
is managed for you, it's hidden, it
works, and you can, make changes to it,

117
00:06:36,334 --> 00:06:39,354
you don't have to think about it, you
don't have to see it, there's no big

118
00:06:39,354 --> 00:06:44,064
directory structures lying everywhere,
this is much better with metadata.

119
00:06:44,694 --> 00:06:48,844
You can also roll back, like we
mentioned, and compact your data over

120
00:06:48,844 --> 00:06:52,795
time so it's not, Big and sparse,
the problem with appending data

121
00:06:52,815 --> 00:06:57,134
on things like object storage, S3.

122
00:06:57,134 --> 00:06:59,684
It can be, very sparse.

123
00:06:59,684 --> 00:07:01,624
You remember the old, hard drives.

124
00:07:01,734 --> 00:07:03,144
We had to keep defragging them.

125
00:07:03,314 --> 00:07:06,623
with the compaction, it's doing
that for you, so that's pretty nice.

126
00:07:06,623 --> 00:07:07,806
lots of engines.

127
00:07:07,806 --> 00:07:13,340
Again, I like Flink and Pi Iceberg
and Trino and Snowflake for this.

128
00:07:13,920 --> 00:07:15,770
Catalog with some RBAC.

129
00:07:16,480 --> 00:07:19,250
Gets you to your iceberg,
whichever cloud you're running on.

130
00:07:19,810 --> 00:07:21,010
Very easy to do.

131
00:07:21,860 --> 00:07:26,570
Now, to get data into Iceberg,
we can append that with NiFi.

132
00:07:27,130 --> 00:07:29,830
There's a processor for that,
a couple actually, depending

133
00:07:29,830 --> 00:07:32,020
on whose NiFi you're using.

134
00:07:32,590 --> 00:07:38,580
And, you could use Snowpark's Python,
just to write some data frames there.

135
00:07:39,040 --> 00:07:40,460
Save that as a table.

136
00:07:40,970 --> 00:07:42,960
Pretty easy, we've got a link down there.

137
00:07:43,380 --> 00:07:46,950
And yes, after that you can
have the data, which is nice.

138
00:07:47,820 --> 00:07:48,980
We'll look at Snowpark.

139
00:07:49,880 --> 00:07:51,880
Again, Python library.

140
00:07:52,470 --> 00:07:53,780
And I can code from anywhere.

141
00:07:53,850 --> 00:07:55,740
I don't have to code from Snowflake.

142
00:07:56,140 --> 00:07:58,800
But if you've got Snowflake,
just use their notebook.

143
00:07:58,900 --> 00:08:00,310
It's a little easier, but whatever.

144
00:08:00,960 --> 00:08:02,440
so you've got this engine.

145
00:08:02,940 --> 00:08:05,320
It's elastic, supports other languages.

146
00:08:06,070 --> 00:08:09,080
We'll pretend Python is the only language
in the world, but there's others.

147
00:08:09,580 --> 00:08:11,980
There's libraries in there
for data engineering,

148
00:08:12,130 --> 00:08:13,630
anything you need to do there.

149
00:08:14,070 --> 00:08:16,470
Lots of machine learning and AI stuff.

150
00:08:16,850 --> 00:08:20,910
Streamlit to build cool,
extremely simple apps.

151
00:08:21,430 --> 00:08:24,830
I wrote one a couple of months
ago and I'm like, let me just

152
00:08:24,830 --> 00:08:26,230
put these three things there.

153
00:08:26,410 --> 00:08:27,660
I'm like, it's a full app.

154
00:08:27,720 --> 00:08:32,950
I use that for my ghost hunting app
that stored things into vectors.

155
00:08:33,450 --> 00:08:35,580
data access super fast.

156
00:08:36,470 --> 00:08:42,280
faster than almost any library out there
with full security you get data frames

157
00:08:42,280 --> 00:08:46,520
you get pandas it looks like what you'd
expect when you're doing pi data stuff

158
00:08:47,500 --> 00:08:51,730
why should you use it very easy to build
these scalable data pipelines you build

159
00:08:51,730 --> 00:08:57,400
them on models apps any kind of data
processing you want to do and you still

160
00:08:57,400 --> 00:09:01,590
get all the governance and security
which is nice and again you can write

161
00:09:01,590 --> 00:09:06,995
this custom code from any notebook or id
IDE and this you know like visual code

162
00:09:06,995 --> 00:09:12,155
and have that automatically push down
into the compute engine You don't have

163
00:09:12,205 --> 00:09:16,245
to Worry if it's going to be performant
or you've got to do anything else.

164
00:09:16,255 --> 00:09:21,485
It just works and works fast
really cool And like I said, it's

165
00:09:21,485 --> 00:09:22,705
where it's jupyter notebooks.

166
00:09:22,705 --> 00:09:26,575
You can do it from from sql
worksheets or snowflake notebooks

167
00:09:26,575 --> 00:09:29,075
visual studio code, which is nice.

168
00:09:29,075 --> 00:09:33,615
I run that Get your apis It's
out there for machine learning

169
00:09:34,095 --> 00:09:35,545
and for data pipelines.

170
00:09:35,995 --> 00:09:38,605
Run this on top of your virtual warehouse.

171
00:09:39,065 --> 00:09:41,955
And again, you could also do this
in Java or Scala if you want.

172
00:09:42,465 --> 00:09:45,455
And you can run this inside
a container and use GPUs if

173
00:09:45,455 --> 00:09:46,445
you need them or have them.

174
00:09:46,455 --> 00:09:48,545
If you have them, that's pretty awesome.

175
00:09:49,045 --> 00:09:50,455
Pretty easy to do that.

176
00:09:50,955 --> 00:09:52,315
Should store some data.

177
00:09:52,815 --> 00:09:54,215
Which is a good idea.

178
00:09:54,735 --> 00:09:56,025
let's look at NiFi.

179
00:09:56,105 --> 00:09:58,445
Now, patching NiFi in the past.

180
00:09:58,880 --> 00:10:02,720
It's been thought of as a Java
tool, or a tool that had nothing

181
00:10:02,720 --> 00:10:05,260
to do with Python, or very minimal.

182
00:10:05,690 --> 00:10:12,950
fortunately, in the latest version,
we have added in 9520, and now we're

183
00:10:13,450 --> 00:10:20,370
even further along, the ability to run
your Python code, not just as a little

184
00:10:20,390 --> 00:10:27,175
script somewhere, not just to a REST
endpoint, but as code inside of Python.

185
00:10:27,605 --> 00:10:32,295
of our flow, which makes it
very powerful, very easy to use.

186
00:10:32,395 --> 00:10:36,305
And I've written a number of
components to try different things out.

187
00:10:36,305 --> 00:10:40,655
I've got one that'll pull company
names out of chunks of text.

188
00:10:41,455 --> 00:10:47,045
And this uses Hugging Faced, NLP,
Spacey, which is underrated, and

189
00:10:47,045 --> 00:10:49,575
PyTorch to be able to pull these out.

190
00:10:50,275 --> 00:10:54,885
And I use this when I need to grab the
company name someone's talking about.

191
00:10:55,250 --> 00:11:00,000
Maybe to do a lookup on that with
wiki, or grab, stock quotes, whatever.

192
00:11:00,500 --> 00:11:02,210
I got one to caption images.

193
00:11:02,790 --> 00:11:05,480
Now, again, we talked about
that unstructured data.

194
00:11:06,200 --> 00:11:09,390
With Apache NiFi, I can work with images.

195
00:11:09,950 --> 00:11:14,750
so as images come in, I use,
Salesforce Blip image captioning model.

196
00:11:15,180 --> 00:11:18,310
Which I might look at, an
updated one in the future.

197
00:11:18,510 --> 00:11:19,730
This one's pretty good though.

198
00:11:20,080 --> 00:11:21,810
And this gives me captions for my images.

199
00:11:22,655 --> 00:11:25,145
which is nice as part of, flow
when I'm working with that.

200
00:11:25,405 --> 00:11:28,355
Then I got another one, doing ResNet 50.

201
00:11:28,395 --> 00:11:30,235
Probably some upgrades I could do.

202
00:11:30,565 --> 00:11:34,935
This gives me classification labels
for anything I'm working there.

203
00:11:35,625 --> 00:11:38,655
You don't have to download
the images, happens in line.

204
00:11:39,205 --> 00:11:43,095
this one's important if you're
letting people upload images.

205
00:11:43,815 --> 00:11:46,335
using this model I found on Hugging Face.

206
00:11:46,335 --> 00:11:49,245
If you haven't looked at
Hugging Face, sign up.

207
00:11:49,275 --> 00:11:50,315
It's got so much.

208
00:11:50,465 --> 00:11:52,595
So much good information on models.

209
00:11:52,615 --> 00:11:54,885
Snowflake has some cool
Arctic models there.

210
00:11:55,385 --> 00:11:56,165
Everything's there.

211
00:11:56,165 --> 00:11:56,735
It's awesome.

212
00:11:56,965 --> 00:12:00,355
this will give you a score on the
safety of an image if you don't

213
00:12:00,355 --> 00:12:02,855
accidentally upload bad ones.

214
00:12:03,005 --> 00:12:06,905
And if you're not sure if it's
close, put that in the side and

215
00:12:06,905 --> 00:12:08,395
let that be looked at later.

216
00:12:09,145 --> 00:12:13,645
Got another one working on images
that looks at, facial emotions.

217
00:12:14,045 --> 00:12:16,365
Does some image classification with that.

218
00:12:17,330 --> 00:12:18,650
That one's pretty easy.

219
00:12:19,330 --> 00:12:23,890
this one is very helpful when
you're trying to do geo stuff.

220
00:12:24,410 --> 00:12:28,720
And I like this one because once
I, convert an address to latitude

221
00:12:28,720 --> 00:12:32,870
and longitude, then I can start
doing some interesting geo stuff

222
00:12:32,890 --> 00:12:34,360
with Snowflake, which is cool.

223
00:12:35,100 --> 00:12:39,100
This, library, Nominatum, no
relation, from OpenStreetMaps

224
00:12:39,100 --> 00:12:40,900
does a pretty good job.

225
00:12:41,490 --> 00:12:41,990
No.

226
00:12:42,950 --> 00:12:48,050
I played around with four or five
different libraries, and among the

227
00:12:48,050 --> 00:12:51,060
free ones, this is probably the best.

228
00:12:51,860 --> 00:12:53,710
It'll get you pretty close.

229
00:12:53,790 --> 00:12:55,410
Doesn't need a full address.

230
00:12:55,920 --> 00:13:00,630
This is not a bad thing if you don't
know exactly where you are, and you

231
00:13:00,630 --> 00:13:02,590
can't grab that from something else.

232
00:13:02,590 --> 00:13:03,440
This is nice.

233
00:13:03,590 --> 00:13:04,560
I like this library.

234
00:13:05,060 --> 00:13:08,980
Now, we showed you some of the
Python components that I've written.

235
00:13:09,865 --> 00:13:11,015
You can write anything you want.

236
00:13:11,035 --> 00:13:14,635
You can take any of your
Python and put it in here as,

237
00:13:14,665 --> 00:13:17,125
horsepower to process these flows.

238
00:13:17,935 --> 00:13:18,885
But what's a flow?

239
00:13:18,885 --> 00:13:20,095
What's not if I do?

240
00:13:20,645 --> 00:13:24,185
It is a tool for data
ingest, movement, routing.

241
00:13:24,785 --> 00:13:28,265
And it does this at
whatever scale you need to.

242
00:13:28,315 --> 00:13:29,945
Start small, scales out.

243
00:13:30,295 --> 00:13:32,635
Might sound familiar if you use Snowflake.

244
00:13:32,965 --> 00:13:36,040
Some of the same ideas using
the advantages of Snowflake.

245
00:13:36,420 --> 00:13:37,980
clusters and clouds here.

246
00:13:38,400 --> 00:13:40,220
Guarantee you get your data.

247
00:13:40,790 --> 00:13:45,510
And this feature is really awesome for
buffering and built in back pressure.

248
00:13:45,910 --> 00:13:49,920
So that if things slow down,
you're not going to lose things.

249
00:13:49,920 --> 00:13:51,320
It'll queue up and wait.

250
00:13:51,750 --> 00:13:55,150
You can adjust how much
latency or throughput you want.

251
00:13:55,840 --> 00:14:00,510
And while you're doing that, you
could hook that to auto scaling

252
00:14:00,620 --> 00:14:02,160
if you need to scale higher.

253
00:14:02,800 --> 00:14:03,980
Now data provenance.

254
00:14:04,315 --> 00:14:09,855
or lineage is really awesome if you want
to do auditing or just figure out what's

255
00:14:09,855 --> 00:14:15,585
going on with your distributive processes
this keeps a track of everything going on

256
00:14:15,925 --> 00:14:20,015
every record through the system everything
that's done and you can do things that are

257
00:14:20,275 --> 00:14:26,135
push or pull whatever it is there are a
lot of processors out there a bunch come

258
00:14:26,135 --> 00:14:32,185
with default installs there is a ton more
that are companies specific to snowflake

259
00:14:33,085 --> 00:14:36,945
And there's a bunch more in the open
source like the ones I've written that

260
00:14:36,945 --> 00:14:38,955
you can use, and you can write your own.

261
00:14:38,955 --> 00:14:41,095
It's pretty easy with Python or Java.

262
00:14:41,825 --> 00:14:43,055
Everything's visual.

263
00:14:43,725 --> 00:14:47,385
It's got every kind of
security pluggable in there.

264
00:14:47,635 --> 00:14:49,515
Very easy to extend things out.

265
00:14:50,005 --> 00:14:53,115
Very easy to do version control
and everything you expect

266
00:14:53,115 --> 00:14:54,385
when you're developing code.

267
00:14:55,045 --> 00:14:59,985
And you can move binary data,
unstructured data, zip files.

268
00:15:00,550 --> 00:15:01,200
Images.

269
00:15:01,800 --> 00:15:05,710
Data that looks like tables, structured
data, similar structured data.

270
00:15:06,010 --> 00:15:07,030
You can enrich it.

271
00:15:07,760 --> 00:15:12,400
Use, graphical controls
to visual process things.

272
00:15:12,530 --> 00:15:14,570
Do a simple event processing.

273
00:15:14,570 --> 00:15:16,320
We're not replacing Flink here.

274
00:15:16,900 --> 00:15:18,570
Route things where they need to go.

275
00:15:18,580 --> 00:15:21,940
If they need to go to 20 different
databases, I can do that.

276
00:15:22,440 --> 00:15:26,000
Send things into some central
messaging like Kafka or Pulsar.

277
00:15:26,200 --> 00:15:30,150
It supports almost any
modern protocol out there.

278
00:15:31,060 --> 00:15:33,580
Again, that Python in
there is super important.

279
00:15:33,850 --> 00:15:35,370
Everything is parameterized.

280
00:15:35,980 --> 00:15:40,340
for my Python friends, you probably
don't know JDK 21, what's that mean?

281
00:15:40,860 --> 00:15:45,910
it's written in Java, and this makes
it very modern in ability to work with

282
00:15:46,370 --> 00:15:48,130
Python and other things really well.

283
00:15:48,400 --> 00:15:49,780
Things run really fast.

284
00:15:50,100 --> 00:15:52,200
There's a rules engine
in there to help you out.

285
00:15:52,700 --> 00:15:56,520
You can use database tables
as your schema, support Amazon

286
00:15:56,550 --> 00:15:59,280
Glue, OpenTelemetry is supported.

287
00:15:59,700 --> 00:16:05,040
you can integrate with a lot of cool, SaaS
and online things like Zendesk and Slack.

288
00:16:05,540 --> 00:16:07,210
It is, again, scalable.

289
00:16:07,660 --> 00:16:10,100
Things are stored so
you don't lose things.

290
00:16:10,200 --> 00:16:11,410
We're not going to go into that.

291
00:16:11,740 --> 00:16:14,890
We'll go through some
demos, really quickly.

292
00:16:15,815 --> 00:16:19,315
Just want to show you this before
I move over into running coding

293
00:16:19,315 --> 00:16:23,175
and forget all about these slides,
which you can get through, COM 42.

294
00:16:24,155 --> 00:16:25,685
I do a weekly newsletter.

295
00:16:26,275 --> 00:16:27,435
It is everywhere.

296
00:16:27,445 --> 00:16:31,175
It's in LinkedIn, it's on my blog,
it's in Medium, it's in DevTools,

297
00:16:31,175 --> 00:16:33,175
it's in Hashnode, it's in Substack.

298
00:16:33,675 --> 00:16:36,175
everywhere you want, or
just look at it in GitHub.

299
00:16:36,675 --> 00:16:39,425
I cover a lot of cool open source stuff.

300
00:16:39,780 --> 00:16:41,160
Bunch of snowflake stuff.

301
00:16:41,190 --> 00:16:44,310
Everything in unstructured,
structured, semi structured data.

302
00:16:44,710 --> 00:16:46,230
Lots of open source stuff.

303
00:16:46,520 --> 00:16:50,000
If there's any kind of news
or cool tools you want to, get

304
00:16:50,020 --> 00:16:51,490
mentioned, send them to me.

305
00:16:51,490 --> 00:16:52,250
I'll put them out.

306
00:16:52,570 --> 00:16:53,570
Do that weekly.

307
00:16:54,180 --> 00:16:55,730
Let's start automating stuff.

308
00:16:55,760 --> 00:16:57,000
Let's get things running.

309
00:16:57,580 --> 00:16:58,920
Let's see if we've timed out.

310
00:16:59,820 --> 00:17:00,700
We might have.

311
00:17:00,800 --> 00:17:02,440
Maybe I should zoom in a little bit.

312
00:17:03,170 --> 00:17:04,780
Might be a little small here.

313
00:17:05,280 --> 00:17:10,730
So in this is Apache NiFi and
we could look at diversion here.

314
00:17:10,730 --> 00:17:11,990
This is two one.

315
00:17:12,890 --> 00:17:14,660
This is running on my laptop.

316
00:17:15,560 --> 00:17:18,860
And we could take a look at,
I've got six Giga Ram here.

317
00:17:19,360 --> 00:17:21,520
I could see how much storage I have.

318
00:17:22,150 --> 00:17:24,040
I'm running on my little server here.

319
00:17:24,040 --> 00:17:26,460
You can see I'm using JDP 21.

320
00:17:26,970 --> 00:17:28,020
In what version?

321
00:17:28,350 --> 00:17:29,940
So you can see that things are running.

322
00:17:30,930 --> 00:17:33,820
I could see what's going
on in the system real time.

323
00:17:34,180 --> 00:17:40,670
And what's cool about NiFi, this UI,
everything here can be seen through

324
00:17:40,670 --> 00:17:43,630
a REST endpoint, so you don't have
to use the GUI if you don't want.

325
00:17:44,000 --> 00:17:45,510
But this is how I code.

326
00:17:46,100 --> 00:17:48,800
I drag and drop things on here.

327
00:17:49,200 --> 00:17:52,710
And if I wanted to add one of the
Python processors that I just added,

328
00:17:53,420 --> 00:17:55,050
I could just pick one like that.

329
00:17:55,810 --> 00:17:59,110
Or I could look at other things,
like I want something from AWS.

330
00:18:00,100 --> 00:18:02,520
What are the AWS specific things?

331
00:18:02,860 --> 00:18:05,740
What are the things
that are Azure specific?

332
00:18:06,120 --> 00:18:07,500
what's Google stuff?

333
00:18:08,240 --> 00:18:11,050
and I could do things like grab
things from the Google Drive.

334
00:18:11,550 --> 00:18:14,720
I could also look at a lot
of different, file systems.

335
00:18:15,480 --> 00:18:22,230
And other things like MQTT,
WebSockets, Dropbox, Vox, Google Drive.

336
00:18:22,840 --> 00:18:24,090
JSON, all kinds of stuff.

337
00:18:24,160 --> 00:18:27,780
Listen to FTP, TCP, SNMP, UDP.

338
00:18:28,430 --> 00:18:30,120
any attributes you want there.

339
00:18:30,440 --> 00:18:32,700
Very good for working with logs.

340
00:18:33,340 --> 00:18:35,820
Very good for working with files.

341
00:18:36,190 --> 00:18:37,990
And network stuff.

342
00:18:38,920 --> 00:18:43,190
anything you've got a wait on,
it can wait for data to come.

343
00:18:43,560 --> 00:18:44,960
But let's give you an example.

344
00:18:45,330 --> 00:18:47,360
I'm gonna just run this once.

345
00:18:47,900 --> 00:18:51,070
And this is my Python custom processor.

346
00:18:51,755 --> 00:18:57,265
This will grab GTFS data from,
the Boston Transit System.

347
00:18:57,725 --> 00:19:02,695
Now GTFS data is a Google,
transit format that's in Protobuf.

348
00:19:03,495 --> 00:19:05,745
And I have my Python thing get it.

349
00:19:06,625 --> 00:19:07,755
Make sure it's valid.

350
00:19:08,245 --> 00:19:09,915
And then return it as JSON.

351
00:19:10,695 --> 00:19:12,565
So we get JSON out of that.

352
00:19:13,165 --> 00:19:16,745
And then here, this is that
provenance I talked about.

353
00:19:17,430 --> 00:19:20,670
And I can look at the,
data that came through.

354
00:19:21,070 --> 00:19:23,680
And I can see it was a 400K file.

355
00:19:24,090 --> 00:19:27,310
I can see some of the
metadata, where it came from.

356
00:19:27,840 --> 00:19:29,950
And some unique identifications on it.

357
00:19:29,950 --> 00:19:32,960
I could also look at the
raw data if I wanted.

358
00:19:33,620 --> 00:19:35,770
So that data is in system.

359
00:19:36,260 --> 00:19:37,620
It has got process.

360
00:19:37,620 --> 00:19:40,120
I split it up into individual records.

361
00:19:40,750 --> 00:19:44,400
And then I check to see if there's
a special, term in the data.

362
00:19:44,400 --> 00:19:50,775
Um, multi carriage in, GTFS data
means that there's multiple buses or

363
00:19:50,905 --> 00:19:52,605
multiple buses connected together.

364
00:19:52,975 --> 00:19:57,535
Those records are weird, so I process
them separately over here, where I

365
00:19:57,535 --> 00:20:02,135
pull out the extra fields there, split
them up, then put it back into the

366
00:20:02,135 --> 00:20:07,085
other ones, because I want to pull out
special fields, and I can do that with

367
00:20:07,125 --> 00:20:09,515
JSON path, again, not writing code.

368
00:20:10,305 --> 00:20:17,085
And then I'm going to build a new file,
I want to add a, timestamp to that, and

369
00:20:17,115 --> 00:20:22,515
then I'm going to say that this state
is JSON, and I'm going to use, our

370
00:20:22,515 --> 00:20:26,025
query record processor to convert it.

371
00:20:26,405 --> 00:20:31,335
So what I'm going to do is make
sure the route isn't null, and use

372
00:20:31,335 --> 00:20:33,865
that SQL to convert JSON to JSON.

373
00:20:34,225 --> 00:20:39,945
But, if I decided I wanted to use,
output this in another format, I could

374
00:20:39,945 --> 00:20:41,785
go, okay, let me create a new one.

375
00:20:41,785 --> 00:20:47,085
I can make it comma separated
file, or I can create a new one.

376
00:20:47,465 --> 00:20:50,335
So I could just go, let me
create a new service here.

377
00:20:50,875 --> 00:20:58,085
And as you can see, I can make Avro, I
can make my own freeform text, JSON, XML.

378
00:20:58,155 --> 00:21:02,485
I could also look this up and
have it do it dynamically based

379
00:21:02,485 --> 00:21:04,535
on some attributes, which is cool.

380
00:21:05,035 --> 00:21:05,925
But we're not going to do that.

381
00:21:05,925 --> 00:21:07,315
We'll just start that up again.

382
00:21:07,315 --> 00:21:07,365
Cool.

383
00:21:07,865 --> 00:21:09,875
splits and here I got a bunch of splits.

384
00:21:09,965 --> 00:21:16,905
What this one's gonna do is call a
database to find, the route data, get

385
00:21:16,905 --> 00:21:20,380
that route data from another table
and combine it with this record.

386
00:21:21,160 --> 00:21:21,970
So that's cool.

387
00:21:22,390 --> 00:21:27,490
Also in this other one, let's
go to this other bit of code.

388
00:21:28,000 --> 00:21:34,990
I am calling a live set of planes that
I've got running on a server over there.

389
00:21:35,665 --> 00:21:40,765
And this is tracking all the planes
near me using ADS B transponders.

390
00:21:41,515 --> 00:21:43,225
So I'm grabbing that data.

391
00:21:43,695 --> 00:21:46,695
Make sure we will grab one more of these.

392
00:21:47,375 --> 00:21:48,975
I split that data out.

393
00:21:49,945 --> 00:21:51,435
Get the fields I want.

394
00:21:52,355 --> 00:21:54,325
Add a timestamp to the record.

395
00:21:54,605 --> 00:21:56,075
Plus a unique ID.

396
00:21:56,575 --> 00:21:59,665
Filter this out and then I can
push that right to Snowflake.

397
00:22:00,295 --> 00:22:01,665
Very simple here.

398
00:22:02,285 --> 00:22:03,335
How you want to do that.

399
00:22:04,200 --> 00:22:08,500
And then, also I'm splitting that
data out, putting out fields I like,

400
00:22:09,030 --> 00:22:10,980
and then I'm pushing them to Slack.

401
00:22:11,950 --> 00:22:14,230
And pushing things to
Slack is really easy.

402
00:22:14,230 --> 00:22:18,700
I gotta use the token, put it
in the channel I want, and then

403
00:22:18,700 --> 00:22:23,640
just format the data along with
whatever data I want is my output.

404
00:22:24,190 --> 00:22:25,980
And then we just send that to Slack.

405
00:22:26,720 --> 00:22:30,170
when I'm done posting over here,
I can push that into an S3 bucket.

406
00:22:30,650 --> 00:22:31,510
let's see here.

407
00:22:32,220 --> 00:22:33,650
Here's a couple more here.

408
00:22:34,010 --> 00:22:35,550
This one just got sent in.

409
00:22:35,880 --> 00:22:37,280
You gotta keep that running.

410
00:22:38,090 --> 00:22:38,840
Keep that running.

411
00:22:38,840 --> 00:22:39,890
Send a couple more.

412
00:22:40,570 --> 00:22:42,740
And you can see we got new ones coming in.

413
00:22:43,480 --> 00:22:45,600
And it's just, the different data here.

414
00:22:45,640 --> 00:22:47,570
Altitude, the time stamp.

415
00:22:48,070 --> 00:22:49,230
Yeah, I'm sending too many.

416
00:22:49,320 --> 00:22:50,750
They don't like when I send that many.

417
00:22:51,220 --> 00:22:52,960
I have the free Slack account.

418
00:22:53,050 --> 00:22:53,480
Sorry, Slack.

419
00:22:54,190 --> 00:22:55,020
I won't send too many messages.

420
00:22:55,520 --> 00:22:58,360
But yeah, I could just send
anything I want over there.

421
00:22:59,105 --> 00:23:02,245
what planes, what their
altitude, that sort of thing.

422
00:23:02,795 --> 00:23:05,215
just to give you an example
while the code is running.

423
00:23:05,715 --> 00:23:07,115
pretty easy to do.

424
00:23:07,625 --> 00:23:11,765
drop your Python in there, and it
doesn't look like anything different.

425
00:23:12,185 --> 00:23:16,425
Like I said, in this one we don't have
one, but we could just decide, okay, let

426
00:23:16,455 --> 00:23:22,325
me add a new Python processor, this fake
record one, add it here, connect it, set

427
00:23:22,325 --> 00:23:24,285
some parameters, and you're good to go.

428
00:23:24,405 --> 00:23:25,575
thanks for watching my talk.

