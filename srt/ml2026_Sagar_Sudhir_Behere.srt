1
00:00:00,500 --> 00:00:01,340
Speaker 25: Hello everyone.

2
00:00:02,060 --> 00:00:05,750
Before we get into machine
learning models or architecture,

3
00:00:06,140 --> 00:00:07,640
I wanna start with a simple idea.

4
00:00:08,140 --> 00:00:13,270
Third party risk management was
never designed for scale, speed,

5
00:00:13,629 --> 00:00:16,090
or complexity we operate in today.

6
00:00:16,590 --> 00:00:21,030
Most third party programs are
built for periodic reviews, static

7
00:00:21,300 --> 00:00:23,550
questionnaires, and manual judgements.

8
00:00:24,420 --> 00:00:29,520
But today, enterprises depend on
hundreds, sometimes thousands of

9
00:00:29,640 --> 00:00:36,960
third parties operating across cloud
or APIs, global supply chains, and

10
00:00:36,960 --> 00:00:38,970
regulated financial ecosystems.

11
00:00:39,470 --> 00:00:41,720
And risk doesn't wait for annual reviews.

12
00:00:41,720 --> 00:00:48,160
So today I want to show you how we have
been rearchitecting, TPRM as a machine

13
00:00:48,160 --> 00:00:50,800
learning driven intelligent system.

14
00:00:51,580 --> 00:00:53,380
Not just a compliance function.

15
00:00:53,880 --> 00:00:55,980
This is not theoretical.

16
00:00:56,580 --> 00:01:02,760
This is what we have actually being built,
tested, broken, and rebuilt at scale.

17
00:01:03,260 --> 00:01:06,919
Let me start with a quick
introduction so you know where

18
00:01:06,919 --> 00:01:08,479
these lessons are coming from.

19
00:01:08,979 --> 00:01:13,235
My name is Saga Berry and I lead third
party risk management oversight at a

20
00:01:13,235 --> 00:01:15,035
blockchain based stable point company.

21
00:01:15,535 --> 00:01:20,774
We operate in the digital financial
ecosystem, and we are a FinTech and one of

22
00:01:20,774 --> 00:01:23,205
the largest issuer of stablecoin globally.

23
00:01:23,705 --> 00:01:27,635
The reason I mentioned that is
because we operate in a high risk,

24
00:01:28,175 --> 00:01:33,365
fast changing environment where
regulatory expectations evolve quickly.

25
00:01:34,055 --> 00:01:38,915
Vendors change constantly, and
risk signals shift in real time.

26
00:01:39,415 --> 00:01:40,975
I've spent over a decade across.

27
00:01:41,395 --> 00:01:45,595
Third party and enterprise
risk in many fortune hundreds.

28
00:01:45,744 --> 00:01:49,195
Big four consulting, FinTech, thanks.

29
00:01:49,765 --> 00:01:54,435
Across all of these environments,
one patent kept repeating, risk

30
00:01:54,435 --> 00:01:59,085
teams were doing the right thing,
but the system itself wasn't designed

31
00:01:59,085 --> 00:02:01,100
for speed, scale, or intelligence.

32
00:02:01,600 --> 00:02:05,650
My work today sits at an
intersection of risk management,

33
00:02:05,650 --> 00:02:07,330
machine learning, and automation.

34
00:02:07,780 --> 00:02:12,040
Not to cut corners, but to
make risk management smarter,

35
00:02:12,040 --> 00:02:14,110
faster, and more defensible.

36
00:02:14,610 --> 00:02:18,420
So what you'll hear today isn't
a theory or a vendor pitch.

37
00:02:19,020 --> 00:02:20,280
It's a playbook.

38
00:02:20,400 --> 00:02:24,090
We used to reduce our
onboarding time dramatically.

39
00:02:24,590 --> 00:02:29,000
While improving risk outcomes,
auditability and regulatory

40
00:02:29,030 --> 00:02:34,270
confidence, I am sure many of you
have already experimenting with AI

41
00:02:34,540 --> 00:02:37,690
or machine learning in your third
party risk management programs.

42
00:02:38,500 --> 00:02:40,299
My goal here today is simple.

43
00:02:40,799 --> 00:02:44,279
I want you to walk away
with two or three ideas.

44
00:02:44,760 --> 00:02:48,089
You can actually take back
and apply to your program.

45
00:02:48,630 --> 00:02:52,049
Or your platform or even
your product roadmap.

46
00:02:52,549 --> 00:02:52,880
All right.

47
00:02:53,030 --> 00:02:57,020
Before we talk about solutions,
I wanna level set on a problem

48
00:02:57,020 --> 00:02:58,280
we have all dealing with.

49
00:02:59,030 --> 00:03:02,090
Modern enterprises don't
just have vendors anymore.

50
00:03:02,330 --> 00:03:07,755
They have an entire ecosystem, hundreds,
sometimes thousands of third parties.

51
00:03:08,255 --> 00:03:13,415
Your cloud providers, SaaS platforms,
payment processors, data vendors,

52
00:03:13,865 --> 00:03:20,555
open source dependencies, each one
expand your attack surface and more

53
00:03:20,555 --> 00:03:22,475
importantly, your risk surface.

54
00:03:22,975 --> 00:03:26,995
But most traditional programs were
designed for a very different world.

55
00:03:27,495 --> 00:03:30,945
They assume risk changes slowly.

56
00:03:31,575 --> 00:03:34,815
They assume reviews can
happen annually or quarterly.

57
00:03:35,715 --> 00:03:40,715
They assume static questionnaires
and manual reviews are enough.

58
00:03:41,215 --> 00:03:46,065
That assumption no longer holds risk
today moves at a speed of software,

59
00:03:46,455 --> 00:03:48,315
not at the speed of audit cycles.

60
00:03:48,815 --> 00:03:54,995
At the same time, regulatory expectations
are going in the opposite direction.

61
00:03:55,835 --> 00:03:57,725
Not loser, but tighter.

62
00:03:58,415 --> 00:03:59,855
More transparency.

63
00:04:00,320 --> 00:04:04,100
More explainability, more
continuous oversight.

64
00:04:04,700 --> 00:04:06,920
So we are stuck with a contradiction.

65
00:04:07,420 --> 00:04:14,050
We are expected to move faster while
proving more using systems that ne

66
00:04:14,230 --> 00:04:16,240
were never designed to scale this way.

67
00:04:17,170 --> 00:04:20,710
And the contradiction shows up
very clearly in our traditional

68
00:04:20,710 --> 00:04:23,290
risk management structure.

69
00:04:23,790 --> 00:04:27,660
Moving on to next slide, the
traditional structural misalignment.

70
00:04:27,720 --> 00:04:31,080
This slide is very uncomfortable
because it describes how

71
00:04:31,080 --> 00:04:32,490
most of us have been trained.

72
00:04:33,270 --> 00:04:38,190
Let's start with a calendar based
reviews, annual or quarterly assessment.

73
00:04:38,340 --> 00:04:45,920
Assume risk politely awaits its turn, but
breaches, outages, financial distress.

74
00:04:46,700 --> 00:04:48,575
They don't care about
your review calendars.

75
00:04:48,995 --> 00:04:51,295
If something changes in month two.

76
00:04:52,100 --> 00:04:55,700
You don't discover until month 12
if, and that's if you're lucky.

77
00:04:56,200 --> 00:05:01,750
Your next is static questioners,
one size fit, all surveys, same

78
00:05:01,750 --> 00:05:04,990
questions, same order, same depth,
regardless of vendor profile.

79
00:05:05,890 --> 00:05:11,560
So low risk vendors buried in unnecessary
questions and high risk vendors often

80
00:05:11,560 --> 00:05:12,745
don't get the depth they deserve.

81
00:05:13,245 --> 00:05:14,775
Everyone is frustrated.

82
00:05:14,775 --> 00:05:19,245
Your vendors, your business teams, and
your, of course your risk teams as well.

83
00:05:19,745 --> 00:05:22,055
And finally over there is
manual evidence review.

84
00:05:22,655 --> 00:05:25,685
Humans is reading hundreds
of unstructured documents.

85
00:05:26,075 --> 00:05:31,205
Your policies, soc reports,
certifications, PDF after PF stacked.

86
00:05:31,705 --> 00:05:35,075
They don't just create bottleneck,
it creates inconsistencies.

87
00:05:35,575 --> 00:05:39,115
Imagine this two analysts can
read the same document and each

88
00:05:39,115 --> 00:05:42,775
reach different conclusions, and
neither one is technically wrong.

89
00:05:43,495 --> 00:05:47,215
So what we end up with
is not a people problem.

90
00:05:47,245 --> 00:05:53,335
It's a structural misalignment between
how risk behaves today and how TPRM

91
00:05:53,335 --> 00:05:55,375
systems were originally designed.

92
00:05:55,875 --> 00:06:00,820
And that realization is what
forced us to rethink DPRM.

93
00:06:01,320 --> 00:06:06,630
Not as a checklist or process,
but a intelligent system.

94
00:06:07,130 --> 00:06:12,800
So the question becomes, if this
model doesn't scale, then what does?

95
00:06:13,300 --> 00:06:17,500
So once you accept that the
traditional TPRM doesn't scale, the

96
00:06:17,500 --> 00:06:22,270
real question becomes, what does
scalable model actually look like?

97
00:06:23,020 --> 00:06:24,850
For us, the answer wasn't.

98
00:06:25,750 --> 00:06:28,090
Add more people or buy bigger tools.

99
00:06:28,420 --> 00:06:33,420
It was Rearchitecting, TPRM as a
machine learning driven system, and

100
00:06:33,420 --> 00:06:38,060
at a higher level, this architecture
has four moving parts, which I've

101
00:06:38,060 --> 00:06:39,350
listed over there on the slide.

102
00:06:40,040 --> 00:06:41,375
First one is data inges.

103
00:06:41,875 --> 00:06:47,425
So instead of relying only on what
vendor sense, we ingest data continuously

104
00:06:47,545 --> 00:06:52,615
through policies attestations,
public disco closures, intel threat

105
00:06:52,615 --> 00:06:54,805
intelligence, financial signals.

106
00:06:55,305 --> 00:06:56,085
The goal is simple.

107
00:06:56,585 --> 00:06:58,085
Do not wait for risk to be reported.

108
00:06:58,585 --> 00:07:01,475
Observe it directly
when wherever possible.

109
00:07:01,975 --> 00:07:04,115
Second is intelligent processing.

110
00:07:04,615 --> 00:07:08,005
This is where machine learning and
NLP starts doing the heavy lifting,

111
00:07:08,875 --> 00:07:14,515
extracting control evidence from
unstructured document, normalizing

112
00:07:14,515 --> 00:07:16,525
it and making it comparable.

113
00:07:17,035 --> 00:07:18,775
Humans are great at judgment.

114
00:07:19,165 --> 00:07:22,645
They're not great at
reading some 300 policies.

115
00:07:23,145 --> 00:07:28,365
Then we move into risk evaluation
instead of static scoring,

116
00:07:28,780 --> 00:07:30,800
we evaluate risk dynamically.

117
00:07:31,575 --> 00:07:37,145
Based on vendor risk profiles,
your criticality control maturity

118
00:07:37,205 --> 00:07:43,265
and absorbed signals, and just as
important, every determination comes

119
00:07:43,265 --> 00:07:46,745
with explainability and not just score.

120
00:07:47,245 --> 00:07:49,015
And finally, you have
continuous monitoring.

121
00:07:49,735 --> 00:07:52,135
This is where the biggest shift happens.

122
00:07:52,675 --> 00:07:59,155
TPRM stops being episodic and starts
behaving like a living system.

123
00:07:59,950 --> 00:08:03,520
The architecture runs
continuously, not annually.

124
00:08:04,020 --> 00:08:08,760
One of the first bottlenecks we
attacked was manual evidence review.

125
00:08:09,260 --> 00:08:14,570
Think about how much time TPRM team
spends reading PDFs or your SOC reports.

126
00:08:14,575 --> 00:08:14,605
Kz.

127
00:08:15,105 --> 00:08:15,755
This is where.

128
00:08:16,255 --> 00:08:20,365
Natural language processing
changed everything for us.

129
00:08:20,515 --> 00:08:24,415
We trained NLP models to extract
and normalize control evidences from

130
00:08:24,535 --> 00:08:29,615
unstructured vendor artifacts instead
of asking Does this document look okay?

131
00:08:30,125 --> 00:08:33,875
The model asks, does this
document demonstrate the control?

132
00:08:34,685 --> 00:08:39,035
We actually care about it identifies
the control, extracts the relevant

133
00:08:39,035 --> 00:08:41,675
text, maps it back to the requirements.

134
00:08:42,560 --> 00:08:43,730
And this is critical.

135
00:08:44,210 --> 00:08:48,440
Every extracted data point
maintains full traceability.

136
00:08:48,940 --> 00:08:55,810
You can click from risk conclusion all
the way back to risk extract sentences

137
00:08:55,810 --> 00:09:01,320
in this source document that matters,
not just for audit, but for trust.

138
00:09:01,820 --> 00:09:03,200
The impact were immediate.

139
00:09:03,350 --> 00:09:07,670
Thousands of documents processed
automatically, massive reduction

140
00:09:07,670 --> 00:09:12,470
in manual workload, and far more
consistent outcomes across reviewers.

141
00:09:12,950 --> 00:09:14,960
Humans didn't disappear from the process.

142
00:09:15,740 --> 00:09:22,760
They moved up the value chain from
reading documents to validating judgments.

143
00:09:23,270 --> 00:09:26,180
Once evidence extraction was
no longer the bottleneck.

144
00:09:26,765 --> 00:09:30,935
We could tackle something
even bigger as a redundancy.

145
00:09:31,435 --> 00:09:36,055
Once we solved the document review,
we ran into a different problem.

146
00:09:36,595 --> 00:09:42,355
We were still asking vendors way too
many questions, and not because we

147
00:09:42,415 --> 00:09:47,425
loved questionnaire, but because the
different frameworks kept asking the

148
00:09:47,425 --> 00:09:50,455
same thing in a slightly different way.

149
00:09:51,415 --> 00:09:52,200
So we turned to.

150
00:09:52,855 --> 00:10:00,075
Semantic similarity models instead
of treating question as a text

151
00:10:00,075 --> 00:10:03,615
string, we treated them as meaning.

152
00:10:04,115 --> 00:10:09,185
If two question, meaning two questions
are asking for the same control,

153
00:10:09,185 --> 00:10:13,355
the system knows it, even if the
wording is completely different.

154
00:10:13,855 --> 00:10:16,225
That resulted in eliminating redundancies.

155
00:10:16,630 --> 00:10:22,480
The model clusters overlapping questions
across frameworks, be it SOC or iso

156
00:10:22,570 --> 00:10:24,760
internal policy or Regulatory requirement.

157
00:10:25,750 --> 00:10:29,530
So instead of five nearly
identical questions, the vendor

158
00:10:29,530 --> 00:10:32,980
sees one intelligent question.

159
00:10:33,480 --> 00:10:39,050
That alone removed a lot of friction,
but we didn't just stop there.

160
00:10:39,550 --> 00:10:43,675
The system adapts follow up questions
based on how the vendor answers.

161
00:10:44,175 --> 00:10:47,265
If the control clearly exists
and is mature, we move on.

162
00:10:48,195 --> 00:10:56,225
If something looks weak, can ambiguous,
that's where we dig deep depth where

163
00:10:56,225 --> 00:10:58,595
it matters, and speed where it doesn't.

164
00:10:59,095 --> 00:11:05,155
Finally, context aware routing instead
of dumping every question on a single

165
00:11:05,155 --> 00:11:09,145
vendor contact, the model routes the
question to the right stakeholder.

166
00:11:09,790 --> 00:11:16,150
Security to security SMEs, finance
to finance teams, response quality

167
00:11:16,150 --> 00:11:19,090
went straight up back and forth.

168
00:11:19,090 --> 00:11:20,050
Went straight down.

169
00:11:20,830 --> 00:11:26,080
That is the moment the vendor stopped
saying, this feels painful, and started

170
00:11:26,080 --> 00:11:27,790
saying, this actually makes sense.

171
00:11:28,540 --> 00:11:33,370
Now, if you're relying on machine
learning this heavily, there's a

172
00:11:33,370 --> 00:11:34,785
natural question that comes up.

173
00:11:35,285 --> 00:11:39,395
Whenever I talk to ML in risk
management, someone inevitably will

174
00:11:39,395 --> 00:11:43,385
ask, but what if the model is wrong?

175
00:11:43,885 --> 00:11:45,655
That's the right question to ask.

176
00:11:46,155 --> 00:11:48,915
We never trust a single model
to make a risk determination.

177
00:11:49,665 --> 00:11:55,275
Instead, we use assembling models,
multiple independent models, evaluating

178
00:11:55,275 --> 00:11:58,425
the same risk signal each model.

179
00:11:58,995 --> 00:12:01,185
Looks at data slightly different.

180
00:12:02,025 --> 00:12:05,205
And if they agree, confidence goes up.

181
00:12:05,415 --> 00:12:10,665
If they disagree, the system
flags for cases automatically.

182
00:12:11,165 --> 00:12:15,005
So every risk output comes
with a confidence score.

183
00:12:15,395 --> 00:12:17,390
High confidence, the
workflow moves forward.

184
00:12:18,050 --> 00:12:20,345
Low confidence, a human steps in.

185
00:12:20,845 --> 00:12:24,655
We didn't remove human judgment, we
aimed it where it actually adds value.

186
00:12:25,155 --> 00:12:31,445
This also solved a major governance
concern, single model bias, black box

187
00:12:31,445 --> 00:12:33,845
decisions, regulatory explainability.

188
00:12:34,625 --> 00:12:39,665
With multiple models, you can
explain why a conclusion was reached

189
00:12:40,085 --> 00:12:42,005
and how confident we are in it.

190
00:12:42,505 --> 00:12:45,955
So at this point, something
important has changed.

191
00:12:46,585 --> 00:12:49,165
TPRM was no longer just fast.

192
00:12:49,225 --> 00:12:50,425
It was smarter.

193
00:12:51,115 --> 00:12:58,315
More consistent and more defensible,
and that set the stage for a bigger

194
00:12:58,315 --> 00:13:06,045
shift of all moving from a periodic
review to a continuous intelligence.

195
00:13:06,545 --> 00:13:12,215
Up until now, everything I've shown you
focused on making assessments better, but

196
00:13:12,270 --> 00:13:14,040
the real breakthrough happened when we.

197
00:13:14,730 --> 00:13:18,270
Stopped asking, how do
you improve reviews?

198
00:13:18,770 --> 00:13:24,260
And started asking instead, why are
we still relying on reviews at all?

199
00:13:25,250 --> 00:13:28,010
Traditional TPRM assumes
risks changes slowly.

200
00:13:28,510 --> 00:13:34,640
So we check it periodically but
modern risk doesn't behave that way.

201
00:13:35,140 --> 00:13:40,555
So we moved from periodic assessments
to an event driven monitoring.

202
00:13:41,545 --> 00:13:46,305
Instead of waiting for 12 months to
rediscover risk system, continuously

203
00:13:46,305 --> 00:13:48,525
ingest signals from multiple domains.

204
00:13:49,035 --> 00:13:53,955
When something materially changes,
a vulnerability disclosure, a

205
00:13:53,955 --> 00:13:59,900
breach notification, a financial
signal, an operational anomaly, the

206
00:13:59,905 --> 00:14:04,475
system doesn't wait, it reacts, and
that is the important distinction.

207
00:14:05,135 --> 00:14:08,225
We are not continuously reassessing
everything all the time.

208
00:14:08,975 --> 00:14:16,025
That would just create a noise at a faster
speed, but we are listening for meaningful

209
00:14:16,145 --> 00:14:20,285
events and only then triggering actions.

210
00:14:20,855 --> 00:14:25,355
So TPRM stops being a calendar
driven process and starts behaving

211
00:14:25,355 --> 00:14:27,125
like a monitoring capability.

212
00:14:27,575 --> 00:14:30,460
The obvious next question is,
what exactly are we monitoring?

213
00:14:30,960 --> 00:14:34,860
To make continuous monitoring work,
we need diverse, reliable data.

214
00:14:35,360 --> 00:14:38,630
We group our inputs into five
primary signal categories.

215
00:14:39,130 --> 00:14:44,200
First, threat intelligence,
real time feeds, vulnerability

216
00:14:44,200 --> 00:14:46,645
databases, indicators of compromise.

217
00:14:47,145 --> 00:14:52,215
This gives us early visibility into
emerging cyber risk, often before

218
00:14:52,215 --> 00:14:53,925
vendor formally discloses them.

219
00:14:54,425 --> 00:14:59,975
Second security advisories,
CVE disclosures, vendor

220
00:14:59,975 --> 00:15:02,645
bulletins, patch announcements.

221
00:15:02,975 --> 00:15:08,515
When something critical drops, we
know immediately and we can correlate

222
00:15:08,515 --> 00:15:09,715
it to the effective vendors.

223
00:15:10,215 --> 00:15:11,690
Third, financial indicators.

224
00:15:12,190 --> 00:15:18,430
Credit ratings earning signals,
market stress indicators, because

225
00:15:18,430 --> 00:15:23,595
financial distress often shows up
before a fan operational failure.

226
00:15:24,095 --> 00:15:29,435
And fourth, public disclosures,
breach notifications, regulatory

227
00:15:29,465 --> 00:15:31,595
finding enforcement actions.

228
00:15:32,565 --> 00:15:34,715
These are high signal events.

229
00:15:35,215 --> 00:15:37,615
And they deserve immediate attention.

230
00:15:38,115 --> 00:15:43,715
And finally, the behavioral patterns
change in mental behavior, operational

231
00:15:43,775 --> 00:15:46,565
anomalies shift in activity.

232
00:15:47,065 --> 00:15:51,705
These are often subtle, but
incredibly powerful when you

233
00:15:51,705 --> 00:15:53,735
see them early individually.

234
00:15:53,735 --> 00:15:58,175
None of these signals tell you this
full story, but when you correlate

235
00:15:58,175 --> 00:16:00,245
them across domains and over time.

236
00:16:00,980 --> 00:16:05,510
You start seeing risk forming,
not just risk reported.

237
00:16:06,010 --> 00:16:10,690
That's what allows us to move
from detection to prediction.

238
00:16:11,190 --> 00:16:15,345
Up to this point, we have been
talking about detection, but the real

239
00:16:15,395 --> 00:16:20,400
value of machine learning in TPRM
isn't just detecting risk faster.

240
00:16:21,060 --> 00:16:24,480
It is detecting risk before
it fully materializes.

241
00:16:24,990 --> 00:16:28,290
This is where predictive
risk detection comes in.

242
00:16:28,790 --> 00:16:32,690
We have trained models on historic
incident data across cyber

243
00:16:32,690 --> 00:16:36,230
events, your operational failures
and financial stress scenarios.

244
00:16:36,830 --> 00:16:40,510
The goal wasn't to predict the
exact next breach, but the goal was

245
00:16:40,510 --> 00:16:47,740
to identify patterns that tend to
slow up before anything goes wrong.

246
00:16:48,240 --> 00:16:49,860
What we found was very interesting.

247
00:16:50,360 --> 00:16:52,910
Major incidences are rarely random.

248
00:16:53,749 --> 00:16:55,910
They usually pre proceed.

249
00:16:55,910 --> 00:17:01,969
They're usually proceeded by subtle
signals such as missed patches,

250
00:17:02,120 --> 00:17:06,170
delayed disclosures, changes in
operational behavior, financial stress.

251
00:17:06,949 --> 00:17:07,610
Individually.

252
00:17:07,610 --> 00:17:11,954
These signals look harmless, but
when you analyze them together over

253
00:17:11,985 --> 00:17:14,634
time, patterns start to emerge.

254
00:17:15,134 --> 00:17:17,624
That's where machine learning excels.

255
00:17:18,224 --> 00:17:22,514
Connecting weak signals humans
would never correlate manually.

256
00:17:23,014 --> 00:17:30,124
So in store, instead of reacting after
an incident, we can intervene early as

257
00:17:30,124 --> 00:17:35,434
targeted questions trigger reassessments
or escalate controls before impact.

258
00:17:35,934 --> 00:17:39,594
This is the difference between
managing risk and anticipating it.

259
00:17:40,094 --> 00:17:45,344
Now, whenever you talk about prediction
and machine learning in regulated

260
00:17:45,344 --> 00:17:49,604
environment, that's a very reasonable
concern that always comes up.

261
00:17:50,104 --> 00:17:54,574
The question is, how do you explain
this to an auditor who a regulator,

262
00:17:54,574 --> 00:17:59,884
or even your senior leadership 'cause
prediction without explainability?

263
00:18:00,245 --> 00:18:04,954
It's just a guesswork, and that's not
acceptable in risk management, right?

264
00:18:05,454 --> 00:18:10,014
So the first design principle we
enforced was transparent reasoning.

265
00:18:10,514 --> 00:18:16,394
Every risk score comes with explanation
which factors contributed, how

266
00:18:16,394 --> 00:18:18,134
strongly and why they mattered.

267
00:18:18,704 --> 00:18:19,064
No.

268
00:18:19,064 --> 00:18:22,745
Black boxes is second
audit trail preservation.

269
00:18:23,254 --> 00:18:25,205
Every decision is traceable.

270
00:18:25,354 --> 00:18:29,584
End to end from source data to model
output to final human judgment.

271
00:18:30,084 --> 00:18:32,904
If someone asks, why did
we rate this vendor high?

272
00:18:33,445 --> 00:18:35,874
You can show them precisely.

273
00:18:36,374 --> 00:18:38,924
And finally, regulatory alignment.

274
00:18:39,424 --> 00:18:43,744
This architecture was built with
regulatory expectation in mind.

275
00:18:44,244 --> 00:18:48,554
That is strong spar,
defensibility, and consistency.

276
00:18:49,484 --> 00:18:52,514
Machine learning didn't
weaken our governance model.

277
00:18:52,934 --> 00:18:54,344
It strengthened it.

278
00:18:54,764 --> 00:18:54,794
Okay.

279
00:18:55,294 --> 00:19:00,154
At this point, TPRM stops being
viewed as a compliance obligation.

280
00:19:00,654 --> 00:19:06,594
It starts being seen as a trusted
intelligence capability, and that

281
00:19:06,594 --> 00:19:10,344
shift had very real business impact.

282
00:19:10,844 --> 00:19:14,474
So let's talk about what all
of this actually delivered.

283
00:19:15,104 --> 00:19:18,634
Not in theory, but in
practice First scale.

284
00:19:19,134 --> 00:19:23,964
We can now assess continuous monitoring
thou, we can now assess and continuously

285
00:19:23,964 --> 00:19:28,494
monitor thousands of vendors without
growing headcount at the same rate.

286
00:19:29,004 --> 00:19:35,514
That's critical because vendor ecosystems
grow faster than risk teams ever will.

287
00:19:36,014 --> 00:19:40,169
Second response velocity, what
used to take weeks or months?

288
00:19:40,534 --> 00:19:44,284
We can now detect and
respond in NR in ours.

289
00:19:44,419 --> 00:19:48,619
A time difference matters, especially
when you are dealing with cyber

290
00:19:48,619 --> 00:19:50,239
operational financial risk.

291
00:19:50,739 --> 00:19:52,029
Third is consistency.

292
00:19:52,989 --> 00:19:58,989
Machine learning helped us standardize how
risk is evaluated, reducing review or to

293
00:19:58,989 --> 00:20:05,849
reviewer variability that makes outcome
more predictable and far easier to defend.

294
00:20:06,349 --> 00:20:08,119
And finally, strategic intelligence.

295
00:20:08,619 --> 00:20:11,469
TPRM stopped being a back office function.

296
00:20:12,159 --> 00:20:18,459
It became a source of insight for
leadership, highlighting concentration

297
00:20:18,459 --> 00:20:25,299
risk, systemic exposure, and emerging
patterns across the ecosystem.

298
00:20:25,799 --> 00:20:29,879
And that brings me to the most
important shift of all for TPRM.

299
00:20:30,379 --> 00:20:32,254
When you s. Step back.

300
00:20:33,034 --> 00:20:37,204
What machine learning really did
for us wasn't just automation.

301
00:20:37,704 --> 00:20:43,164
It changed how TPRM is perceived
instead of being a compliance check,

302
00:20:43,164 --> 00:20:51,624
the box TPRM became a core intelligent
capability inside a resilient digital

303
00:20:51,684 --> 00:20:56,679
ecosystem, one that combines scale,
explainability, and adaptability.

304
00:20:57,179 --> 00:21:03,209
Machine learning gave us the ability
to see risk earlier, respond faster,

305
00:21:03,539 --> 00:21:09,149
explain decisions clearer and adapt
continuously as vendor and threat evolved.

306
00:21:09,649 --> 00:21:12,229
That combination is
what regulators expect.

307
00:21:12,799 --> 00:21:18,889
It's what leadership needs, and it's
what modern digital ecosystem requires.

308
00:21:19,389 --> 00:21:25,839
This is what allows organizations to
move faster without losing control.

309
00:21:26,339 --> 00:21:29,369
So let me leave you
with one final thought.

310
00:21:29,869 --> 00:21:36,269
If there's one thing I hope you take
away from today, it's this machine

311
00:21:36,269 --> 00:21:38,429
learning doesn't replace risk judgment.

312
00:21:38,939 --> 00:21:40,409
It amplifies it.

313
00:21:41,129 --> 00:21:42,449
You don't need to start.

314
00:21:42,929 --> 00:21:45,959
With a perfect model or
a massive transformation.

315
00:21:46,679 --> 00:21:52,559
Start small, automate evidence
extraction, reduce redundant questions.

316
00:21:52,979 --> 00:21:57,739
Introduce signal driven
monitoring, but once risk teams

317
00:21:57,799 --> 00:22:03,169
stops chasing questionnaires,
they start anticipating risk.

318
00:22:03,669 --> 00:22:07,119
With that, I would like to conclude
thank you for your time and hope

319
00:22:07,119 --> 00:22:09,069
you all have a wonderful day ahead.

