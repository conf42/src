1
00:00:00,500 --> 00:00:05,239
Hello, and welcome to this talk on
the future of observability and what

2
00:00:05,259 --> 00:00:07,319
you can expect over the next 10 years.

3
00:00:07,819 --> 00:00:10,159
I'm Shamshree Wilson,
and I work at NetData.

4
00:00:10,659 --> 00:00:15,369
NetData is a real time observability
platform with a unique distributed

5
00:00:15,369 --> 00:00:18,109
architecture and an open source core.

6
00:00:19,029 --> 00:00:21,249
This talk isn't really
about NetData, the product.

7
00:00:22,074 --> 00:00:25,904
though I might, talk about a few of the
decisions we've made along the way and

8
00:00:25,904 --> 00:00:27,384
some of the principles that we follow.

9
00:00:27,764 --> 00:00:30,294
If you'd like to find out
more about NetData, you can

10
00:00:30,294 --> 00:00:33,054
check out our website at www.

11
00:00:33,114 --> 00:00:33,574
netdata.

12
00:00:33,844 --> 00:00:34,194
cloud.

13
00:00:34,694 --> 00:00:36,254
Now let's get started with the talk.

14
00:00:36,754 --> 00:00:40,574
Let's start by discussing a
little bit about the evolving

15
00:00:40,574 --> 00:00:42,854
definition of observability.

16
00:00:43,354 --> 00:00:46,224
What is observability
and what does it mean?

17
00:00:46,659 --> 00:00:49,199
and has this meaning
transformed over time.

18
00:00:49,699 --> 00:00:54,179
Now the traditional definition of
observability is that it is the

19
00:00:54,279 --> 00:00:59,159
ability to observe and understand
the internal state of a system by

20
00:00:59,159 --> 00:01:01,289
examining its external outputs.

21
00:01:01,789 --> 00:01:04,719
And when we talk about external
outputs, we usually mean

22
00:01:04,869 --> 00:01:06,789
metrics, logs, and traces today.

23
00:01:07,289 --> 00:01:11,624
In other words, observability
is to understand what is

24
00:01:11,634 --> 00:01:13,104
happening with the system.

25
00:01:13,604 --> 00:01:17,784
Now, this traditional definition
came out of control systems, and

26
00:01:17,834 --> 00:01:21,644
that explains some of the terminology
that you see here, when we talk about

27
00:01:21,644 --> 00:01:23,834
internal state, external outputs.

28
00:01:24,334 --> 00:01:28,404
But today, observability has,
has evolved over all these years.

29
00:01:29,274 --> 00:01:33,364
And it's a term that's primarily
used for people who are managing

30
00:01:33,424 --> 00:01:34,814
IT infrastructure these days.

31
00:01:35,314 --> 00:01:39,354
And IT infrastructure, as we all
know, has been growing ever complex.

32
00:01:39,894 --> 00:01:45,624
and it's, deployed across multiple
kinds of environments, and it

33
00:01:45,624 --> 00:01:49,764
scales up, it scales down, and
there's hundreds, thousands of

34
00:01:49,764 --> 00:01:51,114
services talking to each other.

35
00:01:51,445 --> 00:01:53,604
So it's, it is a very complex environment.

36
00:01:54,195 --> 00:01:59,124
So the current reality of observability
is that it's really the digital

37
00:01:59,124 --> 00:02:01,344
nervous system of modern enterprises.

38
00:02:01,354 --> 00:02:01,414
Thank you.

39
00:02:01,914 --> 00:02:06,684
So it's no longer just about logs,
metrics, and traces, but what we're

40
00:02:06,684 --> 00:02:11,454
trying to get out of observability is
to achieve real time comprehension, real

41
00:02:11,464 --> 00:02:16,544
time understanding of the complex digital
ecosystems through the lens of causality.

42
00:02:16,874 --> 00:02:21,484
And causality is an important term
here because what we expect out

43
00:02:21,484 --> 00:02:25,484
of observability today is not just
to understand what is happening,

44
00:02:26,084 --> 00:02:27,714
but also why it is happening.

45
00:02:28,494 --> 00:02:31,444
what to do about it, and
all of this in real time.

46
00:02:31,944 --> 00:02:36,914
So this is now the new evolved
definition of observability, if you may.

47
00:02:37,414 --> 00:02:42,004
So now that we've talked about
what observability is, let's just

48
00:02:42,474 --> 00:02:48,364
pause and think about what is the
current state of observability?

49
00:02:48,374 --> 00:02:50,794
What are the problems
with observability today?

50
00:02:51,294 --> 00:02:55,094
So, You can see that there's, there's
some problems that are called out

51
00:02:55,094 --> 00:02:58,694
here, and it's, it's been classified
into three different categories.

52
00:02:59,164 --> 00:03:02,994
Too little observability, too
complex observability, and

53
00:03:02,994 --> 00:03:04,564
too expensive observability.

54
00:03:04,804 --> 00:03:09,254
And as you can see by that interconnected
graph here, it, it's actually a mix

55
00:03:09,254 --> 00:03:13,534
of all of these things, and it kind of
bleeds into each different category.

56
00:03:14,034 --> 00:03:17,284
So let's, let's start by talking
about too little observability.

57
00:03:17,784 --> 00:03:18,714
What does this mean?

58
00:03:18,804 --> 00:03:24,684
So what this means is that the,
the, the system either doesn't

59
00:03:24,684 --> 00:03:26,304
have any observability at all.

60
00:03:26,454 --> 00:03:29,514
So there's nobody really
looking at what's going on.

61
00:03:30,034 --> 00:03:34,734
or there's very limited visibility,
which means that there's some

62
00:03:34,734 --> 00:03:36,804
monitoring of a few metrics.

63
00:03:36,944 --> 00:03:40,694
There might be a dashboard here or
there, but it's not really capturing

64
00:03:40,694 --> 00:03:42,344
the full state of the system.

65
00:03:43,064 --> 00:03:46,644
And this is a very dangerous situation
to be in because, you might have

66
00:03:46,644 --> 00:03:50,304
a false sense of security that
I have monitoring for my system.

67
00:03:50,854 --> 00:03:56,574
But you just have a very partial view, and
that partial view might be telling you a

68
00:03:56,574 --> 00:03:58,994
very different story from, from reality.

69
00:03:59,494 --> 00:04:01,224
So limited visibility is a problem.

70
00:04:01,764 --> 00:04:04,144
And, the other problem is late detection.

71
00:04:04,464 --> 00:04:09,284
So when we talk about too little,
observability, it also means that Maybe

72
00:04:09,284 --> 00:04:13,554
you have the metrics that you need, but
you don't have the sampling intervals

73
00:04:13,554 --> 00:04:17,914
that you need, which means that you're
collecting the data, but it's too late.

74
00:04:18,004 --> 00:04:24,174
So you're missing out on these short micro
spikes or bursts, or, you know, you're

75
00:04:24,174 --> 00:04:25,934
finding out about the issue too late.

76
00:04:26,294 --> 00:04:28,824
So late detection becomes
a big problem as well.

77
00:04:29,644 --> 00:04:33,854
performance bottlenecks either
directly due to the kind of

78
00:04:33,854 --> 00:04:35,524
observability that you've implemented.

79
00:04:35,879 --> 00:04:39,029
Or the fact that you're missing
out on identifying the performance

80
00:04:39,049 --> 00:04:40,759
bottlenecks becomes a problem too.

81
00:04:41,419 --> 00:04:45,409
and all of these things kind of
contribute to delayed troubleshooting.

82
00:04:45,629 --> 00:04:50,049
So you don't have the right tool
set to be able to understand and

83
00:04:50,059 --> 00:04:51,889
troubleshoot an issue in real time.

84
00:04:52,849 --> 00:04:57,769
And when this happens, somewhere down
the line, you're going to have outages.

85
00:04:58,399 --> 00:05:00,819
And outages are very, very expensive.

86
00:05:01,549 --> 00:05:04,289
And they're getting more
expensive, with each passing year.

87
00:05:04,789 --> 00:05:07,669
And all of these things, of course,
contribute to reduced engineering

88
00:05:07,669 --> 00:05:11,829
efficiency and, overheads when it
comes to maintenance and operations.

89
00:05:12,609 --> 00:05:16,209
So, clearly, too little
observability is not a situation

90
00:05:16,209 --> 00:05:17,919
that anybody really wants to be in.

91
00:05:18,419 --> 00:05:20,169
What's the second category here?

92
00:05:20,509 --> 00:05:26,169
So, now we're talking about when you
have observability, but it's too complex.

93
00:05:26,275 --> 00:05:29,735
and this is again, you know,
complex observability leads to

94
00:05:30,195 --> 00:05:32,005
many of the same sort of issues.

95
00:05:32,855 --> 00:05:36,125
Such as delayed troubleshooting,
the fact that you're not able to

96
00:05:36,125 --> 00:05:40,245
identify outages on time, reduced
engineering efficiency, and so on.

97
00:05:40,815 --> 00:05:46,885
it also has, different aspects that makes
it hard to scale, when you're, as your,

98
00:05:47,115 --> 00:05:49,209
as your system and as your team scales.

99
00:05:49,950 --> 00:05:53,050
And especially hard when you're
dealing with hybrid infrastructure

100
00:05:53,050 --> 00:05:54,240
and dynamic environments.

101
00:05:54,860 --> 00:05:58,440
let me give a little bit of an example
about what I mean when I say too

102
00:05:58,470 --> 00:06:01,940
complex observability, just so that
it's easier for you to understand.

103
00:06:02,440 --> 00:06:08,140
think about a case where, you're starting
off with, with a system, and it's,

104
00:06:08,160 --> 00:06:12,550
it's initially the MVP version of the
product, and you set up, observability.

105
00:06:13,065 --> 00:06:16,675
So maybe you start off with a
Prometheus and Grafana stack,

106
00:06:16,955 --> 00:06:18,155
and it's not complex, right?

107
00:06:18,175 --> 00:06:20,285
It was pretty easy for you to set it up.

108
00:06:20,955 --> 00:06:24,025
You're experienced in Prometheus
and Grafana, and you put

109
00:06:24,025 --> 00:06:25,175
everything up, it's fine.

110
00:06:25,995 --> 00:06:31,915
The problem happens when, as time
passes, you add more features, more

111
00:06:31,915 --> 00:06:35,655
services to your stack, there's
no different kinds of applications

112
00:06:35,665 --> 00:06:39,925
running in tandem, there's messaging
between them, Maybe your environment

113
00:06:39,935 --> 00:06:42,165
scales, you're not on prem anymore.

114
00:06:42,255 --> 00:06:45,725
You're hosted as part
of your system on AWS.

115
00:06:46,085 --> 00:06:47,645
there's some services on prem.

116
00:06:47,855 --> 00:06:50,705
Maybe there's other services
running on other clouds.

117
00:06:51,275 --> 00:06:53,665
And now you have a
complex system in place.

118
00:06:54,295 --> 00:06:59,615
And the question then arises,
were you able to keep up with your

119
00:06:59,615 --> 00:07:03,285
observability that you set up on
Prometheus and Grafana over time?

120
00:07:04,100 --> 00:07:07,020
and that's, that's becomes a
very interesting challenge.

121
00:07:07,200 --> 00:07:11,150
Because what started off as
simple may no longer be simple.

122
00:07:11,410 --> 00:07:15,680
because as you scale, there's more
things that you want to monitor.

123
00:07:15,900 --> 00:07:18,290
So every time you need to take
the decision, do I have the

124
00:07:18,290 --> 00:07:20,880
right, Prometheus exporters?

125
00:07:21,250 --> 00:07:22,940
am I getting all the right metrics?

126
00:07:23,375 --> 00:07:24,815
how should I visualize this?

127
00:07:24,935 --> 00:07:28,795
because spending a lot of time creating
custom dashboards is, not going to be a

128
00:07:28,795 --> 00:07:31,555
fun thing to do as your services scale.

129
00:07:32,115 --> 00:07:35,885
So you might have to rely on community
dashboards, but then you're not 100

130
00:07:35,885 --> 00:07:39,985
percent sure if the community dashboard
is capturing everything that you need.

131
00:07:40,975 --> 00:07:46,990
So it, it adds up over time into one
cognitive overload for you, and then

132
00:07:46,990 --> 00:07:50,050
there's, of course, going to be other
business related tasks that, that

133
00:07:50,050 --> 00:07:51,620
your team, need to spend time on.

134
00:07:51,840 --> 00:07:55,300
So they don't really have that
time to dedicate building custom

135
00:07:55,300 --> 00:08:00,590
dashboards and writing custom
queries on PromQL, or wherever.

136
00:08:00,970 --> 00:08:03,840
And over time, it becomes
really, really complex.

137
00:08:03,850 --> 00:08:06,830
So it starts off simple, but then
it becomes complex over time.

138
00:08:07,330 --> 00:08:10,550
And now finally, let's look at the
third category, which is too expensive.

139
00:08:10,835 --> 00:08:14,475
And this really kind of touches upon
all of these different issues, right?

140
00:08:14,895 --> 00:08:19,105
So we have these modern observability
platforms, whether we're talking

141
00:08:19,105 --> 00:08:22,545
about tools like Datadog, Dynatrace,
which are really powerful, right?

142
00:08:22,585 --> 00:08:24,135
They have all the features that you need.

143
00:08:24,395 --> 00:08:28,045
They're gathering all the right
metrics and logs and traces and

144
00:08:28,055 --> 00:08:31,675
have really neat visualizations
to kind of present them to you.

145
00:08:32,635 --> 00:08:33,995
But they're also really expensive.

146
00:08:34,715 --> 00:08:39,225
and while that might not be a problem for
large enterprises who have money to spare.

147
00:08:39,825 --> 00:08:45,005
it becomes problematic for smaller
enterprises and startups and companies

148
00:08:45,005 --> 00:08:48,875
that are starting out because they
don't have, that amount of, capital

149
00:08:48,925 --> 00:08:50,715
to invest into observability.

150
00:08:51,025 --> 00:08:52,695
And what do they do at that point, right?

151
00:08:52,695 --> 00:08:54,605
So what are the choices
that they're left with?

152
00:08:54,965 --> 00:08:58,105
Either they say that, you know, we're
not going to invest that much in

153
00:08:58,105 --> 00:09:02,635
observability, and, you know, we'll
see about it when it comes to it.

154
00:09:02,825 --> 00:09:05,945
And that, which means that they fall
back to too little observability.

155
00:09:06,445 --> 00:09:08,325
Which is a problematic scenario to be in.

156
00:09:09,085 --> 00:09:11,735
or they try to say, we'll do it ourselves.

157
00:09:12,015 --> 00:09:16,175
And that takes them to the second
category of too complex observability.

158
00:09:16,795 --> 00:09:20,055
So this, this becomes a
problem when we think about how

159
00:09:20,145 --> 00:09:22,295
observability can be democratized.

160
00:09:23,295 --> 00:09:25,575
And it also becomes a problem
when we think about the kind

161
00:09:25,575 --> 00:09:26,855
of architectures in place.

162
00:09:27,005 --> 00:09:32,050
So, later in this talk I'll, I'll
be speaking about centralized versus

163
00:09:32,080 --> 00:09:35,710
distributed or decentralized architectures
when it comes to observability.

164
00:09:36,100 --> 00:09:41,140
And why this is the, the crux
when it comes to why observability

165
00:09:41,140 --> 00:09:42,230
becomes too expensive.

166
00:09:42,730 --> 00:09:46,030
So this is a little bit about the
current state of observability.

167
00:09:46,930 --> 00:09:50,560
And now let's look at, let's
look at the future, right?

168
00:09:50,560 --> 00:09:52,110
Because that's what this talk is about.

169
00:09:52,610 --> 00:09:55,330
Let's look at what's headed
our way over the next decade.

170
00:09:55,830 --> 00:10:00,770
So I like to term the next 10 years, the
great infrastructure explosion, because

171
00:10:00,780 --> 00:10:05,060
that's, that's absolutely what we're going
to be able, what we're going to be seeing.

172
00:10:05,560 --> 00:10:10,160
there's, there is already, and there
will be an even bigger data center boom.

173
00:10:10,870 --> 00:10:15,990
so the global data center market,
which is around 500 billion, worth

174
00:10:16,160 --> 00:10:21,925
today in 2025 will grow to nearly 2
trillion by the end of the decade.

175
00:10:22,425 --> 00:10:24,645
and that, so that's, that's
a huge amount of growth.

176
00:10:24,765 --> 00:10:30,225
And considering what's happening with
AI and the AI arms race that we are

177
00:10:30,225 --> 00:10:34,685
in, with all the major, tech companies
really locked into, you know, the

178
00:10:34,685 --> 00:10:39,325
race towards, super intelligence, we,
you know, we might be underestimating

179
00:10:39,325 --> 00:10:40,585
it when we say 2 trillion.

180
00:10:40,715 --> 00:10:43,175
the data center market might
actually be much larger.

181
00:10:43,675 --> 00:10:49,675
And a different way to look at this is
that the energy consumption that data

182
00:10:49,675 --> 00:10:55,345
centers globally would be consuming,
even at a conservative estimate by

183
00:10:55,415 --> 00:11:01,995
2035 would be 8 percent of the world's
total electricity, which means that a

184
00:11:01,995 --> 00:11:07,770
large part of human existence and our
world in general is going to be built

185
00:11:07,770 --> 00:11:12,340
around supporting, these data centers
where we are cultivating intelligence.

186
00:11:12,840 --> 00:11:16,320
Compute capacity has been growing
at two and a half times faster

187
00:11:16,320 --> 00:11:17,610
than Moore's Law recently.

188
00:11:18,110 --> 00:11:23,885
And all of this means that there's going
to be A lot more data centers running a

189
00:11:23,885 --> 00:11:26,175
lot more different kinds of workloads.

190
00:11:26,605 --> 00:11:30,665
there's going to be a lot more IT
infrastructure which needs to underpin

191
00:11:30,675 --> 00:11:38,145
all of this and Eventually, it means
that we would need better and more

192
00:11:39,085 --> 00:11:42,385
observability So that the teams
who are maintaining and managing

193
00:11:42,385 --> 00:11:44,145
all of this and can keep it running

194
00:11:44,645 --> 00:11:50,630
That brings us to the human
factor So DevOps and SRE jobs are

195
00:11:50,740 --> 00:11:55,120
actually growing, multiple times
faster than traditional IT jobs.

196
00:11:55,620 --> 00:12:00,010
Now I know that, you know, there's been a
lot of talk about what's going to happen

197
00:12:00,010 --> 00:12:02,700
with AI and people's jobs in general.

198
00:12:03,110 --> 00:12:06,180
And of course, AI will
affect the jobs market.

199
00:12:06,610 --> 00:12:11,360
There are certain jobs or certain
roles that people are doing today

200
00:12:12,040 --> 00:12:13,330
that will get automated away.

201
00:12:13,755 --> 00:12:16,855
Or at least parts of that
job will get automated away.

202
00:12:17,535 --> 00:12:23,565
but at least what, what I believe
is going to happen is that the,

203
00:12:23,575 --> 00:12:25,705
the amount of work will increase.

204
00:12:26,205 --> 00:12:32,665
So the average organization is going to
be managing five times more services.

205
00:12:33,215 --> 00:12:35,305
by 2030, that's, that's in five years.

206
00:12:35,805 --> 00:12:40,225
which means that there's going to be a
multiple fold increase in infrastructure

207
00:12:40,225 --> 00:12:45,255
teams needed overall, which means that the
amount of total jobs available is going to

208
00:12:45,285 --> 00:12:47,708
increase, especially for DevOps and SRE.

209
00:12:47,708 --> 00:12:50,245
And this is directly related
to the data center boom.

210
00:12:51,045 --> 00:12:54,545
You're going to have huge number
of data centers, with, you know,

211
00:12:54,575 --> 00:12:56,855
increasing complexity of workloads.

212
00:12:57,675 --> 00:13:01,325
And you're going to have, you're
going to need teams to manage this.

213
00:13:02,185 --> 00:13:02,685
Now, of course.

214
00:13:03,395 --> 00:13:05,245
You know, AI is going to be a factor here.

215
00:13:05,845 --> 00:13:12,245
so the, the kind of work, that your
average SRE does is going to be different.

216
00:13:12,855 --> 00:13:14,695
But you're still going to
need a lot more of them.

217
00:13:15,195 --> 00:13:18,585
And a combination of these
things means that observability

218
00:13:18,605 --> 00:13:20,035
is mission critical now.

219
00:13:20,535 --> 00:13:24,785
The average cost of downtime
is more than 9, 000 a minute.

220
00:13:25,285 --> 00:13:28,795
And you know this number is from a few
years ago I'm sure that it's grown to

221
00:13:28,805 --> 00:13:34,555
even higher now by now and it keeps
growing up every year so Downtime is

222
00:13:34,575 --> 00:13:39,185
something that you want to minimize
to the most infinitesimal possible

223
00:13:39,355 --> 00:13:44,400
value as much as you can Unobserved
or, you know, partially understood or

224
00:13:44,400 --> 00:13:49,560
misunderstood failures can cascade today
across thousands of different services,

225
00:13:49,580 --> 00:13:55,400
distributed systems, nodes, multiple
clouds, which makes it a lot harder

226
00:13:55,490 --> 00:13:59,330
when you don't have the visibility to
understand what really happened, right?

227
00:13:59,680 --> 00:14:01,720
Because you need to understand
what happened so that you can

228
00:14:01,720 --> 00:14:03,100
prevent it from happening again.

229
00:14:03,600 --> 00:14:09,860
And all of this means that good
observability is survival critical and

230
00:14:09,860 --> 00:14:11,480
it's not just a nice to have anymore.

231
00:14:11,980 --> 00:14:15,460
So now that we've talked a little
bit about what's headed our way

232
00:14:15,460 --> 00:14:18,990
in the next 10 years, and here
are some references as well.

233
00:14:19,060 --> 00:14:22,370
I know that the text is kind of
small, but, you can download the

234
00:14:22,370 --> 00:14:25,080
slides afterwards and you can look
at where these numbers come from

235
00:14:25,580 --> 00:14:27,280
that I was quoting in the stats here.

236
00:14:27,780 --> 00:14:29,140
So let's, let's talk about the stats.

237
00:14:29,620 --> 00:14:32,430
what the observability
equation really is, right?

238
00:14:32,950 --> 00:14:36,370
And, and this is the, the
traditional observability equation.

239
00:14:36,380 --> 00:14:39,920
You might have seen it, heard it
in other, in other talks and other

240
00:14:40,320 --> 00:14:41,730
sessions and books and so on.

241
00:14:42,000 --> 00:14:45,280
That observability is made up
of metrics, logs, and traces.

242
00:14:45,780 --> 00:14:49,810
Now, I think where we are today and
where we're headed in the next ten years,

243
00:14:50,110 --> 00:14:52,090
we need a new observability equation.

244
00:14:53,040 --> 00:14:55,020
And it's not just about
metrics, logs, and traces.

245
00:14:55,425 --> 00:14:59,785
So the way that I see this is the new
observability equation is made up of

246
00:14:59,785 --> 00:15:04,815
three things, but those three things
are data, context, and intelligence.

247
00:15:05,315 --> 00:15:10,435
So data is all of the data,
including metrics, logs, traces,

248
00:15:10,465 --> 00:15:12,615
any, any sort of raw system output.

249
00:15:13,255 --> 00:15:17,145
And they form the foundational
layer of observability.

250
00:15:17,145 --> 00:15:19,565
But observability is
just not limited to data.

251
00:15:19,955 --> 00:15:22,875
Observability isn't just
metrics, logs, and traces, right?

252
00:15:22,975 --> 00:15:26,065
It just forms a foundational
layer on which the new

253
00:15:26,075 --> 00:15:27,445
observability equation is built.

254
00:15:27,945 --> 00:15:30,475
The second part of this
equation is context.

255
00:15:30,975 --> 00:15:33,375
So context is really important.

256
00:15:33,465 --> 00:15:37,735
Without context, your data,
even though you have it, might

257
00:15:37,735 --> 00:15:39,075
be telling you a wrong story.

258
00:15:39,565 --> 00:15:41,585
It might not be telling
you a story at all.

259
00:15:41,975 --> 00:15:45,985
So it's not really providing the
value that it should without context.

260
00:15:46,485 --> 00:15:49,025
So, what do I mean by context?

261
00:15:50,005 --> 00:15:52,095
Context really is a rich metadata.

262
00:15:52,395 --> 00:15:56,295
So it's, it's information about
the data that explains the data.

263
00:15:56,995 --> 00:16:01,195
For example, it might be an anomaly
pattern which says, you know, here's

264
00:16:01,485 --> 00:16:04,665
your, the value for your, CPU percentage.

265
00:16:05,150 --> 00:16:09,940
but here's additional metadata which
tells if this is normal or not for

266
00:16:09,950 --> 00:16:13,490
this system, for this particular
component within this system.

267
00:16:14,180 --> 00:16:20,050
So that piece of extra metadata
now tells you that, you know, it

268
00:16:20,060 --> 00:16:21,470
tells you a better story, right?

269
00:16:21,490 --> 00:16:26,950
So you have a value, CPU is at 90%,
but maybe the, the anomaly pattern

270
00:16:26,950 --> 00:16:31,550
tells you that CPU for this component
on this system is always at 90%.

271
00:16:31,685 --> 00:16:36,385
So that's a very different thing from
just telling you that the CPU is 90%.

272
00:16:36,385 --> 00:16:39,245
you can, you know, anomaly
patterns is just one example.

273
00:16:39,255 --> 00:16:41,555
You could have, correlation information.

274
00:16:41,675 --> 00:16:45,935
So when you have information about a
single metric, you also have correlation,

275
00:16:45,935 --> 00:16:51,215
which says that here's what happened
with other metrics or other logs, while

276
00:16:51,245 --> 00:16:53,135
this metric was exhibiting this value.

277
00:16:53,635 --> 00:16:58,220
So this is You know, at NADATA,
we really believe that context is

278
00:16:58,220 --> 00:17:02,520
important, and, and we try to make
our, our tool, our product, more

279
00:17:02,520 --> 00:17:06,250
opinionated, so that it's bringing you
these opinions about what you're seeing.

280
00:17:06,560 --> 00:17:09,120
It's not just showing you data,
but it's also trying to tell

281
00:17:09,120 --> 00:17:10,890
you a story about the data.

282
00:17:11,390 --> 00:17:14,080
I'm not saying that, you know,
we're, we've solved, the, the

283
00:17:14,080 --> 00:17:17,290
equation when it comes to context,
there's a lot more that we can do.

284
00:17:17,290 --> 00:17:21,560
And there's a lot more that the industry
in general should be doing, about making,

285
00:17:21,630 --> 00:17:27,120
metadata a first class citizen, because
there's, there's a lot of, important

286
00:17:27,120 --> 00:17:31,650
information when it comes to, you
know, what, what the business KPIs are

287
00:17:31,690 --> 00:17:33,370
that your system is trying to achieve.

288
00:17:33,790 --> 00:17:38,260
what's the real world events that are
happening that are related to what your

289
00:17:38,270 --> 00:17:40,190
observability data is trying to tell you.

290
00:17:40,520 --> 00:17:46,186
And bringing those things together, will
provide, you know, exponential value.

291
00:17:46,186 --> 00:17:52,190
Now, the, the third component to the new
observability equation, is intelligence.

292
00:17:52,800 --> 00:17:54,700
And yes, I'm talking about AI.

293
00:17:55,180 --> 00:18:00,790
because up until now, intelligence
was solely provided by humans.

294
00:18:00,840 --> 00:18:04,770
And we were limited by, the amount
of time, the amount of people

295
00:18:05,040 --> 00:18:08,520
that, you know, could be working
on this at one point in time.

296
00:18:09,210 --> 00:18:13,790
but, With the, the capability
increases in AI that we've been seeing

297
00:18:13,790 --> 00:18:18,310
recently, it means that we have this
layer of intelligence that we can

298
00:18:18,310 --> 00:18:21,110
start applying to data and context.

299
00:18:21,780 --> 00:18:26,140
And context becomes ever more
important because, humans, And the

300
00:18:26,140 --> 00:18:29,080
human teams had experience to rely on.

301
00:18:29,450 --> 00:18:33,750
They had a better inherent understanding
of context when it came to the data.

302
00:18:34,300 --> 00:18:39,830
For example, you know, a team of SREs
or, or DevOps engineers would already

303
00:18:39,830 --> 00:18:43,890
know if, if they were working on an
e commerce platform, that there would

304
00:18:43,900 --> 00:18:45,610
be certain trends when it came to.

305
00:18:46,005 --> 00:18:48,355
a particular sale that the
company was running, right?

306
00:18:48,395 --> 00:18:51,145
It might be a Black Friday
sale, it might be something else

307
00:18:51,145 --> 00:18:53,725
which is region specific, but
they would already know that.

308
00:18:54,415 --> 00:18:59,785
whereas if you're, if you're relying
on an AI, platform or an AI agent

309
00:18:59,825 --> 00:19:03,795
to do this, maybe the AI agent knows
this as well, but maybe it doesn't.

310
00:19:03,865 --> 00:19:07,445
maybe it's, been biased based
on the training data, so it's

311
00:19:07,455 --> 00:19:09,185
not really expecting that this.

312
00:19:09,625 --> 00:19:12,905
this context, exists
for, for your use case.

313
00:19:13,735 --> 00:19:19,245
So to be able to provide that context
as metadata means that, the, the, the

314
00:19:19,305 --> 00:19:23,165
AI intelligence that you're relying
on becomes a lot more capable as well.

315
00:19:23,665 --> 00:19:29,035
And we are going to be, seeing a lot
more, from autonomous agents, and

316
00:19:29,035 --> 00:19:31,105
predictive modeling when it comes to AI.

317
00:19:31,365 --> 00:19:33,035
you know, we'll, we'll speak
more about this as well.

318
00:19:33,535 --> 00:19:38,715
But this new equation, what this
does is that, you know, you, you, you

319
00:19:38,715 --> 00:19:40,695
have data as your foundational layer.

320
00:19:41,115 --> 00:19:46,465
You add a rich metadata in the form of
context on top, and then you have this

321
00:19:46,565 --> 00:19:51,555
intelligence layer that can try to kind
of combine this, stir the stew, if you

322
00:19:51,555 --> 00:19:55,465
will, and really bring out of it insights.

323
00:19:55,915 --> 00:19:57,575
And that's, that's the ideal, if you will.

324
00:19:58,020 --> 00:20:02,340
end product or output of the
observability equation is the

325
00:20:02,340 --> 00:20:04,940
fact that you get true insights.

326
00:20:05,110 --> 00:20:09,490
So you're getting true insights into your
infrastructure, into your system about

327
00:20:09,670 --> 00:20:14,930
what's, you know, what's going on, why
is it going on, what, what you need to

328
00:20:14,930 --> 00:20:16,660
do about it if you need to do about it.

329
00:20:17,280 --> 00:20:19,590
And you, all of this is
happening in real time.

330
00:20:20,140 --> 00:20:23,460
So this is the ideal new
observability equation that I

331
00:20:23,490 --> 00:20:25,520
think, both companies and teams use.

332
00:20:26,250 --> 00:20:27,150
should be talking about.

333
00:20:27,650 --> 00:20:30,960
Now let's dive a little bit
deeper into each of these three

334
00:20:30,960 --> 00:20:33,210
different parts of the equation.

335
00:20:33,710 --> 00:20:35,030
Starting with data, right?

336
00:20:35,090 --> 00:20:36,530
So starting with the foundation.

337
00:20:37,490 --> 00:20:41,440
And I really want to stress on the
importance of high fidelity data.

338
00:20:41,940 --> 00:20:43,490
So what does high fidelity mean?

339
00:20:44,110 --> 00:20:48,500
High fidelity means that you,
you have all of the data.

340
00:20:48,530 --> 00:20:50,750
So you're getting all of
the different metrics.

341
00:20:50,770 --> 00:20:55,650
You're not missing out on any of the
metrics, but you're also getting as

342
00:20:55,650 --> 00:20:59,340
many samples per metric that you can.

343
00:20:59,840 --> 00:21:04,540
And more is better in this case,
because higher resolution, which

344
00:21:04,540 --> 00:21:08,990
means that you're, for example,
if you're capturing data for a

345
00:21:08,990 --> 00:21:14,270
particular metric every single second,
that's high resolution compared to.

346
00:21:14,620 --> 00:21:18,650
capturing data every 10 seconds
or every 60 seconds, or in some

347
00:21:18,650 --> 00:21:22,300
cases, you know, there are tools
out there which only capture data

348
00:21:22,350 --> 00:21:24,620
every five minutes, every 10 minutes.

349
00:21:25,300 --> 00:21:29,240
and that, you know, it does
give you, you know, you could

350
00:21:29,330 --> 00:21:30,710
still plot a chart out of it.

351
00:21:30,720 --> 00:21:34,830
You can put it on a dashboard, but that
number, if you're gathering it every,

352
00:21:34,900 --> 00:21:39,210
every minute or every five minutes is
very different from, you know, the, the

353
00:21:39,210 --> 00:21:41,140
story that you can tell if you're getting.

354
00:21:41,555 --> 00:21:43,815
per second information
for that same metric.

355
00:21:44,605 --> 00:21:47,965
You get way better insights into
what's really happening with

356
00:21:47,965 --> 00:21:51,135
that metric, with your system,
when you get a high resolution.

357
00:21:51,635 --> 00:21:56,295
So, you know, every time that you increase
your sampling intervals, you're, you're

358
00:21:56,305 --> 00:21:59,265
missing out on unknown unknowns, right?

359
00:21:59,765 --> 00:22:04,065
And this, this is a, this is
a really hidden issue because

360
00:22:04,185 --> 00:22:05,785
you don't see what's going on.

361
00:22:06,165 --> 00:22:07,965
So you don't know if
it's a problem or not.

362
00:22:08,905 --> 00:22:11,675
So it, this is why we call
it an unknown unknown.

363
00:22:12,175 --> 00:22:17,335
so if you really want to future proof
your troubleshooting and, future proof

364
00:22:17,335 --> 00:22:22,595
your observability stack overall, then
you should be waiting towards, you know,

365
00:22:22,645 --> 00:22:25,935
leaning towards higher resolution data.

366
00:22:26,665 --> 00:22:30,455
The more data that you can have,
you know, the, the lower sampling

367
00:22:30,455 --> 00:22:32,045
intervals that you have, the better.

368
00:22:32,545 --> 00:22:37,065
And of course, a side effect of
this is that if you're relying on

369
00:22:37,065 --> 00:22:40,905
machine learning, if you're relying
on artificial intelligence, then,

370
00:22:41,225 --> 00:22:46,615
you know, a higher data density is
always better because it allows the AI

371
00:22:46,615 --> 00:22:48,965
systems to learn the right patterns.

372
00:22:49,385 --> 00:22:54,985
If you provide a machine learning
algorithm or an AI model with sparse

373
00:22:55,015 --> 00:22:59,900
data, then, you it finds it harder to
learn the right patterns, or sometimes

374
00:22:59,900 --> 00:23:01,530
it might arrive at the wrong patterns.

375
00:23:02,090 --> 00:23:05,200
Give it the, you know, the more
data that you give it and the more

376
00:23:05,230 --> 00:23:08,450
context that you give it, it can
arrive at the right conclusions.

377
00:23:08,950 --> 00:23:10,500
So what are the new rules of data?

378
00:23:11,000 --> 00:23:13,350
Rule number one is collect everything.

379
00:23:13,580 --> 00:23:15,660
You can filter out the stuff
that you don't need later.

380
00:23:16,590 --> 00:23:20,270
Rule number two, you know, the
better the sampling frequency,

381
00:23:20,430 --> 00:23:21,720
the better the outcome.

382
00:23:21,720 --> 00:23:22,680
Okay.

383
00:23:23,085 --> 00:23:25,575
Rule number three is don't
have artificial limits.

384
00:23:25,855 --> 00:23:30,505
So don't have a limit which says
that you can only collect x number

385
00:23:30,505 --> 00:23:36,945
of samples per y interval and you can
only store it for, you know, x days.

386
00:23:37,275 --> 00:23:40,715
If that limit is artificial, if that
limit is something that, you know,

387
00:23:40,725 --> 00:23:44,605
a tool is introducing but it's not
natural to your system, to your

388
00:23:44,605 --> 00:23:47,075
workflow, don't settle for that limit.

389
00:23:47,575 --> 00:23:51,805
And finally, In an ideal world, you want
to store everything indefinitely, right?

390
00:23:52,175 --> 00:23:56,075
Maybe you need to archive it,
maybe you need to digest the data.

391
00:23:56,525 --> 00:24:00,215
But still, it's available there
if you want to go back in time.

392
00:24:00,865 --> 00:24:03,425
Because very often what happens
when you're troubleshooting an

393
00:24:03,425 --> 00:24:07,765
issue is that you want to know
what happened when similar issues

394
00:24:07,975 --> 00:24:09,395
like this happened in the past.

395
00:24:09,845 --> 00:24:12,125
And you need to be able to
go back and look at data.

396
00:24:12,175 --> 00:24:14,945
which means that you want to
have, you know, some form of

397
00:24:14,945 --> 00:24:16,075
that data available to you.

398
00:24:16,575 --> 00:24:21,305
So if, if you're designing your
observability stack today, you know, I

399
00:24:21,305 --> 00:24:25,675
would play very, I would pay very close
attention to these new rules of data

400
00:24:26,175 --> 00:24:31,105
because the real impact that it can have,
on your team and on the work that you

401
00:24:31,105 --> 00:24:35,865
do is that you can have significantly
faster mean timed resolution.

402
00:24:36,360 --> 00:24:38,380
when you have access
to high fidelity data.

403
00:24:38,940 --> 00:24:42,590
it allows you to catch micro
anomalies before they cascade

404
00:24:42,680 --> 00:24:45,700
and start creating problems in
different parts of your system.

405
00:24:46,420 --> 00:24:52,260
And, the, the high fidelity data also
makes, learning historical patterns to

406
00:24:52,260 --> 00:24:56,850
reveal future issues, a much more exact
science, than it otherwise would be.

407
00:24:57,350 --> 00:25:01,210
So, to sum up, you know, the difference
between low fidelity and high fidelity

408
00:25:01,240 --> 00:25:06,150
is that You know, you're dealing with,
real time, one second samples versus

409
00:25:06,180 --> 00:25:10,130
dealing with ten seconds to sixty seconds
or even worse, sampling intervals.

410
00:25:10,870 --> 00:25:15,260
you're, you know, you have the
option to gather any number

411
00:25:15,260 --> 00:25:16,765
of metrics that you want to.

412
00:25:17,065 --> 00:25:19,695
So, you know, in, in
theory, it's unlimited.

413
00:25:19,755 --> 00:25:22,235
You can get anything and
everything that you need.

414
00:25:22,555 --> 00:25:26,255
So you shouldn't be metered by how
many metrics you're collecting, because

415
00:25:26,275 --> 00:25:30,545
anytime, any of these things, have a
metering in terms of the price, then it

416
00:25:30,545 --> 00:25:34,215
becomes a decision for the team to take
that, you know, we can't get everything

417
00:25:34,215 --> 00:25:38,015
that we need, we're going to get charged
by the number of metrics or the amount

418
00:25:38,055 --> 00:25:43,015
or the usage of metrics or logs, for
example, then you have to decide, okay,

419
00:25:43,045 --> 00:25:44,565
you know, I need to take a decision.

420
00:25:44,615 --> 00:25:47,575
I'm opting in for only
this subset of metrics.

421
00:25:47,695 --> 00:25:51,265
so you, you, you become, you get
stuck in a partial metric situation.

422
00:25:51,275 --> 00:25:52,335
You don't want to be there.

423
00:25:52,655 --> 00:25:55,855
you, you want to have full freedom on
getting all the data that you need.

424
00:25:56,855 --> 00:25:58,935
And a similar situation
with retention as well.

425
00:25:59,505 --> 00:26:03,285
So in, in an ideal world, you
want to be able to retain.

426
00:26:04,050 --> 00:26:08,180
the data up to what is
naturally, meaningful for your

427
00:26:08,200 --> 00:26:09,410
workload, for your system.

428
00:26:09,920 --> 00:26:13,240
You don't want an artificial
limitation that's being decided

429
00:26:13,240 --> 00:26:17,770
by, by a vendor or a tool, that is
not natural to what you're doing.

430
00:26:18,270 --> 00:26:22,510
So we talked about data, high fidelity
data, which, which is the foundation

431
00:26:22,920 --> 00:26:24,810
of the new observability equation.

432
00:26:25,620 --> 00:26:26,910
let's talk about, metadata.

433
00:26:27,410 --> 00:26:28,440
Rich metadata.

434
00:26:29,125 --> 00:26:32,195
can be thought of as
context at scale, right?

435
00:26:32,255 --> 00:26:34,905
So you're getting context from
different parts of your system

436
00:26:35,215 --> 00:26:38,025
for different kinds of metrics,
and you're getting it at scale.

437
00:26:38,485 --> 00:26:41,375
And, and this really, you know,
is where the magic happens.

438
00:26:41,875 --> 00:26:45,535
So this, this could be all kinds
of different information, right?

439
00:26:45,535 --> 00:26:49,845
So you could have business context in
here, which, which talks about, you know,

440
00:26:49,845 --> 00:26:51,695
what, what is the revenue impact to you?

441
00:26:51,995 --> 00:26:55,955
what are the, the touch points in the
customer journey or the user journey

442
00:26:55,995 --> 00:26:57,845
while the user is using the system?

443
00:26:58,620 --> 00:27:02,930
you can have context around the
business transaction flow and also

444
00:27:02,930 --> 00:27:06,560
SLA, SLO implications because that
matters a lot for, for certain

445
00:27:06,560 --> 00:27:07,750
products and certain teams.

446
00:27:08,250 --> 00:27:10,120
Then there's the operational context.

447
00:27:10,320 --> 00:27:14,580
so service dependencies, changes in the
infrastructure that are being rolled out.

448
00:27:15,110 --> 00:27:16,270
deployment events.

449
00:27:16,470 --> 00:27:21,060
So when, when a new build is being
deployed, you need all of that information

450
00:27:21,060 --> 00:27:23,070
to, again, tell a better story, right?

451
00:27:23,640 --> 00:27:27,120
and it's not even if, you know, every
time it doesn't have to be about a

452
00:27:27,120 --> 00:27:31,050
major change in the infrastructure
or, or a new build being deployed.

453
00:27:31,220 --> 00:27:33,760
It could be a minor
configuration update as well.

454
00:27:33,900 --> 00:27:37,750
But that information is still critical
because without it, some of the other

455
00:27:37,750 --> 00:27:39,640
data might not tell you the full story.

456
00:27:40,140 --> 00:27:44,400
And then you have environmental
context, which is, real world events.

457
00:27:44,520 --> 00:27:48,370
So what's happening in the world
today that might be influencing

458
00:27:48,370 --> 00:27:49,960
what your system is experiencing.

459
00:27:50,330 --> 00:27:54,140
There could be regional impacts
due to outages that are not related

460
00:27:54,140 --> 00:27:58,040
to your product, but maybe AWS
is down and that impacts you.

461
00:27:58,730 --> 00:28:01,595
Or maybe there's another third
party service, that does that.

462
00:28:02,255 --> 00:28:06,085
You know, that, that's announcing
its status on their status page,

463
00:28:06,245 --> 00:28:07,725
which is accessible publicly.

464
00:28:08,135 --> 00:28:12,455
But that story isn't being told as
part of your observability story.

465
00:28:12,815 --> 00:28:15,985
And it just adds extra work
for, for somebody to go and

466
00:28:15,985 --> 00:28:17,435
manually connect the dots.

467
00:28:17,895 --> 00:28:20,875
In an ideal world, those things
would happen automatically.

468
00:28:21,375 --> 00:28:25,135
So there's all of these different
various market conditions which

469
00:28:25,135 --> 00:28:27,925
can, which can influence what
your system is experiencing.

470
00:28:28,005 --> 00:28:33,425
And all of that, could become context that
your observability, equation could use.

471
00:28:33,925 --> 00:28:37,815
And then there's the opinionated insights
that I was talking about earlier.

472
00:28:37,895 --> 00:28:42,745
so this is an area where we've,
started, trying to, get some of

473
00:28:42,745 --> 00:28:47,215
this done as part of the NetData
product, including, anomaly detection.

474
00:28:47,235 --> 00:28:50,505
So when, when we do anomaly
detection at NetData, we don't

475
00:28:50,505 --> 00:28:51,985
do it as an optional thing.

476
00:28:52,165 --> 00:28:52,725
We do it for.

477
00:28:53,145 --> 00:28:54,795
every single metric that we collect.

478
00:28:55,055 --> 00:28:59,045
It means that every metric comes
with this extra piece of metadata

479
00:28:59,345 --> 00:29:03,185
that tells, you know, is what
you're seeing anomalous or not.

480
00:29:03,685 --> 00:29:08,365
Similarly, there's the ability to, we ha
NetData has a scoring engine, which allows

481
00:29:08,365 --> 00:29:13,405
you to do, correlation of what's happening
at any time interval of your choice.

482
00:29:13,585 --> 00:29:17,875
So you understand which metrics are
behaving in a correlated fashion

483
00:29:17,875 --> 00:29:19,035
at that point in time or not.

484
00:29:19,945 --> 00:29:23,205
But of course, there's a lot more that
we could do in terms of, you know,

485
00:29:23,205 --> 00:29:24,915
root cost ranking and things like that.

486
00:29:25,415 --> 00:29:29,205
So what all of this means is that
when you have rich metadata, when

487
00:29:29,205 --> 00:29:34,205
you have the right context, you
can tell a much more meaningful,

488
00:29:34,795 --> 00:29:37,235
interesting, and useful story, right?

489
00:29:37,745 --> 00:29:42,235
So instead of just saying that
triggering a raw alert, saying that

490
00:29:42,245 --> 00:29:46,015
you have, you know, CPU has hit 90
percent because it's hit some threshold.

491
00:29:46,960 --> 00:29:50,850
When you have the right context, you
could actually tell a story which

492
00:29:50,850 --> 00:29:55,760
says, you know, this particular revenue
critical service is experiencing

493
00:29:55,770 --> 00:29:58,300
never before seen slowdown, right?

494
00:29:58,610 --> 00:30:03,440
So it's, it's an anomalous slowdown
during peak shopping hour because

495
00:30:03,440 --> 00:30:04,770
we have that context as well.

496
00:30:05,280 --> 00:30:09,850
And we can have the context which says
that this is likely happening due to

497
00:30:09,850 --> 00:30:12,980
a recent deployment because you have
the time when the deployment happened.

498
00:30:12,980 --> 00:30:14,985
And you can see when
the slowdown occurred.

499
00:30:15,465 --> 00:30:16,945
started after that, right?

500
00:30:17,445 --> 00:30:21,645
So just, just look at the difference
between these two things, and you

501
00:30:21,645 --> 00:30:25,625
will understand, you know, where
observability needs to be for you.

502
00:30:26,125 --> 00:30:30,205
And now, you know, the third, spoke in the
wheel, of the new observability equation,

503
00:30:30,815 --> 00:30:32,895
the giant pink elephant in the room, AI.

504
00:30:33,395 --> 00:30:37,405
So, I like to think of AI as
the, the force multiplier.

505
00:30:37,905 --> 00:30:42,845
And What's going to happen with
AI in the next 10 years is, it's

506
00:30:42,855 --> 00:30:44,845
really anybody's guess almost.

507
00:30:45,665 --> 00:30:51,015
because the, the, the capabilities of
the models that are available today,

508
00:30:51,375 --> 00:30:56,635
especially with the success of the
transformer architecture and, the

509
00:30:56,645 --> 00:30:58,815
takeoff of large language models.

510
00:30:59,800 --> 00:31:04,020
that that has happened after Chad GPT was
launched and the world really got their

511
00:31:04,020 --> 00:31:10,110
taste of, you know, what these models are
capable of, has been really something.

512
00:31:10,510 --> 00:31:14,450
The models are almost doubling
in capability every six months.

513
00:31:14,950 --> 00:31:19,210
So it's very hard to predict what
exactly will happen, but there are trends

514
00:31:19,240 --> 00:31:23,870
that we can see and there's, there's,
there's things that we can expect that

515
00:31:23,880 --> 00:31:25,490
will happen over the next 10 years.

516
00:31:25,990 --> 00:31:30,780
Specialized AI agents is something
that it's already starting to happen.

517
00:31:30,820 --> 00:31:33,930
And I, I see this as one area
where we're going to see a lot more

518
00:31:33,930 --> 00:31:35,370
traction over the next few years.

519
00:31:35,870 --> 00:31:38,990
And there will be, agents that
are optimized for certain things.

520
00:31:39,140 --> 00:31:42,830
we are already seeing models that are
optimized for reasoning, so that they

521
00:31:42,870 --> 00:31:46,100
take some time to think through the
answer before giving you a response.

522
00:31:46,560 --> 00:31:52,100
So reasoning agents will become very,
key for, for causal analysis across,

523
00:31:52,190 --> 00:31:54,640
services and across a large system.

524
00:31:55,140 --> 00:31:58,840
we will also have memory augmented
agents for historical pattern matching

525
00:31:58,970 --> 00:32:01,890
because the, you know, when we're
dealing with observability data.

526
00:32:02,455 --> 00:32:05,135
And all of you know this,
there's a huge amount of data.

527
00:32:05,895 --> 00:32:08,525
so it's not like you can take all
of the data and fill it into the

528
00:32:08,525 --> 00:32:10,105
context of a large language model.

529
00:32:10,105 --> 00:32:15,395
In most cases, it's either not possible
or it's prohibitively expensive today.

530
00:32:15,945 --> 00:32:18,115
Of course, the cost is going
to come down in the future.

531
00:32:18,625 --> 00:32:23,225
but there will be different, different
models of how, memory will be

532
00:32:23,265 --> 00:32:25,555
handled by, by models in the future.

533
00:32:26,055 --> 00:32:30,495
This is, this is one area, memory is one
area where today's models are lacking.

534
00:32:30,995 --> 00:32:35,235
There will also be planning agents,
that, that will, You know, sit or embed

535
00:32:35,295 --> 00:32:40,075
in your observability stack and, and
are looking at, you know, achieving

536
00:32:40,075 --> 00:32:44,825
specific goals or targets, whether
it's capacity optimization, capacity

537
00:32:44,825 --> 00:32:48,895
planning, or cost management, the
planning agents should be, you know,

538
00:32:48,905 --> 00:32:50,515
will be able to run independently.

539
00:32:50,765 --> 00:32:51,730
And they're always there.

540
00:32:51,730 --> 00:32:55,585
They're always looking through the
data, always looking for ways in

541
00:32:55,585 --> 00:32:59,535
which you can optimize your costs
or, or plan your capacity better.

542
00:33:00,035 --> 00:33:02,945
And, and they're, they're going through
the data, they're going through the

543
00:33:02,945 --> 00:33:06,175
context and they're bringing back
insights to you that, you know, here's

544
00:33:06,175 --> 00:33:07,355
something that, that could be done.

545
00:33:07,635 --> 00:33:10,705
And maybe there's things that can be
automated as well when it comes to,

546
00:33:10,725 --> 00:33:12,405
automatic scaling and things like that.

547
00:33:12,905 --> 00:33:18,025
and, and the final point here, is
about autonomous, RAG enabled agents.

548
00:33:18,695 --> 00:33:22,885
RAG stands for retrieval augmented
generation, which means that you can

549
00:33:22,885 --> 00:33:27,445
provide, for example, documentation
information, you can provide access

550
00:33:27,905 --> 00:33:33,745
to your JIRA, or to your incident
management system, and the agent,

551
00:33:33,835 --> 00:33:37,285
the AI agent, would then be able
to look at all this information

552
00:33:37,785 --> 00:33:41,535
based on a search for relevancy.

553
00:33:42,295 --> 00:33:46,450
And You know, based on that, it can
provide you either here's the steps that

554
00:33:46,450 --> 00:33:51,610
you need to do, or in some cases, you
can automate it so that certain runbooks

555
00:33:51,610 --> 00:33:53,760
or playbooks get executed automatically.

556
00:33:54,430 --> 00:33:58,950
I'm already seeing, a lot of startups out
there who are experimenting in this space,

557
00:33:59,060 --> 00:34:03,320
and I expect that we're going to see a
lot more of this, in the next few years.

558
00:34:03,820 --> 00:34:08,310
So the technical capabilities, of
these models, are, are an area that

559
00:34:08,980 --> 00:34:11,020
it's, it's worth spending time on.

560
00:34:11,940 --> 00:34:14,930
there, there's a lot that can be
done here and we're, we're barely

561
00:34:14,930 --> 00:34:18,070
scratching the surface with the things
that I'm talking about on the slide.

562
00:34:18,470 --> 00:34:21,530
but for exam, I'm just
giving you a few examples.

563
00:34:21,540 --> 00:34:23,180
There's a lot more that
could be done, of course.

564
00:34:23,680 --> 00:34:29,570
So when it comes to parsing large
amounts of logs at scale, trying to

565
00:34:29,570 --> 00:34:35,400
understand complex error patterns, you
can really use LLMs to transform the

566
00:34:35,400 --> 00:34:37,100
way that this is being done currently.

567
00:34:37,600 --> 00:34:39,240
You can do multi hop reasoning.

568
00:34:39,510 --> 00:34:43,080
So you can try to connect the dots
between, you know, what you see on

569
00:34:43,080 --> 00:34:45,830
service A with what you see on service B.

570
00:34:46,780 --> 00:34:51,350
You can do vector similarity searches
across incidents and try to understand.

571
00:34:51,850 --> 00:34:54,580
Is what I'm seeing right now unique?

572
00:34:54,700 --> 00:34:57,230
Is it related to incidents
that have happened in the past?

573
00:34:57,640 --> 00:35:01,540
You know, what were the playbooks or
runbooks that were executed when this

574
00:35:01,580 --> 00:35:03,360
kind of issue happened in the past?

575
00:35:03,790 --> 00:35:06,530
does it make sense to re
execute those things now?

576
00:35:07,030 --> 00:35:07,370
Sorry.

577
00:35:07,870 --> 00:35:12,200
And the other thing that, you know,
will happen and is already starting

578
00:35:12,200 --> 00:35:16,470
to happen is a real time prompt
engineering based on the system state.

579
00:35:16,875 --> 00:35:21,245
So prompt engineering is just a
fancy term for, tweaking how you're

580
00:35:21,245 --> 00:35:23,665
interacting with a large language model.

581
00:35:23,805 --> 00:35:25,645
So what are you asking it to do?

582
00:35:26,415 --> 00:35:30,475
Because the, the, the way in which
you ask it to do something is

583
00:35:30,755 --> 00:35:33,755
determining, you know, the quality of
the output that you get in response.

584
00:35:34,255 --> 00:35:38,505
So, you know, if, if you don't, if
you just have a static prompt and

585
00:35:38,505 --> 00:35:41,175
that's what the large language model
you're interfacing with all the

586
00:35:41,175 --> 00:35:46,410
time, then you get a very similar,
same ish response all the time.

587
00:35:46,580 --> 00:35:50,420
But when it comes to something
as dynamic as, you know, your, a

588
00:35:50,420 --> 00:35:53,380
complex IT infrastructure and the
observability system that you're

589
00:35:53,380 --> 00:35:58,100
trying to build on top of it, the,
the prompting needs to be, adaptable

590
00:35:58,100 --> 00:35:59,470
and dynamic in real time as well.

591
00:35:59,970 --> 00:36:03,540
And really where we're going to see
a lot of the lift off or a lot of

592
00:36:03,540 --> 00:36:09,010
the, the impetus over the next few
years is from agents, AI agents that

593
00:36:09,010 --> 00:36:10,810
can operate autonomously, right?

594
00:36:11,310 --> 00:36:17,760
So you give it a policy, you give it a
set of directions, and then it can take

595
00:36:17,760 --> 00:36:20,300
that and then operate autonomously.

596
00:36:20,300 --> 00:36:22,220
It can modify alert thresholds.

597
00:36:22,540 --> 00:36:25,050
It can, scale your system up and down.

598
00:36:25,550 --> 00:36:29,040
It can, enable traces when it
thinks it's important to do.

599
00:36:30,040 --> 00:36:33,690
It can, it can create service
dependency mapping on its own.

600
00:36:34,140 --> 00:36:38,460
So some of these things might sound a
little bit scary to you that, you know,

601
00:36:38,890 --> 00:36:43,920
if you're, if you're handing off this
to, a non human, what's going to happen?

602
00:36:44,620 --> 00:36:47,050
And, and it's a fair fear
and I understand that.

603
00:36:47,520 --> 00:36:52,480
but as the capability of these models
grow and grow, And of course the models

604
00:36:52,480 --> 00:36:54,400
can do things that humans cannot.

605
00:36:54,410 --> 00:37:00,000
They can, you know, be working on the
problem 24 hours a day, looking through

606
00:37:00,130 --> 00:37:02,670
vaster amounts of data than a human could.

607
00:37:03,170 --> 00:37:07,490
You will start to see that you're
better off with the tools than without.

608
00:37:07,990 --> 00:37:10,620
And we're, we're already
approaching that territory.

609
00:37:11,070 --> 00:37:13,620
Some of you might have already
started using some of these,

610
00:37:13,670 --> 00:37:17,440
automated and autonomous tools
and self healing and so on.

611
00:37:17,990 --> 00:37:20,890
but it's just going to become much
more prevalent, in the future.

612
00:37:21,390 --> 00:37:25,560
And what all of this again means is that
we're, we're living in a new reality.

613
00:37:26,260 --> 00:37:30,670
potential workflows that did
not exist before exist today.

614
00:37:31,125 --> 00:37:35,165
So, you could have a scenario
like this example here, where you

615
00:37:35,165 --> 00:37:40,545
detect a memory leak, the agent
then does a git blame analysis and

616
00:37:40,575 --> 00:37:45,285
understands the, the particular
commit which created the memory leak.

617
00:37:46,165 --> 00:37:51,455
The agent automatically deploys a
rollback decision, and then it, it has

618
00:37:51,455 --> 00:37:54,985
to do an auto scaling adjustment to,
to get the system into the right state.

619
00:37:54,985 --> 00:37:58,325
And all of this could happen
in an automated fashion.

620
00:37:58,325 --> 00:38:01,490
Transcribed And you would just get
a report that this happened, and

621
00:38:01,490 --> 00:38:02,930
this is a very possible workflow.

622
00:38:03,430 --> 00:38:06,140
And you can also see new
KPIs coming out of this.

623
00:38:06,280 --> 00:38:10,590
So X percent of routine incidents
resolved without human intervention

624
00:38:10,620 --> 00:38:14,380
could become a new KPI for,
for SRE teams and DevOps teams.

625
00:38:15,080 --> 00:38:16,940
So it's, it is a new reality.

626
00:38:17,270 --> 00:38:20,320
it's a very interesting
world that, that we live in.

627
00:38:20,820 --> 00:38:24,310
So now that we talked about
this new observability equation.

628
00:38:24,700 --> 00:38:27,010
About data and context and intelligence.

629
00:38:27,350 --> 00:38:31,880
I just want to touch a little bit
about the, the underlying architecture

630
00:38:31,940 --> 00:38:34,510
of the observability stack and why.

631
00:38:35,110 --> 00:38:38,170
you know, at Net Data, we believe
that decentralization is the way

632
00:38:38,170 --> 00:38:42,280
forward, and it's, you know, it's,
it's really not very complicated.

633
00:38:42,730 --> 00:38:45,640
Modern systems are distributed by nature.

634
00:38:46,140 --> 00:38:48,875
Why should we force their
observability to be centralized?

635
00:38:49,375 --> 00:38:54,725
That feels like an artificial limitation
that we're forcing on to a natural system.

636
00:38:55,225 --> 00:38:59,745
There's a lot of different reasons in
which and why centralization fails.

637
00:39:00,245 --> 00:39:05,395
So the minute you bring in centralization,
and by centralization what I mean is,

638
00:39:05,925 --> 00:39:11,365
when you think of your usual observability
platforms, what happens is that you're

639
00:39:11,385 --> 00:39:16,680
collecting logs and metrics and traces
and you're forwarding all of this into

640
00:39:16,680 --> 00:39:18,760
the central data lake or repository.

641
00:39:19,040 --> 00:39:20,330
It might be in somebody's cloud.

642
00:39:20,960 --> 00:39:23,750
You know, you might be self hosting
it yourself, but there's this giant

643
00:39:23,760 --> 00:39:25,710
central repository where everything sits.

644
00:39:25,930 --> 00:39:26,160
Right?

645
00:39:26,660 --> 00:39:29,330
And this creates all kinds of problems.

646
00:39:29,760 --> 00:39:33,630
Now, of course, it becomes a single point
of failure for the entire observability

647
00:39:33,630 --> 00:39:37,540
stack, because if your central store
goes down, you have zero observability.

648
00:39:38,040 --> 00:39:42,110
There's also exponential cost,
that comes from all of this

649
00:39:42,110 --> 00:39:43,460
data movement and storage.

650
00:39:43,700 --> 00:39:47,370
Especially if you're using, you
know, a cloud based provider where

651
00:39:47,420 --> 00:39:48,900
everything is stored on their cloud.

652
00:39:49,170 --> 00:39:52,450
It means that you're not just
paying for their service, but

653
00:39:52,450 --> 00:39:54,370
you're also paying for the egress.

654
00:39:54,460 --> 00:40:00,880
So all of the data leaving your, system
and, you know, getting transmitted to

655
00:40:00,880 --> 00:40:02,105
their cloud is stored on their cloud.

656
00:40:02,365 --> 00:40:05,025
And of course, you're paying
the downstream costs for the

657
00:40:05,035 --> 00:40:06,615
amount of storage that it takes.

658
00:40:07,145 --> 00:40:11,465
Because as systems get complex and
larger, and we're talking about

659
00:40:11,935 --> 00:40:15,955
thousands of services, hundreds of
thousands of nodes, and millions of

660
00:40:15,955 --> 00:40:20,925
metrics, to actually store all of this
centrally is, is a gargantuan task.

661
00:40:21,045 --> 00:40:24,065
And it costs, so it costs
the vendor who is doing it.

662
00:40:24,355 --> 00:40:28,075
It costs you, to pay the vendor
or to self host it yourself.

663
00:40:28,575 --> 00:40:30,765
And it, it quickly becomes a bottleneck.

664
00:40:31,265 --> 00:40:34,675
You also have, cross team
bottlenecks, access control issues

665
00:40:34,685 --> 00:40:36,225
when it comes to centralization.

666
00:40:36,225 --> 00:40:39,715
You know, you know, who do you provide
access to, to this giant central repo?

667
00:40:39,725 --> 00:40:42,275
How, how, how carefully do
you control that access?

668
00:40:42,815 --> 00:40:46,085
you can, you can run into data
sovereignty issues, compliance

669
00:40:46,085 --> 00:40:50,455
headaches, especially if you have multi
region, multi continent operations.

670
00:40:50,955 --> 00:40:54,515
And, Another thing that you will, of
course, see is that your, your query

671
00:40:54,545 --> 00:40:56,675
performance starts to degrade at scale.

672
00:40:57,445 --> 00:41:01,155
And again, this would mean that you
need to make these decisions, right?

673
00:41:01,245 --> 00:41:02,625
Do I need all of this data?

674
00:41:02,925 --> 00:41:04,675
Maybe I need to get rid
of some of the data.

675
00:41:04,975 --> 00:41:09,355
Maybe I need to filter out some
of the data so that I can query

676
00:41:09,365 --> 00:41:12,835
it at a reasonable timeframe
and use it, make it usable.

677
00:41:13,335 --> 00:41:17,335
So decentralization, so if your
architecture is decentralized or

678
00:41:17,335 --> 00:41:20,875
distributed, which means that instead
of storing everything in one single

679
00:41:20,875 --> 00:41:25,155
place, you're storing it all over the
place, and your, your querying is then

680
00:41:25,155 --> 00:41:30,245
intelligent enough to fetch all of it in
real time, solves a lot of these problems.

681
00:41:30,695 --> 00:41:34,845
So this is the way that NetData
has been architected from day one.

682
00:41:35,365 --> 00:41:38,925
And this is one of the reasons
that it allows us to do some of

683
00:41:38,925 --> 00:41:42,395
the things that your traditional
observability platform cannot do.

684
00:41:42,720 --> 00:41:47,490
things like unlimited metrics, per second
granularity, so real time for everything,

685
00:41:47,780 --> 00:41:51,810
and only detection for everything,
while keeping the costs really down.

686
00:41:52,310 --> 00:41:56,910
So, the real world impact of going
distributed, would mean that you can, you

687
00:41:56,910 --> 00:42:02,730
can reduce data transfer costs by up to
90%, you get 65 percent faster mean time

688
00:42:02,730 --> 00:42:07,980
to detection, and, you know, this is,
you know, indirectly happening because

689
00:42:07,980 --> 00:42:10,420
of the distributed architecture, right?

690
00:42:10,440 --> 00:42:12,700
Because, because of the
distributed architecture, you

691
00:42:12,700 --> 00:42:14,160
can have high fidelity data.

692
00:42:14,470 --> 00:42:17,910
And because of the high fidelity
data, you're able to capture the,

693
00:42:18,200 --> 00:42:21,330
you know, the, the right patterns,
tell the right stories with the data.

694
00:42:22,185 --> 00:42:26,095
And that leads to faster mean time to
detection and mean time to resolution.

695
00:42:26,595 --> 00:42:28,985
And of course, this means that,
you know, teams can operate

696
00:42:28,995 --> 00:42:30,265
a lot more independently.

697
00:42:30,585 --> 00:42:32,175
You don't have a single point of failure.

698
00:42:32,215 --> 00:42:36,645
So even if your centralization points
fail, you still have direct access to

699
00:42:36,645 --> 00:42:41,895
the systems and the observability data
originating from those individual nodes.

700
00:42:42,395 --> 00:42:45,845
And you can vastly reduce the
amount of money that you're

701
00:42:45,845 --> 00:42:47,555
spending on network and storage.

702
00:42:48,125 --> 00:42:51,675
And, you know, downstream
the licensing costs as well.

703
00:42:52,515 --> 00:42:57,625
And, you know, you're able to process
the metrics where they originate.

704
00:42:57,935 --> 00:43:01,565
And this becomes important as well,
because As an example, if you're

705
00:43:01,565 --> 00:43:05,135
trying to do anomaly detection, you
know, where are you doing the anomaly

706
00:43:05,135 --> 00:43:08,315
detection becomes important because
you're processing the data, right?

707
00:43:08,335 --> 00:43:12,115
If you're processing it centrally,
you're losing out a little bit on

708
00:43:12,195 --> 00:43:16,065
the context that you had, about
where this metric came from.

709
00:43:16,825 --> 00:43:22,805
Because this, you know, a CPU of 90
percent on one node and CPU on 90

710
00:43:22,805 --> 00:43:27,165
percent on another node could mean two
entirely different things based on the

711
00:43:27,165 --> 00:43:28,410
workloads that are running on them.

712
00:43:28,910 --> 00:43:32,530
So now that we've talked about the
new observability equation and the

713
00:43:32,530 --> 00:43:39,650
underlying architecture, you know, are
you as an organization, as an engineer,

714
00:43:39,770 --> 00:43:41,490
are you ready for the next 10 years?

715
00:43:41,890 --> 00:43:42,210
Right?

716
00:43:42,380 --> 00:43:45,030
So this, this is what's,
what's really important to us.

717
00:43:45,530 --> 00:43:48,730
So for organizations, I, you
know, here are some of the touch

718
00:43:48,730 --> 00:43:52,050
points that I think you should be
thinking about, when it comes to

719
00:43:52,050 --> 00:43:54,670
observability and, the next few years.

720
00:43:55,615 --> 00:43:59,185
You should really focus on
reducing the sampling intervals.

721
00:43:59,995 --> 00:44:03,215
High fidelity data is
super, super important.

722
00:44:03,835 --> 00:44:08,145
So if you can get per second granularity
at scale, you're going to be able to

723
00:44:08,145 --> 00:44:13,365
do so much more with your observability
data that you are doing today.

724
00:44:13,865 --> 00:44:16,095
And this is related, of course.

725
00:44:16,595 --> 00:44:18,605
You should really focus on real time.

726
00:44:18,635 --> 00:44:22,105
So it's, you know, observability
isn't just about historical analysis.

727
00:44:22,135 --> 00:44:25,355
You're not just looking at, you
know, the last X hours or weeks

728
00:44:25,355 --> 00:44:29,635
of data, in a historical fashion,
the data needs to be real time.

729
00:44:30,095 --> 00:44:33,805
You need to be able to see the issues
as they're happening so that you

730
00:44:33,805 --> 00:44:37,675
can take action before it becomes
a problem, not just afterwards.

731
00:44:38,175 --> 00:44:41,505
And ideally you want to democratize
observability across the teams.

732
00:44:41,535 --> 00:44:45,165
You don't want to silo the
observability into just one team, right?

733
00:44:45,185 --> 00:44:49,085
It shouldn't just be, you know,
this is the SRE team and they're

734
00:44:49,085 --> 00:44:51,395
in charge of all observability.

735
00:44:51,405 --> 00:44:53,125
Nobody else needs to look at this data.

736
00:44:53,385 --> 00:44:57,850
Ideally, You know, you should be able
to share that with the developers and

737
00:44:57,870 --> 00:45:01,210
whoever else, you know, management,
whoever needs to see this data, should

738
00:45:01,210 --> 00:45:02,810
have some form of access to the data.

739
00:45:03,310 --> 00:45:08,330
And the, the platform or the tool
that you're using should be usable by

740
00:45:08,350 --> 00:45:10,160
all of these different stakeholders.

741
00:45:10,660 --> 00:45:15,020
You should, As an organization,
invest in AI driven anomaly detection

742
00:45:15,020 --> 00:45:16,750
and automated root cause analysis.

743
00:45:16,950 --> 00:45:21,110
So, really look at if the platform that
you're using today, if you are using

744
00:45:21,110 --> 00:45:23,720
one, offers you these capabilities.

745
00:45:24,130 --> 00:45:28,930
If it doesn't, then it's time to evaluate,
a few tools and platforms that do.

746
00:45:29,430 --> 00:45:32,870
And finally, you know, I would
really recommend implementing a

747
00:45:32,870 --> 00:45:35,050
decentralized observability architecture.

748
00:45:35,935 --> 00:45:40,785
And really think about, how you
can minimize the centralization, in

749
00:45:40,785 --> 00:45:44,745
your, in your system overall going
forward due to, you know, for all

750
00:45:44,745 --> 00:45:46,015
of the reasons that we talked about.

751
00:45:46,515 --> 00:45:48,515
So this was for organizations.

752
00:45:49,295 --> 00:45:52,295
Now let's talk a little bit
about the individuals, right?

753
00:45:52,485 --> 00:45:55,315
For, for the SRE DevOps
professionals of today.

754
00:45:55,815 --> 00:45:57,215
What are the things that
you should be doing?

755
00:45:58,080 --> 00:46:01,470
So of course you should be mastering
cloud native observability and AI ops.

756
00:46:01,510 --> 00:46:03,200
I don't think I need to tell anybody this.

757
00:46:03,380 --> 00:46:05,120
People are already
doing this on their own.

758
00:46:06,090 --> 00:46:09,700
I think you should start thinking
beyond, basic infrastructure as

759
00:46:09,730 --> 00:46:14,380
code to, to policy as code, to
security as code practices as well.

760
00:46:14,880 --> 00:46:18,780
it would be really good if you can develop
expertise in causal analysis and real

761
00:46:18,810 --> 00:46:23,570
time analytics, because this allows you
to tell those better stories, right?

762
00:46:23,870 --> 00:46:24,970
So you can actually.

763
00:46:25,515 --> 00:46:27,315
this is a skill that you can develop.

764
00:46:27,825 --> 00:46:31,155
You can, you can try to tell
better cause and effect stories.

765
00:46:31,425 --> 00:46:35,085
So here's what you see, here's,
you know, what could have cost it.

766
00:46:35,585 --> 00:46:39,305
And then being able to present that
either through a dashboard, through a

767
00:46:39,305 --> 00:46:43,295
report, through, through a tool, you
know, it, it gives you a lot of power

768
00:46:43,795 --> 00:46:45,265
when, when you can tell the right stories.

769
00:46:45,370 --> 00:46:48,750
you should develop multi cloud and
edge computing skills, because like we

770
00:46:48,750 --> 00:46:53,760
discussed, as, as the workloads become
more complex, as the data center explosion

771
00:46:53,760 --> 00:46:58,470
happens, you're, you're not going to
have your entire workload running on a

772
00:46:58,470 --> 00:47:00,840
single system, on, on a single cloud.

773
00:47:01,390 --> 00:47:06,150
It, it's going to be spread out, and
you need to be able to, understand

774
00:47:06,260 --> 00:47:08,400
different systems and different scales.

775
00:47:08,900 --> 00:47:13,380
And finally, I think it would be, very
useful for you, for your career in

776
00:47:13,380 --> 00:47:18,910
general, if you can gain expertise working
with and orchestrating AI agents, right?

777
00:47:19,400 --> 00:47:24,840
So, get, get familiar with with
AI in general, with AI agents, get

778
00:47:24,850 --> 00:47:28,300
comfortable with them, understand
their intricacies, understand what

779
00:47:28,300 --> 00:47:32,060
they're good at, what they're bad at,
understand the scenarios where they fail.

780
00:47:32,730 --> 00:47:36,480
And doing that will give you a big
head start, in, in the next decade.

781
00:47:36,980 --> 00:47:42,590
So I hope that, this is, useful
advice for both organizations and for

782
00:47:42,590 --> 00:47:47,230
individuals, and that, you'll be able to
do great things, with this information.

783
00:47:47,730 --> 00:47:53,070
So that's, you know, that's about the,
the, the guidance that, that I would

784
00:47:53,070 --> 00:47:57,680
provide for the next 10 years and, and
to leave you with, with a quote, that I

785
00:47:57,680 --> 00:48:04,160
made up just now, I think that future SREs
will manage a hundred times more services.

786
00:48:04,660 --> 00:48:07,950
But they'll be doing it, with 10
times less manual intervention.

787
00:48:08,210 --> 00:48:12,460
So there will be a lot more work
for SREs, but that work, will be,

788
00:48:12,570 --> 00:48:16,650
less manual burdens involved, right?

789
00:48:17,150 --> 00:48:20,360
So finally to wrap up, it's
time to go back to the future.

790
00:48:20,360 --> 00:48:23,890
there's, there's this quote that I really
like, I really enjoy, from William Gibson.

791
00:48:23,890 --> 00:48:26,520
It says, the future is already here.

792
00:48:27,195 --> 00:48:29,665
It's just not evenly distributed yet.

793
00:48:30,165 --> 00:48:33,915
And I think this is really true, about a
lot of the things that we talked about.

794
00:48:34,415 --> 00:48:38,985
That some of these capabilities, some
of these tools are already here, but

795
00:48:39,085 --> 00:48:40,875
everybody is not aware about them.

796
00:48:41,205 --> 00:48:42,605
Everybody is not using them.

797
00:48:43,025 --> 00:48:47,835
So there's, there's pockets of space,
in the industry where, where, you know,

798
00:48:47,835 --> 00:48:52,155
people are aware they are using them and
they're miles ahead because of this fact.

799
00:48:52,655 --> 00:48:57,615
So, as, as my final, statement as part
of the talk, I would, I would like to

800
00:48:57,615 --> 00:49:01,745
tell that you should not wait until
your monitoring tools are obsolete.

801
00:49:02,345 --> 00:49:06,675
You should not wait until your
dashboards are, you know, either

802
00:49:06,695 --> 00:49:08,265
peddling lies or half truths.

803
00:49:09,205 --> 00:49:12,065
You should not wait until your
teams are drowning in alerts.

804
00:49:12,835 --> 00:49:16,935
And you definitely don't want to wait
until your systems are too complex

805
00:49:17,005 --> 00:49:18,375
for humans to manage them alone.

806
00:49:18,875 --> 00:49:21,105
The tools of tomorrow are here today.

807
00:49:21,405 --> 00:49:21,925
Use them.

808
00:49:22,885 --> 00:49:25,905
So embrace high fidelity, real time data.

809
00:49:26,105 --> 00:49:27,525
Don't settle for less.

810
00:49:28,025 --> 00:49:29,305
Use AI agents.

811
00:49:29,805 --> 00:49:31,865
Get comfortable with them.

812
00:49:32,365 --> 00:49:36,555
Let the machines do what humans cannot
or don't want or should not be doing.

813
00:49:37,055 --> 00:49:37,995
Go distributed.

814
00:49:38,705 --> 00:49:42,535
True fidelity in your data demands
decentralized architecture.

815
00:49:43,035 --> 00:49:47,875
And then finally, demand insights from
your observability stack, from your

816
00:49:47,885 --> 00:49:51,995
observability platform, not just more
charge, more dashboards, and more noise.

817
00:49:52,495 --> 00:49:52,955
Don't settle.

818
00:49:53,455 --> 00:49:53,865
Thank you.

819
00:49:54,365 --> 00:49:55,605
thanks for listening to the talk.

820
00:49:55,715 --> 00:49:58,255
like I said at the start, if
you'd like to find out more about

821
00:49:58,275 --> 00:50:02,045
NetData to the company, you can
visit our website at netdata.

822
00:50:02,085 --> 00:50:02,505
cloud.

823
00:50:02,895 --> 00:50:06,925
You can also check out our GitHub You
can just search for NetData on GitHub.

824
00:50:07,575 --> 00:50:10,525
And if you have any questions
about the talk, I'd be more than

825
00:50:10,525 --> 00:50:12,795
happy to discuss it with you.

826
00:50:13,295 --> 00:50:14,555
Here's my email.

827
00:50:14,605 --> 00:50:15,835
You can find me on LinkedIn.

828
00:50:16,335 --> 00:50:20,015
And you can also find me on Substack,
where again, I talk a little bit about

829
00:50:20,025 --> 00:50:22,315
the future, what's in store for us.

830
00:50:23,095 --> 00:50:23,585
Thank you.

831
00:50:24,215 --> 00:50:24,865
Thanks again.

832
00:50:25,205 --> 00:50:25,505
Bye.

