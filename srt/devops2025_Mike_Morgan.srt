1
00:00:00,010 --> 00:00:00,470
Hi everyone.

2
00:00:01,230 --> 00:00:04,170
My name is Mike Morgan and I
head up the technical account

3
00:00:04,180 --> 00:00:05,780
management team here at Buildkite.

4
00:00:06,150 --> 00:00:09,520
I've been at Buildkite for about
four years now, during which time

5
00:00:09,520 --> 00:00:13,450
I've held a number of roles, both on
the pre and post sale side, working

6
00:00:13,450 --> 00:00:16,649
with, with pretty much all of our
enterprise customers during that time.

7
00:00:16,650 --> 00:00:19,800
And, and that is a list that consists
of some of the most exciting and

8
00:00:19,830 --> 00:00:21,510
innovative, tech companies on the planet.

9
00:00:21,590 --> 00:00:24,709
so before I get started here, please
go ahead and check out buildkite.

10
00:00:24,709 --> 00:00:27,890
com for all of the logos and testimonials
and all of that kind of stuff.

11
00:00:28,600 --> 00:00:32,140
So, during my tenure here, I've
worked with many teams of all, on

12
00:00:32,140 --> 00:00:35,010
all sorts of different types of
projects to help them do CI better.

13
00:00:35,040 --> 00:00:37,980
And one theme that's come up
again and again during this

14
00:00:37,980 --> 00:00:40,290
time is that of flaky tests.

15
00:00:40,369 --> 00:00:46,190
So, really, as teams and projects grow,
and tech stacks increase in complexity,

16
00:00:46,220 --> 00:00:50,160
and developer velocity increases,
there's really that need to better

17
00:00:50,180 --> 00:00:52,890
understand and address flaky tests.

18
00:00:53,150 --> 00:00:57,135
and If this problem isn't understood,
addressed and and really woven into

19
00:00:57,145 --> 00:00:59,935
the culture of your organization,
then this can have a profoundly

20
00:00:59,935 --> 00:01:06,355
negative effect on your ability to
ship and build quality software and

21
00:01:06,355 --> 00:01:09,565
meet those KPIs that are probably
being set by your leadership teams.

22
00:01:10,055 --> 00:01:14,345
So in this session, I'm going to talk
a bit about test flakiness and why it's

23
00:01:14,345 --> 00:01:18,765
a bad thing, what it means for your
organization, and most importantly, a few

24
00:01:18,765 --> 00:01:20,735
strategies to really nip this in the bud.

25
00:01:21,235 --> 00:01:23,005
So to kick this off.

26
00:01:24,000 --> 00:01:26,670
We're going to start with the
basics and real simple stuff

27
00:01:26,710 --> 00:01:28,610
like what is a flaky test.

28
00:01:29,230 --> 00:01:33,240
so given that you're listening to
a webinar about flaky tests at a

29
00:01:33,240 --> 00:01:36,490
DevOps conference, the chances are
that you already know what flaky

30
00:01:36,490 --> 00:01:39,390
tests are, probably overly familiar
with them to be quite honest.

31
00:01:39,700 --> 00:01:43,300
But for those of you that are
uninitiated, a flaky test could be

32
00:01:43,300 --> 00:01:47,490
summarized as a test that sometimes
passes and sometimes fails without

33
00:01:47,490 --> 00:01:48,920
any changes to the underlying code.

34
00:01:48,920 --> 00:01:52,350
So it's just flaps without
any rhyme or reason.

35
00:01:52,715 --> 00:01:57,885
And really, there's a ton of
reasons that a test could be flaky.

36
00:01:58,185 --> 00:02:00,955
now I'm not going to go into each one
of these because there's a ton of them.

37
00:02:00,955 --> 00:02:02,305
You probably know a lot of these anyway.

38
00:02:02,305 --> 00:02:03,405
It'd be a waste of everyone's time.

39
00:02:03,405 --> 00:02:05,755
So I just threw a bunch of
them on this slide here.

40
00:02:05,994 --> 00:02:09,769
but, this is by no means a conclusive
list, but it's definitely, some of

41
00:02:09,769 --> 00:02:11,209
the ones that we see very often.

42
00:02:11,209 --> 00:02:14,780
I mean, of all of these different
reasons, and again, kind of working

43
00:02:14,780 --> 00:02:16,109
from custom with customers, rather.

44
00:02:16,109 --> 00:02:19,764
I mean, this Purely anecdotal, but I
think like order dependency, environment

45
00:02:19,784 --> 00:02:22,904
issues and resource constraints are
probably the ones we see most often.

46
00:02:23,124 --> 00:02:24,714
but again, anecdotal, statement there.

47
00:02:24,714 --> 00:02:27,774
So please take it with a pinch of
salt, but, during this talk, we're

48
00:02:27,774 --> 00:02:31,234
going to be calling back some of these,
and, and referring to them as we go

49
00:02:31,234 --> 00:02:32,764
through some mitigation strategies.

50
00:02:33,264 --> 00:02:36,104
So, what is the impact of flaky tests?

51
00:02:36,494 --> 00:02:40,804
Well, first of all, we can say that flaky
tests undermine developer confidence.

52
00:02:40,854 --> 00:02:43,954
so fundamentally, if the result
of a test can be changed simply by

53
00:02:43,964 --> 00:02:47,174
spamming, the retry button in your
CI, then you really have to ask,

54
00:02:47,174 --> 00:02:48,274
what value does it really offer?

55
00:02:48,774 --> 00:02:55,004
As is customary these days, I asked
ChatGPT to define to me what a test

56
00:02:55,004 --> 00:02:59,754
is, and ChatGPT told me in software
engineering, a test is a process

57
00:02:59,754 --> 00:03:02,914
or procedure used to verify that a
software application or system behaves

58
00:03:03,124 --> 00:03:05,314
as expected under specified conditions.

59
00:03:05,705 --> 00:03:10,610
so, With that in mind, if you're
testing a supposedly in a controlled

60
00:03:10,610 --> 00:03:14,600
environment with supposedly controlled
data set, but you're still finding

61
00:03:14,600 --> 00:03:16,969
you're getting unpredictable results
on back to back test runs, and

62
00:03:16,970 --> 00:03:21,920
it's probably safe to say that that
definition is not really being met on.

63
00:03:21,940 --> 00:03:23,040
The result of this is that.

64
00:03:23,655 --> 00:03:29,165
Engineers working on the project don't
really know whether the, the test that

65
00:03:29,165 --> 00:03:31,295
they're, they're testing the thing
with is actually broken or whether

66
00:03:31,295 --> 00:03:32,765
it's just a test that's playing up.

67
00:03:33,125 --> 00:03:36,135
So the best possible outcome in this
scenario is that the engineer, in

68
00:03:36,135 --> 00:03:39,105
question is just mashing that retry
button until they get a green build,

69
00:03:39,235 --> 00:03:42,155
which is, you know, obviously not ideal.

70
00:03:42,425 --> 00:03:45,545
if a test is being flaky, then
the, the worst case scenario

71
00:03:45,545 --> 00:03:46,505
is you're gonna ship something.

72
00:03:46,875 --> 00:03:49,895
broken and dodgy to production
and ultimately this just erodes

73
00:03:49,895 --> 00:03:52,995
confidence across the board
and is universally a bad thing.

74
00:03:53,495 --> 00:03:57,285
So another bad thing about flaky
tests, it slows down your CI pipelines.

75
00:03:57,785 --> 00:04:01,004
Apart from obviously eroding the
confidence, which is a bad thing,

76
00:04:01,235 --> 00:04:02,374
slowing down CI is even worse.

77
00:04:02,375 --> 00:04:05,985
When a test fails, you'll
probably have some sort of retry

78
00:04:06,005 --> 00:04:07,735
logic that's baked into your CI.

79
00:04:07,735 --> 00:04:10,955
It's gonna introduce some latency
into your build as it retries.

80
00:04:11,365 --> 00:04:14,394
maybe you just have the necessary
gear to just rerun that single test

81
00:04:14,395 --> 00:04:17,535
spec, which might only take a couple
of seconds in the best case scenario.

82
00:04:17,905 --> 00:04:20,185
But if you're batching a number
of tests together in a single job

83
00:04:20,195 --> 00:04:23,395
running in CI, then you might find
that it reruns a whole load of stuff.

84
00:04:23,705 --> 00:04:27,725
that's gonna add expense, both in
terms of, yeah, time and money.

85
00:04:28,700 --> 00:04:32,350
So I work with a ton of customers helping
these build quite better and optimize

86
00:04:32,350 --> 00:04:37,030
their software delivery pipelines, and
I don't have any data to corroborate

87
00:04:37,030 --> 00:04:38,870
this purely anecdotal statement.

88
00:04:38,870 --> 00:04:42,330
But from the customers I work with,
a vast majority of their build

89
00:04:42,340 --> 00:04:44,009
time is spent executing tests.

90
00:04:44,330 --> 00:04:48,299
So by having to rerun tests due
to flaky behavior, they're turning

91
00:04:48,299 --> 00:04:50,930
the test phase into something
that's even more of a time sink.

92
00:04:51,440 --> 00:04:54,840
and these days, everyone has
KPIs about build performance and

93
00:04:54,840 --> 00:04:57,329
across the board, improving test
performance is a good way to do that.

94
00:04:57,430 --> 00:04:58,400
To meet those KPIs.

95
00:04:58,400 --> 00:05:01,790
So paralyzing tests, of course, can be
the ultimate cheat code for slashing

96
00:05:01,790 --> 00:05:05,460
build times across, across the board,
but ensuring that tests are well

97
00:05:05,460 --> 00:05:09,810
written and behave as expected and
don't need to be retried continuously is

98
00:05:09,810 --> 00:05:11,130
definitely going to be a close second.

99
00:05:11,630 --> 00:05:15,730
Now, thirdly, there's a monetary value
associated with tests being flaky.

100
00:05:15,780 --> 00:05:20,125
so flaky tests, as we said, As I
mentioned there, they erode confidence,

101
00:05:20,155 --> 00:05:21,985
they slow things down, they're annoying.

102
00:05:22,215 --> 00:05:24,675
but the impact goes beyond just
getting on everyone's nerves, there's

103
00:05:24,675 --> 00:05:26,855
also a financial hit here too.

104
00:05:26,875 --> 00:05:30,054
So, retrying a test is going
to burn some compute time.

105
00:05:30,124 --> 00:05:32,174
whether it's just a single
test or maybe it's a batch of

106
00:05:32,174 --> 00:05:33,454
tests that need to be retried.

107
00:05:33,694 --> 00:05:36,454
that's going to be some more money that
you're going to be giving to AWS or

108
00:05:36,454 --> 00:05:39,324
GCP or whoever is hosting that compute.

109
00:05:39,824 --> 00:05:42,524
The longer build times will hurt
productivity, which means there's,

110
00:05:42,764 --> 00:05:45,224
you know, there's engineer salaries
that are not necessarily being put

111
00:05:45,224 --> 00:05:48,494
to their best use and, and overall
the quality of the thing you're

112
00:05:48,494 --> 00:05:50,054
shipping is going to be impacted.

113
00:05:50,054 --> 00:05:54,354
And I think perhaps maybe a bit
harder to attribute a dollar value

114
00:05:54,354 --> 00:05:58,794
directly, to a flaky test in, in that
particular sense, but still it's worth

115
00:05:58,794 --> 00:06:02,885
mentioning, and, you know, over time
and at scale, both in terms of the

116
00:06:02,885 --> 00:06:05,255
number of bills and the number of
engineers involved, I mean, this can

117
00:06:05,755 --> 00:06:09,385
So I asked Chech EPT, as I'm
wont to do, to provide some

118
00:06:09,385 --> 00:06:11,055
statistics about flaky tests.

119
00:06:11,455 --> 00:06:15,195
And, and here's what it told me,
with, with citations, no less.

120
00:06:15,745 --> 00:06:18,925
Google reports that 16 percent
of their tests are flaky.

121
00:06:19,425 --> 00:06:25,575
60% of developers are regularly encounter
flaky tests and order dependency

122
00:06:25,575 --> 00:06:30,005
is a dominant cause of flakiness,
responsible 59% of flaky tests followed

123
00:06:30,035 --> 00:06:32,825
by test infrastructure problems at 28%.

124
00:06:33,265 --> 00:06:38,105
So based on my experience working with,
again, hundreds or potentially even

125
00:06:38,105 --> 00:06:41,945
thousands of customers, my anecdotal
take on this would be that 16% of Tesla

126
00:06:41,945 --> 00:06:43,685
flaky is, is probably about right.

127
00:06:43,810 --> 00:06:46,206
based on what we see with
customers, give or take, 60 percent

128
00:06:46,206 --> 00:06:48,545
of devs encounter flaky tests.

129
00:06:48,575 --> 00:06:51,435
that I didn't say that stat seems a
little bit optimistic to be quite honest.

130
00:06:51,455 --> 00:06:54,205
my feeling is that that would be,
something close to a hundred percent,

131
00:06:54,235 --> 00:06:56,915
maybe not necessarily a hundred
percent, but certainly something,

132
00:06:57,035 --> 00:06:58,235
that's, that's higher than 60%.

133
00:06:58,715 --> 00:07:01,245
So, we've established
that flaky tests are bad.

134
00:07:01,670 --> 00:07:03,660
annoying and they cost money
and they can erode trust.

135
00:07:03,880 --> 00:07:06,210
So what we're going to do is now
dive in some strategies that you

136
00:07:06,210 --> 00:07:11,210
can use to address flaky test
problems in your environment.

137
00:07:11,979 --> 00:07:18,699
So first of all, we're going to want
to isolate and identify flaky tests.

138
00:07:18,710 --> 00:07:22,160
So to be able to address flaky tests
you need to be able to establish

139
00:07:22,190 --> 00:07:23,970
whether a test is flaky to begin with.

140
00:07:23,970 --> 00:07:25,414
So in this session, we're not
just talking about flaky tests.

141
00:07:25,715 --> 00:07:29,925
Broken tests or those those do
share qualities with flaky tests,

142
00:07:29,925 --> 00:07:33,175
obviously, but instead we're going
to be specifically looking at tests

143
00:07:33,535 --> 00:07:36,425
that exhibit that flaky behavior.

144
00:07:36,475 --> 00:07:40,245
so, first thing we're going to want
to do is look for signs of flakiness.

145
00:07:40,245 --> 00:07:44,805
So the real obvious one here, first
things first, is a test exhibiting flaky

146
00:07:44,805 --> 00:07:47,035
behavior versus just being a broken test.

147
00:07:47,335 --> 00:07:50,105
So we're going to be looking for
certain distinct patterns of behavior

148
00:07:50,145 --> 00:07:53,710
to determine, whether a Test is flaky.

149
00:07:54,210 --> 00:07:56,940
First one would be intermittent test
failures without any code changes.

150
00:07:56,950 --> 00:08:01,960
So this could be a case of a different
test result across multiple retries in a

151
00:08:01,960 --> 00:08:07,510
single CI run, or unpredictable results
across multiple commits and CI runs for a

152
00:08:07,510 --> 00:08:09,680
portion of code that hasn't been touched.

153
00:08:10,470 --> 00:08:13,080
So when nothing obvious has
changed, but your tests have given

154
00:08:13,080 --> 00:08:16,130
unpredictable results, there's probably
something flaky happening there.

155
00:08:16,360 --> 00:08:19,660
We could probably at least
take a second glance and assume

156
00:08:19,660 --> 00:08:21,150
that test could be flaky.

157
00:08:21,650 --> 00:08:26,660
tests that are passing locally but
failing in, in your CI environment.

158
00:08:26,740 --> 00:08:31,320
so if it works locally on your laptop,
then it should work in CI, right?

159
00:08:31,410 --> 00:08:31,750
well.

160
00:08:32,435 --> 00:08:36,115
Probably in many cases, that's true,
but there's also good reasons why not.

161
00:08:36,175 --> 00:08:39,935
and it could be an environmental
factor caused by, either, you

162
00:08:39,935 --> 00:08:42,955
know, something local or again,
something in your CI environment.

163
00:08:42,985 --> 00:08:45,505
An environment, that you run
your tests in is very important.

164
00:08:45,535 --> 00:08:48,145
And we'll be taking a close look
at that in a second, but this,

165
00:08:48,225 --> 00:08:50,985
this would be definitely a behavior
that could indicate some flakiness.

166
00:08:51,615 --> 00:08:57,035
and thirdly, When the outcome of the test
seems to depend on a non deterministic

167
00:08:57,035 --> 00:09:00,825
factor could be like the time of day
it's run or the environment again that

168
00:09:00,825 --> 00:09:06,345
it's run in or any, any other variable
that just doesn't seem to, doesn't

169
00:09:06,355 --> 00:09:08,705
seem to imply stability, I guess.

170
00:09:09,065 --> 00:09:12,525
So to be honest, I mean, this one, it
really is a less of a distinct pattern

171
00:09:12,525 --> 00:09:15,765
of failure and just something that's The,
face value can seem extremely random.

172
00:09:16,175 --> 00:09:18,955
in these cases, there can often
appear to be no rhyme or reason.

173
00:09:19,065 --> 00:09:21,515
a test just flaps
unpredictably when it runs.

174
00:09:21,515 --> 00:09:23,825
And again, if you kind of see that,
then it's a pretty good chance

175
00:09:23,825 --> 00:09:26,545
that the test is, is flaky there.

176
00:09:27,095 --> 00:09:29,585
so really we're just looking
to try and understand how a

177
00:09:29,585 --> 00:09:30,915
test performs over many runs.

178
00:09:31,145 --> 00:09:34,625
And if we're seeing a patchwork of
like red and green results every time

179
00:09:34,985 --> 00:09:38,585
it's run, across many builds, that,
that would certainly imply flakiness.

180
00:09:39,045 --> 00:09:42,475
So depending on the size and complexity
of your code base, identifying these

181
00:09:42,475 --> 00:09:44,575
things manually, it might be possible.

182
00:09:44,905 --> 00:09:45,985
But on the other hand, it is not.

183
00:09:46,215 --> 00:09:47,505
It is 2025 now.

184
00:09:47,505 --> 00:09:49,465
You probably shouldn't have
to do these things by hand.

185
00:09:49,475 --> 00:09:52,915
It's way better, just to pay
a bunch of money to, buy some

186
00:09:52,915 --> 00:09:54,555
software to have it done for you.

187
00:09:55,245 --> 00:09:58,095
So, second thing, how can
we identify flaky tests?

188
00:09:58,315 --> 00:10:02,355
Maybe try, rerunning it a few times,
real brute force technique, Not

189
00:10:02,385 --> 00:10:06,455
particularly sophisticated here, just
rerun the test and see what happens.

190
00:10:06,455 --> 00:10:08,665
So flaky tests, of course, by
their nature, they tend to give you

191
00:10:08,665 --> 00:10:10,155
different results across multiple runs.

192
00:10:10,965 --> 00:10:14,265
And if you want to see if a test is
constantly failing or flapping, then

193
00:10:14,305 --> 00:10:15,965
why not just rerun it a bunch of times?

194
00:10:16,325 --> 00:10:20,785
And I don't necessarily mean just as part
of your standard CI workflow, but just,

195
00:10:20,785 --> 00:10:25,315
you know, for the purpose of testing and
trying to surface this kind of flakiness.

196
00:10:25,815 --> 00:10:28,955
So if you run your test job a
bunch of times, ten times, I don't

197
00:10:28,955 --> 00:10:30,265
know, whatever, does it exhibit.

198
00:10:30,460 --> 00:10:31,520
Flaky looking behavior.

199
00:10:31,530 --> 00:10:34,110
If so, then yes, it probably is flaky.

200
00:10:34,610 --> 00:10:38,670
And, finally here, utilize,
test observability tools

201
00:10:38,680 --> 00:10:40,200
and, flaky detection tools.

202
00:10:40,290 --> 00:10:43,200
so I have actually a full
section on this coming up.

203
00:10:43,210 --> 00:10:45,750
And it's, it's very important that
I don't just burn through all my

204
00:10:45,750 --> 00:10:46,990
content in the first few minutes here.

205
00:10:47,000 --> 00:10:50,650
So I'm, I'm, I'm don't really want to
repeat myself later on in the session.

206
00:10:50,650 --> 00:10:52,940
So I'm going to keep it real
brief here and go into more depth.

207
00:10:53,105 --> 00:10:56,205
as we, as we get to that point,
but, it did seem worthwhile kind of

208
00:10:56,205 --> 00:10:58,945
talking about very briefly at the
top of the, the, the session here.

209
00:10:58,965 --> 00:11:02,885
So if you're working in a large project
and a large team, it's totally reasonable

210
00:11:02,885 --> 00:11:06,735
to expect that, that your view, could
be somewhat blinkered to the thing that

211
00:11:06,735 --> 00:11:11,245
you're working on currently, and, your
own CI runs rather than like looking at

212
00:11:11,245 --> 00:11:14,545
your, your colleagues runs and all of
the different, Pipeline runs that have

213
00:11:14,595 --> 00:11:17,615
been occurring and looking at the test
results and seeing if the thing that

214
00:11:18,065 --> 00:11:20,895
is flaky for you appears to be flaky
for everyone else to really understand

215
00:11:20,895 --> 00:11:25,085
the scope of the problem so that
that can be, in itself a mammoth ask.

216
00:11:25,145 --> 00:11:27,605
So the good thing is now there's a
number of tools that can help you out

217
00:11:27,605 --> 00:11:30,865
with this, which will collect test
results across any number of builds.

218
00:11:30,865 --> 00:11:34,475
They'll surface failing and flaky
behavior, assist with debugging

219
00:11:34,485 --> 00:11:36,705
in general and really kind of
give you a ton of visibility.

220
00:11:36,705 --> 00:11:40,775
So unless your project is
really small and manageable.

221
00:11:41,540 --> 00:11:44,020
I would really recommend,
exploring this avenue.

222
00:11:44,520 --> 00:11:47,820
So the takeaway from this first
strategy would be that reliable

223
00:11:47,820 --> 00:11:50,700
testing begins with understanding
which tests are flaky to begin with.

224
00:11:50,780 --> 00:11:54,360
A test that fails doesn't
necessarily make it flaky.

225
00:11:54,480 --> 00:11:57,320
I mean flakiness is is
another quality entirely.

226
00:11:58,070 --> 00:12:01,150
Really use the means at your disposal
to look at your tests, their behavior,

227
00:12:01,480 --> 00:12:04,770
and determine which ones are just
totally broken and those ones that are

228
00:12:04,770 --> 00:12:08,080
flapping due to some external factor.

229
00:12:08,580 --> 00:12:13,570
So the second strategy, optimize
test design and implementation.

230
00:12:14,070 --> 00:12:18,560
So, perhaps with some exceptions, most
test flakiness can be attributed, to

231
00:12:18,600 --> 00:12:20,540
a, a test design and implementation.

232
00:12:20,570 --> 00:12:24,610
And we spoke about that statistic a
moment ago where 59 percent of flaky

233
00:12:24,610 --> 00:12:26,610
tests are the result of order dependence.

234
00:12:26,660 --> 00:12:29,080
and this is something that
can absolutely be addressed,

235
00:12:29,260 --> 00:12:30,410
through improved test design.

236
00:12:30,560 --> 00:12:33,370
And I have a couple of things,
to consider here, which, which

237
00:12:33,400 --> 00:12:34,750
might seem pretty obvious.

238
00:12:35,045 --> 00:12:37,855
to, to some, but then also
again, it's worth covering to

239
00:12:37,855 --> 00:12:39,915
build that solid foundation.

240
00:12:40,415 --> 00:12:45,825
So the first thing we have here is Test
atomism, or making your tests atomic.

241
00:12:46,345 --> 00:12:50,515
So, primarily this is going to be
relevant to unit tests, but there's

242
00:12:50,515 --> 00:12:54,705
also, some atomic principles that
we can apply to, your integration

243
00:12:54,705 --> 00:12:55,985
tests and end to end tests as well.

244
00:12:56,485 --> 00:12:58,565
So what is an atomic test,
for those who do not know?

245
00:12:59,215 --> 00:13:03,285
so an atomic test, is a test
that's designed to be independent.

246
00:13:03,295 --> 00:13:06,755
So meaning it doesn't depend on any,
the execution of any, any other tests.

247
00:13:06,755 --> 00:13:09,635
It doesn't rely on the
state set by other tests.

248
00:13:10,095 --> 00:13:15,115
Again, think of that 59 percent order
dependency stat, which honestly, Sounds,

249
00:13:15,195 --> 00:13:17,495
sounds like a lot, but I don't have
any better data, so, so I'll have to

250
00:13:17,495 --> 00:13:22,275
do, but anyway, for, designing tests
to run independently, hypothetically,

251
00:13:23,035 --> 00:13:25,385
that number could be much, much lower.

252
00:13:25,465 --> 00:13:28,495
0%, possibly, I don't know, but,
tests that don't rely on other tests,

253
00:13:28,665 --> 00:13:29,895
inherently gonna be more reliable.

254
00:13:30,395 --> 00:13:33,645
so an atomic test should be tightly
scoped, so where appropriate,

255
00:13:34,135 --> 00:13:37,215
test a single unit functionality,
such as a method or a class,

256
00:13:37,235 --> 00:13:39,145
rather than a bunch of things.

257
00:13:39,345 --> 00:13:43,475
so, yeah, if you've ever heard of
the expression, less is more, so when

258
00:13:43,475 --> 00:13:45,295
you write your tests, live by that.

259
00:13:45,795 --> 00:13:47,765
An atomic test should be deterministic.

260
00:13:47,775 --> 00:13:51,195
So ideally, you want your test to provide
the same result every single time.

261
00:13:51,895 --> 00:13:55,305
Nice big green tick next to
that, that build, given the

262
00:13:55,305 --> 00:13:57,315
same inputs every time it's run.

263
00:13:57,425 --> 00:14:00,155
So, whenever possible
you avoid randomness.

264
00:14:00,360 --> 00:14:04,270
In the inputs, again, on the subject
of inputs, stabilize those inputs,

265
00:14:04,530 --> 00:14:07,700
provide static and predefined
data sets that, that you know, are

266
00:14:07,710 --> 00:14:09,760
good, that your tests can utilize.

267
00:14:10,300 --> 00:14:12,350
And also, clean up the state between runs.

268
00:14:12,350 --> 00:14:15,135
And again, we're going to, going
to talk about environment, to run

269
00:14:15,135 --> 00:14:16,365
your testing in a moment here.

270
00:14:16,425 --> 00:14:19,315
but in the context, of determinism,
we want to ensure that your tests

271
00:14:19,315 --> 00:14:22,665
are clean and predictable and
reproducible test bed to run in.

272
00:14:23,165 --> 00:14:23,645
Oh, there we go.

273
00:14:24,195 --> 00:14:25,205
And performant.

274
00:14:25,295 --> 00:14:26,675
So, tests should be small.

275
00:14:26,755 --> 00:14:30,555
Ideally testing, again, a single
thing using a predictable data set.

276
00:14:30,575 --> 00:14:34,465
It should be hypothetically,
theoretically quick to execute.

277
00:14:34,565 --> 00:14:38,085
So these high performance tests are
perfectly suited to be run during your

278
00:14:38,085 --> 00:14:43,215
CI process, and they should be less
susceptible to timeouts in most cases.

279
00:14:43,715 --> 00:14:48,960
So I mentioned at the top of the section,
that atomic Primarily, refers to unit

280
00:14:48,960 --> 00:14:53,010
tests, and your end to end and integration
tests, should not be atomic in the same

281
00:14:53,010 --> 00:14:57,620
way as your unit tests, but they can
employ atomic like principles, so your

282
00:14:57,620 --> 00:15:01,850
integration tests can be atomic to an
extent, so while not as small as unit

283
00:15:01,850 --> 00:15:05,890
tests, integration tests should focus on
single integration points or interactions,

284
00:15:06,230 --> 00:15:10,055
and you want to avoid Coupling multiple
unrelated components, in in a single

285
00:15:10,055 --> 00:15:13,895
integration test, if you can avoid it,
each integration test should have a clear,

286
00:15:13,945 --> 00:15:20,240
and single purpose like does service a,
you know, correctly pass that, response

287
00:15:20,250 --> 00:15:21,670
from service B or something like that.

288
00:15:22,100 --> 00:15:25,930
And of course, you know, you should
use mocking, dependencies whenever

289
00:15:25,970 --> 00:15:28,320
appropriate, to avoid issues.

290
00:15:29,300 --> 00:15:33,570
For your end to end tests, usually these
can't be strictly atomic because they're

291
00:15:33,570 --> 00:15:35,360
less likely to be small or isolated.

292
00:15:35,700 --> 00:15:40,030
However, they can also be designed
to focus on a single, well defined

293
00:15:40,320 --> 00:15:44,680
user journey or scenario, minimizing
overlap with other end to end tests.

294
00:15:45,040 --> 00:15:48,570
And in the spirit of, of those four tenets
of atomism that I listed a moment ago.

295
00:15:48,950 --> 00:15:53,440
the test should use a clean state between
runs, use well defined test data to

296
00:15:53,440 --> 00:15:57,930
ensure consistency, and use mocks and
stubs, for those, third party APIs.

297
00:15:58,430 --> 00:16:02,890
So, next for this strategy,
adopt mocking and stubbing.

298
00:16:03,190 --> 00:16:06,520
so, rather than relying on downstream
services or other dependencies

299
00:16:06,530 --> 00:16:09,725
to provide correct response in a
test, think about utilizing mocks

300
00:16:09,725 --> 00:16:13,175
and stubs to provide, quick and
predictable responses to your tests.

301
00:16:13,175 --> 00:16:16,775
And there's a ton of different
libraries, modules out there that

302
00:16:16,865 --> 00:16:20,145
integrate with whatever language
you're using that can really take,

303
00:16:20,195 --> 00:16:21,245
some of the heavy lifting here.

304
00:16:21,245 --> 00:16:24,165
So by no means something that you
necessarily have to build out yourself.

305
00:16:24,195 --> 00:16:25,925
this is, is well trodden ground.

306
00:16:26,345 --> 00:16:26,705
And.

307
00:16:27,070 --> 00:16:31,020
Again, in the interest of making
tests truly atomic, we should be

308
00:16:31,020 --> 00:16:33,940
looking to narrow the scope and make
things as deterministic as possible.

309
00:16:33,960 --> 00:16:38,730
So, mocking data is totally reasonable,
in, in, in these circumstances.

310
00:16:39,230 --> 00:16:44,310
And, of course, if you have a database
or API or other dependency that's

311
00:16:44,330 --> 00:16:46,695
acting up and causing things to
break, then we certainly want to, I

312
00:16:46,695 --> 00:16:50,430
don't want tests to surface this, but
that would probably be in the realm

313
00:16:50,430 --> 00:16:55,050
of like your end to end tests, and
specifically I'm more referring to unit

314
00:16:55,050 --> 00:16:56,650
and potentially integration tests here.

315
00:16:57,150 --> 00:17:01,260
Now, finally, you'll want to
structure your test hierarchically.

316
00:17:01,700 --> 00:17:07,030
so you've probably seen this image before,
which I grabbed off Google Image Search.

317
00:17:07,270 --> 00:17:08,270
It's the test pyramid.

318
00:17:08,630 --> 00:17:11,360
and what we have at the bottom
is our unit tests, we have the

319
00:17:11,360 --> 00:17:14,590
integration tests in the middle,
and we have the end to end tests.

320
00:17:15,010 --> 00:17:16,820
Tests at the top.

321
00:17:17,320 --> 00:17:20,150
So at the base, the unit test,
you're testing your individual

322
00:17:20,170 --> 00:17:23,200
components, your functions, your
methods, everything in isolation.

323
00:17:23,200 --> 00:17:27,540
These tests are fine grained, focused,
just like we discussed a moment ago.

324
00:17:27,590 --> 00:17:28,520
They're nice and fast.

325
00:17:28,690 --> 00:17:31,130
They give as much code coverage
as possible and they make

326
00:17:31,130 --> 00:17:33,040
it really easy to identify.

327
00:17:33,505 --> 00:17:37,475
A root cause in the event of a failure,
so unit test should be should be quick

328
00:17:37,475 --> 00:17:39,575
and computationally inexpensive to run.

329
00:17:39,575 --> 00:17:44,165
So ideally, they would cover as
much surface area as possible, and

330
00:17:44,175 --> 00:17:48,425
when they fail, it points to a very
specific method or class that will

331
00:17:48,435 --> 00:17:49,975
really help you to do some debugging.

332
00:17:50,475 --> 00:17:53,525
So your, your middle section is
your integration tests focused on,

333
00:17:53,935 --> 00:17:56,625
you know, different pieces of how,
how your application work together.

334
00:17:56,645 --> 00:18:01,965
And, this one thing talking to a database,
for example, or some microservice or API

335
00:18:01,965 --> 00:18:05,685
or whatever, these integration tests are
much more thorough, and more expensive

336
00:18:05,685 --> 00:18:10,035
as a result, but you, you need Less
of them than your unit tests, although

337
00:18:10,555 --> 00:18:14,165
less tightly scoped than unit tests,
a failure in integration tests should

338
00:18:14,165 --> 00:18:17,865
still point to an interaction between
specific components again, which will

339
00:18:17,865 --> 00:18:22,175
help with debugging and then at the top,
finally, you have your end to end tests.

340
00:18:22,175 --> 00:18:25,805
So replicating entire workflows
in in real world environments to

341
00:18:25,805 --> 00:18:29,265
ensure that the entire application
as a whole is working as it should.

342
00:18:29,745 --> 00:18:33,340
So end to end tests are probably It's
going to be the most expensive and

343
00:18:33,340 --> 00:18:37,750
fragile, probably using something
like Selenium, which, you know,

344
00:18:38,310 --> 00:18:41,100
loves to get stuck waiting for a
page element to appear or become

345
00:18:41,100 --> 00:18:42,610
interactive or something like that.

346
00:18:43,430 --> 00:18:47,700
But, we'll give you that, that,
Representation of a user experience.

347
00:18:48,200 --> 00:18:50,870
So you want to use these
sparingly for critical path stuff.

348
00:18:51,100 --> 00:18:53,890
when one fails, at the very
least, you want to be able to

349
00:18:53,890 --> 00:18:55,870
see where and how it failed.

350
00:18:56,100 --> 00:18:58,780
and from a debugging perspective,
it's going to be, involve a little

351
00:18:58,780 --> 00:19:01,500
more work than perhaps, you know,
looking at a unit test that's failed.

352
00:19:01,910 --> 00:19:03,610
depending on the tool
you're using, of course.

353
00:19:03,880 --> 00:19:07,470
but maybe you debugging tooling to
help with this, and maybe like APM

354
00:19:07,480 --> 00:19:10,360
traces or something like that that
can help provide some context here.

355
00:19:10,860 --> 00:19:14,580
So, like, why, why is the
test pyramid important?

356
00:19:14,640 --> 00:19:20,080
well, a perfectly executed test pyramid,
will mean that everything is being tested,

357
00:19:20,800 --> 00:19:25,470
perfectly, everything's tightly scoped,
little overlap as possible, and when flaky

358
00:19:25,470 --> 00:19:29,910
test behavior occurs, there's as little
noise as possible in your reporting.

359
00:19:29,910 --> 00:19:32,420
It becomes much easier to see
where the problem is occurring,

360
00:19:32,700 --> 00:19:33,415
which is really, really important.

361
00:19:33,715 --> 00:19:38,655
You know, vital for efficient
and effective debugging.

362
00:19:38,845 --> 00:19:43,515
So, for strategy two here, The takeaway
is design your test to be tightly

363
00:19:43,515 --> 00:19:47,395
scoped, performant, predictable,
repeatable, and remove reliance from

364
00:19:47,395 --> 00:19:50,405
other services and dependencies by
mocking and stubbing where possible.

365
00:19:50,405 --> 00:19:54,855
And really, this will help improve
simplicity and, and reduce noise.

366
00:19:55,355 --> 00:19:59,495
Okay, third strategy we're going to
look at is improving the stability

367
00:19:59,495 --> 00:20:00,695
of your testing environment.

368
00:20:00,855 --> 00:20:03,185
so we briefly talked about this.

369
00:20:03,695 --> 00:20:08,585
Touched on the importance of keeping
a clean and a test environment

370
00:20:09,045 --> 00:20:11,905
during some of the previous,
the previous strategies there.

371
00:20:11,915 --> 00:20:14,525
But this is really an important
consideration when it comes

372
00:20:14,525 --> 00:20:15,815
to keeping flaky tests at bay.

373
00:20:15,815 --> 00:20:18,405
So I figured this really
warrants its own section.

374
00:20:19,115 --> 00:20:22,995
Now, a bad testing environment could be
caused by really any number of factors.

375
00:20:23,635 --> 00:20:27,275
You could be maybe using long
running host to build on, and

376
00:20:27,335 --> 00:20:31,725
perhaps someone didn't previously
clean up after a build, and it left.

377
00:20:31,870 --> 00:20:36,380
something there that that's resulting in
in unexpected behavior, or maybe even,

378
00:20:36,700 --> 00:20:39,880
you know, you have multiple jobs running
in series from a single build and one

379
00:20:39,880 --> 00:20:43,340
of the previous jobs didn't clean up
the workspace and the workspace becomes

380
00:20:43,340 --> 00:20:45,360
polluted and that causes strange behavior.

381
00:20:46,040 --> 00:20:48,240
Maybe you have dependencies
that get messy.

382
00:20:48,260 --> 00:20:52,780
Maybe your build infrastructure is being
stood up manually or inconsistently

383
00:20:52,780 --> 00:20:54,590
rather than using a bunch of automation.

384
00:20:54,990 --> 00:20:56,530
perhaps external dependencies.

385
00:20:56,855 --> 00:20:59,835
Not working reliably, or
perhaps the test infrastructure

386
00:20:59,835 --> 00:21:01,165
itself is just not suitable.

387
00:21:01,165 --> 00:21:04,375
It's under resourced, not enough memory
or CPU or network bandwidth or whatever.

388
00:21:04,385 --> 00:21:06,435
I mean, these are just a few examples.

389
00:21:06,435 --> 00:21:10,205
And again, pretty much infinite reasons
why the test environment could be

390
00:21:10,335 --> 00:21:12,655
in a bad way or just not suitable.

391
00:21:12,965 --> 00:21:17,105
But we've got some best practices here
to hopefully avoid some of the common

392
00:21:17,435 --> 00:21:19,825
pitfalls that you might experience.

393
00:21:20,185 --> 00:21:24,615
So you want to strive to
achieve a consistent test setup.

394
00:21:24,615 --> 00:21:27,260
And this one is It's a very obvious one.

395
00:21:27,630 --> 00:21:31,220
You want your test environment to be
predictable and as consistent as possible.

396
00:21:31,500 --> 00:21:33,570
And there's many different ways that
you can do this depending on what

397
00:21:33,570 --> 00:21:37,560
your CI tooling is and what it allows
for you to do and how you design

398
00:21:37,560 --> 00:21:39,240
your infrastructure and whatever.

399
00:21:39,480 --> 00:21:45,100
So, how do I keep my
build environment clean?

400
00:21:45,475 --> 00:21:51,680
So many CI platforms provide the
ability to run CI jobs in a fully

401
00:21:51,900 --> 00:21:54,380
hosted, managed ephemeral environment.

402
00:21:54,805 --> 00:21:58,545
that only live for the duration of the job
and they would be spun up and you run the

403
00:21:58,545 --> 00:22:03,525
job in, in that, that, that, that runner
or, that, that container or environment

404
00:22:03,535 --> 00:22:05,285
or whatever that the CI provider provides.

405
00:22:05,635 --> 00:22:08,955
And then once the job is completed
that, that compute is torn down

406
00:22:08,955 --> 00:22:10,615
and deleted and it's not reused.

407
00:22:10,935 --> 00:22:14,685
So these kind of environments
really eliminate the risk of

408
00:22:14,685 --> 00:22:18,365
workspace being contaminated by
previously run jobs or build.

409
00:22:18,365 --> 00:22:21,815
It's really a nice clean
state for every single job.

410
00:22:22,055 --> 00:22:26,185
so although the environment will be
free from contamination, it's good to

411
00:22:26,195 --> 00:22:29,390
bear in mind that there The piece of
infrastructure itself will only be as

412
00:22:29,390 --> 00:22:31,470
good as its original configuration.

413
00:22:31,480 --> 00:22:35,310
So it might be immune from
previous runs messing it up.

414
00:22:35,320 --> 00:22:38,240
But if the dependencies aren't being
handled properly, then you're still

415
00:22:38,240 --> 00:22:40,350
gonna have a bad time or run into issues.

416
00:22:40,500 --> 00:22:43,460
these kind of managed ephemeral
runners are really useful, really,

417
00:22:43,460 --> 00:22:44,880
in any number of scenarios.

418
00:22:45,835 --> 00:22:48,725
In my opinion, particularly when
you're looking at like Mac or iOS

419
00:22:48,735 --> 00:22:51,515
jobs, I'm definitely like an Apple guy.

420
00:22:51,565 --> 00:22:53,865
I have been for, for many years.

421
00:22:53,915 --> 00:22:57,985
I have countless Macs during this
time, but my experience has really led

422
00:22:57,985 --> 00:23:01,595
me to the conclusion that whilst they
work really great as desktop machines,

423
00:23:01,675 --> 00:23:05,875
there can be a real handful when you
manage them as headless CI runners.

424
00:23:05,895 --> 00:23:09,045
And there's a litany of hurdles
that you need to overcome to get

425
00:23:09,045 --> 00:23:10,475
them to work well in this scenario.

426
00:23:10,525 --> 00:23:14,065
But maintaining a clean, stable
workspace is in, in my opinion, at

427
00:23:14,065 --> 00:23:15,575
least one of the more challenging ones.

428
00:23:15,885 --> 00:23:18,735
you know, when you're self managing
Macs for CI, it's really common to see

429
00:23:18,735 --> 00:23:21,065
machines that are really long running.

430
00:23:21,535 --> 00:23:24,535
You know, rather than kind of that
short lived one and done configuration

431
00:23:24,535 --> 00:23:25,505
that we just mentioned there.

432
00:23:25,815 --> 00:23:29,655
and, and in these scenarios, there would
be some attempt to perhaps clean the user

433
00:23:29,655 --> 00:23:34,005
folder between jobs so that there's like a
relatively clean workspace and, you know,

434
00:23:34,105 --> 00:23:35,735
a predictable environment to run tests.

435
00:23:35,735 --> 00:23:40,105
But this is often quite difficult to
achieve, successfully and effectively.

436
00:23:40,365 --> 00:23:44,145
So being able to delegate the
responsibility for managing Mac, CIM for

437
00:23:44,145 --> 00:23:49,425
entirely, instead of having, you know,
That long running, Mac to have something

438
00:23:49,425 --> 00:23:52,795
that's completely ephemeral that runs, you
know, presumably in a VM somewhere before

439
00:23:52,795 --> 00:23:54,915
being recycled is pretty compelling.

440
00:23:55,345 --> 00:23:59,935
And I'm certain that anyone listening
here who's ever, whether in the past

441
00:23:59,935 --> 00:24:02,785
or currently, is responsible for
maintaining a closet filled with Mac

442
00:24:02,785 --> 00:24:06,875
minis or maybe a bunch of cloud based
Mac instances would definitely agree

443
00:24:06,915 --> 00:24:08,965
that this is, kind of, a brutal job.

444
00:24:09,085 --> 00:24:11,295
And, not having to do
that or worry about that.

445
00:24:11,810 --> 00:24:14,620
This issue on a Mac would,
would be just be fantastic.

446
00:24:14,730 --> 00:24:18,980
So, I am obliged at this point to
mention that at Billkite, we do have

447
00:24:18,980 --> 00:24:20,620
this capability for Linux and Mac.

448
00:24:20,620 --> 00:24:23,120
So, if you're looking for
ephemeral runners, for your CI

449
00:24:23,180 --> 00:24:26,390
jobs fully managed, then we can
certainly help you out with that.

450
00:24:26,890 --> 00:24:31,100
next up at Billkite, we do provide
a hybrid CI solution as well.

451
00:24:31,100 --> 00:24:34,630
So where customers like own and
manage the build infrastructure, you

452
00:24:34,630 --> 00:24:36,130
know, hybrid in the sense that we.

453
00:24:36,300 --> 00:24:39,770
We run the control plane as a SAS
service, and then you manage the compute

454
00:24:39,920 --> 00:24:41,570
yourself wherever you want to deploy it.

455
00:24:41,980 --> 00:24:46,240
and we, we do offer a few out of the
box agent stacks to achieve this using,

456
00:24:46,490 --> 00:24:51,020
you know, Kubernetes or, using EC2
instances, all auto scaled and handles

457
00:24:51,020 --> 00:24:53,360
orchestration and job life cycles for you.

458
00:24:53,360 --> 00:24:56,340
And in both cases, much
like those managed runners.

459
00:24:56,475 --> 00:25:00,465
I mentioned a moment ago, in these
scenarios, CI jobs can be configured

460
00:25:00,495 --> 00:25:04,355
to run on on fresh infra each time
before it's terminated and recycled

461
00:25:04,355 --> 00:25:07,825
so that each, again, each CI job
gets its own clean environment.

462
00:25:07,955 --> 00:25:10,075
I can't speak for other CI
providers on this capability.

463
00:25:10,265 --> 00:25:13,345
I'm, I believe that some
others do, should be possible.

464
00:25:13,345 --> 00:25:15,145
So just take a look at the
docs and see what's available.

465
00:25:15,145 --> 00:25:17,825
Again, I can only vouch
for Billkite in this case.

466
00:25:19,115 --> 00:25:25,295
And whether your infrastructure uses
short lived one and done configuration or

467
00:25:25,295 --> 00:25:29,445
you have a long, long running instances
dedicated to running CI jobs, definitely

468
00:25:29,445 --> 00:25:30,795
consider running things in Docker.

469
00:25:30,865 --> 00:25:34,285
I think it's pretty, you know,
pretty ubiquitous capability for any

470
00:25:34,285 --> 00:25:36,205
modern CI solution that can do this.

471
00:25:36,205 --> 00:25:39,065
And, definitely at Buildkite we
have plugins that you can integrate

472
00:25:39,065 --> 00:25:41,965
into your CI workflows to facilitate
this really nice and easy.

473
00:25:42,355 --> 00:25:45,900
you know, if something is running,
you know, A docker container, you

474
00:25:45,900 --> 00:25:50,250
know, again, it's, it's a real nice
predictable environment to run your tests.

475
00:25:50,250 --> 00:25:53,280
And I was actually just talking
with a customer of ours about

476
00:25:53,280 --> 00:25:54,430
this very, very recently.

477
00:25:54,430 --> 00:25:57,820
There are a very well known grocery
delivery service, and they were

478
00:25:57,820 --> 00:25:59,590
describing how they do just this.

479
00:25:59,590 --> 00:26:01,860
So in that case, jobs are executed.

480
00:26:02,570 --> 00:26:06,110
Using Bilkai, using our Docker
plugin, so that every single job

481
00:26:06,120 --> 00:26:07,640
gets that fresh container to run in.

482
00:26:07,640 --> 00:26:12,590
They have, a single base image that all
of their, their build and test jobs use,

483
00:26:12,590 --> 00:26:15,690
and this, you know, the same base image
as they use in production, in fact.

484
00:26:16,040 --> 00:26:19,450
And then they have a bunch of scripts,
that would run when this container

485
00:26:19,450 --> 00:26:22,270
is orchestrated, depending on, like,
the pipeline that, like, the CI

486
00:26:22,270 --> 00:26:25,770
pipeline that's running, or the The
application is building, and that would

487
00:26:25,770 --> 00:26:29,740
pull in additional dependencies at
runtime to give them what they need.

488
00:26:29,790 --> 00:26:33,150
and this really just gives them that
predictable, consistent, consistent

489
00:26:33,170 --> 00:26:35,170
environment, to run, per job.

490
00:26:35,670 --> 00:26:38,050
Next up, you want to try and
determine which failures are

491
00:26:38,050 --> 00:26:39,300
related to infrastructure.

492
00:26:39,410 --> 00:26:44,520
so, Trying to determine whether a
job failure is being caused by the

493
00:26:44,520 --> 00:26:48,040
environment or the infrastructure
might not always be easy.

494
00:26:48,070 --> 00:26:50,460
You know, these things can manifest
themselves in a number of different ways.

495
00:26:50,460 --> 00:26:52,730
It's often not immediately obvious.

496
00:26:52,730 --> 00:26:56,090
However, you want to keep your eye
out for a few things such as timeouts

497
00:26:56,090 --> 00:27:00,710
and IO errors, you know, tests that
are failing, supposedly touching other

498
00:27:00,710 --> 00:27:04,980
services, or, you know, tests taking
significantly longer between runs.

499
00:27:05,155 --> 00:27:08,825
you know, if there's that huge discrepancy
with latency between test runs, that,

500
00:27:08,845 --> 00:27:12,385
that would certainly indicate that there's
something that's happening, perhaps at

501
00:27:12,385 --> 00:27:14,445
the infrastructure environment level.

502
00:27:15,305 --> 00:27:18,085
And it's pretty much a given that
you'll have some sort of infrastructure

503
00:27:18,105 --> 00:27:20,875
monitoring set up in your environment,
you know, you know, whether it's

504
00:27:20,945 --> 00:27:25,325
Datadog or, or, you know, Elastic
or something that's open source that

505
00:27:25,325 --> 00:27:26,225
you're running to monitor stuff.

506
00:27:26,415 --> 00:27:28,385
system stats and, and, metrics.

507
00:27:28,785 --> 00:27:31,375
But you, what you want to be doing is
using this in your build environment too.

508
00:27:31,375 --> 00:27:34,625
You want to be looking for, for spikes
in various metrics that correlate

509
00:27:34,955 --> 00:27:36,515
to undesirable test behavior.

510
00:27:36,555 --> 00:27:39,565
And, and then this can certainly point
to things like resource constraints.

511
00:27:39,685 --> 00:27:43,135
and then this isn't, this isn't,
By any means, like a golden ticket,

512
00:27:43,145 --> 00:27:46,495
an infallible or conclusive,
way of picking out problems.

513
00:27:46,495 --> 00:27:47,795
But it's certainly worth monitoring.

514
00:27:47,825 --> 00:27:51,065
And certainly when you correlate
behaviors, it can be quite powerful.

515
00:27:51,115 --> 00:27:55,225
and definitely if your observability
tool can can do stuff like, you know, use

516
00:27:55,225 --> 00:28:02,115
custom tags on, metrics that you can tag
containers, or, or instances by job ID and

517
00:28:02,115 --> 00:28:04,295
then, look at the relationship between.

518
00:28:05,025 --> 00:28:09,145
That build, build job by ID and the
metrics associated with that, whether

519
00:28:09,145 --> 00:28:12,095
it's a container or an instance or
whatever, and then, and then see if

520
00:28:12,095 --> 00:28:14,965
you can see that correlation between
failures and, and various different

521
00:28:14,965 --> 00:28:18,505
spikes, then, this is really going to
help you immensely for, for debugging

522
00:28:18,505 --> 00:28:19,805
things like resource constraints.

523
00:28:20,305 --> 00:28:23,825
So, going back to that, that grocery
delivery service that I mentioned earlier,

524
00:28:23,875 --> 00:28:27,610
when they observe a test failure due
to a test bombing out, their default

525
00:28:27,610 --> 00:28:31,950
behavior is they then scan the build
logs, from that job and they look for

526
00:28:31,950 --> 00:28:35,450
signs of failure, that, that, that
implies that it's infrastructure related.

527
00:28:35,450 --> 00:28:38,800
So, like, they have a list of
various different error message

528
00:28:38,800 --> 00:28:40,240
strings that would point to.

529
00:28:40,740 --> 00:28:44,720
This is an infrastructure problem, and
then if this is true, then, that would

530
00:28:44,720 --> 00:28:46,230
go to the developer experience team.

531
00:28:46,230 --> 00:28:48,240
And then if it's something that
looks like it's app shaped, then it

532
00:28:48,240 --> 00:28:50,320
would just go back to the app team
and then they can fix the problem.

533
00:28:50,330 --> 00:28:53,280
So it's a nice way of kind of
separating these things out and

534
00:28:53,280 --> 00:28:54,900
then, assigning responsibility.

535
00:28:55,030 --> 00:28:58,200
So hopefully your CI solution
provides you with the ability to.

536
00:28:58,455 --> 00:29:01,425
You know, perhaps store your logs
and do some, provide a means to

537
00:29:01,425 --> 00:29:05,105
programmatically access them, not
necessarily just through the UI, but via

538
00:29:05,105 --> 00:29:09,225
the API or store them locally or copy
them to an S3 bucket and run a Lambda

539
00:29:09,225 --> 00:29:12,815
function across them or anything to
look for, for, for certain behaviors.

540
00:29:13,125 --> 00:29:14,485
definitely going to be a useful thing.

541
00:29:14,785 --> 00:29:18,395
to be able to identify whether something's
pointing to a system or environment issue.

542
00:29:18,625 --> 00:29:22,795
so finally, Replace external
dependencies with mocks where you can.

543
00:29:22,845 --> 00:29:23,795
And we've already touched on mocks.

544
00:29:23,795 --> 00:29:25,335
I'm not going to go,
burn any time on this.

545
00:29:25,335 --> 00:29:30,635
But, in, in the subject of System
environment, stability and consistency.

546
00:29:30,685 --> 00:29:34,355
we want to remove dependencies on
downstream services wherever possible.

547
00:29:34,355 --> 00:29:36,755
So if we have a test that's
interacting with a downstream

548
00:29:36,755 --> 00:29:38,345
service, use a mock instead.

549
00:29:38,435 --> 00:29:42,055
and it will not only make it
performant, but then, Reduce that,

550
00:29:42,095 --> 00:29:46,215
that, the risk of some unknown
variable, causing your test to flap.

551
00:29:46,715 --> 00:29:50,845
So, the TLDR from this strategy,
a consistent test environment

552
00:29:50,875 --> 00:29:53,465
is crucial for reliability and
there's many ways to achieve this

553
00:29:53,465 --> 00:29:54,515
depending on your requirements.

554
00:29:54,525 --> 00:29:58,440
So you really just need to pick and
choose, what your CI tool provides,

555
00:29:58,490 --> 00:30:01,760
relative to those needs and, and
really lean into that where you can.

556
00:30:02,260 --> 00:30:06,020
So, number four of five,
strategy number four.

557
00:30:06,260 --> 00:30:10,030
You want to level up your test
management and monitoring tools.

558
00:30:10,530 --> 00:30:14,285
So, I'm going to preface this
section by saying that Billkite

559
00:30:14,295 --> 00:30:16,495
has test monitoring built in.

560
00:30:16,505 --> 00:30:20,115
It has our own test monitoring and
management tool called Test Engine.

561
00:30:20,555 --> 00:30:24,575
And this talk is really not intended to be
a sales pitch for the product, although I

562
00:30:24,575 --> 00:30:28,385
will mention it in passing as it relates
to some of the subjects in this section.

563
00:30:28,445 --> 00:30:32,495
So, we would love for you to test,
Test engine out by if your organization

564
00:30:32,495 --> 00:30:35,425
maybe give us some money, but it's
by no means the only tool out there

565
00:30:35,425 --> 00:30:37,145
that provides test visibility.

566
00:30:37,145 --> 00:30:40,885
So, you know, alternatively, you can
just take a look and pick one that

567
00:30:40,885 --> 00:30:44,255
fits your use case budget and has
all of the features that you need.

568
00:30:44,595 --> 00:30:47,155
so I'll be speaking at a high level
here rather than just, talking

569
00:30:47,155 --> 00:30:48,735
about our own thing specifically.

570
00:30:48,955 --> 00:30:53,285
but that said, I mean, I've worked in
this, this Billkite for quite some time.

571
00:30:53,345 --> 00:30:56,075
And prior to the launch of test
engine, the main pain point that

572
00:30:56,075 --> 00:30:57,455
customers encountered was around.

573
00:30:58,050 --> 00:30:58,550
Testing.

574
00:30:58,600 --> 00:31:03,160
So, especially when your engineering
team is large and the code base is also

575
00:31:03,160 --> 00:31:06,370
large and there's tons of tests being
executed, it really becomes challenging

576
00:31:06,700 --> 00:31:08,090
to see the forest for the trees.

577
00:31:08,420 --> 00:31:12,740
you know, as an engineer, you can see
that a test fails for you sometimes,

578
00:31:12,740 --> 00:31:15,970
and then you're mashing retry until it
goes away and everything's green again.

579
00:31:16,320 --> 00:31:20,680
but Does everyone have that same
problem with this same test as you

580
00:31:20,690 --> 00:31:25,520
put yourself in the shoes of the the
DevEx owner, the DevEx leader or really

581
00:31:25,520 --> 00:31:28,740
anyone in the DevEx team, you know,
there's going to be a subset of tests

582
00:31:29,060 --> 00:31:32,300
that are slow and flaky for everyone,
and they're causing, you know, a

583
00:31:32,300 --> 00:31:34,320
significant cumulative impact on both.

584
00:31:34,775 --> 00:31:39,255
Build times and wait times and engineer
productivity, and, and compute spend.

585
00:31:39,585 --> 00:31:43,595
and to be able to really address this,
having appropriate tooling to, to surface

586
00:31:43,595 --> 00:31:47,465
these kinds of issues and monitor them and
assist in debugging and where possible,

587
00:31:47,515 --> 00:31:50,425
you know, help address the problem
is going to be a massive win for you.

588
00:31:50,925 --> 00:31:52,555
So there's a couple of things
you want to be looking for.

589
00:31:53,470 --> 00:31:55,670
In your test monitoring tool.

590
00:31:56,170 --> 00:31:59,330
So, it wants to be able to
integrate with your CI workflows.

591
00:31:59,900 --> 00:32:02,360
so, first of all you want to make
sure it's got broad support for

592
00:32:02,360 --> 00:32:04,250
all of your, required technologies.

593
00:32:04,750 --> 00:32:07,410
Most organizations, especially when
they hit like a certain size, is going

594
00:32:07,410 --> 00:32:08,860
to have a pretty varied tech stack.

595
00:32:08,990 --> 00:32:12,695
And being able to view all of this
together, In a single tool is going

596
00:32:12,695 --> 00:32:16,355
to be invaluable versus, you know,
having separate tools for, you

597
00:32:16,355 --> 00:32:19,565
know, this language and this system
and things like that, everything

598
00:32:19,565 --> 00:32:24,305
together in a single place is going
to be, very, very valuable to you.

599
00:32:24,855 --> 00:32:27,965
And additionally, it wants to be able
to integrate with your CI pipeline.

600
00:32:27,965 --> 00:32:31,615
So, test observability can be
a real rabbit hole and probably

601
00:32:31,955 --> 00:32:36,265
should be a separate function
from your CI workflow UX.

602
00:32:36,365 --> 00:32:39,915
you know, you want your build
logs and, and, and, the, the.

603
00:32:40,355 --> 00:32:44,485
Outcome of the build jobs in one
place and perhaps some additional

604
00:32:44,485 --> 00:32:48,225
tooling to drill into to your test
results adjacent to that, but the two

605
00:32:48,225 --> 00:32:51,195
should be tightly integrated because
that will massively help usability.

606
00:32:51,225 --> 00:32:56,455
So you know, imagine running a test
in your CI workflow and then you see a

607
00:32:56,455 --> 00:32:59,675
failure in the build logs and then you're
able to click through from that build

608
00:32:59,675 --> 00:33:02,905
log into the test failure, which takes
you into a dashboard with debugging data

609
00:33:02,905 --> 00:33:04,885
and run history and things like that.

610
00:33:04,885 --> 00:33:05,995
So it's kind of this.

611
00:33:06,495 --> 00:33:10,865
Slick workflow to go from clicking
run build to looking at the build

612
00:33:10,865 --> 00:33:14,255
results to looking at the failed
test to looking deep dive into the

613
00:33:14,255 --> 00:33:17,565
test history and that debugging data
that's going to be invaluable for you.

614
00:33:18,065 --> 00:33:21,005
So observability, very important.

615
00:33:21,505 --> 00:33:24,785
The problem that most teams are trying
to solve is that, you know, whilst your

616
00:33:24,785 --> 00:33:29,965
CI pipeline probably gives you tons
of logs and metrics and details about

617
00:33:29,965 --> 00:33:33,345
that one build that you're running
currently, it, it probably doesn't.

618
00:33:33,640 --> 00:33:38,670
help you aggregate the test behavior from
many builds and surface common issues.

619
00:33:38,670 --> 00:33:42,490
And, whilst build logs can be super
useful for debugging, they're also

620
00:33:42,490 --> 00:33:45,730
very verbose and kind of a rough data
source for you used for things like

621
00:33:45,750 --> 00:33:49,600
trend analysis and really kind of any
kind of manual analysis at all, really.

622
00:33:50,020 --> 00:33:53,930
So the first thing's first
from your test observability

623
00:33:53,930 --> 00:33:55,190
tool, you'll want dashboards.

624
00:33:55,520 --> 00:33:57,660
you want charts, you want
metrics all over the place.

625
00:33:57,890 --> 00:33:59,990
Like how reliable is my test?

626
00:33:59,990 --> 00:34:00,055
Okay.

627
00:34:00,885 --> 00:34:03,325
My test suite, rather, all
the tests individually.

628
00:34:03,355 --> 00:34:05,975
Like, how long does my
test suite take to execute?

629
00:34:06,075 --> 00:34:08,025
Are there any particular trends
that I should worry about?

630
00:34:08,035 --> 00:34:09,905
Like, what are my worst performing tests?

631
00:34:10,295 --> 00:34:14,175
this is a screenshot I just grabbed from
our own testing tool, and, blanking out

632
00:34:14,175 --> 00:34:17,355
pertinent information, but, showing just,
you know, here's a test, here's a test.

633
00:34:17,625 --> 00:34:20,455
This one's deemed flaky, like,
what's the history of this?

634
00:34:20,595 --> 00:34:22,715
how long is this test
taking to execute over time?

635
00:34:22,715 --> 00:34:26,765
Here I can, I mean, this is perhaps a
bad test, but we can kind of see some

636
00:34:26,765 --> 00:34:29,985
of the behavior of this, if there's
been degradation over time, or if

637
00:34:29,985 --> 00:34:33,535
there's a very particular point in
time where, things started going awry.

638
00:34:34,035 --> 00:34:35,995
So you want to use flaky test detection.

639
00:34:36,165 --> 00:34:38,200
so you should be able
to surface flaky tests.

640
00:34:38,300 --> 00:34:42,270
so at Billkite, we again, we have these
huge customers and they run billions

641
00:34:42,270 --> 00:34:43,750
and billions of tests every year.

642
00:34:43,800 --> 00:34:48,280
and probably the number one things they
want to know from, our test observability

643
00:34:48,280 --> 00:34:49,880
tool is like, which tests are flaky?

644
00:34:50,080 --> 00:34:51,410
How often are they flaky?

645
00:34:51,440 --> 00:34:53,070
And when did they become flaky?

646
00:34:53,410 --> 00:34:57,660
And there's Undoubtedly an infinite
number of ways that an engineering or

647
00:34:57,660 --> 00:35:01,060
could fine tune their tests to make their
builds a little bit more performant But

648
00:35:01,060 --> 00:35:04,180
the most significant improvement that
can be made is is likely identifying

649
00:35:04,180 --> 00:35:07,250
and squashing the bugs And this kind of
specialized tooling will really help with

650
00:35:07,260 --> 00:35:11,460
that and ensure that your team and your
organization has the ongoing visibility

651
00:35:11,460 --> 00:35:12,840
across the issue when it occurs.

652
00:35:13,690 --> 00:35:17,930
So, specialized test focused tooling
should at the very least be able to group

653
00:35:18,580 --> 00:35:23,090
many historic executions given tests so
that But when you identify a particularly

654
00:35:23,110 --> 00:35:26,540
problematic test, you're able to flip
through that history and, and, you know,

655
00:35:26,540 --> 00:35:30,800
see all those historic runs, examine
latency and debugging failures over many

656
00:35:30,800 --> 00:35:32,700
different, invocations of that test.

657
00:35:33,200 --> 00:35:38,280
so you'll want to be able to, assign
responsibility for a bad test to someone.

658
00:35:38,680 --> 00:35:41,870
So, you know, being able to
identify a test as being flaky

659
00:35:41,870 --> 00:35:43,270
and bad is great, but then what?

660
00:35:43,380 --> 00:35:44,910
do you, do you fix it yourself?

661
00:35:45,000 --> 00:35:45,410
Maybe.

662
00:35:45,470 --> 00:35:46,540
is it your responsibility though?

663
00:35:47,040 --> 00:35:49,780
You know, is this, does this fall within
your remit or do you need to go and chase

664
00:35:49,790 --> 00:35:51,520
some, someone else up to go and do it?

665
00:35:51,840 --> 00:35:54,940
so a lot of these, test observability
tools now have this ability

666
00:35:54,940 --> 00:35:57,000
to assign a test to someone.

667
00:35:57,000 --> 00:36:00,210
So, you see a flaky test and now you
can make it somebody else's problem.

668
00:36:00,310 --> 00:36:01,910
if it's their responsibility, of course.

669
00:36:02,020 --> 00:36:05,030
but you would then be able to
take that flaky test and assign

670
00:36:05,030 --> 00:36:06,180
it to a person or a team.

671
00:36:06,685 --> 00:36:10,085
Maybe they get a report or a
notification, maybe when they log in

672
00:36:10,735 --> 00:36:13,545
it will give them some reminders that
this is something they need to fix.

673
00:36:13,545 --> 00:36:18,345
But at the very least it gives team,
product, org wide visibility into who

674
00:36:18,345 --> 00:36:20,515
needs to address this flaky test problem.

675
00:36:21,065 --> 00:36:24,955
and finally, you want to be able to take
those flaky tests and quarantine them.

676
00:36:25,215 --> 00:36:27,055
to prevent pipeline blockages.

677
00:36:27,245 --> 00:36:30,865
so, yeah, some test management tooling,
including our own test engine has this

678
00:36:30,865 --> 00:36:35,395
ability, so you see a, a flaky test and
then you can quarantine it and prevent

679
00:36:35,395 --> 00:36:36,815
it from, from blocking the build.

680
00:36:37,245 --> 00:36:40,585
so this is really just the next step
beyond just having the metrics and

681
00:36:40,595 --> 00:36:42,305
the knowledge that a test is flaky.

682
00:36:42,305 --> 00:36:45,805
Now you can actually do something with
it and use this, leverage this tooling to

683
00:36:45,805 --> 00:36:47,665
make decisions about which tests can run.

684
00:36:48,035 --> 00:36:51,215
now, we're going to touch on, something
about test quarantining in a moment here,

685
00:36:51,215 --> 00:36:56,145
but really being able to proactively
alter test execution based off.

686
00:36:56,365 --> 00:36:59,735
you know, observed performance or
flakiness is kind of a superpower

687
00:36:59,995 --> 00:37:04,925
in these kind of tools and the TLDR
would be the observability is key.

688
00:37:04,955 --> 00:37:07,665
So not having the correct tool
for the job is going to make

689
00:37:07,665 --> 00:37:08,935
your life significantly harder.

690
00:37:08,935 --> 00:37:11,905
And you should really lean into tooling
that not just monitors tests, but

691
00:37:11,905 --> 00:37:16,365
also provides automation and proactive
resolution of flakies when they occur.

692
00:37:16,865 --> 00:37:19,625
Okay, so, final strategy number five here.

693
00:37:19,955 --> 00:37:22,345
you want to foster a reliability.

694
00:37:22,845 --> 00:37:25,955
So here at Buildkite, we spend a lot of
time obsessing about test reliability

695
00:37:26,325 --> 00:37:27,525
and build performance, of course.

696
00:37:27,525 --> 00:37:30,345
So naturally, all of our
tests, they work perfectly.

697
00:37:30,585 --> 00:37:33,625
they never exhibit any flaky
behavior and all of our builds

698
00:37:33,745 --> 00:37:35,925
pass green every single time.

699
00:37:35,925 --> 00:37:39,845
So, of course, that is a
ridiculous, egregious lie.

700
00:37:40,230 --> 00:37:43,370
Just like any other software engineering
team, we have flaky tests and we

701
00:37:43,380 --> 00:37:44,950
have build fails all over the place.

702
00:37:45,230 --> 00:37:47,920
we do dog food, our own products
to ensure we have visibility.

703
00:37:47,920 --> 00:37:50,450
And we try to foster culture
around good test practice.

704
00:37:50,480 --> 00:37:53,730
But in reality, that that's
really easier said than done.

705
00:37:54,080 --> 00:37:55,410
we, we're very lucky.

706
00:37:55,410 --> 00:37:57,700
We have, honestly, some of the
most talented, sophisticated, and

707
00:37:57,700 --> 00:38:01,390
expensive engineering teams, in the
world as customers of Buildkite.

708
00:38:01,970 --> 00:38:06,230
And if they struggle with flaky tests,
I mean, what, what chance do we have?

709
00:38:06,600 --> 00:38:08,130
but you know, the truth is.

710
00:38:08,740 --> 00:38:11,260
You can have the best tooling in the
world at your disposal and the most

711
00:38:11,260 --> 00:38:13,910
sophisticated dev and build environments
and all the money in the world that

712
00:38:13,920 --> 00:38:17,720
you know, you can hire the world class
developers and all of that kind of stuff.

713
00:38:17,980 --> 00:38:21,360
But if you're not building that culture
around test reliability, then then

714
00:38:21,360 --> 00:38:25,260
eventually you'll you too will be, you
know, configuring test jobs with 15 auto

715
00:38:25,260 --> 00:38:28,670
retries to get your builds to finish
green and everyone get mad at you.

716
00:38:28,740 --> 00:38:29,980
And that's a bad thing.

717
00:38:30,470 --> 00:38:30,840
So.

718
00:38:31,340 --> 00:38:35,050
We've retried to instill
that culture at Bilkite.

719
00:38:35,150 --> 00:38:38,950
We've built and evolved
our test engine project on.

720
00:38:39,000 --> 00:38:42,650
The team has retried to live
and breathe test reliability and

721
00:38:42,650 --> 00:38:45,490
again, to kind of rework it into
all of our engineering practices.

722
00:38:46,430 --> 00:38:51,930
So first of all, you want to make
unblocking everyone else a top priority.

723
00:38:52,050 --> 00:38:54,990
So first and foremost, it's
everyone's responsibility to

724
00:38:54,990 --> 00:38:57,160
keep Tests passing reliability.

725
00:38:57,160 --> 00:39:00,620
And if you encounter a test
that's exhibiting flaky behavior,

726
00:39:00,620 --> 00:39:02,050
then just do something about it.

727
00:39:02,150 --> 00:39:06,235
you know, if it's not your test or you
can't fix it, then talk to someone.

728
00:39:06,325 --> 00:39:09,015
you know, you, you would assign
that test to the necessary personal

729
00:39:09,015 --> 00:39:10,425
team and then they can look into it.

730
00:39:10,700 --> 00:39:14,520
and hopefully you have a tool that,
that allows you to do this, something

731
00:39:14,520 --> 00:39:18,730
like, you know, test engine or, or
some other test observability tool,

732
00:39:18,790 --> 00:39:22,310
or, you know, maybe you assign it in,
you know, your, your dedicated tool, or

733
00:39:22,330 --> 00:39:24,190
maybe write a JIRA ticket or whatever.

734
00:39:24,470 --> 00:39:29,000
just, just action when you see,
a flaky test, you know, that old

735
00:39:29,000 --> 00:39:32,390
saying that you see in the subways,
if you see something, say something.

736
00:39:32,470 --> 00:39:34,510
well, this is true for,
for testing as well.

737
00:39:34,540 --> 00:39:35,620
Live, live by that.

738
00:39:36,050 --> 00:39:39,290
so if you see, you log in perhaps and
you, you see that tests are broken and

739
00:39:39,290 --> 00:39:42,470
they're assigned to you, then make it your
business to, to prioritize fixing that.

740
00:39:42,480 --> 00:39:43,220
Help your colleagues out.

741
00:39:43,360 --> 00:39:44,830
They will really appreciate you for it.

742
00:39:45,330 --> 00:39:48,600
So quarantining tests is fine and good.

743
00:39:48,710 --> 00:39:49,870
In fact, great.

744
00:39:50,050 --> 00:39:52,010
But then it's important that
you do something after that.

745
00:39:52,020 --> 00:39:56,140
So, if a test is flaky or broken, then you
might have that ability to quarantine it.

746
00:39:56,170 --> 00:40:00,210
And it's good because it removes
it from the test suite run, and

747
00:40:00,210 --> 00:40:02,960
it means that the build's passed
and everyone's happy about that.

748
00:40:02,960 --> 00:40:03,600
Everything's green.

749
00:40:04,150 --> 00:40:04,770
but then what?

750
00:40:05,380 --> 00:40:09,410
Like, the thing that the test was testing
is now untested, which is not great.

751
00:40:09,460 --> 00:40:12,650
So Quick test quarantining
should be a temporary measure.

752
00:40:12,700 --> 00:40:15,310
once you've established that a test is
problematic and should be quarantined,

753
00:40:15,310 --> 00:40:19,000
it's probably time to fix it and then
alternatively, you know, make a decision

754
00:40:19,220 --> 00:40:20,490
whether it should be kept around at all.

755
00:40:20,990 --> 00:40:23,020
when necessary, swarm.

756
00:40:23,610 --> 00:40:28,430
So, there have been several, maybe,
Perhaps even more than several occasions

757
00:40:28,430 --> 00:40:31,480
when, Billkite has seen big swathes of
red through all of our test results.

758
00:40:31,670 --> 00:40:34,800
Everything's failing, and to the
point where it can't just be ignored.

759
00:40:34,800 --> 00:40:39,870
And on these occasions, the team will
down tools and swarm on the flaky tests

760
00:40:39,870 --> 00:40:41,170
to try and get these problems addressed.

761
00:40:41,180 --> 00:40:44,740
So, typically, this works really
well, and enough progress is made in

762
00:40:44,740 --> 00:40:46,060
a relatively short period of time.

763
00:40:46,060 --> 00:40:49,630
The things turn green again,
things get moving, and, we can

764
00:40:49,650 --> 00:40:50,650
all kind of move on from it.

765
00:40:51,260 --> 00:40:54,355
but obviously this is not A great
strategy to be used day to day.

766
00:40:54,355 --> 00:40:55,325
It's disruptive.

767
00:40:55,325 --> 00:41:00,985
It means that everyone has to down tools
and, and pivot to looking at tests on

768
00:41:01,025 --> 00:41:04,585
takes everyone away from that, that, you
know, the important work of, shipping

769
00:41:04,585 --> 00:41:07,235
features and providing shareholder value.

770
00:41:07,235 --> 00:41:11,965
So, in these cases that swarming on
test is a really good break glass

771
00:41:11,975 --> 00:41:15,145
tool that you can use to, fix problems
when things have really gone awry.

772
00:41:15,455 --> 00:41:20,065
but, it's probably not like a day to
day, solution for addressing tests.

773
00:41:20,565 --> 00:41:23,495
and finally, you want to
avoid silos of information.

774
00:41:23,555 --> 00:41:25,895
Fixing tests is not always
straightforward, and depending on the

775
00:41:25,895 --> 00:41:29,735
nature of the failure, it could be buried
deep down in the stack, in some library

776
00:41:29,735 --> 00:41:34,695
that's being used, it might require some
specialist knowledge to debug, and really

777
00:41:34,695 --> 00:41:37,695
engineers like anyone else on the planet
learn from experience, and you know,

778
00:41:37,695 --> 00:41:42,265
if someone gains experience debugging,
An obscure issue, or maybe not even an

779
00:41:42,275 --> 00:41:43,775
obscure issue, just any issue at all.

780
00:41:43,845 --> 00:41:47,095
And they can apply that knowledge in the
future and address it to future, bugs

781
00:41:47,105 --> 00:41:48,425
that they're, they're trying to fix.

782
00:41:48,435 --> 00:41:51,205
So, we want to really give
those engineers that experience.

783
00:41:51,245 --> 00:41:55,055
ensure the team gets exposed to
debugging tests throughout the stack

784
00:41:55,055 --> 00:41:59,550
and make it, a priority to document
this stuff right You know, write ups in

785
00:41:59,550 --> 00:42:03,630
internal CMSs and knowledge bases and
think about how you can mob on test to

786
00:42:03,630 --> 00:42:05,250
share the information and experience.

787
00:42:05,250 --> 00:42:08,510
And, as with anything in software
engineering, it's very important to just

788
00:42:08,530 --> 00:42:11,100
avoid knowledge silos wherever possible.

789
00:42:11,150 --> 00:42:16,330
because that's gonna make it significantly
harder and prolonged to, to debug

790
00:42:16,360 --> 00:42:18,530
these complex and, and obscure.

791
00:42:19,120 --> 00:42:19,900
test failures.

792
00:42:19,970 --> 00:42:25,370
So the final TLDR here is, the people
and processes are as important as the

793
00:42:25,370 --> 00:42:29,950
tools, making, fixing tests a priority
for organization is going to be key

794
00:42:30,040 --> 00:42:33,030
to removing flakies from the equation.

795
00:42:33,530 --> 00:42:34,040
So

796
00:42:34,540 --> 00:42:37,950
that's my five strategies for helping
getting a handle on flaky tests.

797
00:42:38,060 --> 00:42:44,000
just to list them again, we have, figure
out which tests are flaky to begin with.

798
00:42:44,500 --> 00:42:48,410
Optimize your test design, work
on environment stability, use

799
00:42:48,410 --> 00:42:52,310
the best tooling for the job,
and foster a reliability culture.

800
00:42:52,910 --> 00:42:56,270
Now, maybe you are doing
some of these things already.

801
00:42:56,330 --> 00:42:59,150
I mean, maybe you're doing all of these
things and you don't have any flakier test

802
00:42:59,150 --> 00:43:01,730
at all, in which case, I don't really know
where you're watching this presentation,

803
00:43:01,730 --> 00:43:05,600
but thank you for for tuning it anyway,
but whatever the case, I really hope

804
00:43:05,600 --> 00:43:10,310
this, this has been helpful, useful,
insightful, maybe thought provoking.

805
00:43:10,380 --> 00:43:11,500
and, if you're.

806
00:43:11,910 --> 00:43:14,290
Team is actually struggling
with flaky tests.

807
00:43:14,330 --> 00:43:18,440
then I can confidently, absolutely assure
you that you're, you're not on your

808
00:43:18,440 --> 00:43:19,750
own and you're in very good company.

809
00:43:19,750 --> 00:43:23,370
And again, I work with many world
class engineering organizations

810
00:43:23,370 --> 00:43:26,610
as part of my role at Billkite and
testing is a struggle for all of them.

811
00:43:26,700 --> 00:43:30,300
And really that's why we've built
this kind of tooling to help folks

812
00:43:30,340 --> 00:43:34,560
understand, monitor, debug, and
ultimately mitigate test issues.

813
00:43:34,650 --> 00:43:34,920
So.

814
00:43:35,420 --> 00:43:40,630
Just to close this out, I want to say
a big thank you to the Conf42 team

815
00:43:40,630 --> 00:43:42,050
for putting this whole event together.

816
00:43:42,410 --> 00:43:46,190
And I'd really like to encourage
you to pop along to Billkite.

817
00:43:46,190 --> 00:43:47,050
com and check us out.

818
00:43:47,050 --> 00:43:50,430
We have a world class CICD
platform focused on scalability,

819
00:43:50,440 --> 00:43:52,430
security, and flexibility.

820
00:43:52,730 --> 00:43:54,690
We have a test observability project.

821
00:43:55,615 --> 00:43:58,275
test engine, which will help you
solve as many of these, these

822
00:43:58,275 --> 00:44:00,225
issues, as, as I described today.

823
00:44:00,725 --> 00:44:02,985
and we also have a hyper
scalable package management and

824
00:44:02,985 --> 00:44:04,265
distribution platform as well.

825
00:44:04,295 --> 00:44:05,545
So definitely check that out.

826
00:44:05,645 --> 00:44:10,150
and if you'd like to, join the
likes of Uber, Slack, Shopify, Blog,

827
00:44:10,170 --> 00:44:13,070
Pinterest, and many, many others.

828
00:44:13,330 --> 00:44:14,690
just, you know, just to name a few.

829
00:44:14,940 --> 00:44:15,930
Then head up to buildkite.

830
00:44:15,930 --> 00:44:18,440
com, sign up for a free trial, and
come and talk to us about this stuff.

831
00:44:18,470 --> 00:44:22,590
we're looking forward to chatting
to you about your test problems.

832
00:44:22,740 --> 00:44:24,670
And with that, I'd like to say
thank you very much, everyone.

833
00:44:24,860 --> 00:44:25,540
Thanks for tuning in.

