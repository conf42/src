1
00:00:00,500 --> 00:00:01,069
Speaker 49: Hi everyone.

2
00:00:01,729 --> 00:00:05,359
Thank you for joining me today
I'll be talking about privacy by

3
00:00:05,359 --> 00:00:07,459
design for DevOps and data systems.

4
00:00:08,360 --> 00:00:13,190
I'm excited to show you how we can build
secure and privacy respecting systems

5
00:00:13,280 --> 00:00:16,250
without sacrificing usable user usability.

6
00:00:16,750 --> 00:00:17,950
My name is Vivic Chidi.

7
00:00:18,400 --> 00:00:22,540
I'm a data engineer and I focus on
privacy centric data architectures

8
00:00:22,570 --> 00:00:23,560
and real time streaming.

9
00:00:24,060 --> 00:00:28,890
I've spent many years working on privacy
aware systems and infrastructure, and

10
00:00:28,890 --> 00:00:35,160
today at Con 42 DevOps 2026, we'll
dig into where privacy, compliance

11
00:00:35,160 --> 00:00:39,240
and modern DataOps and DevOps
data engineering actually meet.

12
00:00:39,740 --> 00:00:42,920
This goes way beyond
just ticking some boxes.

13
00:00:43,535 --> 00:00:46,595
It's about rethinking how we build
our systems from the ground up.

14
00:00:47,555 --> 00:00:52,655
And as a quick disclaimer, this talk is
no way affiliated with the company that

15
00:00:52,655 --> 00:00:58,865
I work for, and nothing presented here
reflects my company's internal systems.

16
00:00:59,765 --> 00:01:02,555
I'm just sharing what I've
learned from my own experience

17
00:01:03,125 --> 00:01:04,505
working with data and privacy.

18
00:01:05,005 --> 00:01:06,300
So why does this matter?

19
00:01:06,800 --> 00:01:06,950
No.

20
00:01:06,950 --> 00:01:10,760
Right now there are more than 140
countries that have active privacy

21
00:01:10,760 --> 00:01:17,000
laws, and the average cost of data
breach hit about $5 million in 2024,

22
00:01:17,630 --> 00:01:19,400
and that's the latest data that we have.

23
00:01:19,910 --> 00:01:23,240
And $5 million is a pretty big number.

24
00:01:23,990 --> 00:01:26,870
And here's something
that's even more alarming.

25
00:01:27,370 --> 00:01:31,960
Researchers can re-identify
about 87% of Americans.

26
00:01:32,830 --> 00:01:35,950
From just the zip code
date of birth and gender.

27
00:01:36,790 --> 00:01:39,330
And this is not some distance theory.

28
00:01:39,330 --> 00:01:41,040
This is actually happening right now.

29
00:01:41,580 --> 00:01:43,290
Privacy isn't optional anymore.

30
00:01:43,470 --> 00:01:47,070
It's a hard requirement for
any serious DevOps data system,

31
00:01:47,790 --> 00:01:50,490
both technically for businesses.

32
00:01:50,990 --> 00:01:52,755
So now what is privacy wear design?

33
00:01:53,255 --> 00:01:58,115
Privacy by design is all about building
privacy protections straight into your

34
00:01:58,145 --> 00:01:59,885
architecture right from the start.

35
00:02:00,335 --> 00:02:05,045
So you're not trying to bolt it on at the
end, you're starting it from the get go.

36
00:02:05,525 --> 00:02:09,935
And for DevOps developers, that means
layering privacy controls at every stage.

37
00:02:10,325 --> 00:02:14,765
Of the data pipeline from collecting data
on the client side to storage backends,

38
00:02:14,795 --> 00:02:19,315
to analytics, to processing, to deploying
pipelines, you're ensuring that there

39
00:02:19,315 --> 00:02:21,235
is no data that is getting stored.

40
00:02:21,235 --> 00:02:24,515
There's no private private data
that's getting stored, and privacy

41
00:02:24,515 --> 00:02:26,135
needs to be baked from the get go.

42
00:02:26,385 --> 00:02:29,144
Let's talk about core
privacy by design principles.

43
00:02:29,834 --> 00:02:32,144
So there are four privacy principles
that I want to talk about.

44
00:02:32,804 --> 00:02:34,244
The first one is be proactive.

45
00:02:35,220 --> 00:02:38,579
So you anticipate and prevent
privacy issues before they pop up.

46
00:02:39,079 --> 00:02:41,480
And the next thing is default to privacy.

47
00:02:41,980 --> 00:02:46,239
Give the users maximum protection
automatically without needing

48
00:02:46,239 --> 00:02:47,739
them to change their settings.

49
00:02:48,219 --> 00:02:54,099
So if you are asking for users to
change things so that they become

50
00:02:54,099 --> 00:02:57,369
private, or their data becomes private,
you're not doing something right.

51
00:02:57,869 --> 00:03:00,719
Third thing is make privacy
a part of the design itself.

52
00:03:01,139 --> 00:03:06,089
So you're not doing privacy as a last
minute add-on to check off a few boxes,

53
00:03:06,089 --> 00:03:10,559
but from the get go analyzing what data
should be collected, what data should not

54
00:03:10,559 --> 00:03:15,259
be collected, whether a data collection,
would make sense for your product.

55
00:03:15,559 --> 00:03:20,299
But more than that, whether that collected
data would cause any privacy issues,

56
00:03:20,569 --> 00:03:22,969
should that data get into the wrong hands.

57
00:03:23,329 --> 00:03:24,649
So try to collect as little.

58
00:03:24,754 --> 00:03:26,014
Data as you possibly can.

59
00:03:26,014 --> 00:03:27,664
And finally, full functionality.

60
00:03:28,054 --> 00:03:31,894
So not only are you collecting little
data, but while you are collecting

61
00:03:31,894 --> 00:03:35,074
little data, are your apps still working?

62
00:03:35,164 --> 00:03:37,194
Are your DevOps pipeline still working?

63
00:03:37,494 --> 00:03:38,514
Like, how can you do that?

64
00:03:39,474 --> 00:03:42,604
So let's talk about how this is possible.

65
00:03:43,104 --> 00:03:45,894
So the first one is, let's talk
about anonymization models.

66
00:03:46,404 --> 00:03:52,404
So key anonymity makes sure that every
record in your data sets blends in with

67
00:03:52,404 --> 00:03:55,134
at least K minus one other records.

68
00:03:55,294 --> 00:03:59,624
For example typical case 50
for, decent private systems.

69
00:03:59,624 --> 00:04:04,304
So if your data looks just
like at least 49 others.

70
00:04:05,034 --> 00:04:07,914
Then you are meeting care
anonymity, and this works well

71
00:04:07,914 --> 00:04:12,224
for user analytics especially
when you do demographic analytics.

72
00:04:12,444 --> 00:04:14,844
But you don't want to
expose individual data.

73
00:04:15,344 --> 00:04:17,204
And the second one is L diversity.

74
00:04:17,534 --> 00:04:20,834
This one takes it a bit more
further, so it makes sure that

75
00:04:20,864 --> 00:04:25,434
even within those groups there
is a mix of sensitive attributes.

76
00:04:25,834 --> 00:04:28,264
And this is crucial for stuff
like healthcare data where you

77
00:04:28,264 --> 00:04:32,374
don't want anyone in a group
sharing the same diagnosis.

78
00:04:32,874 --> 00:04:36,054
And let's talk about how
this can be put in practice.

79
00:04:36,304 --> 00:04:41,134
Differential privacy, is a little
mathematical, but by adding carefully

80
00:04:41,164 --> 00:04:45,154
calibrated noise to your query results,
you make sure that no one can tell if

81
00:04:45,154 --> 00:04:47,134
a specific person's data is in the mix.

82
00:04:47,504 --> 00:04:52,244
Epsilon data between 0.1 and one give you
strong privacy without ruining your stats.

83
00:04:52,334 --> 00:04:55,484
And the main idea here is you
can't pick out individuals, but

84
00:04:55,484 --> 00:04:56,954
you still see overall trends.

85
00:04:57,774 --> 00:05:00,294
Give DevOps has some,
great tools for this.

86
00:05:00,294 --> 00:05:01,614
Differential privacy.

87
00:05:01,764 --> 00:05:03,384
You can use nor js to implement this.

88
00:05:03,384 --> 00:05:05,814
And you can even do local
differential privacy in the browser.

89
00:05:06,274 --> 00:05:09,604
This way you get useful analytics
and your privacy stays protected.

90
00:05:10,104 --> 00:05:13,734
Let's talk about sdo, randomization
and tokenization as a next slide.

91
00:05:14,644 --> 00:05:17,554
So pseudo randomization or
tokenization are essential when

92
00:05:17,554 --> 00:05:18,844
you deal with personal data.

93
00:05:19,264 --> 00:05:23,974
So pseudo anonymization swaps out
identifying fields for pseudonyms,

94
00:05:24,334 --> 00:05:28,264
so you keep the links you need for
analytics, but hide real identities

95
00:05:29,014 --> 00:05:30,719
and tokenization takes us further.

96
00:05:31,229 --> 00:05:35,839
It replaces sensitive data with
the random tokens and stores real

97
00:05:35,839 --> 00:05:37,549
stuff in a separate secure world.

98
00:05:38,119 --> 00:05:41,359
So you are delineating your data
from one system to another, that

99
00:05:41,359 --> 00:05:42,889
everything is not sitting in one place.

100
00:05:42,889 --> 00:05:46,459
So somebody has to gain this additional
access to go bring that data together.

101
00:05:46,889 --> 00:05:50,309
So your analytics will never touch
actual PII, but you can still get

102
00:05:50,309 --> 00:05:52,589
insights and if you ever need to.

103
00:05:52,969 --> 00:05:56,659
You re-identify the data, then you know
you can go back and join that data.

104
00:05:56,659 --> 00:05:59,479
And you only do that for legitimate
reasons like customer support.

105
00:05:59,979 --> 00:06:03,149
And then let's talk about
a few advanced techniques.

106
00:06:03,529 --> 00:06:04,699
So if you really wanna to.

107
00:06:05,359 --> 00:06:07,009
Cutting edge privacy.

108
00:06:07,309 --> 00:06:09,139
Look at home morphic encryption.

109
00:06:09,199 --> 00:06:11,779
It lets you run computations
or encrypted data.

110
00:06:11,809 --> 00:06:13,249
You don't need to decrypt anything.

111
00:06:13,669 --> 00:06:19,039
So imagine running a machine learning
model that stay accurate up to 95% while

112
00:06:19,039 --> 00:06:21,799
the data itself never gets exposed.

113
00:06:22,069 --> 00:06:24,499
And the second one is secure
multi-party computation.

114
00:06:24,499 --> 00:06:29,149
This is basically your sharing data
to another system where that system.

115
00:06:29,804 --> 00:06:33,044
Processes a bunch of data and your system
processes a bunch of data, and then

116
00:06:33,044 --> 00:06:35,664
you're sharing that across the board.

117
00:06:36,114 --> 00:06:37,319
And then let's talk about.

118
00:06:37,819 --> 00:06:40,504
Integrating TPIs into
DevOps pipelines, right?

119
00:06:40,684 --> 00:06:43,834
So data production and impact
assessments, data production, impact

120
00:06:43,834 --> 00:06:46,554
assessments, DPIs is a continuous process.

121
00:06:46,554 --> 00:06:50,174
It's not a one-time exercise and you
embed these directly into your CI

122
00:06:50,174 --> 00:06:52,034
ICD pipelines into your workflows.

123
00:06:52,284 --> 00:06:53,124
Contact automatic.

124
00:06:53,274 --> 00:06:56,964
Privacy scans and ensure that
there's no PIN sensitive data that's

125
00:06:56,964 --> 00:06:58,584
getting pushed into code commits.

126
00:06:58,874 --> 00:07:02,904
You make sure that your pipelines
are gated by requiring DPI

127
00:07:02,904 --> 00:07:06,624
approval before deploying changes
that process, personal data.

128
00:07:07,434 --> 00:07:10,764
And you track these privacy metrics
alongside your performance and

129
00:07:10,764 --> 00:07:12,144
security on a continuous mode.

130
00:07:12,424 --> 00:07:16,024
So you surface that on a daily level to
find out what instances are occurring,

131
00:07:16,024 --> 00:07:20,354
how many, accidental privacy pushes
are happening into your pipelines.

132
00:07:20,444 --> 00:07:20,474
Okay.

133
00:07:20,974 --> 00:07:25,744
And, the actionable architecture for
privacy versus systems is minimize the

134
00:07:25,744 --> 00:07:28,984
data, collect only what's necessary,
I'm recapping what we discussed so far.

135
00:07:28,984 --> 00:07:32,824
So collect only what's necessary and
encrypt the data wherever possible.

136
00:07:33,544 --> 00:07:36,664
Do role-based access, control
and give access to the people

137
00:07:36,664 --> 00:07:38,194
only who deserve it and need it.

138
00:07:38,194 --> 00:07:42,064
Not deserve it per se, but need it, and
finally, there is an automated compliance.

139
00:07:42,064 --> 00:07:43,744
You're not manually checking this stuff.

140
00:07:43,744 --> 00:07:46,624
It's happening automatically by itself.

141
00:07:47,304 --> 00:07:48,864
That's about it for this talk.

142
00:07:48,864 --> 00:07:50,514
I really appreciate you listening to this.

143
00:07:50,574 --> 00:07:53,124
Thank you so much and good
luck in building secure

144
00:07:53,124 --> 00:07:54,654
Privacy Compliance Systems.

145
00:07:55,014 --> 00:07:55,584
Have a great day.

146
00:07:55,944 --> 00:07:55,974
Okay.

