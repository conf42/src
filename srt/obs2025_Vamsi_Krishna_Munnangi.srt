1
00:00:00,500 --> 00:00:01,370
Hi everyone.

2
00:00:01,670 --> 00:00:04,430
Welcome to conference
42 Observability 2025.

3
00:00:04,459 --> 00:00:07,660
My name is VMs Kris and I work
as a senior software engineer at

4
00:00:07,660 --> 00:00:10,330
Walmart, where I primarily work
in infrastructure engineering.

5
00:00:11,260 --> 00:00:15,190
And today I'll be presenting
on cloud native observability,

6
00:00:15,730 --> 00:00:19,330
building visible, resilient,
and scalable enterprise systems.

7
00:00:19,600 --> 00:00:23,529
So this talk dives into how advanced
monitoring analytics transform

8
00:00:23,529 --> 00:00:26,169
enterprise systems in modern day.

9
00:00:27,025 --> 00:00:28,495
Cloud native architectures.

10
00:00:28,554 --> 00:00:32,334
So this is a topic which is close to
my work on AI driven API Resilience.

11
00:00:32,394 --> 00:00:36,894
So the next 30 minutes, we'll explore
the shift from traditional monitoring

12
00:00:36,894 --> 00:00:41,575
to observability, real world metrics,
the pillars of observability and

13
00:00:41,575 --> 00:00:43,614
practical implementation strategies.

14
00:00:43,675 --> 00:00:44,905
So let's get started.

15
00:00:45,405 --> 00:00:48,645
So what it takes from monitoring
terms, ability, let's begin

16
00:00:48,645 --> 00:00:50,355
with the fundamental comparison.

17
00:00:50,535 --> 00:00:53,535
Traditional monitoring
focuses on predefined metrics.

18
00:00:53,565 --> 00:00:57,675
So think about an CPU stage or
a request counts, which are a

19
00:00:57,675 --> 00:01:00,435
limited, have a limited context.

20
00:01:00,935 --> 00:01:06,885
So now we need to, which collects
displays and, data from specific sources.

21
00:01:06,945 --> 00:01:12,195
It often creates silos where teams
see disconnected components leading to

22
00:01:12,195 --> 00:01:18,935
reactive fixes after users are impacted,
reacts to predefined alerts or thresholds

23
00:01:18,935 --> 00:01:22,145
notifying users when something is a mess.

24
00:01:22,275 --> 00:01:25,635
Contract to this, we have a
cloud native observability.

25
00:01:26,490 --> 00:01:32,130
It gives us a holistic view through
metrics, logs, traces, unifying

26
00:01:32,130 --> 00:01:34,230
visibility across microservices.

27
00:01:34,590 --> 00:01:40,520
This enables proactive detection, so just
imagine a catching a latency spike just

28
00:01:40,520 --> 00:01:45,470
before it affects users and faster root
cause analysis, which is critical and

29
00:01:45,470 --> 00:01:50,960
distributed systems across the industry
with this kind of high volume traffic.

30
00:01:51,260 --> 00:01:52,940
So what is the relation between.

31
00:01:53,535 --> 00:01:55,995
The observability and monitoring.

32
00:01:56,085 --> 00:02:01,185
So observability can seen as an extension
of monitoring where monitoring provides

33
00:02:01,185 --> 00:02:05,085
the data, and observability provides
a deeper insights and understanding.

34
00:02:05,445 --> 00:02:10,065
Monitoring is a necessary component
of observability, but observability

35
00:02:10,065 --> 00:02:15,155
goes beyond the scope of traditional
monitoring practices enables us

36
00:02:15,425 --> 00:02:20,105
for proactive investigation and
resolution of issues by analyzing them.

37
00:02:20,605 --> 00:02:23,515
As a system behavior and
identifying root causes.

38
00:02:23,995 --> 00:02:27,385
In essence, monitoring tells
you when something is wrong,

39
00:02:27,745 --> 00:02:32,150
but while observability tells
you why and how to fix that.

40
00:02:32,650 --> 00:02:36,190
So now let's see some real
world implementation metrics.

41
00:02:36,829 --> 00:02:39,019
So these are some tangible results.

42
00:02:39,019 --> 00:02:44,599
Practical implementation has delivered a
40% faster resolution time for incidents,

43
00:02:45,019 --> 00:02:47,929
which is down from hours to minutes.

44
00:02:48,409 --> 00:02:54,049
Thanks to the correlated data and
studies show that 25% re resource

45
00:02:54,049 --> 00:02:59,959
efficiency is achieved by optimizing
infrastructure utilization directly

46
00:02:59,959 --> 00:03:03,349
impacting 35% of database load reduction.

47
00:03:03,950 --> 00:03:05,244
And what about the detection speed?

48
00:03:05,744 --> 00:03:10,964
It's almost around 200 millisecond
milliseconds, which is near time.

49
00:03:11,464 --> 00:03:16,749
An identification and it supports
58% faster incident detection goal.

50
00:03:17,290 --> 00:03:22,689
These metrics come from production
systems handling complex scenarios

51
00:03:22,719 --> 00:03:27,149
during Thanksgiving, black Friday,
and holiday traffic peak times.

52
00:03:27,599 --> 00:03:30,929
So these stats rely on tools
which are pulled up from

53
00:03:31,079 --> 00:03:33,959
Dynatrace and GCP Stack Driver.

54
00:03:34,459 --> 00:03:35,239
So moving on.

55
00:03:35,239 --> 00:03:36,619
Next we have.

56
00:03:37,129 --> 00:03:39,349
Few pillars for observability.

57
00:03:39,409 --> 00:03:42,439
So let's observe this triangle.

58
00:03:42,709 --> 00:03:48,649
Every small triangle is important to get
the details from an atomic level to an

59
00:03:48,649 --> 00:03:50,929
holistic point of view of an application.

60
00:03:51,469 --> 00:03:57,439
So observability, rest on these three
pillars, metrics, logs and traces.

61
00:03:57,859 --> 00:03:58,939
Water metrics.

62
00:03:58,999 --> 00:04:00,829
So metrics are quantifiable.

63
00:04:01,504 --> 00:04:06,394
So think about a latency or an error
rates that are tracked in real time.

64
00:04:06,934 --> 00:04:08,314
So these are your systems.

65
00:04:08,314 --> 00:04:11,734
Vital signs, logs are timestamped.

66
00:04:11,734 --> 00:04:17,554
Records of events and errors offering a
detailed audit trial, for instance, in a

67
00:04:17,614 --> 00:04:22,714
microservice failure, logs pinpoint to the
exact state change and also pinpoint to

68
00:04:22,714 --> 00:04:26,194
the exact line of code which went wrong.

69
00:04:26,484 --> 00:04:33,104
And moving onto the traces, these are like
these map from end to end request path

70
00:04:33,104 --> 00:04:35,504
revealing dependencies, broad bottlenecks.

71
00:04:35,504 --> 00:04:41,744
So in the distributed setup, a single
request might span around to 10 services.

72
00:04:41,864 --> 00:04:47,624
So these traces help us see the
full picture in API Resiliency work.

73
00:04:47,624 --> 00:04:52,754
Prometheus is used for metrics
ELK, four logs and JA for traces.

74
00:04:53,304 --> 00:04:59,844
This thread enables to proactively manage
85% faster issue resolution, which is a

75
00:04:59,844 --> 00:05:03,265
key metric for enterprise scalability.

76
00:05:03,765 --> 00:05:09,465
And now let's see how it behaves in a
stateless was a stateful services I.

77
00:05:09,965 --> 00:05:14,555
We need to differentiate between the
observability in these two services.

78
00:05:14,705 --> 00:05:15,455
So what is it?

79
00:05:15,455 --> 00:05:16,895
Stateless services.

80
00:05:17,555 --> 00:05:21,215
Think about load balanced
APIs, which focus on throughput

81
00:05:21,275 --> 00:05:24,215
latency, andal resource use.

82
00:05:25,085 --> 00:05:28,775
So we monitor these with
horizontal scaling in mind.

83
00:05:29,015 --> 00:05:33,784
So the benefits of these are simplified,
scaling, easy to troubleshoot

84
00:05:34,395 --> 00:05:36,135
in case of an individual issues.

85
00:05:36,465 --> 00:05:40,244
So now, whereas coming to the
state, full services like databases,

86
00:05:40,754 --> 00:05:45,104
which require tracking persistence,
metrics like replication, health

87
00:05:45,135 --> 00:05:47,655
consistency, and storage performance.

88
00:05:47,895 --> 00:05:52,514
So for an example, in Google Cloud
Spanner, we ensure that, the data

89
00:05:53,025 --> 00:05:54,914
deliverability is across multi regions.

90
00:05:55,454 --> 00:06:00,494
The benefits of this are users
experience continuity in that usage.

91
00:06:01,260 --> 00:06:05,340
It's a rich personalization and
potentially more complex features.

92
00:06:05,340 --> 00:06:10,999
When all these get added and coming
to observability strategies, they

93
00:06:10,999 --> 00:06:15,469
may vary notably between stateless
and stateful services due to their

94
00:06:15,469 --> 00:06:20,109
differing approaches to the state
management stateless services, which

95
00:06:20,109 --> 00:06:24,459
are easier to scale demand, greater
focus on tracing individual requests.

96
00:06:25,194 --> 00:06:30,564
And analyzing errors in contrast to the
state full services, which require a

97
00:06:30,564 --> 00:06:34,839
broader view that captures persistent
state and ensures session continuity.

98
00:06:35,339 --> 00:06:40,139
And now the shared patterns include
health probes and anomaly deduction.

99
00:06:40,169 --> 00:06:46,109
Implementing of these to balance
the stateless API performance with

100
00:06:46,109 --> 00:06:52,919
stateful database reliability, it'll
support us in 25% of latency reduction.

101
00:06:53,219 --> 00:06:58,349
So this dual approach is critical
for hybrid cloud native systems,

102
00:06:58,849 --> 00:07:01,879
and then comes the distributed
tracing implementation.

103
00:07:02,179 --> 00:07:03,109
So what is this one?

104
00:07:03,439 --> 00:07:06,949
So distributed tracing is a
cornerstone of observability.

105
00:07:07,639 --> 00:07:08,989
First instrument.

106
00:07:08,989 --> 00:07:09,589
Your code.

107
00:07:09,769 --> 00:07:13,339
Add a trace context propagation
using libraries like open

108
00:07:13,339 --> 00:07:15,319
telemetry across service calls.

109
00:07:16,099 --> 00:07:18,619
Next, configure intelligent sampling.

110
00:07:19,339 --> 00:07:22,159
Capture a hundred percent
of critical parts.

111
00:07:22,609 --> 00:07:28,149
But just as a side note, do
not sample a lot of traffic.

112
00:07:28,149 --> 00:07:28,899
Just sample.

113
00:07:29,399 --> 00:07:33,239
The data, which you want to do
it, say one in thousand requests.

114
00:07:33,419 --> 00:07:37,949
So we need to include both normal
and exceptional patterns and

115
00:07:37,949 --> 00:07:41,729
then centralize the pipeline
with a scalable infrastructure.

116
00:07:41,849 --> 00:07:47,099
Think about something as a Kafka
or a GCB per sub to ingest all

117
00:07:47,099 --> 00:07:49,289
this data and trace it out.

118
00:07:49,679 --> 00:07:52,399
So this process millions of spans daily.

119
00:07:52,899 --> 00:07:56,679
And finally perform the
root cause analysis.

120
00:07:57,129 --> 00:08:02,319
Visualize traces in JG to spot
bottlenecks, because these ties

121
00:08:02,319 --> 00:08:07,809
directly to your 85% resolution metrics
from studies at an industry level.

122
00:08:08,309 --> 00:08:13,529
So another key topic is
API Gateway observability.

123
00:08:13,769 --> 00:08:14,609
So what is that?

124
00:08:14,789 --> 00:08:19,649
So API Gateway observability refers
to the ability to monitor, measure,

125
00:08:19,649 --> 00:08:25,229
and gain actionable insights into the
behavior, performance, and health of

126
00:08:25,259 --> 00:08:31,799
API traffic as it flows through an API
gateway, it helps ensure that the APIs

127
00:08:31,799 --> 00:08:34,559
are reliable, secure, and performant.

128
00:08:35,059 --> 00:08:39,199
But now why exactly the API
Gateway Observability matters.

129
00:08:40,159 --> 00:08:44,629
This acts as the entry point for
microservices in distributed systems.

130
00:08:45,199 --> 00:08:50,089
The gateway is where the request ends from
the external world to the internal world.

131
00:08:50,959 --> 00:08:53,719
So the API Gateways
are observability hubs.

132
00:08:54,379 --> 00:08:59,839
Where we can get track traffic insights,
request volumes, service dependencies,

133
00:08:59,839 --> 00:09:01,789
and rate limiting effectiveness.

134
00:09:01,879 --> 00:09:06,859
So these traffic volumes could vary
from managing few hundred requests per

135
00:09:06,859 --> 00:09:11,119
second to tens of thousands of requests
per second during our peak times.

136
00:09:11,619 --> 00:09:16,319
And this provides a centralized control
plane for routing, authentication,

137
00:09:16,589 --> 00:09:18,199
rate limiting and caching.

138
00:09:18,699 --> 00:09:20,770
So what does observability enables?

139
00:09:20,859 --> 00:09:26,739
In the API gateways, it gives us real
time visibility into the API traffic

140
00:09:26,739 --> 00:09:32,530
issue detection, whether it might be
latency errors, anonymous auditing and

141
00:09:32,530 --> 00:09:35,020
compliance, and also security monitoring.

142
00:09:35,380 --> 00:09:38,170
So when I say security
monitoring, that is the key.

143
00:09:38,439 --> 00:09:41,859
It monitors authentication
events and attack patterns.

144
00:09:42,639 --> 00:09:45,620
Which we can, give them
to the InfoSec team.

145
00:09:45,620 --> 00:09:47,569
Hey, these are the, attacks coming up.

146
00:09:48,069 --> 00:09:51,819
So there are a few other
performance metrics like latency and

147
00:09:51,819 --> 00:09:53,560
throughput, which are very critical.

148
00:09:53,649 --> 00:09:58,569
So 25% latency restriction could
be achieved by optimizing gateway

149
00:09:58,569 --> 00:10:03,310
routing, which is a direct
application of this particular

150
00:10:03,310 --> 00:10:05,580
slide through API gateways with.

151
00:10:06,080 --> 00:10:07,160
Native observability.

152
00:10:07,290 --> 00:10:07,980
What is that?

153
00:10:07,980 --> 00:10:09,240
So how can we achieve that?

154
00:10:09,240 --> 00:10:13,620
So we can do that by Azure, API
Management plus some application

155
00:10:13,620 --> 00:10:18,930
insights and Epic E plus A Stack
Driver or Kong Engine EngineX,

156
00:10:19,770 --> 00:10:23,040
TONY, with Prometheus and Grafana.

157
00:10:23,100 --> 00:10:27,250
So these are the tools and there are
like other tools like G-C-P-A-P-I,

158
00:10:27,250 --> 00:10:32,230
gateway with Integrated monitoring
Dashboards makes this actionable too.

159
00:10:32,730 --> 00:10:35,880
So are there any gateway specific
challenges you are facing?

160
00:10:36,400 --> 00:10:36,760
Let's see.

161
00:10:36,760 --> 00:10:44,920
Yeah, so best practices include enable
structured access logs with AL IDs.

162
00:10:45,130 --> 00:10:48,880
So these IDs help us
relate from an end to end.

163
00:10:49,465 --> 00:10:52,795
Monitoring of what happened
for that particular request.

164
00:10:53,335 --> 00:10:57,835
And also use rate limiting and
circuit breakers with metrics exposure

165
00:10:58,555 --> 00:11:03,115
instrument your GA API gateway with
tracing head dust, which includes

166
00:11:03,115 --> 00:11:05,545
request id and also transparent.

167
00:11:06,045 --> 00:11:12,495
Monitor latency percentiles such
as P 50, P 95, P 99 for the SLO

168
00:11:12,495 --> 00:11:18,885
enforcement track End-to-end transaction
parts using distributed tracing.

169
00:11:19,385 --> 00:11:20,105
So moving on.

170
00:11:20,105 --> 00:11:23,165
Next we have an observability
pipeline design.

171
00:11:23,585 --> 00:11:27,425
A robust observability
pipeline has four stages.

172
00:11:27,665 --> 00:11:32,285
Collection gathers telemetry
from services and infrastructure.

173
00:11:32,585 --> 00:11:37,385
Think of agents in this case on
each and every node processing of

174
00:11:38,165 --> 00:11:40,475
normalization and filtering data.

175
00:11:40,535 --> 00:11:45,665
So stream processes are used to
reduce noise to keep only critical

176
00:11:45,665 --> 00:11:49,685
signals for a few hundred millisecond
milliseconds detection space.

177
00:11:49,715 --> 00:11:51,165
That's an as an example.

178
00:11:51,430 --> 00:11:57,455
So this is a kind of filtering out the
additional noise or to pull out the

179
00:11:57,455 --> 00:11:59,314
exact data, which we are looking for.

180
00:12:00,259 --> 00:12:07,069
And then storage uses tried strategies,
so hot storage for recent data, cold

181
00:12:07,550 --> 00:12:12,499
storage for archives, which helps
us in balancing cost and access.

182
00:12:12,770 --> 00:12:16,339
Aligning with 25% of resource efficiency.

183
00:12:17,329 --> 00:12:21,589
Now visualization turns
data into dashboards.

184
00:12:21,709 --> 00:12:25,999
There are many tools like Grafana
to spot anonymous in real time,

185
00:12:25,999 --> 00:12:28,609
supporting 40% faster resolution.

186
00:12:29,109 --> 00:12:34,739
These pipelines become the
backbone, our system, and now we

187
00:12:34,739 --> 00:12:37,949
have high availability strategies.

188
00:12:38,069 --> 00:12:40,379
So what are high availability strategies?

189
00:12:41,009 --> 00:12:46,409
High availability ensures minimal
service disruption and maximum uptime.

190
00:12:46,794 --> 00:12:52,019
From an observability perspective,
the ability to detect, understand and

191
00:12:52,019 --> 00:12:56,639
response to failures is critical to
achieving true resilience and scalability.

192
00:12:57,139 --> 00:13:01,969
High availability is a non-negotiable
in a distributor system, so for that,

193
00:13:01,969 --> 00:13:07,159
we deploy collectors and storage
across multiple availability zones

194
00:13:07,159 --> 00:13:08,724
to avoid single point of failure.

195
00:13:09,224 --> 00:13:11,925
So what about the residency
and failover visibility?

196
00:13:12,104 --> 00:13:16,935
We need to monitor at a node level,
zone level, and even at regional level

197
00:13:16,935 --> 00:13:22,814
for residency and track failover events
and measure meantime to recovery,

198
00:13:22,844 --> 00:13:31,054
which is termed as MTTR in the industry
levels, and then alert on degraded

199
00:13:31,054 --> 00:13:33,694
clusters or failed availability zones.

200
00:13:34,194 --> 00:13:39,204
And now how to monitor across a cross
zone or a region observability for

201
00:13:39,204 --> 00:13:41,484
a multi-cloud high availability.

202
00:13:42,204 --> 00:13:48,655
So these rails to correlate the metrics
logs and trace across the clouds detect

203
00:13:48,655 --> 00:13:54,415
latency between regions and audit cross
cloud failover success and data sync.

204
00:13:54,415 --> 00:13:58,634
Integrity designed for
graceful degradation, which

205
00:13:58,634 --> 00:14:00,014
helps us in reduce tele.

206
00:14:00,514 --> 00:14:05,694
If any backend fails, ensure core
functionality, test this during your

207
00:14:05,694 --> 00:14:08,274
P loads to get all the corner cases.

208
00:14:08,334 --> 00:14:12,414
But that might be, again,
our own individual case.

209
00:14:12,414 --> 00:14:17,144
If your system behaves well during low p
ds, then to get all the common cases, then

210
00:14:17,144 --> 00:14:18,764
yes, of course we can go ahead with that.

211
00:14:19,264 --> 00:14:24,084
And then we need to do a local buffering
temporarily which temporarily stores

212
00:14:24,145 --> 00:14:29,214
metrics when any connection drops,
or a multi chanting alerts, ensuring

213
00:14:29,814 --> 00:14:34,224
teams getting notifications via Slack
email and also PagerDuty alerts.

214
00:14:34,344 --> 00:14:35,185
So these.

215
00:14:35,469 --> 00:14:39,969
Strategies underpin 50% of our
downtime reduction, which are

216
00:14:39,969 --> 00:14:42,999
critical for enterprise resilience.

217
00:14:43,499 --> 00:14:47,219
And now how do we do a capacity
planning and what are the

218
00:14:47,369 --> 00:14:49,289
optimizing steps we need to perform?

219
00:14:50,069 --> 00:14:55,769
Capacity planning starts with historical
patterns, aggregate all the metrics over

220
00:14:55,769 --> 00:14:57,959
the months to understand utilization.

221
00:14:58,199 --> 00:15:01,679
Ideal data in this case spans.

222
00:15:02,194 --> 00:15:08,555
Over six months of the API traffic and
then forecast this demand with machine

223
00:15:08,555 --> 00:15:13,925
learning, predict seasonal spikes
like Thanksgiving, black Friday, or

224
00:15:13,925 --> 00:15:19,565
holiday day traffic, which helps us
optimizing resource allocation and then

225
00:15:19,625 --> 00:15:25,865
optimize dynamically using auto-scaling,
balancing cost and performance, which

226
00:15:25,865 --> 00:15:27,635
helps in continuous improvement.

227
00:15:28,010 --> 00:15:32,540
Of refining these models aligning
approximately 25% utilization

228
00:15:32,540 --> 00:15:33,800
of resource efficiency.

229
00:15:34,300 --> 00:15:39,020
And now for the continuous implement
improvement we need to establish

230
00:15:39,020 --> 00:15:43,670
a feedback loops to regularly
compare for casted projections

231
00:15:43,760 --> 00:15:47,600
against the actual consumption
patterns, redefining prediction

232
00:15:47,600 --> 00:15:49,760
models and allocation strategies.

233
00:15:50,260 --> 00:15:52,750
And now what is, what about
the implementation roadmap?

234
00:15:53,140 --> 00:15:54,850
So let's outline this roadmap.

235
00:15:55,420 --> 00:15:59,590
Start with foundational building,
set up metrics with Prometheus

236
00:16:00,040 --> 00:16:02,230
and centralized logging with ELK.

237
00:16:02,530 --> 00:16:05,980
Define your SLIs and
SLOs for core services.

238
00:16:06,790 --> 00:16:09,490
Then move on to the advanced capabilities.

239
00:16:09,820 --> 00:16:14,260
Deploy JAAR for tracing
across microservices and build

240
00:16:14,260 --> 00:16:16,090
custom dashboards for teams.

241
00:16:16,590 --> 00:16:21,780
In the optimization phase, add an
automate detection with AI and automate

242
00:16:21,840 --> 00:16:27,780
remediation for common issues, which are
building for 58 of our incident detection.

243
00:16:28,620 --> 00:16:32,880
As per industry research, this
phased approach has guided

244
00:16:32,880 --> 00:16:36,000
40% resolution improvement.

245
00:16:36,524 --> 00:16:39,554
Plan to start with one service and scale.

246
00:16:40,244 --> 00:16:43,425
Always remember, start small and scale.

247
00:16:43,724 --> 00:16:47,564
This will reduce the impacts
that any new changes brings in.

248
00:16:48,064 --> 00:16:52,054
And that wraps our journey to
the cloud native observability.

249
00:16:52,054 --> 00:16:56,164
We have covered the shift from
monitoring real world metrics,

250
00:16:56,164 --> 00:16:59,014
pillars, and implementation strategies.

251
00:16:59,044 --> 00:17:03,964
Thank you all for your attention and I
hope everybody enjoy the session today.

252
00:17:04,204 --> 00:17:04,804
Thanks a lot.

