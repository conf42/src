1
00:00:00,500 --> 00:00:01,250
Speaker 18: Hello folks.

2
00:00:01,250 --> 00:00:05,150
I'm Kian and I'm working
as a tech credit Oracle.

3
00:00:05,869 --> 00:00:11,340
I'm here to talk about the most critical
challenges in modern platform engineering.

4
00:00:11,969 --> 00:00:16,830
So how to execute a massive
infrastructure migration without

5
00:00:16,830 --> 00:00:18,780
dropping a single customer request.

6
00:00:19,650 --> 00:00:24,750
So the goal isn't just to replace the
old system and to with the new system.

7
00:00:25,260 --> 00:00:30,540
The goal is to design for the jro
downtime, any enterprise, any high

8
00:00:30,590 --> 00:00:32,240
high throughput saan environments.

9
00:00:32,490 --> 00:00:34,440
This is not just a nice to have feature.

10
00:00:34,470 --> 00:00:36,710
This is this is a commercially imperative.

11
00:00:37,520 --> 00:00:41,750
So we are going to talk about a, I'm
going to talk about a case study.

12
00:00:42,380 --> 00:00:47,750
So on how we successfully migrated a
critical legacy gateway to a modern,

13
00:00:47,750 --> 00:00:49,160
highly available architecture.

14
00:00:49,660 --> 00:00:54,110
The transformation was executed without
a single single service interruption.

15
00:00:54,710 --> 00:00:59,329
The so this and this, the, this
case study has proven that a

16
00:00:59,329 --> 00:01:04,174
complex infrastructure migration
can be possible with zero downtime.

17
00:01:04,674 --> 00:01:09,619
So let's start by defining why we
have zero tolerance for downtime.

18
00:01:10,119 --> 00:01:14,994
So when you process billions of
requests and using a platform, and

19
00:01:14,994 --> 00:01:19,424
if you want to replace the platform
with the new platform, like you need

20
00:01:19,424 --> 00:01:22,784
to copy all the functional, all the
functionality of the old system onto

21
00:01:22,784 --> 00:01:24,764
the new system without any disruption.

22
00:01:25,264 --> 00:01:25,924
So the.

23
00:01:26,424 --> 00:01:30,884
The risks involved with that is
people usually use the synthetic

24
00:01:31,094 --> 00:01:34,654
testing on the old system, and
like people write the synthetic

25
00:01:34,774 --> 00:01:38,434
test, like unit test and integrate
integration test on the old system,

26
00:01:39,094 --> 00:01:40,864
and they replay it on the new system.

27
00:01:41,224 --> 00:01:45,319
And once the test pass, they
simply cut the traffic from

28
00:01:45,319 --> 00:01:46,669
the old system, new system.

29
00:01:47,169 --> 00:01:50,809
So that solution doesn't work if we have.

30
00:01:51,799 --> 00:01:55,759
If you have a lot of undocumented changes
in the old system and there is a lot of

31
00:01:55,759 --> 00:01:57,609
tribal knowledge in that went in there.

32
00:01:58,109 --> 00:01:59,459
So what's the solution here?

33
00:01:59,699 --> 00:02:04,859
So we realized that automated reliability
engineering and continuous validation

34
00:02:04,859 --> 00:02:07,914
using actual production traffic like.

35
00:02:08,339 --> 00:02:12,269
We'll work like these are the methods
to prevent service interruption.

36
00:02:13,079 --> 00:02:18,779
So this led us to a strategy of a phase
rollout combined with advanced shadow

37
00:02:18,779 --> 00:02:23,209
traffic validation framework that we
need to te that we use to test the

38
00:02:23,209 --> 00:02:27,279
production traffic a safely and yeah.

39
00:02:27,329 --> 00:02:29,739
So let's look at the system
that that we have to replace.

40
00:02:30,239 --> 00:02:35,629
So the system that we are trying
to replace is a legacy monolithic

41
00:02:35,629 --> 00:02:38,339
gateway based out of Ruby and ra.

42
00:02:39,119 --> 00:02:45,359
So it has a session management and
authentication and authorization logic.

43
00:02:45,869 --> 00:02:51,189
And also routing all together, jammed
in one in one service, in one monolith.

44
00:02:52,049 --> 00:02:56,409
So the risks so associated with
this monolith is the blast radius.

45
00:02:56,779 --> 00:03:01,079
One change in routing will, will, could,
if a routing, if there is an issue with

46
00:03:01,079 --> 00:03:05,629
routing changes, that could severely
impact other concerns on the same service.

47
00:03:06,619 --> 00:03:11,119
And there were also resource
concerns like the ruby and Symetra.

48
00:03:11,124 --> 00:03:14,679
It's an open source and it
wasn't an advanced gateway.

49
00:03:14,679 --> 00:03:19,509
So it is not leveraging the multi-core
compute resources that we've been using.

50
00:03:20,379 --> 00:03:23,724
And also if at all we need
to scale a single concern we

51
00:03:23,724 --> 00:03:25,194
have to scale this uniformly.

52
00:03:25,244 --> 00:03:27,914
So that is a scalability issue as well.

53
00:03:28,809 --> 00:03:30,639
And there was also an operat.

54
00:03:30,639 --> 00:03:32,639
The operational risk was also mounting.

55
00:03:32,829 --> 00:03:36,394
There was less expertise in the
in the company with the ruby.

56
00:03:37,174 --> 00:03:41,334
And also there were a lot of
security vulnerabilities and yeah.

57
00:03:41,334 --> 00:03:47,429
And these all these are the reasons why
why we want to replace a, this legacy

58
00:03:47,429 --> 00:03:49,434
model with the modern architecture.

59
00:03:49,934 --> 00:03:54,664
So the tight couple route architecture
was the root cause of our scalability

60
00:03:54,664 --> 00:03:56,704
issues and and the operational risk.

61
00:03:57,570 --> 00:04:02,154
The tight doubling of concerns meant that
like I said earlier that a minor update

62
00:04:02,154 --> 00:04:07,134
to routing could impact could break the
authentication stack and vice versa.

63
00:04:07,634 --> 00:04:10,789
And there was also a scalability
inefficiency as well.

64
00:04:11,289 --> 00:04:14,019
And and this is training
the computer resources.

65
00:04:14,839 --> 00:04:18,184
If at all we need to scale a
concern at the peak traffic, we

66
00:04:18,184 --> 00:04:19,744
have to scale this uniformly.

67
00:04:20,084 --> 00:04:25,184
So our objective was clear transition
to a modern enterprise gateway and

68
00:04:25,184 --> 00:04:29,294
decouple the concerns while maintaining
the continuous service availability.

69
00:04:29,804 --> 00:04:35,734
So by, we need to migrate this complex
service safely without any downtime.

70
00:04:36,074 --> 00:04:37,919
So this is where the real challenge began.

71
00:04:38,419 --> 00:04:43,909
So migration, like what are the
migration challenges and and validations

72
00:04:43,909 --> 00:04:46,619
that that what are the challenges
and what are the validations that,

73
00:04:46,619 --> 00:04:48,149
that, I'm gonna talk about them here.

74
00:04:48,649 --> 00:04:52,719
Undocumented legacy behavior, like
moving away from the, like the

75
00:04:52,719 --> 00:04:53,979
sea bottle to the new gateway.

76
00:04:54,169 --> 00:04:54,654
There was a lot of.

77
00:04:55,459 --> 00:04:58,999
Undocumented changes in the Legacy
Gateway where traditional testing

78
00:04:59,409 --> 00:05:03,039
cannot catch if we run the test
cases on the new system and.

79
00:05:03,539 --> 00:05:05,659
And then most danger dangerously.

80
00:05:05,709 --> 00:05:09,644
And earlier I mentioned, we are
gonna use a shadow traffic or shadow,

81
00:05:09,704 --> 00:05:12,464
replay mechanism to run the parity.

82
00:05:12,804 --> 00:05:15,859
There the performance impact
was there, there could be

83
00:05:15,859 --> 00:05:17,359
performance impact with that.

84
00:05:17,719 --> 00:05:22,049
So if this shadow I I'll deep
dive into this shadow shadow.

85
00:05:22,549 --> 00:05:24,489
Traffic framework in the latest slides.

86
00:05:24,939 --> 00:05:28,519
So there was a performance issue
I'm gonna talk about like, how did

87
00:05:28,519 --> 00:05:30,409
we overcome that performance issue.

88
00:05:30,949 --> 00:05:35,569
So the performance impact is one, and
another one is like we need to come

89
00:05:35,569 --> 00:05:39,879
up innovatively come up, came up with
a, which I work around to to skip

90
00:05:39,879 --> 00:05:41,559
the double execution of the request.

91
00:05:42,039 --> 00:05:47,254
And other one is we want to offload
the authentication logic to, to a

92
00:05:47,254 --> 00:05:48,904
different independent microservice.

93
00:05:49,864 --> 00:05:52,569
So these were the challenges
with the migration.

94
00:05:53,164 --> 00:05:56,604
So let's dive into how the validation
the chart of framework will work.

95
00:05:57,104 --> 00:06:00,104
This is the core technical
innovation that actually enabled

96
00:06:00,554 --> 00:06:05,204
the downtime zero downtime with a
state marketing charter framework.

97
00:06:06,114 --> 00:06:09,024
So we had to solve the double
execution problem as well.

98
00:06:09,294 --> 00:06:13,704
So our solution was based on replay
mechanism that works in five steps.

99
00:06:14,314 --> 00:06:16,864
There was a processor
on the legacy gateway.

100
00:06:17,624 --> 00:06:22,884
And that processor whenever a, the Legacy
Gateway receives a request, it usually

101
00:06:22,884 --> 00:06:28,074
process the request and by forwarding
the response back to the client, this

102
00:06:28,074 --> 00:06:33,254
gateway will actually asynchronously
use a processor to run the parity.

103
00:06:33,524 --> 00:06:38,534
So this processor will actually
copy the customer request and file

104
00:06:38,534 --> 00:06:40,244
it on the new con gateway system.

105
00:06:41,084 --> 00:06:44,314
And before firing that
it'll create a parity id.

106
00:06:44,704 --> 00:06:50,404
And it also saves the downstream
response and also the client downstream

107
00:06:50,404 --> 00:06:54,614
request and also the client response,
both of them in a NoSQL store.

108
00:06:55,004 --> 00:06:58,694
And it fires the same request
with a, with the added query

109
00:06:58,694 --> 00:07:00,404
parameter called parity id.

110
00:07:00,814 --> 00:07:02,254
And it replace on the new system.

111
00:07:02,614 --> 00:07:04,534
So when Congress use it, Kong.

112
00:07:04,949 --> 00:07:08,089
Should do the same thing as the
Legacy Gate West does, and it

113
00:07:08,089 --> 00:07:09,649
should forward to the downstream.

114
00:07:10,279 --> 00:07:16,109
So to avoid if the request if the rest API
call is is read, then there is no issue.

115
00:07:16,619 --> 00:07:20,789
But if it is, other than read, we
want to skip the double execution.

116
00:07:21,119 --> 00:07:22,154
For that reason, we just.

117
00:07:22,654 --> 00:07:25,474
We deployed a interceptor
on the backend system.

118
00:07:25,864 --> 00:07:31,234
So whenever backend receives this request,
the intercept will, interceptor, will kick

119
00:07:31,234 --> 00:07:37,759
in, and inter, whenever interceptor sees
the power DID, it basically reduce the.

120
00:07:38,344 --> 00:07:40,414
Like downstream response from the cache.

121
00:07:40,414 --> 00:07:44,809
And it, it responds to the Kong
gateway and Kong gateway operated

122
00:07:44,809 --> 00:07:46,649
back to the legacy gateway.

123
00:07:47,189 --> 00:07:52,239
And when everything is in place,
the, all of this the processor

124
00:07:52,239 --> 00:07:55,904
built, run the parity and save the
differences in a NoSQL store pack.

125
00:07:56,404 --> 00:07:59,464
So now now let's look at it.

126
00:08:00,134 --> 00:08:02,944
I'm gonna talk about what are
these differences and all of

127
00:08:02,944 --> 00:08:04,314
that in the latest slides.

128
00:08:04,614 --> 00:08:07,314
So now let's look at how we
implemented this framework

129
00:08:07,614 --> 00:08:09,804
across the in the migration.

130
00:08:09,809 --> 00:08:09,889
Okay.

131
00:08:10,389 --> 00:08:13,709
So the migration is it's an
eight phase implementation work.

132
00:08:14,009 --> 00:08:15,869
The first one is planning
and provisioning.

133
00:08:16,189 --> 00:08:21,799
We used a I used a Terraform and
and all of the routes in the old

134
00:08:21,799 --> 00:08:26,289
Kite way have been coded in the,
in, in the YAML format for the conc.

135
00:08:26,859 --> 00:08:29,679
And all the infrastructure
changes, everything.

136
00:08:29,769 --> 00:08:34,589
We I used Terraform to, to document
for the deployment, and that's where

137
00:08:34,589 --> 00:08:36,209
the planning and provisioning went in.

138
00:08:36,629 --> 00:08:41,799
And once we have it ready I I
did the, they were benchmark

139
00:08:42,039 --> 00:08:43,269
test results from Kong.

140
00:08:43,539 --> 00:08:49,259
So we used them and we did I did some
of the low testing and then we, we

141
00:08:49,259 --> 00:08:51,689
deployed Kong Kong as a separate cluster.

142
00:08:52,379 --> 00:08:55,719
And then the next step is shadow
framework shadow framework,

143
00:08:55,779 --> 00:08:57,069
like the validation part.

144
00:08:57,569 --> 00:08:59,699
So here, the validation part.

145
00:09:00,029 --> 00:09:05,989
While validating in the validation
basically we cannot validate entire

146
00:09:05,989 --> 00:09:09,289
production traffic because if we
validate entire production traffic,

147
00:09:09,289 --> 00:09:11,449
the traffic will double and the back.

148
00:09:11,449 --> 00:09:12,469
We also have to.

149
00:09:13,034 --> 00:09:17,234
Scale the backend, backend or downstream
systems to handle all of the traffic.

150
00:09:17,774 --> 00:09:21,924
So for that reason we we sampled
we sampled the one fifth,

151
00:09:22,314 --> 00:09:23,634
like one fifth of the traffic.

152
00:09:23,769 --> 00:09:29,529
It gets replayed on the New Kong gateway
so that if we run this over the period

153
00:09:29,779 --> 00:09:33,524
the, based on the probability, the
assumption is that all of the a p routes

154
00:09:33,524 --> 00:09:35,864
and all of the functions will be covered.

155
00:09:36,044 --> 00:09:37,454
So that's the idea here.

156
00:09:37,889 --> 00:09:39,429
And the next step would be.

157
00:09:39,674 --> 00:09:40,784
Authentication offloading.

158
00:09:41,054 --> 00:09:44,074
So the plan is this one is a bigger topic.

159
00:09:44,074 --> 00:09:49,134
So this we, I had followed the, taken
the similar approach to, to offload

160
00:09:49,134 --> 00:09:50,694
this authentication and authorization.

161
00:09:51,264 --> 00:09:52,964
Let's select to simplify that.

162
00:09:53,064 --> 00:09:55,574
The next step is offload
the authentication.

163
00:09:55,574 --> 00:09:58,604
Consent has been offloaded
to a new microservice.

164
00:09:59,104 --> 00:10:00,614
And then and then testing.

165
00:10:00,914 --> 00:10:04,579
Like I said earlier, we use the
sampling and we just replayed the

166
00:10:04,579 --> 00:10:09,519
month, the traffic, and and after
that if this has been like once

167
00:10:09,519 --> 00:10:11,109
the, once there are no differences.

168
00:10:11,409 --> 00:10:13,569
We had incrementally rolled out and.

169
00:10:14,069 --> 00:10:17,039
I rolled out the traffic from
Old Gateway to the new Gateway

170
00:10:17,449 --> 00:10:21,129
and then we did the final cutover
and then retired the old gateway.

171
00:10:21,489 --> 00:10:25,059
So that's the eight, eight phase
implementation framework that

172
00:10:25,059 --> 00:10:27,019
we followed for this migration.

173
00:10:27,519 --> 00:10:29,949
So let's look at now how the.

174
00:10:30,449 --> 00:10:35,489
How that shadow traffic, like what has the
shadow traffic validation has revealed?

175
00:10:35,989 --> 00:10:36,170
Okay.

176
00:10:36,170 --> 00:10:37,339
We jumped little bit,

177
00:10:37,839 --> 00:10:40,690
so the shadow traffic
findings were most critical.

178
00:10:41,200 --> 00:10:42,980
So because because.

179
00:10:43,645 --> 00:10:46,405
They revealed they're gonna reveal
the hidden production behaviors

180
00:10:46,825 --> 00:10:51,454
and whatever the synthetic test
has has completely mi might miss.

181
00:10:51,994 --> 00:10:54,994
So we performed two, two key verification.

182
00:10:55,264 --> 00:10:59,615
First, we looked at downstream
system verification, downstream

183
00:10:59,615 --> 00:11:03,675
request verification comparing the
request payload Kong sent to the

184
00:11:03,675 --> 00:11:05,595
backend against the legacy system.

185
00:11:05,595 --> 00:11:09,555
The legacy gateway was sent to
the backend, and another one is.

186
00:11:10,200 --> 00:11:15,440
Client response validation, like what
the legacy gateway has been forwarding

187
00:11:15,440 --> 00:11:17,390
to the clients for the same request.

188
00:11:17,390 --> 00:11:20,450
And also how what Kong is
sending back to the clients.

189
00:11:20,950 --> 00:11:23,495
And and the next one is
progressive sampling.

190
00:11:23,525 --> 00:11:26,179
Like I said the plan was to
not run on entire traffic.

191
00:11:26,679 --> 00:11:31,159
Rather to sample the traffic, like one
fifth of the we use a random function

192
00:11:31,159 --> 00:11:36,719
to to divide the request id, and based
on based on that, like we implemented a

193
00:11:36,719 --> 00:11:41,169
sampler to sample the traffic and so that
the processor will not run, replay the

194
00:11:41,169 --> 00:11:45,524
request for for every every production
request is sent replay on the new system.

195
00:11:46,024 --> 00:11:47,854
So using U using this.

196
00:11:47,904 --> 00:11:53,729
So these results were used for the,
before the final to make any corrections

197
00:11:53,729 --> 00:11:58,569
in the new gateway and before cutting
over the real traffic to the new gateway.

198
00:11:59,069 --> 00:12:02,339
Let's look at operational
impact and benefits.

199
00:12:02,839 --> 00:12:07,459
So the ultimate measure of success
for this project is that we achieved

200
00:12:07,519 --> 00:12:12,089
zero customer facing incident despite
processing millions of request.

201
00:12:12,589 --> 00:12:16,519
This wasn't just about downtime, it
delivered tangible business benefits.

202
00:12:17,449 --> 00:12:21,619
We saw a cost reduction, 20% cost
reductions in the compute resources

203
00:12:21,680 --> 00:12:23,889
because using the modern architecture.

204
00:12:24,459 --> 00:12:29,589
Furthermore we deployed a vendor supported
platform that the vendor supports the

205
00:12:29,959 --> 00:12:34,695
security vulnerabilities patches and there
were a lot of out of the box features that

206
00:12:34,695 --> 00:12:40,300
comes from the vendor supported gateway
and also, like we were able to and the

207
00:12:40,330 --> 00:12:44,759
vendor gateways it, it's compliant with
security so we don't have to do additional

208
00:12:44,759 --> 00:12:46,449
work when it comes to the security.

209
00:12:46,449 --> 00:12:47,629
So it is compliant.

210
00:12:48,129 --> 00:12:51,584
And there were side effects
like automated deployments and

211
00:12:51,954 --> 00:12:55,505
crucially the new architecture has
reduced the meantime to recovery.

212
00:12:55,505 --> 00:12:59,285
And there were automated rollbacks
making the system more reliable.

213
00:12:59,975 --> 00:13:01,655
So let's talk about that.

214
00:13:01,755 --> 00:13:04,545
New architecture and fundamental changes.

215
00:13:04,755 --> 00:13:08,025
Fundamental changes like
we can build and scale.

216
00:13:08,525 --> 00:13:13,445
So let's dive into that 40% cost
efficiency because it is directly,

217
00:13:13,445 --> 00:13:16,775
it is a direct result of moving
away from monolithic architecture.

218
00:13:17,675 --> 00:13:19,945
The biggest driver was
independent scaling.

219
00:13:20,405 --> 00:13:25,040
We no longer had to scale the entire
heavy ruby stack just to handle the

220
00:13:25,040 --> 00:13:26,600
lightweight authentication checks.

221
00:13:27,100 --> 00:13:30,890
By decoupling the authentication
workload we optimized the

222
00:13:30,949 --> 00:13:33,150
resource allocation dramatically.

223
00:13:33,660 --> 00:13:38,819
And secondly, we elevated the ruby
bottleneck, like not being able

224
00:13:38,819 --> 00:13:43,589
to use the multi-core, and we got
more work out of less hardware.

225
00:13:44,009 --> 00:13:47,729
And finally, we saw a massive
reduction in operational overhead

226
00:13:48,449 --> 00:13:51,839
and vendor supported patches and
automated deployment pipelines.

227
00:13:52,339 --> 00:13:56,719
So the efficiency gain was just one
time save wasn't just one time saving.

228
00:13:56,719 --> 00:14:01,279
It is continuous benefit that, that is,
that funded the that, that is, that will

229
00:14:01,279 --> 00:14:02,929
be funding the future innovation as well.

230
00:14:03,459 --> 00:14:06,430
Let's look at the broader
architecture benefits this, that,

231
00:14:06,430 --> 00:14:08,199
this transformation has unlocked.

232
00:14:08,699 --> 00:14:11,769
So the architecture shift is foundation.

233
00:14:11,989 --> 00:14:17,489
Now the authentication op concern
can independently scale and there

234
00:14:17,489 --> 00:14:19,139
were multi-tenant capabilities.

235
00:14:19,259 --> 00:14:22,629
And also we don't have to dual migration.

236
00:14:22,709 --> 00:14:27,239
If the company brought in, another
acquisition or so like it, it

237
00:14:27,239 --> 00:14:31,539
has multi-tenant capabilities for
suppose in the use case the company

238
00:14:31,539 --> 00:14:33,329
acquired, bunch of other companies.

239
00:14:33,599 --> 00:14:38,100
So we were able to use the same same
platform to route the traffic to their

240
00:14:38,100 --> 00:14:42,739
backend services once the companies
merch and regarding compliance also,

241
00:14:42,739 --> 00:14:47,790
the it is simplified because of because
of the vendor support and also we were

242
00:14:47,790 --> 00:14:52,900
able to use the latest libraries, e
even if the vendor doesn't support,

243
00:14:52,960 --> 00:14:57,740
we were able to apply patches if there
were any security vulnerabilities and

244
00:14:57,740 --> 00:14:59,900
there was engineering velocity as well.

245
00:15:00,460 --> 00:15:05,660
Because because we don't have to because
we are, we minimize the blasted Yes.

246
00:15:05,710 --> 00:15:06,160
And.

247
00:15:06,925 --> 00:15:10,940
And the number of unit tests that we
run when we deploy a change to a K two,

248
00:15:11,060 --> 00:15:15,045
when we're deployed authentication the
meantime has substantially dropped.

249
00:15:15,435 --> 00:15:18,465
So this has improved the velocity
of the engineering team as well.

250
00:15:18,965 --> 00:15:23,165
So let's say like, how did we execute
it without a dropping a single request?

251
00:15:23,265 --> 00:15:23,775
Let's see.

252
00:15:23,775 --> 00:15:24,405
In the next slide.

253
00:15:24,405 --> 00:15:28,135
So on what are the, what are
the key key success factors?

254
00:15:28,635 --> 00:15:28,755
I.

255
00:15:29,255 --> 00:15:33,575
Definitely the shadow traffic validation
is the is the key factor for the success.

256
00:15:33,935 --> 00:15:38,545
And another one is incremental rollout,
just to avoid any just to reduce the

257
00:15:38,725 --> 00:15:40,585
blast radius if something goes wrong.

258
00:15:41,305 --> 00:15:43,075
And the infrastructure as a code.

259
00:15:43,575 --> 00:15:47,855
The Terraform using Terraform
has speeded up the, planning and

260
00:15:47,855 --> 00:15:49,535
provisioning of the new gateway easily.

261
00:15:50,255 --> 00:15:55,735
And then automated analysis, like in
the traffic validation part, the we

262
00:15:55,735 --> 00:15:59,575
were automatically able to evaluate
the differences between the old and

263
00:15:59,575 --> 00:16:03,685
the new gateway and able to quickly
fix the problems in the, fix the

264
00:16:03,685 --> 00:16:05,125
functional issues in the new gateway.

265
00:16:05,625 --> 00:16:05,850
And then.

266
00:16:06,350 --> 00:16:08,150
And then, yeah, phased implementation.

267
00:16:08,400 --> 00:16:13,900
This phased implementation also
helped us maintaining the production

268
00:16:13,900 --> 00:16:19,380
stability and and the throughput and
the transition, like we basically

269
00:16:19,380 --> 00:16:21,570
reducing the blast radis as well here.

270
00:16:22,070 --> 00:16:24,380
Here we are gonna talk
about the lessons learned.

271
00:16:24,450 --> 00:16:28,560
The success factor we just covered
are around just a PA gateways.

272
00:16:28,620 --> 00:16:33,030
They form a universal difference
or a blueprint for any complex

273
00:16:33,030 --> 00:16:34,890
model to microservice transition.

274
00:16:34,890 --> 00:16:40,160
The big key, key takeaway is that
for legacy system, the validation or

275
00:16:40,220 --> 00:16:42,140
testing is the superior is superior.

276
00:16:42,140 --> 00:16:43,610
It's, it definitely helped us.

277
00:16:44,325 --> 00:16:47,835
The synthetic test, all of this would
have helped, but definitely would

278
00:16:47,835 --> 00:16:52,420
have given the confidence that we
got from the validation and we have

279
00:16:52,465 --> 00:16:56,605
also learned about the store on the
forward pattern is crucial for the

280
00:16:56,605 --> 00:17:01,565
risk management and it also decouples
execution from the actual backend state,

281
00:17:01,715 --> 00:17:06,025
ensuring that like the shadow traffic
doesn't execute one request twice.

282
00:17:06,430 --> 00:17:10,790
And finally the validation
results also helped us to fix

283
00:17:10,790 --> 00:17:12,200
the problems in the new gateway.

284
00:17:13,130 --> 00:17:15,640
So yeah these lessons are
applicable whether you are.

285
00:17:16,380 --> 00:17:20,470
Like these lessons will help not
just with the like I said the

286
00:17:20,470 --> 00:17:23,770
authentication authorization concern
was separated from the legacy gateway.

287
00:17:23,770 --> 00:17:24,850
We using the same pattern.

288
00:17:24,850 --> 00:17:27,400
So this is not just for gateways.

289
00:17:27,550 --> 00:17:32,900
This can be applicable when you're trying
to migrate any backend infrastructure any

290
00:17:32,900 --> 00:17:35,160
backend rest a p kind of infrastructure.

291
00:17:35,340 --> 00:17:37,070
So this can be applicable there as well.

292
00:17:37,570 --> 00:17:39,670
And what does this mean for the future?

293
00:17:40,170 --> 00:17:45,130
So in, in conclusion, this case study
validates the complex infrastructure

294
00:17:45,190 --> 00:17:49,510
transformations can be executed while
maintaining while strictly enforcing the

295
00:17:49,540 --> 00:17:54,870
service level objectives, we prove that
phase rollout and the traffic validation

296
00:17:54,900 --> 00:17:57,185
enables a truly seamless migration.

297
00:17:57,685 --> 00:17:59,275
The business impact was substantial.

298
00:17:59,635 --> 00:18:03,325
We achieved zero downtime, delivered
over 20% in infrastructure cost

299
00:18:03,325 --> 00:18:06,985
reductions, and significantly
enhanced our security posture.

300
00:18:07,745 --> 00:18:11,855
This project provided a comprehensive
methodologist for industry demonstrating

301
00:18:11,945 --> 00:18:16,025
like how to execute high stakes
migrations without service disruptions.

302
00:18:16,525 --> 00:18:20,255
Yeah the core key takeaway
is invest in automated.

303
00:18:21,020 --> 00:18:25,340
Automated reliability engineering and
the validation framework for these

304
00:18:25,340 --> 00:18:29,790
kind of migration patterns or to
modernize any backend infrastructure

305
00:18:30,410 --> 00:18:32,240
that, that relates to this pattern.

306
00:18:33,015 --> 00:18:33,305
Okay.

307
00:18:33,805 --> 00:18:34,095
Yeah.

308
00:18:34,595 --> 00:18:35,045
Thank you.

309
00:18:35,175 --> 00:18:38,325
Now I'm open for any questions and also.

310
00:18:38,770 --> 00:18:43,190
Just in case, if you if you don't agree
or if you see any improvements just

311
00:18:43,190 --> 00:18:44,870
find me on LinkedIn and connect with me.

312
00:18:44,870 --> 00:18:46,010
We can chat over there.

313
00:18:46,670 --> 00:18:47,000
Thank you.

314
00:18:47,000 --> 00:18:47,420
Thank you.

315
00:18:47,450 --> 00:18:49,890
Thank you for thank you for having me.

316
00:18:50,325 --> 00:18:50,545
Bye.

