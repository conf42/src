1
00:00:00,410 --> 00:00:01,510
Hey everyone, I'm Amar.

2
00:00:01,590 --> 00:00:03,590
I'm thrilled to welcome
you to the session.

3
00:00:04,090 --> 00:00:08,390
As a senior manager of software
engineering at Lowe's, I have had

4
00:00:08,400 --> 00:00:13,419
the privilege of leading the teams
that design and deliver platforms

5
00:00:13,419 --> 00:00:17,490
capable of supporting millions of
users and millions of transactions.

6
00:00:17,990 --> 00:00:25,035
Today we are here to learn some of it
and then deep dive into how real time

7
00:00:25,035 --> 00:00:30,075
data is transforming customer experience
at a scale, discover how millions of

8
00:00:30,075 --> 00:00:36,024
interactions are being processed in near
real time or every second, and then how

9
00:00:36,024 --> 00:00:41,455
do we build ML powered recommendation
engines and then how Python helps

10
00:00:41,455 --> 00:00:43,405
create fast, efficient data systems.

11
00:00:43,905 --> 00:00:45,075
just think about this, right?

12
00:00:45,535 --> 00:00:49,435
if you could process over a million
records or million transactions every

13
00:00:49,435 --> 00:00:54,885
second, while, maintaining a near,
uptime, a near perfect uptime, and

14
00:00:54,885 --> 00:00:56,895
provide actionable insights quickly.

15
00:00:57,220 --> 00:00:57,460
Right.

16
00:00:58,030 --> 00:01:02,490
Imagine the competitive advantage that,
brings whether it is delivering lightning

17
00:01:02,490 --> 00:01:07,890
fast personalization recommendations
or enabling, real time decision making

18
00:01:07,890 --> 00:01:11,990
or proactively addressing customer
needs, before they even arise.

19
00:01:12,490 --> 00:01:16,060
In this session, we will uncover
the architectures required, and

20
00:01:16,080 --> 00:01:19,650
strategies that companies are
using to achieve these goals.

21
00:01:19,790 --> 00:01:24,149
I'll share, the real, world examples,
walk you through, the architectures,

22
00:01:24,199 --> 00:01:28,939
technical designs, and probably, the
challenges and how do we overcome that.

23
00:01:29,439 --> 00:01:33,139
whether you are here to supercharge your
technical expertise, gather actionable

24
00:01:33,139 --> 00:01:37,089
insights, to tackle, the current
challenges that you are, your team is

25
00:01:37,089 --> 00:01:41,829
facing, or probably like, just simply
exploring the incredible possibilities

26
00:01:41,829 --> 00:01:46,869
of, the real time analytics, you're
in the right place, let's get, going.

27
00:01:47,369 --> 00:01:51,099
So let's understand, the
customer expectations.

28
00:01:51,129 --> 00:01:53,389
What are the customers expecting, lately?

29
00:01:54,189 --> 00:01:59,469
so in today's hyperconnected digital
landscape, I think, a, a survey

30
00:01:59,469 --> 00:02:03,879
indication, says that 82 percent of
the customers now expect real time.

31
00:02:04,489 --> 00:02:09,809
personalized experiences that instantly,
adapt to their individual preference

32
00:02:09,849 --> 00:02:12,069
and then probably immediate need, right?

33
00:02:12,899 --> 00:02:17,399
so the companies who have
strategically deployed and try to

34
00:02:17,399 --> 00:02:22,349
solve these, real time personalized,
situation, have seen a remarkable,

35
00:02:22,559 --> 00:02:27,109
26 percent boost in the customer
satisfaction and an extraordinary,

36
00:02:27,549 --> 00:02:30,129
a customer, lifetime value of 3.

37
00:02:30,130 --> 00:02:30,539
2, X.

38
00:02:31,039 --> 00:02:36,329
So let's get, to, how do we, deal
with, dealing with, These, scenarios

39
00:02:36,329 --> 00:02:38,169
to unlock the customer insights, right?

40
00:02:38,939 --> 00:02:43,849
in today's competitive, digital space,
I think, primarily the industry leaders,

41
00:02:43,899 --> 00:02:47,499
Amazon could be Amazon or could be
Netflix or in the, in the, in the

42
00:02:47,499 --> 00:02:52,259
space that they are leading are ha,
harnessing, the power of, like advanced

43
00:02:52,259 --> 00:02:56,299
algorithms, and probably mission
learning models to process and analyze,

44
00:02:56,859 --> 00:02:58,684
massive streams of data in real time.

45
00:02:59,104 --> 00:03:05,234
By managing over 1 million, customer
transactions every second, these companies

46
00:03:05,234 --> 00:03:12,284
are able to extract immediate or valuable
insights instantly to understand what

47
00:03:12,334 --> 00:03:15,114
is the guest or customer is expecting.

48
00:03:15,344 --> 00:03:15,514
Right.

49
00:03:15,964 --> 00:03:20,024
Just try to understand how does
this algorithms work or what is

50
00:03:20,024 --> 00:03:21,944
this real time data about it.

51
00:03:21,964 --> 00:03:26,694
I am going to deep dive much into
the technical aspects pretty soon.

52
00:03:27,544 --> 00:03:32,054
But imagine how does this algorithms
work and then how do we get to

53
00:03:32,054 --> 00:03:36,434
a state of talking through these
real time monitoring or predictive

54
00:03:36,434 --> 00:03:37,824
analysis or sentiment analysis.

55
00:03:38,734 --> 00:03:41,084
So the first step in the
process is the data collection.

56
00:03:41,389 --> 00:03:45,359
So in this particular, scenario, it
talks about, the data that is being

57
00:03:45,359 --> 00:03:49,869
collected from various sources like
web activity, IoT devices, and the

58
00:03:49,889 --> 00:03:51,289
transactions that are happening.

59
00:03:51,559 --> 00:03:56,429
And then the data is in the raw format,
so it has to be processed, cleansed.

60
00:03:56,919 --> 00:04:00,149
so some, some examples could be
removing duplicates or handling missing

61
00:04:00,149 --> 00:04:04,569
values or maybe, adding synthetic data
in case the data is not available.

62
00:04:04,619 --> 00:04:07,129
All these steps are being done during
the pre processing step, right?

63
00:04:07,899 --> 00:04:12,179
And this data is being fed to,
the, pre trained, models, to,

64
00:04:12,229 --> 00:04:13,569
pre trained ML models, right?

65
00:04:13,659 --> 00:04:18,109
at this point of time, what do we
certainly look around is we understand,

66
00:04:18,619 --> 00:04:22,429
or analyze the behavior, patterns
such as like collaborative filter for

67
00:04:22,619 --> 00:04:26,809
recommendations, and then probably
predict the outcome, say in, in, in

68
00:04:26,809 --> 00:04:28,929
the case of retail, situation, right?

69
00:04:29,099 --> 00:04:31,589
is the, what is the guest looking to buy?

70
00:04:31,689 --> 00:04:33,579
What is the guest looking to buy?

71
00:04:33,629 --> 00:04:38,139
What item are probably, is the guest
going to make the transaction, right?

72
00:04:38,599 --> 00:04:43,089
And then, we could be also able to
segment customers dynamically based on

73
00:04:43,089 --> 00:04:45,289
some of these real time interactions.

74
00:04:45,889 --> 00:04:51,189
So, So, one biggest, I think, so these
models will, enable the decision, a

75
00:04:51,229 --> 00:04:57,309
decision engine, wherein algorithms, that
trigger actions, these algorithms trigger

76
00:04:57,399 --> 00:05:01,779
actions such as, personalized offers
or product recommendations or alerts.

77
00:05:02,379 --> 00:05:06,739
Say probably like, say, once this
particular, event is being triggered, but

78
00:05:06,769 --> 00:05:08,839
we also wanted to have a feedback loop.

79
00:05:09,004 --> 00:05:15,374
which talks about, the user responses,
are fed back into the system to improve,

80
00:05:15,554 --> 00:05:16,974
the models through continuous learning.

81
00:05:17,484 --> 00:05:21,454
So if I'm just trying to jot down, the
things that we just talked about, so

82
00:05:21,484 --> 00:05:24,474
there's a data collection system, there's
a data processing system, and you have

83
00:05:24,474 --> 00:05:29,104
an ML model, which, the model will be
pre trying to determine, Behavioral

84
00:05:29,534 --> 00:05:33,884
pattern, predict outcomes, and then
probably do the segmentations as required.

85
00:05:33,884 --> 00:05:39,084
And then the outcome of that is a decision
engine, which will be, the, say some

86
00:05:39,084 --> 00:05:42,924
examples could be like, in the case
of retail, it could be a personalized

87
00:05:42,924 --> 00:05:48,184
offers, or it could be, a personalized
pricing or a dynamic, personalization,

88
00:05:48,224 --> 00:05:49,724
is something that would be done.

89
00:05:49,754 --> 00:05:53,714
And then the data would also be
retrained with the feedback loop, right?

90
00:05:54,454 --> 00:05:59,402
So let us dive into, how, the,
industry retailers are using or

91
00:05:59,402 --> 00:06:03,022
how, some of the success stories
that we wanted to talk about, right?

92
00:06:03,282 --> 00:06:07,202
So here I have put down like
two examples, like, so, probably

93
00:06:07,202 --> 00:06:10,882
like in, in, in a retail industry
and then the dynamic pricing.

94
00:06:11,152 --> 00:06:14,522
So I wanted to take a minute and
understand, help you out or, talk through,

95
00:06:14,522 --> 00:06:18,222
you know, What are all the use cases
that are possible, by using it, right?

96
00:06:18,222 --> 00:06:19,592
Some of the use cases possible.

97
00:06:20,012 --> 00:06:23,882
So let's assume that, let's take
an example of Amazon, where, it

98
00:06:23,902 --> 00:06:27,522
recommends products dynamically
based on browsing, probably purchase

99
00:06:27,522 --> 00:06:30,122
history and similar user data.

100
00:06:30,692 --> 00:06:34,922
So the outcome of this is obviously higher
conversion and then it also gives us

101
00:06:35,112 --> 00:06:37,452
the customer, it enhances the loyalty.

102
00:06:38,032 --> 00:06:42,442
So when it comes to banking and finance,
the fraud detection algorithms, flag

103
00:06:42,522 --> 00:06:48,092
unusual activity or a transaction in,
real time prevents, so the outcome result

104
00:06:48,142 --> 00:06:50,102
is to prevent losses and improve trust.

105
00:06:50,192 --> 00:06:54,592
Say, I think some of us, who might have
been shopping in a different state or

106
00:06:54,592 --> 00:06:59,752
shopping in a different place where the
system did not detect the, transaction

107
00:06:59,752 --> 00:07:01,872
in the past street, it tries to block it.

108
00:07:01,882 --> 00:07:07,072
So, in a near real time, it helps
to avoid, the fraud transactions or

109
00:07:07,132 --> 00:07:11,562
probably if our cards are being lost,
it helps in detecting that information

110
00:07:11,562 --> 00:07:13,692
and try to, stop the wrong transactions.

111
00:07:14,312 --> 00:07:19,022
So when it comes to, healthcare, I think
probably, monitoring, the patient vitals

112
00:07:19,042 --> 00:07:24,602
through, IOT devices and, and alerting
doctors, about anomalies, instantly.

113
00:07:25,152 --> 00:07:29,642
so this will help with the faster
response and, probably like the

114
00:07:29,642 --> 00:07:31,712
better patient outcome, right?

115
00:07:31,772 --> 00:07:36,012
and when it comes to the media, so I
think, the Netflix plays a bigger role,

116
00:07:36,152 --> 00:07:38,572
in a day to day, life of an person.

117
00:07:38,572 --> 00:07:40,932
So Netflix, Netflix,
probably like personalizes.

118
00:07:41,317 --> 00:07:43,837
content recommendation,
based on the viewing history.

119
00:07:43,857 --> 00:07:49,007
And then, so probably like a time
based pattern, like it derives a mood,

120
00:07:49,007 --> 00:07:53,777
the mood of a person, and then tries
to detect the time based pattern.

121
00:07:53,777 --> 00:07:56,907
And then probably based on
the demographics, right?

122
00:07:57,487 --> 00:08:01,377
so this will give increased viewer
engagement and retention, right?

123
00:08:01,887 --> 00:08:06,392
when it comes to retail, so some of
these Personalized discount offers to

124
00:08:06,932 --> 00:08:11,302
in store customers based on their mobile
app, using and browsing data, right?

125
00:08:11,802 --> 00:08:16,822
It turns out to be improved sales
and then in store experiences.

126
00:08:17,322 --> 00:08:21,912
When it comes to airlines, airlines can
dynamically, airlines do dynamically,

127
00:08:22,432 --> 00:08:27,582
adjust the pricing, or suggest add
ons based on user search behavior.

128
00:08:28,112 --> 00:08:31,992
it's an, again, optimized revenue
and better customer targeting.

129
00:08:32,302 --> 00:08:38,532
So this directly, empowers, the
customer, And this directly, empowers,

130
00:08:38,592 --> 00:08:42,862
enhanced customer experience and it
also helps in the customer revenue.

131
00:08:43,212 --> 00:08:47,592
So, it's, it's a win win situation
in, the cases where the real time,

132
00:08:47,632 --> 00:08:48,912
analytics are being, deployed.

133
00:08:49,412 --> 00:08:52,352
So let's move on to the next, slide.

134
00:08:53,062 --> 00:08:57,142
So let's try to understand, I have
just, gone through, talking about,

135
00:08:57,312 --> 00:09:01,462
a quick rundown of, what is, what
are the components of building

136
00:09:01,462 --> 00:09:02,792
a real time, framework, right?

137
00:09:03,512 --> 00:09:05,792
So let's try to understand,
a little, detail.

138
00:09:05,872 --> 00:09:09,822
So, It's primarily, five
to six, six step process.

139
00:09:09,822 --> 00:09:13,432
So, it's a data ingestion,
where, millions of transactions

140
00:09:13,452 --> 00:09:15,392
per second, is being handled.

141
00:09:15,442 --> 00:09:17,972
I'm going to talk through what
are all the, design in detail.

142
00:09:18,612 --> 00:09:23,402
but, in this particular case, there
are various, sources of, data ingestion

143
00:09:23,462 --> 00:09:27,922
and then these data ingestion helps
to handle millions of transactions

144
00:09:27,922 --> 00:09:31,722
to get from diverse, sources like
web, IoT devices, transaction

145
00:09:31,722 --> 00:09:33,792
systems, payment systems, et cetera.

146
00:09:34,782 --> 00:09:38,982
so, and then the next step in the
process is, you call it as a stream

147
00:09:38,982 --> 00:09:43,382
processing, you want this to be,
processed as a stream, not as a batch.

148
00:09:43,792 --> 00:09:48,812
so, during this layer, we process
the data, cleanse the data, and

149
00:09:48,812 --> 00:09:53,042
then, like remove duplicates, and
then take actionable, insights to

150
00:09:53,052 --> 00:09:54,702
get into the actionable insights.

151
00:09:54,702 --> 00:09:58,212
You, like, this is a critical
layer through which, where the

152
00:09:58,212 --> 00:09:59,522
data cleansing happens, right?

153
00:10:00,092 --> 00:10:04,682
And, then, we will have the data storage,
obviously, when it comes to the real

154
00:10:04,682 --> 00:10:08,752
time analytics in order to take some,
insights into it, the, the data will be

155
00:10:08,752 --> 00:10:13,832
stored, in, like says, either, NoSQL or
SQL or, depends on the data sometimes we

156
00:10:13,832 --> 00:10:16,107
wanted to store in time series databases.

157
00:10:16,917 --> 00:10:19,847
And here come, and next the
mission learning engine.

158
00:10:20,337 --> 00:10:23,787
in this case, we, we will
try to generate the real time

159
00:10:23,787 --> 00:10:26,997
recommendations, predictions, and
provide the customer insights.

160
00:10:27,657 --> 00:10:33,247
And then, the insights has to be, sent to,
the, like, for the consumers to consume.

161
00:10:33,817 --> 00:10:35,207
and, like we, we will.

162
00:10:35,862 --> 00:10:39,202
Empower this data through
APIs or through service layer.

163
00:10:39,822 --> 00:10:44,852
so at this phase you have a two mechanisms
where we call it as a push delivery and

164
00:10:44,852 --> 00:10:49,182
pull delivery where, the model is emitting
the data so someone can consume it.

165
00:10:49,652 --> 00:10:54,232
Or, in another case where we, have
a pull delivery where we keep it

166
00:10:54,232 --> 00:10:57,242
open and the consumers will start
listening to us and start getting it.

167
00:10:57,712 --> 00:11:02,862
So, depends on, the use case situation,
the teams, will choose the option.

168
00:11:03,512 --> 00:11:07,822
And when it all comes down to, as
we implement the great, framework,

169
00:11:07,822 --> 00:11:09,202
we also wanted to ensure that.

170
00:11:09,767 --> 00:11:13,007
There's monitoring and observability
pieces has been covered, right?

171
00:11:13,457 --> 00:11:18,087
so, just to ensure that the application
runs stable, accurate and scalable.

172
00:11:18,587 --> 00:11:23,137
so let's try to dive, deep into,
the, data ingestion pipeline, right?

173
00:11:23,197 --> 00:11:26,797
So I'm going to take one, one
of the technology as an example.

174
00:11:26,847 --> 00:11:29,997
And then, probably I'll say,
maybe what are all the tools

175
00:11:30,027 --> 00:11:31,727
available, to do some of this work.

176
00:11:31,937 --> 00:11:31,997
Thank you.

177
00:11:32,617 --> 00:11:36,667
so I'm taking Kafka as an example,
as a streaming platform where, so

178
00:11:36,737 --> 00:11:41,917
from various sources of, the, systems
are the data, emitting systems.

179
00:11:41,967 --> 00:11:44,807
we will start sending the data to a Kafka.

180
00:11:45,447 --> 00:11:49,067
and then, we will start, having a
processing layer read from the Kafka.

181
00:11:49,457 --> 00:11:50,597
If you are imagining a
web, a web application.

182
00:11:50,647 --> 00:11:54,827
web or IOT devices or mobile
apps or could be social media or

183
00:11:54,827 --> 00:11:56,197
could be transactional system.

184
00:11:56,207 --> 00:11:58,217
All these will start
emitting the data to Kafka.

185
00:11:58,757 --> 00:12:04,127
So we provide an API layer or the system
provides an API layer or, through other

186
00:12:04,127 --> 00:12:06,257
mechanisms, it starts emitting Kafka.

187
00:12:07,112 --> 00:12:10,652
When we create a af, when we,
consider Kafka, for our data ion

188
00:12:10,982 --> 00:12:15,602
aspect, some things to consider,
for an better performing Kafka.

189
00:12:15,602 --> 00:12:18,682
So probably segmenting, the.

190
00:12:19,372 --> 00:12:23,072
Kafka, by the customer
partition, or probably like

191
00:12:23,072 --> 00:12:24,192
I'll put in a different word.

192
00:12:24,492 --> 00:12:28,862
Partitioned, by, the customer or
event type is required to maintain

193
00:12:28,862 --> 00:12:30,362
the sequencing if required.

194
00:12:30,872 --> 00:12:34,162
and probably like, obviously Kafka
comes up with the compression.

195
00:12:34,212 --> 00:12:38,112
It has, I believe, three of them, Snap,
EasyZip, and, and I forgot the other one,

196
00:12:38,492 --> 00:12:40,852
but I think depends on, the need basis.

197
00:12:40,852 --> 00:12:44,652
I think using one of these algorithm will
help for the faster throughput, right?

198
00:12:45,142 --> 00:12:47,832
when, when we are reading or
when we are, writing into it.

199
00:12:48,122 --> 00:12:52,562
So I, I personally like to use Snappy
for faster throughput, but I think all

200
00:12:52,572 --> 00:12:54,307
the other, compressions would work great.

201
00:12:54,307 --> 00:12:59,567
When it comes to, streaming, I think
the popular things in the market

202
00:12:59,567 --> 00:13:01,597
is like Apache Spark and Flink.

203
00:13:02,067 --> 00:13:06,347
so one thing to consider when we are
thinking about, one of these, streaming

204
00:13:06,347 --> 00:13:11,627
processing system is to enrich, data
by, joining, the customer, profiles,

205
00:13:11,727 --> 00:13:13,717
either in a, Redis or DynamoDB.

206
00:13:14,367 --> 00:13:18,957
so aggregate metrics such as clicks,
probably like purchase or, session

207
00:13:18,967 --> 00:13:23,917
duration and then probably like,
filter out, like, the invalid data or

208
00:13:24,007 --> 00:13:27,787
probably like we just, in a simpler
words, filter out the noisy enables

209
00:13:27,787 --> 00:13:32,097
or noisy records, using, some of
these Python rule, based, engines.

210
00:13:32,097 --> 00:13:32,837
Thanks.

211
00:13:33,337 --> 00:13:38,347
So, when it comes to storage, I think
there are multiple variations of storage.

212
00:13:38,417 --> 00:13:42,447
We have an hot storage probably
for the real time data.

213
00:13:42,817 --> 00:13:46,627
So, DynamoDB or Elasticsearch and
then probably if you wanted to do

214
00:13:46,627 --> 00:13:48,597
some historical analysis and create.

215
00:13:49,012 --> 00:13:51,942
year on year comparison or
month on month comparison.

216
00:13:51,952 --> 00:13:57,492
So storing the data in, S3 or HDFS or
GCS buckets, will play a bigger, role.

217
00:13:58,282 --> 00:14:03,012
when it comes to, the, a critical
portion of it to get the insights,

218
00:14:03,062 --> 00:14:06,232
out of it, like feeding the
data to mission learning, model.

219
00:14:06,512 --> 00:14:11,652
So it's training and, inference where
we wanted to train, models, offline.

220
00:14:12,057 --> 00:14:16,007
so that you're using some customer
interaction, probably like from the data

221
00:14:16,007 --> 00:14:21,607
lakes and then, use, frameworks like
say PyTorch, to, do some of this work.

222
00:14:21,607 --> 00:14:26,607
And then probably, one thing is that it's
always better to store in, store these

223
00:14:26,617 --> 00:14:28,537
models in a centralized model registry.

224
00:14:29,122 --> 00:14:33,372
And then, some models, using this,
one of these fast API based REST

225
00:14:33,372 --> 00:14:36,822
service or probably model serving
framework like TensorFlow, serving.

226
00:14:37,772 --> 00:14:43,592
and, absolutely use Feature Store, like
Feast, to handle, the real time features.

227
00:14:44,302 --> 00:14:48,492
when it comes to API layer, I think,
just to, integrate close with the

228
00:14:48,492 --> 00:14:52,552
API can be written in many languages
and many, frameworks, but just to

229
00:14:52,552 --> 00:14:56,052
integrate closely and, continue to use
the same libraries across the board.

230
00:14:56,562 --> 00:15:00,572
it is better to expose RESTAR,
gRPC APIs to solve this real time

231
00:15:00,572 --> 00:15:02,582
recommendations and then probably insight.

232
00:15:02,582 --> 00:15:07,502
So certainly we can use FAST and FLASK
APIs to derive some of this information.

233
00:15:08,482 --> 00:15:13,752
when it comes to Now the monitoring
aspects of it, I think, we wanted to use

234
00:15:13,752 --> 00:15:19,442
Prometheus, to monitor this Kafka metrics,
like, is there a consumer lag, or is the

235
00:15:19,442 --> 00:15:21,942
data flowing through, properly, et cetera.

236
00:15:21,972 --> 00:15:26,142
And then we wanted to use Grafana
dashboards for, pipeline performance.

237
00:15:26,142 --> 00:15:31,107
And then, you know, We wanted to use
Elk Stack to see and write the errors.

238
00:15:31,987 --> 00:15:36,317
so in order to achieve, the high,
performance patterns, certainly

239
00:15:36,317 --> 00:15:41,217
consider, parallel processing is one
thing to consider something like, use,

240
00:15:41,317 --> 00:15:45,547
Python's multiprocessing or tools like
Ray or CPU based tasks or ensure that

241
00:15:46,047 --> 00:15:47,977
Thread safe, data processing, right?

242
00:15:48,597 --> 00:15:52,027
and, see, the places where the
caching can be enabled, right?

243
00:15:52,087 --> 00:15:55,717
Caching frequently used data
like user profiles in Redis or,

244
00:15:55,767 --> 00:15:57,377
to reduce, database queries.

245
00:15:57,937 --> 00:16:02,017
And then, back pressured, handling
like, so use Kafka consumer groups

246
00:16:02,017 --> 00:16:03,727
to dynamically scale consumers.

247
00:16:03,747 --> 00:16:07,057
and autoscale compute resources
in a cloud, like using

248
00:16:07,277 --> 00:16:09,617
Kubernetes or AWS Lambda, etc.

249
00:16:10,117 --> 00:16:10,337
Right.

250
00:16:10,857 --> 00:16:14,847
So when it comes to the tech stack,
as I was just talking about, like data

251
00:16:15,167 --> 00:16:20,327
ingestion, we can use, Kafka, AWX,
Kinesis, Stream Processing, Apache Spark,

252
00:16:20,377 --> 00:16:22,837
and then probably like Apache Flink.

253
00:16:22,837 --> 00:16:26,687
And when it comes to the storage,
DynamoDB, S3, Cassandra, Influx,

254
00:16:26,747 --> 00:16:29,967
Druid, a lot many variations in
that when it comes to ML training

255
00:16:29,967 --> 00:16:31,437
or inference, TensorFlow, PyTorch.

256
00:16:32,127 --> 00:16:36,127
and then, I call as a data delivery
or a serving layer where we can use

257
00:16:36,187 --> 00:16:40,467
FastAPI, Flask, TensorFlow, Serving,
or when it comes to a monitoring

258
00:16:40,467 --> 00:16:43,447
and observability, Elk Stack, right?

259
00:16:44,127 --> 00:16:49,717
So, as we talk about, millions of
transactions, how do we want it to ensure

260
00:16:49,717 --> 00:16:53,497
that the scalability considerations
are being, in picture, right?

261
00:16:54,242 --> 00:16:58,712
So, as we just talked about, ensure
that, the Kafka is being partitioned,

262
00:16:58,762 --> 00:17:02,862
enough, and then it is being partitioned
by consumer region or a customer region

263
00:17:02,862 --> 00:17:04,532
to handle, the load distribution, right?

264
00:17:05,212 --> 00:17:08,432
sharding, use sharded Redis
cluster for low latency lookups.

265
00:17:08,442 --> 00:17:12,332
And then when it comes to horizontal
scaling, scale, stream processing,

266
00:17:12,382 --> 00:17:14,042
by adding more work on outside.

267
00:17:14,552 --> 00:17:20,622
So some of the challenges, when it comes
to delivering, the, high performance or

268
00:17:20,662 --> 00:17:23,872
high scalable near real time with 99.

269
00:17:23,872 --> 00:17:28,382
9 percent uptime, systems include say,
how do we handle high data ingestion rate?

270
00:17:28,442 --> 00:17:30,952
Use Kafka with, probably
like multiple partitions.

271
00:17:30,952 --> 00:17:34,732
And then, if you want low
latency predictions, deploy the

272
00:17:34,742 --> 00:17:37,177
ML edge or close to the, edge.

273
00:17:37,637 --> 00:17:41,527
consumer, or customer as much as possible
when it comes to data consistency.

274
00:17:41,977 --> 00:17:45,227
like during the stream processing
use exactly once, processing in

275
00:17:45,267 --> 00:17:49,987
Flink or Spark, for fault tolerance
use Retrace and then probably like,

276
00:17:50,017 --> 00:17:51,267
the dead letter queues in Kafka.

277
00:17:52,167 --> 00:17:58,737
So, , this framework, ensures that, we
are, we are building a scalable system

278
00:17:58,737 --> 00:18:00,927
for real time customer analytics.

279
00:18:01,357 --> 00:18:06,477
and at the same time, we also
ensure that it po powers, the,

280
00:18:06,527 --> 00:18:08,467
actionable insights, immediately.

281
00:18:08,517 --> 00:18:13,207
So, like what is the, ROI by
implementing in the real world, right?

282
00:18:13,717 --> 00:18:18,207
So, obviously, as we just, understand,
it's an increased revenue where,

283
00:18:18,257 --> 00:18:22,997
we boost sales by 30 to 35 percent
to 50 percent through, data driven

284
00:18:22,997 --> 00:18:27,557
personalization and, targeting,
targeted market strategies, right?

285
00:18:28,057 --> 00:18:32,777
And then, improved customer
loyalty, reduced customer churn by

286
00:18:32,897 --> 00:18:37,087
up to 25 percent with predictive
insights and tailored engagement.

287
00:18:37,977 --> 00:18:41,605
H U, higher customer satisfaction
through, personalization and,

288
00:18:41,655 --> 00:18:43,045
like proactive support, right?

289
00:18:43,745 --> 00:18:48,845
obviously, it all boils down to how do we
operate efficiently, cutting down support

290
00:18:48,845 --> 00:18:50,955
and marketing expenses, by 20 to 30%.

291
00:18:51,455 --> 00:18:53,705
so what's the future
of, customer experience?

292
00:18:53,755 --> 00:18:59,635
so when we talk about, the customer
experience, it, it is not an, It used

293
00:18:59,635 --> 00:19:04,325
to be the case where the customer
experience used to be the same for,

294
00:19:04,425 --> 00:19:08,525
all the customers that are entering
a store or all the customers that

295
00:19:08,525 --> 00:19:10,135
are trying to get a service, right?

296
00:19:10,585 --> 00:19:14,845
So, the future of customer
experience is hyper personalized,

297
00:19:14,895 --> 00:19:17,445
predictive experiences per customer.

298
00:19:17,575 --> 00:19:24,595
So, say if Amar wants to, Go
to, say target, and then he gets

299
00:19:24,615 --> 00:19:26,325
to see what he wants to see.

300
00:19:26,825 --> 00:19:31,075
and then, so it helps with intelligent
AI driven customer support where,

301
00:19:31,125 --> 00:19:36,495
the, Answers to some of these customer
queries can be, quicker, no wait times,

302
00:19:36,595 --> 00:19:39,475
and then better customer, interactions.

303
00:19:40,165 --> 00:19:44,835
so, and like using this, we will
have the real time omni channel

304
00:19:44,835 --> 00:19:49,005
interactions, a customer moving from a
web to mobile app to a store and vice

305
00:19:49,005 --> 00:19:52,695
versa, or, coming from a marketing
channel, coming from a marketing app.

306
00:19:52,955 --> 00:19:56,505
So this empowers to have like a
seamless integration across the board.

307
00:19:57,005 --> 00:20:01,115
And then, using this adaptive analytics,
we can be able to derive some of

308
00:20:01,135 --> 00:20:02,785
these strategic insights, right?

309
00:20:03,525 --> 00:20:07,985
advanced real time data analytics
is, revolutionizing the complete

310
00:20:08,005 --> 00:20:11,825
customer experience and then,
delivering, intelligent insights.

311
00:20:12,345 --> 00:20:17,605
And then, like customizing or deeply
personalized interactions, between

312
00:20:17,605 --> 00:20:21,315
the customer and the customer
reps, across every touch point.

313
00:20:21,815 --> 00:20:26,455
So what are the key takeaways, from
this, real time data analytics?

314
00:20:26,455 --> 00:20:32,140
So, I think enhanced customer expect,
expectation, Enhanced customer

315
00:20:32,140 --> 00:20:38,110
satisfaction and this both boils
down to increasing the ROI, right?

316
00:20:38,610 --> 00:20:40,290
So what are the next
steps in this journey?

317
00:20:40,810 --> 00:20:47,030
so like understand, what is the, what
is, what data is available and then

318
00:20:47,050 --> 00:20:51,430
probably understand the critical touch
points and then identifying, the high

319
00:20:51,430 --> 00:20:53,510
potential, analytical opportunities.

320
00:20:53,870 --> 00:20:55,960
Not every data can be, used.

321
00:20:56,010 --> 00:21:00,510
the data that is continuous and
the data that is required for the

322
00:21:00,510 --> 00:21:04,740
business and the data that enhances
the business, operations, enhances

323
00:21:04,740 --> 00:21:08,830
the customer experience, enhances the
ROI is where we wanted to set some

324
00:21:08,830 --> 00:21:13,370
time and then understand, the customer
interactions, and the data available

325
00:21:13,510 --> 00:21:15,100
and probably start with the next step.

326
00:21:15,730 --> 00:21:19,450
And then, depends on the technology
stack that is already existing

327
00:21:19,450 --> 00:21:22,910
and then depending on the data
volumes that is available.

328
00:21:23,380 --> 00:21:27,990
we will start deploying, advanced
real time analytics platform, ensuring

329
00:21:28,000 --> 00:21:32,500
that, there's a seamless technological
adoption and then probably ensure that

330
00:21:32,500 --> 00:21:34,460
it is cross functional, adaptability.

331
00:21:35,230 --> 00:21:35,730
so.

332
00:21:36,230 --> 00:21:38,100
In all the cases, we
don't need to use Kafka.

333
00:21:38,100 --> 00:21:40,760
If you are getting like say 10
transactions every one hour,

334
00:21:40,840 --> 00:21:43,830
probably like they might directly
emit to some of these, databases.

335
00:21:43,880 --> 00:21:45,090
We don't need to have Kafka.

336
00:21:45,710 --> 00:21:47,630
example I was just talking about, right?

337
00:21:47,690 --> 00:21:52,150
So as I just called out earlier,
in every case, we don't need to

338
00:21:52,180 --> 00:21:54,900
have the same set of technology
or same set of streams, right?

339
00:21:55,070 --> 00:21:58,590
I was just talking to you about how do
we handle and high performance things.

340
00:21:58,990 --> 00:22:00,285
There might be cases where.

341
00:22:00,425 --> 00:22:05,035
The data volume is low and the insights
that we are taking might not be immediate.

342
00:22:05,125 --> 00:22:06,875
It could be a batch processing, right?

343
00:22:07,275 --> 00:22:11,265
So understand the business acumen
and understand the business use

344
00:22:11,265 --> 00:22:14,945
case and start deriving, the
architecture around those patterns.

345
00:22:15,445 --> 00:22:21,115
So, as, we are almost end of our,
session, let's embrace, the real

346
00:22:21,155 --> 00:22:23,045
time, evaluation, right, revolution.

347
00:22:23,525 --> 00:22:27,565
So when it comes to real time data
analytics, as, we talked multiple times,

348
00:22:27,565 --> 00:22:32,285
it is transforming how the customer
experience, should look like, right?

349
00:22:32,915 --> 00:22:37,975
and probably, businesses start gaining the
competitive advantage and then probably

350
00:22:37,975 --> 00:22:43,225
building the customer, relation and, drive
a sustainable growth, in this journey.

351
00:22:43,725 --> 00:22:48,405
So as, we come to the end, thank you for
taking, time in, attending this session.

352
00:22:49,130 --> 00:22:53,430
if you would like to learn more or if you
have any questions, feel free to reach out

353
00:22:53,460 --> 00:22:58,640
to me, on my LinkedIn, and you can reach
out to my LinkedIn at amarnathimmadisetti.

354
00:22:59,280 --> 00:23:01,520
thank you again and, have a good day.

