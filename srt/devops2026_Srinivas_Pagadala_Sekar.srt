1
00:00:00,500 --> 00:00:04,400
Speaker 41: Hello everyone and
welcome to Con 40 two's DevOps 2026.

2
00:00:04,580 --> 00:00:08,420
I am Riva p Shaker and thank
you for joining me today.

3
00:00:09,260 --> 00:00:13,910
I'm excited to share with you a topic
that has become increasingly critical

4
00:00:13,910 --> 00:00:18,785
in our industry as we navigate the
complexity of modern software delivery.

5
00:00:19,285 --> 00:00:24,505
Today we are going to explore how
GitHub's observability and artificial

6
00:00:24,505 --> 00:00:30,085
intelligence converge to create resilient
systems that automatically detect and

7
00:00:30,085 --> 00:00:35,475
recover from failures significantly
reducing downtime and minimizing the

8
00:00:35,475 --> 00:00:37,905
need for human inter intervention.

9
00:00:38,684 --> 00:00:41,625
So this isn't just about automation for.

10
00:00:42,330 --> 00:00:43,290
Automation's sake.

11
00:00:43,860 --> 00:00:48,510
It's about fundamentally transforming
how we think about reliability and

12
00:00:48,510 --> 00:00:51,060
resilience in our CICD pipelines.

13
00:00:51,660 --> 00:00:56,180
Moving from reactive posture
to a truly proactive one where

14
00:00:56,240 --> 00:01:00,920
our systems can heal themselves
before problems impact our users.

15
00:01:01,905 --> 00:01:04,354
So let me tell you a small story.

16
00:01:04,544 --> 00:01:07,964
Let's picture yourself as a
platform operations engineer.

17
00:01:08,774 --> 00:01:13,794
And it's about 3:00 AM in the morning
your phone buses and you get alerted.

18
00:01:14,784 --> 00:01:17,564
Another deployment has gone sideways.

19
00:01:17,654 --> 00:01:20,884
So you scramble out of your
bed, open up your laptop, and

20
00:01:20,884 --> 00:01:22,714
start the familiar archeology.

21
00:01:23,244 --> 00:01:27,624
So you started to wonder like
what changed, who deployed

22
00:01:27,624 --> 00:01:29,524
what where was the drift?

23
00:01:29,704 --> 00:01:35,704
And honestly, this was my life
for years, and I know many

24
00:01:35,704 --> 00:01:37,054
of you have been there too.

25
00:01:37,554 --> 00:01:42,144
Today I want to show you how we can
write a different ending to that story.

26
00:01:42,644 --> 00:01:47,444
So before we dive into the technical
content let me take a moment to

27
00:01:47,444 --> 00:01:49,054
introduce myself and share a bit.

28
00:01:49,634 --> 00:01:50,864
About my background.

29
00:01:51,854 --> 00:01:57,284
I currently serve as a senior site
reliability manager, and I have been

30
00:01:57,284 --> 00:02:02,354
fortunate to spend over 15 years working
across various domains in our industry.

31
00:02:03,254 --> 00:02:07,454
I have played roles as a DevOps
engineer, cloud infrastructure designer.

32
00:02:07,709 --> 00:02:11,279
A platform engineer and a
cloud native tech as well.

33
00:02:11,879 --> 00:02:15,659
So throughout this journey, I have
worked on building resilient Kubernetes

34
00:02:15,659 --> 00:02:21,340
platforms by implementing GI Ops practices
at scale working with organizations

35
00:02:21,390 --> 00:02:27,210
that span multiple industries including
retail, finance, banking, and the

36
00:02:27,210 --> 00:02:29,190
technology modernization initiatives.

37
00:02:30,180 --> 00:02:35,040
So what truly drives me is the passion
for transforming how organizations

38
00:02:35,070 --> 00:02:41,530
deploy and manage software in production
environments, and helping teams to move

39
00:02:41,530 --> 00:02:47,380
more fragile manual processes to robust
automated systems that can withstand

40
00:02:47,410 --> 00:02:48,460
the pressure of modern business demands.

41
00:02:48,960 --> 00:02:54,739
When I'm not immersed in platform
engineering or writing pipelines I love

42
00:02:54,739 --> 00:03:00,019
following auto sports and I explore
a lot into the fascinating world of

43
00:03:00,019 --> 00:03:05,609
autonomous automotive engineering and
innovation, which increasingly shares

44
00:03:05,609 --> 00:03:09,569
my parallels with the self-healing
systems that we will be discussing today.

45
00:03:10,069 --> 00:03:15,049
So let me start by painting a picture
of the critical challenge that we face

46
00:03:15,049 --> 00:03:20,069
today because understanding the problem
deeply is essential to appreciate the

47
00:03:20,069 --> 00:03:21,869
solution that we are going to explore.

48
00:03:22,829 --> 00:03:29,699
The statistics here are quite sobbing and
they represent a real pain that many of

49
00:03:29,699 --> 00:03:32,039
you have likely experienced firsthand.

50
00:03:32,539 --> 00:03:36,289
When we look at the production
incidents across the industry we find

51
00:03:36,289 --> 00:03:41,089
that approximately 70 percentage of
them are caused by deployment failures

52
00:03:41,719 --> 00:03:43,909
using traditional CICD pipelines.

53
00:03:44,409 --> 00:03:45,969
Think about that for a moment.

54
00:03:46,069 --> 00:03:52,009
Nearly three quarters of the issues
that wake up your on-call engineers

55
00:03:52,039 --> 00:03:55,129
in the middle of the night aren't
due to infrastructure failures

56
00:03:55,129 --> 00:03:57,804
or external factors, so they.

57
00:03:58,594 --> 00:04:04,704
Our cost by the very process we use to
deliver value to our customers even more

58
00:04:04,704 --> 00:04:09,524
concerning is that the average meantime
to recovery for failed deployments without

59
00:04:09,524 --> 00:04:15,854
automation typically ranges from two
to four hearts, which in today's world

60
00:04:16,554 --> 00:04:23,334
represents not just the revenue lost, but
eroded customer trust and competitiveness.

61
00:04:24,309 --> 00:04:29,049
So the fundamental difference between
traditional CICD and what we are calling

62
00:04:29,049 --> 00:04:31,929
modern platform comes down to one word.

63
00:04:32,589 --> 00:04:33,909
It's the technical posture.

64
00:04:34,409 --> 00:04:38,669
Traditional approaches are inherently
reactive, where teams scramble to

65
00:04:38,669 --> 00:04:43,139
fix issues only after they have
already broken in production.

66
00:04:43,639 --> 00:04:48,319
This creates a cascading series of
problems where manual interventions

67
00:04:48,319 --> 00:04:53,329
create delays due to human errors
or the problems that compound to

68
00:04:53,329 --> 00:04:56,419
initial problems and pile up overtime.

69
00:04:56,804 --> 00:05:01,124
Which increases the incident
fatigue and also degrades the

70
00:05:01,124 --> 00:05:02,654
team's performance and moral.

71
00:05:03,154 --> 00:05:07,324
So in contrast modern platforms
take a proactive approach where

72
00:05:07,324 --> 00:05:11,014
self-healing systems detect and
resolve issues automatically.

73
00:05:11,514 --> 00:05:16,794
So this isn't about replacing human
intelligence, it is about argumenting it.

74
00:05:17,294 --> 00:05:22,935
Allowing our engineers to focus on
innovation rather than fighting fires.

75
00:05:23,435 --> 00:05:27,794
Through intelligent automation we
can reduce oil, minimize downtime,

76
00:05:28,004 --> 00:05:32,814
and maintain reliability at scale
in ways that were simply impossible

77
00:05:32,814 --> 00:05:34,284
with the manual processes.

78
00:05:34,784 --> 00:05:37,905
This brings us to the critical
question that will frame

79
00:05:37,905 --> 00:05:39,284
our entire discussion today.

80
00:05:39,645 --> 00:05:47,094
So how do we move from what I call is
deploy and pray to deploy with confidence?

81
00:05:47,594 --> 00:05:51,014
So how do we build the systems
that are resilient by design, that

82
00:05:51,014 --> 00:05:56,085
can detect problems early and that
can take corrective action faster

83
00:05:56,085 --> 00:05:57,764
than any human operator code?

84
00:05:58,264 --> 00:06:00,979
So that's exactly what we
are going to explore today.

85
00:06:01,479 --> 00:06:05,159
So in today's session we've, it
is designed in a way to give you

86
00:06:05,159 --> 00:06:10,149
both a conceptual framework and a
practical insights needed to architect

87
00:06:10,149 --> 00:06:15,239
resiliency ICD pipelines using
Tecton and Argo CD that can truly

88
00:06:15,239 --> 00:06:17,489
heal themselves when problems occur.

89
00:06:17,989 --> 00:06:22,729
So we are going to present a GI ops native
approach to self-healing deployments that

90
00:06:22,729 --> 00:06:28,459
stands on three powerful pillars, each
essential to the overall architecture.

91
00:06:28,959 --> 00:06:33,829
The first pillar is observability
which provides real time visibility

92
00:06:33,889 --> 00:06:39,739
into our systems health performance
metrics and deployment status

93
00:06:39,769 --> 00:06:41,589
across the entire platform.

94
00:06:42,089 --> 00:06:45,169
So you simply cannot
heal what you cannot see.

95
00:06:45,270 --> 00:06:49,290
And the comprehensive observability
is where, is the key foundation

96
00:06:49,290 --> 00:06:52,410
upon which all the self-healing
capabilities are built.

97
00:06:52,910 --> 00:06:59,030
So we need to know, not just that
something is wrong, but we need

98
00:06:59,030 --> 00:07:05,540
to know what precisely what is
wrong and how severe it is and what

99
00:07:05,540 --> 00:07:07,280
the broader context looks like.

100
00:07:07,780 --> 00:07:11,950
The second pillar is the automation,
which eliminates the manual

101
00:07:11,950 --> 00:07:17,230
toil and human error through
intelligent policy driven deployment

102
00:07:17,230 --> 00:07:19,720
processes and recovery mechanisms.

103
00:07:20,320 --> 00:07:25,190
So this goes beyond simple scripts
when we are talking about sophisticated

104
00:07:25,190 --> 00:07:31,340
automation that can make nuanced
decisions based on multiple signals.

105
00:07:32,030 --> 00:07:38,090
And which can execute complex recovery
procedures and also maintain detailed

106
00:07:38,090 --> 00:07:40,370
audit trails for every action taken.

107
00:07:40,870 --> 00:07:45,450
So the third pillar is the AI integration
where mission learning, power decision

108
00:07:45,450 --> 00:07:51,660
making is done from the patterns in your
specific environment, and it optimizes

109
00:07:51,660 --> 00:07:54,370
reliability at scale over, over time.

110
00:07:54,870 --> 00:07:59,010
So this is where the system evolves
from simply responding to known failure

111
00:07:59,010 --> 00:08:03,360
modes, to predicting and preventing
issues that haven't even occurred yet.

112
00:08:03,860 --> 00:08:08,930
So our overarching goal today is to show
you how to drive reliability at scale

113
00:08:09,140 --> 00:08:13,860
across modern Kubernetes platforms while
maintaining the development velocity

114
00:08:13,890 --> 00:08:16,170
and reducing the operational overhead.

115
00:08:16,670 --> 00:08:19,130
So these aren't just competing concerns.

116
00:08:19,160 --> 00:08:24,830
They are synergistic when approached
correctly, and that's the paradigm

117
00:08:24,830 --> 00:08:27,050
shift that we are aiming for.

118
00:08:27,550 --> 00:08:31,120
So let me walk you through the
roadmap for our session today.

119
00:08:31,170 --> 00:08:37,080
So you know what to expect and can see how
each piece builds upon the previous ones.

120
00:08:37,580 --> 00:08:41,530
We will begin with the GitHubs
GitHubs fundamentals exploring

121
00:08:41,530 --> 00:08:45,940
why GitHubs provides the essential
foundation for resilient deployments.

122
00:08:46,665 --> 00:08:50,745
And why you cannot build truly
self-healing systems without this

123
00:08:50,745 --> 00:08:53,085
declarative, git centric approach.

124
00:08:53,585 --> 00:08:59,505
From there, we will move into resilience
patterns examining the key use cases

125
00:08:59,505 --> 00:09:03,495
and the real world scenarios where
self-healing mechanisms prevent

126
00:09:03,525 --> 00:09:05,255
outages before they impact the users.

127
00:09:05,755 --> 00:09:09,595
Next we will take a deep dive
into self-healing architecture,

128
00:09:09,925 --> 00:09:14,965
unpacking exactly how deployments can
automatically detect problems and fix

129
00:09:14,965 --> 00:09:16,765
themselves without human intervention.

130
00:09:17,265 --> 00:09:22,145
So to ground this in reality we will
then examine an airline industry

131
00:09:22,145 --> 00:09:26,585
case study, a learning from mission
critical systems where downtime is

132
00:09:26,585 --> 00:09:30,635
quite literally isn't an option, and
where the stakes couldn't be higher.

133
00:09:31,135 --> 00:09:35,335
Following that, we will explore AI
integration in depth discussing how

134
00:09:35,335 --> 00:09:39,935
to implement intelligent automation
throughout your CICD pipeline,

135
00:09:39,965 --> 00:09:42,815
practical, and in a more achievable way.

136
00:09:43,315 --> 00:09:47,565
We will then talk through the high
level flow of the complete architecture

137
00:09:47,565 --> 00:09:53,355
of self hailing deployments, ensuring
you understand how all the pieces

138
00:09:53,355 --> 00:09:55,425
fit together into a cohesive system.

139
00:09:55,925 --> 00:09:59,975
Finally, we will conclude with the
best practices and the key takeaways

140
00:10:00,025 --> 00:10:04,495
giving you actionable strategies that
you can begin implementing immediately

141
00:10:04,495 --> 00:10:09,015
in your organization regardless
of where you are currently in your

142
00:10:09,015 --> 00:10:10,965
current DevOps maturity journey.

143
00:10:11,465 --> 00:10:14,945
So let's establish our foundation
by understanding what GitHub's

144
00:10:14,945 --> 00:10:19,595
really is, because there is often
confusion about this particular term.

145
00:10:20,095 --> 00:10:23,395
GitHubs is a way of implementing
continuous deployment for cloud

146
00:10:23,395 --> 00:10:27,565
native applications that focuses
on a developer centric experience

147
00:10:27,565 --> 00:10:29,215
when operating infrastructure.

148
00:10:29,715 --> 00:10:34,625
So fundamentally it means using tools
that developers are relat familiar

149
00:10:34,625 --> 00:10:40,205
with, particularly git and continuous
deployment tools to manage not just the

150
00:10:40,205 --> 00:10:44,615
application code, but the infrastructure
and the configuration as well.

151
00:10:45,115 --> 00:10:48,295
So there are four core
principles that define GitHubs.

152
00:10:48,325 --> 00:10:51,805
And understanding these is
very crucial to everything else

153
00:10:51,805 --> 00:10:54,045
that we will discuss First.

154
00:10:54,155 --> 00:10:57,695
GIT serves as the single source of truth
for everything in your environment.

155
00:10:57,905 --> 00:11:03,775
So this means infrastructure definitions,
application configurations, and policies

156
00:11:03,775 --> 00:11:09,435
and all and other configs are all
defined declaratively in version control.

157
00:11:09,805 --> 00:11:12,355
Providing a complete audit
trail and change history.

158
00:11:12,855 --> 00:11:19,305
So when someone asks who changed what and
when Git gives you the answer instantly.

159
00:11:19,365 --> 00:11:20,745
And Immutably.

160
00:11:21,245 --> 00:11:24,245
So GitHub's relies on
declarative configuration

161
00:11:24,305 --> 00:11:26,285
rather than imperative commands.

162
00:11:26,615 --> 00:11:31,975
So you describe your desire state, not
the steps to get there, but the system

163
00:11:31,975 --> 00:11:36,205
automatically figures out how to achieve
that state and maintains it continuously.

164
00:11:36,705 --> 00:11:40,875
So this is a profound shift from
traditional operations where we

165
00:11:40,875 --> 00:11:44,515
execute sequence of commands and
hope they work correctly in order.

166
00:11:45,015 --> 00:11:49,445
Third is the automated reconciliation
that ensures continuous synchronization

167
00:11:49,505 --> 00:11:54,305
between your git repositories and your
cluster guaranteeing that the actual

168
00:11:54,305 --> 00:11:56,705
state always matches the decide state.

169
00:11:57,205 --> 00:11:59,935
So this isn't a one-time deployment.

170
00:11:59,995 --> 00:12:03,895
It is a continuous process where
Drift is deducted and corrected

171
00:12:03,895 --> 00:12:08,295
automatically maintaining consistency
without manual intervention.

172
00:12:08,795 --> 00:12:11,435
And finally everything is
observable and auditable.

173
00:12:11,585 --> 00:12:19,355
So you have complete visibility into
what changed, when they changed it, and

174
00:12:19,355 --> 00:12:21,695
why that particular change was made.

175
00:12:22,195 --> 00:12:26,525
So every modification is tracked, reviewed
through a standard development practice

176
00:12:26,525 --> 00:12:31,405
like pull request, and most importantly
and completely, it is reversible.

177
00:12:31,905 --> 00:12:36,405
And now why does GitHub's matter
specifically for resilience?

178
00:12:37,035 --> 00:12:39,765
So there are several
compelling reasons for that.

179
00:12:40,665 --> 00:12:45,435
First, it provides instant rollback
capability, meaning any deployment

180
00:12:45,435 --> 00:12:48,585
can be rolled back immediately
with a simple git revert.

181
00:12:48,975 --> 00:12:53,025
And no complex procedures or
lengthy runbooks are required.

182
00:12:53,525 --> 00:12:59,705
Second, you get built in change
management where every change goes

183
00:12:59,705 --> 00:13:04,805
through a peer review via pull request,
ensuring quality and knowledge sharing

184
00:13:04,805 --> 00:13:06,665
before anything reaches production.

185
00:13:07,165 --> 00:13:11,365
And the automatic drift direction
means that manual changes or

186
00:13:11,365 --> 00:13:15,805
configuration drift are identified
immediately preventing those.

187
00:13:16,055 --> 00:13:18,855
Frustrations like he, it worked yesterday.

188
00:13:19,365 --> 00:13:23,295
Scenarios that plague pretty
much the operations team when

189
00:13:23,295 --> 00:13:26,445
someone makes an undocumented
change directly into the cluster.

190
00:13:26,945 --> 00:13:30,395
So GitHub's eliminates this entire
category of problems by making

191
00:13:30,395 --> 00:13:34,535
it impossible for the cluster
state to deviate from Git without

192
00:13:34,535 --> 00:13:36,095
being deducted and corrected.

193
00:13:36,595 --> 00:13:40,195
Now that we understand GitHubs
conceptually, let's look at the

194
00:13:40,195 --> 00:13:43,675
specific cloud native stack that
implements these principles.

195
00:13:43,675 --> 00:13:47,935
In practice, the architecture
consists of four key components

196
00:13:47,965 --> 00:13:49,915
that work together seamlessly.

197
00:13:50,725 --> 00:13:54,295
So at the foundation we have
our Git repository, which

198
00:13:54,295 --> 00:13:55,855
serves as the source of truth.

199
00:13:56,575 --> 00:14:00,985
This repository contains application code,
infrastructure definitions, and Kubernetes

200
00:14:00,985 --> 00:14:04,705
manifest that describes everything
about your desired system state.

201
00:14:05,205 --> 00:14:09,495
So this isn't just storing files
and git, it is treating Git as the

202
00:14:09,525 --> 00:14:13,855
authoritative of how your entire
system should look like and behave.

203
00:14:14,355 --> 00:14:19,545
Next we have our CI pipeline, which in
our case is implemented using Tecton.

204
00:14:20,415 --> 00:14:24,905
So tecton is a cloud native solution that
everybody is aware about which handles the

205
00:14:24,905 --> 00:14:27,395
building, testing and security scanning.

206
00:14:27,425 --> 00:14:30,835
Stamping of images and
image promotion workflows.

207
00:14:31,335 --> 00:14:37,125
What makes Teton particularly powerful is
that the pipelines themselves are defined

208
00:14:37,125 --> 00:14:42,285
as native Kubernetes resources, meaning
they follow the same declarative patterns

209
00:14:42,285 --> 00:14:46,585
and benefit from the same operational
practices similar to your applications.

210
00:14:47,085 --> 00:14:51,310
So in the CI pipeline is responsible
for taking code from developers,

211
00:14:51,310 --> 00:14:52,780
validating it thoroughly.

212
00:14:53,200 --> 00:14:55,750
And producing artifacts that
are ready for deployment.

213
00:14:56,680 --> 00:14:59,950
Then comes the CD controller
implemented with Argo cd.

214
00:15:00,880 --> 00:15:04,030
This is where the magic of
continuous reconciliation happens.

215
00:15:05,020 --> 00:15:09,190
Argo CD continuously monitors your
plus state, compares it against the

216
00:15:09,190 --> 00:15:14,680
design state, and GI detects any drift
that occurs and enables automated

217
00:15:14,680 --> 00:15:16,420
rollback when health checks fail.

218
00:15:17,275 --> 00:15:22,285
So this isn't just a deployment tool,
it is an active reconciliation engine

219
00:15:22,315 --> 00:15:27,115
that always works in the background
to ensure that your cluster matches

220
00:15:27,115 --> 00:15:31,645
your kit repository, which will
internally depict your desired state.

221
00:15:32,145 --> 00:15:36,375
Finally, we have the Kubernetes cluster
itself, where applications actually run.

222
00:15:36,885 --> 00:15:40,575
This cluster has an integrated
observability stack that is

223
00:15:40,575 --> 00:15:41,905
built built along with it.

224
00:15:42,745 --> 00:15:46,535
So that can include various tools
like Prometheus four metrics

225
00:15:46,575 --> 00:15:51,705
Grafana for visualization and open
telemetry for distributor tracing.

226
00:15:52,205 --> 00:15:56,015
So these tools provide the real time
insights that enable self-healing cap

227
00:15:56,045 --> 00:16:01,715
capabilities feeding information back to
Argo City and other components so they

228
00:16:01,715 --> 00:16:06,095
can make intelligent decisions about the
health and behavior of your applications.

229
00:16:06,595 --> 00:16:10,465
So let's, let me walk you through some
real world use cases that illustrate

230
00:16:10,465 --> 00:16:12,265
the power of GitHubs in practice.

231
00:16:12,265 --> 00:16:16,495
Because these scenarios represent
problems that many of you

232
00:16:16,495 --> 00:16:17,965
are dealing with right now.

233
00:16:18,465 --> 00:16:22,035
Understanding how GitHubs solves
these specific challenges will help

234
00:16:22,035 --> 00:16:26,085
you see where you might apply these
patterns in your own environments.

235
00:16:26,585 --> 00:16:30,605
The first use case is an automatic
rollback on health failures.

236
00:16:31,265 --> 00:16:35,065
So imagine a scenario where a
new deployment introduces a sub

237
00:16:35,365 --> 00:16:39,175
subtile memory leak, and that
isn't caught during testing.

238
00:16:40,045 --> 00:16:44,275
So in a traditional environment,
this plays out painfully, the

239
00:16:44,275 --> 00:16:48,845
deployment goes out, memory usage
gradually climbs the application

240
00:16:48,845 --> 00:16:53,315
eventually becomes unresponsive and
the ops team gets paged about it.

241
00:16:53,720 --> 00:16:57,500
And they will have to diagnose the
issue, identify which deployment

242
00:16:57,500 --> 00:16:59,630
cost it, and then manually roll back.

243
00:17:00,590 --> 00:17:07,310
So on the onset, like this entire process
typically takes around 30 minutes or more

244
00:17:08,000 --> 00:17:13,550
during which your users are experiencing
degraded service or even outages.

245
00:17:14,050 --> 00:17:18,510
With GitHubs Argo CD is continuously
monitoring the health of your

246
00:17:18,510 --> 00:17:21,390
applications against defined thresholds.

247
00:17:22,080 --> 00:17:26,310
So when it detects the degradation
pattern, it automatically initiates

248
00:17:26,310 --> 00:17:30,090
a rollback to the last known good
version in under two minutes.

249
00:17:30,750 --> 00:17:31,500
How good is that?

250
00:17:32,190 --> 00:17:36,990
So often before even users even
notice that there was an issue,

251
00:17:37,080 --> 00:17:41,190
the system heals itself faster
than any operator could respond.

252
00:17:41,690 --> 00:17:45,670
The second use case can address
configuration, drift deduction pictures,

253
00:17:45,670 --> 00:17:49,570
a scenario where someone on the team
needs to make a quick fix in production,

254
00:17:50,200 --> 00:17:54,270
maybe adjusting a config map to a
change to change a connection timeout.

255
00:17:54,770 --> 00:17:57,830
They make the change directly in the
cluster with Cube CTL because it's

256
00:17:57,830 --> 00:18:03,170
urgent, and then they fully intend
to update the Git repository later.

257
00:18:04,145 --> 00:18:08,485
But when they get pulled into
something else and forget, it is

258
00:18:08,485 --> 00:18:11,725
as equivalent to the situation
in a traditional environment.

259
00:18:12,225 --> 00:18:16,305
So in a traditional environment, this
drift goes unnoticed until the next

260
00:18:16,305 --> 00:18:21,445
deployment cycle when suddenly things
break because the deployment overrides

261
00:18:21,445 --> 00:18:25,975
that manual change that was made by
the ops team and nobody remembers what

262
00:18:25,975 --> 00:18:27,865
was changed or why it was changed.

263
00:18:28,150 --> 00:18:34,420
With GI Ops, this manual config map change
is detected within seconds because Argo

264
00:18:34,420 --> 00:18:38,890
CDs that the cluster state has no longer
matches the MA matches your GIT report.

265
00:18:39,390 --> 00:18:44,640
It can either alert the teams immediately
or depending on your policies or

266
00:18:44,640 --> 00:18:50,190
your settings automatically Argo
can reconcile back to the Git state,

267
00:18:50,820 --> 00:18:55,460
ensuring that all change flows through
your proper change management process.

268
00:18:55,960 --> 00:18:59,735
So the third use case tackles
multi environment consistency.

269
00:19:00,710 --> 00:19:06,330
Addressing that classic term works
on my mission syndrome that has

270
00:19:06,330 --> 00:19:08,250
plagued our industry for decades.

271
00:19:08,760 --> 00:19:10,740
I hope you all agree with me on this.

272
00:19:11,610 --> 00:19:14,520
You have all experienced the
scenario where something works

273
00:19:14,520 --> 00:19:17,670
perfectly in development but
fails mysteriously in production.

274
00:19:18,170 --> 00:19:21,830
Often this happens because the
environments aren't truly identical.

275
00:19:22,400 --> 00:19:24,590
They are subtile
configuration differences.

276
00:19:25,185 --> 00:19:29,415
Different versions of dependencies
and manual changes that were made in

277
00:19:29,415 --> 00:19:31,335
one environment but not the others.

278
00:19:31,835 --> 00:19:35,555
With GitHubs the same Git commit
is promoted through environments

279
00:19:35,585 --> 00:19:37,205
ensuring predictable behavior.

280
00:19:37,705 --> 00:19:41,395
So your production deployment is
using exactly the same configuration

281
00:19:41,395 --> 00:19:45,475
and manifest that worked in staging,
eliminating an entire class of

282
00:19:45,475 --> 00:19:47,005
environment related failures.

283
00:19:47,505 --> 00:19:50,705
So let me continue with three
more use cases that demonstrate

284
00:19:50,705 --> 00:19:54,755
the breadth of problems that self
failing GI Ops systems can address.

285
00:19:55,535 --> 00:19:58,895
So these scenarios go deeper
into the kinds of runtime

286
00:19:59,145 --> 00:20:01,935
issues that are particularly
challenging to handle manually.

287
00:20:02,435 --> 00:20:06,235
The fourth use case specifically which
involves a dependency failure handling.

288
00:20:06,415 --> 00:20:09,895
Like when a database connection
pool becomes exhausted.

289
00:20:10,765 --> 00:20:15,085
So this is particularly not a good
scenario because it often happens

290
00:20:15,135 --> 00:20:20,495
gradually under load when new
requests start failing because they

291
00:20:20,495 --> 00:20:24,185
can't get the database connections,
error rates climb up, but the

292
00:20:24,185 --> 00:20:27,815
application pods themself appear
healthy because they are not crashing.

293
00:20:28,595 --> 00:20:30,665
They're just unable to
serve the request properly.

294
00:20:31,165 --> 00:20:35,515
In a traditional setup by the time
someone notices and starts investigating

295
00:20:35,615 --> 00:20:40,985
you might have already lost ground
on a lot of significant traffic

296
00:20:41,315 --> 00:20:46,245
and customer transactions with Argo
City monitoring not just pod health,

297
00:20:46,275 --> 00:20:49,995
but application level metrics like
error rates and response times.

298
00:20:50,745 --> 00:20:55,275
Can be helpful in detecting this
degradation pattern and automatically

299
00:20:55,325 --> 00:20:58,115
rolling back to the previous
version while simultaneously

300
00:20:58,165 --> 00:20:59,845
alerting the team about the issue.

301
00:21:00,345 --> 00:21:03,315
So the system recognizes that
something about the new deployment

302
00:21:03,315 --> 00:21:06,675
is causing a problem with the
database connectivity, and it takes

303
00:21:06,675 --> 00:21:08,145
a corrective action immediately.

304
00:21:08,645 --> 00:21:12,815
In the fifth use case, we can, we
examine like the cannery deployment

305
00:21:12,815 --> 00:21:16,025
failures, which represent a
more sophisticated failure mode.

306
00:21:17,000 --> 00:21:19,880
So you have deployed a new
version using a cannery strategy.

307
00:21:19,880 --> 00:21:23,540
Initially routing just a small
percentage of your traffic to the

308
00:21:23,540 --> 00:21:27,020
new version while keeping most
traffic on the stable version.

309
00:21:27,520 --> 00:21:31,300
During the cannery phase you detect
elevated error rates or increase

310
00:21:31,300 --> 00:21:32,500
latency in the new version.

311
00:21:33,220 --> 00:21:38,920
So in a traditional setup, someone has to
be actively monitoring these metrics and,

312
00:21:39,420 --> 00:21:43,480
and, have to make a decision about
whether the difference is significant

313
00:21:43,480 --> 00:21:44,830
and manually haul the rollout.

314
00:21:45,330 --> 00:21:50,640
So this might require constant human
attention and introduces the risk

315
00:21:50,670 --> 00:21:52,560
of missing the signal in the noise.

316
00:21:53,060 --> 00:21:57,790
With GitHubs and proper health checks
integrated into your cannery analysis,

317
00:21:58,300 --> 00:22:02,410
the system automatically hals promotion
when the cannery version shows

318
00:22:02,410 --> 00:22:04,015
degradation compared to the baseline.

319
00:22:04,515 --> 00:22:07,575
When the traffic stays on the
stable version, an alert is sent to

320
00:22:07,575 --> 00:22:11,105
the team, which with the detailed
metrics about what went wrong

321
00:22:11,795 --> 00:22:15,895
and the problematic version never
reaches full production in this case.

322
00:22:16,525 --> 00:22:20,095
So here we are preventing a
situation where what could

323
00:22:20,095 --> 00:22:21,325
have been a major incident.

324
00:22:21,825 --> 00:22:23,715
The 60 use case deals with the network.

325
00:22:24,040 --> 00:22:27,590
Partition recovery which is
particularly relevant for distributed

326
00:22:27,590 --> 00:22:31,190
systems running across multiple
clusters or availability zones.

327
00:22:31,690 --> 00:22:35,020
Imagine a scenario where a network
partition temporarily isolates

328
00:22:35,020 --> 00:22:36,280
a part of your infrastructure.

329
00:22:36,780 --> 00:22:41,520
During this partition, some automated
processes or operator might take

330
00:22:41,520 --> 00:22:44,910
changes trying to compensate for
what appears to be a fail note.

331
00:22:45,410 --> 00:22:49,010
When the partition heals, you
suddenly have inconsistent state

332
00:22:49,040 --> 00:22:50,180
across your infrastructure.

333
00:22:50,960 --> 00:22:54,680
So traditional systems can get into
very confused states during these

334
00:22:54,680 --> 00:22:58,790
scenarios requiring manual intervention
to reconcile the differences.

335
00:22:59,290 --> 00:23:05,170
With GitHubs, once the partition heals
Argo CD recognizes that certain parts of

336
00:23:05,170 --> 00:23:10,090
the infrastructure have been drift from
the desired state in Git and automatically

337
00:23:10,090 --> 00:23:14,530
reconciles them, bringing everything back
into alignment without human intervention.

338
00:23:15,030 --> 00:23:18,750
So the system records from partition
automatically because it always

339
00:23:18,750 --> 00:23:23,460
has get as the authoritative source
of truth that it can return to.

340
00:23:23,960 --> 00:23:26,840
So now let's examine the
architecture of self-healing

341
00:23:26,840 --> 00:23:28,430
deployments at a deeper level.

342
00:23:29,360 --> 00:23:33,680
Understanding how the various layers
work together to create resilient systems

343
00:23:34,180 --> 00:23:38,620
in this architecture is built with four
distinct layers, each with specific

344
00:23:38,620 --> 00:23:42,250
responsibilities that contribute to
the overall self-healing capability.

345
00:23:42,750 --> 00:23:46,680
The first layer is observability,
which serves as the sensory

346
00:23:46,710 --> 00:23:48,660
system for your platform.

347
00:23:49,160 --> 00:23:52,910
This includes metrics collection
through Prometheus, log aggregation

348
00:23:52,910 --> 00:23:58,850
through Loki or the ELK Stack and also
distributed traces using ager or tempo.

349
00:23:59,350 --> 00:24:00,010
Together.

350
00:24:00,070 --> 00:24:04,570
These tools provide comprehensive system
visibility, allowing you to understand

351
00:24:04,930 --> 00:24:09,280
not just what is happening right
now, but also the historical patterns

352
00:24:09,280 --> 00:24:10,780
and the context around that event.

353
00:24:11,280 --> 00:24:15,600
Without rich observability, self-healing
is impossible because the system

354
00:24:15,600 --> 00:24:20,920
has no way that it can detect a
problem or verify the remediation

355
00:24:20,920 --> 00:24:22,450
action that actually worked.

356
00:24:22,950 --> 00:24:26,250
The second layer is the analysis
engine, which takes all the

357
00:24:26,310 --> 00:24:28,320
observability data and make sense of.

358
00:24:28,820 --> 00:24:33,450
This includes health checks performed
by Argo cd the SLO and the SLI

359
00:24:33,660 --> 00:24:38,180
monitoring metrics that compares
actual performance against defined

360
00:24:38,270 --> 00:24:43,460
objectives and anomaly detection
algorithms that can identify unusual

361
00:24:43,460 --> 00:24:45,800
patterns that might indicate problems.

362
00:24:46,300 --> 00:24:51,160
This layer is responsible for identifying
issues before they escalate into full

363
00:24:51,160 --> 00:24:55,960
blown outages, giving the system time
to take preventive action rather than

364
00:24:56,080 --> 00:24:58,730
reaching our reacting to failures.

365
00:24:59,230 --> 00:25:03,670
So the third layer is the decision
and the action layer where the system

366
00:25:03,670 --> 00:25:05,950
actually responds to detected problems.

367
00:25:06,520 --> 00:25:09,790
So this includes auto rollback
capabilities through Argo rollouts.

368
00:25:10,520 --> 00:25:14,300
Autoscaling through horizontal pod,
auto scalers, or vertical pod auto

369
00:25:14,300 --> 00:25:18,990
scalers, or even using a Kubernetes,
even driven auto scalers too

370
00:25:19,040 --> 00:25:20,360
which is otherwise called a cada.

371
00:25:20,860 --> 00:25:24,220
These these tools can be
used and helpful for most

372
00:25:24,220 --> 00:25:25,960
sophisticated event driven scaling.

373
00:25:26,615 --> 00:25:32,365
And for traffic shifting we can leverage
service measures like TO or linker D.

374
00:25:32,865 --> 00:25:37,015
This layer embodies the self-healing
capabilities taking concrete actions to

375
00:25:37,015 --> 00:25:41,905
resolve issues automatically based on
the analysis from the previous layer.

376
00:25:42,405 --> 00:25:46,665
The fourth and the final layer is the
GitHub's reconciliation, which closes

377
00:25:46,665 --> 00:25:50,955
the loop by updating git with any
remediation actions that were taken.

378
00:25:51,765 --> 00:25:56,205
Creating a complete audit trail of
what happened and why it happened.

379
00:25:57,075 --> 00:26:01,725
So this layer also handles
notifications to the teams, ensuring

380
00:26:01,725 --> 00:26:06,405
that while the system is healing
itself automatically, humans remain

381
00:26:06,405 --> 00:26:09,045
informed and can intervene if required.

382
00:26:09,545 --> 00:26:13,565
This creates a perfect balance between
automation and human oversight,

383
00:26:14,165 --> 00:26:18,275
where routine issues are handled
automatically, but the team always

384
00:26:18,275 --> 00:26:21,695
has visibility and the ability to
make manual control when needed.

385
00:26:22,195 --> 00:26:26,045
Let me show you what self-healing
mechanisms look like in practice

386
00:26:26,045 --> 00:26:29,165
by walking through a specific
example of health-based auto

387
00:26:29,165 --> 00:26:32,635
rollback with a progressive
traffic shifting will look like.

388
00:26:33,535 --> 00:26:36,665
So this will help you understand
the detailed mechanism mechanics

389
00:26:36,665 --> 00:26:39,905
of how these systems operate
in real deployment scenarios.

390
00:26:40,775 --> 00:26:44,335
Argo rollout is a powerful tool
that extends Kubernetes deployments

391
00:26:44,335 --> 00:26:47,535
with advanced deployment strategies
like cannery and bluegreen.

392
00:26:48,345 --> 00:26:53,565
And what makes it particularly valuable
for self-healing is its ability

393
00:26:53,565 --> 00:26:55,445
to monitor cannery deployments.

394
00:26:56,355 --> 00:26:59,745
With progressive traffic shifting
and automatically rolling

395
00:26:59,745 --> 00:27:01,665
back when health checks fail.

396
00:27:02,445 --> 00:27:04,965
So here is how this works in practice.

397
00:27:05,445 --> 00:27:08,805
So you start by routing just five
percentage of your traffic to

398
00:27:08,805 --> 00:27:12,225
the new version, while keeping 95
percentage on the stable version.

399
00:27:12,725 --> 00:27:15,935
So during this initial phase, the
system is closely monitoring three

400
00:27:15,935 --> 00:27:20,555
categories of metrics, error rates to
see if the new version is generating

401
00:27:20,555 --> 00:27:21,780
more errors than the baseline.

402
00:27:22,030 --> 00:27:27,590
Latency measurements that can to detect
if response times have degraded and

403
00:27:27,590 --> 00:27:31,130
resource usage patterns to identify
if the new version is consuming more

404
00:27:31,130 --> 00:27:36,570
memory or CPU than expected in any of
these metrics exceed defined thresholds,

405
00:27:36,660 --> 00:27:38,310
the rollout is automatically halted.

406
00:27:38,810 --> 00:27:40,910
The system doesn't wait
for a complete failure.

407
00:27:40,940 --> 00:27:44,690
It detects degradation early and
stops the promotion process before the

408
00:27:44,690 --> 00:27:47,510
problematic version can impact more users.

409
00:27:48,010 --> 00:27:52,890
If the cannery phase looks healthy traffic
is gradually increased, perhaps to 10%,

410
00:27:52,950 --> 00:27:56,040
then to 20 percentage, and then to 50%.

411
00:27:56,140 --> 00:27:58,465
With all the health checks
at each stage of the.

412
00:27:58,965 --> 00:28:04,115
This progressive approach means that
even if a problem only manifests under

413
00:28:04,115 --> 00:28:07,865
higher load, you can catch it before
it affects your entire user base.

414
00:28:08,645 --> 00:28:12,245
So looking at the sample code
on this slide, you can see how

415
00:28:12,275 --> 00:28:14,225
this is configured in practice.

416
00:28:14,765 --> 00:28:18,605
So we define a rollout
resource that specify cannery

417
00:28:18,605 --> 00:28:20,525
strategy with multiple steps.

418
00:28:21,305 --> 00:28:23,170
Each steps can set ic.

419
00:28:23,435 --> 00:28:29,555
Traffic wait pause for a duration to
collect the metrics and run analysis

420
00:28:29,555 --> 00:28:31,805
templates that define the health checks.

421
00:28:32,305 --> 00:28:36,235
The analysis template references
Prometheus queries that check your

422
00:28:36,235 --> 00:28:38,965
success rate against defined thresholds.

423
00:28:39,775 --> 00:28:43,675
So the crucial configuration
here is the auto promotion

424
00:28:43,675 --> 00:28:44,905
enabled, which is set to false.

425
00:28:45,405 --> 00:28:49,005
Which means that the system won't
automatically proceed to the next step

426
00:28:49,065 --> 00:28:56,055
unless the analysis passes and rollback
on health check failures when it is

427
00:28:56,055 --> 00:29:00,435
set to true, which enables automatic
rollback if any health check fails.

428
00:29:01,125 --> 00:29:04,150
So this mechanism also extends
to resource auto scaling.

429
00:29:04,850 --> 00:29:10,340
So where the system can detect memory
or CPO pressure patterns and scale

430
00:29:10,340 --> 00:29:15,580
horizontally by adding more pods or
by vertically by increasing resource

431
00:29:15,580 --> 00:29:20,650
limits, and then return to the
baseline when the load normalizes.

432
00:29:21,150 --> 00:29:24,930
So this prevents resource
exhaustion, failures, and ensures

433
00:29:24,930 --> 00:29:29,010
your applications to handle traffic
spikes without any degradation.

434
00:29:29,510 --> 00:29:33,380
Now I want to share a real world
case study from the airline industry,

435
00:29:33,560 --> 00:29:38,000
specifically focusing on a flight booking
system because this represents one of

436
00:29:38,000 --> 00:29:42,020
the most demanding environments where
self-healing systems prove their value.

437
00:29:42,520 --> 00:29:45,490
So this case study illustrates both
the extreme requirements and the

438
00:29:45,490 --> 00:29:48,150
dramatic benefits that can be achieved.

439
00:29:48,850 --> 00:29:54,140
The challenge facing this airline
was multifaceted and unforgiving.

440
00:29:54,920 --> 00:30:01,460
First, they had a requirement for a 99.9%
uptime, which translates to a maximum

441
00:30:01,460 --> 00:30:05,990
of 4.32 minutes of downtime per month.

442
00:30:06,980 --> 00:30:11,120
This isn't a soft goal, so this
is a contractual obligation tied

443
00:30:11,120 --> 00:30:13,790
to the service level agreements
with partners and customers.

444
00:30:14,750 --> 00:30:19,190
Second, they need, they needed to
handle the peak traffic that could

445
00:30:19,190 --> 00:30:23,660
spike to 10 times normal load during
holidays and special promotions.

446
00:30:24,440 --> 00:30:28,160
So when customers are mostly likely
to be booking travel and when the

447
00:30:28,160 --> 00:30:31,490
business generates a significant
portion of its annual re revenue.

448
00:30:31,990 --> 00:30:36,130
Third, the system operates across
multiple regions and must handle

449
00:30:36,130 --> 00:30:40,930
regional failures gracefully continuing
to serve customers even when the

450
00:30:40,930 --> 00:30:42,910
entire data centers become unavailable.

451
00:30:43,410 --> 00:30:47,340
And they need to operate in a highly
regulated industry where every change

452
00:30:47,340 --> 00:30:51,780
must be auditable for compliance
purposes, requiring detailed tracking

453
00:30:51,780 --> 00:30:54,360
of who made what, change, when, and why.

454
00:30:54,860 --> 00:30:59,110
So the traditional approach where
they were using before implementation

455
00:30:59,170 --> 00:31:01,475
GitHubs was plagued with problems.

456
00:31:01,975 --> 00:31:05,425
Manual deployments had to be
scheduled during carefully

457
00:31:05,425 --> 00:31:06,625
planned maintenance windows.

458
00:31:07,105 --> 00:31:12,265
Which meant they could only deploy
new features or fixes at only specific

459
00:31:12,415 --> 00:31:18,395
times, limiting their agility When
problems did occur rollback procedures

460
00:31:18,395 --> 00:31:23,735
took 15 to 20 minutes, which
sounds fast, but was far too slow.

461
00:31:23,885 --> 00:31:28,295
Given their uptime requirements,
those 15 minutes could represent

462
00:31:28,295 --> 00:31:31,265
thousands of failed bookings
and frustrated cu customers.

463
00:31:31,765 --> 00:31:35,005
So they also struggle with the
configuration drift between regions

464
00:31:35,035 --> 00:31:38,725
where manual changes in one region
wherein properly replicated to the

465
00:31:38,725 --> 00:31:44,205
others, leading to inconsistent behavior
and difficult to diagnose issues which

466
00:31:44,205 --> 00:31:49,695
is perha perhaps like more concerning
and human error during emergency

467
00:31:49,695 --> 00:31:51,520
situations was at a constant risk.

468
00:31:52,020 --> 00:31:55,080
So when engineers are under
pressure to fix a production outage.

469
00:31:55,580 --> 00:32:01,010
Mistakes happen and those mistakes
can be expensive and can compound

470
00:32:01,010 --> 00:32:03,680
more to the actual original problem.

471
00:32:04,180 --> 00:32:08,230
So the solution this airline implemented
during GitHub's principles transformed

472
00:32:08,230 --> 00:32:12,550
their operations in fundamental ways,
and the results speak for themself.

473
00:32:13,450 --> 00:32:17,230
They implemented multi-region GitHubs
with a single GIT repository serving as

474
00:32:17,230 --> 00:32:21,820
the source of truth and multiple Argo
CD instances deployed in each region.

475
00:32:22,660 --> 00:32:26,470
So this architecture provided automated
regional fail load capabilities

476
00:32:26,470 --> 00:32:30,430
and eliminated configuration
drifts between regions entirely.

477
00:32:30,930 --> 00:32:33,750
Because every region was
continuously reconciling

478
00:32:34,140 --> 00:32:35,710
against the same GI repository.

479
00:32:36,210 --> 00:32:41,930
So they adopted a blue, green deployment
strategy that enabled true zero downtime

480
00:32:42,260 --> 00:32:46,040
deployments, where the new version
is fully deployed alongside the whole

481
00:32:46,040 --> 00:32:51,365
old baseline version, and it gets
validated thoroughly, and then the actual

482
00:32:51,630 --> 00:32:53,490
traffic is switched over instantly.

483
00:32:54,450 --> 00:32:58,140
So this approach provided instant
rollback capability because if

484
00:32:58,140 --> 00:33:02,160
anything goes wrong, you simply switch
traffic to the blue environment,

485
00:33:02,190 --> 00:33:03,630
which is still running unhealthy.

486
00:33:04,130 --> 00:33:08,720
So the traffic shifting decisions
were based on real-time metrics, not

487
00:33:08,720 --> 00:33:13,190
just successful deployment of the
pods, but also the actual evidence

488
00:33:13,190 --> 00:33:16,310
that the application was performing
correctly under production load.

489
00:33:16,810 --> 00:33:19,480
The cell feeling scenarios
they implemented cover a

490
00:33:19,480 --> 00:33:20,920
range of common failure nos.

491
00:33:21,420 --> 00:33:22,200
Sorry, modes.

492
00:33:22,300 --> 00:33:27,240
They can automatically scale databases
connection pools when pressure is

493
00:33:27,240 --> 00:33:32,390
detected, preventing the connection,
exhaustion issues as we discussed earlier.

494
00:33:32,890 --> 00:33:37,120
And they also have auto rollback
configured for latency spikes where

495
00:33:37,120 --> 00:33:42,520
if the new version shows increased a
response time, it automatically rolled

496
00:33:42,520 --> 00:33:44,830
back before the customers before.

497
00:33:44,880 --> 00:33:46,490
The customers notice the issue.

498
00:33:46,990 --> 00:33:49,960
So they implemented an automatic
regional traffic routing.

499
00:33:50,470 --> 00:33:54,100
So if one region becomes degraded,
the traffic is shifted to healthy

500
00:33:54,100 --> 00:33:55,840
regions without manual intervention.

501
00:33:56,340 --> 00:34:00,900
They even have a memory leak deduction
that can automatically restart pods

502
00:34:00,960 --> 00:34:05,940
when memory usage patterns indicate a
leak and buying time for the engineering

503
00:34:05,940 --> 00:34:08,040
team to implement a proper fix.

504
00:34:08,540 --> 00:34:13,820
The results achieved were remarkable and
exceeded even their ambitious targets.

505
00:34:14,660 --> 00:34:18,800
Meantime to recovery was reduced
from 20 minutes to 90 seconds.

506
00:34:18,970 --> 00:34:22,960
More than a tenfold improvement
over six months of operation.

507
00:34:22,960 --> 00:34:26,460
They had zero manual rollbacks
meaning every rollback that was

508
00:34:26,460 --> 00:34:29,695
needed was performed automatically
by the system faster and more

509
00:34:29,695 --> 00:34:31,830
reliably than a human could.

510
00:34:32,330 --> 00:34:39,609
So most impressively they
achieved 99.995% uptime.

511
00:34:40,570 --> 00:34:47,610
Actually exceeding their already
stringent target of 99.99%, which this

512
00:34:47,610 --> 00:34:51,210
improvement translated directly to the
business value in terms of revenue,

513
00:34:51,470 --> 00:34:57,120
and customer satisfaction maintained
at the highest level, and also focusing

514
00:34:57,120 --> 00:35:01,770
on the operational cost reduced by
eliminating after our emergency responses.

515
00:35:02,270 --> 00:35:05,090
So now let's explore the
artificial intelligence.

516
00:35:05,090 --> 00:35:08,030
How artificial intelligence and
machine learning can enhance

517
00:35:08,060 --> 00:35:12,800
self-healing pipelines, taking them
from reactive automation to truly

518
00:35:12,800 --> 00:35:14,780
predictive and intelligence systems.

519
00:35:15,680 --> 00:35:18,920
This represents the cutting edge
of where the industry is heading.

520
00:35:19,865 --> 00:35:23,195
And while not every organization
will implement all of these

521
00:35:23,195 --> 00:35:26,675
capabilities immediately,
understanding the possibilities

522
00:35:26,705 --> 00:35:28,835
helps you to plan your roadmap.

523
00:35:29,335 --> 00:35:33,675
Intelligent anomaly deduction uses
machine learning models that learn the

524
00:35:33,675 --> 00:35:38,265
normal patterns of your applications
and can also detect anomalies

525
00:35:38,265 --> 00:35:40,245
before they turn into incidents.

526
00:35:41,145 --> 00:35:45,645
So this goes far beyond simple,
threshold based alerts, right?

527
00:35:45,895 --> 00:35:50,035
For example, the system might learn
that your application typically has a

528
00:35:50,035 --> 00:35:56,475
subtile increase in your memory usage
or time that resets each day, and it

529
00:35:56,475 --> 00:36:00,685
can distinguish between its normal
pattern from a true memory leak.

530
00:36:01,630 --> 00:36:06,250
So if it can identify a subtile
memory leak 30 minutes before they

531
00:36:06,250 --> 00:36:08,260
would cause an out of memory error.

532
00:36:08,740 --> 00:36:12,940
Giving the system the time to take
preventive action like restarting the pods

533
00:36:12,970 --> 00:36:15,160
with the new version ready to take over.

534
00:36:15,660 --> 00:36:21,240
So the predictive rollback takes this
step further by predicting the probability

535
00:36:21,240 --> 00:36:23,550
of failure based on historical patterns.

536
00:36:24,525 --> 00:36:28,455
So the system might analyze a new
deployment and determine that it has

537
00:36:28,455 --> 00:36:34,145
85% similarity to a previous deployment
that failed, perhaps because it, it

538
00:36:34,145 --> 00:36:38,315
changes the same components or had
similar performance characteristics

539
00:36:38,815 --> 00:36:41,035
based on this pattern recognition.

540
00:36:41,065 --> 00:36:41,425
Recognition.

541
00:36:41,685 --> 00:36:45,675
It can recommend a rollback
before any actual failure occurs.

542
00:36:46,175 --> 00:36:49,925
Essentially learning from the past
mistakes to avoid repeating them.

543
00:36:50,425 --> 00:36:54,115
The intelligent cannery analysis
are just the analysis duration

544
00:36:54,115 --> 00:36:59,005
dynamically based on traffic patterns
rather than using fixed time windows.

545
00:36:59,505 --> 00:37:03,475
So if you're running a cannery during
unusual traffic conditions, perhaps

546
00:37:03,475 --> 00:37:08,635
during a major sale or a traffic spike
from a social media, the system can

547
00:37:08,635 --> 00:37:13,555
extend the cannery phase to collect more
data before making a promotion decision.

548
00:37:14,055 --> 00:37:18,465
So this ensures that you have
statistically significant data to

549
00:37:18,465 --> 00:37:23,625
make informed decisions regardless
of when you happen to deploy.

550
00:37:24,125 --> 00:37:30,065
So the root cause analysis capabilities
can correlate metrics, logs, and traces

551
00:37:30,605 --> 00:37:32,375
automatically when an issue occurs.

552
00:37:33,035 --> 00:37:38,845
So it can generate hypothesis about what
went wrong with your confidence scores

553
00:37:39,295 --> 00:37:41,695
and suggest specific remediation actions.

554
00:37:42,550 --> 00:37:47,160
This dramatically reduces the time
engineers spend diagnosing the issues,

555
00:37:47,310 --> 00:37:51,570
allowing them to focus on implementing
fixes rather than directive work.

556
00:37:52,070 --> 00:37:56,030
So the resource optimization
uses historical data to predict

557
00:37:56,090 --> 00:37:59,890
resource needs based on deployment
characteristics the time of the day

558
00:37:59,890 --> 00:38:01,570
of the week, and other patterns.

559
00:38:02,470 --> 00:38:07,850
The system can automatically adjust
HPA and VPA parameters to optimize both

560
00:38:07,850 --> 00:38:12,980
performance and cost, ensuring you are
not over provision, over provisioning the

561
00:38:12,980 --> 00:38:18,520
resources during low traffic periods or
under provisioning during the peak times.

562
00:38:19,020 --> 00:38:24,290
So finally security anomaly detection
can identify unusual deployment patterns

563
00:38:24,320 --> 00:38:26,660
that might indicate a compromise.

564
00:38:27,160 --> 00:38:32,820
Or it can flag potential security issues
before they are being exploited and

565
00:38:32,820 --> 00:38:36,720
automatically block suspicious deployments
before they can impact your environment.

566
00:38:37,590 --> 00:38:41,850
So this adds an important security layer
to your self-healing capabilities as well.

567
00:38:42,350 --> 00:38:46,130
So let me walk you through the
comprehensive framework for

568
00:38:46,130 --> 00:38:49,400
integrating AI driven intelligence
into your deployment pipeline.

569
00:38:50,195 --> 00:38:54,455
So this architecture creates a
continuous feedback loop that observes,

570
00:38:54,485 --> 00:38:56,885
learns and heals automatically.

571
00:38:57,575 --> 00:39:01,465
And understanding how these
pieces connect together is

572
00:39:01,525 --> 00:39:03,555
essential for the implementation.

573
00:39:04,055 --> 00:39:07,475
The first phase is a continuous
monitoring phase where we collect

574
00:39:07,475 --> 00:39:11,960
metrics logs and traces in real time
from all the components of the system.

575
00:39:12,460 --> 00:39:17,500
So this includes tracking health checks
to ensure pods are responding correctly

576
00:39:17,810 --> 00:39:22,240
monitoring the service level objectives to
measure whether they, we are meeting our

577
00:39:22,240 --> 00:39:28,810
reliability objectives and also observing
resource usage patterns and establishing

578
00:39:28,810 --> 00:39:33,600
baseline behavior for what normally it'll
look like in a specific environment.

579
00:39:34,100 --> 00:39:38,299
So this baseline is crucial because
normal looks different for every

580
00:39:38,299 --> 00:39:39,680
application and organization.

581
00:39:40,180 --> 00:39:43,959
So the second phase is anomaly
deduction, where AI and mission

582
00:39:43,959 --> 00:39:47,619
learning models compare the current
state against the established baseline.

583
00:39:48,119 --> 00:39:53,129
This uses both static thresholds for known
failure conditions and dynamic pattern

584
00:39:53,129 --> 00:39:55,499
reorganization for more subtle issues.

585
00:39:55,999 --> 00:40:01,099
So the models are continuously learning
of what normal looks like and can

586
00:40:01,099 --> 00:40:06,109
identify deviations even when they
don't cross predefined thresholds.

587
00:40:07,009 --> 00:40:11,810
So this catches issues at the
traditional alerting might miss.

588
00:40:12,650 --> 00:40:16,760
So the third phase is the risk
assessment, where the system

589
00:40:17,000 --> 00:40:20,930
can calculate the probability of
failure based on multiple signals.

590
00:40:21,455 --> 00:40:25,415
It determines the severity and the
potential impact of the directed

591
00:40:25,415 --> 00:40:31,085
anomaly and decides whether to simply
monitor the situation, alert the team,

592
00:40:31,115 --> 00:40:32,975
or take immediate automated action.

593
00:40:33,935 --> 00:40:41,184
So this decision tree prevents alert fat
from false positives while ensuring real

594
00:40:41,184 --> 00:40:43,429
issues that gets immediate attention.

595
00:40:43,929 --> 00:40:46,809
So the fourth phase is the
automated remediation when

596
00:40:46,899 --> 00:40:49,119
recovery actions are executed.

597
00:40:49,619 --> 00:40:54,269
This might include triggering auto
rollback through Argo CD to return

598
00:40:54,269 --> 00:41:00,089
to a known good version, dynamically
scaling resources up and down or down

599
00:41:00,089 --> 00:41:04,919
to address the capacity issues or
shifting traffic away from problematic

600
00:41:04,919 --> 00:41:10,704
instances or restarting the pods while
ensuring that no good images are used.

601
00:41:11,204 --> 00:41:15,464
So these actions happen in seconds, far
faster than any human could respond.

602
00:41:16,214 --> 00:41:19,484
So the fifth phase is the
GitHub's reconciliation, where the

603
00:41:19,484 --> 00:41:23,269
system updates the Git with the
remediation actions that were taken.

604
00:41:24,029 --> 00:41:27,959
Creates a comprehensive audit trail
showing exactly what happened and

605
00:41:27,959 --> 00:41:32,069
why it happened, and also notifies
the team with detailed analysis

606
00:41:32,069 --> 00:41:37,529
including metrics, logs, traces, and
recommendations for long-term fixes.

607
00:41:38,219 --> 00:41:43,649
So this model ensures that automated
actions don't happen in a black box, but

608
00:41:43,709 --> 00:41:46,134
they are fully transparent and auditable.

609
00:41:46,634 --> 00:41:49,634
So the sixth and the final phase
is continuous learning, where the

610
00:41:49,634 --> 00:41:54,075
outcomes of all the actions are
fed back into the AI models to

611
00:41:54,075 --> 00:41:56,144
improve their accuracy over time.

612
00:41:56,644 --> 00:42:02,084
The system redefines thresholds and
policies based on what works and

613
00:42:02,084 --> 00:42:06,524
what doesn't continuously improvising
deduction accuracy with each instant.

614
00:42:07,394 --> 00:42:10,934
So this creates a virtual cycle
where your self feeling capabilities

615
00:42:10,934 --> 00:42:14,884
become more effective the longer
they, effective the longer they run.

616
00:42:15,384 --> 00:42:19,505
So as we think about implementing
these self-healing systems there are

617
00:42:19,505 --> 00:42:24,785
four core architectural principles
that must guide your design decisions.

618
00:42:25,714 --> 00:42:29,495
So these principles ensure that the
self failing capabilities are reliable,

619
00:42:29,884 --> 00:42:32,495
trustworthy, and continuously improving.

620
00:42:33,470 --> 00:42:37,470
The first principle is the continuous
loop creating an unbroken cycle

621
00:42:37,470 --> 00:42:43,319
of observe, detect, and then
decide, act, and then learn.

622
00:42:44,249 --> 00:42:48,120
So this isn't a linear process
that ends after remediation.

623
00:42:48,689 --> 00:42:51,450
It is a continuous cycle that
enters constant vigilance

624
00:42:51,450 --> 00:42:52,979
and ongoing improvement.

625
00:42:53,479 --> 00:42:55,099
So the system is always observing.

626
00:42:55,444 --> 00:42:59,164
Always ready to deduct and
respond to issues and always

627
00:42:59,164 --> 00:43:00,634
learning from what happens.

628
00:43:01,384 --> 00:43:05,314
The second principle is full
automation for common failures,

629
00:43:05,854 --> 00:43:07,174
so no human intervention.

630
00:43:07,354 --> 00:43:11,044
No human intervention should
be required for routine issues

631
00:43:11,104 --> 00:43:12,664
that the system has seen before.

632
00:43:13,414 --> 00:43:17,495
The system responds instantly to
known patterns, preventing incidents

633
00:43:17,524 --> 00:43:19,205
before they impact the users.

634
00:43:20,119 --> 00:43:23,899
So this doesn't mean humans
are remote from the loop.

635
00:43:23,959 --> 00:43:28,669
It means they are freed from the toil
and can focus on more novel problems

636
00:43:28,759 --> 00:43:32,499
and strategic improvements rather than
fighting the same fires repeatedly.

637
00:43:32,999 --> 00:43:34,829
The third principle is
complete auditability.

638
00:43:35,369 --> 00:43:40,519
So every action must be recorded and
git and your observability tools create

639
00:43:40,999 --> 00:43:45,109
by creating an immutable history for
compliance and debugging purposes.

640
00:43:45,609 --> 00:43:49,989
So when, sometime when something happens
you can trace back through the entire

641
00:43:49,989 --> 00:43:53,799
chain of events to understand exactly
what occurred, what decisions were

642
00:43:53,799 --> 00:43:55,719
made, and what actions were taken.

643
00:43:56,589 --> 00:44:01,089
So this is crucial, not just for
compliance, but for building the

644
00:44:01,089 --> 00:44:02,769
trust in your self healing systems.

645
00:44:03,269 --> 00:44:09,339
The fourth principle is self-improvement
where AI models learn from each

646
00:44:09,369 --> 00:44:14,139
incident and continuously refine their
understanding and response capabilities.

647
00:44:14,189 --> 00:44:18,339
Over a period of time, the system
doesn't maintain a static set of rules.

648
00:44:18,399 --> 00:44:23,229
It evolves based on experience becoming
more accurate and detecting issues

649
00:44:23,289 --> 00:44:27,184
more effective at as they resolve
them, as it encounters more scenarios.

650
00:44:27,684 --> 00:44:31,224
So understanding the high level
journey from deduction to resolution

651
00:44:31,414 --> 00:44:34,474
is critical for implementing
effective self hailing systems.

652
00:44:34,974 --> 00:44:38,184
So let me walk you through the
practical steps for building these

653
00:44:38,184 --> 00:44:39,774
capabilities in your organization.

654
00:44:40,274 --> 00:44:45,494
So start with observability because you
simply cannot heal what you cannot see.

655
00:44:46,124 --> 00:44:50,074
As we discussed earlier, before
automating anything implement

656
00:44:50,224 --> 00:44:51,484
comprehensive metrics collection.

657
00:44:52,174 --> 00:44:56,364
Log aggregation and focus
more on distributed tracing.

658
00:44:56,864 --> 00:45:01,074
Define clear service level objectives
and service level indicators

659
00:45:01,104 --> 00:45:04,764
that represent what healthy
looks like for your applications.

660
00:45:05,264 --> 00:45:09,354
Ensure that you have the visibility
needed to detect the problems

661
00:45:09,354 --> 00:45:11,774
quickly and understand their impact.

662
00:45:12,274 --> 00:45:15,724
Many organizations make the mistake
of jumping straight into automation

663
00:45:15,724 --> 00:45:20,014
without establishing proper observability
first, and they end up with the

664
00:45:20,014 --> 00:45:23,914
systems that take automated actions
based on insufficient information.

665
00:45:24,414 --> 00:45:28,404
Move to gradual automation rather than
trying to automate everything at once.

666
00:45:28,884 --> 00:45:34,104
Begin with a manual roll gate in your
pipeline so humans can review each

667
00:45:34,104 --> 00:45:39,074
deployment, add auto rollback, or for
your most critical services first.

668
00:45:39,569 --> 00:45:42,709
Where the value is highest and
then the stakes are greatest.

669
00:45:43,209 --> 00:45:48,999
As your team builds confidence in the
automated behaviors and refines the

670
00:45:48,999 --> 00:45:53,019
health checks and thresholds, expand
the automation to more services.

671
00:45:53,889 --> 00:45:58,149
So this gradual approach prevents
automation from making things

672
00:45:58,149 --> 00:46:02,459
worse and allows your team to learn
and adjust as you go test your

673
00:46:02,459 --> 00:46:06,029
healing capabilities explicitly
using chaos engineering practices.

674
00:46:06,529 --> 00:46:11,449
And also intentionally inject failures
into your system to verify that

675
00:46:11,449 --> 00:46:13,519
auto recovery works as expected.

676
00:46:14,019 --> 00:46:19,619
So this might feel uncomfortable at first
breaking things on purpose, but it is far

677
00:46:19,619 --> 00:46:23,799
better to discover problems with yourself
feeling pipelines during a controlled

678
00:46:23,799 --> 00:46:26,004
test than during a real instant.

679
00:46:26,504 --> 00:46:29,324
Practice your instant response
procedures during these chaos

680
00:46:29,324 --> 00:46:34,874
experiments so the team knows what to
expect and how to intervene if needed.

681
00:46:35,374 --> 00:46:39,734
Also maintain human oversight toward,
throughout the process thoroughly.

682
00:46:40,234 --> 00:46:45,634
And initially, AI should suggest
actions and human should decide whether

683
00:46:45,634 --> 00:46:51,394
to take them, audit all the auto
automated actions to ensure that they

684
00:46:51,424 --> 00:46:53,794
are appropriate and they are effective.

685
00:46:54,294 --> 00:46:59,544
Also maintain manual override capability
for unexpected scenarios where automated

686
00:46:59,544 --> 00:47:01,404
responses might not be appropriate.

687
00:47:01,904 --> 00:47:05,564
So the goal isn't to remove
humans from operations.

688
00:47:05,594 --> 00:47:10,184
It is to argument human capabilities
and allow people to focus on

689
00:47:10,184 --> 00:47:13,484
the problems that truly require
human judgment and creativity.

690
00:47:13,984 --> 00:47:18,034
So to conclude, let me share some
best practices that have proven

691
00:47:18,034 --> 00:47:22,414
essential for operational excellence
when running self-healing pipelines.

692
00:47:23,284 --> 00:47:27,514
So these guidelines come from a real
world experience and represent patterns

693
00:47:27,544 --> 00:47:29,764
that successfully organizations follow.

694
00:47:30,264 --> 00:47:34,464
So GitHub's hygiene is
foundational to everything else.

695
00:47:35,004 --> 00:47:39,234
Make small, frequent commits rather
than large and risky changes.

696
00:47:40,159 --> 00:47:46,129
Always have better practice to write
clear commit messages that explain not

697
00:47:46,129 --> 00:47:48,619
just what changed, but why it changed.

698
00:47:49,129 --> 00:47:53,619
Providing context for the future
ERs implement proper branch

699
00:47:53,619 --> 00:47:56,859
protection rules so no one can push
directly to production branches.

700
00:47:57,359 --> 00:48:02,849
And also adjust the rules in your repo
to have code review for all the changes.

701
00:48:02,899 --> 00:48:06,469
Treating infrastructure and
configuration with the same rigor where

702
00:48:06,469 --> 00:48:08,449
you apply to the application code.

703
00:48:08,949 --> 00:48:13,479
And these practices might seem
like a overhead initially, but

704
00:48:13,559 --> 00:48:17,764
they do prevent far more problems
than they cause in the longer run.

705
00:48:18,264 --> 00:48:22,404
So the progressive delivery practices
should be universal rather than

706
00:48:22,434 --> 00:48:28,014
optional, and always use cannery or
blue-green deployment strategies.

707
00:48:28,014 --> 00:48:33,524
Even for services you consider low risk
because you never know when a change

708
00:48:33,554 --> 00:48:35,594
will have unexpected consequences.

709
00:48:36,094 --> 00:48:39,994
So start with your low risk services
to build confidence and refine

710
00:48:39,994 --> 00:48:45,054
your practices, and also monitor
closely during early deployments

711
00:48:45,084 --> 00:48:49,284
and be prepared to adjust the
thresholds based on what you learn.

712
00:48:49,784 --> 00:48:52,904
Gradually increase the scope of
automation as your team becomes

713
00:48:52,904 --> 00:48:57,554
comfortable with the patterns and con
and confident in the health checks.

714
00:48:58,054 --> 00:49:02,464
Documentation and runbooks
might remain essential even in

715
00:49:02,464 --> 00:49:04,084
highly automated environments.

716
00:49:04,744 --> 00:49:08,824
So document all self-healing behavior
so everyone on the team understand

717
00:49:08,824 --> 00:49:12,904
what the system will do automatically
and and under what conditions.

718
00:49:13,894 --> 00:49:17,524
Create detailed runbooks for h cases
that require human intervention.

719
00:49:18,184 --> 00:49:23,204
Also share those learnings and
insights across teams to so that

720
00:49:23,204 --> 00:49:25,024
the knowledge doesn't stay siloed.

721
00:49:25,524 --> 00:49:29,424
Maintain UpToDate troubleshooting
guides that explain both how to work

722
00:49:29,454 --> 00:49:34,304
with the automated systems and how to
make a manual control when necessary

723
00:49:34,804 --> 00:49:39,724
and continuous improvement should be
baked into your culture and processes.

724
00:49:40,384 --> 00:49:46,264
And also review incidents and near misses
regularly so that even the ones that

725
00:49:46,264 --> 00:49:50,799
were handled automatically to understand
if there are deeper issues to address.

726
00:49:51,299 --> 00:49:55,109
Tune the AI models based on actual
outcomes rather than assumptions.

727
00:49:55,609 --> 00:49:57,829
Iterate on thresholds and policies.

728
00:49:57,889 --> 00:50:02,749
As you gather more data about your
specific applications and usage patterns.

729
00:50:03,249 --> 00:50:07,119
Conduct what is regular like regular
meetups with the team or like

730
00:50:07,119 --> 00:50:11,529
regular retrospectives where the
team can discuss what's working, what

731
00:50:11,529 --> 00:50:13,629
isn't, and what could be improved.

732
00:50:14,129 --> 00:50:17,489
This commitment to ongoing improvement
ensures that your self-healing

733
00:50:17,489 --> 00:50:22,069
capabilities become more effective
over time rather than stagnating

734
00:50:22,099 --> 00:50:23,629
and getting stuck at a point.

735
00:50:24,129 --> 00:50:28,239
I hope this session has given you both,
information, detailed information and

736
00:50:28,239 --> 00:50:33,129
practical guidance for building the self
hailing CICD pipelines in your own stacks.

737
00:50:33,999 --> 00:50:38,439
And I would love to connect with you on
LinkedIn to continue this conversation,

738
00:50:38,499 --> 00:50:42,849
share ideas and experiences and support
each other as we all work toward

739
00:50:42,879 --> 00:50:45,099
more resilient, reliable systems.

740
00:50:45,939 --> 00:50:49,319
Let's stay connected and stay in touch
and learn from each other's journey.

741
00:50:50,219 --> 00:50:50,969
That's my time.

742
00:50:50,999 --> 00:50:54,879
Thank you for your time
today and con 42 for this.

743
00:50:54,889 --> 00:50:58,279
Thank you Con 42 for this excellent
opportunity to share my experiences

744
00:50:58,279 --> 00:50:59,389
with the DevOps community.

745
00:50:59,929 --> 00:51:00,259
Cheers.

