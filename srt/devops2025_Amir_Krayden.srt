1
00:00:00,789 --> 00:00:01,859
Well, hi, everyone.

2
00:00:01,910 --> 00:00:03,249
Amir here from Sensor.

3
00:00:03,250 --> 00:00:05,750
I'm the co founder and chief product.

4
00:00:06,269 --> 00:00:09,890
I'm very happy to, be part
of this session in COM 42.

5
00:00:10,220 --> 00:00:11,705
DevOps 2025.

6
00:00:12,175 --> 00:00:15,584
the name of this session is
Kubernetes minus the blind spots.

7
00:00:16,134 --> 00:00:20,474
We're going to focus around how to
solve our Kubernetes observability

8
00:00:20,475 --> 00:00:25,834
challenge challenges and how that
becomes a little bit easier when you

9
00:00:25,835 --> 00:00:28,095
can incorporate real time topology.

10
00:00:28,595 --> 00:00:30,335
so what are we going to cover today?

11
00:00:31,135 --> 00:00:34,125
First of all, all of you,
most of your Kubernetes users.

12
00:00:34,285 --> 00:00:35,655
So, you know, it's great.

13
00:00:36,175 --> 00:00:38,435
But it's also very hard to troubleshoot.

14
00:00:38,735 --> 00:00:40,295
we will delve into why it's hard.

15
00:00:40,715 --> 00:00:45,105
To troubleshoot when we go a little
further, we were going to connect it

16
00:00:45,145 --> 00:00:48,995
with the challenges of traditional
observability toolkit, meaning if you have

17
00:00:48,995 --> 00:00:56,374
the classical logs, matrix, whatever, why
that coming alone is not enough in modern

18
00:00:56,375 --> 00:01:00,735
dynamic environments, we're going to
cover the concept of real time topology.

19
00:01:01,735 --> 00:01:08,035
How it different, how it different from
dependency mapping, what to do, why

20
00:01:08,045 --> 00:01:10,544
and how to implement it in real life.

21
00:01:10,895 --> 00:01:17,185
We'll give a few real world use cases
and try to tie in the concept of topology

22
00:01:17,485 --> 00:01:22,374
into, into one of your tools, into
one of the tools in your toolkit with

23
00:01:22,374 --> 00:01:24,144
regard to the troubleshooting process.

24
00:01:24,794 --> 00:01:28,254
We will see why eBPF is
a very important enabler.

25
00:01:28,404 --> 00:01:33,764
with regards to that, why the use of
EBPF, EBPF tools, EBPF open sources,

26
00:01:34,074 --> 00:01:38,484
can yield the very, can yield very high
benefits, to, enable the implementation

27
00:01:38,494 --> 00:01:43,044
of real time topologies, and eventually
a few tips of how to get started,

28
00:01:43,274 --> 00:01:46,804
whether it will be with a commercial
tool or, with an open source tool.

29
00:01:47,304 --> 00:01:49,974
So as we know, Kubernetes is great.

30
00:01:50,874 --> 00:01:51,584
We use it.

31
00:01:51,944 --> 00:01:53,224
We understand what we use it.

32
00:01:53,234 --> 00:01:55,064
Sometimes we don't
understand what we use it.

33
00:01:55,634 --> 00:01:58,564
But it brings a lot of
interesting challenges with

34
00:01:58,564 --> 00:01:59,684
regards to troubleshooting.

35
00:01:59,754 --> 00:02:02,734
There's a few, a few reasons to that.

36
00:02:03,144 --> 00:02:05,664
First and foremost,
the distributed nature.

37
00:02:06,404 --> 00:02:11,154
Like, there's no need to explain, but
when we used an, when we used to an

38
00:02:11,154 --> 00:02:15,464
implemented monolith, they had a lot of
problems from operational perspective.

39
00:02:15,464 --> 00:02:17,834
They had a lot of problems from managing.

40
00:02:18,089 --> 00:02:22,919
these, these big piece of code, but
from other aspect, they were closed,

41
00:02:22,989 --> 00:02:27,299
they were easier to debug with the
distributed nature of Kubernetes.

42
00:02:27,399 --> 00:02:28,559
A lot of things happen.

43
00:02:28,559 --> 00:02:31,979
But first of all, we have a lot
of more moving parts and do.

44
00:02:32,019 --> 00:02:37,799
Kubernetes is very uniquely situated
because it's a software layer, but it's a

45
00:02:37,799 --> 00:02:40,369
kind of a software infrastructure layer.

46
00:02:40,869 --> 00:02:45,839
In reality, when there's an issue and
the kubernetes based deployment, it takes

47
00:02:45,839 --> 00:02:51,129
time and it's not that easy to understand
whether it is an application issue.

48
00:02:51,549 --> 00:02:55,489
Or a Kubernetes issue, meaning
how do you differentiate, this

49
00:02:55,489 --> 00:02:59,549
blur, the blurred line, between
application and infrastructure issue.

50
00:03:00,049 --> 00:03:02,179
The second one would be the dynamics.

51
00:03:02,399 --> 00:03:06,549
the ephemeral environments or, the
ephemeral, nature of Kubernetes

52
00:03:06,549 --> 00:03:08,709
environment, which pods are coming up.

53
00:03:09,369 --> 00:03:12,069
C put are coming up, port
ports are going down.

54
00:03:12,379 --> 00:03:17,539
CICD pipelines are causing configuration
changes are coding causing code changes.

55
00:03:17,869 --> 00:03:19,339
It's, of course not that easy.

56
00:03:19,899 --> 00:03:24,559
to track that, which also it's also
adding more complexity, more complexity

57
00:03:24,559 --> 00:03:28,319
when we're trying to cope with
travel shooting, complex networking.

58
00:03:29,059 --> 00:03:32,159
A lot of the things that Kubernetes
does is to create obstruction upon

59
00:03:32,159 --> 00:03:33,749
obstructions upon obstructions.

60
00:03:33,849 --> 00:03:36,519
You can deploy two pods
on the same server.

61
00:03:36,529 --> 00:03:40,869
You can deploy two pods in adjacent
servers, but Kubernetes will try its

62
00:03:40,999 --> 00:03:45,449
best to make it seems like they're
all one part of the same application.

63
00:03:45,859 --> 00:03:49,179
And that, in real life, can
create a lot of troubleshooting,

64
00:03:49,469 --> 00:03:51,399
troubleshooting cumbersomes.

65
00:03:51,399 --> 00:03:56,009
So elements like for DNS or proxies
or the CNI plug in, which eventually

66
00:03:56,009 --> 00:04:00,504
creates A flat networking model
between them are also adding to

67
00:04:00,504 --> 00:04:03,374
the, adding to the troubleshooting,
troubleshooting complexity.

68
00:04:03,794 --> 00:04:06,109
Eventually, it's not that easy to do.

69
00:04:06,479 --> 00:04:10,269
Sniff the network or look at each and
every packet and understand what is

70
00:04:10,279 --> 00:04:15,139
going on just by, by means of the, the
sheer obstruction layers that kubernetes

71
00:04:15,159 --> 00:04:21,009
is imposing into the deployment shared
resources, and noisy, noisy neighbors.

72
00:04:21,329 --> 00:04:22,549
Of course, if we were.

73
00:04:23,319 --> 00:04:27,939
In a world in which I have complete
control over the infrastructure, meaning

74
00:04:27,949 --> 00:04:31,689
that when I'm looking at a pod, or
I'm looking at an application part,

75
00:04:32,129 --> 00:04:36,349
I don't need to account for what the
infrastructure is doing, whether the

76
00:04:36,349 --> 00:04:41,304
infrastructure directly or Because of
another deployment, which is situated

77
00:04:41,304 --> 00:04:46,444
on the same note and by itself can cause
metrics and cause other, other telemetry,

78
00:04:47,384 --> 00:04:49,634
other telemetry to be very noisy.

79
00:04:49,914 --> 00:04:53,564
So all of that is adding to the
to the aggregate complexity.

80
00:04:53,894 --> 00:04:57,394
if you will, black box or black boxes.

81
00:04:58,129 --> 00:05:00,149
Meaning things that I
don't really control.

82
00:05:00,149 --> 00:05:04,229
This could be infrastructure pieces,
pods, which are pods, which are

83
00:05:04,229 --> 00:05:07,309
third parties, which I'm using or
containers, which are third parties

84
00:05:08,169 --> 00:05:10,229
and automation complexity by itself.

85
00:05:10,229 --> 00:05:10,949
We touched it.

86
00:05:11,029 --> 00:05:15,749
We had a lot of other sessions in the
past about how CST pipeline by itself

87
00:05:16,249 --> 00:05:19,849
is essentially a noise generator
when coming to troubleshooting.

88
00:05:19,869 --> 00:05:24,679
The fact that I'm trying not only
to troubleshoot or trying to shoot

89
00:05:24,679 --> 00:05:28,899
at the target, but that target is,
All the while, also, also moving.

90
00:05:29,379 --> 00:05:33,459
so I think in, in very high terms,
that should give you a glance

91
00:05:33,459 --> 00:05:36,289
about, the different elements
which are the cause of complexity,

92
00:05:36,549 --> 00:05:37,809
with regard to troubleshooting.

93
00:05:38,779 --> 00:05:41,219
We gave an example of logs
on the right hand side.

94
00:05:42,099 --> 00:05:47,749
I think logs are a very good example when
speaking about distributed environments

95
00:05:47,749 --> 00:05:51,379
because you're looking at a single
point in time in a single element.

96
00:05:51,709 --> 00:05:55,149
And I always say it, the problem
with logs is that they require more

97
00:05:55,159 --> 00:05:58,299
logs and rarely on the same place.

98
00:05:58,799 --> 00:06:02,649
The challenges with the observability
toolkits as they exist today.

99
00:06:03,039 --> 00:06:05,379
so a lot of these,
observability toolkits were.

100
00:06:05,799 --> 00:06:11,979
Actually, born, raised, nurtured, for, for
the world of, for the world of monolith.

101
00:06:12,379 --> 00:06:16,549
So, a lot of things that, started
there, like matrix and log and traces

102
00:06:16,859 --> 00:06:21,659
are really Coming from a world in which
you should have a complete view over

103
00:06:21,669 --> 00:06:26,319
your application or your environment
and as we touched Touched on before

104
00:06:26,949 --> 00:06:31,819
that's how that is something which is
a massively changing Massively change

105
00:06:32,089 --> 00:06:36,719
when looking at distributed environment
metrics Everybody knows the concept,

106
00:06:36,719 --> 00:06:42,779
but we're looking at some sort of
some sort of a quantity Or a number.

107
00:06:43,029 --> 00:06:46,289
Usually it's there to track
some sort of a performance, a

108
00:06:46,289 --> 00:06:48,109
performance, telemetry aspect.

109
00:06:48,469 --> 00:06:53,509
So they allow you to track trends
or to see absolute values, but

110
00:06:54,329 --> 00:06:55,669
eventually they're just numbers.

111
00:06:56,284 --> 00:07:00,074
You have nothing to do with them if
you don't have some higher context

112
00:07:00,454 --> 00:07:04,454
to what they mean, to what they mean,
in, to what they mean in the current

113
00:07:04,464 --> 00:07:06,324
time or in the current situation.

114
00:07:06,754 --> 00:07:09,934
For example, what's causing
this spike in the CPU usage?

115
00:07:10,184 --> 00:07:13,704
There's a lot of other questions that
you need to answer that will help you

116
00:07:13,704 --> 00:07:18,534
understand whether the spike in the CPU
usage is something, worrisome or just

117
00:07:18,714 --> 00:07:20,304
normal behavior of the application.

118
00:07:20,944 --> 00:07:23,004
logs to the very course.

119
00:07:23,719 --> 00:07:28,589
They can provide you some granular detail
about an issue, but they're fragmented.

120
00:07:28,589 --> 00:07:33,289
You see bits and pieces of logs from
various elements in the system, and

121
00:07:33,449 --> 00:07:39,379
there's no one else other than an engineer
that can tie together whether log A on

122
00:07:39,429 --> 00:07:44,304
point, on point A and log B on point
B has anything to do with one another.

123
00:07:44,605 --> 00:07:47,785
Or generally, are there
correlated, to the same issue?

124
00:07:48,255 --> 00:07:51,445
for example, I have a pod,
I have an issue in a pod.

125
00:07:52,004 --> 00:07:54,485
What would that mean on
a downstream element?

126
00:07:55,055 --> 00:07:59,345
For sure, we'll see logs there, but how do
I do the logical connection between them?

127
00:07:59,715 --> 00:08:01,474
it's an interesting, an
interesting question.

128
00:08:01,705 --> 00:08:05,335
A good example would be losing
a connection to a database, from

129
00:08:05,335 --> 00:08:08,785
a client pod, and then seeing
the log on, the database pod.

130
00:08:09,275 --> 00:08:11,185
There is a connection
to, to be made there.

131
00:08:11,715 --> 00:08:12,205
traces.

132
00:08:12,535 --> 00:08:16,985
which is, oftenly, oftenly
used, to go and go over, a lot

133
00:08:16,985 --> 00:08:18,285
of these type of, challenges.

134
00:08:18,535 --> 00:08:20,325
So they show service, interaction.

135
00:08:20,585 --> 00:08:23,035
The real question is, how
do you put them in place?

136
00:08:23,405 --> 00:08:25,475
And what is the effort,
to put them in place?

137
00:08:25,970 --> 00:08:28,640
Meaning essentially
instrumenting the environment.

138
00:08:28,840 --> 00:08:34,270
It could be easy for some element, which
already supports it, but for legacy

139
00:08:34,270 --> 00:08:39,630
code or for other types of code bases,
it could be a challenge by itself.

140
00:08:39,690 --> 00:08:44,870
eBPF, of course, is a technology
that could help a lot with that.

141
00:08:45,750 --> 00:08:49,520
But achieving really traceable
code on a high scale, high scale

142
00:08:49,520 --> 00:08:51,590
environment is, is not that easy.

143
00:08:51,790 --> 00:08:54,480
Very similar to getting
coverage, coverage on code.

144
00:08:54,900 --> 00:08:59,150
so, essentially, what we see in a lot
of environments, people have, they

145
00:08:59,150 --> 00:09:03,320
want, to instrument the environment,
but the gap, to get the environment

146
00:09:03,320 --> 00:09:05,880
fully instrumented is very, very large.

147
00:09:06,380 --> 00:09:09,600
moving forward, let's speak about
the concept of real time topology.

148
00:09:10,390 --> 00:09:13,240
I think just for a basic
understanding, all of us.

149
00:09:13,935 --> 00:09:16,005
Always has some sort of a chart.

150
00:09:16,305 --> 00:09:17,835
It could be something that we drew.

151
00:09:17,935 --> 00:09:22,705
It could be an architecture slide or
architectures chart of the environment.

152
00:09:23,125 --> 00:09:26,115
And the reason behind that is
to give us the overall context.

153
00:09:26,525 --> 00:09:26,925
Right.

154
00:09:27,755 --> 00:09:31,485
There's a lot of moving part, but
if I want to capture in a glance

155
00:09:31,515 --> 00:09:35,435
what the system is doing, how
it's built in high level terms.

156
00:09:35,715 --> 00:09:39,625
So for all of us, the concept of
using a chart is very, very natural.

157
00:09:40,125 --> 00:09:44,485
Another concept which was
evolved throughout the years is

158
00:09:44,485 --> 00:09:46,629
the concept of dependency net.

159
00:09:47,390 --> 00:09:54,080
Maybe I can create some sort of a static
chart but dynamically generated chart

160
00:09:54,260 --> 00:10:00,870
updated in updated in some sort of a low
time interval, and create some sort of the

161
00:10:00,870 --> 00:10:06,500
view, which mimics what the environment
looks like, at least in the in the last

162
00:10:06,500 --> 00:10:10,680
couple of minutes or in the last couple
of hours, real time topology as we will

163
00:10:10,720 --> 00:10:14,750
touch it is like the involvement on that
into something which is fully dynamic.

164
00:10:15,460 --> 00:10:18,980
Real time topology in that aspect
should provide us the context

165
00:10:18,980 --> 00:10:21,060
required to eliminate blind spots.

166
00:10:21,590 --> 00:10:26,580
I want to generate more, more,
more, bits and pieces of context.

167
00:10:27,280 --> 00:10:32,100
And help me understand how the
metrical part traces parts logs part

168
00:10:32,370 --> 00:10:36,210
are really connected in real life
and what should come into my mind.

169
00:10:36,210 --> 00:10:40,160
And with that real time topology is one
of, one of the most important tools.

170
00:10:40,550 --> 00:10:41,200
What is it?

171
00:10:41,370 --> 00:10:45,690
If we'll try to summarize it, it's
like map of system relationship in the

172
00:10:45,690 --> 00:10:51,380
Kubernetes case, it will be encompassing
all the cluster layers and then spaces

173
00:10:51,380 --> 00:10:55,720
layer going to the more granular
elements like pods and services.

174
00:10:55,885 --> 00:11:00,215
From the software perspective, most
of the time, if it's a complex enough

175
00:11:00,245 --> 00:11:03,685
and smart enough, smart enough, real
time topology, it will incorporate

176
00:11:03,695 --> 00:11:07,625
the infrastructure, element each in.

177
00:11:08,005 --> 00:11:12,855
It could go even much higher and show
the interactions, but not only the

178
00:11:12,855 --> 00:11:16,875
interaction as a line from here to
there, but also really, these are the

179
00:11:16,905 --> 00:11:21,665
type of APIs, the type of information,
the type of transactions that are

180
00:11:21,725 --> 00:11:23,185
traversing between the elements.

181
00:11:24,025 --> 00:11:25,485
Why should it matter?

182
00:11:25,965 --> 00:11:27,045
Why should it matter to us?

183
00:11:27,445 --> 00:11:31,505
The ability to do this logical
connection, of course, gives us an

184
00:11:31,515 --> 00:11:36,065
element that we would have been doing
in the troubleshooting process anyway.

185
00:11:36,675 --> 00:11:41,405
If I have an issue, let's say an
API which now returns an error.

186
00:11:41,655 --> 00:11:46,835
What I would do as an engineer is
start and backtrace that and try to

187
00:11:46,835 --> 00:11:48,835
follow a logical, a logical line.

188
00:11:49,465 --> 00:11:53,090
My API is Bringing an error to the user.

189
00:11:53,370 --> 00:11:56,860
So it's probably experiencing,
experiencing some sort

190
00:11:56,860 --> 00:11:58,070
of an issue by itself.

191
00:11:58,340 --> 00:12:00,210
What would be the cause of that issue?

192
00:12:00,580 --> 00:12:01,460
Let's look at the log.

193
00:12:01,520 --> 00:12:02,470
Let's look at the metric.

194
00:12:03,010 --> 00:12:05,050
If I understand that, let's backtrace.

195
00:12:05,370 --> 00:12:09,030
One step before that, could it be that
something like a database transaction

196
00:12:09,320 --> 00:12:12,930
couldn't be completed or it couldn't
free the value and so on and so forth.

197
00:12:13,250 --> 00:12:18,190
And we're starting to essentially build
a path on a graph or a path on topology.

198
00:12:18,690 --> 00:12:22,680
So the concept should be clear
because essentially we're mimicking

199
00:12:23,310 --> 00:12:27,480
the natural way of thinking for
engineers, which is the backtracing.

200
00:12:27,940 --> 00:12:31,880
So that allows us to connect individual
points into some sort of a more.

201
00:12:32,110 --> 00:12:32,680
Full view.

202
00:12:33,430 --> 00:12:34,340
How does it work?

203
00:12:34,680 --> 00:12:39,610
so essentially we're taking or
we're continuously updating, the

204
00:12:39,610 --> 00:12:43,600
real time topology to reflect the
real time changes, new services,

205
00:12:43,920 --> 00:12:45,980
new deployments, new interactions.

206
00:12:46,480 --> 00:12:51,750
And that allows us to grasp this,
massive amount of data in, in a way,

207
00:12:51,750 --> 00:12:53,720
which is, easy to, easy to present.

208
00:12:54,220 --> 00:12:55,900
Let's give, let's give an example.

209
00:12:56,400 --> 00:12:59,650
What we see in the following example
is the real time topology, in this case

210
00:12:59,650 --> 00:13:05,060
generated by eBPF, which allows us to, to
track both external services, something

211
00:13:05,060 --> 00:13:09,290
which is not part of our deployment,
but also deep dive into the structure

212
00:13:09,330 --> 00:13:14,560
of Kubernetes, looking at namespaces,
looking at the deployments and the

213
00:13:14,560 --> 00:13:19,290
pods, and even the interaction between
them to the protocol or API level.

214
00:13:19,760 --> 00:13:26,020
So for sure, if later was, we'll discuss
it, if something happened here, metrical

215
00:13:26,060 --> 00:13:31,980
evidence, log evidence, trace evidence,
being able to, being able to project that.

216
00:13:32,505 --> 00:13:37,525
On the topology could, explain a lot
of things or as part of the elimination

217
00:13:37,525 --> 00:13:41,595
process in troubleshooting allowed
us to focus on the right things.

218
00:13:42,095 --> 00:13:43,625
Let's give, let's give an example.

219
00:13:44,165 --> 00:13:47,707
Let's think about a kubernetes,
kubernetes, kubernetes pod, which is

220
00:13:47,707 --> 00:13:51,997
failing on readiness could happen because
a lot of recent, a lot of reasons.

221
00:13:52,617 --> 00:13:58,047
And let's say it's failing due to,
IO, it's failing due to storage.

222
00:13:58,147 --> 00:13:59,367
It could be a local storage.

223
00:13:59,657 --> 00:14:00,807
It could be, by the way.

224
00:14:01,177 --> 00:14:02,237
An external storage.

225
00:14:02,607 --> 00:14:04,877
Database in that aspect is a storage.

226
00:14:05,227 --> 00:14:08,147
Any sort of, an external,
external repository would

227
00:14:08,157 --> 00:14:09,537
be, a storage in that case.

228
00:14:10,427 --> 00:14:16,207
Being able to see that on a graph, being
able to connect the dots, understanding

229
00:14:16,237 --> 00:14:21,217
that there is a correlation between how
this pod, readiness in this case, prose

230
00:14:21,297 --> 00:14:26,167
is behaving with regards to an element
is connected to, it's much easier to

231
00:14:26,167 --> 00:14:28,067
deduct when you have a real time map.

232
00:14:28,787 --> 00:14:29,857
I'm connected to.

233
00:14:30,152 --> 00:14:32,852
This database, this storage
element, so on and so forth.

234
00:14:33,352 --> 00:14:35,292
Logical for me to look at that.

235
00:14:35,582 --> 00:14:40,372
so with regards to that, mapping the real
topology or mapping the pod dependencies

236
00:14:40,702 --> 00:14:45,522
allows us to start and shrink the
amount of element that we're looking at.

237
00:14:45,842 --> 00:14:48,602
And in that case, it allows
us to understand that we need

238
00:14:48,602 --> 00:14:50,202
to focus on certain aspects.

239
00:14:50,212 --> 00:14:55,832
For in this example, The desire, which
is really, really, really the root cause

240
00:14:56,192 --> 00:15:00,482
and then eventually decide what to do,
either adjust the probe configuration

241
00:15:00,722 --> 00:15:06,212
or change other configuration, enable,
enable, enable different storage

242
00:15:06,232 --> 00:15:08,232
or other storage configuration.

243
00:15:08,732 --> 00:15:13,242
Another example, which a lot of time,
or a lot of you have faced in the past

244
00:15:13,242 --> 00:15:17,052
is throttling, especially throttling
when there is changes in the dynamics

245
00:15:17,052 --> 00:15:19,042
of the deployment going up or down.

246
00:15:19,352 --> 00:15:21,272
for example, in this case, the scenario.

247
00:15:21,477 --> 00:15:24,977
We're speaking about an application,
which, which is under high traffic.

248
00:15:25,757 --> 00:15:29,427
This usually in a lot of deployment,
there will be a connection between

249
00:15:29,427 --> 00:15:33,357
the amount of traffic to the size
of the deployment through a load

250
00:15:33,357 --> 00:15:35,607
balancer, through whatever other means.

251
00:15:35,857 --> 00:15:40,047
But essentially what we're looking
at here is the triggering of an

252
00:15:40,047 --> 00:15:42,247
HPA, of a Port Authorized Scaling.

253
00:15:42,727 --> 00:15:46,407
but eventually a lot of the
time the deployment will scale.

254
00:15:47,127 --> 00:15:51,477
It doesn't mean that the
infrastructure scale at the same pace.

255
00:15:52,017 --> 00:15:56,827
Or is well enough ready in
order to absorb the scaling.

256
00:15:57,127 --> 00:16:00,327
And a lot of time in this
situation, we see things like

257
00:16:00,347 --> 00:16:02,137
throttling, the number of pods.

258
00:16:03,097 --> 00:16:06,907
Has changed, but eventually we're
looking at, throttling from the

259
00:16:06,907 --> 00:16:08,537
infrastructure, perspective.

260
00:16:08,777 --> 00:16:13,937
And even though we scaled, we get into a
scenario in which we get malfunctions or

261
00:16:13,937 --> 00:16:19,467
we getting into the, the specific pods
not being able to handle, handle the load.

262
00:16:20,047 --> 00:16:23,077
how does it, how real time
topology will help here.

263
00:16:23,287 --> 00:16:25,207
So of course, one being able to.

264
00:16:25,552 --> 00:16:29,332
Visually see that being able to
see the different interaction,

265
00:16:29,652 --> 00:16:31,562
visualizing the resource allocation.

266
00:16:31,662 --> 00:16:31,972
Are there?

267
00:16:32,492 --> 00:16:36,482
More nodes or nodes group, being
allocated as part of the code scaling.

268
00:16:36,742 --> 00:16:40,932
and that would enable teams together
with the other, other observability

269
00:16:41,132 --> 00:16:46,002
telemetry data, to either adjust the
resources or limit the bandwidth or.

270
00:16:46,277 --> 00:16:50,277
To do whatever it takes in order
to, in order to solve it, but just

271
00:16:50,277 --> 00:16:53,747
the ability to understand that
this is an infrastructure issue.

272
00:16:53,767 --> 00:16:59,167
This is not an application issue is, of
course, already, at least 50 percent 50

273
00:16:59,647 --> 00:17:02,127
percent steps toward towards the solution.

274
00:17:02,627 --> 00:17:07,667
Misconfigured network policies, something
that we've all seen and suffered from.

275
00:17:07,927 --> 00:17:11,157
I think it's a very easy case
to understand how topology can

276
00:17:11,407 --> 00:17:16,507
help because the fact that real
time topology is in many aspects.

277
00:17:17,142 --> 00:17:19,772
Very similar to a network topology map.

278
00:17:19,942 --> 00:17:22,722
So I think it's an easy
aspect to understand.

279
00:17:22,922 --> 00:17:26,322
And other aspects, like we spoke about
before, which are real time topology.

280
00:17:26,392 --> 00:17:30,014
Topology is not only logical
connectivity, it could also be

281
00:17:30,014 --> 00:17:33,182
infrastructure connectivity, or
whatever connectivity, but I think

282
00:17:33,182 --> 00:17:35,652
this one is very easy to comprehend.

283
00:17:35,992 --> 00:17:40,112
So, for example, we have a pod or a
microservice deployed, and it failed

284
00:17:40,112 --> 00:17:41,612
to communicate with another service.

285
00:17:42,282 --> 00:17:45,142
And it doesn't even have,
even have to happen.

286
00:17:45,492 --> 00:17:47,962
It could happen sporadically,
but could happen because of the

287
00:17:47,962 --> 00:17:50,512
configuration change in real life.

288
00:17:50,552 --> 00:17:54,532
the complexities of getting into this,
this scenario could be almost infinite.

289
00:17:54,982 --> 00:17:57,382
that's happening because
of a network policy.

290
00:17:57,882 --> 00:17:59,167
It would be very hard.

291
00:17:59,597 --> 00:18:05,057
Or very hard without real time topology
to either comprehend that that is the

292
00:18:05,057 --> 00:18:10,257
issue or more than that, understand
which adjacent services I should look at.

293
00:18:10,597 --> 00:18:15,157
it could be also very hard to debug
only with something like logs.

294
00:18:15,547 --> 00:18:15,907
Right.

295
00:18:15,957 --> 00:18:17,597
There is a lot of sessions going on.

296
00:18:17,827 --> 00:18:20,947
It could be any single session,
which, which is failing, which

297
00:18:20,947 --> 00:18:22,507
will give us an issue here.

298
00:18:22,757 --> 00:18:27,137
So really, the lack of detailed logs
or visibility or traces, if that

299
00:18:27,137 --> 00:18:32,127
specific path is not, instrumented
in the, in the deployment, it's a

300
00:18:32,127 --> 00:18:34,307
very hard scenario to, to diagnose.

301
00:18:34,717 --> 00:18:38,957
But the ability here to have the service
to service mapping, looking at the APIs,

302
00:18:38,997 --> 00:18:41,997
maybe on the topology itself, mark.

303
00:18:42,567 --> 00:18:46,397
When there's an issue in one of
these paths, be another great,

304
00:18:46,477 --> 00:18:48,787
another great, idea, to do that.

305
00:18:49,007 --> 00:18:53,427
but the ability to, to show that or even
understand it might be here because.

306
00:18:54,032 --> 00:18:58,322
I see logical connectivity, or I thought
there should be a logical connectivity,

307
00:18:58,322 --> 00:19:03,262
but there is none, meaning that this
could be really a network policy issue.

308
00:19:03,672 --> 00:19:07,822
it's a great example for, for the use
of that and the amount of automation

309
00:19:07,822 --> 00:19:09,032
that, you can put around it.

310
00:19:09,737 --> 00:19:12,547
can expose a lot of these,
a lot of these cases.

311
00:19:13,047 --> 00:19:15,117
So what enables real time topology?

312
00:19:15,397 --> 00:19:17,637
essentially, there's ways to implement.

313
00:19:17,637 --> 00:19:22,087
It could be done with things
like distributed traces if the

314
00:19:22,087 --> 00:19:25,817
environment is instrumented, that
essentially following the distributed,

315
00:19:25,887 --> 00:19:31,177
distributed traces path could be a
tool to create real time topology.

316
00:19:31,417 --> 00:19:33,977
The one we're focusing on
here is, of course, the use of

317
00:19:33,977 --> 00:19:36,537
EDPF and with the use of EDPF.

318
00:19:37,257 --> 00:19:40,727
the ability to dynamically
create a real time topology.

319
00:19:41,227 --> 00:19:45,927
eBPF, I think, today, most of you
already, already understand it.

320
00:19:46,217 --> 00:19:51,087
It's a piece of technology, a part that's
natively supported by, the Linus kernel,

321
00:19:51,187 --> 00:19:52,897
extended Berkeley packet filtering.

322
00:19:53,477 --> 00:19:58,147
Essentially, the way to think about
it is a small type of VM or feature

323
00:19:58,157 --> 00:20:04,107
within the kernel that allow us to run
or safely run Low level instrumentation

324
00:20:04,107 --> 00:20:10,547
code beats or pieces of code, which from
one hand are instrumented and the kernel

325
00:20:10,817 --> 00:20:15,437
on the other hand, will bring data to
user space, kind of benefiting from,

326
00:20:15,507 --> 00:20:17,967
both of, both of the world, main uses.

327
00:20:17,967 --> 00:20:19,617
So of course traffic analysis.

328
00:20:19,617 --> 00:20:20,727
That was the original.

329
00:20:21,287 --> 00:20:25,857
used for eBPF, the ability
to trace network requests,

330
00:20:25,937 --> 00:20:27,897
messages, packets, sniff them.

331
00:20:28,457 --> 00:20:33,437
Of course, the most classical use, the
importance for topology is the ability

332
00:20:33,437 --> 00:20:38,867
to track APIs and the ability to, and
the ability to, and the ability to

333
00:20:38,867 --> 00:20:41,357
trick, essentially any kind of, session.

334
00:20:41,787 --> 00:20:44,367
infrastructure, app and
performance monitoring.

335
00:20:44,577 --> 00:20:49,387
So it's not only the ability to hook
into, the networking stack, but also

336
00:20:49,387 --> 00:20:54,617
the ability to hook into kernel calls
or to hook into application calls.

337
00:20:55,367 --> 00:21:00,327
These are very important because it
allows eDPF to create a granular, more

338
00:21:00,367 --> 00:21:02,217
granular level of instrumentation.

339
00:21:02,747 --> 00:21:07,727
What kernel calls are being called could
be important for a lot of use cases.

340
00:21:07,727 --> 00:21:11,997
What application calls are being
issued by the application could

341
00:21:12,007 --> 00:21:16,017
be important for things like
opening encrypted streams in HTTP2.

342
00:21:16,517 --> 00:21:19,957
It could be the use of
open sources and tracing.

343
00:21:20,007 --> 00:21:21,967
That's essentially how to auto instrument.

344
00:21:22,562 --> 00:21:23,452
pieces of code.

345
00:21:23,542 --> 00:21:27,432
If I can find in memory those
function calls, of course, I can add

346
00:21:27,592 --> 00:21:32,162
I can at the trace point and enable
that enable that instrumentation.

347
00:21:32,522 --> 00:21:37,642
And all of these things which are very
relevant to observability are naturally

348
00:21:37,662 --> 00:21:42,862
very, very important for security with
a change of the use case networking

349
00:21:42,862 --> 00:21:48,542
through enforced network policies and
infrastructure and app call to Protect

350
00:21:48,572 --> 00:21:53,782
or detect misbehavior, misbehavior
with regards to things like open source

351
00:21:53,792 --> 00:21:56,702
usage or kernel usage, resource usage.

352
00:21:56,952 --> 00:22:02,442
So a lot of similar aspects, but the use
cases on top are very much different.

353
00:22:02,862 --> 00:22:04,632
So why does it matter?

354
00:22:05,592 --> 00:22:09,212
eBPF, of course, enables
deep observability.

355
00:22:09,792 --> 00:22:11,152
It's also native in Linux.

356
00:22:11,632 --> 00:22:15,222
In my perspective, that's maybe one
of the most important things, it's

357
00:22:15,222 --> 00:22:20,242
like the native tool being, put there
in order for people implementing

358
00:22:20,662 --> 00:22:22,492
this type of, this type of mechanism.

359
00:22:22,922 --> 00:22:27,272
it's, invaluable, for complex
distributed environments, both from

360
00:22:27,272 --> 00:22:29,372
security and observability perspective.

361
00:22:29,872 --> 00:22:32,912
So why is eBPF ideal
for distributed systems?

362
00:22:33,032 --> 00:22:35,152
there's a lot of, building blocks to that.

363
00:22:36,062 --> 00:22:40,962
put a little bit of a caveat, caveat
here, also very, very, dependent on

364
00:22:40,972 --> 00:22:45,962
the implementation itself, but on the
forefront, if you're doing it right.

365
00:22:46,582 --> 00:22:53,672
The ability to implement low latency
monitoring, small pieces of eBPF code,

366
00:22:54,212 --> 00:22:58,382
bringing the relevant data, capturing
the right element from the network

367
00:22:58,382 --> 00:23:02,862
pockets can allow us to discover
the environment and understand the

368
00:23:02,862 --> 00:23:05,242
connectivity without creating confusion.

369
00:23:06,002 --> 00:23:10,692
Latency overhead, or without creating
other aspects, which will implement the

370
00:23:10,692 --> 00:23:12,402
overall performance of the application.

371
00:23:12,972 --> 00:23:17,462
Enhanced visibility, so the ability
to go and discover, or to go

372
00:23:17,462 --> 00:23:21,452
and instrument specific elements
which are important for us.

373
00:23:21,452 --> 00:23:23,722
It could be, It could
be distributed queues.

374
00:23:23,912 --> 00:23:25,672
It could be database transaction.

375
00:23:25,942 --> 00:23:32,122
It could be whatever is important enough
for us to deep dive into and we don't have

376
00:23:32,152 --> 00:23:38,662
necessarily an alternative to monitor it
at a fine level, at a fine granular level.

377
00:23:39,472 --> 00:23:40,582
Improved efficiency.

378
00:23:40,892 --> 00:23:45,422
So, as a building block,
security, observability.

379
00:23:45,737 --> 00:23:50,347
What have you, it can reduce the need
for other or specific diagnostic tools.

380
00:23:50,577 --> 00:23:54,957
So being able to use eBPF as a
building block to build the rest of

381
00:23:54,957 --> 00:23:56,877
the chain makes, makes a lot of sense.

382
00:23:56,907 --> 00:24:00,527
We already see it, by the way, not
only from a visibility perspective,

383
00:24:00,577 --> 00:24:04,977
but eBPF is being used on the
platform to implement many things.

384
00:24:04,977 --> 00:24:08,487
So it could be load balancers, it
could be, it could be, service meshes.

385
00:24:09,132 --> 00:24:12,432
The observability tools, security
tools, and the scalability.

386
00:24:12,742 --> 00:24:17,822
this being a very native, Linux, Linux
supported, Linux supported technology

387
00:24:18,502 --> 00:24:23,812
being basically automatically enabled on
each and every node, is super important.

388
00:24:24,092 --> 00:24:28,412
Maybe on a different aspect, the
ability to implement it not only for.

389
00:24:29,012 --> 00:24:33,732
Physical node, but for example,
implemented for, other ephemeral, other

390
00:24:33,732 --> 00:24:38,492
ephemeral, technologies with sidekicks or
things like that, can allow us to create

391
00:24:38,492 --> 00:24:40,732
a more consistent, observability approach.

392
00:24:41,232 --> 00:24:42,002
auto discovery.

393
00:24:42,992 --> 00:24:46,352
so in auto discovery, we're
going one layer above it.

394
00:24:46,362 --> 00:24:46,817
It's not auto discovered.

395
00:24:47,567 --> 00:24:52,487
Only I will call it the data, but it's
how we connect this data to a data

396
00:24:52,487 --> 00:24:57,567
model, something which takes it one
layer above it and could allow us start

397
00:24:57,757 --> 00:25:04,037
would allow us to start and speak about
things like hierarchies or structure or,

398
00:25:04,537 --> 00:25:09,007
or create a more formal way of defining
that, that would be the first aspect.

399
00:25:09,007 --> 00:25:13,017
And, of course, with that, things like
the use of machine learning or the use

400
00:25:13,017 --> 00:25:15,527
of pattern matching in order to take.

401
00:25:15,877 --> 00:25:20,967
the data from Kubernetes in order to take
the data that we get from eBPF and match

402
00:25:20,967 --> 00:25:27,087
them or mesh them together to create not
only a dynamic graph but to put this data

403
00:25:27,087 --> 00:25:31,147
modeling on top of it and essentially
creating something which is repeatable.

404
00:25:32,067 --> 00:25:37,487
This is a pod, this is a new pod or a
new service, we have a node group or

405
00:25:37,487 --> 00:25:42,397
we have we have identified a part of
our application which is a database

406
00:25:42,397 --> 00:25:46,337
or a business logic and then again and
again and again even when data changes

407
00:25:46,707 --> 00:25:50,587
still have it in the, still have it in
the same format, without essentially

408
00:25:50,587 --> 00:25:53,507
requiring, requiring any, manual updates.

409
00:25:54,007 --> 00:25:55,017
common pitfalls.

410
00:25:55,207 --> 00:26:02,167
even though the concept is relatively,
relatively simple, it still, it

411
00:26:02,177 --> 00:26:05,107
has a lot of, a lot of ways to
get, to get in the wrong direction.

412
00:26:05,507 --> 00:26:07,197
one, incomplete instrumentation.

413
00:26:07,697 --> 00:26:10,877
Eventually, we're trying to
create a graph or we're trying

414
00:26:10,877 --> 00:26:12,567
to create or look at a chart.

415
00:26:12,947 --> 00:26:17,237
The chart will be as good as the data
which is there creating the chart.

416
00:26:17,637 --> 00:26:24,507
So missing critical data like logs
or API or events or any other data

417
00:26:24,557 --> 00:26:29,867
eventually will create an incomplete
view and it will not be used.

418
00:26:30,137 --> 00:26:35,237
So it's important not only to think
about what data should go into there.

419
00:26:35,667 --> 00:26:40,067
Of course, eBPF can help with
that, but also to have in mind

420
00:26:40,067 --> 00:26:43,187
what is the most important parts
to have there to begin with.

421
00:26:43,947 --> 00:26:45,577
Stale topology maps.

422
00:26:46,227 --> 00:26:50,847
I think in many aspects, the
tendency maps are maybe suffering

423
00:26:50,847 --> 00:26:52,137
from that exact problem.

424
00:26:52,437 --> 00:26:56,737
Kubernetes environment or any distributed
environments are constantly changing.

425
00:26:56,737 --> 00:27:03,342
If the topology updates are not, Automatic
and sure if you're not being updated in a

426
00:27:03,342 --> 00:27:08,142
quick enough cadence, then the risk of is
the risk is, of course, taking decision

427
00:27:08,142 --> 00:27:12,842
today on yesterday's data, assuming
the topology can solve everything.

428
00:27:12,852 --> 00:27:17,502
I think the most trivial one, but
topology is, it's very powerful for

429
00:27:17,532 --> 00:27:22,452
troubleshooting, but it's rarely a
smoking gun, meaning what it gives you

430
00:27:22,952 --> 00:27:28,132
is it gives you the more context that
you need in order to go in the direction.

431
00:27:28,902 --> 00:27:31,762
of the root cause, but by
itself is not a root cause.

432
00:27:31,762 --> 00:27:36,592
It's just a way to formalize or
to model the observability data.

433
00:27:37,092 --> 00:27:41,152
Above that, or a layer above that, that's
the entirety of the concept of AIOps.

434
00:27:41,352 --> 00:27:47,582
Assuming that I have that, and assuming
I can extract this context, Can I now

435
00:27:47,582 --> 00:27:52,812
go and automate the root cause process,
or go at least, into a process which

436
00:27:52,812 --> 00:27:57,352
allows me to lower the amount of alerts
that I have and go in the direction

437
00:27:57,352 --> 00:27:58,832
of, in the direction of the solution.

438
00:27:59,332 --> 00:28:00,502
Tips for getting started.

439
00:28:01,002 --> 00:28:05,582
Assuming you want to do it by yourself
or with a combination of open source

440
00:28:05,582 --> 00:28:10,072
tools or with commercial tools,
which allow, remember the metadata.

441
00:28:10,072 --> 00:28:10,207
Yeah.

442
00:28:10,857 --> 00:28:15,857
You can create, or help yourself,
and choose a strategy with

443
00:28:15,857 --> 00:28:19,697
regards to creating, to creating
the data, the dynamic topology.

444
00:28:19,697 --> 00:28:24,227
But the Kubernetes metadata,
it's eventually detects things

445
00:28:24,227 --> 00:28:29,707
like labels or names or config
maps or events in Kubernetes.

446
00:28:30,107 --> 00:28:33,527
If you're putting them on the real
time topology, that's eventually how

447
00:28:33,527 --> 00:28:37,907
you populate that, with, Populate
that with the most important context.

448
00:28:37,907 --> 00:28:40,047
Yeah, I have the structure now.

449
00:28:40,087 --> 00:28:45,187
I also have everything on top automate
the topology map So when you're

450
00:28:45,477 --> 00:28:50,237
looking and scrubbing for solutions,
the dynamically up the topology think

451
00:28:50,247 --> 00:28:52,397
about these elements How fast are they?

452
00:28:53,237 --> 00:28:55,397
How fast are they updating?

453
00:28:55,847 --> 00:29:00,227
Do they take into consideration of the
Kubernetes data, the infrastructure

454
00:29:00,227 --> 00:29:06,387
data, how fast they are to respond to
frequent state changes, and eventually

455
00:29:06,387 --> 00:29:08,167
avoid context binding pitfalls.

456
00:29:08,877 --> 00:29:11,717
Environments are large,
environments are complex.

457
00:29:12,217 --> 00:29:16,797
It might be beneficial not to try and
model anything, everything, and not to try

458
00:29:16,797 --> 00:29:19,247
and put everything in there to begin with.

459
00:29:19,637 --> 00:29:22,857
It's our role as engineers to
understand the Pareto rule.

460
00:29:23,287 --> 00:29:27,357
What are the 80 percent of things
that are important enough for me, to

461
00:29:27,357 --> 00:29:29,557
look at, and then expand the process.

462
00:29:29,557 --> 00:29:33,857
It could be good enough to start with
something which is Relatively small

463
00:29:33,867 --> 00:29:38,447
or relatively small, but sufficient to
give me the view that I need and then

464
00:29:38,467 --> 00:29:42,757
go and go in and enlarge that as the
system involved or as I have, have I

465
00:29:42,767 --> 00:29:45,827
have, as I have more resources to do so.

466
00:29:46,327 --> 00:29:48,397
I hope you enjoyed that.

467
00:29:48,817 --> 00:29:51,597
If there's any question,
just shoot us an email.

468
00:29:51,817 --> 00:29:54,707
and I really enjoyed having
this session with you.

469
00:29:55,037 --> 00:29:55,497
Thank you.

