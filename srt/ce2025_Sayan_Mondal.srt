1
00:00:00,140 --> 00:00:03,270
It's absolutely fantastic
to be here at Con 42 2025.

2
00:00:03,600 --> 00:00:05,699
Thanks for taking the
time to join the session.

3
00:00:06,150 --> 00:00:09,440
Today we're diving into a topic that
challenges traditional thinking about

4
00:00:09,440 --> 00:00:13,760
reliability, which is the chaos first
mindset and not like laying back

5
00:00:13,770 --> 00:00:15,250
and not being proactive about it.

6
00:00:15,870 --> 00:00:19,340
So buckle up because it's going
to be fun and exciting ride

7
00:00:19,370 --> 00:00:21,210
into proactive resilience.

8
00:00:22,100 --> 00:00:23,260
So a little bit about me.

9
00:00:23,350 --> 00:00:24,190
My name is Shahid.

10
00:00:24,259 --> 00:00:27,119
I am a senior software engineer
at Harness and also the maintainer

11
00:00:27,119 --> 00:00:28,349
and community manager at Litmus.

12
00:00:28,765 --> 00:00:31,555
from time to time I also work
as an LFX mentor for the Linux

13
00:00:31,555 --> 00:00:33,355
Foundation programs for Litmus Chaos.

14
00:00:33,825 --> 00:00:34,575
So that's about me.

15
00:00:35,075 --> 00:00:36,555
So what's the game plan for today?

16
00:00:36,944 --> 00:00:40,975
We're going to start with a discussion
on internal developer platforms, IDP.

17
00:00:41,184 --> 00:00:43,534
Why they're gaining so much
traction and why they matter.

18
00:00:43,884 --> 00:00:47,904
then we'll move into the biggest challenge
in cloud native development, reliability.

19
00:00:48,334 --> 00:00:52,424
The very thing we built our systems
for, yet struggle with so much.

20
00:00:52,794 --> 00:00:56,114
From there, we'll introduce chaos
engineering, talk about the chaos first

21
00:00:56,114 --> 00:00:59,644
principle and explore how it plays a
crucial role in platform engineering

22
00:01:00,284 --> 00:01:04,014
We'll also see some of the tools And
a hands on demo as well, which we

23
00:01:04,014 --> 00:01:08,094
intentionally have kept to break things
and on purpose And finally we'll discuss

24
00:01:08,094 --> 00:01:11,164
the future where this space is heading
and how you can integrate chaos into

25
00:01:11,164 --> 00:01:13,669
your own platform journey So let's go.

26
00:01:14,169 --> 00:01:18,409
Before we jump into that, let's take a
step back and talk about what's really

27
00:01:18,409 --> 00:01:19,989
happening in Cognitive Development.

28
00:01:20,699 --> 00:01:24,519
Now, today, it's fast, it's dynamic,
and it's constantly evolving.

29
00:01:24,739 --> 00:01:30,409
We've got CICD pipelines, DevOps, SecOps,
Configuration Management, Observability,

30
00:01:30,409 --> 00:01:32,179
Analytics, and the list keeps growing.

31
00:01:32,809 --> 00:01:36,589
And yet, despite all these
advancements, one problem still remains.

32
00:01:36,699 --> 00:01:37,959
That failure is inevitable.

33
00:01:38,744 --> 00:01:42,494
The more we distribute our systems,
the more dependencies we introduce and

34
00:01:42,494 --> 00:01:46,284
the harder it becomes to predict what
will break and when there are a couple

35
00:01:46,284 --> 00:01:49,324
of things, which I'll talk about later
as well, but there are things like.

36
00:01:49,864 --> 00:01:54,104
cascading failures or failures due to
complicated architecture now imagine an

37
00:01:54,254 --> 00:01:58,584
authentication service going down Suddenly
the payment systems user dashboards

38
00:01:58,584 --> 00:02:01,364
and even notifications stop working
right because they're all dependent.

39
00:02:01,384 --> 00:02:05,284
They're all dependent on it So this is
called a cascading failure and is a huge

40
00:02:05,284 --> 00:02:08,784
problem in cloud native system In fact,
80 percent of cloud native applications

41
00:02:08,784 --> 00:02:14,089
experience downtimes due to such issues
And, also instead of one big application,

42
00:02:14,089 --> 00:02:17,929
we now have hundreds of tiny services
running across multiple clusters, which

43
00:02:17,929 --> 00:02:21,419
talk to each other over networks, depend
on APIs, rely on different data stores.

44
00:02:21,739 --> 00:02:25,199
And if something fails, figuring
out where, what, and where

45
00:02:25,319 --> 00:02:26,779
can be incredibly difficult.

46
00:02:27,469 --> 00:02:31,339
there's unpredictability,
and, unpredictability is the

47
00:02:31,419 --> 00:02:33,199
default today, unfortunately.

48
00:02:33,569 --> 00:02:36,959
containers can crash, nodes can go
offline, there could be network issues,

49
00:02:36,959 --> 00:02:38,509
and, Everything is always changing.

50
00:02:38,889 --> 00:02:41,359
So how do we build resilience
in such an unpredictable world?

51
00:02:41,789 --> 00:02:44,329
The reality is failure is
no longer an exception.

52
00:02:44,379 --> 00:02:45,389
It is a given.

53
00:02:45,459 --> 00:02:47,909
And the question isn't how
do we prevent failures?

54
00:02:47,979 --> 00:02:49,749
Rather, how do we prepare for them?

55
00:02:50,219 --> 00:02:52,499
So that's where we start
shifting our mindsets.

56
00:02:52,999 --> 00:02:55,699
Now let's talk about internal
developer platforms or IDPs.

57
00:02:56,529 --> 00:02:59,419
These platforms are designed to
streamline developer experience,

58
00:02:59,459 --> 00:03:03,579
enabling teams to deploy and manage
applications with minimal friction.

59
00:03:04,089 --> 00:03:07,759
Think of them as self service
portals where developers can request

60
00:03:07,789 --> 00:03:11,389
infrastructure, manage deployments,
and ensure governance without needing

61
00:03:11,399 --> 00:03:12,759
to interact with multiple teams.

62
00:03:13,449 --> 00:03:14,449
But here's the challenge.

63
00:03:14,679 --> 00:03:17,419
IDPs work great when
everything is running smoothly.

64
00:03:17,749 --> 00:03:20,829
The moment something breaks,
things can spiral out of control.

65
00:03:21,489 --> 00:03:24,569
That's why integrating chaos
testing into IDP is critical.

66
00:03:24,809 --> 00:03:28,559
It ensures that failures do not
just get handled after they happen,

67
00:03:28,639 --> 00:03:30,059
but are proactively addressed.

68
00:03:30,559 --> 00:03:33,789
Let's zoom in on the real issue,
which is cloud native complexity,

69
00:03:33,790 --> 00:03:34,999
which I talked about before.

70
00:03:35,619 --> 00:03:38,770
We have moved from monolith to
microservices, and while that's great

71
00:03:38,770 --> 00:03:42,200
for flexibility, it also introduces
a massive reliability challenge.

72
00:03:42,875 --> 00:03:45,495
Your code doesn't exist
in isolations anymore.

73
00:03:45,675 --> 00:03:49,895
It's running on a web of interconnected
services, APIs, databases, third

74
00:03:49,895 --> 00:03:51,075
party dependencies, and so on.

75
00:03:51,575 --> 00:03:55,005
The problem is, when one thing fails,
it can trigger a domino effect.

76
00:03:55,255 --> 00:03:57,985
And in cloud native world, these
failures are happening all the time.

77
00:03:58,664 --> 00:04:01,685
So instead of just hoping for the
best, we need a new approach, one

78
00:04:01,685 --> 00:04:05,185
that assumes failure from the starts
and prepare for it proactively.

79
00:04:06,155 --> 00:04:10,095
Let's compare old school DevOps with
today's cloud native reliability.

80
00:04:10,145 --> 00:04:14,855
Back then, we built a single application,
deployed it in maybe once a quarter

81
00:04:14,865 --> 00:04:16,685
and had time to test things out.

82
00:04:17,615 --> 00:04:21,615
Fast forward to today, we are deploying 10
times as many microservices at a lightning

83
00:04:21,615 --> 00:04:23,285
speed across hundreds of environments.

84
00:04:23,695 --> 00:04:26,475
And with all this complexity,
we have to ensure reliability.

85
00:04:26,475 --> 00:04:27,515
So how do we even do that?

86
00:04:28,105 --> 00:04:30,735
Traditional monitoring and
incident response aren't

87
00:04:30,735 --> 00:04:31,975
really enough for us anymore.

88
00:04:32,165 --> 00:04:33,605
And, we need to inject.

89
00:04:34,245 --> 00:04:39,445
Chaos deliberately test failure scenarios
in real time and make resilience

90
00:04:39,445 --> 00:04:43,745
an active part of our development
process Now outages are expensive.

91
00:04:43,975 --> 00:04:48,285
They lead to financial loss reputational
damage frustrated users and whatnot

92
00:04:48,635 --> 00:04:51,495
Some of the biggest tech giants have
experienced massive failures due to these

93
00:04:51,495 --> 00:04:55,975
different type of issues And sometimes the
problem isn't even the application itself.

94
00:04:56,035 --> 00:05:01,085
It could be Bad code unhandled edge
cases or even unexpected system load

95
00:05:01,370 --> 00:05:05,230
there could be a series of cascading
failures, that can take down systems.

96
00:05:05,230 --> 00:05:08,950
For example, a similar event happened
with Slack, where it impacted

97
00:05:08,950 --> 00:05:10,460
thousands of businesses worldwide.

98
00:05:10,630 --> 00:05:15,000
And during the incidents, during, whenever
incidents happen, service often lock too

99
00:05:15,000 --> 00:05:18,960
much data or retry too aggressively, which
sort of overloads the system even more.

100
00:05:19,335 --> 00:05:24,725
And this sort of results in users unable
to access the services, leading to all

101
00:05:24,725 --> 00:05:26,285
this drop in trust and frustration.

102
00:05:26,525 --> 00:05:28,925
Even with cloud providers and
Kubernetes, infrastructure is

103
00:05:28,925 --> 00:05:30,205
never 100 percent reliable.

104
00:05:30,395 --> 00:05:34,245
You could have device failures like
hard drives crashing, power supply

105
00:05:34,245 --> 00:05:35,785
fail, or memory leaks building up.

106
00:05:36,430 --> 00:05:41,330
And, a financial company recently lost
over, 55 million because of the failure

107
00:05:41,330 --> 00:05:44,550
in one part of their infrastructure that
prevented transactions from processing.

108
00:05:45,140 --> 00:05:48,980
Sometimes, it's not code or
hardware, but how we handle these

109
00:05:48,980 --> 00:05:50,730
incidents making things much worse.

110
00:05:51,100 --> 00:05:54,030
if auto scaling is not configured
properly, let's say, then service

111
00:05:54,030 --> 00:05:56,340
crashing would not be even detected.

112
00:05:56,340 --> 00:05:59,240
If teams do not have right alerts,
they don't even know sometimes

113
00:05:59,320 --> 00:06:02,190
when something has gone wrong
until the user starts complaining.

114
00:06:02,830 --> 00:06:08,240
Having an active channel of alerts or
having a good way around how you can

115
00:06:08,240 --> 00:06:10,840
handle incidents is also something
that can trigger these things.

116
00:06:11,340 --> 00:06:13,050
So what exactly is chaos engineering?

117
00:06:13,380 --> 00:06:16,020
At its core, it's about
running control experiment to

118
00:06:16,020 --> 00:06:17,240
simulate real world failures.

119
00:06:17,270 --> 00:06:20,170
And you already know it because
you're in con 42 in chaos engineering.

120
00:06:20,170 --> 00:06:23,190
So we're not going to spend too much
time on what is chaos engineering.

121
00:06:23,210 --> 00:06:25,310
Rather, we'll talk about
the principles later.

122
00:06:25,410 --> 00:06:29,870
So instead of waiting for a real outage to
happen, we just introduce feel a failure

123
00:06:29,900 --> 00:06:33,550
ourselves and measure the impact and then
we learn how to recover quickly, right?

124
00:06:33,700 --> 00:06:36,360
So that is our goal to build
something, build confidence in

125
00:06:36,360 --> 00:06:40,490
our system and want our systems to
withstand unexpected disruptions.

126
00:06:41,130 --> 00:06:44,330
Now, here's the big idea, the chaos
for the chaos first principle.

127
00:06:44,570 --> 00:06:45,300
So instead of.

128
00:06:45,620 --> 00:06:49,470
So instead of treating failures as real
anomalies, we assume they're inevitable.

129
00:06:50,110 --> 00:06:53,620
And instead of reacting to failures,
we prepare for them up front.

130
00:06:54,130 --> 00:06:58,030
By injecting chaos early and often,
we make resilience a core part of

131
00:06:58,030 --> 00:06:59,950
our platform engineering process.

132
00:07:00,080 --> 00:07:03,290
And this means fewer surprises,
faster recovery, and much

133
00:07:03,290 --> 00:07:04,410
more reliable system overall.

134
00:07:04,910 --> 00:07:07,970
Platform engineers are the backbone
of reliability in modern system.

135
00:07:08,310 --> 00:07:11,630
But let's be honest, no matter
how well you design a platform,

136
00:07:11,960 --> 00:07:14,080
unexpected failures will still happen.

137
00:07:14,080 --> 00:07:17,415
Microsoft that's why chaos engineering
is a must have in platform engineering,

138
00:07:18,075 --> 00:07:21,785
as it allows us to test system
behavior under failure conditions,

139
00:07:22,065 --> 00:07:25,335
uncover hidden weaknesses, and also
continuously improve, our reliability

140
00:07:25,335 --> 00:07:28,085
postures through all these different
components that you see right now.

141
00:07:28,585 --> 00:07:32,635
Chaos engineering is not just a
testing practice, it's a mindset shift.

142
00:07:32,865 --> 00:07:37,615
And by running chaos experiments, platform
engineers gain real insights into how

143
00:07:37,615 --> 00:07:41,850
their system behaves under stress,
Now imagine deploying a new feature.

144
00:07:42,110 --> 00:07:45,990
Wouldn't it be great to know how it would
react to database failures or network

145
00:07:45,990 --> 00:07:48,950
latencies or certain spikes in traffic?

146
00:07:49,330 --> 00:07:53,050
Now chaos engineering helps uncover
these weaknesses spot weak spots early.

147
00:07:54,020 --> 00:07:57,310
With proactive resilience, we
can identify system bottlenecks

148
00:07:57,370 --> 00:07:58,570
before they impact users.

149
00:07:58,990 --> 00:08:02,370
We can improve incident response time
with well tested failure scenarios.

150
00:08:02,420 --> 00:08:06,440
We can also ensure that our infrastructure
self heals and recovers efficiently.

151
00:08:07,030 --> 00:08:10,140
This leads to more self sustaining
platform, actually, that can

152
00:08:10,140 --> 00:08:11,280
handle real world uncertainty.

153
00:08:11,280 --> 00:08:15,390
And whenever we are using IDPs
or platform engineering, we

154
00:08:15,390 --> 00:08:16,530
are already thinking about it.

155
00:08:16,540 --> 00:08:19,380
This is just, One more step down
the layer, one more addition to

156
00:08:19,510 --> 00:08:20,750
what you're already thinking of.

157
00:08:21,250 --> 00:08:23,640
Now, there are some fantastic
tools out there that can make

158
00:08:23,650 --> 00:08:24,840
chaos engineering accessible.

159
00:08:25,380 --> 00:08:28,940
Litmus Chaos, for example, is an open
source framework designed for cloud

160
00:08:28,990 --> 00:08:32,490
native chaos engineering and Backstage,
on the other hand, helps organize

161
00:08:32,720 --> 00:08:34,110
developer workflows really well.

162
00:08:34,780 --> 00:08:39,580
Combined, these tools make it easier
than ever to adopt a chaos first mindset.

163
00:08:40,080 --> 00:08:43,810
The future is fully automated, AI
driven chaos experiments that integrate

164
00:08:43,810 --> 00:08:46,460
seamlessly into the development
lifecycle and so on and so forth.

165
00:08:47,020 --> 00:08:50,120
but we're heading towards a world
where chaos testing isn't really

166
00:08:50,120 --> 00:08:53,330
an afterthought rather It's like
a built in part of every platform.

167
00:08:53,590 --> 00:08:58,760
So these are some of the tools we have
narrowed down to but it's an Not really

168
00:08:58,760 --> 00:09:03,190
an exhaustive list and you can pick any
tools which sort of adhere to this goal

169
00:09:03,520 --> 00:09:06,620
But for this presentation i'm going
to show litmus chaos and backstage

170
00:09:06,620 --> 00:09:11,080
together Now before we jump into how
you can use these tools I want to talk

171
00:09:11,090 --> 00:09:13,460
about the vision what we plan to do.

172
00:09:13,470 --> 00:09:15,165
What is the ultimate goal of it?

173
00:09:15,755 --> 00:09:19,275
So to truly integrate chaos into
platform engineering, we envision

174
00:09:19,305 --> 00:09:22,535
a structured journey built
around the key, four key pillars.

175
00:09:23,055 --> 00:09:25,035
Define and execute chaos experiments.

176
00:09:25,275 --> 00:09:28,135
The foundational step is to
define chaos experiments that fit

177
00:09:28,135 --> 00:09:29,765
your specific application needs.

178
00:09:30,275 --> 00:09:33,385
This means identifying appropriate
failure scenarios based on your

179
00:09:33,425 --> 00:09:37,645
infrastructure, whether it could be
cloud native, legacy, Linux based,

180
00:09:37,675 --> 00:09:39,465
Windows, or even mainframe applications.

181
00:09:40,075 --> 00:09:43,315
Initially, these experiments may be
executed manually to gain insights

182
00:09:43,325 --> 00:09:45,845
into potential failure points,
but later they can be automated.

183
00:09:46,765 --> 00:09:47,905
Chaos as a service.

184
00:09:47,975 --> 00:09:50,945
The next step is to make chaos
engineering self serviceable.

185
00:09:51,435 --> 00:09:55,075
By enabling chaos as a service,
teams can easily apply and disable

186
00:09:55,105 --> 00:09:56,535
chaos experiments on demand.

187
00:09:57,015 --> 00:10:00,435
This reduces friction, makes it
easy for application teams to

188
00:10:00,435 --> 00:10:01,935
integrate chaos into their workflows.

189
00:10:02,395 --> 00:10:06,245
And this helps greatly when people
are using platform, platform tools.

190
00:10:06,745 --> 00:10:08,475
Automate chaos in CI CD pipeline.

191
00:10:08,895 --> 00:10:12,325
Once chaos experiments are well
defined and serviceable, the

192
00:10:12,325 --> 00:10:13,485
focus shifts to automation.

193
00:10:14,065 --> 00:10:17,395
The goal is to integrate chaos
engineering into CICD pipelines as

194
00:10:17,395 --> 00:10:20,985
well, which allows failures to be
tested with every release that we do.

195
00:10:21,765 --> 00:10:25,115
Now with a push of a button, teams
can trigger chaos experiments and

196
00:10:25,115 --> 00:10:28,735
validate the system resilience
before production or deployment.

197
00:10:29,235 --> 00:10:32,755
Lastly, we enable observability
and automated chaos evaluation.

198
00:10:33,225 --> 00:10:36,735
we already know observability is key
when we want to track certain metrics.

199
00:10:37,060 --> 00:10:38,790
To understand the impact of chaos.

200
00:10:39,290 --> 00:10:43,080
And before we jump in a quick mention
to Namtu, who is a maintainer who

201
00:10:43,080 --> 00:10:45,880
helped with the litmus plugin for
backstage, which was just introduced

202
00:10:45,900 --> 00:10:47,410
and, is working great for us.

203
00:10:47,570 --> 00:10:47,940
Thanks.

204
00:10:48,890 --> 00:10:50,280
Now it's demo time.

205
00:10:51,200 --> 00:10:54,810
So let me explain the different,
three different aspects of this demo.

206
00:10:55,200 --> 00:10:57,400
So we have the app configuration.

207
00:10:57,650 --> 00:11:00,450
we have the entities, YAML, and
then we have a small Jeff, which

208
00:11:00,450 --> 00:11:01,380
is showing you how it works.

209
00:11:01,925 --> 00:11:06,275
So the app configuration YAML is where
you configure the target, which would

210
00:11:06,285 --> 00:11:11,035
be your litmus URL, where litmus is
deployed for you, which would also require

211
00:11:11,045 --> 00:11:15,555
a litmus authentication token, which
you have to export locally or in your

212
00:11:15,555 --> 00:11:19,805
cloud provider so that it can access,
it can bypass, or it can authenticate

213
00:11:19,805 --> 00:11:23,425
itself with litmus so that it can help
you with the chaos engineering flows.

214
00:11:23,850 --> 00:11:25,750
I'll talk about how you can
get this auth code later.

215
00:11:26,210 --> 00:11:29,980
and once you've done that, you go to
the entities YAML in backstage, where

216
00:11:29,980 --> 00:11:33,760
you have to paste in an annotation
of the project ID of Litmus Kiosk.

217
00:11:34,130 --> 00:11:37,980
So you can copy your project ID that
you're currently in, in Litmus, and

218
00:11:38,100 --> 00:11:41,340
you can paste that for backstage
to understand which project you're

219
00:11:41,340 --> 00:11:42,990
using to run your Kiosk applications.

220
00:11:43,130 --> 00:11:45,470
So I'll show all of this
in a demo right next.

221
00:11:45,850 --> 00:11:50,340
but there's other two, configuration YAML
that we need to modify and you see a small

222
00:11:50,350 --> 00:11:53,820
GIF, which is showing you how things look
in the backstage and how you can go to

223
00:11:53,820 --> 00:11:55,490
the actual workflows and check it out.

224
00:11:56,240 --> 00:11:57,820
So with that, let's get started.

225
00:11:58,420 --> 00:11:59,410
All right.

226
00:11:59,440 --> 00:12:00,840
So this is our backstage plugin.

227
00:12:00,860 --> 00:12:02,130
The URL is just litmus.

228
00:12:02,130 --> 00:12:03,600
com slash backstage plugin repo.

229
00:12:03,600 --> 00:12:06,800
You can either clone this repo or if
you already have your own repository.

230
00:12:07,080 --> 00:12:12,261
You can add this package in
your web folder or like in your

231
00:12:12,261 --> 00:12:16,370
application and then just modify the
entities And the app config file.

232
00:12:17,040 --> 00:12:20,470
So with that, I think that should be
good enough But let me also go back

233
00:12:20,470 --> 00:12:25,360
to visual code and show you the same
So you would have in your packages in

234
00:12:25,360 --> 00:12:30,230
your app Package json, you would see
this, backstage plugin litmus So you

235
00:12:30,230 --> 00:12:34,220
can use or install this package and with
that you would be able to modify your

236
00:12:34,220 --> 00:12:37,300
entities yaml You And you would also
be able to modify your app config file.

237
00:12:37,800 --> 00:12:42,560
So once you do that, we will quickly see
how we can modify this or create this

238
00:12:42,600 --> 00:12:47,860
litmus auth token, and then use it or
export it and put in your external IP or

239
00:12:47,870 --> 00:12:51,090
your IP of the, wherever litmus is hosted.

240
00:12:51,190 --> 00:12:53,510
And then in the entities, you
can just put in your project

241
00:12:53,510 --> 00:12:54,810
ID from your project as well.

242
00:12:55,310 --> 00:12:57,030
So we have litmus deployed already.

243
00:12:57,360 --> 00:12:58,770
So this is our litmus instance.

244
00:12:59,365 --> 00:13:01,155
So we're in the LitmusCurse
portal right now.

245
00:13:01,155 --> 00:13:04,105
So once you're in the portal, you
would see the admin or whichever

246
00:13:04,105 --> 00:13:05,765
user you're logged in as setting.

247
00:13:05,935 --> 00:13:07,155
So go to the account setting.

248
00:13:07,325 --> 00:13:10,675
Once you do that, you will see
there's a API token section.

249
00:13:10,735 --> 00:13:12,665
So from here, you can create a new token.

250
00:13:12,845 --> 00:13:17,455
Once you create a token, you
can choose the TTL and you can

251
00:13:17,455 --> 00:13:18,705
also give it to no expiration.

252
00:13:18,715 --> 00:13:22,415
And then once you enter it, you will
be seeing the token created along

253
00:13:22,415 --> 00:13:24,515
with the specific value of the token.

254
00:13:24,545 --> 00:13:25,635
So feel free to.

255
00:13:26,010 --> 00:13:29,970
copy that value and whenever you are
exporting or using the backstage app

256
00:13:30,340 --> 00:13:34,370
Or like hosting the backstage app or
wherever you want to use it Just put

257
00:13:34,370 --> 00:13:39,970
this litmus auth token this exact key
And export it with that value with

258
00:13:39,970 --> 00:13:42,910
the key value that you just copied So
once you do that, you should be able

259
00:13:42,910 --> 00:13:48,640
to authorize your litmus instance with
the backstage And we have a backstage

260
00:13:48,670 --> 00:13:50,400
as well also running on the local port.

261
00:13:50,440 --> 00:13:54,605
So it is localhost 3000 where Backstage
is running And since I have installed

262
00:13:54,615 --> 00:13:58,355
the package, it's also showing me the
backstage litmus demo, which is configured

263
00:13:58,795 --> 00:14:03,045
now for any reason, if you are new and
you want to get started with it, you

264
00:14:03,045 --> 00:14:06,465
can follow our follow along tutorial
as well from docs or litmus kiosk.

265
00:14:06,465 --> 00:14:06,865
io.

266
00:14:07,085 --> 00:14:10,095
It explains in detail how you can
set up potato hair or this demo

267
00:14:10,095 --> 00:14:14,095
application or any other queries you
have about might have about litmus.

268
00:14:14,095 --> 00:14:16,655
So you can feel free to go
to the docs and check it out.

269
00:14:17,320 --> 00:14:19,510
But for now, we'll just focus
on this backstage plugin.

270
00:14:20,090 --> 00:14:21,910
So we already have this project.

271
00:14:21,920 --> 00:14:22,920
Let me go into this component.

272
00:14:23,430 --> 00:14:25,420
This is one configuration
right now because we haven't

273
00:14:25,420 --> 00:14:26,810
done much wider setting.

274
00:14:27,130 --> 00:14:29,900
So we'll see the owner, the system,
the type, the tags and everything.

275
00:14:30,120 --> 00:14:31,710
How many, chaos hubs are there?

276
00:14:31,710 --> 00:14:32,580
How many infras are there?

277
00:14:32,580 --> 00:14:34,210
How many experiments you have run so far?

278
00:14:34,650 --> 00:14:37,980
But let's go to the main one,
which is the litmus tag here.

279
00:14:38,150 --> 00:14:41,240
The other tags you can work together and
build your own IDP solution out of it.

280
00:14:41,510 --> 00:14:43,930
But for now, what we want to
focus on is the litmus tag.

281
00:14:43,950 --> 00:14:44,850
So let's click on that.

282
00:14:44,960 --> 00:14:46,380
You'll see there's a couple of options.

283
00:14:46,520 --> 00:14:47,770
There's experiment docs.

284
00:14:47,770 --> 00:14:48,790
There's the API docs.

285
00:14:49,040 --> 00:14:49,800
We have the hub.

286
00:14:49,810 --> 00:14:50,790
You have the community.

287
00:14:51,010 --> 00:14:54,390
in the hub itself, you would see there's
a litmus chaos sub listed down, which

288
00:14:54,390 --> 00:14:57,580
has different experiments, different
faults and the environment as well.

289
00:14:57,580 --> 00:14:59,990
So if I click on this, it'll
take me straight to the hub.

290
00:15:00,155 --> 00:15:03,115
Which is the repository of all the
different faults available for us to

291
00:15:03,125 --> 00:15:06,385
experiment and play around with or
combine into different scenarios with.

292
00:15:06,725 --> 00:15:10,135
So this is a list of the different
templates and these are the list of

293
00:15:10,135 --> 00:15:13,785
the different faults that you can
use to create your own hypothesis.

294
00:15:13,985 --> 00:15:17,135
So if I go to the generic one, let's
say Kubernetes, you'll see there are

295
00:15:17,145 --> 00:15:21,835
different types of, Faults like power
off, node faults, or docker faults, or.

296
00:15:22,820 --> 00:15:26,550
Pod memory hog or pod specific faults
so you can combine them and create

297
00:15:26,550 --> 00:15:29,910
anything you want to so this is an
instance of litmus Which is pulled

298
00:15:29,910 --> 00:15:33,180
into backstage and you can manage and
everything manage and control everything

299
00:15:33,180 --> 00:15:37,610
from backstage itself for environments
We have backstage n which is again the

300
00:15:37,670 --> 00:15:41,705
infrastructure a chaos infrastructure
where our which is enabled for us So if

301
00:15:41,705 --> 00:15:46,495
I have to just show that to you, if I
just do a kubectl get pods in backstage

302
00:15:46,545 --> 00:15:50,135
and backstage is the namespace where I
have deployed this kiosk infrastructure.

303
00:15:50,555 --> 00:15:53,755
So you would see that we have our
execution plane components running here.

304
00:15:53,895 --> 00:15:55,325
So this will be able to help you.

305
00:15:55,875 --> 00:15:59,525
Execute the kiosk experiment also
detect if your application is present

306
00:15:59,525 --> 00:16:02,495
in this cluster or not And since it
is deployed in a cluster wide access,

307
00:16:02,765 --> 00:16:07,525
so any application demo application
Discoverable within this like present

308
00:16:07,525 --> 00:16:09,095
within this cluster would be discoverable.

309
00:16:09,095 --> 00:16:13,365
So if I just do kubectl get namespace,
you'll see I have a namespace called

310
00:16:13,365 --> 00:16:17,480
demo and in the demo namespace itself
you will see that pot tattoo head,

311
00:16:17,650 --> 00:16:19,090
our demo application is deciding.

312
00:16:19,590 --> 00:16:23,720
If I do this, you will see that there's
a potato hat, potato head, potato main,

313
00:16:23,720 --> 00:16:25,470
potato right, left arm and all that.

314
00:16:25,800 --> 00:16:29,690
So these, this is a demo, application
and it is also in the same cluster scope.

315
00:16:29,910 --> 00:16:33,205
So I'll be able to target this
demo application from, My,

316
00:16:33,445 --> 00:16:35,005
backstage namespace as well.

317
00:16:35,505 --> 00:16:39,605
So since everything is connected,
I'll just close them both and go back.

318
00:16:39,685 --> 00:16:43,455
And I had already run one experiment
of deleting a hat and you can see the

319
00:16:43,455 --> 00:16:45,055
experiments are also showing up here.

320
00:16:45,505 --> 00:16:47,895
So from here itself,
you can see the status.

321
00:16:47,895 --> 00:16:51,145
You can read on this again, or you
can visually like manually go to the.

322
00:16:51,645 --> 00:16:55,435
So if I click on this, I should also
be taken to the same execution details.

323
00:16:55,915 --> 00:16:59,485
And you would see that I have the
potato pod delete application I ran.

324
00:16:59,845 --> 00:17:04,125
And I had a probe which is just doing
a health check if it's running or not.

325
00:17:04,505 --> 00:17:09,085
just doing a health check to the specific
FQDN if it's healthy or not And it

326
00:17:09,085 --> 00:17:13,125
did pass successfully because I was
deleting the hat But if I just had to

327
00:17:13,135 --> 00:17:17,855
rerun this I will just rerun this for
an instant and come back to backstage

328
00:17:17,915 --> 00:17:22,035
reload You'll see that another instance
of this has already started So this

329
00:17:22,035 --> 00:17:26,025
helps greatly when you're just managing
everything from a single portal and for

330
00:17:26,025 --> 00:17:30,555
us backstage would be that portal And
when you're modifying things in litmus

331
00:17:30,585 --> 00:17:36,020
enabling githubs Managing multiple faults
Adding your own personal private hubs.

332
00:17:36,210 --> 00:17:39,190
Everything could be managed right from
within this one single destination.

333
00:17:39,190 --> 00:17:43,090
So this will be a single source from where
you can manage your own, chaos experiment,

334
00:17:43,120 --> 00:17:44,410
experimentation application flow.

335
00:17:44,910 --> 00:17:50,150
So that was what I wanted to showcase,
what I wanted to show to you.

336
00:17:50,200 --> 00:17:54,290
And since this is anyways going to be a
simple pod lead, it will function fine.

337
00:17:54,485 --> 00:18:00,745
If you had Grafana or any other
observability tool added with

338
00:18:00,745 --> 00:18:03,235
Prometheus, you would be also
able to visualize the same.

339
00:18:04,085 --> 00:18:08,805
So with that, I would just like
to finish on showcasing this and

340
00:18:08,805 --> 00:18:11,515
just wanted to put an emphasis
on this backstage litmus plugin.

341
00:18:11,935 --> 00:18:14,405
Now everything is also available as
part of the documentation as well.

342
00:18:14,405 --> 00:18:15,375
So feel free to explore that.

343
00:18:15,845 --> 00:18:19,955
But this is just empowering what
you can do when you have a platform

344
00:18:19,955 --> 00:18:22,505
like backstage or when you have
an open source tool this which can

345
00:18:22,505 --> 00:18:24,455
help integrate into your idp setup.

346
00:18:25,435 --> 00:18:30,355
So let's jump back to the future slides
So what is the future for us now as

347
00:18:30,375 --> 00:18:32,165
chaos engineering continues to evolve?

348
00:18:32,385 --> 00:18:35,720
We must establish best practices
and industry standards So

349
00:18:35,720 --> 00:18:36,810
here's what we envision.

350
00:18:37,270 --> 00:18:39,120
A maturity model for chaos engineering.

351
00:18:39,190 --> 00:18:42,940
So to develop a structured framework to
help organizations assess their current

352
00:18:42,970 --> 00:18:44,410
level of chaos engineering adoption.

353
00:18:44,770 --> 00:18:47,440
Now this model would also ensure
a clear path from beginner to

354
00:18:47,440 --> 00:18:48,990
advanced chaos engineering practices.

355
00:18:49,320 --> 00:18:52,810
And this helps orgs get like an overview
of where they are at and where they

356
00:18:52,810 --> 00:18:56,930
have to go We also want chaos budgeting
and guardrails This would be another

357
00:18:56,930 --> 00:19:00,880
framework which defines the acceptable
levels of disruptions or downtimes

358
00:19:01,110 --> 00:19:05,100
for different components This helps
teams allocate resources effectively

359
00:19:05,160 --> 00:19:09,160
while also ensuring experiments are
conducted safely Now lastly automation

360
00:19:09,160 --> 00:19:10,990
and observability improvements too.

361
00:19:11,220 --> 00:19:14,830
We want to enhance the automated
evaluation of chaos experiments Improve

362
00:19:14,830 --> 00:19:18,880
observability frameworks to integrate
chaos engineering insights into your

363
00:19:18,880 --> 00:19:20,700
own, their own monitoring systems.

364
00:19:21,270 --> 00:19:25,260
So these steps will drive the future of
chaos engineering adoption in platform

365
00:19:25,270 --> 00:19:28,660
engineering as well, specifically
making it essential, an essential

366
00:19:29,040 --> 00:19:30,480
practice for platform engineers.

367
00:19:31,240 --> 00:19:33,180
With that, I'd like to
finish my presentation.

368
00:19:33,450 --> 00:19:34,840
Thanks for attending my talk today.

369
00:19:35,100 --> 00:19:38,280
Here are a few links you can use to
connect with me or explore the project.

370
00:19:38,510 --> 00:19:39,970
And thank you once more
for listening to me.

