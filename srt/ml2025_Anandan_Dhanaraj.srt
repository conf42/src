1
00:00:00,049 --> 00:00:01,159
I'm ach.

2
00:00:01,849 --> 00:00:05,720
I'm happy to be here in Con
40 to Machine Learning 2025.

3
00:00:06,290 --> 00:00:08,210
I'm a senior software engineer.

4
00:00:08,360 --> 00:00:13,670
I have spent several years in building
a high performance microservices.

5
00:00:14,420 --> 00:00:18,860
I'm here to talk about how to
build a high performance scalable

6
00:00:19,520 --> 00:00:22,160
data pipeline for financial domain

7
00:00:24,500 --> 00:00:24,710
in.

8
00:00:25,384 --> 00:00:29,405
Today's volatile financial market
where millisecond can means

9
00:00:29,464 --> 00:00:31,354
million in lost opportunities.

10
00:00:31,775 --> 00:00:35,635
Traditional financial data
processing platform is failing to

11
00:00:35,635 --> 00:00:39,805
meet the increasing large volume
of data processing in short time.

12
00:00:40,345 --> 00:00:46,165
This architecture explains how to build
an high performance scalable data pipeline

13
00:00:46,165 --> 00:00:50,995
for the financial institution to handle
millions of transaction in short period

14
00:00:50,995 --> 00:00:53,545
of time with exceptional performance.

15
00:00:55,495 --> 00:01:00,235
The challenges of modern financial data
processing be categorized into volume

16
00:01:00,235 --> 00:01:06,055
constraint, latency requirement, and
data heterogeneity, volume constraint,

17
00:01:08,305 --> 00:01:12,565
legacy system struggle to handle
thousands of transaction in a

18
00:01:12,895 --> 00:01:18,355
second during normal processing
time, often it completely failing.

19
00:01:19,450 --> 00:01:23,290
During market volatility, even when
volumes can increase by order of

20
00:01:23,350 --> 00:01:26,140
magnitude latency requirements.

21
00:01:26,260 --> 00:01:30,460
Modern trading platform operate
in microsecond environment where

22
00:01:30,490 --> 00:01:34,420
even millisecond delays can
result in significant monetary

23
00:01:34,420 --> 00:01:38,110
losses and missed opportunities.

24
00:01:38,110 --> 00:01:43,810
For question adjustment data
heterogeneity financial institution

25
00:01:43,810 --> 00:01:45,185
must integrate data from.

26
00:01:45,730 --> 00:01:54,430
Multiple formats like Swift Jsl file
and in different delivery channels

27
00:01:54,430 --> 00:02:01,300
like FTP, MQ channels, Kafka topic,
and many more, which creates complexity

28
00:02:01,300 --> 00:02:03,220
that compounds performance challenges.

29
00:02:05,445 --> 00:02:08,590
Let me explain this architecture
of this data pipeline.

30
00:02:08,660 --> 00:02:10,759
This, let's start with
the canonical schema.

31
00:02:10,759 --> 00:02:13,340
Normalization is the very first beginning.

32
00:02:13,880 --> 00:02:17,660
Stage where data represent in
standardized format to enable

33
00:02:17,660 --> 00:02:21,050
the seamless data integration
with cross platform capabilities.

34
00:02:22,250 --> 00:02:22,970
Layer is.

35
00:02:23,405 --> 00:02:28,114
Collects and the process data extremely
quickly while having backup mechanism

36
00:02:28,114 --> 00:02:30,185
that automatically switches operation.

37
00:02:30,364 --> 00:02:35,915
If the primary system fails, which have
the retry capabilities in the enrichment

38
00:02:35,915 --> 00:02:39,905
framework, raw financial data will be
enhanced with additional information like

39
00:02:40,204 --> 00:02:45,515
security details, account details, and fx,
and any conversion, and the calculation

40
00:02:45,515 --> 00:02:47,405
will be done as part of this section.

41
00:02:48,875 --> 00:02:49,805
Storage layer.

42
00:02:49,865 --> 00:02:54,035
So once this enrichment is done,
the data will be flattened.

43
00:02:54,065 --> 00:02:59,225
That will be stored into our Oracle
database with unique indexing and

44
00:02:59,225 --> 00:03:04,625
the parting for each data sets in
the analytical consumption layer.

45
00:03:05,135 --> 00:03:10,280
We are using the advanced realtime
visualization to generate dashboards

46
00:03:10,355 --> 00:03:13,505
using the predictive analytics
with faster performance queries.

47
00:03:14,505 --> 00:03:19,545
Let we deeply explain about the canonical
schema normalization in the input

48
00:03:19,545 --> 00:03:23,624
format, multiple trading platform,
delivering data in proprietary formats

49
00:03:24,284 --> 00:03:28,874
with inconsistent field structure,
different time zones, daytime format.

50
00:03:29,354 --> 00:03:33,794
So this transformation engine is capable
of dynamically mapping the field based

51
00:03:33,794 --> 00:03:35,504
on the predefined schema structure.

52
00:03:37,034 --> 00:03:41,444
Which eliminates this input, format,
issues, and validation layer.

53
00:03:41,564 --> 00:03:45,074
The schema validation is enforced to
prevent from the incompatible data

54
00:03:45,074 --> 00:03:49,034
type and other formatting issue and
the configurable exception handling.

55
00:03:49,034 --> 00:03:52,634
Ensure a data integrity through
the throughout the pipeline,

56
00:03:53,264 --> 00:03:57,659
standardized output, uniformly
structured transaction with consistent

57
00:03:58,099 --> 00:04:02,114
identifiers and the normalized
field enabling seamless downstream

58
00:04:02,114 --> 00:04:04,634
processing regardless of data origin.

59
00:04:05,634 --> 00:04:11,094
As part of the enrichment framework,
the perfor, our enrichment framework

60
00:04:11,244 --> 00:04:15,624
in corporates, multiple reference
data to enrich the raw transaction

61
00:04:15,624 --> 00:04:18,054
into complete data endpoints.

62
00:04:18,294 --> 00:04:21,864
This help to spend less time
to analyze multiple platform

63
00:04:21,864 --> 00:04:23,669
for different data points.

64
00:04:24,669 --> 00:04:26,649
Strategic data storage.

65
00:04:28,659 --> 00:04:32,469
We have, we are using the Oracle
database for the better performance.

66
00:04:32,769 --> 00:04:34,869
As part of the strategic data storage.

67
00:04:34,899 --> 00:04:38,139
We are first analyze while
onboarding the data set.

68
00:04:38,559 --> 00:04:38,859
We will.

69
00:04:39,834 --> 00:04:44,314
Analyze the query patents and data
volumes will be for, to analyze that

70
00:04:44,374 --> 00:04:49,244
load of the data and partnering will be
done for all the required dataset with.

71
00:04:49,994 --> 00:04:52,364
Advanced time series optimization.

72
00:04:53,204 --> 00:04:59,024
Custom design indexes and optimized
query plans are used to better results.

73
00:04:59,714 --> 00:05:04,454
Our database architecture have
dramatically reduced complex query

74
00:05:04,574 --> 00:05:07,244
response time and production environments.

75
00:05:07,814 --> 00:05:11,564
This strategy was engineered to
optimally distribute load across

76
00:05:11,564 --> 00:05:14,384
multiple node for max availability.

77
00:05:14,999 --> 00:05:15,839
This achieving?

78
00:05:15,839 --> 00:05:20,789
Yes, 78% reduction in average
query latency for a mys

79
00:05:21,029 --> 00:05:22,679
critical financial operation.

80
00:05:23,609 --> 00:05:25,709
So mission critical financial operation,

81
00:05:26,709 --> 00:05:28,209
auto-scaling mechanism.

82
00:05:28,239 --> 00:05:33,429
I. Our auto schooling mechanism
that maintain consistent performance

83
00:05:33,489 --> 00:05:39,669
during market volatility automatically
adjusting resource van tra transaction

84
00:05:39,909 --> 00:05:43,219
volumes spikes during peak periods.

85
00:05:43,339 --> 00:05:47,779
Our predictive scaling algorithm
analyze historical patterns and current

86
00:05:47,779 --> 00:05:52,969
market conditions to proactively
provision resources before demand

87
00:05:52,969 --> 00:05:58,309
materialize, maintaining latency SLAs
even during extreme volume events.

88
00:06:00,469 --> 00:06:02,269
The performance benchmark.

89
00:06:02,839 --> 00:06:08,299
Our performance benchmark showcases
the architecture throughput capability

90
00:06:08,689 --> 00:06:12,559
capabilities with substantial capacity
to handle market driven search.

91
00:06:12,619 --> 00:06:13,339
When record.

92
00:06:13,699 --> 00:06:17,809
These metrics were validated under
simulated market volatility conditions

93
00:06:18,079 --> 00:06:22,999
matching historical flash crash
scenario, ensuring robust during extreme

94
00:06:22,999 --> 00:06:25,444
events our architecture is able to.

95
00:06:25,834 --> 00:06:30,394
Process 300 k transaction per
minute in the normal scenario.

96
00:06:30,454 --> 00:06:37,094
But in the, during the burst, it able
to process four 50 K messages between

97
00:06:37,094 --> 00:06:39,494
the within eight to 10 millisecond

98
00:06:41,504 --> 00:06:42,344
response time.

99
00:06:42,654 --> 00:06:46,674
The availability is 99.99
percentage with this architecture.

100
00:06:47,674 --> 00:06:52,464
Our implementation case studies results
as categorized into three sections

101
00:06:52,464 --> 00:06:57,354
like transaction processing capacity,
system availability, cost efficiency.

102
00:06:57,894 --> 00:07:02,994
Our implementation achieved the four
50 percentage increase in transaction

103
00:07:02,994 --> 00:07:07,464
processing capacity, enabling the system
to handle significant higher volume

104
00:07:07,554 --> 00:07:09,654
without performance and degradation.

105
00:07:10,014 --> 00:07:14,754
This eliminated previously common
processing backlogs during market events.

106
00:07:15,704 --> 00:07:20,624
The system reliability, unplanned
downtime was virtually eliminated

107
00:07:20,954 --> 00:07:24,674
with a reduction from approximately
few hours to under three minutes.

108
00:07:25,124 --> 00:07:25,664
Component.

109
00:07:25,664 --> 00:07:29,864
Component failures now resolve
quickly compared to the lengthy

110
00:07:29,864 --> 00:07:31,334
outages and legacy system.

111
00:07:31,694 --> 00:07:34,814
With operational recovery, time
improves from hours to minutes.

112
00:07:35,144 --> 00:07:39,224
Cost efficiency, pre-transaction
infrastructure costs were reduced

113
00:07:39,224 --> 00:07:43,214
by 68% through more efficient
resource utilization and the

114
00:07:43,214 --> 00:07:44,864
elimination of T processing.

115
00:07:47,334 --> 00:07:51,414
Complete pipeline lifecycle through
accusation to the conception.

116
00:07:51,714 --> 00:07:57,174
So when the publisher is publishing
that data into the data bus are

117
00:07:59,694 --> 00:08:01,374
raw, consumer is able to.

118
00:08:02,289 --> 00:08:04,029
Lesson messages from the database.

119
00:08:04,089 --> 00:08:08,619
For each dataset, we have the separate
MQ channels to listen the messages.

120
00:08:08,649 --> 00:08:12,729
The ingestion layer is able to
quickly read the messages from the

121
00:08:12,759 --> 00:08:16,959
database and it publish into the,
our internal processing layers.

122
00:08:17,049 --> 00:08:23,259
There we have the enrichment layer, which
transforms the raw data into the complete

123
00:08:23,259 --> 00:08:27,459
data set by invoking the reference
data and enrich all the data endpoints.

124
00:08:27,999 --> 00:08:28,989
And once this is.

125
00:08:29,604 --> 00:08:30,204
Enriched.

126
00:08:30,354 --> 00:08:32,964
We are using the Oracle database, which is

127
00:08:35,914 --> 00:08:41,464
Oracle database where that data is
stored in flattened format and also

128
00:08:41,834 --> 00:08:46,094
it able to perform like thousands
of records and a second without

129
00:08:46,304 --> 00:08:48,609
compromising any performance and.

130
00:08:49,979 --> 00:08:52,720
We have, and then we have
the analytical layer.

131
00:08:52,960 --> 00:08:57,140
We are creating the dashboard
using the predictive analytic with

132
00:08:57,140 --> 00:08:58,970
the past of performance query.

133
00:08:58,970 --> 00:09:01,450
So we will not see any
lag in the dashboards and

134
00:09:02,260 --> 00:09:06,240
and key takeaways and next
steps from this architecture.

135
00:09:06,720 --> 00:09:06,900
The.

136
00:09:07,665 --> 00:09:10,635
This presented architecture
has demonstrated the ability

137
00:09:10,635 --> 00:09:13,755
to process hundreds of thousand
financial transactions per second

138
00:09:13,755 --> 00:09:15,855
with sub millisecond latency.

139
00:09:16,095 --> 00:09:20,235
Providing that properly designed system
can meet the demands of modern financial

140
00:09:20,235 --> 00:09:23,505
market implementation consideration.

141
00:09:23,565 --> 00:09:25,455
When it comes to the
implementation consideration.

142
00:09:25,515 --> 00:09:27,075
Organization should begin with the.

143
00:09:27,075 --> 00:09:29,405
Comprehensive audit of current data flows.

144
00:09:29,735 --> 00:09:33,545
Identify the most critical
performance bench bottleneck and

145
00:09:33,605 --> 00:09:37,025
implement the architecture using an
incremental approach that delivers

146
00:09:37,085 --> 00:09:39,155
measurable improvement at each stage.

147
00:09:39,875 --> 00:09:44,825
Architectural balance success depends on
balancing multiple competing priorities

148
00:09:44,825 --> 00:09:48,905
like latency versus throughput,
complexity versus maintainability, and

149
00:09:48,935 --> 00:09:50,855
specialized optimization versus flex.

150
00:09:51,185 --> 00:09:55,355
Flexibility, a adoption to change
requirements future direction.

151
00:09:55,385 --> 00:09:56,435
Our research continue to.

152
00:09:56,850 --> 00:10:00,210
Continues into integrating machine
learning for predictive scaling

153
00:10:00,510 --> 00:10:03,960
and anomaly detection, exploring
graph database technologies for

154
00:10:04,230 --> 00:10:07,950
relationship modeling and implementing
zero downtime upgrade paths.

155
00:10:08,950 --> 00:10:10,850
Thank you for visiting this presentation.

156
00:10:10,930 --> 00:10:16,935
I hope this presentation helped to
build a high performance, scalable get

157
00:10:16,935 --> 00:10:18,735
up pipeline for the financial domain.

158
00:10:19,455 --> 00:10:19,875
Thank you.

