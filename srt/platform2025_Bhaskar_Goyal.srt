1
00:00:00,750 --> 00:00:01,410
Hello everyone.

2
00:00:01,620 --> 00:00:05,560
My name is Pastor Gole and I am
a software engineer at Google.

3
00:00:06,110 --> 00:00:10,820
Today I'll be presenting a talk on
building bulletproof AI systems from

4
00:00:10,820 --> 00:00:12,890
container kiosk to production paradise.

5
00:00:13,730 --> 00:00:19,139
The journey is often described as a
nightmare a chaotic world of containers,

6
00:00:19,169 --> 00:00:21,330
configurations, and last minute fires.

7
00:00:21,689 --> 00:00:23,099
We are here to change that narrative.

8
00:00:24,025 --> 00:00:27,925
In this session we'll explore
how to transform that deployment

9
00:00:27,925 --> 00:00:29,905
narrative into a production paradise.

10
00:00:30,235 --> 00:00:33,504
We'll look at battle tested
architectural patents that will help

11
00:00:33,504 --> 00:00:39,084
you build bulletproof AI platforms that
actually deliver on their promises.

12
00:00:39,584 --> 00:00:43,095
Let's start by acknowledging the
reality of enterprise AI deployment.

13
00:00:43,304 --> 00:00:46,664
Many organizations, despite
their best effort, run into

14
00:00:46,664 --> 00:00:48,614
significant hurdles that lead.

15
00:00:49,169 --> 00:00:51,360
To delays and even project failures.

16
00:00:51,390 --> 00:00:53,940
These challenges generally
fall into three categories.

17
00:00:54,390 --> 00:00:56,910
The first is environment inconsistencies.

18
00:00:57,360 --> 00:01:01,200
We have all heard it like,
but it works on my machine.

19
00:01:01,229 --> 00:01:04,140
This syndrome leads to the
unpredictable behavior.

20
00:01:04,440 --> 00:01:08,220
When models move from a data
scientist laptop to a staging or a

21
00:01:08,220 --> 00:01:12,270
production environment, inconsistent
libraries, dependencies, and even

22
00:01:12,270 --> 00:01:15,660
hard work can cause the morals to
behave different or fail entirely.

23
00:01:16,160 --> 00:01:18,500
The next are the manual
deployment processes.

24
00:01:19,100 --> 00:01:23,750
These are often error prone, incredibly
time consuming, and create bottlenecks.

25
00:01:23,990 --> 00:01:28,220
They frequently rely on specialized
knowledge of a few key individuals.

26
00:01:28,580 --> 00:01:32,360
If that person is on vacation, good
luck deploying that critical update.

27
00:01:33,170 --> 00:01:38,620
And finally, this leads to the production
fires those late night emergencies caused

28
00:01:38,620 --> 00:01:42,945
by unforeseen discrepancies between
what was tested and what is now live.

29
00:01:43,720 --> 00:01:48,580
These fires not only burn out our
teams, but also erode confidence

30
00:01:48,580 --> 00:01:50,530
in the AI initiatives themselves.

31
00:01:50,920 --> 00:01:55,720
The result is a deployment cycle that
stretches from days into works, or

32
00:01:55,750 --> 00:01:59,979
even months drastically diminishing
the returns of our AI system.

33
00:02:00,479 --> 00:02:03,029
So what does a production
paradise look like?

34
00:02:03,689 --> 00:02:07,439
Leading organizations have already
transformed their AI deployment

35
00:02:07,439 --> 00:02:10,479
processes and the results are dramatic.

36
00:02:10,839 --> 00:02:13,089
Success isn't just about avoiding failure.

37
00:02:13,119 --> 00:02:16,299
It's about creating a streamlined,
efficient, and reliable system.

38
00:02:16,869 --> 00:02:21,399
Imagine reducing your model deployment
time from weeks to may hours.

39
00:02:21,789 --> 00:02:23,619
Think about cutting configuration drifts.

40
00:02:24,279 --> 00:02:26,949
And errors by as much as 80%.

41
00:02:27,249 --> 00:02:28,209
This is achievable.

42
00:02:28,629 --> 00:02:34,029
We are talking about true deployment
to production parity, which eliminates

43
00:02:34,029 --> 00:02:35,949
those nasty late night surprises.

44
00:02:36,449 --> 00:02:41,154
Furthermore, success means having
scalable infrastructure that grows with

45
00:02:41,154 --> 00:02:43,619
demand without breaking your budget.

46
00:02:44,009 --> 00:02:48,419
It means empowering your data scientists
with a self-service platforms,

47
00:02:48,719 --> 00:02:50,474
allowing them to innovate faster.

48
00:02:51,299 --> 00:02:54,724
All while maintaining the governance
and control the business needs.

49
00:02:55,054 --> 00:02:57,244
This is the paradise we are aiming for.

50
00:02:57,744 --> 00:03:02,064
Okay, now let's look at what the
session mode map will look like.

51
00:03:02,844 --> 00:03:03,924
How did we get here?

52
00:03:04,314 --> 00:03:07,164
I have structured our journey
today into four key parts.

53
00:03:07,644 --> 00:03:09,404
This is your part to AI platform success.

54
00:03:10,094 --> 00:03:13,809
First, we'll dive into
containerization strategies.

55
00:03:14,454 --> 00:03:19,674
Specifically help ER and Kubernetes can
eliminate environment inconsistencies.

56
00:03:20,034 --> 00:03:24,324
Next, we will cover infrastructure as
core patterns using Terraform and help

57
00:03:24,624 --> 00:03:27,444
to prevent my configuration drifts.

58
00:03:27,834 --> 00:03:32,124
Then we'll explore GitHub's workflows
to automate our pipelines and

59
00:03:32,154 --> 00:03:34,524
make them version and trustworthy.

60
00:03:34,854 --> 00:03:37,254
Finally, we'll look at
real world ROI data.

61
00:03:37,674 --> 00:03:42,834
From organizations that have successfully
implemented these patterns at scale,

62
00:03:43,334 --> 00:03:45,494
our foundation begins
with containerization.

63
00:03:45,854 --> 00:03:47,564
This is what solves the classic.

64
00:03:47,624 --> 00:03:51,914
It works on my machine problem
by unpacking and packaging our

65
00:03:51,914 --> 00:03:57,364
models with all their dependencies,
libraries, and runtime configurations

66
00:03:57,364 --> 00:03:58,864
into a single portable unit.

67
00:03:59,409 --> 00:04:01,589
But AI workloads have unique needs.

68
00:04:02,489 --> 00:04:06,539
That generalization approaches
often don't address.

69
00:04:06,749 --> 00:04:11,489
We need to think about GPU acceleration
and access to other specialized hardware.

70
00:04:11,789 --> 00:04:17,159
We are often dealing with very large model
artifacts that can be gigabytes in size,

71
00:04:17,609 --> 00:04:23,249
which has implications for image storage
and memory requirements, and our inference

72
00:04:23,249 --> 00:04:26,880
and scaling patterns are often very
difficult from traditional web services.

73
00:04:27,510 --> 00:04:29,789
The impact of getting this right is huge.

74
00:04:30,464 --> 00:04:35,924
As a principal, Mr. Engineer at Forbes
500 financial services company, put

75
00:04:35,924 --> 00:04:42,374
it containerization, reduces our model
deployment failure by 65% and eliminates

76
00:04:42,374 --> 00:04:45,405
an entire class of environment rated bugs.

77
00:04:46,004 --> 00:04:50,674
This is the power of a solid
container foundation To build on that

78
00:04:50,674 --> 00:04:54,369
foundation, we need to use docker
patterns that are optimized for ai.

79
00:04:55,025 --> 00:04:56,944
Let's look at three key ones.

80
00:04:57,215 --> 00:04:59,465
First, multi-stage builds.

81
00:04:59,525 --> 00:05:04,325
This involves using separate development
and production stages in a Docker file.

82
00:05:05,015 --> 00:05:11,015
This results in the much smaller,
more secure final image, leading to

83
00:05:11,015 --> 00:05:15,604
faster pools, a reduced attack surface,
and lower resource consumption.

84
00:05:16,265 --> 00:05:19,145
Second, the hardware aware base images.

85
00:05:19,474 --> 00:05:21,694
Don't use a generic base
image for everything.

86
00:05:22,085 --> 00:05:24,034
Use Cuda enabled base images.

87
00:05:24,634 --> 00:05:29,074
Especially for your GPU workloads
to get maximum performance for

88
00:05:29,074 --> 00:05:32,944
CPU based inference services,
use CPU optimized images.

89
00:05:33,274 --> 00:05:37,204
This simple step can drastically
improve performance and minimize cost.

90
00:05:37,984 --> 00:05:41,164
And the third, a smart
layer caching strategy.

91
00:05:41,734 --> 00:05:45,184
Structure your DOC files to
maximize build cash hits.

92
00:05:45,574 --> 00:05:49,474
This means placing stable dependencies
like system packages at the beginning

93
00:05:49,474 --> 00:05:53,854
of the file and frequently changing code
like your model artifacts towards the end.

94
00:05:54,229 --> 00:05:54,619
Together.

95
00:05:54,619 --> 00:05:59,689
These patterns significantly reduce
image size and build times accelerating

96
00:06:00,349 --> 00:06:02,329
then entire development cycle.

97
00:06:02,829 --> 00:06:06,669
Once we have a optimized containers,
we need a way to manage the mat scale.

98
00:06:06,849 --> 00:06:08,259
This is where Kubernetes comes up.

99
00:06:08,259 --> 00:06:12,449
A Kubernetes provides an orchestration
layer that makes our contained rise.

100
00:06:12,509 --> 00:06:15,569
AI deployments truly
production ready kuban it is.

101
00:06:16,304 --> 00:06:20,954
Gives us powerful capabilities
like horizontal part auto scaling.

102
00:06:21,404 --> 00:06:26,924
This allows us to automatically scale
our inference services based on CPU

103
00:06:27,194 --> 00:06:32,054
Memory, our even custom metrics, like the
number of prediction requests per second.

104
00:06:32,474 --> 00:06:37,334
This ensures we have the capacity to
meet demand without over provisioning

105
00:06:37,334 --> 00:06:40,394
resources for our expensive hardware.

106
00:06:40,769 --> 00:06:44,669
We can use GPU, no pools
and resource quotas.

107
00:06:44,759 --> 00:06:49,949
This allows us to efficiently and
fairly allocate these costly GPU

108
00:06:50,279 --> 00:06:55,639
resources across multiple teams and
workloads, ensuring maximum utilization.

109
00:06:56,479 --> 00:06:59,959
Finally, Kubernetes enables
advanced deployment strategies

110
00:06:59,959 --> 00:07:01,669
like can redeployments.

111
00:07:01,939 --> 00:07:05,749
We can gradually roll out our new
model versions to a small subset of

112
00:07:05,749 --> 00:07:07,729
users, monitor their performance.

113
00:07:08,464 --> 00:07:13,054
And then proceed with a full
rollout, significantly reducing

114
00:07:13,054 --> 00:07:14,914
the risk of a bad deployment.

115
00:07:15,414 --> 00:07:21,684
Now let's move to an next pillar
infrastructure as a code or I-A-C-I-A-C

116
00:07:21,684 --> 00:07:26,124
transform your AI platform and
from a fragile, manually configured

117
00:07:26,124 --> 00:07:31,674
snowflake system into a reproducible
version, controlled and automated one.

118
00:07:32,274 --> 00:07:34,254
The key benefit is environment parity.

119
00:07:34,644 --> 00:07:39,444
With IEC, you can create identical
deployments, development, testing,

120
00:07:39,444 --> 00:07:41,784
and production environments
with a click of a button.

121
00:07:42,024 --> 00:07:45,954
Eliminating the it works in dev surprises.

122
00:07:46,434 --> 00:07:50,934
IEC is also your key to
boost disaster recovery.

123
00:07:51,354 --> 00:07:55,284
If an outage occurs, you can rebuild
the entire platform, reducing

124
00:07:55,284 --> 00:07:57,024
downtime from days to hours.

125
00:07:57,354 --> 00:08:01,674
It also ensures consistent compliance
and governance by allowing you.

126
00:08:02,304 --> 00:08:07,454
B to implement security controls
and access patterns as code applied

127
00:08:07,514 --> 00:08:09,614
uniformly across all departments.

128
00:08:10,114 --> 00:08:14,404
Terraform patterns for AI
infrastructure is very important

129
00:08:14,764 --> 00:08:17,734
when implementing IESE for ai.

130
00:08:18,574 --> 00:08:19,954
Terraform is a fantastic tool.

131
00:08:20,884 --> 00:08:23,764
Let's look at three battle
tested Terraform patterns.

132
00:08:23,824 --> 00:08:25,829
First, a module based architecture.

133
00:08:26,419 --> 00:08:30,679
Instead of writing monolithic
configurations, define reusable

134
00:08:30,679 --> 00:08:34,099
infrastructure modules for common
air components like model, race,

135
00:08:34,879 --> 00:08:37,099
inference, clusters, or feature stores.

136
00:08:37,489 --> 00:08:42,949
This promotes consistency, accelerates
development, and simplifies maintenance.

137
00:08:43,280 --> 00:08:46,879
Second, a clear environment
pro promotion strategy.

138
00:08:46,879 --> 00:08:50,749
Use separate directories or
terraform workspaces to manage

139
00:08:50,749 --> 00:08:51,889
your different environments.

140
00:08:52,310 --> 00:08:53,479
This ensures that.

141
00:08:54,199 --> 00:08:57,019
Identical configurations
are applied consistently.

142
00:08:57,079 --> 00:09:01,129
As you promote changes through your
CI slash CD pipelines, minimizing

143
00:09:01,129 --> 00:09:05,869
discrepancies and critically
use remote state management.

144
00:09:06,379 --> 00:09:12,919
Use your Terraform State files in a remote
shared location, like an S3 bucket, or

145
00:09:13,129 --> 00:09:15,974
Google storage with locking enabled.

146
00:09:16,894 --> 00:09:20,674
This is crucial for team
collaboration, prevents corruption

147
00:09:20,734 --> 00:09:24,934
and maintain an authoritative
record of your infrastructure.

148
00:09:24,934 --> 00:09:25,654
Configuration.

149
00:09:26,284 --> 00:09:31,444
Always ensure encryption and versioning
are enabled for security and auditability.

150
00:09:31,944 --> 00:09:35,624
This brings us to GitHubs,
which ties everything together.

151
00:09:36,014 --> 00:09:37,934
GitHubs transform our manual.

152
00:09:38,744 --> 00:09:44,864
Deployment processes into fully
automated auditable workflows by making

153
00:09:44,864 --> 00:09:47,264
GI the central hub of our operations.

154
00:09:47,594 --> 00:09:50,024
The key component are simple but powerful.

155
00:09:50,924 --> 00:09:54,284
Git becomes a single source of
truth for both infrastructure

156
00:09:54,284 --> 00:09:56,744
and applications configurations.

157
00:09:57,074 --> 00:10:02,264
We use pull based deployment operators
like Argo CD or Flux that automatically

158
00:10:02,264 --> 00:10:07,874
sync the cluster state with what's defined
in gi and we have continuous verification.

159
00:10:08,144 --> 00:10:11,594
That detects and remedies and
drifts from the desired state.

160
00:10:12,554 --> 00:10:14,654
The enterprise benefits are immense.

161
00:10:15,224 --> 00:10:20,204
We see an 80% reduction in deployment
time and a complete audit trail for

162
00:10:20,204 --> 00:10:24,884
compliance, self-healing architecture
that maintains its desired state

163
00:10:25,214 --> 00:10:30,074
and simplified rollbacks, just
you just revert a get command.

164
00:10:30,374 --> 00:10:30,884
Simple.

165
00:10:31,384 --> 00:10:34,624
So what does this all
mean for your business?

166
00:10:34,744 --> 00:10:38,824
The return on investment is tangible
and significant organizations

167
00:10:38,824 --> 00:10:43,354
adopting these patterns have seen
an 85% reduction in deployment time.

168
00:10:43,474 --> 00:10:49,414
For complex AI workloads moving from weeks
to hours, they have experienced 73% fewer

169
00:10:49,414 --> 00:10:54,184
production incidents due to environment,
consistency and automated testing.

170
00:10:54,244 --> 00:10:58,504
We have achieved 40% infrastructure
cost saving through better resource

171
00:10:58,509 --> 00:10:59,639
utilization and auto scaling.

172
00:11:00,439 --> 00:11:04,399
And perhaps most importantly,
they have been able to get three

173
00:11:04,399 --> 00:11:08,149
times more models into production,
dramatically increasing the throughput

174
00:11:08,419 --> 00:11:10,579
and impact of AI delivery pipelines.

175
00:11:11,079 --> 00:11:14,859
I want to leave with a clear, actionable
plan to get started on this one.

176
00:11:15,279 --> 00:11:17,439
Here's a 30 day plan for some quick ones.

177
00:11:17,919 --> 00:11:22,419
Start with by contain rising
one high value AI workload

178
00:11:22,749 --> 00:11:24,424
using multi-stage docker build.

179
00:11:25,299 --> 00:11:29,019
Establish basic Terraform modules for
your key infrastructure components,

180
00:11:29,019 --> 00:11:32,789
and set up GitHub's repository
to manage configurations and

181
00:11:32,789 --> 00:11:34,619
automate your initial deployments.

182
00:11:35,119 --> 00:11:38,659
From there, you can accelerate
towards production readiness

183
00:11:38,719 --> 00:11:40,699
within 90 day transformation plan.

184
00:11:41,539 --> 00:11:44,059
Implement Kubernetes
clusters with GPU support.

185
00:11:44,569 --> 00:11:48,109
Introduce candry deployment
strategies for safer model

186
00:11:48,109 --> 00:11:50,419
updates, and set up comprehensive.

187
00:11:51,139 --> 00:11:56,329
Monitoring and observability solutions
tailored for your AI workloads to ensure

188
00:11:56,509 --> 00:11:58,549
ongoing performance and reliability.

189
00:11:59,049 --> 00:12:03,519
By embracing these principles of
containerization infrastructure as code

190
00:12:03,519 --> 00:12:08,284
and GitHubs, you are truly move from
container kiosk to a production paradise.

191
00:12:08,349 --> 00:12:11,859
You can build AI platforms that
are not just powerful, but also

192
00:12:11,859 --> 00:12:13,924
reliable, scalable, and bulletproof.

193
00:12:14,739 --> 00:12:19,549
Thank you so much for your time, and
happy to answer any questions you have.

194
00:12:19,629 --> 00:12:20,409
Thank you for this.

