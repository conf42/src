1
00:00:01,200 --> 00:00:02,009
Hello everyone.

2
00:00:02,070 --> 00:00:03,270
I'm Vij Bajo.

3
00:00:03,930 --> 00:00:06,660
Today I'm going to talk about
something I deal with all the time,

4
00:00:07,500 --> 00:00:11,100
how we can use machine learning to
make log analysis faster and easier.

5
00:00:11,820 --> 00:00:14,760
If you have ever spent hours digging
through logs trying to figure out

6
00:00:14,760 --> 00:00:16,560
what went wrong, this talk is for you.

7
00:00:17,250 --> 00:00:21,240
I'll walk you through how ML can help
detect patterns spot tissue early, and

8
00:00:21,240 --> 00:00:23,160
made debugging a lot less frustrating.

9
00:00:25,305 --> 00:00:27,255
Let's talk about the reality we all face.

10
00:00:27,764 --> 00:00:31,034
Traditional debugging is still
largely slow and reactive.

11
00:00:31,634 --> 00:00:35,745
We wait for alerts, dig through logs, and
hope we can connect the dots fast enough.

12
00:00:36,555 --> 00:00:40,305
On top of that, we are dealing
with sheer volume, millions of log

13
00:00:40,305 --> 00:00:42,285
lines, most of which are noise.

14
00:00:42,285 --> 00:00:44,834
It's like trying to find a
sentence in Australia dictionary.

15
00:00:45,855 --> 00:00:47,475
This leads to one painful metric.

16
00:00:47,685 --> 00:00:50,115
MTTR mean time to resolution.

17
00:00:50,685 --> 00:00:54,555
It keeps rising, and with that comes
stress pager fatigue and delays

18
00:00:54,555 --> 00:00:56,115
that ripple into customer impact.

19
00:00:56,925 --> 00:00:57,645
What's missing?

20
00:00:57,945 --> 00:00:58,665
Intelligence?

21
00:00:59,175 --> 00:01:01,815
Something that can help us
make sense of the mess quickly.

22
00:01:02,145 --> 00:01:04,605
Not just search faster, but think smarter.

23
00:01:06,630 --> 00:01:07,920
Now let's flip the perspective.

24
00:01:08,370 --> 00:01:12,300
What if instead of treating logs like
noise, we saw them as a gold mine.

25
00:01:13,080 --> 00:01:16,440
Logs are packed with patterns,
repetitive behavior, known

26
00:01:16,440 --> 00:01:18,030
error trails and hidden signals.

27
00:01:18,600 --> 00:01:22,140
They're often hiding in plain
sight, just waiting to be uncovered.

28
00:01:22,815 --> 00:01:24,525
More importantly, logs are real.

29
00:01:25,005 --> 00:01:26,745
They reflect exactly what happened.

30
00:01:27,045 --> 00:01:30,045
User activity, system
behavior failure traces.

31
00:01:30,495 --> 00:01:31,995
There's no modeling guesswork.

32
00:01:32,595 --> 00:01:34,425
It's raw, authentic telemetry.

33
00:01:34,935 --> 00:01:38,175
And here's the thing, manual
analysis just can't keep up.

34
00:01:38,730 --> 00:01:42,179
Humans are great at spotting
obvious behaviors, but ML can

35
00:01:42,179 --> 00:01:45,960
uncover subtle correlations and
patterns that would otherwise be

36
00:01:45,960 --> 00:01:47,820
invisible, especially at scale.

37
00:01:50,039 --> 00:01:54,000
Even though logs are rich in value,
there are major hurdles that can

38
00:01:54,000 --> 00:01:55,980
make analyzing them a real challenge.

39
00:01:56,310 --> 00:01:58,620
First, the noise signal ratio is brutal.

40
00:01:59,520 --> 00:02:00,390
Most locks are noise.

41
00:02:00,990 --> 00:02:04,470
Finding something meaningful feels
like digging through sand for a needle.

42
00:02:05,280 --> 00:02:07,289
Second, formal inconsistency.

43
00:02:08,100 --> 00:02:09,329
Format, inconsistency.

44
00:02:09,780 --> 00:02:11,459
Different teams, different systems.

45
00:02:12,090 --> 00:02:13,440
Every log looks different.

46
00:02:13,530 --> 00:02:17,400
Some are JSON, some are plain
text and some are semi-structured.

47
00:02:17,760 --> 00:02:18,989
Passing becomes a nightmare.

48
00:02:19,739 --> 00:02:21,540
Then there are temporal gaps.

49
00:02:21,989 --> 00:02:24,329
Logs don't always line up neatly in time.

50
00:02:25,049 --> 00:02:29,220
You miss cause effect, relationships and
important, closer or lost in between.

51
00:02:30,015 --> 00:02:32,115
And finally, rare event detection.

52
00:02:32,595 --> 00:02:37,095
The critical stuff, it often shows
up once quietly, not in a burst,

53
00:02:37,275 --> 00:02:40,455
and those are the things that
ML can help us actually surface.

54
00:02:42,735 --> 00:02:47,085
So how do we actually extract
meaningful patterns from messy log data?

55
00:02:47,835 --> 00:02:50,865
Let's look at a few core ML
techniques that make this possible.

56
00:02:51,945 --> 00:02:55,455
First one is clustering helps
us group similar log entries.

57
00:02:55,770 --> 00:02:59,490
It's like letting the algorithm say,
Hey, these events looks related.

58
00:03:00,390 --> 00:03:04,740
Embeddings convert law raw
data into a format that models

59
00:03:04,740 --> 00:03:05,820
can actually understand.

60
00:03:06,329 --> 00:03:10,260
Think of it like turning unstructured
text into numbers that carry meaning.

61
00:03:11,190 --> 00:03:13,860
Third is dimensionality
reduction like PCA.

62
00:03:14,430 --> 00:03:17,730
Help us remove the noise and focus
on the most important features.

63
00:03:18,725 --> 00:03:20,195
The next is sequence model.

64
00:03:20,195 --> 00:03:20,495
Sequence.

65
00:03:20,495 --> 00:03:24,185
Models like L SDMs can
identify patterns across time.

66
00:03:24,665 --> 00:03:28,715
This is the key for catching time-based
anomalies or repetitive behavior.

67
00:03:29,524 --> 00:03:32,915
All of these sequences or
techniques work together to make

68
00:03:32,915 --> 00:03:36,364
patterns, recognition, more stakes,
more scalable and intelligent.

69
00:03:38,225 --> 00:03:41,584
So when it comes to finding
anomalies in logs, there are

70
00:03:41,584 --> 00:03:42,964
a few main schools of thought.

71
00:03:44,070 --> 00:03:45,660
Each with its own strengths.

72
00:03:45,990 --> 00:03:48,120
First, we have statistical models.

73
00:03:48,690 --> 00:03:51,450
These are great for known
patterns and baselines.

74
00:03:52,110 --> 00:03:57,630
Things like zco analysis or moving
averages can quickly flex spikes or drops.

75
00:03:58,320 --> 00:04:03,150
But for more complex, hidden issues, we
need machine learning models, techniques

76
00:04:03,210 --> 00:04:08,430
like isolation, forest auto encoders,
and one class SBMs can detect patterns

77
00:04:08,430 --> 00:04:10,680
that humans or simple rules just miss.

78
00:04:11,535 --> 00:04:16,034
And in many production systems, the most
practical solution is a hybrid approach.

79
00:04:16,575 --> 00:04:19,635
Combining rule-based filters
with ML bag verification.

80
00:04:20,325 --> 00:04:23,175
With feedback loops, the
system gets smarter over time.

81
00:04:24,195 --> 00:04:28,065
The key is to choose the right mix
based on scale, noise level, and

82
00:04:28,065 --> 00:04:29,955
what kind of anomalies you're after.

83
00:04:31,784 --> 00:04:35,955
Here is a high level look at how
everything fits together in a real world.

84
00:04:35,955 --> 00:04:37,545
ML powered log analysis system.

85
00:04:38,489 --> 00:04:39,960
It starts with log collection.

86
00:04:40,320 --> 00:04:44,849
We gather logs from multiple sources,
services, apps, infrastructure layers.

87
00:04:45,150 --> 00:04:48,809
The goal is to cast a wide net
so we don't miss any signal.

88
00:04:49,530 --> 00:04:50,880
Then comes pre-processing.

89
00:04:51,419 --> 00:04:52,710
This is a critical step.

90
00:04:52,770 --> 00:04:57,135
We clean, normalize and structure the
raw logs into a machine readable format.

91
00:04:57,945 --> 00:05:00,215
Think of this as a
translating chaos in a mess.

92
00:05:01,650 --> 00:05:02,520
Sorry, into order.

93
00:05:03,539 --> 00:05:08,159
After that, we feed into the ML
pipeline where models perform pattern

94
00:05:08,159 --> 00:05:10,260
recognition and anomaly detection.

95
00:05:10,770 --> 00:05:14,039
This is where all those technologies
we talked about earlier, kick in.

96
00:05:14,819 --> 00:05:18,929
Finally, the insight engine
takes those signals and turns

97
00:05:18,929 --> 00:05:20,429
it into something useful.

98
00:05:20,969 --> 00:05:24,539
Alerts, probably dashboards,
visualizations, so team can,

99
00:05:24,599 --> 00:05:26,309
teams can act, not just observe.

100
00:05:26,474 --> 00:05:32,385
So this full pipeline ensures that we go
beyond just logging and actually deliver

101
00:05:32,715 --> 00:05:34,484
operational intelligence in real time.

102
00:05:36,494 --> 00:05:39,614
So let's see what all of
this looks like in practice.

103
00:05:40,199 --> 00:05:41,459
A real world example, right?

104
00:05:41,909 --> 00:05:44,789
Incorporating machine learning
into observability and incident

105
00:05:44,789 --> 00:05:48,599
management isn't just theoretical,
it has tangible benefits.

106
00:05:49,259 --> 00:05:52,949
Take the H Estonia's Health and
Welfare Information System Center,

107
00:05:53,129 --> 00:05:57,119
for example, by implementing
Elastic's observability platform.

108
00:05:57,150 --> 00:06:00,239
They streamline their incident
management processes, achieving

109
00:06:00,239 --> 00:06:02,519
a 40% reduction in MTTR.

110
00:06:03,390 --> 00:06:07,530
Similarly, a major e-commerce
platform reported a 40% decrease

111
00:06:07,530 --> 00:06:12,030
in MTTR after deploying an AI
driven root cause analysis tool.

112
00:06:12,960 --> 00:06:17,520
These cases underscored the substantial
impact ML can have on improving system

113
00:06:17,520 --> 00:06:19,469
reliability and reducing downtime.

114
00:06:22,409 --> 00:06:27,719
Even with all the power of m. L, debugging
is never a fully hands off process.

115
00:06:28,199 --> 00:06:30,030
Machine learning can surface patterns.

116
00:06:30,405 --> 00:06:31,815
But it lacks context.

117
00:06:32,115 --> 00:06:36,585
It doesn't know if a spike is normal
for Black Friday or truly unusual.

118
00:06:37,275 --> 00:06:38,295
That's where human come in.

119
00:06:39,224 --> 00:06:42,615
Engineers play a vital role in
validating what the models find.

120
00:06:42,885 --> 00:06:46,575
They help fine tune alerts,
provide a edge case context, and

121
00:06:46,575 --> 00:06:47,895
correct the system when needed.

122
00:06:48,705 --> 00:06:50,865
And over time this
becomes a feedback loop.

123
00:06:51,375 --> 00:06:54,195
The system learns from human
responses and gets smarter.

124
00:06:55,034 --> 00:06:58,365
The more you interact with it and
more, the more useful it becomes.

125
00:06:58,844 --> 00:07:02,534
So this isn't just an AI tool, it's
a collaboration between machine

126
00:07:02,534 --> 00:07:06,344
learning and human insight that
builds trust and reliability.

127
00:07:08,549 --> 00:07:12,509
As much as I believe in the power
of ml, it's important to recognize

128
00:07:12,509 --> 00:07:16,319
its limitations and build and
responsibly build responsibility.

129
00:07:16,319 --> 00:07:19,199
To say first transparency is a challenge.

130
00:07:19,529 --> 00:07:23,819
Many advanced models work like
black boxes, which can hurt trust.

131
00:07:24,419 --> 00:07:27,119
Engineers need to know
why an alert is fired.

132
00:07:27,329 --> 00:07:29,939
If we can't explain it,
then we don't trust it.

133
00:07:30,569 --> 00:07:35,489
Second model maintenance ML
models aren't set and forget.

134
00:07:36,509 --> 00:07:41,789
They drift over time as system behavior
changes without regular retraining, their

135
00:07:41,789 --> 00:07:44,339
accuracy drops and that creates risk.

136
00:07:44,939 --> 00:07:48,659
And the third, ethical concerns,
not all systems get equal.

137
00:07:48,659 --> 00:07:50,729
Attention bias can creep in.

138
00:07:50,789 --> 00:07:54,959
For example, if alerting prioritizes
revenue critical services and

139
00:07:54,959 --> 00:07:58,199
ignores others, we need to
design with fairness in mind.

140
00:07:58,919 --> 00:07:59,609
The takeaway here.

141
00:08:00,284 --> 00:08:01,184
ML isn't magic.

142
00:08:01,304 --> 00:08:01,994
It's a tool.

143
00:08:02,234 --> 00:08:05,834
It works best when paid with human
responsibility and continue scale.

144
00:08:08,624 --> 00:08:11,324
To wrap up, here are
the four key takeaways.

145
00:08:11,324 --> 00:08:14,264
I hope you'll leave with
first, scale and speed.

146
00:08:14,834 --> 00:08:16,215
ML brings scale.

147
00:08:16,244 --> 00:08:17,775
We simply can't achieve normal.

148
00:08:18,704 --> 00:08:23,085
It spots patterns faster across
more data with less noise.

149
00:08:23,984 --> 00:08:28,215
Second root cause detection, pattern
recognition helps us debug smarter.

150
00:08:28,574 --> 00:08:31,125
We get to the why faster,
not just the what.

151
00:08:31,635 --> 00:08:33,405
That's where real time savings happen.

152
00:08:33,944 --> 00:08:35,564
Third, reduce noise.

153
00:08:35,954 --> 00:08:36,464
Smarter.

154
00:08:36,464 --> 00:08:38,895
Anomaly detection means
fewer false rhythms.

155
00:08:39,224 --> 00:08:43,515
It improves focus, less alert,
fat fatigue, more clarity, and

156
00:08:43,515 --> 00:08:45,405
finally, incremental adoption.

157
00:08:46,035 --> 00:08:48,405
You don't have to go all
in one day on day one.

158
00:08:48,915 --> 00:08:49,905
Start small.

159
00:08:49,954 --> 00:08:51,634
Prove value, build trust.

160
00:08:52,145 --> 00:08:55,624
That's how we evolve debugging
one smarter layer at a time.

161
00:08:56,584 --> 00:09:01,175
I truly believe ML is not just a trend
here, it's a path forward to building

162
00:09:01,234 --> 00:09:03,155
calm, more intelligent systems.

163
00:09:05,645 --> 00:09:07,114
So that's it from my side.

164
00:09:07,204 --> 00:09:08,824
Thank you so much for
joining this session.

165
00:09:08,854 --> 00:09:13,775
I really hope this gave you some
insights useful into how ML can help

166
00:09:13,775 --> 00:09:15,905
streamline debugging and reduce noise.

167
00:09:16,470 --> 00:09:20,370
If it did spark any ideas, questions, or
if you're working on similar challenges,

168
00:09:20,670 --> 00:09:23,920
I would love to know what you think about
this and discuss this more on LinkedIn.

169
00:09:24,520 --> 00:09:27,010
If you want to connect to me, please
feel free to connect me on LinkedIn.

170
00:09:27,100 --> 00:09:27,700
Thank you guys.

