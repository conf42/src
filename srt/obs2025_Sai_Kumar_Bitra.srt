1
00:00:00,900 --> 00:00:01,530
Hello everyone.

2
00:00:01,890 --> 00:00:02,250
This is ra.

3
00:00:03,450 --> 00:00:07,170
I'm working as a principal software
engineer at and t. Today I'm going to

4
00:00:07,170 --> 00:00:10,709
talk about scaling observability for
real-time personalization engines.

5
00:00:11,700 --> 00:00:13,410
So let's get into the talk.

6
00:00:14,489 --> 00:00:15,119
Let me share.

7
00:00:16,119 --> 00:00:20,469
So first let's talk about why
observability matters for personalization.

8
00:00:20,469 --> 00:00:21,880
So that's the first question, right?

9
00:00:22,270 --> 00:00:25,299
So personalization drives
engagements and they get revenue.

10
00:00:25,330 --> 00:00:28,689
For example, companies that accelerate
personalization, they see to, they

11
00:00:28,689 --> 00:00:30,790
see up to 40% of more revenue costs.

12
00:00:31,239 --> 00:00:35,909
And as, as basically users respond
for highly relevant contents, right?

13
00:00:36,474 --> 00:00:40,894
So they got to be good at observability
to reach that personalization revenue

14
00:00:40,894 --> 00:00:46,824
growth and for the real time experiences,
the, there, there is zero room for errors.

15
00:00:46,904 --> 00:00:50,324
Users always expect the response
to be minimally seconds, and there

16
00:00:50,324 --> 00:00:53,964
shouldn't be any deviation of the
content that present to the customer.

17
00:00:54,054 --> 00:00:58,044
So the so the delays or
failures that directly hurt the

18
00:00:58,044 --> 00:00:59,604
satisfaction and conversions.

19
00:01:00,309 --> 00:01:05,279
Apart from that, there is like complex ML
decisions that need validations, right?

20
00:01:05,649 --> 00:01:09,789
So whenever we present the
recommendations to the customers,

21
00:01:09,789 --> 00:01:13,329
we gotta see are we presenting the
right content at the right time?

22
00:01:13,379 --> 00:01:17,129
Are we having any issues while
presenting the content to the customers?

23
00:01:17,619 --> 00:01:18,820
Is lack of insight.

24
00:01:19,554 --> 00:01:23,764
Is provisionally equal to lost
users trust and missed opportunity.

25
00:01:23,774 --> 00:01:27,934
We got to be careful on while in the
recommendation engine there it's basically

26
00:01:27,934 --> 00:01:30,344
for personalization engine products.

27
00:01:31,945 --> 00:01:35,504
Apart from that, there is competitive
edge when we are collecting these

28
00:01:35,504 --> 00:01:39,824
metrics, logs, traces, that helps
continue to improve algorithms and

29
00:01:39,824 --> 00:01:41,834
user experiences altogether, right?

30
00:01:42,164 --> 00:01:46,404
So we gotta keeping our personalization
engine a step ahead in the competition

31
00:01:46,454 --> 00:01:50,615
from other other competitors,
whoever is in the same phase as

32
00:01:50,615 --> 00:01:52,415
personalized engine product level.

33
00:01:53,194 --> 00:01:57,709
So these so observability overall
is like very important and it's,

34
00:01:57,709 --> 00:01:59,959
plays a key role for personalization.

35
00:02:00,449 --> 00:02:04,689
So next we are going to talk about
at the same time there are challenges

36
00:02:04,719 --> 00:02:09,399
to, to get to this observability for
real time machine learning systems.

37
00:02:09,769 --> 00:02:12,280
As I said observability is
important for personalization.

38
00:02:12,280 --> 00:02:15,650
At the same time, there are a couple
challenges it has so basically like

39
00:02:15,700 --> 00:02:20,170
the personalization systems, how
these complex distributed pipelines.

40
00:02:20,790 --> 00:02:25,190
Where there are many components will get
involved, like services, data pipelines,

41
00:02:25,250 --> 00:02:28,010
ML models, and all those components.

42
00:02:28,040 --> 00:02:32,790
So we got to be good track of these,
all the logs traces and metrics.

43
00:02:32,840 --> 00:02:33,950
Otherwise we get lost.

44
00:02:33,950 --> 00:02:35,840
So end-to-end tracking
should be important.

45
00:02:36,420 --> 00:02:40,980
So the second complexity we could see
is like there would be high data volume,

46
00:02:41,040 --> 00:02:44,700
like millions of events could come for
the real time events when the users are

47
00:02:44,700 --> 00:02:45,900
interacting with the website, right?

48
00:02:46,240 --> 00:02:50,580
So these real time without adding latency
we got to capture these telemetry logs.

49
00:02:50,580 --> 00:02:51,750
Which is difficult part.

50
00:02:52,110 --> 00:02:53,490
So that's one more challenge.

51
00:02:53,550 --> 00:02:56,790
And these ML decisions is
like a black box thing.

52
00:02:57,145 --> 00:03:02,005
We don't know when there is a drop of
drop of growth to the recommendations we,

53
00:03:02,005 --> 00:03:03,625
what we are suggesting to the customer.

54
00:03:03,985 --> 00:03:07,525
So it's hard to track like whether
the recommendation is driving the

55
00:03:07,525 --> 00:03:11,705
recommendations properly or our rules
have been executing properly or we

56
00:03:11,705 --> 00:03:16,635
lost anything or the source systems or
capturing the data, what we wanted for

57
00:03:16,635 --> 00:03:19,844
the specific user behavior and et cetera.

58
00:03:20,124 --> 00:03:24,045
So this stacking of these ML
models fail loose is also hard.

59
00:03:25,230 --> 00:03:27,730
And these personalization.

60
00:03:27,760 --> 00:03:30,549
Now architecture is like
multi-channel user journey.

61
00:03:30,549 --> 00:03:33,899
Sometimes like user can come
to the website at the same time

62
00:03:33,899 --> 00:03:37,880
he can go to the store or he
can browse through mobile apps.

63
00:03:37,910 --> 00:03:40,760
So all sorts of these
channels will be there.

64
00:03:40,810 --> 00:03:45,320
We got to be track the customer throughout
these multiple channels so that we can

65
00:03:45,320 --> 00:03:49,640
stitch the user properly and we can
troubleshoot if anything goes wrong.

66
00:03:49,980 --> 00:03:53,840
So Observ, everybody place a challenge
here for multichannel user journey.

67
00:03:54,670 --> 00:03:57,820
And the fifth challenge is going
to be the latency sensitivity.

68
00:03:57,820 --> 00:04:01,750
So we all saying like an observability
and tag these metrics and things, but

69
00:04:01,750 --> 00:04:03,849
at the same time it's not that easy.

70
00:04:03,880 --> 00:04:08,464
Without without having I. Heavy
overhead on these telemetric

71
00:04:08,464 --> 00:04:09,575
collections and things.

72
00:04:09,825 --> 00:04:12,435
So we know, we all know that
the, it is important to collect

73
00:04:12,435 --> 00:04:13,719
the data but at what cost?

74
00:04:13,719 --> 00:04:17,160
Like we got to see how we maintain
lightweight real time engines.

75
00:04:17,570 --> 00:04:18,590
So that's one thing.

76
00:04:18,590 --> 00:04:20,780
The latency and sensitivity
is going to be challenged.

77
00:04:21,250 --> 00:04:21,610
Yeah.

78
00:04:21,745 --> 00:04:26,575
Now and next we are going to talk
about a. System architecture overview.

79
00:04:26,605 --> 00:04:29,555
So this is a typical
representation architecture

80
00:04:29,555 --> 00:04:30,935
for real time personalization.

81
00:04:30,965 --> 00:04:35,725
Engines like where the request
comes into APIs the three APIs, and

82
00:04:35,725 --> 00:04:38,974
it goes to decision engine where
that recommendation takes place.

83
00:04:39,315 --> 00:04:43,375
From there, it get interacts with
the model service where models been

84
00:04:43,375 --> 00:04:44,695
retain and rules been executed.

85
00:04:45,065 --> 00:04:49,754
And these model service interact with
the data report where the user profiles

86
00:04:49,754 --> 00:04:52,814
or what or whatnot are the content
recommendations will be placed in.

87
00:04:53,265 --> 00:04:58,365
So meanwhile there is an event ingestion
will happen where the users user interface

88
00:04:58,365 --> 00:05:02,025
event streams to these event ingestions
will happen, for example, through Kafka.

89
00:05:02,445 --> 00:05:05,275
These event tions will be
happening into the model.

90
00:05:05,440 --> 00:05:11,080
So all these components multiple component
attractions the metrics, logs, traces

91
00:05:11,380 --> 00:05:14,230
all together to be feeding or ingested.

92
00:05:14,995 --> 00:05:17,965
Into the centralized
observability pipeline.

93
00:05:18,325 --> 00:05:24,415
So this is a typical system architecture
for these personalization engines in

94
00:05:24,565 --> 00:05:30,205
thel services like ingestion, pipelines,
station engine, API layer data, and

95
00:05:30,205 --> 00:05:35,565
then all together all these telemetry
sources to be it in to aggregate the

96
00:05:35,565 --> 00:05:39,930
data and ingest them into observability
pipeline, which is centralized.

97
00:05:41,250 --> 00:05:44,619
So this is how the observability
comes into the picture for

98
00:05:44,650 --> 00:05:46,030
for personalization engines.

99
00:05:46,509 --> 00:05:50,289
And now we go to the scaling observability
in a real time architecture, right?

100
00:05:50,619 --> 00:05:54,635
So I. As we speak, like this is
really high scale architectures

101
00:05:54,635 --> 00:05:58,045
in a real time real time event,
conceptions and personalization.

102
00:05:58,375 --> 00:06:01,525
So end to end tracing is very important.

103
00:06:01,525 --> 00:06:04,474
User coming to and the a p is getting hit.

104
00:06:04,504 --> 00:06:08,825
And then it interact with model,
interface and trying to interact

105
00:06:08,825 --> 00:06:13,124
with data data we post to face
the data and execute the rules.

106
00:06:13,349 --> 00:06:19,039
And enabling end-to-end visibility of
each personalization response is critical.

107
00:06:19,939 --> 00:06:22,639
And the second thing is
unified instrumentation.

108
00:06:22,669 --> 00:06:28,609
Like we got to adopt open telemetry as
DK across services for standard metrics.

109
00:06:28,909 --> 00:06:32,989
Logs and trace collection is important
unified instrumentation so a PS

110
00:06:32,989 --> 00:06:37,789
instrument code once and export
telemetry to multiple back backend.

111
00:06:37,839 --> 00:06:40,329
So we got to be like,
unified instrumentation

112
00:06:40,329 --> 00:06:41,299
instead of bits and pieces.

113
00:06:42,404 --> 00:06:45,224
And the second, third thing
is high throughput metrics.

114
00:06:45,334 --> 00:06:48,914
There would be as we speak, like
millions of events could could

115
00:06:48,914 --> 00:06:50,584
happen in production environments.

116
00:06:50,664 --> 00:06:53,879
When the application been
accessed by external users, right?

117
00:06:54,299 --> 00:06:58,059
So we already have a time series
database like Prometheus and and all

118
00:06:58,059 --> 00:06:59,829
sorts of metrics to be collected.

119
00:07:00,909 --> 00:07:04,479
Aggregate and avoid overwhelming
storage, for example, like compute

120
00:07:04,929 --> 00:07:08,829
percentile, latencies and sample
fine-grained events as such thing.

121
00:07:08,829 --> 00:07:12,759
So high throughput metrics are very
important to be collected and log

122
00:07:12,759 --> 00:07:17,499
aggregation, like streaming logs Jason
structure logs and the model decision

123
00:07:17,499 --> 00:07:18,879
into a central system like elk.

124
00:07:19,359 --> 00:07:22,809
So whenever we got issues double
two, troubleshooting through

125
00:07:22,809 --> 00:07:25,809
these kind of a and cloud logging
things would be easy for us.

126
00:07:26,209 --> 00:07:30,939
Like we can track them as like user
sessions and through trace IDs and things.

127
00:07:31,649 --> 00:07:36,119
And the last thing is like scalable
storage and retention, like employee, a

128
00:07:36,119 --> 00:07:41,159
backend that can handle high data volumes
like scalable observability platform

129
00:07:41,219 --> 00:07:43,739
or data lake or long term analysis.

130
00:07:44,039 --> 00:07:47,729
So we need to ensure retention
policy sometimes when we want

131
00:07:47,729 --> 00:07:51,390
to go back to the time and then
review what was the trend and.

132
00:07:51,429 --> 00:07:52,929
What was that issue is about?

133
00:07:53,339 --> 00:07:57,119
So historical trends can be analyzed
without infin storage growth.

134
00:07:57,119 --> 00:08:00,649
So we gotta be careful about
scalability of storage as well.

135
00:08:00,719 --> 00:08:03,969
Part of these real time architecture
when we are scaling for observability.

136
00:08:04,965 --> 00:08:08,440
And the and the other thing is like
key metrics that matter, right?

137
00:08:08,780 --> 00:08:12,809
So there will be these key metrics which
are important for the troubleshoot.

138
00:08:12,809 --> 00:08:14,069
We got to be collected.

139
00:08:14,100 --> 00:08:18,270
One is like latency and throughput,
like measures how fast and how

140
00:08:18,330 --> 00:08:20,010
many recommendations we are saw.

141
00:08:20,069 --> 00:08:27,390
And, at now, how much SLA you would
personalize the APIs been responding

142
00:08:27,419 --> 00:08:32,945
and how APIs been interacting with the
model interface or for the data reports?

143
00:08:33,025 --> 00:08:37,079
LA throughput one thing to be a key
metric for us for observability.

144
00:08:37,794 --> 00:08:40,104
And the second thing is
error rates and timeout.

145
00:08:40,104 --> 00:08:42,894
So we always got to watch out
for five accessories or four

146
00:08:42,894 --> 00:08:45,514
accessories and and are we.

147
00:08:45,529 --> 00:08:50,209
Returning any default content or default
recommendations to the customers and

148
00:08:50,209 --> 00:08:54,529
all that data help us, like when we see
when our conversion rates are go down

149
00:08:54,869 --> 00:08:58,979
and any, all of these spikes indicate
issues in the ML service or data layer.

150
00:08:58,979 --> 00:09:03,030
So error rates and timeouts
is one thing as a key metric.

151
00:09:03,079 --> 00:09:03,919
We got to be log.

152
00:09:04,519 --> 00:09:08,049
And the other thing is engagement
metrics, tie observability to business

153
00:09:08,049 --> 00:09:13,019
KPIs, like we got to be engaged, click
through rates and conversion rates

154
00:09:13,049 --> 00:09:14,969
dwell time, et cetera, in real time.

155
00:09:14,999 --> 00:09:18,779
And so a drop in engagement might
signal always personalization isn't

156
00:09:18,779 --> 00:09:20,469
happening fine to the customer.

157
00:09:20,469 --> 00:09:23,229
So there is a drop in the
conversion rate and system.

158
00:09:23,829 --> 00:09:26,949
So that's engagement metrics
is one of the important thing.

159
00:09:26,949 --> 00:09:29,409
And another thing is model
performance indicator.

160
00:09:29,919 --> 00:09:32,709
So custom metrics like ML model itself.

161
00:09:32,739 --> 00:09:35,709
Example, the confidence score
distribution of recommendations.

162
00:09:36,309 --> 00:09:41,109
Or frequency of each model, variant
being used on drift metrics, comparing

163
00:09:41,109 --> 00:09:43,419
live input data to training data.

164
00:09:43,479 --> 00:09:46,499
These help detect when the
model's quantity is degrading.

165
00:09:46,569 --> 00:09:50,079
Model performance indicator is
also a good thing a good key

166
00:09:50,079 --> 00:09:52,099
metric for us for the observation.

167
00:09:52,614 --> 00:09:54,594
And another is like resource utilization.

168
00:09:54,654 --> 00:09:57,774
So always we got to keep an eye
in our infrastructure level.

169
00:09:57,774 --> 00:09:59,094
What is the CPU consumption?

170
00:09:59,094 --> 00:10:01,594
What is the memory consumption
and what is the throughput to our

171
00:10:01,599 --> 00:10:05,769
steam like example Kafka topics
and what are the key lens in it?

172
00:10:06,049 --> 00:10:07,219
So all sorts of things.

173
00:10:07,219 --> 00:10:10,939
We got to be, keep an eye on it to
ensure infrastructure can always

174
00:10:10,939 --> 00:10:14,539
hand load Certain changes could
explain like performance outlets.

175
00:10:14,589 --> 00:10:17,409
So these are the key metrics
that the, that, that matter.

176
00:10:18,279 --> 00:10:18,729
For us.

177
00:10:18,729 --> 00:10:21,989
And let's jump into tracing
the personalization journey.

178
00:10:22,229 --> 00:10:26,529
So as part of this personalized
personalized engines architecture

179
00:10:26,559 --> 00:10:29,559
the journey how easy do we can
trace that is the thing, right?

180
00:10:29,859 --> 00:10:35,229
So we got to distribute trace example like
when, when a user visits personalization

181
00:10:35,229 --> 00:10:40,089
content and a trace follows through
the request, so the request through the

182
00:10:40,089 --> 00:10:44,409
API to the decision engine and into the
model, so we sent to the database, right?

183
00:10:44,709 --> 00:10:46,419
So we got to be each segment level.

184
00:10:46,419 --> 00:10:47,684
We got to have a trace record.

185
00:10:47,919 --> 00:10:51,579
So we will know like how the
interaction is happening among

186
00:10:51,579 --> 00:10:53,379
these components with the handshake.

187
00:10:53,809 --> 00:10:56,019
Apart from that w in
telemetry for tracing.

188
00:10:56,019 --> 00:11:00,359
We can, you, we got have these w in
telemetry logs, traces and metrics

189
00:11:00,359 --> 00:11:05,309
together where we have to have these
tracing in recommendation engine level.

190
00:11:05,489 --> 00:11:09,329
And after that we got to have in a model
interface and you enter at DB level.

191
00:11:09,379 --> 00:11:12,489
We will, we got, we chunked
it into different levels to

192
00:11:12,489 --> 00:11:17,049
track what is happening and in
which layer it is happening to.

193
00:11:17,229 --> 00:11:22,089
It'll be easy later to visualize the
call flow and bottleneck identification.

194
00:11:22,089 --> 00:11:26,499
So traces always help to pinpoint
which layer is taking time.

195
00:11:26,499 --> 00:11:30,219
For example, where API is taking
this much milliseconds and your.

196
00:11:30,224 --> 00:11:33,254
DB interaction is taking this much
level and your model interfaces

197
00:11:33,614 --> 00:11:35,204
interaction is taking this much level.

198
00:11:35,204 --> 00:11:39,404
So it gives a clear, in depth
view of those metrics alone.

199
00:11:39,694 --> 00:11:41,854
It enables targeted optimization for us.

200
00:11:42,964 --> 00:11:46,144
And other thing is like
correlating with user action.

201
00:11:46,154 --> 00:11:49,864
When we are tracing these user
requests through session IDs or trace

202
00:11:49,864 --> 00:11:54,459
IDs, so it always like correlate,
we can correlate the break comes of

203
00:11:54,459 --> 00:11:58,209
that request like this, a low score
correlating a pool experience, for

204
00:11:58,209 --> 00:12:00,339
example, user size, scale recommendation.

205
00:12:00,609 --> 00:12:04,329
We can easily identify throughout
that trace and we can provide

206
00:12:04,329 --> 00:12:05,949
like visual context for debugging.

207
00:12:06,314 --> 00:12:09,254
So correlating with user actions
is one of the good thing for

208
00:12:09,254 --> 00:12:11,804
tracing and sampling strategy.

209
00:12:12,074 --> 00:12:15,224
So for example, trace every
request may be infeasible, right?

210
00:12:15,224 --> 00:12:20,054
So we got to go by batches wise
categorize wise, collect traces

211
00:12:20,054 --> 00:12:23,354
for a normal scenarios and
represent to slice of traffic.

212
00:12:23,834 --> 00:12:28,034
So this keeps like overhead low
while enduring us to collect traces.

213
00:12:28,394 --> 00:12:32,324
So sampling strategy is like one,
one good tracing for personal

214
00:12:32,714 --> 00:12:33,944
personalization journeys.

215
00:12:35,099 --> 00:12:37,649
And the other is like logging for making.

216
00:12:38,234 --> 00:12:41,694
So whenever we have logging comes
into picture for observation, we

217
00:12:41,694 --> 00:12:43,374
gotta have a rich and structured logs.

218
00:12:43,424 --> 00:12:47,204
Logging in a personalization engine
should capture key imports, outputs

219
00:12:47,204 --> 00:12:49,604
of the ML engine for each request.

220
00:12:49,634 --> 00:12:54,554
So log details like user ID or relevant
features like which brackets segments

221
00:12:54,554 --> 00:12:58,364
customer fall into, and maybe an
explanation score or a reason could.

222
00:12:58,709 --> 00:12:59,430
Could be possible.

223
00:12:59,430 --> 00:13:03,390
If you're available these kind of rich
and structure logs help us later to make

224
00:13:03,390 --> 00:13:05,130
our life easy when we troubleshoot issues.

225
00:13:05,830 --> 00:13:09,580
Apart from that, like we can enable
root cause analysis, like when something

226
00:13:09,580 --> 00:13:13,365
goes wrong we, we always show like
relevant content to the customer, right?

227
00:13:13,365 --> 00:13:16,785
So logs can reveal what
the customer model saw.

228
00:13:16,900 --> 00:13:20,895
And for instance log might show
input or user segment is now like.

229
00:13:21,095 --> 00:13:24,695
That means like it didn't give the
response as anticipated, give bad

230
00:13:24,695 --> 00:13:26,105
default recommendation to the customer.

231
00:13:26,155 --> 00:13:30,810
So root cause analysis, it enables us when
we follow through these logging properly.

232
00:13:31,885 --> 00:13:35,964
And we have a trace log correlation,
integrate logs with tracing context.

233
00:13:35,964 --> 00:13:40,540
As earlier we talk whenever the
movement request comes in, everything,

234
00:13:40,780 --> 00:13:45,250
the trace completely goes with the
trace ID or session ID or request id.

235
00:13:45,620 --> 00:13:49,390
Through that we can easily track at
these distributes distributed traces

236
00:13:49,390 --> 00:13:54,530
so you can jump into associated logs
across services for the same request.

237
00:13:54,590 --> 00:13:57,900
So it would be easy to raise,
log with these correlation IDs.

238
00:13:58,995 --> 00:14:01,875
Other is anonymize the protect,
anonymize and protect data.

239
00:14:01,905 --> 00:14:05,615
So logging always we feel just
simply log, but always we got to

240
00:14:05,615 --> 00:14:07,175
take care of any sensitive data.

241
00:14:07,475 --> 00:14:08,075
Are we logging?

242
00:14:08,175 --> 00:14:12,145
And because of GDPR concerns and other
things and observability platform

243
00:14:12,145 --> 00:14:13,795
should enforce data handling policy.

244
00:14:13,795 --> 00:14:15,655
So while still giving the engineers.

245
00:14:16,135 --> 00:14:17,785
Enough info to troubleshoot.

246
00:14:17,785 --> 00:14:20,635
So we got to be careful while logging.

247
00:14:20,685 --> 00:14:23,295
As we have a freedom to
have enough to troubleshoot.

248
00:14:23,575 --> 00:14:25,875
At the same time, we gotta be
cautious about sensitive data

249
00:14:25,875 --> 00:14:27,515
of the customers to be logged.

250
00:14:28,580 --> 00:14:30,410
Another is like use log levels wisely.

251
00:14:30,770 --> 00:14:35,440
So whether you go with info log or error
log or warning log and and we don't

252
00:14:35,440 --> 00:14:38,080
need to, we don't need to like, puff up.

253
00:14:38,150 --> 00:14:41,120
The logs like always make sure
only warning level at what

254
00:14:41,120 --> 00:14:42,170
level we have to give and error.

255
00:14:43,005 --> 00:14:46,315
Error logging type which log
patterns we need to be log.

256
00:14:46,375 --> 00:14:49,375
So it could be, it would be
easy when we segregate, when

257
00:14:49,375 --> 00:14:50,515
we troubleshoot in the logs.

258
00:14:50,565 --> 00:14:54,345
Always the go faster and then
warning and then info for help.

259
00:14:56,950 --> 00:15:02,230
And the other thing is like scaling
observability patterns and best practices.

260
00:15:02,740 --> 00:15:06,680
So avoid metrics overload always
practices like avoid metrics overload.

261
00:15:06,980 --> 00:15:07,850
Be mindful.

262
00:15:07,865 --> 00:15:11,015
Of metric cardinality, for example.

263
00:15:11,405 --> 00:15:14,555
This metric labeled with
every user or item id, right?

264
00:15:14,555 --> 00:15:19,025
So it would be a huge for each
customer if we log for each user.

265
00:15:19,025 --> 00:15:23,385
So we got to be aggregate them
as a category wise or percentile,

266
00:15:23,385 --> 00:15:25,215
latency wise or something like that.

267
00:15:25,315 --> 00:15:29,485
So to be smart so we can we
can avoid the overload in such

268
00:15:29,485 --> 00:15:30,805
a way if we go with the login.

269
00:15:31,375 --> 00:15:32,965
And raise and log sampling.

270
00:15:32,965 --> 00:15:36,755
Sampling we can go with the sampling
of couple requests and then we see the

271
00:15:36,755 --> 00:15:41,555
pattern and we can group them as so we
can go with reduce noise and cost for it.

272
00:15:41,555 --> 00:15:43,475
So while retaining diagnostic power.

273
00:15:43,865 --> 00:15:44,405
Exactly.

274
00:15:44,405 --> 00:15:47,260
So trace and log sampling as
a pattern we got to follow.

275
00:15:47,895 --> 00:15:49,875
And batching and B, buffering.

276
00:15:49,905 --> 00:15:53,115
So use characters like ING
telemetry characters, so Kafka

277
00:15:53,115 --> 00:15:57,405
and buffer telemetry data, batch
taste, exports, and sending chunks.

278
00:15:57,465 --> 00:16:02,445
So the application is in block or
sending each span, so buffer logs to

279
00:16:02,445 --> 00:16:04,595
avoid disk Iwo becoming bottleneck.

280
00:16:05,730 --> 00:16:06,480
So that's one thing.

281
00:16:06,480 --> 00:16:10,080
And scalable storage and
querying, implement a tiered

282
00:16:10,080 --> 00:16:11,550
storage observability data.

283
00:16:11,550 --> 00:16:13,200
So recent data stays fast.

284
00:16:13,530 --> 00:16:17,350
Queryable stores and everything
would be like, fast enough if we

285
00:16:17,350 --> 00:16:19,000
have a storage and querying properly.

286
00:16:19,510 --> 00:16:22,480
And other thing is like a
resiliency of observability.

287
00:16:23,620 --> 00:16:27,735
Systems like T, your monitoring
pipeline, whatever that pipeline is

288
00:16:27,735 --> 00:16:29,715
about and critical part of the system.

289
00:16:29,715 --> 00:16:34,005
Scale out your metrics and logging
backends, always set up alerts.

290
00:16:34,005 --> 00:16:34,965
Don't forget about it.

291
00:16:34,965 --> 00:16:36,855
And observability systems themselves.

292
00:16:36,855 --> 00:16:41,985
Example, if the log forward lags or
from the is behind and observability

293
00:16:41,985 --> 00:16:45,455
outage can be especially painful
during a production incident so we

294
00:16:45,455 --> 00:16:47,345
got to be mindful of that as well.

295
00:16:50,080 --> 00:16:54,250
And the other thing is like
incident observability in action.

296
00:16:55,910 --> 00:16:56,930
The scenario.

297
00:16:57,230 --> 00:16:59,240
Is like silent model drifting.

298
00:16:59,390 --> 00:17:04,360
It is when it tells about it a story of
which is an incident of observability

299
00:17:04,630 --> 00:17:09,440
interaction imagine an engagement
of an engagement gradually drop over

300
00:17:09,440 --> 00:17:11,030
a week for personalization field.

301
00:17:11,030 --> 00:17:11,630
When we are.

302
00:17:11,915 --> 00:17:15,685
Showing a component to the customer
and it's its engagement got dropped.

303
00:17:16,025 --> 00:17:18,695
Then component crashed but
component didn't crash.

304
00:17:18,695 --> 00:17:21,565
And B less visibility of those
components by the customers.

305
00:17:21,565 --> 00:17:23,605
But the recommendations
became lesser relevant.

306
00:17:23,915 --> 00:17:27,525
So without observability, this
scenario is hard to tag down what was

307
00:17:27,525 --> 00:17:28,875
happening to that components, right?

308
00:17:29,295 --> 00:17:32,085
So detection through metrics and alerts.

309
00:17:32,115 --> 00:17:35,415
So without strong observability,
setup, an alert tried.

310
00:17:35,565 --> 00:17:39,125
When click through rate or it
got deeper, 10% below baseline.

311
00:17:39,125 --> 00:17:42,905
At the same time, the custom model
drift metrics showed the distribution.

312
00:17:43,815 --> 00:17:47,685
So input have, has shifted from
training data, but this correlation

313
00:17:47,685 --> 00:17:51,345
hinted the model was no longer
turned to current user behavior.

314
00:17:51,845 --> 00:17:55,205
So through that detection also it
was, it become hard to identify

315
00:17:55,205 --> 00:17:56,285
what was happening, right?

316
00:17:56,335 --> 00:17:58,885
And the next is like using
tracers to pinpoint impact.

317
00:17:58,885 --> 00:18:02,575
So engineers pulled up to distribute
at tracers for user session and they

318
00:18:02,575 --> 00:18:06,950
analyze the things and the tracers
show normal latency and everything.

319
00:18:07,940 --> 00:18:11,090
A pattern image, like many users
received a default content.

320
00:18:11,090 --> 00:18:12,760
So that's where the table light in.

321
00:18:13,180 --> 00:18:17,840
So it indicated that model was unsure
and it fall back to the default content,

322
00:18:17,840 --> 00:18:19,370
which is recommended to the customers.

323
00:18:19,710 --> 00:18:20,790
So root cause, logs.

324
00:18:20,790 --> 00:18:24,480
So the other thing is we could see
the logs as earlier we discussed.

325
00:18:24,570 --> 00:18:28,180
What is a good good things or
metrics, key metrics to log.

326
00:18:28,540 --> 00:18:31,200
So we, we used to log for
a model as well whether.

327
00:18:31,740 --> 00:18:36,400
We recommended a personalized
recommendation or a default content

328
00:18:36,490 --> 00:18:37,540
recommended to the customer.

329
00:18:37,540 --> 00:18:40,120
We could see in the logs like,
always the default content being

330
00:18:40,120 --> 00:18:44,000
recommended because customer is not
falling into a specific segment.

331
00:18:44,495 --> 00:18:48,265
So that's where the dev identified,
like they see like upstream

332
00:18:48,605 --> 00:18:52,275
whether etel is collecting the
customer's segment properly.

333
00:18:52,275 --> 00:18:55,565
Example, customer segment
is if age is greater than 30

334
00:18:55,565 --> 00:18:56,825
and recommend this content.

335
00:18:57,075 --> 00:19:00,855
So the age attribute profile
attribute is missing to be collection.

336
00:19:00,855 --> 00:19:03,705
So that's where that would cause
light and they identify through that.

337
00:19:04,185 --> 00:19:07,005
Logs played a key role there
and the resolution and takeaway.

338
00:19:07,005 --> 00:19:08,085
So thanks for observability.

339
00:19:08,535 --> 00:19:13,915
The team being identified the issue of
root cause yours, and they fix the ETL

340
00:19:13,915 --> 00:19:17,815
things and postmodern showed prior to
their observability tools, like after,

341
00:19:17,875 --> 00:19:22,465
after the observability they three x
times they debugging time got sailed.

342
00:19:22,665 --> 00:19:25,875
And all those metrics
were, are looking awesome.

343
00:19:26,025 --> 00:19:29,565
So this is Abu story of
observability as a sample way.

344
00:19:30,105 --> 00:19:34,025
And the other is like tools and
stack in action like, open Telemetry.

345
00:19:34,135 --> 00:19:37,795
So there are multiple tools like
part of these personalization

346
00:19:37,795 --> 00:19:38,845
engines we gotta use.

347
00:19:39,185 --> 00:19:41,675
There are a stack of tools which
we are going to talk right now.

348
00:19:41,915 --> 00:19:44,075
So open telemetry is one of
the thing that is a open source

349
00:19:44,075 --> 00:19:45,755
standard for instrumentation.

350
00:19:45,995 --> 00:19:49,325
So it use, its collects metrics,
traces, logs in a unified way.

351
00:19:49,555 --> 00:19:54,110
So we, it has it come up, it comes up
with SDK so we can we can integrate

352
00:19:54,110 --> 00:19:57,800
this into personalized microservices
and allowing easy export of telemetry.

353
00:19:58,310 --> 00:20:00,470
And other is like Prometheus and Grafana.

354
00:20:00,500 --> 00:20:04,270
So all the metrics can be blend in
and and they even that, that heavy

355
00:20:04,270 --> 00:20:09,390
load at a scale it can easily and
Grafana provides like dashboards,

356
00:20:09,390 --> 00:20:13,300
visualization, and from all the
requests high rates of requests as well.

357
00:20:14,715 --> 00:20:16,545
And we have distributed rein system.

358
00:20:16,920 --> 00:20:22,000
So Jaguar kin is deployed for tracing,
so receiving spans from open telemetry

359
00:20:22,000 --> 00:20:25,360
engineers, so distributed tracing
system jargon, kin are upgrade.

360
00:20:25,520 --> 00:20:30,520
So we can use those tools and logging
pipeline example, elk elastic log session

361
00:20:30,520 --> 00:20:35,000
and Kibana where we use like session IDs
or user IDs that can be easily tracked and

362
00:20:35,330 --> 00:20:37,670
traced down even with multiple channels.

363
00:20:37,670 --> 00:20:38,630
That request comes through.

364
00:20:38,630 --> 00:20:40,880
We can stitch that customer
request through and we can

365
00:20:40,880 --> 00:20:41,990
easily troubleshoot those.

366
00:20:42,865 --> 00:20:44,815
And other thing is like advanced.

367
00:20:44,815 --> 00:20:48,275
There are a PM and advanced tools
like in addition to these open source

368
00:20:48,275 --> 00:20:53,725
tools, there are Dynatrace Datadog
the, these a PM solutions provide very

369
00:20:53,725 --> 00:20:58,045
good very good anomaly detection on
metrics and a assisted boost cause

370
00:20:58,045 --> 00:21:02,695
analysis and the tools can completely
the DIY stack by catching sub subtle.

371
00:21:03,590 --> 00:21:03,950
Issues.

372
00:21:03,950 --> 00:21:05,890
So these are the tool stack.

373
00:21:06,320 --> 00:21:10,095
We have through, and there is
this this is the same thing,

374
00:21:10,095 --> 00:21:14,375
like what are the tools enabling
observability scale, instrumentational

375
00:21:14,375 --> 00:21:15,365
and telemetry collection.

376
00:21:15,365 --> 00:21:19,325
We can use the open telemetry and
for metrics collection dashboards,

377
00:21:19,355 --> 00:21:21,570
we can use Prometheus Grafana.

378
00:21:22,100 --> 00:21:25,820
And for distributed tracing,
we can use Jaguar Grafana tempo

379
00:21:25,820 --> 00:21:27,260
and log aggregation and search.

380
00:21:27,260 --> 00:21:33,450
We use Elastic Elk Stack and for
advanced monitoring and a PM as

381
00:21:33,450 --> 00:21:38,440
we discussing time now, trace and
Log or New Relic they're too good.

382
00:21:38,710 --> 00:21:41,230
And for trays, log storage at Scale.

383
00:21:41,260 --> 00:21:43,870
Kafka Open Telemetry
Collector, or Click House.

384
00:21:44,410 --> 00:21:47,980
At, on data Lake those are, those,
these are the good tools that could

385
00:21:47,980 --> 00:21:50,920
be helpful for personalization
engine built in products.

386
00:21:51,010 --> 00:21:52,480
So these tools really
helpful for observability.

387
00:21:55,180 --> 00:21:59,320
Another is like platform impact of
scaled observability, so if we use this

388
00:21:59,860 --> 00:22:03,220
observability in a better way, so the
impacts are going to be like, dramatic

389
00:22:03,220 --> 00:22:07,750
reduction in outages, so there won't be
much outages like, now with the robust

390
00:22:07,930 --> 00:22:12,340
observability issues are caught and
resolved faster, and one case study

391
00:22:12,340 --> 00:22:16,930
saw that no resolution of these issues
could be like, improved by 75% after

392
00:22:16,930 --> 00:22:18,940
adopting modern observability practices.

393
00:22:19,380 --> 00:22:22,590
So it's really helps for
reduction in outages.

394
00:22:22,640 --> 00:22:25,355
And another is like improved
performance and reliability.

395
00:22:25,520 --> 00:22:29,660
The team can identify bottlenecks
and they can clearly see through

396
00:22:29,720 --> 00:22:33,870
where exactly we fall back in the
performance stage and they can improve

397
00:22:33,870 --> 00:22:38,010
those company's performance so they
can easily solve the customer issues.

398
00:22:38,040 --> 00:22:41,790
And that, that is important thing in
the real time real time environment.

399
00:22:41,920 --> 00:22:43,360
To deliver the quick thing with no issues.

400
00:22:44,785 --> 00:22:46,585
There is like higher
competence and deployment.

401
00:22:46,585 --> 00:22:49,825
So observability data gave
developers a good boost.

402
00:22:49,855 --> 00:22:53,885
Like it's not even internal developers and
for the external customers as well, like

403
00:22:53,975 --> 00:22:58,695
push updates faster, they know whenever a
new model to be introduced and new future

404
00:22:58,695 --> 00:23:03,095
come in they immediately can incorporate
and they could see how it's behaving.

405
00:23:03,305 --> 00:23:06,635
So this improved the velocity of
experimentation, personalization.

406
00:23:08,120 --> 00:23:10,640
And other is like a better
alignment with the business goals.

407
00:23:10,700 --> 00:23:14,250
So because telemetry was tied
into engagement metrics, right?

408
00:23:14,250 --> 00:23:18,490
So the platform team now speaks the
same language as product owners, they

409
00:23:18,490 --> 00:23:22,330
can demonstrate how the latency been
improved and everything so better.

410
00:23:22,690 --> 00:23:26,840
So we got to be now the product
owners and business goals and

411
00:23:26,840 --> 00:23:30,155
developers could be aligned properly
when we have a proper observability.

412
00:23:30,905 --> 00:23:34,695
So cost worth has been benefits
managed, like initially adding

413
00:23:34,695 --> 00:23:36,615
so much instrumentation raise it.

414
00:23:36,615 --> 00:23:37,845
Concerns about like cost.

415
00:23:38,095 --> 00:23:41,425
Hey we got to bring that tool
and this tool, and now we have

416
00:23:41,425 --> 00:23:44,095
this cost been increased and
everything, but at what cost?

417
00:23:44,185 --> 00:23:47,610
These all things, or in fact it,
it brought a lot of observability.

418
00:23:48,270 --> 00:23:51,390
So a lot of insight to what's
happening in our product.

419
00:23:51,390 --> 00:23:55,190
So sometimes putting money on those
tools is like what, while when compared

420
00:23:55,190 --> 00:23:59,880
to the revenue, what we are going to
get after we have a good good product.

421
00:24:01,685 --> 00:24:04,235
And next is like lessons
learned and best practices.

422
00:24:04,235 --> 00:24:07,025
So always build in
observability from day one.

423
00:24:07,115 --> 00:24:08,795
So don't bottom monitoring later.

424
00:24:09,155 --> 00:24:12,425
So design personalization systems
with observability in mind.

425
00:24:12,605 --> 00:24:15,555
So always keep always don't
think about performance thing.

426
00:24:15,555 --> 00:24:19,515
Always think about observability
once we deploy any product into the

427
00:24:19,515 --> 00:24:24,745
production or we got to always think
like how easily we can track, like

428
00:24:24,805 --> 00:24:26,245
to troubleshoot if an issue comes in.

429
00:24:26,755 --> 00:24:27,865
So that's one thing.

430
00:24:27,865 --> 00:24:31,465
And treat logs, metrics
status as first class citizen.

431
00:24:31,465 --> 00:24:34,295
So all three telemetry
types hand in hand goes.

432
00:24:34,295 --> 00:24:39,245
So we got to be detailed logging of
the important components and things.

433
00:24:39,855 --> 00:24:43,066
So always treat those things
as first class agents.

434
00:24:43,215 --> 00:24:46,515
And other is like design for
visibility, non just performance.

435
00:24:46,515 --> 00:24:50,945
So always whenever whenever we do
something to developers, they always

436
00:24:50,945 --> 00:24:52,535
think how can I improve the performance?

437
00:24:52,565 --> 00:24:56,705
They always keep that, you
always wear that suit of how

438
00:24:56,705 --> 00:24:57,815
can I improve the performance?

439
00:24:57,845 --> 00:25:00,665
What we got to be there is other side
we got to see is like observability.

440
00:25:01,415 --> 00:25:05,165
So how better coding I can done
in part of observability so

441
00:25:05,165 --> 00:25:09,155
that, logging and all the systems
interactions, how can properly can go.

442
00:25:09,765 --> 00:25:12,825
And what are tools I built
in to track these ones?

443
00:25:12,825 --> 00:25:17,325
I push into the pro and there is
like import the team, the data.

444
00:25:18,960 --> 00:25:22,550
Train new engineers or data scientists
importance of observability and

445
00:25:22,700 --> 00:25:25,890
new knowledge on these testing and
tools, and encourage the culture so

446
00:25:25,890 --> 00:25:29,070
that this makes incidents response
more collaborative and effective.

447
00:25:29,430 --> 00:25:32,420
So it can be easily fixable and
troubleshoot and improve our product

448
00:25:32,480 --> 00:25:37,350
health and others like irate and
improve like observability is not set.

449
00:25:37,620 --> 00:25:39,600
Not set and forget it.

450
00:25:39,600 --> 00:25:44,240
Continuously refine what you monitor, so
today you built in something to monitor

451
00:25:44,240 --> 00:25:46,600
dashboards or or logging and anything.

452
00:25:46,600 --> 00:25:49,210
But tomorrow you introduce one
new use case and new feature.

453
00:25:49,210 --> 00:25:53,340
So you got to adapt to that and
enhance that properly so that new

454
00:25:53,340 --> 00:25:55,310
use cases can be track even properly.

455
00:25:56,840 --> 00:25:58,880
And final thoughts and call to action.

456
00:25:59,000 --> 00:26:00,410
Observability is product critical.

457
00:26:00,500 --> 00:26:03,920
So it's basically in the
real time ML environments.

458
00:26:03,980 --> 00:26:08,900
Monitoring isn't just an ops
concern, it directly influences user

459
00:26:08,900 --> 00:26:10,520
experience and trusting of product.

460
00:26:10,520 --> 00:26:12,290
So we always get to be careful.

461
00:26:12,740 --> 00:26:16,570
So observability is a product critical
and trust through a transparency.

462
00:26:17,095 --> 00:26:22,955
So whenever, as we earlier discuss
that model, always this model model or

463
00:26:22,955 --> 00:26:24,905
recommendation engine part is a black box.

464
00:26:24,905 --> 00:26:27,605
Whether we always recommending
the right content at right

465
00:26:27,605 --> 00:26:29,045
time to the customers or not.

466
00:26:29,085 --> 00:26:34,385
Proper observations, having at model
level is the key so that we can clearly

467
00:26:34,385 --> 00:26:37,835
see through what is going on and why
the conversion rate been dropped down.

468
00:26:38,630 --> 00:26:42,120
And take action, like one way
to your current observability in

469
00:26:42,120 --> 00:26:44,870
your current projects which are
projects you guys working on.

470
00:26:44,870 --> 00:26:49,430
And take a look back, step back and look
at the observability, what all we have

471
00:26:49,430 --> 00:26:51,980
and when things fall back things go bad.

472
00:26:52,070 --> 00:26:57,870
How can I immediately fix and how can I
keep my product healthy so that it can

473
00:26:57,870 --> 00:27:02,280
bring more revenue, without without any
inter intervene to the production floors

474
00:27:02,880 --> 00:27:04,680
and there is like continuous improvement.

475
00:27:05,100 --> 00:27:07,740
So scaling observability
is an ongoing journey.

476
00:27:07,860 --> 00:27:11,430
So use telemetry site and devices
improvements both them up.

477
00:27:11,700 --> 00:27:16,700
So we got to always continuously
improve metrics, cases and

478
00:27:16,700 --> 00:27:17,930
all those things in product.

479
00:27:18,680 --> 00:27:19,610
And call to action.

480
00:27:20,030 --> 00:27:23,900
So we always embrace the mindset
that if you can't measure it,

481
00:27:23,960 --> 00:27:25,550
so you can't improve it, right?

482
00:27:25,820 --> 00:27:29,480
So bring a clarity to the complexity
of realtime personalization.

483
00:27:29,820 --> 00:27:33,090
So by observing its model, so
your personalization ending and

484
00:27:33,090 --> 00:27:35,310
your users will thank you later.

485
00:27:35,370 --> 00:27:39,240
So once you have this mindset
suit yourself for the observation.

486
00:27:40,395 --> 00:27:41,685
Okay guys, sir, thank you.

487
00:27:41,735 --> 00:27:45,065
That's all from my end and you
can reach me for anything to my

488
00:27:45,065 --> 00:27:50,625
LinkedIn profile and you guys have
a wonderful rest of conference.

489
00:27:50,955 --> 00:27:51,375
Thank you.

