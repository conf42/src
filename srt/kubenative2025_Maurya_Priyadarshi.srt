1
00:00:00,500 --> 00:00:02,750
Hi, I am Maia Pshi.

2
00:00:03,650 --> 00:00:07,819
I am from Manipal University
India, currently working at Meta

3
00:00:07,819 --> 00:00:14,871
Platforms, and I'm happy to be
speaking at count 42 Co native 2025.

4
00:00:15,371 --> 00:00:20,341
I have a lot of experience on ai and
we have developed some very interesting

5
00:00:20,521 --> 00:00:27,570
AI tools here at Meta, but this is my
own research on building resilient ai.

6
00:00:28,441 --> 00:00:34,221
Something which most of us may not
think about is about an enterprise

7
00:00:34,251 --> 00:00:38,271
for, about a framework for
enterprise security and governance.

8
00:00:38,771 --> 00:00:43,751
So we are transforming AI security
and governance from a compliance

9
00:00:43,751 --> 00:00:46,510
burden to strategic advantage.

10
00:00:47,010 --> 00:00:48,001
So let's look about it.

11
00:00:48,501 --> 00:00:51,770
So the first one is the AI
transformation challenge.

12
00:00:52,671 --> 00:00:54,800
Artificial intelligence
isn't just changing.

13
00:00:54,800 --> 00:01:00,710
Business is basically fundamentally
redefining enterprise operations.

14
00:01:01,210 --> 00:01:05,110
This is from streamlining automated
decision making to supercharging,

15
00:01:05,110 --> 00:01:07,180
predictive and analytics.

16
00:01:07,750 --> 00:01:13,270
AI systems are now critical
engines driving productivity and

17
00:01:13,270 --> 00:01:15,310
innovation across every industry.

18
00:01:15,810 --> 00:01:21,925
So we all know this, but what are some
of the security risks related to ai?

19
00:01:22,195 --> 00:01:23,575
Let's talk about that.

20
00:01:24,415 --> 00:01:25,735
In a bit detail now.

21
00:01:25,825 --> 00:01:27,415
So what are different areas?

22
00:01:27,415 --> 00:01:33,295
What I've found is the critical AI
security risk, so this can be classified

23
00:01:33,295 --> 00:01:36,265
into data leakage, model manipulation.

24
00:01:36,765 --> 00:01:40,245
Compliance features, bias and fairness.

25
00:01:40,305 --> 00:01:43,845
So wherever there comes ai, we
know that there could be a data

26
00:01:43,845 --> 00:01:48,795
leakage issue, which is unauthorized
exposure of sensitive data, which

27
00:01:48,795 --> 00:01:52,695
is PII, trade secrets we know about.

28
00:01:52,905 --> 00:01:56,495
We know all about the different
unauthorized exposure to the PII data.

29
00:01:56,995 --> 00:02:00,325
Now, the next one is the
model manipulation, right?

30
00:02:00,325 --> 00:02:03,685
It's a malicious alteration
of the AI behavior.

31
00:02:04,185 --> 00:02:07,365
Because it's in the hands of
who is developing it, how it is

32
00:02:07,485 --> 00:02:12,325
being developed, and what are the
different sources to which it has

33
00:02:12,325 --> 00:02:15,866
some access to compliance failures.

34
00:02:16,025 --> 00:02:18,030
Of course someone is developing ai.

35
00:02:18,455 --> 00:02:23,315
It doesn't mean that they have done
all the compliance checks on this,

36
00:02:23,675 --> 00:02:28,025
and of course the last one, but
not the least, is the bias and the

37
00:02:28,025 --> 00:02:30,365
fairness, which we all know about.

38
00:02:30,365 --> 00:02:31,565
There could be bias in the ai.

39
00:02:32,065 --> 00:02:38,035
Now talking about the innovation risk gap,
okay, so let's talk about the traditional

40
00:02:38,365 --> 00:02:43,885
security frameworks are proving like
critically inadequate for dynamic and

41
00:02:43,975 --> 00:02:46,285
complex landscape of the AI systems.

42
00:02:46,785 --> 00:02:54,075
And the laing disconnect is, generates
the significant vulnerabilities for this.

43
00:02:54,575 --> 00:02:57,605
Next we are going to talk
about the framework foundation.

44
00:02:58,085 --> 00:03:01,475
Which consists of four core pillars.

45
00:03:02,105 --> 00:03:07,265
The first one is the governance
structures, which is establishing

46
00:03:07,265 --> 00:03:13,175
robust governance frameworks with clear
roles and responsibilities to ensure

47
00:03:13,265 --> 00:03:17,345
accountable oversight and strategic
decision making for AI systems.

48
00:03:17,845 --> 00:03:22,755
The next one is the technical
safeguard, which is like deploying.

49
00:03:23,145 --> 00:03:28,605
Advanced technical safeguards
including cutting edge encryption

50
00:03:28,665 --> 00:03:33,915
and granular access controls to
fortify AI environments against

51
00:03:33,945 --> 00:03:36,465
evolving threats and data breaches.

52
00:03:37,095 --> 00:03:41,145
Compliance alignment is the next
one, the operations seemingly with

53
00:03:41,150 --> 00:03:45,270
the global regulatory compliance
and proactively doing it.

54
00:03:45,770 --> 00:03:48,080
And the next one is continuous monitoring.

55
00:03:48,860 --> 00:03:51,680
Without continuous and dynamic monitoring.

56
00:03:52,130 --> 00:03:57,710
That is also one of the, one of the core
pillar of the framework of the foundation.

57
00:03:58,210 --> 00:04:04,270
Moving on the comprehensive AI governance
framework, so AI governance code,

58
00:04:04,570 --> 00:04:08,740
which is comprising senior executives
and cross-functional leadership.

59
00:04:09,280 --> 00:04:12,940
This board bold spearheads
the strategic direction.

60
00:04:13,360 --> 00:04:17,680
Gratifies critical policies and
optimizes resource deployment

61
00:04:18,160 --> 00:04:19,990
for all AI initiatives.

62
00:04:20,040 --> 00:04:25,110
That's the board which is primarily
responsible for it, AI ethics committee.

63
00:04:25,440 --> 00:04:30,880
Now, this becomes very important when
there are like cross-functional teams and

64
00:04:30,880 --> 00:04:35,830
there are legal counsels, which who needs
to be involved in the AI development.

65
00:04:36,400 --> 00:04:39,830
Because when you are developing
something, the ethics has to be

66
00:04:39,830 --> 00:04:44,410
kept in mind and there has to be
finally a review panel, right?

67
00:04:44,410 --> 00:04:48,190
So review panel consists of
like expert engineers, security

68
00:04:48,190 --> 00:04:53,980
specialists, compliance officers, and
the panel forms rigorous technical

69
00:04:53,980 --> 00:04:55,990
evaluation of the AI systems.

70
00:04:56,290 --> 00:05:01,420
The engineers may not be equipped
enough for all of these, so that's

71
00:05:01,420 --> 00:05:03,880
why there needs to be all of these.

72
00:05:04,540 --> 00:05:05,320
Framework.

73
00:05:05,820 --> 00:05:07,530
Let's look at the fortifying.

74
00:05:07,530 --> 00:05:12,030
AI excellence, strategic
identity and access management.

75
00:05:12,120 --> 00:05:14,240
This is how do you get access?

76
00:05:14,630 --> 00:05:14,900
How?

77
00:05:14,900 --> 00:05:16,880
How does everyone get access to this?

78
00:05:16,880 --> 00:05:19,100
And who are the people
who gets access to it?

79
00:05:19,850 --> 00:05:25,730
And this is like implementing a
robust and multifactor authentication.

80
00:05:25,730 --> 00:05:29,840
We all know it to secure the
critical AI systems access.

81
00:05:30,830 --> 00:05:37,050
Defining and enforcing granular rule-based
access meticulously tailored to dynamic

82
00:05:37,080 --> 00:05:40,110
AI development and deployment functions.

83
00:05:40,620 --> 00:05:43,350
Now, streamlining the operations
is the next big thing.

84
00:05:43,380 --> 00:05:45,030
Dynamic access policies.

85
00:05:45,030 --> 00:05:49,590
We talked about that and the
traditional IAM identity and access

86
00:05:49,590 --> 00:05:55,350
management frameworks, which often
fall short of and struggling to address

87
00:05:55,350 --> 00:05:57,720
complex dynamic access platforms.

88
00:05:57,720 --> 00:05:58,410
The focus is.

89
00:05:58,725 --> 00:06:02,505
On development of the AI and nobody
thinks about identity and access

90
00:06:02,505 --> 00:06:08,565
management, and this point of mine is
fortifying all of these for AI excellence.

91
00:06:09,065 --> 00:06:15,405
Next is something which is real
timely detection and how do we do it?

92
00:06:15,405 --> 00:06:19,095
Is data ingestion like it consists
of this, all of these things,

93
00:06:19,095 --> 00:06:23,835
data ingestion, then response,
coordination, pattern analysis.

94
00:06:24,335 --> 00:06:25,445
Alert generation.

95
00:06:25,535 --> 00:06:29,525
So data ingestion is seamless
real time ingestion and continuous

96
00:06:29,525 --> 00:06:31,745
monitoring of the critical AI systems.

97
00:06:32,255 --> 00:06:36,365
And then comes to response
coordination, which is coordinated

98
00:06:36,365 --> 00:06:38,585
incident response and remediation.

99
00:06:39,245 --> 00:06:43,295
And the pattern analysis, sophisticated
machine learning algorithm.

100
00:06:43,865 --> 00:06:49,955
Proactively detecting and pinpointing
subtle deviations and critical anomalies

101
00:06:49,955 --> 00:06:52,745
from established operational baselines.

102
00:06:52,805 --> 00:06:57,575
Alert generation is the last one, but
not the least, which is AMA automated

103
00:06:57,605 --> 00:07:03,155
high priority alert generation for rapid
notification of security incidents,

104
00:07:03,655 --> 00:07:07,555
curricular performance issues, and of
course potential compliance violations.

105
00:07:07,555 --> 00:07:10,780
So this forms the real
time anomaly detection.

106
00:07:11,280 --> 00:07:15,090
Then automated compliance
workflows towards that.

107
00:07:15,090 --> 00:07:15,930
Let's look at it.

108
00:07:16,110 --> 00:07:21,720
So that is like regulatory mapping,
identifying applicable regulations,

109
00:07:21,720 --> 00:07:26,310
G-D-P-R-V-C-F-C-C-P-A, industry
specific requirements and MAP

110
00:07:26,310 --> 00:07:28,800
requirements to AI operation.

111
00:07:29,430 --> 00:07:34,320
The second one is control implementation,
deploying automated controls for

112
00:07:34,320 --> 00:07:38,280
data handling, consent management,
and audit trail generation.

113
00:07:39,046 --> 00:07:40,756
Next is continuous assessment.

114
00:07:40,756 --> 00:07:44,655
We talked about it, which is
compliance checks, C analysis,

115
00:07:45,135 --> 00:07:46,635
and remediation tracking.

116
00:07:47,116 --> 00:07:49,785
And next is the reporting generation.

117
00:07:50,355 --> 00:07:55,526
Automated compliance reports,
regulatory submissions, and

118
00:07:55,585 --> 00:07:57,296
stakeholder communication.

119
00:07:57,796 --> 00:08:01,095
Oh, let's look at monitoring
and explainability tools.

120
00:08:01,546 --> 00:08:06,346
So model performance monitoring comes
under it, which is real time accuracy

121
00:08:06,346 --> 00:08:10,636
and drift detection, performance
degradation alerts, resource

122
00:08:10,636 --> 00:08:15,705
utilization, tracking output quality
assessments, and explainability

123
00:08:15,705 --> 00:08:20,056
features, which is decision pathways
importance analysis and mitigation.

124
00:08:20,476 --> 00:08:24,886
So comprehensive monitoring
bills stakeholder trust by

125
00:08:24,886 --> 00:08:29,296
providing transparency into
AI decision making process.

126
00:08:29,656 --> 00:08:33,286
And ensuring consistent,
reliable performance.

127
00:08:33,785 --> 00:08:36,126
Next is building stakeholder trust.

128
00:08:36,186 --> 00:08:42,846
So you know, here is where the stakeholder
comes into picture and how do we build

129
00:08:42,846 --> 00:08:47,586
that trust with the stakeholder and the
executive Confidence is the first one.

130
00:08:47,586 --> 00:08:48,306
Well-defined.

131
00:08:48,306 --> 00:08:53,196
Governance frameworks and robust
risk management strategies provide

132
00:08:53,196 --> 00:08:55,626
leadership with comprehensive oversight.

133
00:08:56,031 --> 00:08:57,531
And strategic control.

134
00:08:57,980 --> 00:09:01,820
All AI initiatives enabling
informed decision making.

135
00:09:02,320 --> 00:09:07,241
Then there is regulatory readiness,
which is efficient compliance workflows

136
00:09:07,330 --> 00:09:12,631
and through audit trails ensuring
consistent clearance to regulations.

137
00:09:13,290 --> 00:09:17,161
Employee assurance, which is
also important, is transparent.

138
00:09:17,370 --> 00:09:22,230
AI operations and clear ethical
guidelines cultivate a workplace

139
00:09:22,230 --> 00:09:24,540
where employees confidently integrate.

140
00:09:25,290 --> 00:09:30,870
AI augmented processes fostering
innovation and collaboration.

141
00:09:31,370 --> 00:09:33,380
Now let's talk about some of the benefits.

142
00:09:34,070 --> 00:09:39,491
Fortified risk mitigation is one of the
benefits, which is proactive monitoring

143
00:09:39,540 --> 00:09:44,790
and robust controls, which drastically
reduces scale security incidents.

144
00:09:45,570 --> 00:09:48,960
Next is accelerated cost
Efficiency is one of the benefit,

145
00:09:48,960 --> 00:09:50,970
which is automation slashes.

146
00:09:51,661 --> 00:09:57,031
Manual compliance efforts and ex
expedites incident resolution.

147
00:09:57,531 --> 00:10:02,540
And the next one is enhanced
stakeholder trust, which is transparent

148
00:10:02,540 --> 00:10:07,251
and accountable governance, which
cultivates unwavering confidence

149
00:10:07,251 --> 00:10:13,291
from regulators strengthens customer
loyalty and empowers internal teams

150
00:10:13,291 --> 00:10:14,971
fostering widespread adoption support.

151
00:10:15,471 --> 00:10:21,320
Expedited AI deployment, integrated
security controls and streamlined

152
00:10:21,320 --> 00:10:25,371
approval pathways, which accelerate
the launch of new AI initiative.

153
00:10:25,401 --> 00:10:29,091
You don't have to go back every
time and do this all over again,

154
00:10:29,091 --> 00:10:30,440
every time when you're doing it.

155
00:10:30,440 --> 00:10:34,731
This is like once, once it is established
and all the things are in place that

156
00:10:34,731 --> 00:10:37,235
we have expedited AI deployment.

157
00:10:37,735 --> 00:10:41,005
And then there is strategic
competitive advantage.

158
00:10:41,005 --> 00:10:46,555
Of course, there's a lot of competition
in the industry and implementing

159
00:10:46,605 --> 00:10:52,620
some advanced framework like this
is basically redefining AI security

160
00:10:52,620 --> 00:10:58,095
and governance, transforming it
from a more compliance burden into

161
00:10:58,095 --> 00:11:00,405
a powerful strategic differentiator.

162
00:11:00,905 --> 00:11:06,705
And this framework is engineer engineered
to fuel sustainable innovation,

163
00:11:06,765 --> 00:11:10,625
providing crystal clear guidance
for the responsible AI development.

164
00:11:11,315 --> 00:11:15,425
Organizations can now boldly
pursue ambitious AI initiatives.

165
00:11:15,965 --> 00:11:21,215
Assured that comprehensive safeguards
are firmly in place and with

166
00:11:21,215 --> 00:11:23,675
this strategic approach, the AI.

167
00:11:24,610 --> 00:11:29,710
This positions the AI security and
governance as a potent enabler of

168
00:11:29,710 --> 00:11:32,710
innovation and not em impediments.

169
00:11:32,800 --> 00:11:37,430
It's basically in the right direction,
the positive direction for this now

170
00:11:37,430 --> 00:11:39,740
future proofing your AI innovation.

171
00:11:39,920 --> 00:11:44,810
It's elevating your AI security
and governance from a challenge to

172
00:11:44,810 --> 00:11:46,880
a distinct competitive advantage.

173
00:11:47,380 --> 00:11:49,780
And this consists of three points here.

174
00:11:49,870 --> 00:11:54,370
How it does it, future proof is
strategically assess AI readiness,

175
00:11:54,970 --> 00:12:00,475
activate the core component, core
framework component, and cultivating

176
00:12:00,595 --> 00:12:02,635
unwavering stakeholder trust.

177
00:12:02,635 --> 00:12:05,845
We all talked about this in detail,
but these are, this is future

178
00:12:05,845 --> 00:12:08,635
proofing your AI innovation.

179
00:12:09,135 --> 00:12:12,555
Alright, so that's the
end of my presentation.

180
00:12:13,215 --> 00:12:14,985
And thank you very much.

