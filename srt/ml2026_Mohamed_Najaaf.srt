1
00:00:00,500 --> 00:00:01,190
Speaker 14: Hi everyone.

2
00:00:01,260 --> 00:00:02,310
My name is MoMA Naja.

3
00:00:02,550 --> 00:00:07,859
I work as a solutions architect at Amazon
Web Services, and today I wanted to

4
00:00:07,859 --> 00:00:13,939
talk about how you can build generative
AI applications at AWS sustainably.

5
00:00:14,719 --> 00:00:16,555
So I'm going to walk through.

6
00:00:17,494 --> 00:00:21,125
Some of the design concentrations
and what are the techniques you can

7
00:00:21,125 --> 00:00:26,194
adopt when you're building majority
applications so that you can reduce

8
00:00:26,194 --> 00:00:30,544
your overall carbon emissions that
are emitted by your gene applications.

9
00:00:31,044 --> 00:00:36,504
So I wanted to start the session
by showing you some study based on

10
00:00:36,504 --> 00:00:40,505
the data available from Deloitte,
Gartner Goldman Sachs and few other

11
00:00:41,025 --> 00:00:42,495
major companies around the world.

12
00:00:43,185 --> 00:00:47,595
And you might be already familiar
or have heard of this energy spikes.

13
00:00:48,095 --> 00:00:53,545
That are being spiked all, all across
the world due to this AI models, right?

14
00:00:53,545 --> 00:00:57,835
And we know that the AI models are
getting bigger day after day or each

15
00:00:57,835 --> 00:01:03,845
month, we see a new releases of LLM from
a leading companies all around the globe.

16
00:01:04,595 --> 00:01:10,415
And so on the left hand side, what
is the model training size index.

17
00:01:10,775 --> 00:01:15,245
And from 2014 all the way to
2024, what we have seen is.

18
00:01:16,205 --> 00:01:24,425
It has increased more than 350,000 times,
and this is this spy coordinated in

19
00:01:24,425 --> 00:01:31,495
the year 20 21, 20 22 when the chat GPT
started to make waves around the world.

20
00:01:31,495 --> 00:01:36,545
And people started using more and
more AI and curious about how they

21
00:01:36,545 --> 00:01:41,315
can integrate that into their own
organizations and applications.

22
00:01:42,110 --> 00:01:47,210
And we've seen people building
and training these models with

23
00:01:47,210 --> 00:01:50,390
the vast amounts of data, right?

24
00:01:50,930 --> 00:01:57,620
And that takes a more, a huge amount
of energy from all across the world.

25
00:01:58,120 --> 00:02:02,380
And on the right hand side, what you can
see is the total data center electricity

26
00:02:02,740 --> 00:02:07,300
demand from 2022 to 20 20, 30, 20, 30.

27
00:02:07,360 --> 00:02:09,970
And you can see that, that the.

28
00:02:10,470 --> 00:02:13,830
It has been quite steady since 2022.

29
00:02:13,950 --> 00:02:20,589
2020. And then from 2022 it has
started to spike all across the world.

30
00:02:20,679 --> 00:02:28,660
And what we are expecting to see by 2030
is almost 800 tent hours of electricity

31
00:02:28,660 --> 00:02:34,529
will be needed to power these data
centers powering the ai workloads.

32
00:02:35,029 --> 00:02:41,769
And we also have certain studies by
Goldman Sachs research, which they

33
00:02:41,769 --> 00:02:46,089
are forecasting that 60 percentage
of the increasing electricity

34
00:02:46,089 --> 00:02:50,379
demands from data centers will need
to be met by burning fossil fuels.

35
00:02:50,479 --> 00:02:55,109
And that's not something that we
encourage, or we none of us need want.

36
00:02:55,709 --> 00:02:58,469
And because this could increase
the carbon emissions, the

37
00:02:58,469 --> 00:02:59,639
global carbon emissions spike.

38
00:03:00,089 --> 00:03:04,019
Approximately 2020 220 million tons.

39
00:03:04,019 --> 00:03:05,369
And that's a huge amount.

40
00:03:06,179 --> 00:03:11,579
And another article from World Economic
Forum says that the computational

41
00:03:11,579 --> 00:03:16,899
power required for AI is doubling
approximately every hundred days.

42
00:03:16,989 --> 00:03:22,269
And that, and these figures are
quite staggering and shocking and

43
00:03:22,269 --> 00:03:24,969
with the pace the AI is evolving.

44
00:03:25,469 --> 00:03:30,419
We need to be responsible when we
are developing or creating generative

45
00:03:30,469 --> 00:03:32,779
workloads in our own organizations, right?

46
00:03:32,779 --> 00:03:38,839
And we need to think about how
sustainably can we achieve this and

47
00:03:38,839 --> 00:03:40,729
get this business use case right?

48
00:03:40,729 --> 00:03:43,274
And that's the main agenda
for my session here.

49
00:03:43,344 --> 00:03:45,679
What, and how AWS can help you with that.

50
00:03:46,179 --> 00:03:52,039
So one of the things I wanted to touch
upon is how AWS can lower your carbon

51
00:03:52,039 --> 00:03:54,439
footprint by up to 99 percentage, right?

52
00:03:54,439 --> 00:04:00,059
And this is by migrating your workload,
which you may have on prem all the

53
00:04:00,059 --> 00:04:01,559
it workloads that you have there.

54
00:04:01,739 --> 00:04:05,699
And by shifting into a energy
efficient data center that

55
00:04:05,699 --> 00:04:07,709
AWS has all around the world.

56
00:04:08,209 --> 00:04:13,979
And Amazon had a. Climate p
pledge in 2019 to be a hundred

57
00:04:13,979 --> 00:04:20,039
percentage renewable resources in
order to, power our data centers.

58
00:04:20,039 --> 00:04:26,479
And we achieved this goal by 20
23, and we are quite proud of that.

59
00:04:26,509 --> 00:04:30,409
And all of our data centers are
powered using this a hundred

60
00:04:30,409 --> 00:04:32,029
percentage renewable resources.

61
00:04:32,329 --> 00:04:33,104
And we also.

62
00:04:33,964 --> 00:04:38,364
Integrate the efficiency at the
hardware layer, at the cooling the

63
00:04:38,364 --> 00:04:41,094
water that is being recycled to cool.

64
00:04:41,094 --> 00:04:45,324
These servers and the machines that
we have in the data centers, and

65
00:04:45,324 --> 00:04:50,304
the carbon free energy that's being
produced and used by the data centers

66
00:04:50,604 --> 00:04:57,064
and all the managed services that we
have at a W Ss or optimized workloads.

67
00:04:57,829 --> 00:05:03,219
So when you're thinking about creating
or developing a generat, a application

68
00:05:03,719 --> 00:05:09,089
when you create that inside or within
AWS by default, you achieve that

69
00:05:09,089 --> 00:05:14,539
sustainability and you reduce your
carbon footprint by up to 99 percentage.

70
00:05:14,589 --> 00:05:15,879
And that's a huge number.

71
00:05:16,379 --> 00:05:19,559
And I also want to touch about
the managed services, right?

72
00:05:19,559 --> 00:05:24,039
So when you have migrated to or
when you're, if you're planning

73
00:05:24,039 --> 00:05:29,319
to create something with AWS, what
are, what we all recommend is start

74
00:05:29,319 --> 00:05:32,809
with the managed services or the
serverless services that we have.

75
00:05:33,309 --> 00:05:37,739
The reason is simply because we take
that undifferentiated heavy lifting

76
00:05:37,739 --> 00:05:41,879
for you, whether it's maintaining
the hardware or the underlying

77
00:05:41,879 --> 00:05:47,339
networks, or the security of the data
center, or the patching of the oil.

78
00:05:47,339 --> 00:05:49,889
So you don't have to worry about
the underlying infrastructure.

79
00:05:49,919 --> 00:05:53,609
You can't just focus on developing
your application, setting the right

80
00:05:53,609 --> 00:05:57,779
configurations and the access policies.

81
00:05:57,779 --> 00:06:00,209
Those kind of stuff where we focus.

82
00:06:00,299 --> 00:06:04,319
On more of the basic the
heavy duty stuff for you.

83
00:06:04,859 --> 00:06:09,819
And we do that with sustainability,
with cost efficiency and operational

84
00:06:09,819 --> 00:06:11,529
efficiency and everything in mind, right?

85
00:06:11,979 --> 00:06:17,129
So that's gonna help you to achieve that
sustaina sustainability in, in a longer.

86
00:06:17,629 --> 00:06:22,039
And you may ha you may already have
started using some of the services that

87
00:06:22,039 --> 00:06:26,519
you see on the screen, or might have,
already aware of the services such as EKS,

88
00:06:26,519 --> 00:06:31,109
which is our elastic Kubernetes service,
which is for Kubernetes orchestration.

89
00:06:31,649 --> 00:06:35,459
Amazon petroc is our fully managed
serverless service where you can

90
00:06:35,459 --> 00:06:39,729
access 200 plus foundation models
with the use of a single API.

91
00:06:40,334 --> 00:06:48,294
And Petro Core is a newer service where
you can build agent systems, multi-agent

92
00:06:48,354 --> 00:06:52,844
tick orchestrations that you want
to implement at a enterprise, scale.

93
00:06:53,624 --> 00:06:58,244
And SageMaker has been our, one of our
popular a ML service for people have

94
00:06:58,274 --> 00:07:03,264
been utilizing to create train models
and deploy models at scale as well.

95
00:07:03,374 --> 00:07:04,279
The takeaway is simple.

96
00:07:05,004 --> 00:07:09,714
When you have the option, when your
organization policies allows, always go

97
00:07:09,714 --> 00:07:14,959
for the managed and serverless services
to give you that that to remove that

98
00:07:15,049 --> 00:07:20,809
undifferentiated heavy lifting and the
opera, operational, burners that you do

99
00:07:20,809 --> 00:07:25,549
not want to, and we take that chunk from
you where you can focus on your more

100
00:07:25,549 --> 00:07:27,829
important application related tasks.

101
00:07:28,329 --> 00:07:30,779
And the next important factor
is model selection, right?

102
00:07:30,779 --> 00:07:34,859
And we know that lots and lots of
models are coming up every month.

103
00:07:35,519 --> 00:07:40,639
And so when, whenever you are
creating an application you start

104
00:07:40,639 --> 00:07:44,749
with an particular model, but later
down the line you will see that the

105
00:07:44,749 --> 00:07:46,309
same provider has an updated model.

106
00:07:46,669 --> 00:07:48,529
So this is a frequent process, right?

107
00:07:48,529 --> 00:07:53,439
At regular you will have to evaluate
your model and decide does that.

108
00:07:54,324 --> 00:07:57,714
Does that meet your criteria, your
business use case, the latency

109
00:07:57,714 --> 00:08:02,414
or the hallucination, or a bunch
of evaluation criteria that your

110
00:08:02,414 --> 00:08:06,599
organization has and once only passes,
then you make that change internally.

111
00:08:06,729 --> 00:08:11,169
So there are different criteria
that each individual company

112
00:08:11,529 --> 00:08:14,199
takes into consideration, such
as multilingual capabilities.

113
00:08:14,679 --> 00:08:18,489
Does it need to speak any other
language than, or, generate text in

114
00:08:18,489 --> 00:08:20,109
any other language other than English?

115
00:08:20,829 --> 00:08:27,219
Or does it needs to be a proprietary model
or an open source model, or which size the

116
00:08:27,219 --> 00:08:29,169
number of parameters and co copper size.

117
00:08:29,519 --> 00:08:33,389
Or does it need to be a generalist
model or do we need to make

118
00:08:33,389 --> 00:08:34,889
it a Des Moines specific?

119
00:08:34,919 --> 00:08:38,729
If you need to be held more
integrated with financial services

120
00:08:38,729 --> 00:08:42,329
terms or healthcare domain,
do we need to fine tune it?

121
00:08:43,164 --> 00:08:47,904
So this is some of the questions that
people ask when selecting the model.

122
00:08:48,534 --> 00:08:53,244
And what we see is that you often don't
need the biggest and the most intelligent

123
00:08:53,424 --> 00:08:56,004
LLM to power your applications.

124
00:08:56,494 --> 00:09:02,204
For example, you might not need a
cloud Opus 4.6 4.6, which is the

125
00:09:02,204 --> 00:09:04,124
latest one that they released recently.

126
00:09:04,674 --> 00:09:09,564
But your business use case might
be achieved by using a cloud

127
00:09:09,564 --> 00:09:11,964
HighQ for 4.5 instead, right?

128
00:09:12,684 --> 00:09:16,289
So when you use a much lighter
model, that means you're.

129
00:09:16,889 --> 00:09:18,689
First of all, you are not spending much.

130
00:09:18,689 --> 00:09:20,149
It is a, much cheaper model.

131
00:09:20,479 --> 00:09:23,779
At the same time, it doesn't need
that many resources to get that

132
00:09:23,779 --> 00:09:28,669
inference, which in turn means it
doesn't require that much energy.

133
00:09:28,699 --> 00:09:34,099
So when you're using a lighter
weight model, it, most of the time

134
00:09:34,159 --> 00:09:38,449
it says you are consuming lesser
energy compared to a larger model.

135
00:09:39,019 --> 00:09:43,219
So selecting a model, which
is at the lightweight.

136
00:09:43,719 --> 00:09:45,609
Is an important factor.

137
00:09:46,109 --> 00:09:48,299
And next one is about evaluations.

138
00:09:48,299 --> 00:09:53,334
And we have the Amazon Petro evaluations
where you can evaluate your models and

139
00:09:53,334 --> 00:09:58,884
also if you have knowledge basis and
you want to evaluate against a different

140
00:09:58,884 --> 00:10:03,594
set of models and parameters such as
helpfulness, faithfulness, correctness,

141
00:10:03,714 --> 00:10:05,874
all the way to harmfulness and refusal.

142
00:10:05,874 --> 00:10:09,114
Those are some of the
parameters that we have.

143
00:10:09,114 --> 00:10:12,624
And if you also have certain custom
evaluation metrics, you can also integrate

144
00:10:12,624 --> 00:10:16,134
that into this to identify which model.

145
00:10:16,569 --> 00:10:18,099
Is the best option for you.

146
00:10:18,599 --> 00:10:22,739
And the other aspect is the moral
customization and adaptation.

147
00:10:23,139 --> 00:10:30,129
So what we have seen is when we
are building ai, generative AI

148
00:10:30,129 --> 00:10:35,529
applications, we often try to tweak
the prompts in order to achieve it.

149
00:10:35,919 --> 00:10:37,479
A specific output that we want.

150
00:10:37,479 --> 00:10:41,649
So at the bottom of the screen you
can see is the pro prompt engineering,

151
00:10:42,159 --> 00:10:46,629
and this is the easiest and the
most energy efficient way to.

152
00:10:47,129 --> 00:10:53,619
Get, customize and a model and some,
and we know that sometimes or, so for

153
00:10:53,619 --> 00:10:58,899
specific use cases, it might always not
work the best way, and that's why we have

154
00:10:59,229 --> 00:11:01,189
a bunch of other options as well, right?

155
00:11:01,459 --> 00:11:02,569
Such as retrieval.

156
00:11:02,929 --> 00:11:05,594
Augmented generation is where
you connect to your own.

157
00:11:06,499 --> 00:11:11,719
Proprietary internal data and you
have a vector store so that when you

158
00:11:11,719 --> 00:11:17,079
are generating the text it always
fetches the data from your internal

159
00:11:17,079 --> 00:11:23,300
data to, to reduce the hallucination
and also gives you more accurate data

160
00:11:23,330 --> 00:11:25,520
like a customer's data based on your.

161
00:11:25,805 --> 00:11:27,515
Organization informations.

162
00:11:28,235 --> 00:11:31,564
And then we have the fine tuning and
then we have the full fine tuning

163
00:11:31,564 --> 00:11:36,445
as well as parameter efficient fine
tuning theft which is basically a way

164
00:11:36,445 --> 00:11:40,855
to fully tune the model so that it is.

165
00:11:40,855 --> 00:11:46,455
So we keep the, it is subset of,
labeled data sets and, we are

166
00:11:46,455 --> 00:11:53,285
teaching the foundation, the LLM,
to to give a much better results.

167
00:11:53,285 --> 00:11:54,815
That's something that we
are looking at, right?

168
00:11:54,815 --> 00:11:58,905
So basically teaching,
teaching the foundation model

169
00:11:59,865 --> 00:12:00,945
and the parameter revision.

170
00:12:00,945 --> 00:12:07,035
Fine tuning is where you only tweaks
a subset of parameters, not the entire

171
00:12:07,095 --> 00:12:09,465
parameters, but in full fine tuning you.

172
00:12:10,080 --> 00:12:13,470
It does the full parameter customization.

173
00:12:13,470 --> 00:12:16,410
So those are like some of the
options that you have there.

174
00:12:17,050 --> 00:12:22,260
And lastly, we have the training from
scratch which only very small number

175
00:12:22,260 --> 00:12:26,310
of customers globally does because it's
really expensive to train something

176
00:12:26,310 --> 00:12:31,290
from scratch, and the amount of data
that's needed is also much greater and.

177
00:12:32,130 --> 00:12:37,530
It also is the high yes, energy
consuming and the highest carbon

178
00:12:37,530 --> 00:12:38,970
emitting in a process, right?

179
00:12:38,970 --> 00:12:44,370
And this is something that nobody wants
to do, but only very few use cases will

180
00:12:44,370 --> 00:12:47,160
lead to explore that approach as well.

181
00:12:47,660 --> 00:12:50,390
And choosing the right
silicone is really important.

182
00:12:50,540 --> 00:12:55,330
If you are going for a service where
you are training your model, your

183
00:12:55,330 --> 00:12:57,005
own, and you are, you want to sell.

184
00:12:57,685 --> 00:13:00,985
The right infrastructure,
the right virtual machine in

185
00:13:00,985 --> 00:13:02,445
order to train the models.

186
00:13:02,505 --> 00:13:05,085
And we give you a couple
of options there, right?

187
00:13:05,235 --> 00:13:06,735
And that is training instances.

188
00:13:06,765 --> 00:13:08,595
Train are custom.

189
00:13:08,835 --> 00:13:13,395
Amazon built silicon chips,
which keeps you up to 25% more

190
00:13:13,395 --> 00:13:15,765
energy efficient than comparable.

191
00:13:15,770 --> 00:13:17,355
Easy to instances, right?

192
00:13:18,035 --> 00:13:21,420
So there is no question that is
more sustainable and much better.

193
00:13:22,350 --> 00:13:23,430
Performing as well.

194
00:13:24,150 --> 00:13:28,740
And we have the Rainium two instances, the
next generation, which offers three times

195
00:13:28,740 --> 00:13:30,510
more energy efficient than rainium one.

196
00:13:30,990 --> 00:13:35,120
And I'm also happy to say that it's
not included in the slide, but we just

197
00:13:35,120 --> 00:13:38,475
released in the last rain that we have
rainium three instances, which are

198
00:13:38,805 --> 00:13:43,485
four times more energy efficient and
performing than the previous train too.

199
00:13:44,025 --> 00:13:44,805
So it's just.

200
00:13:45,360 --> 00:13:46,530
Getting better and better.

201
00:13:46,590 --> 00:13:52,620
So whenever you are thinking about moral
training, we advise you to make use of

202
00:13:52,680 --> 00:13:57,210
the silicon chips to get that better price
performance as well as sustainability

203
00:13:57,210 --> 00:14:00,090
and reduction in the carbon emissions.

204
00:14:00,990 --> 00:14:05,550
And if you also have inference
workloads, you and you.

205
00:14:06,420 --> 00:14:11,540
There is also an option to use your non
GPU chips, and that's where GRAVITANT

206
00:14:11,540 --> 00:14:15,650
comes into play and it again gives
you a much better price performance

207
00:14:16,100 --> 00:14:18,590
when compared to it's Amazon.

208
00:14:18,650 --> 00:14:24,240
Easy to alternatives and it also
takes up to 60 percentage less energy

209
00:14:24,840 --> 00:14:26,970
compared to an a c two in as well.

210
00:14:27,875 --> 00:14:30,905
So these are some of the hardware
options that give you, right?

211
00:14:31,315 --> 00:14:36,325
And how you can reduce and make your
applications more sustain sustainable.

212
00:14:36,825 --> 00:14:41,165
And next I wanted to talk about the
model deployment inference optimization.

213
00:14:41,165 --> 00:14:45,655
So these are the different boxes that
you see here are some of the techniques

214
00:14:46,075 --> 00:14:51,055
that you can use to reduce the overall
model size and optimize memory usage.

215
00:14:51,105 --> 00:14:54,400
So when you're using, when you're
reducing your modal size or when you're

216
00:14:54,400 --> 00:14:57,000
optimizing memory usage, you are only.

217
00:14:57,525 --> 00:15:02,955
Using the most required amount
of resource and energy, right?

218
00:15:02,955 --> 00:15:07,015
So it is very important that you you
consider these different techniques

219
00:15:07,375 --> 00:15:09,325
when you build such a applications.

220
00:15:10,310 --> 00:15:15,560
We have a model compilation, which
is optimizing models for hardware.

221
00:15:15,950 --> 00:15:19,490
We have model compression, which is
reducing model size, very efficiently

222
00:15:19,970 --> 00:15:21,950
and all the way to distillation.

223
00:15:21,980 --> 00:15:26,780
And this is where we have a larger
model such as op cloud, Opus 4.6.

224
00:15:27,350 --> 00:15:32,960
And we have a smaller model, like a
student model where it teaches all

225
00:15:32,960 --> 00:15:35,420
its learnings to the underlying model.

226
00:15:35,480 --> 00:15:36,920
And the idea is to.

227
00:15:37,910 --> 00:15:42,800
Let the smaller model handle most
of the queries efficiently and

228
00:15:42,800 --> 00:15:48,460
with, almost the same accuracy as,
or intelligence of the cloud opus.

229
00:15:48,520 --> 00:15:51,655
So these are like some of the
techniques we can explore and

230
00:15:51,825 --> 00:15:55,825
in order to achieve that similar
energy efficiency, but also without

231
00:15:56,385 --> 00:15:58,300
sacrifice, sacrificing the accuracy.

232
00:15:58,780 --> 00:15:59,650
Accuracy as well.

233
00:16:00,150 --> 00:16:04,080
And lastly, I wanted to touch upon the
importance of observability, right?

234
00:16:04,080 --> 00:16:08,800
And it is similar to everything
that we have been doing so

235
00:16:08,800 --> 00:16:10,270
far in the IT world, right?

236
00:16:10,660 --> 00:16:15,660
Whenever we are building something it's
important that we observe or we monitor

237
00:16:15,660 --> 00:16:21,400
that we need to create the corresponding
dashboards with the right metrics,

238
00:16:21,460 --> 00:16:23,860
with the right alarms to get notified.

239
00:16:24,310 --> 00:16:25,780
And the same applies here.

240
00:16:26,170 --> 00:16:30,640
And we have the native observability
toll called Amazon CloudWatch, which

241
00:16:30,640 --> 00:16:35,990
can help you and monitor this bedro
metrics or if you have certain bedro

242
00:16:35,990 --> 00:16:37,610
guardrails that you have configured.

243
00:16:37,610 --> 00:16:43,450
So guardrails are a way to apply
responsible AI into your applications.

244
00:16:43,850 --> 00:16:45,690
This would for example.

245
00:16:46,305 --> 00:16:50,945
Identifying toxicity or, foul
language or harmful less.

246
00:16:51,125 --> 00:16:54,545
So these are some of the factors
you can configure in the cart rails.

247
00:16:54,965 --> 00:16:57,215
And you can basically
monitor that in a dashboard.

248
00:16:57,695 --> 00:17:01,680
And which user and what is the input
and output that you're getting, right?

249
00:17:02,290 --> 00:17:06,010
So it's really important that you
continuously at every stage, monitor that.

250
00:17:06,730 --> 00:17:07,630
And if you're using.

251
00:17:08,500 --> 00:17:09,850
More of a SageMaker.

252
00:17:09,880 --> 00:17:14,350
Then we have to DecisionMaker profiler
and AWS neuron monitor to monitor

253
00:17:14,350 --> 00:17:19,300
the training metrics and then memory
info, the GPU utilization, memory

254
00:17:19,300 --> 00:17:20,950
utilization, and all that good stuff.

255
00:17:21,310 --> 00:17:22,780
And lastly, we have the nvidia.

256
00:17:22,780 --> 00:17:26,520
So you are using Nvidia processes
that you have, the system manager

257
00:17:26,670 --> 00:17:29,070
interface that you can leverage as well.

258
00:17:29,570 --> 00:17:31,910
So that's all I wanted to cover today.

259
00:17:31,970 --> 00:17:37,280
It's just to give you a few options
and to think about how you can

260
00:17:38,120 --> 00:17:39,945
build your application sustainably.

261
00:17:39,945 --> 00:17:45,165
And we touched upon using, making use
of managed services or server services

262
00:17:45,555 --> 00:17:49,925
like by truck and, the base model
selection is also really important to

263
00:17:49,925 --> 00:17:55,855
use the lightweight models and evaluate
that and and if it meets your business

264
00:17:55,855 --> 00:17:57,775
use case, you'd always go for that.

265
00:17:57,835 --> 00:18:03,725
So you get low cost and, but also good
sustainability into your application.

266
00:18:04,015 --> 00:18:08,245
So we have the model customization
techniques where we touched upon prompt

267
00:18:08,245 --> 00:18:12,265
engineering, fine tuning and rack
approaches that you should consider.

268
00:18:12,895 --> 00:18:17,275
And we also talked about the deployment
inference optimization, things like

269
00:18:17,275 --> 00:18:22,055
model distillation, model compression,
and pruning those kind of stuff.

270
00:18:22,715 --> 00:18:25,695
And silicon choice is
also really important.

271
00:18:25,695 --> 00:18:29,895
So if you've been thinking
about training your own models.

272
00:18:30,315 --> 00:18:34,965
Always recommended to use train
for training, which gives you much

273
00:18:34,965 --> 00:18:39,015
better price, performance and also
the energy efficiency that you need.

274
00:18:39,615 --> 00:18:46,305
And lastly, the continuous improvement is
really, so this is not like a one single

275
00:18:46,515 --> 00:18:48,665
journey or ride you build and go away.

276
00:18:48,665 --> 00:18:52,505
It's not that the case, you just
need to continuously monitor.

277
00:18:53,135 --> 00:18:55,895
You will need to change the
underlying foundation model.

278
00:18:56,515 --> 00:19:01,135
So it's always important to
monitor criteria applications.

279
00:19:01,435 --> 00:19:01,465
Okay.

280
00:19:01,965 --> 00:19:04,365
So that's all I wanted to cover today.

281
00:19:04,455 --> 00:19:09,665
I hope this session has been useful
and what is a couple of links to

282
00:19:09,695 --> 00:19:14,400
the AWS blocks where we talk about
how you can sustainably optimize the

283
00:19:14,460 --> 00:19:19,230
Generat A workloads and also AI ML
workloads in general sustainability.

284
00:19:19,230 --> 00:19:22,260
And you will also see this design
considerations and the things

285
00:19:22,260 --> 00:19:24,750
that we just discussed in detail.

286
00:19:24,955 --> 00:19:25,885
In those blocks.

287
00:19:26,455 --> 00:19:27,985
So thank you so much for your time.

288
00:19:27,985 --> 00:19:32,005
I hope this has been useful session
and have a good rest of the day.

289
00:19:32,665 --> 00:19:32,935
Thank you.

