1
00:00:00,570 --> 00:00:06,390
Hello everyone, this is Peush K. Thank
you for joining me today here I have

2
00:00:06,630 --> 00:00:13,590
more than 20 years of IT experience in
US healthcare, developing and maintaining

3
00:00:13,770 --> 00:00:16,020
software projects and programs.

4
00:00:16,680 --> 00:00:22,620
So here today we are going to talk
through a topic that's rapidly

5
00:00:22,620 --> 00:00:28,320
gaining traction across healthcare it
implementing AI driven observability.

6
00:00:30,015 --> 00:00:36,945
As SE practices for reliable healthcare
systems, the integration of artificial

7
00:00:36,945 --> 00:00:43,095
intelligence in healthcare systems
offer tremendous potential to improve

8
00:00:43,635 --> 00:00:50,599
patients' outcomes, but it is also
presenting unique reliability challenges

9
00:00:52,185 --> 00:00:57,960
when AI systems supporting critical
healthcare functions experience downtime.

10
00:00:59,085 --> 00:01:03,375
The consequences can
directly impact patient care.

11
00:01:04,724 --> 00:01:11,115
This representation and this presentation,
which we are going to discuss here will

12
00:01:11,115 --> 00:01:18,795
say how site reliability engineering SRE
practices are being successfully adopted

13
00:01:19,395 --> 00:01:25,245
to maintain exceptional availability
in AI powered healthcare platforms

14
00:01:25,995 --> 00:01:27,975
based on real world implications.

15
00:01:28,455 --> 00:01:35,774
Across multiple healthcare institutions,
our focus on how these practices ensure

16
00:01:35,774 --> 00:01:42,015
the reliability of AI powered platform
is critical for healthcare environments,

17
00:01:42,824 --> 00:01:49,395
when even milliseconds of downtime
can have a life altering consequences.

18
00:01:50,475 --> 00:01:56,570
Whether you are a healthcare provider,
a tech vendor, or an SRE team.

19
00:01:57,179 --> 00:02:03,479
Is stepping into regulated domains, you
will walk away with actionable insights

20
00:02:04,320 --> 00:02:11,490
and established blueprint for implementing
these practices in real world environment.

21
00:02:15,240 --> 00:02:20,880
In this slide, we will discuss the
AI healthcare reliability challenges.

22
00:02:21,660 --> 00:02:25,829
So we start how this unique
reliability challenges.

23
00:02:26,160 --> 00:02:33,420
Are affecting healthcare, artificial
intelligence, AI systems increasingly

24
00:02:33,420 --> 00:02:38,460
influence clinical decision
making, diagnosis, treatment

25
00:02:38,585 --> 00:02:40,505
planning, and patient monitoring.

26
00:02:41,745 --> 00:02:48,305
A model failure or downtime doesn't
just create technical issues.

27
00:02:49,605 --> 00:02:55,105
It directly affects patient's
outcomes, regulatory compliance.

28
00:02:56,085 --> 00:03:02,175
Hipaa, FDA and other healthcare
standards, we have to comply with

29
00:03:02,235 --> 00:03:07,425
all these standards based on the
geography to successfully implementing

30
00:03:07,455 --> 00:03:10,185
and using AI in healthcare facility.

31
00:03:11,475 --> 00:03:16,785
From a regulatory standpoint, we
face a stringent requirements, hipaa,

32
00:03:16,845 --> 00:03:23,505
which is health insurance Portability
and Accountability Act in US GDPR.

33
00:03:24,465 --> 00:03:29,895
Which is General Data Protection
Regulation in Europe and Food and

34
00:03:29,895 --> 00:03:36,795
Drug Administration, FDA, they
all see AI as a medical device.

35
00:03:38,025 --> 00:03:45,795
This imposes a strict controls
on how we update, monitor,

36
00:03:46,635 --> 00:03:49,545
and validate these systems.

37
00:03:50,790 --> 00:03:55,920
Healthcare AI system faces unique
challenges that the traditional

38
00:03:55,950 --> 00:03:58,470
SRE approaches must adopt.

39
00:03:58,890 --> 00:04:05,340
To address patient safety directly
depends on systems reliability,

40
00:04:06,480 --> 00:04:11,220
creating zero tolerance situation
for certain types of failure.

41
00:04:12,930 --> 00:04:19,079
Meanwhile, complex regulatory
requirements add additional constraint.

42
00:04:20,130 --> 00:04:25,260
To how system can be
monitored, tested, and updated.

43
00:04:27,630 --> 00:04:33,900
Finally, there is a constant push and
pull between innovation and reliability.

44
00:04:35,040 --> 00:04:39,630
AI evolves rapidly, but
healthcare infrastructure is

45
00:04:39,630 --> 00:04:43,050
built on safety and stability.

46
00:04:44,220 --> 00:04:46,650
This mismatch requires.

47
00:04:46,695 --> 00:04:53,625
A refined approach to SRE
one that merges technical

48
00:04:53,625 --> 00:04:57,645
robustness with clinical caution.

49
00:04:59,085 --> 00:05:02,445
So here, let's move on to third ipo.

50
00:05:02,445 --> 00:05:09,645
Important aspect, how it is affecting
the case studies, which shows the

51
00:05:09,645 --> 00:05:13,575
automated observability and its benefits.

52
00:05:15,120 --> 00:05:21,480
The observability transforms into
impacts our case studies, access

53
00:05:21,480 --> 00:05:31,020
multiple hospitals and shows a 78%
reduction in meantime to resolution

54
00:05:31,230 --> 00:05:34,995
and TTR for critical incidents.

55
00:05:35,895 --> 00:05:36,595
But how?

56
00:05:37,575 --> 00:05:41,275
By integrating AI powered observability.

57
00:05:42,015 --> 00:05:47,924
Platforms that continuously
monitor not just technical metrics,

58
00:05:48,974 --> 00:05:51,375
but clinical relevance too.

59
00:05:53,179 --> 00:05:58,489
These systems achieved a
92% variance detection rate,

60
00:06:00,570 --> 00:06:08,820
capturing early sign of AI drift or model
flatness before they could affect care.

61
00:06:10,635 --> 00:06:17,025
65% decrease in incidents we
saw year over year in clinical

62
00:06:17,025 --> 00:06:19,845
incidents, and that is under 30.

63
00:06:21,105 --> 00:06:22,425
It is a practice.

64
00:06:23,625 --> 00:06:30,855
Our multi-institutional cases studies
demonstrates dramatic improvements through

65
00:06:30,855 --> 00:06:36,795
implementing automated observability
tools, healthcare organizations.

66
00:06:37,335 --> 00:06:42,854
Participating in our research deployed
specialized monitoring solutions

67
00:06:43,425 --> 00:06:50,474
designed to detect subtle anomalies
in AI prediction patterns before they

68
00:06:50,474 --> 00:06:53,745
manifested as clinically relevant issues.

69
00:06:55,724 --> 00:07:01,125
Organizations like Mount
Sinai and Mayo Clinic are

70
00:07:01,125 --> 00:07:05,835
developing AI to prevent issues.

71
00:07:06,344 --> 00:07:14,565
Rather than reacting to them, the key
is a shift from reactive monitoring

72
00:07:15,405 --> 00:07:20,565
to proactive variance detection
using machine learning techniques

73
00:07:21,284 --> 00:07:25,484
on logs, metrics, and user behavior.

74
00:07:29,265 --> 00:07:33,825
Now, let's cover how establishing
meaningful SLIs and SLOs.

75
00:07:34,724 --> 00:07:41,655
For healthcare, AI service
level indicators, SLIs and

76
00:07:41,655 --> 00:07:47,414
service level objectives, SLOs
shapes the foundation of SRE.

77
00:07:48,765 --> 00:07:54,195
However, in healthcare, they
require a tailored lens.

78
00:07:55,485 --> 00:08:01,695
We categorize them into technical,
clinical, and operational indicators.

79
00:08:02,565 --> 00:08:03,735
Let's cover technical sli.

80
00:08:05,190 --> 00:08:09,210
Inference latency, less
than 200 milliseconds

81
00:08:11,220 --> 00:08:20,670
and model drift, which we aim to keep
below 0.5% to ensure diagnostic relevance.

82
00:08:21,960 --> 00:08:31,380
Batch processing completion, 99.9% API
Availabilities for supporting systems.

83
00:08:31,905 --> 00:08:40,365
99.95%. Clinically, we track
diagnostic suggestions, accuracy

84
00:08:40,395 --> 00:08:47,175
and override frequency, an indicator
of clinicians trust in the system.

85
00:08:49,005 --> 00:08:53,190
Operational metrics includes
time to detect variance.

86
00:08:53,880 --> 00:08:59,130
And recovery time objectives
under clinical and uh, sli.

87
00:08:59,340 --> 00:09:03,569
You can see here that there is a
documentation completeness score.

88
00:09:03,990 --> 00:09:09,180
There is a clinician's override
frequency under operational recover,

89
00:09:09,329 --> 00:09:15,329
incident response time, recovery time
objectives, and change failure rate.

90
00:09:16,319 --> 00:09:21,300
By aligning these with key clinical
performance indicators, KPIs.

91
00:09:22,815 --> 00:09:25,605
Like treatment relevance or documentation.

92
00:09:26,175 --> 00:09:34,275
Completedness, we ensure that our AI
goals serve both system and patients.

93
00:09:35,055 --> 00:09:41,265
The collaboration between SRE teams
and clinician is very crucial here.

94
00:09:42,495 --> 00:09:46,395
SRE teams across participating
healthcare organizations

95
00:09:47,025 --> 00:09:50,360
established novel SLIs and SLOs.

96
00:09:50,520 --> 00:09:54,210
Is specifically designed for AI systems.

97
00:09:55,590 --> 00:10:02,130
The most successful implementations
balanced traditional reliability metrics

98
00:10:02,550 --> 00:10:08,340
with healthcare specific indicators
tied directly to clinical outcomes.

99
00:10:10,255 --> 00:10:15,480
In one project, we found that
tracking clinician override

100
00:10:15,480 --> 00:10:18,395
frequency was more informing.

101
00:10:19,230 --> 00:10:27,180
Then raw model accuracy, and it directly
indicated trust and ultimate adoption.

102
00:10:28,920 --> 00:10:33,990
These metrics were developed through
close collaboration between SRE

103
00:10:33,990 --> 00:10:36,420
teams and medical professional.

104
00:10:36,420 --> 00:10:44,430
Ensuring then technical reliability is
aligned with the patient care priorities.

105
00:10:47,430 --> 00:10:48,540
Chaos engineering.

106
00:10:49,110 --> 00:10:51,900
In healthcare ai, a safe approach.

107
00:10:52,320 --> 00:10:56,880
When you see, and you read and listen,
this word chaos, you definitely

108
00:10:56,880 --> 00:11:01,800
want to go far, far away if you
are in the healthcare industry.

109
00:11:02,790 --> 00:11:07,950
But here I would say that the
chaos engineering may sound

110
00:11:07,950 --> 00:11:13,710
risky in a healthcare context,
but with the right safeguards,

111
00:11:13,920 --> 00:11:17,280
it's a game changer We conduct.

112
00:11:17,655 --> 00:11:25,245
Fault injection experiments in sandbox
environment using synthetic patient data.

113
00:11:26,145 --> 00:11:28,485
That would be a isolated environment.

114
00:11:29,715 --> 00:11:32,715
Then we move on to control failure.

115
00:11:33,405 --> 00:11:39,405
As you see here, it is graduated
fault injection with safeguards.

116
00:11:39,675 --> 00:11:40,575
What does it mean?

117
00:11:41,025 --> 00:11:46,965
So this allows us that we will
go to conduct intentional.

118
00:11:47,340 --> 00:11:51,060
Fault injection experiments
in the Sandboxed environment.

119
00:11:52,020 --> 00:11:55,740
This allows us to test
resilience of AI system.

120
00:11:56,580 --> 00:12:02,250
Like what happens if an inference
service slows down or if a

121
00:12:02,250 --> 00:12:04,560
prediction pipeline fails.

122
00:12:05,790 --> 00:12:12,510
Every step is reviewed by clinical
experts and before it even go to

123
00:12:12,690 --> 00:12:15,915
pre-production, and the result is.

124
00:12:17,025 --> 00:12:19,095
Better system is strengthening.

125
00:12:19,845 --> 00:12:25,725
For example, we identified a
cascading failure risk in a sepsis

126
00:12:25,755 --> 00:12:32,010
decision model that would have gone
unnoticed without simulated faults.

127
00:12:34,920 --> 00:12:41,805
With a graduated testing approach
and a tight clinical oversight, chaos

128
00:12:41,865 --> 00:12:44,745
engineering becomes not just safe.

129
00:12:46,290 --> 00:12:48,720
It essential for reliability.

130
00:12:50,310 --> 00:12:55,170
Our framework for chaos engineering
practices safely applied to healthcare

131
00:12:55,230 --> 00:13:01,710
AI systems and it revealed how controlled
failure testing identified resilience

132
00:13:01,710 --> 00:13:07,380
gap in prediction pipelines before
they affect production environment.

133
00:13:08,910 --> 00:13:11,545
Organization implemented,
graduated approach.

134
00:13:12,120 --> 00:13:17,730
Beginning with fully isolated
environment using synthetic data as

135
00:13:17,730 --> 00:13:20,970
confidence grew more sophisticated.

136
00:13:20,970 --> 00:13:27,510
Failure models were tested, and always
with multiple layers of safeguards and

137
00:13:27,510 --> 00:13:34,205
clinical experts oversight to prevent
any potential impact to patient care.

138
00:13:39,465 --> 00:13:41,285
Now let's cover the error budgeting.

139
00:13:41,880 --> 00:13:48,689
For clinical AI implications,
error budgeting is another core

140
00:13:48,750 --> 00:13:54,989
SRE principle, and it adapts
beautifully to healthcare ai.

141
00:13:56,459 --> 00:14:01,050
Here we define reliability
targets based on the clinical

142
00:14:01,109 --> 00:14:03,810
impacts of each system component.

143
00:14:05,130 --> 00:14:10,560
For instance, an AI system
generating post visit summaries.

144
00:14:11,295 --> 00:14:17,205
Has lower risk compared to one
recommending oncology treatment.

145
00:14:19,605 --> 00:14:22,125
We are going in a step
by step progression here.

146
00:14:22,545 --> 00:14:28,275
Define reliability targets, then
allocate error budgets, then move on

147
00:14:28,275 --> 00:14:33,195
to implement graduated alerting, which
is a tiered by budget consumption rate.

148
00:14:33,795 --> 00:14:36,615
Then we move to balance
innovation and stability.

149
00:14:37,185 --> 00:14:43,215
Where we adjust deployment pace based
on the budget of the facility, we

150
00:14:43,215 --> 00:14:45,495
distribute error budgets accordingly.

151
00:14:46,665 --> 00:14:51,045
We use safeguard alerting and
aligned deployment pace with

152
00:14:51,045 --> 00:14:53,535
the remaining budget one.

153
00:14:53,535 --> 00:14:54,735
Hospitalist studied.

154
00:14:55,125 --> 00:15:03,975
We saw a 64% reduction in non-actionable
alerts after implementing this approach.

155
00:15:04,305 --> 00:15:05,025
How impressive.

156
00:15:05,775 --> 00:15:06,585
Importantly.

157
00:15:07,035 --> 00:15:13,995
This allowed them to innovate more
rapidly in low risk areas while

158
00:15:13,995 --> 00:15:16,245
safeguarding critical functions.

159
00:15:17,805 --> 00:15:23,145
This tiered approach allows more
aggressive innovations in the low

160
00:15:23,145 --> 00:15:29,265
risk areas, and enforcing is stricter
controls or components directly

161
00:15:29,265 --> 00:15:31,635
influencing clinical decisions.

162
00:15:32,895 --> 00:15:36,105
And as I just told you, it's
very impressive reduction rate.

163
00:15:39,074 --> 00:15:43,844
So now let's discuss how
continuous verification of AI

164
00:15:43,844 --> 00:15:45,974
model in production is required.

165
00:15:46,844 --> 00:15:54,915
We all know that AI is based on the
data and we, nobody can disagree that

166
00:15:54,915 --> 00:15:57,944
the real world data is ever changing.

167
00:15:58,964 --> 00:16:00,790
So unlike traditional softwares.

168
00:16:01,980 --> 00:16:05,280
AI models degrade over time.

169
00:16:08,580 --> 00:16:09,870
So what happens here?

170
00:16:10,290 --> 00:16:15,990
Continuous verification helps
us manage that degradation.

171
00:16:18,060 --> 00:16:24,210
Predeployment, we validate
models against benchmark dataset.

172
00:16:25,740 --> 00:16:30,000
We then use shadow deployments
running new models.

173
00:16:30,915 --> 00:16:38,295
In parallel without affecting live
outcomes to observe their performance.

174
00:16:39,704 --> 00:16:45,165
AI models are living systems,
they degrade, and so we

175
00:16:45,225 --> 00:16:50,025
treat model verification as a
continuous multi-phase pipeline.

176
00:16:51,735 --> 00:16:56,235
The step we took here is all you can
see on this screen, pre-deployment

177
00:16:56,235 --> 00:16:59,355
validation, shadow deployment in parallel.

178
00:17:00,300 --> 00:17:04,770
Canary rollout with rollback
triggers and live drift

179
00:17:04,770 --> 00:17:08,220
monitoring and age case flagging.

180
00:17:09,419 --> 00:17:15,329
Canary release allow controlled
traffic shifts with rollback triggers.

181
00:17:16,379 --> 00:17:23,909
One organization reported a 72%
drop in model related incidents

182
00:17:24,359 --> 00:17:26,754
using this multi-stage verification.

183
00:17:29,460 --> 00:17:34,740
Continuous verification techniques
in AI model enable safe deployment.

184
00:17:36,150 --> 00:17:42,270
And this approach combined traditional
AB testing with healthcare specific

185
00:17:42,270 --> 00:17:48,450
verification steps, including
clinical review panels, and, and we

186
00:17:48,450 --> 00:17:54,150
have seen an impressive reduction
in the incidents Importantly.

187
00:17:54,705 --> 00:17:58,004
We don't just rely on statistics.

188
00:17:59,415 --> 00:18:06,105
We include clinician panels to review
age case scenarios before full rollout.

189
00:18:07,034 --> 00:18:11,355
This ensures safety while
supporting innovation.

190
00:18:13,845 --> 00:18:14,534
Next slide.

191
00:18:14,625 --> 00:18:15,705
We are coming here.

192
00:18:16,064 --> 00:18:21,495
Git Ops for healthcare, AI
compliance and reliability.

193
00:18:22,575 --> 00:18:26,985
I have captured into three broad
segments here, infrastructure and

194
00:18:26,985 --> 00:18:32,655
code, automated deployment pipelines,
and then compliance automation.

195
00:18:35,985 --> 00:18:41,445
Git Ops brings the consistency
of software engineering into

196
00:18:41,445 --> 00:18:43,875
healthcare AI infrastructure.

197
00:18:45,465 --> 00:18:49,845
We define infrastructure as
code and what does that mean?

198
00:18:49,920 --> 00:18:57,180
We versioned infrastructure definitions,
declarative system configurations,

199
00:18:58,680 --> 00:19:04,110
automated compliance validation,
and change history documentation.

200
00:19:06,240 --> 00:19:11,580
We have continuously watching the
continuous integration and continuous

201
00:19:11,580 --> 00:19:13,980
deployment ci cd pipelines here.

202
00:19:15,720 --> 00:19:19,050
This supports regulatory
mandates from hipaa.

203
00:19:20,100 --> 00:19:26,490
High Trust, which is health Information
Trust alliances and GDPR by ensuring

204
00:19:27,090 --> 00:19:29,880
DUPLICABILITY and auditability.

205
00:19:30,900 --> 00:19:36,690
Automated pipelines include clinical
sign-off and auto rollback mechanism.

206
00:19:39,120 --> 00:19:44,070
Automated deployment pipelines also
covers that SLO value violations.

207
00:19:44,220 --> 00:19:47,310
We would be able to
roll back automatically.

208
00:19:47,774 --> 00:19:51,825
Immutable artifact management,
progressive delivery patterns,

209
00:19:52,935 --> 00:19:54,795
PHI, which is protected.

210
00:19:54,795 --> 00:19:59,774
Health information is tightly
controlled and rollback is triggered

211
00:20:00,014 --> 00:20:02,055
if the SLO violation is detected.

212
00:20:03,135 --> 00:20:06,915
Git ops let us deploy changes confidently.

213
00:20:07,725 --> 00:20:10,425
Even in regulated environments.

214
00:20:11,595 --> 00:20:15,615
Several hospitals, networks
are now using tools.

215
00:20:16,320 --> 00:20:21,750
Like Argo CD and Flex
CD for the same purpose.

216
00:20:23,910 --> 00:20:29,790
This is how we are really using
and enhancing the SRE system.

217
00:20:34,290 --> 00:20:39,870
Let's talk about the financial
impact of SRE in healthcare ai.

218
00:20:41,850 --> 00:20:44,285
So that's all the numbers
we are showing here.

219
00:20:45,435 --> 00:20:50,865
Initial investment in observability
and SRE tooling is non-trivial.

220
00:20:51,195 --> 00:20:58,275
Definitely there is a initial, significant
investment is required, but the ROI

221
00:20:58,275 --> 00:21:02,175
comes fast across cases studies.

222
00:21:02,475 --> 00:21:07,005
The average time to break even
was seven and a half month.

223
00:21:07,995 --> 00:21:12,495
Some institution got returns
in as little as four months.

224
00:21:13,965 --> 00:21:19,275
Financial analysis across participating
organizations demonstrates that

225
00:21:19,275 --> 00:21:24,825
despite initial investment healthcare
system achieve substantial returns

226
00:21:25,125 --> 00:21:31,845
within months, there is also
a softer but critical benefit.

227
00:21:31,850 --> 00:21:39,975
IT clinicians trust and satisfaction
improved due to fewer disruptions.

228
00:21:40,590 --> 00:21:44,550
Leading to higher
adoption rate of AI tools,

229
00:21:47,160 --> 00:21:52,740
one large hospital system saved
over $1.2 million annually.

230
00:21:53,370 --> 00:22:01,170
After adopting these practices
and saw a noticeable boost in AI

231
00:22:01,170 --> 00:22:06,720
adoption and clinical satisfaction,
how impressing it is because if

232
00:22:06,780 --> 00:22:08,550
the clinicians are satisfied.

233
00:22:08,940 --> 00:22:13,140
They will be motivated to use the
AI tools, and that is where we need

234
00:22:13,140 --> 00:22:17,730
to make sure that their satisfaction
index continuously grows and

235
00:22:17,730 --> 00:22:19,980
improves through SRE practices.

236
00:22:22,860 --> 00:22:28,110
Here comes the time when we want
to bring all of that together.

237
00:22:28,680 --> 00:22:34,680
If you are thinking of adopting these
practices, here is the blueprint.

238
00:22:36,120 --> 00:22:37,950
First of all, we need to make sure.

239
00:22:38,324 --> 00:22:45,104
That we formulate a cross-functional
team and what the team comprises of

240
00:22:45,614 --> 00:22:51,735
it should have SRE engineers with
healthcare background, clinical

241
00:22:51,735 --> 00:22:59,415
stakeholders, compliance and security
specialist, and then AI ML engineers.

242
00:23:01,215 --> 00:23:03,794
Then we move on to
assessment and planning.

243
00:23:05,264 --> 00:23:07,725
Then these assessments maps out.

244
00:23:08,129 --> 00:23:13,050
Critical system and evaluate
your technical depth.

245
00:23:15,030 --> 00:23:21,389
It is part of system inventory,
current reliability metrics, baseline

246
00:23:23,190 --> 00:23:27,450
compliance, requirement mapping,
and technical depth evaluation.

247
00:23:30,540 --> 00:23:33,629
Third step would be
foundational implementation,

248
00:23:34,230 --> 00:23:35,400
including observability tools.

249
00:23:37,350 --> 00:23:42,840
Service level objectives, their
definitions and continuous

250
00:23:42,990 --> 00:23:48,660
implementation, continuous
deployment, pipeline improvements.

251
00:23:50,520 --> 00:23:56,250
It includes the, and that's the
foundational, because we don't want to

252
00:23:56,250 --> 00:24:01,710
go at a larger scale in the beginning,
and that's very important aspect here.

253
00:24:02,010 --> 00:24:04,110
So we go slow, but we go.

254
00:24:05,745 --> 00:24:11,565
With less and less errors and
more benefits, uh, very soon,

255
00:24:12,764 --> 00:24:19,155
organizations that follows these
phased approach showed significantly

256
00:24:19,185 --> 00:24:25,395
higher reliability scores and smoother
scaling of their AI initiatives.

257
00:24:27,165 --> 00:24:33,285
This actionable blueprint provides a
structured approach for SRE leaders.

258
00:24:34,050 --> 00:24:38,400
Implementing and maintaining
reliable AI systems.

259
00:24:39,570 --> 00:24:43,320
Our research identified that
organization that achieving the

260
00:24:43,320 --> 00:24:47,879
highest reliability score followed
a similar implementation pattern.

261
00:24:48,930 --> 00:24:52,680
Starting from the cross-functional
team formulation and to ensure

262
00:24:52,680 --> 00:24:57,270
both technical and clinical
perspectives are well represented.

263
00:24:58,800 --> 00:25:02,670
The most successful implementation
maintained a phased approach.

264
00:25:03,660 --> 00:25:08,640
Establishing core observability
and measurement capabilities before

265
00:25:08,640 --> 00:25:14,460
advancing to more sophisticated
practices like chaos, engineering

266
00:25:15,360 --> 00:25:17,490
and advanced automation.

267
00:25:20,490 --> 00:25:24,585
So what are the key
takeaways and next steps?

268
00:25:26,970 --> 00:25:29,430
So, healthcare, ai, right?

269
00:25:29,580 --> 00:25:32,160
It needs to be specialized AI approaches.

270
00:25:34,195 --> 00:25:39,540
A standard SRE practice must be
adopted to address the unique

271
00:25:39,540 --> 00:25:44,820
reliability requirements and regulatory
constraints of healthcare environment.

272
00:25:46,410 --> 00:25:52,110
Financial ROI is achieved and it's
significant, but before we come to

273
00:25:52,110 --> 00:25:56,820
the ROI, we also want to make sure
that we go with the cross-functional

274
00:25:57,000 --> 00:26:00,630
corroboration and then we go smart small.

275
00:26:01,020 --> 00:26:05,280
Measurable constantly and then
expand method methodologically.

276
00:26:06,630 --> 00:26:10,770
So the most successful implementation
bridge the gap between the technical

277
00:26:10,770 --> 00:26:17,970
reliability and clinical impacts through
structured collaboration between SRE

278
00:26:17,970 --> 00:26:20,400
teams and healthcare professionals.

279
00:26:25,170 --> 00:26:27,960
The future of AI in healthcare.

280
00:26:28,620 --> 00:26:34,170
Depends on our ability to make
it safe, scalable, and stable.

281
00:26:35,550 --> 00:26:42,780
Despite initial investment requirement,
healthcare organizations constantly

282
00:26:42,870 --> 00:26:50,715
achieved substantial returns through
improved reliability, reduced incidents,

283
00:26:51,629 --> 00:26:54,149
and better resource utilization.

284
00:26:55,379 --> 00:26:57,355
The integration of AI in healthcare.

285
00:26:58,080 --> 00:27:04,050
Continues to accelerate making
reliable operations, not just

286
00:27:04,050 --> 00:27:09,240
a technical consideration, but
patient safety imperatives.

287
00:27:10,350 --> 00:27:15,570
By implementing the SRE practices
outlined in this presentation, healthcare

288
00:27:15,600 --> 00:27:22,530
organizations can reconcile the seemingly
competing demands of rapid AI innovation.

289
00:27:23,115 --> 00:27:30,555
With the stability requirement of critical
care infrastructures, as I told you that

290
00:27:30,555 --> 00:27:34,275
SRE is the crucial part of this journey.

291
00:27:35,265 --> 00:27:35,685
Thank you.

