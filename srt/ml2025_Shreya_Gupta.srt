1
00:00:02,160 --> 00:00:03,690
Hello everyone, and welcome.

2
00:00:03,990 --> 00:00:09,630
My name is Gupta and I'm excited to be
speaking with you today at Con 42, we

3
00:00:09,630 --> 00:00:14,490
are going to dive into a critical and
rapidly evolving area resource allocation

4
00:00:14,490 --> 00:00:16,710
for AI workloads in cloud computing.

5
00:00:17,520 --> 00:00:21,690
The rise of AI applications from deep
learning models to complex machine

6
00:00:21,690 --> 00:00:26,070
learning pipelines has completely changed
the game for cloud resource management.

7
00:00:26,640 --> 00:00:28,470
These aren't your
traditional applications.

8
00:00:28,740 --> 00:00:34,379
They have unique demands, complexities,
and behaviors that require much

9
00:00:34,379 --> 00:00:39,360
more sophisticated strategies for
allocating compute, memory, and

10
00:00:39,360 --> 00:00:42,089
network resources effectively as we'll.

11
00:00:42,089 --> 00:00:45,480
Explore AI workloads are
fundamentally different.

12
00:00:45,840 --> 00:00:50,010
They exhibit significant resource
variability, often requiring

13
00:00:50,010 --> 00:00:52,140
specialized hardware like GPUs.

14
00:00:52,485 --> 00:00:53,355
Or TPOs.

15
00:00:53,655 --> 00:00:58,335
Their performance is highly sensitive
to latency and throughput, and their

16
00:00:58,335 --> 00:01:02,475
resource needs can change dramatically
depending upon the face of execution.

17
00:01:02,865 --> 00:01:05,085
Think training versus inference.

18
00:01:05,565 --> 00:01:10,275
These unique characteristics create
novel challenges that traditional cloud

19
00:01:10,275 --> 00:01:12,555
management systems often struggle with.

20
00:01:13,320 --> 00:01:18,149
This talk, we will examine the convergence
of AI and cloud infrastructure.

21
00:01:18,389 --> 00:01:23,609
Looking critically, how can we effectively
manage resources in this new landscape?

22
00:01:24,609 --> 00:01:26,259
So let's ask this question.

23
00:01:26,679 --> 00:01:28,209
Why is this so important?

24
00:01:28,809 --> 00:01:30,339
Look at the scale and impact.

25
00:01:30,804 --> 00:01:35,274
The global AI infrastructure market
is projected to reach a staggering

26
00:01:35,545 --> 00:01:39,654
2 99 $0.64 billion by 2026.

27
00:01:40,104 --> 00:01:44,244
That's not just not, that's not
just growth, it's explosive growth.

28
00:01:44,604 --> 00:01:49,914
Their annual growth rate at 35.6%
is significantly outpacing that

29
00:01:49,914 --> 00:01:53,455
traditional IT sectors, CAGR since 2021.

30
00:01:54,024 --> 00:01:57,744
This signifies a massive
shift in investment and focus

31
00:01:57,744 --> 00:01:59,244
towards AI capabilities.

32
00:02:00,205 --> 00:02:04,825
But it's not just about spending
more, it's about spending smarter.

33
00:02:05,425 --> 00:02:09,805
Implementing AI driven resource
management itself feels significant.

34
00:02:09,805 --> 00:02:10,464
Benefits.

35
00:02:10,795 --> 00:02:14,485
Industry analysis show that these
intelligent systems can lead

36
00:02:14,485 --> 00:02:16,435
to transformative improvements.

37
00:02:16,795 --> 00:02:21,894
We are seeing potential for up to 40%
improvement in resource utilization,

38
00:02:22,224 --> 00:02:27,745
meaning less wasted capacity through
techniques like ML powered allocation.

39
00:02:28,284 --> 00:02:34,224
Simultaneously, organizations are
documenting 25 to 35% reduction

40
00:02:34,375 --> 00:02:38,935
in operational cost, primarily
infrastructure expenditure, while

41
00:02:39,204 --> 00:02:43,825
maintaining strict performance banks,
benchmark and service level agreements.

42
00:02:43,825 --> 00:02:44,274
Status.

43
00:02:44,784 --> 00:02:49,495
SLS, this dual optimization,
improving efficiency and reducing cost

44
00:02:49,704 --> 00:02:54,834
represents a true paradigm shift in
cloud economics Driven by the very AI

45
00:02:54,834 --> 00:02:56,724
technologies we are trying to support.

46
00:02:57,724 --> 00:03:01,084
To allocate resources effectively,
we first need to understand

47
00:03:01,204 --> 00:03:02,794
what the workload needs.

48
00:03:03,094 --> 00:03:06,844
This is where workload profiling
and resource estimation comes in.

49
00:03:07,234 --> 00:03:12,694
How much C-P-U-G-P-U memory and IO
will a specific AI task require,

50
00:03:12,784 --> 00:03:15,499
and when we have several approaches.

51
00:03:16,189 --> 00:03:18,109
First static analysis.

52
00:03:18,469 --> 00:03:21,529
This involves looking into
the AI model's architecture.

53
00:03:21,559 --> 00:03:25,489
Its hyper parameters and known
pattern before execution.

54
00:03:25,729 --> 00:03:28,729
For established workloads,
this can predict computational

55
00:03:28,729 --> 00:03:33,289
demands with reasonably high
precision, around 80 to 85%.

56
00:03:33,950 --> 00:03:36,349
Second, historic pattern analysis.

57
00:03:36,739 --> 00:03:42,079
Here we analyze past execution,
data telemetry to identify

58
00:03:42,140 --> 00:03:43,879
temporal signal signatures.

59
00:03:44,479 --> 00:03:48,319
Advanced time series modeling and
techniques like wave decomposition

60
00:03:48,499 --> 00:03:51,919
help forecast future requirements
based on past behavior.

61
00:03:52,699 --> 00:03:54,199
Third, online monitoring.

62
00:03:54,559 --> 00:03:56,599
This is about real-time assessment.

63
00:03:56,899 --> 00:04:01,009
We continuously sample critical system
matrix during the runtime itself.

64
00:04:01,339 --> 00:04:05,569
Things like compute, saturations,
memory, bandwidth usage, and IO

65
00:04:05,569 --> 00:04:09,445
throughput to understand the immediate
needs and dynamically adjust.

66
00:04:10,565 --> 00:04:11,945
Last, but not the least.

67
00:04:12,065 --> 00:04:13,265
Hybrid approaches.

68
00:04:13,625 --> 00:04:16,774
Often the best strategies
combines these three methods.

69
00:04:17,015 --> 00:04:20,675
We integrate complementary techniques
to enhance prediction accuracy.

70
00:04:21,005 --> 00:04:25,474
However, it is important to note that
for completely new and unpredictable

71
00:04:25,474 --> 00:04:31,085
workload patterns, accuracy can degrade
perhaps for the 60 to 65% range.

72
00:04:31,955 --> 00:04:34,385
Getting this profile right is crucial.

73
00:04:34,745 --> 00:04:38,825
As noted, significant workload
intelligence systems have led to the

74
00:04:38,825 --> 00:04:45,185
infrastructure cost reductions of up to
35% and utilization improvements of 40%.

75
00:04:45,695 --> 00:04:51,935
The accuracy of our profiling directly
impacts how efficiently and stably we

76
00:04:51,935 --> 00:04:54,304
can operate these next AI workloads.

77
00:04:55,304 --> 00:04:59,984
Okay, so once we have an estimate
of what the resources need, how do

78
00:04:59,984 --> 00:05:03,584
we actually assign those resources
to the workloads waiting to run?

79
00:05:04,124 --> 00:05:07,544
This is the job of the scheduler for ai.

80
00:05:07,634 --> 00:05:10,064
We need intelligence
scheduling mechanisms.

81
00:05:10,635 --> 00:05:12,405
First priority based.

82
00:05:12,735 --> 00:05:14,235
Not all workloads are equal.

83
00:05:14,745 --> 00:05:19,605
ML algorithms can analyze patterns
and history to anticipate needs

84
00:05:19,665 --> 00:05:24,765
and inten intent and intelligently
preempt lower priority tasks to

85
00:05:24,765 --> 00:05:29,055
ensure critical jobs get the resources
they need when they need them.

86
00:05:29,520 --> 00:05:31,080
Second, fair share.

87
00:05:31,349 --> 00:05:34,200
In multi-tenant environments,
we need fairness.

88
00:05:34,530 --> 00:05:39,510
Sophisticated frameworks ensure equitable
resource distribution across different

89
00:05:39,510 --> 00:05:44,040
users or groups while still aiming
for the overall system efficiency.

90
00:05:44,760 --> 00:05:45,510
Deadline aware.

91
00:05:46,140 --> 00:05:51,480
Many AI tasks, especially in production
pipelines have critical time constraints.

92
00:05:52,275 --> 00:05:57,974
The scheduler uses predictive model that
factor in computational complexity, data

93
00:05:57,974 --> 00:06:02,565
throughput, and dependency to ensure
tasks complete within their deadlines.

94
00:06:03,435 --> 00:06:08,205
Optimization resource allocation
is fundamentally a complex,

95
00:06:08,475 --> 00:06:10,695
multi-dimensional optimization problem.

96
00:06:11,055 --> 00:06:15,344
Advanced approximation algorithms are
used to find computationally feasible

97
00:06:15,344 --> 00:06:17,835
solutions that are close to the optimum.

98
00:06:18,105 --> 00:06:23,685
Even if finding the perfect solution is
intractable, often an NP hard problem.

99
00:06:24,750 --> 00:06:28,590
Despite these advancements, the
inherent computational complexity,

100
00:06:28,590 --> 00:06:33,719
the NP heart nature of the optimal
multidimensional allocation means we

101
00:06:33,719 --> 00:06:36,599
often rely on design heuristic approaches.

102
00:06:37,020 --> 00:06:42,270
This aim to balance practical real
world performance with theoretical ality

103
00:06:42,270 --> 00:06:44,789
guarantees it's a constant trade off.

104
00:06:45,789 --> 00:06:48,849
Okay, so AI workloads are rarely static.

105
00:06:49,210 --> 00:06:50,680
Their demands fluctuate.

106
00:06:51,039 --> 00:06:54,610
Autoscaling is the key to
handle this dynamic efficiently.

107
00:06:55,060 --> 00:06:59,650
A typical AI powered autoscaling
architecture involves several layers.

108
00:07:00,070 --> 00:07:03,340
First monitoring layer,
this is the foundation.

109
00:07:03,550 --> 00:07:07,150
It collects detailed telemetry
data across multiple dimensions.

110
00:07:07,435 --> 00:07:13,465
Your CPU, your GPU Memory network
application specific matrix, providing

111
00:07:13,465 --> 00:07:18,145
a rich granular view of the system,
state and resource utilization.

112
00:07:18,685 --> 00:07:21,895
Sophisticated deep learning
models are often used to

113
00:07:21,895 --> 00:07:23,815
analyze these complex patterns.

114
00:07:24,265 --> 00:07:26,365
Second layer is your analysis engine.

115
00:07:26,664 --> 00:07:27,534
This is the brain.

116
00:07:28,015 --> 00:07:33,085
It implements pattern recognization
and decision models often leveraging

117
00:07:33,085 --> 00:07:34,885
reinforcement learning algorithms.

118
00:07:35,245 --> 00:07:35,545
It.

119
00:07:35,874 --> 00:07:39,474
Continuously evaluates historic
performances and real time

120
00:07:39,474 --> 00:07:43,494
systems state to decide when
and how to adjust resources.

121
00:07:43,765 --> 00:07:46,344
Should we scale up,
scale down, or stay Put.

122
00:07:47,539 --> 00:07:49,434
Third execution layer.

123
00:07:49,974 --> 00:07:54,414
This layer translates the decision
made by your analysis engine into the

124
00:07:54,414 --> 00:07:56,694
concrete actions for your infrastructure.

125
00:07:57,085 --> 00:08:00,129
Provisioning new virtual machines,
adjusting container replicas,

126
00:08:00,439 --> 00:08:01,729
modifying resource limits.

127
00:08:03,129 --> 00:08:08,469
While these AI powered auto-scaling
systems showed remarkable capabilities

128
00:08:08,469 --> 00:08:13,059
in control tests achieving prediction
accuracy of often around 95%,

129
00:08:13,449 --> 00:08:16,599
they face significant challenge
in the real world production.

130
00:08:17,049 --> 00:08:18,580
We confronted sorry.

131
00:08:18,760 --> 00:08:22,809
When confronted with unexpected
traffic patterns, anonymous

132
00:08:22,809 --> 00:08:24,730
behaviors or system events.

133
00:08:25,120 --> 00:08:28,240
Prediction accuracy can
deteriorate significantly.

134
00:08:28,539 --> 00:08:31,780
Sometimes it drops to as low as 72%.

135
00:08:32,319 --> 00:08:36,789
This highlights the ongoing need
for robustness and better handling

136
00:08:36,789 --> 00:08:39,069
for unseen for circumstances.

137
00:08:40,069 --> 00:08:44,105
Autoscaling Achieves resource
elasticity, the ability to

138
00:08:44,105 --> 00:08:46,145
adjust resources dynamically.

139
00:08:46,505 --> 00:08:49,055
This is implemented primarily in two ways.

140
00:08:49,670 --> 00:08:52,040
Vertical scaling and horizontal scaling.

141
00:08:52,400 --> 00:08:54,650
Let's first talk about vertical scaling.

142
00:08:55,130 --> 00:08:59,480
This means adjusting the resources
allocated to the existing instances.

143
00:08:59,900 --> 00:09:05,060
Think adding more CPU or GPU or
memory to our running virtual machine.

144
00:09:05,390 --> 00:09:09,110
Using GPU partitioning technologies
to allocate fraction of A GPU,

145
00:09:09,350 --> 00:09:13,250
employing memory ballooning techniques
without super within hypervisors.

146
00:09:13,700 --> 00:09:18,190
These limitations are often the
limitations here are often physical

147
00:09:18,190 --> 00:09:22,245
hardware constraints, operating system
support and application compatibility.

148
00:09:22,865 --> 00:09:28,390
Not all application can handle graceful
and resource changing underneath them.

149
00:09:29,080 --> 00:09:30,725
Second is horizontal scaling.

150
00:09:31,315 --> 00:09:34,975
This involves adding or
removing entire instances.

151
00:09:35,245 --> 00:09:38,515
So remember, vertical scaling
means adjusting the resources

152
00:09:38,515 --> 00:09:40,195
within the existing instances.

153
00:09:40,525 --> 00:09:46,375
Horizontal scaling, we add or remove
the entire in instances common mechanism

154
00:09:46,375 --> 00:09:50,965
of include auto scaling groups that can
manage a pool of identical instances,

155
00:09:51,205 --> 00:09:55,135
replica controllers in orchestration
systems like Kubernetes, and load

156
00:09:55,135 --> 00:09:58,675
balancing mechanisms to distribute
traffic across the available instances.

157
00:09:59,230 --> 00:10:02,350
The main challenge here is
the coordination overhead.

158
00:10:02,920 --> 00:10:07,060
As you scale up out to many instances,
managing them and ensuring that they

159
00:10:07,060 --> 00:10:11,680
work together efficiently introduces
complexity and throughput gains can

160
00:10:11,680 --> 00:10:13,630
diminish beyond a certain point.

161
00:10:14,620 --> 00:10:18,760
These elasticity mechanisms are
absolutely critical for modern ai.

162
00:10:19,360 --> 00:10:23,920
Especially generative AI workloads that
we have today, which can exhibit dramatic

163
00:10:23,920 --> 00:10:29,540
peak andro in resource demands during
different processing or phasing slides.

164
00:10:30,540 --> 00:10:32,280
Okay, so now let's talk about this.

165
00:10:32,730 --> 00:10:37,140
Containers have become a standard way to
package and deploy applications, including

166
00:10:37,140 --> 00:10:42,300
AI, workloads, orchestration platforms,
manage these containers at scale.

167
00:10:43,335 --> 00:10:47,775
Kubernetes has largely emerged as the
prominent orchestration platform for ai.

168
00:10:48,135 --> 00:10:51,675
It provides a robust framework
with several key components,

169
00:10:51,945 --> 00:10:56,025
networking layer, storage, layer
data, plane and control plane.

170
00:10:56,445 --> 00:11:01,365
Your networking layer facilitates seamless
communication between containers, often

171
00:11:01,365 --> 00:11:03,855
using sophisticated overlay networks.

172
00:11:04,065 --> 00:11:08,400
Your storage layer ensures data
persistence across container life cycles

173
00:11:08,670 --> 00:11:11,055
crucial for stateful AI applications.

174
00:11:11,325 --> 00:11:16,575
Often using dynamic volume plugins, data
plane, this is where the work happens.

175
00:11:16,965 --> 00:11:21,855
Components like the ber culet on each
note coordinate with container run times

176
00:11:21,855 --> 00:11:24,705
to execute the workload control plane.

177
00:11:25,245 --> 00:11:29,565
The brains of the operation, the
API Server Intelligence Scheduler,

178
00:11:29,835 --> 00:11:33,860
and Robb Robust controller managers
orchestrate the entire system.

179
00:11:35,445 --> 00:11:39,975
Empirical studies consistently show
Kubernetes outperforming alternates

180
00:11:39,975 --> 00:11:44,025
like Docker Swamp in large scale
deployments, managing thousands of

181
00:11:44,025 --> 00:11:47,445
nodes and containers efficiently,
while maintaining high resource

182
00:11:47,445 --> 00:11:49,335
utilization and workload throughput.

183
00:11:49,815 --> 00:11:52,395
However, this comes at a cost.

184
00:11:52,875 --> 00:11:56,565
Kubernetes introduces significant
architectural complexity

185
00:11:56,565 --> 00:11:57,735
and operational challenges.

186
00:11:58,485 --> 00:12:02,355
Organization needs specialized
expertise and comprehensive training.

187
00:12:02,745 --> 00:12:07,755
It's a high barrier to entry compared
to Simpl solutions, but it ultimately

188
00:12:07,755 --> 00:12:11,745
delivers the superior scalability
and flexibility needed for the

189
00:12:11,745 --> 00:12:13,905
complex AI computing environments.

190
00:12:14,905 --> 00:12:19,735
Underlying much of cloud computing and
resource allocation is virtualization,

191
00:12:20,005 --> 00:12:21,745
abstracting the physical hardware.

192
00:12:22,135 --> 00:12:24,115
Several techniques are vital for ai.

193
00:12:24,745 --> 00:12:30,324
CPU virtualization techniques like
Intel VTX and A MDV provide hardware

194
00:12:30,324 --> 00:12:35,635
assistance to reduce the overhead of
virtualizing the CPU, enabling near native

195
00:12:35,635 --> 00:12:41,185
performance for compute intensive tasks,
memory virtualizations technique like

196
00:12:41,425 --> 00:12:47,215
SLAT, second level address translation,
numa, non-uniform memory access.

197
00:12:47,530 --> 00:12:52,060
Awareness in the hypervisor and
transparent pace sharing improves memory

198
00:12:52,180 --> 00:12:54,880
efficiency and introduce across latency.

199
00:12:55,060 --> 00:12:58,090
Critical for data
intensive AI applications.

200
00:12:58,990 --> 00:13:00,640
Input output virtualization.

201
00:13:01,180 --> 00:13:03,245
Getting data in and
out quickly is crucial.

202
00:13:04,060 --> 00:13:08,080
Technologies like single route I
virtualization, para virtualization

203
00:13:08,080 --> 00:13:12,800
drivers, and direct di direct
device assignment, bypass some of

204
00:13:12,800 --> 00:13:16,280
the virtualizations over it for
network and storage operations.

205
00:13:16,280 --> 00:13:19,430
Enhancing performance GPU virtualization.

206
00:13:20,120 --> 00:13:24,890
Essential for many AI workloads
method range from AI remoting to

207
00:13:24,890 --> 00:13:29,090
hardware assisted partitioning, and
time slicing mechanism that allow

208
00:13:29,300 --> 00:13:34,160
multiple concurrent users or workloads
to share a single physical GPU.

209
00:13:35,810 --> 00:13:37,640
It is important to add a caveat here.

210
00:13:38,240 --> 00:13:42,810
While these advancements offer real
native while these advancements offer

211
00:13:43,110 --> 00:13:47,340
near native performance under optimal
conditions, or for a specific workload

212
00:13:47,340 --> 00:13:51,990
types, performance degradation can
still be significant, particularly for

213
00:13:51,990 --> 00:13:55,080
applications that are highly IO intensive.

214
00:13:55,410 --> 00:13:59,160
The virtualization layer, however,
thin still introduces some overhead.

215
00:14:00,160 --> 00:14:03,400
Ultimately, a major driver
for sophisticated resource

216
00:14:03,400 --> 00:14:06,219
allocation is cost optimization.

217
00:14:06,550 --> 00:14:10,300
How can we run these expensive
AI workloads more economically?

218
00:14:10,750 --> 00:14:12,395
Several strategies are the key here.

219
00:14:12,964 --> 00:14:17,709
First, VM allocation policies
intelligently allocating

220
00:14:17,709 --> 00:14:19,060
virtual machine resources.

221
00:14:19,344 --> 00:14:23,994
Based on the actual need rather than over
promising can yield significant savings,

222
00:14:24,174 --> 00:14:29,844
potentially 30% cost reduction workload
placement, strategically distributing

223
00:14:29,844 --> 00:14:34,134
workloads across different availability
zones or even geographic regions.

224
00:14:34,134 --> 00:14:37,584
Based on the resource availability
and pricing difference can achieve

225
00:14:37,584 --> 00:14:43,434
a saving up to 25% resource right
sizing this wind bots precisely

226
00:14:43,434 --> 00:14:45,114
matching the allocated resources.

227
00:14:45,324 --> 00:14:51,055
CPU memory, GPU to the actual measured
workload requirement, avoiding any waste.

228
00:14:51,444 --> 00:14:54,055
This can contribute another 20% reduction.

229
00:14:54,504 --> 00:14:55,674
Commitment discounts.

230
00:14:56,004 --> 00:15:00,204
Cloud providers offered substantial
discounts, offered 45% or more

231
00:15:00,204 --> 00:15:03,954
for long-term commitments, like
reserved instances or saving plans

232
00:15:04,134 --> 00:15:05,844
compared to on demand pricing.

233
00:15:06,174 --> 00:15:09,174
Leveraging these for a
baseline workload is crucial.

234
00:15:10,269 --> 00:15:13,269
Traditionally, traditional
cloud environments often

235
00:15:13,269 --> 00:15:18,039
operate as a surprisingly low
efficiency, maybe 30 to 45%.

236
00:15:18,579 --> 00:15:22,629
Implementing these optimized
allocation policies can dramatically

237
00:15:22,629 --> 00:15:27,549
increase utilization rates,
pushing them towards 65 to 75%.

238
00:15:28,029 --> 00:15:33,549
However, these figure often represent
idolized scenarios where organizations

239
00:15:33,549 --> 00:15:35,529
can have complete flexibility.

240
00:15:36,099 --> 00:15:38,889
Real world constraints can
limit the achievable sale.

241
00:15:39,504 --> 00:15:40,884
Achievable savings.

242
00:15:41,884 --> 00:15:46,744
Okay, so shifting resources dynamically
in multi-tenant environment introduces

243
00:15:46,744 --> 00:15:48,994
unique security consideration.

244
00:15:49,444 --> 00:15:53,644
Sharing isn't always caring when it
comes to sensitive data and workloads.

245
00:15:54,124 --> 00:15:55,729
Key security aspects include.

246
00:15:57,649 --> 00:15:58,099
Sorry.

247
00:15:58,459 --> 00:15:59,689
Isolation mechanism.

248
00:16:00,109 --> 00:16:02,809
Strong boundaries are
essential Hypervisor.

249
00:16:02,809 --> 00:16:06,439
Security features and kernel level
isolation prevent unauthorized

250
00:16:06,439 --> 00:16:09,529
access between different workloads
running on the same physical

251
00:16:09,529 --> 00:16:12,139
hardware authorization systems.

252
00:16:12,409 --> 00:16:17,154
Robot robust policy engines must govern
who or what can make resource allocation

253
00:16:17,394 --> 00:16:22,249
decisions adhering to the principle of
least privilege, continuous validation.

254
00:16:22,864 --> 00:16:24,935
Security isn't a one-time check.

255
00:16:25,175 --> 00:16:28,534
We need continuous monitoring of
allocation decisions against the

256
00:16:28,534 --> 00:16:33,514
security policy for defense in
depth site channel protection.

257
00:16:34,054 --> 00:16:38,374
When multiple tenants share physical
resources, especially accelerators like

258
00:16:38,374 --> 00:16:42,994
GPUs, there's a risk of information
leakage through subtle timing variations

259
00:16:42,994 --> 00:16:45,814
or cache aspect cache access patterns.

260
00:16:46,294 --> 00:16:48,724
Mitigation against these
attacks are crucial.

261
00:16:49,759 --> 00:16:54,139
So container platforms like Kubernetes
also presents with their own security

262
00:16:54,139 --> 00:16:58,639
challenges when used for AI hardening
measures such as restrictive port

263
00:16:58,639 --> 00:17:03,169
security policies, network policies to
control traffic flow and runtime security

264
00:17:03,169 --> 00:17:05,689
monitoring are necessary additions.

265
00:17:06,109 --> 00:17:11,304
Security must be baked into an allocation
strategy, not bolted on afterwards.

266
00:17:12,304 --> 00:17:15,604
Okay, so now if we have come to our
end of the presentation, let's talk

267
00:17:15,604 --> 00:17:17,644
about where is this field heading.

268
00:17:18,214 --> 00:17:20,224
Several exciting directions are emerging.

269
00:17:20,794 --> 00:17:26,944
First, automated workload characterization
using AI itself, deep learning,

270
00:17:27,214 --> 00:17:31,684
transfer, learning casual inference to
automatically fingerprint, workloads,

271
00:17:31,745 --> 00:17:33,934
adapt quickly to new application types.

272
00:17:34,174 --> 00:17:38,764
And pinpoint resource bottlenecks
without manual analysis, energy aware

273
00:17:38,824 --> 00:17:43,294
allocation, implementing scheduling,
and scaling policies that are

274
00:17:43,294 --> 00:17:45,574
conspicuous of power consumptions.

275
00:17:45,874 --> 00:17:50,884
This includes dynamic voltage, frequency
scaling and consolidating workloads

276
00:17:50,884 --> 00:17:55,054
based on thermal characteristics to
reduce the overall energy footprint.

277
00:17:55,984 --> 00:18:00,244
Carbon aware computing, taking
energy awareness a step further by

278
00:18:00,244 --> 00:18:03,784
scheduling workloads to align with
the availability of clean energy.

279
00:18:04,384 --> 00:18:08,914
This involves integrating with grid
carbon intensity forecast, and renewable

280
00:18:08,914 --> 00:18:12,874
energy production patterns to run
computation when it is the greenest.

281
00:18:13,714 --> 00:18:19,354
In conclusion, integrating AI into
resource management creates a fascinating

282
00:18:19,354 --> 00:18:23,974
meta recursive system, AI optimizing
the infrastructure needed to run ai.

283
00:18:25,099 --> 00:18:30,799
This enables unprecedented automation
and efficiency, but also introduces

284
00:18:31,039 --> 00:18:32,929
novel complexity and challenges.

285
00:18:32,929 --> 00:18:36,259
As we have discussed, this
technical analysis that we have

286
00:18:36,259 --> 00:18:39,949
done today highlights the critical
need for continued research.

287
00:18:40,339 --> 00:18:43,099
We need to address Al, sorry.

288
00:18:43,459 --> 00:18:47,509
We need to address algorithmic
limitations, improve the robustness

289
00:18:47,509 --> 00:18:51,649
of these systems, especially when
facing the unexpected, undeveloped,

290
00:18:51,649 --> 00:18:53,989
standardized benchmarking methodologies.

291
00:18:54,194 --> 00:18:56,864
To objectively compare
different approaches.

292
00:18:57,134 --> 00:19:00,644
The journey of efficiently
powering the AI revolution is

293
00:19:00,644 --> 00:19:02,144
still very much under the way.

294
00:19:03,014 --> 00:19:06,434
I hope this presentation helped
you in a way to understand it.

295
00:19:07,964 --> 00:19:08,504
Thank you.

