1
00:00:00,360 --> 00:00:01,290
Hello everyone.

2
00:00:01,290 --> 00:00:03,960
My name is Max, and today we're
gonna talk about the aggressive

3
00:00:03,960 --> 00:00:08,580
optimization of large scale language
models in order to make sure that we

4
00:00:08,580 --> 00:00:11,130
can make them work on tiny devices.

5
00:00:11,670 --> 00:00:15,750
So let's start with a few words
about us, me, myself, I'm Max Roski.

6
00:00:15,750 --> 00:00:17,940
I'm senior software engineer VGS.

7
00:00:18,610 --> 00:00:23,035
While my cos speaker today is Alexander
Gev, the professor of software engineer

8
00:00:23,035 --> 00:00:25,165
at Lud National Technical University.

9
00:00:25,255 --> 00:00:26,695
Also shout out to my.

10
00:00:27,100 --> 00:00:29,740
Why that helped me arrange this
beautiful presentation here.

11
00:00:29,950 --> 00:00:30,850
So yeah, my thanks.

12
00:00:31,720 --> 00:00:33,070
Okay, let's start from the challenge.

13
00:00:33,160 --> 00:00:38,680
Why actually do we need optimize those
LLMs for big devices By today's standards

14
00:00:38,780 --> 00:00:43,100
the all new large scale variations
of those models actually require

15
00:00:43,100 --> 00:00:47,059
significant competition resources,
which may result, as you may guessed

16
00:00:47,059 --> 00:00:51,559
in, overlooking the possibility of their
practical deployment of low end resource

17
00:00:51,559 --> 00:00:53,210
constrained devices, which may include.

18
00:00:53,749 --> 00:00:57,589
Field operations, offline systems,
and better device and such.

19
00:00:57,960 --> 00:01:02,520
Our resource goal today is to find out
whether the aggressive model optimization

20
00:01:02,520 --> 00:01:06,630
techniques such as architecture
streaming, pruning quantization, can

21
00:01:06,630 --> 00:01:11,910
enable the LMS to operate efficiently
under the extreme hardware constraints.

22
00:01:12,550 --> 00:01:15,550
So as you may guess, our
demonstration approach will be.

23
00:01:16,485 --> 00:01:20,745
To use practical task in order to
evaluate our optimization strategy.

24
00:01:21,385 --> 00:01:26,515
The task will be, we'll try to
fine tune and use our model to

25
00:01:26,515 --> 00:01:30,895
generate ISO compliant software
requirements basically without it.

26
00:01:31,495 --> 00:01:35,905
Not any product can start properly
and finish not only software.

27
00:01:36,125 --> 00:01:41,275
We thought of it as a great task to
evaluate the, and measure the trade

28
00:01:41,275 --> 00:01:45,725
offs between model compression, resource
consumption, and output stability.

29
00:01:47,045 --> 00:01:51,605
Alright, so before we start optimize
pretty much anything else, we need to

30
00:01:51,605 --> 00:01:54,095
find out which model will suit the best.

31
00:01:54,335 --> 00:01:59,015
Beside of the different alternatives
there, we have been looking on the model

32
00:01:59,015 --> 00:02:08,265
that A is open source B is not already
compressed not already compressed, not

33
00:02:08,265 --> 00:02:09,945
any techniques has been applied to it.

34
00:02:10,245 --> 00:02:16,275
So we need something that is kinda blank
sheet, but already with a small size.

35
00:02:16,605 --> 00:02:21,035
So as you may guess from all the
alternatives we stopped on GPT two.

36
00:02:21,485 --> 00:02:26,205
Why as I mentioned before, it's
open source so we can play around as

37
00:02:26,205 --> 00:02:29,685
much as we want without encountering
licensing or API limitations.

38
00:02:29,745 --> 00:02:33,855
Second is relatively lightweight,
both on parameter size with

39
00:02:33,885 --> 00:02:36,075
117 million parameters.

40
00:02:36,375 --> 00:02:42,375
Oh, and by the role model size,
it's only 500 megabytes which is

41
00:02:42,375 --> 00:02:43,965
not that much as you may guess.

42
00:02:44,454 --> 00:02:47,275
Critically also that we have
been looking for the GP D two.

43
00:02:47,995 --> 00:02:53,524
Is really great at generative
possibilities despite the fact

44
00:02:53,524 --> 00:02:57,665
that it's buzz gen and it had
been released in two 29, 20 19.

45
00:02:58,385 --> 00:03:02,255
It's actually really good at producing
coherent, dramatically sound.

46
00:03:02,255 --> 00:03:08,515
The does aligned outputs, so it's actually
very good, real solid for fine tuning

47
00:03:08,515 --> 00:03:10,674
for speci, specifical demands domains.

48
00:03:11,620 --> 00:03:16,979
And finally as a bonus point, pretty
much it's actually have a transparent

49
00:03:16,979 --> 00:03:22,999
and well-documented architecture,
which allows us, allowed us to dive

50
00:03:22,999 --> 00:03:27,949
deep into finding out the limitations
of those optimization techniques.

51
00:03:28,249 --> 00:03:29,869
So it was actually great.

52
00:03:30,389 --> 00:03:33,959
So to sum up, if we had only one
model to carry with us into forest

53
00:03:34,559 --> 00:03:36,839
on an old laptop, it'll be GPT two.

54
00:03:38,079 --> 00:03:39,129
Let's start with the plan.

55
00:03:39,249 --> 00:03:41,379
So it'll be really simple one.

56
00:03:41,589 --> 00:03:42,609
First of all, we'll go.

57
00:03:42,669 --> 00:03:47,679
We are going to cover up the theory, then
the practical results and conclusion.

58
00:03:48,609 --> 00:03:50,149
So let's start with theory.

59
00:03:51,749 --> 00:03:54,749
First of all, we are going to look for.

60
00:03:55,094 --> 00:03:56,594
It's a student model concept.

61
00:03:56,594 --> 00:04:02,934
The core idea basically student model
is a instance of the basic model

62
00:04:03,084 --> 00:04:06,984
by, but the difference that you can
play around as much as you want.

63
00:04:06,984 --> 00:04:08,694
So basically you can screw it up.

64
00:04:09,204 --> 00:04:13,329
Maybe you will need to redo the things,
but it won't affect the original.

65
00:04:13,329 --> 00:04:19,924
So consider this something like a clone,
but you can work with as much as you want.

66
00:04:20,899 --> 00:04:23,539
So yeah, that's pretty much it.

67
00:04:24,469 --> 00:04:27,919
Metrics by which we are
going to measure the success.

68
00:04:28,099 --> 00:04:29,119
It's four of them.

69
00:04:29,419 --> 00:04:33,109
First of all, perplexity, the
model accuracy, whether can

70
00:04:33,109 --> 00:04:34,969
it output things correctly.

71
00:04:35,389 --> 00:04:40,669
CPU memory usage, as you may guess, it
will be really critical for measuring

72
00:04:40,669 --> 00:04:45,114
success for like of performance
on low resource, low end devices.

73
00:04:45,984 --> 00:04:49,944
Inter interference speed, faster
response time, pretty straightforward

74
00:04:50,004 --> 00:04:51,864
and model size basically.

75
00:04:51,864 --> 00:04:55,704
The smaller, the better general
pattern here, as you noticed is

76
00:04:55,744 --> 00:05:00,544
the smaller better, the smaller
faster equals better usually.

77
00:05:02,390 --> 00:05:05,930
Let's start with defining what
we actually define as aggressive

78
00:05:05,930 --> 00:05:08,330
methods and non-aggressive methods.

79
00:05:08,930 --> 00:05:11,930
Let's start with non aggressive
methods, because there are really

80
00:05:12,109 --> 00:05:15,515
only one that we can specify
today is a knowledge distillation.

81
00:05:16,204 --> 00:05:22,594
Is the technique when student mimics
teacher tissue model closely, like without

82
00:05:22,594 --> 00:05:24,484
any changes at all to architecture.

83
00:05:24,844 --> 00:05:28,064
While aggressive methods, as you
may guess, is those methods there

84
00:05:28,544 --> 00:05:34,724
will interfere with model like
architecture layers, count and stuff.

85
00:05:34,784 --> 00:05:36,104
So there are three of them.

86
00:05:36,194 --> 00:05:40,425
Architecture, treatment they will
have, like this technique is all

87
00:05:40,425 --> 00:05:42,825
around reducing layers in neuro count.

88
00:05:43,170 --> 00:05:48,090
Pruning, remove less important
parameters in weights and ization.

89
00:05:48,420 --> 00:05:52,230
Basically changing the AC accuracy
of the weights from like fourth

90
00:05:52,410 --> 00:05:54,810
float 42 into integer eight.

91
00:05:55,570 --> 00:05:55,960
Standard.

92
00:05:57,085 --> 00:05:57,745
Okay, cool.

93
00:05:57,775 --> 00:06:02,755
So let's start with covering up the
basics of the each optimization technique.

94
00:06:02,905 --> 00:06:06,085
So with architecture, treatment,
the main concept here, if it's

95
00:06:06,085 --> 00:06:07,795
too big, just make it smaller.

96
00:06:08,465 --> 00:06:12,305
So I would say, you should imagine
this, our model as a building.

97
00:06:12,365 --> 00:06:17,945
So like every floor and room can be
considered as layer and neurons here.

98
00:06:18,545 --> 00:06:22,175
And pretty much we just, in
those technique, we just remove

99
00:06:22,175 --> 00:06:23,795
unnecessary layers and neurons.

100
00:06:25,295 --> 00:06:25,805
That's it.

101
00:06:26,685 --> 00:06:29,955
With layers, we, when we remove
layers, we just reduce the number

102
00:06:29,955 --> 00:06:31,905
of protesting stages in the model.

103
00:06:32,565 --> 00:06:37,935
While with neurons, we decrease the number
of computational euros per layer, right?

104
00:06:38,025 --> 00:06:41,305
So also we have a glossary
on the bottom of the slides.

105
00:06:41,305 --> 00:06:44,785
So feel free to pause the window
and check it out, like the better

106
00:06:44,785 --> 00:06:46,315
definition of layer in euro.

107
00:06:46,975 --> 00:06:47,875
So that's pretty much it.

108
00:06:48,655 --> 00:06:49,315
Pruning.

109
00:06:49,555 --> 00:06:51,715
If it's not important, just cut it off.

110
00:06:51,995 --> 00:06:55,655
I think we can imagine for this
purpose, we can imagine a model like

111
00:06:55,655 --> 00:07:01,735
a tree when every branch is connection
between neurons and even each branch has

112
00:07:01,735 --> 00:07:04,165
leaves, which is parameters and weights.

113
00:07:04,285 --> 00:07:09,055
So with pruning, we can actually
decide which weights we can

114
00:07:09,265 --> 00:07:12,325
disable and just not use it all.

115
00:07:12,595 --> 00:07:16,995
Interesting thing here that while
we disabling and turning off some

116
00:07:16,995 --> 00:07:19,215
parameters and weights, it also can.

117
00:07:19,605 --> 00:07:25,915
Like indirectly affect the neurons because
when you delete and turn off some of the

118
00:07:25,915 --> 00:07:32,055
neurons some of the weights in parameters,
it may just turn off some neurons.

119
00:07:32,055 --> 00:07:33,585
So it's really interesting here and here.

120
00:07:34,395 --> 00:07:35,175
Quantization.

121
00:07:35,325 --> 00:07:39,135
Basically the main concept here is you
may see heat precision isn't critical.

122
00:07:39,315 --> 00:07:41,475
We do not need use heavy tools.

123
00:07:42,015 --> 00:07:44,515
Basically speaking we are turning off.

124
00:07:45,135 --> 00:07:51,325
Like changing the basic float 32
standard and like into integer eight.

125
00:07:51,955 --> 00:07:56,365
So it will be less, our model
will be less precise, but it'll

126
00:07:56,365 --> 00:08:00,895
be great in like CPU performance
and much more easier to work with.

127
00:08:01,645 --> 00:08:01,914
Yeah.

128
00:08:01,914 --> 00:08:06,075
So we just make sure that our
weights are, turned, turned

129
00:08:06,075 --> 00:08:07,995
into smaller, lightweight ones.

130
00:08:08,325 --> 00:08:11,445
Those effects, as you may see
on the graph, it affects only

131
00:08:11,445 --> 00:08:12,615
weights and parameters here.

132
00:08:15,255 --> 00:08:18,545
And for the non-aggressive
optimization methods here

133
00:08:18,775 --> 00:08:23,785
distillation can be considered as a
mutation of the teacher model here.

134
00:08:24,475 --> 00:08:27,325
So it's the student that copies
from the experienced teacher.

135
00:08:27,955 --> 00:08:29,305
Very important note here.

136
00:08:29,815 --> 00:08:33,865
This thing, this method is non
aggressive because it does not

137
00:08:33,865 --> 00:08:37,175
interfere with model architecture.

138
00:08:37,775 --> 00:08:42,935
Therefore, we do not need to be worried
about performance de degradation here.

139
00:08:43,535 --> 00:08:43,865
Good.

140
00:08:46,625 --> 00:08:51,015
Also the most important know that
we want to live here is that.

141
00:08:51,520 --> 00:08:54,400
Distillation is really, will work on it.

142
00:08:54,400 --> 00:08:59,410
It'll work good if only the
student model has not anything

143
00:08:59,410 --> 00:09:00,760
changed in the architecture.

144
00:09:00,760 --> 00:09:05,680
So the, both the student model and teacher
one should be pretty much the same.

145
00:09:07,510 --> 00:09:11,150
And here, as you may see in the
graph it's not affecting anything

146
00:09:11,150 --> 00:09:15,140
in the architecture, but it
just learns the outputs and the

147
00:09:15,140 --> 00:09:16,430
weights from the tissue once.

148
00:09:16,430 --> 00:09:17,770
So it's all in once.

149
00:09:19,285 --> 00:09:19,735
Okay, cool.

150
00:09:19,735 --> 00:09:23,805
So as you, we noticed those three
aggressive methods, architecture,

151
00:09:23,805 --> 00:09:29,895
dreaming, proving quantization
are playing with models,

152
00:09:30,075 --> 00:09:32,205
architecture in different way.

153
00:09:32,295 --> 00:09:35,625
One in, for instance, for
archite architecture dreaming.

154
00:09:35,625 --> 00:09:39,945
We just remove some of the layers
while pruning, for instance, just

155
00:09:39,945 --> 00:09:41,775
remove not layers, but weights.

156
00:09:42,315 --> 00:09:45,795
And quantization just reducing
the accuracy of those weights

157
00:09:45,795 --> 00:09:46,695
but not removing them.

158
00:09:46,905 --> 00:09:50,145
So they're really the
same, but in different way.

159
00:09:50,865 --> 00:09:53,715
Alright, so yeah, that's the
summary for you if you want to

160
00:09:53,715 --> 00:09:54,915
pass out video and check it out.

161
00:09:56,355 --> 00:09:58,995
So let's start with part
two, practical research.

162
00:09:59,815 --> 00:10:03,085
As you may guessed, we already
have an order of experiments here.

163
00:10:03,145 --> 00:10:08,335
So first of all we will our, like this
part will switch in several steps.

164
00:10:08,635 --> 00:10:11,725
First one, we are gonna fine tune our
model because without fine tuning,

165
00:10:11,725 --> 00:10:13,375
it does not make much many sense.

166
00:10:13,965 --> 00:10:17,655
And therefore we'll continue with
architecture, treatment, distillation,

167
00:10:17,865 --> 00:10:21,045
pruning, quantization, and then
we're gonna combine those methods.

168
00:10:21,675 --> 00:10:26,485
We are gonna explain later why we do
this methods in this specific order.

169
00:10:26,545 --> 00:10:27,265
So let's start.

170
00:10:28,235 --> 00:10:32,405
But before we start, we need
to get the basic benchmark of

171
00:10:32,485 --> 00:10:35,155
training the base role, DPT two.

172
00:10:35,785 --> 00:10:37,045
So it's pre-trained.

173
00:10:37,390 --> 00:10:42,730
Non-specialized and it'll struggle with
any task specific like structure there.

174
00:10:43,190 --> 00:10:47,210
Those, some metrics on the left that we
specify before, perplexity usage, memory

175
00:10:47,210 --> 00:10:49,160
usage, inference, speed, and model size.

176
00:10:49,520 --> 00:10:52,100
And therefore they are prompt results.

177
00:10:52,100 --> 00:10:57,530
So you can see like visually what is
going on, as you may see on the output.

178
00:10:57,770 --> 00:11:01,940
It's not hallucinating, I would
say, but it's not that semantically.

179
00:11:01,940 --> 00:11:02,660
Correct.

180
00:11:03,330 --> 00:11:06,690
Which may result in, the possibility
and the requirement of actually fine

181
00:11:06,690 --> 00:11:11,705
tuning our model to do specific tasks
like iso requirements generation.

182
00:11:14,500 --> 00:11:14,890
Okay.

183
00:11:14,970 --> 00:11:19,900
I think the first step after comparing
with the role GPT two we need to

184
00:11:19,900 --> 00:11:22,270
fine tune the model basically.

185
00:11:22,630 --> 00:11:23,140
I'm sorry.

186
00:11:23,560 --> 00:11:27,460
Basically this model will, there
will no optimization applied first.

187
00:11:28,150 --> 00:11:31,600
Our model is fine tuned towards ISO
compliance software requirements

188
00:11:31,600 --> 00:11:36,190
that I set and it'll useful
GPTT architecture and it'll the

189
00:11:36,190 --> 00:11:37,780
reference point for our comparisons.

190
00:11:37,780 --> 00:11:41,340
As you may see, after we fine tune
it to our, to to our specific task,

191
00:11:41,340 --> 00:11:42,780
the output is much more better.

192
00:11:43,320 --> 00:11:45,860
So that's I would say reference point.

193
00:11:47,150 --> 00:11:47,600
Yeah.

194
00:11:47,660 --> 00:11:51,110
So with fine tune, with fine tuning
model, everything got better there.

195
00:11:51,625 --> 00:11:54,475
Only perplexity got a little bit worse.

196
00:11:54,565 --> 00:12:00,115
I would say perplexity value starting
from one up to one and five are really

197
00:12:00,175 --> 00:12:06,145
solid, and we'll provide you the quality
of the model that we're, that you will

198
00:12:06,145 --> 00:12:08,935
see that is good visually at least.

199
00:12:09,415 --> 00:12:11,155
Okay, so let's start with the first step.

200
00:12:11,155 --> 00:12:13,975
I think the most mandatory
one, architecture dreaming.

201
00:12:14,055 --> 00:12:18,315
In our practical research, we remote,
we removed half of the GPT two layers.

202
00:12:18,855 --> 00:12:20,265
And reduce hidden size.

203
00:12:20,535 --> 00:12:28,425
Basically, we shifted from 12 layers
to six from like 768 dimensions

204
00:12:28,425 --> 00:12:34,905
to 384, which means same inputs,
but just lighter architecture

205
00:12:34,935 --> 00:12:36,705
and fighter and faster execution.

206
00:12:37,185 --> 00:12:40,455
You may also ask about how did
we pick which layers to trim?

207
00:12:40,895 --> 00:12:43,355
The strategy there was
actually really simple one.

208
00:12:44,075 --> 00:12:46,100
We just removed layers
symmetrically from center.

209
00:12:46,725 --> 00:12:50,565
Which allows us to preserve bottom
layers that are responsible for basic

210
00:12:50,565 --> 00:12:52,215
syntax and token level understanding.

211
00:12:52,485 --> 00:12:58,275
And also we preserve top layers, which
are for higher level structure, for

212
00:12:58,275 --> 00:13:00,585
context, you know what I'm saying?

213
00:13:01,375 --> 00:13:04,825
Basically speaking, this allow
allowed us to maintain both low level

214
00:13:04,825 --> 00:13:06,175
and high level processing there.

215
00:13:06,625 --> 00:13:07,075
Awesome.

216
00:13:08,455 --> 00:13:11,575
So let's compare the performance here.

217
00:13:12,055 --> 00:13:14,065
As you may see, perplexity
gone a little bit worse.

218
00:13:15,050 --> 00:13:16,160
Because like it's gone.

219
00:13:16,250 --> 00:13:23,030
Not that accurate, but as you may see,
the output to the prompt is really solid.

220
00:13:23,180 --> 00:13:23,990
It's pretty much the same.

221
00:13:24,260 --> 00:13:25,130
It hasn't changed.

222
00:13:25,460 --> 00:13:30,050
But here we also notice here that
the memory usage actually increased.

223
00:13:30,500 --> 00:13:30,890
Why?

224
00:13:30,940 --> 00:13:31,720
When we.

225
00:13:31,795 --> 00:13:33,985
We reduce our CP usage.

226
00:13:34,255 --> 00:13:37,705
Somebody has to pay and
this somebody is wrong.

227
00:13:38,455 --> 00:13:39,415
Basically speaking.

228
00:13:39,515 --> 00:13:44,315
We with optimizations like quantization
layer treatment, we reduce processing

229
00:13:44,315 --> 00:13:51,065
time, but they will compensate in tensor
sizes, caching, parallel processing,

230
00:13:51,095 --> 00:13:54,215
which will increase their wrong memory.

231
00:13:55,035 --> 00:13:56,115
Why this matters.

232
00:13:56,770 --> 00:14:01,540
CPU time equals the energy cost
on battery powered by the devices.

233
00:14:02,080 --> 00:14:05,380
And most of the time CPU U
is the biggest power draw.

234
00:14:05,440 --> 00:14:10,920
While RO is something that we
can neglect and it's generally

235
00:14:10,920 --> 00:14:13,640
more available, but not limited.

236
00:14:13,790 --> 00:14:16,940
So it's actually, we consider
it as a cheap to access.

237
00:14:17,565 --> 00:14:20,865
But a little bit limited on the
old laptops and microcontrollers,

238
00:14:20,895 --> 00:14:22,425
which is, in our use case.

239
00:14:22,425 --> 00:14:29,865
It's not that problematic as we more lean
towards providing the proper experience

240
00:14:29,865 --> 00:14:33,375
with CPU in order to make sure that
they can work on embedded devices.

241
00:14:35,655 --> 00:14:38,985
Yeah, so in this graph, it's pretty
much the summary of all the things

242
00:14:38,985 --> 00:14:40,965
that we talked before with the graphs.

243
00:14:40,965 --> 00:14:46,210
As you as you may see on the x. Axis we
have starting from the Rob GT two and at

244
00:14:46,210 --> 00:14:50,170
the end you will, we will compare also
combine sensors there, the RAM users

245
00:14:50,170 --> 00:14:52,930
there changed by almost 200 megabytes.

246
00:14:52,990 --> 00:14:55,390
Not that big, but something
that we have to consider on.

247
00:14:55,570 --> 00:14:56,830
Okay, let's continue.

248
00:14:57,550 --> 00:15:01,060
You may also ask, but why
don't we apply distillation

249
00:15:02,020 --> 00:15:04,300
and therefore it's break point.

250
00:15:05,200 --> 00:15:08,120
The thing is that we student
modeled it's trained.

251
00:15:08,630 --> 00:15:12,230
In this process, the student model trained
to mimic, make a bigger teacher model.

252
00:15:12,860 --> 00:15:17,690
But after we chopped hard, the
student model it just too small

253
00:15:17,690 --> 00:15:18,590
to understand the teacher.

254
00:15:19,700 --> 00:15:25,060
It means that student just can does not
have enough layers to understand teacher

255
00:15:25,540 --> 00:15:30,920
outputs, which means that our student
model will try to compensate it and it'll

256
00:15:30,920 --> 00:15:35,000
result in hallucinations and repetitions.

257
00:15:36,555 --> 00:15:40,695
Yeah, this graph pretty much
basically represents the effect that

258
00:15:40,695 --> 00:15:42,105
we are trying to represent here.

259
00:15:42,495 --> 00:15:48,075
While at some point, student model
will just stop, basically understand

260
00:15:48,075 --> 00:15:52,035
what model tries to learn and to teach.

261
00:15:52,515 --> 00:15:57,435
So it'll do the only thing it's possible
to do with architecture when LLMs.

262
00:15:57,750 --> 00:15:59,130
Hallucination and repetition.

263
00:15:59,130 --> 00:15:59,970
As we mentioned before.

264
00:16:00,690 --> 00:16:04,490
As a result, we finishing critical
failure point and here there's

265
00:16:04,490 --> 00:16:08,810
nothing we can do with distillation
and we need to drop it off.

266
00:16:09,710 --> 00:16:10,160
Yeah.

267
00:16:10,280 --> 00:16:14,260
As you may see with a performance
snapshot for distillation, the

268
00:16:14,260 --> 00:16:21,570
output is straight up non-usable
with propensity well is for it's.

269
00:16:22,140 --> 00:16:24,810
As you may see by output, it's not usable.

270
00:16:25,140 --> 00:16:27,090
So yeah, we just skip it all together.

271
00:16:27,690 --> 00:16:30,780
That's why it's called non
aggressive, as you may see.

272
00:16:30,830 --> 00:16:33,980
As a quick conclusion here, non
aggressive methods are not working

273
00:16:33,980 --> 00:16:35,360
right with aggressive ones.

274
00:16:35,510 --> 00:16:35,840
Okay?

275
00:16:37,220 --> 00:16:37,580
Yeah.

276
00:16:37,610 --> 00:16:39,410
So let's continue with pruning.

277
00:16:39,860 --> 00:16:40,400
With pruning.

278
00:16:40,430 --> 00:16:43,790
As we mentioned before, the theoretical
part, really more parameters

279
00:16:43,790 --> 00:16:45,770
that have low or no contribution.

280
00:16:46,310 --> 00:16:48,050
How do we choose those weights?

281
00:16:48,260 --> 00:16:52,140
We just rank all weights by
getting their absolute value,

282
00:16:52,230 --> 00:16:53,700
then just remove the one one.

283
00:16:53,760 --> 00:16:55,290
Yeah, it's pretty much simple.

284
00:16:55,290 --> 00:16:56,430
It's not that hard.

285
00:16:57,050 --> 00:17:01,490
We just muting the quietest voices
in the nose, noisy room, and why it's

286
00:17:01,490 --> 00:17:05,660
working, because many weights are
in large, more are close to zero,

287
00:17:05,660 --> 00:17:07,370
which allows them to balance out.

288
00:17:07,800 --> 00:17:12,660
Some like context and for the larger
models, it'll be really great in

289
00:17:12,660 --> 00:17:14,130
order to have general context.

290
00:17:14,580 --> 00:17:20,020
But in our use case, the le less memory,
faster competition, but only slightly.

291
00:17:21,070 --> 00:17:24,460
Yeah, from the benchmark here,
perplexity here is pretty much the

292
00:17:24,460 --> 00:17:25,955
same as for architecture training.

293
00:17:26,330 --> 00:17:27,860
But the memory, use it really hard.

294
00:17:28,020 --> 00:17:33,870
Like going really like big, I would
say as you may see it sometimes it

295
00:17:33,870 --> 00:17:39,140
hallucinates and just does not do
anything, but no worries will get better

296
00:17:39,140 --> 00:17:41,540
results after we combine those methods.

297
00:17:42,110 --> 00:17:46,780
Quantization quantization, as we said
before in theoretical part, when we

298
00:17:46,780 --> 00:17:51,640
convert them, model weights, accuracy from
flow through the two into enter eight.

299
00:17:52,280 --> 00:17:56,120
We make sure, like we don't
delete anything but just make the

300
00:17:56,120 --> 00:17:57,740
model a little bit less precise.

301
00:17:58,100 --> 00:18:04,490
So here we'll get a huge benefit
of less CPU load and basically

302
00:18:04,490 --> 00:18:05,840
no work actual changes needed.

303
00:18:05,840 --> 00:18:08,480
So pretty much no performance
degradation there.

304
00:18:10,490 --> 00:18:14,390
Okay, so when we go to performance,
you will see that the memory

305
00:18:14,390 --> 00:18:18,370
usage there is pretty much
almost non didn't change at all.

306
00:18:19,720 --> 00:18:25,810
While the model size is produced
drastically with the CPU usage at 12%,

307
00:18:25,870 --> 00:18:31,630
12 and a half percent, while basic
RGBT model will occupy like 45% of your

308
00:18:31,660 --> 00:18:34,240
like low end device, which is huge.

309
00:18:34,330 --> 00:18:39,760
And you'll see, and you'll see you later
see by, see it by yourself, by at the end

310
00:18:39,760 --> 00:18:41,950
of the talk while we get the conclusion.

311
00:18:42,730 --> 00:18:47,400
So we are coming up to the combined
optimization here and, in those

312
00:18:47,430 --> 00:18:50,730
optimization methods, we are combining
treatment, pruning and quantization.

313
00:18:51,060 --> 00:18:56,760
So at the end, we'll have fine tuned
model, fine tuned, GPT model for one,

314
00:18:56,760 --> 00:19:04,080
task, three model to six layers, prune 40%
of smaller weights, and we'll quantize.

315
00:19:04,920 --> 00:19:08,550
Two in eight from flow through
the two for CPU Efficiency.

316
00:19:09,150 --> 00:19:12,530
As as you may see, we don't use
distillation at all because it just

317
00:19:12,530 --> 00:19:17,440
does not stack neither work with
aggressive methods combined about in

318
00:19:17,440 --> 00:19:20,590
this case, there are no methods that
will allow distillation work with

319
00:19:20,590 --> 00:19:22,780
aggressive methods as architecture.

320
00:19:22,780 --> 00:19:24,070
Defense is too drastic.

321
00:19:25,045 --> 00:19:29,065
Okay, so with the performance snapshot
of our best combined methods here,

322
00:19:29,335 --> 00:19:31,415
perplexity changed by really little bit.

323
00:19:31,475 --> 00:19:36,405
While CPU usage there got almost
twice as small while memory usage,

324
00:19:36,405 --> 00:19:41,335
as you may see, is almost like 200
megabytes higher with inference

325
00:19:41,335 --> 00:19:43,585
speed really perfect, almost newer.

326
00:19:43,585 --> 00:19:44,515
Perfect, I would say.

327
00:19:44,605 --> 00:19:49,495
And the model size shrunk into like
almost five times of the original.

328
00:19:50,365 --> 00:19:51,835
Yeah, it's really perfect.

329
00:19:51,835 --> 00:19:52,195
One.

330
00:19:52,520 --> 00:19:56,380
So let's see how it's gonna
represent in different metrics.

331
00:19:57,410 --> 00:20:02,020
So we'll we can see here by using
the company combined optimization

332
00:20:02,020 --> 00:20:06,650
methods those, our students, our
student model will reduce CPU load by

333
00:20:06,650 --> 00:20:09,890
almost a half compared to row G PT two

334
00:20:12,480 --> 00:20:15,870
which makes it significantly more
suitable for battery powered CPU only

335
00:20:15,870 --> 00:20:18,480
devices where energy is critical.

336
00:20:19,680 --> 00:20:23,850
Basically it means less CPU, faster
response times, lower energy costs,

337
00:20:24,180 --> 00:20:26,700
everyone's happy memory usage.

338
00:20:27,080 --> 00:20:32,040
As you may see, the change there is
not that big, but it's a kind of trade

339
00:20:32,040 --> 00:20:35,310
off that we pay for lower CPU costs.

340
00:20:35,730 --> 00:20:40,200
It's acceptable, especially in Muslim
low end laptops and body boards.

341
00:20:40,200 --> 00:20:44,520
But this, there is the thing that we
should consider of a memory usage.

342
00:20:44,640 --> 00:20:47,400
This is the thing that pays
off for the all optimizations.

343
00:20:48,705 --> 00:20:50,355
Perplexity actually remained the same.

344
00:20:50,925 --> 00:20:52,875
So we were really happy with the results.

345
00:20:53,595 --> 00:20:59,415
So it means at the end we can conclude
that the optimization, the combining

346
00:20:59,415 --> 00:21:05,235
optimization techniques are not equal
degradation if fine tuning done right,

347
00:21:06,285 --> 00:21:10,685
model size as you may see it shrink
almost in five times of the original,

348
00:21:11,015 --> 00:21:12,725
basically smaller model, same brain.

349
00:21:14,525 --> 00:21:16,625
We are going to part three conclusions.

350
00:21:16,985 --> 00:21:20,695
So you might also thought if their
methods are so good, why should

351
00:21:20,695 --> 00:21:22,345
we like not always use them?

352
00:21:23,305 --> 00:21:28,645
It me, like basically we omitted
the fact that it's work perfectly

353
00:21:28,705 --> 00:21:31,045
only on fine tuned models.

354
00:21:31,645 --> 00:21:35,665
So actually why fine tuning map
like fine tuning technique matters.

355
00:21:36,200 --> 00:21:39,110
Fine tuning aligns the model
to a single atomic task.

356
00:21:39,410 --> 00:21:45,780
Therefore, that specific focus make the
model resistant to aggressive compression

357
00:21:46,230 --> 00:21:50,700
While out of the box models are kinda
generalist and they're like not good in

358
00:21:50,700 --> 00:21:52,380
anything else, like jack of all trades.

359
00:21:52,710 --> 00:21:56,730
And they basically, they're trying to
do everything and they will be really

360
00:21:56,730 --> 00:21:59,370
fragile towards like applying any of them.

361
00:22:00,540 --> 00:22:00,960
Yeah.

362
00:22:02,220 --> 00:22:07,490
So with the League of Mind,
Alexandra, we defined definition

363
00:22:07,490 --> 00:22:09,350
of Reist model resistance.

364
00:22:09,410 --> 00:22:14,390
It means that it defines how well
model maintains output quality

365
00:22:14,390 --> 00:22:15,830
after aggressive compression.

366
00:22:16,420 --> 00:22:20,920
So by high resistance, we define
fine tuned models for one clear

367
00:22:20,920 --> 00:22:25,260
task, which means strong internal
signal, the translation to low

368
00:22:25,260 --> 00:22:27,360
dependency on full architecture.

369
00:22:27,675 --> 00:22:32,865
When quality drops really slowly, even
heavy pruning on quantization and for

370
00:22:32,865 --> 00:22:37,305
the lower resistance, we define this
and the, as the general purpose model

371
00:22:37,365 --> 00:22:43,075
that has like many overlapping skills
and even the smallest optimization there

372
00:22:43,135 --> 00:22:47,965
would result into breaking the visual
results and the performance metrics.

373
00:22:49,855 --> 00:22:53,625
Also, we have a graph to we also
applied the same methods to the raw

374
00:22:53,625 --> 00:22:55,785
G PT two, and the results were not.

375
00:22:56,625 --> 00:22:57,645
Usable at all.

376
00:22:58,125 --> 00:23:02,385
So as you may see for the fine
tuned models it's almost changed

377
00:23:02,435 --> 00:23:04,175
like it hasn't changed at all.

378
00:23:04,235 --> 00:23:07,115
While for BT two it
changed really drastically.

379
00:23:07,175 --> 00:23:10,925
So as you may see, the re resistance
team here is really important.

380
00:23:11,405 --> 00:23:15,625
So fine tuning models here
is really important by as

381
00:23:15,835 --> 00:23:17,845
treatment stable perplexity.

382
00:23:18,190 --> 00:23:22,780
Almost grow slowly or not growing at
all while general purpose models break

383
00:23:22,780 --> 00:23:25,690
fast and perpetuate their skyrockets.

384
00:23:26,480 --> 00:23:27,530
So it's, it is great.

385
00:23:27,530 --> 00:23:31,460
While fine tuning mon, like fine
tuning models allows us to focus on

386
00:23:31,460 --> 00:23:37,020
the single task which is kinda reducing
general noise and general models

387
00:23:37,200 --> 00:23:39,060
in the controversy are too broad.

388
00:23:39,090 --> 00:23:41,520
Even small cuts will disrupt everything.

389
00:23:42,090 --> 00:23:44,220
So basically speaking optimizations.

390
00:23:44,520 --> 00:23:47,670
Combining the optimization
methods work best, but the model

391
00:23:47,670 --> 00:23:49,710
knows what it's supposed to do.

392
00:23:51,420 --> 00:23:51,960
Yeah.

393
00:23:53,310 --> 00:23:57,360
Without fine tuning, pruning will
break meaning of general model.

394
00:23:57,510 --> 00:24:01,890
Quantization will introduce
noise, which also affect the

395
00:24:01,950 --> 00:24:03,690
visual represe of the output.

396
00:24:03,750 --> 00:24:06,880
And trimming also deletes
the clear within the path.

397
00:24:07,240 --> 00:24:11,500
So at the end with fine tuning,
everything stays coherent even.

398
00:24:11,500 --> 00:24:16,120
And 75% of compression as you
may see, basically fine tuning

399
00:24:16,150 --> 00:24:17,830
isn't a just an extra step.

400
00:24:17,830 --> 00:24:19,960
It's what makes optimization possible.

401
00:24:21,390 --> 00:24:24,330
So yeah, that's pretty much finishing
slide with the final takeaways.

402
00:24:24,510 --> 00:24:26,820
First of all, fine tuning
is the true enabler.

403
00:24:26,940 --> 00:24:30,900
It makes compression possible
without the collapse of the model

404
00:24:30,960 --> 00:24:32,850
while we apply those techniques.

405
00:24:33,150 --> 00:24:35,760
Architecture driven reduces,
depths and size brilliant

406
00:24:35,760 --> 00:24:37,810
remotes, low impact weights.

407
00:24:37,840 --> 00:24:42,310
Quantization boosts CPU Efficiency
while distillation fails when applied

408
00:24:42,310 --> 00:24:43,960
to aggressively minimize students.

409
00:24:44,530 --> 00:24:46,415
Resistance here is a key.

410
00:24:46,630 --> 00:24:50,110
Fine tune models resist degradation,
far better than general models.

411
00:24:50,605 --> 00:24:53,575
So basically speaking, we didn't
just shrink a model, we built

412
00:24:53,575 --> 00:24:57,925
a focused, efficient specialist
and fine tuning may possible.

413
00:24:59,455 --> 00:25:00,625
Thank you very much.

414
00:25:00,705 --> 00:25:01,635
It was my pleasure.

415
00:25:01,665 --> 00:25:04,095
Great pleasure to provide a talk here.

416
00:25:04,215 --> 00:25:04,665
Thank you.

417
00:25:05,325 --> 00:25:05,985
Have a nice day.

