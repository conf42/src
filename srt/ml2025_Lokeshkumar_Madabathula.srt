1
00:00:01,000 --> 00:00:01,950
Hello everyone.

2
00:00:02,070 --> 00:00:03,540
So my name is Kumar Mala.

3
00:00:04,020 --> 00:00:07,350
Thanks for joining my sessions
on optimizing data pipelines

4
00:00:07,350 --> 00:00:08,730
with Azure Data Factory.

5
00:00:09,330 --> 00:00:14,445
In these sessions, I will walk you
through how to improve your data

6
00:00:14,445 --> 00:00:19,035
flows for faster processing and
loss cost, and less manual work.

7
00:00:19,305 --> 00:00:20,415
Let's get into the session.

8
00:00:21,415 --> 00:00:26,905
So today's data challenges, as you
all know, that today data is huge.

9
00:00:27,384 --> 00:00:32,995
So we are getting the data from multiple
systems like I streaming systems.

10
00:00:33,475 --> 00:00:36,775
And different files and file
systems and different sources.

11
00:00:37,255 --> 00:00:42,294
So to handling this much huge data is
nightmare and complex and challenges

12
00:00:43,194 --> 00:00:47,815
and integrating all of these systems
is definitely complex because there

13
00:00:47,815 --> 00:00:51,654
is no relations between these systems
and they don't talk each other.

14
00:00:51,984 --> 00:00:56,245
So to integrating these many systems
with volume of this much volume

15
00:00:56,245 --> 00:00:58,254
of data is complex and challenges.

16
00:00:58,885 --> 00:01:01,284
And because of that,
our process gets delays.

17
00:01:01,790 --> 00:01:06,770
And getting the right insights and
decision making gets poor and it's

18
00:01:06,820 --> 00:01:11,650
it's like main maintenance cost
is more effective, so more cost so

19
00:01:11,740 --> 00:01:14,800
to to overcome these challenges.

20
00:01:15,520 --> 00:01:19,550
I'm trying to introduce, I
will introduce new technology.

21
00:01:19,935 --> 00:01:22,095
So that is Azure Data Factory.

22
00:01:22,425 --> 00:01:26,025
The Azure Data Factory is
one of the Azure services.

23
00:01:26,115 --> 00:01:28,275
It's a cloud data integration tool.

24
00:01:28,935 --> 00:01:32,115
So there is a lot of advantages
of the Azure Data Factory.

25
00:01:32,355 --> 00:01:38,785
I usually call it as a DF, so a DF
because it's, it's supposed to integrate

26
00:01:39,054 --> 00:01:44,905
as many as data sources, like it's
supposed to add RBMS and NoSQL databases

27
00:01:45,445 --> 00:01:48,205
and file systems and realtime systems.

28
00:01:48,715 --> 00:01:54,665
And API systems it'll integrate,
it'll integrate all of the systems and

29
00:01:54,665 --> 00:01:56,555
it'll load into your desired system.

30
00:01:57,185 --> 00:01:59,575
So let's get more insights about the.

31
00:02:01,660 --> 00:02:05,620
So basically Azure data factory
architecture depends on four more

32
00:02:05,710 --> 00:02:09,120
components four or more components,
but the main components are the four.

33
00:02:09,360 --> 00:02:10,620
One is the linkage services.

34
00:02:10,709 --> 00:02:15,320
The linkage services is like
connections between your data

35
00:02:15,320 --> 00:02:17,390
sources and your target systems.

36
00:02:18,095 --> 00:02:22,715
For example, if you are, if you like
to connect your Azure data factory

37
00:02:22,715 --> 00:02:27,095
to Salesforce systems and you want to
extract and transform and load into your

38
00:02:27,095 --> 00:02:31,175
data lake, so you need to create the
linker services to connect your source

39
00:02:31,175 --> 00:02:36,275
system so that you know it'll like a
bridge connection bridge between your

40
00:02:36,275 --> 00:02:38,255
source systems and Azure Data Factory.

41
00:02:39,005 --> 00:02:42,010
And then another component is data sets.

42
00:02:42,070 --> 00:02:44,710
Data sets is the real data structures.

43
00:02:44,800 --> 00:02:48,610
It's like your tables or files,
data structure, what table, what

44
00:02:48,610 --> 00:02:53,140
columns you need to for your
target systems and activities.

45
00:02:53,140 --> 00:02:57,280
The activities is the main configurable
data transformations where you use.

46
00:02:57,775 --> 00:03:01,945
Actually your transformations and
business tools and then pipeline.

47
00:03:01,975 --> 00:03:05,645
Pipeline needs nothing but like
end-to-end solution from where your

48
00:03:05,645 --> 00:03:10,445
link services, data sets and activities
altogether, orchestration, sequence of

49
00:03:11,195 --> 00:03:13,235
end-to-end solution is called pipeline.

50
00:03:14,315 --> 00:03:15,335
Let's get into another slide.

51
00:03:16,505 --> 00:03:20,915
So it's not only provides these
many benefits and it's also reusable

52
00:03:21,245 --> 00:03:22,955
templates, pipeline templates.

53
00:03:22,955 --> 00:03:29,085
Suppose once you create one, one pipeline,
you can use that for another requirements.

54
00:03:29,415 --> 00:03:33,554
Suppose if you have a Salesforce
data, sales data and HR data,

55
00:03:33,945 --> 00:03:37,824
you can use the same pipeline for
that, but with little tweaks, but

56
00:03:37,824 --> 00:03:39,294
you can reusable that pipeline.

57
00:03:39,714 --> 00:03:42,924
So because of that, you can
reduce your manual work and cost.

58
00:03:43,924 --> 00:03:48,404
And another advantage is
dynamic orchestrations.

59
00:03:48,954 --> 00:03:52,374
If, suppose as I said, an example,
suppose if there is a new data comes

60
00:03:52,374 --> 00:03:56,064
into your source system or there is
a data is edited, updated in your

61
00:03:56,064 --> 00:04:00,204
source system, the Azure data factory
pipeline will automatically figure

62
00:04:00,204 --> 00:04:03,924
it out and it'll runs the pipeline
and load into your target system.

63
00:04:03,924 --> 00:04:08,285
So there is no manually you need to
log into the system and you no need to

64
00:04:08,285 --> 00:04:10,174
rerun the pipeline or run the pipeline.

65
00:04:10,535 --> 00:04:15,244
So it'll automatically detect and
it'll load into your target systems.

66
00:04:15,455 --> 00:04:17,015
So we call it that event triggered.

67
00:04:17,449 --> 00:04:20,299
Execution because of that.

68
00:04:20,750 --> 00:04:25,340
So there is no manual intervention
and it'll reduce your cast.

69
00:04:26,840 --> 00:04:30,049
And another thing is error handling.

70
00:04:30,079 --> 00:04:34,979
So when a system, when a pipeline
handles huge data with the different data

71
00:04:35,340 --> 00:04:39,510
sources, definitely there is an address
handling mechanism should be there.

72
00:04:39,900 --> 00:04:43,170
So the data factories also
handles the errors very smartly.

73
00:04:43,810 --> 00:04:49,780
There is a retrain mechanism in case if
your pipeline fails in between, so it'll

74
00:04:49,780 --> 00:04:54,850
retrace where it failed and from the
checkpoint, and it'll process it further,

75
00:04:55,270 --> 00:05:00,160
even in case if it is fails, and it'll
send to alerts through your emails or your

76
00:05:00,160 --> 00:05:05,410
messages or your teams channels so far for
your prompt response to fix the issues.

77
00:05:08,200 --> 00:05:13,745
And because of these advantages, the, when
you use the A DF and it'll improve your

78
00:05:13,745 --> 00:05:21,215
business and faster processing, 78% faster
processing and cost reduction goes to 63%.

79
00:05:21,275 --> 00:05:27,545
And resource efficiency can use be 42%
and you can make 90% of automation.

80
00:05:29,165 --> 00:05:31,835
So there is a case study,
like a financial services.

81
00:05:31,905 --> 00:05:35,415
There is an ETL pipeline with
the legacy tools and that

82
00:05:35,415 --> 00:05:36,705
was running for eight hours.

83
00:05:37,275 --> 00:05:41,805
And when we introduced the A DF pipeline
and with the same transformations

84
00:05:41,805 --> 00:05:45,885
and business logics, it got down
to 45 minutes because of that.

85
00:05:46,035 --> 00:05:48,945
And the processing time dramatically
registered eight from eight hours

86
00:05:49,035 --> 00:05:51,795
to 45 minutes, like 91% improvement.

87
00:05:52,245 --> 00:05:55,635
So because of that, the maintenance
cost register at 70% and

88
00:05:55,635 --> 00:05:59,745
delivering significance within
quarters and within timelines.

89
00:06:00,795 --> 00:06:02,115
Implement these pipelines.

90
00:06:02,415 --> 00:06:06,175
So the best, we should follow the best
practices organize your pipelines.

91
00:06:06,535 --> 00:06:10,855
I just said if you, if your organization
has the sales data and HR data or

92
00:06:10,855 --> 00:06:16,855
other CRM data, so you organize your
pipelines accordingly and always often.

93
00:06:18,250 --> 00:06:23,410
Check in your code into the gate and
leverage runtime integrations and

94
00:06:23,410 --> 00:06:27,510
also enable your Azure Monitor Azure
monitoring system so that so for a

95
00:06:27,510 --> 00:06:29,760
prompt response in case of any failures.

96
00:06:31,800 --> 00:06:35,610
What I would suggest to the
organizations is so looking at your

97
00:06:35,610 --> 00:06:41,950
current pipelines and analyze what
it slows and why it's cost, and then

98
00:06:42,280 --> 00:06:47,080
introduce Azure Data Factory for
better, faster performance, for the

99
00:06:47,080 --> 00:06:49,420
lower cost, and more efficient way.

100
00:06:50,420 --> 00:06:53,460
Thank you so much for
watching and joining my show.

101
00:06:54,255 --> 00:06:59,415
Azure Data Factory has helped me a lot
and as well as my team as well to better

102
00:06:59,745 --> 00:07:02,115
to build a better and faster pipelines.

103
00:07:02,655 --> 00:07:07,995
I hope it helps you too, so feel free to
connect me if you would like to chat more.

104
00:07:08,955 --> 00:07:09,735
Thank you so much.

105
00:07:09,735 --> 00:07:10,515
Have a nice day.

