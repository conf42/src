1
00:00:01,560 --> 00:00:07,229
So hello at Cov 42 Site
Reliability Engineering 2025.

2
00:00:08,460 --> 00:00:11,060
Glad that you're here
and listening to my talk.

3
00:00:12,290 --> 00:00:12,470
Yeah.

4
00:00:12,470 --> 00:00:16,135
Let's let's buckle up
and deep dive into it.

5
00:00:17,135 --> 00:00:20,555
The session title is Real
Time Earthquake Color System.

6
00:00:21,050 --> 00:00:23,810
Leveraging serverless
architecture with confluent Kafka.

7
00:00:24,620 --> 00:00:33,000
Basically, it's project which it's
mostly built in AWS cloud with specific

8
00:00:33,000 --> 00:00:39,720
services, mostly serverless and also
in combination with confluent Kafka,

9
00:00:39,750 --> 00:00:43,410
just for the message distribution.

10
00:00:44,385 --> 00:00:49,170
And achieving a fast kind of
delivery of the notifications

11
00:00:51,690 --> 00:00:58,480
where I some bold lines about what I'm
doing, what or what I did in the past.

12
00:00:59,680 --> 00:01:04,120
Cloud development, cyber
security, or kind of projects,

13
00:01:04,120 --> 00:01:07,180
architectures, designs consultancy.

14
00:01:08,155 --> 00:01:13,815
Everything like this project in data
science domain from data extraction,

15
00:01:13,815 --> 00:01:21,735
data modeling until, let's say about
full a TL pipeline or anything like that.

16
00:01:23,115 --> 00:01:32,175
So just trying to provide from real
scenario data, something that's practical.

17
00:01:32,670 --> 00:01:39,300
So just having practical projects
based on specific demands of data.

18
00:01:40,500 --> 00:01:43,800
Multiple security research is
for top pro Romanian banks.

19
00:01:44,790 --> 00:01:45,720
So yeah.

20
00:01:45,720 --> 00:01:51,350
Also this one is is a very
interesting to let's say pen test,

21
00:01:51,650 --> 00:01:53,030
if you want to call it like that.

22
00:01:54,080 --> 00:01:56,690
An a s company builder is serverless.

23
00:01:57,365 --> 00:02:00,605
Niche for one year right now.

24
00:02:01,805 --> 00:02:07,475
If you have any questions after this talk,
ideas, remarks, anything like that, you

25
00:02:07,475 --> 00:02:09,815
could contact me on mail or on LinkedIn.

26
00:02:10,955 --> 00:02:13,535
This, these are my addresses,

27
00:02:15,905 --> 00:02:16,355
so yeah.

28
00:02:16,625 --> 00:02:17,975
Enough about me.

29
00:02:18,125 --> 00:02:19,515
Let's let's try to.

30
00:02:20,895 --> 00:02:22,665
Explain what's in this project.

31
00:02:24,465 --> 00:02:28,944
The story of behind the
research project is like

32
00:02:29,944 --> 00:02:32,284
spread between these questions.

33
00:02:33,004 --> 00:02:36,354
So these these bullet points
are let's say questions.

34
00:02:37,524 --> 00:02:39,954
For example, the first one, why
the need for high performance

35
00:02:39,954 --> 00:02:41,754
earth throwing notification system?

36
00:02:42,850 --> 00:02:49,599
I start right by providing you
an interesting story of what we

37
00:02:49,599 --> 00:02:56,100
currently have in Romania in terms
of Romanian alerting system, which

38
00:02:56,100 --> 00:02:57,600
is provided by the government.

39
00:02:58,859 --> 00:03:04,850
It's the Romanian alert application,
which reaches like very low.

40
00:03:05,225 --> 00:03:07,235
Hardware in our telephones.

41
00:03:07,295 --> 00:03:13,655
So it's basically delivered with the
telephone and they're just emitting these

42
00:03:13,655 --> 00:03:19,835
kind of alerts or a bad weather, critical
accidents, critical infrastructure

43
00:03:19,835 --> 00:03:21,455
failures, anything like that.

44
00:03:23,075 --> 00:03:27,730
But an interesting point
here is that in very.

45
00:03:29,480 --> 00:03:34,100
Large of the cases on
notifications, we receive those

46
00:03:34,100 --> 00:03:38,060
alerts after the event unfolded.

47
00:03:38,450 --> 00:03:42,740
For example, after the
bad weather was just gone.

48
00:03:43,910 --> 00:03:48,080
So if you are seeing the
rainbow, you'll just receive a

49
00:03:48,410 --> 00:03:50,450
message, so that's not very good.

50
00:03:50,690 --> 00:03:56,230
Although the project and what the
government develops is is very very good.

51
00:03:57,065 --> 00:03:58,025
Very nice ideas.

52
00:04:00,305 --> 00:04:07,795
This is why I was thinking back then one
year and a half ago, why not having also

53
00:04:08,095 --> 00:04:10,435
a kind of notification for earthquakes

54
00:04:12,565 --> 00:04:13,075
in that time.

55
00:04:13,105 --> 00:04:18,385
I was thinking to just have the
notifications when the event

56
00:04:18,655 --> 00:04:25,075
happening is happening because I was
thinking that I cannot, I. Predict.

57
00:04:25,375 --> 00:04:27,685
So I cannot have a machine learning model.

58
00:04:28,225 --> 00:04:32,995
We just which is just trained on
previous data and just will predict

59
00:04:32,995 --> 00:04:36,405
something because it's so critical
and it's let's say unpredictable,

60
00:04:37,425 --> 00:04:40,335
let's say it's a natural happening.

61
00:04:41,655 --> 00:04:45,975
So I started to look around
and see what kind of data.

62
00:04:46,830 --> 00:04:53,185
Could I use, because I cannot just
put in the ground some some systems

63
00:04:54,114 --> 00:04:57,985
and just have some some data from
from those, at least not right now,

64
00:04:58,825 --> 00:05:05,060
because in what I'm about to say to
tell you is where I found basically

65
00:05:05,060 --> 00:05:07,920
that the data and in which project.

66
00:05:08,445 --> 00:05:13,065
I found that there is an institute
of seismic movements and earthquake

67
00:05:14,485 --> 00:05:19,495
researches in links to the
Romanian government, which has

68
00:05:19,495 --> 00:05:24,535
some very talented researchers and
very interesting projects there.

69
00:05:25,315 --> 00:05:28,735
And in those projects there was
a kind of very interesting one

70
00:05:29,635 --> 00:05:34,085
called rapid Alerting System.

71
00:05:34,430 --> 00:05:39,980
Or something like that, which was based
on some physical systems placed in

72
00:05:40,060 --> 00:05:46,720
northwest of Romania in the, let's say
the well-known area seismic movements,

73
00:05:49,600 --> 00:05:52,450
at least in the couple of years.

74
00:05:53,500 --> 00:05:59,250
So they placed those systems in those
areas, and they are just emitting data.

75
00:06:00,390 --> 00:06:03,810
When the very little system
movement is detected.

76
00:06:04,980 --> 00:06:07,080
So until now, very interesting thing.

77
00:06:08,160 --> 00:06:15,340
But I was also thinking okay, so
that town is like in distance,

78
00:06:15,970 --> 00:06:20,020
like 500 kilometers from Bucharest,
which is the capital of Romania.

79
00:06:21,160 --> 00:06:24,400
So if we are translating
like the kilometers in some.

80
00:06:25,060 --> 00:06:26,110
Time thing.

81
00:06:26,380 --> 00:06:30,280
There are some couple of minutes
until the full waves, for example,

82
00:06:30,280 --> 00:06:33,140
like a seven fourth wave hits.

83
00:06:34,190 --> 00:06:39,115
There are like a couple of minutes
or at least one two minutes until

84
00:06:39,115 --> 00:06:45,315
the full waves are received in est
in in our towns, like in the north,

85
00:06:45,315 --> 00:06:49,935
central North, three minutes, two,
three minutes, something like that.

86
00:06:51,045 --> 00:06:56,895
So I was thinking what, having this
advantage, so having the advantage

87
00:06:56,895 --> 00:07:04,845
of time, distance, and time use
those systems, basically that data,

88
00:07:05,295 --> 00:07:11,395
which is exposed by them in a browser
application, I think right now and

89
00:07:11,395 --> 00:07:13,555
I can use, just listen to that data.

90
00:07:15,610 --> 00:07:21,160
And use it to spread it to
certain consumers, basically, and

91
00:07:21,190 --> 00:07:26,650
subscribers, which can be people
or critical infrastructure things.

92
00:07:28,210 --> 00:07:35,290
So I just responded almost to also
the second question, where to find

93
00:07:35,290 --> 00:07:36,935
good real time earthquake data.

94
00:07:37,625 --> 00:07:40,265
So like I've said this institute provides.

95
00:07:41,015 --> 00:07:42,775
Very interesting projects.

96
00:07:43,285 --> 00:07:48,235
And that particular one with the
systems placed in the ground, emitting

97
00:07:48,265 --> 00:07:53,905
data at the very little assessment
movements, it was very helpful.

98
00:07:54,265 --> 00:07:58,395
So we integrated with that
with that emitting system.

99
00:07:58,845 --> 00:08:04,534
And we are listening for for
events, basically why choosing

100
00:08:04,534 --> 00:08:05,975
the serverless ecosystem.

101
00:08:06,814 --> 00:08:15,814
So I am ambassador in AWS or serverless
lambdas and anything related to

102
00:08:15,814 --> 00:08:22,924
that, it's from a perspective, due
to my previous projects, I integrated

103
00:08:23,254 --> 00:08:26,074
Lambdas serverless, like anywhere.

104
00:08:26,734 --> 00:08:30,814
But beside the knowledge,
it was also practical.

105
00:08:32,014 --> 00:08:35,674
Why I'm saying practical is because
for this particular project, we

106
00:08:35,674 --> 00:08:39,354
don't need nonstop servers like 24 7.

107
00:08:39,984 --> 00:08:44,984
We just need some some listeners
which need to be prepared when the

108
00:08:44,984 --> 00:08:51,874
earthquake hits, just to spin up those
kind of instances, containers, and use

109
00:08:51,874 --> 00:08:54,544
them to just spread and distribute.

110
00:08:55,069 --> 00:08:57,409
Very fast, the notifications.

111
00:08:58,279 --> 00:08:59,299
This was the first thing.

112
00:08:59,299 --> 00:09:01,159
The second thing is also the costs.

113
00:09:01,879 --> 00:09:07,669
'cause yeah, if we have like servers
up 24 7, it's also a matter of costs

114
00:09:09,019 --> 00:09:14,229
be involved and how to scale them
just on the fly, like in milliseconds.

115
00:09:14,949 --> 00:09:18,809
So these two things were the
base pillars of choosing.

116
00:09:20,444 --> 00:09:22,574
A s Lambda serverless.

117
00:09:24,194 --> 00:09:27,074
Moving on to the architecture of the
serverless identification system.

118
00:09:29,024 --> 00:09:33,514
What you see here is the base
architecture, which is the

119
00:09:33,514 --> 00:09:35,344
first version, let's say.

120
00:09:36,434 --> 00:09:40,329
I'll try to explain you like in
big lines because we will deep

121
00:09:40,359 --> 00:09:42,224
dive in the next next slides.

122
00:09:43,609 --> 00:09:49,219
What in the gray is like the backbone
of the system, which has the Lambda

123
00:09:49,399 --> 00:09:55,369
producers, which I'm calling like
Lambda producers 'cause of the Kafka

124
00:09:55,699 --> 00:09:59,819
integration and also the Lambda consumers.

125
00:09:59,889 --> 00:10:03,489
In the right in the center,
you have the topics partitions,

126
00:10:03,729 --> 00:10:05,559
the backbone of Kafka itself.

127
00:10:07,419 --> 00:10:09,909
The yellow ones are like the.

128
00:10:10,614 --> 00:10:12,744
Error handling mechanisms.

129
00:10:13,134 --> 00:10:21,114
SQS, that letter queues just for
putting all kind of errors like

130
00:10:21,424 --> 00:10:26,764
if there are errors networking or
latency or things like that, we'll

131
00:10:27,514 --> 00:10:30,964
just retry on that code execution.

132
00:10:31,564 --> 00:10:36,184
But if it's like a major failure, we'll
just put it in a particular queue.

133
00:10:36,574 --> 00:10:42,364
And keep it there for a manual studying
of the logs and of the error itself.

134
00:10:43,954 --> 00:10:50,224
The blue ones are for monitoring, just
monitoring the number of instances, how

135
00:10:50,224 --> 00:10:56,724
the system behaves, costs, and are very
important because we are I will show

136
00:10:56,724 --> 00:11:00,104
you afterwards, we are also scaling up.

137
00:11:01,019 --> 00:11:05,249
So we need to see the costs and
how the system behaves, like

138
00:11:05,249 --> 00:11:11,639
little, let's say, central kind of
monitoring or everything for errors.

139
00:11:11,639 --> 00:11:18,849
And yeah, also the end channels, if you
view them you can view them in the right

140
00:11:19,869 --> 00:11:25,084
Twilio, like SMS mail through A-S-S-E-S.

141
00:11:25,994 --> 00:11:30,594
Or through Telegram, which is
like a backup thing from from

142
00:11:30,594 --> 00:11:32,064
our end, from our perspective.

143
00:11:33,864 --> 00:11:40,314
The purple ones are for CICD, like
pipelines for creating the infrastructure,

144
00:11:40,314 --> 00:11:48,144
but also we are thinking you, you can
view it in the next slides to scale up

145
00:11:48,144 --> 00:11:52,724
and down through this kind of CICD idea.

146
00:11:54,959 --> 00:12:00,979
The build some scannings just
automation things on the repository.

147
00:12:01,669 --> 00:12:06,689
And the block with with green,
it's like the application, the web

148
00:12:06,689 --> 00:12:11,219
application where the end subscribers
or the creator infrastructure members

149
00:12:11,609 --> 00:12:15,089
could join into the backbone system.

150
00:12:15,629 --> 00:12:21,419
What I'm trying to say, join is
like just subscribing the system.

151
00:12:22,019 --> 00:12:27,299
Provide maybe a telephone number or some
kind of informations about the subscriber

152
00:12:28,439 --> 00:12:33,179
and based on them, you can see there
that there is a database that's a Dynamo

153
00:12:33,179 --> 00:12:35,309
database connected to the backbone.

154
00:12:35,489 --> 00:12:39,449
Of the system, which
will be just iterated.

155
00:12:39,449 --> 00:12:42,509
So we will just iterate through the
members, through the subscribers

156
00:12:42,509 --> 00:12:46,479
and try to notify everyone.

157
00:12:48,489 --> 00:12:53,679
Also, some lambda functions there
for the different kind of APIs, some

158
00:12:53,679 --> 00:12:55,749
protections through the firewall.

159
00:12:56,769 --> 00:13:00,719
The main application was we're thinking
to put it in like a Kubernetes.

160
00:13:01,454 --> 00:13:08,014
Infrastructure, but we will see maybe use
what kind of serverless also for that one.

161
00:13:10,934 --> 00:13:14,084
Architecture of the base notification
system, like I've said in the previous

162
00:13:14,084 --> 00:13:16,794
slide, this is the backbone the gray area.

163
00:13:16,884 --> 00:13:22,464
If you call it like this, you can see
the Kafka producers, Kafka consumers,

164
00:13:22,854 --> 00:13:25,124
and the Kafka topics in in the middle.

165
00:13:26,124 --> 00:13:31,084
In the Kafa producers, we have
this thing called multithreading.

166
00:13:31,504 --> 00:13:38,144
Just using the, let's say, full
capacity, full resources of instances

167
00:13:38,144 --> 00:13:43,834
of Lambda the CPU, so just using the.

168
00:13:45,184 --> 00:13:51,564
The full capacity and try to separate,
let's say threads and link them to

169
00:13:51,564 --> 00:13:54,744
certain partitions or topics in Kafka.

170
00:13:55,194 --> 00:14:01,614
So this is the beginning of rapid
or fast production and notification.

171
00:14:02,004 --> 00:14:04,074
Basically the first the first step.

172
00:14:06,609 --> 00:14:13,119
We're trying to put like a, also a
topic roulette on top of what Kafka has

173
00:14:15,399 --> 00:14:20,649
like for topics, partitions, and
try to not override messages in

174
00:14:20,649 --> 00:14:22,329
the same partitions, same topics.

175
00:14:22,329 --> 00:14:26,979
Just try to have a thread and
the partition and put the message

176
00:14:27,129 --> 00:14:31,719
on that and try to to connect
it to the Kafka consumers.

177
00:14:32,559 --> 00:14:38,949
This is also like a an interesting
key idea of this project to have the

178
00:14:38,979 --> 00:14:44,949
partitions and basically topics in
Kafka triggering certain instances of

179
00:14:44,949 --> 00:14:47,949
Kafka called the, called consumers.

180
00:14:49,669 --> 00:14:57,444
Achieving a, a parley list, distributing
the message the most fast way possible.

181
00:14:57,999 --> 00:14:58,479
Can be.

182
00:15:00,169 --> 00:15:05,589
The yellow box is monitoring
and mo monitoring, at least

183
00:15:05,589 --> 00:15:07,649
for the kind of major failures.

184
00:15:08,729 --> 00:15:14,189
So for the air errors, which are
put in like the error queue and for

185
00:15:14,219 --> 00:15:19,100
the failed events, health events,
just networking, latency, things

186
00:15:19,100 --> 00:15:21,795
like that, we will try to retry.

187
00:15:22,945 --> 00:15:24,775
Two times, third, three times.

188
00:15:27,535 --> 00:15:31,225
We have the end channels where
the notification is sent.

189
00:15:31,525 --> 00:15:36,505
SMS mailed Telegram is
back up and the dynamo.

190
00:15:37,285 --> 00:15:40,665
So the database, the blue one
is where the subscribers are

191
00:15:40,715 --> 00:15:43,225
just present and registered.

192
00:15:44,225 --> 00:15:49,345
And the Kafa producers listen for
that data, which I've explained

193
00:15:49,345 --> 00:15:55,715
before, which is provided through
through the systems of the institute.

194
00:15:58,235 --> 00:16:03,405
Another interesting thing here will deep
dive bit into the architecture of Kafka

195
00:16:03,405 --> 00:16:05,295
integration in the serverless context.

196
00:16:05,985 --> 00:16:13,915
Like I said, Kafka is is here to somehow
parallel distribute the messages, the

197
00:16:13,915 --> 00:16:20,595
notifications, and to also trigger
parallel pins of plum duck consumers.

198
00:16:21,705 --> 00:16:26,205
So every partition we will have
a certain code execution, like a

199
00:16:26,265 --> 00:16:31,875
certain, trigger, let's say certain
lambda as a consumer, which will

200
00:16:32,755 --> 00:16:35,575
try to notify a certain subscriber.

201
00:16:36,955 --> 00:16:43,155
We have the threads just using the
resources, the topic roulette, build on

202
00:16:43,155 --> 00:16:52,280
top what Kafka or they have or topics,
partitions, and try to fill up every.

203
00:16:52,975 --> 00:17:00,295
Partition to be able to trigger from
it the Lambda consumers, which have the

204
00:17:00,295 --> 00:17:07,980
logic to just iterate the database of
subscribers and notify them in the end.

205
00:17:10,225 --> 00:17:10,285
Yeah.

206
00:17:10,285 --> 00:17:13,395
So here we also have some yellow dots.

207
00:17:13,860 --> 00:17:17,960
Which represent like the scaling
possibilities, so scaling in terms

208
00:17:17,960 --> 00:17:23,120
of resources, threads and scaling
in terms of partitions topics.

209
00:17:23,750 --> 00:17:28,850
And in the, again, in the Kafka
consumers just trying to scale

210
00:17:29,570 --> 00:17:36,135
the triggers based on what Kafka
has and also the Lambda consumers,

211
00:17:37,135 --> 00:17:38,795
the Lambda Kafka producers key elements.

212
00:17:40,060 --> 00:17:45,790
We have the multithreading explained
in the previous slide using those

213
00:17:45,790 --> 00:17:50,980
resources from the Lambda instances
thread lock to not write in the

214
00:17:50,980 --> 00:17:53,050
same topics from separate threads.

215
00:17:54,190 --> 00:17:57,490
Using partition number to not
override data in partitions and

216
00:17:57,490 --> 00:18:01,610
using lambda retrial and sqs
that letter for major failures.

217
00:18:02,630 --> 00:18:07,190
And the retry is just for latency,
networking, minor problems.

218
00:18:09,095 --> 00:18:10,535
Consumer key elements.

219
00:18:11,735 --> 00:18:18,085
Here are some a again, some
nice ideas from our perspective.

220
00:18:18,475 --> 00:18:23,875
Custom timestamp blanc lock for
mechanism for parallel dynamo update.

221
00:18:24,205 --> 00:18:28,875
So we are trying to here
was an interesting dilemma,

222
00:18:29,115 --> 00:18:31,545
let's say, because we.

223
00:18:32,430 --> 00:18:32,970
We saw that

224
00:18:35,010 --> 00:18:39,420
there were multiple notifications
for a certain subscriber.

225
00:18:39,810 --> 00:18:45,535
So let's say one, one one,
one man and one fellow could

226
00:18:45,985 --> 00:18:47,785
receive multiple notifications.

227
00:18:47,935 --> 00:18:54,385
And the second guy could not receive
anything just because the, there

228
00:18:54,385 --> 00:18:59,515
was a, like a. Race between them
and the first one won and just

229
00:18:59,515 --> 00:19:01,255
had all the notifications there.

230
00:19:01,645 --> 00:19:07,265
So we wanted to just lock
somehow certain subscribers.

231
00:19:07,385 --> 00:19:15,065
If those were notified, basically
the code already was applied on them

232
00:19:15,065 --> 00:19:19,685
and they received, or it's like in
progress of receiving a notification

233
00:19:20,615 --> 00:19:24,235
and not, trying to notify it
again, just moving to the next one.

234
00:19:25,015 --> 00:19:31,945
This is like the lock per
subscriber, per item to have for

235
00:19:32,005 --> 00:19:35,555
each individual one one notification.

236
00:19:36,815 --> 00:19:40,055
Optimistic locking in the custom
dyna concurrency control is.

237
00:19:41,105 --> 00:19:45,095
Method, just build on top of the first
one with the custom timestamp log.

238
00:19:45,755 --> 00:19:50,945
We are just putting like a version and
we know if the version changed, if we

239
00:19:50,945 --> 00:19:53,165
need to notify the subscriber or not.

240
00:19:53,795 --> 00:19:58,595
Just let's say another fence of
achieving the concurrency and the

241
00:19:58,595 --> 00:20:04,805
fast thing, but also to notify all the
subscribers and not miss one of them.

242
00:20:05,960 --> 00:20:11,360
Yeah, using lambda retry que
for errors, like for the Lambda

243
00:20:11,450 --> 00:20:17,330
producers and Lambda consumers using
a Coldstar worm start methodology.

244
00:20:18,500 --> 00:20:23,620
We are seeing that using a worm
start helps because it's the

245
00:20:23,620 --> 00:20:26,290
last phase of the notification.

246
00:20:26,890 --> 00:20:29,290
So started from the producer.

247
00:20:29,710 --> 00:20:31,750
Reaching the Kafka cluster.

248
00:20:31,750 --> 00:20:35,600
And after that we need something
that's in a ready state.

249
00:20:36,110 --> 00:20:42,240
Just have it in there, a number of
instances and lambdas, and try to have

250
00:20:42,240 --> 00:20:48,710
them in a ready state, just ready to go
to a list of subscribers, execute the code

251
00:20:48,710 --> 00:20:51,645
for reach, and send the notifications.

252
00:20:52,925 --> 00:21:01,235
But this is again, an interesting
procedure also in auto scaling basically.

253
00:21:01,355 --> 00:21:06,455
So if you have like thousands of
subscribers or reaching a certain

254
00:21:06,905 --> 00:21:12,305
number of subscribers, maybe you
need certain instances in Worm State,

255
00:21:13,295 --> 00:21:16,450
resting cold start, things like that.

256
00:21:19,235 --> 00:21:22,475
The Lambda Kafka consumer
integration with database.

257
00:21:22,475 --> 00:21:24,845
So there is like a custom dynam blocking.

258
00:21:24,995 --> 00:21:30,575
This was the first version, like I've
explained a bit in the previous slide.

259
00:21:31,355 --> 00:21:36,045
So from the earthquake topic, the
partition we're just trying to

260
00:21:36,045 --> 00:21:38,715
trigger a certain lamb execution.

261
00:21:39,435 --> 00:21:44,505
And from that execution, the
code will act on certain items

262
00:21:45,075 --> 00:21:46,305
and using this kind of lock.

263
00:21:47,730 --> 00:21:51,440
Like the timestamp on, on
which we are placing the log.

264
00:21:51,740 --> 00:21:59,120
We know that an item is in progress of
being updated or it was just updated like

265
00:21:59,170 --> 00:22:02,560
one millisecond ago, something like that.

266
00:22:03,340 --> 00:22:08,290
And the next trigger, so the trigger
topic two will go to the next one.

267
00:22:08,680 --> 00:22:13,240
So just going like a domino
thing, but from the front.

268
00:22:13,615 --> 00:22:17,785
So view viewing like a domino row
from the front and just go going

269
00:22:17,785 --> 00:22:20,785
gr gradually with each trigger.

270
00:22:20,935 --> 00:22:26,105
So with which it, with it with which,
which partition just going through each

271
00:22:26,765 --> 00:22:29,795
subscriber and notify each one of them.

272
00:22:31,205 --> 00:22:37,785
This is four, just using the full,
full capacity of triggers and not

273
00:22:37,785 --> 00:22:40,665
wasting like multiple triggers.

274
00:22:40,665 --> 00:22:45,705
So multiple code executions
on a certain item, although

275
00:22:46,455 --> 00:22:48,035
there is there is like a loop.

276
00:22:48,215 --> 00:22:54,095
So going gradually through the
items, we can end up using the

277
00:22:54,095 --> 00:22:56,585
same trigger for the same item.

278
00:22:57,665 --> 00:23:02,025
Yeah, it won't be, notified
like twice or three times.

279
00:23:02,745 --> 00:23:07,035
But we will have we have a
waste, let's say of time.

280
00:23:07,335 --> 00:23:08,595
So trigger the trigger.

281
00:23:08,595 --> 00:23:14,655
We lacked like twice on the same
item, but just updating it once.

282
00:23:15,975 --> 00:23:16,245
Yeah.

283
00:23:16,785 --> 00:23:20,045
So this is like the first version
of what we were trying to achieve.

284
00:23:22,150 --> 00:23:27,180
Concurrency of notifications and
also notifying the subscribers in

285
00:23:27,180 --> 00:23:34,710
a best way and notifying all the
subscribers, the custom dynamo,

286
00:23:34,710 --> 00:23:36,560
segmented subscriber processing.

287
00:23:37,670 --> 00:23:43,550
This was like the second version
of the Lambda consumers logic.

288
00:23:45,065 --> 00:23:50,345
So from the previous version, we are
trying to segment the list of subscribers

289
00:23:50,975 --> 00:23:54,825
and having like logs per certain segments.

290
00:23:55,545 --> 00:24:01,675
So for example, one trigger from a
partition from Kafka will just act on

291
00:24:01,675 --> 00:24:07,735
a certain segment and just lock that
segment with a block ID or something

292
00:24:07,735 --> 00:24:11,785
like that from ID one to ID 400.

293
00:24:12,325 --> 00:24:12,955
Something like that.

294
00:24:12,955 --> 00:24:16,675
So just the trigger will act
on, on that particular segment.

295
00:24:17,605 --> 00:24:21,465
So in this one the the
triggers won't conflict.

296
00:24:22,065 --> 00:24:24,285
There won't be any race conditions.

297
00:24:24,285 --> 00:24:27,975
Let's say each trigger will be
allocated to a certain segment

298
00:24:28,095 --> 00:24:30,435
and will just do the work.

299
00:24:30,465 --> 00:24:34,965
So the trigger can be viewed like
worker, so the worker will do

300
00:24:34,965 --> 00:24:36,645
the job just for that segment.

301
00:24:37,320 --> 00:24:43,940
So from now we can view that domino
ization from the parallel from

302
00:24:43,940 --> 00:24:50,780
the lateral view, we are viewing
with workers, like soldiers, just

303
00:24:50,810 --> 00:24:56,610
particular segments of items which are
basically subscribers, peoples people,

304
00:24:56,790 --> 00:24:58,980
infrastructure members, things like that.

305
00:25:00,960 --> 00:25:04,735
Just on them the triggers
the workers will focus.

306
00:25:05,920 --> 00:25:12,015
This is an improvement in terms of how
we are reaching those subscribers through

307
00:25:13,275 --> 00:25:16,085
Kafka, partitions topics and lambda code.

308
00:25:16,655 --> 00:25:21,715
Different kind of lambda code
executions, which are done in parallel.

309
00:25:23,305 --> 00:25:28,255
Also moving with this
parallel view through the end.

310
00:25:29,035 --> 00:25:33,745
So having the triggers, the workers
act in parallel the subscribers.

311
00:25:33,805 --> 00:25:41,735
But although having one worker for
a segment like one to 400 or 4,000

312
00:25:42,305 --> 00:25:46,700
is like a lot, even though there
will be like, lo, very low level

313
00:25:46,910 --> 00:25:52,310
party threading in that particular
segment, you only have one worker.

314
00:25:53,060 --> 00:25:55,040
It's a hard work there.

315
00:25:56,060 --> 00:26:00,740
This is like the second version,
so this is like a parallel

316
00:26:00,740 --> 00:26:02,960
distribution and segmentation.

317
00:26:02,960 --> 00:26:09,620
In Lambda Kafka consumer, there are like
maxim invocations and locks per segment.

318
00:26:10,145 --> 00:26:16,825
Also like domino style, but
we led multiple executions.

319
00:26:17,245 --> 00:26:22,545
So multiple code triggers to just
act on let's say particular segments.

320
00:26:22,605 --> 00:26:28,870
So we allow multiple workers to
work on the same segment and doing

321
00:26:28,870 --> 00:26:30,965
that low level multithreading.

322
00:26:32,990 --> 00:26:35,630
So letting, I dunno, maybe two.

323
00:26:35,930 --> 00:26:40,810
Like in this example, we let two
triggers just act on on on the

324
00:26:40,810 --> 00:26:46,680
same segment so they can resolve
or notify and execute the code.

325
00:26:46,830 --> 00:26:52,650
Or faster, let's say we can
allocate, like in this example,

326
00:26:52,650 --> 00:26:55,500
maximum workers like five.

327
00:26:55,815 --> 00:26:59,625
Or anything like that, just depending
on the segmentation, which is

328
00:26:59,625 --> 00:27:01,645
done like in an automatic way.

329
00:27:02,065 --> 00:27:06,905
Just just having the nu the maximum
number of subscribers, maybe dividing

330
00:27:07,475 --> 00:27:13,620
somehow that list and having the
numbers of locks Mexican workers.

331
00:27:14,085 --> 00:27:14,655
Things like that.

332
00:27:15,015 --> 00:27:19,155
So what's the important idea is that
we are viewing the domino from the

333
00:27:19,155 --> 00:27:25,125
lateral view and we have multiple
soldiers located for the same segment.

334
00:27:26,025 --> 00:27:29,205
They will not conflict with
each other, there won't be any

335
00:27:29,205 --> 00:27:30,765
risk conditions between them.

336
00:27:31,395 --> 00:27:36,585
They will just act like multiple workers
are dividing the work in the same segment.

337
00:27:38,055 --> 00:27:39,915
This is like the improve the.

338
00:27:42,150 --> 00:27:47,950
Improved version of the distribution
between the Kafka code execution the

339
00:27:48,010 --> 00:27:54,850
Lambda code execution and the Dynamo
where we store the subscribers.

340
00:27:56,410 --> 00:27:58,540
So this is like the level right now.

341
00:27:59,379 --> 00:28:04,604
But we're also trying other things and
just improve the speed and everything,

342
00:28:04,604 --> 00:28:09,344
like I've said with it, that granular
multithreading apply that segment level.

343
00:28:09,735 --> 00:28:12,819
So it is very blow level I could say.

344
00:28:13,149 --> 00:28:18,964
So we will just keep on researching and
see what's the best for for that moment.

345
00:28:18,964 --> 00:28:23,084
Increasing every time with
new, new possible ways.

346
00:28:24,254 --> 00:28:25,504
Again, one interesting thing.

347
00:28:26,314 --> 00:28:30,354
So like I've said Kafka was integrating
here just to resolve this par

348
00:28:30,354 --> 00:28:36,894
distribution, but instead of Kafka, we
could use Kinesis from AWS, which could

349
00:28:37,164 --> 00:28:45,324
work well or a different kind of cues
to just be to just be close to the.

350
00:28:46,209 --> 00:28:52,519
To the cloud, not go to to, to the
internet, the cluster and just come

351
00:28:52,519 --> 00:28:54,850
back, just be in the same network.

352
00:28:54,850 --> 00:29:02,229
May, maybe it could decrease the
latency, but this is for the later

353
00:29:03,790 --> 00:29:06,279
some some things about auto scaling.

354
00:29:07,239 --> 00:29:12,009
Which is very interesting from this
project because we don't want to have

355
00:29:12,039 --> 00:29:18,889
manual interventions when it reaches like
a base kind of stable state, the project.

356
00:29:19,519 --> 00:29:23,659
We won't need those manual
interventions anymore.

357
00:29:24,349 --> 00:29:28,909
This is why we are thinking to put
in place a kind of auto scaling,

358
00:29:29,689 --> 00:29:34,939
scaling up or down the system
based on the number of subscribers.

359
00:29:35,344 --> 00:29:42,274
So if we have like a range between,
I dunno, zero and 201 or 301,

360
00:29:42,784 --> 00:29:46,324
we'll just increase with threads.

361
00:29:46,444 --> 00:29:54,324
So basically resources allocated in Lambda
producer instances topics, partitions,

362
00:29:54,324 --> 00:30:01,134
the Kapha clusters, and also the number of
triggers and number of Lambda consumers.

363
00:30:03,444 --> 00:30:11,454
Yeah, like the warm state consumers, if
we are going down with the subscribers,

364
00:30:11,484 --> 00:30:18,974
we can just lower the resources on
the fly again, if we, yeah, it's hard

365
00:30:19,004 --> 00:30:25,624
to predict something to autoscale
through the roof before having that

366
00:30:25,894 --> 00:30:27,904
major earthquake, but at least.

367
00:30:28,699 --> 00:30:33,499
There is a possibility to scale it
based on this number of subscribers.

368
00:30:33,679 --> 00:30:41,199
For now this is our idea for now, but
we can think about some other features,

369
00:30:41,439 --> 00:30:47,869
let's say which could be linked
auto scaling, not predicting 'cause.

370
00:30:48,469 --> 00:30:49,399
Predicting is hard.

371
00:30:49,489 --> 00:30:50,029
Hard.

372
00:30:50,029 --> 00:30:53,839
We need something that's realistic.

373
00:30:55,579 --> 00:31:00,769
So this was made through custom
scripts like some Python combined

374
00:31:00,769 --> 00:31:07,979
with Azure with A-W-S-C-L-I commands
for for Lambdas, for the Kafka

375
00:31:08,099 --> 00:31:10,929
clusters, just on some custom scripts.

376
00:31:11,829 --> 00:31:15,749
And these were placed in a
lambda, like an external lambda.

377
00:31:15,779 --> 00:31:20,999
We just, which orchestrates the
entire, auto scaling and everything.

378
00:31:23,219 --> 00:31:28,009
This kind of external lamb does
act also like for the previous

379
00:31:29,149 --> 00:31:33,949
segmentation, just allocating
different kind of maximum workers.

380
00:31:34,739 --> 00:31:39,689
Segmentation ID just on the
fly, again, viewing the number

381
00:31:39,689 --> 00:31:44,749
of subscribers, just doing the
calculations and having these numbers.

382
00:31:46,384 --> 00:31:49,024
The custom auto scaling using Terraform.

383
00:31:49,114 --> 00:31:50,794
So infrastructure as a code.

384
00:31:51,514 --> 00:31:57,699
So right now we are trying to reduce
the custom scripts, so custom shells

385
00:31:57,759 --> 00:32:01,259
and things like that and just move
to an infrastructure as a code.

386
00:32:01,259 --> 00:32:04,339
Something that's more
used in every project.

387
00:32:04,339 --> 00:32:09,039
Right now, software project is the
same kind of logic, just, we're

388
00:32:09,039 --> 00:32:12,759
also provisioning the infrastructure
through infrastructure as a code.

389
00:32:13,059 --> 00:32:18,399
It's all the services and
everything, and we also use it.

390
00:32:19,389 --> 00:32:26,159
That's the interesting part, to scale
or up or down with GitHub actions.

391
00:32:26,249 --> 00:32:33,959
Or we could use, something in AWS for
code deployment because we need to check

392
00:32:33,959 --> 00:32:40,780
the number of subscribers and to trigger
the Terraform execution with a new list

393
00:32:40,900 --> 00:32:47,300
of what we need, like parameters, like we
need two threads, we need four threads.

394
00:32:47,300 --> 00:32:49,230
We need four topics.

395
00:32:49,470 --> 00:32:53,980
So these are the kind of, numbers,
which are placed on a certain file,

396
00:32:54,550 --> 00:32:59,070
which is sent to the Terraform
just to execute on on that list.

397
00:33:00,420 --> 00:33:06,440
So it's a very interesting approach of how
infrastructure is a code act or scaling

398
00:33:07,250 --> 00:33:13,900
up or down or serverless, lambda AWS
architecture and, ion system basically.

399
00:33:15,070 --> 00:33:15,310
Yeah.

400
00:33:15,340 --> 00:33:21,550
These were like in the area of development
operations, how to operate entire system

401
00:33:21,700 --> 00:33:28,330
when it's like in a stable position
after we figure out the services,

402
00:33:28,390 --> 00:33:30,610
what we need and so on and so forth.

403
00:33:31,670 --> 00:33:38,700
This like a separate lane of, how
to keep it automatic, let's say also

404
00:33:38,700 --> 00:33:42,100
in for creation of the resources
and the services and everything.

405
00:33:42,970 --> 00:33:47,470
Then to have it a certain time step of
them, if something happens, we could

406
00:33:47,470 --> 00:33:53,220
revert a certain step, certain snapshot
of the resources and the architecture.

407
00:33:53,970 --> 00:33:58,670
And we could also use it for scaling
up or down, depending of the.

408
00:34:01,055 --> 00:34:03,020
Number of subscribers in in this phase.

409
00:34:05,390 --> 00:34:07,870
So this is the project.

410
00:34:08,320 --> 00:34:10,870
This is the researches.

411
00:34:11,120 --> 00:34:14,660
These are the researches done until now.

412
00:34:15,380 --> 00:34:19,830
We yeah, we are in touch with some
we have some discussions how to.

413
00:34:21,225 --> 00:34:27,140
Proceed further how to do other
interesting researches for these kind of

414
00:34:27,650 --> 00:34:36,050
earth failure notifications May maybe to
integrate it in what there is on provided

415
00:34:36,050 --> 00:34:41,870
by the governments, not only Romania, who
knows to have it like an alternative on

416
00:34:41,870 --> 00:34:44,670
just having some kind of integrations.

417
00:34:45,000 --> 00:34:51,540
Between what's already there or
providing new ideas, anything like that.

418
00:34:52,350 --> 00:34:56,810
Yeah, just bringing something
to the table, a kind of

419
00:34:58,020 --> 00:35:00,210
innovation we could we could say.

420
00:35:01,770 --> 00:35:05,315
So if you have any questions
or remarks or opinions.

421
00:35:06,660 --> 00:35:10,170
Please feel free to contact
me on mail, LinkedIn.

422
00:35:11,130 --> 00:35:15,530
Just be aware that the project
itself, it's it's registered.

423
00:35:15,920 --> 00:35:23,060
So yeah, if you want to, if you have some
inspiration from it or you want to do

424
00:35:23,060 --> 00:35:30,004
something similar, please just contact
us or me just in in the first place and

425
00:35:30,004 --> 00:35:32,404
we could align and we could discuss.

426
00:35:32,840 --> 00:35:37,730
Firstly, so like I've said, it's a
protected project under some licenses.

427
00:35:38,680 --> 00:35:42,580
Yeah, we also have some open
discussions now and there.

428
00:35:42,580 --> 00:35:44,710
So yeah.

429
00:35:46,780 --> 00:35:47,290
Thank you.

430
00:35:47,290 --> 00:35:48,790
And thank you for being here.

431
00:35:48,850 --> 00:35:51,220
It was it was pleasure.

432
00:35:51,639 --> 00:35:57,279
And thanks for the, thanks to the
organizers for inviting me to present.

433
00:35:57,684 --> 00:35:58,935
This this research.

434
00:35:59,745 --> 00:36:00,134
Thank you.

