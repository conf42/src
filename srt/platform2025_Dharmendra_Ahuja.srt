1
00:00:00,500 --> 00:00:00,815
Hi everyone.

2
00:00:01,085 --> 00:00:01,920
Thank you for joining.

3
00:00:02,380 --> 00:00:06,135
My name is Bin Aja and I'm
a DevOps lead within IBM.

4
00:00:06,415 --> 00:00:07,015
Today.

5
00:00:07,075 --> 00:00:10,045
Today we are going to talk about
ethical AI deployment at scale.

6
00:00:10,415 --> 00:00:13,985
And like we are going to discuss about
the platform engineering approaches.

7
00:00:14,225 --> 00:00:17,615
So today we are going to dive
into one of the most critical

8
00:00:17,615 --> 00:00:21,525
challenges in modern tech which
is ethical AI deployment at scale.

9
00:00:22,015 --> 00:00:23,915
This is not the theoretical discussion.

10
00:00:23,915 --> 00:00:26,765
It is like a hard engineering problem,
which we are facing every day.

11
00:00:26,795 --> 00:00:32,805
And the key to solving this problem is to,
it lies with us like platform engineers.

12
00:00:33,135 --> 00:00:38,225
So our, over the few minutes we will move
from understanding the core dilemma we are

13
00:00:38,225 --> 00:00:41,545
facing to the tangible ROI of solving it.

14
00:00:41,575 --> 00:00:45,625
And finally we'll be talking about
like a practical blueprint for building

15
00:00:45,625 --> 00:00:49,485
ethical safeguards, which is directly
into the fabric of our AI platforms.

16
00:00:49,825 --> 00:00:50,420
So let's get started.

17
00:00:50,920 --> 00:00:55,290
So today we are seeing is 83% of
enterprise accelerate AI integration

18
00:00:55,650 --> 00:00:59,449
and platform team are facing
more lot of pressure to deploy

19
00:00:59,449 --> 00:01:01,789
AI responsibly and maintaining.

20
00:01:02,479 --> 00:01:03,529
The velocity as well.

21
00:01:03,800 --> 00:01:08,590
So this session, how we are embedding like
ethical AI safeguards into the deployment

22
00:01:08,590 --> 00:01:13,630
process and so that we can create
the pipeline faster and most lively.

23
00:01:13,729 --> 00:01:17,990
And so that AI systems so that we
can the stakeholder trust, right?

24
00:01:18,440 --> 00:01:20,990
So now we have a platform
engineer dilemma, right?

25
00:01:21,240 --> 00:01:24,780
The dilemma we all recognize,
like we are caught between a two.

26
00:01:25,515 --> 00:01:30,985
Powerful challenges which seems
to be like opposing forces, right?

27
00:01:31,225 --> 00:01:35,960
On the one hand we are facing
like the velocity pressure and so

28
00:01:35,960 --> 00:01:40,720
that we need to ship the product
faster and support more models.

29
00:01:41,110 --> 00:01:44,440
And on the other hand the other
hand, we have a non-negotiable

30
00:01:44,620 --> 00:01:46,090
mandate for responsible ai.

31
00:01:46,360 --> 00:01:48,715
So we must embed like a fairness.

32
00:01:49,215 --> 00:01:52,235
Avoid biases and protect
the privacy as well.

33
00:01:52,895 --> 00:01:56,595
And also and also ensuring
the compliance, right?

34
00:01:56,745 --> 00:02:01,355
So the consequences of getting things
wrong are severe and it, and like

35
00:02:01,355 --> 00:02:02,645
a reputational damage is there.

36
00:02:03,015 --> 00:02:06,785
And there are regulatory fines
and also a total loss of user

37
00:02:06,785 --> 00:02:08,615
trust if things go wrong.

38
00:02:08,855 --> 00:02:10,920
So the old mantra used to be like.

39
00:02:11,280 --> 00:02:15,480
We can go fast and break things,
but in today's world, breaking

40
00:02:15,480 --> 00:02:17,520
things, breaking people.

41
00:02:17,790 --> 00:02:22,710
So the challenge here is like, how do we
maintain a balance between a breakneck

42
00:02:22,710 --> 00:02:25,670
speed without breaking our commitments?

43
00:02:26,170 --> 00:02:30,010
So now we are going to talk about
the ROI of ethical AI platforms.

44
00:02:30,350 --> 00:02:33,140
The good news is that it
is not just a cost center.

45
00:02:33,890 --> 00:02:38,810
Ethical AI platforms building these
platforms deliver like a massive and

46
00:02:38,810 --> 00:02:40,310
measurable return on investments.

47
00:02:40,700 --> 00:02:44,740
So think about it like what happens
when you automate ethical checks.

48
00:02:45,080 --> 00:02:46,640
First you accelerate the velocity.

49
00:02:47,150 --> 00:02:48,770
That is the benefit we get.

50
00:02:49,130 --> 00:02:51,860
And like by catching issues
early in the pipeline, we

51
00:02:51,860 --> 00:02:54,210
avoid the mass massive delays.

52
00:02:54,510 --> 00:02:55,575
And also we avoid the rework.

53
00:02:56,155 --> 00:02:59,135
Required when problems are
found late in the production.

54
00:02:59,405 --> 00:03:03,465
So it leads to it leads to the
improvement in the deployment efficiency.

55
00:03:03,465 --> 00:03:07,755
So we see two 86% improvement
in deployment efficiency, right?

56
00:03:08,085 --> 00:03:10,935
And the second one is like we
are de-risking the deployment.

57
00:03:11,275 --> 00:03:17,595
So automated testing is leading
to 89% detection rate for bias

58
00:03:17,595 --> 00:03:21,915
issues pre-production and 84%
enhancement and compliance.

59
00:03:22,250 --> 00:03:23,360
So that is a big achievement.

60
00:03:23,670 --> 00:03:26,010
This is not just about avoiding fines.

61
00:03:26,040 --> 00:03:30,399
It is more about getting more
user trust and this trust

62
00:03:30,450 --> 00:03:32,459
translate into adoption, right?

63
00:03:32,760 --> 00:03:37,709
So as we can see like organizations
are like there, there's a 92% increase

64
00:03:37,709 --> 00:03:39,750
in sustainable AI adoption these days.

65
00:03:40,289 --> 00:03:43,859
And because developers are
trusting the platform and end

66
00:03:43,859 --> 00:03:44,974
user is trusting the outward.

67
00:03:45,474 --> 00:03:48,114
So this is how ethics becomes, right?

68
00:03:48,214 --> 00:03:50,855
A competitive advantage, not a bottleneck.

69
00:03:51,214 --> 00:03:54,269
So that's why it is important
that we integrate ethics.

70
00:03:54,950 --> 00:03:55,729
Into the picture.

71
00:03:56,149 --> 00:03:59,179
So now we will talk about
automated ethical testing in

72
00:03:59,179 --> 00:04:00,869
the next slide in the pipelines.

73
00:04:01,049 --> 00:04:02,579
So how do we achieve it?

74
00:04:02,579 --> 00:04:04,509
It starts by shifting left.

75
00:04:04,809 --> 00:04:08,289
Ethical testing must be automated
and integrated into the pipelines.

76
00:04:08,499 --> 00:04:11,189
This means treating and ethical
failures like a build failure.

77
00:04:11,744 --> 00:04:12,765
Or a unit test failure.

78
00:04:13,034 --> 00:04:17,084
So as soon as a model is committed,
the pipeline should be automatically

79
00:04:17,154 --> 00:04:19,075
run it against the battery of test.

80
00:04:19,515 --> 00:04:23,705
And we should check for a statistical
parity differences and equal opportunity.

81
00:04:24,094 --> 00:04:31,155
And and also using using tools like AI
Fairness 360 or lan, we can achieve that.

82
00:04:31,515 --> 00:04:35,135
And the key here is the process
should be automate and mandatory.

83
00:04:35,385 --> 00:04:36,705
It is like a quality gate.

84
00:04:37,015 --> 00:04:40,534
If the model does not pass this
gate and the bias check the

85
00:04:40,534 --> 00:04:42,034
pill should fail automatically.

86
00:04:42,455 --> 00:04:46,784
And this moves ethical validation
from manual to automated validation

87
00:04:47,145 --> 00:04:52,284
and so that we can automate this week
long processes and we can and we can

88
00:04:52,284 --> 00:04:55,525
also reduce validation cycle by 55%.

89
00:04:56,025 --> 00:05:00,914
Oh, like close to 56%
and 91% rapid issues.

90
00:05:00,944 --> 00:05:04,694
And it all mitigation effectiveness
also, and this is this is the

91
00:05:04,694 --> 00:05:08,764
foundation here and making like
ethics is like a non-negotiable

92
00:05:09,164 --> 00:05:10,694
part of the definition of done.

93
00:05:11,194 --> 00:05:15,714
So now we will talk about talking about
the platform native vehicle frameworks.

94
00:05:16,074 --> 00:05:19,314
If you talk about the tools, so
tools are not just enough, right?

95
00:05:19,344 --> 00:05:24,814
We need to build a, like a platform
framework which means like a big banking

96
00:05:24,964 --> 00:05:28,754
ethics into the infrastructure we provide.

97
00:05:29,384 --> 00:05:31,274
So we need to bake ethical
frameworks into it.

98
00:05:31,584 --> 00:05:36,054
So instead of asking every development
team to figure out on their own, if

99
00:05:36,054 --> 00:05:41,064
we can, if the platform team provides
like a curated, pre-approved ethical,

100
00:05:41,164 --> 00:05:43,174
tooling as a service we offer.

101
00:05:43,444 --> 00:05:48,034
So what we can do is we can standardize
model cards, which will auto-populate

102
00:05:48,145 --> 00:05:49,705
the fairness metrics, right?

103
00:05:50,005 --> 00:05:53,454
Second one we can do is we can
pre-build compliance data pipelines

104
00:05:53,794 --> 00:05:56,824
with the anonymization building, right?

105
00:05:57,124 --> 00:06:01,744
Third one is we can provide the approved
libraries for bias mitigation, right?

106
00:06:01,894 --> 00:06:05,644
So provide, by providing these,
services like this as like a managed

107
00:06:05,644 --> 00:06:10,084
services, we ensure like a consistency
into the product and we reduce the

108
00:06:10,324 --> 00:06:12,764
cognitive load for product teams.

109
00:06:13,274 --> 00:06:18,385
And also we guarantee every model
deployed into our platform meets like

110
00:06:18,455 --> 00:06:20,865
baseline ethical standard by default.

111
00:06:21,495 --> 00:06:25,759
So we make the right way and we make the
things right way and in the easy way.

112
00:06:26,259 --> 00:06:30,570
Now we are going to talk about the case
study for a healthcare AI platform.

113
00:06:30,929 --> 00:06:34,509
So let's make like a concrete
healthcare study discussion.

114
00:06:34,719 --> 00:06:37,349
So the stake here could
be could not be higher.

115
00:06:37,349 --> 00:06:41,399
We are dealing with a lot of patient
data and health data, so the platform

116
00:06:41,399 --> 00:06:44,869
team implemented these one of the
platform team implemented these exact

117
00:06:44,869 --> 00:06:49,219
principles, which we just talked
about, and they embedded like a ethical

118
00:06:49,270 --> 00:06:51,700
testing for bias in diagnostic models.

119
00:06:52,059 --> 00:06:56,540
And built the privacy preserving data
workflow directly into the platform.

120
00:06:56,930 --> 00:07:02,400
And and it gives 67% reduction in the
privacy incidents, and it gives like

121
00:07:02,400 --> 00:07:04,979
a 79% increase in clinical trust.

122
00:07:05,300 --> 00:07:11,730
And 92% sustainable a adoption
and 74% fewer bias concern,

123
00:07:12,180 --> 00:07:13,290
which is a big achievement.

124
00:07:13,830 --> 00:07:19,350
Now so the doctor could focus, on a
patient care because they trusted the

125
00:07:19,350 --> 00:07:24,850
platform more and at time ensured that the
model was fair and data was safe, right?

126
00:07:25,000 --> 00:07:29,220
So now we are going to talk about
the cloud scale ethical monitoring.

127
00:07:29,500 --> 00:07:32,740
By implementing monitoring
in place, it helps a lot.

128
00:07:32,945 --> 00:07:37,685
So how, so deployment is not just
about finishing the line, right?

129
00:07:37,925 --> 00:07:40,955
It is more about how
we can, how model can.

130
00:07:41,790 --> 00:07:46,770
Like model can degrade in the real world
through model drift and data drift, right?

131
00:07:47,060 --> 00:07:50,630
So what was fair yesterday might
not be the, might not be fair today.

132
00:07:50,910 --> 00:07:54,180
This is where cloud scale
ethical monitoring helps.

133
00:07:54,400 --> 00:07:56,550
So we need to monitor pipelines.

134
00:07:57,240 --> 00:08:00,510
So we need to monitor productions
in the real time and like by.

135
00:08:00,585 --> 00:08:05,205
Like by monitoring like real time events
which has probably been processed like

136
00:08:05,205 --> 00:08:07,905
thousands of events per second, right?

137
00:08:08,085 --> 00:08:13,395
And checking for drift and and
violations against our ethical baselines.

138
00:08:13,395 --> 00:08:14,955
So we should be monitoring all of these.

139
00:08:15,285 --> 00:08:19,045
And for that we need a
dedicated monitoring stack.

140
00:08:19,245 --> 00:08:21,005
So perhaps using tools like Fiddler.

141
00:08:21,860 --> 00:08:27,180
Or s core which like the, we can grade,
we can achieve the great outcomes.

142
00:08:27,180 --> 00:08:31,440
Like the outcome is to detect
91% violations we can detect and

143
00:08:31,440 --> 00:08:34,680
it'll improve the response time.

144
00:08:34,770 --> 00:08:38,040
Like we can increase, like you
can get a 78% faster incident

145
00:08:38,040 --> 00:08:40,300
response time with the help of this.

146
00:08:40,800 --> 00:08:44,550
So like automated alerting and
remediation workflows, engage the

147
00:08:44,550 --> 00:08:48,190
right teams immediately whenever
the potential issues arise.

148
00:08:48,290 --> 00:08:52,490
So now a critical question
is like, who decide?

149
00:08:52,540 --> 00:08:54,040
What is fair, right?

150
00:08:54,430 --> 00:08:58,990
So the, it cannot be like an
emerging decision made in a vacuum.

151
00:08:59,040 --> 00:09:00,870
Like what, who decide what is fair?

152
00:09:01,140 --> 00:09:04,440
This is where the cross-functional
governance is very much essential.

153
00:09:04,780 --> 00:09:08,710
We need to build a, like a lightweight
council with legal compliance,

154
00:09:08,710 --> 00:09:10,270
ethics and business, right?

155
00:09:10,620 --> 00:09:12,060
With the business representatives.

156
00:09:12,370 --> 00:09:14,395
The job here is to define what our.

157
00:09:15,315 --> 00:09:18,265
What are like, what are
the fairness thresholds?

158
00:09:18,565 --> 00:09:20,875
What is our definition of bias?

159
00:09:20,955 --> 00:09:24,305
So our job as a platform
engineer is to encode that.

160
00:09:24,965 --> 00:09:29,905
Into the policy as a code and so that
we can turn their human decisions

161
00:09:29,905 --> 00:09:31,985
into automated gates in our pipeline.

162
00:09:32,385 --> 00:09:36,645
Like that's why we are saying like a
policy as a code and the CO and this

163
00:09:36,645 --> 00:09:41,445
kind of collaboration which will make
the system more scalable and estimate,

164
00:09:41,745 --> 00:09:44,925
and it is why organization using this.

165
00:09:45,205 --> 00:09:50,635
Model report I'll get 75% better
project success rate and 91%

166
00:09:50,665 --> 00:09:52,765
effectiveness in the mitigating issues.

167
00:09:53,265 --> 00:09:57,255
So now it is very important, like
we build our ethical a platform.

168
00:09:57,475 --> 00:09:59,245
So how do we do that, right?

169
00:09:59,245 --> 00:10:01,735
So for that we need to define
the platform requirements.

170
00:10:02,005 --> 00:10:04,765
We need to integrate ethics
into the CICD pipelines.

171
00:10:04,765 --> 00:10:08,275
And then we have to develop
operation monitoring, right?

172
00:10:08,335 --> 00:10:12,385
So first we need to, pick one
high visibility model and audit

173
00:10:12,385 --> 00:10:16,295
it and create audit it for a
single metric like gender bias.

174
00:10:16,505 --> 00:10:18,545
And then tool up, like
experiment with the.

175
00:10:19,045 --> 00:10:23,215
Then we do experiment with one open
source framework and run it manually

176
00:10:23,215 --> 00:10:25,675
or model and see what it finds.

177
00:10:25,975 --> 00:10:31,065
Then we can third, integrate it like
and add like a single ethical test for

178
00:10:31,065 --> 00:10:35,730
a non-blocking check into our pipeline
and let developers see the report.

179
00:10:36,635 --> 00:10:41,935
And then fourth, like we can mandate like
we can graduate the test like a blocking

180
00:10:41,935 --> 00:10:45,715
it for our critical applications, right?

181
00:10:45,775 --> 00:10:48,195
And then we can add more tasks into it.

182
00:10:48,245 --> 00:10:52,085
And then we can develop operational
monitoring into it and formalize our

183
00:10:52,085 --> 00:10:54,670
governance council which is not a journey.

184
00:10:55,170 --> 00:10:57,360
It is like a journey, not a one stop.

185
00:10:57,700 --> 00:10:59,950
Like another flip you
with the project, right?

186
00:10:59,950 --> 00:11:03,700
So it is, it takes time to
develop the monitoring in place.

187
00:11:04,090 --> 00:11:06,340
So the key takeaway here is indeed.

188
00:11:07,265 --> 00:11:08,285
Ethical AI into it.

189
00:11:08,915 --> 00:11:09,515
Don't append.

190
00:11:09,515 --> 00:11:14,165
So ethical AI s should be more
infrastructure components or not.

191
00:11:14,405 --> 00:11:18,095
It should not be an afterthought
or add on to an existing platform.

192
00:11:18,515 --> 00:11:22,475
And we should automate the validation
building automated ethical AI

193
00:11:22,475 --> 00:11:27,185
testing, like ethical testing into
CICP pipelines to catch issues

194
00:11:27,185 --> 00:11:28,795
early without slowing deployment.

195
00:11:29,415 --> 00:11:31,005
And that is like govern through platform.

196
00:11:31,005 --> 00:11:33,825
So implement like a platform
native governance, which is

197
00:11:33,825 --> 00:11:35,825
scales with our AI initiatives.

198
00:11:36,175 --> 00:11:38,725
And it was through like with
the regulatory requirements.

199
00:11:39,225 --> 00:11:45,145
So when it is done right, the ethical
ai framework don't slow the deployment.

200
00:11:45,645 --> 00:11:51,085
It accelerate the sustainable AI
option by building confidence into the

201
00:11:51,085 --> 00:11:52,555
system and increasing the user trust.

202
00:11:53,285 --> 00:11:57,375
So thank you for attending and please
let me know if you have any questions.

