1
00:00:00,270 --> 00:00:02,100
Good morning, afternoon, everyone.

2
00:00:03,090 --> 00:00:04,560
Thank you for joining me.

3
00:00:04,560 --> 00:00:10,100
Today we are going to explore a
key shift in how we approach system

4
00:00:10,100 --> 00:00:11,840
monitoring and incident management.

5
00:00:12,340 --> 00:00:16,450
As distributed systems grow in
complexity, traditional monitoring

6
00:00:16,540 --> 00:00:18,610
methods are simply not enough.

7
00:00:19,390 --> 00:00:24,669
Today we will see how a powered
observability, specifically

8
00:00:24,669 --> 00:00:26,605
with Splunk, can help us.

9
00:00:27,340 --> 00:00:32,290
Beyond reactive troubleshooting
and into the realm of predictive

10
00:00:32,349 --> 00:00:34,690
sub minute incident resolution.

11
00:00:35,190 --> 00:00:39,809
By the end of the session, you'll
have a deeper understanding of how

12
00:00:39,809 --> 00:00:45,449
to improve system reliability, reduce
downtime, and take control of incidents

13
00:00:45,949 --> 00:00:48,454
before they affect your users.

14
00:00:48,899 --> 00:00:49,649
Let's dive in.

15
00:00:50,149 --> 00:00:51,589
The observability challenge.

16
00:00:52,089 --> 00:00:56,229
Today's distributed architecture have
triggered an observability crisis.

17
00:00:57,129 --> 00:01:02,800
Traditional siloed monitoring tools
produce floods of uncorrelated

18
00:01:02,800 --> 00:01:08,020
alerts, leaving IT teams were and
struggling to respond effectively.

19
00:01:08,520 --> 00:01:12,779
Modern systems generate vast
volumes of metrics, logs and

20
00:01:12,779 --> 00:01:18,029
traces, but capturing the right
data in context is the first hurdle.

21
00:01:18,899 --> 00:01:23,999
Detecting meaningful patterns across
diverse signals is increasingly difficult,

22
00:01:24,660 --> 00:01:29,285
and without intelligent correlation,
alerts remain noise rather than insect.

23
00:01:29,785 --> 00:01:33,895
The real challenge is turning the
data into actionable insights,

24
00:01:34,255 --> 00:01:39,595
ensuring observability translates into
better performance and resilience.

25
00:01:40,095 --> 00:01:45,925
The evolution of observability in
the early 20 century monitoring

26
00:01:46,134 --> 00:01:51,805
was about basic metrics like
CPU usage, memory, or the space.

27
00:01:52,525 --> 00:01:54,685
It was reactive, just an alert.

28
00:01:55,255 --> 00:02:01,225
When thresholds were crossed in 2000
tens, locks became central to debugging

29
00:02:01,734 --> 00:02:04,255
with tools like ELK and Splunk.

30
00:02:04,734 --> 00:02:09,435
Aggregating them for better visibility,
but locks were still siloed.

31
00:02:10,155 --> 00:02:13,675
Then in 2010, sorry, we saw the shift.

32
00:02:14,165 --> 00:02:19,960
To full observability, not just about
what's happening but why it's happening.

33
00:02:20,320 --> 00:02:27,160
Bringing metrics, logs, and traces
together, providing richer insights today.

34
00:02:27,610 --> 00:02:28,030
Yay.

35
00:02:28,270 --> 00:02:30,790
Powered observability
takes it even further.

36
00:02:31,510 --> 00:02:38,200
Analyzing trends, predictive failures,
and even initiating automated responses.

37
00:02:38,700 --> 00:02:41,880
Moving us from reactive to
proactive incident management.

38
00:02:42,380 --> 00:02:44,870
Observability as a strategic imperative.

39
00:02:45,370 --> 00:02:51,490
Most 94% of enterprises now consider
digital services critical to their

40
00:02:51,490 --> 00:02:57,630
core business, reflecting how
essential software and infrastructure

41
00:02:57,930 --> 00:03:00,060
are to delivering customer value.

42
00:03:00,990 --> 00:03:05,730
Eight 54% of organizations still lack.

43
00:03:06,465 --> 00:03:07,935
Full stack observability.

44
00:03:08,435 --> 00:03:13,505
Without comprehensive visibility across
infrastructure, applications, and user

45
00:03:13,505 --> 00:03:21,125
experiences, teams struggle to detect,
diagnose, and fix problems quickly.

46
00:03:21,625 --> 00:03:28,454
This lead to downtime, lost revenue,
and damages damaged customer impact.

47
00:03:28,954 --> 00:03:34,564
The good news organizations embracing
a powered observability have reduced

48
00:03:34,684 --> 00:03:41,334
their mean time to resolution by 45%,
and those with mature practices are

49
00:03:41,334 --> 00:03:44,035
seeing a 3.5 times return on investment.

50
00:03:44,934 --> 00:03:47,545
Observability has evolved beyond it.

51
00:03:48,205 --> 00:03:52,795
It's now a strategic capability
driving business performance.

52
00:03:53,295 --> 00:03:54,730
Observability platform.

53
00:03:55,230 --> 00:03:59,880
Splunk uses machine learning to
detect anomalies across different

54
00:03:59,880 --> 00:04:02,370
data layers in the metrics layer.

55
00:04:02,520 --> 00:04:08,160
It learns normal behavior over time,
like memory usage or request rates,

56
00:04:08,520 --> 00:04:11,190
and adjust thresholds automatically.

57
00:04:11,690 --> 00:04:16,310
Whereas in the log layer, it
analyzes patterns using NLP.

58
00:04:16,789 --> 00:04:22,700
To spot rare or unexpected log
entries like authentication failures.

59
00:04:23,200 --> 00:04:29,350
And in the traces layer, it detects
abnormal latencies or service hubs

60
00:04:29,530 --> 00:04:33,870
in microservices, which helps to
spot issues before they escalate.

61
00:04:34,370 --> 00:04:38,825
The real power comes when these
anomalies are correlated across layers.

62
00:04:39,395 --> 00:04:42,485
For example, a spike in response time.

63
00:04:43,205 --> 00:04:48,605
Tied to an error log burst and traced
back to the slow backend microservice

64
00:04:49,325 --> 00:04:54,725
instead of just more alerts coming from
all the systems, Splunk gives you cross

65
00:04:54,725 --> 00:05:00,275
layer visibility, reducing investigation
time, and increasing understanding.

66
00:05:00,775 --> 00:05:05,965
Once the pattern is detected,
Splunk can automatically execute

67
00:05:05,965 --> 00:05:10,405
remediation actions like tree
starting service or scaling resources.

68
00:05:10,905 --> 00:05:13,305
Automatic remediation with yay.

69
00:05:13,575 --> 00:05:17,535
Power observability isn't
just about directing problems,

70
00:05:18,075 --> 00:05:19,365
it's about taking action.

71
00:05:20,085 --> 00:05:24,075
When a pattern is detected,
automated remediation can kick in.

72
00:05:25,005 --> 00:05:29,925
This is where playbooks and intelligent
decision making comes into play.

73
00:05:30,425 --> 00:05:35,045
For example, if there is a spike
in CPU usage, the system might

74
00:05:35,045 --> 00:05:36,605
scale resources automatically.

75
00:05:37,295 --> 00:05:41,135
Start service if it detects
degraded performance.

76
00:05:41,945 --> 00:05:43,895
DA doesn't just stop there.

77
00:05:44,225 --> 00:05:50,135
It learns from past incidents improving
its ability to respond to future issues.

78
00:05:50,975 --> 00:05:56,315
In complex environments like Kubernetes
or hybrid clouds, this proactive

79
00:05:56,315 --> 00:06:03,385
approach is crucial in ensuring minimal
downtime as and faster recovery times.

80
00:06:03,885 --> 00:06:06,765
Let's dive into technical architecture

81
00:06:07,265 --> 00:06:07,955
data collection.

82
00:06:08,455 --> 00:06:13,245
We started the source data collection,
Splunk universal forwarders and

83
00:06:13,245 --> 00:06:19,005
lightweight agents designed for reliable
log forwarding from on-prem systems.

84
00:06:19,995 --> 00:06:25,515
For metrics and tracers, we leverage open
telemetry and open source vendor neutral

85
00:06:25,515 --> 00:06:30,195
standard that allows deep instrumentation
across apps and infrastructure.

86
00:06:31,155 --> 00:06:36,315
Splunk also supports out of the box
integration with cloud services like

87
00:06:36,465 --> 00:06:43,665
AWS, Azure and GCP, making it seamless
to ingest data from cloud native sources.

88
00:06:44,165 --> 00:06:47,645
As the data is gathered, now
we need to process the pipeline

89
00:06:48,145 --> 00:06:49,705
process using pipeline, sorry.

90
00:06:50,305 --> 00:06:54,055
Once the data is ingested, it
enters Splunk processing pipeline.

91
00:06:54,745 --> 00:06:58,015
Here we perform field
extraction and enrichment.

92
00:06:58,615 --> 00:07:04,495
Adding context like geolocation,
device info and mode normalization

93
00:07:04,495 --> 00:07:08,785
ensures that regardless of the
source, your data is structured.

94
00:07:09,190 --> 00:07:11,770
And searchable in a unified format.

95
00:07:12,270 --> 00:07:18,720
Then we inject correlation IDs, which
are essential for tying logs, metrics,

96
00:07:18,750 --> 00:07:21,480
and traces together across services.

97
00:07:22,290 --> 00:07:25,560
This allows true end-to-end observability.

98
00:07:26,060 --> 00:07:30,440
The AI L layer, this is where
the intelligence kicks in.

99
00:07:30,940 --> 00:07:34,510
The A ML layer starts
with feature extraction.

100
00:07:34,975 --> 00:07:37,525
Identifying signals from noise.

101
00:07:38,025 --> 00:07:42,944
Splunk then applies pattern recognition
models to baseline system behavior Over

102
00:07:42,944 --> 00:07:50,105
time when deviations occur, anomaly
detection engine alerts you often before

103
00:07:50,225 --> 00:07:51,815
users even notice something wrong.

104
00:07:52,315 --> 00:07:57,280
Visualization and response, whatever
we have process now becomes actionable.

105
00:07:58,255 --> 00:08:04,765
Splunk offers rich real-time dashboards
that display KPI, error rates,

106
00:08:04,855 --> 00:08:11,305
service latency, and more automated
alerting routes, insights to the

107
00:08:11,305 --> 00:08:16,855
right teams via Slack or PagerDuty,
or email or any alerting mechanism.

108
00:08:17,755 --> 00:08:21,205
And with remediation workflows,
you can go a step further

109
00:08:21,955 --> 00:08:26,875
regarding scripts or runbooks to
fix common issues automatically.

110
00:08:27,375 --> 00:08:30,555
Here the Splunk strength
we should consider.

111
00:08:31,455 --> 00:08:36,284
Splunk is built for massive scalable
to handle millions of events per second

112
00:08:36,885 --> 00:08:39,075
without compromising query performance.

113
00:08:39,794 --> 00:08:44,565
The system is distributed for all
tolerant and optimized for high ingestion

114
00:08:44,565 --> 00:08:50,745
environments, which makes it strong fit
for enterprise grade observability needs.

115
00:08:51,245 --> 00:08:53,255
Feature selection and model tuning.

116
00:08:53,755 --> 00:09:00,325
When building AI models for observability,
what we feed into the model is just

117
00:09:00,625 --> 00:09:02,815
as important as how we tune it.

118
00:09:03,315 --> 00:09:06,795
We go beyond basic metrics
like CPU and memory.

119
00:09:07,590 --> 00:09:12,630
We combine signals from infrastructure,
application, behavior, user interactions,

120
00:09:12,960 --> 00:09:17,010
and business performance to get the
complete picture of system health.

121
00:09:17,510 --> 00:09:19,580
A good model needs careful tuning.

122
00:09:20,240 --> 00:09:22,839
If it's too sensitive, you get noise.

123
00:09:23,349 --> 00:09:26,444
If it's too specific, you
miss out real problems.

124
00:09:27,399 --> 00:09:32,319
We refine our models using real
world failures, improving accuracy

125
00:09:32,680 --> 00:09:34,509
and reducing false alarms over time.

126
00:09:35,009 --> 00:09:38,849
Let's look at a case study
of an e-commerce company

127
00:09:39,349 --> 00:09:41,060
during their annual sales event.

128
00:09:41,329 --> 00:09:42,785
Traffic spikes to 10 times.

129
00:09:43,045 --> 00:09:43,824
Its regular volume.

130
00:09:44,324 --> 00:09:47,885
Critical services like payment
processing comes under immense pressure.

131
00:09:48,385 --> 00:09:50,305
Using Yay powered observability.

132
00:09:50,455 --> 00:09:56,335
The team trained machine learning models
on normal operating behavior and potential

133
00:09:56,335 --> 00:10:01,485
failure patterns as traffic client
yay detected a bottleneck in payment

134
00:10:01,485 --> 00:10:06,605
processing, which detected 15 minutes
earlier than traditional alerting system

135
00:10:06,635 --> 00:10:08,615
would've triggered Autoscaling actions

136
00:10:09,115 --> 00:10:10,105
this earlier.

137
00:10:10,605 --> 00:10:15,345
Helped prevent a slow checkout
process that could have cost

138
00:10:15,705 --> 00:10:17,505
2.3 million in lost sales.

139
00:10:18,005 --> 00:10:24,335
This shows how a driven observability
can turn data into real time

140
00:10:24,515 --> 00:10:30,035
actionable insights to prevent
issues before impacting the business

141
00:10:30,535 --> 00:10:33,625
building autonomous remediation workflows.

142
00:10:34,345 --> 00:10:36,565
Now let's explore autonomous remediation.

143
00:10:37,375 --> 00:10:41,665
It starts with anomaly detection,
using machine learning algorithms to

144
00:10:41,665 --> 00:10:43,795
spot deviations in system behavior.

145
00:10:44,455 --> 00:10:49,105
Next, the system performs root cause
analysis, leveraging correlation

146
00:10:49,105 --> 00:10:51,625
engines to identify the cost of issues.

147
00:10:52,125 --> 00:10:56,055
Once the cause is determined, the system
selects the appropriate remediation

148
00:10:56,055 --> 00:10:57,975
response based on the historical data.

149
00:10:58,755 --> 00:11:02,835
For example, if scaling resources
worked in the past, it might

150
00:11:02,835 --> 00:11:03,915
trigger that action again.

151
00:11:04,575 --> 00:11:08,385
The system then executes
these actions automatically.

152
00:11:09,315 --> 00:11:14,865
Finally, after the remediation, the system
verifies the results, gathering feedback

153
00:11:15,585 --> 00:11:17,895
to continuously improve its responses.

154
00:11:18,705 --> 00:11:19,965
This learning loop.

155
00:11:20,465 --> 00:11:25,595
Allows the system to handle more complex
incidents automatically over time.

156
00:11:26,095 --> 00:11:32,035
Implementation framework To implement
AI powered observability, we start

157
00:11:32,035 --> 00:11:36,150
in identifying critical services and
defining instrumentation standards.

158
00:11:36,980 --> 00:11:41,710
This ensures that data collection
is consistent and comprehensive.

159
00:11:42,210 --> 00:11:47,070
We also set data retention policies
to balance cost and compliance.

160
00:11:47,880 --> 00:11:52,380
On the technical side, we deploy
correlation agents, configuring

161
00:11:52,380 --> 00:11:56,160
data pipelines, and build service
map to visualize dependencies.

162
00:11:56,970 --> 00:12:00,900
We implement machine learning
models to detect anomalies and

163
00:12:00,900 --> 00:12:04,530
align our observability data
with incident management process.

164
00:12:05,030 --> 00:12:10,130
This framework is designed to provide
incremental value at each stage.

165
00:12:10,925 --> 00:12:14,675
From initial data collection
to full maturity, which takes

166
00:12:14,675 --> 00:12:16,025
about four to six months.

167
00:12:16,525 --> 00:12:18,565
Next steps on your observability journey.

168
00:12:19,065 --> 00:12:24,615
Start by assessing your current maturity
using an online tool to identify

169
00:12:24,615 --> 00:12:26,565
gaps in your observability practices.

170
00:12:27,065 --> 00:12:30,725
With this assessment, you can
develop a tailored strategy

171
00:12:30,905 --> 00:12:31,685
to enhance your observability.

172
00:12:32,185 --> 00:12:32,665
Next.

173
00:12:32,965 --> 00:12:38,605
We recommend running a pilot project in
the targeted area demonstrating quick

174
00:12:38,605 --> 00:12:41,905
wins while minimizing risk as you scale.

175
00:12:41,905 --> 00:12:47,935
Building internal capabilities through
training ensures long-term success.

176
00:12:48,435 --> 00:12:54,274
Our team is here to help guide you
through these steps, ensuring you maximize

177
00:12:54,274 --> 00:12:59,554
the value of a powered observability
and make your system more resilient.

178
00:13:00,054 --> 00:13:01,824
I should talk about the challenges.

179
00:13:02,694 --> 00:13:06,894
Like any technology, yay, power
observability has its own challenge.

180
00:13:07,734 --> 00:13:12,054
Implementing this solution requires
solid data strategy integration

181
00:13:12,144 --> 00:13:15,864
with existing systems and continuous
tuning to ensure accuracy.

182
00:13:16,584 --> 00:13:22,034
Additionally, there may be an initial
learning curve as team adjusts to the

183
00:13:22,904 --> 00:13:24,824
predictive nature of AI power tools.

184
00:13:25,324 --> 00:13:29,859
A power observability with Splunk
can revolutionize how we manage

185
00:13:30,339 --> 00:13:32,319
monitor modern distributed systems.

186
00:13:33,009 --> 00:13:38,199
By moving from reactive monitoring to
predictive management, we can not only

187
00:13:38,199 --> 00:13:42,969
reduce instant resolution time, but
also improve the overall reliability

188
00:13:43,119 --> 00:13:44,889
and performance of water systems.

189
00:13:45,389 --> 00:13:46,589
Thank you for your time today.

