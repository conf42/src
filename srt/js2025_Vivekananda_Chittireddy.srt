1
00:00:00,500 --> 00:00:01,130
Hi everyone.

2
00:00:01,729 --> 00:00:05,659
Welcome to this session on Privacy by
Design for JavaScript Data Systems.

3
00:00:06,350 --> 00:00:09,740
I'm excited to share how we can build
secure and compliant architectures

4
00:00:09,770 --> 00:00:13,370
that respect user privacy while
maintaining full system functionality.

5
00:00:13,870 --> 00:00:15,250
Allow me to introduce myself.

6
00:00:15,750 --> 00:00:18,180
I'm re I'm a data engineer at Petta.

7
00:00:18,630 --> 00:00:22,020
I specialize in privacy centric
data architectures and real

8
00:00:22,020 --> 00:00:23,580
time data streaming systems.

9
00:00:24,360 --> 00:00:26,040
I spent many years at Meta.

10
00:00:26,565 --> 00:00:31,365
Building architectures and systems
that are privacy aware, both in data

11
00:00:31,365 --> 00:00:33,165
infrastructure and data processing.

12
00:00:33,665 --> 00:00:37,355
Today in this conference, we're gonna
explore the critical intersection

13
00:00:37,355 --> 00:00:41,765
of privacy, compliance, and modern
JavaScript data engineering.

14
00:00:42,265 --> 00:00:45,325
This is not just about
following regulations.

15
00:00:45,325 --> 00:00:49,405
This is about fundamentally rethinking
how we architect our systems.

16
00:00:50,335 --> 00:00:54,295
And as a quick disclaimer, what I'm
about to share is in no way reflection

17
00:00:54,295 --> 00:00:56,095
of how we build systems at matter.

18
00:00:56,935 --> 00:01:00,995
This is purely out of my experience
in building data systems that are

19
00:01:00,995 --> 00:01:02,735
privacy compliant and privacy aware.

20
00:01:03,235 --> 00:01:05,725
Without further ado, let's get started.

21
00:01:05,815 --> 00:01:07,615
We're talking about why this matters.

22
00:01:08,115 --> 00:01:12,525
Currently, we have about 140 countries
which have strong privacy regulations

23
00:01:13,035 --> 00:01:15,180
and the average cost of data breach.

24
00:01:16,000 --> 00:01:22,600
Is in the millions, about $5 million in
2024, and that's a pretty price, pretty

25
00:01:22,600 --> 00:01:24,480
high price for any firm to take on.

26
00:01:25,260 --> 00:01:30,640
And the scary part is out of the data
that is, that was lost or that was

27
00:01:30,640 --> 00:01:37,770
stolen 80% of 87% of US citizens can be
identified using simply three attributes

28
00:01:37,770 --> 00:01:39,555
like zip code, date of birth, and gender.

29
00:01:40,055 --> 00:01:43,385
And this is not some theory, this
is actually happening right now,

30
00:01:43,885 --> 00:01:45,475
and privacy is no longer optional.

31
00:01:45,775 --> 00:01:51,685
It's a technical and business requirement
for any modern Java data script systems.

32
00:01:52,185 --> 00:01:53,955
So what exactly is privacy design?

33
00:01:54,455 --> 00:01:59,095
It's a proactive approach that
embeds both privacy protections into

34
00:01:59,095 --> 00:02:02,415
the system, architecture from the
ground up, and privacy compliance.

35
00:02:03,360 --> 00:02:06,990
And for JavaScript developers, this means
integrating privacy controls through

36
00:02:06,990 --> 00:02:09,060
the entire data engineering lifecycle.

37
00:02:09,990 --> 00:02:13,770
From the moment data is collected on
the client side through the storage

38
00:02:13,830 --> 00:02:18,510
in no JS packets, all the way to
processing and analytics, privacy

39
00:02:18,510 --> 00:02:20,220
must be baked in at every step.

40
00:02:20,405 --> 00:02:20,755
There

41
00:02:21,255 --> 00:02:24,085
are four core principles
that I wanna discuss.

42
00:02:25,070 --> 00:02:27,560
The first thing is
proactive and not reactive.

43
00:02:28,340 --> 00:02:32,540
We anticipate and prevent privacy
issues even before they occur.

44
00:02:33,040 --> 00:02:34,540
The second one is privacy.

45
00:02:34,540 --> 00:02:40,050
As a default, we choose to set in
maximum privacy controls in the

46
00:02:40,050 --> 00:02:42,530
system architecture automatically.

47
00:02:42,620 --> 00:02:46,460
Without user intervention, we do not
expect them to come and choose that.

48
00:02:46,460 --> 00:02:49,010
We would rather wanna
enable that from the get go.

49
00:02:49,700 --> 00:02:50,330
The third thing.

50
00:02:51,320 --> 00:02:53,600
Privacy is embedded
into the design itself.

51
00:02:54,380 --> 00:02:57,650
As in as you get started with
the design of the product.

52
00:02:57,770 --> 00:02:59,600
That's when you start
thinking about privacy.

53
00:02:59,600 --> 00:03:00,860
And it's not an afterthought.

54
00:03:00,860 --> 00:03:04,250
It's not something that you add
on at the tail end because we

55
00:03:04,250 --> 00:03:05,640
all know that doesn't scale.

56
00:03:06,140 --> 00:03:10,730
And finally, full functionality,
because we're all passionate

57
00:03:10,730 --> 00:03:13,850
developers and engineers and
architects, and we don't want to

58
00:03:13,850 --> 00:03:16,100
compromise the product functionality.

59
00:03:16,940 --> 00:03:19,370
So we'll be talking about how to do this.

60
00:03:19,870 --> 00:03:24,970
Privacy centric as well as keeping full
functionality, keeping the product usable.

61
00:03:25,470 --> 00:03:27,390
Let's dive into some concrete techniques.

62
00:03:27,890 --> 00:03:29,330
The first one is K annuity.

63
00:03:29,830 --> 00:03:33,880
K annuity ensures that every record
in your dataset is indistinguishable

64
00:03:34,090 --> 00:03:35,710
from at least K minus one.

65
00:03:35,950 --> 00:03:43,050
Other records say if you choose your K
two B five, then any individual data.

66
00:03:43,965 --> 00:03:49,485
Individuals data looks identical to four
other individuals, and this prevents

67
00:03:49,985 --> 00:03:55,415
hackers or users from, when I say
user system users, from tracing

68
00:03:55,415 --> 00:03:59,375
back that individual and figuring
out their demographic traits.

69
00:03:59,875 --> 00:04:03,645
And the other concept is l
Diversity, which extends this

70
00:04:03,645 --> 00:04:07,215
by ensuring diverse, sensitive
attribute values within each group.

71
00:04:07,275 --> 00:04:10,065
So not only are the
people, group together.

72
00:04:10,455 --> 00:04:12,765
But each group has varied characteristics.

73
00:04:12,765 --> 00:04:17,955
So this is essential for healthcare
data especially, or any sensitive

74
00:04:17,955 --> 00:04:22,005
protective data where for example,
in healthcare setting, you wouldn't

75
00:04:22,005 --> 00:04:26,350
want all the patients sharing same
medical condition in one group.

76
00:04:27,160 --> 00:04:30,400
If that group's data is lost, then
that would defeat the purpose.

77
00:04:30,400 --> 00:04:31,060
So you wanna.

78
00:04:31,560 --> 00:04:36,040
Scatter it all over so it's not easy to
trace back to, to a specific individual.

79
00:04:36,540 --> 00:04:39,990
And next, how does this differential
privacy look in practice?

80
00:04:40,770 --> 00:04:43,490
And differential privacy is
where math gets very interesting.

81
00:04:43,940 --> 00:04:47,500
So you add in this, we basically
add calibrated mathematical

82
00:04:47,500 --> 00:04:48,850
noise to carry results.

83
00:04:48,850 --> 00:04:52,960
So we ensure that the presence or
absence of any single individual does

84
00:04:52,960 --> 00:04:55,000
not sig significantly affect outcomes.

85
00:04:55,500 --> 00:04:56,850
And Epsilon values.

86
00:04:56,880 --> 00:05:01,150
This is a defense of privacy
concept between 0.1 to 1.0, give

87
00:05:01,200 --> 00:05:05,550
strong privacy gies while still
maintaining statistical utility.

88
00:05:05,550 --> 00:05:09,660
If you go beyond 1.0, you might be adding
too much noise and you cannot trace back,

89
00:05:09,660 --> 00:05:13,960
between 0.1 and one will give you enough
salting into the data and that will still

90
00:05:13,960 --> 00:05:15,525
keep statistical utility of the data.

91
00:05:16,025 --> 00:05:19,295
And the key insight here is that
individual records become impossible

92
00:05:19,295 --> 00:05:23,925
to identify and aggregate patterns
become become clear or remain clear.

93
00:05:24,615 --> 00:05:27,825
And finally, for JavaScript
implementations, we have libraries

94
00:05:27,825 --> 00:05:32,735
like no differential privacy for no js
that can implement local differential

95
00:05:32,735 --> 00:05:34,355
privacy directly in the browser.

96
00:05:34,925 --> 00:05:39,035
And this enables privacy, safe
analytics without sacrificing the

97
00:05:39,035 --> 00:05:40,985
the insights your business needs.

98
00:05:41,485 --> 00:05:44,245
The next type of privacy
that I wanna talk about is

99
00:05:44,325 --> 00:05:46,515
pseudonymization and tokenization.

100
00:05:47,425 --> 00:05:50,690
Pseudonymization and tokenization are
your bread and butter for handling

101
00:05:50,690 --> 00:05:53,730
PII with pseudo anonymization.

102
00:05:53,760 --> 00:05:58,440
You are gonna replace identifying
fields with pseudonyms while maintaining

103
00:05:58,510 --> 00:06:00,520
referential integrity for analytics.

104
00:06:01,020 --> 00:06:07,600
Think about replacing, an SSN with an
internal nont traceable ID that won't,

105
00:06:08,170 --> 00:06:12,190
that cannot be associated back to a user
if the data gets lost or gets stolen

106
00:06:12,190 --> 00:06:16,140
or gets accessed by an a stakeholder
who is not supposed to access that.

107
00:06:16,640 --> 00:06:19,700
And tokenization pushes this
further by generating random

108
00:06:19,700 --> 00:06:22,580
tokens that map sensitive data
stored in a separate secure vault.

109
00:06:23,180 --> 00:06:24,680
So this is even more secure.

110
00:06:25,475 --> 00:06:28,905
And the beauty is that your analytics
pipeline never touches real PII.

111
00:06:28,905 --> 00:06:33,135
So basically, the analytical pipeline
is running on those randomized tokens

112
00:06:33,135 --> 00:06:38,285
that got generated and let's, analysts
still derive meaningful results, but

113
00:06:38,285 --> 00:06:42,485
when you really have to access that, then
you can bring that data for m accesses.

114
00:06:42,535 --> 00:06:47,355
You can bring that data and join that
back to the to the tokens and, let's

115
00:06:47,355 --> 00:06:49,805
talk about some advanced technologies.

116
00:06:50,305 --> 00:06:53,705
For those who are ready to push
boundaries homomorphic encryption let's

117
00:06:53,705 --> 00:06:57,245
you perform computations and encrypted
data without ever decrypting it.

118
00:06:57,725 --> 00:07:02,015
So imagine running machine learning models
that maintain up to 95% accuracy while

119
00:07:02,015 --> 00:07:03,515
the data remains encrypted throughout.

120
00:07:04,185 --> 00:07:07,965
Secure multiparty computation enables
multiple organizations to jointly

121
00:07:07,965 --> 00:07:11,565
combine their combined data without
anyone seeing others' information.

122
00:07:12,320 --> 00:07:14,300
Think about something to
the effect of blockchain.

123
00:07:14,770 --> 00:07:18,540
And these are actually being implemented
today in healthcare and finance.

124
00:07:18,540 --> 00:07:22,120
And I literally wrote a paper
on healthcare analytics through

125
00:07:22,120 --> 00:07:26,060
differential differential privacy
and homomorphic techniques.

126
00:07:26,810 --> 00:07:30,020
Let's talk about healthcare because that's
definitely an interesting case study.

127
00:07:30,520 --> 00:07:32,995
In, in, in one of the case studies
that I had done, we had worked with

128
00:07:32,995 --> 00:07:36,085
the healthcare consortium that we
needed to enable medical research

129
00:07:36,085 --> 00:07:39,295
across multiple institutions
without any exposing patient data.

130
00:07:39,815 --> 00:07:43,625
And using JavaScript based federated
learning with differential privacy.

131
00:07:44,115 --> 00:07:48,225
Patient records never left hospital
servers, so ML models were trained

132
00:07:48,230 --> 00:07:51,705
locally on encrypted data and
only differentially private.

133
00:07:51,989 --> 00:07:53,999
Model updates were shared centrally.

134
00:07:54,509 --> 00:07:59,364
And the result is a fully hippy
and GDPR compliant data sets while

135
00:07:59,364 --> 00:08:01,314
enabling breakthrough research.

136
00:08:01,685 --> 00:08:06,355
And this was keeping data local and
sharing only privacy preserved insights.

137
00:08:06,565 --> 00:08:10,215
And think about this in terms of
of, the amount of research that can

138
00:08:10,215 --> 00:08:15,105
happen, the amount of change that can
be brought about in sensitive areas

139
00:08:15,105 --> 00:08:20,115
like cancer research or other hip
hop protected categories where you do

140
00:08:20,115 --> 00:08:25,105
want, doctors and hospitals to share
their knowledge across, organizations

141
00:08:25,855 --> 00:08:27,310
while still protecting data privacy.

142
00:08:27,370 --> 00:08:29,825
And you wouldn't wanna
share this data as it is.

143
00:08:30,185 --> 00:08:30,965
So this is where we are.

144
00:08:31,465 --> 00:08:35,110
We've explored a concept and I've written
a paper on it of how do you implement

145
00:08:35,110 --> 00:08:39,520
models locally in, in a hospital, but
share that results, share the differential

146
00:08:39,520 --> 00:08:44,575
results of the hospital the aggregates
or the model results to, to to the next

147
00:08:44,575 --> 00:08:45,985
one so they can build on top of it.

148
00:08:46,485 --> 00:08:48,635
Let's talk about one more case study.

149
00:08:48,965 --> 00:08:51,375
And this one is financial services.

150
00:08:51,835 --> 00:08:53,215
In financial services.

151
00:08:53,315 --> 00:08:56,725
We implemented another tokenization
at the point of payment collection.

152
00:08:57,205 --> 00:09:00,475
So card details are immediately
tokenized as they're collected.

153
00:09:00,565 --> 00:09:06,005
Enable, enabling real time fraud detection
without exposing actual card numbers.

154
00:09:06,425 --> 00:09:09,275
So we basically token as that
information and send it to the

155
00:09:09,275 --> 00:09:12,605
back office for quick real time
analytics using streaming pipelines.

156
00:09:13,265 --> 00:09:15,995
And at the same time customer
behavioral analytics are non key

157
00:09:16,325 --> 00:09:19,490
aggregations providing valuable
insights while protecting user privacy.

158
00:09:19,990 --> 00:09:23,810
Imagine you're, you are, you're
using a third party company or to

159
00:09:23,810 --> 00:09:25,340
run analytics on your customer.

160
00:09:25,340 --> 00:09:30,000
You're sharing with credit score companies
or anybody like that where you don't

161
00:09:30,000 --> 00:09:33,180
wanna share your data, but you don't
want to share it at an individual level.

162
00:09:33,240 --> 00:09:38,155
This is where a key ization or s
clustering or, they come in handy.

163
00:09:38,155 --> 00:09:42,335
And, if you create a cohort of
25, 50 people that'll protect

164
00:09:42,335 --> 00:09:46,405
your your data while while still
giving you great insights into

165
00:09:46,675 --> 00:09:48,445
customer behavior and analytics.

166
00:09:48,945 --> 00:09:54,075
So let's talk about the architectural
blueprint for oh implementing this.

167
00:09:54,575 --> 00:09:57,070
The first thing is the layer
one is data collection.

168
00:09:57,520 --> 00:10:01,680
So we implement consistent concern
management and minimize collection

169
00:10:01,680 --> 00:10:06,810
scope and immediately apply
pseudonymization using browser APIs.

170
00:10:07,800 --> 00:10:11,115
So right as you bring data,
you are very cognizant of what

171
00:10:11,115 --> 00:10:12,375
you're bringing into the system.

172
00:10:12,375 --> 00:10:14,205
Instead of just bringing
all kinds of data.

173
00:10:14,705 --> 00:10:18,005
The second layer is storage, and
here where we encrypt all the data

174
00:10:18,005 --> 00:10:21,625
that is that is sitting addressed
and we are separating, identifying

175
00:10:21,625 --> 00:10:23,815
information into different databases.

176
00:10:24,295 --> 00:10:28,545
You have one that is like truly
sensitive pi i a that sits separately

177
00:10:28,545 --> 00:10:34,065
from the nonsensitive data, like
transactional records or whatever that is.

178
00:10:34,355 --> 00:10:37,985
And you ensure that there are
strict access controls that are

179
00:10:37,985 --> 00:10:39,875
delineating data across both systems.

180
00:10:40,375 --> 00:10:44,495
Clear three is processing and you're
applying differential privacy across

181
00:10:44,495 --> 00:10:48,005
all queries and using secure and
place or sensitive operations.

182
00:10:48,005 --> 00:10:51,065
You're not just like running user
level information, you're trying

183
00:10:51,065 --> 00:10:56,675
to aggregate care ization K means
clustering or any of these techniques

184
00:10:56,675 --> 00:10:57,785
that we have discussed so far.

185
00:10:58,580 --> 00:11:01,730
And for the analytics, you're
only generating insights

186
00:11:01,730 --> 00:11:03,230
from anonymized data sets.

187
00:11:03,230 --> 00:11:07,190
You're going back to your storage layer
too, that you're only extracting or using

188
00:11:07,190 --> 00:11:09,600
the data from the de-identified data.

189
00:11:10,100 --> 00:11:12,560
And you're main maintaining
comprehensive audit trails.

190
00:11:12,560 --> 00:11:17,420
And this really comes in handy especially
for one, if there is unfortunate

191
00:11:17,420 --> 00:11:21,410
breach or two when you are getting a
third party to certify your system.

192
00:11:21,410 --> 00:11:22,950
Especially this is, for.

193
00:11:23,370 --> 00:11:25,010
Folks in banking and insurance.

194
00:11:25,010 --> 00:11:26,710
This the best and also healthcare.

195
00:11:27,380 --> 00:11:28,890
The layer five is access.

196
00:11:28,920 --> 00:11:32,570
You are, you're enforcing a role-based
access control system where you

197
00:11:32,570 --> 00:11:36,890
are ensuring that there is purpose,
limitations, and you are managing

198
00:11:36,890 --> 00:11:38,480
your user rights thoroughly.

199
00:11:39,110 --> 00:11:42,670
And in this one, the most important
thing is principle of least privileges.

200
00:11:42,670 --> 00:11:45,790
You don't want to give people
access to data that they

201
00:11:45,790 --> 00:11:47,140
don't really need access to.

202
00:11:47,640 --> 00:11:53,630
And then let's talk about the JavaScript
tools and patterns for implementation.

203
00:11:53,630 --> 00:11:58,010
You've got powerful tools at your
disposal and nojs use the built in crypto

204
00:11:58,010 --> 00:12:03,620
module for encryption and no different
to privacy for implementing TP and JSO

205
00:12:03,620 --> 00:12:05,360
Web token for secure token management.

206
00:12:05,690 --> 00:12:08,960
And on the brows browser side,
you have a subtle crypto, API

207
00:12:08,960 --> 00:12:09,805
for client side encryption.

208
00:12:10,305 --> 00:12:13,125
You can implement local difference
and privacy before data leaves

209
00:12:13,125 --> 00:12:14,985
the browser and use privacy.

210
00:12:14,985 --> 00:12:19,695
Preserving analytics, SDKs,
architecturally adopting privacy first,

211
00:12:19,695 --> 00:12:23,385
API design patterns, implementing
zero knowledge authentication where

212
00:12:23,385 --> 00:12:27,855
possible, and considering federated data
processing to keep data distributed.

213
00:12:28,355 --> 00:12:31,865
This is definitely a lot of information
on this slide that I, went through,

214
00:12:31,865 --> 00:12:36,065
but you'll have access to this deck
so you can go back and reference it.

215
00:12:36,565 --> 00:12:43,185
And finally let me wrap this up
with common pitfalls to, to avoid

216
00:12:43,685 --> 00:12:46,135
firstly logging PIA in plain text.

217
00:12:46,375 --> 00:12:51,320
Your application logs, error messages,
debug, output, or offense showed

218
00:12:51,320 --> 00:12:53,005
indefinitely and are searchable.

219
00:12:53,785 --> 00:12:57,535
Once stack trace with user data
can undo all your privacy works.

220
00:12:58,035 --> 00:13:04,215
Secondly, weak ization using MD I
assets, hashes or sequential id.

221
00:13:04,365 --> 00:13:05,685
Isn't reallys anonymization?

222
00:13:06,555 --> 00:13:08,865
It's just a sophistication.

223
00:13:09,435 --> 00:13:15,815
These patterns are trivially reversible
and I've, I'm yet to see a system, where

224
00:13:15,815 --> 00:13:19,765
we, where people don't confuse or an
organization where people don't confuse,

225
00:13:20,155 --> 00:13:22,945
hashing with encrypting they're traceable.

226
00:13:22,945 --> 00:13:24,835
They can be laid back.

227
00:13:25,795 --> 00:13:27,475
And third thing is our collection mindset.

228
00:13:28,150 --> 00:13:32,530
Trying to grab every data on the get go
without figuring out a plan of how you

229
00:13:32,530 --> 00:13:37,330
plan to use it or how you try to protect
it is a very bad precedent to start with.

230
00:13:38,110 --> 00:13:39,670
Think about it very proactively.

231
00:13:39,670 --> 00:13:43,750
Think about what you want to do with that
data before you even collect that data

232
00:13:43,750 --> 00:13:46,690
and be very methodical about doing this.

233
00:13:47,190 --> 00:13:50,230
Fourthly implementing privacy
only on the client side.

234
00:13:50,730 --> 00:13:53,370
Browser based controls are
important, but never sufficient.

235
00:13:53,370 --> 00:13:57,100
Any, and any developer with
their tools can bypass them.

236
00:13:57,100 --> 00:14:00,780
So ensure that you're implementing
both on the client side and on the

237
00:14:00,780 --> 00:14:02,220
server side or the backend side.

238
00:14:02,720 --> 00:14:08,090
And fifthly ignoring data retention,
keeping your data forever, just in case.

239
00:14:08,180 --> 00:14:12,980
While it's many privacy regulations
including GDPR, California Privacy

240
00:14:12,980 --> 00:14:18,700
Raw, and not just only that and in an
unfortunate incident, all that data

241
00:14:18,700 --> 00:14:23,180
that's been sitting around that is
is much more expensive if a, if an

242
00:14:23,180 --> 00:14:24,740
intruder gets access to that data.

243
00:14:25,240 --> 00:14:30,380
And the good thing is all these things can
be absolutely prevented with proper design

244
00:14:30,660 --> 00:14:34,340
with the techniques that, that we have
discussed on this presentation so far.

245
00:14:34,840 --> 00:14:40,780
And lastly thank you for sorry
one more slide, key takeaways.

246
00:14:41,020 --> 00:14:43,429
So let me leave you with
four other key takeaways.

247
00:14:43,459 --> 00:14:44,854
The first thing is embed privacy early.

248
00:14:45,354 --> 00:14:48,504
Retrofitting privacy into an existing
system is expensive on whiskey.

249
00:14:48,834 --> 00:14:49,915
Design it from day one.

250
00:14:50,415 --> 00:14:52,454
Second thing is layer your defenses.

251
00:14:52,484 --> 00:14:54,135
No single technique is strong enough.

252
00:14:54,794 --> 00:14:58,185
Third one, balance
privacy with utility only.

253
00:14:58,185 --> 00:15:01,544
Extract data where you need and
ensure that you're protecting

254
00:15:01,544 --> 00:15:02,595
whatever you extracting.

255
00:15:03,345 --> 00:15:05,159
And fourth, stay compliant by design.

256
00:15:05,659 --> 00:15:09,500
Privacy by design principles naturally
align with all the privacy laws out

257
00:15:09,500 --> 00:15:13,129
there like G-D-P-R-C-C-P or hipaa
and a bunch of other regulations.

258
00:15:13,459 --> 00:15:18,990
And good architecture makes you compliant
by a natural effort and not as a burden.

259
00:15:19,490 --> 00:15:23,629
And finally, thank you so much for
the opportunity for allowing me

260
00:15:23,629 --> 00:15:26,555
to speak with you and I hope you.

261
00:15:27,055 --> 00:15:30,504
Took something valuable away from this
conference and from this presentation.

262
00:15:30,975 --> 00:15:32,565
And good luck with you on your future.

263
00:15:32,565 --> 00:15:33,105
Diverse.

264
00:15:33,605 --> 00:15:33,995
Thank you.

