1
00:00:00,500 --> 00:00:00,890
Hi.

2
00:00:01,010 --> 00:00:01,700
Hello everyone.

3
00:00:01,830 --> 00:00:02,969
My name is John Komar.

4
00:00:03,840 --> 00:00:06,150
I'm a former senior engineer at Google.

5
00:00:06,360 --> 00:00:07,500
Currently now work at Meta.

6
00:00:08,010 --> 00:00:12,090
I have around like 13 years of
experience working in different

7
00:00:12,200 --> 00:00:13,820
stacks of like software development.

8
00:00:14,690 --> 00:00:18,290
I'm mostly a full stack development,
but my current recent experience has

9
00:00:18,290 --> 00:00:23,380
been mostly working on Android native
applications at Meta slash Facebook.

10
00:00:24,310 --> 00:00:28,420
So yeah, today I'm going to
present, by white paper, which is

11
00:00:28,420 --> 00:00:32,350
pre-computer surround lightning
for like reload latency.

12
00:00:33,220 --> 00:00:36,839
And I hope we gain something useful
out of this in this conference.

13
00:00:37,330 --> 00:00:38,410
Yeah, let's get started.

14
00:00:38,910 --> 00:00:42,280
So of the main things which we are
going to cover in this conference

15
00:00:42,280 --> 00:00:45,370
and this particular presentation as
what is the system architecture of

16
00:00:45,370 --> 00:00:49,380
this system what are the performance
advantages of using this approach

17
00:00:49,890 --> 00:00:51,235
compared to the conventional approaches?

18
00:00:51,900 --> 00:00:57,320
How does predictive modeling help
make this system more performant

19
00:00:57,410 --> 00:01:00,500
than the conventional ways on
how sound lights are handled?

20
00:01:01,310 --> 00:01:04,310
What are the user experience
advantages of using the system and

21
00:01:04,310 --> 00:01:06,885
what are the challenges which I
faced when I was writing this paper,

22
00:01:07,225 --> 00:01:08,660
as well as developing the system?

23
00:01:09,160 --> 00:01:13,090
So one of the major challenges in the
existing system of ambient lighting,

24
00:01:13,090 --> 00:01:14,410
so I'll give you some context.

25
00:01:14,680 --> 00:01:15,400
Ambient lighting.

26
00:01:15,570 --> 00:01:18,225
When I say ambient lighting,
it is generally when you are

27
00:01:18,240 --> 00:01:19,590
watching something on your screen.

28
00:01:19,680 --> 00:01:23,040
Let's say you're watching a movie
or you're playing games, you can

29
00:01:23,040 --> 00:01:25,650
have these like colorful surround
lights set up in your home.

30
00:01:26,150 --> 00:01:29,540
So what's of the new companies, for
example, let's say Phillips and a

31
00:01:29,540 --> 00:01:31,010
bunch of other manufacturers now.

32
00:01:31,730 --> 00:01:35,420
They have released these smart
bulbs, which can sync with the video

33
00:01:35,420 --> 00:01:37,020
content being played on your screen.

34
00:01:37,620 --> 00:01:40,890
So the usual conventional way of
doing this is they would have a

35
00:01:40,890 --> 00:01:42,600
camera pointed towards the screen.

36
00:01:42,930 --> 00:01:45,780
The camera would see what is
being played on the screen, and

37
00:01:46,230 --> 00:01:48,960
then there's a small box which is
going to process the information

38
00:01:48,960 --> 00:01:50,190
which is happening in the video.

39
00:01:50,490 --> 00:01:54,030
Then it would send out commands to
the different bulbs or the different

40
00:01:54,030 --> 00:01:58,590
lights set up across the room to change
their colors to match in the screen.

41
00:01:59,090 --> 00:02:04,310
Now the biggest problem in this entire
approach is latency because everything

42
00:02:04,310 --> 00:02:09,410
is computed on the fly, especially like
camera, seeing what exactly is going on.

43
00:02:09,440 --> 00:02:13,649
Then there's this like computation,
which is happening on the device locally.

44
00:02:14,369 --> 00:02:15,449
What they are seeing.

45
00:02:15,719 --> 00:02:17,699
Then deciding which colors make sense.

46
00:02:18,135 --> 00:02:21,704
Which brightness makes sense, and then
sending out the signals to the bulbs.

47
00:02:22,184 --> 00:02:25,704
This entire process is heavily
computational expensive, not to

48
00:02:25,704 --> 00:02:30,325
mention the added latency and passing
these signals usually over ZigBee

49
00:02:31,015 --> 00:02:32,329
to these lakes around smart lights.

50
00:02:32,829 --> 00:02:37,390
And because they have to do this on
the runtime, there are limited number

51
00:02:37,390 --> 00:02:41,059
of colors or limited number of like
brightness numbers these machines

52
00:02:41,059 --> 00:02:45,530
can play around with because the more
computational heavy they make it, it

53
00:02:45,530 --> 00:02:47,630
is going to add more to the latency.

54
00:02:48,290 --> 00:02:51,619
So this entire system is physically
bound by the challenges of physics,

55
00:02:51,709 --> 00:02:54,859
basically how much time it takes to
process certain amount of information,

56
00:02:55,130 --> 00:02:56,540
and how much time it takes.

57
00:02:56,609 --> 00:03:00,720
To send those signal to smart lights
or smart bulbs or like different

58
00:03:00,829 --> 00:03:03,529
light strips to solve this problem.

59
00:03:03,630 --> 00:03:10,500
What I came up with this approach was to
have this pre computation or basically.

60
00:03:11,000 --> 00:03:14,810
Pre-processing what content was
being displayed on the screen

61
00:03:14,810 --> 00:03:16,190
via the signal pass through.

62
00:03:16,430 --> 00:03:19,970
So this approach does not really
rely on a camera or a physical device

63
00:03:19,970 --> 00:03:24,230
looking at what's going on in the
screen, but basically trying to

64
00:03:24,230 --> 00:03:26,755
pre-process what this scene might entail.

65
00:03:27,680 --> 00:03:30,740
And this predictive buffer
basically predicts what the lights

66
00:03:30,740 --> 00:03:31,940
in this thing are going to be.

67
00:03:32,510 --> 00:03:33,800
I'll explain this with an example.

68
00:03:33,860 --> 00:03:38,580
Let's say there's, you're watching a movie
and there's a police car chase theme.

69
00:03:39,150 --> 00:03:42,880
Now, we know in most countries, like the
police cars have red and blue lights.

70
00:03:43,660 --> 00:03:48,100
So this system would pre-compute that
in this particular car chase scene, the

71
00:03:48,100 --> 00:03:51,340
scene would require blue and red color
lights flicking through the system.

72
00:03:51,820 --> 00:03:55,899
So the system can pre-compute and
pre-prepare these like encodings

73
00:03:56,079 --> 00:03:59,609
and send these signals to the, but
right before the scene starts on

74
00:03:59,609 --> 00:04:01,935
the screen, so the users would not.

75
00:04:02,640 --> 00:04:07,289
Experience any latency as well as
immediately the signals will be

76
00:04:07,289 --> 00:04:11,310
passed onto the surround lights as the
screening is being posted onto the on

77
00:04:11,310 --> 00:04:16,029
the main screen, what happens in the
conventional systems as I have personally

78
00:04:16,029 --> 00:04:20,529
witnessed that myself, even though
when the police car chase seen is over.

79
00:04:21,130 --> 00:04:25,030
Is when the bulbs start reacting
to those scenes like you would

80
00:04:25,030 --> 00:04:26,620
see blue and red colored lights.

81
00:04:26,680 --> 00:04:30,010
I'm just giving examples,
would flicker corresponding

82
00:04:30,010 --> 00:04:31,419
to the police car chase scene.

83
00:04:31,719 --> 00:04:33,669
But the scene is no
longer even on the screen.

84
00:04:34,299 --> 00:04:36,789
So this whole experience
is like really jarring.

85
00:04:36,789 --> 00:04:41,560
The latency is like extremely high and
this entire experience is supposed to

86
00:04:41,560 --> 00:04:43,479
be more immersive and more engaging.

87
00:04:43,975 --> 00:04:47,265
But in my personal experience as well
as the server which we have conducted

88
00:04:47,265 --> 00:04:51,045
with internal test users, this entire
experience of the conventional way of

89
00:04:51,045 --> 00:04:56,055
doing this thing has been, I would say,
completely disconnected from what you're

90
00:04:56,055 --> 00:04:59,055
watching on the screen versus what
you're experiencing in the smart lights.

91
00:04:59,595 --> 00:05:03,825
So this approach of like pre
computation, not only bridge that

92
00:05:03,825 --> 00:05:07,300
latency barrier, but also gives you a
lot of flexibility to play around with.

93
00:05:07,800 --> 00:05:12,370
So in this case we did some benchmark
analysis and we found out that even

94
00:05:12,370 --> 00:05:16,130
though the conventional systems had the
latency of around, let's say I think it

95
00:05:16,130 --> 00:05:20,240
was around like 200, 300 milliseconds,
but with our approach, we can basically

96
00:05:20,240 --> 00:05:24,650
send out the signals with the overall
latency of 60.7 milliseconds, which is

97
00:05:24,650 --> 00:05:30,260
almost like a more than one 10th less
latency compared to pen menstrual system.

98
00:05:30,760 --> 00:05:35,800
And also because we have the capability
of pre-cutting this information

99
00:05:36,040 --> 00:05:39,100
regarding the scenes in the movie
or games you are playing or the

100
00:05:39,100 --> 00:05:40,360
TV content which you're watching.

101
00:05:40,450 --> 00:05:45,320
This could be even be streamed or
d platforms or online streaming

102
00:05:45,320 --> 00:05:48,435
like Netflix or like Amazon
Prime or Disney Plus, et cetera.

103
00:05:48,935 --> 00:05:53,360
So because this information is
like pre-processed, the system

104
00:05:53,360 --> 00:05:57,110
has the capability of peeking
through the frames, which would be

105
00:05:57,110 --> 00:05:58,670
visible in the next few seconds.

106
00:05:59,585 --> 00:06:03,425
So with this look ahead approach,
not only the system can pre-compute

107
00:06:03,635 --> 00:06:08,015
the light signals it has to send
out for the currency, it can even

108
00:06:08,015 --> 00:06:10,085
do a look ahead for the next scene.

109
00:06:10,775 --> 00:06:15,095
So that not only saves the time for
pre-pro when the scene actually comes,

110
00:06:15,155 --> 00:06:21,425
which avoids this jarring experience in
case the stream buffers or like loads, it

111
00:06:21,425 --> 00:06:24,610
also creates the smooth experience that
when you transition to the next scene.

112
00:06:25,190 --> 00:06:26,479
The lights are already pre compute.

113
00:06:26,750 --> 00:06:32,400
It completely avoids even the possibility
of having a delay because we already

114
00:06:32,400 --> 00:06:37,179
pre buffered what the smart lights
would be showing in terms of color

115
00:06:37,419 --> 00:06:40,699
or brightness, two seconds before
the scene was even on the screen.

116
00:06:41,199 --> 00:06:45,069
And also because of this pre computation,
it can support like multiple frame rates.

117
00:06:45,069 --> 00:06:46,780
For example, let's say
you are playing games.

118
00:06:47,119 --> 00:06:49,969
The frequency is not really
around like 24 60 hertz.

119
00:06:50,059 --> 00:06:53,359
It can even go up to one 20 hertz,
even new monitors about two 40 hertz.

120
00:06:54,260 --> 00:06:57,950
So with this approach, it has not
capped by the frame rate of what you

121
00:06:57,950 --> 00:07:01,969
are watching, but basically human
time of how much few seconds of

122
00:07:01,969 --> 00:07:03,289
content which you're going to process.

123
00:07:03,500 --> 00:07:04,849
For example, if you're playing a game.

124
00:07:05,780 --> 00:07:09,744
The game developers in their gaming
engines could pre encode the signals

125
00:07:10,044 --> 00:07:13,299
of the colors the game environment has.

126
00:07:13,900 --> 00:07:18,250
So e, because the character can move in
like certain directions in the map, they

127
00:07:18,250 --> 00:07:22,000
can pre-compute that if character moves,
let's say the direction of snow, we can

128
00:07:22,000 --> 00:07:23,469
make the entire room like snow white.

129
00:07:23,469 --> 00:07:27,400
Or if the character moves towards the
sea, we can make the entire room blue.

130
00:07:27,549 --> 00:07:31,330
So all this pre computation can
be done, and also game developers

131
00:07:31,330 --> 00:07:32,825
can pre-code certain information.

132
00:07:33,685 --> 00:07:38,245
Which the system can handle and directly
send the signals and make sure they're

133
00:07:38,245 --> 00:07:39,865
ready before the scene actually comes.

134
00:07:40,854 --> 00:07:43,335
Also we developed a system
for error collection.

135
00:07:43,335 --> 00:07:46,635
So for example, when this theme is
being shown, or like the predictive

136
00:07:47,235 --> 00:07:51,265
analysis being done for the next
few scenes it can pre-check itself

137
00:07:51,474 --> 00:07:55,505
that the colors it is sending to the
smart lights actually makes sense.

138
00:07:56,375 --> 00:07:58,595
I'll explain this with
an example, for example.

139
00:07:59,420 --> 00:08:02,960
Let's say a character is looking
at a beautiful rainbow in the sky,

140
00:08:03,460 --> 00:08:08,530
even though the primary color in the
ceiling would be the blue sky the

141
00:08:08,530 --> 00:08:13,300
conventional systems in that case would
actually show blue lights in the room.

142
00:08:13,780 --> 00:08:17,229
But actually what the user is
actually looking at is the rainbow.

143
00:08:17,650 --> 00:08:20,045
So in this particular case, with
the machine learning algorithms.

144
00:08:20,844 --> 00:08:23,484
It can predetermine, what
is the main subject and main

145
00:08:23,484 --> 00:08:24,955
focus on that particular scene?

146
00:08:25,045 --> 00:08:26,575
In this case, it's a rainbow.

147
00:08:27,295 --> 00:08:30,685
So with this new system, it can actually
send rainbow colors, which could be

148
00:08:30,775 --> 00:08:34,765
spread across the entire room and
make the scene more immersive compared

149
00:08:34,765 --> 00:08:37,854
to existing lights, which would only
show I would say maybe sky blue.

150
00:08:38,354 --> 00:08:42,524
And as, as I previously mentioned, the
conventional systems because they have to

151
00:08:42,524 --> 00:08:44,294
rely on this like real time processing.

152
00:08:44,354 --> 00:08:46,424
They have to rely on the camera input.

153
00:08:46,424 --> 00:08:48,074
They have to rely on sending the signals.

154
00:08:48,689 --> 00:08:52,319
On the fly, they're bound by
the physical limitations on

155
00:08:52,319 --> 00:08:53,609
how much they can pre-computer.

156
00:08:54,059 --> 00:08:58,049
So instead of using all in the entire
color gamut, and instead of using

157
00:08:58,049 --> 00:09:01,079
the entire brightness spectrum from
like zero to a hundred percent, let's

158
00:09:01,079 --> 00:09:03,689
say, they would play it really safe.

159
00:09:03,689 --> 00:09:05,954
They would only play with
like major primary colors.

160
00:09:06,574 --> 00:09:11,604
The systems are really risk adverse that
they would not tinker with the brightness.

161
00:09:11,604 --> 00:09:13,974
They would not tinker with like
too many color combinations

162
00:09:14,184 --> 00:09:15,744
and risks of getting it wrong.

163
00:09:16,314 --> 00:09:21,074
So to play it they usually play within
a very safe range of colors, but in

164
00:09:21,074 --> 00:09:25,544
our system, the new system, because it
is pre-cutting and it has algorithms

165
00:09:25,544 --> 00:09:26,629
and it has checks and balances.

166
00:09:27,554 --> 00:09:31,004
To make sure that the scene and the light
signal it's sending is actually correct.

167
00:09:31,424 --> 00:09:34,484
It can actually play with the entire
color spectrum of all the million

168
00:09:34,634 --> 00:09:38,714
color combinations it can produce
from RGB different combinations of RGB

169
00:09:38,714 --> 00:09:42,164
lights, and it can also go from like
zero to a hundred percent brightness.

170
00:09:42,164 --> 00:09:44,564
For example, let's say you're
playing a horror game, it

171
00:09:44,564 --> 00:09:45,824
can actually go completely.

172
00:09:46,274 --> 00:09:49,724
To create that immersion that you're
playing a horror team or horror scene.

173
00:09:50,054 --> 00:09:53,804
Or if you are watching a really nice,
let's say the latest Superman movie with

174
00:09:53,804 --> 00:09:57,584
like bright, beautiful colors, it can
go to a hundred percent brightness with

175
00:09:57,584 --> 00:10:02,294
like really immersive bright colors to
match what the content creator intended

176
00:10:02,294 --> 00:10:03,854
the movie or the content will look like.

177
00:10:04,354 --> 00:10:07,984
How did we achieve really sub
hundred millisecond response time.

178
00:10:08,594 --> 00:10:10,784
These are the key areas
which we actually focus on.

179
00:10:10,884 --> 00:10:13,704
The first one being like really
efficient frame processing.

180
00:10:13,734 --> 00:10:18,504
If we do not have a good frame processing
algorithm in place, or we do not have

181
00:10:18,504 --> 00:10:23,124
good pre encoding logic in place there
is no way getting around it if you

182
00:10:23,124 --> 00:10:28,224
are not able to find the colors that
a particular screen should be showing.

183
00:10:28,574 --> 00:10:31,814
And if, because this is going to the
bottleneck, just processing the raw

184
00:10:31,814 --> 00:10:37,279
content or the video content and then the
language colors to show the other areas

185
00:10:37,279 --> 00:10:39,559
in this system are basically optimization.

186
00:10:40,009 --> 00:10:44,869
Basically, pre computing is basically
applying this frame, processing to few

187
00:10:44,869 --> 00:10:49,849
frames in the future, making sure that
you have enough buffer when those scenes

188
00:10:49,849 --> 00:10:51,739
come onto the screen, you already have.

189
00:10:52,239 --> 00:10:53,109
Those screens early.

190
00:10:53,829 --> 00:10:54,849
No communication.

191
00:10:54,849 --> 00:11:00,369
Basically talking between the smart
lights because your system knows

192
00:11:00,699 --> 00:11:02,529
how far those smart bulbs are.

193
00:11:02,529 --> 00:11:04,989
Let's say your smart bulbs
are really far spread out in

194
00:11:04,989 --> 00:11:06,369
your huge home theater system.

195
00:11:07,269 --> 00:11:10,329
Or let's say the system is even being
used in a commercial theater system

196
00:11:10,809 --> 00:11:14,614
because the physical limitations
that the birds or the lights are.

197
00:11:15,304 --> 00:11:21,034
Located really far away, you can pre
calibrate for that LA agency because

198
00:11:21,034 --> 00:11:23,104
you know that bulb is the time.

199
00:11:23,134 --> 00:11:26,104
It takes one second to act the
signal to reach to that bulb.

200
00:11:26,584 --> 00:11:30,394
And because we have pre computation,
you can actually send the light

201
00:11:30,394 --> 00:11:34,204
signal for the scene, which is
coming a second later, so that it is

202
00:11:34,204 --> 00:11:35,644
completely in sync with the video.

203
00:11:36,274 --> 00:11:40,504
So even, and this would have been, and
this actually is completely impossible

204
00:11:40,504 --> 00:11:43,444
in the current system because the
current systems rely on the camera.

205
00:11:44,209 --> 00:11:46,549
They cannot please send
the signal to the bulb.

206
00:11:46,609 --> 00:11:50,749
And if the bulb is like really far
away, or even if there's a network

207
00:11:50,749 --> 00:11:55,129
lab, let's say your outer is not super
efficient and there's a delay between

208
00:11:55,179 --> 00:11:59,679
your setup box or your, let's say,
smart device between those bulbs,

209
00:11:59,979 --> 00:12:04,219
the latency is really pronounced and
it's like really bad user experience.

210
00:12:05,209 --> 00:12:08,474
And by using dedicated systems
for pre-com computing this.

211
00:12:09,269 --> 00:12:11,939
The better hardware we have,
the better GPUs we have.

212
00:12:12,369 --> 00:12:17,439
This system can be scaled further in
terms of depth, basically reducing the

213
00:12:17,439 --> 00:12:21,380
time it takes of frame processing as well
as how much pre computation we can do.

214
00:12:21,880 --> 00:12:26,260
Moving on aside from these
approaches, we can further improve

215
00:12:26,260 --> 00:12:31,730
the system to improve the overall
immersion by identifying the scene.

216
00:12:31,790 --> 00:12:34,960
For example, in this the previous
example, which I mentioned, for example,

217
00:12:34,960 --> 00:12:39,910
rainbow in the Sky classifying the
scene that this, the image which the

218
00:12:39,910 --> 00:12:43,780
user is looking at right now, the scene
user is looking at is rainbow in the

219
00:12:43,780 --> 00:12:48,605
sky, makes or breaks what the signal is
going to be because existing systems.

220
00:12:49,330 --> 00:12:53,555
We just rely on what is the maximum amount
of color or what is the maximal present

221
00:12:53,555 --> 00:12:58,115
color in this scene, which is going to
be blue, but in our scene classification

222
00:12:58,115 --> 00:13:01,865
system, it is going to actually
recognize this is not just a blue sky.

223
00:13:02,135 --> 00:13:03,525
It has a rainbow inside it.

224
00:13:03,780 --> 00:13:06,815
That way it can completely
differentiate which kind of color

225
00:13:06,815 --> 00:13:08,735
signals to stent to the smart poles.

226
00:13:09,235 --> 00:13:12,895
It can also do color mapping because
I mentioned previous systems.

227
00:13:12,895 --> 00:13:16,915
They rely on camera input if your
screen does not have color accurate.

228
00:13:17,600 --> 00:13:21,050
Let's say display or the display
itself is not of high quality.

229
00:13:21,110 --> 00:13:25,280
Or let's say you are watching content
on CRT or some other data hardware.

230
00:13:25,850 --> 00:13:29,690
Their systems also fail because
they completely rely on what exactly

231
00:13:29,690 --> 00:13:33,350
is on the screen, not what signals
are being sent to the screen.

232
00:13:34,010 --> 00:13:35,690
We can also do motion analysis.

233
00:13:36,190 --> 00:13:40,300
Let's say your monitor has really
high frame rate, one 40 hertz.

234
00:13:40,720 --> 00:13:45,130
And because the exist existing
systems that rely on camera, the

235
00:13:45,130 --> 00:13:49,900
monitors frame rate could be far
higher than the computational

236
00:13:49,900 --> 00:13:51,430
capabilities of those systems.

237
00:13:51,790 --> 00:13:55,640
In that case, the seams could actually
be changing faster than what their

238
00:13:55,640 --> 00:13:58,070
systems can compute and send to the buds.

239
00:13:58,130 --> 00:14:00,710
In that case, those
systems have so much lag.

240
00:14:01,160 --> 00:14:04,250
It's almost impossible to keep
the video or the game content

241
00:14:04,250 --> 00:14:06,140
in sync with the smart lights.

242
00:14:06,640 --> 00:14:10,630
Also with focus election, it is similar
to scheme classification, but also it can

243
00:14:10,630 --> 00:14:14,170
identify if there's like a main accuracy
or if there's like a some emotional theme.

244
00:14:14,530 --> 00:14:18,805
It can also determine which areas
to focus on Embassy itself and.

245
00:14:19,400 --> 00:14:24,920
With the adoption of the system over
time, content creators movie producers,

246
00:14:25,025 --> 00:14:30,220
as well as other people who are in the
department of color mixing, or like this

247
00:14:30,220 --> 00:14:35,230
video engineering, they can eventually
employ pattern learning algorithms on how

248
00:14:35,230 --> 00:14:36,970
accurate the previous predictions work.

249
00:14:37,480 --> 00:14:41,570
And this system can go grow more
accurate over a period of time.

250
00:14:42,350 --> 00:14:45,230
And this entire coding could
be very similar to, let's say,

251
00:14:45,260 --> 00:14:47,120
how some engineers work today.

252
00:14:47,825 --> 00:14:51,155
Whenever we watch a movie,
there are in the background.

253
00:14:51,155 --> 00:14:54,785
There are like so many sound engineers
who have watched that content, who

254
00:14:54,785 --> 00:14:58,595
have specifically chosen that this
sound should go to studio speakers.

255
00:14:58,595 --> 00:14:59,975
This sound should go to the roofer.

256
00:15:00,305 --> 00:15:02,715
This sound should go to the AT speakers.

257
00:15:03,345 --> 00:15:06,735
Very similar when there is an
entire ecosystem built around the

258
00:15:06,735 --> 00:15:10,395
sound part of the video, we are
watching eventually down the line.

259
00:15:10,665 --> 00:15:14,380
I believe we can also have
this like surround part of.

260
00:15:14,880 --> 00:15:17,580
Different smart lights
reacting to the video content.

261
00:15:18,080 --> 00:15:22,280
So the major application of this
particular approach is going to be giving

262
00:15:22,280 --> 00:15:26,830
immersion gaming industry, I believe
has already crossed in terms of profits.

263
00:15:27,280 --> 00:15:29,635
The other like sports in
terms of the money it makes.

264
00:15:30,190 --> 00:15:34,180
The game is getting very popular, be it
like phone games, like consoles or pc.

265
00:15:34,750 --> 00:15:38,260
So having a more immersive gaming
experience, basically the game which

266
00:15:38,260 --> 00:15:40,510
you're playing your entire room reacts to.

267
00:15:40,510 --> 00:15:44,440
It is going to only add to the overall
experience of the users, of the gamers.

268
00:15:45,290 --> 00:15:49,690
We can also adopt this approach
not only for gamers and like

269
00:15:49,690 --> 00:15:50,740
cinematic movie watchers.

270
00:15:51,415 --> 00:15:54,835
But also for education content as
well as reducing the visual comfort.

271
00:15:55,435 --> 00:16:01,215
Sometimes for people who have reduced
impaired visibility, having lights in

272
00:16:01,215 --> 00:16:05,115
the room which react to the content
could make the experience somewhat

273
00:16:05,115 --> 00:16:07,695
more pleasant as and soothing.

274
00:16:07,965 --> 00:16:13,305
And also people who are not able to focus
entirely on the minute details on the

275
00:16:13,305 --> 00:16:18,905
screen, they could still get the overall
experience or overall, the vibe of the

276
00:16:18,905 --> 00:16:22,715
movie or the vibe of the scene, just
simply because of the surround lights

277
00:16:23,105 --> 00:16:25,145
reacting to the content on the screen.

278
00:16:25,645 --> 00:16:28,855
Some of the optimizations, which could
be done specific to the content types.

279
00:16:28,855 --> 00:16:33,595
Basically as the gaming progresses
we have moved on to having 60 hertz

280
00:16:33,595 --> 00:16:35,215
being the benchmark of smooth gaming.

281
00:16:35,545 --> 00:16:38,725
Now, one 20 hertz is pretty
common, and with the advancements

282
00:16:38,725 --> 00:16:43,160
in new GPUs, new monitor hardware
as well as virtual reality, it

283
00:16:43,160 --> 00:16:44,600
is only going to keep going up.

284
00:16:45,470 --> 00:16:47,570
Since our logic is not
really capped to the.

285
00:16:47,990 --> 00:16:51,920
To the frequency of content and
based on the pre encoding and the

286
00:16:51,920 --> 00:16:55,320
pre detection of what the scenes are
going to be, our system is going to

287
00:16:55,320 --> 00:16:58,800
easily scale with the improvements
in gaming over a period of time.

288
00:16:59,300 --> 00:17:04,440
Cinemas I believe have been very
stagnant in terms of the exp movie

289
00:17:04,440 --> 00:17:05,820
watching experience in the past.

290
00:17:05,850 --> 00:17:06,900
I would say 30 years.

291
00:17:07,350 --> 00:17:12,150
Aside from new IMAX and sound formats,
they haven't been really radical

292
00:17:12,150 --> 00:17:15,300
improvements in the cinema watching
experience in the past few years.

293
00:17:15,800 --> 00:17:20,120
Now with this new system, the directors
and the sound engineers and the new

294
00:17:20,120 --> 00:17:24,540
field of I would say occupation maybe
the sound light engineers can actually

295
00:17:24,580 --> 00:17:28,030
include this new information that
corresponding to this movie scene.

296
00:17:28,450 --> 00:17:30,790
The theater lights
should react in this way.

297
00:17:31,030 --> 00:17:32,860
The backlight should react in this way.

298
00:17:32,860 --> 00:17:35,650
If it's a horror movie, the
lights should flicker in this way.

299
00:17:35,890 --> 00:17:38,050
For example, imagine
watching a movie which has.

300
00:17:38,550 --> 00:17:42,200
A thunderstorm scene and the entire
theater lights flash according

301
00:17:42,200 --> 00:17:43,400
to the scene on the movie.

302
00:17:43,540 --> 00:17:45,940
It is going to create a much more
immersive experience compared

303
00:17:45,940 --> 00:17:49,570
to the movies experiences we
are used to watching right now.

304
00:17:50,070 --> 00:17:54,770
And also with the increased short
form content people's focus duration

305
00:17:54,770 --> 00:17:56,810
has gone down significantly.

306
00:17:56,810 --> 00:18:02,490
We are all aware, like I think the
studies say the average focus time now

307
00:18:02,490 --> 00:18:04,050
for people is around seven seconds.

308
00:18:04,440 --> 00:18:05,370
Which is alarming.

309
00:18:05,870 --> 00:18:10,040
But with these surround line lights
and the surround encoding of content,

310
00:18:10,580 --> 00:18:14,450
people might be able to focus more
as it can reduce the distractions

311
00:18:14,780 --> 00:18:17,870
and people can actually focus on the
screen, which they're supposed to.

312
00:18:18,080 --> 00:18:20,690
It can maybe di the lights around
the room when you are focusing,

313
00:18:20,690 --> 00:18:23,685
let's say, on your coding tasks or
you are trying to read your book.

314
00:18:24,185 --> 00:18:27,905
So it'll not be just limited to the
videos, but based on the content

315
00:18:27,935 --> 00:18:30,995
which is on your screen, it can
pre-process and can determine like

316
00:18:31,025 --> 00:18:33,155
what work you are doing on right now.

317
00:18:33,365 --> 00:18:34,715
And it can put you on focus mode.

318
00:18:34,895 --> 00:18:38,735
It'll also be you watching like maybe
some calming videos or listening to

319
00:18:38,735 --> 00:18:42,875
soothing music and the lights in your
room could react accordingly, making

320
00:18:42,875 --> 00:18:44,765
the experience much more deeper.

321
00:18:45,265 --> 00:18:47,725
The architecture, which I briefly
touched upon in the previous

322
00:18:47,725 --> 00:18:49,205
slides we can go over that.

323
00:18:49,255 --> 00:18:55,045
Basically the system is based on
the content input, what the video

324
00:18:55,045 --> 00:18:57,025
or the audio content is actually is.

325
00:18:57,385 --> 00:18:59,845
Then we have this like frame
analysis engine, which.

326
00:19:00,805 --> 00:19:04,495
Which sifts through this content, which
decodes this content and breaks into

327
00:19:04,495 --> 00:19:05,965
this, like what the light should be.

328
00:19:06,235 --> 00:19:08,575
Then there's this predictive
buffer, which is an optimization,

329
00:19:08,575 --> 00:19:11,845
which predicts what the next few
frames are going to look like.

330
00:19:12,475 --> 00:19:15,445
Then there's this node controller,
which is responsible for sending

331
00:19:15,445 --> 00:19:19,020
out the signals, the actual signals
to those surrounding lights.

332
00:19:19,150 --> 00:19:23,005
Could be strips, could be bulbs,
could be any even virtual devices.

333
00:19:23,300 --> 00:19:26,480
For the sake of testing, and eventually
they could be replaced with actual bulbs

334
00:19:27,050 --> 00:19:31,130
and then actual physical lights, which are
responsible for showing the actual colors.

335
00:19:31,630 --> 00:19:35,230
Some of the major challenges,
which I encountered while working

336
00:19:35,230 --> 00:19:38,900
on this particular project
was the latency reduction.

337
00:19:38,900 --> 00:19:42,470
As I previously mentioned, tradit
traditional systems, they completely rely

338
00:19:42,470 --> 00:19:46,250
on camera input, basically their systems.

339
00:19:47,180 --> 00:19:51,620
Watch the video and translate the
way humans do it, which I believe is

340
00:19:51,710 --> 00:19:56,620
really unoptimized way of doing this
because computers and we have like

341
00:19:56,620 --> 00:20:00,610
algorithms which can pre-process this
video or format, we do, we need not

342
00:20:00,610 --> 00:20:02,200
actually rely on what's on the screen.

343
00:20:02,290 --> 00:20:05,260
We can actually pre-compute
software information and we can

344
00:20:05,260 --> 00:20:08,800
even predict what's going to be the
information in the next few seconds.

345
00:20:09,550 --> 00:20:12,550
Also, the way the content
and the frame rates change so

346
00:20:12,550 --> 00:20:13,960
rapidly in today's environment.

347
00:20:14,350 --> 00:20:19,030
Some of those existing systems cannot just
keep up with the way the technology is

348
00:20:19,030 --> 00:20:23,980
advancing the way Movie scenes are much
more immersive, much more color depth.

349
00:20:24,190 --> 00:20:27,280
The games have accept a much
more higher framing rate.

350
00:20:28,150 --> 00:20:31,270
The hardwares that would be built
to keep up with these technologies

351
00:20:31,270 --> 00:20:35,140
are going to be extremely expensive
and would not be consumer friendly.

352
00:20:35,640 --> 00:20:39,880
And the existing systems, they really
have limitations on how much computation

353
00:20:39,880 --> 00:20:43,450
they can actually do while making
the devices affordable and how much

354
00:20:44,080 --> 00:20:47,650
color accurate, how much brightness,
accurate they can actually be with

355
00:20:47,650 --> 00:20:48,910
the technical limitations they have.

356
00:20:49,410 --> 00:20:53,160
So the implementation considerations,
which I made when I write the system.

357
00:20:53,725 --> 00:20:59,585
Basically the systems could scale from
a casual TV or home theater watching

358
00:20:59,585 --> 00:21:04,035
person who has some sound lights and
could scale all the way to a commercial

359
00:21:04,035 --> 00:21:07,365
movie theaters where people go to a
movie theater and actually have the

360
00:21:07,365 --> 00:21:11,655
system running behind the movie and
controlling the smart lights around.

361
00:21:11,655 --> 00:21:11,835
You.

362
00:21:12,285 --> 00:21:16,515
Imagine like a four D movie, but
the chairs would not be shaking.

363
00:21:16,615 --> 00:21:19,015
But still the experience would
be much more immersive and.

364
00:21:19,515 --> 00:21:23,805
The best part about this approach, it
can convert any of the existing theaters.

365
00:21:24,525 --> 00:21:29,865
Into these smart theaters which
correspond or which respond to the

366
00:21:29,865 --> 00:21:33,465
content on the screen with very
minimal additional expenditure.

367
00:21:33,765 --> 00:21:37,395
Converting a regular theater into a 40
theater is going to be significantly

368
00:21:37,395 --> 00:21:41,505
expensive compared to this approach in
which you have to spend like maybe a few

369
00:21:41,505 --> 00:21:46,455
hundred dollars to install these smart
lights and all of the internal logic

370
00:21:46,515 --> 00:21:50,265
to pre-compute how the signals will be
sent to b will take care of it itself.

371
00:21:51,240 --> 00:21:53,910
And this system could easily
live inside a gaming environment.

372
00:21:53,910 --> 00:21:56,520
Let's say you have a PS five, you
have an Xbox, you have a computer,

373
00:21:57,020 --> 00:21:58,550
if this information is preen code.

374
00:21:59,240 --> 00:22:03,920
And all of these devices have
enough hardware to handle this

375
00:22:03,920 --> 00:22:05,930
on the device, live in coding.

376
00:22:06,680 --> 00:22:09,700
And these could easily handle
sending signals to your smart

377
00:22:09,700 --> 00:22:11,200
devices existing in your home.

378
00:22:11,500 --> 00:22:15,135
So this can scale from like entire
movie watching experience to

379
00:22:15,135 --> 00:22:16,780
casual, to professional gamers.

380
00:22:16,840 --> 00:22:18,470
Who consume live visual content.

381
00:22:18,970 --> 00:22:23,060
So I believe this approach could
be the next redefining step

382
00:22:23,240 --> 00:22:25,020
in redefining how we consume.

383
00:22:25,465 --> 00:22:29,275
Media, basically video and audio
content because the performance

384
00:22:29,275 --> 00:22:32,185
is going to be extremely fast
with predictive intelligence.

385
00:22:32,245 --> 00:22:34,855
It can keep up with new contents,
it can keep up with new games,

386
00:22:34,855 --> 00:22:38,155
it can keep up with new movies,
and it can keep up easily.

387
00:22:38,155 --> 00:22:41,845
Keep up with new video formats and
hardware, which is not really possible.

388
00:22:41,905 --> 00:22:45,885
And same systems with the
advancement in surround lights,

389
00:22:45,885 --> 00:22:50,125
better lights, better, smart light
bands, light strips, light lamps.

390
00:22:51,115 --> 00:22:53,425
And the experience is going
to keep getting even better.

391
00:22:53,515 --> 00:22:57,415
And as the system scales, it can handle
like more and more lights, maybe even

392
00:22:57,415 --> 00:22:58,615
like a hundred supplies in the future.

393
00:22:59,485 --> 00:23:03,385
And because it is like pre-com computing
all this information and it is being

394
00:23:03,385 --> 00:23:08,355
computed once, not everyone relying on
the system or using the system has to

395
00:23:08,355 --> 00:23:09,890
compute this information on their device.

396
00:23:10,390 --> 00:23:13,570
So movie producers, content
producers could pre-compute

397
00:23:13,570 --> 00:23:15,280
or prefe this information.

398
00:23:15,390 --> 00:23:20,460
And aside from the video, like
subtitles or surround sound input,

399
00:23:20,550 --> 00:23:24,540
this could be a new input and the
user's devices could just consume

400
00:23:24,540 --> 00:23:27,220
this input and send that smart lights.

401
00:23:27,670 --> 00:23:32,490
Overall, this would reduce the
dependence on consumers and having

402
00:23:32,490 --> 00:23:34,170
to buy really expensive new hardware.

403
00:23:34,170 --> 00:23:35,310
For example, the current hardware.

404
00:23:35,880 --> 00:23:40,260
Which does something similar in a very
inefficient manner is at least two $50.

405
00:23:40,350 --> 00:23:43,830
Not to mention the cost of
buying additional bulbs with

406
00:23:43,950 --> 00:23:46,950
and additional cameras which
go out of it like really fast.

407
00:23:47,450 --> 00:23:51,320
I hope this new system sounds
useful and hopefully it will be

408
00:23:51,320 --> 00:23:52,910
eventually adopted by the industry.

409
00:23:53,540 --> 00:23:57,520
I have written defensive publication for
the same approach with the same name.

410
00:23:57,570 --> 00:24:01,110
It would be good read if you guys
want to find out, and I hope you

411
00:24:01,110 --> 00:24:02,760
have fun and rest of the conference.

412
00:24:02,880 --> 00:24:03,315
Thank you so much.

