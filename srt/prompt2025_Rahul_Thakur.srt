1
00:00:00,500 --> 00:00:05,359
Hey, my name is Raul and I want to talk
about problem driven infrastructure,

2
00:00:06,170 --> 00:00:08,720
starting AI services for scaling re.

3
00:00:09,220 --> 00:00:14,170
Given the increased reduction of the ai,
it is very important to make sure the

4
00:00:14,170 --> 00:00:19,619
infrastructure that those models runs are
also able to scale to the requirements.

5
00:00:20,119 --> 00:00:24,720
In here I will be discussing
about the, about how you can.

6
00:00:25,220 --> 00:00:28,909
IT scales and make your AI
services highly available.

7
00:00:29,409 --> 00:00:33,869
The challenge that traditional AI
infrastructure have performance

8
00:00:34,554 --> 00:00:38,399
bottleneck, single point of
failure in the resource contention.

9
00:00:38,899 --> 00:00:42,259
So we, before we solve the problem,
we need to understand what are

10
00:00:42,259 --> 00:00:47,389
the different challenges that very
specific to the a IE services.

11
00:00:47,889 --> 00:00:52,059
The first thing is that there is
extreme variability in the traffic.

12
00:00:52,559 --> 00:00:55,259
Second is that the
conversations are stateful.

13
00:00:55,769 --> 00:00:58,679
So we have to stay, we have
to store some mistakes.

14
00:00:59,179 --> 00:01:01,099
The third is that resource intensity.

15
00:01:01,279 --> 00:01:04,059
Some of the tasks require lot
more resource than the other.

16
00:01:04,599 --> 00:01:07,594
And then since and then
unexpected failure.

17
00:01:08,094 --> 00:01:12,474
The sorting solution that I want to
propose is instead of serving all the

18
00:01:12,474 --> 00:01:16,434
traffic from the single start of the
service, we can have the multiple charts

19
00:01:16,434 --> 00:01:20,429
of the services serving some percentage
of the traffic, and then we can intend

20
00:01:20,944 --> 00:01:26,224
we can have the intelligent logic to
route the traffic and also fail over

21
00:01:26,224 --> 00:01:28,384
the fail over in case of any failure.

22
00:01:28,884 --> 00:01:33,984
Okay, so I would want to talk
about some basic component.

23
00:01:34,225 --> 00:01:38,035
The first is the AI service charts,
which means that we will be having

24
00:01:38,035 --> 00:01:41,644
the multiple charts of these
services each working independently.

25
00:01:42,144 --> 00:01:44,215
The metadata service,
which is very important.

26
00:01:44,344 --> 00:01:47,884
This is the service which actually
keep track of all the charts

27
00:01:47,884 --> 00:01:50,175
we have in in their dependency.

28
00:01:50,675 --> 00:01:55,585
We, since now we have the multiple, we
also have to route the request to the

29
00:01:55,645 --> 00:01:58,035
charge that appropriate to that request.

30
00:01:58,535 --> 00:02:05,915
We also want our service to be charted to
the multiple availability zone, so that

31
00:02:05,915 --> 00:02:10,880
if there is an outage in one availability
zone, the service is still de functional.

32
00:02:11,380 --> 00:02:15,110
The state management since the
the conversation with the AI is

33
00:02:15,170 --> 00:02:18,530
stateful, so we have to think, we
have to take care of the state also.

34
00:02:18,860 --> 00:02:22,070
And then observability system,
this becomes more important

35
00:02:22,130 --> 00:02:23,450
since we have multiple tasks.

36
00:02:24,200 --> 00:02:30,080
So we have to have the correct
observability to debug the issue.

37
00:02:30,580 --> 00:02:31,990
Okay, yeah.

38
00:02:32,020 --> 00:02:34,420
Want to go deep on some of this.

39
00:02:35,050 --> 00:02:38,560
So first one is that how we will
maintain the session affinity, right?

40
00:02:38,620 --> 00:02:43,870
So we, same request from the
same user should go to the

41
00:02:43,870 --> 00:02:45,910
same chart every time, right?

42
00:02:45,970 --> 00:02:49,330
So that's where the intelligent metadata
routing service come into the picture.

43
00:02:49,790 --> 00:02:53,390
It, it has it'll have the affinity
toward the, towards the chart.

44
00:02:53,890 --> 00:02:56,260
Which already serving
the serving the traffic.

45
00:02:56,760 --> 00:03:01,770
Second is that if we, if the sum
charts are being overloaded, the

46
00:03:02,010 --> 00:03:06,450
routing need to be intelligent enough
to route the traffic to the less the

47
00:03:06,510 --> 00:03:07,980
charts which have the less traffic.

48
00:03:08,480 --> 00:03:11,240
We can also have the
priority based routing.

49
00:03:11,240 --> 00:03:15,220
This can have the multiple
implementation based on the subscription

50
00:03:15,220 --> 00:03:17,440
model or based on the workload.

51
00:03:18,400 --> 00:03:20,560
The type of the workload, so many things.

52
00:03:20,660 --> 00:03:23,150
And also geographical optimization.

53
00:03:23,570 --> 00:03:28,330
So since we have multiple charts based on
the different regions, so we can actually

54
00:03:28,330 --> 00:03:31,890
have the the beta routing to support it.

55
00:03:32,390 --> 00:03:36,470
So yeah, I think we need to talk about
this, how we will be managing the

56
00:03:36,470 --> 00:03:38,430
state, since we already said that this.

57
00:03:39,025 --> 00:03:40,095
Conversations are stateful.

58
00:03:40,935 --> 00:03:43,810
There is a context, there
is some customer settings.

59
00:03:44,170 --> 00:03:49,440
So whenever we whenever we if the
customer is already being served on

60
00:03:49,440 --> 00:03:53,940
one particular card, and then for some
reason we need to fail over to other,

61
00:03:53,940 --> 00:03:58,000
we also move, have to move the, all
the state corresponding to it, right?

62
00:03:58,660 --> 00:04:03,840
The one solution that we can have the,
we can have the centralized database.

63
00:04:04,180 --> 00:04:07,590
Which so what we are seeing that
we will have the multiple parts of

64
00:04:07,590 --> 00:04:11,610
the service running, but then the,
they will have centralized database

65
00:04:11,660 --> 00:04:17,345
for the context so that so that the
computing is being started, but the,

66
00:04:17,375 --> 00:04:19,145
but not the database to be clear.

67
00:04:19,145 --> 00:04:19,235
Right.

68
00:04:19,735 --> 00:04:21,230
The conversation migration.

69
00:04:21,350 --> 00:04:24,640
So if for some reason one chart is
not working correctly, we have to

70
00:04:25,425 --> 00:04:30,945
migrate the, we have to communicate
to that, that this conversation is

71
00:04:30,945 --> 00:04:32,270
being migrated to the other chart.

72
00:04:32,770 --> 00:04:37,495
So the process will be, will first
retrieve the state will find out

73
00:04:37,495 --> 00:04:41,355
the new shard, we'll update the
routing and then we'll we, and also

74
00:04:41,355 --> 00:04:43,345
we can do some pre-cutting also.

75
00:04:43,845 --> 00:04:44,175
Okay.

76
00:04:44,295 --> 00:04:48,415
There are few different strategies
that can be considered when we want to.

77
00:04:48,475 --> 00:04:52,290
So one thing is that we can have the more
chart, which is horizontal scaling, right?

78
00:04:52,290 --> 00:04:56,060
Every time you want to scale, have
the you can have add more chart.

79
00:04:56,560 --> 00:05:00,310
The second one is that if, for
if the nature of one card is like

80
00:05:00,310 --> 00:05:03,825
with handling more load, you can
actually vertically scale it right?

81
00:05:04,325 --> 00:05:07,655
The capacity reservation, this
is we can reserve the the host

82
00:05:07,955 --> 00:05:13,140
to adopt, to the adopt to the
capacity and the cost optimization.

83
00:05:13,620 --> 00:05:16,675
This is also if you use the
reserve, the capacity, it'll

84
00:05:16,675 --> 00:05:18,155
help in the cost optimization.

85
00:05:18,655 --> 00:05:22,585
The important thing is how
can you auto scale, right?

86
00:05:22,585 --> 00:05:26,975
So there can be different different
indicator to the system that

87
00:05:26,975 --> 00:05:30,965
says that the system need to be
scale up or they scaled down.

88
00:05:31,625 --> 00:05:35,325
We will have a queue where we
monitoring the customer request.

89
00:05:35,325 --> 00:05:40,185
If based on the queue size, we can decide
to scale up or scale down the system.

90
00:05:41,035 --> 00:05:45,295
We will have the latency to serve
the request based on some threshold.

91
00:05:45,295 --> 00:05:48,595
We can decide whether the, if the
requests are taking time, then we can

92
00:05:48,595 --> 00:05:50,095
scale up or scale down the system.

93
00:05:50,645 --> 00:05:55,085
Predictive models, this models are like
the, again, AI model, which actually

94
00:05:55,085 --> 00:05:59,365
based on the past history of the usage
the predict the predict the usage pattern.

95
00:05:59,365 --> 00:06:02,695
And then so that we can the
port scale down accordingly.

96
00:06:03,195 --> 00:06:07,185
The one more is the cost aware triggers,
which will help you keep the cost in the

97
00:06:07,185 --> 00:06:12,195
check if so if you have a chart which
are not being utilized, you can actually

98
00:06:12,225 --> 00:06:14,385
bring it down and then control the cost

99
00:06:14,885 --> 00:06:16,415
all tolerance and resolution.

100
00:06:16,735 --> 00:06:20,215
Yeah, so since now we have multiple
charts, it is very important to

101
00:06:20,215 --> 00:06:21,865
make sure they are all running fine.

102
00:06:21,895 --> 00:06:24,685
We can have the health check
correspond to the e card.

103
00:06:24,955 --> 00:06:27,055
We can have the disaster recovery.

104
00:06:27,655 --> 00:06:28,465
Automatic fail.

105
00:06:29,005 --> 00:06:32,065
These are the few things which
we can do to make sure that the

106
00:06:32,065 --> 00:06:33,505
service always be available.

107
00:06:34,005 --> 00:06:38,205
So operational since complexity and
monitoring, since now we have multiple

108
00:06:38,205 --> 00:06:42,005
charts, it's become very important
to to make sure that we are able to

109
00:06:42,005 --> 00:06:43,534
operate all the charts correctly.

110
00:06:43,605 --> 00:06:47,135
The most important thing is that if we
have collecting all kind of metrics,

111
00:06:47,135 --> 00:06:51,684
we can actually build the automated
system to monitor it and then make sure

112
00:06:51,684 --> 00:06:53,274
that all the systems are functional.

113
00:06:53,774 --> 00:06:57,154
The key benefit of using this
approach is that you get close to

114
00:06:57,154 --> 00:06:58,654
a hundred percent availability.

115
00:06:58,824 --> 00:07:01,014
You can scale your service
as much as you want.

116
00:07:01,084 --> 00:07:03,214
There is a cost reduction
because you don't have to

117
00:07:03,214 --> 00:07:04,655
maintain the monolithic system.

118
00:07:05,125 --> 00:07:09,530
You can actually customize
the chart to it need.

119
00:07:09,660 --> 00:07:12,690
So if it needs very basic
resources, you can just have

120
00:07:12,690 --> 00:07:14,010
it so that you don't pay extra.

121
00:07:14,510 --> 00:07:17,070
And yeah, and each
chart actually operates.

122
00:07:17,120 --> 00:07:18,020
Its on its own.

123
00:07:18,020 --> 00:07:20,750
So then there is like no
dependency on each other.

124
00:07:21,250 --> 00:07:22,450
The implementation.

125
00:07:22,660 --> 00:07:26,460
So we have to first build
few charges and then test it.

126
00:07:26,460 --> 00:07:30,020
That your that your code is
functional, and then you are able to

127
00:07:30,070 --> 00:07:31,660
scale up and scale down the charts.

128
00:07:31,660 --> 00:07:35,140
Your metadata service is working
fine, and then you can actually

129
00:07:35,140 --> 00:07:37,220
explore the solution to the more cart.

130
00:07:37,720 --> 00:07:39,220
The future of AI infrastructure.

131
00:07:39,340 --> 00:07:43,420
I think the infrastructure becomes very
important if we want to support this

132
00:07:43,900 --> 00:07:46,110
the the increased demand of the ai.

133
00:07:47,070 --> 00:07:50,539
We have to make sure that that
the infrastructure is always

134
00:07:50,539 --> 00:07:54,559
available and that's where the
starting strategy will help.

135
00:07:55,059 --> 00:07:55,389
Thank you.

