1
00:00:00,240 --> 00:00:00,990
Hello everyone.

2
00:00:01,050 --> 00:00:01,830
My name is Soap.

3
00:00:01,890 --> 00:00:07,020
I am the co-founder and CTO of Ku ai and
today I'm going to talk about practical

4
00:00:07,080 --> 00:00:12,120
tips for building AI applications
or AI agents using LLMs at KU ai.

5
00:00:12,510 --> 00:00:16,660
We have been, working on
building AI applications and,

6
00:00:16,710 --> 00:00:18,740
agents for the last, 18 months.

7
00:00:19,040 --> 00:00:24,710
And, during our journey, we have
identified a bunch of unique, problems.

8
00:00:25,355 --> 00:00:26,975
That people generally face.

9
00:00:27,035 --> 00:00:30,904
And, we have also faced, those
same problems specifically while

10
00:00:30,904 --> 00:00:32,615
building, applications using LLMs.

11
00:00:33,425 --> 00:00:40,290
And, the, agenda for today's talk is
that, we want to, educate, devs about

12
00:00:40,290 --> 00:00:45,780
these problems, so that, when they are
building apps on top of LLMs, they're

13
00:00:45,780 --> 00:00:49,680
aware of these problems and, Also
discuss what are, the solutions that

14
00:00:49,680 --> 00:00:54,040
work for us and, the dev tools or tooling
that we use to solve these problems.

15
00:00:54,120 --> 00:00:58,795
and, by sharing this information,
we want to save time, when devs

16
00:00:58,885 --> 00:01:02,255
are, building applications on
top of, for the first time.

17
00:01:02,755 --> 00:01:05,605
The first thing that you'll need
to solve when you start building

18
00:01:05,635 --> 00:01:10,605
apps on top of, LLMs is, how
to handle LLM inconsistencies.

19
00:01:11,095 --> 00:01:14,810
you, if you have some experience
building, applications using normal

20
00:01:14,810 --> 00:01:16,160
APIs, you would've seen that they don't.

21
00:01:16,875 --> 00:01:17,565
fail that offer.

22
00:01:17,575 --> 00:01:21,235
while building, general applications,
you don't really worry about,

23
00:01:21,264 --> 00:01:24,004
inconsistencies or failures, that much.

24
00:01:24,164 --> 00:01:26,455
if an API fails, you
just let, the API fail.

25
00:01:26,455 --> 00:01:29,854
And, the user, when they refresh
their page, you make another API

26
00:01:29,854 --> 00:01:31,175
call and it'll most probably succeed.

27
00:01:31,555 --> 00:01:32,875
but in case of LLMs.

28
00:01:33,215 --> 00:01:36,175
this is the first thing that you'll
probably need to solve, when you're

29
00:01:36,175 --> 00:01:40,055
actually building an application
because, LLMs have a much higher

30
00:01:40,055 --> 00:01:42,405
error rate, than your normal APIs.

31
00:01:42,765 --> 00:01:47,265
And unless you solve this particular
thing, your application will have a

32
00:01:47,265 --> 00:01:52,925
terrible ux, or user experience because,
in your application, you'll generally

33
00:01:52,925 --> 00:01:55,115
use the LLM response, somewhere else.

34
00:01:55,475 --> 00:01:57,515
And every time the LLM gives you.

35
00:01:58,130 --> 00:02:00,590
A wrong output, your
application will also crash.

36
00:02:00,830 --> 00:02:06,180
So this is the first, problem
that, you should solve, while

37
00:02:06,180 --> 00:02:08,010
building LLM applications.

38
00:02:08,610 --> 00:02:14,790
now before we get into, Like why, how
to solve this, particular problem.

39
00:02:15,070 --> 00:02:18,895
let's just talk about why
this even occurs, in these, in

40
00:02:18,895 --> 00:02:20,335
these specific applications.

41
00:02:20,695 --> 00:02:23,825
so like I mentioned earlier,
if you are working with, normal

42
00:02:23,945 --> 00:02:26,265
APIs, you generally don't worry.

43
00:02:27,135 --> 00:02:32,825
Much about the error rate, in like
fairly stable, APIs, but even the most

44
00:02:32,825 --> 00:02:36,795
stable LLMs give you a much higher
error rate, than your normal APIs.

45
00:02:37,035 --> 00:02:42,675
And the reason for this is LLMs
are inherently non-deterministic.

46
00:02:43,305 --> 00:02:44,445
so what do you mean by that?

47
00:02:44,775 --> 00:02:46,755
so if you look at an LLM under the hood.

48
00:02:47,215 --> 00:02:53,165
they're essentially statistical machines,
that produce token after token based

49
00:02:53,165 --> 00:02:57,164
on, the input prompt and whatever
tokens have been generated previously.

50
00:02:57,675 --> 00:03:02,865
statistical machines are basically
probabilistic and as soon as you bring

51
00:03:02,865 --> 00:03:06,135
probability into software, you are going
to get something non-deterministic.

52
00:03:06,675 --> 00:03:09,405
Now what do we mean by non-deterministic?

53
00:03:09,895 --> 00:03:14,385
You basically, will get a different
output for the same input every time.

54
00:03:14,465 --> 00:03:20,225
you, ask LLM for a response, you could,
I, and I'm pretty sure like you are,

55
00:03:20,895 --> 00:03:24,495
you would have seen this problem while
using, all the different, chat bots

56
00:03:24,495 --> 00:03:26,540
that are available, like chat, GPT or.

57
00:03:27,190 --> 00:03:31,780
the Deep Seeq or Claude chat, you would've
noticed that, every time you give,

58
00:03:31,830 --> 00:03:34,100
give, give an input, for the same input.

59
00:03:34,130 --> 00:03:36,590
Every time you hit retry,
you'll get a different output.

60
00:03:36,870 --> 00:03:39,050
that's the same thing that
will happen with, the LLM

61
00:03:39,050 --> 00:03:40,760
responses in your applications.

62
00:03:41,160 --> 00:03:45,034
you most of the time don't, have
a lot of control or, will you

63
00:03:45,034 --> 00:03:46,204
get the exact output or not?

64
00:03:46,774 --> 00:03:53,850
Now because of this particular problem,
which is, a being non-deterministic, every

65
00:03:53,850 --> 00:03:58,140
time you give it an input, you'll not
always get the response that you want.

66
00:03:58,140 --> 00:04:03,809
for example, if you ask an L-L-M-A-P-I
to generate, JSON, which is a

67
00:04:03,809 --> 00:04:08,539
structured output, You might get
more fields than, what you asked for.

68
00:04:08,539 --> 00:04:10,009
Sometimes you might get less fields.

69
00:04:10,369 --> 00:04:15,129
sometimes you might have, a bracket
missing based on what we have seen.

70
00:04:15,159 --> 00:04:22,669
if you have a normal stable API, you'll
see an error rate of something like 0.1%.

71
00:04:23,169 --> 00:04:27,979
But if you are working with an LLM, even
the most stable, the LLMs, which have

72
00:04:27,979 --> 00:04:31,859
been, here for the longest amount of
time, they'll give you an error rate of,

73
00:04:31,889 --> 00:04:35,819
something like one to 5% based on what
kind of task you're asking it to perform.

74
00:04:36,179 --> 00:04:40,099
And if you are working with chain
LLM responses, basically you,

75
00:04:40,529 --> 00:04:42,085
provide, the L-L-M-A-P-I with.

76
00:04:42,820 --> 00:04:43,480
A prompt.

77
00:04:43,540 --> 00:04:47,890
You take that response and then you
provide it with another prompt, using

78
00:04:47,890 --> 00:04:49,820
the response, that you got earlier.

79
00:04:50,100 --> 00:04:53,190
this is basically a
chained, LLM responses.

80
00:04:53,680 --> 00:04:58,080
you'll see that your error rate gets
compounded and, this particular thing,

81
00:04:58,520 --> 00:05:00,950
will probably not have a solution.

82
00:05:01,450 --> 00:05:05,170
In the LLMs because of
LLMs, like I mentioned, are

83
00:05:05,170 --> 00:05:06,550
inherently non-deterministic.

84
00:05:06,560 --> 00:05:09,190
that is how the architecture is.

85
00:05:09,460 --> 00:05:13,920
So this is something that needs
to be solved in your application.

86
00:05:14,250 --> 00:05:18,840
you can't really wait for like
LLMs to get better and, start

87
00:05:18,840 --> 00:05:19,830
providing better responses.

88
00:05:19,910 --> 00:05:23,755
they will definitely, get better and,
Reduce the error rate, but, I think

89
00:05:23,755 --> 00:05:27,595
as an application developer, it's your
responsibility to take care of this

90
00:05:27,595 --> 00:05:29,065
issue within your application as well.

91
00:05:29,965 --> 00:05:31,825
So what are your options?

92
00:05:32,545 --> 00:05:37,145
The first thing that you should definitely
try out is, retries and timeouts.

93
00:05:37,565 --> 00:05:40,485
Now, these are not new concepts.

94
00:05:40,535 --> 00:05:43,175
if you have worked in software
development for a while now,

95
00:05:43,235 --> 00:05:44,435
you would know what a retry is.

96
00:05:44,465 --> 00:05:45,155
Basically.

97
00:05:45,235 --> 00:05:45,865
when an API.

98
00:05:46,365 --> 00:05:47,445
Gives you a wrong response.

99
00:05:47,594 --> 00:05:52,584
You try it again with some, cool
down period or, maybe not depending

100
00:05:52,584 --> 00:05:54,004
on, how the rate limits are.

101
00:05:54,374 --> 00:05:58,444
retry is basically, you make
an API call the API fails.

102
00:05:58,574 --> 00:06:01,634
you wait for a while and
then you retry it again.

103
00:06:01,684 --> 00:06:02,524
as simple as that.

104
00:06:03,034 --> 00:06:04,924
Now, when you.

105
00:06:05,424 --> 00:06:09,674
Are developing, general applications,
I think retries and timeouts are

106
00:06:09,674 --> 00:06:13,194
something that, are not the first
thing that you would implement.

107
00:06:13,839 --> 00:06:18,549
Because, you just assume, you just go
with the assumption that the API response

108
00:06:18,549 --> 00:06:20,409
rate is going to be fairly reasonable.

109
00:06:20,419 --> 00:06:25,199
they'll, most of the time work and,
like not adding retries and timeouts

110
00:06:25,199 --> 00:06:28,569
to your APIs will, not really
degrade the application performance.

111
00:06:29,139 --> 00:06:32,679
unless like you are working
with very critical, applications

112
00:06:32,679 --> 00:06:34,279
like, something in finance.

113
00:06:34,685 --> 00:06:38,645
Or health where, the operation has to
finish, in which case you'll, definitely

114
00:06:38,825 --> 00:06:40,175
start with retries and timeouts.

115
00:06:40,175 --> 00:06:43,435
But, in our general experience, if
you're working with normal APIs, you

116
00:06:43,435 --> 00:06:44,845
don't really worry about these things.

117
00:06:45,385 --> 00:06:50,755
but because LLM APIs specifically
have a higher error rate, retries

118
00:06:50,755 --> 00:06:55,015
and timeouts are something that,
Need to be implemented from day one.

119
00:06:55,475 --> 00:06:57,455
timeouts again, I think, I
don't need to get into this.

120
00:06:57,455 --> 00:07:03,335
A timeout is basically, you make
an API call and you wait for X

121
00:07:03,365 --> 00:07:05,045
seconds for the API to return.

122
00:07:05,225 --> 00:07:07,445
If it doesn't return an X
seconds for whatever reason.

123
00:07:07,795 --> 00:07:10,815
you terminate that, particular
a p and you try again.

124
00:07:11,220 --> 00:07:17,140
this basically is protection against,
the server, the API server being down.

125
00:07:17,200 --> 00:07:22,490
And, so if you don't do this, and if the
API takes a minute to respond, you, your

126
00:07:22,490 --> 00:07:24,230
application is also stuck for a minute.

127
00:07:24,230 --> 00:07:25,860
And, so are your users.

128
00:07:25,860 --> 00:07:31,860
So a timeout is basically protection
so that if an a p doesn't return in

129
00:07:31,860 --> 00:07:34,860
like a reasonable amount of time,
you cancel that API call and you.

130
00:07:35,405 --> 00:07:37,855
retry that API again, that's
where timeout comes into picture.

131
00:07:38,465 --> 00:07:38,645
cool.

132
00:07:38,675 --> 00:07:41,675
So how do you implement
this into your application?

133
00:07:41,775 --> 00:07:45,165
I would suggest don't, write the
word for retries and timeouts from

134
00:07:45,165 --> 00:07:49,155
scratch because there are a bunch
of, battle tested libraries available

135
00:07:49,155 --> 00:07:52,065
in every language that you can use.

136
00:07:52,155 --> 00:07:54,195
And, with a few lines of code add these.

137
00:07:54,715 --> 00:07:56,365
behaviors to your application.

138
00:07:56,365 --> 00:07:59,065
So let's look at a few examples.

139
00:07:59,475 --> 00:08:02,805
the one that we actually use in
production is, this one called

140
00:08:02,805 --> 00:08:08,575
Tensity by, it's a Python library,
and, it allows you to add retry to

141
00:08:08,575 --> 00:08:10,495
your functions by simply doing this.

142
00:08:10,705 --> 00:08:11,635
you add a decorator.

143
00:08:12,135 --> 00:08:14,345
Which is provided by the,
by this particular library.

144
00:08:14,395 --> 00:08:17,865
you add it to a function and
this function will be retried.

145
00:08:18,085 --> 00:08:21,835
whenever there is an, exception or
error in this particular function.

146
00:08:22,375 --> 00:08:28,565
Now, you'd ideally want more control
over, how many d tries to do, how,

147
00:08:28,925 --> 00:08:32,485
like how long to wait after, every
try and those kind of things.

148
00:08:32,805 --> 00:08:33,375
those.

149
00:08:33,975 --> 00:08:35,805
All options are present in this library.

150
00:08:35,805 --> 00:08:40,045
You can, give it stopping conditions where
you want to stop after three retries.

151
00:08:40,045 --> 00:08:42,445
You want to stop after,
10 seconds of retrying.

152
00:08:42,865 --> 00:08:46,165
you can add a wait time
before every retry.

153
00:08:46,385 --> 00:08:47,495
you can add a fixed wait time.

154
00:08:47,495 --> 00:08:48,875
You can add a random wait time.

155
00:08:49,385 --> 00:08:52,025
all these, different
kinds of, behaviors can.

156
00:08:52,610 --> 00:08:55,200
Be added using this library,
with a few lines of course.

157
00:08:55,200 --> 00:08:58,860
if you are working in Python, this
is our choice, has been working,

158
00:08:59,350 --> 00:09:00,940
very well for us in production.

159
00:09:01,200 --> 00:09:03,210
this is what we, have been
using for a very long time.

160
00:09:03,210 --> 00:09:07,630
So I would recommend this, if you're
working in js, there is a similar library

161
00:09:07,630 --> 00:09:09,280
called Retract, very conveniently.

162
00:09:09,725 --> 00:09:11,585
that you can, that is available on NPM.

163
00:09:11,905 --> 00:09:13,525
similar type of functionalities.

164
00:09:13,525 --> 00:09:15,965
It gives you, retries and timeouts.

165
00:09:16,205 --> 00:09:19,565
Oh, by the way, tenacity also
has, timeout related decorators.

166
00:09:19,965 --> 00:09:20,625
works the same way.

167
00:09:20,675 --> 00:09:23,795
if you want to add a timeout to
particular function, you just add

168
00:09:23,795 --> 00:09:25,475
that decorator, specify the timeout.

169
00:09:25,935 --> 00:09:26,145
yeah.

170
00:09:26,645 --> 00:09:31,625
This library, if you are working with
a JS application, retry is, our choice.

171
00:09:32,135 --> 00:09:37,165
the third option is, basically, a
lot of people who are, developing

172
00:09:37,165 --> 00:09:40,665
LLM applications are using these
frameworks, LLM frameworks to

173
00:09:40,665 --> 00:09:44,560
handle, API calls retries and
like a bunch of different things.

174
00:09:44,860 --> 00:09:48,460
So the most famous LLM frameworks,
framework, which a lot of

175
00:09:48,460 --> 00:09:49,990
people are using is land chain.

176
00:09:50,530 --> 00:09:54,580
And if you are working on top of
land chain, land chain provides

177
00:09:54,580 --> 00:10:00,195
you a, basically a, some,
mechanism to retry, out of the box.

178
00:10:00,325 --> 00:10:01,285
it's called.

179
00:10:01,785 --> 00:10:06,085
the retry output parser, where, you
can use this to make the LLM calls

180
00:10:06,085 --> 00:10:11,355
and whenever the LLM call fails, this
parser will basically, handle retry

181
00:10:11,355 --> 00:10:15,625
on your behalf, by passing, The prompt
again, and, also the previous output,

182
00:10:15,695 --> 00:10:19,279
so that the, L-L-M-A-P has a better
idea that, okay, the last output failed.

183
00:10:19,339 --> 00:10:21,609
And, I'm not supposed to,
give this response again.

184
00:10:22,149 --> 00:10:24,839
So if you're on, then it's
already sorted out for you.

185
00:10:24,839 --> 00:10:25,994
You use the retry output parcel.

186
00:10:26,494 --> 00:10:30,099
Alright, so this sorts out
how to implement retries

187
00:10:30,099 --> 00:10:32,559
and timeouts the next most.

188
00:10:33,059 --> 00:10:37,429
Common reason for, failure or
LLM inconsistency is when you are

189
00:10:37,429 --> 00:10:38,809
working with structured outputs.

190
00:10:39,319 --> 00:10:42,959
So when I say structured output,
something like you asked the LLM to

191
00:10:42,959 --> 00:10:47,769
generate A-J-S-O-N or X-M-L-C-S-V,
even list ra, those kind of things.

192
00:10:47,819 --> 00:10:51,849
whenever you are asking an LLM to
generate a structured output, there

193
00:10:51,849 --> 00:10:54,629
is a slight chance that, there'll be
something wrong with that structure.

194
00:10:55,064 --> 00:10:56,474
Maybe there are some fields missing.

195
00:10:56,514 --> 00:10:57,594
there are extra fields.

196
00:10:57,954 --> 00:11:01,449
in case of JSONs, XMLs, there are
brackets missing, might happen.

197
00:11:01,869 --> 00:11:04,239
So how do you handle that?

198
00:11:04,639 --> 00:11:10,599
the simplest way to do that is to, is
to integrate a schema library instead

199
00:11:10,599 --> 00:11:12,129
of doing it on your own every time.

200
00:11:12,339 --> 00:11:15,589
a schema library could be something
like pedantic and, this is what.

201
00:11:16,519 --> 00:11:19,229
We use, in our production.

202
00:11:19,359 --> 00:11:24,069
PTECH is basically, the most commonly
used data validation library in Python.

203
00:11:24,549 --> 00:11:31,209
And what it does is it allows you to,
create classes, in which you describe

204
00:11:31,209 --> 00:11:37,239
the structure of your response, and
then you use this particular class to,

205
00:11:37,339 --> 00:11:39,649
check whether the LRM response fits.

206
00:11:40,039 --> 00:11:41,629
This particular structure or not.

207
00:11:41,849 --> 00:11:46,429
it'll check for, fields,
extra fields, or less fields.

208
00:11:46,549 --> 00:11:49,639
It'll check for data types,
and a bunch of other options.

209
00:11:49,859 --> 00:11:51,759
on Python, just go for Edan Tech.

210
00:11:51,819 --> 00:11:56,859
it is a tried and tested library,
and, it'll make the data validation

211
00:11:56,859 --> 00:11:59,689
part when you're working with
structured outputs, hassle free.

212
00:12:00,199 --> 00:12:03,779
similarly, if you are working with
NPM, there's something called Yap, same

213
00:12:03,779 --> 00:12:06,109
stuff as pedantic, data validation.

214
00:12:06,469 --> 00:12:12,439
you essentially, define the shape of your
output and, yap basically uses that shape,

215
00:12:12,499 --> 00:12:18,469
which is essentially a class, JS class,
or a JS object to, Check or enforce,

216
00:12:18,519 --> 00:12:20,199
the structure of your LLM responses.

217
00:12:20,619 --> 00:12:24,919
and the idea is to use, these,
these, data validation libraries,

218
00:12:25,019 --> 00:12:26,789
along with, retries and timeouts.

219
00:12:26,789 --> 00:12:29,929
what you basically do is when you
make an L-L-M-E-P-I call, and you

220
00:12:29,929 --> 00:12:34,759
get a response, you pass it through
pedantic or, whatever data validation.

221
00:12:34,839 --> 00:12:39,159
Letter you are using, and if you
get an error, you use the retry, to

222
00:12:39,214 --> 00:12:43,144
like basically let the LLM generate
that structured output again, most

223
00:12:43,144 --> 00:12:48,044
of the time, you will see that,
a couple of retries sorts it out.

224
00:12:48,054 --> 00:12:50,934
it's not like every API call
will feel in the same way.

225
00:12:51,294 --> 00:12:53,904
So if, let's say there
are a few things missing.

226
00:12:54,404 --> 00:12:57,784
In your structured output the first
time when you do a retry, the next

227
00:12:57,784 --> 00:12:58,984
time you'll get the correct output.

228
00:12:59,734 --> 00:13:05,774
but just as a general advice, if
you see that there are particular

229
00:13:05,774 --> 00:13:09,414
kind of issues happening again and
again, you should mention that,

230
00:13:09,694 --> 00:13:11,044
instruction in the prompt itself.

231
00:13:11,494 --> 00:13:15,944
Because what happens is that, when
you do retry, an API call, which

232
00:13:16,334 --> 00:13:19,544
was supposed to take five seconds,
might end up taking 15 to 20 seconds.

233
00:13:19,874 --> 00:13:23,004
And, it'll make your, make
your application feel laggy.

234
00:13:23,844 --> 00:13:26,924
because, at the end of that API
call, you're going to provide

235
00:13:26,924 --> 00:13:29,624
some output to your users and,
they're waiting for that output.

236
00:13:30,054 --> 00:13:34,224
so if, that there are particular kind
of, problems that are happening again

237
00:13:34,229 --> 00:13:39,889
and again, like for example, If, if
you are, generating JSON, using, JSON,

238
00:13:39,889 --> 00:13:43,499
using an L-L-M-A-P-I, and you'll see
that, like the LLM is always using

239
00:13:43,499 --> 00:13:47,319
single code sensor or double code, which
will generally cause issues, you should

240
00:13:47,319 --> 00:13:51,369
specify that as an important point in
your prompt so that, you get the correct

241
00:13:51,369 --> 00:13:53,179
output in the first, attempt itself.

242
00:13:54,129 --> 00:13:59,079
this is just an additional level of check,
but the idea is that the first response

243
00:13:59,079 --> 00:14:01,149
should itself give you the correct output.

244
00:14:01,159 --> 00:14:04,449
anything that is, that is known,
should be mentioned in the prompt

245
00:14:04,449 --> 00:14:07,669
as a special instruction, so that
you don't keep retrying and you

246
00:14:07,669 --> 00:14:09,079
use this an waiting for an output.

247
00:14:09,629 --> 00:14:15,119
one, one additional option worth, one
special mention here is, The structured

248
00:14:15,119 --> 00:14:17,429
output capabilities provided by open ai.

249
00:14:17,829 --> 00:14:23,594
if you're using, GPT models, and
open AI APIs, what you can do is

250
00:14:23,594 --> 00:14:27,764
there is a response format field
where you can specify a class and,

251
00:14:27,794 --> 00:14:32,714
the open air APIs themselves will,
try to enforce the structure.

252
00:14:33,134 --> 00:14:36,539
but this one problem here, which
is if you want to switch out.

253
00:14:37,184 --> 00:14:39,974
The model and, use something
else like Claude Orrock.

254
00:14:40,364 --> 00:14:44,654
then you have basically, lost the
structured output capabilities

255
00:14:44,654 --> 00:14:48,594
because those are not, available
right now in, other LLM APIs.

256
00:14:48,854 --> 00:14:52,724
my suggestion is to just handle the,
schema enforcing and checking in

257
00:14:52,724 --> 00:14:55,604
your application itself so that like
it's easy for you to switch out,

258
00:14:56,004 --> 00:14:57,294
the models and use different models.

259
00:14:57,794 --> 00:15:01,064
That's all for, handling
LLM inconsistencies.

260
00:15:01,114 --> 00:15:02,914
two main things, retries and timeouts.

261
00:15:02,954 --> 00:15:05,174
use them from the start.

262
00:15:05,234 --> 00:15:08,874
if you are working with structured
outputs, use a data validation

263
00:15:08,874 --> 00:15:11,274
library to, check the structure.

264
00:15:11,424 --> 00:15:12,444
you see the options here.

265
00:15:12,484 --> 00:15:13,414
any of these are good.

266
00:15:13,914 --> 00:15:17,974
The next thing that you should, start
thinking about is how to implement

267
00:15:17,974 --> 00:15:20,374
streaming in your LLM application.

268
00:15:20,854 --> 00:15:24,894
generally when you develop
APIs, you, you implement, you

269
00:15:24,894 --> 00:15:26,274
implement normal request response.

270
00:15:26,274 --> 00:15:31,194
you get an a p call and, the server
does some work, and then you, then you

271
00:15:31,194 --> 00:15:32,964
return the entire response in one go.

272
00:15:33,564 --> 00:15:34,044
in.

273
00:15:34,524 --> 00:15:39,414
In case of, LLMs, what happens is
sometimes it might take a long time

274
00:15:39,414 --> 00:15:41,034
for the l LM to generate a response.

275
00:15:41,964 --> 00:15:43,524
That's where streaming comes into picture.

276
00:15:43,524 --> 00:15:48,864
Streaming of your responses allow you
to start returning partial responses,

277
00:15:49,324 --> 00:15:54,174
to the client, even when, the LLM is
not done, done with the generation.

278
00:15:54,764 --> 00:15:55,874
let's look at why.

279
00:15:55,974 --> 00:15:57,774
Streaming is so important.

280
00:15:58,094 --> 00:16:03,544
while building LLM applications, like
I mentioned, LLMs might take long

281
00:16:03,544 --> 00:16:06,399
time for, for generation, to complete.

282
00:16:06,699 --> 00:16:10,289
Now, when your user is,
using your application, most

283
00:16:10,289 --> 00:16:11,339
users are very impatient.

284
00:16:11,369 --> 00:16:11,609
you.

285
00:16:12,369 --> 00:16:15,299
can't ask them to wait for, seconds.

286
00:16:15,349 --> 00:16:16,849
like I'm not even talking about minutes.

287
00:16:17,159 --> 00:16:20,279
if you have a ten second delay
in showing the response, you

288
00:16:20,279 --> 00:16:21,419
might see a lot of drop off.

289
00:16:21,869 --> 00:16:25,929
Um, and like most of the LMS that you
would work with would take five to 10

290
00:16:25,929 --> 00:16:29,609
seconds for even the simplest, prompts.

291
00:16:30,029 --> 00:16:33,479
So how do you improve the ux?

292
00:16:33,984 --> 00:16:36,264
and make sure that your
users don't drop off.

293
00:16:36,694 --> 00:16:38,254
that's where streaming comes into picture.

294
00:16:38,714 --> 00:16:41,634
what streaming allows you
to do is, LLMs generate.

295
00:16:42,134 --> 00:16:43,604
Response is token by token.

296
00:16:43,654 --> 00:16:44,884
they'll generate it word by word.

297
00:16:45,304 --> 00:16:48,934
And, what streaming allows you to do
is, you don't need to wait for the LM to

298
00:16:48,934 --> 00:16:51,244
generate the entire response or, output.

299
00:16:51,664 --> 00:16:55,174
What you can do is as soon as it is done
generating a few words, you can send

300
00:16:55,174 --> 00:16:59,064
them to the client and, start displaying
them on the UI or, what your client

301
00:16:59,064 --> 00:17:02,174
you're using, in this way, the user.

302
00:17:03,059 --> 00:17:07,479
Doesn't really feel the lag,
that, LM generation results in.

303
00:17:07,879 --> 00:17:11,409
what they see is that, as soon as
they type out a prompt, immediately

304
00:17:11,409 --> 00:17:14,379
they start seeing some response
and they can start reading it out.

305
00:17:14,749 --> 00:17:19,699
you, this is a very common
pattern in any chat or LLM

306
00:17:19,699 --> 00:17:21,259
application that you would've used.

307
00:17:21,559 --> 00:17:24,619
As soon as you type something out or
you do an action, you start seeing

308
00:17:25,009 --> 00:17:28,069
partial results on your UI that
is implemented through streaming.

309
00:17:28,619 --> 00:17:35,079
the most common or the most, used way,
to implement streaming is web sockets.

310
00:17:35,529 --> 00:17:40,579
web sockets allow you to send,
generated tokens or words in real time.

311
00:17:40,999 --> 00:17:45,939
The connection is established, between
client and the server, and then, until

312
00:17:45,939 --> 00:17:51,369
the entire generation is completed,
or, as long as the user is, live on

313
00:17:51,369 --> 00:17:55,959
the ui, you can just like reuse that
connection to keep sending response,

314
00:17:56,389 --> 00:17:58,249
as and when it gets, generated.

315
00:17:58,739 --> 00:18:01,349
this is also a bidirectional, um.

316
00:18:02,244 --> 00:18:05,994
communication method, so you
can use the same method to get

317
00:18:05,994 --> 00:18:08,304
some input from the client.

318
00:18:08,544 --> 00:18:12,724
Also, no One drawback of,
web sockets is that they need

319
00:18:12,724 --> 00:18:14,564
some custom, implementation.

320
00:18:14,794 --> 00:18:19,424
you can't just take like your
simple HDP rest server and,

321
00:18:19,424 --> 00:18:20,564
convert it into web socket.

322
00:18:20,564 --> 00:18:23,324
You'll need to redo your implementation.

323
00:18:23,724 --> 00:18:24,924
use new libraries.

324
00:18:25,294 --> 00:18:27,124
probably even new use a new language.

325
00:18:27,124 --> 00:18:32,154
for example, if you are working on
Python, Python is not, very, efficient,

326
00:18:32,704 --> 00:18:34,204
way for implementing web sockets.

327
00:18:34,204 --> 00:18:39,544
You probably want to move to a different
language which handles, threads or

328
00:18:39,634 --> 00:18:44,864
multi-processing in a much better way than
Python, like Golan or Java, or c plus.

329
00:18:45,154 --> 00:18:47,584
so generally web socket implementation.

330
00:18:48,454 --> 00:18:49,774
Is a considerable effort.

331
00:18:50,194 --> 00:18:54,194
And, if all you want to do is
stream LLM responses, it probably

332
00:18:54,194 --> 00:18:55,544
is not the best way to do it.

333
00:18:56,114 --> 00:19:01,794
there is another, solution for
streaming, over SGDP, which is

334
00:19:01,794 --> 00:19:08,174
called Server Set Events, which
basically uses, your, your, um.

335
00:19:09,164 --> 00:19:12,374
as server itself, like basically if
you are on Python and you're using

336
00:19:12,374 --> 00:19:17,224
Flask or Fast API, you won't need
to do a lot of changes to start

337
00:19:17,224 --> 00:19:21,164
streaming, using server sentiments,
code-wise or implementation-wise.

338
00:19:21,194 --> 00:19:22,094
this is a minimal effort.

339
00:19:22,604 --> 00:19:28,484
what this essentially does is,
It'll use the same FTTP, connection,

340
00:19:28,484 --> 00:19:31,214
which your STPI call utilizes.

341
00:19:31,494 --> 00:19:36,234
but instead of, sending the entire
response in one shot, you can

342
00:19:36,234 --> 00:19:39,914
send the response in chunks and,
on your client side, you can,

343
00:19:40,384 --> 00:19:42,004
receive it in and start displaying.

344
00:19:42,504 --> 00:19:45,544
now this is a unidirectional, flow.

345
00:19:45,764 --> 00:19:47,534
it works exactly as a rest API call.

346
00:19:48,159 --> 00:19:54,009
but instead of, the client waiting for
the entire response, to come, the client

347
00:19:54,009 --> 00:19:59,749
starts showing chunks that have been sent
from server, using server sentiments.

348
00:20:00,309 --> 00:20:02,369
implementation wise, it's very simple.

349
00:20:02,369 --> 00:20:07,129
Like you, just need to maybe implement
the generator, if you're using Python

350
00:20:07,459 --> 00:20:09,699
and, Maybe add a couple of headers.

351
00:20:10,299 --> 00:20:13,059
we won't get into specific details
because these are, things that you

352
00:20:13,059 --> 00:20:14,739
can easily Google and, find out.

353
00:20:14,979 --> 00:20:18,809
But, our recommendation if you
want to implement streaming in your

354
00:20:18,809 --> 00:20:22,549
application and you already have a
rest, set up ready on the backend.

355
00:20:22,599 --> 00:20:26,009
just go for server set events,
much, faster implementation,

356
00:20:26,069 --> 00:20:27,839
also much easier to implement.

357
00:20:28,389 --> 00:20:29,949
web sockets is a bit heavy.

358
00:20:30,399 --> 00:20:35,389
And unless you have a specific
use case for, web sockets, I won't

359
00:20:35,389 --> 00:20:37,894
recommend, going that, on that path

360
00:20:38,394 --> 00:20:39,954
streaming is a good solution.

361
00:20:39,954 --> 00:20:45,034
If, the particular task that an
LLM is handling, gets over in a few

362
00:20:45,034 --> 00:20:46,414
seconds, like five to 10 seconds.

363
00:20:46,934 --> 00:20:48,164
but if your task.

364
00:20:48,179 --> 00:20:50,309
It is going to take minutes.

365
00:20:50,369 --> 00:20:52,769
streaming probably is not a good option.

366
00:20:53,229 --> 00:20:55,869
that's where background
jobs come into picture.

367
00:20:56,229 --> 00:21:00,409
if you have a task which can be done in
five to 10 seconds, probably use streaming

368
00:21:00,439 --> 00:21:05,489
and, it's a good way to start showing,
an output, to the user on client side.

369
00:21:05,709 --> 00:21:08,439
but if you have a task which
is going to take minutes.

370
00:21:08,929 --> 00:21:11,809
it is better to handle it
asynchronously instead of

371
00:21:11,809 --> 00:21:13,569
synchronously in your, backend server.

372
00:21:14,029 --> 00:21:15,469
and background jobs help you do that.

373
00:21:15,969 --> 00:21:22,299
So what are these particular use
cases where you, might want to use

374
00:21:22,299 --> 00:21:23,739
background jobs instead of swimming?

375
00:21:24,159 --> 00:21:24,999
think of it this way.

376
00:21:25,359 --> 00:21:29,289
Let's say if you, if you are
building, something like.

377
00:21:30,009 --> 00:21:35,919
An essay generator, and you allow the
user to, enter essay topics in bulk.

378
00:21:35,979 --> 00:21:39,949
So if someone, gives you a single
essay topic, probably, you'll finish

379
00:21:39,949 --> 00:21:41,329
the generation in a few seconds.

380
00:21:41,329 --> 00:21:42,899
And, streaming is the way to go.

381
00:21:43,319 --> 00:21:48,249
But let's say if someone, Gives you a
hundred essay topics, for generation.

382
00:21:48,669 --> 00:21:52,269
I know that this particular task, doesn't
matter how fast the LLM is going to

383
00:21:52,269 --> 00:21:54,169
take minutes at least a few minutes.

384
00:21:54,559 --> 00:21:59,029
And, if you use streaming for this,
streaming, will do all the work

385
00:21:59,059 --> 00:22:03,419
in your backend server, and, until
this particular task is completed,

386
00:22:03,459 --> 00:22:04,749
which is going to be few minutes.

387
00:22:05,259 --> 00:22:11,819
your backend server resources are going
to get, hogged or are going to be, tied

388
00:22:11,819 --> 00:22:16,199
up in this particular task, which is
very inefficient because, like your

389
00:22:16,199 --> 00:22:20,409
backend server's job is basically take
a request, process in a few seconds

390
00:22:20,409 --> 00:22:22,294
and, send it back to the client.

391
00:22:22,699 --> 00:22:24,709
if you start doing things
which take minutes.

392
00:22:25,189 --> 00:22:30,249
You will see that, if you have a lot
of concurrent users, you, your backend

393
00:22:30,249 --> 00:22:35,329
server will be busy and it'll not be
able to, handle tasks, which take a few

394
00:22:35,329 --> 00:22:40,239
seconds and, your APIs will start getting
blocked and, your, your, application

395
00:22:40,239 --> 00:22:41,409
performance will start to degrade.

396
00:22:41,949 --> 00:22:44,519
So what's the solution here?

397
00:22:44,699 --> 00:22:47,759
You, the solution is, you don't handle.

398
00:22:48,284 --> 00:22:52,004
long running tasks in
backend server synchronously.

399
00:22:52,334 --> 00:22:55,394
You handle them in background
jobs asynchronously.

400
00:22:55,454 --> 00:22:56,204
Basically.

401
00:22:56,204 --> 00:22:59,194
when a user gives you a task, which
is going to take minutes, you log

402
00:22:59,194 --> 00:23:03,984
it in a database, a background job
will pick that task up till then, you

403
00:23:03,984 --> 00:23:06,654
tell the, you basically communicate
to the user that, okay, this is

404
00:23:06,654 --> 00:23:08,734
going to take a few minutes, once.

405
00:23:09,139 --> 00:23:12,719
The task is completed, you'll
get a notification, probably

406
00:23:12,719 --> 00:23:14,739
as an email, or on Slack.

407
00:23:15,129 --> 00:23:20,549
And, what you do is you use a background
job to, pick up the task, process it, and

408
00:23:20,549 --> 00:23:22,349
once it's ready, send out a notification.

409
00:23:22,989 --> 00:23:25,779
easiest way to implement this is CR Jobs.

410
00:23:25,829 --> 00:23:29,589
crown Jobs have been here for, I
don't know for a very long time.

411
00:23:29,799 --> 00:23:35,459
very easy to implement, on any
Unix-based, server, which is

412
00:23:35,459 --> 00:23:38,759
probably, what will be used in most
of, production backing servers.

413
00:23:39,189 --> 00:23:43,059
all you need to do is set up a CR
job, which does the processing.

414
00:23:43,519 --> 00:23:46,939
and the CR job runs every few
minutes, checks the database

415
00:23:46,939 --> 00:23:48,169
if there are in pending tasks.

416
00:23:48,599 --> 00:23:51,369
now when your user, comes to you.

417
00:23:52,209 --> 00:23:57,199
With, with a task, you just put it
in a DB and, mark it as pending.

418
00:23:57,749 --> 00:24:00,749
when the crown job wakes up in a
few minutes, it will check for any

419
00:24:00,749 --> 00:24:02,429
pending task and start the processing.

420
00:24:03,269 --> 00:24:07,094
And, on the US side you can probably,
implement some sort of polling.

421
00:24:07,604 --> 00:24:09,344
To check if the task is completed or not.

422
00:24:09,344 --> 00:24:11,834
And once it is completed, you
can display that on the ui.

423
00:24:12,554 --> 00:24:13,974
But, this is an optional thing.

424
00:24:14,004 --> 00:24:18,314
Ideally, if you're using background
jobs, you should also, sorry.

425
00:24:18,524 --> 00:24:23,694
You should also, Separately communicate,
that the task is completed with the user

426
00:24:23,694 --> 00:24:27,834
because, the general, idea is that, when
you, when a task is going to take a few

427
00:24:27,834 --> 00:24:32,564
minutes, your users will probably come
to your platform, submit the task, and

428
00:24:32,564 --> 00:24:34,254
they will, move away from your platform.

429
00:24:34,254 --> 00:24:35,279
So they're not looking at.

430
00:24:35,884 --> 00:24:37,444
the UI of your application.

431
00:24:37,444 --> 00:24:41,584
So you should probably communicate
that the task is completed through

432
00:24:41,584 --> 00:24:43,504
an email or a Slack notification.

433
00:24:43,964 --> 00:24:47,714
so the users who have moved away from,
the UI also know that okay, that,

434
00:24:47,744 --> 00:24:50,114
that generation has been completed.

435
00:24:51,029 --> 00:24:53,149
this works very well, minimal setup.

436
00:24:53,269 --> 00:24:55,789
Nothing new that you probably
need to learn, nothing new

437
00:24:55,789 --> 00:24:56,839
that you need to install.

438
00:24:57,269 --> 00:25:01,619
for the initial stages of your LLM
application, just go for a crown job.

439
00:25:02,099 --> 00:25:08,779
what happens is that as your, application
grows, you'll probably need to scale

440
00:25:08,779 --> 00:25:13,439
this now, if you run multiple crown
jobs, you need to handle which crown job.

441
00:25:13,854 --> 00:25:18,984
pick up which task you need to implement
some sort of, distributed locking and,

442
00:25:19,034 --> 00:25:20,834
all those complexities come into picture.

443
00:25:20,924 --> 00:25:26,024
Basically, crown jobs are good
for the initial stages, but, like

444
00:25:26,084 --> 00:25:27,734
we also started with crown jobs.

445
00:25:27,784 --> 00:25:31,504
we still use crown jobs for some
simple tasks, but there will be a

446
00:25:31,504 --> 00:25:36,204
stage, When you'll need to move away
from crown jobs for scalability, and

447
00:25:36,204 --> 00:25:41,204
for, better retrain mechanisms, that's
where task queue come into picture.

448
00:25:41,654 --> 00:25:47,004
So basically think of task queue as crown
jobs with like more intelligence, where

449
00:25:47,454 --> 00:25:52,834
all the, task management that needs to be
done, is handled by the task queue itself.

450
00:25:53,204 --> 00:25:55,684
when I say task management,
on a very high level, what.

451
00:25:56,344 --> 00:25:59,364
It means is that, you submit
a task to the task queue.

452
00:25:59,504 --> 00:26:04,574
generally a task queue is backed by some
storage, like Redis or some other cache.

453
00:26:04,914 --> 00:26:09,634
the task is stored over there, and then
the task queue handles, basically a

454
00:26:09,634 --> 00:26:13,884
task queue will have a bunch of workers
running and, a ta the task queue will

455
00:26:13,884 --> 00:26:16,814
then handle, how to allocate that work.

456
00:26:17,264 --> 00:26:20,114
To which worker based on like a
bunch of different mechanisms.

457
00:26:20,114 --> 00:26:23,994
Like you can have, priority queues,
you can have a bunch of different

458
00:26:24,204 --> 00:26:26,614
retry mechanisms, and all those things.

459
00:26:26,954 --> 00:26:28,544
two good things about using task queue.

460
00:26:29,184 --> 00:26:30,594
task queues are much easier to scale.

461
00:26:31,004 --> 00:26:35,204
in Aron job, if you go from one to two
to 10 crown jobs, you have to handle a

462
00:26:35,204 --> 00:26:37,894
bunch of, Locking related stuff yourself.

463
00:26:37,954 --> 00:26:40,724
in task queue, it's already,
implemented for you.

464
00:26:40,724 --> 00:26:43,964
So all you can do is increase the
number of workers in a task queue.

465
00:26:44,504 --> 00:26:50,954
And, if you start getting more, tasks or
workload, the, you can just it's as easy

466
00:26:50,954 --> 00:26:54,204
as just changing a number on a dashboard,
to increase the number of workers.

467
00:26:54,744 --> 00:26:57,744
again, like all the, additional handling.

468
00:26:58,644 --> 00:27:01,614
For race conditions, retries,
timeouts, it's already taken care of.

469
00:27:01,614 --> 00:27:04,174
All you need to do is,
provide some configuration.

470
00:27:04,574 --> 00:27:06,494
you also get better
monitoring with task use.

471
00:27:06,744 --> 00:27:11,894
you, every task you comes with
some sort of, monitoring mechanism

472
00:27:11,894 --> 00:27:15,434
or, dashboard where you can
see what are the task currently

473
00:27:15,434 --> 00:27:17,054
running, how much resources there.

474
00:27:17,414 --> 00:27:21,344
Eating up, which tasks are
failing, start or restart tasks

475
00:27:21,344 --> 00:27:22,454
and all those kind of things.

476
00:27:22,874 --> 00:27:25,664
once you start scaling your
application, go for task use.

477
00:27:26,384 --> 00:27:32,544
The task queue that we use in our
production is called rq, which stands

478
00:27:32,544 --> 00:27:37,759
for Redis Q. And, as the name suggest,
it's backed by Redis, and it's a

479
00:27:37,759 --> 00:27:43,419
very simple, library for Qing and
processing background jobs with workers.

480
00:27:43,959 --> 00:27:45,039
very easy setup.

481
00:27:45,039 --> 00:27:47,169
Hardly takes 15 minutes to set it up.

482
00:27:47,169 --> 00:27:50,869
If you already have a Redis, you
don't even need to, set, set up

483
00:27:50,869 --> 00:27:56,239
a red, for RQ and, very simple.

484
00:27:56,329 --> 00:27:57,019
Uh.

485
00:27:57,699 --> 00:28:00,339
mechanism for queuing and processing.

486
00:28:00,429 --> 00:28:04,029
All you need to do is create a queue,
provide it a red connection so that,

487
00:28:04,079 --> 00:28:05,999
it has a place to store the tasks.

488
00:28:06,519 --> 00:28:10,249
when you get a task, queue and queue,
you can, and, this is basically a

489
00:28:10,249 --> 00:28:14,509
function which is going to get called
in the worker to process your tasks.

490
00:28:14,559 --> 00:28:19,779
it's this simple and you can also provide
some arguments for that function and.

491
00:28:20,279 --> 00:28:24,119
The worker for the worker,
you just need to start it like

492
00:28:24,119 --> 00:28:26,149
this, on your command line.

493
00:28:26,329 --> 00:28:29,899
And, it consumes tasks from
Redis and, process them.

494
00:28:29,899 --> 00:28:34,229
If you, want to increase the number
of workers, you just start 10

495
00:28:34,229 --> 00:28:38,869
different workers, connect them to
the same Redis, and, RQ will itself

496
00:28:38,869 --> 00:28:41,759
handle all the, all the complexities.

497
00:28:42,134 --> 00:28:46,609
Of, managing which worker gets what
task, and all those kind of things.

498
00:28:46,669 --> 00:28:49,199
if you're on Python, RQ is the way to go.

499
00:28:50,039 --> 00:28:54,209
Salary provides you with,
similar, functionality.

500
00:28:54,399 --> 00:28:56,979
but we just found that, there
were a bunch of things in salary,

501
00:28:56,979 --> 00:28:58,839
which we did not really need.

502
00:28:59,279 --> 00:29:00,719
and it seemed like an overkill.

503
00:29:01,074 --> 00:29:05,324
so we decided to go with RQH was much
simpler to set up on our end from

504
00:29:05,324 --> 00:29:07,934
thread, what inputs are not working,
what models are working, what models

505
00:29:07,934 --> 00:29:09,494
are not working, and things like that.

506
00:29:09,994 --> 00:29:14,554
if you want an analogy, you can
think of evals as unit testing.

507
00:29:14,754 --> 00:29:16,614
think of it as unit
testing for your prompts.

508
00:29:16,704 --> 00:29:19,404
So this allows you to
take a prompt template.

509
00:29:20,169 --> 00:29:25,539
And individually just test out that
template with a bunch of different values.

510
00:29:26,079 --> 00:29:32,129
and, you can, there are a bunch of,
reasons why you should ideally, use

511
00:29:32,129 --> 00:29:33,689
evals with your prompt templates.

512
00:29:34,219 --> 00:29:37,459
one, it allows you to just test
out the prompts in I isolation,

513
00:29:37,459 --> 00:29:38,419
which makes it very fast.

514
00:29:38,469 --> 00:29:41,719
the same way unit tests are fast, because
you are just checking one function

515
00:29:41,719 --> 00:29:43,009
against different types of inputs.

516
00:29:43,579 --> 00:29:45,778
Using prompts, using, sorry, I'm sorry.

517
00:29:45,828 --> 00:29:51,178
using evals, you will be able to
figure out different things like which

518
00:29:51,178 --> 00:29:55,018
input works, which input doesn't work,
which model works for a particular

519
00:29:55,018 --> 00:29:56,848
task, which model does not work.

520
00:29:57,168 --> 00:30:00,758
you'll be able to compare, costs
of different models for different

521
00:30:00,758 --> 00:30:02,498
types of inputs and so on.

522
00:30:03,018 --> 00:30:06,948
an additional, benefit
of using evals is that.

523
00:30:07,368 --> 00:30:12,478
You can directly, integrate them
with your CICD pipeline so that,

524
00:30:12,478 --> 00:30:15,628
you don't need to manually keep
checking before every release if your

525
00:30:15,628 --> 00:30:19,178
prompts are still working the way,
they're working just like unit test.

526
00:30:19,178 --> 00:30:23,843
You just, hook it up to your CICD pipeline
and, before every commit or I'm sorry,

527
00:30:23,903 --> 00:30:28,128
after every commit or, after every
build, you, straight up run the evals.

528
00:30:28,778 --> 00:30:34,008
And, similar to, assertions in unit tests,
evals also have assertions or checks

529
00:30:34,458 --> 00:30:40,128
where you can check the response, and,
specify whether it is as expected or not

530
00:30:40,128 --> 00:30:42,468
as expected, and pass or fail an eval.

531
00:30:42,858 --> 00:30:45,948
That's how on a very
high level evals work.

532
00:30:46,478 --> 00:30:49,958
we have tried out a bunch of
different, eval libraries.

533
00:30:50,208 --> 00:30:52,488
the one we like the most is Profu.

534
00:30:52,948 --> 00:30:54,418
very easy to set up.

535
00:30:54,668 --> 00:30:58,358
simply works using YAML files.

536
00:30:58,968 --> 00:31:03,688
basically you, you create a YAML file
where you specify your prompt template

537
00:31:03,718 --> 00:31:07,458
and you press, specify a bunch of
inputs, for that prompt template.

538
00:31:07,863 --> 00:31:11,093
And, using, profu is an open source, tool.

539
00:31:11,093 --> 00:31:15,993
So you can just like, straight up install
it from NPM, brand it in your CLI.

540
00:31:16,493 --> 00:31:20,923
and at the end of the Evalue
you get a nice, graph like this.

541
00:31:21,748 --> 00:31:26,198
Which will show you for different
types of inputs, whether the

542
00:31:26,198 --> 00:31:28,118
output has passed the condition.

543
00:31:28,488 --> 00:31:33,538
it'll also allow you to compare
different, models and, there is

544
00:31:33,538 --> 00:31:35,428
some way to compare cost as well.

545
00:31:35,428 --> 00:31:37,228
I don't think they have
displayed it here, but yeah.

546
00:31:37,418 --> 00:31:40,148
cost comparison is also something
that you'll get in the same dashboard

547
00:31:40,808 --> 00:31:45,258
and, You can start off with the open
source version of Profu, but they

548
00:31:45,258 --> 00:31:47,028
also have a cloud hosted version.

549
00:31:47,028 --> 00:31:50,808
So if you want more reliability or
don't want to manage your own instance,

550
00:31:51,058 --> 00:31:52,528
that option is also available.

551
00:31:53,028 --> 00:31:56,898
before we end the talk, let's do a
quick walkthrough of all the different

552
00:31:56,898 --> 00:32:01,588
foundational models, or foundational model
APIs that are available for public use.

553
00:32:02,018 --> 00:32:06,458
The reason for doing this is basically,
this landscape is changing very fast.

554
00:32:06,878 --> 00:32:12,198
So the last time you had gone over all
the available models, I'm pretty sure

555
00:32:12,198 --> 00:32:17,628
that, by now the list of models and
also their, comparisons have changed.

556
00:32:17,698 --> 00:32:21,048
probably the models you
thought, are not that great have

557
00:32:21,048 --> 00:32:22,648
become very good, and so on.

558
00:32:22,828 --> 00:32:26,968
So let's do a quick run through of all the
available models, what are they good at?

559
00:32:26,968 --> 00:32:27,298
What are.

560
00:32:27,763 --> 00:32:29,593
They're not good at
what kind of use cases?

561
00:32:29,623 --> 00:32:35,483
Um, you what case, what kind of use cases
work with a particular kind of model?

562
00:32:35,983 --> 00:32:37,993
let's start with the oldest player OpenAI.

563
00:32:38,413 --> 00:32:44,913
OpenAI has, three main families of
models, which is GPT 4 0 4 O, and o,

564
00:32:45,453 --> 00:32:46,953
which are available for public use.

565
00:32:47,263 --> 00:32:49,693
I think they've deprecated
their three and 3.5 models.

566
00:32:50,043 --> 00:32:52,833
so these are the models that
are available right now.

567
00:32:53,343 --> 00:32:56,673
If you don't know what to
use, just go with open air.

568
00:32:56,743 --> 00:32:58,873
these are the most versatile.

569
00:32:59,533 --> 00:33:03,693
Models, work with wide, they work
very well with wide, wide variety

570
00:33:03,693 --> 00:33:06,423
of tasks, within these models.

571
00:33:06,813 --> 00:33:11,203
between four oh and, four oh mini, the
difference is mainly, the trade off

572
00:33:11,203 --> 00:33:14,553
between, Cost and latency versus accuracy.

573
00:33:14,553 --> 00:33:18,723
So if you have a complex task
or something that requires a bit

574
00:33:18,813 --> 00:33:20,933
more of reasoning, go for four.

575
00:33:21,513 --> 00:33:25,383
if you are worried about cost or if you're
worried about, how fast the response is

576
00:33:25,383 --> 00:33:30,408
going to be, go for four oh mini, but,
it'll basically, give you lesser accuracy.

577
00:33:30,948 --> 00:33:32,688
O is something that I've not tried out.

578
00:33:32,718 --> 00:33:36,028
these are supposed to be,
open as flagship models.

579
00:33:36,478 --> 00:33:37,018
but from.

580
00:33:37,673 --> 00:33:39,593
What I've heard, these
are like fairly new.

581
00:33:39,603 --> 00:33:42,733
before you put it in production,
maybe, test them out thoroughly.

582
00:33:42,983 --> 00:33:47,023
four O and four O Mini have been around
for a while now, so I think, you should

583
00:33:47,023 --> 00:33:48,833
not see a lot of problems, with them.

584
00:33:49,013 --> 00:33:54,193
Also, like reliability wise, as,
according to us, open air APIs

585
00:33:54,193 --> 00:33:55,243
have been the most reliable.

586
00:33:55,728 --> 00:34:02,348
so you don't need to worry about, downtime
or, having to, handle switching models

587
00:34:02,348 --> 00:34:04,178
because, this provider is not working.

588
00:34:04,678 --> 00:34:08,498
The next provider is, philanthropic.

589
00:34:08,958 --> 00:34:13,788
I think for a while, these guys
were working mostly on the, chat.

590
00:34:13,838 --> 00:34:17,768
the APIs were not publicly
available as far as I know.

591
00:34:18,168 --> 00:34:20,618
but I think in the last few
months, I think that has changed.

592
00:34:20,618 --> 00:34:21,758
the APIs are available.

593
00:34:21,758 --> 00:34:24,228
You can just directly, and
they're completely self serve.

594
00:34:24,228 --> 00:34:29,418
You can just directly go, on, anthropics,
anthropics console and, create an API key.

595
00:34:29,418 --> 00:34:32,148
Load up some credit and
get started with it.

596
00:34:32,648 --> 00:34:38,088
If you have any coding related use
case, Claude APIs are your best choice.

597
00:34:38,118 --> 00:34:43,988
I think, as far as coding is concerned,
coding as a particular task, Claude, works

598
00:34:43,988 --> 00:34:46,178
much better than, all the other models.

599
00:34:46,688 --> 00:34:48,368
which is also why you would've seen that.

600
00:34:48,408 --> 00:34:52,558
everyone is using Claude with, They're,
code editors as well, like cursor.

601
00:34:53,138 --> 00:34:56,158
so yeah, if code is what
you want, work with clo.

602
00:34:56,658 --> 00:35:00,888
Next up is gr not to
be confused with x gr.

603
00:35:01,398 --> 00:35:08,678
So Grok is, essentially, a company that
is building, Special purpose chips.

604
00:35:08,708 --> 00:35:12,628
They call them pus, for
running LLMs, which, makes,

605
00:35:12,658 --> 00:35:14,488
their inference time very low.

606
00:35:14,968 --> 00:35:20,628
probably, even the inference cost, So
if latency is what you're trying to

607
00:35:20,628 --> 00:35:26,783
optimize, tryout, grok, grok, cloud, which
is their API, which are their, LLM APIs.

608
00:35:26,848 --> 00:35:30,933
they generally host, most of the
commonly used open source models.

609
00:35:30,983 --> 00:35:33,593
so you have llama, extra Gemma available.

610
00:35:33,643 --> 00:35:39,043
apart from that, a bunch of other things,
Latency wise, they are much faster

611
00:35:39,133 --> 00:35:40,863
than, all the other model providers.

612
00:35:40,863 --> 00:35:45,413
So if you are optimizing for
latency and, these models work for

613
00:35:45,413 --> 00:35:46,993
your particular task, go for it.

614
00:35:47,493 --> 00:35:52,473
Alright, so AWS, mainly works like rock.

615
00:35:52,473 --> 00:35:54,513
They host a lot of open source models.

616
00:35:55,203 --> 00:35:55,653
On.

617
00:35:55,743 --> 00:35:58,783
And along with that, I think
they also have, their own models,

618
00:35:58,813 --> 00:36:00,193
which, we have not tried out yet.

619
00:36:00,633 --> 00:36:07,013
but the biggest USP of using AWS
bedrock would be if you're already

620
00:36:07,013 --> 00:36:12,853
in the AWS, ecosystem and you are,
worried about, your sensitive data,

621
00:36:12,973 --> 00:36:16,503
Getting out of your infra and you
don't want to like, send it to open AI

622
00:36:16,618 --> 00:36:18,393
or cloud or any other model provider.

623
00:36:18,823 --> 00:36:21,193
in that case, bedrock
should be your choice.

624
00:36:21,503 --> 00:36:24,743
one good thing is Bedrock
also hosts cloud APIs.

625
00:36:24,993 --> 00:36:26,703
the limits are lower.

626
00:36:26,953 --> 00:36:27,733
as far as I know.

627
00:36:27,733 --> 00:36:30,903
I think you'll need to talk to the support
and, get your service quota increased.

628
00:36:30,903 --> 00:36:35,388
But, If you are worried about, sensitive
data and you're okay with cloud,

629
00:36:35,458 --> 00:36:36,928
bedrock should work for you very well.

630
00:36:37,528 --> 00:36:40,918
and along with that, they also
host LA and Mixture and a few

631
00:36:40,918 --> 00:36:42,818
other, APIs, multimodal APIs.

632
00:36:43,318 --> 00:36:48,058
Azure is, the last time I checked
Azure is hosting GPT models.

633
00:36:48,733 --> 00:36:52,183
separately, the hosting, which
OpenAI does is separate from Azure.

634
00:36:52,483 --> 00:36:57,983
And, the last time we checked, Azure GPT
APIs were a bit more faster than open air.

635
00:36:58,373 --> 00:37:04,513
So again, oh, if you want to use open
AI APIs and you, want, a slightly

636
00:37:04,513 --> 00:37:08,143
better latency, tryout as Azure, but,
they'll make you fill a bunch of forms.

637
00:37:08,513 --> 00:37:11,003
I think these APIs or, these
models are not publicly

638
00:37:11,003 --> 00:37:12,653
available on Azure for everyone.

639
00:37:13,208 --> 00:37:15,718
Of now, GCP, I've not tried out.

640
00:37:15,798 --> 00:37:18,708
again, I think the setup was a bit
complex, we didn't get a chance to give

641
00:37:18,768 --> 00:37:23,628
it a try, but from what we've heard, the
developer experience is much better now.

642
00:37:23,628 --> 00:37:26,118
So someday we'll give it a try again.

643
00:37:26,118 --> 00:37:29,908
But GCP has, Gemini and the.

644
00:37:30,408 --> 00:37:35,318
Latest, the newest scale on the
block is Deeps sec. if you are active

645
00:37:35,318 --> 00:37:39,258
on Twitter, you would've already
heard, about deeps, SEC's, APIs.

646
00:37:39,828 --> 00:37:45,288
from the chatter, it seems as if
they are at par with own APIs.

647
00:37:45,568 --> 00:37:47,798
again, having tried it out, give it a try.

648
00:37:47,988 --> 00:37:52,188
one concern could be, the hosting,
which is in China, but, um.

649
00:37:52,858 --> 00:37:54,148
definitely give it a try.

650
00:37:54,228 --> 00:37:58,198
probably you might find it, to
be a good fit for your use case.

651
00:37:58,198 --> 00:38:02,208
And, one more thing, deep seeks
models are also open source so

652
00:38:02,238 --> 00:38:03,138
you can host them on your own.

653
00:38:03,638 --> 00:38:04,688
And that's all from me.

654
00:38:04,808 --> 00:38:10,998
I hope you find the information shared
in the stock useful, and it speeds up

655
00:38:10,998 --> 00:38:15,558
your development process when you are
building LM applications and, AI agents.

656
00:38:16,068 --> 00:38:19,288
if you have any, queries or
if you want to, talk more

657
00:38:19,288 --> 00:38:20,608
about this, drop us an email.

658
00:38:20,608 --> 00:38:21,568
You can find our email.

659
00:38:22,288 --> 00:38:27,568
On Ku AI's landing page, or just,
send me a message on LinkedIn.

660
00:38:27,598 --> 00:38:30,528
happy to chat about this
and, go with something.

661
00:38:30,528 --> 00:38:30,798
Awesome.

662
00:38:30,798 --> 00:38:30,858
Bye.

