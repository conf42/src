1
00:00:00,500 --> 00:00:01,250
Hello everyone.

2
00:00:01,640 --> 00:00:06,380
I thank you so much for being here
today at Ative Conference 2025.

3
00:00:07,160 --> 00:00:12,110
My name is Venus Kerr and I'm
deeply honored to speak about the

4
00:00:12,110 --> 00:00:16,730
subject, which is very close to my
heart, how to build ethical AI in

5
00:00:16,730 --> 00:00:22,490
healthcare, and how can we do that
within Kubernetes native architecture.

6
00:00:23,060 --> 00:00:26,840
In recent years, I have been
fortunate to work at the

7
00:00:26,840 --> 00:00:27,925
intersection of machine learning.

8
00:00:28,820 --> 00:00:31,190
Cloud infrastructure and governance.

9
00:00:31,550 --> 00:00:37,190
Through this journey, I have seen how easy
it is to scale models, deploy them, and

10
00:00:37,190 --> 00:00:43,760
push them to production, and yet we bake
in bias, opacity, and compliance risk.

11
00:00:43,910 --> 00:00:47,090
In healthcare, this risk
becomes much more serious.

12
00:00:47,660 --> 00:00:53,210
Decision making made by AI can directly
affect diagnosis, trust, and lives.

13
00:00:53,540 --> 00:00:57,530
So today I want to share
strategies, patterns, and.

14
00:00:58,445 --> 00:01:03,815
Cautionary lessons for making AI
in healthcare not just powerful,

15
00:01:04,085 --> 00:01:09,425
but fair, transparent, and
trustworthy, all while leveraging

16
00:01:10,384 --> 00:01:11,854
Kubernetes as an infrastructure.

17
00:01:12,214 --> 00:01:13,895
Let's begin.

18
00:01:14,395 --> 00:01:16,134
Let's start with the backdrop.

19
00:01:16,794 --> 00:01:20,219
Healthcare is being transformed by AI

20
00:01:20,719 --> 00:01:20,939
for.

21
00:01:21,439 --> 00:01:28,639
Imaging diagnostic diagnosis or for risk
scoring to personalize treatment planning.

22
00:01:29,139 --> 00:01:34,499
Hospitals and medical institutions are
increasingly adopting AI power tools

23
00:01:34,499 --> 00:01:40,019
that analyze radiology images for ca
patient, iteration, or optimized workflow.

24
00:01:40,739 --> 00:01:44,939
Underneath many of these tools
lies Kubernetes native stack.

25
00:01:45,149 --> 00:01:45,629
Why?

26
00:01:45,689 --> 00:01:46,594
Because Kubernetes off.

27
00:01:47,339 --> 00:01:52,049
Offer scalability, orchestration,
resource isolation, things that

28
00:01:52,049 --> 00:01:56,669
traditional monolithic systems
struggles with you can distribute.

29
00:01:56,969 --> 00:02:04,679
Training workload, autoscale, serving,
manage dependencies across microservices,

30
00:02:05,399 --> 00:02:12,269
all elegantly in one cluster, but the same
attribute that make Kubernetes powerful.

31
00:02:12,734 --> 00:02:19,094
Its distribution, dynamic nature
also introduced complexity, treatment

32
00:02:19,094 --> 00:02:26,654
pipelines, data ingestions model
updates, inference services, all spread

33
00:02:26,654 --> 00:02:29,674
across pods and not observability.

34
00:02:29,705 --> 00:02:32,875
Auditability, governance
becomes harder, not easier.

35
00:02:33,355 --> 00:02:36,625
Which brings us to our
central question as to.

36
00:02:37,125 --> 00:02:40,305
Adopt Kubernetes stack for ai.

37
00:02:40,425 --> 00:02:47,745
Are we preparing them to uphold
ethics, fairness, privacy, and trust?

38
00:02:48,075 --> 00:02:48,705
Let's explore.

39
00:02:49,205 --> 00:02:52,180
We have to acknowledge the risk upfront.

40
00:02:52,680 --> 00:02:57,120
AI systems in healthcare
have been troubling biases.

41
00:02:57,120 --> 00:03:03,759
For instance, models trained on
dermatology images have misclassified,

42
00:03:03,819 --> 00:03:09,819
dark skin tones more often leading
to misdiagnosis or predictive model

43
00:03:09,819 --> 00:03:14,439
that undervalue risk in underserved
or minority patient population.

44
00:03:15,039 --> 00:03:17,559
These are not hypothetical.

45
00:03:17,829 --> 00:03:20,679
These are documented real world failures.

46
00:03:21,179 --> 00:03:26,509
Part of the problem is that many AI
operated as operates as black box.

47
00:03:26,719 --> 00:03:31,609
Like these systems, the
models, they are black box.

48
00:03:31,969 --> 00:03:37,689
You feed input in, you get output out,
but you cannot easily inspect why the

49
00:03:37,689 --> 00:03:39,519
model arrived at a particular decision.

50
00:03:40,059 --> 00:03:44,379
That ity is deeply problematic when
you are dealing with the human lives

51
00:03:44,379 --> 00:03:47,679
and decisions, clinicians, patients.

52
00:03:47,994 --> 00:03:49,584
Regulators demand explainability.

53
00:03:50,544 --> 00:03:52,434
Why did you predict that?

54
00:03:52,524 --> 00:03:53,814
And on what basis?

55
00:03:54,564 --> 00:03:59,734
So we have two core biases
on two core issues, actually.

56
00:03:59,734 --> 00:04:06,394
One is bias, unfair outcome across
groups, and opaqueness, lack

57
00:04:06,394 --> 00:04:08,614
of transparency in healthcare.

58
00:04:08,614 --> 00:04:09,995
We can't ignore those.

59
00:04:10,054 --> 00:04:14,104
The question is, can Kuber
help us address them?

60
00:04:14,104 --> 00:04:14,854
The answer is yes.

61
00:04:15,354 --> 00:04:19,224
If we treat ethical behavior
as a part of the infrastructure

62
00:04:19,224 --> 00:04:21,444
itself, we can resolve it.

63
00:04:21,944 --> 00:04:22,394
Let's see,

64
00:04:22,894 --> 00:04:28,959
Kubernetes at first glance is just
container orchestration system, but I

65
00:04:28,959 --> 00:04:33,014
see it as an ethical infrastructure,
a canvas onto which we can embed

66
00:04:33,014 --> 00:04:35,534
governance audits and controls.

67
00:04:36,359 --> 00:04:39,459
Consider the features
Kubernetes gives us controlled.

68
00:04:39,459 --> 00:04:45,039
Rolled out can develop deployments,
admission controllers, automatic

69
00:04:45,039 --> 00:04:49,509
rollbacks, namespace, isolation,
and rich observability while

70
00:04:49,509 --> 00:04:51,549
logging, tracing, and metrics.

71
00:04:51,849 --> 00:04:55,344
Each of these features becomes
an opportunity to insert checks,

72
00:04:56,044 --> 00:05:00,224
not just for performance, but for
fairness, privacy, and transparency.

73
00:05:00,724 --> 00:05:01,924
So what does that mean?

74
00:05:02,164 --> 00:05:09,114
For example, before a model is promoted
to serving an aian web hook could

75
00:05:09,114 --> 00:05:12,834
enforce that a fairness audit has passed.

76
00:05:13,554 --> 00:05:16,824
A rollback could be triggered
if drift is detected.

77
00:05:17,634 --> 00:05:24,504
Logs and audit drills become immutable
record of what was deployed when and what.

78
00:05:25,004 --> 00:05:25,905
Credentials.

79
00:05:26,405 --> 00:05:31,744
Thus Kubernetes gives us guardrails,
checkpoints, and hook hooks by design.

80
00:05:32,194 --> 00:05:34,775
But hooks alone does not guarantee ethics.

81
00:05:35,275 --> 00:05:36,835
You still need to build the logic.

82
00:05:37,135 --> 00:05:40,974
So let's break down
this into three pillars.

83
00:05:41,065 --> 00:05:48,114
I believe they are essential
fairness, explainability, and privacy.

84
00:05:48,655 --> 00:05:51,114
After that, we'll see how they can be.

85
00:05:51,729 --> 00:05:53,469
Composed and scale.

86
00:05:53,969 --> 00:05:55,620
Let's first look into fairness.

87
00:05:56,120 --> 00:06:01,460
When we talk about the fairness
models, we mean models that don't

88
00:06:01,640 --> 00:06:06,289
systematically disadvantage particular
demographic group, but fairness

89
00:06:06,950 --> 00:06:09,500
is not one size With all concept.

90
00:06:10,310 --> 00:06:14,479
There are multiple mathematical
definitions, demographic

91
00:06:14,479 --> 00:06:16,719
parity, equalized odds.

92
00:06:17,604 --> 00:06:20,424
Predictive parity and
trade offs between them.

93
00:06:20,754 --> 00:06:26,484
In practice, what I do is bake fairness
audits into my deployment pipeline.

94
00:06:27,174 --> 00:06:33,594
After each training, job completes a
fairness audit task runs, for example,

95
00:06:33,594 --> 00:06:38,155
using Fair Learn or I-B-M-A-I-F 360.

96
00:06:38,655 --> 00:06:41,599
The task calculates
metrics across subgroups.

97
00:06:42,099 --> 00:06:46,929
Whether they are false negative rates
significantly higher for one group.

98
00:06:47,499 --> 00:06:50,559
Does the model error
vary across population?

99
00:06:51,400 --> 00:06:55,809
If any metrics boilers a
threshold, the pipeline halts.

100
00:06:56,229 --> 00:07:00,789
The model is flagged for review
or roll back in Kubernetes.

101
00:07:01,719 --> 00:07:09,609
This audit job can run as a job or part
of an Argo workflow because it's virgin.

102
00:07:10,109 --> 00:07:12,570
Automated and repeatable.

103
00:07:12,630 --> 00:07:15,090
We avoid human error or oversight.

104
00:07:15,750 --> 00:07:16,650
Let me share a story.

105
00:07:17,150 --> 00:07:22,580
One in one of our deployment, our
fairness audit got a subtle shift.

106
00:07:22,760 --> 00:07:27,099
Over time, one subgroup error
rate had drifted upward.

107
00:07:27,340 --> 00:07:30,490
The system automatically
rolled back to the last one.

108
00:07:31,060 --> 00:07:36,940
Good model, known good model, giving
us time to retrain and rebalance.

109
00:07:36,940 --> 00:07:42,190
Without automation, that drift might have
gone unnoticed until the harm was done.

110
00:07:42,760 --> 00:07:46,450
The fairness checks don't need
to happen only at training.

111
00:07:46,719 --> 00:07:52,724
They can happen continuously, especially
if our system retrains or adapts.

112
00:07:52,749 --> 00:07:56,859
Model introduction, monitoring,
fairness, drift is just as

113
00:07:56,859 --> 00:07:58,599
crucial as performance metrics.

114
00:07:59,469 --> 00:08:04,359
Alright, we have built fairness into
training, but what about explainability?

115
00:08:04,570 --> 00:08:06,874
Let's talk about explainability Next.

116
00:08:07,374 --> 00:08:14,394
Explainability asks, why did the model
decide what it did in healthcare?

117
00:08:14,754 --> 00:08:19,734
Clinicians, patients deserve the
transparency, the two levels of

118
00:08:19,734 --> 00:08:21,654
explanation, local explanation.

119
00:08:22,525 --> 00:08:25,974
For single prediction, for
example, why did the model

120
00:08:25,974 --> 00:08:27,984
assign this patient high risk?

121
00:08:28,385 --> 00:08:34,115
Techniques like line or sharp can
compute per feature contribution.

122
00:08:34,924 --> 00:08:40,445
Global explanations like understanding
the model's behavior in aggregate

123
00:08:40,715 --> 00:08:44,990
feature importance, sensitivity
analysis, pattern discoveries.

124
00:08:45,490 --> 00:08:49,750
In Kubernetes native architecture,
a robot robust pattern is to deploy

125
00:08:49,750 --> 00:08:55,900
an explainability, sidecar, or
microservice alongside the model server.

126
00:08:56,860 --> 00:09:00,960
When an inference arrives,
the main service retains the

127
00:09:00,960 --> 00:09:02,370
predict, returns, the prediction.

128
00:09:02,880 --> 00:09:05,520
The sidecar computes
and returns explanation.

129
00:09:05,580 --> 00:09:09,360
Because these services
scale independently, you can

130
00:09:09,360 --> 00:09:11,250
isolate resources as needed.

131
00:09:12,240 --> 00:09:14,910
You may worry about performance impact.

132
00:09:14,970 --> 00:09:22,290
Yes, explainability does incur latency or
compute cost, but you can mitigate that.

133
00:09:22,290 --> 00:09:27,860
For example, only compute explanations
for flagged requests like edge

134
00:09:27,860 --> 00:09:30,830
cases, high uncertainty productions.

135
00:09:31,340 --> 00:09:34,820
You can c explanations
for repeated inputs.

136
00:09:35,320 --> 00:09:37,895
You can use the asynchronous explanation.

137
00:09:37,895 --> 00:09:41,755
Return predictions first
and then explanation later.

138
00:09:42,475 --> 00:09:43,555
One caveat.

139
00:09:44,125 --> 00:09:48,835
Sometimes explanations stem cells
can reveal sensitive input data.

140
00:09:49,255 --> 00:09:50,485
Input data.

141
00:09:50,875 --> 00:09:53,485
So imagine access and guard.

142
00:09:53,985 --> 00:09:56,415
You have to manage the access
and you have to guard them.

143
00:09:56,955 --> 00:10:00,940
Treat explanations, output with
the same care as prediction output.

144
00:10:01,440 --> 00:10:06,390
So you have to keep that in mind by
combining fairness and explainability.

145
00:10:06,450 --> 00:10:10,650
We built the systems that
are not just accurate, but

146
00:10:10,920 --> 00:10:14,330
comprehensible and justifiable law.

147
00:10:14,330 --> 00:10:15,680
Next, look into the third pillar.

148
00:10:16,100 --> 00:10:16,880
Privacy.

149
00:10:17,380 --> 00:10:20,530
In healthcare, the
patient's data is accurate.

150
00:10:21,370 --> 00:10:25,090
We must design systems that
minimize risk of exposure by design.

151
00:10:25,590 --> 00:10:28,500
One powerful approach
is federated learning.

152
00:10:28,590 --> 00:10:33,530
Instead of centralizing all patient data,
we keep data in the local institutes.

153
00:10:33,530 --> 00:10:35,630
For example, hospitals or clinics.

154
00:10:36,230 --> 00:10:42,590
Model trained locally only models,
updates, or gradients are shared.

155
00:10:43,090 --> 00:10:48,670
The central orchestrator aggregates,
updates and produce a global model.

156
00:10:49,170 --> 00:10:51,060
Rod never leaves the premises.

157
00:10:51,750 --> 00:10:56,580
To strengthen privacy further, we can
incorporate differential privacy before

158
00:10:56,580 --> 00:11:01,080
aggregating, update, add calibrated noise
so that the individual contributions

159
00:11:01,080 --> 00:11:02,520
cannot be reversed engineered.

160
00:11:03,420 --> 00:11:07,650
This gives us mathematical guarantee
that no individual patient needs data

161
00:11:07,650 --> 00:11:11,340
can be deduced even from the aggregate.

162
00:11:11,840 --> 00:11:16,790
Another technique, secure
enclaves, multi-party competition.

163
00:11:17,290 --> 00:11:21,250
Homomorphic encryption are
promising though trade off is

164
00:11:21,310 --> 00:11:24,190
performance or complexity remains.

165
00:11:24,610 --> 00:11:28,120
But combining federated learning
and differential privacy

166
00:11:28,450 --> 00:11:30,010
gives a compelling baseline.

167
00:11:30,510 --> 00:11:32,760
In Kubernetes native setting.

168
00:11:33,000 --> 00:11:37,995
We can spin up local training
pods per hospital cluster.

169
00:11:38,495 --> 00:11:44,125
These pods sent encrypted noise added
updates to the central aggregator pod.

170
00:11:44,625 --> 00:11:49,305
The aggregator logs all updates
and ensures DP algorithm.

171
00:11:50,115 --> 00:11:57,405
Crucially, every step is virgin
audited and is transparent because

172
00:11:57,405 --> 00:11:59,415
data never leaves its origin.

173
00:12:00,345 --> 00:12:04,125
Jurisdictional regulatory
and the governance compliant

174
00:12:04,125 --> 00:12:06,285
challenges are minimized.

175
00:12:06,675 --> 00:12:10,185
And since the updates are
aggregate aggregated, we still

176
00:12:10,185 --> 00:12:13,065
learn from a federation of data.

177
00:12:13,565 --> 00:12:15,665
Let's look into scalability.

178
00:12:16,205 --> 00:12:17,675
How can we scale it?

179
00:12:18,175 --> 00:12:25,075
Because it's one thing to build a fair,
private explainable model in development,

180
00:12:25,135 --> 00:12:29,845
but another to run it reliably in
production at scale, across clusters.

181
00:12:30,345 --> 00:12:33,885
First, don't treat ethics model.

182
00:12:34,845 --> 00:12:37,215
Don't treat ethics module as monolithics.

183
00:12:37,665 --> 00:12:38,564
Separate them.

184
00:12:39,064 --> 00:12:45,064
Fairness, auditor Explanation
Services, privacy aggregator.

185
00:12:45,244 --> 00:12:46,444
Separate them each.

186
00:12:46,444 --> 00:12:50,074
Can auto scale or scale
independently in Kubernetes.

187
00:12:50,574 --> 00:12:57,134
Second, cash and batch if many
requests are repeated, similar,

188
00:12:57,134 --> 00:13:01,484
have similar inputs, cash
explanations, or reuse, fairness.

189
00:13:01,484 --> 00:13:02,414
Audit results.

190
00:13:02,714 --> 00:13:05,444
Don't recompute from scratch each time.

191
00:13:05,944 --> 00:13:08,374
Leverage a synchronous processing.

192
00:13:08,874 --> 00:13:13,434
Some checks can run in the background
while serving predictions quickly.

193
00:13:13,434 --> 00:13:18,079
If an edge case is flagged later,
you may retrack, rescore or notify.

194
00:13:18,949 --> 00:13:24,549
Utilize monitor drift continuously,
not just on performance metrics,

195
00:13:24,609 --> 00:13:29,619
but fairness metrics, privacy
violation used sidecar collectors.

196
00:13:30,354 --> 00:13:32,484
Metrics, pipelines and dashboard.

197
00:13:33,414 --> 00:13:37,674
If drift is the fairness or the
privacy occurs, trigger alerts

198
00:13:37,734 --> 00:13:43,064
or rollback take the advantage of
multi-tenant cluster with isolation.

199
00:13:43,564 --> 00:13:49,804
You must host models for multiple
department or institutions on

200
00:13:49,804 --> 00:13:55,294
the shared cluster, but with
strong namespace isolation.

201
00:13:56,224 --> 00:13:59,254
Policy enforcement and audit segmentation.

202
00:13:59,754 --> 00:14:02,214
Also, think about the
error handling strategies.

203
00:14:02,714 --> 00:14:08,114
If your fairness engine fails,
the fallback to safe mode.

204
00:14:08,474 --> 00:14:13,364
If your explanation service lags,
then degrade, suck gracefully.

205
00:14:13,694 --> 00:14:16,604
The system must be SST and safe by design.

206
00:14:17,504 --> 00:14:21,524
When done right, ethical behavior scales.

207
00:14:22,139 --> 00:14:25,109
The system rather than being a drag on it.

208
00:14:25,609 --> 00:14:30,739
Let's look into regulatory
and ethical alignments.

209
00:14:31,239 --> 00:14:35,259
Ethics does not live in a
vacuum, especially in healthcare.

210
00:14:35,829 --> 00:14:42,489
There are laws, regulations, and
standards to respect HIPAA in US, GDPR in

211
00:14:42,489 --> 00:14:48,759
Europe, FDA rules in medical devices and
emerging AI governance framework like.

212
00:14:49,629 --> 00:14:53,049
EU AI Act, Kubernetes help here too.

213
00:14:53,079 --> 00:14:59,129
Because your infrastructure is immutable,
version controlled, auditable, you

214
00:14:59,129 --> 00:15:06,029
can trace back every deployed model,
every fairness audit, every explanation

215
00:15:06,149 --> 00:15:08,134
request, and every rule back.

216
00:15:08,634 --> 00:15:11,424
That creates an audit trail regulators.

217
00:15:12,009 --> 00:15:14,799
We'll, appreciate that
admission controllers.

218
00:15:14,849 --> 00:15:20,159
OPA gatekeeper can enforce
policy before deployment.

219
00:15:20,549 --> 00:15:26,789
Check that the fairness audit passed,
that privacy guarantees are met, or

220
00:15:26,789 --> 00:15:29,969
that explanation mechanisms are active.

221
00:15:30,569 --> 00:15:34,949
If not, deployment is
refused by integrating ethics

222
00:15:34,949 --> 00:15:36,234
into deployment pipeline.

223
00:15:36,734 --> 00:15:41,384
You reduce friction because
compliance become part of engineering.

224
00:15:41,384 --> 00:15:42,944
Not a separate afterthought.

225
00:15:43,444 --> 00:15:46,684
When clinicians ask,
can I trust the model?

226
00:15:47,224 --> 00:15:50,584
Can you answer yes, here you can.

227
00:15:51,484 --> 00:15:52,624
Here is the audit trail.

228
00:15:52,834 --> 00:15:57,394
Here is a explainability log, and
here is a fair fairness metrics.

229
00:15:57,754 --> 00:15:58,954
That gives the power.

230
00:15:59,134 --> 00:15:59,584
Power.

231
00:15:59,884 --> 00:16:02,584
It's more powerful now because
it has, you have insights.

232
00:16:03,084 --> 00:16:06,384
Let's look into the practical
implementation strategies.

233
00:16:06,884 --> 00:16:11,715
Let me walk you through the phase roadmap
you can use in your own organization.

234
00:16:12,374 --> 00:16:13,545
Let's phase it out.

235
00:16:14,114 --> 00:16:15,674
First phase assessment.

236
00:16:16,094 --> 00:16:19,604
This phase is about
understanding your current state.

237
00:16:19,634 --> 00:16:27,015
Before adding any ethical AI layer,
start by auditing existing AI ML models.

238
00:16:27,914 --> 00:16:33,765
Check with demographic bias, missing
explainability or weak privacy control.

239
00:16:34,394 --> 00:16:44,715
Use open source tool like fair Learn, a IF
360 or Shap to generate baseline metrics.

240
00:16:45,215 --> 00:16:49,165
Identify where these tools can fit
into your Kubernetes AML stack.

241
00:16:49,165 --> 00:16:51,235
For example, adding
fairness checks into the.

242
00:16:52,134 --> 00:16:57,024
Q flow pipeline or monitoring
bias by Prometheus metrics.

243
00:16:57,814 --> 00:17:03,814
Example of hospital readmission
prediction model was audited and found

244
00:17:03,814 --> 00:17:09,275
to underperform for older patient
integration points were identified

245
00:17:09,335 --> 00:17:13,085
to add fairness, evaluation, step
in Q, flow training pipeline.

246
00:17:13,585 --> 00:17:15,685
Second step, second phase.

247
00:17:16,185 --> 00:17:19,875
While implementation you can
take care is integration.

248
00:17:20,715 --> 00:17:24,675
Here you operationalize what
you found during assessment.

249
00:17:25,305 --> 00:17:31,010
Deploy fairness and explainability tools
as sidecar services or Kubernetes job.

250
00:17:31,070 --> 00:17:38,525
For example, lime Pods, running
explanations in the real time or fair

251
00:17:38,585 --> 00:17:41,525
learn jobs running during retraining.

252
00:17:42,025 --> 00:17:46,225
Automate fairness and transparency
testing as a part of CICD.

253
00:17:46,725 --> 00:17:51,745
Use Kubernetes monitoring like
premises ANA to visualize fairness,

254
00:17:51,745 --> 00:17:54,235
drift or explainability coverage.

255
00:17:55,105 --> 00:17:59,995
Example here is like the hospitals
in hospitals with deployed

256
00:17:59,995 --> 00:18:04,345
shaft as a Kubernetes just to
generate batch explanation for.

257
00:18:04,960 --> 00:18:09,250
Every new model version,
automatically logging features

258
00:18:09,250 --> 00:18:12,010
importance to a compliance dashboard.

259
00:18:12,510 --> 00:18:14,100
Third phase is important.

260
00:18:14,160 --> 00:18:17,730
That's scalability, scaling, or monitored.

261
00:18:18,230 --> 00:18:22,850
Once pilot implementations are
proven effective, scale them

262
00:18:22,850 --> 00:18:24,830
across departments or product line.

263
00:18:25,330 --> 00:18:26,110
Define reusable.

264
00:18:26,610 --> 00:18:31,830
Hand chart or operators that automatically
apply fairness checks or privacy

265
00:18:31,830 --> 00:18:34,259
settings to any new ML workload.

266
00:18:35,129 --> 00:18:40,469
Implement organization wide governance
rules via admission controller to

267
00:18:40,469 --> 00:18:43,769
enforce AI policies before deployment.

268
00:18:44,639 --> 00:18:49,620
This creates consistency,
traceability, and compliance at scale.

269
00:18:50,120 --> 00:18:55,459
Example, after success with readmission
models, the hospital extended fairness

270
00:18:55,459 --> 00:19:02,239
and explainability checks to imaging
AI triage models, all governed by

271
00:19:02,239 --> 00:19:04,039
standardized Kubernetes policies.

272
00:19:04,539 --> 00:19:05,774
Hope that helps.

273
00:19:06,394 --> 00:19:07,274
So here.

274
00:19:07,774 --> 00:19:10,174
Lemme give you an hypothetical use case.

275
00:19:10,174 --> 00:19:15,384
The hospital cluster trains a
local risk prediction model daily.

276
00:19:15,444 --> 00:19:20,844
After training of fairness, audit
board runs if passed the model

277
00:19:20,844 --> 00:19:24,354
packaged and promoted to inference.

278
00:19:24,444 --> 00:19:29,064
In survey, each prediction
passed through an explanation

279
00:19:29,064 --> 00:19:33,414
sidecar, a flag, and logs metrics.

280
00:19:33,759 --> 00:19:39,909
Updates flow through the federated
aggregation with differential privacy.

281
00:19:40,419 --> 00:19:45,929
Meanwhile, monitoring dashboards, track
drift, fairness and system health.

282
00:19:46,559 --> 00:19:51,029
Because each component is
modular, you can grow capability.

283
00:19:51,149 --> 00:19:55,474
It rateably, maybe start with fairness,
audit, then add explainability,

284
00:19:56,069 --> 00:19:57,989
then introduce federated learning.

285
00:19:58,199 --> 00:20:02,339
You don't need to enable
every pillar at day one.

286
00:20:02,839 --> 00:20:07,249
So key takeaways for the
healthcare engineers.

287
00:20:07,749 --> 00:20:12,159
Treat ethics as infrastructure,
not an afterthought.

288
00:20:12,429 --> 00:20:16,209
Embed fairness, explainability privacy.

289
00:20:16,209 --> 00:20:17,409
Deep into your stack.

290
00:20:18,009 --> 00:20:19,124
Automate everything.

291
00:20:19,124 --> 00:20:22,279
Manual gates space at scale.

292
00:20:22,339 --> 00:20:23,089
When you scale it.

293
00:20:23,089 --> 00:20:25,404
Use pipeline admission
controller and check.

294
00:20:25,904 --> 00:20:30,044
Roll out incrementally, start
small quick wins, and then expand,

295
00:20:30,884 --> 00:20:35,414
continuously monitor drift,
fairness, privacy, performance.

296
00:20:35,744 --> 00:20:38,984
They are all changing over time,
so it's important to monitor them.

297
00:20:39,854 --> 00:20:46,394
Create human in loop and feedback
loop clinicians, patients, they may

298
00:20:46,394 --> 00:20:48,619
stay in lu, they cannot be excluded.

299
00:20:49,119 --> 00:20:51,369
Ethics is not the enemy of speed.

300
00:20:51,429 --> 00:20:54,639
It's the foundation of
trust and sustainability.

301
00:20:55,139 --> 00:21:00,599
AI holds tremendous promise in healthcare,
more accurate diagnosis, earlier

302
00:21:00,599 --> 00:21:06,589
interventions, better patient outcome,
but without care, we risk amplifying

303
00:21:06,589 --> 00:21:10,354
bias, eroding trust, and causing harm.

304
00:21:11,044 --> 00:21:13,659
Kubernetes gives us a flexible.

305
00:21:14,559 --> 00:21:23,529
Scalable base, which I urge you to do
is elevate ethics, fairness, privacy,

306
00:21:24,489 --> 00:21:30,369
explainability from afterthought to first
class citizen in your infrastructure.

307
00:21:30,869 --> 00:21:32,039
Start small.

308
00:21:32,069 --> 00:21:34,709
Bake in one fairness, check tomorrow.

309
00:21:34,769 --> 00:21:38,639
Deploy one explainable
explanation Sidecar.

310
00:21:38,999 --> 00:21:42,089
Pilot federated training in
a controlled environment.

311
00:21:42,269 --> 00:21:43,349
Major them.

312
00:21:43,439 --> 00:21:46,309
Learn, iterate, and share your learnings.

313
00:21:46,809 --> 00:21:49,629
Let's not just build
smarter AI in healthcare.

314
00:21:49,629 --> 00:21:52,749
Let's build AI that people can trust.

315
00:21:53,249 --> 00:21:54,479
I want to thank you.

316
00:21:54,809 --> 00:21:55,829
Please reach me out.

317
00:21:55,829 --> 00:21:56,759
You're welcome.

318
00:21:57,359 --> 00:22:02,789
If any questions, feel free to reach
me out via LinkedIn or by any means.

319
00:22:03,059 --> 00:22:03,479
Thank you.

