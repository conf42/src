1
00:00:00,500 --> 00:00:04,195
Hey everyone, and welcome to this
session on Reliability Patterns,

2
00:00:04,195 --> 00:00:05,845
information Doctor Systems.

3
00:00:06,445 --> 00:00:11,865
Your host today is maybe we will be
too, and I preferably go by the name.

4
00:00:11,870 --> 00:00:12,130
Great.

5
00:00:12,990 --> 00:00:17,760
I currently work as an SRA and I've spent
several years focusing on reliability

6
00:00:17,760 --> 00:00:20,580
engineering in distributed systems.

7
00:00:20,850 --> 00:00:24,360
I've spent the last three years
building our permission systems

8
00:00:24,470 --> 00:00:26,540
blockchain networks like capital fabric.

9
00:00:27,130 --> 00:00:33,295
You got checkups like Key 12 does the
link and here in outside work stuff.

10
00:00:33,795 --> 00:00:36,045
I like philosophy and mathematics.

11
00:00:36,545 --> 00:00:42,235
So in this talk we are going to
explore, proving architectural and

12
00:00:42,245 --> 00:00:46,355
operational patterns that can help
you keep your fabric network highly

13
00:00:46,355 --> 00:00:49,175
available and silence to failures.

14
00:00:49,635 --> 00:00:54,225
I'll walk you through everything on
how to cluster nodes for foster arounds

15
00:00:54,585 --> 00:00:56,915
to handling Byzantine scenarios.

16
00:00:56,995 --> 00:01:01,455
The practical performance tuning
and yeah, maybe in the end we'll

17
00:01:01,455 --> 00:01:04,004
talk about reward incidents.

18
00:01:04,515 --> 00:01:10,295
Yeah, this session overview so quickly,
we're going to have a fabric overview,

19
00:01:10,685 --> 00:01:15,065
so that we're aligned on the basic
components of like fabric network.

20
00:01:15,455 --> 00:01:19,865
Then next hour we'll dive into
fourth tolerance, no clusters,

21
00:01:19,865 --> 00:01:22,175
focusing on peers and ordering.

22
00:01:22,745 --> 00:01:27,055
Then we'll look at Byzantine
fourth tolerance and how fabric

23
00:01:27,145 --> 00:01:29,155
is evolving in that area.

24
00:01:29,695 --> 00:01:32,605
Then after that, we'll
discuss network partitioning.

25
00:01:33,105 --> 00:01:36,185
How you handle network partitions
because, things like this

26
00:01:36,185 --> 00:01:37,355
happen, bad things happen.

27
00:01:37,804 --> 00:01:42,515
The nurse will talk about performance
tuning for high availability.

28
00:01:43,215 --> 00:01:47,235
Then we would cover monitoring
and all alerting strategies.

29
00:01:48,105 --> 00:01:50,295
So you can detect issues early.

30
00:01:50,625 --> 00:01:55,025
Then we'll review reward failure
scenarios and, learn from them.

31
00:01:55,525 --> 00:01:56,664
What is fabric?

32
00:01:57,145 --> 00:01:59,035
Hyper Fabric is a modular.

33
00:01:59,535 --> 00:02:02,834
Permission blockchain framework or system.

34
00:02:03,285 --> 00:02:08,894
So unlike public blockchains
like Bitcoin, Orum, or Sona it's

35
00:02:08,894 --> 00:02:14,504
designed for environments where
participants are known or semi state.

36
00:02:14,774 --> 00:02:19,124
So you are going to find this in
government or you're going to find

37
00:02:19,124 --> 00:02:23,174
this in financial institutions or
supply chain, that kind of stuff.

38
00:02:23,594 --> 00:02:29,564
Fabric main confidence peers, they
host ledgers and smart contracts

39
00:02:30,114 --> 00:02:32,604
which you could also call chain codes.

40
00:02:32,994 --> 00:02:37,754
Then the ordering service,
puts transactions and put

41
00:02:37,754 --> 00:02:39,464
transactions into blocks.

42
00:02:39,854 --> 00:02:43,639
So we would see how raft, MD ft
can be used there for reliability.

43
00:02:44,389 --> 00:02:47,489
Then maybe shift service or cs or.

44
00:02:48,104 --> 00:02:52,784
Certificate authorities the
issue and manage identities.

45
00:02:53,084 --> 00:02:55,544
Then chain code is a smart contract logic.

46
00:02:55,724 --> 00:03:01,244
Chain codes are installed on ps so
what chain codes to is to endorse

47
00:03:01,304 --> 00:03:02,494
and, and, validate transactions.

48
00:03:03,244 --> 00:03:07,354
We'll talk about we're going to talk
about how to prevent that time data loss.

49
00:03:07,354 --> 00:03:07,774
I know.

50
00:03:08,209 --> 00:03:12,319
Of catastrophic failures
in such a system like this.

51
00:03:12,619 --> 00:03:13,789
And fabric.

52
00:03:13,909 --> 00:03:16,879
The way fabric is designed to
have reliability, it has, it

53
00:03:16,969 --> 00:03:20,899
does like a distributor system
where, you know each and everyone

54
00:03:20,899 --> 00:03:23,489
knows kinds of dough replication.

55
00:03:23,969 --> 00:03:26,609
The consensus is also configurable.

56
00:03:26,609 --> 00:03:30,999
So you can either run
Raft or PFT even solo.

57
00:03:31,499 --> 00:03:33,209
Kafka is also deprecated.

58
00:03:33,619 --> 00:03:38,309
And yeah, the way these peers
sync is or the way this system

59
00:03:38,309 --> 00:03:40,309
sync is through the protocol.

60
00:03:40,809 --> 00:03:45,789
Yeah, this is an example diagram
of a multi organization fabric

61
00:03:45,789 --> 00:03:48,189
network in this organization.

62
00:03:48,189 --> 00:03:50,779
There are about in this network
there are about four organizations

63
00:03:50,839 --> 00:03:53,629
the root organization, which
the ordering service could run.

64
00:03:53,999 --> 00:03:58,389
And established partner needs to have a
separate or there are organization where,

65
00:03:58,389 --> 00:04:01,729
yeah this network has a art organizations.

66
00:04:02,254 --> 00:04:03,934
Organization one, two, Andre.

67
00:04:03,934 --> 00:04:08,244
And if you look at each of these
organizations have at least one

68
00:04:08,654 --> 00:04:10,784
pair running in them a could.

69
00:04:11,294 --> 00:04:15,014
A good a, a good example of a network
that's built for redundancy will

70
00:04:15,014 --> 00:04:16,874
have at least two pairs on them.

71
00:04:17,304 --> 00:04:20,834
So you could see organization
one is running about, about

72
00:04:20,894 --> 00:04:24,844
eight Ps and organization two and
three are running one pair each.

73
00:04:24,874 --> 00:04:29,004
And also like the ordering services
the road organization is running about

74
00:04:29,394 --> 00:04:33,564
three orders which are built on Rev.
So as this store goes on I think a

75
00:04:33,564 --> 00:04:35,154
lot of things will make sense here.

76
00:04:35,634 --> 00:04:40,004
Yeah, it also, if you notice, each of
these organizations are running ac to

77
00:04:40,004 --> 00:04:45,354
manage identity, the big picture here
is that each org has multiple peers.

78
00:04:45,864 --> 00:04:49,564
These peers talk to the
ordering notes to get blocks.

79
00:04:50,224 --> 00:04:55,514
So we, and the way each of this component
talk is through the gossip protocol.

80
00:04:55,514 --> 00:05:01,844
So we rely on the basic protocol for peers
to share their data among themselves.

81
00:05:02,344 --> 00:05:06,874
Yeah, like I said, I architectural
patterns each or two to three PS for

82
00:05:06,874 --> 00:05:10,074
redundancy plus protocol for ledger syn.

83
00:05:11,004 --> 00:05:15,199
The ordering service could have three to
five orders, and you need to select an org

84
00:05:15,289 --> 00:05:17,779
number because NCES runs on this thing.

85
00:05:18,289 --> 00:05:20,099
And yeah, if you're running raft.

86
00:05:20,249 --> 00:05:21,299
Okay, yeah, we'll get more on that.

87
00:05:21,669 --> 00:05:26,759
Yeah, CAS you, the cluster running
with that CAS might not be a problem.

88
00:05:26,819 --> 00:05:31,969
Ca, ca. Cas if you, if your ca goes
down, that will probably not a problem.

89
00:05:32,819 --> 00:05:34,019
The network will still run.

90
00:05:34,019 --> 00:05:38,619
But in cases where clients have to renew
certificates or you want to enroll a

91
00:05:38,649 --> 00:05:40,899
new certificate, that becomes a problem.

92
00:05:41,049 --> 00:05:45,999
So you probably want to have a backup
on cas and yeah, if you are running

93
00:05:46,119 --> 00:05:50,769
on Kubernetes, there's a feature
in Kubernetes for anti affinity.

94
00:05:51,069 --> 00:05:56,889
So you want to spread the Ps or
other ports across different modes.

95
00:05:56,919 --> 00:06:01,599
You don't want all of them running in
same Kubernetes mode because if that

96
00:06:02,169 --> 00:06:05,919
Kubernetes no should go down, for example,
you have your home network going down.

97
00:06:06,459 --> 00:06:06,849
So yeah.

98
00:06:07,349 --> 00:06:11,739
In this, in this section we're
going to talk about force tolerant,

99
00:06:12,039 --> 00:06:15,269
no clusters, how you build fault
node level, force tolerance.

100
00:06:15,449 --> 00:06:18,979
So like I said in fabric you
have two types of nodes, the

101
00:06:18,979 --> 00:06:20,779
PS and the ordering service.

102
00:06:21,099 --> 00:06:25,479
The p at least each org should
have at least two or three years.

103
00:06:25,809 --> 00:06:28,059
Oftentimes in a good network trade.

104
00:06:28,479 --> 00:06:33,739
If one feels the other one can
endorse transactions or save queries.

105
00:06:34,109 --> 00:06:37,619
Go see, ensure that, they
keeping consistent ledger copy.

106
00:06:38,519 --> 00:06:38,669
Yeah.

107
00:06:39,399 --> 00:06:43,779
For the ordering service, we, you
typically roll three to five other if you

108
00:06:43,779 --> 00:06:46,379
are using raft for crash for to Rails.

109
00:06:46,529 --> 00:06:51,029
So if one fill the cluster will
select a new leader as long that

110
00:06:51,209 --> 00:06:52,919
there's still a majority up.

111
00:06:53,279 --> 00:06:56,519
So Raft is based on
leader based consensus.

112
00:06:56,519 --> 00:07:00,459
So at each, every time the
cluster needs a leader.

113
00:07:00,859 --> 00:07:05,829
There's a leader election that goes
on if one of if a leader crashes

114
00:07:05,919 --> 00:07:10,499
or dies and, if you want to, if
you want higher 40 tolerance, it

115
00:07:10,559 --> 00:07:12,389
could go as high as five nodes.

116
00:07:12,779 --> 00:07:18,149
So in five nodes it could probably
be able to tolerate two crashes.

117
00:07:18,239 --> 00:07:25,395
So keep adding, yeah, keep adding
nodes for, higher 40 tolerance.

118
00:07:25,590 --> 00:07:27,210
You should ensure it's a high no.

119
00:07:27,479 --> 00:07:31,669
So we also want to make sure you are not
adding a lot of notes because there's,

120
00:07:32,059 --> 00:07:35,895
you, do you want to make sure that
you are not having so many notes that.

121
00:07:36,505 --> 00:07:39,235
Throw out the cluster from equilibrium.

122
00:07:39,564 --> 00:07:45,145
So this architecture, is crucial
because if we had only one ordering

123
00:07:45,145 --> 00:07:50,155
node, or if we had only a single
pay, any downtime, we just host

124
00:07:50,155 --> 00:07:52,675
the network or the entire network.

125
00:07:53,005 --> 00:07:55,285
We could see why this is,

126
00:07:55,785 --> 00:07:56,975
This is, important.

127
00:07:57,515 --> 00:08:00,245
The way raft works is by having a node.

128
00:08:00,335 --> 00:08:03,395
So we'll just talk a little
bit on raft mechanics.

129
00:08:03,625 --> 00:08:05,485
Raft works by having one no.

130
00:08:05,585 --> 00:08:09,680
As a leader, that leader
proposes transaction batches.

131
00:08:10,160 --> 00:08:16,160
When a majority of the nude
or Fullers acknowledge the

132
00:08:16,160 --> 00:08:17,900
block is considered committed.

133
00:08:18,470 --> 00:08:23,470
If the leader fails one of the follower
steps or, and becomes a leader.

134
00:08:23,800 --> 00:08:28,000
So in a three, no plus losing
one no doesn't necessarily stop

135
00:08:28,000 --> 00:08:31,480
the show because you still have
two nos forming the majority.

136
00:08:31,960 --> 00:08:37,470
That's how we keep ordering online
despite hardware failures or crashes.

137
00:08:37,950 --> 00:08:44,040
Yeah, so to avoid single points of
failure, to summarize the key is

138
00:08:44,050 --> 00:08:45,760
to avoid single point of failure.

139
00:08:46,110 --> 00:08:48,360
No single other error is always a cluster.

140
00:08:48,860 --> 00:08:52,910
Multiple Ps For organization,
you want to have three at least.

141
00:08:53,240 --> 00:08:59,870
So if your identity issuance is
mission critical or very important,

142
00:09:00,080 --> 00:09:01,250
you want to have a backup.

143
00:09:01,970 --> 00:09:07,310
And if you are running a ities, you want
to set up anti affinity to make sure that

144
00:09:07,310 --> 00:09:10,190
ports are not necessarily on the same.

145
00:09:10,880 --> 00:09:15,470
Post as a rule of thumb, you
consider your failure domains like

146
00:09:15,500 --> 00:09:20,990
rejoin data center or even the rack
and, spread the nodes according.

147
00:09:21,490 --> 00:09:26,330
So let's talk a little bit on
fault, uming fault tolerance.

148
00:09:26,675 --> 00:09:28,685
So fabric default.

149
00:09:28,735 --> 00:09:34,525
Consensus is craft is crash fault
tolerant, which means that is raft.

150
00:09:35,005 --> 00:09:36,475
Raft is crash fault tolerant.

151
00:09:36,805 --> 00:09:41,035
But if your nudes, what if your
nudes could be outright initial?

152
00:09:41,035 --> 00:09:46,705
For example, one of your nodes is trying
to reorder the order of transactions.

153
00:09:47,065 --> 00:09:52,545
So yeah, that is a form
that, doesn't take care of.

154
00:09:52,765 --> 00:09:56,695
Bezant for tolerance means that the
system can tolerate notes that act

155
00:09:56,905 --> 00:10:02,005
actively misbehave or attempt to
fuck the chain, or attempt to crazy

156
00:10:02,005 --> 00:10:04,265
things like reorder transactions.

157
00:10:04,685 --> 00:10:10,545
So Fabric introduced A BFT ordering
service called BFT in professional trade.

158
00:10:10,785 --> 00:10:14,325
So this requires three F plus one.

159
00:10:14,825 --> 00:10:15,125
No.

160
00:10:15,365 --> 00:10:22,255
To tolerate failures means that
if you want, for example, one to

161
00:10:22,255 --> 00:10:27,655
tolerate, if you want to tolerate
one 14 root, you need to have four.

162
00:10:27,835 --> 00:10:29,635
So that is three times one plus one.

163
00:10:30,415 --> 00:10:30,715
Yeah.

164
00:10:30,925 --> 00:10:35,915
So every f is the number of
failures or malicious nodes you

165
00:10:35,915 --> 00:10:37,565
want to be able to tolerate.

166
00:10:37,775 --> 00:10:43,915
So if you want to be able to tolerate, two
40 or malicious notes, that is three times

167
00:10:43,975 --> 00:10:46,465
two plus one, which is about seven nodes.

168
00:10:46,735 --> 00:10:51,775
So it, yeah, it's more resource intensive
and has more a message overhead.

169
00:10:52,075 --> 00:10:55,015
You get a stronger guarantee, but
you get a stronger guarantee that the

170
00:10:55,015 --> 00:10:59,035
network will remain consistent even
if some participants act maliciously.

171
00:10:59,535 --> 00:11:04,700
The use cases for this is, if you are
in an environment where you are, you

172
00:11:04,700 --> 00:11:07,070
don't really trust the other members.

173
00:11:07,570 --> 00:11:10,990
Yeah, VFT is really suitable
for this all in know, very

174
00:11:10,990 --> 00:11:13,030
high value finance environment.

175
00:11:13,440 --> 00:11:19,770
You have to summarize raft handles, crash
failures well, but not malicious ones.

176
00:11:20,040 --> 00:11:22,740
So VFT can handle both.

177
00:11:23,640 --> 00:11:26,790
Yeah most deployments would
not immediately jump to BFT

178
00:11:26,790 --> 00:11:29,870
because it's still, it's still
something very new in fabric.

179
00:11:30,230 --> 00:11:35,915
But if you have a, use a use case
for truly un untrusted participants

180
00:11:36,215 --> 00:11:37,790
yeah, then it's worth evaluating.

181
00:11:38,290 --> 00:11:39,075
So yeah.

182
00:11:39,135 --> 00:11:42,335
Let's let's look at
handling network partitions.

183
00:11:42,695 --> 00:11:48,215
So real distributor systems must
handle like a case where the network

184
00:11:48,215 --> 00:11:51,845
itself fails or, becomes segmented.

185
00:11:52,685 --> 00:11:58,215
So let's discuss other partition, let's
say the set of other partition themselves.

186
00:11:58,425 --> 00:12:01,725
So with Ralph, the only
partition, the partition that

187
00:12:01,725 --> 00:12:03,975
has a majority of other roles.

188
00:12:04,410 --> 00:12:07,020
Would continue producing blocks.

189
00:12:07,410 --> 00:12:10,880
So the minority side stores
doesn't create a fork.

190
00:12:11,360 --> 00:12:17,490
So when the, once the partition heals
the minority automatically catches up.

191
00:12:18,480 --> 00:12:23,490
But for pair isolation, if the pairs
are isolated, they stop receiving

192
00:12:23,490 --> 00:12:26,580
blocks with when connectivity returns.

193
00:12:27,030 --> 00:12:32,830
They do a course C based catch up that
is pulling missed blocks from the.

194
00:12:33,235 --> 00:12:39,475
Healthy ps. So as architects, we want
to distribute moves across different

195
00:12:39,475 --> 00:12:44,785
zones or data centers so that we
can still achieve a mi a majority,

196
00:12:44,785 --> 00:12:47,815
even if a region is fought off.

197
00:12:48,315 --> 00:12:51,195
This is a partition diagram.

198
00:12:51,905 --> 00:12:55,475
This network shows, this
diagram shows that the majority

199
00:12:55,475 --> 00:12:56,945
partition keeps ordering.

200
00:12:57,445 --> 00:13:00,365
Going, why the minority stores?

201
00:13:00,575 --> 00:13:04,745
So the big takeaway here is
that no F happens, right?

202
00:13:05,015 --> 00:13:10,385
So the minority side, eventually,
since once the network is restored.

203
00:13:10,885 --> 00:13:15,295
So let's discuss performance
tuning for high availability.

204
00:13:15,555 --> 00:13:20,235
Reliability is not really just
about redundancy, it's also about

205
00:13:20,625 --> 00:13:24,555
ensuring that the system can
handle those without feeling.

206
00:13:25,165 --> 00:13:30,245
Yeah, just like some tips, increased
block size for throughput, you

207
00:13:30,335 --> 00:13:32,195
also want to watch for latency.

208
00:13:32,695 --> 00:13:35,095
You have to tune this for your workload.

209
00:13:35,485 --> 00:13:40,195
If possible, use more
looser or flexible policies.

210
00:13:40,365 --> 00:13:46,665
EG, any two of three orgs can keep
you running if one of the org is down.

211
00:13:47,025 --> 00:13:53,775
So strict policies can cause
a stall or hold if a required

212
00:13:53,775 --> 00:13:55,785
organization goes offline.

213
00:13:56,515 --> 00:14:00,205
Then also like for the
study, level DB is faster.

214
00:14:00,665 --> 00:14:06,475
But CouchDB supports switcher queries,
that is adds like some form of overhead.

215
00:14:06,975 --> 00:14:12,235
Then you also, want to have multiple
channels like izing, workload for

216
00:14:12,235 --> 00:14:14,935
scalability or isolating issues.

217
00:14:15,565 --> 00:14:21,985
Past SSD enough, CPU, you want to ensure
that you are beefed at the hardware level,

218
00:14:21,985 --> 00:14:27,895
so you enough, CPU, memory, SSD, and
even network bandwidth running upgrades.

219
00:14:27,975 --> 00:14:30,495
Update one at the time.

220
00:14:31,365 --> 00:14:31,695
Yeah.

221
00:14:31,695 --> 00:14:34,515
So to avoid total art.

222
00:14:35,015 --> 00:14:37,505
So this is a sample raft config.

223
00:14:37,715 --> 00:14:44,735
So you can see in the XRP how
we define ants in other tmo.

224
00:14:44,855 --> 00:14:48,125
So this is sample other
TMO for a raft cluster.

225
00:14:48,415 --> 00:14:52,720
If you, the key parameters here,
like tick interval, election, tick

226
00:14:53,380 --> 00:14:55,985
and heartbeat, it can be tuned.

227
00:14:56,605 --> 00:14:58,645
Based on the network latency.

228
00:14:59,485 --> 00:15:05,575
Many times default values are often fine
but in high latency or global deployments,

229
00:15:05,575 --> 00:15:08,125
you may want to increase them.

230
00:15:08,755 --> 00:15:14,945
So yeah, you can tune this election
tick a b tick, and also concern us.

231
00:15:15,445 --> 00:15:18,115
So this is an endorsement policy example.

232
00:15:18,145 --> 00:15:19,585
It's like a Jason snippet.

233
00:15:20,155 --> 00:15:23,460
That requires any two or three orgs.

234
00:15:23,665 --> 00:15:28,795
So this means that one org would be
completely offline, but will still

235
00:15:28,795 --> 00:15:30,985
be able to get valid endorsement.

236
00:15:31,285 --> 00:15:36,865
This is much more silent than a
policy that is requiring, or three.

237
00:15:37,295 --> 00:15:42,215
Which means that if one organization
store, then the whole network is

238
00:15:42,315 --> 00:15:45,405
stored or transaction in the is stored.

239
00:15:45,905 --> 00:15:48,005
Yeah, for monitoring and all alerting.

240
00:15:48,155 --> 00:15:50,705
So you want to proactively catch issues.

241
00:15:51,005 --> 00:15:52,625
So we need monitoring.

242
00:15:53,005 --> 00:15:57,325
Kissing notes include like no
health and container restart.

243
00:15:57,865 --> 00:16:00,715
You want to also look like
block production rate, leisure

244
00:16:00,715 --> 00:16:03,935
heights to dictate lacking those.

245
00:16:04,440 --> 00:16:09,090
You want to also check transaction
throughput, latency, resource

246
00:16:09,090 --> 00:16:11,340
usage, and, set expiration.

247
00:16:11,820 --> 00:16:15,230
So a popular combo is a
parameters plus Grafana.

248
00:16:15,570 --> 00:16:18,390
But I think you can use stuff like a LK.

249
00:16:19,200 --> 00:16:23,610
You can also use stuff like
a LK Splunk Datadog for.

250
00:16:24,020 --> 00:16:25,250
Log analysis.

251
00:16:25,600 --> 00:16:29,480
Until you want to out for, you want
to out for no new blocks has been

252
00:16:29,480 --> 00:16:32,740
created or, or even resource tion.

253
00:16:33,240 --> 00:16:33,300
Yeah.

254
00:16:33,800 --> 00:16:34,885
So yeah, all that.

255
00:16:35,720 --> 00:16:40,780
Like proactive, all alerting you
could do is detect ano early.

256
00:16:40,990 --> 00:16:44,050
So you want to check if
the RAF lid has changed.

257
00:16:44,260 --> 00:16:48,070
You want to check on, you want
also all alert on cause failures

258
00:16:48,070 --> 00:16:49,720
or monitor stuff like this.

259
00:16:50,140 --> 00:16:54,380
And if possible, you have taken,
this matrix a lot, a long time.

260
00:16:54,770 --> 00:16:59,450
You want to do like historical
trend analysis on, on this.

261
00:16:59,950 --> 00:17:04,500
Yeah, these are some, like even with all
these patterns, something can go wrong.

262
00:17:04,770 --> 00:17:06,780
So let's look at some examples.

263
00:17:07,130 --> 00:17:07,460
Audrey?

264
00:17:07,460 --> 00:17:08,710
No Audrey, no.

265
00:17:08,980 --> 00:17:09,550
Crash.

266
00:17:10,090 --> 00:17:14,170
Ralph would quickly elect a
nude, neither and the metal.

267
00:17:14,740 --> 00:17:18,790
So this needs no really
manual intervention.

268
00:17:18,790 --> 00:17:19,150
Yeah.

269
00:17:19,480 --> 00:17:20,890
So PS out of sync.

270
00:17:21,290 --> 00:17:25,070
Gossip sometimes can,
automatically catch up.

271
00:17:25,460 --> 00:17:31,460
Only stream, probably the net, their
network failures outside of fabric.

272
00:17:31,850 --> 00:17:37,890
All down strict endorsement policy
probably has frozen the network or

273
00:17:37,890 --> 00:17:40,500
you don't have enough organizations.

274
00:17:40,990 --> 00:17:42,430
Set expiration.

275
00:17:43,360 --> 00:17:48,930
I think if a most case of this
is what happened with T Cash on

276
00:17:48,990 --> 00:17:52,440
CBDC when it went down for months.

277
00:17:52,440 --> 00:17:55,560
I've also, when I worked at gala.

278
00:17:56,060 --> 00:17:58,460
The chain was also based
on Hyperledger fabric.

279
00:17:58,460 --> 00:18:03,080
There are some instances where,
set exploration caused big issues.

280
00:18:03,400 --> 00:18:07,490
Yeah, put sometimes to things like
chin code bugs, like a container

281
00:18:07,490 --> 00:18:10,000
can panic, but other peers.

282
00:18:10,005 --> 00:18:14,670
And also there's partial,
continuity network partition.

283
00:18:15,050 --> 00:18:16,580
Only majority sites.

284
00:18:17,080 --> 00:18:20,710
Keep, only majority side
continues, so minority will

285
00:18:20,710 --> 00:18:23,170
be stored until it reconnects.

286
00:18:23,540 --> 00:18:28,310
Sometimes you have like denial service
act where they are valued in pair

287
00:18:28,310 --> 00:18:33,970
with proposals that leads to slow
endorsement or maybe even a store.

288
00:18:34,330 --> 00:18:36,430
This is going to need rate limits.

289
00:18:36,700 --> 00:18:42,640
So from this incident, we have seen
like that even multi mode setups,

290
00:18:42,850 --> 00:18:49,150
flexible policies, automatic renewals,
and to monitoring are crucial.

291
00:18:49,150 --> 00:18:54,810
So in all of these, I think
monitoring is really important here.

292
00:18:55,310 --> 00:18:59,530
So industry, there's some
industry use cases and, lessons

293
00:19:00,070 --> 00:19:02,370
industry use cases or fabric.

294
00:19:02,370 --> 00:19:05,130
And we've seen fabric
in financial services.

295
00:19:05,460 --> 00:19:07,850
I've seen, fabric and supply chain.

296
00:19:08,060 --> 00:19:13,230
The financial services you is
typically like multi joint deployment,

297
00:19:13,530 --> 00:19:15,900
so three to five afternoons.

298
00:19:15,990 --> 00:19:21,959
If they like, they could also use BFD
also zero down time updates and very

299
00:19:21,959 --> 00:19:24,569
heavy on monitoring supply chain.

300
00:19:24,669 --> 00:19:28,489
This is high volume, so you
need, parallel channels.

301
00:19:28,989 --> 00:19:33,789
And you also have to meet, data is
always available even if some peers feel.

302
00:19:34,289 --> 00:19:39,769
For identity systems very small,
very security focused networks,

303
00:19:39,859 --> 00:19:45,019
they'll po possibly adopt BFT
for malicious fault tolerance.

304
00:19:45,580 --> 00:19:49,720
In every of these cases, the
ops team heavily invest in

305
00:19:49,720 --> 00:19:51,670
monitoring, scaling, and testing.

306
00:19:52,170 --> 00:19:55,320
Best practices, like a
recap of best practices.

307
00:19:55,590 --> 00:20:00,450
You want to have redundant peers and
other nodes, like redundant components.

308
00:20:00,780 --> 00:20:06,869
You want to have flexible endorsements
avoid single or endorsement if possible.

309
00:20:07,349 --> 00:20:10,049
And you also don't have
to require every org.

310
00:20:10,589 --> 00:20:16,129
Regular certificates management
and backup of admin credentials.

311
00:20:16,539 --> 00:20:20,709
Two block sizes and
endorsements for your workload.

312
00:20:21,119 --> 00:20:28,139
Active monitoring actively monitor also
fill over and rolling upgrades stone.

313
00:20:28,830 --> 00:20:29,760
Minimize time.

314
00:20:29,760 --> 00:20:32,610
Time and yeah, to test your assumptions.

315
00:20:32,610 --> 00:20:39,620
You could, shut down, or simulate in
partition to see if the network recovers.

316
00:20:39,620 --> 00:20:43,040
So like sometimes do some chaos testing.

317
00:20:43,620 --> 00:20:49,440
So the conclusion here is hyper
fabric can be very reliable if we

318
00:20:49,440 --> 00:20:51,720
take advantage of its architecture.

319
00:20:52,020 --> 00:20:57,630
If we distribute other replicate pairs,
set up to full endorsement policies,

320
00:20:57,660 --> 00:21:03,420
and build a solid monitoring track,
we have looked at both crash force

321
00:21:03,420 --> 00:21:09,420
tolerance and byzantine force tolerance
and how to handle network partitions.

322
00:21:09,875 --> 00:21:14,310
And how to two performance and
how to avoid reward pitfalls

323
00:21:14,310 --> 00:21:16,460
like certificate expiry.

324
00:21:16,910 --> 00:21:22,190
So by following this reliability
patterns, you'll be well on your way to a

325
00:21:22,250 --> 00:21:25,040
resilient enterprise ready fabric network.

326
00:21:25,540 --> 00:21:30,790
Thank you for your attention and yeah,
this was I hope this was a good talk and.

327
00:21:31,434 --> 00:21:36,264
Gives you some perspective on how
to design reliability patterns

328
00:21:36,504 --> 00:21:38,334
in permission blockchain systems.

329
00:21:38,664 --> 00:21:38,995
Thank you.

330
00:21:39,495 --> 00:21:39,614
Bye.

