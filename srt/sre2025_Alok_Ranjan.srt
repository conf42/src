1
00:00:01,590 --> 00:00:02,550
Welcome everyone.

2
00:00:02,820 --> 00:00:07,320
I'm Alo Engineering Manager for
Storage Platform at Dropbox.

3
00:00:07,770 --> 00:00:12,000
Today we will discuss how we scaled
observability with Grafana Lokey.

4
00:00:14,100 --> 00:00:17,705
Our agenda covers the context
of our observability challenges.

5
00:00:18,585 --> 00:00:22,785
The technical and operational
requirements, we faced our evaluation

6
00:00:22,785 --> 00:00:24,375
of different logging solutions.

7
00:00:24,705 --> 00:00:26,694
And finally why we chose Lowkey.

8
00:00:28,525 --> 00:00:31,665
Before we dive in let me give you
some context about my background.

9
00:00:31,975 --> 00:00:36,265
I'm currently an engineering manager for
the storage platform team at Dropbox.

10
00:00:36,605 --> 00:00:40,775
Prior to this I worked at Big
Switch Networks, VMware and Cisco,

11
00:00:41,195 --> 00:00:44,765
primarily focusing on storage
systems and scalable infrastructure.

12
00:00:45,445 --> 00:00:49,755
My team at Dropbox is responsible
for ensuring our storage systems

13
00:00:49,755 --> 00:00:53,775
can handle massive scale while
maintaining security and performance.

14
00:00:56,255 --> 00:00:59,935
To appreciate why this problem was
so challenging let's first look

15
00:00:59,935 --> 00:01:01,555
at the scale at which we operate.

16
00:01:02,075 --> 00:01:06,015
Dropbox serves over 700 million
users globally with more

17
00:01:06,015 --> 00:01:07,664
than 18 million paying users.

18
00:01:08,539 --> 00:01:12,859
We are handling over a trillion pieces
of content and, we are processing

19
00:01:13,190 --> 00:01:17,600
over billions of new files, uploads
every single of every single day.

20
00:01:20,160 --> 00:01:25,110
Let's start by defining what we mean
by unstructured logs so we can better

21
00:01:25,170 --> 00:01:26,850
understand our problem statement.

22
00:01:27,510 --> 00:01:32,200
Unstructured logs are essentially
raw data outputs that don't adhere

23
00:01:32,200 --> 00:01:34,540
to any defined schema, unlike.

24
00:01:34,935 --> 00:01:36,405
These structured logs, right?

25
00:01:36,405 --> 00:01:41,115
Hi records or traces, which are
neatly formatted at Dropbox.

26
00:01:41,145 --> 00:01:46,455
These logs come from various sources,
including our first party code and

27
00:01:46,505 --> 00:01:50,955
third party debug files, such as
those found in wall log Dropbox.

28
00:01:51,745 --> 00:01:55,794
Because these logs lack a
predefined structure, they offer

29
00:01:55,794 --> 00:01:59,544
a high degree of flexibility and
detail making them incredibly

30
00:01:59,544 --> 00:02:00,864
useful for realtime troubleshoot.

31
00:02:01,595 --> 00:02:06,415
However, that same flexibility
means that analyzing them can be

32
00:02:06,415 --> 00:02:10,075
challenging and time consuming,
especially when you need to quickly

33
00:02:10,135 --> 00:02:12,775
pinpoint an issue during an incident.

34
00:02:15,455 --> 00:02:17,315
Let's go over our problem statement.

35
00:02:17,555 --> 00:02:23,615
As I said, unstructured logs are stored
in wire logs prior to, our implementation

36
00:02:23,675 --> 00:02:29,175
or our adoption of lowkey, we didn't have
an actual logging solution at Dropbox.

37
00:02:29,175 --> 00:02:31,810
Developers used to SSH
into individual boxes.

38
00:02:32,950 --> 00:02:36,100
We had a host rotation
policy of seven days.

39
00:02:36,100 --> 00:02:39,550
That means after seven days, the
host will be rotated out, so all the

40
00:02:39,550 --> 00:02:41,950
logs on that host will be deleted.

41
00:02:42,080 --> 00:02:45,770
Around that same time, we were
also migrating from standalone

42
00:02:45,770 --> 00:02:49,190
host to containers, and as
containers are even more ephemeral.

43
00:02:49,460 --> 00:02:51,080
They can just vanish any moment.

44
00:02:51,800 --> 00:02:54,510
Also, as searching into a container.

45
00:02:54,580 --> 00:02:57,320
To, exactly go to Word log.

46
00:02:57,320 --> 00:02:59,620
Dropbox was also not that easy.

47
00:02:59,680 --> 00:03:01,140
That was also a little cumbersome.

48
00:03:01,650 --> 00:03:03,450
So this was the entire problem statement.

49
00:03:05,320 --> 00:03:08,170
So from there we gathered
a high level requirement.

50
00:03:08,830 --> 00:03:13,350
We needed to provide a user friendly
and secure interface that makes it

51
00:03:13,470 --> 00:03:17,690
easy for production service owners
to look at their unstructured logs.

52
00:03:18,635 --> 00:03:21,785
As I said, our current process
involved manually as searching

53
00:03:21,785 --> 00:03:26,615
into hosts to retrieve logs, which
was not efficient and error prone.

54
00:03:26,865 --> 00:03:29,265
We wanted to automate and
simplify this process.

55
00:03:30,045 --> 00:03:34,395
We wanted to make sure the system is
capable of in ingesting the entire

56
00:03:34,395 --> 00:03:39,195
stream of production logs without any
mo modification to our application code.

57
00:03:40,365 --> 00:03:44,265
Finally the architecture should be
designed to, seamlessly integrate

58
00:03:44,595 --> 00:03:48,345
logs from our acquisitions
and other corporate assets.

59
00:03:50,265 --> 00:03:54,475
Yeah from there we gathered
some more requirements on the

60
00:03:54,475 --> 00:03:56,155
reliability and security side.

61
00:03:56,525 --> 00:04:00,365
We wanted to provide at
least one week log storage.

62
00:04:00,665 --> 00:04:05,095
And as you can see since there was
seven day host policy, host rotation

63
00:04:05,095 --> 00:04:07,375
policy, this requirement came directly.

64
00:04:07,375 --> 00:04:12,395
From there, we analyzed our metrics
and figured out we were ingesting

65
00:04:12,825 --> 00:04:14,325
approximately 150 terabyte.

66
00:04:14,824 --> 00:04:16,294
Per day of logs.

67
00:04:16,395 --> 00:04:19,655
Today it is way more than that,
but that's where we started.

68
00:04:20,165 --> 00:04:24,605
We wanted to make sure the latency
for P 99 ingestion is less than 30

69
00:04:24,605 --> 00:04:27,065
seconds and queries for queries.

70
00:04:27,065 --> 00:04:28,745
It should be less than 10 seconds.

71
00:04:29,085 --> 00:04:33,315
We wanted to make sure our
availability goal is 99%.

72
00:04:34,094 --> 00:04:35,875
We wanted to make sure we have.

73
00:04:36,674 --> 00:04:39,695
MTLS enabled right from the day itself.

74
00:04:40,094 --> 00:04:43,625
So that, we are ensured that every
connection is authenticated, secured.

75
00:04:44,245 --> 00:04:48,094
We wanted to implement strict
access control based on service

76
00:04:48,094 --> 00:04:50,294
ownership by segmenting access.

77
00:04:50,659 --> 00:04:53,779
We could, audit we could
provide provide better auditing.

78
00:04:54,319 --> 00:04:58,659
We can ensure only authorized teams
can view and manage their logs.

79
00:04:59,319 --> 00:05:01,199
To further secure our logs.

80
00:05:01,199 --> 00:05:05,009
We wanted to make sure we are
encrypting all the stored data using

81
00:05:05,009 --> 00:05:06,900
some sort of key management practice.

82
00:05:06,989 --> 00:05:11,819
And we wanted to finally, we wanted
to provide a way to, or a mechanism

83
00:05:12,209 --> 00:05:16,979
for detecting, filtering and redacting
PII information if it ever leaked.

84
00:05:19,139 --> 00:05:23,400
We also wanted to make sure
we call out some non goals.

85
00:05:23,619 --> 00:05:27,010
We did not want to mandate any
changes to our existing log format.

86
00:05:27,499 --> 00:05:31,960
We wanted our system designed in
such a way that it can work with any

87
00:05:31,960 --> 00:05:36,460
unstructured logs we already have so
that teams don't have to migrate to it.

88
00:05:37,159 --> 00:05:42,699
Our goal wasn't to replace any of the,
structured logging or structured logging,

89
00:05:42,699 --> 00:05:44,679
tracing, or any of the metric systems.

90
00:05:45,189 --> 00:05:48,339
We didn't want this system to,
we only wanted this system to

91
00:05:48,339 --> 00:05:52,099
be used as a complimentary tool
which focuses on troubleshooting.

92
00:05:52,709 --> 00:05:56,609
Rather than serving as any other,
observability solution we wanted

93
00:05:56,609 --> 00:05:59,999
to make sure that the solution
was not going to be a replacement

94
00:05:59,999 --> 00:06:01,590
for any of the analytic system.

95
00:06:02,309 --> 00:06:06,969
And lastly, we didn't want to impose
any new logging practice on teams.

96
00:06:06,969 --> 00:06:11,509
We wanted to avoid mandating any,
anything like how logs are produced

97
00:06:11,759 --> 00:06:13,079
or what the format would be.

98
00:06:13,424 --> 00:06:16,414
So that existing team owners
don't have to feel any burden

99
00:06:16,504 --> 00:06:17,824
when this new system is added.

100
00:06:18,824 --> 00:06:22,454
Next, let, next let's take a
look at the evaluation metric.

101
00:06:22,504 --> 00:06:25,694
These, metrics provided us
with a framework for comparing

102
00:06:25,694 --> 00:06:26,654
different approaches.

103
00:06:27,144 --> 00:06:29,994
Cost was definitely the highest priority.

104
00:06:29,994 --> 00:06:36,644
We wanted to make sure that the total cost
of ownership including both opex and CapEx

105
00:06:36,954 --> 00:06:43,684
along with any, potential contract risk
our accounted for performance was another

106
00:06:43,684 --> 00:06:46,224
critical metric we wanted to make sure.

107
00:06:46,704 --> 00:06:50,434
The loading solution could
handle our ingestion rate and,

108
00:06:50,434 --> 00:06:52,384
matches the query latency.

109
00:06:52,454 --> 00:06:53,324
We had in mind.

110
00:06:53,904 --> 00:06:57,524
We definitely wanted a rich
ux, a rich query engine.

111
00:06:57,584 --> 00:07:01,424
And, since our engineers are, were
very familiar with Grafana, we

112
00:07:01,424 --> 00:07:05,414
were highly interested in solution
which could integrate with Grafana.

113
00:07:05,714 --> 00:07:05,744
Okay.

114
00:07:06,644 --> 00:07:10,364
And along with that we wanted to make
sure there is ease of integration

115
00:07:10,364 --> 00:07:12,254
with our current observability tools.

116
00:07:12,739 --> 00:07:15,049
The solution, we wanted a
solution to seamlessly work

117
00:07:15,049 --> 00:07:18,679
across our existing infrastructure
structure without major changes.

118
00:07:19,099 --> 00:07:20,359
And finally, security.

119
00:07:20,419 --> 00:07:24,959
Like we wanted to make sure the there
is robust data protection so that we can

120
00:07:24,959 --> 00:07:27,179
minimize risk of sensitive data exposure.

121
00:07:29,609 --> 00:07:30,959
All right, and yep.

122
00:07:31,840 --> 00:07:34,330
Now let's consider the
do nothing scenario.

123
00:07:34,690 --> 00:07:38,020
The pros of course, were no
additional envi investment.

124
00:07:38,480 --> 00:07:41,990
Cons was we would have to continue
with manual, non-scalable,

125
00:07:42,050 --> 00:07:43,790
inefficient troubleshooting.

126
00:07:44,490 --> 00:07:48,470
So ultimately the status quo
was was not meeting meeting

127
00:07:48,470 --> 00:07:50,270
the, modern observability needs.

128
00:07:50,700 --> 00:07:52,120
So that's why we didn't proceed.

129
00:07:53,120 --> 00:07:57,650
Now let's examine our evaluation of
all the other logging service options.

130
00:07:57,680 --> 00:08:01,220
In this slide, we'll compare several
potential solutions side by side.

131
00:08:01,580 --> 00:08:04,280
So the first one was
externally managed SaaS.

132
00:08:04,280 --> 00:08:07,360
Here we'll be using some we
will use a logging solution

133
00:08:07,360 --> 00:08:09,160
offered by a third party service.

134
00:08:09,700 --> 00:08:13,570
Of course, the pros where it reduces
in-house management overhead.

135
00:08:14,380 --> 00:08:18,040
But the cons was very
high cost of operating it.

136
00:08:18,310 --> 00:08:21,850
And also since we would be sending
entirety of our logs to a third

137
00:08:21,850 --> 00:08:27,240
party service, there was a potential
risk of exposing PII to them.

138
00:08:27,830 --> 00:08:32,040
So that's why the solution was rejected,
both because of cost and security concern.

139
00:08:32,580 --> 00:08:36,700
The second option was managed cloud
logging where things like, managed

140
00:08:36,700 --> 00:08:40,910
search and logging solution will
be hosted on a cloud framework.

141
00:08:41,370 --> 00:08:43,210
Pros was these kind of technologies have.

142
00:08:43,410 --> 00:08:46,210
But have matured enough,
they are scalable.

143
00:08:46,540 --> 00:08:50,360
But again, even for this option
at our scale, the operational

144
00:08:50,360 --> 00:08:52,160
cost was going to be very high.

145
00:08:52,250 --> 00:08:58,471
And we looked at few solutions and we felt
UX was still not where we wanted it to be.

146
00:08:59,335 --> 00:09:02,825
The third option was self-hosted
enterprise, where, we will host

147
00:09:02,825 --> 00:09:08,220
some enterprise grade log management
system on our premises pros where,

148
00:09:08,270 --> 00:09:11,920
you get all the rich feature set,
you get very robust vendor support.

149
00:09:12,400 --> 00:09:15,920
But the cons was the licensing was
still going to be very expensive.

150
00:09:16,350 --> 00:09:19,290
And it would still need to
run on our infrastructure.

151
00:09:19,290 --> 00:09:22,680
So that cost is there plus
overhead of maintaining it.

152
00:09:23,040 --> 00:09:25,020
So that's why we rejected this as well.

153
00:09:25,410 --> 00:09:29,070
The last option was building our own
logging solution just build a custom

154
00:09:29,070 --> 00:09:31,470
developed solution, which fits our need.

155
00:09:31,880 --> 00:09:34,860
Obviously this was the most
exciting option, but this

156
00:09:34,860 --> 00:09:36,150
is not our core competency.

157
00:09:36,150 --> 00:09:40,670
And we realized that it's the, the
observability feel like the vendor

158
00:09:40,670 --> 00:09:46,430
over there are so mature that it is
better to go with some established

159
00:09:46,430 --> 00:09:48,070
solution than, reinventing the meal.

160
00:09:50,230 --> 00:09:51,870
Now let's focus on Grafana.

161
00:09:51,960 --> 00:09:53,860
Loki being open source.

162
00:09:53,910 --> 00:09:54,720
Loki offers a low.

163
00:09:55,320 --> 00:09:59,340
Total cost of ownership compared
to other proprietary solutions

164
00:09:59,710 --> 00:10:02,740
which made it an attractive
option from a budget perspective.

165
00:10:03,650 --> 00:10:08,860
It's optimized for Dropbox Scale, log
ingestion and querying, which means it

166
00:10:08,860 --> 00:10:13,940
can handle our massive volumes of logs we
generate without compromising on speed.

167
00:10:14,790 --> 00:10:18,990
Loki provides a native unified
observability interface through Grafana.

168
00:10:19,650 --> 00:10:23,210
And that was one of the major factors.

169
00:10:23,900 --> 00:10:28,690
Gr loki's structure is architecture
is pretty distributed, so it is

170
00:10:28,690 --> 00:10:31,070
designed to, grow alongside your needs.

171
00:10:31,635 --> 00:10:35,465
Ensuring that, as data volumes
increase our observability

172
00:10:35,465 --> 00:10:37,615
capability would also remain robust.

173
00:10:38,005 --> 00:10:42,740
So overall Grafana, lowkey ticked all
the boxes by combining performance,

174
00:10:42,740 --> 00:10:47,460
cost, efficiency, ease of use and
scalability making it an ideal

175
00:10:47,460 --> 00:10:48,750
choice for our logging needs.

176
00:10:49,750 --> 00:10:50,680
What is Loki?

177
00:10:50,880 --> 00:10:55,080
Loki is an open source project,
meaning it's free to use, inspect and

178
00:10:55,080 --> 00:11:00,000
contribute to, and this has led to, a
very strong community engagement and

179
00:11:00,060 --> 00:11:04,340
ra, rapid iteration, helping it to
evolve into a production grade solution.

180
00:11:04,340 --> 00:11:04,640
Actually.

181
00:11:05,610 --> 00:11:09,230
It is built to scale out across
multiple nodes ensuring that it can

182
00:11:09,230 --> 00:11:11,510
handle increasing volumes of log data.

183
00:11:12,240 --> 00:11:16,510
Lokey is designed with high availability
in mind, so it remains, resilient

184
00:11:16,510 --> 00:11:18,310
even if individual components fail.

185
00:11:18,310 --> 00:11:23,895
I. The system supports multi-tenancy
you, you can securely isolate log

186
00:11:23,895 --> 00:11:25,905
data for different teams or services.

187
00:11:26,365 --> 00:11:29,675
This was critical for our
diverse production environment.

188
00:11:30,525 --> 00:11:35,375
Loki draws inspiration from Prometheus
and it leverages a similar approach to

189
00:11:35,375 --> 00:11:39,275
metrics collection and querying, which
makes it familiar to many of our teams.

190
00:11:39,760 --> 00:11:43,260
Because our internal metrics
system is also quite similar

191
00:11:43,260 --> 00:11:46,330
to to Proteus added score.

192
00:11:46,330 --> 00:11:50,100
Lokey is a log aggregation
system that efficiently, collects

193
00:11:50,100 --> 00:11:54,120
indexes and stores log data while
minimizing resource overhead.

194
00:11:56,565 --> 00:12:00,735
Now that we have talked about why
Lowkey exists and how it differs from

195
00:12:00,735 --> 00:12:04,485
traditional logging solutions, let's
dive into how it works At a high level.

196
00:12:05,175 --> 00:12:09,545
The first thing you need to get you need
to get logs into Lowkey is an agent.

197
00:12:10,025 --> 00:12:11,855
For that, we have prom tail.

198
00:12:12,705 --> 00:12:16,425
One important distinction to make
here is that unlike Prometheus.

199
00:12:16,995 --> 00:12:18,795
Loki does not use a pull model.

200
00:12:18,925 --> 00:12:22,375
Instead of scraping
logs, Loki pushes them.

201
00:12:23,065 --> 00:12:25,225
This is one area where Loki
is different from Prometheus.

202
00:12:26,105 --> 00:12:29,025
So why not follow the same
pull based model as Prometheus?

203
00:12:29,445 --> 00:12:30,465
The answer is simple.

204
00:12:30,465 --> 00:12:34,755
Logs are fundamentally different from
metrics logs has generated at very high

205
00:12:34,755 --> 00:12:38,695
volume and need to be streamed efficiently
rather than pulled periodically.

206
00:12:40,505 --> 00:12:43,415
Before we dive into the technical
details let's start with the

207
00:12:43,415 --> 00:12:45,545
fundamental reason why Lowkey exists.

208
00:12:46,270 --> 00:12:49,130
Many existing log logging
solutions are difficult to

209
00:12:49,130 --> 00:12:51,140
operate and expensive to scale.

210
00:12:51,990 --> 00:12:55,380
Loki was built to solve these
problems by being very simple to

211
00:12:55,380 --> 00:12:57,180
operate and very cost efficient.

212
00:12:57,520 --> 00:13:02,020
One of the key design decisions
that sets Loki apart is that it

213
00:13:02,020 --> 00:13:04,520
does not perform full text indexing.

214
00:13:05,030 --> 00:13:08,600
Instead, it only indexes
metadata about each line.

215
00:13:09,465 --> 00:13:10,635
So why does this matter?

216
00:13:11,005 --> 00:13:15,055
Because in most logging solutions the
biggest cost driver is maintaining

217
00:13:15,055 --> 00:13:20,995
a massive index for full text search
indexing every single word from every

218
00:13:20,995 --> 00:13:25,975
log entry means you need gigabytes,
sometimes terabytes of index storage,

219
00:13:26,485 --> 00:13:30,115
leading to a very high memory
users and expensive infrastructure.

220
00:13:30,830 --> 00:13:32,330
Loki takes a different approach.

221
00:13:32,750 --> 00:13:37,960
It focuses on efficient metadata
indexing instead instead of, full

222
00:13:37,960 --> 00:13:42,010
text indexing, making it much
more lightweight and scalable.

223
00:13:44,050 --> 00:13:47,680
Now let's break down how
logs are structured in Loki.

224
00:13:48,580 --> 00:13:52,300
Every log entry in Loki consists
of three main component,

225
00:13:52,480 --> 00:13:55,090
timestamp, label, and con content.

226
00:13:55,750 --> 00:13:59,680
A timestamp provides precise time
information for when the log was

227
00:13:59,680 --> 00:14:02,020
generated down to the nanoseconds.

228
00:14:02,860 --> 00:14:06,585
Labels are key value pairs that
serve as the metadata index.

229
00:14:07,350 --> 00:14:10,250
Helping Lokey locate logs efficiently.

230
00:14:11,300 --> 00:14:15,680
Content is the actual log message
itself, which is not indexed, but

231
00:14:15,680 --> 00:14:17,480
stored efficiently for retrieval.

232
00:14:18,150 --> 00:14:22,830
By keeping the index small and focusing
only on labels, lokey allows for

233
00:14:22,830 --> 00:14:26,820
faster ingestion and querying with
minimal infrastructure overhead.

234
00:14:27,550 --> 00:14:31,120
This architecture is what makes
Loki highly scalable compared to

235
00:14:31,120 --> 00:14:32,620
traditional logging solutions.

236
00:14:34,510 --> 00:14:37,600
One of the core concepts
in Loki is the log stream.

237
00:14:38,410 --> 00:14:42,100
A log stream is simply a
collection of log entries that

238
00:14:42,100 --> 00:14:44,020
share the exact same level set.

239
00:14:44,620 --> 00:14:48,640
This means that the logs generated
from the same application on

240
00:14:48,640 --> 00:14:52,000
the same server with the same
metadata are grouped together.

241
00:14:52,630 --> 00:14:55,990
For example let's look at
those these log entries.

242
00:14:56,280 --> 00:15:01,320
So logs from dummy service running
on example node one form OneStream,

243
00:15:01,860 --> 00:15:03,600
and logs from dummy service.

244
00:15:03,930 --> 00:15:07,050
Running on example, no two
form a different stream.

245
00:15:07,720 --> 00:15:08,710
By structuring logs.

246
00:15:08,710 --> 00:15:13,570
This way we ensure that the log remains
organized, queryable, and scalable without

247
00:15:13,570 --> 00:15:15,070
the overhead of full text indexing.

248
00:15:16,720 --> 00:15:21,220
Now let's talk about how low
key stores logs efficiently.

249
00:15:21,760 --> 00:15:25,810
Logs in low Loki are grouped into
chunks based on their label set.

250
00:15:26,230 --> 00:15:27,880
Here is how this approach works.

251
00:15:28,330 --> 00:15:31,000
Each log stream is stored
in separate chunks.

252
00:15:32,035 --> 00:15:34,405
These chunks are sorted
in timestamp order.

253
00:15:35,215 --> 00:15:41,185
A chunk continues collecting logs until it
reaches a target size or a timeout occur.

254
00:15:41,965 --> 00:15:46,135
Once a chunk is full, it is compressed
and then flushed to an object store

255
00:15:46,195 --> 00:15:52,855
like AWS S3, Google Cloud Storage Azure
blobs, or even a local file system.

256
00:15:53,785 --> 00:15:57,595
This chunk based approach ensures
that logs are stored in the most

257
00:15:57,595 --> 00:15:59,095
cost effective way possible.

258
00:15:59,725 --> 00:16:03,745
Instead of relying on expensive
databases, lowkey leverages scalable

259
00:16:03,805 --> 00:16:08,605
object storage to reduce cost and
improve re improve retrieval efficiency.

260
00:16:11,275 --> 00:16:17,395
So how do we retrieve logs efficiently
in Loki when a query is executed?

261
00:16:18,235 --> 00:16:22,405
Loki first looks at the metadata
index to determine which chunks

262
00:16:22,405 --> 00:16:23,875
contain the relevant logs.

263
00:16:24,805 --> 00:16:31,145
For example if we query service
equals dummy service and node

264
00:16:31,145 --> 00:16:33,805
id equals example node one.

265
00:16:34,630 --> 00:16:39,730
For logs between timestamp T five
and T seven looking only fetches the

266
00:16:39,730 --> 00:16:42,580
chunks that fall within the time range.

267
00:16:42,670 --> 00:16:46,600
It does not need to scan through
all the logs, just the ones

268
00:16:46,600 --> 00:16:48,250
that match the query criteria.

269
00:16:48,880 --> 00:16:53,320
This makes queries fast, efficient,
and scalable even when dealing

270
00:16:53,320 --> 00:16:55,690
with massive amount of log data.

271
00:16:58,270 --> 00:17:02,810
Now what if we need to query
logs across multiple streams?

272
00:17:03,425 --> 00:17:08,075
Let's say we want logs from both
example node one and example node two.

273
00:17:09,245 --> 00:17:14,405
In that case, Loki will use the index to
determine which chunk match our query,

274
00:17:15,155 --> 00:17:17,255
retrieve only the necessary chunk.

275
00:17:17,345 --> 00:17:22,245
Instead of scanning all the logs
then it'll combine the result and

276
00:17:22,245 --> 00:17:24,045
display them in tools like Grafana.

277
00:17:25,095 --> 00:17:29,395
This query efficiency ensures that
even large scale logging systems.

278
00:17:29,695 --> 00:17:32,695
Remain performant in addition to Grafana.

279
00:17:32,755 --> 00:17:37,905
Loki provides command line utilities
like log CLI, which allow developers to

280
00:17:37,905 --> 00:17:40,365
interact with logs from the terminal.

281
00:17:41,025 --> 00:17:44,285
And you, it can be integrated
with other scripts and workflows.

282
00:17:45,250 --> 00:17:49,120
By leveraging stream based querying
and avoiding full text indexing,

283
00:17:49,510 --> 00:17:53,300
Loki provides a scalable and
cost effective way to store logs

284
00:17:55,760 --> 00:17:59,090
when working with Loki,
label selection is critical.

285
00:17:59,630 --> 00:18:06,390
High cardinality labels like trace id,
user ID, or, dynamic path values can

286
00:18:06,390 --> 00:18:08,580
explore the number of unique log streams.

287
00:18:09,220 --> 00:18:11,380
Hurting performance and increasing cost.

288
00:18:11,800 --> 00:18:18,090
Instead we should favor low cardinality
label labels, like cluster app or

289
00:18:18,090 --> 00:18:20,340
file name to keep things efficient.

290
00:18:20,940 --> 00:18:26,600
For example even combining just three
labels, like log level, status and

291
00:18:26,900 --> 00:18:29,420
path with a few possible values.

292
00:18:29,700 --> 00:18:34,800
It can easily create 36 unique
streams in this case, four into three.

293
00:18:35,260 --> 00:18:37,150
So it adds up pretty fast.

294
00:18:38,150 --> 00:18:41,630
Now let's talk about how
we deploy Loki at Dropbox.

295
00:18:42,800 --> 00:18:47,610
But first let's take a look at a
simplified diagram of Loki's architecture.

296
00:18:48,750 --> 00:18:53,065
At the top, we have the main components
of the right and read paths in yellow.

297
00:18:54,290 --> 00:18:58,760
On the bottom we have an object
storage S3 for storing logs in green.

298
00:18:59,360 --> 00:19:04,010
And on the right side we have some
shared components like the memc clusters

299
00:19:04,010 --> 00:19:10,470
in blue and cluster wide services
like compactor and et CD in orange.

300
00:19:11,220 --> 00:19:15,240
Starting with the right path, logs
are written to the distributors.

301
00:19:15,935 --> 00:19:19,045
Which hash the log
streams log stream labels.

302
00:19:19,685 --> 00:19:23,195
Then they look up the hash ring
in et CD and route them to the

303
00:19:23,195 --> 00:19:25,385
inges that own their hash range.

304
00:19:26,495 --> 00:19:31,015
The ingestors buffer the log chunks in
memory, then eventually flush the logs

305
00:19:31,015 --> 00:19:33,895
to the object storage in the background.

306
00:19:33,945 --> 00:19:38,565
The com the compactor merges
the log indexes, index files in

307
00:19:38,565 --> 00:19:40,795
object storage for faster queries.

308
00:19:41,795 --> 00:19:46,625
For the read path, the query front
end receives a query and splits

309
00:19:46,625 --> 00:19:49,865
it up into subqueries, which are
processed by different queries.

310
00:19:51,170 --> 00:19:55,895
The queries fetch recent logs from the
cache in the ING adjusters, or it fetches

311
00:19:55,895 --> 00:19:58,205
the older logs from the object storage

312
00:19:59,205 --> 00:20:01,875
for a sense of the scale
of our low key deployment.

313
00:20:01,875 --> 00:20:03,435
Here are some top level metrics.

314
00:20:04,155 --> 00:20:08,745
We are processing on the order of 10
gigabytes of logs per second, and we

315
00:20:08,745 --> 00:20:14,525
store these logs for 30 days, which takes
up about 10 petabytes in object storage.

316
00:20:16,205 --> 00:20:16,685
Excuse me.

317
00:20:17,045 --> 00:20:19,055
We have around 1000 tenants.

318
00:20:19,330 --> 00:20:22,920
And our users are making less
than one query per second.

319
00:20:23,190 --> 00:20:28,780
So as you can see, the system is heavily
que skewed towards right over leads.

320
00:20:29,780 --> 00:20:35,430
With Dropbox being a storage company, one
thing we did is use our internal object

321
00:20:35,430 --> 00:20:37,950
storage as a drop in replacement for S3.

322
00:20:38,915 --> 00:20:43,505
This has led to cost savings,
especially the data transfer costs.

323
00:20:44,135 --> 00:20:48,425
With these lower costs, now we are
able to increase our log retention

324
00:20:48,425 --> 00:20:50,685
from one week to four weeks.

325
00:20:51,475 --> 00:20:55,375
One other big difference between
S3 and our internal storage is some

326
00:20:55,735 --> 00:20:56,965
per performance characteristic.

327
00:20:57,740 --> 00:21:03,085
We we noticed that S3 would gradually
scale out to handle large reads.

328
00:21:03,295 --> 00:21:08,165
And because most queries are scanning
many logs, this would lead to timeouts.

329
00:21:08,645 --> 00:21:12,660
Whereas our internal systems
they have reserve capacity and we

330
00:21:12,660 --> 00:21:16,830
also have emergency capacities,
so we don't have the same issue.

331
00:21:17,530 --> 00:21:20,820
Another thing is because the
index files are so large we

332
00:21:20,820 --> 00:21:22,470
still have to write them to S3.

333
00:21:23,470 --> 00:21:28,150
Lowkey supports isolating the access
and storage of logs by tenant.

334
00:21:28,790 --> 00:21:32,630
At Dropbox, a tenant is a service,
which is a group of projects.

335
00:21:33,120 --> 00:21:37,380
Because the logs are stored by tenant,
there are performance implications.

336
00:21:37,750 --> 00:21:42,120
For one service, we had to use
project as the tenant instead of

337
00:21:42,120 --> 00:21:46,290
service, because that service just
produces too much of, too much logs.

338
00:21:47,290 --> 00:21:47,770
Excuse me.

339
00:21:48,150 --> 00:21:52,650
Before Loki, engineers had to
use production SSH access just

340
00:21:52,650 --> 00:21:56,980
to view logs which were present
on, on the service house.

341
00:21:57,560 --> 00:22:01,340
With Loki, we red, we have
redefined the model so that each

342
00:22:01,340 --> 00:22:03,380
service acts as its own tenant.

343
00:22:03,980 --> 00:22:06,830
This means we are aligning with
our existing permission model.

344
00:22:07,365 --> 00:22:11,985
What worked for accessing a service
is now used to secure log access.

345
00:22:12,375 --> 00:22:16,695
So we have extended this model by
adding a group permission, enabling

346
00:22:16,695 --> 00:22:20,225
teams to grant access to their
logs to other teams when needed.

347
00:22:20,795 --> 00:22:24,355
And naturally some teams even
have permissions that allow

348
00:22:24,415 --> 00:22:26,005
them to access all logs.

349
00:22:26,665 --> 00:22:30,445
To pull it all together, we
developed a custom query art proxy.

350
00:22:30,605 --> 00:22:33,995
This proxy intercepts each query
to ensure that the user has.

351
00:22:34,325 --> 00:22:38,565
Proper permissions which avoids
the complexity and cost that would

352
00:22:38,565 --> 00:22:43,435
come from using Grafana as native
RAC an approach that would also,

353
00:22:43,435 --> 00:22:48,085
require us a separate data source
per tenant and also would've needed

354
00:22:48,355 --> 00:22:51,065
us to, update to Grafana Enterprise.

355
00:22:53,590 --> 00:22:58,070
One thing that comes up a lot is teams
trying to share logs with other teams.

356
00:22:58,100 --> 00:23:01,430
For example, if Team A wants to
share their logs with Team B,

357
00:23:01,520 --> 00:23:03,890
team B. Must request permission.

358
00:23:04,590 --> 00:23:08,100
But because the permission is owned by
the logging team, only we can approve it.

359
00:23:09,130 --> 00:23:14,020
While there is a incident or a sev
going on, this delay can be costly.

360
00:23:15,320 --> 00:23:19,330
So our solution for that was break
glass which allows any user with

361
00:23:19,360 --> 00:23:23,620
a justified reason to temporarily
gain access to any service logs.

362
00:23:24,130 --> 00:23:27,370
And we keep an audit trail and
use safeguards on the amount

363
00:23:27,370 --> 00:23:28,690
of logs a user can access.

364
00:23:31,025 --> 00:23:35,485
We run Lokey in two data centers in
separate geographical reasons, and

365
00:23:35,495 --> 00:23:39,395
so that in case one data center goes
down, we still have a availability.

366
00:23:39,765 --> 00:23:41,235
At Dropbox we call this multihoming.

367
00:23:42,205 --> 00:23:45,865
Both data centers use the same
object storage and to route logs and

368
00:23:45,865 --> 00:23:47,875
queries to the correct lokey cluster.

369
00:23:48,235 --> 00:23:51,195
We integrate our DNS
servers with load balancers.

370
00:23:51,295 --> 00:23:53,805
And now I'm going to co go
over some of the scaling

371
00:23:53,805 --> 00:23:57,165
challenges we faced by default.

372
00:23:57,315 --> 00:24:00,575
Locate key ingestors store
a right ahead, log on disc.

373
00:24:01,115 --> 00:24:05,725
That way if the inges crashes before
flushing the logs, it can recover them.

374
00:24:06,155 --> 00:24:09,445
At Dropbox, we disable the
right ahead log to prioritize

375
00:24:09,445 --> 00:24:11,035
availability over durability.

376
00:24:13,095 --> 00:24:17,915
Loki supports per tenant rate limits
to avoid the noisy neighbor problem.

377
00:24:18,395 --> 00:24:22,645
So what we did is set conservative
default rate limits, and teams are

378
00:24:22,645 --> 00:24:25,825
notified by an alert when their
service goes over those limits.

379
00:24:26,285 --> 00:24:29,675
Loki supports overriding rate
limits per tenant in a file.

380
00:24:30,140 --> 00:24:33,980
And we distribute those overrides
to Loki in minutes using our

381
00:24:34,030 --> 00:24:35,440
distributed key value store.

382
00:24:35,800 --> 00:24:40,560
And we hot reload these override
files in Loki component that use it.

383
00:24:43,320 --> 00:24:47,730
To scale, read and writes lowkey
uses a hash ring to shard logs.

384
00:24:48,180 --> 00:24:53,630
Each inges owns a range in the hashing
and periodically registers its range

385
00:24:53,690 --> 00:24:58,460
and health status in the ring, which is
stored in a distributed key value store.

386
00:24:59,210 --> 00:25:03,930
The distributor, which receives logs
from applications uses that ring to

387
00:25:03,930 --> 00:25:05,730
route logs to the correct ingestors.

388
00:25:06,675 --> 00:25:09,255
Plus replicate the logs
to other ingestors.

389
00:25:10,795 --> 00:25:12,655
Here is an example of that.

390
00:25:12,865 --> 00:25:17,275
When a log is ingested, the labels
for the logs plus the tenant, which

391
00:25:17,275 --> 00:25:21,715
is usually the service in Dropbox, is
hashed, and then the distributor takes

392
00:25:21,715 --> 00:25:23,695
the hash and does the lookup in the ring.

393
00:25:25,795 --> 00:25:28,915
Originally, we used et CD as
the backing store for the ring.

394
00:25:29,405 --> 00:25:29,525
Et.

395
00:25:29,525 --> 00:25:31,625
CD is a distributed, consistent.

396
00:25:32,015 --> 00:25:36,125
Key value store that's often used
for coordination and configuration

397
00:25:36,125 --> 00:25:37,685
in distributed systems.

398
00:25:38,375 --> 00:25:41,975
It is notable for being the
default for Kubernetes, and it is

399
00:25:41,975 --> 00:25:44,305
also now widely used at Dropbox.

400
00:25:45,685 --> 00:25:49,915
One issue we had with
HCD was right contention.

401
00:25:50,345 --> 00:25:54,305
Each inges would send a heartbeat
every minute and update the ring.

402
00:25:54,605 --> 00:25:58,505
Also, when an inges joins or leaves
the ring, it updates the ring.

403
00:25:59,230 --> 00:26:02,950
At CD stores, the whole ring as
a binary blob in a single key.

404
00:26:03,640 --> 00:26:07,750
And every time the ring is updated,
the whole ring is replaced with

405
00:26:07,750 --> 00:26:12,360
a read plus a compare and swap
operation with a replication factor

406
00:26:12,360 --> 00:26:15,210
of three and 67 ingestors for each.

407
00:26:15,210 --> 00:26:17,700
We had 201 total ingess.

408
00:26:17,955 --> 00:26:21,145
So as you can imagine, there was
a lot of right activity going on.

409
00:26:23,130 --> 00:26:27,890
Some ways the right contention would
manifest its deployments would take hours

410
00:26:27,950 --> 00:26:33,250
because inges were pushed one at a time,
and these pushes would often fail because

411
00:26:33,250 --> 00:26:38,200
an availability alert would fire also,
because CD is a single point of failure.

412
00:26:38,620 --> 00:26:40,450
We had some outages when ET.

413
00:26:40,450 --> 00:26:41,260
CD went down.

414
00:26:42,640 --> 00:26:46,330
Luckily Grafana made member list
the default distributed key value

415
00:26:46,330 --> 00:26:49,090
store for low key and other projects.

416
00:26:49,210 --> 00:26:50,860
So we replaced et CD with it.

417
00:26:51,370 --> 00:26:55,960
Our member list is a peer-to-peer Gus
protocol, so we no longer had to maintain

418
00:26:55,960 --> 00:26:58,330
another component with member list.

419
00:26:58,330 --> 00:27:02,830
The ring updates are propagated as
smaller deltas instead of the whole ring.

420
00:27:03,430 --> 00:27:07,210
However, one downside is,
it is eventually consistent.

421
00:27:07,600 --> 00:27:11,860
So unlike ET CD, which offers strong
consistency, this means we still

422
00:27:11,860 --> 00:27:14,080
have to deploy the injustice slowly.

423
00:27:14,680 --> 00:27:17,740
But the pros are we no longer have
to deal with right contention.

424
00:27:18,280 --> 00:27:21,580
A single point of failure and
honestly, we haven't had any issue

425
00:27:21,580 --> 00:27:23,170
in the last year using member list.

426
00:27:23,620 --> 00:27:27,490
We are also able to double the amount of
ingestors after migrating to member list.

427
00:27:28,800 --> 00:27:33,650
Log indexes determine the query plan,
including how many log chunks needs

428
00:27:33,650 --> 00:27:37,670
to be fetched and scanned so they have
a good deal of impact on performance.

429
00:27:38,360 --> 00:27:42,020
Grafana changed the index format
from Bold DB to TSDB, which

430
00:27:42,020 --> 00:27:43,580
is based on Prometheus TS db.

431
00:27:43,580 --> 00:27:45,370
So it just works better
with the log label.

432
00:27:46,310 --> 00:27:50,930
In summary, our goal was to scale and
improve observability and Rob box.

433
00:27:51,500 --> 00:27:55,910
We are facing challenges with
manual SSH into hosts and dig

434
00:27:55,910 --> 00:27:57,470
through unstructured logs.

435
00:27:57,960 --> 00:28:00,840
It was a slow error prone
and hard to scale process.

436
00:28:01,575 --> 00:28:05,595
We evaluated several solutions and
ultimately chose Grafana, Loki for

437
00:28:05,595 --> 00:28:09,585
its performance, cost efficiency,
and native multi-tenant support.

438
00:28:10,065 --> 00:28:14,085
As a result, we have significantly
improved log retention, lowered

439
00:28:14,085 --> 00:28:19,035
cost, and enabled fast secure
access to logs across teams.

440
00:28:20,035 --> 00:28:21,775
So that's all for my presentation.

441
00:28:22,120 --> 00:28:22,540
Thank you.

