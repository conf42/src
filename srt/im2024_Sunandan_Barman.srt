1
00:00:00,290 --> 00:00:01,119
Hello everyone.

2
00:00:01,480 --> 00:00:02,500
My name is Shraddha.

3
00:00:02,680 --> 00:00:06,220
In this talk, my co speaker and I
will be sharing some practical tips

4
00:00:06,220 --> 00:00:08,120
for breaking into the AI ML space.

5
00:00:08,840 --> 00:00:12,100
This session is perfect for those
who have, who are new to the field

6
00:00:12,799 --> 00:00:17,779
or have recently graduated, are
industry professions who are, who

7
00:00:17,780 --> 00:00:21,770
are planning to transition into a
new role, or anyone interested in

8
00:00:21,770 --> 00:00:23,690
learning more about the AI ML space.

9
00:00:24,270 --> 00:00:28,244
I hope you find, it valuable
and learn something new.

10
00:00:28,944 --> 00:00:33,065
If at any point in the talk, you have
any questions, feel free to reach

11
00:00:33,065 --> 00:00:35,974
out to us on our LinkedIn channels.

12
00:00:36,244 --> 00:00:39,254
You can find the IDs tagged in the slide.

13
00:00:41,635 --> 00:00:44,305
So let's start with high level
objectives for this talk.

14
00:00:44,714 --> 00:00:48,444
We will start by talking about why
debugging is an invaluable skill.

15
00:00:49,224 --> 00:00:54,270
You will clearly stand out if you
can identify and solve Complex

16
00:00:54,290 --> 00:00:55,540
issues that others cannot.

17
00:00:56,660 --> 00:01:01,720
Then we will discuss the gap that exists
between academic ML and practical ML.

18
00:01:02,190 --> 00:01:06,169
This is particularly interesting
because the new graduates are often

19
00:01:06,169 --> 00:01:10,730
surprised when they discover that machine
learning is more than building models.

20
00:01:11,350 --> 00:01:14,490
The scale of model serving and
delivery is not something fully

21
00:01:14,490 --> 00:01:15,800
covered in graduate schools.

22
00:01:16,530 --> 00:01:20,099
Lastly, as promised, we will
share some practical tips and

23
00:01:20,100 --> 00:01:21,730
tricks to break into the space.

24
00:01:22,730 --> 00:01:27,010
Debugging as a scale is valuable
for any engineers in any space.

25
00:01:27,500 --> 00:01:29,640
specifically in AI.

26
00:01:31,660 --> 00:01:35,240
It is definitely an invaluable
skill in today's automation world.

27
00:01:35,990 --> 00:01:39,150
And in this slide, I'm
going to talk about why.

28
00:01:39,530 --> 00:01:41,640
So, first of all, AI is everywhere.

29
00:01:41,890 --> 00:01:44,680
From social media algorithms
to recommendation systems

30
00:01:44,680 --> 00:01:45,969
and even self driving cars.

31
00:01:46,330 --> 00:01:48,760
AI is all around us.

32
00:01:49,119 --> 00:01:53,260
As companies automate more tasks using
ML, the demand for people who can

33
00:01:53,260 --> 00:01:54,990
troubleshoot these systems will go up.

34
00:01:55,520 --> 00:02:00,780
If an AI makes a mistake or isn't working
as expected, it will impact millions and

35
00:02:00,780 --> 00:02:02,880
billions of users all around the world.

36
00:02:03,240 --> 00:02:07,520
So having the ability to spot
and fix the issues is actually

37
00:02:07,520 --> 00:02:09,009
very crucial and valuable.

38
00:02:10,420 --> 00:02:12,730
Secondly, models aren't perfect.

39
00:02:13,510 --> 00:02:19,560
ML models are built on data, and data
can be messy, incomplete, and biased.

40
00:02:19,970 --> 00:02:23,670
Debugging helps identify when
a model isn't learning what

41
00:02:23,690 --> 00:02:25,370
it should, and when it's not.

42
00:02:25,530 --> 00:02:28,580
Making incorrect predictions
due to underlying data issues.

43
00:02:28,850 --> 00:02:32,250
For example, think about a
recommendation system on a streaming

44
00:02:32,270 --> 00:02:34,380
platform like YouTube or Netflix.

45
00:02:34,850 --> 00:02:40,620
If it starts recommending shows and
videos that users don't like because

46
00:02:40,640 --> 00:02:45,209
of a bug in the model, the platform
could lose engagement and customers.

47
00:02:45,540 --> 00:02:50,570
So that directly impacts the
revenue that the platform generates.

48
00:02:50,900 --> 00:02:54,120
And ultimately revenue is everything.

49
00:02:54,129 --> 00:02:54,189
Thank you.

50
00:02:54,545 --> 00:02:58,075
Right, debugging the system keeps
it running smoothly and make

51
00:02:58,075 --> 00:03:04,125
sure that we have the engagement
that we need from the user.

52
00:03:05,335 --> 00:03:07,915
thirdly, keep the systems
running efficiently.

53
00:03:08,225 --> 00:03:10,794
So as automation grows,
performance matters.

54
00:03:11,094 --> 00:03:14,201
There is a direct resource and
monetary cost involved with

55
00:03:14,201 --> 00:03:15,777
serving these heavy models.

56
00:03:15,777 --> 00:03:21,655
A well debugged system runs faster,
uses less resources, and scales better.

57
00:03:22,145 --> 00:03:27,165
So if you can debug and optimize
a model, you can save company a

58
00:03:27,405 --> 00:03:31,695
lot of money and time by improving
performance and reducing the downtime.

59
00:03:34,465 --> 00:03:37,785
So continuing from the last
slide, my fourth point is it's

60
00:03:37,795 --> 00:03:39,655
all about continuous improvement.

61
00:03:40,125 --> 00:03:42,015
ML models evolve over the time.

62
00:03:42,265 --> 00:03:44,345
They need to be retrained with new data.

63
00:03:44,375 --> 00:03:47,975
And as, as that happens, new
bugs and issues can emerge.

64
00:03:48,465 --> 00:03:50,835
Think of it like tuning a car engine.

65
00:03:51,305 --> 00:03:56,335
Even after the initial build, you,
you need to keep making adjustments

66
00:03:56,365 --> 00:03:59,795
to make sure it runs smoothly,
especially as conditions changed.

67
00:04:00,705 --> 00:04:03,745
Like, and the conditions do
keep changing all the time.

68
00:04:05,245 --> 00:04:08,965
Next point is we want to prevent
big mistakes from happening.

69
00:04:09,275 --> 00:04:13,105
So automation means that systems
are often making decisions

70
00:04:13,105 --> 00:04:14,175
without human intervention.

71
00:04:14,810 --> 00:04:18,820
If an AI system makes a mistake, then
the consequences can be large scale.

72
00:04:19,200 --> 00:04:22,700
For example, think of AI used in finance.

73
00:04:22,940 --> 00:04:28,240
if it incorrectly starts flagging
transactions as fraudulent, this can

74
00:04:28,270 --> 00:04:31,280
cause chaos to all the customers.

75
00:04:31,540 --> 00:04:35,910
So debugging ensures that such
critical systems make accurate

76
00:04:35,960 --> 00:04:37,860
decisions by minimizing the error.

77
00:04:39,040 --> 00:04:42,150
And last point is, it's a
rare and a high demand skill.

78
00:04:42,150 --> 00:04:46,920
So debugging ML and AI models
is a highly specialized skill.

79
00:04:47,300 --> 00:04:51,150
As automation continues to grow,
companies will need experts who can

80
00:04:51,150 --> 00:04:55,770
dive into these complex models and data
pipelines to find and fix these issues.

81
00:04:57,575 --> 00:05:02,275
Overall, in an age where automation
is taking over, having the ability

82
00:05:02,275 --> 00:05:05,375
to debug these complex models
will become like a superpower.

83
00:05:05,695 --> 00:05:11,375
It's a skill that's high in demand already
and can open a lot of, can open up a lot

84
00:05:11,375 --> 00:05:13,165
of career opportunities in the tech world.

85
00:05:15,915 --> 00:05:17,475
Let's move on to the next topic.

86
00:05:17,555 --> 00:05:21,475
We will talk about the gaps that exist
between academic ML and practical ML.

87
00:05:24,115 --> 00:05:26,085
Let's take a pause and
look at these numbers.

88
00:05:26,395 --> 00:05:27,725
These numbers are mind blowing.

89
00:05:29,115 --> 00:05:33,505
Netflix, over 277 million
daily active users worldwide.

90
00:05:34,485 --> 00:05:37,325
YouTube, 500 million daily active users.

91
00:05:37,635 --> 00:05:43,875
Instagram, 2 billion monthly active
users with 500 million daily users

92
00:05:43,895 --> 00:05:46,735
engaging with the app on a daily basis.

93
00:05:47,755 --> 00:05:48,605
Think about the scale.

94
00:05:49,105 --> 00:05:54,175
Each request Mind you, each
request here is actually a user

95
00:05:54,225 --> 00:05:57,955
asking for a customized feed, a
personalized set of recommendations

96
00:05:57,965 --> 00:05:59,375
made specifically for them.

97
00:05:59,855 --> 00:06:02,565
But this customization isn't magic.

98
00:06:02,825 --> 00:06:06,575
It happens by ranking millions
and billions of options available

99
00:06:06,575 --> 00:06:08,235
in the dataset for a given user.

100
00:06:08,740 --> 00:06:12,170
Think about the number of
combinations out of a million options.

101
00:06:12,210 --> 00:06:16,300
How do you pick and choose the top
10 most suitable for a given user?

102
00:06:16,760 --> 00:06:23,120
How do you pick such that user actually
ends up making Taking an action that

103
00:06:23,120 --> 00:06:30,920
will that we want them to take now
Compare these numbers With data sets

104
00:06:30,920 --> 00:06:32,780
that, that are used in graduate schools.

105
00:06:33,110 --> 00:06:36,470
top of my mind, the most popular
data sets that I can remember,

106
00:06:36,620 --> 00:06:40,620
at least the ones that I used
during my time was, Iris Dataset.

107
00:06:40,680 --> 00:06:43,550
Iris, had about one 50 samples.

108
00:06:43,860 --> 00:06:47,910
it consisted of Iris flowers
from three different pieces.

109
00:06:48,250 --> 00:06:51,930
and second one was, . It
contained, grayscale images

110
00:06:51,930 --> 00:06:53,300
of handwritten digits, from s.

111
00:06:53,585 --> 00:07:01,465
It had about 60, 000 training samples and
each image was a 28 by 28 pixel in size.

112
00:07:01,875 --> 00:07:09,995
so we are talking about 60, 70, 000
samples versus millions of daily

113
00:07:09,995 --> 00:07:12,405
active users and billions of requests.

114
00:07:13,360 --> 00:07:14,510
on hourly basis.

115
00:07:14,510 --> 00:07:18,070
So the scale is mind blowing.

116
00:07:21,010 --> 00:07:25,230
So next we'll talk about the heavy duty
models that work behind the scenes.

117
00:07:25,820 --> 00:07:30,930
so ranking is done using complex
models, and it's, they have a very

118
00:07:30,930 --> 00:07:34,790
complicated architecture and they
are trained on a recurring basis.

119
00:07:34,800 --> 00:07:40,055
So the recurring schedule can be hourly,
can be daily, or can be custom, like It

120
00:07:40,055 --> 00:07:44,165
can be trained on every six hours, so
it depends on the type of model, the,

121
00:07:44,455 --> 00:07:51,725
the, the availability of data and how
our entire data pipelines are set up.

122
00:07:52,225 --> 00:07:54,255
So it depends on all these parameters.

123
00:07:54,625 --> 00:07:59,425
so every time a model trains, it
doesn't start training from scratch.

124
00:07:59,605 --> 00:08:03,545
Instead they begin, begin, training
from the last snapshots and

125
00:08:03,785 --> 00:08:06,795
learn new weights based on the
latest trends and data patterns.

126
00:08:07,205 --> 00:08:12,925
so, BERT, as you can see on your
screen, BERT, GPT 3, T5, these are

127
00:08:12,935 --> 00:08:18,694
all tech summarization, chatbot,
recommendation systems, and, These

128
00:08:18,695 --> 00:08:22,915
models are used for all these use
cases, statbots, recommendation

129
00:08:22,915 --> 00:08:24,535
systems, and sentiment analysis.

130
00:08:25,465 --> 00:08:29,175
And each of them have about
billions of parameters.

131
00:08:29,175 --> 00:08:33,825
And ResNet and YOLO, they are used
for image classification and object

132
00:08:33,825 --> 00:08:35,515
detection in computer vision tasks.

133
00:08:36,245 --> 00:08:40,145
YOLO is, actually used for real time
object detection in video streams.

134
00:08:40,485 --> 00:08:44,705
all of them have, very specific
use cases and, they have complex

135
00:08:44,705 --> 00:08:47,605
model architectures and have
billions and millions of parameters.

136
00:08:47,955 --> 00:08:53,415
What these parameters mean, each parameter
is actually a weight that needs to be

137
00:08:53,425 --> 00:08:55,805
updated every time you retrain a model.

138
00:08:56,395 --> 00:09:03,130
So after every time new data is comes into
the, new data comes into the pipeline.

139
00:09:03,770 --> 00:09:05,950
You start the training.

140
00:09:06,030 --> 00:09:07,800
It triggers a new training run.

141
00:09:08,020 --> 00:09:12,940
These model updates all, all these
billions and millions of parameters.

142
00:09:13,240 --> 00:09:17,160
and these process, this process involves
calculating and optimizing weights.

143
00:09:17,560 --> 00:09:21,204
and then, all these parameters
are learned during the practice.

144
00:09:21,365 --> 00:09:23,565
back propagation process of the training.

145
00:09:24,565 --> 00:09:27,825
Compare this with, with your,
with your graduate school models.

146
00:09:28,095 --> 00:09:30,585
The parameters are not, in, in billions.

147
00:09:30,985 --> 00:09:32,205
That's what I'm trying to say.

148
00:09:32,205 --> 00:09:35,065
It's the scale that,
that, that matters here.

149
00:09:37,905 --> 00:09:42,665
So next I want to highlight the data
pipeline, privacy and sensitivity

150
00:09:42,665 --> 00:09:46,485
filtering that takes place, in
the training, data pipelines.

151
00:09:46,795 --> 00:09:53,425
so unlike in university, where, where
you have a data set and you directly

152
00:09:53,425 --> 00:09:57,895
train your model, you perform all
kinds of data processing on it and

153
00:09:57,895 --> 00:10:01,385
augmentation, data augmentation, data
processing, pre processing on it.

154
00:10:01,805 --> 00:10:07,345
But it's not like you, you don't
filter out the data based on

155
00:10:07,375 --> 00:10:09,035
privacy and sensitive information.

156
00:10:09,045 --> 00:10:09,365
Right.

157
00:10:09,575 --> 00:10:14,495
But you already have a data set available
and you directly use it for your use case.

158
00:10:14,855 --> 00:10:22,795
But unlike, unlike that, unlike in
school, Production actually has a lot

159
00:10:22,795 --> 00:10:27,715
of rules which changes from country
to country and region to region.

160
00:10:28,255 --> 00:10:32,445
We have privacy and sensitivity,
sensitive data filtering, phase where

161
00:10:32,685 --> 00:10:37,965
you have to, you have to, filter out
data based on the, specific rules

162
00:10:37,995 --> 00:10:39,965
in that region or in that country.

163
00:10:40,255 --> 00:10:46,095
and these laws, are, developed by
individual countries or, or unions.

164
00:10:46,325 --> 00:10:50,715
So for example, GDPR is one of the, is
considered one of the strictest and the

165
00:10:50,715 --> 00:10:53,035
most comprehensive privacy laws globally.

166
00:10:53,325 --> 00:10:59,925
It was devised by the European union and,
is, It protects the European residents,

167
00:11:00,115 --> 00:11:04,914
European residents, and because it
mandates user consent for data processing.

168
00:11:05,334 --> 00:11:09,094
It gives users the right to data
access and deletions and imposes

169
00:11:09,104 --> 00:11:11,214
heavy fines for noncompliance.

170
00:11:11,664 --> 00:11:15,564
in fact, other, other countries and
regions like the ones that are listed in

171
00:11:15,564 --> 00:11:23,074
the table here, CCPA Bill C 27 by Canada,
LGBT, by Brazil, Australia's Privacy

172
00:11:23,074 --> 00:11:28,894
Act reform, all of them have, in some
sense, been, inspired by the GDPR law,

173
00:11:29,214 --> 00:11:35,064
GDPR took the lead, and, GDPR is like
the first of its kind, that had come into

174
00:11:35,324 --> 00:11:40,554
the picture, and then other, California,
Canada, Brazil, Australia, they did, and

175
00:11:40,554 --> 00:11:44,679
there are many other countries who have
followed the, the, the And they have

176
00:11:44,689 --> 00:11:49,189
come up with their own, laws to protect
their residents, so that they can, the

177
00:11:49,189 --> 00:11:53,219
residents have more control over what
data is being collected about them.

178
00:11:53,549 --> 00:11:55,979
They can delete their data
and they can opt out of it.

179
00:11:58,419 --> 00:12:03,949
So next, so far we have talked about,
What, what, what model is, what, what kind

180
00:12:03,949 --> 00:12:08,979
of parameters the models are trained on in
the industry, what kind of data filtering

181
00:12:08,989 --> 00:12:12,209
happens and why debugging it is complex.

182
00:12:12,489 --> 00:12:18,439
so now let's talk about how, what happens
once the data is filters, data is ready,

183
00:12:18,459 --> 00:12:24,829
the, and the, and you have, You have
fresh models, like you have, you have

184
00:12:24,829 --> 00:12:26,309
trained your models on the clean data.

185
00:12:26,859 --> 00:12:27,869
Now what happens?

186
00:12:28,099 --> 00:12:33,509
So I think in the graduate school,
that's when your task ends, right?

187
00:12:33,609 --> 00:12:37,699
Like you have your model, which is
trained, which is giving out prediction.

188
00:12:37,699 --> 00:12:42,269
You look at the confusion matrix where you
look at the true positive, false negative.

189
00:12:42,479 --> 00:12:45,889
And everything else, and you look
at the recall, you look at the

190
00:12:45,889 --> 00:12:49,149
precision metrics, and you decide
if the model is good enough.

191
00:12:49,529 --> 00:12:57,859
But, this is not where, this is, this is
just a start of, the delivery process in

192
00:12:57,859 --> 00:13:02,139
the, after the model training finishes,
that's the start of the delivery process.

193
00:13:02,619 --> 00:13:06,059
so model will train a
prediction value for each item.

194
00:13:06,079 --> 00:13:09,939
For example, a video or post a
product, it'll predict what kind

195
00:13:09,939 --> 00:13:15,119
of engagement can this, video get
if it is shown to a given user.

196
00:13:15,479 --> 00:13:17,389
but what happens after that?

197
00:13:17,659 --> 00:13:21,539
So there are other business
rules that apply after the

198
00:13:21,539 --> 00:13:23,739
prediction value, is put out.

199
00:13:25,069 --> 00:13:29,959
So for example, on YouTube, you cannot
have more than two videos From the

200
00:13:29,959 --> 00:13:31,889
same channel in a user's timeline.

201
00:13:32,759 --> 00:13:38,009
So don't quote me on the two videos
part, but I'm sure there is, there is

202
00:13:38,049 --> 00:13:42,379
definitely some sort of restriction on
how many videos from us from the same

203
00:13:42,379 --> 00:13:44,079
channel can be shown on user's timeline.

204
00:13:44,729 --> 00:13:49,849
next, you have ads, which are shown
alongside the organic YouTube content.

205
00:13:50,139 --> 00:13:56,019
So ads is, or like a whole different
space where advertisers are, putting

206
00:13:56,019 --> 00:14:00,249
money so that these, these ads,
these videos, these advertisements

207
00:14:00,249 --> 00:14:04,979
can be shown alongside the organic
content, coming out of other channels.

208
00:14:05,259 --> 00:14:11,639
So, how do you, how do you create
a timeline where advertiser

209
00:14:11,659 --> 00:14:14,959
can exhaust their, advertiser
budget is also exhausted.

210
00:14:15,229 --> 00:14:18,749
Advertiser gets, gets, what they want.

211
00:14:18,759 --> 00:14:21,239
Like they get the
engagement on their website.

212
00:14:21,279 --> 00:14:21,829
Whatever.

213
00:14:22,149 --> 00:14:26,079
Whatever they are investing in, we
give them enough return on investment.

214
00:14:26,479 --> 00:14:30,669
And at the same time, we want the
users to also engage organically

215
00:14:30,669 --> 00:14:35,179
on the platform and also like the
content that is created by other

216
00:14:35,199 --> 00:14:37,289
creators and not just advertisements.

217
00:14:37,539 --> 00:14:42,289
So users are usually, no one likes
advertisements, but we want to

218
00:14:42,289 --> 00:14:47,669
make sure that advertisements can
get enough engagement as well as.

219
00:14:47,974 --> 00:14:52,434
the organic videos can also
keep the users engaged.

220
00:14:52,604 --> 00:14:54,484
So how do you create a timeline?

221
00:14:54,714 --> 00:14:58,754
How do you diversify the
timeline such that, not the

222
00:14:58,754 --> 00:15:00,204
user does not feel overwhelmed.

223
00:15:00,584 --> 00:15:02,994
and at the same time you
want to diversify, right?

224
00:15:03,144 --> 00:15:05,564
Like not all videos should
be on the same topic.

225
00:15:05,994 --> 00:15:07,604
You want to diversify.

226
00:15:07,604 --> 00:15:09,854
You want to exhaust advertisers budget.

227
00:15:09,884 --> 00:15:10,404
You want to diversify.

228
00:15:10,414 --> 00:15:13,164
Don't want to show more than certain
number of videos from the same

229
00:15:13,164 --> 00:15:14,374
channel and use this timeline.

230
00:15:14,374 --> 00:15:18,064
And there are so many other rules
that business rules that come

231
00:15:18,064 --> 00:15:21,404
into picture when it comes to
ranking and showing the timeline.

232
00:15:22,404 --> 00:15:26,014
Next, I want to talk about continuous
monitoring and real world validation.

233
00:15:26,334 --> 00:15:30,104
So unlike in schools where you
measure accuracy using simple test

234
00:15:30,104 --> 00:15:33,844
data sets, real time performance
validation is, not possible.

235
00:15:34,284 --> 00:15:40,724
So you, even if your model predicts,
Some prediction value you will you can't

236
00:15:40,724 --> 00:15:44,424
say with confidence if it is true or
not Let's say it it predicts with high

237
00:15:44,424 --> 00:15:48,484
confidence that this user if he sees
this video He will click on it and he

238
00:15:48,484 --> 00:15:55,674
will watch this video fully even if your
model is saying it You can't fully know

239
00:15:55,674 --> 00:16:02,604
if it is true, if it is accurate, unless,
unless you, you, in, in an ideal world,

240
00:16:02,604 --> 00:16:06,784
you, you'd be able to see it, but unless
you'll have to, you can only confirm

241
00:16:07,024 --> 00:16:10,754
it after weeks or months of monitoring
performance and revenue metrics.

242
00:16:11,204 --> 00:16:15,574
And you can't, even if you can,
you can have engagement metrics

243
00:16:15,604 --> 00:16:17,114
at individual user level.

244
00:16:18,314 --> 00:16:21,724
No one, no one in the industry
has the time to go through every

245
00:16:21,724 --> 00:16:23,854
user because we are talking
about millions of users, right?

246
00:16:24,094 --> 00:16:27,364
So we only look at the aggregated metrics.

247
00:16:27,624 --> 00:16:32,494
so creating those aggregated metrics,
understanding how the engagement

248
00:16:32,504 --> 00:16:37,764
is, is either improving, is becoming
neutral to understand, to create

249
00:16:37,764 --> 00:16:42,784
those metrics, to, to monitor it
on daily basis is really important.

250
00:16:44,124 --> 00:16:47,614
and then there also exists
a bias within the models.

251
00:16:47,954 --> 00:16:51,564
for example, the early versions
of tragedy was biased towards

252
00:16:51,594 --> 00:16:53,335
political, political ideologies.

253
00:16:53,645 --> 00:16:59,155
and that did not come up until,
until the model was actually

254
00:16:59,155 --> 00:17:01,835
put out in, up for users to use.

255
00:17:01,835 --> 00:17:06,035
And that's when, the model owners started
getting feedback from the users and the

256
00:17:06,035 --> 00:17:07,665
media that this is what is happening.

257
00:17:08,105 --> 00:17:13,345
And because of that, they had to reiterate
on like iterate on the model and create

258
00:17:13,555 --> 00:17:19,355
a new version, which was, Which was not
biased, so that's, that brings me to

259
00:17:19,355 --> 00:17:21,475
the next point, which is A B testing.

260
00:17:21,535 --> 00:17:27,095
A B testing is the process of
running, running a small test, of

261
00:17:27,135 --> 00:17:32,435
your change, on a small subset of
users before landing the change to the

262
00:17:32,475 --> 00:17:35,475
production or to the final production.

263
00:17:35,575 --> 00:17:40,675
and even after you run an A B test, it's
not like you can just, put like, it's

264
00:17:40,675 --> 00:17:42,215
not like you can just deploy your model.

265
00:17:42,385 --> 00:17:43,165
It's not like that.

266
00:17:43,165 --> 00:17:45,525
You need to prove value
to your stakeholders.

267
00:17:45,535 --> 00:17:50,275
Stakeholders here will mean product
managers, engineering managers, directors.

268
00:17:50,285 --> 00:17:54,445
You have to make sure that you
are, your model is actually

269
00:17:54,475 --> 00:17:56,395
doing what you expect it to do.

270
00:17:56,415 --> 00:17:57,545
For example, if you.

271
00:17:57,900 --> 00:18:00,870
stated that your model will
improve engagement metrics, then

272
00:18:00,870 --> 00:18:04,430
you need to show that it actually
improved engagement metrics.

273
00:18:04,670 --> 00:18:07,500
if you said, it will improve
prediction performance.

274
00:18:07,760 --> 00:18:11,010
and you also have to prove that
it does not cause any other

275
00:18:11,010 --> 00:18:12,669
disruption in, in, in the system.

276
00:18:13,090 --> 00:18:16,490
and all the top line and,
prediction metrics, look normal.

277
00:18:17,880 --> 00:18:21,370
And when you're running, A B tests,
you can have all sorts of issues.

278
00:18:21,370 --> 00:18:26,190
You can, you know, your, your A B
test can suffer with dilution, because

279
00:18:26,219 --> 00:18:29,830
of a parallel learning experiments
and your model, You have trained a

280
00:18:29,830 --> 00:18:33,300
certain model, which is supposed to
be picked in your, your test version,

281
00:18:33,300 --> 00:18:36,860
but it doesn't get big because, there
is a lot, there's a heavy fallback.

282
00:18:36,890 --> 00:18:39,890
Like it, it's, it's falling
back to the production model.

283
00:18:39,909 --> 00:18:44,760
So, your, your test version is actually
not set up correctly and, it's actually

284
00:18:44,760 --> 00:18:46,650
not doing what you expect it to do.

285
00:18:47,820 --> 00:18:49,830
So these are like the big issues that.

286
00:18:50,035 --> 00:18:54,705
That you run into when you're running the
A B test and when you're trying to, you

287
00:18:54,705 --> 00:18:58,335
know, build a new model, build a new model
architecture and put it in production.

288
00:18:59,405 --> 00:19:02,365
yeah, so, lastly, I want to say
that in theory, the goal is to

289
00:19:02,705 --> 00:19:05,285
maximize accuracy and minimize loss.

290
00:19:05,575 --> 00:19:09,435
But in production, you need to consider
system efficiencies, privacy, real

291
00:19:09,485 --> 00:19:11,435
world constraints, and business metrics.

292
00:19:11,925 --> 00:19:13,749
this is why real world ML is so important.

293
00:19:13,910 --> 00:19:17,160
It's all about making trade offs and
ensuring that the model delivers value.

294
00:19:17,550 --> 00:19:19,580
It's not just about high accuracy.

295
00:19:20,100 --> 00:19:24,400
with that, I will pass on the mic
to my co speaker, Sunandan, who

296
00:19:24,410 --> 00:19:28,200
will share some practical tips and
tricks to break into the ML space.

297
00:19:28,650 --> 00:19:31,730
Thank you, Shraddha, for the
wonderful introduction about the vast

298
00:19:31,730 --> 00:19:35,510
difference between the scale of ML in
graduate school versus in production.

299
00:19:35,790 --> 00:19:39,270
Hi, I'm Sunandan, and I'm going to
talk about what are the practical

300
00:19:39,280 --> 00:19:42,600
debugging skills you that upcoming
talent can use in production

301
00:19:42,610 --> 00:19:45,640
environment to debug AI or ML systems.

302
00:19:46,220 --> 00:19:49,070
We need both short term and long
term strategies to avoid the

303
00:19:49,070 --> 00:19:51,660
situation of on call fatigue
when dealing with these systems.

304
00:19:52,215 --> 00:19:54,585
Before we do that, let's
see what a typical ML system

305
00:19:54,605 --> 00:19:55,905
looks like from a high level.

306
00:19:57,865 --> 00:20:01,195
Any typical machine learning system in
a production environment has several

307
00:20:01,195 --> 00:20:04,315
sub components which work together
to give us the desired predictions.

308
00:20:04,895 --> 00:20:08,625
These steps such as model training,
snapshot creation, snapshot

309
00:20:08,625 --> 00:20:11,045
validation, inference systems, etc.

310
00:20:11,365 --> 00:20:15,505
are not simple components, but a whole
large distributed system with its own

311
00:20:15,505 --> 00:20:17,615
components and levels of complexity.

312
00:20:18,925 --> 00:20:21,705
For instance, we don't use raw
data to train a model directly.

313
00:20:22,045 --> 00:20:27,194
Instead, we first clean, filter, apply
privacy rules, and make transformations

314
00:20:27,195 --> 00:20:28,625
to prepare the data for model training.

315
00:20:29,005 --> 00:20:32,545
If there is failure in the data system,
we can end up with corrupted features.

316
00:20:33,145 --> 00:20:36,495
This can lead to low quality snapshots
being created during model training.

317
00:20:36,925 --> 00:20:40,875
If these snapshots are deployed to
more production, they will produce poor

318
00:20:40,875 --> 00:20:44,625
predictions, which in turn can affect
the behavior of software products.

319
00:20:45,120 --> 00:20:47,860
This can result in a different set
of events and outcomes, which can

320
00:20:47,860 --> 00:20:49,580
generate more bad quality data.

321
00:20:49,920 --> 00:20:54,060
In other words, data corruption can
have a ripple effect, impacting not

322
00:20:54,090 --> 00:20:57,890
only the model's performance, but
also the overall behavior of the

323
00:20:57,890 --> 00:20:59,410
software products that rely on it.

324
00:21:00,010 --> 00:21:03,370
This impact is similar for any
other components in this ecosystem.

325
00:21:05,640 --> 00:21:09,310
Visually, these failures, represented
by fire icons, cascade to different

326
00:21:09,310 --> 00:21:10,804
parts of the system pretty quickly.

327
00:21:11,305 --> 00:21:15,005
For example, issues in training can
lead to bad predictions, which can

328
00:21:15,005 --> 00:21:17,315
corrupt the latest snapshot deployed.

329
00:21:18,075 --> 00:21:22,455
Since the outcome of ML models is also
used in future ML predictions, issues in

330
00:21:22,455 --> 00:21:26,145
training data today, if not mitigated on
time, will lead to corrupted snapshots

331
00:21:26,145 --> 00:21:27,525
and predictions for features too.

332
00:21:28,245 --> 00:21:31,265
These interdependencies make
backtracing of issues in ML

333
00:21:31,275 --> 00:21:32,635
models really hard to debug.

334
00:21:33,405 --> 00:21:38,005
As Shraddha explained in a session, due to
scale of the ML systems and consequently

335
00:21:38,015 --> 00:21:42,530
the adverse impact a bad ML system
can have, It is imperative that the on

336
00:21:42,530 --> 00:21:46,750
calls respond to issues quickly and with
proven strategies so that these issues

337
00:21:46,750 --> 00:21:48,480
are mitigated as quickly as possible.

338
00:21:49,140 --> 00:21:52,510
So what should the junior engineers do to
make sure that they don't get overwhelmed?

339
00:21:54,580 --> 00:21:57,310
There are six invaluable debugging
techniques which, in the short

340
00:21:57,320 --> 00:21:59,710
term and the long term, will not
only help the junior engineers.

341
00:22:00,975 --> 00:22:04,015
tackle their on call issues faster,
but also make sure that they're

342
00:22:04,015 --> 00:22:05,195
progressing in their career too.

343
00:22:05,455 --> 00:22:08,235
In the next few slides, let's see
what these techniques help us achieve.

344
00:22:10,615 --> 00:22:13,595
The most critical issues that the
engineers need to deal with called

345
00:22:13,605 --> 00:22:16,675
selves or side events also need
the highest level of attention.

346
00:22:17,340 --> 00:22:21,740
There's a critical incidence where
a system is malfunctioning, causing

347
00:22:21,740 --> 00:22:25,460
service disruption or degraded
performance, and the severity

348
00:22:25,460 --> 00:22:27,539
depends on how bad the impact is.

349
00:22:27,820 --> 00:22:32,160
Training focus of, for engineers in this
area should be to recognize the high

350
00:22:32,160 --> 00:22:33,960
priority issues that impact fairness.

351
00:22:34,100 --> 00:22:40,970
This is done typically by creating
clear guidelines for deciding which

352
00:22:40,970 --> 00:22:43,670
incident qualifies for a CEP and what
should be the severity of the CEP.

353
00:22:44,810 --> 00:22:49,470
The junior engineer should also feel
empowered to file follow up tasks

354
00:22:49,470 --> 00:22:52,850
to prevent future CEPs, even if the
changes are difficult to do immediately.

355
00:22:53,580 --> 00:22:54,420
Why it's important?

356
00:22:54,590 --> 00:22:58,590
Because knowing when to escalate
issues is crucial in industries

357
00:22:58,590 --> 00:22:59,700
like finance or healthcare.

358
00:23:00,330 --> 00:23:02,870
where biases can have
serious implications.

359
00:23:03,530 --> 00:23:06,630
A pro tip I can provide is, we
should encourage the junior engineers

360
00:23:06,630 --> 00:23:10,280
to to document the impact and the
debugging steps, which they are

361
00:23:10,280 --> 00:23:14,270
doing, which will help the senior
engineers respond faster when they

362
00:23:14,280 --> 00:23:16,730
join the incident impact debugging.

363
00:23:17,270 --> 00:23:20,520
This also creates a virtuous cycle,
as these steps can be then integrated

364
00:23:20,530 --> 00:23:22,440
into the runbook for future reference.

365
00:23:25,090 --> 00:23:27,330
Finding incident reports
on time and effectively is

366
00:23:27,330 --> 00:23:28,350
just one part of the puzzle.

367
00:23:29,330 --> 00:23:31,560
Engineers should also take time
to understand the underlying

368
00:23:31,560 --> 00:23:32,860
model and serving architecture.

369
00:23:33,395 --> 00:23:35,985
It is not necessary that the
engineers need to understand each

370
00:23:35,985 --> 00:23:37,075
and every part of the system.

371
00:23:37,865 --> 00:23:40,695
However, it is crucial that they
understand what are the common

372
00:23:40,725 --> 00:23:43,655
failure points which will help in
narrowing down the issues faster.

373
00:23:44,765 --> 00:23:48,535
For example, Amazon's AI hiring
tool favored male candidates

374
00:23:48,535 --> 00:23:51,875
due to a bias introduced in the
training data pipeline, which was

375
00:23:52,165 --> 00:23:55,105
identified, which was not identified
early in the architecture review.

376
00:23:56,165 --> 00:23:59,425
Some walkthrough exercises or demo
runs to show how these issues can

377
00:23:59,425 --> 00:24:00,995
be diagnosed can be useful here.

378
00:24:01,055 --> 00:24:02,785
For example, a fire
drill can be conducted.

379
00:24:03,675 --> 00:24:08,515
Diagnosing issues like data latency
between feature stores and serving layers

380
00:24:08,515 --> 00:24:09,885
could be an example of a walkthrough.

381
00:24:12,055 --> 00:24:15,315
Once the engineer has a good grasp of
the architecture of the model, next

382
00:24:15,315 --> 00:24:18,785
step for them is to learn the end to
end training and deployment workflows.

383
00:24:19,275 --> 00:24:23,805
Concepts like data pre processing, model
training, evaluation, deployment, etc.

384
00:24:24,285 --> 00:24:27,705
are something that they will be facing
when they're on call and, or when

385
00:24:27,705 --> 00:24:30,935
they're called upon to help mitigate
production issues in their models.

386
00:24:31,575 --> 00:24:34,515
Understanding the end to end flow
helps in quicker identification of

387
00:24:34,515 --> 00:24:37,005
root cause, faster in real life issues.

388
00:24:38,010 --> 00:24:42,510
To do this, having the engineers sharpen
their skills on a simpler data set will

389
00:24:42,560 --> 00:24:46,830
teach them how to manage the deployments
and the skills will be invaluable in

390
00:24:46,830 --> 00:24:48,360
showing them the ropes of the system.

391
00:24:49,140 --> 00:24:54,410
An example of how understanding the
end to end workflow can be useful is

392
00:24:55,000 --> 00:24:59,460
when Google Photos started labeling
black individuals as gorillas and

393
00:25:00,160 --> 00:25:05,320
after some investigation, it was found
that it was due to some training data

394
00:25:05,340 --> 00:25:07,180
and model evaluation stages problems.

395
00:25:09,385 --> 00:25:13,295
Another example could be if a junior
engineer is retaining a chatbot model,

396
00:25:13,545 --> 00:25:17,435
they should understand where the new data
is coming from, how the model is updated,

397
00:25:17,755 --> 00:25:20,565
and what downstream services are impacted.

398
00:25:23,355 --> 00:25:26,945
Running effective queries is another
skill set the junior engineer should

399
00:25:26,975 --> 00:25:30,575
continue to work on iteratively
as they work on their projects.

400
00:25:31,515 --> 00:25:34,295
Knowing what to query is a human
skill, and it takes lots of practice

401
00:25:34,295 --> 00:25:35,655
and trial and error to find the skill.

402
00:25:36,660 --> 00:25:40,490
Example of queries that is often
debugged for production is figuring

403
00:25:40,510 --> 00:25:43,660
out why a recommendation system
is returning irrelevant results.

404
00:25:44,460 --> 00:25:48,130
This could be due to missing user
preferences, bad training data,

405
00:25:48,380 --> 00:25:52,390
mad model architecture, or some
issue in the inference system,

406
00:25:52,440 --> 00:25:53,270
or could be something else.

407
00:25:54,010 --> 00:25:57,240
Junior engineers can tag along with
senior engineers debugging the issue

408
00:25:57,270 --> 00:25:59,840
in shadow and learn from the experts.

409
00:25:59,970 --> 00:26:01,670
What are the smoking
and science to look for?

410
00:26:02,365 --> 00:26:05,735
Building the intuition for figuring out
the failures is a skill set which will

411
00:26:05,745 --> 00:26:07,075
help the engineers in the long run.

412
00:26:09,585 --> 00:26:13,345
Mentors play a crucial role in
helping individuals develop their

413
00:26:13,435 --> 00:26:14,985
skills and knowledge in any field.

414
00:26:15,615 --> 00:26:19,795
In the context of debugging complex
systems, mentors can provide valuable

415
00:26:19,795 --> 00:26:24,070
guidance and support to help individuals
overcome challenges and improve

416
00:26:24,070 --> 00:26:25,460
their problem solving abilities.

417
00:26:26,160 --> 00:26:29,000
Finding a mentor can be done through
various challenges such as within

418
00:26:29,010 --> 00:26:31,820
the organization or by connecting
with professionals on LinkedIn

419
00:26:31,890 --> 00:26:33,380
or other engineering communities.

420
00:26:34,060 --> 00:26:38,360
It is important to find a mentor who has
experience and expertise in the specific

421
00:26:38,570 --> 00:26:41,780
area of interest and who can provide
constructive feedback and support.

422
00:26:42,380 --> 00:26:46,710
One example of how mentors can help is
by encouraging Shadowing sessions or

423
00:26:46,790 --> 00:26:51,710
encouraging attendance in the debugging
forums where the engineers can learn from

424
00:26:52,270 --> 00:26:56,210
real world incidents and gain practical
experiences in debugging complex systems.

425
00:26:57,510 --> 00:27:01,180
In addition to finding a mentor, it
is also important to join relevant

426
00:27:01,180 --> 00:27:04,280
communities and forums where the
individuals can connect with peers

427
00:27:04,310 --> 00:27:05,640
and learn from their experiences.

428
00:27:05,640 --> 00:27:09,360
Communities like KD Nuggets and
AI Breakfast Club, they offer

429
00:27:09,360 --> 00:27:13,970
valuable resources and networking
opportunities for engineers

430
00:27:14,040 --> 00:27:15,640
interested in AI and machine learning.

431
00:27:17,645 --> 00:27:21,644
To become a successful AI ML
engineer, it is important that they

432
00:27:21,645 --> 00:27:24,685
also develop the key skills that are
necessary for success in this field.

433
00:27:25,345 --> 00:27:28,465
Skills like debugging expertise,
understanding system design, and

434
00:27:28,465 --> 00:27:31,255
strong querying and querying skills
are just one part of the skill set.

435
00:27:31,905 --> 00:27:34,735
They should also look to
specialize in a specific area.

436
00:27:35,335 --> 00:27:39,435
Now that area could be MLOps, data
engineering, or model interpretability.

437
00:27:40,825 --> 00:27:44,585
Also it's very important that the
AI ML engineer stays ahead of the

438
00:27:44,875 --> 00:27:49,385
curve by engaging in continuous
learning as new tools and frameworks

439
00:27:49,385 --> 00:27:50,235
are being developed all the time.

440
00:27:50,795 --> 00:27:53,285
Finally, it is also very
important to understand the real

441
00:27:53,315 --> 00:27:54,595
world impact of the systems.

442
00:27:54,755 --> 00:27:58,385
Models like the compass system have
had significant real world consequences

443
00:27:58,715 --> 00:28:01,815
and fixing issues with these models
can have a major impact on the

444
00:28:01,815 --> 00:28:03,445
company's reputation and bottom line.

445
00:28:04,095 --> 00:28:08,345
And by understanding the potential
impact of their work, the AI and ML

446
00:28:08,345 --> 00:28:10,945
engineers can ensure that they're
making a positive contribution.

447
00:28:12,605 --> 00:28:15,555
I want to mention that none of this is
possible without the combination of great

448
00:28:15,555 --> 00:28:17,055
culture, great team, and great tool.

449
00:28:18,040 --> 00:28:21,350
A great culture serves as the
foundation for open communication

450
00:28:21,390 --> 00:28:22,740
and values diverse perspectives.

451
00:28:23,450 --> 00:28:26,390
Junior engineers should feel comfortable
knowing that the company culture

452
00:28:26,470 --> 00:28:29,990
respects the diversity of background and
technical skills and values the mantra

453
00:28:30,040 --> 00:28:31,510
that there are no stupid questions.

454
00:28:32,330 --> 00:28:35,830
Complementing this is a great team
where individuals leverage the

455
00:28:35,830 --> 00:28:38,610
unique strengths and collaborate
effectively towards common goals.

456
00:28:39,340 --> 00:28:41,180
This can only be achieved
by leading through example.

457
00:28:41,370 --> 00:28:44,950
Managers and tech lead should ensure
that the team is taking care of each

458
00:28:44,950 --> 00:28:46,450
other during the on call situation.

459
00:28:46,910 --> 00:28:49,640
and making sure on calls are
getting help they need to progress

460
00:28:49,640 --> 00:28:50,710
through the issue mitigation.

461
00:28:51,380 --> 00:28:56,310
Lastly, great tooling enables the team to
avoid repetitive tasks and enables team

462
00:28:56,310 --> 00:28:57,900
members to focus on high impact work.

463
00:28:58,480 --> 00:29:02,170
Automation, whenever possible, to
reduce the possibility of human error

464
00:29:02,360 --> 00:29:05,730
should be a prime focus of team roadmap
to build a great debugging culture

465
00:29:05,730 --> 00:29:06,850
for the product in the long run.

466
00:29:08,980 --> 00:29:09,990
And that concludes my talk.

467
00:29:10,560 --> 00:29:13,480
I hope the listeners gained some
valuable insights and understanding

468
00:29:13,480 --> 00:29:17,590
of debugging techniques required to
succeed in ML production systems.

469
00:29:18,315 --> 00:29:21,265
Please don't hesitate to reach out
to us on LinkedIn for any questions.

470
00:29:21,475 --> 00:29:21,655
Thank you.

