1
00:00:00,500 --> 00:00:01,370
Hello everyone.

2
00:00:02,210 --> 00:00:03,890
Thank you for joining me today.

3
00:00:04,390 --> 00:00:08,549
Today I'm going to talk about
an interesting topic from

4
00:00:08,549 --> 00:00:13,860
dashboard to defenses building
autonomous resilience at scale.

5
00:00:14,360 --> 00:00:18,470
I want to start with something
we have all experienced.

6
00:00:19,459 --> 00:00:23,645
Imagine it's two in the
morning, your phone buses.

7
00:00:24,145 --> 00:00:26,305
You have just been paged.

8
00:00:27,145 --> 00:00:29,004
You pull yourself out of the bat.

9
00:00:29,504 --> 00:00:30,104
Look at that.

10
00:00:30,104 --> 00:00:36,314
Dashboards try to make sense of graphs
and scramble to contain the problem.

11
00:00:37,214 --> 00:00:43,064
And in that moment, how many of you
have thought, why can't the system just.

12
00:00:44,024 --> 00:00:45,134
This itself.

13
00:00:45,634 --> 00:00:49,414
That's exactly the journey I
want to share with you today.

14
00:00:50,134 --> 00:00:56,374
How we can move beyond watching
problems and start building

15
00:00:56,374 --> 00:00:59,074
system that defend themselves

16
00:00:59,574 --> 00:01:02,529
in production environment,
especially at scale.

17
00:01:03,264 --> 00:01:07,104
The 2:00 AM pager is not sustainable.

18
00:01:07,604 --> 00:01:11,475
Dashboards solve problems,
but they don't act.

19
00:01:11,975 --> 00:01:17,460
And while we have all seen heroic
firefighting from engineers,

20
00:01:18,330 --> 00:01:20,420
this model simply doesn't scale.

21
00:01:21,355 --> 00:01:24,774
When you are sell, when
you are serving billions of

22
00:01:24,774 --> 00:01:27,225
requests, the mandate is clear.

23
00:01:27,725 --> 00:01:33,665
We must engineer reliability into
the system itself, not leave it to

24
00:01:33,665 --> 00:01:36,545
the humans to keep everything alive.

25
00:01:37,045 --> 00:01:39,035
Let's start with observability.

26
00:01:39,535 --> 00:01:40,554
The world wave was.

27
00:01:41,065 --> 00:01:48,405
To collect everything, build endless
dashboards, and hope someone noticed

28
00:01:48,405 --> 00:01:50,355
the right spike at the right time.

29
00:01:51,225 --> 00:01:57,365
But let's be honest, humans can't pass
millions of data point in real time.

30
00:01:57,865 --> 00:02:04,370
The new way is SL world driven signals
tied directly to user experience.

31
00:02:04,870 --> 00:02:11,920
We don't just measure CPU or memory, we
measure whether user really succeeded.

32
00:02:12,640 --> 00:02:18,640
That means tracking P 95, request
latency, overall availability

33
00:02:19,030 --> 00:02:20,590
and checkout success rate.

34
00:02:21,460 --> 00:02:28,500
We then set VOS and we use error
budget to decide when to release

35
00:02:28,500 --> 00:02:30,360
quickly and when to slow down.

36
00:02:30,860 --> 00:02:36,140
These turns reliability into
something measurable and actionable

37
00:02:36,640 --> 00:02:39,490
to make observability very useful.

38
00:02:40,120 --> 00:02:41,950
We use proven patterns.

39
00:02:42,610 --> 00:02:47,670
We use SL OS and error budgets
as the release gatekeepers.

40
00:02:48,630 --> 00:02:53,030
We follow red golden
signals and used frameworks.

41
00:02:53,945 --> 00:02:56,735
To ensure consistency
across every service.

42
00:02:57,695 --> 00:03:02,825
Each service has a single golden
dashboard, not dent, so engineers

43
00:03:02,825 --> 00:03:04,535
know exactly where to look.

44
00:03:05,035 --> 00:03:12,475
We take an open telemetry first approach,
so metrics, traces, and locks all

45
00:03:12,475 --> 00:03:14,245
flow through the standard pipeline.

46
00:03:15,115 --> 00:03:20,005
And we enrich metrics
with trace IDs like that.

47
00:03:20,350 --> 00:03:24,160
So an engineer can jump
directly from a latency spike

48
00:03:24,670 --> 00:03:26,530
to the trace that explains it.

49
00:03:27,460 --> 00:03:30,670
This makes observative
more than just data.

50
00:03:31,120 --> 00:03:34,360
It makes, it's a foundation of automation.

51
00:03:34,860 --> 00:03:41,545
Metric traces and locks are
only valuable if they are scale.

52
00:03:42,045 --> 00:03:47,565
For tracing, we propagate W three
trace contacts across every service,

53
00:03:48,405 --> 00:03:50,445
and we use tail based sampling.

54
00:03:50,535 --> 00:03:56,735
So we always keep the interesting data,
the errors, and the outliers for locks.

55
00:03:57,005 --> 00:04:03,275
We use structure json, we redact
sensitive data at the collector, and

56
00:04:03,275 --> 00:04:05,795
we route locks by severity so we can.

57
00:04:06,295 --> 00:04:12,555
We can look into what is necessary
and don't log unnecessary noises.

58
00:04:13,055 --> 00:04:17,605
When these three pillars work
together, you get correlated

59
00:04:17,605 --> 00:04:19,550
telemetry that you can trust.

60
00:04:20,050 --> 00:04:23,050
Agility also has to
extend beyond the backend.

61
00:04:23,550 --> 00:04:26,340
We need change awareness Every deployment.

62
00:04:26,745 --> 00:04:32,085
Every feature flag toggle, so up
as annotations on the dashboards.

63
00:04:32,685 --> 00:04:38,865
That way when the latest pump jumps or
error spikes, we can immediately see

64
00:04:39,225 --> 00:04:41,265
if it lines up with the new release.

65
00:04:41,765 --> 00:04:45,665
We need client visibility,
real user monitoring.

66
00:04:45,665 --> 00:04:51,755
Tell us about load times layout, saved,
browser crashes, broken experiences.

67
00:04:52,255 --> 00:04:54,295
We also need edge visibility.

68
00:04:54,865 --> 00:05:00,745
CDN metrics show us cash, hit ratio
and edge error rates, et cetera.

69
00:05:01,525 --> 00:05:07,225
And finally, we track capacity
and cost before cost saturation,

70
00:05:07,225 --> 00:05:09,625
before it becomes an outage.

71
00:05:10,315 --> 00:05:10,825
And we take.

72
00:05:11,325 --> 00:05:14,955
We take telemetry with the
team and service ownership

73
00:05:14,955 --> 00:05:16,245
to make the cost visible.

74
00:05:16,745 --> 00:05:22,615
This way, we aren't just monitoring
the backend, we are just monitoring the

75
00:05:22,615 --> 00:05:25,075
end-to-end experience and business impact

76
00:05:25,575 --> 00:05:30,615
dashboards alone are still
reactive unless you add prediction.

77
00:05:31,115 --> 00:05:37,535
Static threshold always fails because
what is normal is always changing.

78
00:05:38,285 --> 00:05:44,405
So we build capacity anomaly detector
that learn usage patterns and

79
00:05:44,405 --> 00:05:47,435
alerts before we hit hot limits.

80
00:05:47,935 --> 00:05:52,165
We then added AA models that
forecast latency spikes and

81
00:05:52,165 --> 00:05:56,170
correlated logs, metrics, and
traces to find out root cost faster.

82
00:05:56,670 --> 00:06:03,530
We also have models which deducts
error spikes based on capacity

83
00:06:03,530 --> 00:06:07,760
and like right incoming requests
and treat the issues early.

84
00:06:08,630 --> 00:06:13,070
One of the most powerful innovation
was using the browser JavaScript

85
00:06:13,570 --> 00:06:14,980
error as an anomaly signal.

86
00:06:15,480 --> 00:06:20,520
If a rollout suddenly increases
JSR, the system automatically rolls

87
00:06:20,520 --> 00:06:23,700
back itself before user even notice.

88
00:06:24,200 --> 00:06:28,640
That observability turning
into practical defense

89
00:06:29,140 --> 00:06:32,230
observability only matters if
the alerts lead to an action.

90
00:06:33,190 --> 00:06:38,995
The worldwide was an alert, fires your
human, wakes up, your human, then looked

91
00:06:39,070 --> 00:06:41,560
into the dashboard and mitigate the issue.

92
00:06:42,280 --> 00:06:43,360
That is very slow.

93
00:06:43,860 --> 00:06:50,610
The new way is alerts are just alerts are
used as a controlled signals, so system

94
00:06:50,610 --> 00:06:53,670
identifies the issue and auto corrects.

95
00:06:54,240 --> 00:06:58,260
If a dependence is slow, a circuit
could breaker trips immediately.

96
00:06:58,740 --> 00:07:03,960
If error budget bonds too fast, a new
deployment rolls back automatically.

97
00:07:04,680 --> 00:07:09,780
We use multi bond rate alerts,
some of direct fast spikes,

98
00:07:10,110 --> 00:07:12,030
and others direct slow leaks.

99
00:07:12,530 --> 00:07:14,240
And we only page humans.

100
00:07:14,930 --> 00:07:20,920
When there is a real user impact,
everything else routes to a chat or

101
00:07:20,920 --> 00:07:23,290
tickets, which will be handled offline.

102
00:07:23,790 --> 00:07:27,870
This keeps human focused on the
problem that really matters,

103
00:07:28,350 --> 00:07:30,450
not everything, not noises.

104
00:07:30,950 --> 00:07:35,255
To build a trust, we use a
tire strategy for automation.

105
00:07:35,755 --> 00:07:40,525
Tire one covers safe and
reversible actions like

106
00:07:40,525 --> 00:07:42,445
throttles and circuit breakers.

107
00:07:43,255 --> 00:07:45,685
These always run automatically.

108
00:07:46,185 --> 00:07:51,915
Tire two covers progressive changes
like Softing Regional traffic.

109
00:07:52,455 --> 00:07:56,655
These can start automatically,
but allow for human oversight.

110
00:07:57,155 --> 00:07:59,795
Tire three covers
complex or new incidents.

111
00:08:00,425 --> 00:08:02,285
The system notifies a human.

112
00:08:03,050 --> 00:08:08,390
But also provides runbooks and contacts
to accelerate the fixed faster.

113
00:08:08,890 --> 00:08:13,900
This approach ensures that
automation handles the EC 80%

114
00:08:14,410 --> 00:08:16,720
while human focus on the hard 20%.

115
00:08:17,220 --> 00:08:21,060
Our automation also relies on
proven resilience patterns.

116
00:08:21,560 --> 00:08:26,330
Multiple patterns described here,
the circuit breaker pattern.

117
00:08:26,830 --> 00:08:33,150
Use to stop cascading failures,
bulk heads, isolates, failures.

118
00:08:33,150 --> 00:08:36,600
So one service can't take down everything.

119
00:08:37,100 --> 00:08:39,020
Retry with exponential.

120
00:08:39,050 --> 00:08:44,830
Back off NCS, we don't overwhelm services
that are recovering failover and load

121
00:08:44,830 --> 00:08:47,110
balancing strategies automatically.

122
00:08:47,110 --> 00:08:49,480
Route traffic around unhealthy regions.

123
00:08:49,980 --> 00:08:54,990
And cars and saga patterns help us
scale while maintaining consistency

124
00:08:55,050 --> 00:08:57,030
across distributed transaction.

125
00:08:57,530 --> 00:09:00,860
These patterns form our defense playbook.

126
00:09:01,360 --> 00:09:05,965
Delivery pipelines are not just
about speed, they're about safety.

127
00:09:06,465 --> 00:09:12,890
Manual deployments, get manual deployment
gate, slow us down and address.

128
00:09:13,390 --> 00:09:18,699
Human misclicks, human approved
without contacts, so we build

129
00:09:19,180 --> 00:09:20,920
fully automated pipelines.

130
00:09:21,339 --> 00:09:25,630
Every release goes through a canary
deployment, progressive rollouts,

131
00:09:25,989 --> 00:09:30,729
and automatic rollout, automatic
rollback when new error spike

132
00:09:30,729 --> 00:09:32,664
happen, or SLOs are breached.

133
00:09:33,164 --> 00:09:37,514
We shifted human review from
pre-deployment approvals to

134
00:09:38,084 --> 00:09:44,744
post-deployment analysis when real
system behavior is available, the result,

135
00:09:45,404 --> 00:09:52,094
faster velocity and safer production
pipelines have become resilience engines.

136
00:09:52,594 --> 00:09:54,214
Another critical safeguard is.

137
00:09:55,069 --> 00:09:56,419
Dynamic rate limiting.

138
00:09:57,079 --> 00:10:01,099
Many of our incidents were not
caused by infrastructure failure,

139
00:10:01,699 --> 00:10:04,009
but by workload anomalies.

140
00:10:04,509 --> 00:10:09,339
Your buggy line spams request
our internal batch of floods.

141
00:10:09,339 --> 00:10:14,939
Our APAs static limits always
fails because they don't adopt.

142
00:10:15,749 --> 00:10:20,939
So we build adoptive throttling
that learns normal tropic

143
00:10:20,944 --> 00:10:22,884
per client per endpoint.

144
00:10:23,579 --> 00:10:28,769
Detect sudden deviations and contain
runway workloads automatically.

145
00:10:29,269 --> 00:10:34,099
This mechanism alone has prevented
nearly 30% of our major incident.

146
00:10:34,599 --> 00:10:41,634
Every safeguard we have today was
born from scar tissue, A cascading

147
00:10:41,634 --> 00:10:43,994
outage gave us circuit breakers.

148
00:10:44,494 --> 00:10:44,734
Yeah.

149
00:10:44,734 --> 00:10:48,425
Bad config rollout gave
us progressive deployment.

150
00:10:49,234 --> 00:10:49,384
Yeah.

151
00:10:49,384 --> 00:10:54,264
Runway client gave us a tive
throttling, so we made it cultural.

152
00:10:54,984 --> 00:10:58,314
Every postmortem as two things.

153
00:10:59,154 --> 00:11:01,104
Could this have been auto detected?

154
00:11:01,674 --> 00:11:04,014
Could it have been auto mitigated?

155
00:11:04,764 --> 00:11:06,954
If the answer is yes, we build it.

156
00:11:07,454 --> 00:11:09,584
That's how U turn.

157
00:11:10,394 --> 00:11:14,114
Pain into progress, and
engineers always toil.

158
00:11:14,614 --> 00:11:18,684
The hardest part of all this
is trust engineers, worry

159
00:11:18,684 --> 00:11:20,564
automation, engineers worry.

160
00:11:21,134 --> 00:11:28,174
Automation might make things worse, so
we build it progressively in data mode.

161
00:11:28,384 --> 00:11:31,874
Automation only logs what it have done.

162
00:11:32,374 --> 00:11:38,644
In suggest mode, it recommends action,
but waits for human approval only.

163
00:11:38,644 --> 00:11:39,994
In autonomous mode.

164
00:11:40,714 --> 00:11:47,164
It does act independently, always
within guardrails, every action is

165
00:11:47,194 --> 00:11:50,464
transparent, reversible, and explainable.

166
00:11:50,964 --> 00:11:54,504
Over the time engineers saw that
the automation was actually.

167
00:11:55,074 --> 00:11:59,214
More reliable than a
tired human at 2:00 AM.

168
00:11:59,664 --> 00:12:02,204
2:00 AM trust won't be declared.

169
00:12:02,474 --> 00:12:03,464
It was earned.

170
00:12:03,964 --> 00:12:06,844
Resilient has to go
beyond technical metrics.

171
00:12:07,264 --> 00:12:09,694
It might tie back to business outcomes.

172
00:12:10,594 --> 00:12:14,434
We run UI keep alive to
continuously check front end flows.

173
00:12:15,424 --> 00:12:17,679
We track Java, JavaScript errors.

174
00:12:18,364 --> 00:12:21,634
In the browser as a
frontend anomaly signals.

175
00:12:22,134 --> 00:12:27,384
Most importantly, we watch
business KPIs like card A abandon

176
00:12:27,864 --> 00:12:29,634
and revenue impact, et cetera.

177
00:12:30,134 --> 00:12:36,724
If a release looks technically fine,
but causes amendment to spike, the

178
00:12:36,724 --> 00:12:38,434
system rolled back automatically.

179
00:12:39,334 --> 00:12:42,424
Reliability isn't just
keeping service green.

180
00:12:42,754 --> 00:12:45,514
It is protecting both user and business.

181
00:12:46,014 --> 00:12:49,644
When you connect it all
together, the blueprint is clear.

182
00:12:50,144 --> 00:12:53,114
Metrics become actionable signals.

183
00:12:53,864 --> 00:12:55,454
Alerts become closed.

184
00:12:55,454 --> 00:12:56,654
Loop mitigations.

185
00:12:57,614 --> 00:13:01,784
Deployments evolve into
autonomous pipelines.

186
00:13:02,084 --> 00:13:04,544
Limits become adoptive safeguards.

187
00:13:05,144 --> 00:13:07,634
Firefighting turns into
positive automation.

188
00:13:08,384 --> 00:13:11,114
And fear of automation becomes trust.

189
00:13:11,924 --> 00:13:15,914
That's how you evolve from
the dashboard to defenses.

190
00:13:16,414 --> 00:13:18,064
So let me leave you with this.

191
00:13:18,514 --> 00:13:20,914
Reliability is not PagerDuty.

192
00:13:21,349 --> 00:13:27,754
Reliability is not dashboard
Reliability is autonomous resilience.

193
00:13:28,204 --> 00:13:31,144
Every incident is a chance to automate.

194
00:13:31,549 --> 00:13:36,229
Every scar is a chance to
build defense step by step.

195
00:13:36,499 --> 00:13:40,909
You can build system that don't
just monitor themself, but

196
00:13:40,909 --> 00:13:42,979
systems that defend themself.

197
00:13:43,764 --> 00:13:44,274
Thank you.

