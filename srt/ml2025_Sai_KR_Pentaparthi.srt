1
00:00:01,000 --> 00:00:02,290
Morning or afternoon everyone.

2
00:00:02,830 --> 00:00:08,650
Today we are diving into a critical
and exciting area, HDD generative ai.

3
00:00:09,400 --> 00:00:14,050
Specifically we'll be exploring how
to engineer low latency AI solutions

4
00:00:14,410 --> 00:00:19,510
designed for resource constrained
environments as businesses increasingly

5
00:00:19,510 --> 00:00:24,175
look for real-time AI capabilities, right
where the action happens at the edge.

6
00:00:24,849 --> 00:00:27,759
Understanding how to optimize
these powerful generative

7
00:00:27,759 --> 00:00:29,259
models becomes essential.

8
00:00:30,310 --> 00:00:34,269
We look at systematic ways to
maintain performance while cutting

9
00:00:34,269 --> 00:00:38,379
down on computational needs,
power consumption, and latency.

10
00:00:39,099 --> 00:00:42,250
This opens up fascinating new
possibilities for intelligent

11
00:00:42,250 --> 00:00:44,649
applications directly on edge devices.

12
00:00:45,649 --> 00:00:47,379
Just a little bit about myself.

13
00:00:47,999 --> 00:00:52,349
I'm Ian, principal software
engineer at SD Engineering.

14
00:00:52,349 --> 00:00:56,999
I direct my work focuses on enhancing
global connectivity through advanced

15
00:00:56,999 --> 00:00:58,980
IP satellite network infrastructure.

16
00:00:59,969 --> 00:01:04,079
My experience in network engineering
and software development gives me a

17
00:01:04,079 --> 00:01:07,919
practical perspective on challenges
and opportunities of deploying

18
00:01:07,919 --> 00:01:13,019
complex systems like AI in diverse
environments, including the Edge.

19
00:01:13,470 --> 00:01:16,470
You can find more details or
connect with me on LinkedIn.

20
00:01:17,470 --> 00:01:19,510
So why is Edge AI challenging?

21
00:01:19,840 --> 00:01:22,060
Let's look at edge computing Frontier.

22
00:01:22,750 --> 00:01:26,679
Unlike the seemingly limitless
resources in the cloud, edge devices

23
00:01:26,679 --> 00:01:28,810
operate under significant constraints.

24
00:01:29,200 --> 00:01:31,690
First, limited computational resources.

25
00:01:32,500 --> 00:01:36,340
Think processing power, memory
storage edge devices typically

26
00:01:36,340 --> 00:01:38,500
have much less than cloud servers.

27
00:01:38,830 --> 00:01:40,720
Second, connectivity challenges.

28
00:01:41,530 --> 00:01:46,030
Edge solutions often need to work
reliably, even with spotty, slow,

29
00:01:46,150 --> 00:01:47,950
or sometimes no internet connection.

30
00:01:48,670 --> 00:01:50,560
Third, energy constraints.

31
00:01:51,160 --> 00:01:55,810
Many edge devices run on batteries
or have strict power budgets limiting

32
00:01:55,810 --> 00:01:58,000
how complex our AI models can be.

33
00:01:58,570 --> 00:02:00,160
And finally, realtime requirements.

34
00:02:00,160 --> 00:02:05,140
Many edge applications like autonomous
vehicles or industrial monitoring

35
00:02:05,530 --> 00:02:09,820
demand, immediate low latency responses,
putting high performance demands.

36
00:02:10,105 --> 00:02:11,635
On these constant devices.

37
00:02:12,635 --> 00:02:16,495
Despite this the challenges, the
demand for Edge AI is booming.

38
00:02:17,035 --> 00:02:18,535
Let's look at market trends.

39
00:02:18,865 --> 00:02:20,935
We are seeing strong expansion.

40
00:02:21,535 --> 00:02:26,275
The Edge AI accelerator market, the
specialized hardware is growing incredibly

41
00:02:26,275 --> 00:02:34,115
fast, nearly 36 39% CHER, projected
to hit almost 7.7 billion by 2027.

42
00:02:35,120 --> 00:02:38,030
This signals huge demand for on-device ai.

43
00:02:38,660 --> 00:02:43,100
What's driving this key adoption
drivers are primarily the need for low

44
00:02:43,100 --> 00:02:46,580
latency, over 78% of organization site.

45
00:02:46,970 --> 00:02:49,550
This is crucial for realtime responses.

46
00:02:50,570 --> 00:02:53,420
Then data privacy is another major factor.

47
00:02:53,750 --> 00:02:58,230
Almost 65% of organizations cite
this as another major factor.

48
00:02:59,160 --> 00:03:02,100
Pushing processing locally instead
of sending sensitive data to

49
00:03:02,100 --> 00:03:03,570
the cloud is important for them.

50
00:03:05,100 --> 00:03:07,170
And the industry adapting is adapting.

51
00:03:08,280 --> 00:03:11,880
Model optimization techniques, which
we'll discuss heavily today, are

52
00:03:11,880 --> 00:03:14,700
already used in 82% of deployments.

53
00:03:15,060 --> 00:03:20,610
This shows a clear focus on making complex
AI run effectively on edge hardware,

54
00:03:21,610 --> 00:03:22,930
edge AI market evolution.

55
00:03:23,500 --> 00:03:24,225
Let's look ahead.

56
00:03:24,805 --> 00:03:26,505
We are really at an inflection point.

57
00:03:27,010 --> 00:03:30,730
Currently generative AI at the edge
is still limited by those resources,

58
00:03:30,880 --> 00:03:33,110
resource constraints that were mentioned.

59
00:03:33,710 --> 00:03:39,170
But over the next 12 months, we expect
increasing adoption of optimized edge

60
00:03:39,170 --> 00:03:41,990
AI solutions within eight to 24 months.

61
00:03:42,050 --> 00:03:46,580
The prediction is that most enterprise
need real time edge capabilities

62
00:03:46,640 --> 00:03:48,560
will be act actively deploying them.

63
00:03:49,040 --> 00:03:54,800
Beyond 2025 H, AI is likely to
become ubiquitous, supported by

64
00:03:54,800 --> 00:03:56,510
specialized hardware acceleration.

65
00:03:57,290 --> 00:04:02,750
This rabbit evolution is driven by
hardware advancements, better optimization

66
00:04:02,750 --> 00:04:06,890
techniques like composition and pruning,
and the growing need for privacy.

67
00:04:06,890 --> 00:04:08,420
Preserving local computation.

68
00:04:09,140 --> 00:04:13,429
The shift away from cloud dependency
for realtime tasks is well underway.

69
00:04:14,429 --> 00:04:15,359
Market trends.

70
00:04:15,899 --> 00:04:22,030
Just to reiterate, the those key market
drives drivers because they underscore

71
00:04:22,030 --> 00:04:24,460
the why behind the edge optimization.

72
00:04:25,120 --> 00:04:26,695
The market growth is significant.

73
00:04:28,150 --> 00:04:35,700
As mentioned, 39% of CAGR for accelerators
and 7.6 billion market cap soon.

74
00:04:36,150 --> 00:04:43,080
The primary need for low latency, for
immediate responses and data privacy, both

75
00:04:43,080 --> 00:04:48,240
favoring local on device processing and
crucially, the industry is already heavily

76
00:04:48,240 --> 00:04:53,700
invested in model optimizations, which
is done in like almost 82% of deployments

77
00:04:53,940 --> 00:04:55,740
to overcome hardware limitations.

78
00:04:56,310 --> 00:04:59,190
This sets the stage for the
techniques we are going to

79
00:04:59,190 --> 00:05:00,780
explore in the rest of the talk.

80
00:05:01,780 --> 00:05:03,250
Acceleration solutions?

81
00:05:04,340 --> 00:05:08,250
Software optimization is key
Hardware plays a vital role.

82
00:05:08,910 --> 00:05:13,380
Specialized hardware acceleration
solution dramatically boost performance

83
00:05:13,380 --> 00:05:16,055
and energy efficiency for the edge.

84
00:05:16,245 --> 00:05:16,535
Yeah.

85
00:05:18,360 --> 00:05:23,310
We have neural processing units, N ps,
which are dedicated AI accelerators.

86
00:05:23,340 --> 00:05:29,380
Now common in modern systems on
chips they offer 10 to 15 times the

87
00:05:29,380 --> 00:05:33,700
energy efficiency for a standard CPU
for typical neural network tasks.

88
00:05:34,700 --> 00:05:40,250
Google offers HT ps. Purposely
built edge accelerators providing

89
00:05:40,250 --> 00:05:45,050
significant processing power, like
up to four terra operations in small

90
00:05:45,050 --> 00:05:47,270
low power around two watts package.

91
00:05:47,480 --> 00:05:48,170
That's impressive.

92
00:05:49,220 --> 00:05:54,680
Then mobile GPUs are also increasingly
optimized for AI leveraging their

93
00:05:54,680 --> 00:06:00,950
parallel processing power, and the FPG
accelerators offer re comfortable hardware

94
00:06:01,100 --> 00:06:06,050
adapted for specific AI models and often
very power efficient in production.

95
00:06:07,205 --> 00:06:10,685
These accelerators are designed
specifically for the math

96
00:06:10,685 --> 00:06:14,495
intensive operations common in
ai, like matrix, multiplication,

97
00:06:15,495 --> 00:06:16,905
it, GPU's, overview.

98
00:06:17,655 --> 00:06:22,875
This table gives a snapshot of the diverse
landscape of it, GPU and accelerators.

99
00:06:23,715 --> 00:06:28,305
Don't worry about memorizing the details,
but notice how the key players like

100
00:06:28,305 --> 00:06:37,750
Nvidia Jetson series like Nano Nx, A GX
RTX, ADA AMD rise, AI inter core Intel

101
00:06:37,750 --> 00:06:44,230
score Ultra and PS and R gpu, Google
score, HT ps, Callcom, snapdragons and

102
00:06:44,230 --> 00:06:46,150
pos, and specialists like H Cortex.

103
00:06:48,355 --> 00:06:51,865
Important takeaway here is the
variation in ai performance,

104
00:06:52,045 --> 00:06:55,525
memory, capacity, and bandwidth,
power consumption, and form factor.

105
00:06:56,515 --> 00:07:00,805
Choosing the right hardware depends
heavily on specific application

106
00:07:00,805 --> 00:07:05,245
needs regarding performance,
power, budget, and physical size.

107
00:07:05,935 --> 00:07:10,345
This plays a critical role in the
process of developing these models.

108
00:07:11,345 --> 00:07:14,975
Now let's look at the inference
performance specifically for

109
00:07:14,975 --> 00:07:18,845
large language models, which are
notoriously resource intensive.

110
00:07:19,385 --> 00:07:24,725
Again, this table is illustrative based on
available data, which can be fragmented.

111
00:07:25,640 --> 00:07:26,570
Key things to note.

112
00:07:26,930 --> 00:07:32,450
High-end desktop GPOs like RTX
4 0 9 0 or 3 0 9 0 achieve very

113
00:07:32,450 --> 00:07:34,220
high throughput tokens per second.

114
00:07:34,820 --> 00:07:39,470
Using optimized frameworks like
Nvidia sensors R-T-L-L-M significantly

115
00:07:39,470 --> 00:07:43,520
outperforming less optimized
methods like Lama CCP on the same

116
00:07:43,520 --> 00:07:48,860
hardware for it specific hardware
like Nvidia Jetson's Orient Series.

117
00:07:48,860 --> 00:07:54,440
A MD Rise, AI and Intel Core ultra
running models like LAMA or SSL

118
00:07:54,470 --> 00:07:59,390
seven B, often using ization like
integer four is becoming feasible,

119
00:07:59,810 --> 00:08:03,920
especially with dedicated frameworks
like Tensor, R-T-L-L-M-O-N-N-X.

120
00:08:03,920 --> 00:08:06,350
Runtime with V is ai.

121
00:08:06,930 --> 00:08:12,690
Iex, LLM, open V. However, specific
standardized benchmarking numbers

122
00:08:12,690 --> 00:08:16,170
for throughput and latency on these
edge platforms is still emerging

123
00:08:16,530 --> 00:08:19,980
and depend heavily on software
maturity, on automation levels.

124
00:08:20,220 --> 00:08:24,240
That's why we see, needed data in
the, in specific rows and columns.

125
00:08:26,160 --> 00:08:30,390
The power consumption is drastically
lower for each devices compared to

126
00:08:30,390 --> 00:08:35,490
desktop GPUs highlighting the efficiency
gains, but also performance trade-offs.

127
00:08:36,490 --> 00:08:37,600
Beyond physical hardware.

128
00:08:38,200 --> 00:08:42,970
Hardware is only part of equation
Software specifically, AI frameworks

129
00:08:42,970 --> 00:08:47,200
and optimization engines is
crucial for actually running models

130
00:08:47,200 --> 00:08:49,750
efficiently on the hardware key.

131
00:08:49,750 --> 00:08:53,830
AI frameworks like TensorFlow
Light and PyTorch Mobile are

132
00:08:53,860 --> 00:08:55,390
designed for edge deployment.

133
00:08:56,080 --> 00:09:01,090
NVIDIA sensor RT and its LLM variant
are highly optimized for their GPOs.

134
00:09:02,095 --> 00:09:05,605
Inference automation engines
like ONNX runtime and Intel.

135
00:09:05,605 --> 00:09:06,595
So open wino.

136
00:09:07,765 --> 00:09:12,235
Focus on optimizing models for efficient
execution across various hardware

137
00:09:14,485 --> 00:09:14,965
vendors.

138
00:09:14,965 --> 00:09:18,745
Also provide specific software
stacks like Nvidia, Jetpack,

139
00:09:18,985 --> 00:09:20,665
A MD Rise, and AI software.

140
00:09:21,085 --> 00:09:26,875
Intel's one API Tools and Qualcomm's AI
stack, which bundles drivers, libraries,

141
00:09:26,875 --> 00:09:29,520
and tools for their respective hardware.

142
00:09:30,520 --> 00:09:32,695
The right framework involves trade offs.

143
00:09:33,085 --> 00:09:38,065
As this chart illustrates TensorFlow
light generally offers excellent

144
00:09:38,065 --> 00:09:43,155
hardware support through its de
delegation system, allowing it to

145
00:09:43,155 --> 00:09:48,375
leverage n ps and GPUs effectively,
or in an next runtime, often provide

146
00:09:48,735 --> 00:09:53,475
slightly better raw execution, speed,
and great cross-platform portability.

147
00:09:54,690 --> 00:09:58,710
Py Touch Mobile is often placed
for its developer experience and

148
00:09:59,100 --> 00:10:02,490
see simpler deployment, making
it good for rapid prototyping.

149
00:10:03,210 --> 00:10:07,530
TVM Apache TVM can achieve the
best performance through deep

150
00:10:07,530 --> 00:10:10,980
compiler optimizations, but
also comes with significantly

151
00:10:10,980 --> 00:10:12,480
higher deployment complexity.

152
00:10:13,530 --> 00:10:16,020
So the choice depends on your priorities.

153
00:10:16,380 --> 00:10:17,580
Broad hardware support.

154
00:10:18,600 --> 00:10:21,360
ITF flight, raw speed and portability.

155
00:10:21,660 --> 00:10:25,320
Linux runtime is the one to
explore ease of use by touch

156
00:10:25,320 --> 00:10:30,330
mobile, our maximum performance
at the far cost of complexity, TVR

157
00:10:31,330 --> 00:10:32,210
automation techniques.

158
00:10:32,830 --> 00:10:36,430
Now let's get to the core software
automation techniques, especially

159
00:10:36,430 --> 00:10:38,410
critical for large models like LLMs.

160
00:10:38,410 --> 00:10:39,100
On the edge.

161
00:10:39,490 --> 00:10:43,840
The most crucial technique and the
one we will spend significant time

162
00:10:43,840 --> 00:10:48,610
on is ization, reducing the precision
of numbers used in the model.

163
00:10:49,510 --> 00:10:54,190
Next is network pruning, removing
redundant pairs of the neural network.

164
00:10:54,310 --> 00:10:54,820
Then.

165
00:10:55,420 --> 00:10:58,990
There is a knowledge distillation
training a smaller student model

166
00:10:59,050 --> 00:11:00,640
to mimic a larger teacher model.

167
00:11:01,420 --> 00:11:04,900
Other methods include low rank
approximation or factorization

168
00:11:05,200 --> 00:11:06,760
and various memory optimizations.

169
00:11:07,330 --> 00:11:10,870
Today we'll focus primarily
on top three ization, pruning,

170
00:11:10,930 --> 00:11:12,160
and knowledge distillation.

171
00:11:13,160 --> 00:11:14,350
Start with ization.

172
00:11:14,925 --> 00:11:19,215
The fundamental idea is to use fewer
bits to represent the model's weight

173
00:11:19,215 --> 00:11:23,995
and activations they're using size and
often speeding up comp computation.

174
00:11:24,595 --> 00:11:27,535
There are several approaches
post-training ization.

175
00:11:27,595 --> 00:11:29,425
PTQ is the simplest.

176
00:11:30,745 --> 00:11:35,505
We train our model normally, usually
in 32 bit floating point numbers,

177
00:11:36,075 --> 00:11:39,675
and then convert it into lower
precision like eight bit in TJ.

178
00:11:41,120 --> 00:11:43,965
A represented as in eight or even in four.

179
00:11:43,965 --> 00:11:44,685
Afterwards.

180
00:11:45,195 --> 00:11:48,615
It's easier, but might have
a moderate accuracy cost.

181
00:11:50,325 --> 00:11:51,465
Next is ization.

182
00:11:51,465 --> 00:11:56,415
Our training incorporates the effects
of ization during the training process.

183
00:11:56,985 --> 00:11:59,385
The model learns to be
robust to lower precision.

184
00:12:00,135 --> 00:12:03,045
It's more complex, but usually
preserves better accuracy.

185
00:12:04,335 --> 00:12:08,385
Then dynamic range ization adjust
how contestation is applied

186
00:12:08,385 --> 00:12:11,145
based on the actual values
encountered during inference.

187
00:12:11,595 --> 00:12:14,505
Offering a balance between
accuracy and performance,

188
00:12:14,775 --> 00:12:16,875
especially good at varying inputs.

189
00:12:17,875 --> 00:12:20,100
Precis, precis, our ization.

190
00:12:20,580 --> 00:12:22,620
This visual helps illustrate the concept.

191
00:12:22,980 --> 00:12:25,140
Standard training uses FP 32.

192
00:12:26,370 --> 00:12:29,880
Simply converts this F 32
words into eight or in four.

193
00:12:29,970 --> 00:12:35,760
After training Q 80 simulates this conver
conversion during training, allowing the

194
00:12:35,760 --> 00:12:40,680
model to adapt and advanced technique
is mixed to precision deployment.

195
00:12:41,055 --> 00:12:44,295
Here, you don't have to ize
the entire model uniformly.

196
00:12:44,775 --> 00:12:48,975
You can analyze which layers are most
sensitive to precision loss and keep

197
00:12:48,975 --> 00:12:54,525
them at higher precision like FP 32 or
FP 16, while aggressively contacting

198
00:12:54,525 --> 00:12:56,981
less sensitive layers to inte or integer.

199
00:12:57,615 --> 00:12:57,945
Four.

200
00:12:58,575 --> 00:13:00,525
This offers a fine grain trade off.

201
00:13:01,515 --> 00:13:02,565
Ization is powerful.

202
00:13:02,745 --> 00:13:08,205
Moving from floating point 32 to integer
eight can make models four times smaller

203
00:13:08,235 --> 00:13:10,215
and inference three to four times faster.

204
00:13:11,175 --> 00:13:13,905
Especially on hardware with inte support.

205
00:13:14,595 --> 00:13:18,105
Inference is a stage where model
makes prediction on not seen data.

206
00:13:19,105 --> 00:13:23,845
Let's talk about ization technique,
PTQ, diving deeper into it.

207
00:13:24,565 --> 00:13:25,105
What is it?

208
00:13:25,165 --> 00:13:30,115
Converting pre-trained FP 32 model
after training is complete pro.

209
00:13:30,505 --> 00:13:32,745
It's similar it's simpler and faster.

210
00:13:33,135 --> 00:13:36,525
To implement because we don't need
to modify the training pipeline.

211
00:13:37,185 --> 00:13:38,415
No retraining is needed.

212
00:13:38,805 --> 00:13:43,815
Cons, it generally leads to higher
accuracy loss compared to QA,

213
00:13:45,855 --> 00:13:49,665
especially when going to a very
low precision like integer four.

214
00:13:50,145 --> 00:13:52,500
It often requests calibration data set.

215
00:13:52,785 --> 00:13:56,535
A small representative data
set used to determine the best

216
00:13:56,535 --> 00:13:58,425
way to map the floating point.

217
00:13:58,425 --> 00:14:01,395
32 ranges to integer
or integer four ranges.

218
00:14:03,900 --> 00:14:11,430
For many models, PT Q2 in eight results
in less than 2% accuracy degradation,

219
00:14:11,670 --> 00:14:17,310
which is often acceptable because h the
use cases are like not very generic,

220
00:14:17,310 --> 00:14:19,080
like this very domain specific actually.

221
00:14:20,080 --> 00:14:24,070
Why is ization particularly to integer
eight, so beneficial for hardware?

222
00:14:24,250 --> 00:14:28,390
Let's look at the specs for
NVIDIA eight 10 GPU often found

223
00:14:28,390 --> 00:14:31,870
in servers, but illustrating a
common principle in accelerators.

224
00:14:32,350 --> 00:14:36,070
Notice the performance figure for
standard floating point 32 math.

225
00:14:36,430 --> 00:14:40,930
It delivers 31.2 terra flops,
but look at the par integer eight

226
00:14:41,140 --> 00:14:43,360
performance using its tensor course.

227
00:14:43,810 --> 00:14:45,820
Two 50 Terra operations per second.

228
00:14:46,105 --> 00:14:49,675
Potentially up to 500 tops
with sparsity features.

229
00:14:50,155 --> 00:14:54,295
This massive increase in throughput
for integer eight operations compared

230
00:14:54,295 --> 00:14:58,375
to floating point 32 is why position
is so effective and so critical.

231
00:14:59,275 --> 00:15:04,105
Hardware accelerators often specifically
designed to perform low precision

232
00:15:04,105 --> 00:15:09,395
integer math much faster, and more power
efficiently than a floating point math

233
00:15:10,395 --> 00:15:11,115
ization technique.

234
00:15:12,115 --> 00:15:15,055
What is it simulating the
effects of conversation during

235
00:15:15,055 --> 00:15:16,555
the model training process?

236
00:15:16,855 --> 00:15:20,935
It generally achieves better accuracy
preservation compared to PTQ,

237
00:15:21,265 --> 00:15:25,965
especially at lower bid depths because
the model learns to compensate for

238
00:15:25,965 --> 00:15:27,525
the precision laws during training.

239
00:15:28,785 --> 00:15:29,175
It.

240
00:15:29,265 --> 00:15:30,135
What are the cons?

241
00:15:30,165 --> 00:15:33,555
It's more complex to implement
as you need to modify your.

242
00:15:33,900 --> 00:15:37,290
The training code and pipeline,
it requires access to the original

243
00:15:37,290 --> 00:15:41,940
training data and that MA massive
infrastructure training time is

244
00:15:41,940 --> 00:15:47,340
longer two 80, often keep accuracy
degradation below 1.5% for integer

245
00:15:47,340 --> 00:15:50,160
eight, potentially even better than PTQ

246
00:15:51,160 --> 00:15:52,810
dynamic rate ation.

247
00:15:54,400 --> 00:15:58,480
Adapting the ization parameters
like the scaling factor based on

248
00:15:58,480 --> 00:16:02,440
the range of activation values
observed at runtime, rather than

249
00:16:02,440 --> 00:16:08,380
using fixed parameters determined
offline, what are the pros offers?

250
00:16:08,380 --> 00:16:12,460
A good balance between the simplicity
of PTQ and the accuracy of Q 80.

251
00:16:13,090 --> 00:16:17,530
It adapts to characteristics of
the input data being processed.

252
00:16:17,590 --> 00:16:18,910
It can also reduce.

253
00:16:19,225 --> 00:16:23,305
Memory bandwidth needs as activation
ranges are determined on the floor.

254
00:16:24,115 --> 00:16:25,045
What are the cons?

255
00:16:25,735 --> 00:16:29,455
There can be a potential runtime
overhead associated with calculating

256
00:16:29,455 --> 00:16:31,405
this ranges during inference process.

257
00:16:31,405 --> 00:16:36,475
Because this runs on the edge, although
often minimal, unsupported hardware.

258
00:16:37,475 --> 00:16:39,635
Next condensation technique
mixed to precision.

259
00:16:41,660 --> 00:16:44,690
Using different numerical
precision levels for different

260
00:16:44,690 --> 00:16:46,190
layers within the same model.

261
00:16:48,110 --> 00:16:49,970
So what are the cons with this approach?

262
00:16:50,090 --> 00:16:53,690
Provides a excellent way to
balance compression and accuracy.

263
00:16:56,060 --> 00:16:59,150
We can keep critical sensitive
layers at higher precision.

264
00:16:59,150 --> 00:17:04,820
Example, floating point 16 or floating
point 32, while izing other more robust.

265
00:17:05,865 --> 00:17:09,585
Layers aggressively into
integer eight or integer four.

266
00:17:10,425 --> 00:17:14,235
It requires careful analysis to
determine which layers are sensitive.

267
00:17:14,445 --> 00:17:18,705
It also needs framework and hardware
support to handle computations

268
00:17:18,705 --> 00:17:22,425
involving multiple different data
types within the same inference path.

269
00:17:25,050 --> 00:17:27,990
Studies have shown potential
for significant compression.

270
00:17:27,990 --> 00:17:33,240
Example, up to 30 70% with minimal
accuracy loss, like less than 1% using

271
00:17:33,240 --> 00:17:37,735
mixed precision because when you you
convert some of the layers to inte,

272
00:17:37,735 --> 00:17:39,980
it offers more model compression.

273
00:17:39,980 --> 00:17:40,370
Also,

274
00:17:41,370 --> 00:17:44,890
next to major m automation
technique is neural network pruning.

275
00:17:46,750 --> 00:17:51,460
The core idea is that many large
neural networks are over parameterized.

276
00:17:51,730 --> 00:17:55,000
They have redundant weights or
neurons that don't contribute

277
00:17:55,000 --> 00:17:56,320
much to final output.

278
00:17:56,890 --> 00:18:00,370
Crooning aims to identify and
remove these non-essential parts.

279
00:18:01,030 --> 00:18:04,780
The process generally involves
identifying redundancy by analyzing

280
00:18:04,930 --> 00:18:08,980
weights or activation patterns,
then removing these elements.

281
00:18:09,340 --> 00:18:13,360
This can be structured pruning where
entire filters or neurons are remote.

282
00:18:13,705 --> 00:18:17,635
Often better for hardware speedups
or unstructured pruning where

283
00:18:17,665 --> 00:18:20,965
individual weights are remote
leading to sparks networks.

284
00:18:21,805 --> 00:18:26,125
Techniques like the iterative magnitude
pruning gradually remove small weights

285
00:18:26,185 --> 00:18:28,765
while re retraining the network.

286
00:18:28,765 --> 00:18:33,965
To maintain accuracy sensitive allow
VT analysis is often ever helps

287
00:18:33,965 --> 00:18:37,625
to evaluate how removing certain
parts impact overall performance.

288
00:18:38,825 --> 00:18:43,715
Effective pruning can reduce models
significantly, sometimes 50 to 90%

289
00:18:44,285 --> 00:18:49,385
with very minimal impact on accuracy,
creating leaner, faster networks.

290
00:18:52,295 --> 00:18:55,055
So here's a overview
of the pruning process.

291
00:18:56,555 --> 00:18:58,835
Step one, training initial network.

292
00:18:59,345 --> 00:19:03,725
We start by training a potentially
over network until it converges

293
00:19:03,725 --> 00:19:07,695
and performs step two, identify
and remove unimportant elements.

294
00:19:07,995 --> 00:19:13,035
This is crucial step where you,
we apply the criteria like weight

295
00:19:13,305 --> 00:19:17,745
magnitude activation, frequency,
impact on loss, to decide which parts

296
00:19:17,745 --> 00:19:19,275
are unimportant and remove them.

297
00:19:19,935 --> 00:19:23,475
This might involve setting weights
to zero, unstructured and removing

298
00:19:23,475 --> 00:19:25,935
entire structures like filters,
which is a structured way.

299
00:19:27,600 --> 00:19:30,420
Then step three, fine tune, prune network.

300
00:19:30,750 --> 00:19:34,020
After removing elements, the
network's performance usually

301
00:19:34,020 --> 00:19:39,020
drops, so we retain the smaller
prune network for a few cycles.

302
00:19:39,650 --> 00:19:43,580
This allows the remaining weights
to adapt and often recovers most,

303
00:19:43,580 --> 00:19:45,710
if not all of the lowest accuracy.

304
00:19:46,610 --> 00:19:51,320
This cycle of prune and fine tune
can be repeated iteratively to reach

305
00:19:51,320 --> 00:19:53,330
a desired level of pars and size.

306
00:19:54,330 --> 00:19:56,250
Let's look at specific pruning types.

307
00:19:56,280 --> 00:20:00,060
Magnitude based pruning is perhaps
the simplest and most common.

308
00:20:00,570 --> 00:20:04,840
It operates at the granularity of either
individual weights, weight pruning,

309
00:20:04,840 --> 00:20:09,070
which is unstructured or entire units,
neurons or filters, which are structured.

310
00:20:09,070 --> 00:20:09,400
Pruning

311
00:20:11,410 --> 00:20:15,200
For weight pruning we simply remove
weights with the lower absolute

312
00:20:15,200 --> 00:20:20,060
values closest to zero, assuming they
contribute least for structured pruning.

313
00:20:20,645 --> 00:20:25,055
We might remove entire filters
based on the sum or average

314
00:20:25,055 --> 00:20:26,525
magnitude of their weights.

315
00:20:28,625 --> 00:20:32,465
And the result, unstructured pruning
leads to sparse irregular weight

316
00:20:32,465 --> 00:20:36,515
mattresses, structured pruning
results in smaller but still dense

317
00:20:36,515 --> 00:20:40,680
mattresses, which are often easier
for standard hardware to accelerate.

318
00:20:41,800 --> 00:20:42,815
What are the pros?

319
00:20:43,385 --> 00:20:46,835
Simple concept, high compression
potential, especially unstructured.

320
00:20:47,360 --> 00:20:51,470
Structured pruning often gives better
inference setups on standard hardware.

321
00:20:52,700 --> 00:20:53,510
What are the cons?

322
00:20:53,570 --> 00:20:57,740
Unstructured pruning often needs
specialized hardware or libraries for

323
00:20:57,740 --> 00:21:04,430
speed up while structured pruning is
ser and might impact accuracy, more

324
00:21:05,430 --> 00:21:06,150
pruning technique.

325
00:21:06,780 --> 00:21:10,770
Importance based beyond just
magnitude important based pruning

326
00:21:10,830 --> 00:21:14,910
uses more sophisticated metrices
to decide what to remove.

327
00:21:17,520 --> 00:21:18,570
What is a mechanism?

328
00:21:18,660 --> 00:21:21,540
Instead of just looking at the
weight values, it might consider

329
00:21:21,540 --> 00:21:23,040
the effect on the loss function.

330
00:21:23,100 --> 00:21:26,310
If the weight were remote, using
techniques like tailor expansion

331
00:21:26,310 --> 00:21:31,700
or optimal brain damage, or other
algorithms are they analyzed

332
00:21:31,700 --> 00:21:33,310
neural neuron activation.

333
00:21:33,955 --> 00:21:36,565
Or analyze gradient
information during training.

334
00:21:37,165 --> 00:21:41,905
Goal is to make a more informed,
sophisticated selection of which elements

335
00:21:41,905 --> 00:21:46,615
are truly unimportant for the need for
function, potentially preserving accuracy

336
00:21:46,735 --> 00:21:48,475
better than simple magnitude training

337
00:21:49,475 --> 00:21:50,405
based pruning technique.

338
00:21:52,805 --> 00:21:54,455
What's the mechanism we use here?

339
00:21:54,635 --> 00:21:58,595
As we see in the process diagram, it
involves repeating the cycles of prune.

340
00:21:58,595 --> 00:21:59,255
Fine tune.

341
00:22:00,275 --> 00:22:04,475
We remove a small percentage of weights
or neurons, retain briefly, then

342
00:22:04,475 --> 00:22:06,545
remove some more written and so on.

343
00:22:07,445 --> 00:22:10,745
The goal is to reach the targets
parity or size gradually, rather

344
00:22:10,745 --> 00:22:12,545
than removing everything at once.

345
00:22:13,475 --> 00:22:17,195
This gradual approach generally
leads to better accuracy prevention

346
00:22:17,345 --> 00:22:20,315
for a given level of parity
compared to one shot pruning.

347
00:22:22,745 --> 00:22:25,805
What are the trade-offs it's most
time consuming due to multiple

348
00:22:25,805 --> 00:22:27,635
pruning and retraining steps?

349
00:22:30,185 --> 00:22:33,695
A fascinating related concept
is lottery ticket hypothesis.

350
00:22:34,835 --> 00:22:39,365
It proposes that dense randomly
initiated networks contains spar

351
00:22:39,365 --> 00:22:41,045
subnets called winning tickets.

352
00:22:41,045 --> 00:22:41,135
That.

353
00:22:42,035 --> 00:22:45,485
Trained in isolation from the
start, using their original initial

354
00:22:45,485 --> 00:22:49,445
waste can achieve performance
comparable to full dense network

355
00:22:51,815 --> 00:22:55,715
magnitude Pruning, especially
iterative pruning, is effectively

356
00:22:55,715 --> 00:22:58,775
a method to find these winning
tickets within the large network.

357
00:23:00,275 --> 00:23:01,020
What are the implication?

358
00:23:01,655 --> 00:23:05,045
It suggests that the inherent
sparsity might be fundamental

359
00:23:05,045 --> 00:23:07,265
property of trainable neural networks.

360
00:23:07,955 --> 00:23:10,985
We don't necessarily need
the huge dense network.

361
00:23:11,045 --> 00:23:14,045
We just need to find the right
sparse structure within it.

362
00:23:14,615 --> 00:23:18,455
This provides theoretical backing
for why pruning can be so effective

363
00:23:18,725 --> 00:23:22,175
without catastrophic accuracy
loss, which is important for the H

364
00:23:23,175 --> 00:23:29,130
loss technique, knowledge distillation
for the h. Imagine we have a large,

365
00:23:29,130 --> 00:23:31,290
complex, highly accurate teacher model.

366
00:23:31,950 --> 00:23:35,490
It performs great, but it's
too slow or large for the edge.

367
00:23:37,050 --> 00:23:40,590
We also have a much smaller student
model, which is lightweight enough

368
00:23:40,590 --> 00:23:46,170
for the edge deployment, but might not
be as accurate on its own knowledge.

369
00:23:46,170 --> 00:23:51,670
Stands for is a distillation
process, which works by training the.

370
00:23:52,855 --> 00:23:56,545
Student model, not just on the
ground truth labels, but also to

371
00:23:56,545 --> 00:23:58,195
mimic the output of teacher model.

372
00:23:58,645 --> 00:24:01,885
Specifically, the student learns
from the teacher soft targets

373
00:24:02,365 --> 00:24:05,485
the full potential distribution
across all classes that teacher

374
00:24:05,485 --> 00:24:08,155
predicts even for incorrect classes.

375
00:24:08,695 --> 00:24:12,780
This encodes richer information about how
the teacher model thinks and generalizes.

376
00:24:13,840 --> 00:24:17,770
This allows the small student model to
benefit from the knowledge learned by

377
00:24:17,770 --> 00:24:22,750
the much larger teacher, often achieving
significant better accuracy than if

378
00:24:22,750 --> 00:24:25,000
it were trained only on hard levels.

379
00:24:26,000 --> 00:24:29,580
So for knowledge, distillation based
technique based on the type input,

380
00:24:32,160 --> 00:24:35,580
this is the most common type,
focusing on matching the output

381
00:24:35,580 --> 00:24:37,650
layer of the student to the teacher.

382
00:24:38,100 --> 00:24:42,000
Using those soft labels that were
mentioned, it captures the teacher's

383
00:24:42,000 --> 00:24:43,915
final pre pre prediction reasoning.

384
00:24:44,875 --> 00:24:49,075
Example show a significant parameter
reduction while retaining high

385
00:24:49,075 --> 00:24:51,415
quality up to 95% of teacher.

386
00:24:53,035 --> 00:24:56,215
This goes deeper trying to mass
the activations in intermediate

387
00:24:56,215 --> 00:24:58,225
layers of the student and teacher.

388
00:24:58,435 --> 00:25:02,335
The goal is to encourage the student
to develop similar internal feature

389
00:25:02,335 --> 00:25:04,345
representations as that of the teacher.

390
00:25:05,345 --> 00:25:08,735
We can also categorize distillation
by how the training happens.

391
00:25:08,855 --> 00:25:10,355
This is the standard approach.

392
00:25:10,895 --> 00:25:15,275
First, train the teacher model
completely, and then we can, we

393
00:25:15,275 --> 00:25:18,305
use the frozen pre-trained teacher
model to train the student.

394
00:25:18,995 --> 00:25:21,545
Then there is online,
offline, online distillation.

395
00:25:21,575 --> 00:25:24,665
Here the teacher and student
models are trained simultaneously.

396
00:25:25,295 --> 00:25:28,625
They learn together, potentially
influencing each other, which can

397
00:25:28,625 --> 00:25:33,275
sometimes lead to better results than
the sequential offline train approach.

398
00:25:34,595 --> 00:25:38,585
An interesting variant where a model
distills knowledge from itself.

399
00:25:39,335 --> 00:25:43,535
Often deeper layers of network
act as a teacher for shallower

400
00:25:43,535 --> 00:25:45,725
layers acting as a student.

401
00:25:46,055 --> 00:25:49,775
This can improve the performance
of the single model without

402
00:25:49,775 --> 00:25:53,855
needing a separate teacher used.

403
00:25:53,945 --> 00:25:57,635
This is used when there is a very
large gap between teacher and

404
00:25:57,635 --> 00:25:59,825
student size or model capabilities.

405
00:26:00,825 --> 00:26:03,610
We're optimizing existing large models.

406
00:26:03,760 --> 00:26:07,000
Another crucial strategy is
to use edge optimized model

407
00:26:07,000 --> 00:26:08,350
architectures from the start.

408
00:26:09,310 --> 00:26:13,810
The, these are neural network designs
created specifically for efficiency

409
00:26:13,810 --> 00:26:15,250
on resource constraint devices.

410
00:26:15,700 --> 00:26:17,680
Example includes mobile nets.

411
00:26:18,190 --> 00:26:24,010
Famous for using depth device separable
convolution, which drastically reduces

412
00:26:24,010 --> 00:26:27,760
computations like eight to nine times
compared to standard convolution.

413
00:26:27,790 --> 00:26:32,990
Great for vision tasks, shuffle nets, use
channel shuffling and group convolution

414
00:26:33,300 --> 00:26:36,325
further automations for low power devices.

415
00:26:36,685 --> 00:26:41,885
And then squeeze net employs fire
modules to reduce parameter counts while

416
00:26:41,975 --> 00:26:43,955
maintaining classification accuracy.

417
00:26:44,705 --> 00:26:48,905
Then the Efficient Net uses a
compounding scaling method to

418
00:26:48,905 --> 00:26:54,485
intelligently balance network with
depth and input resolution for excellent

419
00:26:54,485 --> 00:26:56,585
efficiency and accuracy trade-offs.

420
00:26:57,575 --> 00:27:01,715
Using this pre-built architectures
is often more effective than simply

421
00:27:01,715 --> 00:27:06,095
thinking a standard large architecture
as they incorporate efficiency

422
00:27:06,095 --> 00:27:08,075
principles directly into their design.

423
00:27:10,445 --> 00:27:15,215
Let's look at a practical example, an
H Voice assistant using a combination

424
00:27:15,215 --> 00:27:18,995
of the techniques, likely neural
network pruning, and eight bit ization.

425
00:27:19,295 --> 00:27:21,155
A baseline model was optimized.

426
00:27:21,635 --> 00:27:22,385
The results.

427
00:27:22,625 --> 00:27:27,965
The optimized model achieved 98.2%
wake word accuracy with a very low

428
00:27:27,965 --> 00:27:30,485
false activation rate under 0.5%.

429
00:27:31,025 --> 00:27:35,915
It managed 87.3 command
recognition accuracy across

430
00:27:35,915 --> 00:27:37,595
different noisy environments.

431
00:27:38,930 --> 00:27:41,630
Crucially, the model size
for the entire system Wake.

432
00:27:41,630 --> 00:27:47,570
Workplace command recognition was
reduced to just 76 mb, a 73% reduction

433
00:27:47,570 --> 00:27:52,010
from baseline, and the response
latency was only 85 milliseconds.

434
00:27:52,010 --> 00:27:54,775
End to end well within the
threshold for a real time fee.

435
00:27:56,000 --> 00:27:59,270
Knowledge distillation from a
larger teacher model likely help

436
00:27:59,270 --> 00:28:03,260
maintain the higher accuracy despite
the significant size reduction.

437
00:28:04,010 --> 00:28:08,840
This demonstrates how these techniques
deliver tangible, real world benchmark

438
00:28:09,020 --> 00:28:11,240
performance benefits on the edge.

439
00:28:12,240 --> 00:28:17,045
Takeaway and implementation details
go What are the key table covers?

440
00:28:17,775 --> 00:28:20,145
First, establish performance targets.

441
00:28:20,205 --> 00:28:24,805
Be clear about the latency, accuracy,
power, size, budgets before the

442
00:28:24,805 --> 00:28:26,415
start start of optimization.

443
00:28:26,835 --> 00:28:29,385
Secondly, apply the
integrated optimization.

444
00:28:29,715 --> 00:28:31,635
Don't just rely on just one technique.

445
00:28:32,025 --> 00:28:36,405
Combine pruning, ization, distillation,
and potentially architectural choice.

446
00:28:36,705 --> 00:28:40,845
For the best reserves, their effects
are often multi multiplicative.

447
00:28:41,565 --> 00:28:45,015
Third test on target
hardware ator arent enough.

448
00:28:45,105 --> 00:28:49,425
Benchmark and profile directly on
the edge devices we deploy onto

449
00:28:49,635 --> 00:28:51,135
to find the real bottlenecks.

450
00:28:51,495 --> 00:28:53,895
Fourth, iterate with real world data.

451
00:28:54,075 --> 00:28:59,175
Edge environments can be unpredictable,
continuously collect data and refine the

452
00:28:59,175 --> 00:29:01,725
models after the deployment successfully.

453
00:29:01,725 --> 00:29:06,045
Deploying the generative AI at the edge
needs the systematic, iterative approach.

454
00:29:06,525 --> 00:29:08,025
Considering the whole pipeline,

455
00:29:09,025 --> 00:29:11,725
a quick word on benchmarking,
it's absolutely essential.

456
00:29:11,725 --> 00:29:14,935
To know this, we need to
define metrics clearly.

457
00:29:14,935 --> 00:29:19,255
Latency, throughput, accuracy, power,
create a standardized test environment,

458
00:29:19,255 --> 00:29:23,665
simulating real deployment conditions,
measure performance comprehensively

459
00:29:23,665 --> 00:29:28,075
across different hardware and workloads,
and critically use benchmark results to

460
00:29:28,090 --> 00:29:29,770
further guide optimization iterations.

461
00:29:30,865 --> 00:29:34,825
Systematic benchmarking validates
the automation efforts and ensure

462
00:29:34,825 --> 00:29:37,855
the solutions perform reliably and
consistently in the real world.

463
00:29:38,335 --> 00:29:42,755
This is important because if you have
seen, there are multiple choices to make.

464
00:29:43,115 --> 00:29:47,835
So what choices need to be made and
which techniques, which combination

465
00:29:47,835 --> 00:29:49,095
of techniques needs to be done?

466
00:29:49,515 --> 00:29:54,015
We can be only determined if we have a
standard benchmarking, so we can run this

467
00:29:54,015 --> 00:29:58,485
iteratively and find if the combination
actually reaches our benchmarks.

468
00:29:59,485 --> 00:30:01,765
Then finally, the implementation roadmap.

469
00:30:02,665 --> 00:30:06,865
What are the practical steps if we
have to do this First audit model

470
00:30:06,865 --> 00:30:11,915
requirements, understand the Constance
and performance needs, and then prototype

471
00:30:11,915 --> 00:30:16,145
optimization pipelines, test techniques
individually first to see their impact

472
00:30:16,895 --> 00:30:18,935
and implement combined approach.

473
00:30:19,055 --> 00:30:22,385
Integrate the chosen techniques,
pruning, ization, distillation

474
00:30:22,385 --> 00:30:24,455
carefully, and deploy and monitor.

475
00:30:25,125 --> 00:30:26,240
The observability aspect.

476
00:30:26,600 --> 00:30:30,630
Realize the optimized model, but
but include telemetry to gather

477
00:30:30,630 --> 00:30:33,460
real world data performance
data for continuous improvement.

478
00:30:35,050 --> 00:30:38,440
Start with audit, build
systematically, validate thoroughly,

479
00:30:38,500 --> 00:30:39,790
and monitor continuously.

480
00:30:42,550 --> 00:30:46,630
That concludes our look into engineering,
low latency, generative AI for the edge.

481
00:30:47,020 --> 00:30:50,980
By applying techniques like ization
pruning, knowledge distillation, and

482
00:30:50,980 --> 00:30:55,030
choosing appropriate architectures
and hardware, we can overcome resource

483
00:30:55,030 --> 00:30:59,530
constraints and unlock powerful AI
K capabilities directly on the edge.

484
00:30:59,980 --> 00:31:03,220
Thank you for your time and
attention, and have a good day.

