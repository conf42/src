1
00:00:00,360 --> 00:00:08,820
Hey everyone, so I am moder, and
this is gonna be my short talk on go

2
00:00:09,300 --> 00:00:15,840
manifest analysis of for businesses
when it comes to fine tuning LLMs or

3
00:00:15,840 --> 00:00:22,740
deciding between fine tuning and other
techniques out there in order to get

4
00:00:22,740 --> 00:00:25,470
to the final result that the use case.

5
00:00:25,830 --> 00:00:27,270
Company wants to solve.

6
00:00:27,270 --> 00:00:33,810
So like the most, like in every use
case, like the central question that

7
00:00:34,619 --> 00:00:40,459
every business operations or stakeholder
needs to answer is is it possible for

8
00:00:40,459 --> 00:00:48,140
us to use just pre-trained models that
are out there from open ai, from cloud?

9
00:00:48,830 --> 00:00:52,400
The reason being is because
they are out of the box.

10
00:00:52,400 --> 00:00:56,510
There is no need to do
any fine tuning there.

11
00:00:56,839 --> 00:00:59,269
They are fast, they are affordable.

12
00:01:00,050 --> 00:01:06,500
And the next most important question
is, for example, like in our use case.

13
00:01:06,860 --> 00:01:08,510
What is our use case?

14
00:01:08,510 --> 00:01:11,990
And in our use case, is there a need?

15
00:01:12,410 --> 00:01:18,290
A need for any data that is
not public, that is private?

16
00:01:18,530 --> 00:01:24,410
So you want L them to go over that
data, understand patterns in that

17
00:01:24,410 --> 00:01:29,320
data in order to answer some really
important question or was or call

18
00:01:29,320 --> 00:01:33,744
some really important function or
tool if you are building an agent.

19
00:01:34,244 --> 00:01:37,425
AI and then based on that, do something.

20
00:01:37,475 --> 00:01:41,935
So that's where if you have your own
private data do is it required or not?

21
00:01:42,385 --> 00:01:45,955
That's a really important question
that every stakeholder needs to answer.

22
00:01:46,165 --> 00:01:52,125
And then based on this answer, let's
say you need, you do need your data in

23
00:01:52,125 --> 00:01:54,735
the entire use case you wanna resolve.

24
00:01:54,795 --> 00:01:56,475
Then the question is do you.

25
00:01:57,255 --> 00:02:02,965
Fine tune your model on that data, or do
you just every time you wanna answer some

26
00:02:02,965 --> 00:02:09,375
question, you go and you retrieve the
most important bits and pieces of your

27
00:02:09,375 --> 00:02:15,165
private data, and then you just see, add
that in the model without fine tuning it.

28
00:02:15,795 --> 00:02:20,385
So these are two different, I would
say techniques out there where

29
00:02:20,385 --> 00:02:22,005
you can use your private data.

30
00:02:22,505 --> 00:02:26,845
So I feel like to answer this
question on what do you choose?

31
00:02:26,845 --> 00:02:33,045
Do we choose fine tuning or do you
choose rack where you retrieve the

32
00:02:33,045 --> 00:02:37,785
most important bits of data and
then use that as context in the LLM?

33
00:02:38,285 --> 00:02:39,755
I think like it's really important too.

34
00:02:40,310 --> 00:02:44,150
First of all, understand what is
fine tuning and what the fundamentals

35
00:02:44,210 --> 00:02:49,190
are in order and before we can
easily answer this question.

36
00:02:49,190 --> 00:02:54,610
So the fine tuning is all about
adjusting the weights of the

37
00:02:54,610 --> 00:02:57,140
model, with the new weights.

38
00:02:57,170 --> 00:03:01,460
And the new weights will have
knowledge of that private data as well.

39
00:03:01,540 --> 00:03:06,150
fine tuning also means, for example, you
have to modify how the models behave.

40
00:03:06,240 --> 00:03:10,800
You have the models need to unlearn
something or relearn something

41
00:03:10,800 --> 00:03:15,060
because you wanna, you want model
to like, just focus on your data

42
00:03:15,060 --> 00:03:17,190
and then forget about the past.

43
00:03:17,430 --> 00:03:24,320
And the way I learn LS or any deep
learning model models work is as

44
00:03:24,320 --> 00:03:26,420
you train and keep on training.

45
00:03:26,975 --> 00:03:32,225
The latest training always has
a precedent over the old one.

46
00:03:32,225 --> 00:03:39,415
So if you take a LLM or large language
model that's pre-trained on some

47
00:03:39,415 --> 00:03:46,615
natural language data, let's say if
I just find, tune that on medical

48
00:03:46,615 --> 00:03:49,525
data that LLM will eventually like.

49
00:03:50,025 --> 00:03:53,985
Learn, be more focused or learn
more stuff about medical data.

50
00:03:54,445 --> 00:03:59,185
and in order to do that,
it'll also unlearn some of the

51
00:03:59,185 --> 00:04:00,985
stuff that it learned before.

52
00:04:00,985 --> 00:04:04,535
So fine tuning in a sense,
will update your model.

53
00:04:04,835 --> 00:04:05,705
It waits.

54
00:04:06,205 --> 00:04:09,415
And there are two things that,
and then there, and then within

55
00:04:09,415 --> 00:04:12,655
the fine tuning domain, there are
two ways that you can fine tune.

56
00:04:13,165 --> 00:04:20,875
Number one is like you, you update
all the weights, every layer of your

57
00:04:20,905 --> 00:04:25,145
model, which is really expensive
by the way, because, let's say.

58
00:04:26,105 --> 00:04:28,985
The models that we have
right now out there.

59
00:04:29,345 --> 00:04:38,415
So one way to fine tune is like find,
just updating all the weights in, all

60
00:04:38,415 --> 00:04:42,375
the weights right now that we have in
our model, which is really expensive

61
00:04:42,375 --> 00:04:48,585
by the way, because as I said before,
large models, they are really big.

62
00:04:48,585 --> 00:04:51,195
They have one 50 billion parameters wide.

63
00:04:51,195 --> 00:04:53,445
So let's say if you wanna go and.

64
00:04:54,090 --> 00:04:57,600
Update or the entire model.

65
00:04:57,810 --> 00:05:03,030
You need a really big hardware set up,
which is really expensive by the way.

66
00:05:03,240 --> 00:05:10,590
And the other way of fine tuning, which
is also provided out of the works by

67
00:05:11,100 --> 00:05:18,710
Open AI cloud and other companies is just
Fine tuning a few layers towards the end.

68
00:05:18,710 --> 00:05:25,280
It's called, meter efficient fine tuning
because like you are only, and then here

69
00:05:25,280 --> 00:05:28,550
you only fine tuning the last few layers.

70
00:05:29,120 --> 00:05:33,650
The advantage being is like the
last few layers will be able

71
00:05:33,650 --> 00:05:35,540
to retain the knowledge that.

72
00:05:36,365 --> 00:05:39,155
Or learn from your own private data.

73
00:05:39,605 --> 00:05:46,205
Whereas all the other layers in the
model, which are already trained on other

74
00:05:46,205 --> 00:05:52,055
aspects of language, for example, grammar
and other stuff, and then emotions.

75
00:05:52,625 --> 00:05:55,775
So all that knowledge
you still need, right?

76
00:05:55,775 --> 00:06:01,355
So that's the overall idea of just
fine tuning a few layers and normally.

77
00:06:02,315 --> 00:06:04,715
That's much cheaper, that's much faster.

78
00:06:05,165 --> 00:06:09,095
You can, if you have a small
amount of your data, you can easily

79
00:06:09,095 --> 00:06:13,805
fine tune it and then the cost
won't be super high at the time.

80
00:06:13,805 --> 00:06:17,315
Requirement is also pretty low as well.

81
00:06:17,825 --> 00:06:19,691
And then you can move forward with that.

82
00:06:20,191 --> 00:06:28,076
So if I just recap everything right now on
Prompt engineering and then, fine tuning

83
00:06:28,076 --> 00:06:30,996
and rack, and then from scratch, as well.

84
00:06:30,996 --> 00:06:37,671
So if you are fine tuning the
entire model, then it's same as like

85
00:06:37,671 --> 00:06:39,411
building the model from scratch.

86
00:06:39,741 --> 00:06:43,191
It's not same as, but in the end,
for example, like you, since you are.

87
00:06:43,691 --> 00:06:47,531
It's like build, building the model
from scratch because like you have

88
00:06:47,531 --> 00:06:51,911
to update so many different layers
and weight in the entire model.

89
00:06:52,011 --> 00:06:56,241
and then you will end up having your
own cluster model, which you will

90
00:06:56,241 --> 00:07:00,171
have a full control over, but in
the end, like it's gonna be really

91
00:07:00,171 --> 00:07:08,181
expensive to train it from scratch
and then maintain that on the server.

92
00:07:09,051 --> 00:07:13,341
From scratching, train from scratch
is really important pathway.

93
00:07:13,821 --> 00:07:19,761
If you have terabytes or petabytes of
large amount of data and then you know

94
00:07:19,761 --> 00:07:25,281
that, you won't be able to fine tune,
get the results by just fine tuning

95
00:07:25,371 --> 00:07:28,366
the last few layers you need to like.

96
00:07:29,361 --> 00:07:33,111
Train majority of your entire model.

97
00:07:33,591 --> 00:07:40,461
And then in the end, the other important
consideration here is that let's say you

98
00:07:40,461 --> 00:07:46,581
want to keep your model private as well,
so that, so from scratch or taking the

99
00:07:46,581 --> 00:07:51,851
open source model and then retraining
it on your data makes a lot of sense.

100
00:07:52,351 --> 00:07:55,501
Let's say if you have a very small
amount of data and then you are not

101
00:07:55,501 --> 00:08:01,021
concerned about privacy, or if you are
even concerned about privacy based on

102
00:08:01,031 --> 00:08:07,211
the privacy factor, you will decide
to either use an open source model or

103
00:08:07,211 --> 00:08:10,361
you will use, the closed source one.

104
00:08:10,361 --> 00:08:12,161
For example, open AI cloud.

105
00:08:12,551 --> 00:08:16,181
And then when you once, and then once you
decide the close and open source model,

106
00:08:16,721 --> 00:08:18,401
since you have small amount of data.

107
00:08:18,901 --> 00:08:22,441
Then fine tuning the last few
layers makes a lot of sense.

108
00:08:22,441 --> 00:08:26,951
And then as a result, you will see
that, for example, the model will be

109
00:08:27,251 --> 00:08:32,621
better able to answer questions in the
domain of data that you have given.

110
00:08:32,621 --> 00:08:38,381
Let's say if you have given a really
specialized data on health tech,

111
00:08:38,681 --> 00:08:44,201
for example, then you can expect
a fine tuned model to answer.

112
00:08:44,846 --> 00:08:48,776
Health tech questions
better than the model.

113
00:08:48,776 --> 00:08:50,036
That's not fine tuning.

114
00:08:50,586 --> 00:08:55,116
and then for example, why would
you not use fine tuning, let's

115
00:08:55,116 --> 00:08:58,616
say your data is evolving a lot.

116
00:08:59,116 --> 00:09:04,786
Just to give one more example
on when you should use rag.

117
00:09:05,431 --> 00:09:12,871
Or fine tuning is, let's say like you
have a an LLM or an AI agent, which

118
00:09:12,871 --> 00:09:17,071
buys and sells stock based on the news.

119
00:09:17,311 --> 00:09:21,661
Since news is evolving and everything
is evolving, it makes no sense just

120
00:09:21,661 --> 00:09:25,351
to fine tune the LLM on all the news.

121
00:09:25,351 --> 00:09:30,651
It makes more sense to just
retrieve the latest news.

122
00:09:31,161 --> 00:09:38,181
In seconds, and then use that as
the context for the LLM to answer.

123
00:09:38,871 --> 00:09:44,121
Really important question about
whether to execute a trade or not.

124
00:09:44,571 --> 00:09:51,381
So in this case, you need up to date
information and fine tuning is not

125
00:09:51,381 --> 00:09:56,361
a bit feasible because you need to
execute the results really fast.

126
00:09:56,361 --> 00:10:01,131
And then the data over here is
also changing over time as well.

127
00:10:01,291 --> 00:10:04,651
So we can talk about
more about RAG as well.

128
00:10:04,921 --> 00:10:11,041
There are multiple types of rags, which
are, which work in different use cases.

129
00:10:11,371 --> 00:10:17,721
And this is something that can be part
two of this to in the future, as well.

130
00:10:18,221 --> 00:10:21,341
and then the last point over
here is like prompt engineering,

131
00:10:21,341 --> 00:10:22,396
which is pretty much like.

132
00:10:23,121 --> 00:10:27,771
that's all about, for example, adding
some context in the prompt where you think

133
00:10:27,771 --> 00:10:34,011
you only have a few small contexts that
you want LLM to take into account when

134
00:10:34,011 --> 00:10:36,521
answering a question or doing some task.

135
00:10:36,941 --> 00:10:39,671
And that context is pretty
small, then it makes sense.

136
00:10:39,671 --> 00:10:44,381
Just add that context in
pretty much like you are on.

137
00:10:44,881 --> 00:10:45,991
System prompts.

138
00:10:46,231 --> 00:10:52,831
So in short, like you add your small
content and system prom, if you have

139
00:10:53,281 --> 00:11:00,181
data that's changing a lot, then makes
sense to retrieve that data through rag.

140
00:11:01,111 --> 00:11:04,461
If you have a really, if you
have a decent amount of data.

141
00:11:05,271 --> 00:11:09,921
On some domain and then you wanna fine
tune LM to, to work on that domain,

142
00:11:10,461 --> 00:11:12,921
then fine tuning makes more sense.

143
00:11:12,921 --> 00:11:17,001
And then for example, like if you
have large amount of data, then

144
00:11:17,001 --> 00:11:22,851
going from going for building your
LM from scratch makes more sense.

145
00:11:22,901 --> 00:11:27,851
and then of course, for example, every
method that we discussed just now

146
00:11:27,851 --> 00:11:32,111
has different costs and everything.

147
00:11:32,111 --> 00:11:32,621
For sure.

148
00:11:32,621 --> 00:11:33,041
If you are.

149
00:11:33,551 --> 00:11:39,221
Training from scratch, that's gonna
be a really big hassle for sure.

150
00:11:39,551 --> 00:11:43,541
Fine tuning is a bit
cheaper and fine tuning.

151
00:11:44,501 --> 00:11:48,271
The most important thing that
you need is the compute resource

152
00:11:48,271 --> 00:11:49,741
you can easily have, right?

153
00:11:49,861 --> 00:11:55,901
There are so many different, even if you
are using open ai, they, they provide you

154
00:11:55,901 --> 00:11:58,661
with a way to fine tune open AI models.

155
00:11:59,411 --> 00:12:00,041
You don't need to.

156
00:12:00,851 --> 00:12:04,871
To care about compute
resources over here for sure.

157
00:12:04,871 --> 00:12:10,001
They will charge you more per token
for the model, which is fine tuned

158
00:12:10,001 --> 00:12:11,981
versus the one is not fine tuned.

159
00:12:12,761 --> 00:12:17,066
But there's, but in the end, like the
compute resource won't be too much.

160
00:12:17,616 --> 00:12:21,936
if you wanna op, fine tune an open
source model, then for sure you have

161
00:12:21,936 --> 00:12:24,456
to think about compute resources.

162
00:12:24,966 --> 00:12:26,586
The most important cause.

163
00:12:26,991 --> 00:12:31,741
Will be in data preparation because
like there, if there's a certain

164
00:12:31,741 --> 00:12:37,321
format that data needs to have,
and then let's say if Q data is

165
00:12:37,321 --> 00:12:40,621
spread around, it's in bit pieces.

166
00:12:41,311 --> 00:12:46,821
So compiling data collection, preparation
is something that takes so much of the

167
00:12:46,821 --> 00:12:52,221
time and that is where you need also
manpower and that is where you need.

168
00:12:53,016 --> 00:12:56,826
An investment into engineering as well.

169
00:12:57,326 --> 00:13:03,186
And then for example, it, so fine
tuning is not just a one time thing.

170
00:13:03,186 --> 00:13:10,026
It's quite possible your data is changing
every three months or every four months,

171
00:13:10,026 --> 00:13:13,506
or every six months depending on that.

172
00:13:13,986 --> 00:13:17,936
You have to also keep on fine tuning
as well, so you need to have a

173
00:13:17,936 --> 00:13:21,566
schedule for fine tuning your LLMs.

174
00:13:22,066 --> 00:13:27,316
And speaking about the hidden cost here,
the most important thing is like the

175
00:13:27,316 --> 00:13:33,076
delayed implementation because There, data
preparation involved, and then there's

176
00:13:33,076 --> 00:13:39,136
also fine tuning involved, which can take
a few days depending on the size of data.

177
00:13:39,946 --> 00:13:43,366
And then, for example, the most
important thing is, for example,

178
00:13:43,366 --> 00:13:47,176
maintaining that custom model as well.

179
00:13:47,716 --> 00:13:50,306
So yeah, that all adds up.

180
00:13:50,566 --> 00:13:50,786
But.

181
00:13:51,366 --> 00:13:57,246
If your use case is a use case where
you want your model to be an expert in a

182
00:13:57,246 --> 00:14:02,986
really niche domain, and then that, and
then you have your own private data as

183
00:14:02,986 --> 00:14:10,246
well on that domain than fine tuning is a
way to go, and then the benefits will way

184
00:14:10,246 --> 00:14:13,116
more than the cost and everything as well.

185
00:14:13,666 --> 00:14:20,196
as I said before, the cost reduction,
pretty much like what I see everyone

186
00:14:20,196 --> 00:14:27,566
doing, I haven't, people don't retrain
or fine tune the entire LLM because,

187
00:14:27,566 --> 00:14:29,606
just because that's super hard.

188
00:14:30,296 --> 00:14:37,016
In fact, they always go for an
approach where they only fine tune.

189
00:14:37,826 --> 00:14:44,236
One part of the model or last few layers
of the model, and then as a result,

190
00:14:44,236 --> 00:14:46,186
we have seen that's much quicker.

191
00:14:46,936 --> 00:14:52,696
You don't need as much hardware
as you, you required before.

192
00:14:53,416 --> 00:14:58,351
Number three, your knowledge of
your custom data will be so model

193
00:14:58,351 --> 00:15:01,776
will be better able to learn
that knowledge from custom data

194
00:15:02,646 --> 00:15:04,151
and then that knowledge will be.

195
00:15:04,746 --> 00:15:11,716
Stored in a few layers in the model, which
is gonna be, which is even better, versus

196
00:15:11,716 --> 00:15:17,686
spreading your knowledge about your custom
data around 65 billion parameter model.

197
00:15:17,716 --> 00:15:18,616
That's pretty big.

198
00:15:19,116 --> 00:15:21,306
And then we have, I
would say, a couple of.

199
00:15:21,806 --> 00:15:27,296
Tuning things, for example, as well, fine
tuning techniques right now out there.

200
00:15:28,166 --> 00:15:31,916
So yeah, like a last few
layers is a way to go.

201
00:15:32,306 --> 00:15:33,116
Always.

202
00:15:33,616 --> 00:15:38,596
And then moving on to was like the ROI.

203
00:15:39,096 --> 00:15:45,041
The ROI pretty much the entire ROI
equation depends on multiple things.

204
00:15:45,461 --> 00:15:51,161
For example, how periodically
you not wanna retrain everything

205
00:15:51,281 --> 00:15:53,261
after three months or four months.

206
00:15:53,801 --> 00:15:59,921
What's the inference cost quite possible
that if you are using LLM, the inference

207
00:15:59,921 --> 00:16:02,921
cause of a fine tune model is way higher.

208
00:16:03,591 --> 00:16:03,891
that's.

209
00:16:04,566 --> 00:16:05,466
Pretty important.

210
00:16:05,466 --> 00:16:10,026
And then the initial fine tuning
where you have to prepare data

211
00:16:10,506 --> 00:16:11,796
is really important as well.

212
00:16:11,796 --> 00:16:15,706
So all these things, factors,
once again, will go into your

213
00:16:15,706 --> 00:16:19,336
equation to find your ROI.

214
00:16:19,366 --> 00:16:23,436
What's the cause, what's the
advantage out there as well?

215
00:16:24,296 --> 00:16:27,386
in the end, on average, I would say.

216
00:16:27,896 --> 00:16:30,206
it's 20% or 15%.

217
00:16:30,206 --> 00:16:35,646
The cost is higher for a fine tune
model than the other way around.

218
00:16:36,246 --> 00:16:42,516
So if your advantages or the benefits
you will get out of is more than

219
00:16:42,516 --> 00:16:44,706
that, then it's totally makes sense.

220
00:16:45,516 --> 00:16:49,541
and then speaking about the
benefits, they are quite a lot.

221
00:16:50,051 --> 00:16:55,181
for example, if you wanna, if you have a,
if you wanna have LLM, be an expert in a

222
00:16:55,181 --> 00:17:04,451
domain, then fine tuning on that domain
will give you 15 to 30% more improvement

223
00:17:04,451 --> 00:17:08,086
in the accuracy of results compared to.

224
00:17:08,586 --> 00:17:15,216
Out of the box LLMs from open AI or from
cloud and from other vendors out there.

225
00:17:16,026 --> 00:17:19,446
The next most important thing
is, for example, as the model has

226
00:17:19,446 --> 00:17:25,936
more knowledge about that domain
and that knowledge is fed into.

227
00:17:26,701 --> 00:17:32,041
A few layers of the model where the
model can go and fetch that information.

228
00:17:32,581 --> 00:17:37,351
We also see around 50,
60% less hallucination.

229
00:17:37,351 --> 00:17:43,271
So in the use case where you
need higher accuracy, fine tuning

230
00:17:43,271 --> 00:17:44,981
is a way to go here as well.

231
00:17:45,481 --> 00:17:45,751
Yeah.

232
00:17:45,801 --> 00:17:46,491
moving on.

233
00:17:46,491 --> 00:17:49,931
For example, like since you have
spent fine tuning, You don't

234
00:17:49,931 --> 00:17:56,501
have to pass as much context
in your input or system token.

235
00:17:56,501 --> 00:18:01,851
So as a result, you can also, there
are cases where you can expect to have

236
00:18:01,911 --> 00:18:08,056
less inference calls just because like
you need to use less tokens and then.

237
00:18:08,961 --> 00:18:13,401
If you, if reasoning is really important
for you since you have better, more

238
00:18:13,401 --> 00:18:20,986
context, and so reasoning that you will
get out of LLM with the output will be

239
00:18:20,986 --> 00:18:24,856
also much better than before as well.

240
00:18:25,356 --> 00:18:27,856
And then the other benefits that are.

241
00:18:28,536 --> 00:18:33,726
Extremely important to consider
here are like, if you have your own

242
00:18:33,726 --> 00:18:40,516
fine tuned LLM on your own prior,
proprietary data, that also gives you

243
00:18:40,516 --> 00:18:43,066
an advantage because that's your own ip.

244
00:18:43,586 --> 00:18:49,436
that's the own unique model and expert
in some domain that you own as well.

245
00:18:49,886 --> 00:18:51,416
This is something that.

246
00:18:51,916 --> 00:18:57,376
Will surely go on to your asset books of
the company, something that you can easily

247
00:18:57,376 --> 00:19:01,486
think about leveraging or selling or.

248
00:19:01,986 --> 00:19:06,306
Or even like licensing
in the future as well.

249
00:19:06,876 --> 00:19:11,896
That's also gives you a really
huge advantage over, over everyone.

250
00:19:12,466 --> 00:19:17,266
And then for example, if you are fine
tuning on the open source model, it's

251
00:19:17,266 --> 00:19:22,606
quite possible that the fine tune model
on your domain, on the open Source

252
00:19:22,606 --> 00:19:29,056
one will perform much better than any
out of the books top models out there.

253
00:19:30,021 --> 00:19:35,091
From top printers and then in that case,
like you will end up owning everything

254
00:19:35,091 --> 00:19:43,041
that owning the entire LLM or really big
thing, which is an expert in that domain.

255
00:19:43,581 --> 00:19:48,131
and then for example, like if you're
going within an open source route as

256
00:19:48,131 --> 00:19:51,406
well, there's also the advantage of.

257
00:19:51,906 --> 00:19:55,656
Enhance security means
your sensitive data.

258
00:19:55,656 --> 00:20:01,176
Your LLM is gonna be trained or fine
tuned on who will stay within your system.

259
00:20:01,626 --> 00:20:06,706
And it's, and then that's a
much secure way of doing things.

260
00:20:07,396 --> 00:20:10,906
That's really important if you
are in a healthcare space or, or

261
00:20:10,906 --> 00:20:12,766
a government organization that.

262
00:20:13,336 --> 00:20:15,226
Totally cares about your data as well.

263
00:20:16,126 --> 00:20:19,336
And then, or if you are in the
industry where data is very

264
00:20:19,336 --> 00:20:20,626
important, you can't share.

265
00:20:20,626 --> 00:20:26,086
So for compliance reasons, you have to go
with the open source one as well there.

266
00:20:26,586 --> 00:20:34,046
Uh, and then I've also compiled
here different metrics or

267
00:20:34,046 --> 00:20:35,966
different that people reported.

268
00:20:36,701 --> 00:20:42,521
For example, like how fine tuning
were, was able to help them get

269
00:20:42,521 --> 00:20:44,201
better results in different domain.

270
00:20:44,201 --> 00:20:51,921
For example, yeahs are trained over
wide array of data, of language data.

271
00:20:52,421 --> 00:20:55,061
Legal data is a bit different.

272
00:20:55,061 --> 00:20:56,471
You have a different lingo.

273
00:20:56,681 --> 00:20:57,281
like a sim.

274
00:20:57,341 --> 00:21:01,091
A person who hasn't gone through
a law school will surely have.

275
00:21:01,591 --> 00:21:06,211
Have a problem reading all really
big legal documents, right?

276
00:21:06,271 --> 00:21:11,701
That is what, and that is also the case
with out of the books at the lamps.

277
00:21:11,701 --> 00:21:14,401
They're really good, but
they do struggle sometimes.

278
00:21:14,791 --> 00:21:18,791
That is where, like the legal
profession has found that if you fine

279
00:21:18,821 --> 00:21:24,771
tune that, these models on the legal
data, you do get better results.

280
00:21:25,251 --> 00:21:28,191
The same also goes for the healthcare.

281
00:21:28,786 --> 00:21:32,686
it's much better also
the financial services.

282
00:21:32,686 --> 00:21:37,096
For example, if you have your own
priority data, then you wanna train the

283
00:21:37,096 --> 00:21:40,156
model on, that's much better as well.

284
00:21:40,656 --> 00:21:46,866
Maybe you have a model to execute some
trades as before, and then in the,

285
00:21:46,866 --> 00:21:49,236
in that model, that execute train.

286
00:21:49,821 --> 00:21:53,571
One really important input is like
what's happening in the entire world

287
00:21:54,321 --> 00:21:56,301
based on news and all that stuff.

288
00:21:56,301 --> 00:21:58,431
And that's where can easily come in.

289
00:21:59,031 --> 00:22:02,766
And then the fine tuned model will
be able to pick up patterns from

290
00:22:02,766 --> 00:22:05,586
the world much better as well.

291
00:22:06,086 --> 00:22:12,716
And then in manufacturing, fine tuning
is super important here if you are.

292
00:22:13,076 --> 00:22:17,996
And then at the same time, All
the AI agents provide us in

293
00:22:17,996 --> 00:22:19,796
customer service out there.

294
00:22:20,096 --> 00:22:25,976
They have also found that, for
example, fine tuning LLMs to

295
00:22:26,516 --> 00:22:27,811
better understand a business.

296
00:22:28,311 --> 00:22:34,881
For a client makes the LLM perform better
and gives them the comparative advantage

297
00:22:35,001 --> 00:22:39,861
or edge over everyone, other vendors
out there because here you are building

298
00:22:39,861 --> 00:22:45,681
your own mini LLM that understands your
business and then if you are providing

299
00:22:45,681 --> 00:22:50,811
your customers with that, a really
personalized for customer support.

300
00:22:51,381 --> 00:22:56,961
They surely will get better results
than they will get from other AI

301
00:22:57,021 --> 00:22:59,211
customer support vendors out there.

302
00:22:59,691 --> 00:23:03,081
And so because you have better
results, which are the most

303
00:23:03,081 --> 00:23:05,071
important, metric out there.

304
00:23:05,571 --> 00:23:10,521
what are some different
technical considerations?

305
00:23:11,021 --> 00:23:17,421
If you are a tech person, going over this
fine tuning rack and other stuff, so the

306
00:23:17,421 --> 00:23:22,271
most important thing is as we discussed
before, is open source of closed source.

307
00:23:22,901 --> 00:23:30,856
It's data like, do you wanna train the
last few layers or the entire model?

308
00:23:30,916 --> 00:23:33,926
That's really important to consider
as well as I discussed before.

309
00:23:34,426 --> 00:23:38,806
And then if you're going for
the rack architecture, given

310
00:23:38,806 --> 00:23:42,106
that you need that data in real
time, that's revolving as well.

311
00:23:42,606 --> 00:23:46,146
they are some of the most important
techniques I've found so far

312
00:23:46,566 --> 00:23:52,176
are here, there is a different
concepts that you wanna understand.

313
00:23:52,776 --> 00:23:58,396
This is something that can be, Some
other talk where I go into detail on

314
00:23:58,396 --> 00:24:00,556
different rag techniques out there.

315
00:24:01,056 --> 00:24:06,636
So lastly, on a really high level
comparison between RAG and fine tuning,

316
00:24:06,636 --> 00:24:09,836
the cause, for fine tuning is fix.

317
00:24:09,836 --> 00:24:14,791
Whereas for rag, every
time you call an LLM.

318
00:24:15,226 --> 00:24:22,726
You might wanna call a rack to fetch
relevant information from your databases,

319
00:24:22,726 --> 00:24:25,456
or you have a real time course for sure.

320
00:24:25,956 --> 00:24:29,766
So initial course is really
high with the fine tuning.

321
00:24:30,351 --> 00:24:35,991
With rack, it's much lower With fine
tuning you alway, you always have to

322
00:24:36,041 --> 00:24:42,401
retrain or refin tune after the interval
once you get more and more data.

323
00:24:42,901 --> 00:24:47,381
And then for rack, you just have to
update your knowledge basis every time

324
00:24:47,381 --> 00:24:53,141
the LLM needs to extract some information
from knowledge base, LM can extract.

325
00:24:53,906 --> 00:24:58,671
The real time up to date
information as well for sure.

326
00:24:58,671 --> 00:25:03,591
For example, in the RAG pipeline, like
you have to call rag, you have to pass

327
00:25:03,591 --> 00:25:09,321
that to LLM, and then when you call
LLM, you also have to add rag data

328
00:25:09,351 --> 00:25:15,741
tokens into LLM, so you have more token
usage, so you have a really higher.

329
00:25:16,241 --> 00:25:21,551
Latency as well compared to fine
tune, reasoning, it's better or even

330
00:25:21,551 --> 00:25:24,101
same with the bot techniques as well.

331
00:25:24,651 --> 00:25:31,011
in terms of infrastructure, you
need a different infrastructure in.

332
00:25:31,446 --> 00:25:37,896
In both cases, if you are using OP out
of the box, LLM providers, there's no

333
00:25:37,896 --> 00:25:43,746
infrastructure required because you,
they just want you to provide them

334
00:25:43,746 --> 00:25:49,746
with data in some certain format, and
then once you upload data in open ai,

335
00:25:49,746 --> 00:25:53,166
it just does the fine tuning for you.

336
00:25:53,586 --> 00:25:58,416
There's no need to build or maintain
anything at all on your side.

337
00:25:59,391 --> 00:26:04,311
On the other hand for the rack,
you do need to build your vector

338
00:26:04,311 --> 00:26:07,001
database to store your embeddings.

339
00:26:07,601 --> 00:26:11,171
And pretty much there is some
investment required data as well.

340
00:26:11,671 --> 00:26:15,961
I think we already recovered
data sensitivity as well before.

341
00:26:16,461 --> 00:26:22,701
So in, in summary, you'll choose
fine tuning when the knowledge

342
00:26:22,731 --> 00:26:24,831
is not changing rapidly.

343
00:26:25,331 --> 00:26:29,141
You have a really high
volume of Curie, right?

344
00:26:29,171 --> 00:26:36,071
And then it makes sense to find, tune
it versus using rack because like

345
00:26:36,071 --> 00:26:37,541
you have a really high cost we have.

346
00:26:38,041 --> 00:26:40,401
And then, you will use fine tuning more.

347
00:26:40,901 --> 00:26:45,311
when you have really lar large amount
of domain knowledge that is required

348
00:26:45,821 --> 00:26:48,281
by the LLM to answer a question, right?

349
00:26:48,311 --> 00:26:53,741
That they're, and then in the end,
for example, like you need really fast

350
00:26:53,741 --> 00:26:58,181
results, like you need really high
accuracy reserve, that's where you will

351
00:26:58,181 --> 00:27:01,301
for surely go for fine tuning as well.

352
00:27:01,801 --> 00:27:11,101
And then you will choose rag, Where
the data is evolving by a lot and

353
00:27:11,101 --> 00:27:18,241
you need to extract relevant data in
real that's evolving in real time.

354
00:27:18,741 --> 00:27:22,812
You also use rag, for example,
if you wanna do citations, right?

355
00:27:23,682 --> 00:27:30,912
Fine tuning LL M1, know which document
had that information that LM learned?

356
00:27:31,752 --> 00:27:37,108
But with rag, since you are adding
extra knowledge required by LLM.

357
00:27:37,722 --> 00:27:41,382
In the context, LLM will,
you can easily cite as well.

358
00:27:41,982 --> 00:27:47,502
If you need citations, then RAG is
a way to go and you will use rag.

359
00:27:47,502 --> 00:27:52,092
If you have only small amount of data,
that fine tuning won't make any sense.

360
00:27:52,092 --> 00:27:56,772
You like, hey, I'll just add
that data in the context as well.

361
00:27:57,642 --> 00:28:01,297
and then once again, like you
don't wanna spend so much time.

362
00:28:01,797 --> 00:28:03,507
Initially fine tuning.

363
00:28:03,507 --> 00:28:04,587
There's no budget.

364
00:28:05,097 --> 00:28:08,157
That's also where fine tuning
is where to go as well.

365
00:28:08,787 --> 00:28:13,227
I do also have a few right now case
studies that I have mentioned here.

366
00:28:14,067 --> 00:28:19,422
I will be adding the link
of this artifact as well, so

367
00:28:19,422 --> 00:28:21,522
people can easily learn from it.

368
00:28:22,022 --> 00:28:28,622
So once again, key takeaways, like we
have different techniques out there.

369
00:28:29,287 --> 00:28:30,667
There's fine tuning.

370
00:28:31,177 --> 00:28:32,407
There's fine tuning.

371
00:28:32,407 --> 00:28:34,057
Only a few layers.

372
00:28:34,477 --> 00:28:36,277
There is rack.

373
00:28:36,577 --> 00:28:43,697
So depending on different scenarios, every
technique is really important, right?

374
00:28:44,417 --> 00:28:51,377
So we learn that fine tuning can be
something of an investment where you will

375
00:28:51,377 --> 00:28:54,462
end up building your own prior prietary.

376
00:28:55,247 --> 00:29:01,067
Model that works really well in,
in the domain you operate in.

377
00:29:01,157 --> 00:29:06,287
This is something that you can license
in the end, in the future or sell

378
00:29:06,797 --> 00:29:09,557
that's an asset that belongs to you.

379
00:29:10,057 --> 00:29:13,647
we also learned that, they are the cost.

380
00:29:14,147 --> 00:29:20,387
The direct course of fine tuning is the
initial course that you have to spend

381
00:29:20,867 --> 00:29:26,717
money on where you need data preparation,
and then we also learn that there is

382
00:29:26,717 --> 00:29:32,357
also the indirect course as well as
associated with that, and then fine

383
00:29:32,357 --> 00:29:39,257
tuning can be done faster if you only
like fine tune a small part of the model.

384
00:29:39,972 --> 00:29:44,142
Which would also means that your
knowledge will be better learned

385
00:29:44,142 --> 00:29:48,552
by the model as well, because it's
all confined to that small part.

386
00:29:49,052 --> 00:29:55,282
And then there is also a possibility
that I. Your use case will need

387
00:29:55,342 --> 00:29:58,072
both rack and both fine tuning.

388
00:29:58,072 --> 00:30:03,292
It's quite possible that there are two
types of information there is fixing.

389
00:30:03,292 --> 00:30:04,462
There's evolving.

390
00:30:04,882 --> 00:30:11,632
What's in evolving information can become
part of the rack, what's fixed and what's

391
00:30:11,632 --> 00:30:15,022
really in high large quantity data.

392
00:30:15,442 --> 00:30:21,502
That can easily become your fine tuning
can be used in fine tuning as well.

393
00:30:22,042 --> 00:30:23,662
so customer support.

394
00:30:24,022 --> 00:30:25,732
All the last tickets.

395
00:30:26,232 --> 00:30:30,882
That will resolve, pretty much
can be used for fine tuning LLM

396
00:30:30,882 --> 00:30:35,712
to tell LM, Hey, that's how you
resolve a customer support curie.

397
00:30:36,192 --> 00:30:42,642
And then the information about
policies refund what's available in

398
00:30:42,642 --> 00:30:46,182
our data or what's available in stock.

399
00:30:46,332 --> 00:30:46,987
This is.

400
00:30:47,492 --> 00:30:48,272
In the company.

401
00:30:48,272 --> 00:30:53,292
This is the evolving information, and
this can be part of the rag, as well.

402
00:30:53,292 --> 00:30:56,982
So there can be a use case
where you need both of these

403
00:30:56,982 --> 00:30:59,052
things, depending on the context.

404
00:30:59,602 --> 00:30:59,962
yep.

405
00:31:00,022 --> 00:31:04,402
So this is on a really high
level, what this is all about.

406
00:31:04,952 --> 00:31:11,132
why use LLM for, why use fine tuning and
why is that important for every business?

407
00:31:11,132 --> 00:31:13,802
Say tune for more talks in the future.

408
00:31:14,802 --> 00:31:15,072
Bye.

