1
00:00:00,500 --> 00:00:00,970
Hi.

2
00:00:01,484 --> 00:00:05,765
And welcome to this presentation
on 15 Essential Metrics for NGINX.

3
00:00:06,185 --> 00:00:07,255
I'm Dave McAllister.

4
00:00:07,295 --> 00:00:11,264
I'm the Senior Open Source Technologist
here for NGINX, which is part of

5
00:00:11,264 --> 00:00:14,714
F5, and I've been involved in the
open source scene since longer

6
00:00:14,714 --> 00:00:16,265
than it's been called open source.

7
00:00:17,005 --> 00:00:21,584
I'm glad to be here at the COM
42 DevOps presentation to talk

8
00:00:21,595 --> 00:00:26,935
about a project that's incredibly
important in today's internet world.

9
00:00:27,484 --> 00:00:31,184
Nginx, of course, is one of the most
popular web servers, and as you can

10
00:00:31,184 --> 00:00:39,035
see from the Netcraft survey, we're up
there running about 39 percent of all of

11
00:00:39,035 --> 00:00:42,245
the state of computers as of November.

12
00:00:42,915 --> 00:00:44,275
Netcraft runs this monthly.

13
00:00:44,275 --> 00:00:48,575
I don't have the latest numbers for
December at my fingertips, but not only

14
00:00:48,575 --> 00:00:57,145
that, not only does Nginx support HTTP
servers, indexes files, HTTP 2, HTTP3 and

15
00:00:57,145 --> 00:01:01,215
supports SSL, TSL, SNI, FastCache, etc.

16
00:01:01,705 --> 00:01:04,240
It can also be reverse proxy.

17
00:01:04,270 --> 00:01:05,720
It can be a load balancer.

18
00:01:06,170 --> 00:01:11,660
And in fact, as a load balancer, it's
also supports in band health checks.

19
00:01:12,590 --> 00:01:15,420
But many things can impact
the performance of NGINX.

20
00:01:15,730 --> 00:01:18,330
You can see an increase in request.

21
00:01:18,580 --> 00:01:22,020
You can see higher disk
throughput or network throughput.

22
00:01:22,370 --> 00:01:27,010
You might be starting to run out of
resources like CPU or memory, or So, your

23
00:01:27,010 --> 00:01:30,620
applications, your web applications may
actually have problems in them as well.

24
00:01:31,610 --> 00:01:32,850
Monitoring is essential.

25
00:01:33,240 --> 00:01:37,200
In our DevOps world, we need to be
able to look and understand what's

26
00:01:37,210 --> 00:01:42,319
happening at all times with our
applications, as well as how we are

27
00:01:42,319 --> 00:01:44,340
feeding that to our client base.

28
00:01:44,840 --> 00:01:46,799
And let's start by talking
about collecting the data.

29
00:01:47,129 --> 00:01:49,329
Now, Nginx has two categories.

30
00:01:49,829 --> 00:01:52,279
characterizations of projects.

31
00:01:52,319 --> 00:01:56,769
One is the Nginx open source core
project, and the metrics there

32
00:01:56,769 --> 00:02:02,509
are gathered through this, this
thing called status stub module.

33
00:02:03,009 --> 00:02:07,239
And over on the right, you can see
basically you add this single line.

34
00:02:07,494 --> 00:02:07,994
to your nginx.

35
00:02:07,994 --> 00:02:12,864
conf file, and you now will have
the ability to grab those metrics.

36
00:02:13,234 --> 00:02:16,264
There's eight metrics plus a
health check inside of here.

37
00:02:16,634 --> 00:02:20,384
Nginx Plus, which is a commercial
project, so I'm not as familiar with

38
00:02:20,384 --> 00:02:25,804
it, adds an API endpoint to query
specific metrics, and it has a lot of

39
00:02:25,804 --> 00:02:28,244
metrics, somewhere around 130 or so.

40
00:02:28,709 --> 00:02:32,409
And they get into far more details
and drill into more things.

41
00:02:33,229 --> 00:02:37,479
Both these offerings have the ability
to scrape logs so that you have even

42
00:02:37,489 --> 00:02:41,639
more capability of pulling metrics
and pulling data out of those.

43
00:02:42,149 --> 00:02:45,129
And while your choice of
visualization is left up to you,

44
00:02:45,439 --> 00:02:47,509
we use Grafana heavily internally.

45
00:02:47,984 --> 00:02:53,564
The NGINX Plus project does have
some built in dashboard capability.

46
00:02:54,304 --> 00:02:58,254
And with that, let's step
to the next part of this.

47
00:02:58,754 --> 00:03:01,624
You're going to hear me hammer
this, but the most essential

48
00:03:01,625 --> 00:03:06,430
metrics You need to track, depend
on your specific goals and needs.

49
00:03:07,320 --> 00:03:13,719
This is a set of metrics that will cover
a lot of general cases, but you may need

50
00:03:13,730 --> 00:03:15,929
to go deeper into certain characteristics.

51
00:03:16,500 --> 00:03:21,160
But keep in mind, these metrics
are not an absolute must do.

52
00:03:21,549 --> 00:03:26,040
They fit to a very common general
purpose case of running NGINX.

53
00:03:26,310 --> 00:03:28,480
Your mileage may vary.

54
00:03:29,030 --> 00:03:35,090
aligning these things with the various,
capabilities, we have request metrics.

55
00:03:35,300 --> 00:03:38,499
We can look at those as,
how do we get things, what's

56
00:03:38,500 --> 00:03:40,020
happening going to the server?

57
00:03:40,290 --> 00:03:42,869
Response messages, how is the
server feeding back to it?

58
00:03:43,149 --> 00:03:47,410
And so on down into things like
caching metrics, SSL metrics,

59
00:03:47,410 --> 00:03:50,860
because they do take performance
hits, as well as security metrics.

60
00:03:50,870 --> 00:03:53,840
Security is incredibly
important in today's internet.

61
00:03:54,375 --> 00:03:59,625
And so all these things feed
together to help us understand what's

62
00:03:59,625 --> 00:04:01,835
happening with each of these pieces.

63
00:04:02,095 --> 00:04:05,395
So with that, let's take
a look at the next set.

64
00:04:05,895 --> 00:04:07,685
Let's start with active connections.

65
00:04:08,045 --> 00:04:11,435
Obviously, this metric indicates
the number of currently active

66
00:04:11,435 --> 00:04:15,475
connections in your server, but it
helps you understand the load that

67
00:04:15,485 --> 00:04:18,235
is being placed on your server.

68
00:04:18,775 --> 00:04:22,695
It helps you start looking at things
like, does it scale your resources?

69
00:04:22,975 --> 00:04:25,615
Are your configurations running optimally?

70
00:04:25,915 --> 00:04:28,125
Do you need to handle
traffic more efficiently?

71
00:04:28,385 --> 00:04:31,405
It can lead to things like
understanding load balancing structures.

72
00:04:31,725 --> 00:04:37,295
How many active things are coming in
as a baseline for what you need to

73
00:04:37,295 --> 00:04:39,524
do to make NGINX perform efficiently.

74
00:04:39,904 --> 00:04:43,385
NGINX was built on the concept of
performance and efficiency all the way

75
00:04:43,385 --> 00:04:47,275
back when it was undertaken to solve
what was then known as the 10k problem.

76
00:04:48,255 --> 00:04:50,265
With this, there are two variables.

77
00:04:50,460 --> 00:04:54,680
The first one, connections
active, is in nginx open source.

78
00:04:55,390 --> 00:05:01,509
You can also, if you're using nginx plus,
use the connections, underscore, nginx

79
00:05:01,539 --> 00:05:06,049
underscore connections active, which is an
API input to which you can grab metrics.

80
00:05:06,599 --> 00:05:10,639
All of these metrics I'm talking about,
with the exception potentially of

81
00:05:10,879 --> 00:05:15,269
the health check, are most used in a.

82
00:05:15,679 --> 00:05:16,859
Strip chart model.

83
00:05:16,889 --> 00:05:21,479
In other words, a time series display
aggregated together so that you can

84
00:05:21,489 --> 00:05:26,119
best understand what's happening to
the underlying system at any given

85
00:05:26,119 --> 00:05:30,879
moment request rate, which is the next
piece inside of this request rate.

86
00:05:30,939 --> 00:05:33,959
Obviously, it's the number of requests
that are being handled per second.

87
00:05:34,439 --> 00:05:38,069
So this starts getting into
not just the connections.

88
00:05:38,079 --> 00:05:38,919
How many things are per second?

89
00:05:39,019 --> 00:05:40,219
pinging you at a time.

90
00:05:40,559 --> 00:05:44,069
This is how many things are
being asked of you at a time.

91
00:05:44,479 --> 00:05:46,499
And it helps you understand
your traffic problems.

92
00:05:46,819 --> 00:05:50,969
Most of the time, you're going to see this
in a time series display as a sawtooth.

93
00:05:51,419 --> 00:05:52,889
During the day, things will go up.

94
00:05:52,899 --> 00:05:54,529
During the night, things will go down.

95
00:05:54,869 --> 00:05:55,819
But not always.

96
00:05:56,069 --> 00:06:00,654
And because of this, You can start
looking at how to balance both your

97
00:06:00,654 --> 00:06:05,414
resources and your load balancer against
your traffic patterns, but sudden

98
00:06:05,414 --> 00:06:10,054
spikes in request rates can indicate
certain things that you need to be

99
00:06:10,054 --> 00:06:12,824
aware of, such as a traffic search.

100
00:06:13,154 --> 00:06:16,214
Did marketing decide that they're
going to start offering a new

101
00:06:16,224 --> 00:06:19,014
special between the hours of 3 a.

102
00:06:19,014 --> 00:06:19,194
m.

103
00:06:19,194 --> 00:06:19,764
and 6 a.

104
00:06:19,764 --> 00:06:19,964
m.?

105
00:06:20,464 --> 00:06:23,314
Is there an attack going
on against your system?

106
00:06:23,604 --> 00:06:26,894
Has somebody launched a
DDoS attack against you?

107
00:06:27,174 --> 00:06:31,534
And with this, in both cases, the
variable of choice would be requestTime.

108
00:06:32,094 --> 00:06:36,964
And requestTime is going to show you,
basically, the number and how many

109
00:06:37,414 --> 00:06:40,314
are being used on a per second basis.

110
00:06:40,814 --> 00:06:49,474
Important metric to have, because it is
the thing that is driving the engine.

111
00:06:49,974 --> 00:06:55,094
So response time, so if you think about
this, the one before request rate,

112
00:06:55,814 --> 00:07:01,964
response time says how long is the server
taking before it sends things back out.

113
00:07:02,374 --> 00:07:08,314
So response time metrics are not just
the process of your request, but also if

114
00:07:08,324 --> 00:07:12,449
it processes and sends something through
to an upstream server, In other words,

115
00:07:12,449 --> 00:07:14,169
you're using a microservices approach.

116
00:07:14,169 --> 00:07:15,469
You've got another application.

117
00:07:15,769 --> 00:07:18,719
You're pinging something
else for data, a database.

118
00:07:18,919 --> 00:07:23,269
This is going to indicate to you how
long it's taking before it is ready

119
00:07:23,269 --> 00:07:26,889
to send a response back to the client.

120
00:07:27,829 --> 00:07:32,739
This lets you understand how well
your applications are running, as

121
00:07:32,739 --> 00:07:36,439
well as start identifying bottlenecks,
particularly in a microservices basis.

122
00:07:37,129 --> 00:07:43,219
So once again, it's not just the serve web
server, it's also the upstream servers.

123
00:07:43,499 --> 00:07:48,259
And again, here you're going to want
to combine request time, how long

124
00:07:48,289 --> 00:07:50,519
things are taking, with response time.

125
00:07:51,059 --> 00:07:53,539
How long do things take
when they're coming in?

126
00:07:54,009 --> 00:07:56,589
How long do things take
before they go back out?

127
00:07:57,059 --> 00:08:00,449
And you can think of this sort
of as an endpoint measurement

128
00:08:00,739 --> 00:08:02,459
in a microservices, viewpoint.

129
00:08:02,799 --> 00:08:06,809
We now know how long between
activities that are happening.

130
00:08:07,109 --> 00:08:12,049
And again, in open source, use request
time and upstream response time to give

131
00:08:12,059 --> 00:08:14,129
you those two pieces of information.

132
00:08:14,629 --> 00:08:16,619
As much as we'd love to be
able to say everything was

133
00:08:16,619 --> 00:08:18,949
perfect, there's always errors.

134
00:08:19,189 --> 00:08:24,859
That's why we have so much focus on
our DevOps monitoring environments.

135
00:08:24,879 --> 00:08:27,379
It's because something
is likely to go wrong.

136
00:08:27,959 --> 00:08:32,629
I used to do a talk on the Murphy's
Laws for observability, and that's one

137
00:08:32,639 --> 00:08:34,469
of the things that's always very true.

138
00:08:34,879 --> 00:08:38,629
Something will always go wrong,
probably at the worst possible time.

139
00:08:39,484 --> 00:08:42,804
So monitoring error rates, in
this case, is mostly looking

140
00:08:42,804 --> 00:08:44,934
for 400 and 500 status codes.

141
00:08:45,264 --> 00:08:48,544
And it's crucial for identifying
those errors that exist with

142
00:08:48,544 --> 00:08:52,184
servers or with your applications.

143
00:08:52,574 --> 00:08:57,534
Again, keep in mind that most of
our web environments are no longer

144
00:08:57,534 --> 00:09:00,284
just sending static pages out.

145
00:09:00,284 --> 00:09:03,494
They're connecting to applications, and
they're connecting to databases, they're

146
00:09:03,494 --> 00:09:05,114
connecting to other things behind this.

147
00:09:05,899 --> 00:09:10,059
However, when you start seeing
a high error rate, it can mean

148
00:09:10,059 --> 00:09:11,169
a lot of different things.

149
00:09:11,419 --> 00:09:15,049
But the place to start checking, very
honestly, is the denial of service

150
00:09:15,049 --> 00:09:19,399
attack, to make sure that you aren't
being attacked from the outside.

151
00:09:19,909 --> 00:09:21,459
Look at application bugs.

152
00:09:21,529 --> 00:09:23,219
Is the application doing something wrong?

153
00:09:23,649 --> 00:09:27,319
Or do you simply have a mis
configuration, something that's causing

154
00:09:27,609 --> 00:09:30,529
a breakage between the incoming client.

155
00:09:30,914 --> 00:09:34,844
and the server, or as we
return that information.

156
00:09:35,434 --> 00:09:38,534
And here status is your
variable of choice.

157
00:09:38,594 --> 00:09:41,314
Status will give you that
particular set of information.

158
00:09:41,814 --> 00:09:45,824
We do still have to consider
our underlying resources,

159
00:09:45,924 --> 00:09:47,414
CPU and memory usage.

160
00:09:48,124 --> 00:09:53,825
NGINX uses resources And even
though it is designed to be small,

161
00:09:53,895 --> 00:09:58,415
efficient, and highly performant,
you may still need to optimize the

162
00:09:58,425 --> 00:10:01,035
resources, especially as loads grow.

163
00:10:01,515 --> 00:10:05,825
And so looking at these can start
telling you what your resource

164
00:10:05,825 --> 00:10:08,955
optimization, whether you need
hardware upgrades, as well as

165
00:10:08,955 --> 00:10:10,725
helping some of the planning aspects.

166
00:10:11,175 --> 00:10:14,875
Here with open source, we recommend
using system monitoring tools,

167
00:10:14,875 --> 00:10:19,005
top H top VM set and NGINX plus.

168
00:10:19,365 --> 00:10:24,115
provide specific endpoints to allow
you to look at what process CPU

169
00:10:24,125 --> 00:10:27,765
is being used by NGINX and process
memory is being used by NGINX.

170
00:10:28,295 --> 00:10:32,345
But in any case, it's important to
look at what the underlying structure

171
00:10:32,355 --> 00:10:34,760
is as you are building these out.

172
00:10:35,260 --> 00:10:39,410
SSL handshakes are also equally important.

173
00:10:39,770 --> 00:10:44,780
And in fact, in many ways, the SSL
handshake is something that But

174
00:10:44,920 --> 00:10:47,020
we can't live without these days.

175
00:10:47,660 --> 00:10:55,980
So the SSL handshake, for instance,
lets us start looking at how well we

176
00:10:55,990 --> 00:10:59,780
are driving things as they go through.

177
00:11:00,280 --> 00:11:04,690
The long handshake time usually means
there's a configuration problem.

178
00:11:05,410 --> 00:11:08,190
And so if something's taking
a long handshake between these

179
00:11:08,190 --> 00:11:12,320
things, it can also mean that
your hardware is running slow.

180
00:11:13,060 --> 00:11:17,130
And that will be indicated by the
previous one looking at CPU and memory.

181
00:11:17,930 --> 00:11:21,970
Nginx, Open Source, and Plus
both provide variables to look

182
00:11:21,970 --> 00:11:24,500
at that handshake time frame.

183
00:11:25,130 --> 00:11:27,430
That's incredibly crucial these days.

184
00:11:27,500 --> 00:11:29,370
We need these secure connections.

185
00:11:29,690 --> 00:11:33,640
This will let you know that your
secure connection is not slowing

186
00:11:33,640 --> 00:11:35,500
down your requests and responses.

187
00:11:36,000 --> 00:11:39,180
And then, as we move through, throughput.

188
00:11:39,680 --> 00:11:44,195
to quote John Mashey, one of my favorite
computer scientists, bandwidth lives

189
00:11:44,195 --> 00:11:49,755
forever, or, sorry, latency lives
forever, throughput costs money.

190
00:11:50,550 --> 00:11:55,630
And so if we overscale or underscale,
it's costing us money in one ways.

191
00:11:55,970 --> 00:12:00,990
So throughput measures the total
amount of data that is sent to clients.

192
00:12:01,520 --> 00:12:03,350
And it helps us understand our data flow.

193
00:12:03,670 --> 00:12:07,570
That data flow is crucial because
we have to be able to handle

194
00:12:07,580 --> 00:12:10,560
that required bandwidth at peak.

195
00:12:11,230 --> 00:12:15,950
And so understanding our peak
loads in terms of connections,

196
00:12:16,230 --> 00:12:18,300
request rates, and throughput.

197
00:12:18,700 --> 00:12:23,220
mean that we can understand how best to
balance our resources, particularly for

198
00:12:23,220 --> 00:12:29,295
instance in a Elastic environment, be it
a Kubernetes environment or something else

199
00:12:29,305 --> 00:12:32,295
that can scale up and down based on load.

200
00:12:33,155 --> 00:12:37,515
Keep in mind that throughput is
also one of your limiting factors.

201
00:12:38,035 --> 00:12:41,735
Your limiting factors are going
to determine the ultimate point

202
00:12:41,755 --> 00:12:43,555
of contact to what you are.

203
00:12:44,055 --> 00:12:48,225
ByteSent for Nginx open source,
plus will give you this information.

204
00:12:48,455 --> 00:12:52,875
I don't know if it's nginx, open source,
or plus will give you that information

205
00:12:53,115 --> 00:12:57,945
so that you can best understand what's
going on in the throughput basis.

206
00:12:58,445 --> 00:13:00,775
And then finally, blocked request.

207
00:13:01,155 --> 00:13:06,385
I'm thinking of this is finally maybe not
blocked request can help you identify.

208
00:13:06,930 --> 00:13:08,420
potential security threats.

209
00:13:09,220 --> 00:13:11,190
You have to worry about
things that are blocked.

210
00:13:11,390 --> 00:13:17,130
If you see a high number of blocked
requests, you got possibly an ongoing

211
00:13:17,130 --> 00:13:21,810
attack, somebody trying to penetrate
your system, or you may have a bunch

212
00:13:21,810 --> 00:13:23,900
of just unauthorized access attempts.

213
00:13:24,300 --> 00:13:28,470
This does not always mean
some outside malicious actor.

214
00:13:29,000 --> 00:13:34,320
It can be as simple as configuration
error or security policy being

215
00:13:34,320 --> 00:13:36,080
a little over tweaked for this.

216
00:13:36,400 --> 00:13:39,730
Status again will tell you that
your blocked request is going here.

217
00:13:40,050 --> 00:13:44,730
With Nginx Plus, you can get a specific
security blocked request count.

218
00:13:45,230 --> 00:13:49,460
And those will help you understand,
really, whether you're going to

219
00:13:49,460 --> 00:13:53,850
need to break off the attack or
to revisit your security policies.

220
00:13:54,350 --> 00:13:56,070
Oh, and cache hit ratio.

221
00:13:56,880 --> 00:14:01,600
If you're using Nginx as a reverse
proxy, you really do want to have it

222
00:14:01,610 --> 00:14:03,450
hit the cache as much as possible.

223
00:14:04,095 --> 00:14:08,035
Far more efficient, faster,
more performant, better,

224
00:14:08,595 --> 00:14:10,255
all of the above for this.

225
00:14:10,985 --> 00:14:14,515
However, if you're getting a high
cache rate hit, it means you're

226
00:14:14,515 --> 00:14:15,855
getting lots of things from the cache.

227
00:14:16,005 --> 00:14:16,655
That's good.

228
00:14:17,525 --> 00:14:19,395
That reduces the load upstream.

229
00:14:19,895 --> 00:14:21,685
It means your response times are faster.

230
00:14:22,165 --> 00:14:25,335
And we know that users really
only care about their request.

231
00:14:26,095 --> 00:14:29,725
Whether it succeeded or
failed, and how long it took.

232
00:14:29,985 --> 00:14:33,695
And so the more you can cache, the
happier your users are likely to be here.

233
00:14:34,105 --> 00:14:37,395
Upstream Cache Status in Nginx
Open Source will tell you that.

234
00:14:37,895 --> 00:14:39,605
So there are these great metrics.

235
00:14:40,355 --> 00:14:41,235
So what?

236
00:14:41,855 --> 00:14:43,275
They're just a bunch of numbers.

237
00:14:43,655 --> 00:14:48,245
So let's take a look at some of
the ways that these metrics might

238
00:14:48,295 --> 00:14:50,365
actually apply and indicate it.

239
00:14:50,380 --> 00:14:53,960
let's take a first scenario on
monitoring active connections,

240
00:14:54,270 --> 00:14:55,250
and you've got a spike.

241
00:14:55,590 --> 00:14:59,810
You can see baseline active connections
about 500, normal distribution

242
00:15:00,150 --> 00:15:03,960
varies by about 10 percent up and
down, but all of a sudden you've

243
00:15:03,970 --> 00:15:06,550
got a spike that's hit to 1500.

244
00:15:07,050 --> 00:15:12,180
So your spike is substantial, and
your spike is probably going to

245
00:15:12,180 --> 00:15:14,260
be a serious outlier condition.

246
00:15:15,230 --> 00:15:16,390
Is it a traffic search?

247
00:15:16,390 --> 00:15:18,450
Did marketing start doing a special?

248
00:15:18,880 --> 00:15:23,090
Did, for you, those of you who
are, long enough back to remember

249
00:15:23,090 --> 00:15:25,360
Slashdot, maybe you got Slashdotted?

250
00:15:26,220 --> 00:15:31,070
All those things suddenly mean that
you're going to see a lot of activity.

251
00:15:31,480 --> 00:15:35,060
And because you're now getting more
active connections, it's likely that

252
00:15:35,060 --> 00:15:38,750
your request rate is rising, or your
response rate is going to be slower,

253
00:15:39,060 --> 00:15:40,840
and possibly even your error rate.

254
00:15:41,210 --> 00:15:43,120
You don't have the same thing.

255
00:15:43,170 --> 00:15:45,350
And those are all caused by
an active connection spike.

256
00:15:45,900 --> 00:15:48,530
But, it could also indicate a DDoS attack.

257
00:15:49,130 --> 00:15:52,010
Crosscheck your 4 0 4 and four
oh threes to see what's going

258
00:15:52,010 --> 00:15:53,300
on with those error cases.

259
00:15:53,900 --> 00:15:56,780
You might also suddenly be
running out of limitations.

260
00:15:57,080 --> 00:16:02,540
Maybe something has happened that your
load balancer is trying to do some weird

261
00:16:02,960 --> 00:16:08,410
things in re reflecting things to a single
system rather than multiple systems, and

262
00:16:08,410 --> 00:16:10,660
so your spread is not quite the same.

263
00:16:10,930 --> 00:16:14,530
So you may need to look at resources
that are underlying this structure.

264
00:16:15,030 --> 00:16:18,910
Scenario 2, also in Active
Connections, a gradual increase.

265
00:16:19,030 --> 00:16:21,040
Same baseline, same normal operation.

266
00:16:21,440 --> 00:16:24,600
But now, all of a sudden,
your numbers are increasing.

267
00:16:24,710 --> 00:16:26,590
Oh, and sorry, there's a typo.

268
00:16:26,590 --> 00:16:28,100
I left out a zero on the one.

269
00:16:28,200 --> 00:16:29,790
100 should be 1000.

270
00:16:30,350 --> 00:16:31,800
And the request rate goes up.

271
00:16:32,000 --> 00:16:34,260
Equivalently, the response rate goes up.

272
00:16:34,290 --> 00:16:35,230
The error rate goes up.

273
00:16:35,730 --> 00:16:36,480
Maybe this is a good thing.

274
00:16:37,055 --> 00:16:39,865
Maybe you're growing the number of
people that are using your system.

275
00:16:40,275 --> 00:16:43,105
That's a good thing, which means
that you need to look at planning

276
00:16:43,285 --> 00:16:45,095
and scaling your resources.

277
00:16:45,555 --> 00:16:50,095
Here you can use the gradual increase,
looking at throughput, looking at

278
00:16:50,095 --> 00:16:54,560
requests, looking at response times,
to start planning What you need to

279
00:16:54,560 --> 00:16:58,170
adapt to add the resources necessary.

280
00:16:59,060 --> 00:17:01,950
Maybe your systems are getting saturated.

281
00:17:02,230 --> 00:17:05,670
Maybe you're running out of memory on a
subsystem and it can't handle the load

282
00:17:06,020 --> 00:17:09,480
and therefore it's slowing down, which
is going to slow down everything else.

283
00:17:09,700 --> 00:17:14,600
So you need to look at whether or not
your underlying system structure is

284
00:17:14,640 --> 00:17:16,100
capable of meeting the structures.

285
00:17:16,550 --> 00:17:22,100
And then if you're in a ephemeral
environment where you can be elastic

286
00:17:22,100 --> 00:17:25,200
behavior, are you scaling correctly?

287
00:17:25,750 --> 00:17:30,060
Because are you scaling to such a
point that you're not scaling an

288
00:17:30,060 --> 00:17:32,740
application microservice correctly?

289
00:17:33,050 --> 00:17:37,850
Or maybe your load balancer isn't
recognizing new services as they come on.

290
00:17:38,350 --> 00:17:38,800
And then.

291
00:17:39,300 --> 00:17:42,250
Also in connections, all of a
sudden things are just bouncing

292
00:17:42,260 --> 00:17:43,950
all the heck over the place.

293
00:17:44,320 --> 00:17:46,850
You don't quite understand, but
you look at this thing and your

294
00:17:46,880 --> 00:17:51,570
connections are going, 400, then they
double, then they drop down, then they

295
00:17:51,580 --> 00:17:55,580
triple, and then they go back to the
mean, and you're all over the board.

296
00:17:55,980 --> 00:17:59,460
And each of these things, by the way,
does tend to impact the other things.

297
00:17:59,540 --> 00:18:02,330
More connections usually means a
higher request rate, which means

298
00:18:02,330 --> 00:18:06,400
usually more response time, which
can mean an increase in errors.

299
00:18:07,130 --> 00:18:08,860
But, you've got intermittent traffic.

300
00:18:09,350 --> 00:18:11,650
Where is the intermittent
traffic coming from?

301
00:18:11,720 --> 00:18:13,990
Again, is it something periodic?

302
00:18:14,000 --> 00:18:16,350
And you will see these slopes.

303
00:18:16,770 --> 00:18:21,270
All of a sudden, somebody has decided
that they need to run a report

304
00:18:21,560 --> 00:18:24,840
and it's hitting your system and
eating all of the resources talking

305
00:18:24,840 --> 00:18:26,560
to your data repositories here.

306
00:18:26,940 --> 00:18:30,280
Or, you've got a bunch of batch
work that's suddenly being

307
00:18:30,300 --> 00:18:32,060
kicked off for whatever reason.

308
00:18:32,400 --> 00:18:36,980
Or, again, you've been slash dotted
or you've got a marketing attempt.

309
00:18:37,360 --> 00:18:39,890
But something is causing
the activity going here.

310
00:18:40,390 --> 00:18:42,570
It doesn't necessarily
mean an outside force.

311
00:18:42,690 --> 00:18:44,638
It could be a load balancing issue.

312
00:18:44,638 --> 00:18:48,590
Your load balancer needs to be
looked at the configuration so that

313
00:18:48,590 --> 00:18:51,170
you can make sense that the load
balancer is handling the traffic

314
00:18:51,440 --> 00:18:56,730
appropriately to be able to pass the
traffic to the resources as necessary.

315
00:18:57,340 --> 00:19:01,125
And then Your applications can bottleneck,
and remember those bottlenecks can

316
00:19:01,135 --> 00:19:02,895
be outside of your application.

317
00:19:03,225 --> 00:19:07,725
Likewise, in, for instance, a Kubernetes
environment, you may actually be looking

318
00:19:07,725 --> 00:19:15,415
at your application being perfect, scaling
correctly, but being put into a node, a

319
00:19:15,415 --> 00:19:21,195
pod, that has a noisy neighbor problem,
who's eating all the memory, even though

320
00:19:21,365 --> 00:19:23,615
your application is perfectly correct.

321
00:19:23,995 --> 00:19:26,835
Your application will show up
as the problem, not the one

322
00:19:26,835 --> 00:19:27,805
that's eating all the memory.

323
00:19:28,330 --> 00:19:31,370
Look at your underlying structures
and underlying resources.

324
00:19:31,870 --> 00:19:34,810
So let's now take a look at the other one
that I talked about, a blocked request.

325
00:19:35,080 --> 00:19:37,150
And the reason I want to talk
about blocked requests is that

326
00:19:37,200 --> 00:19:41,140
quite often, it has to deal
with the security aspects here.

327
00:19:41,640 --> 00:19:44,220
Active connections, but you can see
down here, your blocked requests

328
00:19:44,220 --> 00:19:45,720
are running about 5 per minute.

329
00:19:45,890 --> 00:19:49,190
That's actually, not
too out of the ordinary.

330
00:19:49,560 --> 00:19:51,620
Maybe even a little low at times here.

331
00:19:52,240 --> 00:19:53,935
And you're running 4 to 6.

332
00:19:54,215 --> 00:19:58,505
Regular basis for operations
and all of a sudden your blocked

333
00:19:58,505 --> 00:20:01,255
request scale by a factor of 10.

334
00:20:01,755 --> 00:20:08,355
There is something going on and this
is usually one of the three things.

335
00:20:08,495 --> 00:20:09,425
It's a security threat.

336
00:20:09,875 --> 00:20:15,534
Somebody is attacking your system
and here go back and looked at that

337
00:20:15,534 --> 00:20:20,543
status issues to see what's going
on and seeing if you're getting

338
00:20:20,543 --> 00:20:22,630
a lot of failed authorizations.

339
00:20:22,790 --> 00:20:23,370
inside of here.

340
00:20:24,150 --> 00:20:26,110
You may have changed
something in your firewall.

341
00:20:26,210 --> 00:20:28,090
You may have changed a WAF configuration.

342
00:20:28,620 --> 00:20:32,750
Check for recent changes because
those recent changes may be blocking

343
00:20:32,760 --> 00:20:35,480
things that are false positives.

344
00:20:36,020 --> 00:20:39,520
And those false positives not only
can be from a configuration change.

345
00:20:40,000 --> 00:20:44,710
But maybe you've set up a whole new
series of security rules that are looking

346
00:20:44,750 --> 00:20:47,270
more deeply at the incoming connections.

347
00:20:47,770 --> 00:20:51,080
If you have security rules, you've
also got configurations for those.

348
00:20:51,560 --> 00:20:57,190
And so blocked request spiking is
usually some level of security threat.

349
00:20:57,640 --> 00:21:04,260
but it can also be generated by
internal problems or internal concerns.

350
00:21:04,760 --> 00:21:06,810
Same thing for a gradual increase.

351
00:21:07,390 --> 00:21:10,050
You might have become
a target for attackers.

352
00:21:10,050 --> 00:21:13,940
You may have just suddenly
shown up on somebody's radar and

353
00:21:13,940 --> 00:21:15,775
they are beginning to scale up.

354
00:21:16,105 --> 00:21:18,505
And a tag here up your security.

355
00:21:18,725 --> 00:21:22,235
Again, looking at the other
features and other functionalities,

356
00:21:22,505 --> 00:21:23,985
your requests are going up here.

357
00:21:24,315 --> 00:21:27,245
Is your status still staying the same.

358
00:21:28,045 --> 00:21:33,695
Your security rules can play a role
here, but it can also be a bot approach

359
00:21:33,795 --> 00:21:38,405
that somebody's botting you to death
for this or again, your resource.

360
00:21:38,695 --> 00:21:41,565
So this could be suddenly
slowing things down.

361
00:21:41,575 --> 00:21:44,095
You or not accepting things
the same way they are.

362
00:21:44,755 --> 00:21:48,625
Remember that blocks still take resources.

363
00:21:49,065 --> 00:21:56,435
So as your blocks start going up, then
all of a sudden your performance may

364
00:21:56,535 --> 00:22:02,015
decline because you're no longer capable
of meeting the needs of these things.

365
00:22:02,725 --> 00:22:04,245
lots of different pieces that can happen.

366
00:22:04,535 --> 00:22:07,385
But it's very important to
understand this gradual increase.

367
00:22:08,255 --> 00:22:10,365
And then, high variability.

368
00:22:10,645 --> 00:22:11,625
Similar structure.

369
00:22:11,935 --> 00:22:13,695
Bouncing the heck all over the place.

370
00:22:13,715 --> 00:22:15,125
Bouncing all over the place.

371
00:22:15,755 --> 00:22:19,145
Blocks request 5 to 30 to 10 to 40 to 5.

372
00:22:20,065 --> 00:22:22,305
This is an intermittent
attack, more than likely.

373
00:22:22,385 --> 00:22:23,795
Somebody is just probing.

374
00:22:24,405 --> 00:22:26,575
And it could be a malicious actor.

375
00:22:26,915 --> 00:22:28,305
It could be an automated script.

376
00:22:28,315 --> 00:22:30,835
It could be botting coming in to you.

377
00:22:31,115 --> 00:22:34,795
But nonetheless, something from
the outside is probably doing this.

378
00:22:35,095 --> 00:22:40,145
And it's most often seen through
load balancer or proxy issue.

379
00:22:40,645 --> 00:22:44,625
Because of a blocking structure, you
end up with an inconsistent load.

380
00:22:44,915 --> 00:22:47,235
If you're blocking, you're
not passing through.

381
00:22:47,645 --> 00:22:51,425
And therefore, your load balancer
is not quite sure how to load

382
00:22:51,435 --> 00:22:54,515
balance because it's got a lot of
things coming in, but not all of

383
00:22:54,515 --> 00:22:55,965
them are being passed through here.

384
00:22:56,615 --> 00:23:00,705
In this case, you might want to take a
look at what your load balancer structure

385
00:23:00,705 --> 00:23:02,745
looks like in its configuration basis.

386
00:23:03,225 --> 00:23:07,890
You can also look for application
misconfigurations or Maybe you have

387
00:23:07,890 --> 00:23:12,520
some vulnerabilities that somebody is
taking advantage of just every now and

388
00:23:12,520 --> 00:23:14,030
then hoping to slide underneath you.

389
00:23:15,295 --> 00:23:18,455
While I've talked about metrics, I
also want to bring up, OpenTelemetry.

390
00:23:18,455 --> 00:23:23,670
OpenTelemetry module for NGINX is an
open source project that brings the

391
00:23:23,810 --> 00:23:28,200
OTEL tracing environments through NGINX.

392
00:23:28,570 --> 00:23:32,730
Fully supports W3C trace,
OTLP, gRPC formats.

393
00:23:33,150 --> 00:23:38,750
And we did this because NGINX's number
one concern is performance and efficiency.

394
00:23:39,080 --> 00:23:41,420
And we looked at the current
modules that were out.

395
00:23:41,595 --> 00:23:42,095
Out there.

396
00:23:42,335 --> 00:23:47,675
We found that most of them had a 50
percent hit on request performance,

397
00:23:48,115 --> 00:23:49,225
and that was unacceptable.

398
00:23:49,405 --> 00:23:52,775
So we wrote our own native
module performance hit.

399
00:23:53,130 --> 00:23:53,680
It's about 10%.

400
00:23:54,330 --> 00:23:56,680
Tracing always hits your performance.

401
00:23:56,690 --> 00:24:00,500
Sorry guys, it does for this,
but 10 percent is not too bad.

402
00:24:01,220 --> 00:24:05,890
Now, the nice thing is that this is
set up inside of your Nginx application

403
00:24:05,890 --> 00:24:09,540
configuration, so it doesn't require
any separate structures or work.

404
00:24:09,770 --> 00:24:10,430
Inside of here.

405
00:24:10,840 --> 00:24:14,220
It allows you to
dynamically control traces.

406
00:24:14,510 --> 00:24:17,430
You can use cookies, you can use
tokens, you can use variables.

407
00:24:17,830 --> 00:24:23,260
And that tracing means you can turn it on
and off, scale it up and down as you need.

408
00:24:23,855 --> 00:24:26,625
And so without stopping your
environment, you can actually

409
00:24:26,625 --> 00:24:28,625
control your tracing throughput.

410
00:24:29,595 --> 00:24:33,785
short answer, the client request
comes in, the request goes into the

411
00:24:33,785 --> 00:24:38,705
application that feeds back to NGINX
and NGINX sends the request back.

412
00:24:39,275 --> 00:24:45,085
NGINX is the open telemetry collector
which via OTLP can send it off to

413
00:24:45,085 --> 00:24:49,535
an OTEL collector which then can go
into your web app, it can go into

414
00:24:49,535 --> 00:24:51,855
your storage, it can be displayed by.

415
00:24:52,290 --> 00:24:56,940
Almost everything these days because
everybody is using open telemetry.

416
00:24:57,470 --> 00:25:02,440
Likewise, open telemetry now supports
metrics and logs, and you will see

417
00:25:02,450 --> 00:25:08,340
those also becoming available, as
part of our expansion into, more

418
00:25:08,350 --> 00:25:13,980
monitoring capability for the NGINX,
open source core and NGINX plus.

419
00:25:14,480 --> 00:25:16,210
So here's a quick example.

420
00:25:16,775 --> 00:25:22,460
of the configuration, load the module
into it, you set up the exporter

421
00:25:22,520 --> 00:25:26,420
point, which is basically you're
talking to the OTEL collector, you

422
00:25:26,420 --> 00:25:29,720
tell it to listen in on it, and then
you say, I want to see the traces.

423
00:25:30,120 --> 00:25:32,370
TraceInject, and ProxyPaths.

424
00:25:32,890 --> 00:25:38,810
So each of these pieces, and it has the
ability to do sampling structures of all

425
00:25:38,810 --> 00:25:45,710
sorts, and because you can enable and
disable via a variable, you can have it

426
00:25:45,720 --> 00:25:48,700
on and only turn it on when you need it.

427
00:25:49,200 --> 00:25:50,850
The other side is Prometheus.

428
00:25:50,850 --> 00:25:55,910
And Prometheus is pretty much
de facto metrics structure.

429
00:25:56,370 --> 00:26:00,460
Prometheus, Nginx Prometheus
exporter is an open source exporter

430
00:26:00,480 --> 00:26:04,310
that takes Nginx metrics into a
form that Prometheus can scrape.

431
00:26:04,800 --> 00:26:08,290
And it reads either the open source
side, through the status module,

432
00:26:08,340 --> 00:26:09,830
or it can read the PLUS API.

433
00:26:10,390 --> 00:26:11,440
You can see, find it there.

434
00:26:11,780 --> 00:26:16,540
It supports the open source core, the
PLUS product, but it also supports the

435
00:26:16,650 --> 00:26:21,765
two Kubernetes projects in Gridscape
Controller and Gateway Fabric.

436
00:26:22,205 --> 00:26:26,295
And here you can see things like,
I can now see the types of things,

437
00:26:26,475 --> 00:26:33,655
how it's set up, and this allows
me to bring, again, metrics into a

438
00:26:33,655 --> 00:26:38,595
common vernacular so that they can be
displayed through your choice of tools.

439
00:26:39,145 --> 00:26:42,415
concluding, we've looked at
some key metrics for Nginx.

440
00:26:42,885 --> 00:26:47,235
Remember, they are Basic
and generic is structure.

441
00:26:47,695 --> 00:26:48,815
It's a starting point.

442
00:26:49,285 --> 00:26:50,835
Your mileage may vary.

443
00:26:51,155 --> 00:26:53,815
The things that are important to
you are dependent on what your

444
00:26:53,855 --> 00:26:56,375
outcome, desired outcome is.

445
00:26:56,885 --> 00:27:01,145
We looked at some scenarios, a couple
of them around haptic connections

446
00:27:01,175 --> 00:27:03,015
as well as blocked connections here.

447
00:27:03,345 --> 00:27:08,925
And We did underlie some of the
hardware concerns and software concerns.

448
00:27:09,255 --> 00:27:12,615
In a microservices environment, it's
not just enough to know that your

449
00:27:12,615 --> 00:27:14,545
server, web server, is working well.

450
00:27:14,865 --> 00:27:17,425
You also need to know that the
upstream servers are working well

451
00:27:17,695 --> 00:27:19,535
in conjunction with your web server.

452
00:27:20,205 --> 00:27:22,455
And it's not just the metrics.

453
00:27:23,255 --> 00:27:26,735
My favorite quote, and I'm going to
still use it here, the most effective

454
00:27:26,735 --> 00:27:29,065
debugging tool is still careful thought.

455
00:27:29,620 --> 00:27:31,940
Coupled with judiciously
placed print statements.

456
00:27:32,640 --> 00:27:37,640
Today, those print statements are our
metrics, our traces, and our logs.

457
00:27:38,270 --> 00:27:44,570
They give us the ability to use our
own insight and our own understanding

458
00:27:44,570 --> 00:27:49,060
of our environments to make sure
that what we deliver is what our

459
00:27:49,060 --> 00:27:51,440
clients and our users are looking for.

460
00:27:51,940 --> 00:27:55,365
And with that, My thanks
for, joining in today.

461
00:27:55,705 --> 00:28:00,795
You can find docs on performance for
open source, as well as plus, here.

462
00:28:01,035 --> 00:28:04,615
My slides are always on GitHub
and check me out on LinkedIn.

463
00:28:04,995 --> 00:28:08,725
I also mentioned two open source
projects that you may be interested in.

464
00:28:09,075 --> 00:28:14,045
The Nginx Prometheus Exporter and the
Open Telemetry Module for Nginx, both

465
00:28:14,065 --> 00:28:15,805
which are available through our GitHub.

466
00:28:16,305 --> 00:28:21,595
And with that, thanks again, and
enjoy the rest of COM 42 DevOps.

