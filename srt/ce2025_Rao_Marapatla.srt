1
00:00:00,130 --> 00:00:00,919
Hi, everyone.

2
00:00:01,680 --> 00:00:02,380
Good morning.

3
00:00:02,510 --> 00:00:03,260
Good evening.

4
00:00:04,250 --> 00:00:05,680
Hope you guys are doing good.

5
00:00:06,180 --> 00:00:07,000
This is the raw.

6
00:00:07,670 --> 00:00:10,389
Welcome back by another talk.

7
00:00:10,600 --> 00:00:18,470
today we were talking about how we can
build API ecosystem for our organizations.

8
00:00:18,970 --> 00:00:21,220
Definitely this will help us a lot.

9
00:00:21,270 --> 00:00:27,040
again, welcome to this presentation on
integration AI driven analytics and API

10
00:00:27,040 --> 00:00:29,030
ecosystem in nonprofit organizations.

11
00:00:29,530 --> 00:00:34,760
Let's explore how this framework combines
advanced one on one analytics with

12
00:00:35,690 --> 00:00:38,330
multi platform data synchronization.

13
00:00:38,830 --> 00:00:46,180
We're going to talk about how we're going
to bring data from different social media

14
00:00:46,209 --> 00:00:51,989
platforms into your existing database.

15
00:00:52,539 --> 00:00:53,310
so let's see.

16
00:00:53,810 --> 00:01:01,110
In today's world, there are a number
of third party vendors who can provide

17
00:01:01,110 --> 00:01:07,770
their service to bring data from all
the social channels like Facebook,

18
00:01:07,890 --> 00:01:14,260
LinkedIn, TikTok, YouTube, and all other
platforms, because they have already

19
00:01:14,350 --> 00:01:17,670
predefined, scripts, the programs, right?

20
00:01:18,210 --> 00:01:21,130
So they will provide the services.

21
00:01:21,920 --> 00:01:22,100
it's.

22
00:01:23,005 --> 00:01:28,605
Eventually it's our data, so they use
our, our credentials and whatnot, right?

23
00:01:28,665 --> 00:01:32,565
And, they're going to charge,
yearly subscriptions, to

24
00:01:32,565 --> 00:01:34,755
accommodate all the data needs.

25
00:01:35,525 --> 00:01:40,755
So today I wanted to explain
and provide, different ways, to

26
00:01:40,755 --> 00:01:43,389
avoid that, third party tools.

27
00:01:43,389 --> 00:01:46,549
And how you can inbuilt it.

28
00:01:47,049 --> 00:01:56,219
Your own platform data using this approach
to bring into your, the databases, right?

29
00:01:56,999 --> 00:01:58,159
yes, definitely.

30
00:01:58,219 --> 00:02:00,009
there is a scripting is involved.

31
00:02:00,139 --> 00:02:04,269
and there are the technologies which
we need to accommodate this process,

32
00:02:04,309 --> 00:02:06,429
which I'm going to discuss today.

33
00:02:07,089 --> 00:02:10,189
But definitely this is, Maintenance free.

34
00:02:10,549 --> 00:02:16,429
And, we don't have to pay, so many
dollars, on yearly basis, right?

35
00:02:16,469 --> 00:02:19,839
If it is a one time, so
maybe we can consider it.

36
00:02:19,899 --> 00:02:23,129
But what we need to do, we're
going to end up paying every

37
00:02:23,139 --> 00:02:25,239
year for the same subscription.

38
00:02:25,759 --> 00:02:29,039
So this solution is what we're
talking today, which will help

39
00:02:29,099 --> 00:02:35,089
us to bring your data into your
database using these approaches.

40
00:02:35,919 --> 00:02:39,669
assume that, assume that you
have the different data flows.

41
00:02:39,769 --> 00:02:44,649
for instance, the data you're looking
for the Facebook, Instagram, LinkedIn,

42
00:02:44,649 --> 00:02:49,964
Twitter, TikTok, Meta, all that,
all the social platforms, right?

43
00:02:50,014 --> 00:02:54,354
that it VA media or,
organic social media, right?

44
00:02:54,354 --> 00:02:54,684
Yeah.

45
00:02:55,204 --> 00:03:00,084
what we are going to do if, as
you see, my screen here, so we

46
00:03:00,084 --> 00:03:03,104
need the login details, right?

47
00:03:03,554 --> 00:03:04,934
So we are going to build.

48
00:03:05,224 --> 00:03:10,144
A script, with within the,
shehar, or no JS N right.

49
00:03:10,654 --> 00:03:18,454
And then, we prepare that API and there
are the two options we can accommodate to

50
00:03:18,454 --> 00:03:23,804
schedule that, the script, which or the a
PA which we build, and that will bring the

51
00:03:23,804 --> 00:03:27,334
data and that convert into the CS three.

52
00:03:27,924 --> 00:03:32,589
And they'll place it to the secure
location based on the, Based on

53
00:03:32,599 --> 00:03:38,229
the target location, which we
described in our, the script, right?

54
00:03:38,529 --> 00:03:39,859
And then, yes.

55
00:03:39,889 --> 00:03:40,289
Okay.

56
00:03:40,299 --> 00:03:42,389
Let's say, there are
a couple of scenarios.

57
00:03:42,679 --> 00:03:45,209
First, yes, we need the login details.

58
00:03:46,079 --> 00:03:49,399
The platform which you are
going to access, right?

59
00:03:49,579 --> 00:03:55,799
And then, we need to prepare the
script, with either Shehar or the, node

60
00:03:55,799 --> 00:04:01,689
Jason, with all that, a PE details and
endpoint and all those things, which

61
00:04:01,689 --> 00:04:03,009
we are going to talk in a minute, huh.

62
00:04:03,559 --> 00:04:04,879
and then, we need.

63
00:04:05,154 --> 00:04:08,544
To prepare the script and it
will automatically download the

64
00:04:08,554 --> 00:04:11,184
data from, the social platforms.

65
00:04:11,444 --> 00:04:15,054
It's not only social platform like
any platform if you wanted to bring,

66
00:04:15,064 --> 00:04:17,064
which is APA supported, right?

67
00:04:17,844 --> 00:04:21,514
Eventually, these solutions
will work on that, right?

68
00:04:21,604 --> 00:04:22,744
And, prepare that.

69
00:04:23,244 --> 00:04:29,904
The API and if you wanted to go with the
window scheduler, or if you wanted to go

70
00:04:29,904 --> 00:04:35,604
to AWS Lambda, right nowadays, like many
people are using the AWS services, right?

71
00:04:35,904 --> 00:04:38,484
So it's again, it's not that expensive.

72
00:04:38,484 --> 00:04:42,594
If you wanted to go to the AWS
Lambda, it's relatively very

73
00:04:42,594 --> 00:04:45,044
cheap for the subscription, right?

74
00:04:45,779 --> 00:04:51,844
For the scheduling, but if you don't want
to, if you don't have AWS services, it's

75
00:04:51,844 --> 00:04:57,424
okay, but still you can use the window
scheduler right to trigger ever jobs.

76
00:04:57,634 --> 00:05:02,514
And then, I will talk about how
to set up the, the scheduler also.

77
00:05:02,514 --> 00:05:04,474
So then, the schedule will run.

78
00:05:05,384 --> 00:05:12,154
And that will pick up the API, which you
have that script and that script will

79
00:05:12,194 --> 00:05:17,104
talk to their source and then bring the
data and put it into the CSV location.

80
00:05:18,054 --> 00:05:24,384
And from CSV to the database, so you
can use the AWS Lambda, or maybe you can

81
00:05:24,394 --> 00:05:26,624
use another mechanism to the database.

82
00:05:26,964 --> 00:05:31,204
Then from there, your BI analytics
come into the picture, right?

83
00:05:31,704 --> 00:05:35,004
So now how this is going to work, right?

84
00:05:35,104 --> 00:05:39,074
let's say, this is what, where we are
going to talk a little bit details, right?

85
00:05:39,104 --> 00:05:46,119
as in that, as every a PA, which has
their own end points and based on

86
00:05:46,149 --> 00:05:49,119
the KPA, which we are looking for.

87
00:05:49,449 --> 00:05:53,439
So we need to make sure, And go
through that API documentation,

88
00:05:53,489 --> 00:05:54,559
whether that is supported.

89
00:05:54,849 --> 00:05:55,119
Yep.

90
00:05:55,199 --> 00:05:58,959
If yes, then what are the pros and cons?

91
00:05:58,999 --> 00:06:00,699
How we need to get the data.

92
00:06:01,149 --> 00:06:05,549
So all this, application, they have
really, good API documentation.

93
00:06:05,929 --> 00:06:08,649
We just need to go through
and get the data, right?

94
00:06:08,739 --> 00:06:11,702
for instance, we're talking about
the, like maybe Sprout Social, right?

95
00:06:11,702 --> 00:06:13,489
Sprout Social or the Meta ads.

96
00:06:13,994 --> 00:06:16,384
So they have this process API.

97
00:06:16,434 --> 00:06:21,424
and they'll provide that endpoints matrix,
and there are some other parameters,

98
00:06:21,444 --> 00:06:22,624
which we're talking here, right?

99
00:06:22,714 --> 00:06:26,614
So let's say if you're looking
for, a customer ID or the topic ID.

100
00:06:26,824 --> 00:06:31,074
you need to go through that and get
those details and you can pass those

101
00:06:31,084 --> 00:06:33,604
information into your script, right?

102
00:06:33,944 --> 00:06:35,724
So before we implement the script.

103
00:06:35,899 --> 00:06:37,619
what I do normally, right?

104
00:06:37,619 --> 00:06:43,499
I use the postman to make sure how it is
responding, how the data is coming, right?

105
00:06:43,799 --> 00:06:47,139
My numbers are matching with
the source or not, right?

106
00:06:47,439 --> 00:06:50,489
So that is the tip I can
provide you here, right?

107
00:06:50,529 --> 00:06:53,339
so before we move to the script, right?

108
00:06:53,429 --> 00:06:57,019
And similarly, let's say if you're
looking at the meta ads and yeah,

109
00:06:57,019 --> 00:07:01,409
metadata also has then not really a
good API, in the documentation and

110
00:07:01,409 --> 00:07:03,149
they have some other parameters also.

111
00:07:03,149 --> 00:07:07,664
We talk meta app, id, meta
pay inside fields, meta app

112
00:07:07,664 --> 00:07:09,659
pay inside, pay Inside Limit.

113
00:07:09,869 --> 00:07:10,629
And what not.

114
00:07:11,089 --> 00:07:15,239
And similarly, if you're looking at
the Google Ads, they have that API

115
00:07:15,239 --> 00:07:19,049
documentation and the endpoints,
and the Bing Ads, the Bing Ads,

116
00:07:19,099 --> 00:07:22,759
I would say it's a little tricky,
but it's not that complicated.

117
00:07:22,799 --> 00:07:24,664
so they have a really good documentation.

118
00:07:25,164 --> 00:07:26,684
So there are other parameters.

119
00:07:26,734 --> 00:07:28,654
Let's say if you see Google Ads, right?

120
00:07:28,704 --> 00:07:32,874
The client ID, client secret,
and let's token endpoint.

121
00:07:33,274 --> 00:07:36,754
So these are the other parameters
which we need to consider, right?

122
00:07:37,004 --> 00:07:41,904
so once let's say, If you wanted to
bring entire universe from your social

123
00:07:41,904 --> 00:07:46,074
channel, or if you're looking for the
specific metrics you wanted to bring

124
00:07:46,074 --> 00:07:48,494
into your reporting platform, right?

125
00:07:48,804 --> 00:07:53,424
So that's where we need to, we need to
have that final, the requirements, right?

126
00:07:53,924 --> 00:07:57,384
So if you wanted to bring, entire
universe from the, source systems,

127
00:07:57,384 --> 00:07:59,404
yes, you can absolutely, consider that.

128
00:07:59,884 --> 00:08:01,224
And you need to prepare the script.

129
00:08:01,449 --> 00:08:06,024
ba based on the KPIs, we are looking
for, so there are some key points

130
00:08:06,024 --> 00:08:07,554
which we need to consider, right?

131
00:08:07,584 --> 00:08:14,084
so how many KPIs we are bringing
and, if that, that KPA, which we

132
00:08:14,084 --> 00:08:17,774
are looking for is supported by
that a PA or not, so that, that is

133
00:08:17,774 --> 00:08:19,484
a first thing we need to do, right?

134
00:08:19,534 --> 00:08:21,634
there are some KPIs, which, we.

135
00:08:22,134 --> 00:08:26,744
Which, these platforms may not support,
but what we need to do in that case,

136
00:08:26,814 --> 00:08:30,684
we need to raise a support ticket
to the, source platform, team, and

137
00:08:30,774 --> 00:08:32,224
they will, provide those details.

138
00:08:33,004 --> 00:08:38,344
But if, as long as, if that KPI is
existent, supported by the API, so you,

139
00:08:38,354 --> 00:08:43,114
and you have that credential in the API
endpoints, you can go ahead and build it.

140
00:08:43,614 --> 00:08:47,024
Another thing what we need to consider
is like the paginations, right?

141
00:08:47,064 --> 00:08:51,244
so we need to make sure, the
paginations is rightly set up, right?

142
00:08:51,314 --> 00:08:55,824
then the other point, other
important point is a majority

143
00:08:55,824 --> 00:08:57,714
of the social, platforms.

144
00:08:57,714 --> 00:09:01,824
So based on my observation,
the mostly refresh.

145
00:09:01,989 --> 00:09:05,689
On, around nine, after
nine, am EST, right?

146
00:09:05,689 --> 00:09:10,749
Like maybe if you schedule your, the
scheduler, before nine o'clock 9:00 AM

147
00:09:10,749 --> 00:09:14,199
EST, so you may not see, up to date data.

148
00:09:14,279 --> 00:09:15,089
and again, right?

149
00:09:15,139 --> 00:09:17,419
this is based on the
documentation on my experience.

150
00:09:17,419 --> 00:09:21,094
Maybe, if you have the different
needs on that scheduling and how.

151
00:09:21,594 --> 00:09:23,374
How fresh data you are looking for.

152
00:09:23,814 --> 00:09:27,484
I recommend you to, go through
the, the documentation and set

153
00:09:27,654 --> 00:09:30,024
your scheduling accordingly, right?

154
00:09:30,124 --> 00:09:35,164
the same approach will applicable for all
other the platforms, being it's a tick

155
00:09:35,164 --> 00:09:38,424
tock or, LinkedIn or the Twitter, right?

156
00:09:38,524 --> 00:09:43,383
only thing on the Twitter side,
the sentiment for the Twitter,

157
00:09:43,384 --> 00:09:44,954
we need to focus on that.

158
00:09:45,024 --> 00:09:50,014
if you have the requirement to bring
the sentiment from the, X platform.

159
00:09:50,054 --> 00:09:55,384
So you need to make sure whether that X
platform is support, the sentiment or not.

160
00:09:55,434 --> 00:09:55,734
Yeah.

161
00:09:56,344 --> 00:09:59,744
these are the findings, which I
would like to share a value or

162
00:09:59,744 --> 00:10:01,424
in a connecting with the API.

163
00:10:01,924 --> 00:10:03,639
so then once we build the.

164
00:10:04,139 --> 00:10:05,589
so what do we need to do?

165
00:10:05,599 --> 00:10:10,389
we need to prepare the Windows
scheduler to bring the data

166
00:10:10,539 --> 00:10:12,739
on schedule basis, right?

167
00:10:13,199 --> 00:10:16,559
because you don't want to do
the manual every day, right?

168
00:10:16,609 --> 00:10:19,309
so that's when the scheduler will help us.

169
00:10:19,309 --> 00:10:25,279
It is a Windows inbuilt, the tool and
many, many People, you know, using it,

170
00:10:25,699 --> 00:10:29,229
but it's pretty, pretty straightforward
to set up the windows scheduler.

171
00:10:29,319 --> 00:10:32,639
so you're going to find the windows,
but the only thing is that you should

172
00:10:32,789 --> 00:10:36,939
have the admin privileges to utilize
the scheduler, the features, right?

173
00:10:37,339 --> 00:10:40,349
so open the windows scheduler
from, from your start.

174
00:10:40,449 --> 00:10:45,339
program, and, and, they need to
create that task and look for the,

175
00:10:45,669 --> 00:10:48,939
the script, which are the API, which
we developed to bring the data.

176
00:10:49,249 --> 00:10:51,179
For instance, I like the Facebook, right?

177
00:10:51,559 --> 00:10:55,329
If we prepare that API, using the
mechanism, which we talked before.

178
00:10:55,954 --> 00:11:01,533
and place it into your, C drive or D
drive, where, wherever you want it to

179
00:11:01,563 --> 00:11:03,303
utilize that program automatically.

180
00:11:03,683 --> 00:11:07,573
And, the same mission, like use the
window scheduler, open the window

181
00:11:07,573 --> 00:11:11,603
scheduler and go to the task and
create the, just give it a name.

182
00:11:12,073 --> 00:11:16,278
And, you need to set A few things
they're like now browse that file

183
00:11:16,278 --> 00:11:19,268
of the APA, which we created the
Facebook for instance, right?

184
00:11:19,358 --> 00:11:24,868
And look for that and the set the
refresh timing and give that target.

185
00:11:24,988 --> 00:11:28,188
let's say if you wanted to bring it
to the article or, Amazon redshift.

186
00:11:28,688 --> 00:11:31,268
So create the same structure how you have.

187
00:11:31,673 --> 00:11:37,303
In your file, so that way, this file,
the new scheduler will, refresh and

188
00:11:37,323 --> 00:11:39,453
load the data into the same target file.

189
00:11:39,943 --> 00:11:43,243
So you need to make sure you
define the target file as well.

190
00:11:43,733 --> 00:11:47,413
So only thing you need to consider
here is when you're creating the

191
00:11:47,453 --> 00:11:52,063
target file, make sure, data types
and the sequence, all that in the

192
00:11:52,063 --> 00:11:53,603
same as you were, in your script.

193
00:11:54,103 --> 00:11:58,330
So then, when you set up this, your
scheduler, make sure to run two, three

194
00:11:58,330 --> 00:12:02,139
test runs, with a small amount of
data, and, and then that's it, right?

195
00:12:02,149 --> 00:12:05,779
once you have the, the, scheduler runs
and create the data aggregate file,

196
00:12:05,949 --> 00:12:07,919
it will go and roll into the table.

197
00:12:07,969 --> 00:12:09,939
I just wanted to repeat, one more time.

198
00:12:09,939 --> 00:12:13,679
Just, first thing we need to have
the valid, user login details.

199
00:12:14,119 --> 00:12:17,759
And the second thing, we need to make
sure what KPIs, which we are targeting

200
00:12:17,759 --> 00:12:20,219
to bring into your, the database.

201
00:12:20,669 --> 00:12:23,199
And the third thing is
that KPIs are support.

202
00:12:23,369 --> 00:12:25,169
by API or not, right?

203
00:12:25,169 --> 00:12:28,239
That is another thing
which we need to do it.

204
00:12:28,519 --> 00:12:32,829
So once we pass all these things, the
next thing is, try to, work on the

205
00:12:33,389 --> 00:12:37,309
postman, So that way, If everything
is working fine, or if you see any

206
00:12:37,309 --> 00:12:42,799
hiccups and whatnot, then, you need to
create that package, what we call it

207
00:12:42,799 --> 00:12:47,679
as, the package of the API and bring
that package into your local system.

208
00:12:47,919 --> 00:12:52,449
And then the next step is, create
the scheduler, and, before you

209
00:12:52,519 --> 00:12:56,479
enable the scheduler, make sure
you create the target table, right?

210
00:12:56,719 --> 00:13:01,599
In the scheduler, all what we
need to do is use that package and

211
00:13:01,659 --> 00:13:07,399
define the target and define when
the scheduler has to be run, right?

212
00:13:07,419 --> 00:13:10,054
So once you set up all
this, you're And run it.

213
00:13:10,104 --> 00:13:13,124
So your data will come and
sit in your, the database.

214
00:13:13,804 --> 00:13:15,554
Yeah, so that's pretty simple.

215
00:13:15,584 --> 00:13:18,364
So this will work for
all other applications.

216
00:13:18,454 --> 00:13:22,964
so that way, we don't have to opt,
other third party tools and paying so

217
00:13:22,964 --> 00:13:25,454
much of dollars in, on a yearly basis.

218
00:13:25,764 --> 00:13:27,796
So this is our data.

219
00:13:27,816 --> 00:13:34,816
And yes, the technologies we need, one
we need, either C sharp, or, nor Jason,

220
00:13:35,231 --> 00:13:39,500
that, and then, the window scheduler
is, I think pretty straightforward

221
00:13:39,530 --> 00:13:41,780
and little bit, the CSL knowledge.

222
00:13:42,240 --> 00:13:46,110
another key point is, you need to
have that admin privileges to say,

223
00:13:46,210 --> 00:13:50,390
to enable the scheduler or they
create the table in the database.

224
00:13:50,680 --> 00:13:51,040
Yeah.

225
00:13:51,230 --> 00:13:54,660
So if you have all these
things, you can go to start.

226
00:13:55,160 --> 00:13:59,780
And now let's talk about the impact of
digital transformation in nonprofits.

227
00:13:59,890 --> 00:14:02,730
If you see the leading organizations
successfully adapted to the

228
00:14:02,760 --> 00:14:06,050
AI and cloud based solutions
for donor management is 78.

229
00:14:06,150 --> 00:14:11,045
2%. which is really, the good
number and efficiency improvement.

230
00:14:11,095 --> 00:14:11,385
Yeah.

231
00:14:11,385 --> 00:14:15,565
So greater productivity through
automated workflow and digital tools.

232
00:14:15,695 --> 00:14:18,165
We see, around 56.

233
00:14:18,165 --> 00:14:21,405
2 percent and, no overhead deductions.

234
00:14:21,905 --> 00:14:24,105
It's closely close to 43.

235
00:14:24,175 --> 00:14:26,765
8%. Engagement boost is 67.

236
00:14:26,825 --> 00:14:29,595
4%. And the API integration, right?

237
00:14:29,765 --> 00:14:32,965
If you see the API integration,
that's where we are almost in a 62.

238
00:14:32,965 --> 00:14:36,645
7%. And a cloud infrastructure.

239
00:14:36,845 --> 00:14:39,277
Yes, I would say, it's 45.

240
00:14:39,277 --> 00:14:45,210
5%. API, AI driven analytics,
revolution, donor relationships,

241
00:14:45,260 --> 00:14:48,860
private, donor retention or,
annual donations, predictions and

242
00:14:48,870 --> 00:14:50,840
accuracy, mission learning models.

243
00:14:50,840 --> 00:14:55,530
So all this will help us to, all
that AI driven analytics for donor

244
00:14:55,530 --> 00:15:01,135
management system and, a little bit, on
the architecture level, like now, it's

245
00:15:01,135 --> 00:15:03,115
a high volume data processing, right?

246
00:15:03,215 --> 00:15:06,600
It's because, based on, on the
data, what we were using, right?

247
00:15:07,020 --> 00:15:09,500
Our robust infrastructure
handles, for almost 4.

248
00:15:09,550 --> 00:15:14,050
7 TB of donor monthly and
enabling real time decisions

249
00:15:14,070 --> 00:15:15,190
making across all the platforms.

250
00:15:15,690 --> 00:15:19,420
And enterprise media intelligency,
it's industry leading, which

251
00:15:19,590 --> 00:15:20,650
if you see it's close 99.

252
00:15:20,650 --> 00:15:25,070
9%, which is pretty high,
right up to while processing 1.

253
00:15:25,070 --> 00:15:29,410
2 million daily media mentions,
ensuring comprehensive brand monitoring.

254
00:15:30,000 --> 00:15:33,340
And there is a comprehensive social
analytics, the advanced processing 2.

255
00:15:33,340 --> 00:15:36,870
3 millions, cross platform
social interaction, delivering

256
00:15:36,920 --> 00:15:38,540
actionable engagement insights.

257
00:15:39,340 --> 00:15:43,340
Performance, marketing analytics,
multi cloud, redundancy,

258
00:15:43,790 --> 00:15:45,660
advanced executive framework.

259
00:15:45,820 --> 00:15:49,300
Yeah, this all will, consider as a,
our system architecture overview.

260
00:15:49,680 --> 00:15:53,280
we'd like to, spend a little time on,
AWS Lambda infrastructure performance.

261
00:15:53,830 --> 00:15:57,785
I do see, the Lambda, performance
also really good, right?

262
00:15:57,835 --> 00:16:00,975
the enterprise grid reliability is 99.

263
00:16:01,075 --> 00:16:04,745
95%. And, lightning fast response is 95%.

264
00:16:04,745 --> 00:16:08,225
Scalable performance is really good.

265
00:16:08,615 --> 00:16:12,555
dramatic cost saving, I do see,
really, good number over there.

266
00:16:12,555 --> 00:16:18,410
The advances are, Serverless architecture,
leveraging AWS Lambda, event driven

267
00:16:18,920 --> 00:16:23,420
compute model, it's dynamically
allocating up to, 10 GB memory, and

268
00:16:23,460 --> 00:16:27,940
executing functions with, almost 99.

269
00:16:27,940 --> 00:16:31,270
9 percent availability across
multiple global regions, and scalable

270
00:16:31,310 --> 00:16:35,794
performance, multi region, resilience,
yeah, so all this is, considered as

271
00:16:35,794 --> 00:16:37,982
AWS Lambda infrastructure performance.

272
00:16:38,482 --> 00:16:39,902
So then, let's talk about.

273
00:16:40,442 --> 00:16:44,872
AI Powered Donor Analytics Platform,
it's a 360 degrees donor view,

274
00:16:44,922 --> 00:16:47,192
processing, as I mentioned, like the 4.

275
00:16:47,192 --> 00:16:50,562
2 TB of donors information
daily equivalent to 2.

276
00:16:50,562 --> 00:16:54,012
1 million donor profiles,
which is pretty high.

277
00:16:54,412 --> 00:16:57,982
Predictive Analytics Engine,
yes, it is analyzing, 3.

278
00:16:57,982 --> 00:17:03,327
5. 3. 2 million donor behavior patterns
daily with 94 percent accuracy, right?

279
00:17:03,327 --> 00:17:06,667
94 percent accuracy is really,
it's a good percentage.

280
00:17:07,127 --> 00:17:14,837
The advanced research infrastructure,
executing, 234 complex queries daily with

281
00:17:14,837 --> 00:17:17,357
a lightning fast 98 ms response time.

282
00:17:17,907 --> 00:17:20,727
a simple neutral networks with 96.

283
00:17:20,977 --> 00:17:25,077
5 predictive accuracy across,
diverse donors segment.

284
00:17:25,577 --> 00:17:27,957
So real time data processing, yeah.

285
00:17:27,957 --> 00:17:30,207
So the, this enterprise is great platform.

286
00:17:30,207 --> 00:17:31,197
Harm is cutting edge.

287
00:17:31,257 --> 00:17:31,267
Aye.

288
00:17:32,017 --> 00:17:36,337
And distributed computing technology
to, to transform nonprofit

289
00:17:36,407 --> 00:17:38,807
operations by processing over 8.

290
00:17:38,887 --> 00:17:45,377
7 million donor interaction
monthly across, 2345 organizations.

291
00:17:45,877 --> 00:17:50,237
And now if you're looking to 360 degree
donor view system, this is really

292
00:17:50,297 --> 00:17:55,637
impactful for any organizations when
you have a 360 degree donor view.

293
00:17:55,807 --> 00:17:57,817
exactly the donor, right?

294
00:17:57,877 --> 00:18:00,927
where he is, how much healthy
he is, how much he's donating,

295
00:18:00,927 --> 00:18:02,067
where else he's donating.

296
00:18:02,557 --> 00:18:05,107
and all that information, right?

297
00:18:05,627 --> 00:18:07,827
the instant social, intelligency.

298
00:18:07,917 --> 00:18:11,637
And the precision error optimization,
a data driven reliability.

299
00:18:11,867 --> 00:18:17,447
yeah, so this all will help us, you
know, to know better about your donor.

300
00:18:18,407 --> 00:18:21,987
And the more we know about the
donor, the more we can, get the

301
00:18:21,987 --> 00:18:26,157
funds, from the donors and some
predictive analytics engines, right?

302
00:18:26,187 --> 00:18:30,597
Advanced data processing, Intelligent
ad integrations, dynamic donor

303
00:18:30,657 --> 00:18:35,967
profiling, assembling machine learning,
architecture, real time data processing.

304
00:18:36,247 --> 00:18:40,617
So if we consider all these things,
this really, help us, to have

305
00:18:40,617 --> 00:18:42,597
that, really, good productivity.

306
00:18:42,597 --> 00:18:47,651
They're the most effective analytics
engine established in our organizations.

307
00:18:47,651 --> 00:18:49,167
Advances such infrastructure.

308
00:18:49,447 --> 00:18:53,667
so this is nowadays, this really helped
us a lot to understand more, right?

309
00:18:54,047 --> 00:18:57,887
the light, lightning, fast
processing, human, understanding

310
00:18:57,987 --> 00:19:02,407
robust, add intelligency,
holistic sets, scalability, right?

311
00:19:02,437 --> 00:19:05,477
so this will really, help us
advance such infrastructure.

312
00:19:05,527 --> 00:19:09,367
digital analytics and API ecosystem,
the core platforms, like there are like,

313
00:19:09,467 --> 00:19:13,687
seven, systems we can talk about the
core platform components, implementation

314
00:19:13,687 --> 00:19:19,477
architecture, operational consideration,
restful API integration, scalability

315
00:19:19,477 --> 00:19:22,987
framework, compliance and security
layer, and, performance telemetry.

316
00:19:23,487 --> 00:19:25,987
At the conclusion,
transformative potential, right?

317
00:19:26,027 --> 00:19:30,227
Our revolution digital analytics
framework has already demonstrated

318
00:19:30,237 --> 00:19:35,477
extraordinary impact with participating
non profits seeing an average 47 percent

319
00:19:35,517 --> 00:19:40,957
of increase in donor engagement and
a 42 percent improvement in retention

320
00:19:40,997 --> 00:19:45,447
rate by seamlessly integrating AI
driven analytics across multiple

321
00:19:45,447 --> 00:19:47,587
platforms and processing over 8.

322
00:19:47,587 --> 00:19:50,357
7 million monthly donor infrastructure.

323
00:19:50,627 --> 00:19:53,887
So we are empowering organizations
to transform data into meaningful

324
00:19:53,887 --> 00:19:56,327
relationships and measurable results.

325
00:19:56,827 --> 00:20:03,467
Yeah, so with that, so establishing
this, API ecosystem to bring the

326
00:20:03,477 --> 00:20:05,227
multiple data, multiple, social.

327
00:20:05,757 --> 00:20:11,527
Platform data into your, your database now
without relying on the third party tools.

328
00:20:11,567 --> 00:20:12,357
it's really helped.

329
00:20:12,597 --> 00:20:17,607
one, you can do a lot of cost
saving and to, it's your data.

330
00:20:17,617 --> 00:20:21,367
You are owning your data on
your understanding the data.

331
00:20:21,747 --> 00:20:25,367
and three, you can do, really,
good analytics, out of it.

332
00:20:26,187 --> 00:20:27,837
Thank you so much, for your time.

333
00:20:27,837 --> 00:20:31,567
Hope, hope you have, something,
useful, for your organization today.

334
00:20:31,897 --> 00:20:32,907
Thank you so much.

