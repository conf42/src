1
00:00:00,500 --> 00:00:01,160
Hello everyone.

2
00:00:01,290 --> 00:00:03,719
Welcome to Comp 42 20 25.

3
00:00:04,270 --> 00:00:08,710
My name is Webpa and today I'll
walk you through how retrieval

4
00:00:08,710 --> 00:00:13,030
augmented generation, also known
as drag, offers a huge improvement

5
00:00:13,030 --> 00:00:16,299
over conventional chatbots and
the enterprise support domain,

6
00:00:16,900 --> 00:00:21,490
especially when it comes to accuracy,
scalability, and knowledge integration.

7
00:00:21,990 --> 00:00:26,290
We'll first understand how like
traditional chat botts work traditional

8
00:00:26,290 --> 00:00:32,310
chatbots even the more modern ones that
use intent classification or basic NLP

9
00:00:32,650 --> 00:00:38,020
rely on static knowledge and like hardcode
hardcoded flows or pre-train models with

10
00:00:38,080 --> 00:00:43,280
limited ability to adapt they're built
using rule-based systems that scales

11
00:00:43,280 --> 00:00:45,905
poorly and can easily break on edge cases.

12
00:00:46,515 --> 00:00:50,905
They're trained on like a snapshot of
data, which becomes absolute very quickly.

13
00:00:51,325 --> 00:00:56,534
And the session state management is often
brittle or non-existent making multi-tone

14
00:00:56,534 --> 00:00:59,044
context attention weak or absent.

15
00:00:59,734 --> 00:01:01,744
So in other words, none of these scale.

16
00:01:01,744 --> 00:01:04,624
In real world customer service
environment where like data is.

17
00:01:05,504 --> 00:01:10,134
Continuously evolving and there
are diverse user queries every day.

18
00:01:10,634 --> 00:01:13,455
As a result this causes
a maintenance overhead.

19
00:01:13,604 --> 00:01:16,514
You need to manually update
knowledge bases continuously.

20
00:01:16,964 --> 00:01:20,494
And there is a risk of hallucination
when the model doesn't know the

21
00:01:20,494 --> 00:01:22,115
answer, but still guesses it.

22
00:01:22,684 --> 00:01:27,004
And the most critical one, like the
system can't handle novel or low frequency

23
00:01:27,004 --> 00:01:32,115
queries because they're explicitly
encoded in the training set or a dialog

24
00:01:32,115 --> 00:01:34,494
tree which creates a feedback loop.

25
00:01:34,524 --> 00:01:38,164
Loop of high operational
overhead and poor user trust.

26
00:01:38,664 --> 00:01:43,884
Here comes rag in the picture and rag
shifts the paradigm by introducing

27
00:01:43,884 --> 00:01:47,184
real time document retrieval as
the very first step to response

28
00:01:47,184 --> 00:01:48,944
generation in the chat bots.

29
00:01:49,694 --> 00:01:53,594
The first step involves understanding
the query the user has asked, or using

30
00:01:53,644 --> 00:01:55,824
advanced transformers with semantic.

31
00:01:56,324 --> 00:02:01,445
Later the query is embedded via bird or
sentence transformers then matched against

32
00:02:01,445 --> 00:02:09,794
a vector store like F-A-I-S-S or pine Cone
or Westpa to retrieve the top K passages.

33
00:02:09,895 --> 00:02:14,464
And once those latest information
is retrieved this data is fed

34
00:02:14,464 --> 00:02:18,754
into the prompt for language model
enabling like a grounded generation.

35
00:02:19,579 --> 00:02:23,959
And in case if the data updates
are detected in any of those while

36
00:02:23,959 --> 00:02:28,519
receiving any of these data, the delta
of those information gets in text

37
00:02:28,519 --> 00:02:33,249
automatically and gets made and is then
made available in the next retrieval

38
00:02:33,249 --> 00:02:37,089
cycle and saving the efforts on fine
tuning the models again and again.

39
00:02:37,589 --> 00:02:41,389
Along with that like traditional
bots typically fail at handling

40
00:02:41,439 --> 00:02:45,369
composite or like multi intent
queries due to linear dial up flows

41
00:02:45,759 --> 00:02:48,399
and like limited memory as well.

42
00:02:49,174 --> 00:02:53,074
Rag in the situation introduces
a multi-stage reasoning.

43
00:02:53,264 --> 00:02:58,284
So the way it works is like the
complex inputs are passed into smaller

44
00:02:58,314 --> 00:03:02,835
structured sub-questions into using
semantic and syntactic segmentations.

45
00:03:03,335 --> 00:03:06,695
And then each component is
independently and individually

46
00:03:06,695 --> 00:03:08,255
queried against the vector store.

47
00:03:08,745 --> 00:03:14,115
The system then can pull from multiple
domains or knowledge clusters, and then

48
00:03:14,175 --> 00:03:19,255
the retrieve snippets are fused using
either concatenation or specialized

49
00:03:19,255 --> 00:03:21,815
summarization layer fusion in decoder.

50
00:03:22,645 --> 00:03:26,275
Then the language model generates a
comprehensive, unified answer that

51
00:03:26,275 --> 00:03:28,225
covers all parts of the inputs.

52
00:03:28,285 --> 00:03:32,425
Something like, which is very difficult
to achieve with a traditional bot.

53
00:03:32,925 --> 00:03:33,405
Later.

54
00:03:33,505 --> 00:03:38,665
Due to all this applications prag
is able to achieve a higher accuracy

55
00:03:38,665 --> 00:03:43,136
over traditional chat bots and LLM
models because of its ability to

56
00:03:43,225 --> 00:03:44,876
ground responses with real data.

57
00:03:45,376 --> 00:03:48,956
The language model is conditioned
on retrieved documents.

58
00:03:48,986 --> 00:03:53,225
So the outputs are based on actual
enterprise knowledge and not the

59
00:03:53,225 --> 00:03:57,936
parametric memory or like the
stale memory, which is out of date.

60
00:03:58,436 --> 00:04:02,696
And in this situation, if let's say the
relevant documents for some outage or

61
00:04:02,966 --> 00:04:08,096
downtime aren't retrieved the system
can return, fall to fallback messages or

62
00:04:08,225 --> 00:04:13,886
uncertainty signals instead of guessing,
improving the trust and transparency.

63
00:04:14,785 --> 00:04:20,565
Along with that rag systems can estimate
confidence based on retrieval scores and

64
00:04:20,565 --> 00:04:26,626
response entropy, enabling you to set
threshold enabling you to set thresholds.

65
00:04:26,736 --> 00:04:31,866
And escalate uncertain responses or ask
clarifying questions in situations when

66
00:04:31,866 --> 00:04:36,381
the model is not able to, or if the
estimate con if the estimated confidence

67
00:04:36,381 --> 00:04:38,871
level is below the set threshold.

68
00:04:39,371 --> 00:04:44,761
Apart from like the ask accuracy
aspect of the rag, rag based system,

69
00:04:45,061 --> 00:04:49,501
the other very big benefit that rag
based chatbot brings in improving

70
00:04:49,501 --> 00:04:50,912
the user experience that it can.

71
00:04:51,346 --> 00:04:57,356
We augmented with session level memory,
with using embeddings or key value stores.

72
00:04:57,926 --> 00:05:03,836
So basically what it can do is then it
can track user intent, slot fills, and

73
00:05:03,886 --> 00:05:09,766
previous interactions to maintain context
even across like multi tone conversations.

74
00:05:10,256 --> 00:05:12,811
This in turn results
and dramatically more.

75
00:05:13,316 --> 00:05:16,976
Coherent dialogue flows and like
higher task completion rates.

76
00:05:17,396 --> 00:05:24,176
He elevating the user's experience apart
from this in a domain of customer support.

77
00:05:24,206 --> 00:05:28,136
Even like the advanced automation
there will be a point when some

78
00:05:28,136 --> 00:05:30,056
conversation needs escalations.

79
00:05:30,546 --> 00:05:34,756
And in this situation, having an
AI agent that is able to store

80
00:05:34,756 --> 00:05:37,726
context and intent and intent.

81
00:05:38,551 --> 00:05:42,611
In those situations, the handoff to
human agents helps in bringing the

82
00:05:42,611 --> 00:05:47,281
human agent up to the speed of the
issue very quickly because the rag

83
00:05:47,281 --> 00:05:50,701
based system has all the context
and intents already stored in there.

84
00:05:50,761 --> 00:05:51,781
Its database.

85
00:05:52,291 --> 00:05:56,841
So with RAG, we can achieve like a
structured handoff to the human agents

86
00:05:56,921 --> 00:05:58,771
through delivering a package with.

87
00:05:59,396 --> 00:06:03,556
History retrieved document
and generated responses.

88
00:06:04,131 --> 00:06:09,111
An agent no longer needs to ask questions
again and again or dig for background.

89
00:06:09,471 --> 00:06:13,821
They get full context, including
the intent chain and prior decision

90
00:06:13,821 --> 00:06:18,501
points so that they are already up to
the speed with the customer's issue.

91
00:06:19,231 --> 00:06:23,421
This in turn reduces average
handling time for a of a customer

92
00:06:23,721 --> 00:06:28,251
and improves both agent satisfaction
and first time resolution.

93
00:06:28,351 --> 00:06:28,771
Rates.

94
00:06:29,271 --> 00:06:34,991
For, from a design view these
all can be easily integrated with

95
00:06:34,991 --> 00:06:39,611
any of the CRM applications that
any organization is using for the

96
00:06:39,611 --> 00:06:41,561
support operations in their company.

97
00:06:42,061 --> 00:06:48,701
And so in, in order to make sure your, a
rag based bot is up to date with all the

98
00:06:48,701 --> 00:06:51,311
knowledge and all the latest information.

99
00:06:51,801 --> 00:06:55,441
And this is all because like in
a live system document, in the

100
00:06:55,441 --> 00:06:57,061
knowledge base are bound to change.

101
00:06:57,421 --> 00:07:00,871
So in order to make sure your rag
based chat bot is always up to date

102
00:07:01,321 --> 00:07:04,511
we need to establish a continuous
knowledge update pipelines.

103
00:07:04,691 --> 00:07:10,541
So as soon as like content, like policy,
document or support articles is updated.

104
00:07:11,026 --> 00:07:16,651
In the source systems like Confluence
or any of the CMS, system that the

105
00:07:16,651 --> 00:07:20,971
company is using, it needs to get
ingested and queued for reprocessing.

106
00:07:21,631 --> 00:07:26,271
And once that happens, a background
job should vectorize these content

107
00:07:26,271 --> 00:07:32,601
using embedding models like BGE mini,
LM, et cetera, and then update the

108
00:07:32,601 --> 00:07:38,691
semantic index in a vector database
like F-I-A-F-A-I-S-S or any of the

109
00:07:38,691 --> 00:07:40,891
corresponding data vector databases.

110
00:07:41,391 --> 00:07:41,781
Later.

111
00:07:42,201 --> 00:07:46,551
So what this does is like later,
whenever a user query comes in, the

112
00:07:46,551 --> 00:07:50,511
retriever immediately surfaces the
latest relevant content, ensuring

113
00:07:50,511 --> 00:07:54,761
the response reflects the current
state without any model fine tuning.

114
00:07:55,576 --> 00:08:01,306
And the generator model conditions its
output on the updated context, enabling

115
00:08:01,356 --> 00:08:05,286
it to answer questions about the new
feature poli or any of the policy

116
00:08:05,286 --> 00:08:10,246
changes or known issue on the same
day the document subject was changed.

117
00:08:10,606 --> 00:08:15,256
So the response will always be with
respect to the latest updated data.

118
00:08:15,756 --> 00:08:20,836
So all this is works fine in the
ideal world but in order to have

119
00:08:20,836 --> 00:08:25,751
like a performance or like a well
running rag based chat bot we need to

120
00:08:25,751 --> 00:08:28,051
have a quality data knowledge base.

121
00:08:29,040 --> 00:08:33,361
Because racks performance, I would
say is directly tied to the quality

122
00:08:33,361 --> 00:08:34,800
of the knowledge base you have.

123
00:08:35,341 --> 00:08:39,900
So in other words, this is a situation
of where like you put in garbage

124
00:08:39,900 --> 00:08:41,221
and you will get a garbage out.

125
00:08:41,430 --> 00:08:46,341
So in order to make sure like that
doesn't happen sorry that doesn't happen.

126
00:08:46,441 --> 00:08:50,011
We need to make sure all the
redundant data or near duplicate

127
00:08:50,011 --> 00:08:54,391
documents or contents are
removed from the content base.

128
00:08:55,236 --> 00:08:59,455
And other ways we can do is like
normalizing conflicting entries.

129
00:08:59,506 --> 00:09:04,786
That can in some way con confuse the
retrieval mechanism as well as the

130
00:09:04,866 --> 00:09:06,906
LLM models in generating response.

131
00:09:07,881 --> 00:09:12,671
There should be a structured KB
using clear topical taxonomy with

132
00:09:12,671 --> 00:09:16,841
fallback links in situation if
the content is not available.

133
00:09:17,491 --> 00:09:21,321
Along with that, like there should be
metadata tagging for all the content

134
00:09:21,391 --> 00:09:25,961
so that it's easily available for
filtering and at the time of retrie.

135
00:09:26,461 --> 00:09:32,501
So this is like a high level
information about like the technical

136
00:09:32,711 --> 00:09:37,161
technology stacks involved in
each layer of rag based chat bot.

137
00:09:37,651 --> 00:09:38,761
For knowledge base.

138
00:09:38,811 --> 00:09:41,841
There should be proper document
vectorization and metadata

139
00:09:41,871 --> 00:09:43,881
tagging for easy retrieval.

140
00:09:44,381 --> 00:09:47,931
The retrieval systems should be
should be equipped with vector

141
00:09:47,931 --> 00:09:52,071
databases and semantic search
for the models to easily select.

142
00:09:52,801 --> 00:09:59,141
Then the language model can be any of the
l LMS that are out in there in the market

143
00:09:59,141 --> 00:10:01,691
for actually generating the responses.

144
00:10:02,111 --> 00:10:06,661
And the last piece is the security which
is the most crucial one because, even

145
00:10:06,661 --> 00:10:10,731
after the retrievals there should be
one layer of access controls and query

146
00:10:10,731 --> 00:10:14,721
filtering that should be in place to
make sure there is no data breach and

147
00:10:14,991 --> 00:10:19,951
an un unauthorized data does not get
sent or gets displayed to the customers.

148
00:10:20,451 --> 00:10:26,821
So now coming to the future avenues of
rad based chat bots since it's context

149
00:10:26,821 --> 00:10:31,941
awareness and and working with real
time data, it has opened door to a very

150
00:10:31,941 --> 00:10:34,611
hyper-personalized and behavior aware.

151
00:10:34,856 --> 00:10:36,376
Support experience.

152
00:10:36,926 --> 00:10:40,746
Customers there can be customer
specific retrieval, basically

153
00:10:40,826 --> 00:10:45,866
segmenting vector indexes per
customer profile or organization.

154
00:10:46,536 --> 00:10:51,686
Along with that it can also detect
anomalies and usage logs because and

155
00:10:51,686 --> 00:10:56,486
these logs can be readily made available
to these, to these chat bots so that

156
00:10:56,486 --> 00:10:58,496
it can predict some sort of outage.

157
00:10:58,496 --> 00:11:00,506
And then proactively surface health.

158
00:11:01,006 --> 00:11:03,856
This is one of the use cases
we are currently working on.

159
00:11:03,886 --> 00:11:07,046
And our goal is to achieve
a state where we detect an

160
00:11:07,046 --> 00:11:09,056
outage even before it happens.

161
00:11:09,946 --> 00:11:13,706
Along with that since the models are
connected with live data even if, let's

162
00:11:13,706 --> 00:11:18,426
say a bot has been trained or has been
embedded with one language, since it's it

163
00:11:18,426 --> 00:11:23,776
can always retrieve new information it is
possible to have the bot respond in any

164
00:11:23,776 --> 00:11:28,606
other language as per the user's need,
which makes it perfect for a organization

165
00:11:28,606 --> 00:11:32,336
which has a global presence and has
customer base spread across the growth.

166
00:11:32,836 --> 00:11:37,186
And along with this, the next
evolution in multimodal rag where

167
00:11:37,246 --> 00:11:41,506
inputs and and retrieve content
are just not limited to text.

168
00:11:41,966 --> 00:11:45,386
Some of the emerging capabilities
currently we have is the

169
00:11:45,386 --> 00:11:46,856
visual search and retrieval.

170
00:11:47,576 --> 00:11:52,696
Basically customer uploads an image
and then the rag model tries to find

171
00:11:52,696 --> 00:11:57,046
a match against the corresponding
visual kbs example, like a photo of

172
00:11:57,196 --> 00:12:01,766
damaged product a error screenshot
that the customer just uploaded.

173
00:12:02,126 --> 00:12:05,611
And based on that, it is able to
troubleshoot and based on that, the

174
00:12:06,331 --> 00:12:09,241
chat bot is able to troubleshoot
what the customer issue is.

175
00:12:09,886 --> 00:12:14,756
Apart from that retrieving instructional
videos based on any text that customer

176
00:12:14,756 --> 00:12:19,996
provided so that it can provide more
helpful information or resources to

177
00:12:19,996 --> 00:12:21,646
the customer to solve their issues.

178
00:12:22,186 --> 00:12:27,136
And later it could be like a fusion
of both text and video and vice versa.

179
00:12:27,261 --> 00:12:32,236
That, can be implemented with the
rag based chat bots architecturally

180
00:12:32,236 --> 00:12:36,596
and technically these for us to
for us to enable these features we

181
00:12:36,596 --> 00:12:38,216
need to have some sort of vision.

182
00:12:38,216 --> 00:12:38,936
ENC coders.

183
00:12:39,346 --> 00:12:44,836
Example I think clip and blip two
are some of the widely used encoders

184
00:12:44,896 --> 00:12:46,486
that out there in the market.

185
00:12:46,486 --> 00:12:49,326
And some sort of cross model retrievers.

186
00:12:49,366 --> 00:12:54,136
Which will for sure help in expanding
the use cases for Rag Beyond Text.

187
00:12:54,636 --> 00:12:59,786
Now here are some stats that we
collected that shows a tangible

188
00:12:59,816 --> 00:13:03,656
improvement in customer support
operations across multiple KPIs.

189
00:13:04,166 --> 00:13:08,746
These have directly impacted operational
efficiency and customer satisfaction.

190
00:13:09,721 --> 00:13:14,931
These efficiency gains stem primarily
from improved first contract resolution,

191
00:13:14,931 --> 00:13:21,371
as you can see, has a number of
around 75 to 85% compared to 45 to 55.

192
00:13:21,971 --> 00:13:26,081
And also the reduced need of
clarification exchange is another

193
00:13:26,081 --> 00:13:30,731
thing that has proven that the rag
based model has been tremendously

194
00:13:30,821 --> 00:13:35,531
very helpful in enhancing the user's
experience while trying to find support.

195
00:13:36,031 --> 00:13:41,066
To wrap up the evolution for traditional
AI chat bots to rag powered system

196
00:13:41,066 --> 00:13:45,576
reference represents a transform,
transformative advancement and

197
00:13:45,576 --> 00:13:47,166
the customer support technology.

198
00:13:47,646 --> 00:13:52,676
Since it enables like grounded real time
answer that scales with knowledge changes.

199
00:13:52,706 --> 00:13:57,796
So the customers are always, given
like the latest information it reduces

200
00:13:57,796 --> 00:14:02,266
the retraining cycles and lowers the
operational overhead, which saves

201
00:14:02,306 --> 00:14:07,656
any enterprise organizations a lot of
money context continuity, even across

202
00:14:07,656 --> 00:14:12,606
long interaction so that customers
don't get irritated in explaining

203
00:14:12,606 --> 00:14:14,136
the same things again and again.

204
00:14:14,796 --> 00:14:19,286
And, the use cases are still like,
since it's an evolving technology,

205
00:14:19,286 --> 00:14:23,646
there are many more room for
more evolution in this domain.

206
00:14:24,146 --> 00:14:26,516
So thank you so much
everyone for your time.

207
00:14:26,776 --> 00:14:30,706
I'll be happy to answer any question
regarding RAG and how it can be used

208
00:14:30,706 --> 00:14:33,206
to improve customer support operations.

209
00:14:33,536 --> 00:14:33,896
Thank you.

