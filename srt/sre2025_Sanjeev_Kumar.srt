1
00:00:00,500 --> 00:00:01,580
Hello everybody.

2
00:00:01,790 --> 00:00:02,630
My name's Sanjeev.

3
00:00:03,560 --> 00:00:08,280
Today my top is ensuring it
infrastructure, reliability through

4
00:00:08,280 --> 00:00:10,850
robust monitoring and alerting.

5
00:00:11,350 --> 00:00:13,770
So this slide is about introduction.

6
00:00:13,829 --> 00:00:16,769
What is monitoring what is alerting.

7
00:00:17,069 --> 00:00:18,990
So let's begin by understanding.

8
00:00:19,349 --> 00:00:25,439
Why monitoring and alerting are
essential to system reliability in the

9
00:00:25,439 --> 00:00:32,070
cloud in today's fast moving digital
infrastructure cloud system handle

10
00:00:32,070 --> 00:00:34,519
huge amount of dynamic workload.

11
00:00:34,910 --> 00:00:38,690
Any small issue like high latency.

12
00:00:39,364 --> 00:00:45,574
Memory spikes can quickly
snowball into service disruption.

13
00:00:46,355 --> 00:00:53,224
That's where monitoring alert, step
in monitoring tool give us a real time

14
00:00:53,405 --> 00:01:00,425
visuality into system health metric
like CPU uses latency and through output

15
00:01:00,925 --> 00:01:04,775
allow us to detect problem easily.

16
00:01:05,465 --> 00:01:09,005
Actionable alerting is just as important.

17
00:01:09,575 --> 00:01:12,305
It is not enough to get notified.

18
00:01:12,785 --> 00:01:18,245
Alerts must be meaningful,
targeted, and free from noise.

19
00:01:18,965 --> 00:01:25,865
This helps avoid alert fat,
which happens when team gets

20
00:01:26,405 --> 00:01:31,685
over blamed with non-critical
alerts and start ignoring them.

21
00:01:32,210 --> 00:01:39,649
As the research showed, integrating
reverse tool like Datadog New Relic

22
00:01:39,919 --> 00:01:48,540
reduced downtown downtime approximately
50% while improving resource utilization.

23
00:01:49,470 --> 00:01:54,390
Ultimately, this translate
into greater system stability,

24
00:01:55,020 --> 00:02:01,050
fastest incidents response, and
better experience for the users.

25
00:02:01,300 --> 00:02:02,740
Why monitor matters.

26
00:02:03,070 --> 00:02:09,400
Now let's talk about why monitoring is
critical in today's cloud environment.

27
00:02:10,060 --> 00:02:17,250
Cloud platform like AWS Azure
and Google Cloud have made it

28
00:02:18,210 --> 00:02:24,185
incredibly easy to scale application,
but that flexibility comes.

29
00:02:24,705 --> 00:02:32,055
Complexity service services are spread
across regions API, microservices,

30
00:02:32,055 --> 00:02:38,535
containers, and sometimes even
across hybrid or multi-cloud setup.

31
00:02:39,015 --> 00:02:44,085
In such dynamic system,
things can go wrong fast.

32
00:02:44,475 --> 00:02:48,560
You might see unexpected
latency, intermittent failure,

33
00:02:49,060 --> 00:02:51,285
or even complete outage.

34
00:02:51,785 --> 00:02:59,075
And often without a monitoring system
in place, you wouldn't know until

35
00:02:59,075 --> 00:03:01,955
you, until user start complaining.

36
00:03:02,615 --> 00:03:06,665
Monitoring tool XA eyes and ears.

37
00:03:07,385 --> 00:03:13,195
They continuously track performance
indication and system health in real

38
00:03:13,195 --> 00:03:17,185
time without this layer of observability.

39
00:03:17,605 --> 00:03:20,575
You are essentially flying blind.

40
00:03:21,075 --> 00:03:25,865
You are reacting after the
fact rather than productivity.

41
00:03:26,075 --> 00:03:32,305
So solving issue in essence,
monitoring, safety, your approach

42
00:03:32,665 --> 00:03:36,085
from being reached to preventative.

43
00:03:36,235 --> 00:03:41,845
It's become your early ING
system, helping to maintain system

44
00:03:41,845 --> 00:03:44,455
reliability, performance, and trust.

45
00:03:44,955 --> 00:03:47,055
So what are the tools in action?

46
00:03:47,325 --> 00:03:52,305
Let's look at the tools that
bring monitoring to life

47
00:03:52,365 --> 00:03:54,405
in real world environment.

48
00:03:54,905 --> 00:04:00,535
I have evaluated three popular monitoring
tool that is premature and open

49
00:04:00,535 --> 00:04:02,935
source metrics based monitoring tool.

50
00:04:03,475 --> 00:04:07,855
Great for containerized environmental
like Kubernetes number, those.

51
00:04:08,680 --> 00:04:14,150
Number two is Datadog e-commerce
assess solution that integrate well

52
00:04:14,210 --> 00:04:20,170
across cloud service and provides
powerful dashboard and alerts.

53
00:04:20,830 --> 00:04:26,159
And number three is like New Relic
known for application performance

54
00:04:26,460 --> 00:04:31,919
monitoring, real time observability,
and user experience tracking.

55
00:04:32,400 --> 00:04:34,949
These tools were tested across.

56
00:04:35,280 --> 00:04:41,309
Three major cloud platform
that is AWS Azure Google Cloud.

57
00:04:41,789 --> 00:04:47,999
In addition to simulated environments,
they were deployed in a real world health

58
00:04:47,999 --> 00:04:51,629
insurance company using Micro soft Azure.

59
00:04:52,129 --> 00:04:56,299
This added practical depth testing
how these tools perform under.

60
00:04:57,109 --> 00:05:01,999
Actual workload such as high
traffic during claim processing

61
00:05:01,999 --> 00:05:04,669
and compliance driven application.

62
00:05:04,879 --> 00:05:11,779
The goal was to understand their impact
on latency, resource users throughout

63
00:05:11,899 --> 00:05:19,629
and cost, and most importantly, how they
help detect and resolve issue easily.

64
00:05:20,129 --> 00:05:22,109
How they help detect.

65
00:05:23,070 --> 00:05:23,400
Resolve.

66
00:05:23,500 --> 00:05:24,130
Easily.

67
00:05:24,490 --> 00:05:30,550
This hands-on deployment gave us
valuable insight into the strength

68
00:05:30,609 --> 00:05:35,830
and trade of each tool in both
test and production environments.

69
00:05:36,330 --> 00:05:40,340
Now let's talk about the
essential factor to monitor.

70
00:05:40,990 --> 00:05:46,250
Essential factor to monitor one is
latency throughout output, CPUN,

71
00:05:46,250 --> 00:05:48,680
memory users and operation cost.

72
00:05:48,770 --> 00:05:53,830
Let's go one by one when it comes
to the monitoring, knowing what to

73
00:05:53,830 --> 00:05:56,680
measure, it just as important as having.

74
00:05:57,005 --> 00:05:57,875
Right tools.

75
00:05:58,385 --> 00:06:02,615
Let's walk through the four essential
factor that need to be tracked.

76
00:06:03,335 --> 00:06:04,625
Number one, latency.

77
00:06:05,125 --> 00:06:05,724
Latency.

78
00:06:05,724 --> 00:06:12,065
This measures the time it takes for
a system to respond to a request.

79
00:06:12,364 --> 00:06:17,914
High latency can frustrate user
and signal system slow down or

80
00:06:18,335 --> 00:06:20,674
backend issue through output.

81
00:06:21,549 --> 00:06:27,880
This is how many request or transaction
your system can handle per second.

82
00:06:28,419 --> 00:06:33,969
A high thought put means better
stability and more efficiency.

83
00:06:34,369 --> 00:06:35,599
Resource utilization.

84
00:06:36,099 --> 00:06:42,599
C-P-U-C-P-U-N Memory uses monitoring
these tells you how you, your

85
00:06:42,599 --> 00:06:44,520
infrastructure is being used.

86
00:06:44,940 --> 00:06:50,940
It spikes many indicates performance
bottleneck or in efficient

87
00:06:50,940 --> 00:06:53,490
application operation cost.

88
00:06:53,549 --> 00:06:55,020
That is also one factor.

89
00:06:55,590 --> 00:06:58,499
Other overheaded overlooked, but.

90
00:06:58,999 --> 00:07:05,689
Critical monitoring itself use resources
and some tools incur additional costs.

91
00:07:05,989 --> 00:07:07,279
You need to track this.

92
00:07:07,279 --> 00:07:14,299
Two, ensure that the benefit of
monitoring overweights the expense.

93
00:07:14,689 --> 00:07:20,634
These metrics to together give you
comprehensive view of system health.

94
00:07:21,144 --> 00:07:21,674
They help.

95
00:07:22,669 --> 00:07:26,659
Identify patterns, bottlenecks,
and opportunity to improve

96
00:07:26,659 --> 00:07:29,299
performance before user affected.

97
00:07:29,799 --> 00:07:35,680
Now let's let's see, like what
was like outcomes from using

98
00:07:35,680 --> 00:07:37,539
those tools, monitoring tools.

99
00:07:38,039 --> 00:07:39,539
That is a real world impact.

100
00:07:39,719 --> 00:07:40,949
What is the impact for it?

101
00:07:41,189 --> 00:07:45,119
Now let's look at the real world
impact these monitoring tools had

102
00:07:45,209 --> 00:07:47,969
across different cloud platform.

103
00:07:48,539 --> 00:07:54,559
After integrating tools like
Datadog and New Relic miserable

104
00:07:54,559 --> 00:08:01,150
improvement were observed on AWS
latency drop from a hundred MS to

105
00:08:01,630 --> 00:08:04,719
80 milli per second on all Azure.

106
00:08:05,219 --> 00:08:11,109
Through output increased from
1900 to 2,400 requests per

107
00:08:11,109 --> 00:08:13,119
second across all platform.

108
00:08:13,150 --> 00:08:18,429
Overall, downtown was
reduced by around 15%.

109
00:08:18,929 --> 00:08:23,400
These number might seem iCal at
first, but in cloud environment,

110
00:08:23,819 --> 00:08:26,819
especially with high traffic.

111
00:08:27,319 --> 00:08:30,299
Or mission mission critical application.

112
00:08:30,539 --> 00:08:37,349
Even a few millisecond of latency
or a few percentage point of uptime

113
00:08:37,409 --> 00:08:43,739
can make a huge difference in user
experience and business opportunities.

114
00:08:44,239 --> 00:08:51,259
This shows how effective monitoring tool
doesn't just provide visuality, it's.

115
00:08:51,619 --> 00:08:59,319
Actively help you optimize performance,
scale smarter, and deliver a

116
00:08:59,319 --> 00:09:02,049
more reliable service or users.

117
00:09:02,559 --> 00:09:08,619
These kinds of performance gain also
build trust, and with stakeholders,

118
00:09:08,709 --> 00:09:16,629
especially in regulated environment like
healthcare, finance, where downtime and

119
00:09:16,779 --> 00:09:19,629
delay can have serious consequences.

120
00:09:20,129 --> 00:09:26,079
Let's talk now, trade off and
overheads with monitoring ly board

121
00:09:26,079 --> 00:09:28,509
system performance and reliability.

122
00:09:28,959 --> 00:09:34,374
It is important to understand that
it comes with trade-offs first.

123
00:09:34,874 --> 00:09:38,444
There is a CPU uses overhead in the study.

124
00:09:38,564 --> 00:09:45,104
CPU uses increased by five
to 8% at cross platform after

125
00:09:45,164 --> 00:09:47,144
integrating monitoring tools.

126
00:09:47,624 --> 00:09:49,214
That's because of these tools.

127
00:09:49,214 --> 00:09:55,124
Collect, analyze, and sometimes
visualize washed amount of

128
00:09:55,364 --> 00:09:57,524
data in real var real time.

129
00:09:58,034 --> 00:10:02,264
Second, visa rise of
operation cost around.

130
00:10:02,804 --> 00:10:05,084
12% on average.

131
00:10:05,324 --> 00:10:10,124
This include the cost of the
tools themself and the extra cloud

132
00:10:10,124 --> 00:10:12,164
resources needed to run them.

133
00:10:13,004 --> 00:10:20,654
The these overhead are not necessarily
bad, but they need to be managed smartly.

134
00:10:21,254 --> 00:10:26,769
For instance, avoid full stack
monitoring if only application

135
00:10:26,769 --> 00:10:29,009
level inside are needed.

136
00:10:29,509 --> 00:10:32,149
Tune data collection intervals.

137
00:10:32,569 --> 00:10:36,949
Do you need metrics every second
or every five minute enough?

138
00:10:37,349 --> 00:10:42,170
We need to think on that as well,
monitoring the most critical KPI

139
00:10:42,170 --> 00:10:44,359
instead of tracking everything.

140
00:10:44,930 --> 00:10:50,900
The key takeaway here is that the
monitoring should be a strategic

141
00:10:51,290 --> 00:10:54,739
hand done the benefit far.

142
00:10:55,239 --> 00:11:00,639
The cost, but blindly enabling
all features can lead to the

143
00:11:00,639 --> 00:11:02,920
resource waste and unexpected Bill.

144
00:11:03,420 --> 00:11:06,399
Now let's talk about the preventing alert.

145
00:11:06,899 --> 00:11:11,954
So one of the most common challenges
monitoring is alerting for you when

146
00:11:12,044 --> 00:11:15,824
teams are bombarded with so many alerts.

147
00:11:16,530 --> 00:11:20,550
That they start ignoring
them, even the critical ones.

148
00:11:20,939 --> 00:11:28,050
To prevent this, we need to external and
intelligent alerting, not just noise.

149
00:11:28,550 --> 00:11:31,670
Start by defining a smart threshold.

150
00:11:31,670 --> 00:11:38,569
For example, don't alert when CPU
uses goes over 50% alert when it says

151
00:11:39,290 --> 00:11:43,459
above 90% for a sustained period.

152
00:11:43,959 --> 00:11:46,389
Second use dynamic baselines.

153
00:11:46,569 --> 00:11:52,930
Tools like Datadog and New Relic
can learn what normal looks like.

154
00:11:53,020 --> 00:11:59,729
An alert only hand behavior deviates
significantly group related alerts

155
00:11:59,729 --> 00:12:05,310
together using alert correlation
so teams see the bigger picture

156
00:12:05,609 --> 00:12:07,709
rather than isolated symptoms.

157
00:12:08,234 --> 00:12:12,674
Most importantly, make sure
alert are routed to the right

158
00:12:12,674 --> 00:12:15,074
team with proper context.

159
00:12:15,224 --> 00:12:20,024
So responder know exactly
what action to take.

160
00:12:20,474 --> 00:12:26,924
By doing this, you keep your team
focused on meaningful incidents,

161
00:12:27,494 --> 00:12:35,159
reduced burnout, and respond to real
problem faster in thought effectively.

162
00:12:35,779 --> 00:12:38,719
Alerting should be guide not over.

163
00:12:38,969 --> 00:12:40,649
What may be the best practices?

164
00:12:40,759 --> 00:12:44,209
Now let's bring all together
with some best practices for

165
00:12:44,239 --> 00:12:46,849
effective monitoring and alerting.

166
00:12:47,299 --> 00:12:49,639
Choose the right tool
for your environment.

167
00:12:50,299 --> 00:12:54,699
Not every tool fits every
use case, for example.

168
00:12:54,999 --> 00:12:59,550
Premises is ideal for the
Kubernetes environment with Datadog.

169
00:13:00,050 --> 00:13:06,590
While Datadog signs in hybrid cloud
setup with a strong integration needs

170
00:13:07,090 --> 00:13:10,180
continuously evaluate the cost benefit.

171
00:13:10,270 --> 00:13:15,400
Tradeoff monitoring should be
enhanced reliability, not drain

172
00:13:15,400 --> 00:13:20,500
resources, monitoring what matter
most and fine tune data collection.

173
00:13:21,000 --> 00:13:28,259
Frequency to avoid over conjunction
focus on ana actionable KPIs.

174
00:13:28,799 --> 00:13:34,499
Track metrics that directly impact user
experience and system health latency

175
00:13:34,619 --> 00:13:37,559
error rates throughout output ETC.

176
00:13:37,679 --> 00:13:44,089
Avoid getting lost of divinity
metrics integration, LRT into

177
00:13:44,089 --> 00:13:46,399
the incident response workflow.

178
00:13:47,060 --> 00:13:53,810
Use tool like page duty, slack,
or ServiceNow, to automatically

179
00:13:53,810 --> 00:13:59,229
notify the right team and reach
alerts with contact and speed up

180
00:14:00,009 --> 00:14:03,009
resolution, review and refine.

181
00:14:03,009 --> 00:14:08,889
Alert regularly system, and
evolve what was critical six

182
00:14:08,889 --> 00:14:11,199
months ago might not be today.

183
00:14:11,754 --> 00:14:17,874
Keep alert, up to date, and align
with current performance goal.

184
00:14:18,804 --> 00:14:25,544
Following these practice, ensure
your monitoring setup is efficient,

185
00:14:26,084 --> 00:14:31,774
scalable, and responsive, not
just technically sound, but also

186
00:14:31,984 --> 00:14:33,994
aligned with business needs.

187
00:14:34,294 --> 00:14:39,994
Other way and or future scope
may be AI driven monitoring, so

188
00:14:40,054 --> 00:14:42,634
predict issue before they occur.

189
00:14:43,174 --> 00:14:45,724
So it can be achieved by the ai.

190
00:14:46,144 --> 00:14:52,684
Use AI to analyze pattern
and defect anomalies early.

191
00:14:53,314 --> 00:14:56,974
Preventing outage and performance drops.

192
00:14:57,724 --> 00:15:05,565
Second optimize performance automatically
enable self healing system and dynamic

193
00:15:05,565 --> 00:15:12,585
threshold that has just in real time
reducing the need for manual oversight.

194
00:15:13,085 --> 00:15:20,135
Third, reduce cost and resource users
implement smart auto-scaling energy.

195
00:15:20,635 --> 00:15:24,715
Percent workflow placement and
noise, reducing alert system

196
00:15:24,715 --> 00:15:27,325
to lower operation cost.

197
00:15:27,825 --> 00:15:29,955
Now let's go to the conclusion.

198
00:15:30,885 --> 00:15:36,555
So looking ahead, the future of
monitoring is the increasingly being

199
00:15:36,555 --> 00:15:38,895
saved by AI and machine learning.

200
00:15:39,015 --> 00:15:41,730
Traditional monitoring relies heavily on.

201
00:15:42,230 --> 00:15:47,600
Predefined threshold and aesthetics
rule, but with AI driven monitoring

202
00:15:47,600 --> 00:15:53,960
system can learn from historical
data, detect complex patterns, and

203
00:15:53,990 --> 00:15:57,050
even predict issue before they occur.

204
00:15:57,550 --> 00:16:03,400
Imagine being alert not because
of CPU just spike, but because

205
00:16:03,400 --> 00:16:05,350
the system learned that.

206
00:16:05,850 --> 00:16:10,235
I learned that a specific pattern
of memory and disc behavior

207
00:16:10,295 --> 00:16:13,315
typically precedes a failure.

208
00:16:13,705 --> 00:16:18,585
That is the power of prediction,
monitoring product predictive monitoring.

209
00:16:19,065 --> 00:16:25,895
This approach helps reduce the need for
continuous manual monitoring, minimizing

210
00:16:26,075 --> 00:16:29,615
alert noise and false positives.

211
00:16:29,645 --> 00:16:30,395
Optimize.

212
00:16:30,740 --> 00:16:34,790
Resource allocation Dally
based on expected demand.

213
00:16:35,210 --> 00:16:42,410
AI can also help with auto remediation
where a minor issue are fixed.

214
00:16:42,410 --> 00:16:49,060
Without human inter intervention, saving
time and improving uptime, the next

215
00:16:49,390 --> 00:16:56,280
generation of tools will be smarter, more
adaptive, and better aligned with the.

216
00:16:56,780 --> 00:17:02,240
Evolving complexity of cloud native
environment like container and service

217
00:17:02,290 --> 00:17:08,460
serverless organizing that embrace
these technology only will gain

218
00:17:09,240 --> 00:17:14,790
completely as through the higher system
relevance and lower operation cost.

219
00:17:15,290 --> 00:17:20,090
Thank you for listening me and let
me know if you have any question.

220
00:17:20,210 --> 00:17:20,720
Thank you.

