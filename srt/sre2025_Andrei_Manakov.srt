1
00:00:00,500 --> 00:00:00,980
Hi everyone.

2
00:00:01,400 --> 00:00:04,970
I'm and Andrew Manko In the stock, I
want to cover the evolution of databases

3
00:00:04,970 --> 00:00:09,080
from population systems to modern earth
scale and distributed scale databases.

4
00:00:09,440 --> 00:00:11,840
I also discuss the tradeoffs you
facing on how to manage them.

5
00:00:12,340 --> 00:00:13,899
Let me tell a bit about myself.

6
00:00:13,989 --> 00:00:17,249
I've been working with diss for over
10 years, and right now I'm focusing

7
00:00:17,249 --> 00:00:18,509
on building ML infrastructure.

8
00:00:19,109 --> 00:00:22,049
Our team works on two major
products, much and share chat.

9
00:00:22,109 --> 00:00:25,219
We, which together have all three
a million monthly active users.

10
00:00:25,720 --> 00:00:25,960
Okay.

11
00:00:26,690 --> 00:00:29,570
before we dive into modern
databases, let's take a look quick

12
00:00:29,570 --> 00:00:31,070
look back to the first databases.

13
00:00:31,580 --> 00:00:34,820
This were complex structures or
where data was organized as nasty

14
00:00:34,820 --> 00:00:37,160
trees and or girls in parties.

15
00:00:37,190 --> 00:00:38,960
Working with them was extremely difficult.

16
00:00:39,560 --> 00:00:42,260
First, we had to remain
s to entire hierarchy.

17
00:00:42,260 --> 00:00:45,170
To get the record, you
need second application.

18
00:00:45,170 --> 00:00:47,000
You tied decoupled to the data structure.

19
00:00:47,030 --> 00:00:49,889
Imagine just, renaming a
field and everything breaks.

20
00:00:50,390 --> 00:00:53,960
And finally, there was no separation
between neur logical and physical error.

21
00:00:54,140 --> 00:00:58,014
Applications had to know exactly way
and how the, the data was stored,

22
00:00:58,585 --> 00:01:00,654
which made system fragile and flexible.

23
00:01:01,154 --> 00:01:05,574
In 1970s, code introduced something
that funda fundamentally changed the

24
00:01:05,574 --> 00:01:10,195
way we worked with data relation model
instead of context structures, he

25
00:01:10,195 --> 00:01:14,065
proposed a formal mathematical foundation
based on said theory and first order.

26
00:01:14,065 --> 00:01:14,815
P logic.

27
00:01:15,579 --> 00:01:20,930
I'm sure how, I'm sure many of you studied
collection algebra, at university, but

28
00:01:20,930 --> 00:01:24,860
it wasn't just a new format, it was a new
philosophy code defined 12 principles.

29
00:01:25,010 --> 00:01:28,550
And what's important is that they
addressed nearly all the key problems we

30
00:01:28,550 --> 00:01:30,950
just talked about on the previous slide.

31
00:01:31,040 --> 00:01:33,010
Slide after code, big throw.

32
00:01:33,400 --> 00:01:35,505
We entered the two ation databases.

33
00:01:35,765 --> 00:01:38,190
The idea is, he introduced
very quickly implementing real

34
00:01:38,190 --> 00:01:39,930
systems, why it works so well.

35
00:01:40,430 --> 00:01:45,020
Because relation databases had a clear,
formal foundation, said Syrian logic.

36
00:01:45,290 --> 00:01:49,850
This means that the base could be built
not on intuition, but on mathematics.

37
00:01:50,350 --> 00:01:53,480
A scale emerged in unified standard,
standardized query language.

38
00:01:53,960 --> 00:01:56,790
For the first time, developers
could work, with data declarative

39
00:01:57,180 --> 00:02:01,530
the declar without needing to
know how to store it internally.

40
00:02:02,190 --> 00:02:05,310
In relation, the basis introduced
transactional support based

41
00:02:05,310 --> 00:02:06,810
on the asset principles.

42
00:02:07,310 --> 00:02:09,380
Is a complete fully, or
didn't happen at all?

43
00:02:09,650 --> 00:02:11,810
No more health safety orders
on your shopping cart.

44
00:02:12,060 --> 00:02:14,970
they become a symbol of availability
and consistency, and of course,

45
00:02:15,030 --> 00:02:17,010
normalization played a huge role.

46
00:02:17,355 --> 00:02:21,195
We stopped storing the same data
in 10 different places and the data

47
00:02:21,195 --> 00:02:23,085
structure become clean and predictable.

48
00:02:23,745 --> 00:02:27,495
Skim data was now based on the
domain model, and again, I won't

49
00:02:27,495 --> 00:02:29,055
go into too much details here.

50
00:02:29,055 --> 00:02:32,805
I assume many of you have worked with
the fifth normals from in university.

51
00:02:33,305 --> 00:02:36,755
This was the gold, golden edge of
databases and many systems are still

52
00:02:36,755 --> 00:02:38,405
built on those foundations today.

53
00:02:39,125 --> 00:02:43,115
And some of the systems are still
well known like Oracle or FoxPro,

54
00:02:43,695 --> 00:02:45,175
which, was developed in that time.

55
00:02:45,675 --> 00:02:50,115
However, in the nineties, in era
began there of mass internet taxes.

56
00:02:50,745 --> 00:02:53,775
Now companies started building
web applications, online stores,

57
00:02:53,835 --> 00:02:56,745
digital services, the number
of users good drastically.

58
00:02:57,245 --> 00:03:00,435
And that came, the, dramatic
and case in database load.

59
00:03:01,005 --> 00:03:02,780
So how did this affect databases?

60
00:03:03,610 --> 00:03:06,050
First, the volume of the data skyrocketed.

61
00:03:06,550 --> 00:03:08,395
The foremost demands become critical.

62
00:03:08,455 --> 00:03:10,315
Users expect instantly responses.

63
00:03:10,375 --> 00:03:14,275
No one wanted to wait while they
say, figure out how to join five

64
00:03:14,335 --> 00:03:16,255
and almost different tables.

65
00:03:16,855 --> 00:03:20,155
The key challenge is that relation
databases were usually designed

66
00:03:20,155 --> 00:03:23,395
to run on a seamless server and
precise the, all data locally.

67
00:03:23,965 --> 00:03:28,135
But as the volume and complexity of data
grew, they have begun to struggle and the

68
00:03:28,135 --> 00:03:30,595
response, the industry began to adapt.

69
00:03:30,895 --> 00:03:33,985
And, first of all, valuation
database started, started to

70
00:03:33,985 --> 00:03:35,155
struggle on the heavy load.

71
00:03:35,215 --> 00:03:38,635
The natural first step was a
simple, intuitive, vertical scale.

72
00:03:39,295 --> 00:03:39,955
What does it mean?

73
00:03:40,765 --> 00:03:45,445
We simply upgrade to a more powerful
server, add more arm, a faster disc,

74
00:03:45,445 --> 00:03:47,395
a more CP power, and more CP course.

75
00:03:47,665 --> 00:03:48,295
It makes sense.

76
00:03:48,325 --> 00:03:49,705
Everything stays the same.

77
00:03:50,035 --> 00:03:51,745
One database, one entry point.

78
00:03:52,245 --> 00:03:53,625
The advantages are obvious.

79
00:03:54,165 --> 00:03:54,735
It's simple.

80
00:03:54,765 --> 00:03:58,405
No need to change the architecture, the
da the data is still stored centrally.

81
00:03:58,905 --> 00:04:00,285
We don't lose consistency.

82
00:04:00,675 --> 00:04:03,765
Transactions remain intact,
and we can still use complex

83
00:04:03,855 --> 00:04:05,355
SCO array, including joints.

84
00:04:05,855 --> 00:04:07,050
But this a serious downside.

85
00:04:07,730 --> 00:04:09,130
Vertical scaling has a hard limit.

86
00:04:09,430 --> 00:04:14,375
You want find a sale with 1000
girls if your product has hundreds

87
00:04:14,375 --> 00:04:16,175
of thousands of users online.

88
00:04:16,675 --> 00:04:18,385
Just want to survive

89
00:04:18,885 --> 00:04:22,365
the next logical step after radical
scaling physiological database separation.

90
00:04:22,695 --> 00:04:25,365
But now we do it deliberately
along domain boundaries.

91
00:04:25,935 --> 00:04:29,775
For example, imagine we have three
areas that are loosely coupled after

92
00:04:29,775 --> 00:04:34,755
integration users, and each of them
has its own data logic and all cycle.

93
00:04:35,025 --> 00:04:37,820
So we make the decisions to speed
them into separate databases.

94
00:04:38,375 --> 00:04:39,125
It's so simple.

95
00:04:39,455 --> 00:04:42,515
You not dealing with distributed
transactions or complex tools and

96
00:04:42,515 --> 00:04:43,745
just clearly separate domains.

97
00:04:43,745 --> 00:04:45,965
Minimize KAPLAN and the cheap regulation.

98
00:04:46,955 --> 00:04:49,475
The load is dis the load
is distributed right now.

99
00:04:49,895 --> 00:04:53,525
One user traffic space spike
doesn't affect the reporting system.

100
00:04:53,795 --> 00:04:57,775
If the our service crashes pepper
stay up and running and we deliver,

101
00:04:57,905 --> 00:05:01,925
deliberately avoid joins between,
these domains and we don't need them.

102
00:05:02,045 --> 00:05:04,565
Each domain is related,
handled by its own system.

103
00:05:05,065 --> 00:05:06,565
However there are limits.

104
00:05:07,165 --> 00:05:12,055
It is a lot on one domain increase
is, say the number of his gross tenix.

105
00:05:12,845 --> 00:05:16,055
again, for a situation where a
single can ha, can't handle it,

106
00:05:16,555 --> 00:05:20,905
solution isn't about scaling in
definitly, it's about extracting multi

107
00:05:20,955 --> 00:05:24,855
extracting multi logical workload,
which we can optimize independently.

108
00:05:25,105 --> 00:05:28,645
at this stage, many teams, engineers
came to an important realization.

109
00:05:29,050 --> 00:05:31,150
The fifth normal form isn't minority.

110
00:05:31,330 --> 00:05:34,170
You can live quite well
without, normalization.

111
00:05:34,170 --> 00:05:37,910
You can denormalize your data and
transaction aren't always necessary.

112
00:05:38,480 --> 00:05:41,990
Good performance for the product is
more important than internal elegance

113
00:05:42,050 --> 00:05:44,060
or stick adherence to cross model.

114
00:05:44,560 --> 00:05:46,360
And what does this mean in practice?

115
00:05:47,200 --> 00:05:49,690
Yes, we, let go of
stick exceed guarantees.

116
00:05:49,720 --> 00:05:52,690
It turned out that in most
real world product scanners,

117
00:05:52,690 --> 00:05:54,640
port integrity is overkill.

118
00:05:55,525 --> 00:05:59,485
For example, if a user clicks, like on
the post, it's not the end of the world.

119
00:05:59,515 --> 00:06:02,845
If it gets lost due to a failure,
you don't need the full transactional

120
00:06:02,845 --> 00:06:04,645
support for every minor action.

121
00:06:05,145 --> 00:06:07,185
Second, we normalize data.

122
00:06:07,245 --> 00:06:09,885
In other words, we copy this
fragments to different places.

123
00:06:10,245 --> 00:06:13,665
Yes, it breaks called principles
and may feel like a step back

124
00:06:13,665 --> 00:06:14,865
to previous historic time.

125
00:06:15,645 --> 00:06:20,065
But in, we get fast rates, fever
joints, and bad scalability.

126
00:06:20,485 --> 00:06:22,915
It's not just a technical
decision, it's a product decision.

127
00:06:23,590 --> 00:06:27,830
It allows us to handle massive load, and
response to user quickly and approach.

128
00:06:28,280 --> 00:06:30,050
And of course, this
approach has this downside.

129
00:06:30,260 --> 00:06:32,000
it requires careful thinking.

130
00:06:32,500 --> 00:06:34,570
Need a deep and extended
of the user journey.

131
00:06:34,630 --> 00:06:39,230
You must concise like cons,
constraints where appropriate while

132
00:06:39,230 --> 00:06:42,940
still ensure that consistency in
the places that matter there is

133
00:06:42,940 --> 00:06:44,530
still physical performance asylum.

134
00:06:45,130 --> 00:06:47,680
At this point, we still
rely on a single, server.

135
00:06:47,740 --> 00:06:48,520
You can't add more than.

136
00:06:49,020 --> 00:06:52,620
Thousand CP cores or
several terabytes of memory.

137
00:06:53,120 --> 00:06:57,680
And the next common step is it's
another way to reduce load on the

138
00:06:57,680 --> 00:07:01,730
database, especially in systems
where it's greatly overweight rates.

139
00:07:01,955 --> 00:07:02,615
How does it work?

140
00:07:03,115 --> 00:07:03,685
Very simple.

141
00:07:04,135 --> 00:07:07,605
Each application instances, stores
frequently use data in memory.

142
00:07:07,695 --> 00:07:11,925
For example, a reference data, user
profile, settings, query results.

143
00:07:12,735 --> 00:07:14,265
This gives a great performance.

144
00:07:14,805 --> 00:07:18,585
Instead of querying the database,
the app f the data from Gram.

145
00:07:18,855 --> 00:07:19,815
It's fast, simple.

146
00:07:19,815 --> 00:07:22,665
It does require to request it.

147
00:07:22,665 --> 00:07:23,835
Also reduce database a lot.

148
00:07:24,085 --> 00:07:28,695
so it also use database loads, but
it's, always there's a trade off.

149
00:07:29,195 --> 00:07:30,905
' cause data may become stale.

150
00:07:31,415 --> 00:07:35,945
If something changes in the database and
the local cache doesn't know about it,

151
00:07:36,095 --> 00:07:40,585
the user will, see an updated version
that the back is part of the compromise.

152
00:07:41,275 --> 00:07:43,675
Second question doesn't
scale the system itself.

153
00:07:44,485 --> 00:07:47,265
We still rely on the sa
same single database.

154
00:07:47,265 --> 00:07:52,125
We rise the selling, but we're still
bound the limits on this server and we

155
00:07:52,125 --> 00:07:54,815
are not getting, past those 1000 CP cores.

156
00:07:55,315 --> 00:07:57,925
Now we're approaching one of
the most fundamental concepts

157
00:07:58,075 --> 00:07:59,155
in distributed systems.

158
00:07:59,215 --> 00:08:03,495
The cap theorum the cover here, that
in, in distributed systems, you can

159
00:08:03,495 --> 00:08:07,695
achieve all three of the following
guarantees at the same time, see

160
00:08:07,755 --> 00:08:11,985
consistently consistency or not
see the same date at the same time.

161
00:08:12,225 --> 00:08:12,945
Availability.

162
00:08:13,560 --> 00:08:16,590
Request receive a response,
though it might be a bit over.

163
00:08:16,780 --> 00:08:19,090
outdated part intolerance.

164
00:08:19,420 --> 00:08:22,810
The system keeps operating even,
when there is a network partition.

165
00:08:23,310 --> 00:08:26,430
So as soon as a network
partition happens in the yellow

166
00:08:26,430 --> 00:08:28,290
part systems, it does happen.

167
00:08:28,790 --> 00:08:30,800
You are forced to pick
just two of the three.

168
00:08:31,550 --> 00:08:33,055
That's what the famous triangle says.

169
00:08:33,230 --> 00:08:36,640
Pick two, or let's say you want to,
your system to always respond and

170
00:08:36,640 --> 00:08:38,290
for all north to immune systems.

171
00:08:38,575 --> 00:08:41,215
In that case, you can tolerate
network practitioners.

172
00:08:41,635 --> 00:08:43,855
As network breaks, your system goes down.

173
00:08:44,095 --> 00:08:47,905
It's the classic ca model and it's
typical of traditional relation

174
00:08:47,905 --> 00:08:52,125
databases like graphs or MyQ,
running as a single install.

175
00:08:52,625 --> 00:08:57,125
And obviously such a systems guarantees
stick data, correctness, networking,

176
00:08:57,125 --> 00:08:59,405
garbage or stale data about cancer, right?

177
00:08:59,585 --> 00:09:00,815
Network partitions or classifiers.

178
00:09:01,315 --> 00:09:03,865
And here's where things get
interesting once you want to scale.

179
00:09:03,880 --> 00:09:07,530
To distribute, across nose, you
are forced to sacrifice easily

180
00:09:07,530 --> 00:09:09,060
availability or consistency.

181
00:09:09,960 --> 00:09:13,340
Tap forces us to make deliberate
architectural tradeoffs and

182
00:09:13,340 --> 00:09:15,380
just maybe small fun fact.

183
00:09:15,620 --> 00:09:19,610
The same was using the proposed purely
from empirical observations without

184
00:09:19,610 --> 00:09:21,350
formal, without a formal proof.

185
00:09:22,040 --> 00:09:25,010
However, one was coordinate or,
discussing it about very well.

186
00:09:25,010 --> 00:09:30,080
It was just a hypothesis, but
later it was formally proven.

187
00:09:30,815 --> 00:09:32,285
Maybe in 10 year, in 10 years.

188
00:09:32,785 --> 00:09:36,885
But, cap serum is a great starting point,
but it has one significant limitation.

189
00:09:37,185 --> 00:09:39,795
It only considers what happens
during the n partition.

190
00:09:40,605 --> 00:09:42,195
But what about normal operation?

191
00:09:42,375 --> 00:09:45,975
When the network is stable, after all,
partitions don't happen every day,

192
00:09:46,475 --> 00:09:47,945
and this is the way is more complete.

193
00:09:48,025 --> 00:09:51,085
complete model comes into
employee puzzle system.

194
00:09:51,585 --> 00:09:54,885
Puzzle system stands for
basically if there is a partition.

195
00:09:55,385 --> 00:09:58,475
You have to choose between
availability and consistency.

196
00:09:58,975 --> 00:10:04,465
Also, basically, under normal conditions,
you have to choose between latency and

197
00:10:04,465 --> 00:10:09,165
consistency In the other world, the
model says even when your network is

198
00:10:09,165 --> 00:10:12,795
healthy, you are still facing at head
of between latency and consistency.

199
00:10:13,295 --> 00:10:14,585
Take a look at the chart on the right.

200
00:10:15,245 --> 00:10:17,915
Want a low latency, it
will get faster responses.

201
00:10:18,620 --> 00:10:21,300
Possibly with, with stale
or inconsistent data.

202
00:10:21,800 --> 00:10:23,900
If you wanna take consistency,
you'll need to wait.

203
00:10:24,440 --> 00:10:26,300
The patient has to reach all replicas.

204
00:10:26,600 --> 00:10:28,140
Perform, synchronous replication.

205
00:10:28,350 --> 00:10:29,400
Wait for acknowledgements.

206
00:10:29,610 --> 00:10:31,650
Only there, does the
client get a response?

207
00:10:32,150 --> 00:10:36,590
The, this model gives us a more realistic
picture of how distributed system behave.

208
00:10:37,425 --> 00:10:39,990
Reminds us that tradeoffs
aren't just for emergencies.

209
00:10:40,470 --> 00:10:43,435
They, they're the everyday reality
of distributed architecture.

210
00:10:43,935 --> 00:10:49,005
And after all the tradeoffs and the
partitions in question arises, so when,

211
00:10:49,055 --> 00:10:51,095
should we actually use relation databases?

212
00:10:51,275 --> 00:10:52,505
And answer ship?

213
00:10:52,505 --> 00:10:54,365
Almost always.

214
00:10:54,425 --> 00:10:55,925
Unless you are dealing with extreme load.

215
00:10:56,480 --> 00:10:59,360
If you're hundred, for example, a
thousand request per second relation

216
00:10:59,360 --> 00:11:00,560
database is likely your best hand.

217
00:11:01,040 --> 00:11:02,810
You don't need, you don't
really need to scale it up.

218
00:11:02,900 --> 00:11:07,250
Maybe you just need to do logical
split, maybe cash in, for example.

219
00:11:08,060 --> 00:11:11,900
You'll get all benefit of relation
models, transactions, strong, consistency,

220
00:11:11,900 --> 00:11:16,580
normalization amazing, and best of all,
you don't have to think about cup or

221
00:11:16,610 --> 00:11:19,490
puzzle C. You just walk out of the box.

222
00:11:19,740 --> 00:11:23,430
and this is more important, even
that scenario, you still need to

223
00:11:23,430 --> 00:11:25,050
account for possible failures, right?

224
00:11:25,050 --> 00:11:30,360
So like several scar discs, relation
type aren't magic, and without they

225
00:11:30,360 --> 00:11:31,590
become a single point of failure.

226
00:11:31,950 --> 00:11:35,610
So basically you need to make
some mirroring maybe, or basically

227
00:11:35,850 --> 00:11:36,825
smart strategy of pick happen.

228
00:11:37,325 --> 00:11:42,165
And obviously you can build a scalable
system in traditional national databases.

229
00:11:42,780 --> 00:11:48,390
I'll actually touch on that a bit later
so we can do it with my scale or polyus

230
00:11:49,110 --> 00:11:51,060
without any additional, extensions.

231
00:11:51,560 --> 00:11:55,100
But in it, you'll lose many of
the core advantages at that point.

232
00:11:55,100 --> 00:11:58,550
You're basically using a no scale database
with a scale interface and scaling will

233
00:11:58,550 --> 00:12:00,290
be more complex to configure and maintain.

234
00:12:00,790 --> 00:12:02,320
And let's go.

235
00:12:02,320 --> 00:12:04,425
And we saw the hacks for Australian.

236
00:12:05,080 --> 00:12:07,900
When the single database instance
is no longer enough, especially

237
00:12:07,900 --> 00:12:11,060
in terms of free performance,
many teams, turns to replication.

238
00:12:11,630 --> 00:12:14,030
So what does it actually mean?

239
00:12:14,840 --> 00:12:18,620
We have on primary note, there's only
one node that handles right from there.

240
00:12:18,680 --> 00:12:21,590
The primary automatic replication
data to read only replicas.

241
00:12:22,090 --> 00:12:25,715
It give us virtually unlimited IT
scalability, need to handle more

242
00:12:25,715 --> 00:12:27,185
traffic, just add more replicas.

243
00:12:27,845 --> 00:12:30,005
Users read froma.

244
00:12:30,125 --> 00:12:31,325
It's fast, low, and efficient.

245
00:12:31,825 --> 00:12:32,605
The important point.

246
00:12:32,695 --> 00:12:36,525
Application introduced latency, so
the right goes to the primary, then

247
00:12:36,885 --> 00:12:41,135
it into queue, and only after that
is, is, is propagated to the replicas,

248
00:12:41,795 --> 00:12:44,735
the budget on the system load or,
and the way the application is.

249
00:12:44,735 --> 00:12:49,885
In Synchron, this delay and range from
milliseconds, two seconds as results users

250
00:12:49,885 --> 00:12:54,175
might see data, especially if they write
one note and immediately get from another.

251
00:12:54,675 --> 00:13:00,255
And this is a AP model in cap, the
availability, but is he consistency?

252
00:13:00,755 --> 00:13:05,015
And another key point overall
still goes for a single primary.

253
00:13:05,195 --> 00:13:08,915
So if the primary becomes overloaded,
we hit the right throughput Cell

254
00:13:09,665 --> 00:13:10,775
application is a good solution.

255
00:13:10,775 --> 00:13:14,915
The new system is red heavy
and red light, but it does make

256
00:13:14,915 --> 00:13:16,385
your system fully scalable.

257
00:13:16,415 --> 00:13:19,205
It requires careful consideration
of consistency, tradeoffs.

258
00:13:19,705 --> 00:13:23,465
Sars when we take one large table
and split into, many smaller ones,

259
00:13:23,945 --> 00:13:25,955
distributing them across different nodes.

260
00:13:26,435 --> 00:13:29,535
For example, you might, roll one
group of users to one server, another

261
00:13:29,535 --> 00:13:31,305
group to a second server, and so on.

262
00:13:32,055 --> 00:13:36,105
The key point is you starting with
scale, not just rates, also, right?

263
00:13:36,525 --> 00:13:39,525
Unlike application, we all
right, go to a single primary.

264
00:13:39,825 --> 00:13:42,765
Now each chart handles its own
position of the use traffic.

265
00:13:43,665 --> 00:13:47,145
This allows us to scale horizontally,
serve millions of future, and who

266
00:13:47,145 --> 00:13:51,515
knows, maybe even built our own
version of Facebook first predate data.

267
00:13:51,515 --> 00:13:52,635
The and how you split data.

268
00:13:52,635 --> 00:13:56,235
The personal specific case, the
most common approach is hash based

269
00:13:56,235 --> 00:13:57,675
sharding, for example, by user.

270
00:13:58,175 --> 00:14:03,005
Another option is using a cap table, the
strong switch charts, hold, switch data,

271
00:14:03,995 --> 00:14:08,555
some systems chart by geography, user type
data, or even business domain as well.

272
00:14:09,055 --> 00:14:11,455
But of course, it's
not all smooth selling.

273
00:14:11,815 --> 00:14:12,055
First.

274
00:14:12,415 --> 00:14:15,265
In the design, we prioritize
consistency of availability.

275
00:14:15,775 --> 00:14:18,985
If a chart goes down, part of your
data becomes temporarily unavailable.

276
00:14:19,735 --> 00:14:21,715
Second, you lose multi shark transactions.

277
00:14:21,955 --> 00:14:23,785
You can't update to charts automatically.

278
00:14:23,785 --> 00:14:28,035
Using ac semantics means your data,
model needs to be carefully designed,

279
00:14:28,695 --> 00:14:33,135
analytical, like a simple cell count,
star, become much more complex ate.

280
00:14:33,635 --> 00:14:34,720
and no scale emerge.

281
00:14:35,255 --> 00:14:37,775
The industry response to the
change of scaling databases.

282
00:14:38,195 --> 00:14:40,295
The industry began to
look for new solution.

283
00:14:40,325 --> 00:14:43,205
A new class or data
stores appeared designed.

284
00:14:43,735 --> 00:14:46,580
Not around strict schemas and
messy transaction, but around

285
00:14:46,580 --> 00:14:48,260
scalability and performance.

286
00:14:48,710 --> 00:14:51,860
As I said, we can make our volution
the base code using previous princip,

287
00:14:51,890 --> 00:14:53,690
but it will be just a harder to do.

288
00:14:54,190 --> 00:14:56,260
The design philosophy is also different.

289
00:14:56,800 --> 00:15:00,410
It's very first we began, we begin
by asking what kind of query will our

290
00:15:01,190 --> 00:15:05,090
apps run, when we design the data model
specifically to make, those queries

291
00:15:05,090 --> 00:15:06,680
as fast as efficient as possible.

292
00:15:07,025 --> 00:15:10,445
This is the opposite, o opposite
of the traditional approach where

293
00:15:10,445 --> 00:15:14,285
we build a clean normalize schema
first and only later, try to

294
00:15:14,285 --> 00:15:15,935
optimize growth on top of the of it.

295
00:15:16,435 --> 00:15:20,295
Additionally, the scale system don't
necessarily, necessarily use tables.

296
00:15:20,300 --> 00:15:21,870
They do, they don't require scale.

297
00:15:22,110 --> 00:15:24,240
Database has its own data model interface.

298
00:15:24,270 --> 00:15:26,430
Some work with Jason
documents like MongoDB.

299
00:15:26,940 --> 00:15:28,770
Others use key value pairs like Radius.

300
00:15:29,270 --> 00:15:32,810
And as for transactions in the
classical sense, they're often missing.

301
00:15:33,335 --> 00:15:36,965
Some systems do support local transactions
with a single entity of partition,

302
00:15:36,965 --> 00:15:41,045
some comprehensive operation, but
global asset guarantees across notes

303
00:15:41,165 --> 00:15:42,755
as exception rather than the rule.

304
00:15:43,415 --> 00:15:46,205
Instead scale database, focus
on simplicity for tolerance

305
00:15:46,295 --> 00:15:47,375
and horizontal scalability.

306
00:15:47,875 --> 00:15:49,765
But what if you do need transactions?

307
00:15:49,765 --> 00:15:51,235
The span across multiple nodes.

308
00:15:51,735 --> 00:15:55,095
Once all that is spread across multiple
nodes or database and you change appears,

309
00:15:55,215 --> 00:15:59,865
how can you ensure that all of them is
complete set of appear together, or none

310
00:15:59,865 --> 00:16:02,505
of them do it all to make this work?

311
00:16:02,505 --> 00:16:05,715
We use coordination protocols, for
example, to phase commit or more

312
00:16:05,745 --> 00:16:09,910
advance or ones like Axus or rt, which
are used in consensus based system.

313
00:16:10,410 --> 00:16:14,100
He's a sin in scenario, distributed
transaction to do serious dose.

314
00:16:14,820 --> 00:16:15,810
Firstly, performance.

315
00:16:16,080 --> 00:16:17,990
To commit the transaction,
you have to wait for, for a

316
00:16:17,990 --> 00:16:19,460
response from every participant.

317
00:16:19,460 --> 00:16:21,320
Even if just one of them is slow.

318
00:16:22,040 --> 00:16:25,610
Consensus is inherently slow and
in many cases, resource are locked

319
00:16:25,610 --> 00:16:27,080
until the second phase is complete.

320
00:16:27,890 --> 00:16:29,690
This means as the parish
might be stuck away.

321
00:16:30,190 --> 00:16:32,620
If the K fails, it's
exactly the wrong moment.

322
00:16:32,620 --> 00:16:35,250
The system might, get stuck
in the awkward middle state.

323
00:16:35,580 --> 00:16:38,100
The transaction is committed,
but it's not rolled back either.

324
00:16:38,310 --> 00:16:40,980
This is a classic weakness of
two phase commit, for example.

325
00:16:41,460 --> 00:16:45,260
It's not resilient to kta crashes
and complexity Distributed

326
00:16:45,260 --> 00:16:46,730
transac are hard to implement.

327
00:16:46,760 --> 00:16:51,080
You have to think about ML ized failures
and duplicate in every single step.

328
00:16:51,080 --> 00:16:53,660
All use cases for all, operators.

329
00:16:54,160 --> 00:16:56,710
So distributed transactions are
possible, but have a lot of problems.

330
00:16:56,710 --> 00:17:00,130
That's why in most distributed systems,
we try to avoid them where we can.

331
00:17:00,780 --> 00:17:05,430
And sometimes we need to have, as the
need possibility kept going, but so did

332
00:17:05,430 --> 00:17:08,520
the desire to keep transaction guarantees
A new cluster of the base emerged.

333
00:17:08,940 --> 00:17:12,360
This class is known as distributed
sq. The cover is simple.

334
00:17:12,600 --> 00:17:14,970
Take a traditional scope
database, make it distributed.

335
00:17:15,765 --> 00:17:20,085
But keep guarantees and familiar
interface every, just like in public

336
00:17:20,085 --> 00:17:24,345
customer scale only now it can
scale across dozen or hard of nodes.

337
00:17:24,595 --> 00:17:27,045
scale system, preserved
transactional consistency.

338
00:17:27,105 --> 00:17:30,345
You can use familiar transactions,
inspect, expect state consistency

339
00:17:30,345 --> 00:17:32,955
and design data models using
well known relation patterns.

340
00:17:33,405 --> 00:17:35,685
It makes the from legacy
system much easier.

341
00:17:36,185 --> 00:17:38,495
Yes, and the scale is a
current interface as well.

342
00:17:38,825 --> 00:17:43,195
No need to learn a new language or special
a i And the system has not scalable.

343
00:17:43,285 --> 00:17:45,775
They support automatic
charting, application dispute,

344
00:17:45,775 --> 00:17:47,005
transactions, architecture.

345
00:17:47,245 --> 00:17:51,970
They period closely to no scale
systems, but with relation passade.

346
00:17:52,795 --> 00:17:54,415
But of course, nothing come for free.

347
00:17:54,835 --> 00:17:57,715
Even this automatic charting, you
still have to keep in mind that when

348
00:17:57,715 --> 00:18:02,065
designing your schema, if frequently
perform joint across tables that

349
00:18:02,065 --> 00:18:05,565
live, that live on different charts,
those queries are become very slow.

350
00:18:05,925 --> 00:18:09,945
Even though everything works the same
goes for distributed transactions.

351
00:18:10,515 --> 00:18:13,965
Yes, you can update multiple charts
automatically, but it's going to be slower

352
00:18:14,265 --> 00:18:17,945
as then perform the transaction inside
a single node, which means whoever is

353
00:18:17,945 --> 00:18:21,335
schema design is absolutely critical,
and again, you still need to use it.

354
00:18:21,695 --> 00:18:24,695
Query first approach rather than
traditional domain First approach.

355
00:18:24,695 --> 00:18:28,705
in relation databases from cap
perspective, distributor skill

356
00:18:28,705 --> 00:18:31,970
system usually prioritize consistency
or availability and latency.

357
00:18:32,785 --> 00:18:35,725
Consistency preserved, but due
network issues, part of the system

358
00:18:35,725 --> 00:18:37,375
may become temporarily unavailable.

359
00:18:37,675 --> 00:18:41,395
And now in practice, global transactions
across charts are rarely needed.

360
00:18:41,515 --> 00:18:44,695
Most of our code can be handled
ju defined by no scale databases

361
00:18:45,195 --> 00:18:46,995
with eventual consistency.

362
00:18:47,495 --> 00:18:49,475
And what about learning
a nuclear language?

363
00:18:49,975 --> 00:18:52,165
To be honest, it's not really
a problem for more developers.

364
00:18:52,165 --> 00:18:54,715
So it takes just, it's a
matter of days usually.

365
00:18:55,215 --> 00:18:57,120
And, let's go to more practice.

366
00:18:57,620 --> 00:19:01,080
On this slide, we see how the cap
serum applies to medicine practice.

367
00:19:01,580 --> 00:19:04,340
Radius is a blazing fast
in my key value store.

368
00:19:04,940 --> 00:19:08,000
It can be configured in different
ways depending on your needs.

369
00:19:08,450 --> 00:19:12,050
In this setup, a register run in a
clustered configuration charging.

370
00:19:12,530 --> 00:19:15,890
Each key is hashed and rotate
to a specific chart in W

371
00:19:15,890 --> 00:19:17,450
horizontal screen, oh, sorry.

372
00:19:17,630 --> 00:19:18,170
Gimme a second.

373
00:19:18,670 --> 00:19:19,600
Let's look at this.

374
00:19:19,600 --> 00:19:20,770
Throw a lens of cap.

375
00:19:21,270 --> 00:19:22,410
Consistency is prioritized.

376
00:19:22,680 --> 00:19:25,440
Rise, go to the primary node and
the main correct, which is critical

377
00:19:25,440 --> 00:19:26,820
for things like distributed logs.

378
00:19:26,880 --> 00:19:31,320
For example, our ability sacrificed
if, shark fails may reduce separation.

379
00:19:31,620 --> 00:19:32,760
Strong in consistent data.

380
00:19:33,420 --> 00:19:34,470
Partitions is supported.

381
00:19:34,620 --> 00:19:37,200
Risk can continue operating with
the healthy part of the system.

382
00:19:37,700 --> 00:19:39,965
We're able to scale already
by increase number of charts.

383
00:19:40,365 --> 00:19:43,125
In this configuration, we
sacrifice availability to maintain

384
00:19:43,125 --> 00:19:46,385
consistency, which is important
for correctness, critical use cases

385
00:19:46,385 --> 00:19:48,425
like distributed logs, air section.

386
00:19:48,425 --> 00:19:49,145
You mentioned before.

387
00:19:49,445 --> 00:19:52,205
on this slide, we see already
in a different configuration.

388
00:19:52,445 --> 00:19:55,715
We still use Shagan, but
now his chart is replicated.

389
00:19:56,345 --> 00:20:00,545
Client write to the primary battery
from either primary or replicas.

390
00:20:01,045 --> 00:20:04,735
Boots availability part and
partition tolerance in of some nodes.

391
00:20:04,915 --> 00:20:08,455
If some nodes go down, the
systems stay responsive.

392
00:20:08,955 --> 00:20:12,115
The tradeoff, is consistency
because of application like

393
00:20:12,115 --> 00:20:13,765
that may reach still data.

394
00:20:14,265 --> 00:20:19,215
So the set high availability or stick
consistency, ideal ation or right heavy

395
00:20:19,215 --> 00:20:21,435
systems where a freshness isn't critical.

396
00:20:21,935 --> 00:20:25,055
For contrast, here's another red
configuration with no application and

397
00:20:25,055 --> 00:20:29,905
no shotgun in this case re behaves more
like a traditional error database, a

398
00:20:29,905 --> 00:20:32,845
single instance and write, go to one node.

399
00:20:33,145 --> 00:20:36,025
Now let's take a moment and talk
more specifically about consistency.

400
00:20:36,595 --> 00:20:40,160
What, what you see here is the classic
classification of consistency models.

401
00:20:41,050 --> 00:20:43,120
On the left, we have
traditional relation models.

402
00:20:43,570 --> 00:20:46,510
This mostly describe different levels
of transaction is installation.

403
00:20:47,010 --> 00:20:50,040
On the right we have consistent
models used in distributed systems.

404
00:20:50,340 --> 00:20:55,290
At the top we see the strongest models
where this appear instantly on all nodes.

405
00:20:55,645 --> 00:21:00,595
The further down you go the week is
guarantees data might take time to

406
00:21:00,595 --> 00:21:02,665
propagate or be consistent for a while.

407
00:21:03,195 --> 00:21:05,620
But he think, but he is the thing.

408
00:21:05,800 --> 00:21:09,760
In practice, people often don't talk
about consistency in such formal terms.

409
00:21:10,260 --> 00:21:13,410
Let's break down some of the most common
consistency models used in practice.

410
00:21:14,250 --> 00:21:15,180
Strong consistency.

411
00:21:15,180 --> 00:21:18,420
There is the most reliable
and stick model after, right?

412
00:21:18,450 --> 00:21:20,190
All client immediately
sees the same value.

413
00:21:20,550 --> 00:21:22,140
There is no delay, no certainty.

414
00:21:22,170 --> 00:21:23,400
Everything is perfect in sync.

415
00:21:23,900 --> 00:21:24,890
Eventual consistency.

416
00:21:25,430 --> 00:21:27,080
This is the weakest consistency model.

417
00:21:27,580 --> 00:21:32,260
After a while, right after right, data is
gradually, synchron cross nodes eventually

418
00:21:32,590 --> 00:21:35,200
becomes consistent, but not right away.

419
00:21:35,500 --> 00:21:40,390
In practice, this delay is usually just
a few milliseconds, and it's more than

420
00:21:40,390 --> 00:21:42,130
enough for more real work use cases.

421
00:21:42,550 --> 00:21:46,545
This small model is typical for
distributed, no scale device like DY DB

422
00:21:46,545 --> 00:21:49,670
or Cassandra and Turnable consistency.

423
00:21:50,170 --> 00:21:53,020
Here you can adjust the level
of consistency per request.

424
00:21:53,620 --> 00:21:57,890
For example, consider, the right
success only if it's technology

425
00:21:57,890 --> 00:21:59,240
by at least two out of three.

426
00:21:59,740 --> 00:22:02,230
This approach is supported by
system like Cassandra and cib.

427
00:22:02,730 --> 00:22:05,670
It gives you the flexibility to balance
performance and the availability

428
00:22:06,030 --> 00:22:07,770
the depend on the specific use case.

429
00:22:08,030 --> 00:22:08,390
a letter.

430
00:22:08,390 --> 00:22:12,450
I want to, basically show
different, based on c db, different

431
00:22:12,450 --> 00:22:13,530
configuration of the C db.

432
00:22:14,380 --> 00:22:18,520
DB is a modern distributed no scale
database, inspired by Cassandra by

433
00:22:18,520 --> 00:22:20,050
designed for much higher performance.

434
00:22:20,550 --> 00:22:22,740
One of its key feature
is tuneable consistency.

435
00:22:22,740 --> 00:22:29,310
As a query level, you can define how
many must control, or this gives you

436
00:22:29,340 --> 00:22:33,030
de control or the trade off between
speed, availability, and consistency.

437
00:22:33,530 --> 00:22:37,510
In our request works, the client
connects to any node as the

438
00:22:37,510 --> 00:22:39,125
note becomes to be, a coordin.

439
00:22:39,320 --> 00:22:42,080
the coordinator, figures out
which nodes are responsible for

440
00:22:42,080 --> 00:22:44,120
the requested keys based on hash.

441
00:22:44,620 --> 00:22:47,620
It then follows the request to
the app, appropriate replicas.

442
00:22:47,680 --> 00:22:52,470
For example, if, a pick factor equals
three, the right is sent to three nodes.

443
00:22:52,970 --> 00:22:55,940
The coordinator wastes for acknowledgement
based on the choosing consist level.

444
00:22:56,660 --> 00:22:59,240
Once they're done, it returns
the result to the client.

445
00:22:59,740 --> 00:23:02,560
The key point is the client is
completely unaware of how the data

446
00:23:02,560 --> 00:23:06,070
is distributed or the complexity
is handled by the coordinator.

447
00:23:06,370 --> 00:23:10,600
This allows scale horizontally in
point most performance and resilience.

448
00:23:10,660 --> 00:23:15,580
So as you add monos and, this now single
point of failure or not are equal.

449
00:23:16,080 --> 00:23:20,460
And see how a right works with,
consist, level equals all the client

450
00:23:20,460 --> 00:23:21,930
sends a right to the coordinator.

451
00:23:22,230 --> 00:23:24,645
The coordinator follows
their right to all arabicas.

452
00:23:25,295 --> 00:23:28,820
The right is only successful if all
three arabicas control the operation.

453
00:23:29,360 --> 00:23:33,080
If even one replicas are reachable,
for example, due to any drug

454
00:23:33,080 --> 00:23:35,300
partition, the person is rejected.

455
00:23:35,800 --> 00:23:39,060
What this means, So we have
a strong consistency, lower

456
00:23:39,180 --> 00:23:41,460
availability, and lower performance.

457
00:23:41,820 --> 00:23:47,340
So basically we, prioritize consist
consistency, o over availability

458
00:23:47,825 --> 00:23:52,665
in, in case of any partitioning
or latency in normal behavior,

459
00:23:52,875 --> 00:23:54,460
in normal operation level.

460
00:23:54,960 --> 00:23:57,720
Level, all is just in critical scenarios.

461
00:23:58,080 --> 00:24:00,660
Such a bank transfers payment procession.

462
00:24:00,900 --> 00:24:03,960
On any case where data occurrence
is more important than availability.

463
00:24:04,460 --> 00:24:06,590
In this situation, it is better
to fail the operation than

464
00:24:06,590 --> 00:24:08,210
to risk data in consistency.

465
00:24:08,710 --> 00:24:11,520
Another example, Right work is level one.

466
00:24:11,790 --> 00:24:13,710
The client sends the
right to the coordinator.

467
00:24:13,830 --> 00:24:15,630
The client sends the
right to all replicas.

468
00:24:16,200 --> 00:24:20,710
But the two success after just
one acknowledgements, the remain

469
00:24:20,710 --> 00:24:22,360
replicas updated asynchronously.

470
00:24:23,200 --> 00:24:27,280
What this means high availability,
even if most node are down

471
00:24:27,400 --> 00:24:29,020
right, risk can still proceed.

472
00:24:29,800 --> 00:24:33,773
Let's say we able to, work
just, using 33% of all.

473
00:24:34,670 --> 00:24:36,880
No, consistency is not guaranteed at all.

474
00:24:36,880 --> 00:24:40,440
Clients merit stale data and
maximum performance responses

475
00:24:40,440 --> 00:24:41,640
are returned very quickly.

476
00:24:41,890 --> 00:24:45,970
and this consist level is ideal
for newsfeed ads, analytic

477
00:24:46,060 --> 00:24:47,105
compliance, and et cetera,

478
00:24:47,605 --> 00:24:51,520
and how a right work with
consist level equals chrome.

479
00:24:52,020 --> 00:24:53,910
The client sends the request to
a coordinator, the coordinator

480
00:24:54,360 --> 00:24:56,190
for us to it to all applicants.

481
00:24:57,165 --> 00:25:00,525
The right is successful once
a majority of her confirm and

482
00:25:00,525 --> 00:25:02,385
mainly update asynchronously.

483
00:25:02,885 --> 00:25:06,905
And that's basically a good balance
with we have a good consistency, better

484
00:25:06,905 --> 00:25:13,740
availability than in from level equals
all and still pretty good performance

485
00:25:13,770 --> 00:25:16,010
faster than level equals all mass.

486
00:25:16,010 --> 00:25:18,200
Lower consist level equals one.

487
00:25:18,700 --> 00:25:22,180
So it's a balanced between speed,
availability, and consistency.

488
00:25:22,690 --> 00:25:25,960
Ideal for important, but not
mission critical operations.

489
00:25:26,460 --> 00:25:28,940
And, another you have in databases.

490
00:25:29,440 --> 00:25:33,210
I want to, talk a little bit about in
Giants I. This is a classic data structure

491
00:25:33,330 --> 00:25:36,540
that value used generation databases,
and even in some no scale databases.

492
00:25:36,720 --> 00:25:38,820
Many of you probably ed
from university again.

493
00:25:39,270 --> 00:25:41,040
So I want to go into internals.

494
00:25:41,100 --> 00:25:45,210
In short, all data is stored on this,
but frequently accessed that data is

495
00:25:45,210 --> 00:25:46,890
cashed in memory for better performance.

496
00:25:46,980 --> 00:25:49,680
Immunizing these successes is
key for keeping the since fast.

497
00:25:49,930 --> 00:25:53,800
between optimize for IT fever, disc
lookups, I needed to find a record.

498
00:25:54,295 --> 00:25:54,955
Perfect for it.

499
00:25:54,955 --> 00:25:57,595
Heavy workload, but it's
not ideal for heavy rights.

500
00:25:58,525 --> 00:26:00,295
It lots of insert updates.

501
00:26:00,355 --> 00:26:04,235
The B three extraction is constant,
constantly balancing, which can slow

502
00:26:04,715 --> 00:26:07,625
things down significantly because
it will produce a lot of discards.

503
00:26:08,125 --> 00:26:11,575
And we have another type of,
database on giants which are based

504
00:26:11,575 --> 00:26:14,530
on L lsat, LST three or LS three.

505
00:26:15,030 --> 00:26:17,680
Lti, is designed for higher alterna.

506
00:26:17,680 --> 00:26:22,270
Go first into memory and only later
half flash to disc and batches, which

507
00:26:22,270 --> 00:26:26,590
greatly improves right performance
and reduce disc wear our, the system,

508
00:26:26,590 --> 00:26:30,370
first checks in memory cache, roll
cache, ma table, and immutable tables.

509
00:26:30,790 --> 00:26:31,890
If data isn't found.

510
00:26:31,940 --> 00:26:33,215
There it is from this.

511
00:26:33,715 --> 00:26:36,895
Balloon filter helps keep files
that defin, definitely don't

512
00:26:36,895 --> 00:26:37,945
contain the request scheme.

513
00:26:38,225 --> 00:26:40,505
blue filter, basically
it's a probability set.

514
00:26:40,595 --> 00:26:43,245
It's a data structure which,
which help you understand whether,

515
00:26:43,795 --> 00:26:48,515
data is, exist anywhere or not so
ideal for white heavy workloads.

516
00:26:48,520 --> 00:26:51,815
But if you system is already
heavy with slightly few rights,

517
00:26:51,935 --> 00:26:53,855
a, BT maybe better to fit.

518
00:26:54,355 --> 00:26:56,185
So only choose the storage.

519
00:26:56,185 --> 00:27:01,645
And then based on workload, write heavy
go with LS mt. right heavy choose between.

520
00:27:02,145 --> 00:27:05,835
And a little more about modern
trends with the finish method.

521
00:27:06,555 --> 00:27:11,135
Today many companies take popular, battle
test databases and remove them from the

522
00:27:11,135 --> 00:27:14,665
ground app using the fast language like c
plus, and a heavy optimized architecture.

523
00:27:15,165 --> 00:27:18,735
Important is IP stays the same, the
developers don't need to learn anything,

524
00:27:19,035 --> 00:27:20,565
but under the hood everything is new.

525
00:27:20,775 --> 00:27:26,015
No, no second and full CP
organizations and result is amazing.

526
00:27:26,435 --> 00:27:28,745
My performance game sometimes
techniques or more is the

527
00:27:28,745 --> 00:27:30,095
same features and interfaces.

528
00:27:30,095 --> 00:27:32,425
Developers already know
and won't surprise me.

529
00:27:32,725 --> 00:27:36,040
We usually, everybody knows that we
should not write every from scratch.

530
00:27:36,150 --> 00:27:40,260
Right now we have a lot of companies
which did exactly the same.

531
00:27:40,260 --> 00:27:42,660
They chose, already existing technology.

532
00:27:43,350 --> 00:27:47,860
They write completely using
modern, architecture, and they

533
00:27:47,860 --> 00:27:49,390
make a pretty nice business model.

534
00:27:49,890 --> 00:27:53,910
And how, what's the core
feature of, or, every such,

535
00:27:53,910 --> 00:27:56,450
systems, they use architecture.

536
00:27:57,275 --> 00:27:58,685
Take a look at this image, for example.

537
00:27:58,685 --> 00:28:03,185
On the left, we see a typical multi thread
systems, with tri resources all, docks.

538
00:28:03,365 --> 00:28:05,585
The thread are fighting
over the same ball.

539
00:28:05,975 --> 00:28:09,365
The scales, logs, contention,
CPU under horizon.

540
00:28:09,865 --> 00:28:10,975
Now look at the right side.

541
00:28:11,275 --> 00:28:12,805
Each dock has its own ball.

542
00:28:13,015 --> 00:28:15,445
Each threat has its own
data, its own CPU core.

543
00:28:15,745 --> 00:28:18,415
No contention, no locks,
noon, just pure efficiency.

544
00:28:18,915 --> 00:28:21,105
And this model, as I said,
is called one third per car.

545
00:28:21,605 --> 00:28:25,745
Handles its own workload, its own memory,
its own responsibility, no context,

546
00:28:25,745 --> 00:28:27,815
region nor shared memory bottlenecks.

547
00:28:27,815 --> 00:28:30,365
Everything stays with the CP cache line.

548
00:28:30,905 --> 00:28:34,195
It's the radically fast and it's
perfectly fit for, for modern hardware.

549
00:28:34,695 --> 00:28:38,095
And, in conclusion, what to say that
there's no perfect base, only the

550
00:28:38,095 --> 00:28:41,605
right one for your specific needs
between consistently performance

551
00:28:41,605 --> 00:28:43,075
and scalability are intentional.

552
00:28:43,570 --> 00:28:47,050
The standing of this is what enables
us to design reliable efficiency

553
00:28:47,050 --> 00:28:48,760
in the well justified architecture.

554
00:28:49,260 --> 00:28:49,920
This's all for me.

555
00:28:49,920 --> 00:28:50,460
Thank you.

556
00:28:50,520 --> 00:28:52,650
If you have any questions,
I'll be happy to answer them.

557
00:28:53,190 --> 00:28:55,595
Feel free to reach out any
time, through any of my

558
00:28:55,595 --> 00:28:56,995
availability, available contacts.

559
00:28:57,625 --> 00:28:58,015
Bye-bye.

