1
00:00:01,440 --> 00:00:05,610
Hello everyone and welcome to
our session here at Con 42.

2
00:00:06,660 --> 00:00:12,480
Today we are diving into a crucial but
often overlooked aspect of working with

3
00:00:12,480 --> 00:00:15,450
large language models and AI systems.

4
00:00:16,050 --> 00:00:21,780
How do we evaluate the meaningfully
traditional metrics like Blue and Rouge?

5
00:00:22,170 --> 00:00:27,150
Were a good starting point, but
as AI systems grow more powerful

6
00:00:27,270 --> 00:00:29,490
and complex, we need smarter.

7
00:00:29,910 --> 00:00:33,990
More human-centric approaches to
truly measure their effectiveness.

8
00:00:34,710 --> 00:00:41,010
So in this talk, we'll explore what's
beyond blue and rules, how the evaluation

9
00:00:41,010 --> 00:00:46,260
landscape is evolving, and what modern
techniques are emerging to keep pace

10
00:00:46,350 --> 00:00:48,780
with rapid advancement in generative ai.

11
00:00:49,780 --> 00:00:53,785
Before we dive deeper into
our presentation, let me give.

12
00:00:54,370 --> 00:00:56,290
A quick introduction about ourselves.

13
00:00:56,800 --> 00:01:01,900
I'm aloen, currently leading engineering
at Dropbox, where I work on storage

14
00:01:01,900 --> 00:01:06,970
systems, scalable infrastructure,
and the AI ML platforms that powers

15
00:01:06,970 --> 00:01:08,410
some of our internal tooling.

16
00:01:09,070 --> 00:01:12,970
I completed my masters from Carnegie
Mellon University, and much of

17
00:01:13,000 --> 00:01:17,410
my work today involves balancing
performance, observability, and scale.

18
00:01:18,910 --> 00:01:21,760
And I'm, I work as a applied AI engineer.

19
00:01:22,300 --> 00:01:24,640
I build scalable machine
learning solutions which

20
00:01:24,640 --> 00:01:26,020
focus on real world impact.

21
00:01:26,350 --> 00:01:28,900
I specialize in working
in education technology.

22
00:01:29,380 --> 00:01:33,971
My current focus lies in distributed
machine learning and the world of

23
00:01:33,971 --> 00:01:37,030
agent ai, where I'm trying to create
systems that doesn't just respond to

24
00:01:37,030 --> 00:01:39,370
you, but it keeps reasoning and adapt.

25
00:01:39,820 --> 00:01:43,300
So together, me and Alo, we bring
a shared passion for making AI

26
00:01:43,300 --> 00:01:46,600
systems not just smarter, but more
accountable and more measurable.

27
00:01:47,600 --> 00:01:49,520
Let's begin with a simple scenario.

28
00:01:50,150 --> 00:01:53,810
Imagine you're a part of a
startup launching an AI powered

29
00:01:53,810 --> 00:01:55,310
customer service chat bot.

30
00:01:56,120 --> 00:01:57,470
Sounds familiar, right?

31
00:01:57,950 --> 00:02:03,890
It's a classic use case, automate support,
cut costs, and improved response times.

32
00:02:04,850 --> 00:02:08,450
Now think about what your customers
expect from this chat bot.

33
00:02:09,380 --> 00:02:11,600
It's not just about answering queries.

34
00:02:11,870 --> 00:02:16,580
They want answers that are
fast, relevant, and feel human.

35
00:02:17,390 --> 00:02:20,650
So how do you evaluate whether
your model is doing a good job?

36
00:02:21,820 --> 00:02:23,410
This is where the problem begins.

37
00:02:23,920 --> 00:02:28,300
We can't rely solely on
word overlap or rigid rules.

38
00:02:28,870 --> 00:02:32,110
We need to measure effectiveness
across three key dimensions.

39
00:02:32,725 --> 00:02:33,505
Accuracy.

40
00:02:34,135 --> 00:02:36,085
Is the information factually correct?

41
00:02:36,865 --> 00:02:42,475
Usability is the response phrased in a
way the user can understand and act on.

42
00:02:43,135 --> 00:02:45,055
And reliability does.

43
00:02:45,055 --> 00:02:49,045
The system performs consistently
across different user inputs,

44
00:02:49,225 --> 00:02:51,085
context, and each cases.

45
00:02:51,955 --> 00:02:56,185
This sets the foundation for why we
need to go beyond traditional metrics.

46
00:02:56,845 --> 00:02:59,665
It's no longer just about
language similarity.

47
00:03:00,010 --> 00:03:03,730
It's about actual
performance, trust and impact.

48
00:03:05,590 --> 00:03:10,000
Now that we have set the stage, let's
define what we are actually aiming to

49
00:03:10,000 --> 00:03:15,370
evaluate when it comes to AI systems,
especially large language models.

50
00:03:16,240 --> 00:03:20,080
Traditionally, we have been good
at measuring things like grammar

51
00:03:20,080 --> 00:03:21,675
or overlap with reference answers.

52
00:03:22,585 --> 00:03:27,985
But today's AI systems operate in
open-ended real world context, so we

53
00:03:27,985 --> 00:03:30,445
need a much richer set of objectives.

54
00:03:30,955 --> 00:03:32,305
Here is what we believe.

55
00:03:32,515 --> 00:03:38,335
Every modern evaluation framework
should aim to capture first accuracy.

56
00:03:38,875 --> 00:03:40,045
This one's obvious.

57
00:03:40,255 --> 00:03:42,085
The AI should get facts right.

58
00:03:42,625 --> 00:03:47,545
A model generating confident but wrong
information is dangerous, not useful.

59
00:03:48,295 --> 00:03:50,425
Second bias mitigation.

60
00:03:51,025 --> 00:03:57,055
We want systems that generate inclusive
and fair outputs, and that means actively

61
00:03:57,055 --> 00:04:02,245
identifying and minimizing harmful
stereotypes or skewed viewpoints.

62
00:04:02,785 --> 00:04:07,015
Third coherence, it's not enough
for an answer to be correct.

63
00:04:07,225 --> 00:04:10,455
It needs to make sense in
the flow of a conversation.

64
00:04:11,145 --> 00:04:14,775
This is Es especially critical
for applications like tutoring.

65
00:04:15,135 --> 00:04:17,055
Therapy or customer service.

66
00:04:17,745 --> 00:04:19,485
And finally, reliability.

67
00:04:19,905 --> 00:04:24,855
The AI should perform consistently
across different prompts, languages,

68
00:04:24,915 --> 00:04:27,615
domains, and even user personas.

69
00:04:28,155 --> 00:04:31,545
No surprises, no hallucinations,
no brittle behavior.

70
00:04:32,625 --> 00:04:38,985
These four accuracy, fairness, coherence,
and reliability together form a much

71
00:04:38,985 --> 00:04:43,635
more complete picture of what good
AI actually means in the real world.

72
00:04:44,205 --> 00:04:48,525
And me measuring these effectively
is where the challenge begins.

73
00:04:50,865 --> 00:04:55,275
Before we talk about what's next,
let's take a step back and quickly

74
00:04:55,275 --> 00:05:01,125
revisit what Blue and Rules actually
are and why they have been so dominant

75
00:05:01,155 --> 00:05:03,255
in NLP evaluation for so long.

76
00:05:03,975 --> 00:05:10,215
Blue or bilingual evaluation under study
is essentially a precision based score.

77
00:05:10,725 --> 00:05:15,915
It compares the ngrams or short
chunk of words in a model's output.

78
00:05:16,410 --> 00:05:18,210
To those in a reference sentence.

79
00:05:18,510 --> 00:05:23,050
It's been a staple in machine
translation tasks for years.

80
00:05:23,740 --> 00:05:27,100
Rules on the other hand,
is more recall oriented.

81
00:05:27,730 --> 00:05:32,170
It looks at how much of the reference
text is captured in the model's output.

82
00:05:32,650 --> 00:05:36,370
It's widely used for evaluating
summarization, where the goal

83
00:05:36,370 --> 00:05:40,240
is to see whether the generated
summary includes the important bits.

84
00:05:41,020 --> 00:05:44,230
Now, why were these
metrics adopted so widely?

85
00:05:45,040 --> 00:05:50,020
They're fast language agnostic and they
don't require any human label data.

86
00:05:50,530 --> 00:05:54,460
You can just run them over a
corpus and get numeric scores.

87
00:05:54,760 --> 00:05:56,530
It's clean and scalable.

88
00:05:57,580 --> 00:05:59,650
But, and here's the catch.

89
00:06:00,160 --> 00:06:03,820
Just because something is easy
to compute doesn't mean it truly

90
00:06:03,820 --> 00:06:05,950
captures what quality looks like.

91
00:06:06,460 --> 00:06:12,100
Blue and Rose don't understand meaning
intent tone or factual correctness.

92
00:06:12,430 --> 00:06:14,035
That's the real gap we need to fill.

93
00:06:16,450 --> 00:06:19,940
Let's bring this down to earth
with a real world example that

94
00:06:19,940 --> 00:06:24,410
highlights just how fragile these
tradition traditional metrics can be.

95
00:06:25,760 --> 00:06:29,990
Say we have built a customer support
chat bot for an e-commerce store,

96
00:06:30,380 --> 00:06:34,940
and the user asks a simple question,
what's the size of this jacket?

97
00:06:35,510 --> 00:06:39,920
The chat bot response correctly,
with just one word, 34.

98
00:06:40,910 --> 00:06:42,560
That's perfectly valid answer.

99
00:06:42,890 --> 00:06:48,530
It's concise, accurate, and exactly what
the user needs, but here is the catch.

100
00:06:49,190 --> 00:06:54,020
The blue score for this
response is just 0.016.

101
00:06:54,440 --> 00:06:57,350
That's extremely low and
completely misleading.

102
00:06:58,190 --> 00:06:58,670
Why?

103
00:06:59,240 --> 00:07:03,380
Because Blue is comparing Ngram
overlaps with reference sentences

104
00:07:03,380 --> 00:07:10,760
like it's XXL, or it's small, or it is
34, and since 34 doesn't share enough

105
00:07:10,820 --> 00:07:14,060
bigrams or trigrams with any of these.

106
00:07:14,495 --> 00:07:18,695
It gets penalized even though
it's semantically perfect.

107
00:07:19,595 --> 00:07:24,575
This is the kind of disconnect that
makes blue and ruse unreliable in real

108
00:07:24,575 --> 00:07:27,095
dialogue or open domain scenarios.

109
00:07:27,485 --> 00:07:31,445
They care more about surface level
similarity than actual meaning,

110
00:07:32,090 --> 00:07:36,305
and if we are not careful, we
end up punishing good responses.

111
00:07:36,755 --> 00:07:38,615
Just because they're phrased differently.

112
00:07:40,205 --> 00:07:44,315
Here is another example that really
drives home the limitation of these

113
00:07:44,315 --> 00:07:46,145
surface level metrics like Blue.

114
00:07:46,925 --> 00:07:52,745
Imagine we are evaluating a chat bot
designed to explain HR policies, in

115
00:07:52,745 --> 00:07:54,965
this case, remote work guidelines.

116
00:07:56,165 --> 00:07:59,585
The reference sentence says
employees are permitted to work

117
00:07:59,585 --> 00:08:03,935
remotely up to three days per
week, subject to manager approval.

118
00:08:04,970 --> 00:08:09,350
The model response with staff members
are allowed to telecommute for a

119
00:08:09,350 --> 00:08:13,730
maximum of three days, weekly pending
approval from their supervisor.

120
00:08:14,600 --> 00:08:15,560
Now, pause for a second.

121
00:08:16,610 --> 00:08:19,070
That response is perfectly accurate.

122
00:08:19,460 --> 00:08:25,160
It conveys the exact same meaning just
using slightly different phrasing, but

123
00:08:25,160 --> 00:08:27,830
play it gives this a score of zero.

124
00:08:28,550 --> 00:08:29,150
That's right.

125
00:08:29,270 --> 00:08:37,214
Zero y. Because Blue is looking for direct
overlaps, exact words and ngram matches.

126
00:08:37,604 --> 00:08:41,804
It doesn't understand that staff
members and employees mean the same

127
00:08:41,804 --> 00:08:46,064
thing, or that supervisor are just
managers with a different label.

128
00:08:46,724 --> 00:08:51,885
This is the core flaw blue
and rules semantic equivalence

129
00:08:52,155 --> 00:08:53,954
and conversation quality.

130
00:08:54,435 --> 00:08:58,574
And if we are going to build LMS
that actually communicate well,

131
00:08:58,964 --> 00:09:00,765
we need to evaluate methods.

132
00:09:01,094 --> 00:09:03,824
That reward, meaning not just matching.

133
00:09:06,464 --> 00:09:10,785
Let's take a moment to summarize what
we have seen so far about traditional

134
00:09:10,785 --> 00:09:12,584
metrics like blue and rules.

135
00:09:12,915 --> 00:09:16,814
First, both of these metrics are
built around n gram oral lab,

136
00:09:17,505 --> 00:09:21,344
essentially counting how many words
or phrases match between the morals,

137
00:09:21,344 --> 00:09:23,985
response and a reference answer.

138
00:09:24,990 --> 00:09:26,700
This works well with the task.

139
00:09:26,700 --> 00:09:30,270
Is rigid, like translating
a sentence, word for word or

140
00:09:30,270 --> 00:09:32,160
summarizing with fixed phrasing.

141
00:09:32,670 --> 00:09:37,650
And to be fair, blue and rules have
excelled in domains like machine

142
00:09:37,650 --> 00:09:39,630
translation and text summarization.

143
00:09:40,140 --> 00:09:43,650
They're efficient, widely
adopted, and they helped

144
00:09:43,650 --> 00:09:46,200
standardize benchmarks early on.

145
00:09:47,280 --> 00:09:48,510
But this is the key.

146
00:09:48,780 --> 00:09:52,470
They completely struggle when it
comes to contextual understanding

147
00:09:52,650 --> 00:09:53,850
or semantic variation.

148
00:09:54,780 --> 00:09:59,700
They can't tell if a response makes sense
in a conversation or if it's phrased

149
00:09:59,700 --> 00:10:01,890
differently, but means the same thing.

150
00:10:02,670 --> 00:10:08,345
In other words, blue and ruse rewards
surface similarity, not actual quality.

151
00:10:09,000 --> 00:10:13,200
And that's why we need to evolve
our evaluation toolkit as our

152
00:10:13,200 --> 00:10:15,090
models get more sophisticated.

153
00:10:17,865 --> 00:10:23,625
So what do we mean when the, when
we say Ngram overlap, let's quickly

154
00:10:23,625 --> 00:10:28,755
unpack that because it's the foundation
of both blue and rules, and ngram

155
00:10:28,814 --> 00:10:34,125
is simply a sequence of N words,
A one gram, or unigram just means

156
00:10:34,155 --> 00:10:37,875
individual words like cat runs or fast.

157
00:10:38,385 --> 00:10:43,454
A two gram or bi gram is a pair of
cons, security words like the cat or

158
00:10:43,454 --> 00:10:48,135
runs fast, and a three gram or tri
gram combines three words, for example,

159
00:10:48,194 --> 00:10:50,625
the black cat or runs very fast.

160
00:10:51,285 --> 00:10:56,589
The move, the more overlap you gen
your generated text has with these

161
00:10:56,620 --> 00:11:00,824
kind of sequences from a reference
sentence, the higher your blue

162
00:11:00,824 --> 00:11:02,925
or rose score are going to be.

163
00:11:03,645 --> 00:11:05,175
It's a way to measure fluency.

164
00:11:05,505 --> 00:11:10,785
And similarity without needing deep
understanding, just matching patterns.

165
00:11:11,985 --> 00:11:16,695
Now this diagram below shows how we
move from basic word representation

166
00:11:16,815 --> 00:11:21,525
to increasing contextual complexity
from individual overlapping

167
00:11:21,525 --> 00:11:23,835
words to longer shared sequences.

168
00:11:23,835 --> 00:11:24,615
But here is the limitation.

169
00:11:26,340 --> 00:11:31,530
As we have seen earlier, this structure
can completely mis meaning intent,

170
00:11:31,620 --> 00:11:35,520
or even factual accuracy if the
phrasing is just slightly different.

171
00:11:36,450 --> 00:11:41,850
So while gram overlap gives us something
measurable, it's not the whole picture.

172
00:11:44,790 --> 00:11:48,750
So now that we have seen what
Ngram overlap actually measures,

173
00:11:48,960 --> 00:11:52,740
let's talk about why it's not
enough for today's AI systems.

174
00:11:53,475 --> 00:11:56,415
Especially large language models.

175
00:11:56,895 --> 00:11:58,995
First, it's a surface level measure.

176
00:11:59,535 --> 00:12:01,935
It doesn't understand
what a sentence means.

177
00:12:02,055 --> 00:12:02,865
It just checks.

178
00:12:02,865 --> 00:12:06,944
If the word looks similar, this is
fine for basic translation task,

179
00:12:07,214 --> 00:12:11,235
but it breaks down when the language
gets more flexible or nuanced.

180
00:12:11,895 --> 00:12:14,025
Second, it's easy to game.

181
00:12:14,385 --> 00:12:18,135
You can bump up your blue score by
repeating parts of the reference answer.

182
00:12:18,720 --> 00:12:22,980
Or adding boilerplate phrases even
if your actual output is poor.

183
00:12:23,700 --> 00:12:28,800
Third, and this one's huge, it
ignores factuality and coherence.

184
00:12:29,040 --> 00:12:32,550
Your model might say something
romantically perfect, but factually

185
00:12:32,550 --> 00:12:34,950
wrong, and blue won't catch it.

186
00:12:35,220 --> 00:12:40,260
Or it might output something semantically
spot on, but still get a low score

187
00:12:40,290 --> 00:12:41,700
because the phrasing was different.

188
00:12:42,210 --> 00:12:48,000
And finally, these metrics often
penalize longer or more fluent responses.

189
00:12:48,390 --> 00:12:53,610
If your model rephrases an idea
eloquently or adds context, it can

190
00:12:53,610 --> 00:12:58,650
actually hurt the score because the
Ngram pattern don't match the reference

191
00:12:58,650 --> 00:13:05,280
exactly in short, blue and Ruse gives
us a quick answer, but not a deep one.

192
00:13:05,730 --> 00:13:09,810
And with modern lms, that's
just not good enough anymore.

193
00:13:12,150 --> 00:13:16,050
So now that we have the, we have seen
the limitations of traditional metrics,

194
00:13:16,500 --> 00:13:21,870
let's shift gears and look at what modern
evaluation frameworks actually focuses on.

195
00:13:22,590 --> 00:13:27,540
Instead of counting word overlaps,
these new methods try to assess what

196
00:13:27,540 --> 00:13:29,910
really matters in a model's response.

197
00:13:30,420 --> 00:13:34,020
Qualities that are more aligned
with how humans judge quality.

198
00:13:34,770 --> 00:13:38,820
The first and arguably most
important is factual accuracy.

199
00:13:39,780 --> 00:13:41,940
Is the information actually correct?

200
00:13:42,210 --> 00:13:45,960
Especially in domains like
healthcare, finance, or education?

201
00:13:46,710 --> 00:13:49,410
This isn't just nice
to have, it's critical.

202
00:13:50,370 --> 00:13:51,975
Then we have semantic coherence.

203
00:13:52,365 --> 00:13:53,910
Does the response flow well?

204
00:13:54,240 --> 00:13:57,180
Is it logically structured
and grammatically sound?

205
00:13:57,719 --> 00:13:59,969
Or does it feel like a random text bump?

206
00:14:00,989 --> 00:14:02,849
Next comes answer, relevance.

207
00:14:03,149 --> 00:14:07,829
Does the model stay on topic and directly
address the question being asked?

208
00:14:08,519 --> 00:14:11,999
This is especially important for
systems like chatbots, tutors,

209
00:14:12,060 --> 00:14:13,560
or customer service tools.

210
00:14:14,699 --> 00:14:16,560
We also evaluate context precision.

211
00:14:16,829 --> 00:14:20,189
Did the model respond with
the most accurate detail from

212
00:14:20,189 --> 00:14:21,449
the context it was given?

213
00:14:21,930 --> 00:14:22,919
And conversely.

214
00:14:23,384 --> 00:14:27,854
Context recall, did it include
all the key elements that were

215
00:14:27,854 --> 00:14:29,594
necessary to build a complete answer?

216
00:14:30,794 --> 00:14:36,614
Together these dimensions form a far more
comprehensive and human aligned way of

217
00:14:36,614 --> 00:14:42,134
judging model output, and they are setting
the foundation for NextGen AI benchmarks.

218
00:14:44,354 --> 00:14:48,494
Now let's talk about how we evaluate
the factual accuracy of responses

219
00:14:48,944 --> 00:14:50,624
using a method called fact score.

220
00:14:51,569 --> 00:14:56,640
The core idea here is to break down a
generated response into atomic facts.

221
00:14:57,270 --> 00:15:01,079
That is small, standalone
pieces of information that

222
00:15:01,079 --> 00:15:03,180
can be independently verified.

223
00:15:03,839 --> 00:15:07,949
Each atomic fact is then checked
against a reliable external knowledge

224
00:15:07,949 --> 00:15:10,050
source, like a trusted database.

225
00:15:10,364 --> 00:15:13,185
Verify document or reference corpus.

226
00:15:13,484 --> 00:15:18,374
For example, if the model says the Eiffel
Tower is in Paris and it was built in

227
00:15:18,374 --> 00:15:24,444
1889, that's two separate atomic facts,
and we check each of them individually.

228
00:15:25,314 --> 00:15:29,425
Once this checking is done, we compute
a score based on the proportion of

229
00:15:29,425 --> 00:15:34,134
facts that are supported by the source,
and that becomes the fact score.

230
00:15:34,899 --> 00:15:39,519
This approach provides a granular,
interpretable view of factual

231
00:15:39,699 --> 00:15:44,109
factuality instead of just
evaluating the sentence as a whole.

232
00:15:46,269 --> 00:15:50,510
Alright, let's take a closer
look at how factuality evaluation

233
00:15:50,540 --> 00:15:54,800
actually works in a practice,
especially with model grade systems.

234
00:15:55,655 --> 00:15:58,535
This approach typically
relies on three core inputs.

235
00:15:58,655 --> 00:16:04,025
The prompt, the output, and reference
answer, the first input is the prompt.

236
00:16:04,385 --> 00:16:06,095
That's what we send to the LLM.

237
00:16:06,485 --> 00:16:11,795
It could be a question, a task
instruction, or a real work query like

238
00:16:11,855 --> 00:16:13,325
what is the capital of Switzerland?

239
00:16:14,000 --> 00:16:16,010
The second input is the output.

240
00:16:16,550 --> 00:16:19,100
This is the model's
response to that prompt.

241
00:16:19,550 --> 00:16:23,780
For example, it might say Zurich
or Burn, depending on how it was

242
00:16:23,780 --> 00:16:25,610
trained or what it retrieved.

243
00:16:26,270 --> 00:16:28,490
The third component is the reference.

244
00:16:28,730 --> 00:16:32,420
This is the ideal response that a
model should be should have given,

245
00:16:32,420 --> 00:16:36,829
and this is typically crafted
by the author of the evaluation.

246
00:16:37,280 --> 00:16:40,189
Usually a human or another trusted system.

247
00:16:40,895 --> 00:16:44,195
The factuality check is then
done by comparing the model's

248
00:16:44,225 --> 00:16:45,755
output to the reference.

249
00:16:46,655 --> 00:16:49,265
Does the answer aligned
with the verified facts?

250
00:16:49,955 --> 00:16:54,215
Was the reasoning sound, did
it hallucinate or go off topic?

251
00:16:55,355 --> 00:16:58,205
This approach gives us
a more targeted lens.

252
00:16:58,265 --> 00:17:01,295
We are no longer scoring
for linguistic overlap.

253
00:17:01,594 --> 00:17:04,145
But for truthfulness and correctness.

254
00:17:06,185 --> 00:17:10,715
Now let's dive into a more advanced
model-based approach for evaluating

255
00:17:10,715 --> 00:17:16,235
factuality One that goes far beyond
keyword matching or basic overlap.

256
00:17:17,435 --> 00:17:22,174
What you are seeing here is a factuality
scoring pipeline that leverages

257
00:17:22,174 --> 00:17:27,605
semantic alignment, entailment
classification, and weighted aggregation

258
00:17:27,755 --> 00:17:29,614
to generate what's called a fact score.

259
00:17:30,545 --> 00:17:32,375
We begin with the input text.

260
00:17:32,795 --> 00:17:40,475
We use something like S-B-E-R-T-A sentence
Bert model to identify how well the output

261
00:17:40,475 --> 00:17:42,725
semantic aligns with the reference facts.

262
00:17:43,295 --> 00:17:46,775
The next step is to break the
reference answer into atomic facts.

263
00:17:47,389 --> 00:17:51,800
Labeled a one and then possibly
reframe or relocate them in

264
00:17:51,800 --> 00:17:53,960
different forms marked as a two.

265
00:17:55,159 --> 00:17:58,220
This ensures we are not
overly strict about phrasing.

266
00:17:58,880 --> 00:18:03,800
Now we introduce NLI classification,
natural language inference.

267
00:18:04,250 --> 00:18:08,720
This checks whether the models
output entails, contradicts, or is

268
00:18:08,720 --> 00:18:10,820
natural towards each atomic fact.

269
00:18:11,330 --> 00:18:13,760
This is where the green
and red boxes come in.

270
00:18:14,300 --> 00:18:17,090
Green for support, red for contradiction.

271
00:18:17,930 --> 00:18:22,040
Then comes the aggregation phase where
all of this gets pulled together.

272
00:18:22,550 --> 00:18:25,370
We apply different weights
to different fact types.

273
00:18:25,580 --> 00:18:30,920
For example, facts classified as
a gets a weight of 0.9, whereas

274
00:18:30,920 --> 00:18:35,000
contradictions like D two gets a
lower score or even a zero penalty.

275
00:18:36,500 --> 00:18:41,389
The final result is a fact score,
a weighted structured view of how

276
00:18:41,389 --> 00:18:44,360
factual the LM LMS output really is.

277
00:18:44,600 --> 00:18:49,700
It's nuanced, interpretable, and much
more robust than traditional method,

278
00:18:50,180 --> 00:18:55,370
and most importantly, this method adapts
well across domains and languages.

279
00:18:55,880 --> 00:19:01,070
It evaluates not just what the model says,
but how well it aligns with the truth.

280
00:19:03,860 --> 00:19:08,239
Let's now talk about a modern metric
that actually goes beyond surface level.

281
00:19:08,239 --> 00:19:10,790
Word overlaps birth score.

282
00:19:11,480 --> 00:19:14,300
Birth score is built on top
of contextual embeddings.

283
00:19:14,810 --> 00:19:20,060
It uses models like Bert to assign
vector implantation to each token,

284
00:19:20,060 --> 00:19:24,620
not just based on the word itself,
but how it is used in context.

285
00:19:24,920 --> 00:19:25,400
That's key.

286
00:19:26,090 --> 00:19:28,730
In the example below,
we have two sentences.

287
00:19:28,850 --> 00:19:31,489
The reference says, the
weather is cold today.

288
00:19:31,759 --> 00:19:33,860
The candidate says it's freezing today.

289
00:19:34,310 --> 00:19:38,750
Now, even though there is little
direct overlap, a human would agree.

290
00:19:39,259 --> 00:19:41,330
These two say almost the same thing.

291
00:19:42,110 --> 00:19:46,040
So instead of relying on ngrams,
we generate contextual embeddings

292
00:19:46,250 --> 00:19:47,810
for each word using bird.

293
00:19:48,380 --> 00:19:52,639
These embeddings capture the meaning
of each token in its context.

294
00:19:53,195 --> 00:19:58,295
Then we calculate Pairwise co-sign
similarity between tokens across the

295
00:19:58,295 --> 00:20:00,305
reference and the candidate sentence.

296
00:20:00,755 --> 00:20:05,045
From these, we select the best matching
pairs, aggregate their similarity

297
00:20:05,045 --> 00:20:07,475
scores, and compute a final bird score.

298
00:20:08,225 --> 00:20:12,395
This gives us a much richer sense of
how well the semantic flow is preserved.

299
00:20:12,995 --> 00:20:14,195
Regardless of wordings.

300
00:20:15,065 --> 00:20:20,405
B score excels at evaluating coherence,
paraphrasing, and even subtle rephrasing.

301
00:20:21,080 --> 00:20:23,960
Areas where blue and
rules simply fall short.

302
00:20:24,469 --> 00:20:28,730
In other words, B score helps us
evaluate what the model means,

303
00:20:28,790 --> 00:20:30,949
not just what word it uses.

304
00:20:32,989 --> 00:20:37,190
One of the most critical areas
in AI evaluation today is

305
00:20:37,190 --> 00:20:39,799
addressing toxicity and bias.

306
00:20:40,040 --> 00:20:40,489
Why?

307
00:20:41,120 --> 00:20:45,739
Because even a single harmful
response from an LLM can damage trust.

308
00:20:46,114 --> 00:20:49,755
Safety and inclusivity
in the user experience.

309
00:20:50,385 --> 00:20:54,615
Bias can creep in from training
data, prompt phrasing, or even subtle

310
00:20:54,615 --> 00:21:00,495
model behavior, and without careful
evaluation, it often goes undetected.

311
00:21:01,095 --> 00:21:06,555
To help tackle this, we have tools
like Lang Bite, which stands for

312
00:21:06,555 --> 00:21:08,925
language bias testing environment.

313
00:21:09,585 --> 00:21:12,765
Lang Byte works in a
systematic and scalable way.

314
00:21:13,065 --> 00:21:18,405
First, it selects prompt templates
from a predefined library, for example,

315
00:21:18,765 --> 00:21:20,925
prompts about race, gender, or religion.

316
00:21:21,375 --> 00:21:24,405
Then it generates test cases
based on these templates.

317
00:21:25,035 --> 00:21:29,565
These are structured prompts designed
to pro for biased or toxic behavior.

318
00:21:30,405 --> 00:21:35,205
Next, it executes those prompts
across different LMS and carefully

319
00:21:35,205 --> 00:21:37,125
analyzes the response Finally.

320
00:21:38,340 --> 00:21:43,290
It generates insights highlighting areas
where the model may have shown bias,

321
00:21:43,350 --> 00:21:46,230
tendencies, either overt or subtle.

322
00:21:46,890 --> 00:21:51,420
This lets developers pinpoint
specific weakness and re and retrain

323
00:21:51,420 --> 00:21:55,470
or fine tune models to be more
inclusive, respectful, and safe.

324
00:21:56,460 --> 00:21:59,220
Bias and toxicity aren't just bugs.

325
00:21:59,580 --> 00:22:00,810
They're ethical risks.

326
00:22:01,185 --> 00:22:04,605
And tools like Lang Byte help
us bring transparency and

327
00:22:04,605 --> 00:22:06,345
accountability to the space.

328
00:22:07,305 --> 00:22:10,425
Let's now look at how this
evaluation process works.

329
00:22:10,425 --> 00:22:15,435
In practice using Lang Byte structured
pipeline, the flow starts with

330
00:22:15,435 --> 00:22:17,085
understanding ethical concerns.

331
00:22:18,045 --> 00:22:22,470
I. This includes input from sensitive
communities and domain experts, which

332
00:22:22,470 --> 00:22:26,550
help define what types of harmful or
biased behavior we need to test for.

333
00:22:27,510 --> 00:22:32,040
This leads to the ethical requirement
specification, which creates a formal

334
00:22:32,040 --> 00:22:38,065
model of what needs to be avoided, such as
hate, speech, stereotyping, or toxicity.

335
00:22:39,000 --> 00:22:42,180
Next, we move to test generation here.

336
00:22:42,450 --> 00:22:45,900
We use prompt templates designed
to target those ethical concerns.

337
00:22:46,665 --> 00:22:52,305
From these templates, we generate real
prompt instances, specific test cases.

338
00:22:53,145 --> 00:22:55,725
These prompts are then fed into the L lms.

339
00:22:55,815 --> 00:23:01,065
In the test execution phase, the
model responses are collected, and

340
00:23:01,065 --> 00:23:05,465
these outputs we want to evaluate
for signs of toxicity or bias.

341
00:23:06,185 --> 00:23:07,775
The final step is reporting.

342
00:23:08,240 --> 00:23:13,730
Here the responses are analyzed using
prompt articles or even an evaluator LLM.

343
00:23:14,420 --> 00:23:19,160
The goal is to interpret the outputs and
produce structured evaluation reports.

344
00:23:19,430 --> 00:23:24,650
This end-to-end flow ensures that bias
detection is not just an afterthought.

345
00:23:24,950 --> 00:23:29,870
It's built into the system from the ground
up, guided by real ethical priorities.

346
00:23:31,970 --> 00:23:36,800
Despite advances in automated metrics,
human judgment remains irreplaceable.

347
00:23:37,220 --> 00:23:41,570
Especially in evaluating tone,
coherence, and helpfulness, which

348
00:23:41,570 --> 00:23:43,460
current metrics can't always capture.

349
00:23:44,000 --> 00:23:49,190
That's why we use Pairwise comparison,
where two outputs are shown side by side,

350
00:23:49,250 --> 00:23:52,550
and human judges select the better one.

351
00:23:53,330 --> 00:23:56,720
This is more reliable than
assigning absolute numeric scores.

352
00:23:57,200 --> 00:24:01,970
We then apply probabilistic ranking
models like Bradley Terry, or elo.

353
00:24:02,585 --> 00:24:05,465
To convert these comparisons
into meaningful rankings.

354
00:24:06,035 --> 00:24:11,015
These methods are widely used in platforms
like Chat Botina, and they consistently

355
00:24:11,015 --> 00:24:15,605
reveal gaps that automated metrics
like Blue and Ruse fail to detect.

356
00:24:15,935 --> 00:24:19,925
In fact, two outputs might score
equally in blue, but humans may

357
00:24:19,925 --> 00:24:23,885
strongly prefer one over the other
due to clarity, style, or tone.

358
00:24:24,545 --> 00:24:26,135
So human-centric evaluation.

359
00:24:26,600 --> 00:24:30,350
Does not just validate models,
it helps us uncover what really

360
00:24:30,350 --> 00:24:32,510
matters in user experience.

361
00:24:35,390 --> 00:24:36,770
Let's bring it all together.

362
00:24:37,460 --> 00:24:42,770
We want, we start with fact verification,
breaking model response into atomic facts.

363
00:24:43,205 --> 00:24:47,795
Then verifying each one against trusted
sources to generate an accuracy score.

364
00:24:48,455 --> 00:24:52,205
In parallel, we conduct pairwise
comparison, A versus B, C

365
00:24:52,205 --> 00:24:56,945
versus A, where humans simply
judge which response is better.

366
00:24:57,665 --> 00:25:02,075
Using models like Bradley Terry,
these judgments are aggregated

367
00:25:02,075 --> 00:25:06,215
into a global ranking offering a
probabilistic view of model quality.

368
00:25:06,905 --> 00:25:10,895
We then compared this human ranking
against scores from automated metric.

369
00:25:11,270 --> 00:25:15,350
To identify where our
evaluation system align and more

370
00:25:15,350 --> 00:25:17,510
importantly, where they don't.

371
00:25:18,170 --> 00:25:23,060
Human evaluation are increasingly
taking central stage in measuring

372
00:25:23,090 --> 00:25:28,040
conversational AI quality, not just
contemplating, but also correcting

373
00:25:28,220 --> 00:25:29,600
what automated metrics miss.

374
00:25:30,665 --> 00:25:34,610
With the, with that foundation
lay laid, I'll now hand it over

375
00:25:34,610 --> 00:25:38,360
to Soro who will take us deeper
into how modern metric systems.

376
00:25:38,735 --> 00:25:43,415
Like gal and real world task
evaluations are changing the landscape.

377
00:25:44,885 --> 00:25:48,365
Alright, let's talk about one
of the most promising shifts in

378
00:25:48,365 --> 00:25:50,645
evaluation using the l and m itself.

379
00:25:50,765 --> 00:25:55,480
As a judge, large language models bring in
three very powerful strengths as a result.

380
00:25:56,015 --> 00:25:57,455
The first is context.

381
00:25:57,455 --> 00:26:02,045
Awareness means they can interpret the
nuanced meaning and adapt to the domain.

382
00:26:02,615 --> 00:26:06,455
Scalability, since they can evaluate
massive datasets quickly and very

383
00:26:06,455 --> 00:26:10,595
efficiently, and consistency,
eliminating the subjectivity and the

384
00:26:10,595 --> 00:26:12,965
fatigue human evaluators often bring.

385
00:26:13,205 --> 00:26:14,585
So here is how it works.

386
00:26:14,885 --> 00:26:16,925
A benchmark dataset provides both input.

387
00:26:17,195 --> 00:26:18,845
Prompts and the correct outputs.

388
00:26:19,265 --> 00:26:22,145
The L lms, indeed as suggested
output, and then using a judge

389
00:26:22,145 --> 00:26:25,805
prompt, we ask another LLM, given
the input and the correct output.

390
00:26:25,985 --> 00:26:30,155
Is this the suggested output acceptable
to make sure the judge stays reliable?

391
00:26:30,245 --> 00:26:34,175
We loop in human experts to
assess the S judgment and refine

392
00:26:34,175 --> 00:26:35,345
the judge through feedback.

393
00:26:36,035 --> 00:26:39,605
This feedback loop from LLM
response to LM Judgment to export

394
00:26:39,605 --> 00:26:43,445
audit, which help us continually
improve and the evaluator itself.

395
00:26:44,445 --> 00:26:47,445
So to wrap up the discussion
on evaluation, let's talk

396
00:26:47,445 --> 00:26:49,005
about why automated evaluation.

397
00:26:49,200 --> 00:26:51,480
It is not just helpful,
it's very essential.

398
00:26:52,020 --> 00:26:56,520
First, it ensures we maintain consistent
quality across multiple model versions.

399
00:26:56,850 --> 00:27:00,270
Whether we are fine tuning a base
model or it trading on prompts, we need

400
00:27:00,270 --> 00:27:02,370
stability in how we assess outputs.

401
00:27:02,850 --> 00:27:06,240
Second, it provides an objective
and reproducible performance signal,

402
00:27:06,450 --> 00:27:09,690
removing the human bias and the
ambiguity from the evaluation loop.

403
00:27:10,340 --> 00:27:12,770
Third, it enables rapid testing at scale.

404
00:27:13,280 --> 00:27:15,650
Something manual methods
simply cannot match.

405
00:27:15,950 --> 00:27:19,610
And finally, it supports continuous
improvement cycles, which are foundation

406
00:27:19,760 --> 00:27:22,130
for the modern ML ops workflows.

407
00:27:24,255 --> 00:27:27,495
So now that we have seen why
automated evaluation matters, let's

408
00:27:27,495 --> 00:27:30,765
also to take a look at the key
technologies powering the shift.

409
00:27:31,335 --> 00:27:34,785
And whenever we talk about lms, the
first thing that comes to our mind is

410
00:27:34,785 --> 00:27:37,425
open ai, the Open AI Evolve framework.

411
00:27:37,515 --> 00:27:40,995
This is a standardized way to
define and run evaluation on

412
00:27:40,995 --> 00:27:42,285
large language model outputs.

413
00:27:42,735 --> 00:27:45,945
It's very extensible, modular, and
integrates really well with other

414
00:27:45,945 --> 00:27:48,015
components of the open AI ecosystem.

415
00:27:48,675 --> 00:27:50,865
Then we have GE or generative evaluation.

416
00:27:51,165 --> 00:27:53,145
This goes beyond fixing test cases.

417
00:27:53,505 --> 00:27:56,475
It leverages generative prompts
to dynamically evaluate model

418
00:27:56,535 --> 00:27:58,635
performance across a variety of tasks.

419
00:27:59,175 --> 00:28:02,175
It is especially useful for
edge cases or nuance reasoning.

420
00:28:02,835 --> 00:28:05,265
And then we have REG
evaluation frameworks.

421
00:28:05,595 --> 00:28:08,595
These are specifically tailored
for retrieval augmented generation.

422
00:28:09,315 --> 00:28:13,500
They're not just assessing the output, but
how well the model grounds its answers.

423
00:28:14,010 --> 00:28:15,480
And retrieving context.

424
00:28:15,870 --> 00:28:18,870
So it's about both the what
and the why behind the answer.

425
00:28:19,290 --> 00:28:23,490
And finally, we rely heavily on
evaluation data sets or eval sets.

426
00:28:23,880 --> 00:28:27,120
These include curated prompts,
gold standard outputs, and even

427
00:28:27,120 --> 00:28:30,720
adversarial examples to rigorously
stress test the moral behavior.

428
00:28:31,260 --> 00:28:34,530
Together, these tools form the backbone
of how we benchmark an element scale.

429
00:28:35,530 --> 00:28:39,430
So let's dive a bit deeper and see
how this prominent tool in this space,

430
00:28:39,490 --> 00:28:41,170
the open AI valve framework works.

431
00:28:41,500 --> 00:28:45,280
So this framework allows us to, simplify
all of these things, like it helps us

432
00:28:45,280 --> 00:28:47,470
to build custom evaluation pipelines.

433
00:28:47,830 --> 00:28:51,070
It lets us summarize, code
generation reasoning and dialogue.

434
00:28:51,460 --> 00:28:55,540
But one of the most important strength
and standardizing test methodology is

435
00:28:55,810 --> 00:28:59,050
that you can benchmark across models
and iterations using consistent.

436
00:28:59,829 --> 00:29:02,139
Metrics and it helps
ensure reproducibility.

437
00:29:03,069 --> 00:29:05,889
It's also very natively integrated
into the opening dashboard.

438
00:29:06,159 --> 00:29:09,699
It makes it very seamless to set
up evaluation, view the results,

439
00:29:09,819 --> 00:29:11,110
and monitor change over time.

440
00:29:11,439 --> 00:29:15,219
So if you can see the screenshot below,
you can see you can choose from a

441
00:29:15,219 --> 00:29:18,879
multiple data sources importing chart
completion, uploading a recent NEL

442
00:29:18,879 --> 00:29:22,314
file, creating prompts manually, or
even building custom evaluation logic.

443
00:29:22,899 --> 00:29:26,979
So this flexibility allows team to quickly
spin up robust evaluation workflows

444
00:29:27,249 --> 00:29:28,569
without even reinventing the wheel.

445
00:29:30,865 --> 00:29:34,225
So one of the most powerful aspect
of the open AI valve framework is the

446
00:29:34,225 --> 00:29:36,024
flexibility in defining the criteria.

447
00:29:36,475 --> 00:29:38,365
So you aren't limited to just accuracy.

448
00:29:38,365 --> 00:29:40,915
You can choose what matters
for your application the most.

449
00:29:41,215 --> 00:29:42,985
Let's walk through some of
this buildin options, right?

450
00:29:43,315 --> 00:29:46,435
So you have factuality and semantic
similarity, which s assesses whether

451
00:29:46,435 --> 00:29:49,975
the response is both factually
correct and align in meaning

452
00:29:49,975 --> 00:29:51,195
with the res reference, answer.

453
00:29:51,699 --> 00:29:55,239
For task that requires more of emotional
intelligence, the sentiment evaluation

454
00:29:55,239 --> 00:29:57,759
can flag toxic or very often responses.

455
00:29:58,120 --> 00:30:01,840
Now, let's say you need structural
validation, so we can use schema

456
00:30:01,840 --> 00:30:05,354
matching or check with the output is
a valid, decent format or XML format.

457
00:30:05,799 --> 00:30:07,299
It's basically function calling.

458
00:30:08,049 --> 00:30:11,229
So string check and criteria
match lets you test the presence

459
00:30:11,229 --> 00:30:13,870
or absence of specific tokens
or features you can afford.

460
00:30:13,870 --> 00:30:15,549
Forbidden freezes or required formats.

461
00:30:16,330 --> 00:30:19,269
And if you're building custom metrics,
you can define your own logic.

462
00:30:20,939 --> 00:30:22,289
Using the custom prompt option.

463
00:30:22,339 --> 00:30:26,329
And finally, if you have the need for the
classic, NLP evaluation, there is always

464
00:30:26,329 --> 00:30:30,099
this text qualities that supports blue
and rouge scores or something as simple

465
00:30:30,099 --> 00:30:31,689
as score sign, similarity based scoring.

466
00:30:32,199 --> 00:30:35,499
So all of these F frame, all of these
tools included in the open a L framework.

467
00:30:35,739 --> 00:30:37,620
And that is something that
we, highly recommend use it.

468
00:30:39,354 --> 00:30:43,049
So now let's take it, how easy
was it actually for us to run an

469
00:30:43,049 --> 00:30:46,319
evaluation test with the open air
framework using this one line code?

470
00:30:46,350 --> 00:30:51,749
Okay, in this scenario, we were using
OI EAL combined, which uses GPD 3.5

471
00:30:51,749 --> 00:30:56,369
turbo on the spider SQL benchmarks,
and we took a, maximum of 25 samples.

472
00:30:56,939 --> 00:31:01,349
The framework sends each prompt to the EPI
and evaluates the responses in real time.

473
00:31:01,799 --> 00:31:05,039
And as you can see in the log it
logs every single TTP requires and

474
00:31:05,069 --> 00:31:06,599
tracks a progress sample with sample.

475
00:31:07,259 --> 00:31:10,709
Once all the samples are processed,
pick it a summary of report, in

476
00:31:10,709 --> 00:31:14,289
this case, 20, 20 as responses were
correct and five were incorrect.

477
00:31:14,849 --> 00:31:16,919
A s giving us a score of 80% accuracy.

478
00:31:17,519 --> 00:31:20,549
This output is in saved to these
in a log, which makes it very easy

479
00:31:20,549 --> 00:31:23,789
to review, compare across, model,
or visualize performance later.

480
00:31:24,149 --> 00:31:28,860
And the best part about this, it's
reproducible, testable, and scalable,

481
00:31:29,159 --> 00:31:30,570
all from a single CLA command.

482
00:31:33,030 --> 00:31:36,510
So let's work through a real evaluation
sample from the SQL benchmark.

483
00:31:36,600 --> 00:31:41,159
Yeah, the prompt of the question here
was how many country has a republic

484
00:31:41,189 --> 00:31:42,360
as their former for government?

485
00:31:42,689 --> 00:31:46,570
So the expected SL query is shown
green, select star account from

486
00:31:46,570 --> 00:31:49,810
country where the country is a firm of
government, which is equal to republic.

487
00:31:50,199 --> 00:31:51,909
So a precise match using equality.

488
00:31:51,939 --> 00:31:55,060
The model submission is in the orange,
which use a slightly broad, broader

489
00:31:55,060 --> 00:31:59,169
pattern like, like republic, which
also matches values like a federal

490
00:31:59,169 --> 00:32:02,620
republic system or a Republican
system, which actually introduces

491
00:32:02,620 --> 00:32:03,790
some ambiguity in the result.

492
00:32:04,209 --> 00:32:07,449
So while the model might technically
be correct, in many cases, it's

493
00:32:07,449 --> 00:32:11,080
not aligned with a strict expected
output, and this will be flagged

494
00:32:11,080 --> 00:32:12,550
as incorrect by the eval framework.

495
00:32:13,170 --> 00:32:16,290
The small variations help us
understand not whether just the

496
00:32:16,290 --> 00:32:20,040
model is right, but how closely
it aligns with the exact semantic

497
00:32:20,040 --> 00:32:23,760
and precision, which is especially
critical in structured tasks like SQL.

498
00:32:24,430 --> 00:32:28,149
So below you can see the
next test case queued up.

499
00:32:28,180 --> 00:32:30,700
This happens automatically and
repeatedly over the whole data set.

500
00:32:31,000 --> 00:32:33,940
So this is how we stress test
the model with granular feedback,

501
00:32:33,940 --> 00:32:37,350
especially in domain, something,
as complex, a structured query.

502
00:32:38,819 --> 00:32:40,469
So now we can look.

503
00:32:40,469 --> 00:32:43,824
Now let's take a look at GE value
short for generative evaluation.

504
00:32:43,924 --> 00:32:48,839
It is used as an LLM that
evaluates another LL M'S output.

505
00:32:48,899 --> 00:32:51,059
So GAL has very simple three steps.

506
00:32:51,149 --> 00:32:52,649
First, we define a task.

507
00:32:53,009 --> 00:32:54,029
The evaluation criteria.

508
00:32:54,269 --> 00:32:57,269
For example, you might ask the
model to judge a answer based on

509
00:32:57,269 --> 00:32:58,529
the clarity or the correctness.

510
00:32:59,039 --> 00:33:02,820
Then the models reads its own evaluation
steps using chain of thought reasoning.

511
00:33:03,239 --> 00:33:06,209
This is what gives GE value,
structure and explainability.

512
00:33:06,689 --> 00:33:11,070
Next, it'll use these steps to analyze the
output and produce a detailed judgment.

513
00:33:11,429 --> 00:33:14,159
And finally, it applies a
structured scoring function.

514
00:33:14,219 --> 00:33:19,649
Usually something that like structure
score from the violation text and what.

515
00:33:20,069 --> 00:33:24,090
What is a powerful part here is that
LLM becomes both the evaluator and the

516
00:33:24,090 --> 00:33:27,989
analyst and is very rich, interpretable
feedback, not just a random number.

517
00:33:30,225 --> 00:33:33,495
So let's take a look at how the
GE vial actually applies score

518
00:33:33,495 --> 00:33:34,814
and criteria using assertion.

519
00:33:35,144 --> 00:33:39,044
So each of these assertion define a
specific example and expectation that is

520
00:33:39,044 --> 00:33:40,665
set for the large language model output.

521
00:33:40,995 --> 00:33:44,504
So in the first one, we are going to
use a model graded rubric to ensure

522
00:33:44,504 --> 00:33:45,974
the response is not apologetic.

523
00:33:46,034 --> 00:33:46,664
Apologetic.

524
00:33:47,274 --> 00:33:50,125
This could be used in scenarios
where you want the model to answer

525
00:33:50,125 --> 00:33:53,890
confidently, like in technical support
scenarios or policy decision scenario.

526
00:33:53,965 --> 00:33:54,055
Use.

527
00:33:54,745 --> 00:33:57,325
And in the second use case,
we use a factuality check.

528
00:33:57,485 --> 00:34:01,445
Verify whether the model output aligns
with the specific known fact, like

529
00:34:01,445 --> 00:34:03,245
Sacramento is the capital of California.

530
00:34:03,785 --> 00:34:07,685
These assertion provide more
modular composable test that can

531
00:34:07,685 --> 00:34:09,395
be reused across prompts and tasks.

532
00:34:09,845 --> 00:34:12,215
And this is what gives
GE valves flexibility.

533
00:34:12,635 --> 00:34:15,784
So it's, think of it as a unit
test, but for large language models.

534
00:34:17,720 --> 00:34:21,350
So here is a practical use case
where ge y helps enforce behavior

535
00:34:21,350 --> 00:34:23,000
constraints across customer meetings.

536
00:34:23,510 --> 00:34:25,850
We have taken a common support prompt.

537
00:34:26,300 --> 00:34:29,870
And from audit tracking to product
recommendation and it attach a very

538
00:34:29,870 --> 00:34:34,820
simple grading rule, do not mention that
you are an AI chat bot or AI assistant.

539
00:34:35,390 --> 00:34:37,220
This is very useful in brand settings.

540
00:34:37,220 --> 00:34:40,880
So Anthrop izing the AI in my
conflict design choices, tone

541
00:34:40,880 --> 00:34:42,680
guidelines, or, compliance policies.

542
00:34:42,950 --> 00:34:46,850
So instead of managing each output, we
can automate these tests and get instant

543
00:34:46,850 --> 00:34:48,740
evaluation at scale using GE value.

544
00:34:49,370 --> 00:34:52,880
This will ensure brand alliance
responses without needing a human

545
00:34:52,880 --> 00:34:54,050
in the loop every single time.

546
00:34:56,105 --> 00:34:59,525
So now let's look at the, some of the
same task prompt evaluated on the GE

547
00:34:59,525 --> 00:35:02,735
file and can show pass, fail outcome
depending on the wording and the tone.

548
00:35:03,215 --> 00:35:04,535
In the left side column, we see AI.

549
00:35:05,780 --> 00:35:09,410
Responding correctly in terms of
information, but feeling due to which

550
00:35:09,650 --> 00:35:14,810
phrases like as a AI language model or
a e-commerce assistant, these ed, the

551
00:35:14,810 --> 00:35:16,310
grading constraint that we saw earlier.

552
00:35:16,520 --> 00:35:19,910
In contrast, the right side column
use a very different persona prompt,

553
00:35:20,210 --> 00:35:23,600
a smart, bubbly customer service
rep, and gives answers that are

554
00:35:23,600 --> 00:35:25,640
contextually aligned and brand safe.

555
00:35:25,700 --> 00:35:26,925
Thus PAs the evaluation.

556
00:35:27,490 --> 00:35:31,480
So this shows how geal is adjusted,
judging accuracy, but also behavioral

557
00:35:31,480 --> 00:35:34,510
consistency and how even minor
prompt engineering choices can

558
00:35:34,510 --> 00:35:36,400
flip a result from fail to a pass.

559
00:35:36,910 --> 00:35:40,330
So it's a very important reminder for
us that evaluation is not just what is

560
00:35:40,330 --> 00:35:42,250
being said, but the way it is being said.

561
00:35:44,525 --> 00:35:48,425
So now let's shift gears and look at
how evaluation retrieval argumented

562
00:35:48,425 --> 00:35:50,405
generation or REG systems work.

563
00:35:50,765 --> 00:35:53,225
So in A REG, which is a
retrieval argumented generation

564
00:35:53,225 --> 00:35:54,605
pipeline, has two components.

565
00:35:54,905 --> 00:35:57,785
A retrieval, which factors relevant
context from a knowledge base and

566
00:35:57,785 --> 00:36:01,355
a generator, which formulates the
final response using that context.

567
00:36:01,835 --> 00:36:05,855
So in evaluating REG, it requires
looking at both the stages separately.

568
00:36:06,245 --> 00:36:09,065
For the retrieval, we use
metrics like contextual recall.

569
00:36:09,125 --> 00:36:12,875
They data retrieve all the relevant
documents and contextual precision,

570
00:36:13,115 --> 00:36:16,205
which means whether retrieve document
actually useful to us or not.

571
00:36:16,895 --> 00:36:19,625
And then we look at generators,
which is metric as answer,

572
00:36:19,625 --> 00:36:21,185
relevant as the response.

573
00:36:21,185 --> 00:36:23,495
Address the user questions
and faithfulness.

574
00:36:23,830 --> 00:36:28,000
Checks if the output actually sticks to
the fact in the retrieved context, this

575
00:36:28,000 --> 00:36:31,570
two part evaluation is very essential
because a great generator can still

576
00:36:31,570 --> 00:36:35,350
hallucinate the retriever fields, and
a perfect retriever would not help if

577
00:36:35,350 --> 00:36:37,420
the generation steps misinterprets it.

578
00:36:39,695 --> 00:36:41,675
So now let me understand
the re architecture.

579
00:36:41,675 --> 00:36:45,215
Let's quickly zoom in and see how we
actually evaluate the retriever component.

580
00:36:45,265 --> 00:36:47,995
We use three very important metrics
here, the contextual position.

581
00:36:48,385 --> 00:36:51,355
This tells us whether
retriever is ranked relevant.

582
00:36:51,485 --> 00:36:54,845
Is ranking relevant information
higher than in relevant ones, which

583
00:36:54,845 --> 00:36:57,935
actually means there's a higher score,
would prioritize the right context.

584
00:36:58,115 --> 00:37:04,235
So as you can see, G PT 3.5 performed
best with the 92.23% using very basic IG.

585
00:37:04,805 --> 00:37:07,295
While multi queries with
GPD, it struggles a bit.

586
00:37:07,775 --> 00:37:10,865
So showing how query expansion doesn't
always help if not tuned properly.

587
00:37:11,435 --> 00:37:13,505
Next very important part
is contextual recall.

588
00:37:13,775 --> 00:37:15,605
This will check how much
relevant information your

589
00:37:15,605 --> 00:37:16,925
retriever can actually fetch.

590
00:37:17,465 --> 00:37:21,975
Think of it as coverage, alarm to
scores, an impress of 90% with basic IEG

591
00:37:22,125 --> 00:37:23,535
while again, multi query falls short.

592
00:37:24,435 --> 00:37:27,675
A very interesting pattern that
we just saw here is RD Fusion

593
00:37:27,705 --> 00:37:29,835
with GP PT four, or GPT 3 1 5.

594
00:37:29,835 --> 00:37:34,425
In this case also hits 90%, which shows
how fusion based approach improves

595
00:37:34,425 --> 00:37:35,895
recall through multi retrieval.

596
00:37:36,675 --> 00:37:38,295
And then we have contextual relevancy.

597
00:37:38,715 --> 00:37:42,885
So this combines both of precision and
recall, but it adds a layer of nuance.

598
00:37:42,945 --> 00:37:46,275
I was retrieving chunks that are both
relevant and not bloated with noise.

599
00:37:46,575 --> 00:37:51,855
So RAG fusion with LAMA two leads here
with 83.46% follow close following

600
00:37:51,855 --> 00:37:55,065
very closely, G PT 3.5 is at 87.22.

601
00:37:55,305 --> 00:37:58,245
The Delta fusion approach helps
balance precis precision and recall

602
00:37:58,545 --> 00:38:00,075
while filtering irrelevant text.

603
00:38:00,555 --> 00:38:03,825
So the very important key takeaway
is no single metric is enough.

604
00:38:04,075 --> 00:38:07,255
You need to monitor all three,
especially when choosing between basic

605
00:38:07,255 --> 00:38:08,845
multi query and the fusion strategies.

606
00:38:09,145 --> 00:38:11,035
So while selecting an
LLM, it's very important.

607
00:38:11,035 --> 00:38:14,185
You need to fig figure out these
steps as well, like backends, which

608
00:38:14,185 --> 00:38:16,860
have elements like GP 3.5 or LA two.

609
00:38:19,785 --> 00:38:23,295
So let's take a closer look at how
different evaluation metrics aligns

610
00:38:23,295 --> 00:38:27,315
with human judgment across four very
key important points, coherence,

611
00:38:27,315 --> 00:38:28,995
consistency, fluency, and relevance.

612
00:38:29,355 --> 00:38:33,465
So basic what basically what it is seeing
is, bench benchmarking, compiles and

613
00:38:33,465 --> 00:38:38,131
using two very important correlation
statistics, spear zero and NDL tower,

614
00:38:38,805 --> 00:38:43,095
both of which relevant is relevant for
us to measure how well a metrics scores.

615
00:38:43,430 --> 00:38:44,690
Agree with human ratings.

616
00:38:45,350 --> 00:38:47,270
We'll start of course, with
something very traditional,

617
00:38:47,280 --> 00:38:48,690
Rouge one, Rouge two, and rouge.

618
00:38:49,350 --> 00:38:52,290
You'll notice they consistently
perform very poorly, especially on

619
00:38:52,290 --> 00:38:56,940
the coherence and fluency because they
doesn't really, can correlate on these.

620
00:38:57,285 --> 00:39:01,515
Semantic qualities of generated responses
there is focus on engram overlaps.

621
00:39:01,605 --> 00:39:05,715
So next we have a set of basic embedding
metrics like the bird score or the mover

622
00:39:05,715 --> 00:39:08,715
score, which shows these improvements.

623
00:39:08,955 --> 00:39:11,505
They still fall short of truly
understanding the meaning

624
00:39:11,510 --> 00:39:12,280
across diverse responses.

625
00:39:13,085 --> 00:39:14,585
But now let's look at Uni eal.

626
00:39:14,705 --> 00:39:15,995
It's a learning evaluation metric.

627
00:39:16,295 --> 00:39:19,025
It performs noticeably better,
especially in the coherence.

628
00:39:19,025 --> 00:39:21,395
I fluency still, it's not
the top performer of all.

629
00:39:22,025 --> 00:39:25,985
The real breakthrough comes from l LM
based evaluators like GPT SCORE and geal.

630
00:39:26,855 --> 00:39:28,325
Let's take a look at the score at GE eal.

631
00:39:28,535 --> 00:39:29,945
You're right, so we'll see.

632
00:39:30,035 --> 00:39:32,015
The GE EAL has the high correlation score.

633
00:39:32,065 --> 00:39:33,955
It has the strongest
overall average score.

634
00:39:34,535 --> 00:39:38,005
Point by one, and these all matters
because it shows the model cannot

635
00:39:38,005 --> 00:39:39,835
evaluate other models in a way.

636
00:39:40,435 --> 00:39:42,085
And it is very remarkable.

637
00:39:42,544 --> 00:39:45,424
It's very remarkably close to human
reasoning, especially when we prompt

638
00:39:45,424 --> 00:39:48,575
them with structural criteria and we
enable chain of thought reasoning.

639
00:39:49,145 --> 00:39:52,654
So we also, we should also note
this, you using probability,

640
00:39:52,654 --> 00:39:54,035
you are using chain of thoughts.

641
00:39:54,305 --> 00:39:56,944
It performs slightly worse, but
combining both of these things

642
00:39:57,095 --> 00:39:58,475
gives us really strong signals.

643
00:39:58,955 --> 00:40:02,464
So the key takeaway that we have
there is gval four with both chain

644
00:40:02,464 --> 00:40:05,464
of thought and scoring probabilities
currently lead the pack and

645
00:40:05,515 --> 00:40:07,104
automating evaluation for elements.

646
00:40:08,934 --> 00:40:12,834
So now we see that you know the pipeline
for generating synthetic evaluation

647
00:40:12,834 --> 00:40:16,854
data sets, especially for retrieval,
augmented generation re systems.

648
00:40:17,214 --> 00:40:20,905
We begin this by uploading documents
that can be in PDF format and

649
00:40:20,905 --> 00:40:22,675
docx format, or in any, RX format.

650
00:40:23,065 --> 00:40:25,915
These documents are then chunked or
broken down into smaller segments.

651
00:40:25,930 --> 00:40:30,609
Some chunks may be unqualified, a bit lack
useful information, or it is too noisy.

652
00:40:31,060 --> 00:40:34,569
Next, we generate contextual frame from
disqualified chunks and essentially

653
00:40:34,569 --> 00:40:38,500
prepare passages that would serve us as
a grounding evidence from those contexts.

654
00:40:38,500 --> 00:40:39,700
We now generate goldens.

655
00:40:39,770 --> 00:40:44,060
That is gold ground truth, that the
answer should ideally produce if it had

656
00:40:44,390 --> 00:40:45,770
right access to the right information.

657
00:40:46,490 --> 00:40:49,730
Any ambiguous or low quality goldens
are discarded and unqualified.

658
00:40:50,245 --> 00:40:53,870
Once goldens are finalized, we evolve
queries by increasing their complexity

659
00:40:54,080 --> 00:40:58,220
and difficulty to stress test the system
retrievals and reusing capabilities.

660
00:40:58,760 --> 00:41:02,720
The result of this entire pipeline is a
synthetic evaluation data sets that can

661
00:41:02,720 --> 00:41:06,620
use, that can be used to benchmark both
retrieval and generated performance.

662
00:41:06,980 --> 00:41:09,170
And very importantly,
this processes itrate.

663
00:41:09,650 --> 00:41:13,340
We can loop back and forth and define
edits and ly improve the dataset.

664
00:41:14,765 --> 00:41:18,125
So this kind of synthetic dataset
creation allows us to simulate real

665
00:41:18,125 --> 00:41:21,190
world scenarios while we maintain
full control over evaluation quality.

666
00:41:23,404 --> 00:41:27,455
So as we wrap up the discussion on
evaluation, let's quickly go over what

667
00:41:27,725 --> 00:41:31,265
practical checklist do we have for
building very robust landmark ecosystems?

668
00:41:31,415 --> 00:41:34,565
First, we need to clearly
identify our objectives.

669
00:41:34,625 --> 00:41:36,665
We need to ask what are we measuring?

670
00:41:36,665 --> 00:41:37,955
Why are we measuring it?

671
00:41:37,984 --> 00:41:39,154
Is it really accurate?

672
00:41:39,605 --> 00:41:40,955
We do we need accuracy.

673
00:41:41,044 --> 00:41:42,395
Is it factually correct?

674
00:41:42,424 --> 00:41:43,205
Is it safe?

675
00:41:43,535 --> 00:41:47,555
And so that we could reach on a clear goal
that aligns us with our metric and method.

676
00:41:48,259 --> 00:41:51,049
Second, develop and
diverse evaluation method.

677
00:41:51,350 --> 00:41:53,870
Not a one single metric is
never going to be enough.

678
00:41:54,200 --> 00:41:56,690
We need a mix of automated
automatic scoring, human

679
00:41:56,930 --> 00:41:58,339
review, and model based grading.

680
00:41:59,210 --> 00:41:59,839
Third, you must.

681
00:42:00,450 --> 00:42:02,890
Represent, we must create
representative data sets.

682
00:42:03,400 --> 00:42:06,610
The dataset should represent and
reflect the actual complexity and

683
00:42:06,610 --> 00:42:10,120
diversity of real world inputs that the
model will face during in production.

684
00:42:11,050 --> 00:42:12,700
The fourth point is evaluation.

685
00:42:12,730 --> 00:42:14,440
That evaluation has to be rated.

686
00:42:14,500 --> 00:42:15,640
You can never, I.

687
00:42:16,145 --> 00:42:19,174
Have a one chart solution for
everything you have to evaluate.

688
00:42:19,174 --> 00:42:23,895
As the world, moves forward, you have to
evaluate as the data has a drift in it.

689
00:42:23,994 --> 00:42:28,194
You have to redefine your test cases,
redefine your edge cases, your what

690
00:42:28,194 --> 00:42:32,124
causes a failure, what is meant
by failure and the user feedback.

691
00:42:32,574 --> 00:42:34,914
And then the fifth point is
establishing baseline comparison.

692
00:42:35,214 --> 00:42:37,524
Always compare your model
against the strong baseline.

693
00:42:38,225 --> 00:42:41,194
This helps quantify improvement
and spot regression early.

694
00:42:41,315 --> 00:42:44,975
And finally, we need to
leverage AI rate evaluation.

695
00:42:45,455 --> 00:42:49,785
We need to use frameworks like Gval,
open AI evaluation, or custom LM based

696
00:42:49,785 --> 00:42:54,105
grad to scale evaluation efficiently
without compromising on that.

697
00:42:55,695 --> 00:42:59,685
So if you follow the stick list that
helps you move along from ad hoc testing

698
00:42:59,685 --> 00:43:04,035
to systematic, defensible, and a scalable
solution for the violation process.

699
00:43:05,860 --> 00:43:08,410
So now let's look at a real
world scenario, evaluating a

700
00:43:08,410 --> 00:43:09,910
customer service AI system.

701
00:43:10,600 --> 00:43:13,570
Traditionally, we might have
defaulted to blue or rouge

702
00:43:13,570 --> 00:43:15,040
scores for the model evaluation.

703
00:43:15,310 --> 00:43:18,790
For this use case, it's not, it's
simply that's not enough, right?

704
00:43:19,360 --> 00:43:22,210
These metrics do not capture
the user satisfaction, business

705
00:43:22,210 --> 00:43:23,770
outcome, or operational efficiency.

706
00:43:24,130 --> 00:43:28,585
So instead, we break down the evaluation
into three critical layer first.

707
00:43:28,975 --> 00:43:32,995
The technical performance system and
assessment, we start with building blocks.

708
00:43:32,995 --> 00:43:34,860
It's so simple as LLU component.

709
00:43:35,070 --> 00:43:38,009
How will the system handle the dialogue
management and the quality of the

710
00:43:38,009 --> 00:43:42,330
response in ratio Here we can still use
some LLM focus metrics, but they need

711
00:43:42,330 --> 00:43:44,009
to be task aligned in context of error.

712
00:43:44,805 --> 00:43:47,505
Second, the shift to
customer experience metrics.

713
00:43:47,775 --> 00:43:49,545
This is where many AI models stumble.

714
00:43:49,605 --> 00:43:52,935
We need to take a look at the actual
response team, whether the conversation

715
00:43:52,935 --> 00:43:57,765
feels natural, high quality, and how well
the system handles follow, especially when

716
00:43:58,035 --> 00:44:00,225
multiple turns are needed for evaluation.

717
00:44:00,674 --> 00:44:02,294
And finally, the business impact.

718
00:44:02,564 --> 00:44:04,245
This is the bottom line.

719
00:44:04,245 --> 00:44:06,555
The bottom line of all of
these problems are, is the AI

720
00:44:06,555 --> 00:44:07,995
improving the conversion rates?

721
00:44:08,234 --> 00:44:10,274
Is it actually reducing
the resolution time?

722
00:44:10,754 --> 00:44:13,694
And most importantly, is it
delivering cost savings at scale?

723
00:44:16,004 --> 00:44:17,864
So why do we need this
evaluation techniques?

724
00:44:17,924 --> 00:44:18,914
What do they give us?

725
00:44:19,574 --> 00:44:21,074
They give us trustworthy output.

726
00:44:21,404 --> 00:44:22,119
They make.

727
00:44:22,679 --> 00:44:24,449
Our answers factually correct.

728
00:44:24,689 --> 00:44:28,049
They help us catch hallucination
early on, even before they reach

729
00:44:28,049 --> 00:44:29,339
a user or go into production.

730
00:44:29,909 --> 00:44:31,709
We get readable logical answers.

731
00:44:31,709 --> 00:44:35,669
By enforcing semantic coherence, we
ensure the model response are clear,

732
00:44:35,729 --> 00:44:38,969
internally consistent, and actually
makes sense like a sounding good, right?

733
00:44:39,899 --> 00:44:43,619
The third point is, the techniques
brings us strong task alignment.

734
00:44:43,679 --> 00:44:46,319
The model doesn't just respond
to you because it has to.

735
00:44:46,739 --> 00:44:48,659
It has to focus on the
intent of the prompt.

736
00:44:48,719 --> 00:44:49,829
It should be no wandering.

737
00:44:49,829 --> 00:44:51,119
There should be no going off topic.

738
00:44:51,960 --> 00:44:54,720
And then with is we also have a
signal to noise control, right?

739
00:44:55,170 --> 00:44:59,040
The context precision metrics can
penalize irrelevant or irrelevant, or

740
00:44:59,040 --> 00:45:03,610
made up information, which helps us trim
the fluff and boost, content fidelity.

741
00:45:04,390 --> 00:45:06,280
And then finally we get
the complete coverage.

742
00:45:06,305 --> 00:45:10,145
Context recall, ensure the model captures
all the critical facts from the source.

743
00:45:10,205 --> 00:45:13,745
So nothing important gets left behind
with these evaluation upgrades.

744
00:45:13,745 --> 00:45:17,645
We don't just optimize performance,
we optimize trust, clarity, and we

745
00:45:17,645 --> 00:45:19,444
have real world reliability right now.

746
00:45:22,024 --> 00:45:25,265
So was the real p of all of
these new violation methods.

747
00:45:25,625 --> 00:45:30,695
It's this, we now have a multi scorecard,
not just a single number or a static

748
00:45:30,695 --> 00:45:34,010
metrics, but a fully diagnostic
that tells us exactly what to fix.

749
00:45:34,480 --> 00:45:36,340
Whether the model is struggling
and how we can improve.

750
00:45:37,900 --> 00:45:38,800
And here's the best part.

751
00:45:38,860 --> 00:45:40,960
It is not just for academic
benchmarking anymore.

752
00:45:41,260 --> 00:45:45,610
This approach directly boost real
world user satisfaction because the

753
00:45:45,610 --> 00:45:49,870
feedback is granular, actionable,
and it ties to our actual experience.

754
00:45:50,440 --> 00:45:54,250
So in short, better evaluation
means better models and better

755
00:45:54,250 --> 00:45:55,540
models means happy user.

756
00:45:55,690 --> 00:45:57,370
That is the future we're
trying to build with, right?

757
00:46:00,015 --> 00:46:02,080
All right folks, so that is a wrap.

758
00:46:02,379 --> 00:46:06,069
Thank you so much for listening to
us, and feel free to contact us if

759
00:46:06,069 --> 00:46:10,029
you have any of your evaluation needs
that you need or you want to work on.

760
00:46:10,299 --> 00:46:13,990
If you are any specific use cases that
you want the evaluation frameworks

761
00:46:13,990 --> 00:46:17,410
to support you or any kind of AA
problems, we're happy to help you out.

762
00:46:17,830 --> 00:46:18,730
Thank you for listening in.

763
00:46:19,540 --> 00:46:20,080
Have a good day.

