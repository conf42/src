1
00:00:00,049 --> 00:00:05,760
Hi everyone, I'm Rishabh Mehra and today
I'll be talking about on device large

2
00:00:05,760 --> 00:00:11,900
language models, which I believe is a path
to safer and more efficient AI systems.

3
00:00:12,400 --> 00:00:17,010
Let me start by describing who I am
and why I'm talking about this topic.

4
00:00:17,890 --> 00:00:22,660
So I studied computer science at Stanford
where I was researching on computer

5
00:00:22,660 --> 00:00:24,899
vision systems under Professor Fei Li.

6
00:00:25,809 --> 00:00:29,459
We worked on publishing
papers in MLHC and NeurIPS.

7
00:00:29,550 --> 00:00:31,490
in the computer vision
and healthcare field.

8
00:00:32,000 --> 00:00:36,650
From there, I went to Apple, where I
worked on device machine learning models

9
00:00:36,950 --> 00:00:41,700
to make iOS smarter as a system, as
well as prototyping efforts for Apple

10
00:00:41,700 --> 00:00:46,550
Intelligence, which was a large language
modeling system Apple launched recently.

11
00:00:47,410 --> 00:00:51,140
Now I'm working on Pinnacle, which
is a startup in the intersection

12
00:00:51,140 --> 00:00:52,560
of all three of those fields.

13
00:00:53,060 --> 00:00:56,760
Also, as a fun fact, I have, an
interest in rock climbing and that's

14
00:00:56,760 --> 00:00:58,470
why it shows up in the pie chart there.

15
00:00:58,970 --> 00:01:02,570
Alright, now let's look at the table of
contents for today and what we'll cover.

16
00:01:03,080 --> 00:01:07,440
We'll start with why on device LLMs
matter, why we're talking about these.

17
00:01:08,260 --> 00:01:11,610
We'll talk about state of the art,
what's out there, and we'll see

18
00:01:11,620 --> 00:01:13,460
some examples of how they perform.

19
00:01:13,960 --> 00:01:17,430
We'll go through some real world use
cases which are already deployed.

20
00:01:18,320 --> 00:01:22,250
Then we'll talk about what MVP
is and what the path to this

21
00:01:22,290 --> 00:01:24,060
MVP would be in my opinion.

22
00:01:24,770 --> 00:01:28,880
And we'll conclude with where I see
these models in the next five years

23
00:01:29,000 --> 00:01:30,740
and the applications they'll enable.

24
00:01:31,240 --> 00:01:35,660
All right, let's start with the first
topic, why on device LLMs matter.

25
00:01:36,160 --> 00:01:39,260
Let's start by first understanding
what large language models are.

26
00:01:39,760 --> 00:01:43,830
The way I like to define large language
models, of course at a very high level,

27
00:01:44,150 --> 00:01:47,520
is through a three word game I used to
play with my sister when I was young.

28
00:01:48,200 --> 00:01:52,570
I would say three words, my sister
would follow up with the three most

29
00:01:52,570 --> 00:01:56,800
likely words she think would be best,
and then I would say three words, and

30
00:01:56,800 --> 00:01:58,730
so on until we formed a full story.

31
00:01:59,460 --> 00:02:02,560
Interestingly, large language models
are trying to do the same thing.

32
00:02:03,100 --> 00:02:06,825
You give them a bunch of words, And
they reply with a bunch of words

33
00:02:06,825 --> 00:02:10,215
which they think make the most
logical sense, given what you said.

34
00:02:10,715 --> 00:02:13,905
Of course, this is a very high
level definition, but that's the

35
00:02:13,905 --> 00:02:15,315
task they're trying to perform.

36
00:02:15,815 --> 00:02:18,735
Now let's talk about three
major problems I believe there

37
00:02:18,735 --> 00:02:20,335
are with large language models.

38
00:02:20,865 --> 00:02:23,625
The first is privacy evasion, invasion.

39
00:02:24,125 --> 00:02:28,375
Privacy invasion occurs because the
types of applications that use large

40
00:02:28,375 --> 00:02:31,165
language models need personal data.

41
00:02:31,695 --> 00:02:34,875
They're often you telling it
information or providing it

42
00:02:34,875 --> 00:02:38,345
information through your computer
screen, et cetera, which they process.

43
00:02:38,805 --> 00:02:43,415
This raises major privacy concerns
because you might be sharing your data

44
00:02:43,585 --> 00:02:48,665
with an LLM provider like OpenAI, but
also an intermediary like a startup.

45
00:02:48,665 --> 00:02:50,565
So you need to trust both those companies.

46
00:02:51,065 --> 00:02:54,225
Large language models also have
a massive carbon footprint.

47
00:02:54,945 --> 00:03:00,655
For example, just training GPT 4 in
an article was shown to have emissions

48
00:03:01,205 --> 00:03:05,955
equivalent to driving 18 million
miles in a gasoline car and a single

49
00:03:05,955 --> 00:03:11,005
inference call to GPT 4, again in an
article, was shown to be equivalent of

50
00:03:11,015 --> 00:03:13,385
charging a mobile phone 60 times over.

51
00:03:13,885 --> 00:03:17,665
Then also another problem
with large language models is

52
00:03:17,675 --> 00:03:19,745
their ability to mimic humans.

53
00:03:20,245 --> 00:03:23,785
This is because large language
models are essentially producing

54
00:03:23,785 --> 00:03:29,095
language similar to humans and this
leads to problems such as deepfakes.

55
00:03:29,095 --> 00:03:32,905
The internet is flooded with
deepfakes, but also the possibility

56
00:03:32,905 --> 00:03:34,595
of fraud increasing a lot.

57
00:03:35,185 --> 00:03:39,425
For example, imagine getting a call in the
voice of your mother and your mother tells

58
00:03:39,425 --> 00:03:41,405
you she's hurt and she needs some money.

59
00:03:41,765 --> 00:03:46,115
You'll probably transfer the money,
but later you'll realize that this

60
00:03:46,115 --> 00:03:49,935
was just a large language model
talking in your mother's voice.

61
00:03:50,435 --> 00:03:53,865
This is a very big concern, but
we won't talk about this today

62
00:03:53,865 --> 00:03:55,365
because this is a whole other talk.

63
00:03:55,745 --> 00:03:58,015
We'll cover the first and
the second points today.

64
00:03:58,515 --> 00:04:02,405
So moving these large language
models from the internet to on

65
00:04:02,405 --> 00:04:05,275
device solves the first two problems.

66
00:04:05,825 --> 00:04:09,125
This is because on device
models are off the grid.

67
00:04:09,335 --> 00:04:10,625
They don't need internet.

68
00:04:10,755 --> 00:04:12,555
They don't need any external connection.

69
00:04:13,185 --> 00:04:14,205
There is no middleman.

70
00:04:14,225 --> 00:04:15,675
Everything is on your device.

71
00:04:15,955 --> 00:04:17,755
So there's no privacy risk anymore.

72
00:04:18,255 --> 00:04:23,235
And as a result of these models having
to run on device, they're automatically

73
00:04:23,235 --> 00:04:28,125
smaller, and smaller means that they're
greener, and they essentially have,

74
00:04:28,165 --> 00:04:30,155
will have a lesser carbon footprint.

75
00:04:31,125 --> 00:04:34,305
And this is again, just by
design that you have to make them

76
00:04:34,305 --> 00:04:36,105
smaller to run on the computer.

77
00:04:36,605 --> 00:04:40,705
All right, now let's talk
about state of the art LLMs.

78
00:04:40,815 --> 00:04:42,335
Where is this technology today?

79
00:04:42,345 --> 00:04:43,305
What does it look like?

80
00:04:43,840 --> 00:04:45,550
And we'll do this through two demos.

81
00:04:46,050 --> 00:04:49,560
So for the first demo, I'll give
the following prompt to the LLM.

82
00:04:50,160 --> 00:04:55,290
Hello, I'll be giving a presentation
with the topic on device LLMs, a path to

83
00:04:55,290 --> 00:04:57,800
a safer and more efficient AI systems.

84
00:04:58,620 --> 00:05:02,930
Can you give me a one line opener,
make it engaging and thought provoking?

85
00:05:03,430 --> 00:05:09,425
So I take this prompt and I will
Copy the prompt and then I'll use

86
00:05:09,575 --> 00:05:14,255
OLAMA, which is a way to essentially
run large language models on device.

87
00:05:14,895 --> 00:05:17,585
The model I'm running is LAMA 3.

88
00:05:17,585 --> 00:05:17,585
1.

89
00:05:17,585 --> 00:05:21,225
This is a model by Meta and
it has 8 billion parameters.

90
00:05:21,835 --> 00:05:24,285
This is now loading into
the memory of my computer.

91
00:05:24,855 --> 00:05:29,875
I paste the prompt and run it and
it gives me a reasonable response.

92
00:05:30,095 --> 00:05:31,445
Let's read the first one.

93
00:05:31,945 --> 00:05:36,930
As we increasingly rely on AI, Can
we afford not to rethink the way

94
00:05:37,000 --> 00:05:41,940
we build models from edges of our
networks to edges of our devices?

95
00:05:42,380 --> 00:05:43,000
Makes sense.

96
00:05:43,080 --> 00:05:43,680
It's nice.

97
00:05:43,690 --> 00:05:45,990
It's not perfect, but it's a good start.

98
00:05:46,840 --> 00:05:47,020
All right.

99
00:05:47,060 --> 00:05:50,740
Now just to get a baseline, sorry,
before we get the baseline, we try

100
00:05:50,740 --> 00:05:55,610
to run this on Lama billion, which
is a much bigger model by Facebook.

101
00:05:56,220 --> 00:05:58,430
And this is now loading into memory.

102
00:05:59,040 --> 00:06:01,800
But as I'll soon realize this
doesn't actually load into

103
00:06:01,800 --> 00:06:03,090
the memory of my computer.

104
00:06:03,435 --> 00:06:09,065
I open activity monitor and I see
this model is taking tons of memory.

105
00:06:09,065 --> 00:06:13,085
It's already taking 32 gigs, which
is more than what this computer has.

106
00:06:13,625 --> 00:06:16,625
So this model essentially
cannot run on my device.

107
00:06:17,555 --> 00:06:21,244
so we've seen that the model
size on device is seven to 10

108
00:06:21,245 --> 00:06:22,705
billion, somewhere around there.

109
00:06:23,294 --> 00:06:26,945
now just to get a baseline, we
run the same prompt in GPD 4.

110
00:06:27,025 --> 00:06:27,495
0.

111
00:06:28,364 --> 00:06:30,815
So I paste the prompt, run it.

112
00:06:31,460 --> 00:06:36,750
And I get the response, which is,
imagine smartphone is not just smart,

113
00:06:37,140 --> 00:06:39,360
but a powerhouse of intelligent privacy.

114
00:06:39,960 --> 00:06:42,990
Welcome to the new frontier
with on device LLMs.

115
00:06:43,420 --> 00:06:46,070
So perfect opening, exactly what I wanted.

116
00:06:46,389 --> 00:06:48,839
And that's what we have
come to expect from GPD 4.

117
00:06:48,840 --> 00:06:49,700
0.

118
00:06:50,090 --> 00:06:52,190
But of course, this is
still running in the cloud.

119
00:06:52,580 --> 00:06:56,689
And as we saw, Lama 8, we produced
a response, which made sense, but

120
00:06:56,689 --> 00:06:58,919
it's not at the level of GPD 4.

121
00:06:58,919 --> 00:06:59,230
0 yet.

122
00:06:59,730 --> 00:07:03,230
All right, now for the second test,
we'll do a coding challenge where

123
00:07:03,230 --> 00:07:07,480
we'll get these models to create
Minesweeper, a popular game, and

124
00:07:07,480 --> 00:07:09,280
we'll see how the different models do.

125
00:07:10,280 --> 00:07:14,769
So let's start by copying this
prompt I created to write, make

126
00:07:14,769 --> 00:07:16,159
the models write Minesweeper.

127
00:07:16,439 --> 00:07:21,270
And we'll again start with
Llama8B, the tiny model by Meta

128
00:07:21,270 --> 00:07:22,700
that runs fully on my device.

129
00:07:23,200 --> 00:07:25,520
As you can see, it produces
code which makes sense.

130
00:07:25,530 --> 00:07:28,799
It's calling the init function,
print board, create mines,

131
00:07:29,760 --> 00:07:31,349
count adjacent mines, reveal.

132
00:07:31,429 --> 00:07:34,070
All of this makes sense for
a typical minesweeper game.

133
00:07:34,570 --> 00:07:38,869
So now, it's, Finishing producing
this code, it's written the play

134
00:07:38,869 --> 00:07:41,769
function, which is a while loop
to play this game indefinitely

135
00:07:41,769 --> 00:07:43,079
until you hit a mine, of course.

136
00:07:43,839 --> 00:07:45,979
now I can go ahead and copy the code.

137
00:07:46,479 --> 00:07:52,169
I'll paste the code into a Python
file called minesweeper llama8b.

138
00:07:52,170 --> 00:07:55,589
py.

139
00:07:56,089 --> 00:07:58,329
And then I'll go ahead and run the code.

140
00:07:58,944 --> 00:08:01,334
Unfortunately for Llama, it doesn't run.

141
00:08:02,014 --> 00:08:05,634
it only took me 15 to 20 minutes to
correct it so it wasn't too off and

142
00:08:05,644 --> 00:08:09,504
other iterations produce slightly better
code which sometimes even compiled.

143
00:08:10,054 --> 00:08:12,904
okay now to get a baseline
let's go to GPT 4.

144
00:08:12,904 --> 00:08:17,844
0, paste the prompt and as you can see
it very quickly generates the code.

145
00:08:17,894 --> 00:08:20,554
Of course this is again running
on the server and we expect this

146
00:08:20,554 --> 00:08:23,804
to be great but just to get a
baseline let's see what it does.

147
00:08:24,424 --> 00:08:28,394
The code looks similar to what
was produced by Lama, but of

148
00:08:28,394 --> 00:08:32,664
course, as we know, the quality
is expected to be a lot better.

149
00:08:33,164 --> 00:08:34,154
So I do the same thing.

150
00:08:34,174 --> 00:08:40,644
I copy the code, go back to the terminal,
create a file called minesweeper gpt40.

151
00:08:40,644 --> 00:08:44,584
py, paste the code, and now I run it.

152
00:08:45,084 --> 00:08:47,834
And here we get a perfectly
working version of Minesweeper.

153
00:08:47,834 --> 00:08:51,224
I type 3 4, hit a mine,
and lose in the first turn.

154
00:08:51,584 --> 00:08:53,544
So let's go ahead and play once more.

155
00:08:54,044 --> 00:09:00,274
And here I can essentially run through the
entire game of Minesweeper where I'm going

156
00:09:00,274 --> 00:09:02,814
through different options I've chosen.

157
00:09:03,454 --> 00:09:07,139
This is fast forwarded and
then eventually I write 3 3.

158
00:09:07,749 --> 00:09:09,279
hit a mine and the game ends.

159
00:09:09,929 --> 00:09:11,479
All right, now the interesting part.

160
00:09:12,029 --> 00:09:16,459
We're going to paste the same prompt,
but in a very different model.

161
00:09:16,619 --> 00:09:18,429
This model is called CodeQuin.

162
00:09:19,069 --> 00:09:21,819
This is by the Alibaba group in China.

163
00:09:22,719 --> 00:09:25,799
this model is only a 7
billion parameter model.

164
00:09:26,329 --> 00:09:30,919
So it's a much smaller model than even,
or not much smaller, but a smaller model

165
00:09:30,929 --> 00:09:33,359
than the Lama model we saw earlier.

166
00:09:33,859 --> 00:09:35,739
So it's able to run fully on device.

167
00:09:36,239 --> 00:09:39,949
But the interesting thing about
this model is it's customized and

168
00:09:40,169 --> 00:09:42,179
tuned to do well on coding tasks.

169
00:09:42,189 --> 00:09:48,029
So its strength is coding and it
outperforms LLAMA as we'll see soon.

170
00:09:48,039 --> 00:09:51,899
So right now it's writing code,
it creates the board, counts the

171
00:09:51,899 --> 00:09:55,749
mines, reveals a similar structure
to what LLAMA was producing.

172
00:09:56,749 --> 00:10:00,609
It's a bit slower running on memory, even
though it's a smaller parameter model.

173
00:10:01,119 --> 00:10:05,199
Presumably it's not as
optimized as Llama for MacBooks.

174
00:10:05,699 --> 00:10:07,979
so I'll give it a second
to just finish coding.

175
00:10:08,479 --> 00:10:09,469
Almost there.

176
00:10:09,969 --> 00:10:10,559
Alright.

177
00:10:11,079 --> 00:10:11,559
There we go.

178
00:10:11,859 --> 00:10:12,674
It's finished the setup.

179
00:10:12,724 --> 00:10:14,264
code given some description.

180
00:10:14,264 --> 00:10:20,894
I will go ahead, copy the code and follow
the same procedure where I'll create,

181
00:10:20,954 --> 00:10:28,344
I'll copy the code, create a Python
file called Minesweeper code or q7b.

182
00:10:28,344 --> 00:10:29,564
py, paste the code

183
00:10:30,064 --> 00:10:31,054
and then I run it.

184
00:10:31,554 --> 00:10:36,074
Interestingly, it shows me the locations
of the mines visibly, which is a bit

185
00:10:36,104 --> 00:10:38,784
weird, but it at least runs the code.

186
00:10:39,024 --> 00:10:42,924
Then we write the location of a mine
03, but it doesn't end the game.

187
00:10:43,559 --> 00:10:48,429
Then I write 00, which is not the location
of the mine, and that, that works well.

188
00:10:49,189 --> 00:10:53,039
So essentially it's playing Minesweeper
correctly, except that it shows the

189
00:10:53,039 --> 00:10:56,909
mines at the beginning and hitting
the mines do not end the game.

190
00:10:57,679 --> 00:11:01,049
I played around with this code after
the demo and it took me less than five

191
00:11:01,049 --> 00:11:02,829
minutes to make it fully functional.

192
00:11:03,209 --> 00:11:07,939
So it was definitely higher quality
than what Llama8B had created for me.

193
00:11:08,439 --> 00:11:12,829
All right, we have looked at,
these models qualitatively,

194
00:11:12,829 --> 00:11:14,499
but let's look at some numbers.

195
00:11:15,149 --> 00:11:19,439
sorry, the bottom right number is
hidden because of my video, but it's 78.

196
00:11:19,939 --> 00:11:22,679
So let's start from the top left, LAMA 3.

197
00:11:22,779 --> 00:11:25,729
1, the 8 billion parameter model.

198
00:11:26,329 --> 00:11:28,869
let's look at the chat reasoning
task, so how well it performs

199
00:11:28,869 --> 00:11:31,179
on chat tasks and coding tasks.

200
00:11:31,979 --> 00:11:33,059
So for LAMA 3.

201
00:11:33,059 --> 00:11:38,539
1, The 8 billion parameter model got a
73 on chat and about the same on coding.

202
00:11:39,039 --> 00:11:43,399
The 70 billion got 86 on chats,
an amazing score for such a small

203
00:11:43,399 --> 00:11:45,459
model and the same on coding.

204
00:11:46,149 --> 00:11:47,164
GPT 4.

205
00:11:47,164 --> 00:11:48,459
0 got 86.

206
00:11:48,459 --> 00:11:50,809
7 on chat and 87.

207
00:11:50,809 --> 00:11:52,319
8 on coding.

208
00:11:52,319 --> 00:11:54,159
So again, close to each other.

209
00:11:54,659 --> 00:11:58,199
But CodeGrend, the tiny model
by the Alibaba group, got

210
00:11:58,209 --> 00:12:00,199
78 on the coding challenge.

211
00:12:00,559 --> 00:12:04,449
So much higher than the 8 billion
parameter Lama model like we saw earlier.

212
00:12:04,939 --> 00:12:11,089
And this shows us the importance of having
task specific models, which we'll talk

213
00:12:11,099 --> 00:12:13,099
about more in the coming slides as well.

214
00:12:13,599 --> 00:12:18,279
Next, let's explore the current
real world use cases for on

215
00:12:18,279 --> 00:12:20,039
device large language models.

216
00:12:20,489 --> 00:12:22,989
So these are use cases
already out there in the wild.

217
00:12:23,489 --> 00:12:25,339
Here's a list of these use cases.

218
00:12:25,819 --> 00:12:29,009
Some of these are large language
models and some of these are related

219
00:12:29,009 --> 00:12:32,979
technologies, which use similar
tech to large language models.

220
00:12:33,429 --> 00:12:35,679
So style transfer, this
is already out there.

221
00:12:36,029 --> 00:12:39,749
If you want to convert your
email to a more friendly email

222
00:12:39,749 --> 00:12:41,019
or a more professional email.

223
00:12:41,449 --> 00:12:44,329
This is already possible today
with large language models.

224
00:12:44,939 --> 00:12:46,029
Speech to text.

225
00:12:46,479 --> 00:12:52,219
Converting, your audio to text is a very
important task and this actually works

226
00:12:52,219 --> 00:12:55,169
really well fully on device today already.

227
00:12:55,919 --> 00:12:59,039
probably almost as good as the
server side model to be honest.

228
00:12:59,739 --> 00:13:00,769
Summarization.

229
00:13:01,399 --> 00:13:04,209
summarization does work on device today.

230
00:13:04,429 --> 00:13:06,769
It's not as good as server
side, but it's pretty good.

231
00:13:06,839 --> 00:13:10,359
You can get 80 percent of the
value you'll want out of it.

232
00:13:11,049 --> 00:13:14,489
And finally, translation, same as
summarization, works really well

233
00:13:14,489 --> 00:13:18,089
on device, slightly worse than
server side, but it's almost there.

234
00:13:18,879 --> 00:13:22,799
Now let's actually look at some of these
use cases in action through a demo.

235
00:13:23,299 --> 00:13:27,889
All right, so let's go through the
use cases which we saw before through

236
00:13:27,889 --> 00:13:29,639
the eyes of Apple Intelligence.

237
00:13:29,989 --> 00:13:34,519
Apple Intelligence is a suite of
features Apple launched in iOS 18.

238
00:13:34,519 --> 00:13:35,019
1.

239
00:13:35,019 --> 00:13:37,989
Some of the features in this
demo also came before that.

240
00:13:38,759 --> 00:13:42,189
As you can see, the phone is on
airplane mode, so there's no internet.

241
00:13:42,564 --> 00:13:46,304
This is fully on device, not running
a model on the server or anything.

242
00:13:46,674 --> 00:13:50,004
So what I'm doing here is I'm in the
notes app, and I'm typing an email.

243
00:13:50,664 --> 00:13:52,944
Hi Alex, reached this site.

244
00:13:53,304 --> 00:13:56,784
Good to meet you, but I don't think
now is the right time to collaborate.

245
00:13:57,394 --> 00:13:59,984
While I love your vision
question mark, the product

246
00:14:00,054 --> 00:14:02,494
isn't the right fit, right now.

247
00:14:03,074 --> 00:14:05,664
Hope to collaborate, someday.

248
00:14:06,424 --> 00:14:06,954
RISH.

249
00:14:07,284 --> 00:14:09,684
So not great English, but it's a start.

250
00:14:10,174 --> 00:14:11,944
So I can go ahead and select this.

251
00:14:12,354 --> 00:14:15,124
I can go to writing tools, which
is one of the new features Apple

252
00:14:15,124 --> 00:14:17,714
launched, and I can click on proofread.

253
00:14:18,214 --> 00:14:22,224
So here Apple suggests me
change a couple of words.

254
00:14:22,264 --> 00:14:26,664
RISH comma this side, good
to meet, and then some days

255
00:14:26,684 --> 00:14:27,984
combined into a single word.

256
00:14:28,629 --> 00:14:30,679
Not super helpful, but it's okay.

257
00:14:31,439 --> 00:14:33,359
So let's try some other writing tool.

258
00:14:33,949 --> 00:14:37,239
this time I select writing
tools and I click rewrite.

259
00:14:37,739 --> 00:14:40,779
So rewrite is now going to
rewrite the message as expected.

260
00:14:41,264 --> 00:14:44,154
So it says, Rish, it's great
to meet you on this side.

261
00:14:44,264 --> 00:14:47,404
So I think Rish is the other person
instead of me, which is incorrect,

262
00:14:47,744 --> 00:14:49,244
but let's look at the second sentence.

263
00:14:49,654 --> 00:14:53,784
However, I don't believe this is
the appropriate time to collaborate.

264
00:14:53,784 --> 00:14:57,624
So this sentence and the rest of the
email is a lot better than before.

265
00:14:58,124 --> 00:14:58,904
So it's getting there.

266
00:14:58,904 --> 00:15:00,674
It's actually improving it the way I want.

267
00:15:00,694 --> 00:15:04,124
Let's just remove RISC to make
the job simpler for the LLM.

268
00:15:04,624 --> 00:15:09,054
And this time when I select the
email, I'll go to writing tools, but

269
00:15:09,134 --> 00:15:12,714
I'll just change it to a friendly
tone, so style transfer this time.

270
00:15:13,134 --> 00:15:14,654
And I get the response, Hi Alex.

271
00:15:15,124 --> 00:15:16,144
It's great to meet you.

272
00:15:16,174 --> 00:15:19,424
I think we're both on the same page about
wanting to collaborate in the future.

273
00:15:19,764 --> 00:15:22,474
I just don't think now is the
right time to work together

274
00:15:22,484 --> 00:15:23,804
on this particular project.

275
00:15:24,104 --> 00:15:27,024
I hope we can still find a way to
work with each other in the future.

276
00:15:27,234 --> 00:15:28,494
So work flawlessly.

277
00:15:28,994 --> 00:15:30,854
Next let's test dictation.

278
00:15:30,964 --> 00:15:33,654
Dictation is essentially, speech to text.

279
00:15:33,864 --> 00:15:34,864
I'm seeing the sentence.

280
00:15:34,864 --> 00:15:35,804
You can't hear it.

281
00:15:36,524 --> 00:15:39,224
Hi, this is a test for seeing
how the dictation works.

282
00:15:39,614 --> 00:15:43,194
And it again worked flawlessly, fully
on device, airplane mode is still on.

283
00:15:44,044 --> 00:15:46,924
Next, let's try selecting
this entire email.

284
00:15:47,424 --> 00:15:49,814
And we'll try to create
a summary of this email.

285
00:15:49,814 --> 00:15:51,644
I don't want to go
through this whole email.

286
00:15:52,344 --> 00:15:57,024
So I'll go to writing
tools and click on summary.

287
00:15:57,504 --> 00:16:00,724
Unfortunately, that does not
work without the internet.

288
00:16:01,054 --> 00:16:03,574
So just to make sure it works
with the internet, let's turn

289
00:16:03,574 --> 00:16:05,194
on the internet, writing tools.

290
00:16:05,694 --> 00:16:07,664
And I click on Summary.

291
00:16:08,164 --> 00:16:12,014
And I got a summary telling me the events
with Uber, Bloomberg, Citadel, etc.

292
00:16:12,014 --> 00:16:15,054
And RSVPs are required
for all these events.

293
00:16:15,584 --> 00:16:18,024
So I got all the information
I needed perfectly there.

294
00:16:18,389 --> 00:16:19,809
Next, let's look at Spotlight.

295
00:16:19,839 --> 00:16:21,289
Meet Alex at 10 a.

296
00:16:21,289 --> 00:16:21,539
m.

297
00:16:21,539 --> 00:16:24,419
Airplane mode is on again,
and I got a meeting invite.

298
00:16:24,419 --> 00:16:28,499
So there's NLP matching going
on here, even if it may not be

299
00:16:28,509 --> 00:16:29,699
fully large language models.

300
00:16:29,729 --> 00:16:30,459
That's unknown.

301
00:16:31,069 --> 00:16:32,659
Next, let's look at Translate.

302
00:16:32,669 --> 00:16:34,399
So I try to translate hello.

303
00:16:34,479 --> 00:16:35,969
It doesn't work without the internet.

304
00:16:36,409 --> 00:16:40,479
So I turn on the internet and what I can
do now is I can click the three dots and

305
00:16:40,479 --> 00:16:47,579
click on download languages, scroll down,
And I can simply download Spanish onto my,

306
00:16:47,799 --> 00:16:50,689
iPhone, so now it should work on device.

307
00:16:50,709 --> 00:16:52,089
It also downloads English.

308
00:16:52,609 --> 00:16:57,069
So now I go back into airplane mode
and I write, can I get the churros?

309
00:16:57,279 --> 00:17:01,024
And it translates to Spanish without the
internet because airplane mode is on.

310
00:17:01,314 --> 00:17:02,944
And presumably this is correct.

311
00:17:03,444 --> 00:17:06,914
And I write thank you and it says
gracias, which I know is correct.

312
00:17:07,424 --> 00:17:09,724
Next I have all these
lists of notifications.

313
00:17:09,804 --> 00:17:14,484
I can click show less airplane mode is on
And I get a summary of the notifications.

314
00:17:14,724 --> 00:17:18,244
So summary in this context seems to
work without the internet as well

315
00:17:18,744 --> 00:17:22,804
All right, that's just a rapid fire
of a bunch of features in apple

316
00:17:22,804 --> 00:17:26,724
intelligence that work without the
internet hence running fully on device

317
00:17:27,224 --> 00:17:30,064
Next, let's talk about the path to MVP.

318
00:17:30,984 --> 00:17:34,484
MVP, in my opinion, if we can reach GPD 4.

319
00:17:34,544 --> 00:17:40,274
0 level performance being the accuracy
on device, that would be game changing.

320
00:17:40,284 --> 00:17:45,874
That would be a product that will actually
be extremely valuable in everyday life.

321
00:17:46,374 --> 00:17:51,094
To reach this target, I believe we'll
have to improve the hardware to be able

322
00:17:51,094 --> 00:17:53,214
to run 30 billion parameters on device.

323
00:17:53,574 --> 00:17:57,764
I think these 8 to 10 billion parameter
models are still too small to have enough

324
00:17:57,764 --> 00:18:02,944
information, but we see promising results
from 60, 70 billion parameter models.

325
00:18:02,954 --> 00:18:08,424
I'm sure we can make them smaller,
perhaps to the size of around 30 billion.

326
00:18:08,924 --> 00:18:10,754
Models need to be more specialized.

327
00:18:11,104 --> 00:18:15,609
So like we saw CodeGwent
absolutely outperformed, Lama 3.

328
00:18:15,939 --> 00:18:20,099
18 billion, despite having
similar number of parameters, just

329
00:18:20,099 --> 00:18:21,689
because it was coding specific.

330
00:18:22,299 --> 00:18:23,749
You can go one level deeper.

331
00:18:23,759 --> 00:18:27,519
You can make it Python specific
instead of coding in general,

332
00:18:27,689 --> 00:18:29,189
and that might be even better.

333
00:18:29,190 --> 00:18:32,479
Next, we need to improve adapters.

334
00:18:32,729 --> 00:18:36,339
So adapters are things like
LoRa adapters, which can.

335
00:18:36,759 --> 00:18:39,759
Which again, I can't go into details,
that would be a whole other talk,

336
00:18:40,049 --> 00:18:44,299
but essentially can add a layer
of specialization without having

337
00:18:44,299 --> 00:18:45,889
to recreate a whole new model.

338
00:18:46,309 --> 00:18:50,519
So let's say you have a Python
specialized model, you can then use

339
00:18:50,579 --> 00:18:55,139
a LoRa adapter to make it a model
for creating games using Python.

340
00:18:55,349 --> 00:18:57,829
So we've become even more
specialized using adapters.

341
00:18:58,329 --> 00:19:03,589
So in terms of software innovations,
we already talked about, the second

342
00:19:03,589 --> 00:19:06,919
and third point, which is adapter
mechanisms and specializations.

343
00:19:07,509 --> 00:19:11,069
But one more thing that needs
to be innovated is enhanced

344
00:19:11,079 --> 00:19:12,359
compression techniques.

345
00:19:12,819 --> 00:19:16,749
so not only on the hardware side
can we reduce the parameters, but

346
00:19:16,749 --> 00:19:20,429
we can start compressing these
models into smaller and smaller

347
00:19:20,769 --> 00:19:23,469
models, pulling out the parameters.

348
00:19:23,485 --> 00:19:25,515
by using better compression techniques.

349
00:19:26,145 --> 00:19:30,755
And then on the hardware side, we
need optimized processing units

350
00:19:30,755 --> 00:19:32,255
to run bigger and bigger models.

351
00:19:32,255 --> 00:19:36,405
Instead of 10 billion models, we should be
able to run 30 billion or higher models.

352
00:19:37,085 --> 00:19:39,525
But also we need an enhanced
memory architecture.

353
00:19:39,525 --> 00:19:41,375
This would be the RAM system.

354
00:19:41,935 --> 00:19:45,775
because these models need to be
extremely quick, so they need a

355
00:19:45,775 --> 00:19:47,685
really solid memory architecture.

356
00:19:48,445 --> 00:19:52,045
I'm not a hardware expert, so I can't
go into details about these topics,

357
00:19:52,085 --> 00:19:57,305
but at a high level with, in what I've
discussed with hardware experts, these are

358
00:19:57,305 --> 00:19:59,635
the two main frontiers we need to push.

359
00:20:00,135 --> 00:20:03,055
All right, and then finally,
let's say all of this happens.

360
00:20:03,055 --> 00:20:04,575
We have 30 billion parameters.

361
00:20:04,625 --> 00:20:08,175
These models are running fully on
devices, specialized, have adapters.

362
00:20:08,645 --> 00:20:09,525
What's the outcome?

363
00:20:10,025 --> 00:20:13,395
Let's talk about the real world
use cases these models will enable.

364
00:20:13,845 --> 00:20:15,975
So the first is augmented workflows.

365
00:20:16,825 --> 00:20:20,485
These models will become deeply
integrated into everyone's work life.

366
00:20:20,575 --> 00:20:24,565
You'll be able to share your entire work
context with these models because they're

367
00:20:24,565 --> 00:20:29,105
not sending any data to the server and
they'll streamline every single task

368
00:20:29,105 --> 00:20:33,995
in your life, whether it's emails, data
analysis, presentations, or even like

369
00:20:33,995 --> 00:20:36,095
filling out Excel sheets or writing code.

370
00:20:36,595 --> 00:20:38,565
The next is On Device Therapy.

371
00:20:39,545 --> 00:20:42,785
On Device Therapy will become better
with these models because you'll be

372
00:20:42,785 --> 00:20:47,310
able to share more and you'll have
advice available at all times, in

373
00:20:47,310 --> 00:20:49,040
your pocket or on your computer.

374
00:20:49,810 --> 00:20:50,990
Legal assistance.

375
00:20:51,530 --> 00:20:54,840
This will help both lawyers,
because they'll be able to share

376
00:20:54,890 --> 00:20:59,500
client data with these models,
at any time to get information.

377
00:20:59,520 --> 00:21:02,870
And they can trust these models because
they're running fully on device,

378
00:21:02,940 --> 00:21:07,280
but they can also help the layman
to get legal assistance quickly,

379
00:21:07,660 --> 00:21:09,170
without having to engage a lawyer.

380
00:21:09,200 --> 00:21:12,220
And again, they can trust this model
because it's fully on their device.

381
00:21:12,910 --> 00:21:16,900
So they can have that same,
lawyer client, relationship.

382
00:21:17,645 --> 00:21:21,455
with this model where they know the
model will not breach their trust.

383
00:21:22,285 --> 00:21:24,295
And then finally medical diagnosis.

384
00:21:25,205 --> 00:21:30,195
Medical diagnosis will again be empowered
by these models because you'll again

385
00:21:30,195 --> 00:21:33,245
be more willing to share your personal
data and you'll be willing to share

386
00:21:33,245 --> 00:21:36,765
your entire medical history with these
models because they're fully on your

387
00:21:36,765 --> 00:21:40,575
device so you don't have to worry about
where your data goes and you can get

388
00:21:40,625 --> 00:21:43,765
really accessible healthcare diagnosis.

389
00:21:44,265 --> 00:21:47,735
and potentially even
treatment using these models.

390
00:21:48,235 --> 00:21:49,595
That's all I have for today.

391
00:21:49,805 --> 00:21:51,525
Thank you for coming to the talk.

392
00:21:52,095 --> 00:21:56,045
If you have any questions or
follow ups, please feel free to

393
00:21:56,045 --> 00:21:58,325
email me at rish at pinnacle.

394
00:21:58,655 --> 00:21:59,045
co.

395
00:21:59,675 --> 00:22:00,575
Thank you so much.

396
00:22:01,275 --> 00:22:01,695
Bye.

