1
00:00:00,500 --> 00:00:03,260
Speaker 39: Hello everyone, and
thank you for joining my session

2
00:00:03,260 --> 00:00:05,660
at Conference 42 DevOps 2026.

3
00:00:06,199 --> 00:00:07,910
My name is S Kand.

4
00:00:08,180 --> 00:00:12,709
I work at Dish Network, LLC as a
senior engineering manager leading AI

5
00:00:12,709 --> 00:00:17,690
engineers and automation engineers,
where I focus on bridging AI systems with

6
00:00:17,690 --> 00:00:20,150
secure and compliant DevOps practices.

7
00:00:20,660 --> 00:00:25,280
Today I'll be sharing a framework
I call the AI trust trade, a

8
00:00:25,280 --> 00:00:29,060
practical approach to deploying
reinforcement learning, federated

9
00:00:29,060 --> 00:00:33,800
learning, and explainable AI safely
in regulated production environments.

10
00:00:34,220 --> 00:00:39,500
The goal is simple to help DevOps
team shape AI systems with the same

11
00:00:39,500 --> 00:00:44,630
level of confidence, traceability, and
control that we expect from any critical

12
00:00:44,630 --> 00:00:49,760
software system, but without showing
down, without slowing down innovation.

13
00:00:50,255 --> 00:00:52,805
Okay, and let's start get started.

14
00:00:53,315 --> 00:00:55,745
The real challenge is in deployment.

15
00:00:56,245 --> 00:00:58,405
Let's start with what's really changed.

16
00:00:58,705 --> 00:01:02,635
A few years ago, deploying
AI was the big challenge.

17
00:01:02,845 --> 00:01:05,905
Now, deployment pipelines have mature.

18
00:01:06,175 --> 00:01:11,664
We have Kubernetes CICD infrastructure
as code, and that works beautifully.

19
00:01:12,115 --> 00:01:15,175
The real challenge now isn't about
getting models into production.

20
00:01:15,685 --> 00:01:16,015
It's ab.

21
00:01:16,405 --> 00:01:20,785
It's about operating them
safely once they're there.

22
00:01:21,535 --> 00:01:26,275
AI is moved from experimental
research projects to powering

23
00:01:26,275 --> 00:01:31,765
financial supply chain and customer
facing systems system that directly

24
00:01:31,765 --> 00:01:34,345
impact compliance, risk and trust.

25
00:01:34,855 --> 00:01:38,665
So the new frontier for DevOps is
about safe operations at scale.

26
00:01:39,165 --> 00:01:44,715
Ensuring privacy by design, maintaining
audit auditability, and having

27
00:01:44,715 --> 00:01:50,385
the ability to roll back instantly
if models drift or disbe behave.

28
00:01:50,885 --> 00:01:51,425
Next slide.

29
00:01:51,925 --> 00:01:56,605
Now I'm introducing the AI trust
trade to tackle these challenges.

30
00:01:56,725 --> 00:02:00,145
I introduce what I call
the AI trust trade.

31
00:02:00,655 --> 00:02:03,985
Three complimentary pillars for
trustworthy AI operations at.

32
00:02:04,870 --> 00:02:05,590
Reinforcement.

33
00:02:05,715 --> 00:02:05,795
Learning.

34
00:02:06,100 --> 00:02:09,550
Learning, federated
learning, explainable ai.

35
00:02:10,510 --> 00:02:13,180
So I will get into in
detail about these three.

36
00:02:13,960 --> 00:02:16,015
On high level, what is
reinforcement learning?

37
00:02:17,005 --> 00:02:20,785
Which enables adaptive decision
making that evolves with the

38
00:02:20,785 --> 00:02:25,165
environment, but must stay within
defined control boundaries.

39
00:02:25,705 --> 00:02:30,625
What is federated learning, which allows
organizations to collaborate on model

40
00:02:30,625 --> 00:02:33,085
training without sharing sensitive data.

41
00:02:33,745 --> 00:02:39,175
Explainable ai, which ensures
transparency, accountability, and

42
00:02:39,175 --> 00:02:41,185
confidence in every decision made.

43
00:02:42,055 --> 00:02:45,715
These three approaches together
form the backbone for responsible,

44
00:02:46,105 --> 00:02:49,405
scalable AI in DevOps pipelines.

45
00:02:49,905 --> 00:02:52,425
Let's get into deeper into each category.

46
00:02:52,995 --> 00:02:56,805
Let's start with reinforcement
learning through a DevOps lens.

47
00:02:57,045 --> 00:02:59,435
Let's start with
reinforcement learning or.

48
00:03:00,125 --> 00:03:05,105
In simple words, in a DevOps context,
RL isn't just a research tool.

49
00:03:05,495 --> 00:03:09,335
It's becoming a way to
continuously optimize operations.

50
00:03:09,785 --> 00:03:14,255
For example, in portfolio
optimization, RL dynamically

51
00:03:15,095 --> 00:03:21,065
reallocates assets while respecting
risk and compliance constraints.

52
00:03:21,575 --> 00:03:26,435
In risk management, we can have
automated circuit breakers that

53
00:03:26,435 --> 00:03:28,160
react to volatility in real time.

54
00:03:28,660 --> 00:03:33,760
And in supply chains, RL can optimize
inventory and routing based on

55
00:03:33,760 --> 00:03:36,910
live demand and disruption signals.

56
00:03:37,630 --> 00:03:43,630
The key is building guardrails into or
L pipeline, so the system learns safely

57
00:03:43,990 --> 00:03:46,575
without creating un intended consequences.

58
00:03:47,470 --> 00:03:49,860
So we cover three three examples.

59
00:03:49,950 --> 00:03:52,375
Portfolio optimization, risk management.

60
00:03:52,995 --> 00:03:53,745
Supply chain.

61
00:03:54,375 --> 00:03:58,695
Let's get into operational
requirements for RL systems.

62
00:03:59,195 --> 00:04:03,635
From an operational standpoint, RL
requires a completely different level

63
00:04:03,635 --> 00:04:06,065
of governance compared to static models.

64
00:04:06,335 --> 00:04:07,055
What are those?

65
00:04:07,445 --> 00:04:13,145
Policy versioning, safe exploration,
reward monitoring, meta risk detection.

66
00:04:13,835 --> 00:04:15,155
What does policy monitor mean?

67
00:04:15,515 --> 00:04:16,985
Every reward function.

68
00:04:17,660 --> 00:04:22,370
Exploration policy or constraint must
be version controlled for traceability.

69
00:04:23,270 --> 00:04:24,590
Safe exploration.

70
00:04:24,950 --> 00:04:31,670
We test new policies in shadow mode before
exposing them to real users or assets.

71
00:04:31,940 --> 00:04:32,900
Reward monitoring.

72
00:04:33,260 --> 00:04:37,520
We continuously validate that
learned behaviors align with business

73
00:04:37,520 --> 00:04:40,880
goals and don't exploit loopholes.

74
00:04:41,180 --> 00:04:42,275
Meta risk detection.

75
00:04:42,665 --> 00:04:43,115
This means.

76
00:04:43,805 --> 00:04:47,645
Watching for second order
risk, like reward hacking or

77
00:04:47,675 --> 00:04:49,355
unexpected emergent behavior.

78
00:04:49,715 --> 00:04:54,575
In short, we treat the learning process
itself as something we must observe,

79
00:04:55,115 --> 00:04:57,665
govern, and roll back if needed.

80
00:04:58,165 --> 00:05:00,805
So now I'll get into federated learning.

81
00:05:01,645 --> 00:05:04,880
So federated learning, privacy,
preserving collaboration.

82
00:05:05,540 --> 00:05:06,920
Now let's move on to federated learning.

83
00:05:07,570 --> 00:05:11,110
Which is becoming essential
for industries where data can't

84
00:05:11,110 --> 00:05:12,490
leave its original boundary.

85
00:05:12,850 --> 00:05:17,560
Think bank, think banking, telecom,
healthcare, e-commerce, whatnot.

86
00:05:18,060 --> 00:05:23,220
Fair data learning lets each organization
train models locally on since two data,

87
00:05:23,720 --> 00:05:26,570
while only sharing encrypted updates.

88
00:05:27,050 --> 00:05:31,310
Using secure aggregation, we combine
the updates into a global model

89
00:05:31,820 --> 00:05:34,070
without ever exposing the raw data.

90
00:05:35,045 --> 00:05:36,185
This is the most important.

91
00:05:36,815 --> 00:05:42,065
The result is a system that learns
collectively benefits from diversity

92
00:05:42,245 --> 00:05:47,715
and stays compliant with privacy laws
now move to production pipelines.

93
00:05:47,715 --> 00:05:52,965
For Feder learning to run Feder Learning
in production, we need a DevOps grade

94
00:05:52,965 --> 00:05:59,135
pipelines with four modal versioning,
secure aggregation, drift detection.

95
00:05:59,755 --> 00:06:03,355
Performance validation I'll get
into deeper into each category.

96
00:06:03,745 --> 00:06:04,885
What is model versioning?

97
00:06:05,245 --> 00:06:09,955
So every model update is cry
cryptographically verified and traceable.

98
00:06:10,455 --> 00:06:16,485
Secure aggregation using homomorphic
encryption or secure multi-party

99
00:06:16,485 --> 00:06:21,395
computation, drift detection, so
drift detection to identify when

100
00:06:21,395 --> 00:06:23,465
local models start to diverge.

101
00:06:23,555 --> 00:06:24,965
As I discussed in previous slides.

102
00:06:25,325 --> 00:06:28,415
Or when man malicious
updates are introduced.

103
00:06:29,105 --> 00:06:31,355
Performance validation is very important.

104
00:06:31,895 --> 00:06:37,325
Ensuring the global model performs fairly
and consistently across participants.

105
00:06:38,135 --> 00:06:42,425
This ensures that the federated
system is not just private, it's also

106
00:06:42,425 --> 00:06:45,265
auditable, stable, and reproducible.

107
00:06:45,765 --> 00:06:46,185
Next one.

108
00:06:46,685 --> 00:06:50,765
N. Now the third pillar, and
also most important CO, is

109
00:06:51,275 --> 00:06:53,675
explainable AI or simply Xai.

110
00:06:54,175 --> 00:06:58,830
As AI influences high stakes decisions,
we need to answer why a model made

111
00:06:58,830 --> 00:07:01,740
decision, not just what it decided.

112
00:07:02,550 --> 00:07:07,800
Explainability serves three key needs,
regulators, engineers, stakeholders,

113
00:07:07,800 --> 00:07:12,000
and customers, regulators who require
transparency and auditability.

114
00:07:12,500 --> 00:07:12,860
Engineers.

115
00:07:13,440 --> 00:07:16,405
We are engineers who need
rabbit debugging, tools,

116
00:07:16,795 --> 00:07:18,055
stakeholders and customers.

117
00:07:18,055 --> 00:07:22,175
Most important people who expect
fairness and accountability.

118
00:07:22,835 --> 00:07:26,375
With explainability built
in, we accelerated trust and

119
00:07:26,375 --> 00:07:27,815
troubleshooting at the same time.

120
00:07:28,315 --> 00:07:33,535
Now we'll talk about embedding
explainable AI in ci cd pipelines.

121
00:07:33,655 --> 00:07:36,650
So how do we operate
operation operationalize.

122
00:07:37,225 --> 00:07:40,170
Explainability in DevOps pipelines three.

123
00:07:40,170 --> 00:07:40,710
Ti three.

124
00:07:40,710 --> 00:07:43,545
Like three things that we need to
make sure Predeployment checks,

125
00:07:43,935 --> 00:07:46,185
runtime explanations, explanation.

126
00:07:46,185 --> 00:07:47,975
Monitoring predeployment checks.

127
00:07:48,275 --> 00:07:52,385
It ensures each new model version
maintains stable and interpretable

128
00:07:52,885 --> 00:07:56,545
feature attributions runtime explanations.

129
00:07:57,445 --> 00:08:01,375
This generates justifications
for every live prediction

130
00:08:01,585 --> 00:08:02,960
stored for audit and debugging.

131
00:08:03,460 --> 00:08:04,180
Explanation.

132
00:08:04,180 --> 00:08:08,320
Monitoring tracks the consistency
of explanations over time.

133
00:08:08,680 --> 00:08:12,674
If feature important suddenly
shifts, that's an early wanding

134
00:08:12,674 --> 00:08:14,654
sign of drift or data issues.

135
00:08:14,834 --> 00:08:16,484
This very important monitoring part.

136
00:08:17,024 --> 00:08:21,914
In essence, we treat interpretability as a
first class decision in the CICD process.

137
00:08:22,414 --> 00:08:26,334
So now we talked about reinforcement,
federated, and explainable.

138
00:08:26,754 --> 00:08:28,049
Now getting to the governance.

139
00:08:28,549 --> 00:08:29,329
Which is important.

140
00:08:29,629 --> 00:08:32,569
Governance and observability are
where DevOps principles shine.

141
00:08:33,069 --> 00:08:37,569
Using policy as code, we can
embed compliance rules directly

142
00:08:37,569 --> 00:08:38,890
into deployment pipelines.

143
00:08:39,130 --> 00:08:43,630
Things like modal approval workflows
or explainability thresholds.

144
00:08:43,929 --> 00:08:48,160
Drift dashboards, let us visualize
distributions and prediction

145
00:08:48,160 --> 00:08:52,170
shifts with automated alerts
and rollbacks, progressive roll

146
00:08:52,260 --> 00:08:55,530
rollouts like canneries or AB tests.

147
00:08:55,859 --> 00:09:00,899
Allow us to validate not only
accuracy, but also fairness and

148
00:09:01,199 --> 00:09:03,089
interpretability before full release.

149
00:09:03,660 --> 00:09:07,259
These practices, these three
practices keeps AI releases

150
00:09:07,259 --> 00:09:09,209
both agile and compliant.

151
00:09:09,709 --> 00:09:14,119
So next, talk about reproducible
training and model lineage.

152
00:09:14,619 --> 00:09:19,604
What I mean by that, so Reproductivity
is the foundation of trustworthy ai.

153
00:09:20,134 --> 00:09:20,964
Every dataset.

154
00:09:21,729 --> 00:09:25,419
Hyper parameter and codependency
must be version control.

155
00:09:25,869 --> 00:09:26,799
This is very important.

156
00:09:27,369 --> 00:09:32,649
Training environments should be
containerized and deterministic,

157
00:09:32,709 --> 00:09:37,449
so we can recreate any model on
demand with end-to-end lineage.

158
00:09:37,689 --> 00:09:42,279
We can trace every model from data
injection to deployment, linking

159
00:09:42,369 --> 00:09:45,339
experiments, configurations,
and infrastructures.

160
00:09:46,254 --> 00:09:50,364
This is what makes audits and
root cause investigations actually

161
00:09:50,364 --> 00:09:52,464
feasible in complex AI systems.

162
00:09:52,964 --> 00:09:56,935
So next, this is very important
topic I want to talk about.

163
00:09:56,965 --> 00:09:59,125
The reason is monitoring
meta in production.

164
00:09:59,755 --> 00:10:03,325
A lot of people will make neglect
this one, but I will, I would highly

165
00:10:03,360 --> 00:10:08,425
emphasize on this, even with all that,
AI systems can still face hidden risk.

166
00:10:08,890 --> 00:10:10,150
Three big ones to monitor.

167
00:10:10,690 --> 00:10:14,860
Reward hacking, data
poisoning explanation, drift.

168
00:10:15,520 --> 00:10:20,425
What is reward Hacking where or
agents find loopholes that technically

169
00:10:20,425 --> 00:10:24,715
satisfy their goals, but break
business logic, data poisoning

170
00:10:25,075 --> 00:10:27,085
where federated nodes contribute.

171
00:10:27,585 --> 00:10:34,095
Updates, explanation drift, where
interpreted ability pattern shift

172
00:10:34,335 --> 00:10:37,965
as septal, but powerful indicator
of data or model problems.

173
00:10:38,475 --> 00:10:42,405
By tracking this meta risk, we
move from react to firefighting

174
00:10:42,405 --> 00:10:43,935
by to proactive assurance.

175
00:10:44,324 --> 00:10:46,035
As I mentioned, it's very important.

176
00:10:46,535 --> 00:10:47,314
So next one.

177
00:10:47,525 --> 00:10:47,795
Okay.

178
00:10:48,339 --> 00:10:49,270
Practical blueprint.

179
00:10:49,770 --> 00:10:50,100
Okay.

180
00:10:50,370 --> 00:10:53,969
A lot of people now understood what
is federated learning, explainable

181
00:10:53,969 --> 00:10:56,779
AI and reinforcement learning.

182
00:10:57,259 --> 00:10:58,429
But now where to start?

183
00:10:58,759 --> 00:11:01,429
If you're wondering where to start,
here's a practical blueprint.

184
00:11:01,939 --> 00:11:03,259
Start with pipelines.

185
00:11:03,759 --> 00:11:06,909
Build A-C-I-C-D with
versioning and reproducibility.

186
00:11:06,909 --> 00:11:08,469
First layering.

187
00:11:08,979 --> 00:11:14,109
S integrate ness, drift, and
explainability checks into your gates.

188
00:11:14,609 --> 00:11:18,239
Monitor proactively, as I mentioned
high, I give high importance to this,

189
00:11:18,749 --> 00:11:23,549
include meta risk metrics that that was
discussed in previous slides, not just

190
00:11:23,549 --> 00:11:25,829
performance release with confidence.

191
00:11:26,129 --> 00:11:29,309
Progressive roll rollouts let
you shift safely and quickly.

192
00:11:29,999 --> 00:11:34,589
This stepwise approach, lets
DevOps team bring AI online

193
00:11:34,799 --> 00:11:36,324
without compromising trust.

194
00:11:36,824 --> 00:11:39,434
Closing, unlock the power
of intelligent automation.

195
00:11:39,824 --> 00:11:43,664
To wrap up the AI trust
trade is in just a framework.

196
00:11:43,724 --> 00:11:44,624
It's a mindset.

197
00:11:45,524 --> 00:11:50,084
It helps DevOps team balance
innovation with assurance, shipping,

198
00:11:50,084 --> 00:11:54,434
reinforcement learning, feder
learning, and explainable AI systems

199
00:11:54,434 --> 00:11:57,254
confidently, completely, and at scale.

200
00:11:57,754 --> 00:11:59,435
Thank you all for listening.

201
00:12:00,035 --> 00:12:03,574
I would love to connect afterward
and hear how you are thinking about

202
00:12:03,574 --> 00:12:07,115
AI safety in your DevOps pipelines.

203
00:12:07,175 --> 00:12:12,195
And I also love to hear from your
experience what practices are

204
00:12:12,195 --> 00:12:16,045
you in, like using in in DevOps.

205
00:12:16,345 --> 00:12:16,765
Oops.

206
00:12:17,265 --> 00:12:18,255
Thank you once again.

207
00:12:18,755 --> 00:12:20,314
With that, I would like to sign off.

208
00:12:20,405 --> 00:12:20,825
Thank you.

