1
00:00:00,500 --> 00:00:01,219
Hello everyone.

2
00:00:01,819 --> 00:00:02,960
Thank you for joining today.

3
00:00:03,590 --> 00:00:07,399
We are going to explore something
absolutely foundational to the rapid

4
00:00:07,399 --> 00:00:09,950
expansion of ai, the network fabric.

5
00:00:10,579 --> 00:00:13,489
Before we dive in, let me
quickly introduce myself.

6
00:00:13,760 --> 00:00:18,380
My name is Nam Khan and I'm a solutions
engineer at Cisco on the data center

7
00:00:18,380 --> 00:00:19,880
and the AI infrastructure team.

8
00:00:20,780 --> 00:00:23,600
My job is to make sure the
solutions we build align with our

9
00:00:23,600 --> 00:00:24,740
customer's real business needs.

10
00:00:25,240 --> 00:00:28,810
I've been in the industry for over
23 years working with companies like

11
00:00:28,840 --> 00:00:31,180
Motorola, quest Communications and Cisco.

12
00:00:32,170 --> 00:00:37,030
I hold a dual CCIE certification and
have delivered multiple sessions at

13
00:00:37,030 --> 00:00:39,010
Cisco Life across the US and Europe.

14
00:00:39,510 --> 00:00:41,580
Now, why are we here today?

15
00:00:42,030 --> 00:00:45,510
Because we are living through
one of the biggest infrastructure

16
00:00:45,510 --> 00:00:47,370
shifts in the history of computing.

17
00:00:47,870 --> 00:00:51,380
Every organization is racing
to build or consume AI models.

18
00:00:51,880 --> 00:00:55,630
And the interesting part is
this, the bottleneck in AI isn't

19
00:00:55,720 --> 00:00:57,700
just the GPU speed anymore.

20
00:00:57,910 --> 00:01:01,060
It's how fast those GPUs
can talk to each of them.

21
00:01:01,960 --> 00:01:05,710
You can buy the fastest GPU in the
world, but if they're connected through

22
00:01:05,710 --> 00:01:10,090
a slow or congested network, it's like
driving a Ferrari in a traffic jam.

23
00:01:10,510 --> 00:01:12,670
All that horsepower completely wasted.

24
00:01:13,510 --> 00:01:16,660
So in the session we'll look at
how ethernet, the same technology

25
00:01:16,660 --> 00:01:17,800
that powers the internet.

26
00:01:18,445 --> 00:01:22,735
Has evolved into high performance
fabric for scalable secure

27
00:01:22,975 --> 00:01:24,835
AI training and inference.

28
00:01:25,795 --> 00:01:27,565
Here's a roadmap for our session.

29
00:01:28,045 --> 00:01:34,165
We'll start with the silicon building
blocks, the C-P-U-G-P-U and DPU.

30
00:01:34,315 --> 00:01:37,465
Then we'll look at the different
types of AI clusters and the specific

31
00:01:37,465 --> 00:01:39,235
network requirements for each.

32
00:01:39,775 --> 00:01:42,625
The core of a discussion will
be on network architecture.

33
00:01:42,955 --> 00:01:46,495
Focusing on RDMA, Rocky V two
and Congestion Management.

34
00:01:47,335 --> 00:01:50,425
Finally, we look at the
future with the Ultra Ethernet

35
00:01:50,425 --> 00:01:52,525
Consortium and discuss security.

36
00:01:53,025 --> 00:01:57,765
Let's start by breaking down on what
actually sits in a modern AI server.

37
00:01:58,265 --> 00:02:01,955
It typically contains three
MA major processing units.

38
00:02:02,929 --> 00:02:06,290
The CPU or the Center Processing
Unit, which is the brain of the AI

39
00:02:06,290 --> 00:02:11,450
system, it manages the operating
system, orchestrates data loading

40
00:02:11,750 --> 00:02:14,060
and handles all journal purpose task.

41
00:02:14,839 --> 00:02:18,620
Think of it as a conductor of an
orchestra, making sure everything

42
00:02:18,620 --> 00:02:20,180
stays in sync in harmony.

43
00:02:20,680 --> 00:02:25,180
Next is the GPU or the graphics
processing unit, which can be turned

44
00:02:25,180 --> 00:02:26,860
as the muscle of the AI system.

45
00:02:27,550 --> 00:02:29,560
This is where the heavy lifting happens.

46
00:02:30,024 --> 00:02:34,765
GPUs Excel at massive
parallelism, performing thousands

47
00:02:34,765 --> 00:02:36,144
of metrics multiplication.

48
00:02:36,325 --> 00:02:37,995
One in AI clusters.

49
00:02:38,025 --> 00:02:41,535
GPUs are the most expensive
resources, and everything about the

50
00:02:41,535 --> 00:02:44,055
network is designed for one purpose.

51
00:02:44,415 --> 00:02:45,885
Keep the GPUs busy.

52
00:02:46,605 --> 00:02:50,295
If a GPU is waiting for data,
you're literally burning money.

53
00:02:51,165 --> 00:02:56,715
Finally, the DPU or the data processing
unit is the traffic controller of the

54
00:02:56,715 --> 00:03:02,685
AI system as network scales to 400
gig and 800 gig processing packets.

55
00:03:02,865 --> 00:03:09,545
Eats up CPU cycles, what we call the
infrastructure tax, the DPU offloads

56
00:03:09,545 --> 00:03:14,255
set, tax handling, encryption,
packet inspection and routing.

57
00:03:14,255 --> 00:03:18,875
So the GPU and GPU stay
focused on their workload.

58
00:03:19,375 --> 00:03:24,625
Thinking of DPU as a personal assistant,
making sure the main performance can shine

59
00:03:25,125 --> 00:03:29,535
these three components together form the
backbone of modern AI infrastructure.

60
00:03:30,035 --> 00:03:32,465
So what is an AI cluster?

61
00:03:32,965 --> 00:03:35,275
Isn't it just a rack of servers?

62
00:03:35,775 --> 00:03:40,395
Essentially, it is an interconnected
network of high performance GPUs,

63
00:03:40,575 --> 00:03:42,885
acting as a single supercomputer.

64
00:03:43,385 --> 00:03:46,204
The network here is not just the plumbing.

65
00:03:46,355 --> 00:03:48,484
It is part of the compute fabric.

66
00:03:49,385 --> 00:03:53,524
If the network slows down
the entire cluster stalls.

67
00:03:54,024 --> 00:03:56,964
This brings us two
collective communication.

68
00:03:57,415 --> 00:04:04,105
In standard networking, aox to B. In ai,
a whole group of GPUs need to exchange

69
00:04:04,105 --> 00:04:07,584
data simultaneously to function as a unit.

70
00:04:08,394 --> 00:04:11,334
During training, every
GPU calculates a gradient.

71
00:04:11,769 --> 00:04:15,399
And they must all share that
data to update the model.

72
00:04:16,359 --> 00:04:21,549
We use topology aware algorithms
like rings or trees to ensure

73
00:04:21,609 --> 00:04:23,289
this happens instantly.

74
00:04:23,789 --> 00:04:27,389
If this synchronization lags
training, time explodes.

75
00:04:27,889 --> 00:04:30,824
Now let's distribute between the workload.

76
00:04:31,004 --> 00:04:33,614
Training is where the model learns.

77
00:04:33,794 --> 00:04:35,579
It uses massive data sets.

78
00:04:36,539 --> 00:04:38,129
To teach the model patterns.

79
00:04:38,639 --> 00:04:45,779
It's like teaching a robot to read, walk,
dance, whatever the robot was built for.

80
00:04:46,279 --> 00:04:48,169
Inference is the application phase.

81
00:04:48,259 --> 00:04:53,089
This is when you use the train model
to make predictions or generate text.

82
00:04:53,449 --> 00:04:56,719
It's like asking that robot a
question and getting an answer.

83
00:04:57,219 --> 00:04:59,949
Let's look at the specific
requirement for each of these

84
00:04:59,949 --> 00:05:01,719
clusters as shown in the table.

85
00:05:02,499 --> 00:05:03,219
The bandwidth.

86
00:05:03,939 --> 00:05:09,249
Training requires a high no to node
bandwidth because of that gradient

87
00:05:09,249 --> 00:05:13,950
synchronization, which we talked
about in inference, it's the bandwidth

88
00:05:13,950 --> 00:05:15,809
requirement is relatively lower.

89
00:05:16,309 --> 00:05:19,789
The key metric for the training and
the inferences for the training,

90
00:05:19,789 --> 00:05:24,789
the key metric is how much time
it takes to train the model.

91
00:05:25,449 --> 00:05:28,299
For inference, it is about
latency and higher liability.

92
00:05:28,629 --> 00:05:31,389
So basically, if you think
that you're building a robot.

93
00:05:31,889 --> 00:05:36,629
How much time it takes to build a robot
and how much time it takes to train

94
00:05:36,629 --> 00:05:41,789
the robot so that it can perform the
task, which is, which it's intended for.

95
00:05:42,509 --> 00:05:45,629
Inferencing is just like
executing the robot.

96
00:05:45,629 --> 00:05:51,169
So basically the the end user commands
the robot to walk, dance, or whatever

97
00:05:51,169 --> 00:05:55,885
the purpose it is built for to
execute the purpose it is built for.

98
00:05:56,385 --> 00:05:59,284
Training happens offline, like
when the robot is being built.

99
00:05:59,414 --> 00:06:00,224
It is trained.

100
00:06:00,994 --> 00:06:02,614
It is not available for the user.

101
00:06:02,614 --> 00:06:06,304
It's still being manufactured
by the company or the system,

102
00:06:06,304 --> 00:06:07,234
which is developing it.

103
00:06:08,044 --> 00:06:09,664
Inference is online, right?

104
00:06:09,765 --> 00:06:13,675
The robot is available for
service whenever it's required.

105
00:06:14,175 --> 00:06:17,505
From an infrastructure requirement,
training clusters are massive.

106
00:06:17,715 --> 00:06:22,965
They're like centralized networks because
they require a lot of bandwidth and the

107
00:06:22,965 --> 00:06:27,105
more resources they have, the lesser time
it takes to train a particular model,

108
00:06:28,035 --> 00:06:32,535
influencing clusters, often small and
distributed, something similar to what

109
00:06:32,535 --> 00:06:37,035
we have been using in our data centers,
the regular networks, what we have

110
00:06:37,035 --> 00:06:41,745
been using in the data centers, those
could be used as intrinsic clusters.

111
00:06:42,245 --> 00:06:47,315
So with those differences in mind, let's
talk about what it takes to build a

112
00:06:47,315 --> 00:06:51,635
network that supports these workloads,
specifically a lossless network fabric.

113
00:06:52,565 --> 00:06:57,005
This is where technologies like RDMA,
condition control and specialized

114
00:06:57,065 --> 00:07:02,034
architectures come into play To get the
performance we need for training, we

115
00:07:02,034 --> 00:07:04,549
use RDMA or remote direct memory access.

116
00:07:05,049 --> 00:07:08,320
Standard networking is too slow
because data has to pass through.

117
00:07:08,320 --> 00:07:13,150
The C-P-U-R-D-M-A allows the NIC to
transfer data directly into G p's

118
00:07:13,150 --> 00:07:16,419
memory by passing the CPU entirely.

119
00:07:17,229 --> 00:07:20,890
This gives us the low latency
required for a loss risk fabric.

120
00:07:21,390 --> 00:07:26,819
I often joke that if humans had
RDMA, you could just sit in the room

121
00:07:26,819 --> 00:07:30,330
and by the end of the session, all
my AI networking knowledge would be

122
00:07:30,330 --> 00:07:32,189
copied into your brain instantly.

123
00:07:32,689 --> 00:07:37,909
But for RDMA to work, the network
must act losslessly, meaning no packet

124
00:07:37,909 --> 00:07:40,369
drops even during spikes or congestion.

125
00:07:41,299 --> 00:07:45,709
This requires careful planning around
buffers, queues, and traffic engineering.

126
00:07:46,209 --> 00:07:50,469
Now, we may say we are
using RDMA as a technology.

127
00:07:50,469 --> 00:07:52,034
How about the physical network for ai?

128
00:07:52,944 --> 00:07:54,534
Here is a high level topology.

129
00:07:54,564 --> 00:07:57,654
We typically split the network
into two, which is the front end

130
00:07:57,654 --> 00:07:59,124
network and the backend network.

131
00:07:59,484 --> 00:08:03,234
The front end network is your standard
ethernet network, which you use for your

132
00:08:03,234 --> 00:08:06,744
storage, out of management, user access.

133
00:08:07,414 --> 00:08:09,754
Just the regular data center
which you have been using.

134
00:08:10,205 --> 00:08:13,894
The bank end network is a
place where the magic happens.

135
00:08:14,435 --> 00:08:17,679
This is a dedicated high
speed GPU to GPU fabric.

136
00:08:18,319 --> 00:08:22,819
So any model you're training here,
any robots you're training here, it

137
00:08:22,819 --> 00:08:24,559
has to happen in the backend network.

138
00:08:25,279 --> 00:08:28,759
It is purpose built and it
carries only compute traffic.

139
00:08:29,239 --> 00:08:34,250
The GPU gradients, which you talked about,
the activation, the model TERs, all of

140
00:08:34,399 --> 00:08:36,500
that happens in the backend network.

141
00:08:37,339 --> 00:08:43,719
And of course, since the backend network
uses RDMA because it involves G pt, GP

142
00:08:43,719 --> 00:08:45,939
communication, so it has to be lossless.

143
00:08:46,439 --> 00:08:50,999
Because if any packet drops the
entire collective operation on

144
00:08:50,999 --> 00:08:52,139
the collective communication.

145
00:08:52,319 --> 00:08:57,269
Communication we talked about between
the GPUs will stall or hash to restart.

146
00:08:57,769 --> 00:09:04,244
For the backend network, which runs RDMA,
we have two choices in terms of technology

147
00:09:04,305 --> 00:09:07,214
or in terms of system infrastructure.

148
00:09:07,714 --> 00:09:14,214
Either we could use Infinity Band or we
could use Ethernet Infinity Band have has

149
00:09:14,214 --> 00:09:17,034
been used in high performance computing.

150
00:09:17,064 --> 00:09:21,224
It's a traditional, it's a standard,
but infinity band is propriety.

151
00:09:21,584 --> 00:09:28,184
It is fast, it has better latency
as compared to ethernet, but it is

152
00:09:28,184 --> 00:09:33,074
propriety and all its component, all its
element, the infrastructure to build up.

153
00:09:33,134 --> 00:09:35,210
And InfiniBand, a cluster.

154
00:09:35,270 --> 00:09:36,290
It's expensive.

155
00:09:36,790 --> 00:09:39,430
RDMA is natively supported
on Infiniti Bank.

156
00:09:39,430 --> 00:09:44,849
Like it does not require any tweaks or
adjustments to run RDMA or Infiniti Bank.

157
00:09:44,859 --> 00:09:49,079
You can just directly run
RDMA or Infiniti Band as is.

158
00:09:49,799 --> 00:09:54,719
However, when you talk about ethernet, so
ethernet as we know is our best effort.

159
00:09:54,719 --> 00:09:56,399
Technology, basically.

160
00:09:56,759 --> 00:09:57,814
It's not lossless.

161
00:09:58,769 --> 00:10:00,569
RDME requires lost us.

162
00:10:00,569 --> 00:10:06,829
What we have been talking about and what
we have done to run RDMA or ethernet

163
00:10:06,829 --> 00:10:11,429
is we have developed some industry
standard a particular protocol, which

164
00:10:11,429 --> 00:10:16,399
we're going to talk about in the upcoming
slide, and then we also have to kind

165
00:10:16,399 --> 00:10:21,759
of configure or make the ethernet with
certain condition mechanism to make it.

166
00:10:22,119 --> 00:10:25,810
At on par within InfiniBand without
compromising on its features.

167
00:10:26,139 --> 00:10:28,374
So ethernet as such,
we know is a standard.

168
00:10:29,274 --> 00:10:31,974
Being used widely, very popular.

169
00:10:32,454 --> 00:10:38,624
Most of the folks out there know how to
operate ether, ethernet, the switches, the

170
00:10:38,624 --> 00:10:44,234
infra, the optics, all the infrastructures
are quite standard, and we can run the

171
00:10:44,234 --> 00:10:46,634
entire backend network or e ethernet.

172
00:10:46,935 --> 00:10:48,764
We don't have to change anything except.

173
00:10:49,185 --> 00:10:54,124
For the part where we have to tweak
the RDMA to how to use ethernet.

174
00:10:54,185 --> 00:10:59,215
So we have to customize ethernet for
RDMA and nothing is non-standard.

175
00:10:59,215 --> 00:11:01,235
Everything is quite a standard procedure.

176
00:11:01,735 --> 00:11:07,109
So the big question here, what it takes
to get ethernet on par with Infin Bank.

177
00:11:07,609 --> 00:11:12,744
So the answer is rocky, which is
nothing but RDMA or converge ethernet.

178
00:11:13,254 --> 00:11:17,724
What we do here is we encapsulate
RDMA inside an ethernet frame.

179
00:11:18,594 --> 00:11:23,514
We started with Rocky version one, which
was only limited to layer two domains

180
00:11:23,574 --> 00:11:25,809
because it did not have a UDP IP header.

181
00:11:26,309 --> 00:11:30,540
Then that was slowly upgraded to
Rocky version two, which where

182
00:11:30,540 --> 00:11:32,550
we added the UDP IP headers.

183
00:11:32,920 --> 00:11:36,939
RDMA would be able to route
across layer three networks.

184
00:11:37,600 --> 00:11:42,310
So this is what allows ethernet
to scale AI clusters across racks,

185
00:11:42,310 --> 00:11:44,620
row and entire data center halls.

186
00:11:45,120 --> 00:11:48,210
So let's talk about
more about Rocky V two.

187
00:11:48,710 --> 00:11:56,480
With RO UE two GPU still use RDMA, but
now the RDMA packets ride over ethernet.

188
00:11:57,350 --> 00:12:02,210
The NIC can place data directly into
the GPU memory without involving

189
00:12:02,210 --> 00:12:07,370
the CPU kernel, which cuts latency
from microseconds to nanoseconds.

190
00:12:08,180 --> 00:12:13,010
This is a magic that makes ethernet a
serious contender for AI networking.

191
00:12:13,510 --> 00:12:16,750
Of course, using ethernet
introduces one challenge.

192
00:12:17,250 --> 00:12:20,695
RDA assumes a loss alert
network, which you talked about.

193
00:12:20,995 --> 00:12:26,305
And ethernet is best referred if the
buffer fills up ethernet drop packets.

194
00:12:26,305 --> 00:12:28,285
But RDA cannot tolerate drops.

195
00:12:28,975 --> 00:12:34,045
So introduce two mechanisms here,
starting with the ECN, which is the

196
00:12:34,105 --> 00:12:36,145
explicit condition notification.

197
00:12:37,015 --> 00:12:39,415
ECN does not drop packet insert.

198
00:12:39,880 --> 00:12:40,780
It marks them.

199
00:12:41,410 --> 00:12:45,610
When the switch starts to build
queue pressure, the endpoints then

200
00:12:45,610 --> 00:12:47,950
slow down based on those marks.

201
00:12:48,640 --> 00:12:52,210
This keeps the network stable
and predictable exactly

202
00:12:52,210 --> 00:12:53,830
what AI workloads need.

203
00:12:54,330 --> 00:12:58,545
The second mechanism is PFC
or priority flow control, PFC,

204
00:12:58,694 --> 00:13:03,015
pauses traffic, its specific
priorities when congestion occurs.

205
00:13:03,314 --> 00:13:04,215
This gives us.

206
00:13:04,840 --> 00:13:10,185
Per prietary lossless behavior, even
when multiple traffic types share the

207
00:13:10,185 --> 00:13:16,095
same links, proper tuning of PFC, the
headroom, the pause thresholds, and

208
00:13:16,095 --> 00:13:19,695
the queue management is critical to
deploying Rocky QV two successfully.

209
00:13:20,195 --> 00:13:26,555
Looking ahead, the UAC or the Ultra
Ethernet Consortium Initiative

210
00:13:27,365 --> 00:13:30,920
is an industry group optimizing
ethernet specifically for ai.

211
00:13:31,420 --> 00:13:36,130
They're developing standards for better
congestion control, lower latency, and

212
00:13:36,130 --> 00:13:40,300
brought interoperability to replace
proprietary solutions completely.

213
00:13:40,690 --> 00:13:45,700
So what we are seeing here is UAC
because we know that ethernet in

214
00:13:45,700 --> 00:13:47,650
its own form cannot be used for.

215
00:13:47,900 --> 00:13:53,120
The backend network, what we discussed
about how we use Rocky V two and how

216
00:13:53,120 --> 00:13:54,710
we are using congestion mechanism.

217
00:13:55,040 --> 00:14:01,730
So the goal of the alter Ethernet
consortium team is to ensure that

218
00:14:02,690 --> 00:14:07,720
we develop a standard ethernet or a
modern ethernet, what we call it, or

219
00:14:08,200 --> 00:14:09,910
what they call it as ultra ethernet.

220
00:14:10,170 --> 00:14:15,120
Which will work flawlessly for
building AI backend networks.

221
00:14:15,620 --> 00:14:19,160
Performance is critical,
but so is security.

222
00:14:19,460 --> 00:14:24,860
AI introduces new attack surfaces that
traditions, tools don't always cover.

223
00:14:25,490 --> 00:14:29,300
Some key risk included here,
like I've put it on the slide, is

224
00:14:29,300 --> 00:14:34,100
data poisoning, model extraction,
privacy violations, insider threats.

225
00:14:35,060 --> 00:14:39,710
As AI systems handle more sensitive
and proprietary data, securing

226
00:14:39,710 --> 00:14:44,810
the pipeline just becomes an
important as a accelerating it.

227
00:14:45,310 --> 00:14:50,860
To address these race, we rely
on we many core strategies, which

228
00:14:50,860 --> 00:14:55,380
is like encryption of data and
transit and rest secure designs.

229
00:14:56,145 --> 00:14:57,285
Continuous monitoring.

230
00:14:57,355 --> 00:15:01,825
We monitor the AI network
continuously to di discover any

231
00:15:02,275 --> 00:15:08,445
anomalies in traffic flows or any
anomalies in behaviors of the users.

232
00:15:08,945 --> 00:15:12,995
And then access control,
which is all very important in

233
00:15:12,995 --> 00:15:15,310
aspects of securing our network.

234
00:15:15,810 --> 00:15:19,980
So these guardrails ensure AI
workloads remain secure even

235
00:15:19,980 --> 00:15:21,540
as they scale dramatically.

236
00:15:22,040 --> 00:15:29,000
Talking about dpu, DPU play a huge role
in they enforce these security policies

237
00:15:29,330 --> 00:15:34,420
like encryption and isolation right at
the server edge, operating the work.

238
00:15:34,420 --> 00:15:36,760
So the GPU can focus a
hundred percent on training.

239
00:15:37,480 --> 00:15:42,600
They also help with model partnering
by managing data moment efficiently.

240
00:15:43,100 --> 00:15:44,600
Let's bring everything together.

241
00:15:44,930 --> 00:15:48,770
What we discussed so far
is scalability aspect.

242
00:15:48,920 --> 00:15:52,550
How we are making AI
networks more scalable.

243
00:15:52,970 --> 00:15:56,960
So AI is moving from a single server
to massive clusters like we talked

244
00:15:56,960 --> 00:16:03,770
about, how we require massive amounts
of GPU to cater to the AI demand.

245
00:16:03,770 --> 00:16:04,115
What we have.

246
00:16:04,940 --> 00:16:07,580
So the network must also scale with it.

247
00:16:08,510 --> 00:16:11,630
We discussed about performance, how
ethernet with the Rocky V two can

248
00:16:11,630 --> 00:16:15,500
provide the high throughput, low
latency fabric AI workloads required.

249
00:16:16,000 --> 00:16:20,590
Also, we talked about the UEC or the
Ultra Ethernet consortium initiatives

250
00:16:20,590 --> 00:16:25,415
where they're trying to develop
ultra ethernet, which would work

251
00:16:25,965 --> 00:16:29,115
flawlessly with the AI backend network.

252
00:16:29,615 --> 00:16:33,365
And if you get the network architecture
right, you can build a system that's far

253
00:16:33,365 --> 00:16:36,935
secure and ready for the future of ai.

254
00:16:37,775 --> 00:16:38,795
Thank you for your time.

255
00:16:38,885 --> 00:16:42,605
I hope this was informative, and
I wish you a great confidence.

