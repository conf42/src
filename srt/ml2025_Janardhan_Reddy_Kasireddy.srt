1
00:00:02,100 --> 00:00:06,090
For many of the data engineers,
ETL has been the backbone of

2
00:00:06,090 --> 00:00:07,860
data engineers for decades.

3
00:00:08,490 --> 00:00:11,790
You extract the data,
transform it, and load it.

4
00:00:12,420 --> 00:00:16,920
We have built pipelines, batch
jobs, and data warehouses, but what

5
00:00:16,920 --> 00:00:20,940
I've seen is that data engineering
is just about moving data anymore.

6
00:00:21,720 --> 00:00:25,980
We are being asked to deliver
data faster, more reliable, and

7
00:00:25,980 --> 00:00:27,690
at bigger scale and smarter too.

8
00:00:28,590 --> 00:00:29,790
It's not enough to simply.

9
00:00:30,615 --> 00:00:33,195
Pipeline data into a
warehouse and quality data.

10
00:00:34,125 --> 00:00:38,175
What's happening now is AI and
machine learning are no longer just

11
00:00:38,175 --> 00:00:39,735
a downstream consumers of data.

12
00:00:40,245 --> 00:00:44,625
They're moving up upstream into
our engineering process itself.

13
00:00:45,555 --> 00:00:50,385
AI is becoming embedded inside our
pipelines, detecting lys, generating

14
00:00:50,385 --> 00:00:55,760
transformation logic, auto-scaling
infrastructure, and automatically

15
00:00:56,490 --> 00:00:58,365
automating the data quality checks.

16
00:00:59,025 --> 00:01:04,515
Suddenly we weren't spending
days combining through bad data.

17
00:01:04,545 --> 00:01:08,115
We are getting real time alerts with
anomaly explanation explanations

18
00:01:09,045 --> 00:01:15,705
that freed us up to focus on
deeper improvements rather than

19
00:01:15,705 --> 00:01:17,565
constantly fire fighting data errors.

20
00:01:18,065 --> 00:01:22,955
Before we go deeper into this topic,
let me quickly introduce myself and

21
00:01:22,955 --> 00:01:28,205
why this journey into AI powered data
engineering has been so meaningful for me.

22
00:01:30,184 --> 00:01:35,085
I am Jonathan and I've been working as a
lead data engineer and solutions architect

23
00:01:35,085 --> 00:01:39,945
for over a decade with hands on, hands-on
experience building and modernizing

24
00:01:39,945 --> 00:01:41,835
data platforms at organizations like.

25
00:01:42,434 --> 00:01:45,345
Capital One, FINRA and F-M-C-S-A.

26
00:01:45,914 --> 00:01:50,795
It is an entity of our depart
DOT Department of Transportation.

27
00:01:52,245 --> 00:01:55,725
At Capital One, I was part of a
major modernization effort moving

28
00:01:55,725 --> 00:01:59,360
away from legacy mainframe systems
to micro based microservice based

29
00:01:59,365 --> 00:02:02,975
cloud platform to support credit
card applications like acquisitions,

30
00:02:02,975 --> 00:02:04,975
approvals, and risk decisions.

31
00:02:05,740 --> 00:02:10,329
This gave me firsthand exposure to
transforming complex nested Avro data

32
00:02:10,360 --> 00:02:16,980
into clean analytic ready data mods
while having no while having to deal

33
00:02:16,980 --> 00:02:21,330
with the schema drift, realtime data
ingestion, and the data quality at scale.

34
00:02:22,200 --> 00:02:26,490
And at finra, I have worked with one of
the largest data pipelines I was ever

35
00:02:26,490 --> 00:02:32,130
seen, ingesting over two 50 billion daily
trade events across the US stock market.

36
00:02:32,130 --> 00:02:32,190
It.

37
00:02:33,570 --> 00:02:36,510
We weren't just pro processing
huge volumes of data.

38
00:02:36,510 --> 00:02:41,340
We had strict regulatory timelines
and need and needed zero data loss,

39
00:02:41,340 --> 00:02:43,560
high accuracy and fast availability.

40
00:02:44,490 --> 00:02:50,520
That's where I saw AI and automation
set up in, not just as an add-on for

41
00:02:50,520 --> 00:02:54,300
analysis, but as a core part of the
engineering process, helping us monitor

42
00:02:55,020 --> 00:03:00,510
data quality, detect ly in ingestion
and auto tune processing pipelines.

43
00:03:01,545 --> 00:03:08,505
And most recently at DOT fm F-M-C-S-A,
a late data initiative focused on

44
00:03:08,954 --> 00:03:12,385
safety and compliance reporting
for commercial motor carriers.

45
00:03:14,125 --> 00:03:18,695
And when you are supporting public
safety decisions, you can't afford manual

46
00:03:18,695 --> 00:03:20,315
errors or delay in the data pipeline.

47
00:03:20,345 --> 00:03:25,204
Like here we had to deliver inspection
and violation reports for all 50 states

48
00:03:25,324 --> 00:03:26,884
based on millions of records per month.

49
00:03:27,439 --> 00:03:32,479
From various inspection systems, we
implemented tools like P DQ for automated

50
00:03:32,479 --> 00:03:37,729
data quality checks, and even explored
using ML models to PLA flag suspicious

51
00:03:37,879 --> 00:03:39,289
inspection records in real time.

52
00:03:41,599 --> 00:03:46,734
What I've seen, what I've learned
across the these roles in is that

53
00:03:46,879 --> 00:03:50,479
data engineering is no longer just
about pipelines and warehouses.

54
00:03:50,869 --> 00:03:54,319
It's about reliability,
scalability, and increasing

55
00:03:54,499 --> 00:03:56,119
increasingly about intelligence.

56
00:03:57,119 --> 00:04:01,594
Let's talk about the reality of data
engineering today and why EA has become

57
00:04:01,594 --> 00:04:04,294
such a critical piece of the conversation.

58
00:04:06,434 --> 00:04:11,514
When I first saw these numbers, the
reality hit home 68% of it and engineers.

59
00:04:11,514 --> 00:04:14,094
Time is spent on maintenance
rather than innovation.

60
00:04:15,054 --> 00:04:19,674
And four and a half hours a day is spent
on debugging and fixing pipeline issues.

61
00:04:20,574 --> 00:04:25,314
And 23% of data quality problems
are discovered until they

62
00:04:25,314 --> 00:04:26,484
are already in production.

63
00:04:27,594 --> 00:04:31,854
Honestly, that feels about right
because I've seen, I've been in

64
00:04:31,854 --> 00:04:34,944
those trenches a lot of time.

65
00:04:35,874 --> 00:04:38,874
A lot of time wasn't spent
building new features or

66
00:04:38,874 --> 00:04:40,764
optimizing for business outcomes.

67
00:04:40,794 --> 00:04:44,274
It was spent answering questions
like, why did this pipeline fail?

68
00:04:44,274 --> 00:04:48,384
Now last night, is this a schema
issue or a data quality issue?

69
00:04:49,254 --> 00:04:53,154
Did something happen upstream
change that no one told us about.

70
00:04:54,084 --> 00:04:57,954
And when pipeline breaks,
the pressure builds fast.

71
00:04:58,954 --> 00:05:04,774
Our, for decades, ETL has followed a
very traditional, very rule-based model.

72
00:05:05,114 --> 00:05:09,819
You extract the data from source systems,
you map it to, into a target schema, you

73
00:05:09,819 --> 00:05:14,199
transform it with predefined rules and
tho then load it into a data warehouse.

74
00:05:15,174 --> 00:05:19,884
Sounds simple enough until you are
dealing with hundreds of source systems

75
00:05:19,884 --> 00:05:25,164
constantly evolving schema, dirty
data, and complex transformation logic.

76
00:05:26,094 --> 00:05:29,034
Here is what the traditional
ETL world looks like.

77
00:05:30,864 --> 00:05:34,404
Time consuming manual mapping
between complex data schemes.

78
00:05:35,074 --> 00:05:39,694
A very rigid rule-based transformation
that struggles to adapt to change, like

79
00:05:39,724 --> 00:05:41,649
transformation that are hardcore logic.

80
00:05:41,824 --> 00:05:44,134
Or pre predefined logic.

81
00:05:44,134 --> 00:05:51,574
If X is Y effects, then Y else a Z. But
what happens when upstream data changes?

82
00:05:51,574 --> 00:05:53,974
No new columns, different formats.

83
00:05:54,724 --> 00:05:58,654
These rigid rules break and require
manual inter interventions to patch

84
00:05:58,654 --> 00:06:02,169
them, and reactive error handling.

85
00:06:02,749 --> 00:06:07,090
In traditional world, you don't
find out about errors until

86
00:06:07,280 --> 00:06:08,650
they have been already broken.

87
00:06:09,559 --> 00:06:12,139
Your pipeline are corrupter
corrupted your data.

88
00:06:12,469 --> 00:06:14,809
Data stream data, downstream data.

89
00:06:15,739 --> 00:06:18,379
Either way you are reacting
after the damage is done.

90
00:06:20,089 --> 00:06:26,179
Static performance tuning, and let's
not forget the over suspended, manually

91
00:06:26,269 --> 00:06:31,039
tuning sparko configuration or query
parameters to try and optimize runtime

92
00:06:31,039 --> 00:06:33,019
for a specific data set or a workload.

93
00:06:33,919 --> 00:06:35,179
Once tuned, it's static.

94
00:06:36,964 --> 00:06:41,764
Now compare that with the AI powered ETL
approach, automatic schema discovery,

95
00:06:41,764 --> 00:06:43,804
and intelligent hands-free mapping.

96
00:06:43,804 --> 00:06:49,504
Instead of developers manually mapping
every field, AI algorithms can scan source

97
00:06:49,504 --> 00:06:54,364
data in for data types and recommend or
auto map fields to the target schema.

98
00:06:54,514 --> 00:06:59,994
Even for complex nested J or
evolving source schemas, adapt

99
00:06:59,994 --> 00:07:01,405
to transformation, powered by.

100
00:07:02,704 --> 00:07:04,715
Machine learning rather than static rules.

101
00:07:05,014 --> 00:07:10,145
AI models can learn transformation
patterns from that data, proactive

102
00:07:10,145 --> 00:07:12,634
error de detection and resolution.

103
00:07:12,905 --> 00:07:15,215
AI doesn't wait until the
pipeline fails at runtime.

104
00:07:15,304 --> 00:07:20,164
It proactively analyzes incoming
data before execution, predicts

105
00:07:20,405 --> 00:07:24,184
potential errors and can either
auto resolve or flag issues.

106
00:07:25,054 --> 00:07:28,174
It's a shift from reactive to
predictive pipeline management.

107
00:07:28,674 --> 00:07:33,924
Self optimize, optimizing real time
execution, and perhaps most exciting AI

108
00:07:33,924 --> 00:07:38,215
can monitor job execution in real time,
automatically tuning configurations

109
00:07:38,215 --> 00:07:42,724
like partition partitioning, parallelism
memory allocation based on current

110
00:07:42,724 --> 00:07:44,195
workload and resource conditions.

111
00:07:44,555 --> 00:07:48,575
So you are always operating near
peak efficiency without manual 20.

112
00:07:49,575 --> 00:07:52,365
So beyond just the technical
improvements, let's talk about

113
00:07:52,365 --> 00:07:56,475
the strategic impact AI brings to
ETL and data engineering Overall.

114
00:07:57,705 --> 00:07:58,395
First,

115
00:07:59,395 --> 00:08:00,415
self-optimizing.

116
00:08:00,415 --> 00:08:04,315
Realtime execution means our pipelines
are just on the fly, whether it's

117
00:08:04,465 --> 00:08:09,715
handling sudden data volume spikes, or
adapting to infrastructure bottlenecks.

118
00:08:10,374 --> 00:08:13,224
No more endless manual tuning
or constant babysitting.

119
00:08:13,324 --> 00:08:15,789
But more importantly, it redirect it.

120
00:08:15,875 --> 00:08:17,885
It redirects engineering effect.

121
00:08:17,974 --> 00:08:23,224
Instead of spend spending our days
fixing failed jobs or remapping schemas,

122
00:08:24,844 --> 00:08:28,750
engineers can focus on innovation,
new features and high value of work,

123
00:08:29,370 --> 00:08:31,745
and the numbers speak for themselves.

124
00:08:32,044 --> 00:08:35,375
85% reduction in manual schema
mapping, which speeds up

125
00:08:35,390 --> 00:08:36,949
project timelines dramatically.

126
00:08:38,075 --> 00:08:41,880
92% of data analysts are
intercepted proactively before

127
00:08:41,880 --> 00:08:43,709
they ever reach production reports.

128
00:08:44,640 --> 00:08:48,300
And 43% reduction pipeline
maintenance score freeing up

129
00:08:48,300 --> 00:08:49,829
both budget and bandwidth.

130
00:08:51,089 --> 00:08:54,979
When I think about my experience at
Capital One and capital where even

131
00:08:54,979 --> 00:08:59,689
small data IRS could ripple into
complex risks, having AI practically

132
00:08:59,719 --> 00:09:01,699
catch issues before the surface is in.

133
00:09:02,254 --> 00:09:03,154
Just a time saver.

134
00:09:03,154 --> 00:09:04,354
It's a risk user.

135
00:09:05,764 --> 00:09:07,594
This isn't just about automation.

136
00:09:07,594 --> 00:09:11,554
It's about elevating the entire
role of data engineering into a

137
00:09:11,554 --> 00:09:13,864
more strategic, proactive function.

138
00:09:14,864 --> 00:09:20,494
Oh, first, like AWS Glue, S Serverless,
ETL platform that uses, machine

139
00:09:20,494 --> 00:09:24,004
learning to automatically detect
data schemas and generate mappings.

140
00:09:24,604 --> 00:09:27,814
This drastic drastically reduces
the need for manual coding,

141
00:09:27,814 --> 00:09:32,404
accelerating the time it takes to
integrate and prep, prepare data.

142
00:09:34,174 --> 00:09:38,194
Next Databricks Auto Loader, which
brings intelligent data ingestion

143
00:09:38,194 --> 00:09:39,934
with built-in schema evaluation.

144
00:09:39,934 --> 00:09:44,884
Handling this tool ensures that changing
data structure doesn't break your

145
00:09:44,884 --> 00:09:48,874
pipelines by dynamically adjusting to
new fields or formats as they arrive.

146
00:09:50,074 --> 00:09:52,624
And finally, anomaly detection system.

147
00:09:52,624 --> 00:09:57,304
These leverage machine learning
to continuously scan data

148
00:09:57,304 --> 00:09:59,884
flows for unusual patterns.

149
00:10:00,514 --> 00:10:05,974
They proactively identify anomalies
and potential issues before they impact

150
00:10:06,004 --> 00:10:09,004
business decisions or downstream systems.

151
00:10:10,004 --> 00:10:12,979
If you look at the chart,
you'll see dramatic improvements

152
00:10:12,979 --> 00:10:14,299
across four key areas.

153
00:10:15,809 --> 00:10:19,649
Inquiry query creation time
was cut by more than half.

154
00:10:20,249 --> 00:10:25,769
Debugging time dropped from nearly
90 minutes to about 30, 30 minutes.

155
00:10:26,219 --> 00:10:31,349
Error rates also reduces significantly,
and performance during tuning time was

156
00:10:31,349 --> 00:10:33,164
slash from two hours to under an hour.

157
00:10:33,714 --> 00:10:39,849
Overall, the firms are seeing 65,
60 4% of productivity increase.

158
00:10:39,939 --> 00:10:43,629
This demonstrate how AI not
only speeds a protein task.

159
00:10:44,019 --> 00:10:49,419
But also improves accuracy and performance
in SQL development workflows, or the

160
00:10:49,419 --> 00:10:51,639
transformation development workflows.

161
00:10:52,639 --> 00:10:56,539
This slide shows how AI is
transforming pipeline maintenance

162
00:10:56,749 --> 00:10:58,699
with self-healing data pipelines.

163
00:10:58,789 --> 00:11:03,199
It starts by automatically detecting
anomalies, machine learning models,

164
00:11:03,379 --> 00:11:07,464
fla unusual data patterns before
they snowball into bigger issues.

165
00:11:08,314 --> 00:11:11,504
Then the system moves to diagnose
the issue co using root cause

166
00:11:11,504 --> 00:11:13,634
analysis to classify what went wrong.

167
00:11:14,264 --> 00:11:18,004
Once the cost is are identified,
it moves into the apply fix stage,

168
00:11:18,094 --> 00:11:22,774
pulling an automated correction
from a prebuilt library of solution.

169
00:11:22,774 --> 00:11:24,364
No manual intervention required.

170
00:11:24,424 --> 00:11:25,594
Finally, it enters.

171
00:11:26,479 --> 00:11:29,599
A learn and improve where
the solution is recorded.

172
00:11:29,599 --> 00:11:33,459
So the system keeps getting
smarter and faster at fixing

173
00:11:33,459 --> 00:11:34,749
similar issues in the future.

174
00:11:34,959 --> 00:11:35,439
Future.

175
00:11:36,279 --> 00:11:40,389
This closer loop approach reduces
downtime, boosts reliability, and

176
00:11:40,389 --> 00:11:45,069
shifts engineering engineers away from
firefighting towards strategic work.

177
00:11:46,069 --> 00:11:49,639
We are looking at quality validation
frameworks, which are critical for

178
00:11:49,639 --> 00:11:53,719
maintaining trust in our, in the data
pipelines, especially as we scale with

179
00:11:53,719 --> 00:11:59,449
ai, it starts with defining expectation,
setting rules, either based on domain

180
00:11:59,449 --> 00:12:04,549
expertise or even automatically
discovering patterns from the data itself.

181
00:12:05,599 --> 00:12:11,269
Then we test continuously, not just at the
end, but validating data at every stage.

182
00:12:12,049 --> 00:12:18,109
Like ingestion, transformation and
pre-published, if something fails, those

183
00:12:18,109 --> 00:12:20,419
checks we don't have to jump in manually.

184
00:12:20,599 --> 00:12:25,759
Instead, an auto remediate layer
kicks in, applying machine learning,

185
00:12:25,759 --> 00:12:28,339
recommended fixes for common issues.

186
00:12:30,229 --> 00:12:35,689
And finally, we monitor metrics over
time to track both data quality scores

187
00:12:35,689 --> 00:12:38,149
and how effective our fixes really are.

188
00:12:38,929 --> 00:12:39,859
This process.

189
00:12:40,174 --> 00:12:44,884
Shifts us from react to firefighting
to P to continuous data quality

190
00:12:44,884 --> 00:12:49,774
assurance, something that's incredibly
value in high stakes environment

191
00:12:49,774 --> 00:12:51,994
like a finance or the government.

192
00:12:52,994 --> 00:12:56,324
Let's talk about where we are
and why, where we are heading.

193
00:12:58,064 --> 00:13:02,684
This slides lays out an industry adoption
timeline for ai powder data engineering.

194
00:13:03,674 --> 00:13:09,434
In 2023 to 24, we have seen AI
equal assistance and automated sche

195
00:13:09,434 --> 00:13:13,274
mine inference becoming mainstream
tools in many organizations.

196
00:13:14,084 --> 00:13:19,574
Moving into 24 and 25 were on top
for self-healing pipelines, automated

197
00:13:19,574 --> 00:13:24,129
anomaly detection and correction,
reaching about 60% enterprise adoption.

198
00:13:24,989 --> 00:13:29,294
By 25 and 26, the industry
shifts towards self.

199
00:13:29,864 --> 00:13:33,884
Optimizing systems where infrastructure
dynamically scales and allocate

200
00:13:34,334 --> 00:13:39,284
resources based on business priorities
and looking ahead into 26 and 27.

201
00:13:40,464 --> 00:13:46,194
The big milestone is fully autonomous
data platforms, autonomous data

202
00:13:46,194 --> 00:13:50,784
platform systems that monitor,
monitor, optimize, and remediate

203
00:13:50,784 --> 00:13:52,254
with a minimal human oversight.

204
00:13:53,154 --> 00:13:55,470
The key takeaway, the AI is.

205
00:13:56,289 --> 00:13:58,719
In data engineering isn't
theoretical anymore.

206
00:13:58,869 --> 00:14:03,579
It's a fast moving wave that's
fundamentally changing how we design,

207
00:14:03,909 --> 00:14:06,069
operate, and scale data platforms.

208
00:14:07,069 --> 00:14:10,129
Now that we have seen the potential
of AI in data engineering,

209
00:14:10,159 --> 00:14:13,819
let's talk about how to actually
implement it in a structured way.

210
00:14:15,289 --> 00:14:17,509
This framework has four key steps.

211
00:14:17,509 --> 00:14:18,889
First, assess opportunities.

212
00:14:20,074 --> 00:14:23,104
Look for the RE two hire
first task in your workflows.

213
00:14:23,314 --> 00:14:26,074
This could be manual schema
mapping, tedious data

214
00:14:26,074 --> 00:14:28,534
validation, or re two debugging.

215
00:14:29,074 --> 00:14:30,424
Next, start small.

216
00:14:30,484 --> 00:14:32,404
Don't try to automate everything at once.

217
00:14:34,474 --> 00:14:39,574
Pick one critical pro pipeline or the
process and test a automation there.

218
00:14:40,504 --> 00:14:43,324
Third, measure impact.

219
00:14:43,744 --> 00:14:46,594
Track time, save
improvements in data quality.

220
00:14:47,194 --> 00:14:50,044
And even team satisfaction
to prove the value.

221
00:14:51,154 --> 00:14:58,444
And finally, once you have shown success,
scale gradually expand what work to more

222
00:14:58,444 --> 00:15:05,674
pipelines and parts of your data pla
platform retrospect your like, previous

223
00:15:06,004 --> 00:15:08,984
experience from existing pipelines.

224
00:15:10,094 --> 00:15:11,354
This approach helps.

225
00:15:11,849 --> 00:15:16,109
Reduce risk while ensuring adoption
is sustainable and measurable.

226
00:15:17,109 --> 00:15:18,159
The key takeaways

227
00:15:20,229 --> 00:15:25,179
of this presentation, the first
one is Yay, is transforming ETL.

228
00:15:25,179 --> 00:15:30,609
From manual to autonomous, we are seeing
up to 85% reduction in schema mapping

229
00:15:30,789 --> 00:15:33,939
effort, and 43% cut in maintenance cost.

230
00:15:34,869 --> 00:15:40,269
Second, a self failing pipelines
proactively catch 92% of anomalies

231
00:15:40,269 --> 00:15:42,009
before the cost downstream issues.

232
00:15:42,339 --> 00:15:45,189
That's a huge boost in
data quality and trust.

233
00:15:48,069 --> 00:15:50,410
Third, start small, but start today.

234
00:15:51,099 --> 00:15:55,809
Pick one high effort repeated to process
to automate for quick wins and momentum.

235
00:15:56,559 --> 00:16:00,189
And finally, free your
team for strategic work.

236
00:16:00,219 --> 00:16:00,249
Okay.

237
00:16:00,549 --> 00:16:06,759
Every hour saved on maintenance is an hour
unlock for innovation and business value.

238
00:16:07,759 --> 00:16:08,929
Thank you so much.

239
00:16:09,319 --> 00:16:14,659
Excited to see how we all move forward in
this AI driven future of data engineering.

240
00:16:17,449 --> 00:16:17,779
Thank you so much.

