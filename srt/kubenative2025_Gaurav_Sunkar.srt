1
00:00:00,500 --> 00:00:01,160
Hello everyone.

2
00:00:01,550 --> 00:00:02,479
I'm Gar Siner.

3
00:00:02,840 --> 00:00:06,859
I work at Google Cloud and I'm
excited to talk to you all today about

4
00:00:06,859 --> 00:00:10,790
building intelligent retention engines
that can help with real time churn

5
00:00:10,790 --> 00:00:13,850
detection using AI on Kubernetes.

6
00:00:14,720 --> 00:00:18,650
As part of my prior experience, I have
worked across multiple organizations.

7
00:00:18,875 --> 00:00:20,735
Mainly dig digital marketplaces.

8
00:00:20,735 --> 00:00:24,575
In the past that would
be eBay, Walmart, StockX.

9
00:00:25,115 --> 00:00:31,205
One of the common themes that most of
the digital marketplaces had was trying

10
00:00:31,205 --> 00:00:33,575
to understand and reduce customer churn.

11
00:00:34,415 --> 00:00:35,825
You'd say, why is that important?

12
00:00:36,185 --> 00:00:37,805
I would say for four major reasons.

13
00:00:38,465 --> 00:00:42,425
One, organizations want to make sure
that they're able to increase revenue.

14
00:00:42,980 --> 00:00:46,400
Which is based on customer lifetime
value, which basically goes back

15
00:00:46,400 --> 00:00:49,580
to making sure that the customer
is retained in the system.

16
00:00:50,210 --> 00:00:55,550
Next, they also wanna make sure that the
customer acquisition cost or CAC is low.

17
00:00:55,970 --> 00:00:59,810
It normally costs higher to acquire
a new customer as opposed to

18
00:00:59,810 --> 00:01:01,340
retaining an existing customer.

19
00:01:02,030 --> 00:01:05,810
Third, to make sure that we are
able to have targeted retention

20
00:01:05,810 --> 00:01:11,240
campaigns, having a system that's
able to identify and reduce churn.

21
00:01:11,810 --> 00:01:16,160
Ensures that marketing and customer
success teams can deploy highly targeted

22
00:01:16,160 --> 00:01:21,140
and personalized offers or interventions
instead of broad, un targeted campaigns.

23
00:01:21,590 --> 00:01:22,235
And I would say lastly.

24
00:01:22,940 --> 00:01:24,500
Improved operational efficiency.

25
00:01:24,860 --> 00:01:28,880
A system that's able to automate the
process of identifying at-risk customers

26
00:01:29,210 --> 00:01:34,610
and even trigger automated workflows
can free up bandwidth to focus on more

27
00:01:34,760 --> 00:01:39,500
strategic and important actions that can
evolve the customer journey even further.

28
00:01:40,340 --> 00:01:43,160
Alright, so without further ado,
let's chop straight into it.

29
00:01:44,030 --> 00:01:47,570
My agenda for today is gonna be firstly
talking about the churn challenge,

30
00:01:47,899 --> 00:01:49,715
which will focus on addressing.

31
00:01:50,340 --> 00:01:55,320
Why traditional retention models fail
and what makes them insufficient in

32
00:01:55,320 --> 00:01:57,270
today's dig, digital marketplaces.

33
00:01:58,110 --> 00:02:03,000
Next, we'll touch upon AI power
detection, and how does AI really help

34
00:02:03,240 --> 00:02:05,100
scale up where traditional models fail.

35
00:02:05,910 --> 00:02:09,209
Third, we'll talk the technical
infrastructure where I

36
00:02:09,209 --> 00:02:11,340
detail my proposed solution.

37
00:02:11,760 --> 00:02:14,609
We'll talk about what this means in
terms of successful implementation

38
00:02:14,609 --> 00:02:15,840
and deploying into production.

39
00:02:16,769 --> 00:02:18,930
Fourth, we look into
ethical considerations.

40
00:02:19,670 --> 00:02:26,929
Whenever we talk about AI fairness,
model accuracy, user privacy, consent are

41
00:02:26,929 --> 00:02:29,329
very important terms that come in handy.

42
00:02:29,599 --> 00:02:32,479
And so we need to make sure that
we are able to address those, and

43
00:02:32,479 --> 00:02:34,969
then we finally close and look
at what the future looks like.

44
00:02:35,469 --> 00:02:39,699
All right, so touching upon why
traditional retention models fail.

45
00:02:40,169 --> 00:02:42,749
I would say there's a couple of
reasons why these models aren't

46
00:02:42,749 --> 00:02:46,169
necessarily well set for today's
modern digital environment.

47
00:02:46,739 --> 00:02:49,139
One, they're reactive,
not necessarily proactive.

48
00:02:49,559 --> 00:02:53,849
What this means is by the time they
have processed patterns and got an

49
00:02:53,849 --> 00:02:57,599
output, it's generally too late in
the game and a customer has already

50
00:02:57,599 --> 00:03:01,529
churned and we can't necessarily
do something retroactively next.

51
00:03:01,709 --> 00:03:05,399
They generally are based on core
segmentation logic, which makes sure.

52
00:03:05,889 --> 00:03:10,119
That they are focused on homogeneous
cohorts of customers, but aren't

53
00:03:10,119 --> 00:03:13,539
necessarily able to take in unique
customer characteristics or behavioral

54
00:03:13,539 --> 00:03:15,549
tra trajectories into consideration.

55
00:03:16,329 --> 00:03:20,349
Third, they anomaly are based on
unlimited data, which is generally

56
00:03:20,349 --> 00:03:22,869
transactional, so they are missing.

57
00:03:23,265 --> 00:03:25,575
Attributes such as customer
communications, support,

58
00:03:25,575 --> 00:03:29,394
tickets, even payment requests,
product behavior patterns.

59
00:03:29,634 --> 00:03:32,724
So those are things which I think
are essential to make sure we have a

60
00:03:32,724 --> 00:03:34,404
holistic understanding of the customer.

61
00:03:34,794 --> 00:03:37,464
And lastly, they are
necessarily very flexible.

62
00:03:37,734 --> 00:03:41,454
They have fixed rules and they are
built on very static thresholds

63
00:03:41,934 --> 00:03:46,364
that make it hard to evolve in the
new modern environment to adapt

64
00:03:46,364 --> 00:03:50,564
to marketplace dynamics, even on
different seasonality aspects and

65
00:03:50,564 --> 00:03:52,094
changing customer expectations.

66
00:03:52,594 --> 00:03:55,354
So then talking about how
does AI help in this case?

67
00:03:55,424 --> 00:03:56,624
This is a new age of ai.

68
00:03:56,954 --> 00:03:59,414
I would say there's three major
advantages that AI can offer here.

69
00:03:59,894 --> 00:04:03,064
One instantaneous response times.

70
00:04:03,724 --> 00:04:08,044
So we want models that are able to detect
churn signals and trigger interventions

71
00:04:08,494 --> 00:04:10,174
right when the behavior happens.

72
00:04:10,474 --> 00:04:13,144
And so they need to be
proactive rather than reactive.

73
00:04:13,834 --> 00:04:17,644
Second, we want these models to be very
dynamic, so we wanna make sure that

74
00:04:17,644 --> 00:04:21,004
they're able to continuously learn from
different data streams, they're able

75
00:04:21,004 --> 00:04:25,624
to adapt their detection algorithms and
also evolve to more changing customer

76
00:04:25,624 --> 00:04:27,694
dynamics as well as market dynamics.

77
00:04:28,114 --> 00:04:28,489
And lastly.

78
00:04:28,989 --> 00:04:32,499
These models need to be multi-source
capable, which means that they're

79
00:04:32,499 --> 00:04:36,579
able to look at not just transactional
data, but they're also looking

80
00:04:36,579 --> 00:04:40,719
at support interactions, product
usage metrics, engagement patterns.

81
00:04:40,809 --> 00:04:45,249
So what all this would enable us to do is
to have a very holistic and comprehensive

82
00:04:45,249 --> 00:04:49,974
churn risk assessment, and that's
where it can really scale up versus how

83
00:04:49,974 --> 00:04:51,579
traditional models have normally worked.

84
00:04:52,079 --> 00:04:58,320
Alright talking about the Kubernetes
Native AI Architecture Foundation, right?

85
00:04:58,320 --> 00:05:02,310
Building this real time system
requires a cloud native foundation

86
00:05:02,310 --> 00:05:05,820
that can scale dynamically with
data volume and model complexity.

87
00:05:06,300 --> 00:05:08,760
So this is where
Kubernetes comes in, right?

88
00:05:08,790 --> 00:05:11,040
Kubernetes is the essential
orchestration layer.

89
00:05:11,085 --> 00:05:13,695
For these containerized AI workloads.

90
00:05:14,205 --> 00:05:16,455
Let's break this down
for simpl simplicity.

91
00:05:16,755 --> 00:05:18,765
What do we mean by
containerized AI workloads?

92
00:05:18,765 --> 00:05:20,835
What do we mean by orchestration layer?

93
00:05:21,585 --> 00:05:24,495
Let's think about this, say
like an orchestra, right?

94
00:05:24,495 --> 00:05:27,854
So musical orchestra that most of you
have, or many of us must have attended.

95
00:05:28,424 --> 00:05:31,544
We have different musicians playing
instruments, but then we have a

96
00:05:31,544 --> 00:05:35,055
central conductor who is coordinating
across these different people.

97
00:05:35,655 --> 00:05:38,870
He's making sure that the music
that we listen to is melodious.

98
00:05:39,164 --> 00:05:43,694
Not c He makes sure that when
the drums are playing, the banjo

99
00:05:43,694 --> 00:05:44,834
is probably gonna be silent.

100
00:05:45,164 --> 00:05:49,064
Or when the flute is playing, the guitar
is not gonna be playing at the same time.

101
00:05:49,394 --> 00:05:53,114
So orchestrating these different
aspects, what is an essential

102
00:05:53,294 --> 00:05:55,124
element of hand of a conductor?

103
00:05:55,514 --> 00:05:59,204
So think about Kubernetes as that
conductor in this cloud native

104
00:05:59,204 --> 00:06:03,324
environment which is able to scale,
deploy, and automate different

105
00:06:03,324 --> 00:06:05,184
workflows and different apps.

106
00:06:05,539 --> 00:06:07,129
In this case, different containers.

107
00:06:07,409 --> 00:06:11,429
And it basically is able to make
sure that we have a very scalable

108
00:06:11,429 --> 00:06:14,609
infrastructure to deploy different
machine learning algorithms.

109
00:06:15,109 --> 00:06:20,479
Alright, so then touching upon the
machine learning techniques itself

110
00:06:20,539 --> 00:06:24,619
that I would say are more used, are
more essential for churn detection.

111
00:06:24,619 --> 00:06:27,114
As of today I would say there's three
that I would want to touch upon.

112
00:06:28,034 --> 00:06:31,874
One is recursive or recurrent
neural networks ENSs.

113
00:06:32,354 --> 00:06:37,484
So ENSs are able to process
sequential customer behaviors, right?

114
00:06:37,484 --> 00:06:41,264
They're able to identify patterns
based on a more continuous timeline

115
00:06:41,834 --> 00:06:45,804
and they generally look at certain
aspects that would help flag if

116
00:06:45,804 --> 00:06:47,574
behaviors change in log in frequency.

117
00:06:48,144 --> 00:06:51,714
But then given that they our
focus more on recent behaviors and

118
00:06:51,714 --> 00:06:54,804
necessarily aren't able to track
historical behavior patterns.

119
00:06:55,499 --> 00:06:59,199
We also have L SDMs, which is
long short term memory models.

120
00:06:59,709 --> 00:07:03,189
And how they differ is they're able
to actually capture both immediate

121
00:07:03,609 --> 00:07:05,349
as well as distant behavior signals.

122
00:07:05,589 --> 00:07:08,859
They're able to actually detect
gradual disengagement that can

123
00:07:08,859 --> 00:07:13,389
happen over weeks or even months,
but then bring it all together.

124
00:07:13,449 --> 00:07:16,960
There are ensemble models which are
random forest gradient boostings.

125
00:07:17,530 --> 00:07:21,309
These models actually integrate
deep learning with traditional

126
00:07:21,309 --> 00:07:24,340
machine learning for a much more
comprehensive risk assessment.

127
00:07:24,974 --> 00:07:29,384
So think about this as a customer who
has been logging every week and then

128
00:07:29,384 --> 00:07:31,184
suddenly they don't log in certain weeks.

129
00:07:31,334 --> 00:07:34,844
Trying to understand if that
behavior is normal or truly out

130
00:07:34,844 --> 00:07:37,844
of the ordinary is something that
these models can help detect.

131
00:07:38,344 --> 00:07:42,384
Okay, so the next few slides, I'm actually
gonna touch upon the core technologies

132
00:07:42,384 --> 00:07:45,854
and orchestration that's needed to
implement the system that I'm proposing.

133
00:07:46,149 --> 00:07:49,449
So starting off with the different
data sources and feature engineering

134
00:07:50,019 --> 00:07:53,949
data is an essential element of
any machine learning model, right?

135
00:07:54,309 --> 00:07:56,739
So when we talk about data, what
data are we truly looking at?

136
00:07:57,159 --> 00:08:01,899
We did reference transactional data in
the prior slides, but then trying to

137
00:08:01,899 --> 00:08:07,299
understand session based data the session
duration, frequency feature adoption.

138
00:08:08,169 --> 00:08:11,569
Cart abandonment add to cart
behaviors, interaction with

139
00:08:11,569 --> 00:08:14,689
support tickets, payment typings.

140
00:08:14,839 --> 00:08:18,439
All these, I would say are important
aspects or important data elements that

141
00:08:18,439 --> 00:08:21,949
need to be also captured to make sure
that we are able to really get a holistic

142
00:08:21,949 --> 00:08:23,719
understanding of the customer behavior.

143
00:08:24,219 --> 00:08:27,909
Crucially, we need to also analyze
micro interactions, which could be

144
00:08:27,909 --> 00:08:32,199
click-through rates, navigation patterns,
because these granular signals can

145
00:08:32,289 --> 00:08:36,879
actually often proceed very visible
churn patterns or indicators by weeks

146
00:08:36,879 --> 00:08:40,569
and months, and this really gives us
that intervention window to make sure

147
00:08:40,569 --> 00:08:42,159
we're able to act at the right time.

148
00:08:42,659 --> 00:08:46,089
Talking about the system or the
proposed architecture itself, I would

149
00:08:46,089 --> 00:08:48,829
say there's three major components
that I'm going to touch upon.

150
00:08:49,060 --> 00:08:50,890
Here we're gonna look at Apache Kafka.

151
00:08:51,100 --> 00:08:54,190
We'll talk about Cube flow next,
and then key Native after that.

152
00:08:54,690 --> 00:08:57,190
But then starting here with Apache Kafka.

153
00:08:57,820 --> 00:09:02,950
So Kafka is the heart of the real
time system, and it helps to stream

154
00:09:02,950 --> 00:09:04,780
all customer interaction events.

155
00:09:04,780 --> 00:09:08,200
That could be from mobile apps, it could
be web applications, backend services,

156
00:09:08,200 --> 00:09:10,540
and it helps create a unified event.

157
00:09:10,540 --> 00:09:12,760
Log Kafka Streams.

158
00:09:12,760 --> 00:09:16,750
API is a real time feature
extraction and aggregation system.

159
00:09:17,220 --> 00:09:21,930
That helps detect anomalies and actually
enable pattern recognition based

160
00:09:21,930 --> 00:09:23,700
on how the data is flowing through.

161
00:09:24,180 --> 00:09:28,770
And then these process events and
patterns, trigger model inference requests

162
00:09:29,070 --> 00:09:33,900
that enable instant churn, risk scoring,
and automated intervention workflows.

163
00:09:34,170 --> 00:09:38,130
So again, putting it all together
in the example of the conductor.

164
00:09:38,605 --> 00:09:43,375
Kafka is basically the system that is
enabling, picking up everything that's

165
00:09:43,375 --> 00:09:48,645
happening all around, and making sure that
information is available to the conductor.

166
00:09:48,985 --> 00:09:50,545
As the musicians when needed.

167
00:09:51,045 --> 00:09:51,405
All right.

168
00:09:51,975 --> 00:09:56,064
The next part of this system
is cube flow for machine

169
00:09:56,064 --> 00:09:57,324
learning pipeline orchestration.

170
00:09:57,834 --> 00:10:01,375
Cube Flow is an essential
orchestration framework for managing

171
00:10:01,555 --> 00:10:05,515
complex machine learning workflows
that we had talked about earlier

172
00:10:05,694 --> 00:10:07,824
with the RNN and LSTM models.

173
00:10:08,454 --> 00:10:11,275
Basically think about this as the brain.

174
00:10:11,675 --> 00:10:12,725
Of the engine.

175
00:10:13,085 --> 00:10:20,365
And it enables us to really build very
complex and specific algorithms that

176
00:10:20,365 --> 00:10:24,685
are able to understand and adapt to
different customer behavioral patterns.

177
00:10:25,015 --> 00:10:30,175
The advantage that Q Flow offers
is it is able to automate model

178
00:10:30,175 --> 00:10:35,045
training pipelines that retrain, churn
detection models as new data arrives.

179
00:10:35,345 --> 00:10:37,235
Also, make sure that the
predictions remain accurate.

180
00:10:38,180 --> 00:10:41,240
As the customer behaviors
evolve, and then it does support,

181
00:10:41,240 --> 00:10:44,870
experiment tracking, hyper parameter
tuning, even model versioning.

182
00:10:45,170 --> 00:10:49,690
All of these are critical and
essential aspects that are needed to

183
00:10:49,690 --> 00:10:51,220
maintain a production ready system.

184
00:10:51,760 --> 00:10:56,020
The other thing that it enables us
to do is data scientists can work on

185
00:10:56,590 --> 00:11:01,220
testing and building new algorithms
of frameworks in the backend while

186
00:11:01,370 --> 00:11:02,900
DevOps can actually manage deployment.

187
00:11:03,400 --> 00:11:08,300
And then the third part of
the system, which is K which

188
00:11:08,300 --> 00:11:10,340
enables serverless AI deployment.

189
00:11:10,970 --> 00:11:12,590
Again, what does this truly mean?

190
00:11:12,750 --> 00:11:16,980
So in the example of our orchestra,
although the conductor is the person

191
00:11:16,980 --> 00:11:20,790
who is responsible of making sure
everything works as expected, you

192
00:11:20,795 --> 00:11:22,440
also need a stage manager, right?

193
00:11:22,530 --> 00:11:23,560
What does a stage manager do?

194
00:11:24,180 --> 00:11:27,450
A stage manager is looking at
how the audience is reacting.

195
00:11:27,780 --> 00:11:28,650
Are people leaving?

196
00:11:28,930 --> 00:11:31,000
Do we need the lights to be on or off?

197
00:11:31,060 --> 00:11:32,110
Do we need to make sure that.

198
00:11:32,545 --> 00:11:35,485
A certain musician needs to
go on or go off the stage.

199
00:11:35,875 --> 00:11:40,975
Certain aspects and just ability
to scale based on behavior patterns

200
00:11:41,065 --> 00:11:43,135
is what a stage manager does.

201
00:11:43,435 --> 00:11:47,875
So in this case, K native is that
kind of a stage manager, right?

202
00:11:47,875 --> 00:11:53,875
So it enables automation, deployment,
and even scaling of these workflows

203
00:11:54,325 --> 00:11:56,935
in a very dynamic manner without.

204
00:11:57,430 --> 00:12:00,970
Interaction from a human person, right?

205
00:12:00,970 --> 00:12:06,790
So in a e-commerce kind of environment
or a digital marketplace setup, what this

206
00:12:06,790 --> 00:12:11,590
means is, say on a Black Friday or a Cyber
Monday, you need to make sure all systems

207
00:12:11,590 --> 00:12:13,690
are up and running and able to scale up.

208
00:12:14,200 --> 00:12:17,770
And this needs to be really agile, but
at the same time, on certain days where

209
00:12:17,770 --> 00:12:20,380
there is really less customer interaction.

210
00:12:21,010 --> 00:12:24,300
It should also be able to scale down
and basically be very efficient at it.

211
00:12:24,540 --> 00:12:26,580
So this is where K comes in handy.

212
00:12:27,010 --> 00:12:31,010
K actually integrates pretty
seamlessly with Kafka and it can

213
00:12:31,010 --> 00:12:32,780
trigger model inference functions.

214
00:12:33,020 --> 00:12:35,210
Only even specific behavior events occur.

215
00:12:35,480 --> 00:12:37,490
So this truly makes the
churn prevention system.

216
00:12:38,315 --> 00:12:39,185
Very robust.

217
00:12:39,905 --> 00:12:43,325
Alright, so before we go to the next
slide, I just wanted to summarize

218
00:12:43,385 --> 00:12:44,795
the few things that we talked about.

219
00:12:44,855 --> 00:12:50,645
We looked at Kafka, cube flow, and
K. So again, Kafka is the system

220
00:12:50,645 --> 00:12:52,955
that enables collecting signals.

221
00:12:53,285 --> 00:12:55,685
It could be across different
data sources, clicks, payments,

222
00:12:55,685 --> 00:12:56,945
logins, transaction data.

223
00:12:57,445 --> 00:13:02,035
Kafka Streams, which integrates
with Kafka, helps process patterns,

224
00:13:02,335 --> 00:13:03,715
cleans and aggregates the data.

225
00:13:04,215 --> 00:13:08,865
Cube flow is what enables us
running the AI models LSTM, RNs

226
00:13:08,865 --> 00:13:13,185
ensemble models, and these are run
using cube flow and Kubernetes.

227
00:13:13,695 --> 00:13:17,385
And then finally, K native
enable serverless deployment.

228
00:13:17,635 --> 00:13:22,265
Ability to interact with CRM systems
and make sure that the whole system

229
00:13:22,265 --> 00:13:23,590
works in a very automated manner.

230
00:13:24,090 --> 00:13:28,080
Okay, so let's talk about what
this means in terms of production

231
00:13:28,080 --> 00:13:29,310
infrastructure requirements.

232
00:13:29,340 --> 00:13:33,270
I would say there's three aspects that are
essential having unified data platform.

233
00:13:33,680 --> 00:13:36,530
So this is something that should
enable both batch processing as

234
00:13:36,530 --> 00:13:37,820
well as real-time processing.

235
00:13:38,270 --> 00:13:41,600
And so we need a good unified
customer data platform.

236
00:13:42,290 --> 00:13:47,900
Second is having an API for CR CRM
integration, so restful and GraphQL.

237
00:13:48,400 --> 00:13:50,980
They actually enable real time
synchronization and ensure that

238
00:13:50,980 --> 00:13:54,280
these intervention workflows
have complete customer context.

239
00:13:54,760 --> 00:13:57,880
And then the third thing is
having scalable compute pipelines.

240
00:13:58,240 --> 00:14:03,860
So this helps containers have automatic
resource allocation and scale up or down

241
00:14:03,950 --> 00:14:06,770
as the workload requirements change.

242
00:14:07,340 --> 00:14:09,885
So all these three, I would say essential
elements to make sure we're able to

243
00:14:09,885 --> 00:14:10,845
make this a production ready system.

244
00:14:11,345 --> 00:14:11,675
All right.

245
00:14:11,735 --> 00:14:16,055
So putting this into what it means in
terms of model performance and monitoring.

246
00:14:16,635 --> 00:14:19,725
Although the system should work in
an automated manner, once it's set

247
00:14:19,725 --> 00:14:23,205
up, we need to ensure that we are
able to monitor for model accuracy,

248
00:14:23,505 --> 00:14:25,575
prediction, latency, and even data drift.

249
00:14:26,055 --> 00:14:30,495
The advantages that Kubernetes native
tools like Prometheus and Grafana.

250
00:14:30,990 --> 00:14:33,200
Actually provide these insights upfront.

251
00:14:33,860 --> 00:14:35,450
And to maintain reliability.

252
00:14:35,450 --> 00:14:38,970
We also need to make sure that any
kind of updates that happen need to

253
00:14:38,970 --> 00:14:41,100
be done in a safe and accurate manner.

254
00:14:41,520 --> 00:14:44,940
Again what Kubernetes allows us to
do is to make sure that we're able

255
00:14:44,940 --> 00:14:48,990
to validate any new model versions
against production traffic without

256
00:14:49,020 --> 00:14:50,580
impacting customer experience.

257
00:14:50,880 --> 00:14:53,045
And that's truly where
it sets itself apart.

258
00:14:53,545 --> 00:14:55,405
All right, so bring it all together.

259
00:14:55,565 --> 00:14:59,165
Let's talk about what this means and then
in terms of an implementation roadmap.

260
00:14:59,415 --> 00:15:03,345
So there's four steps to implement the
system in a production environment.

261
00:15:03,825 --> 00:15:06,975
First is the foundation setup, which
is making sure that we are deploying

262
00:15:06,975 --> 00:15:10,575
the Kubernetes clusters with tube
flow, establishing data, pipeline

263
00:15:10,575 --> 00:15:15,265
architecture, and having basic event
streaming with Kafka to make sure that

264
00:15:15,265 --> 00:15:16,770
real time data is getting into the system.

265
00:15:17,270 --> 00:15:20,750
The second aspect, which is basically
having the machine learning model

266
00:15:20,750 --> 00:15:24,510
constructed or created all the system
would eventually run with these models

267
00:15:24,510 --> 00:15:26,490
and automate and update as they go along.

268
00:15:26,970 --> 00:15:30,570
The initial aspect of developing and
training these models using historical

269
00:15:30,570 --> 00:15:34,530
data, implementing feature engineering
pipelines and establishing model

270
00:15:34,530 --> 00:15:39,090
validation frameworks are aspects that
essential in the model development stage.

271
00:15:39,670 --> 00:15:42,280
Once the model is developed, the
production deployment is done

272
00:15:42,280 --> 00:15:44,320
using can, as I mentioned, this is.

273
00:15:44,705 --> 00:15:48,295
Serverless is allows for scalability
and automation of these workflows.

274
00:15:48,625 --> 00:15:50,665
It also helps integrate with CRM systems.

275
00:15:51,025 --> 00:15:53,395
It implements monitoring and
editing for production workflows.

276
00:15:54,055 --> 00:15:57,815
And finally, over a longer term, I
would say optimization and scaling is

277
00:15:57,815 --> 00:16:00,065
an essential component of the system.

278
00:16:00,395 --> 00:16:03,725
So implementing a b testing for
model improvements, enhancing

279
00:16:03,725 --> 00:16:06,575
real-time processing capabilities
and scaling infrastructure

280
00:16:06,575 --> 00:16:07,715
based on production testing.

281
00:16:08,255 --> 00:16:11,925
Are very critical aspects to make
sure this system continues to operate

282
00:16:12,315 --> 00:16:13,605
as expected over the longer term.

283
00:16:14,105 --> 00:16:17,525
Alright, let's talk about model
fairness and transparency.

284
00:16:17,675 --> 00:16:21,215
A truly an efficient system
must also be an ethical one.

285
00:16:21,600 --> 00:16:25,050
So bias detection and ability
to make sure that we are able to

286
00:16:25,050 --> 00:16:26,760
mitigate it is an important aspect.

287
00:16:27,000 --> 00:16:31,230
So we need to regularly audit predictions
across different customer segments

288
00:16:31,500 --> 00:16:35,550
to make sure that there's equitable
treatment and no discriminatory outcomes.

289
00:16:36,190 --> 00:16:37,030
What could this mean?

290
00:16:37,120 --> 00:16:40,660
Say there are certain customers who
don't necessarily log in very frequently.

291
00:16:41,050 --> 00:16:41,555
We don't want to.

292
00:16:42,460 --> 00:16:45,670
Exclude these customers just because
of certain behavioral patterns.

293
00:16:45,880 --> 00:16:49,490
Wanna make sure that it is all
encompassing and able to address

294
00:16:49,490 --> 00:16:50,900
certain aspects of behaviors.

295
00:16:51,650 --> 00:16:55,290
Second, having an explainable
AI implementation, right?

296
00:16:55,290 --> 00:16:59,030
Integrating tools like SHA and Lyme.

297
00:16:59,570 --> 00:17:02,570
They basically provide interpretable
explanation for churn prediction.

298
00:17:03,200 --> 00:17:03,500
Why?

299
00:17:03,500 --> 00:17:06,350
Why this is critical is it
helps customer success teams to

300
00:17:06,350 --> 00:17:08,510
understand if a customer is at risk.

301
00:17:09,125 --> 00:17:13,955
How do they intervene effectively and
finally having good documentation.

302
00:17:13,955 --> 00:17:17,885
So algorithms can run in the backend,
but we need to make sure that there's

303
00:17:17,885 --> 00:17:22,295
enough logging of model decisions and
intervention triggers, both for compliance

304
00:17:22,295 --> 00:17:26,315
purposes as well as to make sure that we
can continuously improve the system and

305
00:17:26,315 --> 00:17:27,800
our retention strategies as we go along.

306
00:17:28,300 --> 00:17:28,660
All right.

307
00:17:28,840 --> 00:17:32,110
Talking further on the aspects of
ethics in terms of user content and

308
00:17:32,110 --> 00:17:36,760
privacy management retention systems
must adhere, I would say, to privacy

309
00:17:36,760 --> 00:17:41,020
by design principles and comply
with regulations like GDPR and CCPA.

310
00:17:41,680 --> 00:17:44,710
Retention is not just about
keeping customers, it's about

311
00:17:44,890 --> 00:17:46,270
actually keeping their trust.

312
00:17:47,150 --> 00:17:50,565
So the ML models need to be flexible
enough to adapt future feature

313
00:17:50,565 --> 00:17:54,015
selection based on individual
customer consent preferences.

314
00:17:54,690 --> 00:17:57,750
This is where, again, Kubernetes
really shines because it can automate

315
00:17:57,750 --> 00:18:02,220
compliance workflows like consent
verification, data retention policies

316
00:18:02,520 --> 00:18:05,190
as part of the ML pipeline in Cube flow.

317
00:18:05,690 --> 00:18:08,630
So what does this all mean
when we talk about the future?

318
00:18:08,780 --> 00:18:12,550
Before we talk about next steps,
I just wanted to recap what we

319
00:18:12,550 --> 00:18:15,490
talked about and give the roadmap
of how this all would come together.

320
00:18:15,800 --> 00:18:19,470
I would say there's four steps to
building the system having the foundation

321
00:18:19,470 --> 00:18:24,460
set up using Kubernetes Kafka and Cube
flow training and deploying our models.

322
00:18:24,550 --> 00:18:30,950
This could be R ns, l sst m. Ensemble
models using again on cube flow and then

323
00:18:30,980 --> 00:18:34,220
production deployment using K native,
making sure there's CRM integration there.

324
00:18:34,580 --> 00:18:38,330
And then optimizing and scaling these
models using AB testing and monitoring

325
00:18:38,330 --> 00:18:39,410
over the longer period of time.

326
00:18:40,140 --> 00:18:44,370
So in summary, I would say intelligent
retention engines are a convergence

327
00:18:44,370 --> 00:18:48,060
of advanced machine learning
techniques or algorithms, cloud native

328
00:18:48,060 --> 00:18:49,680
infrastructure and real time data.

329
00:18:50,340 --> 00:18:53,340
And then by leveraging this
Kubernetes native AI architecture

330
00:18:53,340 --> 00:18:54,300
that I just proposed.

331
00:18:54,900 --> 00:18:57,600
I believe organizations can actually
build systems that are not just

332
00:18:57,600 --> 00:18:59,400
predictive, but truly intelligent.

333
00:18:59,820 --> 00:19:03,690
They are scalable, ethical, and
also responsive, so they're able to

334
00:19:03,840 --> 00:19:08,340
detect churn signals much earlier
before they become irreversible.

335
00:19:08,760 --> 00:19:12,330
So what I just reviewed is essentially
a blueprint for building a super

336
00:19:12,330 --> 00:19:16,230
smart, productive customer service
engine that can run on cloud.

337
00:19:16,680 --> 00:19:21,150
Its main job is to make sure that
customers stop churning or don't leave

338
00:19:21,520 --> 00:19:23,290
before we even know that they're unhappy.

339
00:19:23,620 --> 00:19:26,125
And this can truly help transform
customer tention strateg.

340
00:19:26,495 --> 00:19:27,695
Using Cloud native ai.

341
00:19:28,545 --> 00:19:31,035
So again, thank you so much for your time.

342
00:19:31,285 --> 00:19:33,115
I hope you found this helpful.

343
00:19:33,545 --> 00:19:35,195
It's been a pleasure talking to you all.

344
00:19:35,535 --> 00:19:40,215
If one of you wants to connect with me,
feel free to reach out on LinkedIn, my

345
00:19:40,815 --> 00:19:44,865
at LinkedIn dot coms slash in slash ker.

346
00:19:45,265 --> 00:19:48,025
Or I'd be happy to connect to
discuss further anything at all.

347
00:19:48,425 --> 00:19:49,815
Thanks again and have a great day.

