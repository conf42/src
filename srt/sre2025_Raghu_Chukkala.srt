1
00:00:00,500 --> 00:00:01,190
Hello everyone.

2
00:00:01,250 --> 00:00:03,229
Thank you for being here today.

3
00:00:03,260 --> 00:00:07,760
I'll be presenting on a topic scaling
conversational AI, SRE, challenges

4
00:00:07,760 --> 00:00:09,709
and solution for high availability.

5
00:00:09,709 --> 00:00:10,970
CCA. Systems.

6
00:00:11,470 --> 00:00:15,670
C. CA systems are increasingly used
to handle millions, even billions

7
00:00:15,670 --> 00:00:16,930
of customer interactions every day.

8
00:00:17,915 --> 00:00:22,180
These systems, power virtual
agents, automated chat responses,

9
00:00:22,180 --> 00:00:23,920
voice recognitions, and much more.

10
00:00:24,880 --> 00:00:27,730
With this scale, reliability
becomes absolutely critical.

11
00:00:28,510 --> 00:00:32,560
In this session, we have explored the
unique site Reliability Engineering

12
00:00:32,560 --> 00:00:37,600
Challenges in CCAI and the proven
strategies to overcome them from

13
00:00:37,630 --> 00:00:41,620
observability frameworks and
automated remediation to progressive

14
00:00:41,620 --> 00:00:43,180
rollouts and chaos testing.

15
00:00:44,019 --> 00:00:48,700
I'm excited to share the practical lessons
and battle tested techniques that can help

16
00:00:48,850 --> 00:00:51,610
build more resilient AI driven platform.

17
00:00:52,110 --> 00:00:53,760
The CCA reliability challenge.

18
00:00:54,060 --> 00:00:58,140
Let's start by understanding the scale
and constraints we operate under.

19
00:00:58,140 --> 00:01:03,480
In CCA systems, we aim for 99.99% uptime.

20
00:01:04,110 --> 00:01:07,590
That's just over five minutes
of downtime per month.

21
00:01:08,490 --> 00:01:13,290
This is a non negotiated standard for
enterprise systems, especially when we

22
00:01:13,290 --> 00:01:14,940
are dealing with customer interaction.

23
00:01:15,440 --> 00:01:17,570
The key requirement is latency.

24
00:01:17,960 --> 00:01:21,470
Our target is maximum of 300
milliseconds response time.

25
00:01:22,460 --> 00:01:26,990
This is incredibly tight for
systems powered by heavy NLP models

26
00:01:27,080 --> 00:01:29,030
and multi-service dependencies.

27
00:01:29,530 --> 00:01:35,065
With automation, we've been able to
reduce MTTR mean time to recovery by 73%,

28
00:01:35,715 --> 00:01:40,150
which is a huge win in improving customer
experience and operational efficiency.

29
00:01:40,650 --> 00:01:41,820
Unlike traditional web apps.

30
00:01:42,225 --> 00:01:44,985
Failures in CCA are immediately visible.

31
00:01:45,825 --> 00:01:49,755
They can frustrat customers
cause escalated and directly

32
00:01:49,755 --> 00:01:51,525
impacting business KPIs.

33
00:01:52,025 --> 00:01:57,035
What makes CCI even more challenging
is the unpredictable nature of traffic

34
00:01:57,635 --> 00:02:03,155
and the complex of above integrations
like cms, speech systems, third party

35
00:02:03,155 --> 00:02:08,485
APIs, et cetera, plus the AI model
themselves are not easy to interpret.

36
00:02:09,265 --> 00:02:11,155
They are black boxes in many ways.

37
00:02:11,215 --> 00:02:15,525
This all add ups to a very unique
reliability challenge, which we'll

38
00:02:15,525 --> 00:02:16,815
be discussing in the next slide.

39
00:02:17,315 --> 00:02:19,115
Traditional monitoring limitations.

40
00:02:19,535 --> 00:02:22,655
Traditional monitoring tools were
never built with AI systems in mind.

41
00:02:23,165 --> 00:02:27,665
For instance, transformer based
and LP models, which power intent,

42
00:02:27,665 --> 00:02:31,445
recognition and response generations
are essentially black boxes.

43
00:02:32,075 --> 00:02:34,325
We can't inspect their
internal states easily.

44
00:02:34,825 --> 00:02:38,125
Tools that work well with
microservices often fail here.

45
00:02:39,115 --> 00:02:41,635
Another big problem is load patents.

46
00:02:42,295 --> 00:02:45,655
CCA systems don't follow
typical usage trends.

47
00:02:46,115 --> 00:02:50,825
You might have traffic spikes during
product launches, seasonal sales,

48
00:02:50,825 --> 00:02:55,405
or even random social media trends,
threshold base electing doesn't work

49
00:02:55,405 --> 00:02:59,185
well because it can't predict or
adapt to these irregular patterns.

50
00:02:59,685 --> 00:03:02,025
Then there is issue of
complex dependency chains.

51
00:03:02,025 --> 00:03:06,965
Again, a CA sessions might involve
APIs, databases, voice engines,

52
00:03:07,385 --> 00:03:08,945
third party integrations, et cetera.

53
00:03:09,755 --> 00:03:14,495
A small failure in one backend can
ripple through their entire AI stack,

54
00:03:15,005 --> 00:03:18,785
and traditional monitoring might
not have been able to detect this.

55
00:03:19,285 --> 00:03:24,475
That's why we had to build a new
AI observability model, which will

56
00:03:24,475 --> 00:03:25,765
be discussing in the next slide.

57
00:03:26,265 --> 00:03:28,185
CCA observability framework.

58
00:03:28,665 --> 00:03:33,555
To solve these gaps, we developed a four
layer observability framework tailored

59
00:03:33,555 --> 00:03:39,415
for CCA systems, one business impact
metrics like conversation, completion

60
00:03:39,415 --> 00:03:41,665
rates, and c, a customer satisfaction.

61
00:03:42,115 --> 00:03:44,815
These tell us if users are
getting what they need.

62
00:03:45,685 --> 00:03:48,175
Two model performance metrics.

63
00:03:48,775 --> 00:03:51,805
We track intent, accuracy, fallback rate.

64
00:03:52,195 --> 00:03:53,845
And entity extraction quality.

65
00:03:54,595 --> 00:03:58,105
These gives us insights into
how well the NLP models are

66
00:03:58,105 --> 00:04:00,435
performing system performance.

67
00:04:01,245 --> 00:04:05,535
Here we look at latency, error
rate, throughputs, et cetera.

68
00:04:05,655 --> 00:04:09,945
These are all a traditional ARI
metrics for infrastructure health.

69
00:04:10,545 --> 00:04:14,655
This is about CPU memory,
discus H, and system saturation.

70
00:04:15,155 --> 00:04:18,335
By combining all of these,
we can map a symptom.

71
00:04:18,635 --> 00:04:23,645
Like an increase in fallback response to
its root cause, whether it's a model drift

72
00:04:23,855 --> 00:04:26,105
back in slowdown or infrastructure issue.

73
00:04:26,855 --> 00:04:30,965
This observability model helps
bridge the gap between business

74
00:04:30,965 --> 00:04:32,855
impact and technical telemetry.

75
00:04:33,355 --> 00:04:37,365
Moving on to the next slide,
automated remediation strategies.

76
00:04:37,940 --> 00:04:41,265
Now even with great observability
incidents will happen.

77
00:04:41,445 --> 00:04:44,205
So our next priority is
automated remediation.

78
00:04:45,075 --> 00:04:47,745
Reducing the recovery
time when things go wrong.

79
00:04:48,245 --> 00:04:53,795
Our process starts with anomaly detection
using ML based pattern recognition.

80
00:04:53,975 --> 00:04:58,745
This lets us spot unusual conversation
flows or system metrics in real time.

81
00:04:59,245 --> 00:05:04,075
Then we triage and classify the issue
automatically based on patterns we

82
00:05:04,075 --> 00:05:08,755
learned over time, like intern re
degradation or resource starvation.

83
00:05:09,255 --> 00:05:11,925
Once classified, we trigger
a remediation action.

84
00:05:12,425 --> 00:05:16,445
This might involve scaling up,
rerouting the traffic, throttling,

85
00:05:16,985 --> 00:05:19,115
or even restarting specific services.

86
00:05:19,615 --> 00:05:21,205
And finally, we have a feedback loop.

87
00:05:22,015 --> 00:05:26,455
Each incident teaches a system
something new every time, improving

88
00:05:26,455 --> 00:05:28,450
future detection and resolutions.

89
00:05:28,950 --> 00:05:34,530
This automated approach has helped us
reducing the MTTR by more than 70%.

90
00:05:35,280 --> 00:05:38,129
Which means user barely
noticed something went wrong.

91
00:05:38,629 --> 00:05:41,239
Scaling strategies for peak loads.

92
00:05:41,779 --> 00:05:45,679
Let's talk about scaling,
particularly during peak loads.

93
00:05:46,429 --> 00:05:50,779
Traffic spikes in CCA systems are
hard to predict, but not impossible.

94
00:05:51,499 --> 00:05:58,519
We use predict auto-scaling to anticipate
these based on historical data and events.

95
00:05:59,269 --> 00:06:01,369
Next, we use token based rotting.

96
00:06:02,224 --> 00:06:06,034
A fine-grained way to control
the number of conversations being

97
00:06:06,034 --> 00:06:10,895
processed, preventing overload, while
prioritizing the high value queries.

98
00:06:11,395 --> 00:06:16,195
We also implement conversation shading,
distributing different conversations

99
00:06:16,195 --> 00:06:21,835
to different clusters or chats, so no
single resource pool gets overwhelmed.

100
00:06:22,825 --> 00:06:26,335
And during extreme loads, we
use model optimizations like

101
00:06:26,335 --> 00:06:28,044
switching to lightweight models.

102
00:06:28,674 --> 00:06:31,944
Or reducing precision temporarily
to maintain responsiveness.

103
00:06:32,444 --> 00:06:37,064
All of this helps escape latency
under 300 milliseconds, even

104
00:06:37,064 --> 00:06:39,104
during 10 x traffic bursts.

105
00:06:39,604 --> 00:06:42,550
Moving on to the next
slide, common CCA patents.

106
00:06:43,050 --> 00:06:46,500
Through our operational experience, we
identified several recurring failure

107
00:06:46,500 --> 00:06:51,630
patterns, unique to CCIA, again,
intent classification, degradation.

108
00:06:52,469 --> 00:06:54,390
This often happens gradually.

109
00:06:54,890 --> 00:06:59,840
A new product name or slang throws off
the model and accuracy drops over time.

110
00:07:00,650 --> 00:07:05,719
Latency amplification cascades a
small backend delay gets compounded.

111
00:07:05,719 --> 00:07:09,679
As a AI pipeline processes
laing to exponential latency.

112
00:07:10,179 --> 00:07:11,215
We resource starvation.

113
00:07:11,304 --> 00:07:15,594
A single edge case user query
might consume excessive memory

114
00:07:16,135 --> 00:07:17,724
or CPU affecting all sessions.

115
00:07:18,224 --> 00:07:19,724
Dependency chain failures.

116
00:07:19,994 --> 00:07:24,794
When a single external API or a
backend service goes down, it can take

117
00:07:24,794 --> 00:07:26,715
down the entire conversational flow.

118
00:07:27,494 --> 00:07:31,814
Knowing these patterns helps us
build better detection, alerting,

119
00:07:32,145 --> 00:07:35,444
and chaos testing frameworks, which
will be discussing in the next slide.

120
00:07:35,944 --> 00:07:38,254
Chaos engineering.

121
00:07:38,754 --> 00:07:41,519
This brings us to chaos
engineering, but with a twist.

122
00:07:42,340 --> 00:07:43,869
One that's tailored for ai.

123
00:07:44,369 --> 00:07:48,329
Our goal is to proactively test
what happens when things go wrong

124
00:07:48,329 --> 00:07:51,989
in the AI layer, specifically,
not just infrastructure.

125
00:07:51,989 --> 00:07:56,570
Again, we stimulate scenarios like
corrupted conversation, memory,

126
00:07:57,070 --> 00:08:01,810
misclassified intents in a sequential
order, or even model component

127
00:08:01,810 --> 00:08:03,430
failure due to memory overflow.

128
00:08:03,930 --> 00:08:04,350
Again.

129
00:08:04,350 --> 00:08:07,860
Now the process involves of
performing hypothesis about.

130
00:08:08,295 --> 00:08:14,674
Failure modes, designing targeted
chaos experiments, executing them with

131
00:08:14,674 --> 00:08:18,994
safety controls, and then analyzing
the results and hardening the system.

132
00:08:19,835 --> 00:08:24,544
This AI focused chaos engineering
help us prevent unknown failure modes

133
00:08:24,994 --> 00:08:27,694
from surprising as in a production

134
00:08:28,194 --> 00:08:30,144
progressive deployment models.

135
00:08:31,014 --> 00:08:32,514
All models evolve fast.

136
00:08:32,889 --> 00:08:35,559
But deploying them safely
is also very crucial.

137
00:08:36,219 --> 00:08:39,729
So we use progressive
deployment modes first.

138
00:08:39,939 --> 00:08:44,319
We do shadow testing where a new
model runs alongside production

139
00:08:44,319 --> 00:08:46,089
but doesn't serve real users.

140
00:08:46,569 --> 00:08:48,129
We compare results quietly.

141
00:08:48,819 --> 00:08:51,129
Next we move to a candidate deployment.

142
00:08:51,309 --> 00:08:56,199
We are, a small slices of traffic is
routed to the new model where we monitor

143
00:08:56,199 --> 00:08:59,099
closely again, then we gradually expand.

144
00:08:59,609 --> 00:09:04,349
Traffic based on performance metrics
and confidence level, often adjusting

145
00:09:04,349 --> 00:09:08,389
during business hours to minimize
the impact if anything goes wrong.

146
00:09:08,449 --> 00:09:11,029
We have automated rollback
mechanisms in place.

147
00:09:11,529 --> 00:09:14,859
Once we validated everything,
we move to a full deployment.

148
00:09:15,359 --> 00:09:18,749
This stage rollout ensure we
are innovating safely without

149
00:09:18,754 --> 00:09:20,699
affecting the end user experience.

150
00:09:21,199 --> 00:09:22,489
Moving on to the next slide.

151
00:09:22,989 --> 00:09:25,809
SLIs and SLOs for cost optimization.

152
00:09:26,649 --> 00:09:31,109
Let's now talk about how we balance
performance with cost SLOs or

153
00:09:31,109 --> 00:09:35,729
service level objectives don't
just help ensure reliability.

154
00:09:36,389 --> 00:09:41,269
They also help us avoid over provisioning,
but carefully tracking metrics like

155
00:09:41,689 --> 00:09:47,689
response latency, query throughput,
resource utilization, and intent accuracy.

156
00:09:48,124 --> 00:09:50,314
We can right size our infrastructure.

157
00:09:51,184 --> 00:09:58,204
For example, if a model performs just
as well with 50% fewer GPUs during off

158
00:09:58,204 --> 00:10:00,274
peak hours, that's a massive cost saving.

159
00:10:00,774 --> 00:10:06,894
Across the board, we've seen 30 to 40%
cost optimization by aligning SLOs with

160
00:10:06,894 --> 00:10:09,714
actual user needs and system capacities.

161
00:10:10,214 --> 00:10:11,294
Moving on to the next slide.

162
00:10:12,134 --> 00:10:13,754
Building reliable CCAI.

163
00:10:13,784 --> 00:10:14,954
Key takeaways.

164
00:10:15,454 --> 00:10:20,794
To wrap up, here are the key takeaways
for building reliable CCA systems.

165
00:10:21,544 --> 00:10:25,684
Observability must include AI
specific and business level

166
00:10:25,684 --> 00:10:28,294
metrics, not just CPUs and memories.

167
00:10:28,794 --> 00:10:29,724
Automation is key.

168
00:10:30,054 --> 00:10:32,639
Self feeling systems
drastically reduce downtime.

169
00:10:33,139 --> 00:10:37,604
Use balanced SLOs to avoid
unnecessary cost or refer while

170
00:10:37,604 --> 00:10:39,644
still maintaining a great experience.

171
00:10:40,364 --> 00:10:42,194
Finally, a progressive deployment.

172
00:10:42,989 --> 00:10:46,199
Progressive deployments are very
critical for rolling out changes

173
00:10:46,199 --> 00:10:47,789
without breaking user flows.

174
00:10:48,659 --> 00:10:52,949
These strategies help us build systems
that are both innovative and reliable.

175
00:10:53,729 --> 00:10:56,579
The foundation for enterprise
scale conversational ai.

176
00:10:57,079 --> 00:10:58,999
Thank you all for your time and attention.

177
00:10:59,059 --> 00:11:02,659
I hope the session gave you a useful
insight into unique reliability

178
00:11:02,904 --> 00:11:07,579
challenges of conversational AI systems
and how SRE principles can address them.

179
00:11:08,059 --> 00:11:11,209
I'm happy to take any questions
or feel free to connect with me

180
00:11:11,209 --> 00:11:12,709
later for deeper discussions.

181
00:11:13,279 --> 00:11:17,089
Let's continue building AI that's
not only smart, but also resilient,

182
00:11:17,089 --> 00:11:18,709
scalable, and productivity.

183
00:11:19,429 --> 00:11:20,029
Thank you all.

184
00:11:20,719 --> 00:11:21,019
Bye-bye.

