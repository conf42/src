1
00:00:00,500 --> 00:00:01,460
Hi everyone.

2
00:00:02,420 --> 00:00:03,920
I'm Rich Solinki.

3
00:00:04,040 --> 00:00:07,189
I'm a manager, data
Management in Ally Bank.

4
00:00:07,760 --> 00:00:10,850
Thank you for joining me
today for this presentation.

5
00:00:11,350 --> 00:00:16,390
I will be talking about how we can
transform enterprise data management

6
00:00:16,390 --> 00:00:18,025
using Lakehouse architecture.

7
00:00:18,865 --> 00:00:23,515
A model that combines the best of
both data warehouses and data lakes.

8
00:00:23,965 --> 00:00:27,955
Over the next few minutes, I will
walk you through why this approach

9
00:00:27,955 --> 00:00:32,755
matters, what core principle make
it work, and how it is delivering

10
00:00:32,755 --> 00:00:35,185
the real business impact at scale.

11
00:00:35,685 --> 00:00:38,505
Data explosion, A growing challenge.

12
00:00:38,775 --> 00:00:42,945
As we all know, data is
growing at an incredible pace.

13
00:00:43,214 --> 00:00:51,735
By 2025, global data volumes are expected
to reach 175 terabytes, and almost

14
00:00:51,735 --> 00:00:57,135
one third of that will require real
time processing For enterprises, this

15
00:00:57,135 --> 00:00:59,475
means we are no longer just dealing.

16
00:01:00,219 --> 00:01:05,319
With relational data in clean rows
and columns, we now handle structured,

17
00:01:05,319 --> 00:01:10,090
semi-structured, and unstructured
data coming from countless systems.

18
00:01:10,539 --> 00:01:14,709
These fragmented environments
create serious challenges in

19
00:01:14,709 --> 00:01:19,929
governance, security, and operational
efficiency, making it harder to

20
00:01:19,929 --> 00:01:22,870
deliver timely and trusted insights.

21
00:01:23,370 --> 00:01:27,300
Four years, organizations have
relied on two main approaches, data

22
00:01:27,300 --> 00:01:32,400
warehouses and data lake, and both
have their strengths and weaknesses.

23
00:01:32,910 --> 00:01:37,590
D one on one hand, data
warehouse offers reliability,

24
00:01:37,620 --> 00:01:39,780
consistency, and strong governance.

25
00:01:40,200 --> 00:01:42,030
But they are expensive and rigid.

26
00:01:42,450 --> 00:01:45,480
They struggle with
scalability and flexibility.

27
00:01:45,870 --> 00:01:50,490
Data lakes, on the other hand, are
cheap and highly scalable, but they

28
00:01:50,490 --> 00:01:56,400
often turn into a data swamps with
little governance, unclear ownership,

29
00:01:56,400 --> 00:01:58,770
and no transactional consistency.

30
00:01:59,370 --> 00:02:04,140
So enterprises have been forced to
choose between structure and flexibility,

31
00:02:04,140 --> 00:02:08,520
and that's where the Lakehouse
concept really changes the game.

32
00:02:09,020 --> 00:02:10,790
Lake house architecture.

33
00:02:11,630 --> 00:02:12,890
Bridges the gap.

34
00:02:13,010 --> 00:02:19,130
It combines the reliability and governance
of data warehouses with the scalability

35
00:02:19,130 --> 00:02:21,500
and cost efficiency of data lakes.

36
00:02:21,920 --> 00:02:27,800
With Lakehouse, we can handle diverse
workloads from batch analytics to

37
00:02:27,800 --> 00:02:33,170
realtime streaming and even machine
learning on a single unified platform.

38
00:02:33,710 --> 00:02:36,175
This approach eliminates a trade off.

39
00:02:37,155 --> 00:02:38,535
That used to exist.

40
00:02:38,595 --> 00:02:44,895
It allows us to build one
govern centrally and serve many

41
00:02:44,895 --> 00:02:47,145
different use cases efficiently.

42
00:02:47,645 --> 00:02:51,185
So what are the core capabilities
of like house architecture?

43
00:02:51,425 --> 00:02:55,030
There are four major pillars
that defines the Lakehouse model.

44
00:02:55,990 --> 00:03:01,960
First asset transactions ensure that
we maintain full data integrity even

45
00:03:01,960 --> 00:03:04,360
as we scale to billion of records.

46
00:03:04,860 --> 00:03:09,450
Second schema enforcement
gives us flexibility.

47
00:03:09,950 --> 00:03:14,270
We can support both schema on
read and schema on right models

48
00:03:14,655 --> 00:03:16,735
adopting to evolving business data.

49
00:03:17,235 --> 00:03:22,095
Third is real time Analytics
allows teams to query batch and

50
00:03:22,095 --> 00:03:27,495
streaming data instantly without
building complex ETL pipelines.

51
00:03:27,825 --> 00:03:33,435
And finally, unified Governance provides
a single place for metadata line

52
00:03:33,735 --> 00:03:38,955
tracking and compliance, something that
is critical for enterprise adoption.

53
00:03:39,455 --> 00:03:41,614
Multi zone structure model.

54
00:03:41,674 --> 00:03:46,295
To manage data systematically, we
structure the lakehouse into three

55
00:03:46,295 --> 00:03:50,195
zones, raw, refined, and curated.

56
00:03:50,734 --> 00:03:57,335
The raw zone stores data exactly as
it arrives, preserves the original

57
00:03:57,335 --> 00:03:58,955
format for auditability purpose.

58
00:03:59,455 --> 00:04:03,624
The refined is where
cleansing, validation, and the

59
00:04:03,624 --> 00:04:05,725
standardization of data happens.

60
00:04:06,085 --> 00:04:11,694
Business rules are applied here
and the curated zone contains high

61
00:04:11,694 --> 00:04:16,465
quality analytics ready data sets
for reporting and machine learning.

62
00:04:16,735 --> 00:04:21,019
So basically we are preserving the
data as it is coming in the row.

63
00:04:21,924 --> 00:04:26,515
And then any massaging transformation,
any business rule application will

64
00:04:26,515 --> 00:04:31,645
be taken care as part of refined and
users can use for business analytics.

65
00:04:31,645 --> 00:04:32,574
Curated zone.

66
00:04:32,635 --> 00:04:37,464
This layered design ensures
traceability and promotes data

67
00:04:37,464 --> 00:04:39,204
trust across the enterprise.

68
00:04:39,704 --> 00:04:44,924
So within the lake house, we often
use data vault modeling because it

69
00:04:44,924 --> 00:04:49,904
provides both flexibility and historical
traceability with both of which are

70
00:04:49,904 --> 00:04:53,114
very important thing in modern world.

71
00:04:53,864 --> 00:04:59,024
It is designed for change so we can
involve a schema without breaking

72
00:04:59,024 --> 00:05:01,694
downstream systems In this model.

73
00:05:02,104 --> 00:05:06,784
Hub captures the core business
entities and their unique identifiers.

74
00:05:07,264 --> 00:05:11,824
Link represent the relationship
between those entities and satellite

75
00:05:11,824 --> 00:05:16,504
store descriptive attributes
and full historical context.

76
00:05:16,894 --> 00:05:23,379
So this structure allows us to track
how data changes over time, which is

77
00:05:23,519 --> 00:05:27,004
key for governance and audit compliance.

78
00:05:27,504 --> 00:05:31,854
Scalability means nothing if
performance does not keep up.

79
00:05:32,034 --> 00:05:34,404
So optimization is crucial.

80
00:05:35,304 --> 00:05:42,444
We use column file formats like PERQUE
and ORC, which compress efficiently

81
00:05:42,684 --> 00:05:45,054
and read only the necessary columns.

82
00:05:45,054 --> 00:05:48,909
During a query, we implement
intelligent caching.

83
00:05:49,569 --> 00:05:55,419
For frequently access databases, cutting
down response time for repeated queries.

84
00:05:56,079 --> 00:06:01,299
And through dynamic partitioning,
we organize data by time, geography,

85
00:06:01,359 --> 00:06:06,939
or other dimension, allowing faster
pruning and parallel processing.

86
00:06:07,479 --> 00:06:13,449
Together these techniques give performance
high while maintaining cost efficiency.

87
00:06:13,949 --> 00:06:18,839
Traditional architecture depend
on multiple ETL pipelines.

88
00:06:19,109 --> 00:06:23,879
Data is extracted, transform, and
loaded across several systems.

89
00:06:24,329 --> 00:06:29,219
This creates latency,
complexities and multiple points

90
00:06:29,219 --> 00:06:31,499
of failure in a lake house.

91
00:06:31,854 --> 00:06:37,344
We adopt an ELT approach Instead,
extract and load first, then

92
00:06:37,344 --> 00:06:40,014
transform directly in place.

93
00:06:40,584 --> 00:06:45,655
This means fewer copies, less
maintenance, and faster time to insight.

94
00:06:46,074 --> 00:06:51,684
So by reducing redundant data movement,
teams can focus more on analysis

95
00:06:51,684 --> 00:06:53,784
and less on fixing the pipelines.

96
00:06:54,284 --> 00:07:00,585
Let's talk about automated
governance practices as we scale

97
00:07:00,585 --> 00:07:02,624
governance can't be manual.

98
00:07:02,655 --> 00:07:04,275
It has to be automated.

99
00:07:04,604 --> 00:07:10,155
We use data quality automation to
validate incoming data and flag

100
00:07:10,155 --> 00:07:12,424
anomalies before they affect analytics.

101
00:07:12,815 --> 00:07:18,004
We maintain completely lineage and
audit trails to track how data moves

102
00:07:18,274 --> 00:07:20,555
and transform across the system.

103
00:07:21,315 --> 00:07:24,404
Which simplifies compliance
and troubleshooting.

104
00:07:24,904 --> 00:07:30,334
And with role based access control and
encryption, we ensure that sensitive

105
00:07:30,334 --> 00:07:36,484
data remains protected, which is
still accessible to authorized users.

106
00:07:36,544 --> 00:07:41,224
The proactive governance makes the
system both secure and self-healing.

107
00:07:41,724 --> 00:07:47,785
When organization implement the Lakehouse
model, the results are significant.

108
00:07:48,295 --> 00:07:53,905
Maintenance effort dropped by 60% because
fewer pipelines and tools are needed.

109
00:07:54,624 --> 00:07:57,864
ETL workflows run around 45%.

110
00:07:57,940 --> 00:07:59,710
Faster thanks to in-place.

111
00:07:59,710 --> 00:08:03,910
Transformation and overall
cost of ownership goes down by

112
00:08:03,910 --> 00:08:07,570
roughly 40% beyond the numbers.

113
00:08:07,599 --> 00:08:09,760
The biggest gain is agility.

114
00:08:10,119 --> 00:08:14,140
Teams can deliver insights
faster and respond to business

115
00:08:14,140 --> 00:08:16,420
needs in near real time fashion.

116
00:08:16,920 --> 00:08:21,540
So how does this all come together
in a real world environment?

117
00:08:22,080 --> 00:08:25,769
The architecture usually
has four key layers.

118
00:08:26,129 --> 00:08:31,280
First data ingestion layer that
brings in both batch and streaming

119
00:08:31,280 --> 00:08:33,140
data from different sources.

120
00:08:33,620 --> 00:08:40,280
Second is storage and compute are
decoupled, allowing each to scale

121
00:08:40,280 --> 00:08:42,470
independently based on workload.

122
00:08:42,970 --> 00:08:49,480
Governance and catalog layer maintains
metadata access policies and data lineage.

123
00:08:49,840 --> 00:08:52,840
And finally, the analytics
and conjunction layer.

124
00:08:53,290 --> 00:08:58,780
It provides flexibility for users from
BI tools to data science notebooks.

125
00:08:59,170 --> 00:09:04,450
This end-to-end design gives both
control and freedom to data teams, and

126
00:09:04,450 --> 00:09:07,510
we can use data as a strategic asset.

127
00:09:08,010 --> 00:09:12,000
So in this slide we'll talk about
the implementation considerations.

128
00:09:12,090 --> 00:09:15,360
So for those who are starting
their Lakehouse journey,

129
00:09:15,360 --> 00:09:17,160
a few practical lessons.

130
00:09:17,400 --> 00:09:22,390
First, choose platforms that
support OpenTable formats like data

131
00:09:22,865 --> 00:09:26,105
Lake Apache Iceberg, or Hoodie.

132
00:09:26,195 --> 00:09:27,875
This prevents vendor lock-in.

133
00:09:28,565 --> 00:09:31,355
And future proofed our design.

134
00:09:31,855 --> 00:09:37,315
Second adopter phase migration strategy
begin with known critical workload

135
00:09:37,375 --> 00:09:39,835
to build expertise and confidence.

136
00:09:40,195 --> 00:09:42,895
And third, invest in team training.

137
00:09:43,675 --> 00:09:47,695
Success requires engineers and
analysts to understand both

138
00:09:47,785 --> 00:09:50,095
warehouse and like principles.

139
00:09:50,575 --> 00:09:55,135
It is much about people and
process as it is about technology.

140
00:09:55,635 --> 00:10:02,385
So to wrap up, the lake house architecture
eliminates the longstanding trade off

141
00:10:02,385 --> 00:10:04,815
between data warehouses and data lake.

142
00:10:05,265 --> 00:10:12,315
It provides the governance and reliability
enterprises need, which offering the

143
00:10:12,315 --> 00:10:17,835
flexibility and scalability required
for modern analytics by simplifying

144
00:10:18,795 --> 00:10:20,565
pipelines through multi-zone.

145
00:10:21,300 --> 00:10:23,490
Storage and ELT workflows.

146
00:10:23,490 --> 00:10:27,330
We achieve faster insights
and reduce operational costs.

147
00:10:27,720 --> 00:10:31,800
So ultimately it is about
delivering the measurable business

148
00:10:31,860 --> 00:10:37,890
value, better performance, lower
cost, and trusted data at scale.

149
00:10:38,580 --> 00:10:41,340
Thank you for listening
to this presentation.

150
00:10:41,850 --> 00:10:44,880
Please reach out to me if
you have any questions.

151
00:10:45,390 --> 00:10:46,680
Thank you very much.

