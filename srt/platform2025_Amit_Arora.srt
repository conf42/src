1
00:00:00,780 --> 00:00:01,740
Hello everyone.

2
00:00:01,770 --> 00:00:03,630
My name is Amit Ara.

3
00:00:03,990 --> 00:00:07,680
I'm software engineering manager with
more than 20 plus years of experience.

4
00:00:08,580 --> 00:00:13,710
Today I have pleasure to talk about how
we can engineer a real time translation

5
00:00:13,710 --> 00:00:19,940
platform system that allow truly seamless
communication across languages and

6
00:00:19,940 --> 00:00:24,235
why engineering behind them is just
as important as the AI model themself.

7
00:00:24,735 --> 00:00:29,235
Interconnected world where conversation
happened across borders every second.

8
00:00:29,735 --> 00:00:32,615
IT language is still one of the
biggest barrier to collaboration,

9
00:00:32,795 --> 00:00:34,535
learning, and even safety.

10
00:00:35,035 --> 00:00:39,804
My goal is to show how scalable AI
Infras can bridge that gap and why

11
00:00:39,804 --> 00:00:43,254
translation is moving from being a
convenience to becoming a mission critical

12
00:00:43,254 --> 00:00:44,949
infrastructure for our digital economy.

13
00:00:45,449 --> 00:00:48,210
Okay, so let's first set the stage.

14
00:00:48,710 --> 00:00:53,900
So real time translation platform are
not just research experiment anymore,

15
00:00:54,890 --> 00:00:58,040
they're becoming essential part of
how people communicate globally.

16
00:00:58,540 --> 00:01:00,070
The expectations are very high.

17
00:01:00,570 --> 00:01:03,900
So we are talking about processing
millions of conversation daily.

18
00:01:04,229 --> 00:01:07,049
We have to support more
than 40 plus languages.

19
00:01:07,549 --> 00:01:14,629
We have to hit transition accuracy
above 85% and all of this in.

20
00:01:15,184 --> 00:01:19,824
A very small response time
of under 200 milliseconds.

21
00:01:20,324 --> 00:01:24,524
So traditional translation tool simply
cannot meet the scale and the latency

22
00:01:24,524 --> 00:01:27,754
requirement we're talking about, which
is where the next frontier lies at

23
00:01:27,754 --> 00:01:31,684
the intersection of the advanced AI
models and robust system engineering.

24
00:01:32,184 --> 00:01:36,164
To make sure that we are able to
build this platform must address

25
00:01:36,164 --> 00:01:38,705
four interdependent imperatives.

26
00:01:39,695 --> 00:01:41,345
The first is performance.

27
00:01:41,845 --> 00:01:46,665
The latency has to be very low and
with a very high throughput, users

28
00:01:46,665 --> 00:01:48,765
shouldn't feel any lag in such a system.

29
00:01:49,265 --> 00:01:50,945
The second one is the scalability.

30
00:01:51,035 --> 00:01:55,030
We have to support millions of
concurrence sessions across the globe.

31
00:01:55,530 --> 00:01:56,880
And the system.

32
00:01:56,940 --> 00:01:58,980
And this platform has to be very reliable.

33
00:01:59,520 --> 00:02:02,970
It has to be available even
during failures or heavy loads.

34
00:02:03,510 --> 00:02:10,050
And finally, the accuracy, and it
has to preserve the context, making

35
00:02:10,050 --> 00:02:11,790
sure that meaning is not lost.

36
00:02:12,390 --> 00:02:15,555
People might use idioms,
they might use slang.

37
00:02:16,055 --> 00:02:22,235
They switch languages during mid
sentences or speak in mix of languages.

38
00:02:22,735 --> 00:02:23,785
To build this platform.

39
00:02:23,785 --> 00:02:27,385
The challenge is not to optimize
just one of these things.

40
00:02:27,885 --> 00:02:32,955
All four of these should work together
really well to make sure that platform

41
00:02:32,955 --> 00:02:34,665
is production ready and is usable.

42
00:02:35,165 --> 00:02:36,005
So fun part.

43
00:02:36,035 --> 00:02:37,835
Let's talk about building this one.

44
00:02:37,985 --> 00:02:39,875
So first let's talk
about the architecture.

45
00:02:40,375 --> 00:02:44,065
So the real time translation
systems are typically organized

46
00:02:44,065 --> 00:02:45,685
as the modeler microservice.

47
00:02:46,675 --> 00:02:50,005
Each one does one thing,
but does it really well.

48
00:02:50,505 --> 00:02:53,475
So if we talk about the workflow.

49
00:02:54,075 --> 00:02:57,015
So speech first goes through
your automatic speech

50
00:02:57,015 --> 00:02:58,725
organization to become text.

51
00:02:59,225 --> 00:03:01,864
Then we do pre-processing,
which normalize the text.

52
00:03:02,364 --> 00:03:06,234
After that, we use neural machine
translation, convert insert to

53
00:03:06,234 --> 00:03:10,164
the target language, and then
post-processing cleans up.

54
00:03:10,554 --> 00:03:15,534
And finally we will have text to speech
that generates natural audio output.

55
00:03:16,034 --> 00:03:20,554
So we will keep each of these steps
separate in and so that we can

56
00:03:20,584 --> 00:03:24,814
independently scale, we can upgrade
them, we can monitor these components

57
00:03:24,814 --> 00:03:25,924
without breaking the entire pipeline.

58
00:03:26,424 --> 00:03:31,464
So the modularity is the backbone of the
performance and throughput optimization.

59
00:03:31,965 --> 00:03:35,624
So now coming to the deployment,
how are we gonna do that?

60
00:03:35,684 --> 00:03:43,175
So models are packaged into containers and
we will them by platforms like Kubernetes.

61
00:03:43,675 --> 00:03:46,075
This will give us version control.

62
00:03:46,315 --> 00:03:50,185
We can run domain specific models side
by side with a channel purpose one.

63
00:03:50,935 --> 00:03:53,185
It also enables safe deployments.

64
00:03:54,025 --> 00:03:56,665
We can use bluegreen strategies.

65
00:03:56,845 --> 00:04:01,785
We can run new and old models in
power and roll back instantly.

66
00:04:01,785 --> 00:04:05,794
If something goes wrong, we
can employ Canadian releases.

67
00:04:05,825 --> 00:04:09,454
We gradually rule out updates
to small percent of the users.

68
00:04:09,454 --> 00:04:10,174
See how it.

69
00:04:11,060 --> 00:04:14,949
Does performance wise, and
then if everything is well,

70
00:04:14,949 --> 00:04:16,209
we scale it to everyone.

71
00:04:16,709 --> 00:04:20,759
So this will ensure we never
disrupt mission critical use cases.

72
00:04:21,259 --> 00:04:24,949
So this is very important that centralized
cloud processing is not enough.

73
00:04:25,039 --> 00:04:32,220
Imagine a user in India or Africa
need, they need a sub 200 millisecond

74
00:04:32,220 --> 00:04:36,000
latency, and if you're sending
everything back to US data center,

75
00:04:36,000 --> 00:04:37,680
we'll never meet our requirements.

76
00:04:38,430 --> 00:04:41,130
So that's where edge computing comes in.

77
00:04:42,090 --> 00:04:45,960
We will deploy all these moder
components, A SR and MT ETTS

78
00:04:45,960 --> 00:04:47,760
components closer to the users.

79
00:04:48,260 --> 00:04:50,420
We can reduce the latency by 40%.

80
00:04:50,480 --> 00:04:54,410
We can cut our upstream
bandwidth usage by almost 60%.

81
00:04:54,910 --> 00:04:57,910
And most importantly, we keep the
sensitive data within the local

82
00:04:57,910 --> 00:05:02,770
judication, which helps with the
regulatory compliance each country has.

83
00:05:03,760 --> 00:05:08,124
So if you think about this, edge
computing isn't optional or nice

84
00:05:08,124 --> 00:05:12,499
to have for the system, it's a
fundamental part of the system.

85
00:05:12,999 --> 00:05:15,590
So next is data and hardware.

86
00:05:16,129 --> 00:05:20,030
On the data side, we use streaming
pipeline like Kafka and flank for

87
00:05:20,030 --> 00:05:24,109
load latency, ingestions and batch
pipelines for retraining and analytics.

88
00:05:24,609 --> 00:05:29,379
So on the hardware side, the GPUs
are the bottlenecks, so we have to

89
00:05:29,379 --> 00:05:33,839
make sure that we do efficient GPU
scheduling so the multi multiple

90
00:05:33,839 --> 00:05:35,489
models don't collide during peak loads.

91
00:05:35,989 --> 00:05:42,519
And we use caching aggressively to,
for the common phrases, domain specific

92
00:05:42,669 --> 00:05:47,049
terms, user dictionary to speed up
translation while improving accuracy.

93
00:05:47,889 --> 00:05:50,684
This combination keep the system
both fast and the cost Absent

94
00:05:51,184 --> 00:05:55,324
now, like none of this will
matter if you don't know what's

95
00:05:55,324 --> 00:05:56,554
happening in your production system.

96
00:05:57,394 --> 00:06:02,254
So telemetry is a nervous system, is the
most important part of your platform.

97
00:06:03,064 --> 00:06:05,674
We have to track metrics like latency.

98
00:06:05,854 --> 00:06:09,184
We should track throughput,
accuracy, error rates.

99
00:06:09,684 --> 00:06:11,484
We use dis tracing to pinpoints.

100
00:06:11,484 --> 00:06:13,944
What are the bottlenecks
across microservices.

101
00:06:14,184 --> 00:06:17,214
And we then we feed this user
correction back into the system

102
00:06:17,214 --> 00:06:18,714
to improve accuracy over time.

103
00:06:19,214 --> 00:06:19,419
We have.

104
00:06:20,174 --> 00:06:23,324
For full tolerance, we design
for ancy across regions.

105
00:06:23,324 --> 00:06:27,314
We allow graceful degradation by
falling back to simpler models.

106
00:06:27,314 --> 00:06:32,164
When cheap use are scars, we build
self-healing, automated restart load

107
00:06:32,164 --> 00:06:33,905
balancing when the service fails.

108
00:06:34,594 --> 00:06:36,184
So residency has to.

109
00:06:36,994 --> 00:06:41,944
Be engineered, designed, and
from the start of the platform,

110
00:06:41,944 --> 00:06:43,174
we have to incorporate those.

111
00:06:43,174 --> 00:06:45,179
And it doesn't have to,
it's not accidental.

112
00:06:45,179 --> 00:06:46,799
So it has to be very intentional.

113
00:06:47,129 --> 00:06:51,289
It has to be in the start of the
system and it has to engineer

114
00:06:51,289 --> 00:06:52,809
it from in the whole platform.

115
00:06:53,309 --> 00:06:56,714
So another fun thing let's look
at some of the real world impact.

116
00:06:57,489 --> 00:07:00,279
Of this real time translation platforms.

117
00:07:00,819 --> 00:07:03,479
So in healthcare if there's
a miscommunication between

118
00:07:03,479 --> 00:07:06,119
provide and patient, it can be
a matter of life and a death.

119
00:07:06,619 --> 00:07:10,789
So a hospital system deployed domain
specific medical translation models

120
00:07:10,789 --> 00:07:12,619
at the edge nodes inside hospitals.

121
00:07:13,565 --> 00:07:19,264
And as far as the impact is concerned,
a 35% reduction in critical medical

122
00:07:19,264 --> 00:07:23,284
communication errors that directly improve
patient safety and treatment outcomes.

123
00:07:23,884 --> 00:07:26,840
This is a clear example where
translation literally saves life.

124
00:07:27,339 --> 00:07:27,849
No.

125
00:07:27,899 --> 00:07:30,389
We all have seen this one as education.

126
00:07:31,029 --> 00:07:34,479
In online classroom we have
diverse student population.

127
00:07:34,479 --> 00:07:37,450
They often of, they often struggle
with the language barriers.

128
00:07:37,899 --> 00:07:42,159
So by integrating real time captioning
and translation into the video

129
00:07:42,159 --> 00:07:44,424
conferencing platforms, students are.

130
00:07:45,175 --> 00:07:49,344
Were able to fully participate
regardless of the native language.

131
00:07:49,865 --> 00:07:53,415
The results were more inclusive
accessible learning environment,

132
00:07:53,685 --> 00:07:56,835
and there's no student left behind
because of the language barriers.

133
00:07:57,335 --> 00:07:59,705
So finally coming to the
enterprise Collaboration.

134
00:07:59,975 --> 00:08:03,215
So global team lose efficiency
to the language caps.

135
00:08:03,915 --> 00:08:09,805
You're having meeting across international
teams by embedding, so by embedding

136
00:08:10,105 --> 00:08:13,285
real time translation directly into
the platforms like Teams and Slack.

137
00:08:13,855 --> 00:08:17,245
Your organization can see more
than 40 plus improvements in

138
00:08:17,245 --> 00:08:18,655
collaboration effectiveness.

139
00:08:19,075 --> 00:08:22,225
This can translate to faster
decisions, fewer misunderstanding

140
00:08:22,225 --> 00:08:25,825
that happens, and better outcomes
for international projects.

141
00:08:26,325 --> 00:08:31,985
So from these use cases and from the the
previous slides and previous experiences.

142
00:08:32,405 --> 00:08:34,745
So these are the best
practices that emerge.

143
00:08:35,245 --> 00:08:37,405
First is like you design for ity.

144
00:08:38,095 --> 00:08:41,435
You always do one thing and
you do one thing really well.

145
00:08:41,885 --> 00:08:46,385
You're able to independently
scale individually update that.

146
00:08:46,385 --> 00:08:50,465
Components like separate a SR
and mt TTS we talked about.

147
00:08:50,965 --> 00:08:53,950
Here's hybrid cloud strategy as well as.

148
00:08:54,670 --> 00:08:58,330
Edge architecture, which is a must
have for real time translation system.

149
00:08:58,830 --> 00:09:01,950
You automate your deployments using CICD.

150
00:09:02,450 --> 00:09:06,150
You have to prioritize, and
that's a must have observability.

151
00:09:06,420 --> 00:09:10,650
With the full tracing and feedback
loops, you have to plan it and see

152
00:09:10,650 --> 00:09:12,810
from the start assume this will happen.

153
00:09:13,600 --> 00:09:17,050
Plan for it and make
sure it's implemented.

154
00:09:17,550 --> 00:09:19,290
Like GPU scar.

155
00:09:19,320 --> 00:09:23,520
So you have to optimize for GP usage,
you have to aggressively use caching.

156
00:09:24,020 --> 00:09:26,810
And I think one of the most
important thing is you have to

157
00:09:26,810 --> 00:09:29,210
support contextual translations.

158
00:09:29,710 --> 00:09:33,090
Because the meaning matters more
than the literal words substitutions.

159
00:09:33,630 --> 00:09:37,260
So at the high level, these are
the principle that make translation

160
00:09:37,470 --> 00:09:40,050
platform reliable at scale.

161
00:09:40,550 --> 00:09:42,140
Okay, future direction.

162
00:09:42,640 --> 00:09:44,680
Looking ahead, the
possibilities are exciting.

163
00:09:45,180 --> 00:09:49,770
Imagine we can build personalized
translation models, which are tuned

164
00:09:49,775 --> 00:09:51,835
to individual users and organizations.

165
00:09:52,335 --> 00:09:52,715
We can.

166
00:09:53,320 --> 00:09:56,730
Create those context aware
nmt that uses conversation

167
00:09:56,730 --> 00:09:58,890
history to improve the fidelity.

168
00:09:59,390 --> 00:10:03,050
We can have the ed learning at the edge
that allow training without sending

169
00:10:03,050 --> 00:10:07,770
the con the sensitive data, which
you don't want to send to the cloud.

170
00:10:08,270 --> 00:10:09,980
We can have multi moderate translation.

171
00:10:10,070 --> 00:10:15,605
We can combining speech, text, even visual
will unlock such a richer understanding.

172
00:10:16,105 --> 00:10:20,155
And we must prioritize sustainability,
optimizing g pure workloads,

173
00:10:20,155 --> 00:10:22,195
full lower energy consumptions.

174
00:10:22,555 --> 00:10:27,685
So the future of translation is
intelligent, adaptive, and green.

175
00:10:28,185 --> 00:10:30,500
So let's bring it all together.

176
00:10:31,000 --> 00:10:34,660
So engineering realtime translation
platform is both an AI challenge

177
00:10:34,780 --> 00:10:36,760
and a system engineering challenge.

178
00:10:37,420 --> 00:10:44,560
It requires low latency, pipelines fault
and architecture, T resource management,

179
00:10:44,710 --> 00:10:47,290
robust observability when done right.

180
00:10:47,290 --> 00:10:52,330
These platforms saves life in
healthcare, democratize educations,

181
00:10:52,330 --> 00:10:53,800
and ate global collaboration.

182
00:10:54,300 --> 00:10:58,620
So overall translation is not just
convenience anymore, it's infrastructure.

183
00:10:59,120 --> 00:10:59,780
For digital,

184
00:11:00,280 --> 00:11:01,540
thank you so much.

