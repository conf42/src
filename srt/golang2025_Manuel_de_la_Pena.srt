1
00:00:00,150 --> 00:00:01,680
Hi, I'm Manuel de Rap Pena.

2
00:00:01,740 --> 00:00:03,750
I am a staff software engineer at Docker.

3
00:00:04,410 --> 00:00:06,600
I'm the test container, go
maintainer, which is the library

4
00:00:06,600 --> 00:00:07,530
I'm going to present today.

5
00:00:08,030 --> 00:00:13,430
Argo applications usually consume external
resources such as SQL or no SQL databases.

6
00:00:13,550 --> 00:00:17,870
Message queues for sending US SQL
messages, L up, or mail servers,

7
00:00:18,620 --> 00:00:20,925
cloud services such as S3 or a w.

8
00:00:21,125 --> 00:00:24,980
S Landas and more recently, vector
databases for AI applications.

9
00:00:25,480 --> 00:00:27,640
How do you make sure the
application code interacting

10
00:00:27,640 --> 00:00:29,110
with those services is correct?

11
00:00:29,140 --> 00:00:33,280
When you are writing it, you have the
right tools to test your application

12
00:00:33,280 --> 00:00:38,140
while developing it so that you have
confidence in creating the core behaviors.

13
00:00:39,010 --> 00:00:43,000
One of the most adopted techniques is
using a continuous integration server

14
00:00:43,180 --> 00:00:48,010
such as Jenkins, GitLab runners, or
GI actions in which you or a platform

15
00:00:48,010 --> 00:00:51,940
team configures the servers to run all
your dependencies in the CI pipelines.

16
00:00:52,440 --> 00:00:56,460
At some point, you must make sure that
those dependencies, databases, queues,

17
00:00:56,460 --> 00:01:01,050
code, tors, et cetera, are in exactly
the same version as in production with

18
00:01:01,050 --> 00:01:02,520
the same configuration if possible.

19
00:01:03,360 --> 00:01:06,630
Also, the CA workers must be big
enough to start and run those

20
00:01:06,630 --> 00:01:09,930
dependencies at the same time your
test codes executed in the test.

21
00:01:10,500 --> 00:01:13,850
My question is this kind of setup
convenient for making progress?

22
00:01:14,570 --> 00:01:17,120
Every time you need to update
the dependencies, you need

23
00:01:17,120 --> 00:01:18,675
to update the CI confi.

24
00:01:19,130 --> 00:01:22,280
And many times as a developer,
you don't own the CI service.

25
00:01:23,270 --> 00:01:27,170
On the same hand, when your application
grows and you depend on more services,

26
00:01:27,380 --> 00:01:30,920
you have to start them on the
ci, increasing the probability of

27
00:01:30,920 --> 00:01:35,150
overloading the CI server with concurrent
execution of different CI pipelines.

28
00:01:36,050 --> 00:01:39,440
Finally, how can you reproduce a
failing test in your local machine?

29
00:01:39,440 --> 00:01:42,800
In that scenario, do you need to
start the full blown application

30
00:01:42,800 --> 00:01:44,480
to run just one failing test.

31
00:01:44,980 --> 00:01:48,460
Enter test containers for Go test
containers is a set of open source

32
00:01:48,460 --> 00:01:51,640
libraries that are present in the
most popular programming languages.

33
00:01:52,210 --> 00:01:54,850
I am the official maintainer for
the GO implementation and I'm

34
00:01:54,850 --> 00:01:57,700
going to show you how to use the
library to resolve the issues.

35
00:01:57,700 --> 00:01:58,750
I described it before.

36
00:01:59,560 --> 00:02:02,470
First test containers for Go gives
you programmatic access to the

37
00:02:02,470 --> 00:02:07,030
Docker engine, so you are not able
to create, start, stop, and terminate

38
00:02:07,030 --> 00:02:08,470
containers that in your code.

39
00:02:09,340 --> 00:02:12,610
Instead of configuring the ci, you'll
be adding the containers to the place

40
00:02:12,610 --> 00:02:13,870
they're using in your test code.

41
00:02:14,370 --> 00:02:17,820
Imagine you're working in the persistent
layer instead of spinning up the

42
00:02:17,820 --> 00:02:22,050
entire application to run this layers
test, you just add an instance of your

43
00:02:22,050 --> 00:02:25,770
database where you really need it, so
your test will interact with just a

44
00:02:25,770 --> 00:02:30,870
container representing database test
give you a random port for the well

45
00:02:30,870 --> 00:02:32,340
known port for a given technology.

46
00:02:32,790 --> 00:02:35,190
For Postgres, it is the 5, 4, 3 2 port.

47
00:02:35,265 --> 00:02:37,635
For MySQL, the 3 3 3 0 6 port.

48
00:02:38,295 --> 00:02:42,195
This allows you to run multiple containers
in parallel with three major benefits.

49
00:02:42,524 --> 00:02:46,964
Consistency, test that isolation
and bill speed consistency.

50
00:02:46,964 --> 00:02:50,595
Thanks to docker containers, the
dependency will be isolated in a

51
00:02:50,595 --> 00:02:54,315
container, so you don't need to mess
up your system installing different

52
00:02:54,315 --> 00:02:59,204
technologies with multiple configurations
and apply only to your host instead

53
00:02:59,204 --> 00:03:02,505
of creating an onboarding document
with lot of instruction to set up

54
00:03:02,505 --> 00:03:04,029
the development or test environment.

55
00:03:04,560 --> 00:03:06,600
Do as install docker and run the test.

56
00:03:06,899 --> 00:03:10,640
We call it the clone and Run
experience about that installation.

57
00:03:10,700 --> 00:03:14,210
Each container will have its own
set of data, so depending on how

58
00:03:14,210 --> 00:03:18,680
you start them, by test function, by
test suite, or by test package, you

59
00:03:18,680 --> 00:03:21,980
have more or less containers running
in your system at a given time.

60
00:03:22,460 --> 00:03:26,530
But with no test data pollution,
and for biller speed, thanks to the

61
00:03:26,530 --> 00:03:30,489
containers, spin up technologies as
docker images is a piece of cake.

62
00:03:30,565 --> 00:03:34,704
Of course, depending on the size of
the image, you must pull it first and

63
00:03:34,704 --> 00:03:37,825
it could be js, but talk is cheap.

64
00:03:38,325 --> 00:03:39,195
Let's show the code.

65
00:03:39,695 --> 00:03:42,035
We are gonna present here
a simpler microservice app

66
00:03:42,245 --> 00:03:43,715
for rating conference talks.

67
00:03:43,985 --> 00:03:47,255
It probes an API to track the
ratings of the tax in real time.

68
00:03:47,495 --> 00:03:49,505
Stir the resource in a postgre database.

69
00:03:49,985 --> 00:03:54,245
Also using, already discuss a red pan
as a broker for evidence streaming.

70
00:03:54,995 --> 00:03:58,385
Finally it will use an AWS Lambda
function to calculate some aesthetics

71
00:03:58,535 --> 00:04:02,045
about the ratings of the talk,
we have an ster layer here with

72
00:04:02,045 --> 00:04:05,045
PostgreSQL in the to report GO file.

73
00:04:05,405 --> 00:04:09,575
So if we go to the tax report,
go file, we'll see the repository

74
00:04:09,575 --> 00:04:12,035
pattern holding the PEX connection.

75
00:04:12,725 --> 00:04:15,185
We build a new repository
from the connection stream.

76
00:04:15,815 --> 00:04:18,065
And with that we perform
the crude operations.

77
00:04:18,394 --> 00:04:23,255
Create insert into sql exist.

78
00:04:23,755 --> 00:04:29,605
With this sql, et cetera, for the already
discuss, we follow the same pattern.

79
00:04:29,605 --> 00:04:33,715
We have a repository here, and this
repository has a new repository

80
00:04:33,955 --> 00:04:35,725
receiving a connection stream.

81
00:04:36,685 --> 00:04:41,085
We create the client, we ping
the client and we add find all.

82
00:04:42,075 --> 00:04:45,015
We convert to a key here
about the streaming.

83
00:04:45,015 --> 00:04:48,795
We have here a broker and this
broker with a friend client using

84
00:04:48,795 --> 00:04:50,855
the Kafka, France Go package.

85
00:04:51,365 --> 00:04:55,975
We create a new stream from the connection
stream to the queue, and finally we do

86
00:04:55,975 --> 00:05:01,165
a ping here to verify the queue with
app and we're able to send rating to.

87
00:05:01,665 --> 00:05:05,535
To interact with the AWS Lambda, we
have this Lambda client, which is

88
00:05:05,535 --> 00:05:08,175
basically doing HTP requests to our URL.

89
00:05:08,475 --> 00:05:12,195
So we create the new Lambda client,
basically HTP client with the

90
00:05:12,195 --> 00:05:14,205
Lambda URL as URL of the client.

91
00:05:14,745 --> 00:05:18,855
We're gonna perform a post request
to the URL of the Lambda with this is

92
00:05:18,855 --> 00:05:22,635
the Specter response, and this is the
payload that the Lambda will receive.

93
00:05:23,135 --> 00:05:26,665
Finally, we return the response
so the application can consume

94
00:05:26,665 --> 00:05:28,345
the result from the AWS Lambda.

95
00:05:28,845 --> 00:05:32,205
How do we verify that those
interactions work properly?

96
00:05:32,625 --> 00:05:35,385
The simplest way is to create interaction
test with test container score.

97
00:05:35,775 --> 00:05:38,615
We're gonna see them in
action for the database.

98
00:05:38,615 --> 00:05:42,005
We have a new repository test here,
and we'll consume the progress

99
00:05:42,005 --> 00:05:43,325
module of test container score.

100
00:05:43,825 --> 00:05:47,065
We are gonna create a container with all
the options that we want to customize.

101
00:05:47,065 --> 00:05:51,805
The Postgres instance, the SQL database
that will populate the database, the

102
00:05:51,805 --> 00:05:54,925
name of the database, the credentials,
and we are gonna wait for the

103
00:05:54,925 --> 00:05:59,305
Postgres container until this log
entry appear two times in the lock.

104
00:05:59,805 --> 00:06:03,045
We are gonna make sure the container
is disposed at the end of the test,

105
00:06:03,825 --> 00:06:07,515
we're gonna get the connection stream
to the Postgres instance, which is part

106
00:06:07,515 --> 00:06:09,075
of the API with the Postgres module.

107
00:06:10,065 --> 00:06:14,835
And finally we're gonna create our new
repository, which is the persistent

108
00:06:14,835 --> 00:06:16,335
layer that talks to the database.

109
00:06:16,755 --> 00:06:20,205
If you recall this repository,
received the connection stream

110
00:06:20,235 --> 00:06:21,615
and perform the connections.

111
00:06:22,115 --> 00:06:25,475
Once we have the to repository
here, we are able to perform any

112
00:06:25,715 --> 00:06:32,040
crude operation like creating,
retrieving a talk or checking.

113
00:06:32,060 --> 00:06:32,800
It doesn't exist.

114
00:06:32,905 --> 00:06:34,105
We're gonna execute this.

115
00:06:34,735 --> 00:06:36,145
We run the file test.

116
00:06:36,970 --> 00:06:40,300
See the logs here, we're
gonna see the test containers.

117
00:06:40,300 --> 00:06:43,720
We'll run the test and
create the containers for us

118
00:06:44,220 --> 00:06:47,160
a using in the post 15 Alpine image.

119
00:06:48,090 --> 00:06:52,860
And in just 10 seconds, everything
passes exactly the same.

120
00:06:52,860 --> 00:06:55,465
For the red cast, we consume
the module for Redis.

121
00:06:55,965 --> 00:06:58,755
And create a container
with the run function.

122
00:06:59,085 --> 00:07:00,735
We don't need to customize it here.

123
00:07:00,765 --> 00:07:05,525
We just consume the plain module again,
we clean up the container, we get

124
00:07:05,525 --> 00:07:09,005
the connection stream, we create the
repository from the connection stream.

125
00:07:09,005 --> 00:07:11,615
If you recall, it receives
the connection stream.

126
00:07:12,005 --> 00:07:15,635
And once we have the repository
ready, and once we have the repository

127
00:07:15,635 --> 00:07:19,435
ready, we are able to perform
operations like adding on getting

128
00:07:19,435 --> 00:07:21,835
the results if we run the test here.

129
00:07:22,335 --> 00:07:27,675
Redis, we're gonna see test containers
running the Redis container and

130
00:07:27,675 --> 00:07:30,275
executing the test in just seven seconds.

131
00:07:30,275 --> 00:07:33,185
We are confident that the word
production can work as expected

132
00:07:33,695 --> 00:07:35,555
because it talks to Redis instance.

133
00:07:36,035 --> 00:07:39,965
And finally, for that queue, we
have this test In our application.

134
00:07:39,965 --> 00:07:41,075
We are using Red Panda.

135
00:07:41,105 --> 00:07:43,295
For that reason, we are
gonna use the module for Rand

136
00:07:43,500 --> 00:07:44,420
and Discon continuous goal.

137
00:07:44,920 --> 00:07:48,760
We call the run function from the
red pan module with this image, and

138
00:07:48,760 --> 00:07:51,880
we configured the red pan container
to auto to create the topics.

139
00:07:52,380 --> 00:07:55,290
We calculate the sheet broke
everywhere, L for the red pan container,

140
00:07:55,440 --> 00:07:56,940
and we create our near stream.

141
00:07:57,030 --> 00:08:00,780
If you recall, this stream received the
connection stream and created the Kaf

142
00:08:00,840 --> 00:08:05,570
client, and we are able to execute our
tests against that red panda instance.

143
00:08:05,840 --> 00:08:06,920
We're gonna run them.

144
00:08:07,420 --> 00:08:11,300
Probably takes longer because he's
gonna pull the image and finally in

145
00:08:11,300 --> 00:08:15,800
14 seconds it's able to run the test
against a real red panda instance.

146
00:08:16,010 --> 00:08:19,580
If we run the test again, we're
gonna see that it takes shorter

147
00:08:19,580 --> 00:08:24,580
time because the red panda image is
already pulled just five seconds.

148
00:08:24,790 --> 00:08:25,360
Amazing.

149
00:08:26,185 --> 00:08:28,225
Let's see the code for the AWS Lambda.

150
00:08:28,585 --> 00:08:33,475
It is basically using the AWS Lambda
Go package to create the Lambda and

151
00:08:33,515 --> 00:08:36,605
it will receive this payload and
we'll respond with this response.

152
00:08:37,035 --> 00:08:39,345
here is scanning the request.

153
00:08:39,705 --> 00:08:41,265
Calculate the maths here.

154
00:08:41,265 --> 00:08:42,105
Very simple.

155
00:08:42,375 --> 00:08:45,435
And finally, we have a main function,
which is the start from the Lambda.

156
00:08:45,935 --> 00:08:47,195
Let's see how to test it.

157
00:08:47,795 --> 00:08:51,515
Here we are running make to see
the Lambda, which is basically.

158
00:08:51,860 --> 00:08:55,700
Packaging the Lambda into the
artifact that AWS needs to create.

159
00:08:55,700 --> 00:08:57,350
The Lambda is very basic.

160
00:08:57,620 --> 00:09:00,800
We see here that we're building
the Lambda and creating a thi file

161
00:09:00,800 --> 00:09:02,630
for that name function of thi.

162
00:09:03,130 --> 00:09:06,550
And for the test, we are
gonna use local stack.

163
00:09:06,730 --> 00:09:10,390
Local Stack is emulating all
the AWS services specifically.

164
00:09:10,705 --> 00:09:17,155
Lambda and we're gonna copy the
C file created by the Meg file

165
00:09:17,515 --> 00:09:18,985
into the local stack container.

166
00:09:19,165 --> 00:09:23,395
And we are gonna execute our custom
code in the container after itta.

167
00:09:23,395 --> 00:09:26,785
In this case, we need to create
the Lambda and get its URL.

168
00:09:27,085 --> 00:09:30,115
So for that reason, we are
gonna get the file here.

169
00:09:30,615 --> 00:09:32,655
Create this Lambda
command, which is Lambda.

170
00:09:32,655 --> 00:09:34,005
Create function config.

171
00:09:34,035 --> 00:09:38,205
Wait for the Lambda to be ready, executing
these commands inside the container,

172
00:09:38,265 --> 00:09:42,015
which is provided by test container to
execute commands in the running container.

173
00:09:42,795 --> 00:09:47,055
Finally, we're gonna get the
function, rural configs, and for

174
00:09:47,055 --> 00:09:48,855
that we are gonna pass the response.

175
00:09:49,575 --> 00:09:53,535
And we need the function
URL here for the Lambda.

176
00:09:53,625 --> 00:09:57,465
This URL is the one needed to
perform HDP requests against

177
00:09:57,465 --> 00:09:59,025
the Lambda provided by localist.

178
00:09:59,525 --> 00:10:02,255
So if we continue, we
have another container.

179
00:10:02,615 --> 00:10:03,605
We have the MapIT port.

180
00:10:03,605 --> 00:10:07,085
We're gonna replace this
in the URL of the Lambda.

181
00:10:07,175 --> 00:10:10,685
This is needed because test container
give you a random port for the

182
00:10:10,745 --> 00:10:13,295
running container, for the well
known port for the running container.

183
00:10:13,565 --> 00:10:16,700
So we need to calculate that
and replace it in the URL.

184
00:10:17,285 --> 00:10:20,825
We are gonna pass this
payload in our test.

185
00:10:21,275 --> 00:10:23,256
We calculate the histogram here.

186
00:10:23,345 --> 00:10:28,145
We perform a post request, and
finally we, this is the expected

187
00:10:28,415 --> 00:10:29,615
response from the lambda.

188
00:10:30,115 --> 00:10:31,015
Pretty amazing, right?

189
00:10:31,075 --> 00:10:35,215
We're gonna execute AWS
Lambdas in our local machine.

190
00:10:36,055 --> 00:10:37,645
So we're gonna run the test here

191
00:10:38,145 --> 00:10:43,115
and are gonna see the local stack
container running where the code is

192
00:10:43,115 --> 00:10:44,615
calculating the UL of the Lambda.

193
00:10:45,115 --> 00:10:48,145
And finally we'll execute the
HTP request against that Euro

194
00:10:48,645 --> 00:10:49,365
on the test pass.

195
00:10:49,665 --> 00:10:52,425
We are gonna make it fail,
for example, here, instead of

196
00:10:52,755 --> 00:10:56,565
calculating the average properly,
we are gonna introduce a back here.

197
00:10:57,285 --> 00:11:02,245
And if we run the test, we are able to
detect that back in just a few seconds.

198
00:11:02,305 --> 00:11:05,215
We are able to run locally AWS Landas.

199
00:11:05,935 --> 00:11:08,395
And verify that our code
is working as suspect.

200
00:11:08,895 --> 00:11:09,135
Cool.

201
00:11:09,465 --> 00:11:10,395
We detected it.

202
00:11:10,725 --> 00:11:13,455
Total count is correct, but
the average is incorrect.

203
00:11:14,235 --> 00:11:16,875
We're gonna find more information
about test containers for go.

204
00:11:17,655 --> 00:11:21,795
If you visit the test container.com
website, you can find all the modules that

205
00:11:21,795 --> 00:11:24,435
we support for test continuous Go here.

206
00:11:24,435 --> 00:11:25,305
You can find for.

207
00:11:25,805 --> 00:11:30,245
Databases, retinal databases,
no SQL databases, vector

208
00:11:30,245 --> 00:11:35,195
databases, major brokers, web
ventilators, cloud, et cetera.

209
00:11:35,855 --> 00:11:39,965
Lot of them, all the modules are based
on the container request striped.

210
00:11:40,535 --> 00:11:42,515
With either strip, you're
able to configure your

211
00:11:42,515 --> 00:11:43,895
container are the way you want.

212
00:11:44,165 --> 00:11:47,525
For example, defend the image,
defining the exposure sport.

213
00:11:47,765 --> 00:11:48,695
Defining the network.

214
00:11:48,695 --> 00:11:51,725
This container belongs to
defining the conditions to wait

215
00:11:51,725 --> 00:11:53,195
for this container to be ready.

216
00:11:53,690 --> 00:11:54,200
And more.

217
00:11:54,590 --> 00:11:57,890
For example, one advanced
capability are the like cycle hooks.

218
00:11:58,130 --> 00:12:02,750
You can execute your own code directly
into the container lifecycle, for

219
00:12:02,750 --> 00:12:06,170
example, before the container creates bef.

220
00:12:06,200 --> 00:12:10,970
After the container creates, before the
container starts and after it starts.

221
00:12:11,600 --> 00:12:15,560
Before the container and after the
container stops, before the container

222
00:12:15,560 --> 00:12:20,090
is ready, checking the health checks
and also before and after the container

223
00:12:20,090 --> 00:12:24,260
terminates, you can inject your own
code and execute it in your test code.

224
00:12:24,710 --> 00:12:28,700
You can also create data into the
container, for example, mounting volumes

225
00:12:29,060 --> 00:12:30,650
or copy files into the container.

226
00:12:31,070 --> 00:12:34,100
It's interesting that the files
attributes allow you to copy container

227
00:12:34,100 --> 00:12:35,750
before the container is started.

228
00:12:35,990 --> 00:12:38,210
They are copied after
the container is created.

229
00:12:38,615 --> 00:12:43,325
So when the container starts the data, the
state of the container is already there.

230
00:12:43,715 --> 00:12:47,525
This is really useful for containers
with, depends on an state.

231
00:12:47,585 --> 00:12:53,495
For example, databases you can pass here,
a file to a SQL file to load the database.

232
00:12:53,705 --> 00:12:55,775
So the tests start with
a well known state.

233
00:12:56,275 --> 00:12:58,825
One interesting capability
of test containers is real.

234
00:12:59,305 --> 00:13:02,245
You have probably seen the name
real in the list of containers

235
00:13:02,245 --> 00:13:03,265
that are running in your system.

236
00:13:03,835 --> 00:13:07,315
This real container ISR responsible
for removing containers,

237
00:13:07,315 --> 00:13:11,065
networks, volumes, and images
created by test container for go.

238
00:13:11,215 --> 00:13:14,185
So you don't need to clean up the
containers you create in your test.

239
00:13:14,685 --> 00:13:18,135
Regarding the weather strategies, which
I mentioned before, there are situations

240
00:13:18,135 --> 00:13:21,495
where you have to wait for a, the
container to be in a well-known state.

241
00:13:21,945 --> 00:13:26,505
Instead of adding timeless slips to
your CI pipeline or your or to your tax

242
00:13:26,505 --> 00:13:29,355
execution, you rely on waiter strategies.

243
00:13:30,075 --> 00:13:32,025
You wait for a command to
be executed, a container.

244
00:13:32,910 --> 00:13:36,450
For example like this here,
let's wait for this command to

245
00:13:36,450 --> 00:13:37,770
be executed in the container.

246
00:13:38,550 --> 00:13:41,130
Or for example, we can wait
for a container to exit.

247
00:13:41,630 --> 00:13:42,350
At the same time.

248
00:13:42,350 --> 00:13:46,250
We can wait for a file to be present in a
container and the test will continue after

249
00:13:46,250 --> 00:13:47,870
this file is present in the container.

250
00:13:48,800 --> 00:13:51,170
Also, you can wait for a
health check waiting for the

251
00:13:51,170 --> 00:13:52,310
health check of the container.

252
00:13:52,810 --> 00:13:55,660
You can wait for a port to be
already listening in your container.

253
00:13:56,335 --> 00:14:01,525
For an HDP request, and you can configure
the port you want to wait for, or even

254
00:14:01,525 --> 00:14:06,745
the path, the credential with pathic
out or even expecting as response

255
00:14:06,745 --> 00:14:08,995
code to be present in the response.

256
00:14:09,295 --> 00:14:12,265
You can also configure the
response headers and wait for them

257
00:14:12,325 --> 00:14:13,795
to be present in the response.

258
00:14:14,365 --> 00:14:18,265
Another wave condition that is very
handy is waiting for a lock to be

259
00:14:18,265 --> 00:14:19,795
present in the log of the container.

260
00:14:20,125 --> 00:14:24,685
You can configure it to inspect
a string or a regular expression.

261
00:14:25,185 --> 00:14:26,355
Using Azure records.

262
00:14:26,855 --> 00:14:30,095
You can also combine multiple of
them with wait for all and all of

263
00:14:30,095 --> 00:14:34,025
them will be applied at the same
time and you can wait for a SQL

264
00:14:34,025 --> 00:14:35,915
query to be executed on a database.

265
00:14:36,185 --> 00:14:37,805
It's also possible to use this container.

266
00:14:37,805 --> 00:14:40,865
Go for building docker
images from a Docker file.

267
00:14:41,135 --> 00:14:45,275
If you define this stroke inside the
container, request struck, you will

268
00:14:45,275 --> 00:14:50,495
be able to build this Echo file in
this path and with this repository.

269
00:14:50,495 --> 00:14:51,545
Name and name.

270
00:14:52,520 --> 00:14:56,200
You can also pass arguments to the
build and at the end the container

271
00:14:56,200 --> 00:14:57,370
will be running after the build.

272
00:14:57,870 --> 00:14:59,130
This is what I have here.

273
00:14:59,130 --> 00:15:02,460
I'm sharing with you the CURE
calls to ago where we demonstrated

274
00:15:02,460 --> 00:15:05,280
power of this containers for goal
and also the website with all the

275
00:15:05,280 --> 00:15:06,600
information about the project.

276
00:15:07,170 --> 00:15:08,430
I hope you enjoyed my talk.

277
00:15:08,760 --> 00:15:09,690
Thank you very much.

