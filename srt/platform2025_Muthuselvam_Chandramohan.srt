1
00:00:00,500 --> 00:00:02,510
Good morning, and good evening everyone.

2
00:00:02,510 --> 00:00:04,160
Depends on where you're joining from.

3
00:00:04,320 --> 00:00:05,790
Thanks for joining the session.

4
00:00:05,880 --> 00:00:09,870
My name is MuTu Shandra Mohan,
and I have more than 20 years of

5
00:00:09,870 --> 00:00:12,140
experience in data engineering domain.

6
00:00:12,710 --> 00:00:16,369
And today I'm gonna be talking about
building real time graph database

7
00:00:16,879 --> 00:00:20,880
that can be leveraged for both real
time and analytical capabilities.

8
00:00:21,435 --> 00:00:24,794
And I'm gonna be talking about
the architectural patterns that

9
00:00:24,794 --> 00:00:28,665
can be followed and the key design
considerations that need to be accounted

10
00:00:28,725 --> 00:00:30,225
in building the graph database.

11
00:00:30,964 --> 00:00:34,684
I wanted to start with the putting
the question of why we need to

12
00:00:35,074 --> 00:00:37,144
adopt graph database, right?

13
00:00:37,174 --> 00:00:40,574
What is the problem statement
in building a graph database?

14
00:00:41,095 --> 00:00:44,484
The key thing comes from
handling the relationship data.

15
00:00:44,675 --> 00:00:46,205
In today's scenario, the data.

16
00:00:46,845 --> 00:00:48,195
It's very complex.

17
00:00:48,575 --> 00:00:53,245
And getting analytical information out
of it getting information out of the data

18
00:00:53,245 --> 00:00:58,785
is crucial and especially in handling
complex data patterns like social network

19
00:00:58,785 --> 00:01:04,435
data handling supply chain information
and how to identify fraud from a given

20
00:01:04,435 --> 00:01:08,810
data set and building knowledge graph
from a given data data set that can.

21
00:01:09,450 --> 00:01:14,840
Provide information out of the data is
very complex mainly because of the la

22
00:01:15,320 --> 00:01:20,850
last volume of data, large volume of data
that that we handle on a on a daily basis.

23
00:01:21,310 --> 00:01:25,840
The, that, that brings in the
need for graph database in a

24
00:01:25,840 --> 00:01:27,890
traditional relationship database.

25
00:01:28,410 --> 00:01:34,260
Identifying this complex multi
hop analysis is very difficult and

26
00:01:34,270 --> 00:01:39,660
considering scalability is even more
challenging and that is one of the main

27
00:01:39,660 --> 00:01:42,320
reasons that we adopt graph database.

28
00:01:42,720 --> 00:01:46,250
We will get into more details
as we go through the session.

29
00:01:46,740 --> 00:01:48,240
So as I move along.

30
00:01:48,900 --> 00:01:53,540
When we decide to build a graph
database, what are the challenges?

31
00:01:53,590 --> 00:01:57,970
We as as a platform engineers now
either in the data engineers or in the

32
00:01:57,970 --> 00:02:02,020
platform engineers, what are the common
challenges that we face on a database?

33
00:02:02,710 --> 00:02:05,330
Or in, in building the
lifecycle of the database.

34
00:02:05,390 --> 00:02:08,990
I wanted to cover these three
bullet points that's listed here.

35
00:02:09,050 --> 00:02:13,380
One is when we build the graph
database that we are going to use

36
00:02:13,380 --> 00:02:15,670
it for real time analysis, right?

37
00:02:15,730 --> 00:02:19,510
The often the expectation is
to handle customer calls that

38
00:02:19,510 --> 00:02:21,700
can be addressed in subsecond.

39
00:02:22,245 --> 00:02:28,025
The query response times this is this
is a very a common case when it comes to

40
00:02:28,025 --> 00:02:33,915
fraud and cybersecurity use cases that the
response times need to be fairly quick.

41
00:02:34,315 --> 00:02:37,725
And how do we adopt seamless
integration with the different.

42
00:02:38,335 --> 00:02:42,565
Different applications or systems, for
example, it can be a microservices,

43
00:02:42,625 --> 00:02:47,875
API based systems, or it can be a batch
oriented data integration platform.

44
00:02:48,155 --> 00:02:52,175
Or it can be even with any
of the packaged applications.

45
00:02:52,565 --> 00:02:58,095
So how do we integrate with different
applications and systems that can bring in

46
00:02:58,095 --> 00:03:00,855
heterogeneous data to the graph database?

47
00:03:00,855 --> 00:03:01,845
How do we integrate it?

48
00:03:02,325 --> 00:03:07,395
And then the third thing is how do
we address the scalability concerns?

49
00:03:07,425 --> 00:03:11,705
We can do vertical scalability, but
that has a limitation at some point.

50
00:03:12,105 --> 00:03:16,925
So one of the one of the more,
most common adopted architecture

51
00:03:16,925 --> 00:03:18,555
is the horizontal scalability.

52
00:03:18,555 --> 00:03:22,710
So that it, there is no limitation in
terms of the amount of scale that we can.

53
00:03:23,365 --> 00:03:24,685
We can grow into, right?

54
00:03:24,685 --> 00:03:29,535
So how do we how do we adopt horizontal
scalability for an ever-growing

55
00:03:29,595 --> 00:03:34,095
demand on the data side, and
especially with graph database, right?

56
00:03:34,095 --> 00:03:36,795
So how do we handle
these three challenges?

57
00:03:36,795 --> 00:03:38,595
Is what we are going
to be discussing about.

58
00:03:38,595 --> 00:03:43,015
And these are some of the key
challenges that comes into when we

59
00:03:43,015 --> 00:03:44,815
start implementing graph databases.

60
00:03:45,370 --> 00:03:50,510
And  and there are very few technologies
that kind of  stands out to handle.

61
00:03:50,570 --> 00:03:54,900
These critical, important challenges
when it comes to graph database.

62
00:03:55,400 --> 00:04:00,495
Moving on what are the limitations
of the traditional approaches.

63
00:04:01,060 --> 00:04:05,210
Or in other words like the current
databases that are available.

64
00:04:05,580 --> 00:04:08,880
And when we look at the current database,
I think one of the key thing that

65
00:04:08,930 --> 00:04:10,940
comes out is the relational database.

66
00:04:11,400 --> 00:04:14,295
So what are the challenges
in a relational database?

67
00:04:14,295 --> 00:04:18,615
For a use case that we adopt
with graph database often comes

68
00:04:18,615 --> 00:04:23,195
with a lot of interconnected
relationship among the data.

69
00:04:23,505 --> 00:04:30,085
The key thing is how do we build
build a system that can inherently

70
00:04:30,085 --> 00:04:35,865
provide the ability to for the users
to analyze the data and provide

71
00:04:35,925 --> 00:04:38,415
the information out of it, right?

72
00:04:38,415 --> 00:04:43,525
So that is where the relational database
kind of struggles in terms of building

73
00:04:43,825 --> 00:04:45,600
the relationship within the data.

74
00:04:46,385 --> 00:04:49,985
When we say relationship at a high level,
yes, there is a relational database

75
00:04:49,985 --> 00:04:54,195
that provides one-to-one or one to many
relationship, but what we are talking

76
00:04:54,195 --> 00:04:59,485
about is complex relationship with
that are that are not direct, right?

77
00:04:59,755 --> 00:05:02,665
So that is where the
relational database struggle.

78
00:05:03,035 --> 00:05:07,945
And it is more often realized when
it comes to running the queries to

79
00:05:07,945 --> 00:05:10,225
understand the relationship, right?

80
00:05:10,235 --> 00:05:15,645
For example, one of the use cases like
who is a friend of friend or find all the

81
00:05:15,645 --> 00:05:19,905
friend of friend who share at least three
interest with a particular user, right?

82
00:05:19,905 --> 00:05:22,480
This doesn't come with a
direct  query analysis.

83
00:05:23,155 --> 00:05:26,435
On a relational database when
you want to address this problem,

84
00:05:26,435 --> 00:05:27,935
that's where the second point comes.

85
00:05:28,305 --> 00:05:32,355
It's an exponential com complexity
because often we don't know how many

86
00:05:32,355 --> 00:05:36,735
levels of analysis that we wanted
to perform and how many joints

87
00:05:36,735 --> 00:05:38,115
that we wanted to perform, right?

88
00:05:38,205 --> 00:05:43,125
So in most of the cases in relational
database, this often comes at a point

89
00:05:43,125 --> 00:05:47,965
of making an assumption that how many
levels that you wanted to perform.

90
00:05:48,760 --> 00:05:52,900
That's where the limitation of the
relational database comes into play,

91
00:05:53,230 --> 00:05:55,330
where graph database scores out.

92
00:05:55,640 --> 00:06:02,050
And that multi hop analysis in a end
number of hops that we wanted to perform

93
00:06:02,100 --> 00:06:05,110
graph databases handled so easily.

94
00:06:05,640 --> 00:06:10,490
And the way how it graph database,
addresses this problem is that the

95
00:06:10,490 --> 00:06:16,200
way it stores the data the the graph
database stores the relationship as

96
00:06:16,200 --> 00:06:20,220
a first class citizen rather than
as a data as a property, right?

97
00:06:20,460 --> 00:06:22,620
So relationship is being stored.

98
00:06:22,690 --> 00:06:28,450
And that's how the data is being is
being sorted or stored so that the

99
00:06:29,140 --> 00:06:33,960
traversal queries can be handled
so easily and any sort of indirect

100
00:06:33,960 --> 00:06:40,200
relationships and multi hopp analysis
that, that needs traversal based on

101
00:06:40,200 --> 00:06:44,970
the dynamic results that are fetched
at the previous step can be achieved

102
00:06:45,030 --> 00:06:46,980
using the graph database solution.

103
00:06:47,220 --> 00:06:51,960
And that's how it differentiates itself
from any of the relational database or

104
00:06:51,960 --> 00:06:56,730
document database or any sort of search
database and graph stands out separately.

105
00:06:57,230 --> 00:07:00,720
Moving on what is the core
architectural pattern?

106
00:07:01,000 --> 00:07:03,430
So we are gonna be talking about
different things, but the first

107
00:07:03,430 --> 00:07:06,490
one that I wanted to talk about
is an event driven architecture.

108
00:07:06,860 --> 00:07:11,035
I picked up the event driven architecture
because this is one of the in, this

109
00:07:11,035 --> 00:07:16,915
is a common use case that comes
to different graph implementations

110
00:07:16,975 --> 00:07:20,165
because often the times the graph
implementation, the expectation is that.

111
00:07:20,840 --> 00:07:25,410
How do I handle fraud use cases
and it comes to financial or even

112
00:07:25,410 --> 00:07:31,190
sometimes non-financial institutions
how do I address the fraud and

113
00:07:31,190 --> 00:07:35,840
cybersecurity use cases, which
requires faster response time, right?

114
00:07:35,890 --> 00:07:40,000
And that is where the event driven
architectural comes in when we talk

115
00:07:40,000 --> 00:07:41,425
about event driven architecture.

116
00:07:42,085 --> 00:07:47,175
It often the handles the ability
of implementing or integrating

117
00:07:47,175 --> 00:07:52,285
with an event event based system,
something like Apache, Kafka or any

118
00:07:52,285 --> 00:07:54,275
of the streaming platforms, right?

119
00:07:54,735 --> 00:07:59,125
There are good amount of graph
database that inherently supports.

120
00:07:59,585 --> 00:08:02,205
Integrations with even based systems.

121
00:08:02,235 --> 00:08:06,725
They have connectors with Apache Kafka
or any of the other streaming systems.

122
00:08:07,025 --> 00:08:12,675
And and any sort of even driven
updates that comes from from these

123
00:08:12,675 --> 00:08:18,380
systems can be handled using a
distributed database that kind of.

124
00:08:19,030 --> 00:08:26,090
Routes the calls to a specific partition
within the database that that is driven

125
00:08:26,090 --> 00:08:31,990
based on the relationship that is that is
identified while building the data model.

126
00:08:32,380 --> 00:08:37,630
So it's a hand in hand process that
you identify how you are going to model

127
00:08:37,630 --> 00:08:40,650
your data and that gets a specific.

128
00:08:41,150 --> 00:08:44,650
Storage unit, which is
partitions in a graph database.

129
00:08:45,130 --> 00:08:50,060
And that can be easily retrieved
at the time of handling real time

130
00:08:50,060 --> 00:08:55,640
updates that any sort of event driven
architecture that feeds data in a

131
00:08:55,850 --> 00:09:01,670
real time manner can get easily routed
to the specific graph partitions.

132
00:09:02,160 --> 00:09:06,815
And what it brings us brings the
capability is that it can handle

133
00:09:06,915 --> 00:09:11,620
it, it can it can handle if there
are any outages, it can replay the

134
00:09:11,620 --> 00:09:13,630
data from the streaming platform.

135
00:09:13,630 --> 00:09:17,535
We can quickly replay from
where where we left over.

136
00:09:18,265 --> 00:09:24,165
And multiple consumers can consume from
the same event for different purposes.

137
00:09:24,255 --> 00:09:30,145
Even in terms of a graph database, you
can have a different graph instances that

138
00:09:30,145 --> 00:09:32,545
handle different customer expectations.

139
00:09:32,915 --> 00:09:37,805
That can be one for fraud related use
cases, and that can be one for marketing.

140
00:09:38,160 --> 00:09:38,760
Use case.

141
00:09:39,040 --> 00:09:43,560
Both can be fetching the data
from the same Kafka instance

142
00:09:43,560 --> 00:09:45,160
or the streaming platforms.

143
00:09:46,030 --> 00:09:49,995
So that is one key
consideration to look look into.

144
00:09:50,345 --> 00:09:54,495
Because at the time of defining
your data model you also want to

145
00:09:54,495 --> 00:09:58,395
consideration what sort of systems
that you wanted to integrate with.

146
00:09:58,755 --> 00:10:02,825
And what sort of given driven
architecture that you wanted to adapt to?

147
00:10:03,045 --> 00:10:05,115
There can be instances
where we don't need it.

148
00:10:05,425 --> 00:10:08,315
It can be a pure batch processing systems.

149
00:10:08,585 --> 00:10:12,995
But often it's the case that that
you wanted to have even driven

150
00:10:12,995 --> 00:10:17,610
realtime architecture and and event
driven architecture is a very fairly.

151
00:10:18,250 --> 00:10:22,520
Robust system compared to
other API based integrations.

152
00:10:22,830 --> 00:10:25,500
It depends on the situation, but
even driven architecture is a

153
00:10:25,500 --> 00:10:29,380
much more reliable architectural
pattern that can be looked up to.

154
00:10:29,880 --> 00:10:30,690
Moving on.

155
00:10:30,830 --> 00:10:34,930
What are the different horizontal
scaling strategies that we can adopt?

156
00:10:35,320 --> 00:10:38,880
So we talked about the
the growing needs of data.

157
00:10:39,435 --> 00:10:44,195
When it comes from different sources,
like it can be an iot system or it,

158
00:10:44,200 --> 00:10:50,075
it can be from a these days there
are large volume applications that

159
00:10:50,075 --> 00:10:52,755
generates data in a real time manner.

160
00:10:52,940 --> 00:10:53,880
So how do we handle.

161
00:10:54,565 --> 00:10:56,865
Data that's coming from these systems.

162
00:10:56,865 --> 00:10:59,955
How do we make sure that
we handle them correctly?

163
00:11:00,005 --> 00:11:04,565
So one of the key thing is handling it
through horizontal scaling so that you are

164
00:11:04,565 --> 00:11:07,085
not hitting a limitation at some point.

165
00:11:07,475 --> 00:11:10,975
And horizontal scaling handling
it through distributed systems

166
00:11:10,975 --> 00:11:12,585
is the, is the way to go forward.

167
00:11:13,095 --> 00:11:16,815
And within horizontal scaling
approach what are the different

168
00:11:16,815 --> 00:11:18,105
strategies that you can use?

169
00:11:18,105 --> 00:11:19,895
One is a graph partitioning.

170
00:11:20,395 --> 00:11:25,625
Where the graph database, as we know
is defined from how it has been defined

171
00:11:25,635 --> 00:11:30,125
in the mathematical, the statistical
models vertex and edge based and

172
00:11:30,125 --> 00:11:33,905
how they are connected by each other
and how they comprehend each other.

173
00:11:33,905 --> 00:11:35,555
When it comes to graph partitioning.

174
00:11:36,045 --> 00:11:41,565
This is also at the time of graph data
model modeling and that you define

175
00:11:41,565 --> 00:11:46,590
how you want, what is the key thing
behind your your data partition, right?

176
00:11:46,920 --> 00:11:50,430
Whether it is driven through
vertex partitioning, or it

177
00:11:50,490 --> 00:11:51,810
is through each parting.

178
00:11:52,420 --> 00:11:56,800
It is largely driven by
your your query pattern.

179
00:11:56,800 --> 00:12:01,150
How you are going to be doing it is not
in terms of your data characteristics.

180
00:12:01,150 --> 00:12:04,940
It is going to be how you are going
to be consuming your data from your

181
00:12:05,050 --> 00:12:06,730
querying patterns going to look like.

182
00:12:06,730 --> 00:12:07,780
So depends on that.

183
00:12:08,140 --> 00:12:11,410
You might want to choose between
a vertex based partitioning

184
00:12:11,495 --> 00:12:13,290
or edge based partitioning.

185
00:12:13,600 --> 00:12:16,115
There are different graph
database that supports it.

186
00:12:16,115 --> 00:12:16,395
There are.

187
00:12:16,855 --> 00:12:18,865
Some databases that doesn't support it.

188
00:12:19,165 --> 00:12:23,815
So this is one of the key consideration
when you wanted to adopt a graph database

189
00:12:23,815 --> 00:12:27,505
to see which partition, which type
of graph database that you wanted to

190
00:12:27,505 --> 00:12:31,645
choose, and whether that can support
a ver, a vertex cut partitioning

191
00:12:31,645 --> 00:12:34,105
or a h cut partitioning, right?

192
00:12:34,165 --> 00:12:34,795
That is one.

193
00:12:35,275 --> 00:12:39,095
And the second thing is what kind
of replication strategy, right?

194
00:12:39,405 --> 00:12:42,245
Often when we talk about
distributed databases, there are

195
00:12:42,305 --> 00:12:43,945
different it follows the cap, the.

196
00:12:44,540 --> 00:12:48,260
There are different replication
methodologies that are being adopted.

197
00:12:48,610 --> 00:12:50,410
One is full consistency.

198
00:12:50,690 --> 00:12:54,160
There is there is the other one that's
the eventual consistency model, right?

199
00:12:54,380 --> 00:12:59,080
The eventual consistency model
is that you write to a quorum and

200
00:12:59,080 --> 00:13:00,960
then you read it from a quorum.

201
00:13:00,960 --> 00:13:05,760
So that way you are fairly confident
that you are reading the latest data.

202
00:13:06,220 --> 00:13:10,730
So that is eventually consistent model
is one strongly recommended approach

203
00:13:10,730 --> 00:13:14,520
when it comes to graph database because
it's not gonna be a system of source

204
00:13:14,890 --> 00:13:16,540
it's just a system of reference.

205
00:13:16,540 --> 00:13:21,340
So eventual model is fairly reasonable
to go with and a master slave

206
00:13:21,370 --> 00:13:24,910
replication where you write it down
to one and then you read it from

207
00:13:24,910 --> 00:13:26,440
the replicas, that's another one.

208
00:13:26,440 --> 00:13:29,940
And there are things like consensus
protocol, which is a different.

209
00:13:30,330 --> 00:13:34,230
Approach of eventual, but
it follows a raft protocol.

210
00:13:34,570 --> 00:13:36,730
That's another replication strategy.

211
00:13:37,510 --> 00:13:43,070
So all these are some of the key
aspects to look into when you are

212
00:13:43,070 --> 00:13:45,110
looking for horizontal scaling.

213
00:13:45,500 --> 00:13:50,560
Graph database that support horizontal
scalability and which is essentially one

214
00:13:50,560 --> 00:13:52,540
of the key characteristics of distributed.

215
00:13:53,360 --> 00:13:55,700
Computing or distributed data storage.

216
00:13:55,700 --> 00:13:58,370
I have put it into two different groups.

217
00:13:58,370 --> 00:14:01,250
One is like how you are
going to store your data.

218
00:14:01,590 --> 00:14:05,540
That's your graph partitioning part,
and then what kind of replication

219
00:14:05,540 --> 00:14:07,890
strategy that you wanted to adopt to.

220
00:14:08,190 --> 00:14:12,730
That's more from a computing and
data data storage eventually.

221
00:14:12,950 --> 00:14:17,125
And, 1, 1, 1 key thing to look
into is like graph is again

222
00:14:17,345 --> 00:14:19,125
it's not a stateless system.

223
00:14:19,125 --> 00:14:21,135
It is a, it's a stateful system.

224
00:14:21,435 --> 00:14:25,465
So you wanted to consider about
the storage what kind of storage

225
00:14:25,465 --> 00:14:30,755
that you wanted to, so that your
travels again is fairly easy, right?

226
00:14:30,815 --> 00:14:32,615
So that, that's something to look into.

227
00:14:32,925 --> 00:14:33,165
But.

228
00:14:33,670 --> 00:14:36,160
Distributed database is
the way to go forward.

229
00:14:36,320 --> 00:14:38,590
That's what we wanted
to cover on this one.

230
00:14:39,390 --> 00:14:42,860
The next one is we talked about
this the kind of storage that

231
00:14:43,070 --> 00:14:44,360
we wanted to adopt into, right?

232
00:14:44,810 --> 00:14:49,750
So you decided that you wanted to go
for a distributed database, but if you

233
00:14:49,750 --> 00:14:54,530
are not sure about what kind of, storage
that you wanted to adopt into this is

234
00:14:54,530 --> 00:14:58,340
where your next design aspect or the
architectural aspect comes into play.

235
00:14:58,730 --> 00:15:01,820
What is going to be your use case
that you are going to handle?

236
00:15:01,830 --> 00:15:05,570
Is your use case going to be like
a sub subsecond or a sub Yeah.

237
00:15:05,600 --> 00:15:09,230
Subsecond queries that you
wanted to address in a fraud

238
00:15:09,230 --> 00:15:10,820
or cybersecurity use case.

239
00:15:11,240 --> 00:15:16,500
Or is it is it gonna be a system that
heavily relies on how you are going

240
00:15:16,500 --> 00:15:21,670
to store the data but may not be
needing a sub-second response time?

241
00:15:21,740 --> 00:15:25,895
There, there are use cases like that
often times that you look at to a data

242
00:15:25,895 --> 00:15:30,305
store where you are okay to wait for two,
three seconds or even up to 10 seconds.

243
00:15:30,835 --> 00:15:35,445
But you wanted to be more towards
storing the data in a persistent manner

244
00:15:35,475 --> 00:15:39,615
that you are able to rely upon the
data and faster recovery time, right?

245
00:15:39,615 --> 00:15:44,205
So if you have a use case like
that, and then there are specialized

246
00:15:44,355 --> 00:15:49,795
storages, like document stores or key
value paths columnar databases, right?

247
00:15:50,145 --> 00:15:53,645
Those are again a flavor of
your distributed databases.

248
00:15:54,145 --> 00:15:59,925
May not be truly graph, but it probably
doesn't need a high amount of data.

249
00:15:59,925 --> 00:16:03,505
So you can fairly run better
than a relational database.

250
00:16:03,755 --> 00:16:07,205
That's another specialized storage
that you can look up to, right?

251
00:16:07,365 --> 00:16:11,775
So these are the different flavors in
which you can adopt to a hybrid storage.

252
00:16:12,245 --> 00:16:15,155
Like things like in memory when you
are looking for subsecond, then you

253
00:16:15,155 --> 00:16:17,315
can go for Redis graph or Apache graph.

254
00:16:18,225 --> 00:16:19,355
As as an option.

255
00:16:19,355 --> 00:16:24,075
And when it comes to persistent storage,
like Neo four J Amazon, Neptune,

256
00:16:24,075 --> 00:16:25,665
that's another thing to look at.

257
00:16:25,995 --> 00:16:27,285
And there are specialist storage.

258
00:16:27,345 --> 00:16:28,665
There are tons of things.

259
00:16:28,665 --> 00:16:33,220
And in the market oftentimes
when you have a NoSQL database

260
00:16:33,270 --> 00:16:35,400
it comes with a graph database.

261
00:16:35,410 --> 00:16:37,675
There are cases that
we, we often look up to.

262
00:16:38,210 --> 00:16:43,240
Using the same NoSQL storage with
a graph layer on top of it because

263
00:16:43,240 --> 00:16:47,200
you don't have to adopt multiple
technologies to, to support your use case.

264
00:16:47,620 --> 00:16:53,210
So that's another area to look into and
query query optimization techniques.

265
00:16:53,610 --> 00:16:56,760
This is like post your graph database.

266
00:16:57,070 --> 00:17:00,580
You have adopted a graph database
and then you have you have fed

267
00:17:00,580 --> 00:17:02,980
in a good amount of data into it.

268
00:17:03,070 --> 00:17:04,180
Like how do you.

269
00:17:04,520 --> 00:17:06,140
Optimize your query, right?

270
00:17:06,500 --> 00:17:11,340
So this is where you you put the hat
of a developer and then see what is the

271
00:17:11,340 --> 00:17:16,130
best way in which I can I can define
my query or design my query, right?

272
00:17:16,130 --> 00:17:21,185
So you can look at the option of early
termination that is once you reach a part.

273
00:17:21,845 --> 00:17:25,895
A query result point, then you
don't have to continue the query.

274
00:17:26,125 --> 00:17:28,625
That's an advantage with
the graph database that you

275
00:17:28,625 --> 00:17:29,885
can terminate the query as.

276
00:17:29,885 --> 00:17:33,815
And when you you have reached
the result point, right?

277
00:17:33,935 --> 00:17:36,065
And then there are bidirectional search.

278
00:17:36,065 --> 00:17:40,245
What I mean is that you can
start from multiple points

279
00:17:40,315 --> 00:17:41,630
depends on the query criteria.

280
00:17:42,245 --> 00:17:46,815
And this is where a graph database plays
a critical role that if you wanted to

281
00:17:47,325 --> 00:17:52,535
start with a country, and if you start
with a particular age group, right?

282
00:17:52,805 --> 00:17:59,755
So you can have these two search points
and you convene towards or converge

283
00:17:59,755 --> 00:18:02,765
towards a singular result site, right?

284
00:18:02,765 --> 00:18:05,150
So you can start from bidirectional point.

285
00:18:05,820 --> 00:18:09,300
So that way your query
run times would be faster.

286
00:18:09,360 --> 00:18:12,580
And this is this is something
different from our relate

287
00:18:12,790 --> 00:18:14,950
traditional databases, right?

288
00:18:15,100 --> 00:18:18,610
And and then there is cost base
optimization that you can do.

289
00:18:18,980 --> 00:18:23,980
What it means is that you can find
out what is the degree distribution

290
00:18:23,980 --> 00:18:28,500
for that means when you are on a
particular vertex, you can see how many.

291
00:18:28,865 --> 00:18:31,115
It just are connected to it, right?

292
00:18:31,115 --> 00:18:35,955
So that way you know what is the
criticality of that particular vertex,

293
00:18:36,255 --> 00:18:39,135
and you can do cost-based optimization.

294
00:18:39,445 --> 00:18:43,025
If there are high degree,
then probably you don't want

295
00:18:43,025 --> 00:18:44,285
to run your query from there.

296
00:18:44,285 --> 00:18:46,175
You wanted to start from a much lower.

297
00:18:46,640 --> 00:18:49,530
Vertex, so that way
your results are faster.

298
00:18:49,530 --> 00:18:53,730
So that is one way of cost based
optimization that you can adopt in

299
00:18:53,900 --> 00:18:55,550
with the query optimization technique.

300
00:18:55,920 --> 00:19:00,760
So these are some of the trivial ones,
but can yield the high results when

301
00:19:00,760 --> 00:19:04,825
it comes to query optimization and
then memory management strategies.

302
00:19:04,990 --> 00:19:08,220
This is go hand in hand with
your query optimization.

303
00:19:08,680 --> 00:19:12,130
You can adopt things like
your vertex edge pooling.

304
00:19:12,440 --> 00:19:17,810
So basically you can fragment your memory
and see what graph elements that you

305
00:19:17,810 --> 00:19:22,060
wanted to store in memory, so that way
your garbage collection, GC collection.

306
00:19:22,060 --> 00:19:26,595
Often times that you you will find
these databases use Java heavily,

307
00:19:26,895 --> 00:19:29,835
so the garbage collection, once you
address garbage collection, your

308
00:19:29,835 --> 00:19:31,170
query response times are faster.

309
00:19:31,840 --> 00:19:33,950
So that is the main intention behind it.

310
00:19:34,280 --> 00:19:37,900
And then compress direct
presentations, each graph database

311
00:19:38,220 --> 00:19:40,740
will have its own compression logic.

312
00:19:41,020 --> 00:19:45,230
One of the key thing to look after
is something like CSR which kind of

313
00:19:45,280 --> 00:19:50,410
uses a bit backed adjacent list that,
that helps in compressing the data.

314
00:19:50,460 --> 00:19:54,070
And that kind of helps in
getting the runtime faster.

315
00:19:54,505 --> 00:19:59,265
And then you have the hot cold
separation that this kind of goes in

316
00:19:59,355 --> 00:20:03,675
relation with your monitoring your
database that you identify, what are

317
00:20:03,675 --> 00:20:09,425
the key vertexes or key partitions
that, that are hot and cold which

318
00:20:09,425 --> 00:20:11,625
are like often used versus less used.

319
00:20:11,625 --> 00:20:14,105
You can separate or not separate.

320
00:20:14,605 --> 00:20:19,155
Identify them and try to split
the hard partitions into multiple

321
00:20:19,155 --> 00:20:24,035
different chunks of give more compute
power and more storage so that

322
00:20:24,035 --> 00:20:25,895
the queries can run faster, right?

323
00:20:25,925 --> 00:20:29,745
So these are different areas that
you can look into from a memory

324
00:20:29,745 --> 00:20:33,840
management sta standpoint that
can help lead higher results.

325
00:20:34,340 --> 00:20:37,150
And moving on the integrations pattern.

326
00:20:37,150 --> 00:20:41,280
So this is where we started with when
you want to implement graph database

327
00:20:41,280 --> 00:20:45,690
you wanted to implement in an area
and fashion that's one of the more

328
00:20:45,780 --> 00:20:47,940
often use cases that we have seen.

329
00:20:48,480 --> 00:20:50,900
But it is not limited to that, right?

330
00:20:51,210 --> 00:20:54,480
But even within a near real time
there are multiple different

331
00:20:54,600 --> 00:20:56,580
methodologies that we can opt into.

332
00:20:56,935 --> 00:21:01,845
One of the key thing is the change
data capture which is the logs that are

333
00:21:01,845 --> 00:21:04,485
retrieved from a traditional databases.

334
00:21:04,925 --> 00:21:10,240
Or you can have a custom connector that
kind of reads from a traditional database

335
00:21:10,280 --> 00:21:15,620
and then start pumping in data using a
streaming platform in the form of a CDC.

336
00:21:15,950 --> 00:21:18,110
So you can have data integration.

337
00:21:18,720 --> 00:21:23,640
Pipelines that you can connect to graph
databases or even connectors that are

338
00:21:23,640 --> 00:21:27,865
available using streaming platforms that
you can use as an integration pattern.

339
00:21:28,050 --> 00:21:31,450
This is one of the most common
thing that that often that we

340
00:21:31,450 --> 00:21:33,040
see with graph implementations.

341
00:21:34,010 --> 00:21:36,680
And the second thing is
the graph QL integration.

342
00:21:36,680 --> 00:21:42,095
So this is where you see graph as a
database kind of retrieve the results.

343
00:21:42,795 --> 00:21:44,475
Identify the relationship.

344
00:21:44,815 --> 00:21:47,605
So often we get the question
that you have a graph database,

345
00:21:47,605 --> 00:21:49,105
and why do you need a graph ql?

346
00:21:49,325 --> 00:21:54,045
So this is where we see the heavy
lifting is done by the graph.

347
00:21:54,045 --> 00:21:59,815
That means identifying the unknown
relationship, multi hop analysis scanning

348
00:21:59,815 --> 00:22:05,555
through a large volume of data and
bringing those information out of it.

349
00:22:05,825 --> 00:22:10,625
Is done through graph database, but
then you often need to integrate

350
00:22:11,045 --> 00:22:13,085
through a schematic layer, right?

351
00:22:13,085 --> 00:22:18,735
And where we expose the data to Y
end consumers in the form of a UI

352
00:22:18,735 --> 00:22:21,045
or any other integrations, right?

353
00:22:21,585 --> 00:22:23,595
That is where Graph QL comes into play.

354
00:22:23,595 --> 00:22:27,445
That if you wanted to if you
wanted to match the data between

355
00:22:27,445 --> 00:22:29,155
a customer platform with.

356
00:22:29,760 --> 00:22:35,235
With the cybersecurity or a fraud analysis
data, that is where graph brings these

357
00:22:35,235 --> 00:22:39,555
two data together, stitch them together
in the form of a schematic layer and

358
00:22:39,555 --> 00:22:41,835
present it in the graph QL integration.

359
00:22:42,655 --> 00:22:45,055
So that is another integration
pattern to look into.

360
00:22:45,055 --> 00:22:49,225
It depends again on the use case how
you are going to consume the data.

361
00:22:49,805 --> 00:22:53,845
And then and very own use case
that is the analytical integration

362
00:22:53,845 --> 00:22:58,205
that integrates with warehouses
or from big data platforms.

363
00:22:58,770 --> 00:23:02,190
That often we go with Apache
Spark connectors that kind of

364
00:23:02,190 --> 00:23:06,625
does the heavy lifting with
large data of large data volume.

365
00:23:06,965 --> 00:23:13,065
And you can have connectors like press two
or Tryo if you wanted to run SQL queries.

366
00:23:13,135 --> 00:23:17,025
Because these connectors provide a
schematic layer and you can run and you

367
00:23:17,025 --> 00:23:19,275
can some of these graph database provides.

368
00:23:20,110 --> 00:23:25,815
The ability to run a direct SQL queries
that you translate under the hood to,

369
00:23:25,960 --> 00:23:28,540
to convert them into graph queries.

370
00:23:29,040 --> 00:23:32,530
Moving on monitoring and observability.

371
00:23:32,580 --> 00:23:35,820
This is monitoring and observability
is standard across different

372
00:23:35,820 --> 00:23:39,360
systems, but what is the thing to
look after in the graph database?

373
00:23:40,045 --> 00:23:43,465
The key things to look after is the
query complexity metrics, right?

374
00:23:43,465 --> 00:23:48,235
So you have tons of queries that
that, that are being run, right?

375
00:23:48,745 --> 00:23:52,075
But often you wanted to see what
are the queries that perform bad?

376
00:23:52,525 --> 00:23:56,635
And what is the traversal path
that these queries are taking?

377
00:23:57,595 --> 00:24:01,105
And what are the queries that
often use high memory, right?

378
00:24:01,135 --> 00:24:03,275
How do we how do we improvise them?

379
00:24:03,275 --> 00:24:05,735
So this is the bottleneck
identification layer.

380
00:24:06,045 --> 00:24:09,500
Whether it some of the strategies
that we looked at, whether it needs

381
00:24:09,500 --> 00:24:14,130
partitioning, whether it is a hot, cold
part hot cold partition that, that needs

382
00:24:14,145 --> 00:24:17,535
to be addressed or whether it can be
handled through some compression logic.

383
00:24:17,755 --> 00:24:22,075
Identifying this complexity
metrics is one thing to look after.

384
00:24:22,435 --> 00:24:25,735
And the heat maps again, the
heart VAEs and heart edges.

385
00:24:26,435 --> 00:24:31,605
Often that if you have a customer base
and that is very kind of centric towards

386
00:24:31,605 --> 00:24:36,795
a particular region those are things to
look after from this heat maps, right?

387
00:24:37,275 --> 00:24:38,775
And distributed tracing.

388
00:24:38,825 --> 00:24:43,545
This is a fine grain analysis to
find out when you have a query that

389
00:24:43,845 --> 00:24:47,835
probably doesn't perform good, you
break it down into individual layers.

390
00:24:48,280 --> 00:24:52,380
There are you can achieve it using
any of the logging platforms and

391
00:24:52,380 --> 00:24:56,795
often it is it can be done through
the native graph logs itself.

392
00:24:57,155 --> 00:25:00,485
You can break down the different
layers of the query to find out

393
00:25:00,575 --> 00:25:04,945
which layer is performing back or
or needs to be improvised so you

394
00:25:04,945 --> 00:25:07,685
can get the idea what can be done.

395
00:25:07,935 --> 00:25:13,075
The next bullet points we cover about,
providing a custom dashboard that kind of

396
00:25:13,075 --> 00:25:18,605
captures these these use cases, but what
different metrics that need to be adopted.

397
00:25:18,605 --> 00:25:23,045
Things like cluster coefficient
trends, degree distribution changes,

398
00:25:23,045 --> 00:25:24,845
and component size distribution.

399
00:25:24,845 --> 00:25:29,285
The, so these are some of the metrics
that, that, that needs to be looked into.

400
00:25:29,785 --> 00:25:30,455
And yeah.

401
00:25:31,065 --> 00:25:31,875
Moving on.

402
00:25:31,925 --> 00:25:34,445
So you have a graph
database up and running.

403
00:25:34,565 --> 00:25:37,355
How are we going to back
up and recover the data?

404
00:25:37,705 --> 00:25:41,785
Because you have customers who
rely upon this data, so you need

405
00:25:41,785 --> 00:25:43,795
to have a reliability score on it.

406
00:25:44,215 --> 00:25:46,555
So how do we do backup and recovery?

407
00:25:47,195 --> 00:25:51,785
So one of the most common things
that people recommend is to go

408
00:25:51,785 --> 00:25:54,155
for a full graph backup because.

409
00:25:54,545 --> 00:25:59,945
What that means is that you don't want
to do an incremental backup because

410
00:25:59,945 --> 00:26:03,645
this is not a traditional database that
you get the logs the incremental logs

411
00:26:03,645 --> 00:26:05,755
out of it, and then and then store it.

412
00:26:05,755 --> 00:26:09,105
Because these are all
interconnected graph data.

413
00:26:09,375 --> 00:26:14,755
So often we try to use, to the extent
possible, we try to adopt full graph

414
00:26:14,915 --> 00:26:17,225
backup that is the most reliable one.

415
00:26:17,745 --> 00:26:22,465
But when you when you start implementing
real time solutions that those are

416
00:26:22,465 --> 00:26:28,455
not often directly possible so there
are even sourced incremental backups

417
00:26:28,455 --> 00:26:32,595
that are available that are largely
supported by all these databases.

418
00:26:32,695 --> 00:26:33,000
You just.

419
00:26:33,805 --> 00:26:39,630
Take the running event logs and you
are able to build the snapshots out

420
00:26:39,630 --> 00:26:44,780
of this and that's the next level of
reliable backups that you can get.

421
00:26:45,180 --> 00:26:48,690
But it, there are some trade
offs that need to be looked at.

422
00:26:48,720 --> 00:26:52,520
What is the RTO and what is
RT objective for the for this

423
00:26:52,520 --> 00:26:57,270
particular strategy that you wanted
to set as in business expectation?

424
00:26:58,210 --> 00:27:03,570
Finally there are some advanced techniques
that are that are being used in some

425
00:27:03,700 --> 00:27:08,760
cases which are like multi version
concurrency control that basically.

426
00:27:09,260 --> 00:27:13,180
Takes backup in a real time
manner without blocking, right?

427
00:27:13,580 --> 00:27:18,410
But again, these are things that are
new in the market and that needs that's

428
00:27:18,410 --> 00:27:21,600
being explored in different use cases.

429
00:27:21,700 --> 00:27:25,090
When it comes to some of the
critical use cases, I think those

430
00:27:25,095 --> 00:27:29,385
are still in the experimentation
stage as I would like to point.

431
00:27:29,885 --> 00:27:32,275
Yeah, so this is capacity planning.

432
00:27:32,555 --> 00:27:36,085
So you are ready to
build a graph database.

433
00:27:36,355 --> 00:27:40,615
What are the key things to look up
to when it comes to capacity planning

434
00:27:40,675 --> 00:27:43,415
as a, as UD the architect hat.

435
00:27:44,040 --> 00:27:47,460
What are the out of the box
strategies that are available when

436
00:27:47,460 --> 00:27:49,260
you wanted to do capacity planning?

437
00:27:49,800 --> 00:27:55,290
There are some proven methodologies and
proven tools or accelerators available.

438
00:27:55,630 --> 00:27:58,300
For example, the synthetic
graph generator, that is

439
00:27:58,300 --> 00:28:00,345
a tool that you can use.

440
00:28:00,455 --> 00:28:03,145
So you feed in some of
the informations about.

441
00:28:03,645 --> 00:28:07,995
What, how many tis, how many edges
that you wanted to build, and what is

442
00:28:07,995 --> 00:28:09,885
the data size that you are expecting?

443
00:28:10,225 --> 00:28:14,365
You feed in that information and
it gives you approximate figure

444
00:28:14,365 --> 00:28:15,835
of what you should be looking at.

445
00:28:15,835 --> 00:28:18,085
So that is a great tool to start with.

446
00:28:18,625 --> 00:28:22,605
And one of the most reliable
thing that I would say is the

447
00:28:22,605 --> 00:28:24,105
comprehensive load testing.

448
00:28:24,495 --> 00:28:26,790
That is to actually start a POC.

449
00:28:27,420 --> 00:28:31,860
But you run different scenarios
that, that, like things like

450
00:28:31,860 --> 00:28:33,240
the burst scenarios, right?

451
00:28:33,800 --> 00:28:35,420
You a COOs testing, right?

452
00:28:35,470 --> 00:28:42,330
You test it and find out, because
often with graph database the data is

453
00:28:42,380 --> 00:28:45,840
unpredictable because you often find new.

454
00:28:46,590 --> 00:28:51,010
Patterns of data, because that's the thing
that the graph database, that is, that

455
00:28:51,010 --> 00:28:55,910
you find out different pattern patterns
of data that's something unknown to you.

456
00:28:56,220 --> 00:29:01,920
So the load testing is a great way
in, in my view, but it takes maturity

457
00:29:01,920 --> 00:29:04,340
time and then memory modeling.

458
00:29:04,650 --> 00:29:09,560
This is similar to load testing,
but you you try to build a complex

459
00:29:09,560 --> 00:29:14,375
scenario that you try to query at
least one percentage of your graph

460
00:29:14,885 --> 00:29:20,375
that has a large set of edges and s
that are being connected to, and you

461
00:29:20,375 --> 00:29:24,570
try to query at least one percentage
of the graph data from that, right?

462
00:29:24,575 --> 00:29:28,640
So that is basically you try to
put your system under stress.

463
00:29:29,055 --> 00:29:31,155
And find out how your system performs.

464
00:29:31,495 --> 00:29:34,610
So you are basically planned
for the worst in that case.

465
00:29:34,820 --> 00:29:39,630
This is true when you wanted to
use for your analytical use cases

466
00:29:39,680 --> 00:29:41,270
this memory modeling stresses.

467
00:29:41,770 --> 00:29:43,720
So that, that's from a capacity planning.

468
00:29:43,750 --> 00:29:46,450
Certainly the tools is
the good starting point.

469
00:29:46,780 --> 00:29:50,080
And once you are into some level of
maturity from the implementation,

470
00:29:50,080 --> 00:29:52,780
then you can go for the load
testing or the stress testing.

471
00:29:53,280 --> 00:29:57,480
So the, then that comes the
architectural trade off, right?

472
00:29:57,530 --> 00:30:03,790
So what do you want, how do you want your
system to look like, from a consistency

473
00:30:03,790 --> 00:30:06,430
versus performance standpoint, right?

474
00:30:06,430 --> 00:30:12,430
So often, like we, we talked about
horizontal and distributed systems, right?

475
00:30:13,300 --> 00:30:15,070
How do you want your system to be?

476
00:30:15,070 --> 00:30:19,090
Like, whether you wanted a full
consistency or eventual consistency,

477
00:30:19,720 --> 00:30:22,950
or you wanted handle the performance
as the major thing, right?

478
00:30:22,950 --> 00:30:29,440
So this is where you will have to find
out what is the targets target users

479
00:30:29,440 --> 00:30:33,935
that you wanted to address whether or
use case also that you wanted to address,

480
00:30:33,935 --> 00:30:34,860
whether you wanted to handle the.

481
00:30:35,540 --> 00:30:39,800
A real time use case or you wanted
to handle an analytical use case, so

482
00:30:39,800 --> 00:30:42,620
that kind of drives the cap there.

483
00:30:43,120 --> 00:30:46,160
Whether you wanted to put
consistency first or you wanted

484
00:30:46,160 --> 00:30:48,005
to handle performance first.

485
00:30:48,750 --> 00:30:51,510
So that that, that's
one thing to look after.

486
00:30:51,790 --> 00:30:56,835
So that, that is purely based on what
user base that you wanted to handle.

487
00:30:56,835 --> 00:31:01,575
So oftentimes that I would like to
point out is that even though you have

488
00:31:01,575 --> 00:31:06,375
the same set of data, you might want to
consider multiple graph instances because

489
00:31:06,430 --> 00:31:09,315
your target audience could vary, right?

490
00:31:09,510 --> 00:31:10,265
And that is where it is.

491
00:31:10,935 --> 00:31:12,465
And then flexibility.

492
00:31:12,495 --> 00:31:16,545
This is more in terms of choosing your
graph database, whether you wanted to go

493
00:31:16,545 --> 00:31:21,645
for a typed or a property graph, or it
is an adaptive indexing graph database.

494
00:31:21,645 --> 00:31:26,145
So the way this is different in the
way that the graph stores the data

495
00:31:26,605 --> 00:31:30,725
whether it is like a vertex and edge
based schema, which is a type graph.

496
00:31:30,845 --> 00:31:35,800
And then whether you wanted to handle
the property graph, which is more towards

497
00:31:35,850 --> 00:31:40,410
knowledge, graph building, or whether you
wanted to do an adaptive indexing that you

498
00:31:40,410 --> 00:31:45,200
build an index, run the query, and then
the indexes gets dropped at the end of it.

499
00:31:45,620 --> 00:31:49,370
So it, it's more in terms of
choosing what type of draft database

500
00:31:49,370 --> 00:31:50,805
that you wanted to choose, right?

501
00:31:50,870 --> 00:31:57,140
So these are things to to address at
the time of choosing the database.

502
00:31:57,770 --> 00:32:02,570
At the time of implementing a
particular graph use case, and at the

503
00:32:02,570 --> 00:32:09,150
time of finding out which target user
that you wanted to address, right?

504
00:32:09,150 --> 00:32:13,180
So those are the key things that
will drive this particular strategy.

505
00:32:13,680 --> 00:32:19,020
So we talked about graph database
that, that's being used across and how.

506
00:32:19,530 --> 00:32:22,430
What is the current state
that that it is right now?

507
00:32:22,820 --> 00:32:25,910
What are the future
directions for graph database?

508
00:32:26,040 --> 00:32:29,340
So it, it has its own limitations.

509
00:32:29,560 --> 00:32:34,780
Things that, that needs to be looked
up is that graph often requires

510
00:32:34,780 --> 00:32:38,990
high computing because of the way
it handles com complex queries.

511
00:32:39,350 --> 00:32:43,930
So there are there are work that,
that are being adopted to see how to

512
00:32:44,050 --> 00:32:49,320
improve the hardware acceleration,
to implement things like GPU and

513
00:32:49,320 --> 00:32:51,960
handling graph algorithms natively.

514
00:32:52,310 --> 00:32:56,980
So those are things that are
being in, in, in work right now.

515
00:32:57,380 --> 00:33:01,350
And the machine learning I would say
it is, it has already been into a good.

516
00:33:02,110 --> 00:33:06,580
State right now, because often the
graph has worked along with machine

517
00:33:06,580 --> 00:33:11,680
learning integration, but it is being
expanded largely that both graph

518
00:33:11,680 --> 00:33:15,670
and machine learning comprehend each
other to build complex use cases.

519
00:33:16,000 --> 00:33:20,700
And even in, in some cases, I have
seen AI based implementation users.

520
00:33:21,345 --> 00:33:23,525
Graph to a watched extent, right?

521
00:33:23,525 --> 00:33:27,875
So that that's a major work area
that's currently in in progress.

522
00:33:28,145 --> 00:33:33,025
And then serverless graph databases
are, excuse me like cases where there

523
00:33:33,025 --> 00:33:38,505
are a lot of graph providers that they
provide their offerings in, in the cloud.

524
00:33:38,885 --> 00:33:42,385
There, you don't have to worry
about the hardware behind it.

525
00:33:42,385 --> 00:33:45,215
And that's coming as
a serverless offering.

526
00:33:45,605 --> 00:33:50,115
So that's also another growing area that
I wanted to quickly touch upon where

527
00:33:50,595 --> 00:33:53,320
graph is heading towards graph databases.

528
00:33:53,820 --> 00:33:54,090
Yeah.

529
00:33:54,265 --> 00:33:56,345
At last I wanted to give a quick.

530
00:33:56,780 --> 00:33:59,850
Overview what are the different
areas that we talked about?

531
00:33:59,900 --> 00:34:05,070
Graph being a real time implementation,
near real time implementation that often

532
00:34:05,070 --> 00:34:07,790
follows distributed implementation.

533
00:34:08,190 --> 00:34:12,960
And mainly for addressing the large
amount of data that it handles.

534
00:34:13,590 --> 00:34:17,824
And when it comes to integration, it
indicates with event driven architecture.

535
00:34:18,389 --> 00:34:24,889
To handle use cases on a ML or
fraud or cybersecurity use cases.

536
00:34:25,399 --> 00:34:30,719
And from a storage standpoint we kind of
balance it out between the cost versus

537
00:34:30,719 --> 00:34:35,529
performance the three different scenarios
that we talked about in memory, hybrid

538
00:34:35,529 --> 00:34:39,729
and in memory, hybrid are some of the
key areas that we talked about, and

539
00:34:39,729 --> 00:34:41,529
the persistent storage is another one.

540
00:34:42,019 --> 00:34:44,109
So those are key areas.

541
00:34:44,529 --> 00:34:47,154
I, at the same time too, when
I want to conclude, I wanted to

542
00:34:47,154 --> 00:34:51,024
make sure that, not every case use
case gets implemented with graph.

543
00:34:51,474 --> 00:34:54,894
So we need to be very clear about
what use case that we wanted to

544
00:34:54,894 --> 00:34:58,884
implement and what is the target
audience that we wanted to address.

545
00:34:59,304 --> 00:35:02,484
And the kind of strategy that
we wanted to implement when we

546
00:35:02,889 --> 00:35:04,734
in, when we do data integration.

547
00:35:05,244 --> 00:35:08,134
And so that will eventually
drive us to success.

548
00:35:08,684 --> 00:35:12,714
I hope this session was
very useful and informative.

549
00:35:13,084 --> 00:35:18,434
And I really look forward you in, in,
in any of the future implementations

550
00:35:18,674 --> 00:35:24,044
that, that you implement these strategies
and, I wish you a great day and success.

551
00:35:24,194 --> 00:35:24,674
Thank you.

