1
00:00:00,500 --> 00:00:03,139
Welcome to today's talk about
building real-time feature

2
00:00:03,139 --> 00:00:06,860
pipelines and JavaScript's role in
modern data-driven applications.

3
00:00:07,310 --> 00:00:11,120
In the next few minutes, I'll walk through
how JavaScript can power the entire

4
00:00:11,120 --> 00:00:16,100
feature lifecycle for ingesting events,
computing features, caching them for

5
00:00:16,100 --> 00:00:21,020
sub millisecond rates, and running ML
inference at the edge and in the browser.

6
00:00:21,520 --> 00:00:25,119
Coming to the evolution of web
applications, we've moved from batch

7
00:00:25,119 --> 00:00:29,470
driven server rendered pages to
dynamic personalized applications.

8
00:00:29,860 --> 00:00:35,470
Modern users expect real-time updates
and context aware UI that shift requires

9
00:00:35,470 --> 00:00:39,970
features, computed now not overnight,
and delivered with predictable latency.

10
00:00:40,830 --> 00:00:44,220
In traditional architecture, as you
can see, we've had server rendered

11
00:00:44,220 --> 00:00:47,180
pages, batch data processing,
and the feature updates were

12
00:00:47,540 --> 00:00:49,980
consistently and delayed by a lot.

13
00:00:50,670 --> 00:00:56,449
So in modern data driven applications,
the users are expecting more realtime

14
00:00:56,449 --> 00:01:01,550
feature computations and instant model
inference and hyper-personalized content,

15
00:01:01,730 --> 00:01:05,840
which is modeled according to their needs,
their activity, and their preferences.

16
00:01:06,589 --> 00:01:10,520
To the feature pipeline challenge in
this, the real time pipelines face.

17
00:01:10,520 --> 00:01:11,839
Three hard problems.

18
00:01:12,350 --> 00:01:16,970
Past feature computation, fresh,
yet efficient caching and consistent

19
00:01:16,970 --> 00:01:19,910
logic across node servers,
edge workers, and browsers.

20
00:01:20,410 --> 00:01:24,580
In feature computation we have steps
like processing raw data into useful

21
00:01:24,580 --> 00:01:30,059
features that to at scale with as less
data garbage as possible and maintaining

22
00:01:30,059 --> 00:01:31,559
low latency and high throughput.

23
00:01:32,100 --> 00:01:35,550
Within caching, we want to balance
freshness with performance.

24
00:01:35,759 --> 00:01:40,320
We want to have as close to real time
data as possible, but at the same time,

25
00:01:40,350 --> 00:01:44,520
inter with intelligent storage and
retrieval mechanisms, we need to make

26
00:01:44,520 --> 00:01:47,339
sure that the performance stays intact.

27
00:01:48,134 --> 00:01:53,445
If we miss any one of these, we get stale
experiences, training, serving SKU and

28
00:01:53,445 --> 00:01:58,064
jittery performances which eventually
results in cross environment consistency.

29
00:01:58,564 --> 00:02:01,835
Now coming to why we need
JavaScript for feature pipelines,

30
00:02:01,895 --> 00:02:03,365
JavaScript is universal.

31
00:02:03,904 --> 00:02:09,274
The same modules run on node at
the edge and in browser it has

32
00:02:09,274 --> 00:02:15,575
rich ecosystem for streams, caches,
inference, and syco aate maps cleanly.

33
00:02:15,695 --> 00:02:19,745
So that map cleanly, that maps
cleanly to event driven workloads.

34
00:02:20,490 --> 00:02:24,440
So when we come to native ing support,
it's about built in JavaScript has

35
00:02:24,440 --> 00:02:27,710
built in ing and evade patterns
and event prevent architecture.

36
00:02:28,220 --> 00:02:32,960
That means that, it's readily suited
for realtime ingestions or realtime data

37
00:02:32,960 --> 00:02:38,210
streams, and there is a non characteristic
of non-blocking IO operations, which

38
00:02:38,210 --> 00:02:40,130
is essential for low latency pipelines.

39
00:02:40,935 --> 00:02:45,385
Coming to the ecosystem JavaScript
leverages mature libraries for data

40
00:02:45,385 --> 00:02:51,295
processing, streaming caching, also from
Apache Kafka clients to TensorFlow js.

41
00:02:51,505 --> 00:02:54,235
The JavaScript ecosystem
provides production ready

42
00:02:54,235 --> 00:02:56,155
tools for every pipeline stage.

43
00:02:56,655 --> 00:03:00,415
So when it comes to as I said
previously, when it comes to streaming

44
00:03:00,415 --> 00:03:05,225
data with Kafka, Evenflow looks
like this, it, it starts with an

45
00:03:05,225 --> 00:03:09,605
ingestion and then to transform and
then to publish With Kafka in Node JI.

46
00:03:10,375 --> 00:03:15,115
We consume user events, compute rolling
counts or session stats, and publish

47
00:03:15,115 --> 00:03:17,485
features to red our downstream topics.

48
00:03:18,295 --> 00:03:21,205
We keep transformer, we keep transforms.

49
00:03:21,205 --> 00:03:25,995
Item, pot and window, so retrace and
back pressure, don't corrupt results.

50
00:03:26,385 --> 00:03:30,545
Here's an example and you can see in the
slide which is about consuming new events

51
00:03:30,545 --> 00:03:33,055
or creating events for downstream topics.

52
00:03:33,790 --> 00:03:38,830
And use this Kafka, we try to build a
real time feature serving architecture,

53
00:03:39,330 --> 00:03:41,760
which contains, which essentially
contains four building blocks.

54
00:03:42,260 --> 00:03:46,700
One is a future competition
service in node, where Node just

55
00:03:46,700 --> 00:03:50,240
microservices process the incoming
data streams and compute features

56
00:03:50,550 --> 00:03:55,110
using the con corresponding business
logic and corresponding statistical

57
00:03:55,110 --> 00:03:58,620
transformations, which we build as
data engineers or data scientists

58
00:03:58,650 --> 00:04:00,270
based on the needs of the company.

59
00:04:00,900 --> 00:04:03,840
Two Redis feature store with sensible tls.

60
00:04:04,150 --> 00:04:08,590
We where with the register, we, the
computed features are cash in red

61
00:04:08,830 --> 00:04:12,290
with a configurable time to live
values enabling sub millisecond

62
00:04:12,290 --> 00:04:14,300
retrieval for online inferences.

63
00:04:14,570 --> 00:04:18,520
And coming to three, it's about the
importance of web sockets, which

64
00:04:18,520 --> 00:04:20,320
push updates instead of polling.

65
00:04:20,860 --> 00:04:22,960
And fourth is the edge or browser in.

66
00:04:23,535 --> 00:04:28,725
So the UI reacts instantly because that's
the end goal of interacting and app.

67
00:04:28,995 --> 00:04:31,305
The where the application
interacts with the user.

68
00:04:31,755 --> 00:04:36,015
And that browser applications used
cached features with TensorFlow JS

69
00:04:36,045 --> 00:04:41,745
or O-N-N-X-J js, and any similar
libraries to run ML models locally,

70
00:04:41,955 --> 00:04:43,635
eliminating the server round trips.

71
00:04:44,535 --> 00:04:45,945
So when it comes to.

72
00:04:46,740 --> 00:04:50,460
Let, when it comes to implementing
efficient caching, caching strategies

73
00:04:50,520 --> 00:04:55,290
within Redis or similar tool like a pub
sub server we use multilayer caching

74
00:04:55,350 --> 00:04:57,960
by access pattern, browser memory.

75
00:04:58,020 --> 00:05:04,200
For OnPage hot features, Redis for
shared online reads and edge cache

76
00:05:04,230 --> 00:05:10,020
for global reach, we balance freshness
and cost with TTLs plus pops up or

77
00:05:10,075 --> 00:05:12,415
web sockets to refresh just in time.

78
00:05:12,915 --> 00:05:17,215
As you can see in the diagram, we have
a browser memory cache, and between

79
00:05:17,305 --> 00:05:21,685
the CDN and the actual browser, we
have a distributed cache, which is

80
00:05:21,685 --> 00:05:26,455
distributed across regions and is
readily available for users based on

81
00:05:26,455 --> 00:05:30,625
their needs, based on the activity and
based on the traffic through, from and

82
00:05:30,625 --> 00:05:32,305
through the server for that website.

83
00:05:32,555 --> 00:05:36,784
Because some websites or some
applications do have like peak

84
00:05:36,784 --> 00:05:38,315
times, off peak, et cetera.

85
00:05:38,815 --> 00:05:44,564
So when it comes to adhering to client
side ml inference with JavaScript we have

86
00:05:44,644 --> 00:05:50,380
today browsers and edge runtime can run
realtime ml. We have lot of browsers.

87
00:05:50,500 --> 00:05:54,729
Like recently, Chad GPT released a
browser called Atlas that was completely

88
00:05:54,729 --> 00:05:59,619
built on top of charge GPT and running
AI models within the system locally.

89
00:06:00,189 --> 00:06:03,979
So we want to load a lightweight
model, read the cached features.

90
00:06:04,254 --> 00:06:05,844
Call predict and render.

91
00:06:06,414 --> 00:06:10,164
So what happens with this is
it reduces latency, it protects

92
00:06:10,164 --> 00:06:12,264
privacy and lower serving costs.

93
00:06:12,764 --> 00:06:18,494
Here we have an example of using Tanza j
TensorFlow gs, which actually runs a model

94
00:06:18,604 --> 00:06:21,044
locally on the system, on the computer.

95
00:06:21,344 --> 00:06:25,094
And then it it performs feature
engineering, extract the features

96
00:06:25,094 --> 00:06:29,579
and does a prediction based on the
user activity or user usage patterns.

97
00:06:30,079 --> 00:06:34,519
So when it comes to ensuring server
client logic, consistency we, one

98
00:06:34,519 --> 00:06:37,849
of the biggest challenges in feature
pipelines is maintaining identical

99
00:06:37,849 --> 00:06:39,679
behavior across different environments.

100
00:06:40,099 --> 00:06:44,559
And removing a, like we want to remove
inconsistent feature competition, which

101
00:06:44,559 --> 00:06:48,629
leads to training, serving skew, and
the model performance degrades despite.

102
00:06:48,969 --> 00:06:52,949
Like already we are deploying a
lightweight model onto the system.

103
00:06:53,129 --> 00:06:58,239
So we want to keep the data as pristine
or as clean as possible so that we

104
00:06:58,239 --> 00:07:00,549
avoid unpredictable user experiences.

105
00:07:01,059 --> 00:07:05,169
So for, and we want to ship isomorphic
modules for the feature logic.

106
00:07:05,529 --> 00:07:10,419
We test with same fixtures in node and
the browser, and we locks schemas with

107
00:07:10,419 --> 00:07:15,429
TypeScript types or J schema and monitor
distribution to catch the drift early.

108
00:07:15,929 --> 00:07:19,889
So these are the four steps which
we perform to make sure that we want

109
00:07:19,889 --> 00:07:24,469
to en to ensure the server client
logic is performing consistently

110
00:07:24,469 --> 00:07:25,999
across different environments.

111
00:07:26,499 --> 00:07:30,949
And once we do this performance
tuning, we want to make sure that

112
00:07:30,949 --> 00:07:32,869
we monitor the perfor performance.

113
00:07:33,109 --> 00:07:36,579
So for that, we use a different me
like important metrics like track.

114
00:07:37,154 --> 00:07:42,224
Where we track throughput, error rates,
resource usage, feature freshness, and

115
00:07:42,224 --> 00:07:44,594
end-to-end latency from ingestion to ui.

116
00:07:45,344 --> 00:07:49,604
We use these signals to tune
window sizes, cash details, model

117
00:07:49,604 --> 00:07:51,494
size, and fan out strategies.

118
00:07:51,854 --> 00:07:56,644
I. So some of the key metrics to track
over here are in within the latency

119
00:07:56,644 --> 00:07:58,564
distributions which is very important.

120
00:07:58,774 --> 00:08:02,404
We have the P 50, P 95 and P 99 latency.

121
00:08:02,674 --> 00:08:07,254
We which for like feature competition
retrieval and inference operations.

122
00:08:07,949 --> 00:08:12,789
We watch these latencies and also other
attributes like features per second, cash

123
00:08:12,789 --> 00:08:17,559
hit ratio, web socket delivery success
and skew between training and serving

124
00:08:18,159 --> 00:08:22,500
These metrics will actually help us
to understand where to optimize first.

125
00:08:22,990 --> 00:08:28,830
So in some cases we might need to increase
the resource utilization as I said,

126
00:08:28,830 --> 00:08:33,030
during peak on, off peak times, or in
some cases we need to scale it down to

127
00:08:33,030 --> 00:08:38,010
save some costs, or in some cases we might
need to understand or tweak the feature

128
00:08:38,010 --> 00:08:42,180
engineering so that we can create fresh
features rather than using stale features.

129
00:08:42,680 --> 00:08:47,665
So when it comes to practical application
patterns we generally think about

130
00:08:47,665 --> 00:08:52,095
three common wins recommendations that
update with every interaction which are

131
00:08:52,155 --> 00:08:56,445
highly used in recommendation engines,
be it streaming app or any e-commerce

132
00:08:56,445 --> 00:08:59,685
applications, et cetera, where we
build personalized product or content

133
00:08:59,685 --> 00:09:05,035
recommendations based on user interaction,
item similarity, computations, and and

134
00:09:05,245 --> 00:09:07,435
different AB testing, Strat strategy.

135
00:09:08,125 --> 00:09:12,385
And there's also a thing of personalized
UI and predictive prefetching.

136
00:09:13,015 --> 00:09:17,080
What that does is it does dynamic
content filtering, and it does ui

137
00:09:17,195 --> 00:09:22,045
contextual UI adaptations and based
on the user's next step or the

138
00:09:22,045 --> 00:09:24,305
prediction, it does the local model do.

139
00:09:24,615 --> 00:09:27,675
It fetches it fetches the
data or it fetches what the

140
00:09:27,675 --> 00:09:29,355
user might need in future.

141
00:09:29,685 --> 00:09:31,990
And does adaptive notification timing.

142
00:09:32,680 --> 00:09:36,100
We also have live analytics with
client side aggregations, which

143
00:09:36,100 --> 00:09:39,580
are streamed over web sockets
for us to monitor in the backend.

144
00:09:40,080 --> 00:09:43,140
Some of the architectural best
practices to achieve these goals

145
00:09:43,140 --> 00:09:47,060
are decomp, compute, decouple
compute from serving so you can

146
00:09:47,060 --> 00:09:49,040
scale the end version independently.

147
00:09:49,850 --> 00:09:54,620
Designed for failure with fallback
values and grateful TTLs version.

148
00:09:54,620 --> 00:09:58,775
Features like APIs to roll forward
are back or roll back safely.

149
00:09:59,275 --> 00:10:03,925
So to achieve this, when it comes to
implementation roadmap, we start simple.

150
00:10:04,285 --> 00:10:08,875
We ship offline features with the
cache, and then slowly start adding

151
00:10:08,875 --> 00:10:10,855
real time computation with Kafka.

152
00:10:11,165 --> 00:10:13,415
And then we start by introducing
client side inference.

153
00:10:13,885 --> 00:10:16,705
With small models, which can
be installed locally or which

154
00:10:16,705 --> 00:10:18,415
can understand user patterns.

155
00:10:18,835 --> 00:10:22,685
And then we scale based on observed
metrics which is completely data

156
00:10:22,685 --> 00:10:25,655
driven, D data bag and not assumptions.

157
00:10:26,155 --> 00:10:29,605
Coming to some of the key takeaways
when it comes to building this

158
00:10:29,605 --> 00:10:31,465
end-to-end pipelines using JavaScript.

159
00:10:31,885 --> 00:10:36,655
JavaScript is basically a one language
across the stack that reduces friction

160
00:10:36,715 --> 00:10:41,575
because it can be used on the UI side,
on the client side, on the server side,

161
00:10:41,725 --> 00:10:46,335
so it can be used across the entire stack
so that, it reduces friction within the

162
00:10:46,335 --> 00:10:50,895
engineers and also the makes the code
readable, makes the maintenance easier.

163
00:10:51,355 --> 00:10:54,895
And streaming plus caching delivers
fresh, low latency features.

164
00:10:55,555 --> 00:10:58,735
Client said ML is production
ready and privacy friendly.

165
00:10:59,395 --> 00:11:02,965
Together, these enable
hyper-personalized experiences at scale.

166
00:11:03,465 --> 00:11:04,275
Thank you so much.

167
00:11:04,635 --> 00:11:06,165
I'm Chen Shekar.

168
00:11:06,165 --> 00:11:09,255
Shari Kuri again, and I'm happy
to connect and discuss more.

169
00:11:09,525 --> 00:11:09,915
Thank you.

