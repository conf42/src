1
00:00:01,000 --> 00:00:01,779
Hi everyone.

2
00:00:02,079 --> 00:00:03,400
It's great to be here.

3
00:00:03,610 --> 00:00:07,840
I am Georgina Fu, a machine learning
engineer at gco, where I focus

4
00:00:07,840 --> 00:00:11,680
on building real time AI systems
for video and speech analysis.

5
00:00:12,490 --> 00:00:15,490
Today I want to walk you through a
project that's really important for

6
00:00:15,490 --> 00:00:20,230
everyone in our team, how we are using
AI to detect and intercept harmful

7
00:00:20,230 --> 00:00:21,940
content online, especially things like.

8
00:00:22,580 --> 00:00:26,390
Child sexual abuse material,
and other high risk abuses.

9
00:00:26,930 --> 00:00:30,560
This isn't just a technical show
and tell, I'll take you through the

10
00:00:30,560 --> 00:00:34,609
challenges, the architecture and what
it takes to make this work reliably

11
00:00:34,609 --> 00:00:38,800
at scale, across different regions
and real customer and environments.

12
00:00:39,660 --> 00:00:40,860
So why does this matter?

13
00:00:40,920 --> 00:00:45,520
Let's see, first why we are
even trying to give this type of

14
00:00:45,520 --> 00:00:47,440
solution to the, to our customers.

15
00:00:47,620 --> 00:00:51,880
We are living in a era where people upload
everything from personal vlogs and live

16
00:00:51,880 --> 00:00:53,920
streams to gaming education and more.

17
00:00:54,760 --> 00:00:59,110
A platform like YouTube gets over
500 hours of video per minute,

18
00:00:59,170 --> 00:01:00,730
and that's this one platform.

19
00:01:01,450 --> 00:01:05,350
But within that sea of content,
even a single instance of abuse

20
00:01:05,350 --> 00:01:09,830
material is devastating legally,
reputationally, and morally.

21
00:01:10,760 --> 00:01:12,680
And it's not just in big tech.

22
00:01:13,100 --> 00:01:15,200
We work with smaller platforms too.

23
00:01:15,260 --> 00:01:19,550
Niche streaming sites, educational
platforms, even creators using tools

24
00:01:19,550 --> 00:01:21,860
such as Use Screen or W to host content.

25
00:01:22,490 --> 00:01:25,910
They don't have armies of moderators
who are able to keep up with this

26
00:01:25,910 --> 00:01:27,800
extreme rate of video uploading.

27
00:01:28,460 --> 00:01:31,760
So we ask how can we detect
dangerous content early,

28
00:01:32,030 --> 00:01:35,570
automatically, and responsibly
before it reaches the audience?

29
00:01:36,770 --> 00:01:39,290
Let's be honest, this is not really easy.

30
00:01:39,800 --> 00:01:43,640
There are four big challenges we
identified quickly and we had to overcome.

31
00:01:44,000 --> 00:01:45,980
First, we have class imbalance.

32
00:01:46,370 --> 00:01:48,440
Harmful content is extremely rare.

33
00:01:48,830 --> 00:01:52,670
If you train a model naively, it'll
learn to say everything is fine

34
00:01:52,670 --> 00:01:56,540
and to still be 99.999% accurate.

35
00:01:57,930 --> 00:01:59,220
That's obviously useless.

36
00:01:59,640 --> 00:02:03,630
Class imbalance must be taken
into account during system design,

37
00:02:03,630 --> 00:02:05,280
model training and monitoring.

38
00:02:06,150 --> 00:02:08,280
Second video data is huge.

39
00:02:08,760 --> 00:02:14,340
A 10 minute video in a high definition for
math might be 500 megabytes or even more.

40
00:02:14,760 --> 00:02:20,010
Scanning thousands of videos means dealing
with terabytes of data often in real time.

41
00:02:20,430 --> 00:02:22,170
Smart triggering is critical.

42
00:02:22,170 --> 00:02:26,250
It can help to significantly reduce
the amount of requests the system.

43
00:02:27,185 --> 00:02:28,445
Third, we have privacy.

44
00:02:28,985 --> 00:02:33,515
We can't just dump our user content
into a central server, scan it all.

45
00:02:34,025 --> 00:02:39,755
We are bound by GDPR and more importantly,
by ethics users expect platforms and

46
00:02:39,755 --> 00:02:43,625
infrastructure providers to respect
boundaries, to avoid scanning,

47
00:02:43,625 --> 00:02:47,930
storing, or analyzing their private
content unnecessarily or invasively.

48
00:02:48,770 --> 00:02:51,140
And finally, adversaries evolve.

49
00:02:51,680 --> 00:02:55,790
We have seen harmful content hidden
behind innocent looking cat memes,

50
00:02:56,150 --> 00:03:00,650
obscured by translucent overlays,
distorted with TikTok style filters,

51
00:03:00,980 --> 00:03:04,610
or even encoding into the audio track
using state iconography techniques.

52
00:03:04,610 --> 00:03:06,435
It's wild and it is intentional.

53
00:03:06,915 --> 00:03:08,325
These are not accidents.

54
00:03:08,325 --> 00:03:12,525
There are attempts to bypass any
type of detection system, so our

55
00:03:12,525 --> 00:03:16,185
models can just rely on surface
patterns or static features.

56
00:03:16,215 --> 00:03:21,675
They need to generalize across
manipulations, formats and behaviors.

57
00:03:21,675 --> 00:03:24,225
We haven't explicitly trained them for.

58
00:03:25,940 --> 00:03:30,960
So let's zoom now into our architectures
because this is where everything

59
00:03:30,960 --> 00:03:32,820
we have talked about becomes real.

60
00:03:33,600 --> 00:03:38,370
When we set out to design the system, our
goal wasn't just to build good models,

61
00:03:38,400 --> 00:03:43,170
it was to build the moderation pipeline
that is modular, scalable, and resilient.

62
00:03:43,530 --> 00:03:48,300
Something that could plug into different
environments, serve different types

63
00:03:48,300 --> 00:03:50,250
of customers, and grow over time.

64
00:03:50,640 --> 00:03:52,590
So here's the full pipeline we use.

65
00:03:53,135 --> 00:03:59,165
It all starts with a trigger, and this
is how we design, decide what to scan.

66
00:03:59,645 --> 00:04:03,035
This can be random sampling,
useful for stored content.

67
00:04:03,455 --> 00:04:07,745
It can be met database triggers,
so for example, a spike in uploads

68
00:04:07,745 --> 00:04:11,015
from one account or file with
a suspicious name, similar to

69
00:04:11,375 --> 00:04:12,899
hashes that are available online.

70
00:04:13,795 --> 00:04:17,035
And it can also be custom,
some type of customer signal.

71
00:04:17,275 --> 00:04:21,175
So some customers can take videos
for post publish moderation, and

72
00:04:21,385 --> 00:04:22,735
we'll ingest these events too.

73
00:04:24,205 --> 00:04:27,565
This trigger phase is very important
because we don't want to scan everything.

74
00:04:27,565 --> 00:04:32,305
We prioritize scanning intelligently,
which will save a lot of cost

75
00:04:32,365 --> 00:04:34,375
and will preserve privacy.

76
00:04:35,065 --> 00:04:36,835
Then we have the orchestration layer.

77
00:04:37,409 --> 00:04:40,830
Once the content is selected,
it enter our task engine.

78
00:04:41,189 --> 00:04:45,299
We use seller here, which is a
distributed task queue system, which will

79
00:04:45,299 --> 00:04:49,679
allow us to run tasks, synchronously,
prioritize jobs based on urgency.

80
00:04:49,799 --> 00:04:52,799
We try failed steps and
load balance across.

81
00:04:53,109 --> 00:04:54,520
Multiple worker nodes.

82
00:04:55,390 --> 00:05:00,200
So for, as an example, if a live stream
segment needs scanning in five seconds,

83
00:05:00,290 --> 00:05:05,660
it, this task will jump ahead of a batch
job processing multiple stored content.

84
00:05:06,230 --> 00:05:08,180
We also chart jobs by modality.

85
00:05:08,720 --> 00:05:13,660
So we have vision, audio, and sequence
models that can scale independently.

86
00:05:14,050 --> 00:05:16,600
And then we have pre-processing layer.

87
00:05:17,030 --> 00:05:19,270
Before inference, we
need to prep our media.

88
00:05:19,270 --> 00:05:23,290
So this will include splitting
video into short segments of a few

89
00:05:23,290 --> 00:05:28,390
seconds, extracting and normalizing
audio resizing or re encoding

90
00:05:28,390 --> 00:05:29,710
our model for compatibility.

91
00:05:30,850 --> 00:05:35,830
And data duping content we have
already scanned before this step is

92
00:05:35,830 --> 00:05:39,670
essential to ensure inference runs
consistently and across formats.

93
00:05:40,060 --> 00:05:45,670
Some of our customers upload 4K while
others will use some other cent code deck.

94
00:05:45,670 --> 00:05:50,350
And this layer will standardize every
input so it is ready for the next step.

95
00:05:51,730 --> 00:05:54,250
So right after that, we
have the inference layer.

96
00:05:54,250 --> 00:05:56,980
This is where multi multimodal ai.

97
00:05:58,760 --> 00:06:02,510
We have multiple models running
in parallel, so we have frame

98
00:06:02,510 --> 00:06:03,500
based image classifiers.

99
00:06:04,490 --> 00:06:09,170
We have whisper style speech transcription
and keyword spotting, lightweight temporal

100
00:06:09,260 --> 00:06:11,330
models such as their user, three Ds.

101
00:06:12,200 --> 00:06:15,680
And each of these models will return
its own confidence score and tag.

102
00:06:15,945 --> 00:06:19,955
For example, we have a video that
is marked as high probability

103
00:06:19,955 --> 00:06:21,665
of not safer work content.

104
00:06:21,905 --> 00:06:25,565
And then there is a keyword like
grooming found in the audio.

105
00:06:25,835 --> 00:06:31,055
And then the last model attack the
output as violent motion sequence.

106
00:06:31,505 --> 00:06:34,685
The inference is run in containers
that can scale horizontally

107
00:06:34,895 --> 00:06:38,285
and will log model versions for
traceability and adaptability.

108
00:06:39,730 --> 00:06:43,900
So then we have the post-processing layer
where it will take all the signals that I

109
00:06:43,900 --> 00:06:48,680
described before, and we'll try to combine
them either by normalizing confidence

110
00:06:48,680 --> 00:06:53,595
scores and applying some aggregation
logic or just applying some other

111
00:06:53,605 --> 00:06:55,935
classifier on top of the previous models.

112
00:06:56,445 --> 00:07:00,284
So here we'll attach some
text and optionally enrich

113
00:07:00,284 --> 00:07:01,695
with additional metadata.

114
00:07:01,905 --> 00:07:06,115
For example, the video title, the
language of speech, and so on.

115
00:07:06,715 --> 00:07:08,755
This logic evolves with feedback.

116
00:07:08,815 --> 00:07:13,775
For example, we recently added a rule that
time creates the not safer works scores.

117
00:07:13,865 --> 00:07:17,525
If the video is clearly
educational, like anatomy, lectures.

118
00:07:18,215 --> 00:07:20,765
Then we have the decision layer.

119
00:07:21,105 --> 00:07:23,895
This is where we make the
final moderation decision.

120
00:07:24,255 --> 00:07:29,315
This might mean flagging the content
for manual review, auto blocking if the

121
00:07:29,315 --> 00:07:33,785
score is high enough, sending an alert
to the customer or just locking the

122
00:07:33,785 --> 00:07:36,250
result silently if it is borderline.

123
00:07:37,085 --> 00:07:39,455
We also store a decision trace.

124
00:07:39,490 --> 00:07:43,780
So what the model outputs contributed
to the flag when it happened and why.

125
00:07:44,320 --> 00:07:46,660
And this entire stack is loosely coupled.

126
00:07:46,660 --> 00:07:51,370
So it this means that we are able to
scale or replace some individual component

127
00:07:51,370 --> 00:07:52,960
without breaking the whole system.

128
00:07:53,710 --> 00:07:56,260
So this pipeline is the core
of our moderation engine.

129
00:07:56,470 --> 00:07:59,980
It is designed not to just analyze
content, but to do it in a way that

130
00:07:59,980 --> 00:08:04,990
is scalable, auditable, and adapt,
adaptable to real world use cases.

131
00:08:06,680 --> 00:08:11,600
Let's give a little bit more information
about the models as I, me, I mentioned

132
00:08:11,600 --> 00:08:16,070
we don't rely on a single AI model,
but we use a mo multi-model ensemble.

133
00:08:16,640 --> 00:08:19,250
So for video frames, we use CNNs.

134
00:08:19,520 --> 00:08:23,240
They're similar to what powers
not safer work filters, but fine

135
00:08:23,240 --> 00:08:24,880
tuned on stricter data sets.

136
00:08:25,220 --> 00:08:30,320
So think this step as detecting nudity
or graphic violence in individual frames.

137
00:08:31,370 --> 00:08:33,170
Then we have audio models.

138
00:08:33,170 --> 00:08:35,900
We transcribe with a
whisper based pipeline.

139
00:08:36,140 --> 00:08:39,320
And then we check the transcription
for keywords, for example.

140
00:08:39,630 --> 00:08:43,890
We have a video where code can transcribe
a phrase like this is our little secret,

141
00:08:44,160 --> 00:08:49,730
and this will be will flag will cause
relevant tag in the output of this model.

142
00:08:50,995 --> 00:08:55,255
For temporal analysis, we use
lightweight JRU or 3D CNN models

143
00:08:55,255 --> 00:08:56,935
to detect patterns over time.

144
00:08:57,325 --> 00:09:01,345
This is critical for spot actions
like slapping or some other type

145
00:09:01,345 --> 00:09:06,655
of violence, or generally content
that will unfold slowly, sub

146
00:09:07,165 --> 00:09:10,165
across five or 10 seconds of video.

147
00:09:11,060 --> 00:09:15,260
So all the signals are passed, as
we discussed in a decision layer.

148
00:09:15,350 --> 00:09:20,680
This layer will fuse the output
of all the models and either using

149
00:09:20,680 --> 00:09:23,260
score logic or some meta classifier.

150
00:09:23,650 --> 00:09:26,495
It'll try to make sure
that nothing is missed.

151
00:09:26,740 --> 00:09:31,400
So even if one model misses something
this layer will combine everything to make

152
00:09:31,400 --> 00:09:37,225
sure that the accuracy is much better than
the accuracy of the individual models.

153
00:09:38,525 --> 00:09:42,925
So now let's move from what we built to
how it can actually be used in the wide.

154
00:09:43,435 --> 00:09:46,285
This system can be integrated
with a variety of platforms,

155
00:09:46,435 --> 00:09:47,665
each with different needs.

156
00:09:48,190 --> 00:09:51,340
For video platforms, moderation
happens at upload time.

157
00:09:51,640 --> 00:09:53,650
Think of it like a pre-flight check.

158
00:09:53,650 --> 00:09:58,600
So before the video goes live, it
gets scanned to ensure it's safe, live

159
00:09:58,600 --> 00:10:01,420
streaming services, use it in real time.

160
00:10:01,730 --> 00:10:05,510
The content is processed in short
chunks, about five seconds at a time,

161
00:10:05,900 --> 00:10:09,925
and then we can flag problematic content
midstream while it is still happening.

162
00:10:10,750 --> 00:10:14,860
And for hosting providers, the system
can scan stored content passively.

163
00:10:14,860 --> 00:10:20,210
For example, it might sample and review
10% of a library of a video library

164
00:10:20,210 --> 00:10:22,460
every hour with no disruption to users.

165
00:10:23,270 --> 00:10:27,770
For example, a customer can ask to
help them scan a backlog of 10,000

166
00:10:28,250 --> 00:10:30,050
old videos they have inherited.

167
00:10:30,050 --> 00:10:33,980
During a platform migration, the
system will flag just a tiny percent

168
00:10:33,980 --> 00:10:37,760
of them, and then even that small
percentage may contain a handful of

169
00:10:37,760 --> 00:10:42,970
series violations that will be that
otherwise would be legal and reputational

170
00:10:42,970 --> 00:10:45,670
risk if they had gone unnoticed.

171
00:10:46,685 --> 00:10:50,165
So everything plugs in via
simple APIs or webhooks.

172
00:10:50,165 --> 00:10:51,875
No deep integration required.

173
00:10:52,085 --> 00:10:57,365
And this way, even link teams can add
Indus industrial grade moderation without

174
00:10:57,365 --> 00:10:59,495
building, building it from scratch.

175
00:11:00,495 --> 00:11:06,255
Now I know that some may wonder after all
this discussion so far that since we're

176
00:11:06,255 --> 00:11:11,515
scanning video uploads, analyzing live
streams and scanning whole collections

177
00:11:11,515 --> 00:11:15,385
of video, how do we make sure that we
are not just building some surveillance?

178
00:11:15,430 --> 00:11:16,300
Infrastructure.

179
00:11:16,300 --> 00:11:20,690
And this is a fair point when you're
dealing with sensitive content, trust

180
00:11:20,690 --> 00:11:22,700
is not just technical, it is ethical.

181
00:11:23,180 --> 00:11:26,840
So let show you what we have done
to build privacy first, responsible

182
00:11:26,840 --> 00:11:29,210
moderation into the system from the start.

183
00:11:29,960 --> 00:11:34,010
So first of all, we never
start our persist user content.

184
00:11:34,425 --> 00:11:36,495
Unless it is absolutely necessary.

185
00:11:36,825 --> 00:11:42,285
Once a video audio is scanned or stored
the original media is dis discarded.

186
00:11:42,465 --> 00:11:46,455
We only keep the metadata we need,
like tax scores or timestamps,

187
00:11:46,455 --> 00:11:48,015
and only when a flag is raised.

188
00:11:48,735 --> 00:11:52,815
Then we rely on randomized sampling,
as I mentioned, or event based figures.

189
00:11:53,355 --> 00:11:56,475
That means we're not just
scanning everything all the time.

190
00:11:56,745 --> 00:12:01,395
Instead, we design our pipeline to mimic
how a human trust and safety team would

191
00:12:01,395 --> 00:12:05,955
triage so it would target high risk
content while still preserving fairness

192
00:12:05,955 --> 00:12:07,755
and unpredictability in their work.

193
00:12:08,655 --> 00:12:10,725
Third, our system is compliance.

194
00:12:10,725 --> 00:12:15,705
Aware by design, we respect regional
data laws like GDPR and others, meaning

195
00:12:15,765 --> 00:12:21,105
we never scan without consent and don't
retain content outside allowed timeframes.

196
00:12:21,105 --> 00:12:24,015
And we deploy different lodging
depending on jurisdiction.

197
00:12:24,555 --> 00:12:28,305
For example, we avoid certain types of
scanning countries with stricter laws.

198
00:12:28,485 --> 00:12:29,865
Around personal data.

199
00:12:30,705 --> 00:12:34,995
Finally, we regularly audit our
models for bias and fairness.

200
00:12:35,265 --> 00:12:39,495
We uncovered early on that some of
our models was di disproportionately

201
00:12:39,495 --> 00:12:43,635
flagging windows with darker lighting
or skin tones, and that wasn't

202
00:12:43,635 --> 00:12:45,105
acceptable from the beginning.

203
00:12:45,105 --> 00:12:49,125
So we retrain the models with more balance
data sets and that it bias detection

204
00:12:49,125 --> 00:12:51,105
hooks into our validation process.

205
00:12:51,855 --> 00:12:55,125
In short, we are trying to build a
system, not just to be technically

206
00:12:55,125 --> 00:12:57,225
strong, but also ethically responsible.

207
00:12:59,385 --> 00:13:04,675
Now looking ahead while we have
built a strong foundation, we

208
00:13:04,675 --> 00:13:07,855
know that the real power of this
system lies in how we scale it.

209
00:13:08,725 --> 00:13:11,875
Our goal is to make AI powered
moderation feel native to the

210
00:13:11,875 --> 00:13:15,835
internet infrastructure, not the
bolt on service, but the core part

211
00:13:15,835 --> 00:13:17,815
of how content flows across the web.

212
00:13:18,160 --> 00:13:19,900
So here is what we are working on.

213
00:13:20,260 --> 00:13:24,520
First, we're exploring an edge based
moderation, meaning our detection models

214
00:13:24,520 --> 00:13:29,260
run directly on CDN Edge nodes, close to
where content is uploaded or streamed.

215
00:13:29,830 --> 00:13:34,160
That reduces latency sales
bandwidth and keeps data local.

216
00:13:34,820 --> 00:13:37,580
We are also building region
aware moderation, pipelines.

217
00:13:38,865 --> 00:13:40,785
Vary widely around the world.

218
00:13:41,265 --> 00:13:44,535
What's acceptable in one country
might be illegal in another.

219
00:13:44,745 --> 00:13:48,315
So we're designing a system that
can dynamically adjust thresholds

220
00:13:48,315 --> 00:13:53,145
or swap jurisdiction specific model
variants depending on the geography

221
00:13:53,145 --> 00:13:54,585
of the user or the customer.

222
00:13:55,685 --> 00:13:57,905
Another priority is
lightweight deployment.

223
00:13:58,325 --> 00:14:03,545
We are optimizing our models to run
under 200 megabytes so they can live

224
00:14:03,545 --> 00:14:08,585
comfortably on edge servers without
taking up a valuable compute in practice.

225
00:14:08,585 --> 00:14:12,665
This means that we strip down
dependencies, simplify architectures

226
00:14:12,665 --> 00:14:15,785
and compressing inference graphs
as much as possible without

227
00:14:15,905 --> 00:14:17,525
sacrificing accuracy, of course.

228
00:14:18,155 --> 00:14:19,925
And finally, we're developing a set of.

229
00:14:20,025 --> 00:14:21,285
Customer facing tools.

230
00:14:21,645 --> 00:14:25,485
We have APIs and dashboards that allow
our customers to review moderation

231
00:14:25,485 --> 00:14:29,535
events, customize their thresholds,
and receive real time alerts.

232
00:14:30,015 --> 00:14:33,825
Imagine a streaming platform getting
notified within seconds if using

233
00:14:33,825 --> 00:14:37,605
content appears mid broadcast, and
being able to act on it instantly.

234
00:14:38,490 --> 00:14:41,880
All of this is in active
development and early stage rollout.

235
00:14:42,180 --> 00:14:45,450
So when we talk about the future of
trust and safety online, we don't

236
00:14:45,450 --> 00:14:47,520
just imagine it sitting in the cloud.

237
00:14:47,520 --> 00:14:52,650
We see distributed, embedded at the
edge, tuned for local context, and

238
00:14:52,650 --> 00:14:55,020
accessible to platforms of all sizes.

239
00:14:57,020 --> 00:15:00,440
So let me wrap up here with some
lessons we have learned along the way.

240
00:15:00,900 --> 00:15:03,180
First orchestration is key.

241
00:15:03,300 --> 00:15:07,380
Our models are great, but without
task use retries monitoring, it

242
00:15:07,380 --> 00:15:09,060
would all break down under load.

243
00:15:09,510 --> 00:15:13,230
Then class imbalance isn't just
about lik it affects how our

244
00:15:13,230 --> 00:15:14,850
customers perceive our system.

245
00:15:15,180 --> 00:15:19,290
False positives erode trust,
so product taking is essential.

246
00:15:20,450 --> 00:15:22,340
Then infrastructure really matters.

247
00:15:22,340 --> 00:15:27,470
We have hardbacks where a queue failed
silently and nothing can count for hours.

248
00:15:27,860 --> 00:15:31,580
Logging and fall, ba fallback
logic is really important

249
00:15:31,580 --> 00:15:33,170
and can save such systems.

250
00:15:34,040 --> 00:15:36,920
And finally, trust isn't
earned by being perfect.

251
00:15:36,920 --> 00:15:38,780
It's earned by being transparent.

252
00:15:39,110 --> 00:15:44,540
So logs, scores, explanations, any co
context we can offer to our customers

253
00:15:44,940 --> 00:15:47,070
help them with building trust.

254
00:15:47,520 --> 00:15:51,540
So if you are building an ai model
for safety try to think beyond

255
00:15:51,540 --> 00:15:55,140
the model and think the whole
system and the product in scope.

256
00:15:56,410 --> 00:16:00,400
Thank you everyone for giving the
opportunity to talk about this system.

257
00:16:00,640 --> 00:16:03,955
It's we are still early in this
journey, but I truly believe that

258
00:16:04,000 --> 00:16:08,410
AI moderation is becoming not
just practical, but essential, and

259
00:16:08,440 --> 00:16:10,180
whether you're building platforms.

260
00:16:10,180 --> 00:16:13,175
Delivering content or just care
about the safer internet, this is

261
00:16:13,175 --> 00:16:16,985
something that we need to really
focus in building altogether.

262
00:16:17,555 --> 00:16:18,125
So thanks.

263
00:16:18,365 --> 00:16:18,875
Thank you.

264
00:16:18,935 --> 00:16:22,295
Thanks a lot for your time, and
I would love to chat if you are

265
00:16:22,295 --> 00:16:24,555
working on anything in this space.

