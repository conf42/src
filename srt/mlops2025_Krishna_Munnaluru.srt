1
00:00:00,500 --> 00:00:03,830
Hello everyone and thank
you for joining my session.

2
00:00:04,100 --> 00:00:10,100
My name is Krishna Uluru, and today
I'll be talking about how we built a

3
00:00:10,100 --> 00:00:14,930
production ready ML ops pipeline for
telecommunications focusing on a neuro

4
00:00:14,930 --> 00:00:17,450
network based over IP monitoring system.

5
00:00:17,950 --> 00:00:23,169
This work demonstrates how AI driven
monitoring can transform carrier

6
00:00:23,169 --> 00:00:24,505
grade wise over IP operations.

7
00:00:25,005 --> 00:00:29,834
With both high accuracy and
automated deployment pipelines.

8
00:00:30,164 --> 00:00:33,914
By the end of the session, I hope I will.

9
00:00:34,425 --> 00:00:39,435
You will see how integrating ML ops
with telecom can make networks more

10
00:00:39,435 --> 00:00:41,864
resilient, cost effective, and scalable

11
00:00:42,364 --> 00:00:42,855
here did.

12
00:00:43,079 --> 00:00:44,790
Here is what I will cover today.

13
00:00:44,879 --> 00:00:45,510
First.

14
00:00:46,010 --> 00:00:49,400
I will describe the challenges with
traditional telecom monitoring.

15
00:00:49,820 --> 00:00:56,269
Then I will introduce our ML lops
architecture, followed by a closer look

16
00:00:57,230 --> 00:00:59,510
at the neural network model we developed.

17
00:00:59,690 --> 00:01:06,410
Next, I will walk through the deployment
strategy showing how we ensure

18
00:01:06,410 --> 00:01:08,600
reliability and scalability in production.

19
00:01:08,905 --> 00:01:12,744
Finally, I will share the
business results and the roadmap.

20
00:01:13,244 --> 00:01:14,594
For ongoing improvements,

21
00:01:15,094 --> 00:01:17,944
traditional monitoring has gaps.

22
00:01:18,444 --> 00:01:20,904
So let's start with the current
state of white monitoring.

23
00:01:21,404 --> 00:01:23,804
Traditional tools only provide
about 40% visibility into the

24
00:01:23,804 --> 00:01:27,554
network, leaving blind spots where
critical issues can go Unnoticed.

25
00:01:28,054 --> 00:01:29,344
They're also reactive.

26
00:01:29,884 --> 00:01:34,384
It typically takes 15 to 30 minutes to
detect and respond to issues in telecom.

27
00:01:34,384 --> 00:01:37,984
That delay can mean thousands of
drop calls and frustrated customers.

28
00:01:38,644 --> 00:01:42,934
And finally, the statistical
models used for quality predictions

29
00:01:43,085 --> 00:01:45,095
only reach around 65% accuracy.

30
00:01:45,595 --> 00:01:50,165
This gap is exactly why we
need an AI driven approach.

31
00:01:50,875 --> 00:01:55,135
Capable of understanding complex
patterns and operating in real time.

32
00:01:55,635 --> 00:02:00,065
ML Lops Architecture Overview
our architecture brings together

33
00:02:00,125 --> 00:02:04,450
real time data ingestion, feature
engineering, automated training,

34
00:02:04,660 --> 00:02:07,965
deployment, and monitoring into a
single production ready pipeline.

35
00:02:08,015 --> 00:02:11,555
We use Kafka for streaming
wise or IP events.

36
00:02:11,765 --> 00:02:17,345
Cube flow for training, orchestration,
ml, flow for model registry and

37
00:02:17,345 --> 00:02:19,295
monitoring through Prometheus and Grafana.

38
00:02:19,745 --> 00:02:24,785
The key here is that all of these
components are tied together,

39
00:02:24,845 --> 00:02:26,825
so models aren't just trained.

40
00:02:26,825 --> 00:02:28,830
They are continuously deployed,
absorbed, and improved.

41
00:02:29,330 --> 00:02:31,345
Let me highlight four
essential components.

42
00:02:31,845 --> 00:02:38,085
First realtime data ingestion with
Kafka processes thousands of events per

43
00:02:38,085 --> 00:02:41,865
second at sub 100 milliseconds latency.

44
00:02:42,365 --> 00:02:47,305
Second, the automated training
pipeline orchestrated by cube flow,

45
00:02:47,335 --> 00:02:51,905
which runs distributed GPU based
training with hyper parameter training.

46
00:02:52,405 --> 00:02:54,220
Third deployment automation.

47
00:02:54,720 --> 00:02:58,890
We use blue green deployment models
for zero downtime and instant

48
00:02:58,890 --> 00:03:01,109
rollback if performance degrades.

49
00:03:02,040 --> 00:03:07,500
And fourth comprehensive monitoring
where custom metrics and dashboards give

50
00:03:07,500 --> 00:03:14,100
visibility into both deployment, both
model performance and system health.

51
00:03:14,600 --> 00:03:22,640
Together these ensure a carrier grade
reproducible and resilient ML lifecycle.

52
00:03:23,140 --> 00:03:29,020
Our neural network combines CNN layers to
extract features from packet sequences and

53
00:03:29,020 --> 00:03:34,960
LSTM layers to recognize temporal patterns
across sessions, we added innovations

54
00:03:34,960 --> 00:03:40,329
like custom emitting layer for protocol
specific features and attention mechanism

55
00:03:40,389 --> 00:03:45,939
to focus on anomalies and hierarchical
feature extractor across timescales.

56
00:03:46,449 --> 00:03:52,869
The result, 88% prediction accuracy
with a 23 point movement over

57
00:03:52,869 --> 00:03:54,459
traditional statistical methods.

58
00:03:55,269 --> 00:03:58,179
That's a major leap in how
accurately we can predict and

59
00:03:58,179 --> 00:04:00,100
preempt call quality issues.

60
00:04:00,600 --> 00:04:04,139
The model is only as good as its
features, so we built a fully

61
00:04:04,139 --> 00:04:08,400
automated feature pipeline and the
edge high throughput collectors

62
00:04:08,400 --> 00:04:10,920
capture sip, RTP, and QS metrics.

63
00:04:11,549 --> 00:04:16,350
These are processed in real time
with Apache Beam handling over

64
00:04:16,409 --> 00:04:21,090
10,000 events per second while
extracting 45 plus features.

65
00:04:21,510 --> 00:04:26,490
We use recursive feature elimination
for automatic feature selection and

66
00:04:26,490 --> 00:04:32,700
feast as a feature store to ensure
consistency across training and inference.

67
00:04:33,480 --> 00:04:39,005
All of this is logged and monitored
to maintain data quality and lineage.

68
00:04:39,505 --> 00:04:42,235
Training is orchestrated
through cube flow pipelines.

69
00:04:42,745 --> 00:04:47,515
We use TensorFlow data valuation for
schema checks and drift detection.

70
00:04:48,085 --> 00:04:50,724
Then run distributed
GPU training at scale.

71
00:04:51,715 --> 00:04:57,884
Every experiment, parameters,
metrics, artifacts, is tracked in ML

72
00:04:57,884 --> 00:05:00,459
flow, so results are reproducible.

73
00:05:00,959 --> 00:05:06,659
Models are versioned and registered with
clear approval workflows so teams can

74
00:05:06,659 --> 00:05:09,479
roll forward or backward confidently

75
00:05:09,979 --> 00:05:14,719
in production system maintains 96%
anomaly detection accuracy while

76
00:05:14,719 --> 00:05:19,789
handling thousands of concurrent
requests with 99.99% available.

77
00:05:20,289 --> 00:05:22,809
We achieve this with containerized models.

78
00:05:22,869 --> 00:05:28,149
Intensive flow serving orchestrated by
Kubernetes with horizontal auto-scaling.

79
00:05:28,649 --> 00:05:32,939
Bluegreen deployments provide
zero downtime and edge inference

80
00:05:32,939 --> 00:05:37,889
Nodes across regions deliver
responses in under 50 milliseconds.

81
00:05:38,549 --> 00:05:40,949
This is a true carrier
grade ML deployment.

82
00:05:41,449 --> 00:05:44,599
Comprehensive monitoring
system has been de developed.

83
00:05:45,099 --> 00:05:47,109
Monitoring goes beyond just accuracy.

84
00:05:47,439 --> 00:05:53,019
We track model performance metrics
like precision and recall data

85
00:05:53,019 --> 00:05:57,369
quality metrics like feature rift
and missing values, system health

86
00:05:57,459 --> 00:05:59,019
such as latency and throughput.

87
00:05:59,519 --> 00:06:03,839
When drift is detected, retraining
is automatically triggered.

88
00:06:04,259 --> 00:06:05,279
We also run.

89
00:06:05,729 --> 00:06:09,449
AB tests in production and
use self-healing mechanisms

90
00:06:09,449 --> 00:06:10,919
for infrastructure issues.

91
00:06:11,489 --> 00:06:15,329
This makes the system
adaptive, not just reactive.

92
00:06:15,829 --> 00:06:19,069
Trust is critical in telecom
operations, so we have built

93
00:06:19,129 --> 00:06:20,929
explainability into the pipeline.

94
00:06:21,229 --> 00:06:26,119
We use shop values for feature importance.

95
00:06:26,854 --> 00:06:33,034
Counterfactual explanations to show how
outcomes could change and dashboards

96
00:06:33,124 --> 00:06:36,544
that track feature attribution over time.

97
00:06:37,044 --> 00:06:41,124
Automated model cards and audit logs
document every version and decision.

98
00:06:41,624 --> 00:06:46,124
This means engineers can always
trace a prediction back to its root

99
00:06:46,124 --> 00:06:51,254
cause, essential for both regulatory
compliance and operator trust.

100
00:06:51,754 --> 00:06:57,044
What does all this mean in practice
with this system fault detection

101
00:06:57,044 --> 00:07:02,744
time has dropped from 30 to 15 to 30
minutes, down to two to five minutes.

102
00:07:03,244 --> 00:07:05,224
Network visibility is greatly improved.

103
00:07:05,724 --> 00:07:10,259
Licensing costs are reduced through
open source tooling and uptime

104
00:07:10,319 --> 00:07:11,699
has reached carrier grade levels.

105
00:07:12,199 --> 00:07:17,419
Most importantly, customer satisfaction
improved because quality issues are

106
00:07:17,419 --> 00:07:19,939
caught before they affect end users.

107
00:07:20,439 --> 00:07:22,459
We rolled this out in phases.

108
00:07:22,959 --> 00:07:26,629
Phase one, the foundation, the data
collection, feature engineering,

109
00:07:26,729 --> 00:07:29,279
and an initial model, phase two.

110
00:07:29,779 --> 00:07:35,509
Is automation CICD, pipelines,
testing registries and dashboards.

111
00:07:35,809 --> 00:07:40,969
Phase three is scale distributed
training and geographic deployments.

112
00:07:41,719 --> 00:07:47,299
Phase four, ongoing optimization,
ensembles tuning and knowledge transfer.

113
00:07:47,799 --> 00:07:52,239
This phased approach allowed us
to deliver incremental value while

114
00:07:52,239 --> 00:07:54,004
building towards the full vision.

115
00:07:54,504 --> 00:07:56,364
To wrap up three points.

116
00:07:56,574 --> 00:07:59,994
First, production ready ml
requires end-to-end thinking

117
00:07:59,994 --> 00:08:01,344
from data to monitoring.

118
00:08:01,764 --> 00:08:06,764
Second, automation drives reliability,
especially at telecom scale.

119
00:08:07,454 --> 00:08:11,224
And third, explainability builds
trust, which is essential for

120
00:08:11,224 --> 00:08:13,384
adoption in mission critical systems.

121
00:08:13,654 --> 00:08:14,464
These principles.

122
00:08:15,109 --> 00:08:19,669
Guided us in creating a system that's
not just research project, but a

123
00:08:19,669 --> 00:08:22,039
reliable production grade ML ops system.

124
00:08:22,539 --> 00:08:23,769
Thank you for your time.

125
00:08:23,829 --> 00:08:27,549
I hope this session gave you
insight into how ML Ops can

126
00:08:27,549 --> 00:08:29,934
revolutionize telecom monitoring.

127
00:08:30,434 --> 00:08:30,884
Thank you.

