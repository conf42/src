1
00:00:00,050 --> 00:00:07,500
Everything fails all the time is a famous
quote from Amazon's CTO, Werner Vogels.

2
00:00:08,000 --> 00:00:13,580
This means that software and distributed
systems may eventually fail because

3
00:00:14,260 --> 00:00:15,609
something can always go wrong.

4
00:00:16,109 --> 00:00:21,129
So what happens when your containerized
applications face network disruptions?

5
00:00:21,629 --> 00:00:25,779
So in this session, you will
learn how to proactively validate

6
00:00:25,819 --> 00:00:27,819
your containerized applications.

7
00:00:27,939 --> 00:00:28,479
Resilience.

8
00:00:28,939 --> 00:00:35,249
using AWS Fault Injection Service,
a FIAS, turning potential network

9
00:00:35,289 --> 00:00:41,279
challenges into opportunities for
strengthening your architecture.

10
00:00:41,779 --> 00:00:42,479
Hello, everyone.

11
00:00:42,979 --> 00:00:44,869
My name is Sunil Govindankutty.

12
00:00:45,529 --> 00:00:49,639
I am an enterprise support
lead at Amazon Web Services.

13
00:00:50,139 --> 00:00:56,574
I'm also a member of our Resilience
technical community, where I focus

14
00:00:56,634 --> 00:00:59,144
on the chaos engineering area.

15
00:00:59,644 --> 00:01:03,604
I'm excited to be here talking
today, talking to you today

16
00:01:03,664 --> 00:01:06,054
about this particular topic.

17
00:01:06,554 --> 00:01:08,284
Now let's see the agenda.

18
00:01:08,784 --> 00:01:16,054
We'll start with an overview of
Amazon ECS and AWS Fargate options.

19
00:01:16,064 --> 00:01:20,124
You have, to run container
workloads in AWS.

20
00:01:20,624 --> 00:01:26,075
We'll then see some of the, network
faults that can happen that you

21
00:01:26,075 --> 00:01:28,065
might have seen in real world.

22
00:01:28,565 --> 00:01:35,425
We'll look at an overview of FIS and
the various features and components

23
00:01:35,465 --> 00:01:38,215
that the service has and supports.

24
00:01:38,715 --> 00:01:44,177
We'll also see a reference
flow of a chaos engineering

25
00:01:44,177 --> 00:01:46,469
experiment triggered through FIS.

26
00:01:46,969 --> 00:01:53,139
We'll then conclude with a demo of
a sample application that is running

27
00:01:53,369 --> 00:02:01,429
containers on Amazon ECS using Fargate
and see what the application behavior

28
00:02:01,879 --> 00:02:05,479
would look like in case of network faults.

29
00:02:05,979 --> 00:02:11,859
ECS, or Elastic Container Service,
provides a managed platform that

30
00:02:11,979 --> 00:02:19,179
provisions compute, network, and storage
that you need for your containers.

31
00:02:19,679 --> 00:02:26,759
It manages the scheduling of containers,
the self healing of containers,

32
00:02:26,839 --> 00:02:28,739
the auto scaling of containers.

33
00:02:29,239 --> 00:02:34,509
And it integrates well with our,
container registry, the secrets manager,

34
00:02:35,059 --> 00:02:42,679
the load balancing service, et cetera,
that allows you to focus on just your

35
00:02:42,739 --> 00:02:48,049
application and your requirements
for containers and leaving the

36
00:02:48,079 --> 00:02:54,379
management of the container platform,
the orchestration platform to ECS.

37
00:02:54,879 --> 00:02:56,589
Now where do you run these containers?

38
00:02:57,139 --> 00:02:59,729
So you have a few options.

39
00:02:59,729 --> 00:03:06,219
One of them is AWS Fargate,
which is a serverless compute

40
00:03:06,339 --> 00:03:08,349
platform for containers.

41
00:03:09,039 --> 00:03:17,499
It runs each of the containers
in its own kernel and manages the

42
00:03:17,549 --> 00:03:20,819
life cycle of the underlying host.

43
00:03:21,679 --> 00:03:31,099
So you eliminate the need for customers
to upgrade, patch and to scale the

44
00:03:31,129 --> 00:03:35,699
underlying hosts needed by the containers.

45
00:03:36,199 --> 00:03:41,569
Now we'll see some of the core
constructs of the ECS platform.

46
00:03:42,069 --> 00:03:43,479
One of them is a task.

47
00:03:44,009 --> 00:03:49,509
So in, you specify the compute
requirements, your networking

48
00:03:49,539 --> 00:03:55,819
requirements, the permissions, and the
configurations you need for one or more

49
00:03:55,879 --> 00:03:59,779
of your containers in a task definition.

50
00:04:00,279 --> 00:04:05,369
Then tasks are created from
this definition, matches all

51
00:04:05,369 --> 00:04:10,309
of the attributes that you have
specified in the definition file.

52
00:04:10,809 --> 00:04:16,479
And then I mentioned, the orchestration
activities of, healing, failed containers,

53
00:04:16,609 --> 00:04:23,159
replacing them, managing the scaling of
the, containers, performing deployments

54
00:04:23,569 --> 00:04:27,519
of latest code to existing containers.

55
00:04:28,179 --> 00:04:33,809
All of those activities are managed
by, what's called a service.

56
00:04:34,469 --> 00:04:38,859
And it also registers these
containers to load balancers

57
00:04:38,899 --> 00:04:41,809
to accept the end user traffic.

58
00:04:42,309 --> 00:04:46,869
Then you also have what's called a
cluster, which is a logical grouping

59
00:04:46,869 --> 00:04:51,949
of services and tasks in an AWS region.

60
00:04:52,449 --> 00:04:56,349
You can then use IAM or identity
and access management service.

61
00:04:57,094 --> 00:05:01,504
to control the permissions of
users to these clusters and

62
00:05:01,504 --> 00:05:03,694
its underlying components.

63
00:05:04,194 --> 00:05:09,974
Now let's do a quick, take a quick
look at, networking for Fargate tasks.

64
00:05:10,974 --> 00:05:20,014
Each task gets its own network interface,
its own ENI using an IP from a subnet that

65
00:05:20,024 --> 00:05:22,684
you have provided to the ECS platform.

66
00:05:23,184 --> 00:05:28,024
And then you can apply security
groups to a task that will control

67
00:05:28,034 --> 00:05:33,794
the ingress and egress of the traffic
from the underlying containers.

68
00:05:34,294 --> 00:05:39,464
In case that, you have a bunch of
containers running in terms of, a spike

69
00:05:39,464 --> 00:05:45,404
in traffic or something like that, you
can attach secondary CIDR blocks to your

70
00:05:45,544 --> 00:05:49,214
VPC to address that particular need.

71
00:05:49,714 --> 00:05:51,464
Now we've looked at the, networking.

72
00:05:51,464 --> 00:05:59,832
What are some of the, fault actions, fault
events that could happen in a real world.

73
00:06:00,332 --> 00:06:04,852
One of them is latency, which is
a delay in communication between

74
00:06:05,522 --> 00:06:07,512
services and their dependencies.

75
00:06:08,012 --> 00:06:14,417
you could see behaviors like Network,
congestion or, a route change resulting

76
00:06:14,417 --> 00:06:20,857
in latency or a DNS resolution, delay
that, again, resulting in latency.

77
00:06:21,357 --> 00:06:27,737
And then you have network black hole
where packets are silently dropped.

78
00:06:27,827 --> 00:06:32,057
Here you might see DCP
connections hanging.

79
00:06:32,607 --> 00:06:38,777
You might see, loss of connectivity
between dependencies and.

80
00:06:39,427 --> 00:06:42,817
You might not get an error
response from underlying

81
00:06:43,007 --> 00:06:45,337
dependencies in an event like this.

82
00:06:45,837 --> 00:06:52,087
Another flavor of a network fault is
packet loss, where the packets doesn't

83
00:06:52,087 --> 00:06:54,997
reach their intended destination.

84
00:06:55,497 --> 00:06:59,447
So with this, you might see,
retransmission attempts at the TCP layer.

85
00:06:59,457 --> 00:07:03,707
You might see degraded
performance across your workload.

86
00:07:04,302 --> 00:07:09,342
You might see, bursts or random
patterns of network packets

87
00:07:09,342 --> 00:07:10,902
in your monitoring tools.

88
00:07:11,402 --> 00:07:16,142
Now, given that, some of these, network
faults could happen or would happen

89
00:07:16,522 --> 00:07:22,942
in your container environment, how
do you prepare for that eventuality?

90
00:07:23,442 --> 00:07:29,472
That's where FIS, Fault Injection
Service from AWS, comes into picture.

91
00:07:29,972 --> 00:07:34,962
It's a fully managed service for
running fault injection experiments.

92
00:07:35,462 --> 00:07:41,582
It comes pre packaged with scenarios
of various fault actions that you can

93
00:07:41,582 --> 00:07:45,342
take and run against your workload.

94
00:07:45,842 --> 00:07:51,652
You can, run it through the AWS
management console or CLIs or integrating

95
00:07:51,652 --> 00:07:53,962
with any tools of your choice.

96
00:07:54,462 --> 00:07:58,472
So you can get started in a
matter of minutes, in fact.

97
00:07:58,972 --> 00:08:06,162
And it allows you to test real
world conditions from as simple as

98
00:08:06,782 --> 00:08:13,842
stopping off or termination of an
EC2 instance to as complex as a power

99
00:08:13,842 --> 00:08:15,962
interruption in an availability zone.

100
00:08:16,462 --> 00:08:22,892
FIS also fully embraces the concept
of safeguards that allows you to

101
00:08:22,902 --> 00:08:27,602
limit the blast radius of your
chaos engineering experiments.

102
00:08:28,522 --> 00:08:34,857
So you could Specify an alarm, and
if that were to go off, FIS will

103
00:08:35,407 --> 00:08:39,797
stop all the experiment and the
fault actions it is performing and

104
00:08:39,797 --> 00:08:41,897
then try to roll back those actions.

105
00:08:42,397 --> 00:08:46,497
Now let's look at a reference
flow for the service.

106
00:08:46,997 --> 00:08:52,037
So at a very high level, we start with
an experiment template that packages the

107
00:08:52,037 --> 00:08:59,187
different fault actions and the targets
that will be affected by those actions.

108
00:08:59,687 --> 00:09:05,327
and safeguards that you want to
enforce while running the experiment.

109
00:09:05,827 --> 00:09:11,857
FIS then performs these actions on the
AWS resources that are specified as the

110
00:09:11,857 --> 00:09:15,077
targets when you start the experiment.

111
00:09:15,577 --> 00:09:19,387
You can then monitor the
experiment using CloudWatch.

112
00:09:19,887 --> 00:09:25,847
FIS also integrates with Amazon
EventBridge, allowing you to use

113
00:09:25,847 --> 00:09:28,977
a monitoring tool of your choice.

114
00:09:29,477 --> 00:09:34,547
to observe the behavior of
the application in question.

115
00:09:35,047 --> 00:09:40,297
And experiments, once started, will
automatically stop when all of the

116
00:09:40,407 --> 00:09:42,487
actions specified are complete.

117
00:09:43,337 --> 00:09:48,897
Or, as I mentioned earlier, you can
optionally configure an alarm, and if that

118
00:09:48,907 --> 00:09:52,592
were to go off, the actions are stopped.

119
00:09:53,092 --> 00:09:58,512
And once the experiment is complete, you
can, view the results to identify any

120
00:09:58,552 --> 00:10:04,052
performance concerns or observability
issues or resilience challenges.

121
00:10:04,962 --> 00:10:10,012
You could also then use the report
of Kiosk Engineering as an evidence

122
00:10:10,182 --> 00:10:15,162
of testing to your compliance
group or a security department.

123
00:10:15,662 --> 00:10:21,312
Now that we have seen, a high level
capabilities of FIS, let's take a sample

124
00:10:21,312 --> 00:10:28,892
application running on an ECS cluster
and let's try to inject network faults.

125
00:10:29,782 --> 00:10:33,802
So in this case, this sample
application is a CRUD API.

126
00:10:34,702 --> 00:10:42,982
Running on an ECS Fargate cluster,
getting data to and from an RDS database.

127
00:10:43,932 --> 00:10:50,352
So we will run NetworkFalse against
this dependency and see what the

128
00:10:50,392 --> 00:10:53,332
behavior of the application looks like.

129
00:10:53,832 --> 00:10:59,677
Okay, now you have the AWS
console where, We have our sample

130
00:10:59,717 --> 00:11:02,507
application up and running.

131
00:11:03,007 --> 00:11:09,277
So let's take a look at the
cloud formation stacks for our

132
00:11:09,927 --> 00:11:12,007
application and its components.

133
00:11:12,507 --> 00:11:19,292
So we have an app stack that contains all
the infrastructure components needed for

134
00:11:19,292 --> 00:11:26,602
our application, including the ECS Fargate
cluster, the RDS database, et cetera.

135
00:11:27,102 --> 00:11:29,522
And then we have a monitoring stack.

136
00:11:30,332 --> 00:11:36,852
So one of the prerequisites with any
type of experimentation is that you

137
00:11:36,852 --> 00:11:44,152
have an observability and monitoring
mechanism in place for your applications.

138
00:11:44,652 --> 00:11:50,122
Otherwise, It's the old pre
false in the forest saying.

139
00:11:50,622 --> 00:11:56,092
So for our sample application, we will
be doing the monitoring using CloudWatch.

140
00:11:56,592 --> 00:12:01,262
As I mentioned earlier, FIS integrates
with third party monitoring tools

141
00:12:01,612 --> 00:12:03,202
through EventBridge as well.

142
00:12:03,702 --> 00:12:07,262
And then we have our experiment stack.

143
00:12:07,802 --> 00:12:12,472
So the experiment stack contains
the various network fault actions

144
00:12:12,972 --> 00:12:18,027
that we will be performing
against our sample application.

145
00:12:18,527 --> 00:12:23,487
Now let's take a closer look at the
application stack and its outputs.

146
00:12:23,987 --> 00:12:28,047
So one of the outputs is the API URL.

147
00:12:28,047 --> 00:12:35,427
So this is the end point of our sample
API that we will be testing with.

148
00:12:35,927 --> 00:12:41,897
Now, let's see how this
API behaves from an IDE.

149
00:12:41,907 --> 00:12:45,407
Let's, do a get on this endpoint.

150
00:12:45,907 --> 00:12:50,627
As you can see, the API
returns a few items.

151
00:12:51,027 --> 00:12:57,527
So basically, the functionality
behind the API is to do Create, read,

152
00:12:57,527 --> 00:13:01,257
update, delete operations for items.

153
00:13:01,307 --> 00:13:05,277
And I was able to get a list of
items that are currently in the

154
00:13:05,277 --> 00:13:08,117
database through this API call.

155
00:13:08,617 --> 00:13:09,057
Good.

156
00:13:09,157 --> 00:13:14,947
now the other thing we want to do,
one of the other prerequisites for

157
00:13:15,447 --> 00:13:19,197
experimentation is load generation.

158
00:13:19,697 --> 00:13:26,317
So as you start on the chaos engineering
journey, we recommend that You start

159
00:13:26,337 --> 00:13:30,187
with the lower environment, whether
it's a testing or a staging environment.

160
00:13:30,687 --> 00:13:35,547
And the challenge there is that
you might not have the real user

161
00:13:35,577 --> 00:13:38,207
traffic in those environments.

162
00:13:38,897 --> 00:13:46,617
So you need to generate some type of
synthetic traffic that mimics your real

163
00:13:46,637 --> 00:13:53,747
production traffic so that when you run
these experiments, you get a sense of what

164
00:13:53,747 --> 00:13:57,027
could happen in a real world condition.

165
00:13:57,527 --> 00:14:02,917
So in our case, for the sample
application, I'm going to be using

166
00:14:02,917 --> 00:14:05,587
the open source tool called Artillery.

167
00:14:06,032 --> 00:14:10,752
to generate some synthetic
load against our API.

168
00:14:11,252 --> 00:14:18,112
So basically it's just going to
do a, post action, against our

169
00:14:18,152 --> 00:14:22,112
API endpoint for 20 minutes.

170
00:14:22,612 --> 00:14:26,802
So let's go to our Cloud 9 IDE console.

171
00:14:27,532 --> 00:14:30,752
and kick off that load generation script.

172
00:14:31,252 --> 00:14:35,732
So you can see here, the load
generation tool is hitting our

173
00:14:35,762 --> 00:14:43,352
API endpoint and getting a 201
item created HTTP status code.

174
00:14:43,852 --> 00:14:44,402
That's good.

175
00:14:44,452 --> 00:14:50,802
so let's keep the load generation
script running here while we go take

176
00:14:50,812 --> 00:14:54,942
a look at our, ECS cluster, right?

177
00:14:54,942 --> 00:15:01,042
So our Compute layer for, the API is, ECS.

178
00:15:01,542 --> 00:15:05,132
So as I mentioned in the slides,
there are a few constructs with ECS.

179
00:15:05,152 --> 00:15:06,452
One of them is a cluster.

180
00:15:06,492 --> 00:15:09,992
So we have one cluster
running in this account.

181
00:15:10,617 --> 00:15:17,547
With one service, and this service
has a couple of tasks that are

182
00:15:17,687 --> 00:15:22,617
on the running state that is
supporting our application workload.

183
00:15:23,607 --> 00:15:27,007
And each of these tasks
have two containers.

184
00:15:27,457 --> 00:15:32,237
One is the application container, the app
container with the application code, and

185
00:15:32,237 --> 00:15:39,942
the other is an SSM agent container, which
is one of the Prerequisites for running

186
00:15:40,562 --> 00:15:43,682
network faults against an ECS Fargate.

187
00:15:44,287 --> 00:15:50,177
Cluster, you can refer to all the
AWS public documentation around the

188
00:15:50,197 --> 00:15:52,797
prerequisites for network faults.

189
00:15:53,297 --> 00:15:57,867
So we have the cluster, the services,
the task, all of that is good.

190
00:15:58,387 --> 00:16:04,227
now let's take a quick peek at our,
database layer, our RDS endpoint.

191
00:16:04,917 --> 00:16:10,857
So you can see that, we have, a MySQL
database supporting our application.

192
00:16:10,922 --> 00:16:17,122
And this is the endpoint of that
cluster and this is the port

193
00:16:17,182 --> 00:16:19,462
the database is listening on.

194
00:16:19,482 --> 00:16:25,892
So we'll be using this information
while we run the experiments later.

195
00:16:26,392 --> 00:16:31,272
So now that we have looked at
the application architecture,

196
00:16:31,992 --> 00:16:36,912
let's take a quick peek at our
observability tooling as well.

197
00:16:37,412 --> 00:16:39,442
So we have a dashboard here.

198
00:16:39,862 --> 00:16:43,757
That we've created through
our experiment stack.

199
00:16:43,757 --> 00:16:50,447
Let's take a look at, let's say the last
15 minutes or so, we, yeah, where we've

200
00:16:50,457 --> 00:16:55,817
started running some load generation
script against our application, right?

201
00:16:55,817 --> 00:16:59,577
So it's still, catching
up with the observability

202
00:16:59,597 --> 00:17:01,867
metrics for our, application.

203
00:17:01,867 --> 00:17:06,267
So we'll, keep an eye on this,
let it catch up with the load

204
00:17:06,267 --> 00:17:13,327
generation script while We take
a look at, the AWS FIS console.

205
00:17:13,827 --> 00:17:21,697
So here you can see that as a part
of our experiment stack, we have

206
00:17:21,997 --> 00:17:25,227
three, experiment templates created.

207
00:17:25,557 --> 00:17:30,177
So one of them here is the network
latency, the other black hole, and

208
00:17:30,177 --> 00:17:32,627
the other, the last one packet loss.

209
00:17:33,027 --> 00:17:33,897
So these are the.

210
00:17:34,467 --> 00:17:40,107
Various network faults we looked at,
during the, review of the slides earlier.

211
00:17:40,607 --> 00:17:45,587
Now let's dive deep into this
network latency, experiment.

212
00:17:46,087 --> 00:17:50,487
Let me just do an update of the
template so you can clearly see

213
00:17:50,487 --> 00:17:52,547
the various components of it.

214
00:17:53,267 --> 00:17:58,877
So the two key things, or
two key concepts we covered.

215
00:17:59,377 --> 00:18:03,357
with an experiment template
is the action and the target.

216
00:18:03,947 --> 00:18:10,187
So the action specifies the fault that
you want to test against your workload.

217
00:18:10,947 --> 00:18:15,937
And as, I mentioned earlier, you can
sequence a bunch of fault actions or

218
00:18:15,937 --> 00:18:20,557
you can run them in parallel or mix
them up, etc. So in our case, what

219
00:18:20,557 --> 00:18:25,947
we're going to do is we're going
to run this network latency fault

220
00:18:25,987 --> 00:18:28,947
action against our ECS cluster.

221
00:18:29,897 --> 00:18:35,247
And as you can see here, you see
there is a bunch of services that

222
00:18:35,787 --> 00:18:38,627
our, FIS service integrates with.

223
00:18:38,967 --> 00:18:43,457
So right now, let's focus on
the network latency action.

224
00:18:43,957 --> 00:18:47,757
And then based on the action that
you selected, you can specify

225
00:18:47,787 --> 00:18:50,097
a certain number of parameters.

226
00:18:50,727 --> 00:18:54,877
So for this latency action,
one item is the delay.

227
00:18:54,877 --> 00:18:59,357
What is the latency
that we want to add to?

228
00:18:59,717 --> 00:19:05,347
our network traffic between the
compute layer and the database layer.

229
00:19:05,847 --> 00:19:10,627
So in this case, I'm selecting the
default which is 200 and then we're

230
00:19:10,627 --> 00:19:13,377
going to run this for five minutes.

231
00:19:13,877 --> 00:19:18,052
And you can see in the source So
in this field here, I've specified

232
00:19:18,172 --> 00:19:22,012
the endpoint of our RDS database.

233
00:19:22,262 --> 00:19:28,632
So that is the source that we
want to introduce latency for.

234
00:19:29,132 --> 00:19:33,132
Now that we have specified all of
that information, let's take a look

235
00:19:33,172 --> 00:19:38,382
at the second major component of
the template, which is the target.

236
00:19:38,882 --> 00:19:43,702
So in our case, the target
is a set of ECS tasks.

237
00:19:44,202 --> 00:19:47,202
We are specifying the ECS cluster name.

238
00:19:47,732 --> 00:19:56,172
and the ECS service name to identify
the tasks that FIS should be targeting

239
00:19:56,872 --> 00:19:58,952
for this particular experiment.

240
00:19:59,452 --> 00:20:03,512
And, another key thing
here is the, permission.

241
00:20:03,522 --> 00:20:07,402
So as I mentioned, earlier, you
can control the permissions and

242
00:20:07,402 --> 00:20:10,182
experiment has through, IAM role.

243
00:20:10,682 --> 00:20:15,672
And then controlling the blast
radius of an experiment is important.

244
00:20:15,672 --> 00:20:22,272
So you can specify CloudWatch alarms
as stop conditions for an experiment.

245
00:20:22,482 --> 00:20:26,972
So in this sample application,
you're going to leave that as blank.

246
00:20:27,472 --> 00:20:31,332
So let's go back to the
experiment template itself.

247
00:20:31,832 --> 00:20:36,002
So before we start running any
experiments, we want to make

248
00:20:36,002 --> 00:20:40,972
sure that we understand the
steady state of our application.

249
00:20:41,877 --> 00:20:46,607
so we have the prerequisites in
place in terms of observability

250
00:20:46,607 --> 00:20:49,237
in monitoring and load generation.

251
00:20:49,957 --> 00:20:53,587
The next thing is we understand the
steady state of the application.

252
00:20:53,587 --> 00:20:56,117
What is the, output of the metrics?

253
00:20:56,127 --> 00:20:59,852
What is the value of the key
metrics that we are interested in?

254
00:21:00,692 --> 00:21:04,862
So let's go back and take a look at our
CloudWatch dashboards that should have

255
00:21:04,912 --> 00:21:08,462
caught up with the load generation script.

256
00:21:08,462 --> 00:21:12,542
So let's say in the last five minutes,
what does the metrics look like?

257
00:21:12,732 --> 00:21:17,852
So as I, as we expect, there
are no errors from our API.

258
00:21:18,402 --> 00:21:23,082
So that's the other thing, if there are
errors in an application that you are

259
00:21:23,482 --> 00:21:28,062
trying to run experiments on, you might
want to pause that experimentation,

260
00:21:28,672 --> 00:21:31,252
go back, and fix those errors, right?

261
00:21:31,252 --> 00:21:35,112
So if you have known errors, you
would want to address that before

262
00:21:35,312 --> 00:21:37,412
starting with any experimentation.

263
00:21:37,912 --> 00:21:44,212
And then you can see here that we have two
key widgets that, we have identified for

264
00:21:44,222 --> 00:21:47,662
observability of our application workload.

265
00:21:48,382 --> 00:21:51,452
One of them is the API gateway latency.

266
00:21:51,522 --> 00:21:58,032
So from a. System perspective, from let's
say a system operator perspective, what

267
00:21:58,052 --> 00:22:00,352
does the application health look like?

268
00:22:00,582 --> 00:22:04,112
And what are the measures
of those different metrics?

269
00:22:04,612 --> 00:22:10,012
At the same time, you would want to
also understand the behavior of the

270
00:22:10,052 --> 00:22:13,482
application from an end user perspective.

271
00:22:13,922 --> 00:22:18,592
Sometimes there could be a disconnect
between what an operator thinks health

272
00:22:18,592 --> 00:22:21,362
of an application is versus an end user.

273
00:22:21,982 --> 00:22:27,102
So you would want to have a pane of
glass into each of those perspectives

274
00:22:27,442 --> 00:22:30,012
to understand the system behavior.

275
00:22:30,422 --> 00:22:32,542
So that's what we are
trying to demonstrate here.

276
00:22:33,042 --> 00:22:40,212
So from an, system perspective, our
latency is around 22 to 25 milliseconds.

277
00:22:40,752 --> 00:22:46,852
And the P90 value from an end
user perspective is around.

278
00:22:47,232 --> 00:22:51,502
75 to 150 milliseconds.

279
00:22:52,262 --> 00:22:55,612
So that's the steady
state of our application.

280
00:22:56,282 --> 00:23:02,582
And hopefully these metrics are well
within the threshold that we have,

281
00:23:03,012 --> 00:23:08,362
agreed upon with our end user, with
our application owners saying that,

282
00:23:08,372 --> 00:23:13,662
this is the latency metric that, our
application will be running with.

283
00:23:14,212 --> 00:23:16,782
We now have the prerequisites.

284
00:23:17,032 --> 00:23:20,462
We have a steady state understanding.

285
00:23:21,092 --> 00:23:26,242
Now the next thing before we run any
type of experimentation would be to

286
00:23:26,672 --> 00:23:29,182
come up with a hypothesis, right?

287
00:23:29,672 --> 00:23:33,192
Especially, in this case
with network latency.

288
00:23:33,692 --> 00:23:41,752
So the network latency, our hypothesis
is that in the event of a latency in

289
00:23:41,752 --> 00:23:48,252
the network path between our compute
layer and the database layer, we

290
00:23:48,252 --> 00:23:52,562
believe that the overall latency
of the application would go up.

291
00:23:53,062 --> 00:23:57,392
It would still be well within
the threshold of, what we

292
00:23:57,392 --> 00:24:01,712
consider as healthier, healthy
state for our application.

293
00:24:02,102 --> 00:24:05,562
So the latency would go up, it
will bump up, but it's going to be

294
00:24:05,572 --> 00:24:11,162
well within the thresholds that we
have defined for our application.

295
00:24:11,662 --> 00:24:17,102
So let's see if that hypothesis is going
to hold true by running this experiment.

296
00:24:17,602 --> 00:24:23,692
So I'm going to start the experiment,
add a tag and then I'm going to You

297
00:24:23,692 --> 00:24:26,822
know, provide a value for the tag.

298
00:24:27,322 --> 00:24:31,502
So this will help us, differentiate
between the various runs of an

299
00:24:31,502 --> 00:24:37,572
experiment in the event that, we are
attempting the same experiment multiple

300
00:24:37,602 --> 00:24:40,672
times as we observe different things.

301
00:24:41,172 --> 00:24:44,682
Now, once I start to start the
experiment, Aquarius is going

302
00:24:44,682 --> 00:24:47,282
to ask me to confirm that.

303
00:24:47,782 --> 00:24:50,142
And once I have that ready.

304
00:24:50,642 --> 00:24:54,572
You can see the experiment going
through a couple of phases.

305
00:24:54,572 --> 00:24:59,322
It starts with an initiating phase
where it collects all the resources

306
00:24:59,322 --> 00:25:03,282
and everything it needs and then
moves into a running state and

307
00:25:03,282 --> 00:25:07,402
then into either a failed state
or a completed state, in a minute.

308
00:25:07,902 --> 00:25:13,382
And then if we look at our actions,
in for this experiment template, with

309
00:25:13,382 --> 00:25:16,782
this experiment, we only have one,
action, which is a network latency.

310
00:25:16,782 --> 00:25:17,432
That's going to be.

311
00:25:17,702 --> 00:25:19,432
Also running.

312
00:25:19,932 --> 00:25:24,472
we are running that for five
minutes in this particular case.

313
00:25:25,382 --> 00:25:31,562
Now, if I look at the resources, if you
remember in the template, we only gave

314
00:25:31,592 --> 00:25:37,842
the cluster name and the service name,
but FIS has identified that these are

315
00:25:37,872 --> 00:25:45,152
the two tasks that it should be running
these latency experiments against.

316
00:25:45,652 --> 00:25:49,702
So we've started this a couple
of minutes ago, so we'll give

317
00:25:49,732 --> 00:25:54,322
the observability tooling a
minute or so to catch up as well.

318
00:25:54,962 --> 00:25:59,562
Let's take a look at a couple of other
features and functions So here under the

319
00:25:59,572 --> 00:26:06,672
timeline, you can see the various actions
FIS is performing and where they are.

320
00:26:06,672 --> 00:26:11,412
So in the event of, multiple
actions, you would see them getting

321
00:26:11,422 --> 00:26:18,202
queued up or running at any time
while the execution is ongoing.

322
00:26:19,047 --> 00:26:25,187
And if there are any, log events
associated with that particular experiment

323
00:26:25,197 --> 00:26:27,327
run, you can see that here as well.

324
00:26:27,827 --> 00:26:31,287
So now that it's been a couple
of minutes, after we started the

325
00:26:31,287 --> 00:26:37,897
experiment, let's take a look at
our, application monitoring tool.

326
00:26:38,357 --> 00:26:46,147
So here our p90 values was in the
steady state, 75 to, 150 milliseconds.

327
00:26:46,147 --> 00:26:50,127
Let's see how it has changed
with this experimentation.

328
00:26:50,627 --> 00:26:56,297
Yeah, so you can see as expected
the average latency has gone up

329
00:26:56,307 --> 00:27:02,487
from a system perspective and a real
user monitoring perspective, right?

330
00:27:02,487 --> 00:27:14,267
So from 75 to 150, it's now gone up to 262
milliseconds, almost, double the measure

331
00:27:14,277 --> 00:27:18,947
that we have seen initially and that is
continuously continue to trending up.

332
00:27:19,447 --> 00:27:22,537
But, we can observe that
there are no errors.

333
00:27:22,557 --> 00:27:25,977
The application is still
continuing to function well.

334
00:27:26,447 --> 00:27:30,407
there are no errors being
returned, from a user perspective

335
00:27:30,507 --> 00:27:32,127
or from a system perspective.

336
00:27:33,107 --> 00:27:40,197
Now, the big question is, this increase in
latency, is that within the threshold that

337
00:27:40,207 --> 00:27:44,497
we have agreed upon with our system users?

338
00:27:44,767 --> 00:27:49,217
So that's a decision that you will
have to make with the stakeholder.

339
00:27:49,227 --> 00:27:53,447
Say we saw the, that
the latency has gone up.

340
00:27:53,777 --> 00:27:58,997
is that latency still acceptable
in an event like this?

341
00:27:59,757 --> 00:28:00,207
If it is not.

342
00:28:01,142 --> 00:28:04,632
You will have to take a look at
some remedial actions, right?

343
00:28:04,632 --> 00:28:10,482
So if the latency were to go up beyond
a certain threshold, maybe fail over

344
00:28:10,482 --> 00:28:15,712
your database from your primary to
standby or, something like that.

345
00:28:16,272 --> 00:28:22,172
So those are the, Potential actions that
you can take with these observations.

346
00:28:22,672 --> 00:28:26,072
Oh, let's go back and take a
look at that experiment template.

347
00:28:26,102 --> 00:28:30,432
I think it should be
finishing up here, any minute.

348
00:28:30,932 --> 00:28:38,492
so we'll let that, complete while we,
look at the next, experiment template.

349
00:28:38,992 --> 00:28:42,942
So let's try the, black hole one.

350
00:28:43,442 --> 00:28:50,122
So in this case, what we are doing
is trying to create a network

351
00:28:50,142 --> 00:28:56,032
black hole between our compute
layer and the database layer.

352
00:28:56,912 --> 00:29:03,612
So if I look at the action for this
particular experiment template, you

353
00:29:03,612 --> 00:29:05,612
can see it is network black hole.

354
00:29:06,112 --> 00:29:09,412
And what are the parameters
this action supports?

355
00:29:09,502 --> 00:29:11,792
So again, we're going to
run it for five minutes.

356
00:29:12,292 --> 00:29:17,232
And this is the port number that our
RDS cluster was listening at, right?

357
00:29:17,232 --> 00:29:21,722
So I've specified that as one of
the parameters for this action.

358
00:29:22,392 --> 00:29:28,192
And any TCP traffic coming into
that port will be black holed.

359
00:29:28,692 --> 00:29:33,532
And then in terms of targets,
it's still the set of ECS tasks.

360
00:29:33,862 --> 00:29:37,802
that will be targeted by our experiment.

361
00:29:38,302 --> 00:29:41,852
Now let's go back and make sure our
previous experiment is completed.

362
00:29:41,862 --> 00:29:42,692
Yes, it has.

363
00:29:43,192 --> 00:29:49,832
And now coming back to the black hole
experiment, you already have You know,

364
00:29:49,832 --> 00:29:55,842
the prerequisites, observability, load
generation, we've, looked at the steady

365
00:29:55,842 --> 00:29:59,192
state metrics of our, application as well.

366
00:29:59,692 --> 00:30:02,002
And now the next thing is hypothesis.

367
00:30:02,012 --> 00:30:06,232
So what is our hypothesis for
this particular experiment?

368
00:30:06,732 --> 00:30:11,512
So here, what we are saying is that if
there is a network black hole between

369
00:30:11,542 --> 00:30:18,332
our compute layer and the database
layer, we believe our application is

370
00:30:18,332 --> 00:30:24,822
going to recognize that, and it's going
to return a user friendly error code or

371
00:30:24,822 --> 00:30:28,602
error message to the application users.

372
00:30:29,472 --> 00:30:34,962
So now let's see if that hypothesis
is going to be valid or not.

373
00:30:35,462 --> 00:30:41,272
Now before we do that, let's look at
our observability tooling real quick

374
00:30:41,272 --> 00:30:43,262
and make sure that it is getting back.

375
00:30:43,282 --> 00:30:50,682
Yeah, so once our experiment is complete
for latency, you can see that the metrics

376
00:30:50,682 --> 00:30:53,032
are getting back to their original state.

377
00:30:53,412 --> 00:31:01,082
So this will also help us give the
confidence that our application is healing

378
00:31:01,082 --> 00:31:05,292
from that network latency event 30.

379
00:31:05,792 --> 00:31:11,972
Okay, now let's go back to the
black hole experiment template and

380
00:31:11,972 --> 00:31:14,792
start that, then give it a name,

381
00:31:15,292 --> 00:31:18,022
then start the experiment.

382
00:31:18,522 --> 00:31:24,102
So as we saw earlier, it's going to go
through a couple of phases, and, perform

383
00:31:24,102 --> 00:31:26,462
the actions that we have specified.

384
00:31:26,972 --> 00:31:31,272
And it's going to still target
those couple of ECS tasks that

385
00:31:31,272 --> 00:31:34,662
are running our application code.

386
00:31:35,387 --> 00:31:42,927
So we'll give this experiment a minute or
so to, impact these resources and we will

387
00:31:42,947 --> 00:31:46,267
see, what our application behavior is.

388
00:31:46,767 --> 00:31:52,667
So while that's happening, let's look
at the last template here, which follows

389
00:31:52,667 --> 00:31:58,377
a similar construct, which is, in
this case, it is network packet loss.

390
00:31:59,112 --> 00:32:02,582
So let's take a look at the
different parameters for

391
00:32:02,592 --> 00:32:05,142
that particular action here.

392
00:32:05,642 --> 00:32:09,452
So for the packet loss action,
one of the parameters you can

393
00:32:09,452 --> 00:32:11,832
specify is the loss percentage.

394
00:32:11,862 --> 00:32:14,762
So by default it is 7, but
you can, configure that.

395
00:32:15,552 --> 00:32:19,932
And similar to the latency,
you can specify a source.

396
00:32:20,417 --> 00:32:25,717
Where you would want to inject this
network packet loss for and another

397
00:32:26,037 --> 00:32:32,137
important thing that I want to call out
in terms of sources is if let's say your

398
00:32:32,137 --> 00:32:38,507
application is connecting to DynamoDB
or S3 and using them as the storage

399
00:32:38,507 --> 00:32:47,227
layer, then you can just specify that
literal text in the sources And FIS

400
00:32:47,257 --> 00:32:53,607
will kind, do the, IP resolution for
you and then make sure that any traffic

401
00:32:53,617 --> 00:32:59,517
that is going out to DynamoDB and S3
is impacted by this network fault.

402
00:32:59,697 --> 00:33:04,597
You don't have to, provide the IP range
for these services or anything like that.

403
00:33:05,367 --> 00:33:11,137
The resolution of these literal
texts to appropriate IP addresses are

404
00:33:11,147 --> 00:33:13,547
handled behind the scenes by DynamoDB.

405
00:33:14,047 --> 00:33:21,067
And, like we did here, you can specify a
domain name of, let's say, a third party.

406
00:33:21,127 --> 00:33:26,377
Let's say you have a dependency, an
application dependency that is on

407
00:33:26,377 --> 00:33:31,607
premise or running elsewhere other
than AWS, maybe a SaaS product.

408
00:33:32,457 --> 00:33:35,597
Then you can specify the
domain name of those.

409
00:33:35,962 --> 00:33:41,802
end points of those dependencies that
your application might be communicating,

410
00:33:41,812 --> 00:33:45,052
through APIs and add that here.

411
00:33:45,522 --> 00:33:50,542
So what that would do is then,
inject latency or packet loss

412
00:33:50,792 --> 00:33:53,392
into that particular network path.

413
00:33:53,612 --> 00:33:58,972
So that will help you test and
validate, any potential network

414
00:33:58,972 --> 00:34:04,022
faults that can happen with those
third party dependencies as well.

415
00:34:04,522 --> 00:34:10,452
So let me cancel out the packet loss
experiment template and let's see

416
00:34:10,472 --> 00:34:14,192
where our black hole experiment is.

417
00:34:14,692 --> 00:34:15,532
Oh, okay.

418
00:34:15,632 --> 00:34:19,392
let's see where the, experiment status.

419
00:34:19,892 --> 00:34:23,722
I'm going to go refresh
the, dashboard here.

420
00:34:23,792 --> 00:34:30,952
Oh, so now you can see that in the
event of this particular experiment,

421
00:34:31,452 --> 00:34:37,022
We are seeing errors both from a user
perspective and from a system perspective.

422
00:34:37,372 --> 00:34:46,357
API gateway is throwing 500 errors
and the End user in case in our case

423
00:34:46,357 --> 00:34:52,397
artillery is actually seeing timeouts
So this was not the behavior that we

424
00:34:52,467 --> 00:35:00,087
expected right, we were expecting that
in this case our Application will handle

425
00:35:00,107 --> 00:35:07,037
that timeout that net black hole between
our compute layer and database layer

426
00:35:07,322 --> 00:35:11,222
And return an appropriate error code or
error message or something like that.

427
00:35:11,862 --> 00:35:17,602
So this is an important observation of
the application behavior that we want to

428
00:35:17,602 --> 00:35:20,322
go back to the drawing board and correct.

429
00:35:20,822 --> 00:35:26,272
So those are the two things, two network
faults that I wanted to show you today.

430
00:35:26,652 --> 00:35:35,492
So we had a sample API application that
was performing, CRUD actions and we saw

431
00:35:35,852 --> 00:35:42,027
the behavior of that application when
we injected network latency as well as

432
00:35:42,152 --> 00:35:45,272
network black hole into that workload.

433
00:35:46,012 --> 00:35:50,152
So with respect to latency, we
did see a spike in the average

434
00:35:50,152 --> 00:35:52,252
latency and the P nine D value.

435
00:35:52,252 --> 00:35:57,222
For that API endpoint, you would have
to take a look and see, if those metrics

436
00:35:57,222 --> 00:35:59,592
are within the acceptable threshold.

437
00:36:00,277 --> 00:36:05,737
And then in case of black hole, we do need
to do some work with our application to

438
00:36:05,737 --> 00:36:13,187
identify that event and return appropriate
error messages to the end users.

439
00:36:13,687 --> 00:36:15,637
So again, thank you for your time.

440
00:36:15,667 --> 00:36:19,087
really had fun talking
about this topic with you.

441
00:36:19,547 --> 00:36:21,087
Have a good rest of the day.

