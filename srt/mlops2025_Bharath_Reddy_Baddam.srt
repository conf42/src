1
00:00:04,800 --> 00:00:05,130
Everyone.

2
00:00:05,460 --> 00:00:06,360
My name is Barta.

3
00:00:07,290 --> 00:00:11,070
I have around nine years of
experience in Salesforce cloud

4
00:00:11,130 --> 00:00:13,380
technologies and AI driven solution.

5
00:00:14,460 --> 00:00:18,900
I have worked with finance,
healthcare, and real estate industries.

6
00:00:19,560 --> 00:00:24,360
Today we are going to talk about a
problem that resonates across industries.

7
00:00:25,200 --> 00:00:30,990
Why so many promising machine learning
models never make it into production?

8
00:00:31,770 --> 00:00:36,180
And how we lop peers practices
can help bridge that gap.

9
00:00:37,080 --> 00:00:42,570
Think of this talk as a roadmap from
research to production ready systems

10
00:00:42,660 --> 00:00:48,690
covering the challenges, orchestration,
infras monitoring, and ca cd.

11
00:00:50,280 --> 00:00:54,000
Today's agenda, our journey
will move across five pillars.

12
00:00:54,570 --> 00:00:59,040
The MOPS challenge, why so many
model get stuck in the lab?

13
00:00:59,910 --> 00:01:05,070
Yml pipeline orchestration tools and
techniques to automate workflows.

14
00:01:06,120 --> 00:01:07,620
Production infrastructure.

15
00:01:08,130 --> 00:01:11,040
How to make Y ML systems
scalable and reliable.

16
00:01:11,820 --> 00:01:12,630
Monitoring and observability.

17
00:01:14,430 --> 00:01:18,540
Ensuring model continues to
deliver value after deployment.

18
00:01:19,800 --> 00:01:22,260
CI cd for EML adapting.

19
00:01:22,740 --> 00:01:23,850
DevOps best practices.

20
00:01:24,255 --> 00:01:24,975
Machine learning.

21
00:01:25,935 --> 00:01:30,015
By the end of this session, you
will have a complete picture of

22
00:01:30,074 --> 00:01:36,465
what M-L-O-P-S maturity looks like
and our organizations can, the main

23
00:01:36,465 --> 00:01:41,205
challenge, the M-L-O-P-S challenge,
let's start with the problem study.

24
00:01:41,205 --> 00:01:46,965
Show that 20% of ML model server
ever make it to production.

25
00:01:47,715 --> 00:01:52,690
That means four out of PRO five
projects never deliver value.

26
00:01:52,990 --> 00:01:53,490
BM research.

27
00:01:55,110 --> 00:01:56,190
Why does this happen?

28
00:01:58,080 --> 00:02:02,970
Some key barriers include reproducibility,
gaps between research and engineering,

29
00:02:03,600 --> 00:02:09,660
lack of standardized deployment process,
insufficient monitoring and maintenance

30
00:02:09,660 --> 00:02:18,510
of framework and inadequate testing
methodologies for particularly ML systems.

31
00:02:19,410 --> 00:02:22,019
Poor integration with enterprise systems.

32
00:02:22,995 --> 00:02:27,525
I am sure you have seen brilliant
proof of concepts die in a lab

33
00:02:28,275 --> 00:02:30,735
because of these exact issues.

34
00:02:37,185 --> 00:02:40,065
The M-L-O-P-S evaluation

35
00:02:42,190 --> 00:02:47,895
over the years, M-L-O-P-S has matured
in stages manually, ML process,

36
00:02:48,375 --> 00:02:51,375
isolated notebooks, one of deployments.

37
00:02:52,380 --> 00:02:58,230
No repeatability, and we have EML pipeline
automation, basic training automation,

38
00:02:58,470 --> 00:03:00,510
but limited to production integration.

39
00:03:01,410 --> 00:03:06,630
CACD for EML, bringing DevOps,
rigor, automated testing, structured

40
00:03:06,630 --> 00:03:09,600
releases, and inconsistent deployment.

41
00:03:10,350 --> 00:03:12,120
Full E-M-L-O-P-S maturity.

42
00:03:12,125 --> 00:03:12,745
End-to-end.

43
00:03:12,810 --> 00:03:18,840
Automation monitoring, drift detection,
retraining rollback strategies.

44
00:03:19,815 --> 00:03:26,025
For organizations that reach
maturity, reduce deployment times by

45
00:03:26,025 --> 00:03:31,605
80% and it'll sustain 95% accuracy
in the production environments.

46
00:03:32,204 --> 00:03:35,355
This is not a just technical
gist transformation.

47
00:03:35,774 --> 00:03:36,734
It's a cultural world.

48
00:03:36,795 --> 00:03:41,084
This engineers and operation
teams collaborates seamlessly.

49
00:03:45,405 --> 00:03:47,445
ML pipeline orchestration.

50
00:03:48,375 --> 00:03:50,325
Now let's dive into orchestration.

51
00:03:50,775 --> 00:03:54,315
Imagine the ML'S lifecycle
as a factory assembly.

52
00:03:55,454 --> 00:04:00,225
If any step is manual or fragile,
the whole process slows down.

53
00:04:00,975 --> 00:04:08,175
A fully automated MI pipeline should cover
data prep, cleaning, future engineering

54
00:04:08,175 --> 00:04:14,540
validation, and model training in that
hypermarket tuning, tracking experiments.

55
00:04:15,525 --> 00:04:20,865
Model evolution testing against
holdout data sets and validating

56
00:04:20,865 --> 00:04:26,295
business KPIs, deployment, packaging,
and serving models consistent.

57
00:04:27,555 --> 00:04:32,145
And the other one is monitoring
continuous watch for drift

58
00:04:32,655 --> 00:04:34,635
error and performance issues.

59
00:04:36,015 --> 00:04:41,265
Without this orchestration, every new
model is a reinvention of the wheel.

60
00:04:42,599 --> 00:04:46,320
And yummy pipeline
orchestration components.

61
00:04:47,520 --> 00:04:51,299
Different teams choose different
orchestration tools based on their eco

62
00:04:51,869 --> 00:04:57,330
ecosystem, for example, or QB flow.

63
00:04:57,989 --> 00:05:03,270
QB flow is the one thing it has,
uh, Kubernetes native platform.

64
00:05:03,809 --> 00:05:04,109
Great.

65
00:05:04,260 --> 00:05:08,729
If our org is already invested in
Kubernetes, it has strong model serving.

66
00:05:10,185 --> 00:05:11,475
Pipeline Libraries.

67
00:05:12,345 --> 00:05:17,115
EML Flow, it's it's lightweight,
experiment tracking model,

68
00:05:17,115 --> 00:05:19,965
registry, and language specific.

69
00:05:20,535 --> 00:05:24,705
Perfect for the experimentation
and for that particular heavy

70
00:05:24,705 --> 00:05:27,855
teams and airflow, Apache airflow.

71
00:05:28,425 --> 00:05:34,000
The general workflow engine, it is
not specific to ML specific, but

72
00:05:34,130 --> 00:05:36,360
great for complex data, heavy process.

73
00:05:37,815 --> 00:05:44,474
In practice, many organizations have mix
and match of these, uh, comparison tools.

74
00:05:45,615 --> 00:05:50,205
For example, you might use ML
flow for experiment tracking while

75
00:05:50,414 --> 00:05:53,175
deploying models, we have a QB flow.

76
00:05:54,465 --> 00:06:01,544
When picking tools always ask, does this
fit our workflow, our skillset, and are we

77
00:06:01,544 --> 00:06:05,594
forcing the team unnecessary complexity.

78
00:06:06,810 --> 00:06:08,580
Production infra structure.

79
00:06:09,270 --> 00:06:14,520
Even the best model fail without
the right infrastructure here, what

80
00:06:14,520 --> 00:06:16,470
modern ML infrastructure look like.

81
00:06:17,520 --> 00:06:22,320
Infrastructure has cored with terraform
are pmi, ensuring environments

82
00:06:22,320 --> 00:06:26,045
are reproductive containerization.

83
00:06:26,490 --> 00:06:31,260
With docker package dependencies
to eliminate, it works on

84
00:06:31,260 --> 00:06:34,410
machine deployment automation.

85
00:06:34,784 --> 00:06:39,885
Like CACD pipelines, pushing
code and model across dev test.

86
00:06:39,974 --> 00:06:45,525
Broad scalability, Kubernetes,
auto scaling for spikes in

87
00:06:45,525 --> 00:06:50,085
production, fast optimization,
dynamic scaling, and right size.

88
00:06:51,270 --> 00:06:57,044
The main goal is to build infrastructure
that as a, as the model themselves.

89
00:06:58,455 --> 00:07:02,715
Real time inference
architecture, many applicants.

90
00:07:03,675 --> 00:07:08,955
Many applications, fraud detection
chart bots, recommendation engines

91
00:07:09,705 --> 00:07:11,535
require real time predictions.

92
00:07:11,745 --> 00:07:14,955
That means 50 to 20 millisecond latency.

93
00:07:16,485 --> 00:07:17,715
Key design considerations.

94
00:07:18,765 --> 00:07:23,895
Horizonal scaling Kubernetes clusters that
scale out with the demand load balance,

95
00:07:24,405 --> 00:07:31,785
ensuring spread evenly with health checks
cashing, using Redis for frequent queries.

96
00:07:32,190 --> 00:07:37,110
It'll reduce the compute
hardware acceleration kind

97
00:07:37,110 --> 00:07:39,660
of GPUs, pus wherever needed.

98
00:07:40,260 --> 00:07:45,300
One example, like a healthcare client,
a realtime insurance eligibility checks

99
00:07:45,750 --> 00:07:51,990
the reduced wait times from minutes
to milliseconds because the model on

100
00:07:52,470 --> 00:07:59,790
an optimized auto-scaling architecture
and real time inference architectures.

101
00:08:00,540 --> 00:08:07,620
For example, our scaling load, balancing
caching layers, hardware acceleration.

102
00:08:08,280 --> 00:08:09,480
These are the things

103
00:08:15,600 --> 00:08:17,100
monitoring observability.

104
00:08:17,730 --> 00:08:19,830
Deploying a model is not the end.

105
00:08:20,490 --> 00:08:21,990
Beginning model.

106
00:08:22,080 --> 00:08:23,219
Drift has changes.

107
00:08:23,760 --> 00:08:25,500
Monitoring covers three layers.

108
00:08:26,040 --> 00:08:26,970
Data drift.

109
00:08:27,300 --> 00:08:32,549
Statistical checks, scale divergence
and population stability index.

110
00:08:33,539 --> 00:08:39,960
For model performances accuracy, we
F1 A UC, and we have to continuously

111
00:08:40,530 --> 00:08:42,330
track against those benchmarks.

112
00:08:43,230 --> 00:08:46,050
Operational metrics, latency, thorough.

113
00:08:46,050 --> 00:08:46,380
Put.

114
00:08:46,965 --> 00:08:49,335
As well as resource
utilization is the main.

115
00:08:49,995 --> 00:08:54,975
If you are not monitoring
your flying behind even that's

116
00:08:54,975 --> 00:08:56,655
a dangerous in production.

117
00:08:57,225 --> 00:08:58,755
We have to maintain that properly.

118
00:08:59,895 --> 00:09:04,725
Model monitoring systems,
automated retraining deployments.

119
00:09:05,025 --> 00:09:07,095
How do we respond for that?

120
00:09:07,814 --> 00:09:08,660
How do we respond?

121
00:09:08,685 --> 00:09:09,855
The drift is detected.

122
00:09:09,915 --> 00:09:14,980
Automated retraining, common triggers,
performance drops below threshold.

123
00:09:15,750 --> 00:09:19,800
Time-based schedules, weekly or
monthly retaining database triggers

124
00:09:19,800 --> 00:09:24,660
significant distribution shift, and
we have to follow the distribution

125
00:09:25,260 --> 00:09:26,580
and deployment strategies.

126
00:09:26,970 --> 00:09:30,390
Deployments, new models will run silently.

127
00:09:30,390 --> 00:09:36,240
In parallel canary releases roll out five
person of the traffic before scaling up

128
00:09:37,200 --> 00:09:42,200
and AB testing, measuring the business
impact, we have to test it thoroughly.

129
00:09:43,020 --> 00:09:44,490
And automated rollback.

130
00:09:44,700 --> 00:09:49,740
If the new model under profits, if it is
not performing perfectly, we have to roll

131
00:09:49,740 --> 00:09:55,200
back that, uh, functionality and we need
the deployment strategies to be in place.

132
00:09:55,620 --> 00:10:01,140
These combinations ensures both
agility and safety particular

133
00:10:08,465 --> 00:10:08,685
and.

134
00:10:09,375 --> 00:10:11,444
Particularly automated retraining.

135
00:10:11,745 --> 00:10:15,405
We spoke right based time
based database volume.

136
00:10:16,125 --> 00:10:17,955
Those are the main key things.

137
00:10:19,064 --> 00:10:20,955
And CACD for machine learning.

138
00:10:21,975 --> 00:10:24,435
Finally, we have to
speak about CACD for EML.

139
00:10:25,064 --> 00:10:27,615
DevOps, transform Software Engineering.

140
00:10:28,064 --> 00:10:31,185
Ya Melo peers does the same for.

141
00:10:32,564 --> 00:10:37,905
Ski testing MLCA CD Pipeline
data validation schema checks,

142
00:10:38,265 --> 00:10:41,595
missing value detection model.

143
00:10:41,835 --> 00:10:43,064
Model performance test.

144
00:10:43,545 --> 00:10:47,805
Ion checks prevent black
sliding integration test.

145
00:10:47,895 --> 00:10:51,735
We have to validate the entire
pipeline and load and performance

146
00:10:51,735 --> 00:10:56,535
test ensuring the production
readiness based on the application.

147
00:10:56,970 --> 00:10:58,650
Security and compliance test.

148
00:10:59,010 --> 00:11:01,980
We have to follow GDPR,
hip, uh, PA production.

149
00:11:02,370 --> 00:11:05,310
We have to, security is the main concern.

150
00:11:05,730 --> 00:11:12,570
Without these all models, whatever
these will, if these are not there, we,

151
00:11:13,200 --> 00:11:15,000
that particular models will be failing.

152
00:11:15,000 --> 00:11:21,545
Production with them becomes
predictable, reputable, and it's there.

153
00:11:22,845 --> 00:11:23,065
And.

154
00:11:28,785 --> 00:11:35,430
In my specific strategy for testing,
we have different data validation, test

155
00:11:35,790 --> 00:11:39,870
model, performance test integration,
test load, and performance test.

156
00:11:40,320 --> 00:11:44,220
We have to verify the data for data
validation, and we have to check the

157
00:11:44,880 --> 00:11:47,280
delivery scenario, model performance test.

158
00:11:47,505 --> 00:11:51,690
We had to check whether
we met the model or not.

159
00:11:52,260 --> 00:11:57,330
If not, we have to check that particular
and the integration test, validate the

160
00:11:57,330 --> 00:12:02,610
entire pipeline and load performance test,
ensuring the production, the readiness,

161
00:12:03,090 --> 00:12:05,910
security, compliance and everything.

162
00:12:07,260 --> 00:12:15,120
And coming to the real challenge of this
is most of the ML models never, never

163
00:12:15,300 --> 00:12:17,730
have the never reach the production.

164
00:12:18,360 --> 00:12:21,360
MLO PS offers a way
far from orchestration.

165
00:12:21,839 --> 00:12:27,540
Infrastructure monitoring and
C. A C. It means S is not just

166
00:12:27,540 --> 00:12:32,819
about the models, but about the
system culture and collaboration.

167
00:12:33,449 --> 00:12:38,640
If there is no one takeaway, read machine
learning as a product, not a project.

168
00:12:39,510 --> 00:12:43,650
Product requires lifecycle
management, testing, monitoring,

169
00:12:43,709 --> 00:12:45,030
and continuous improvement.

170
00:12:46,500 --> 00:12:48,540
These are the key things for us.

171
00:12:49,140 --> 00:12:50,339
Thank you for your time.

172
00:12:51,045 --> 00:12:52,814
If you have any questions,
please reach out.

173
00:12:53,265 --> 00:12:53,474
Thank.

