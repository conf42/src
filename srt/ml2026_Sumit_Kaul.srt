1
00:00:00,300 --> 00:00:07,260
Speaker 33: Welcome to Con 42
Machine Learning Conference 2026.

2
00:00:07,590 --> 00:00:11,160
My name is Sumit Call and I'm a
staff software engineer at PHI.

3
00:00:11,660 --> 00:00:17,060
And topic for my today's talk is
LLM, agent for site reliability

4
00:00:17,560 --> 00:00:21,189
production, safe architectures
for AI Powered Incident Response.

5
00:00:21,689 --> 00:00:22,680
So when we think about.

6
00:00:23,430 --> 00:00:28,285
LLM agent or site liability or
production or incident response we

7
00:00:28,285 --> 00:00:31,454
start thinking about data, right?

8
00:00:31,694 --> 00:00:35,474
So the modern incident response
is overwhelming in many ways.

9
00:00:35,595 --> 00:00:39,735
We have too many alerts, we have
too much data, we have too little

10
00:00:39,735 --> 00:00:41,504
time to respond to those, right?

11
00:00:41,985 --> 00:00:46,425
And the key question we have
to ask is, how can LLM help SRE

12
00:00:46,425 --> 00:00:48,074
without putting production at risk?

13
00:00:48,574 --> 00:00:51,814
And if you look at the very
theme of it, we do not fail

14
00:00:51,814 --> 00:00:53,884
incidents because we lack data.

15
00:00:54,334 --> 00:00:58,234
We fail because humans cannot
process it all under pressure.

16
00:00:58,984 --> 00:01:05,224
So this talk is about using a lenss to
reduce that load in a safe manner, right?

17
00:01:05,945 --> 00:01:10,065
And if you think from a problem
statement perspective also, so incident

18
00:01:10,065 --> 00:01:14,205
response is becoming harder and not
easier despite better tooling, right?

19
00:01:14,880 --> 00:01:21,270
So if you look at the core idea, LLM
agents can help reason faster, but

20
00:01:21,270 --> 00:01:23,730
only if they're designed in a safe way.

21
00:01:24,230 --> 00:01:27,560
And a production safe environment is
very important because when we talk

22
00:01:27,560 --> 00:01:32,300
about incident responses, we're always
dealing in a production environment.

23
00:01:32,630 --> 00:01:37,979
And this is not about demos or co-pilots
that can take risky actions, but do things

24
00:01:37,979 --> 00:01:40,360
in a much more safer and trusted way.

25
00:01:40,860 --> 00:01:45,210
So with that, we're going to
get started for our Con 42

26
00:01:45,210 --> 00:01:46,440
Machine Learning Conference.

27
00:01:46,800 --> 00:01:52,129
And this talk will basically talk about
how we'll use a L LMS in a real incident

28
00:01:52,129 --> 00:01:55,249
response without breaking trust or system.

29
00:01:55,749 --> 00:01:59,770
So the escalating Incident
response challenge, let's talk

30
00:01:59,770 --> 00:02:02,020
about that, what exactly it is.

31
00:02:02,110 --> 00:02:04,960
So if I had to describe
modern SRE reality.

32
00:02:05,934 --> 00:02:09,474
We have thousands of alerts per day
across increasingly complex system.

33
00:02:09,474 --> 00:02:13,324
So if I were to give you example
of our organization we have

34
00:02:14,194 --> 00:02:17,944
somewhere around 4 million messages
per day coming for one endpoint.

35
00:02:17,944 --> 00:02:20,974
We have more than 200
endpoints like these.

36
00:02:21,244 --> 00:02:25,894
And if you can, just for the
example, it can easily go o over.

37
00:02:26,465 --> 00:02:30,125
10 million requests per day,
sometimes few hours, right?

38
00:02:30,545 --> 00:02:36,464
So it's a very high volume of alerts
per day across very complex systems.

39
00:02:36,884 --> 00:02:42,864
So it's very important to call out
the alert fatigue and the shift from a

40
00:02:42,864 --> 00:02:45,594
proactive to the reactive operations.

41
00:02:46,344 --> 00:02:49,284
When humans are overloaded
because of that.

42
00:02:49,284 --> 00:02:52,864
And I think if you look at the core
reason for that, those humans are not

43
00:02:52,864 --> 00:02:58,234
overload because they are ineffective,
but because the system complexity

44
00:02:58,234 --> 00:03:05,044
exceeds the human cognitive limits, and
if we have to tie this together, high

45
00:03:05,044 --> 00:03:11,644
alert volume, complex systems, time
pressure results in poor outcomes, right?

46
00:03:12,244 --> 00:03:14,974
And the key problem still stays.

47
00:03:15,364 --> 00:03:17,764
In terms of, it's not lack of data.

48
00:03:18,034 --> 00:03:19,624
We all have enough data.

49
00:03:19,924 --> 00:03:22,924
It's too much data under time pressure.

50
00:03:23,344 --> 00:03:28,384
So that's why we are going to
talk about why LMS matter for SRE.

51
00:03:28,884 --> 00:03:31,374
So why LMS matter for SRE?

52
00:03:32,274 --> 00:03:36,334
So let's walk through what we see
on the left hand side of the slide.

53
00:03:36,834 --> 00:03:42,714
We have observability data, we have an LLM
processing, we have an actionable insight.

54
00:03:42,804 --> 00:03:46,044
And then there is SRE decision, right?

55
00:03:46,314 --> 00:03:48,534
And if you look in the right
hand side, we're talk, we're

56
00:03:48,534 --> 00:03:50,124
talking about pattern recognition.

57
00:03:51,054 --> 00:03:56,154
So we all know we connect various
different signals across logs, metrics,

58
00:03:56,154 --> 00:03:59,904
traces, no matter whether you come
from Open Elementary, whether you have

59
00:03:59,904 --> 00:04:04,564
Datadog implementation, whether you
have New Relic, Grafana, Prometheus.

60
00:04:05,079 --> 00:04:08,649
Any kind of logging system, metrics
system, or trace system, right?

61
00:04:08,649 --> 00:04:13,579
So we already identified that we have
enough tools in our ecosystem, and we have

62
00:04:13,579 --> 00:04:16,309
enough data which gets adjusted into this.

63
00:04:16,490 --> 00:04:19,070
So pattern recognition is more about LL.

64
00:04:19,070 --> 00:04:22,729
M would connect these weak signals
across your logs, across your

65
00:04:22,760 --> 00:04:26,689
metrics, across your traces,
and then we'll do the synthesis.

66
00:04:27,049 --> 00:04:27,770
Around it.

67
00:04:28,079 --> 00:04:31,709
And then census is mostly
talking about compressing massive

68
00:04:31,709 --> 00:04:36,759
telemetry into something human
can act on, synthesizing the data.

69
00:04:37,239 --> 00:04:42,759
And then we have reasoning, which
LM models are great at, right?

70
00:04:42,999 --> 00:04:48,669
Which is generating plausible hypothesis
instead of staring at dashboards.

71
00:04:49,209 --> 00:04:51,009
And when we talk about.

72
00:04:51,564 --> 00:04:57,489
Any SRE workflow or any incident response
workflow, anytime in a symptom comes or

73
00:04:57,489 --> 00:05:02,614
a production system is down, we come out
with these different hypothesis theories,

74
00:05:02,884 --> 00:05:06,574
and then based on the data, looking
at the dashboard, looking at different

75
00:05:06,574 --> 00:05:08,224
traces, we kind of start connecting.

76
00:05:08,224 --> 00:05:10,319
There is a huge amount of
data and we time filter it.

77
00:05:10,939 --> 00:05:15,454
So there's a lot of time lost around
that reasoning, around that hypothesis.

78
00:05:15,454 --> 00:05:20,494
And then we come to the conclusion
so then we can utilize lns.

79
00:05:21,259 --> 00:05:26,599
On those places where we lose majority
of the data, because when the production

80
00:05:26,599 --> 00:05:32,629
system goes down, time is what matters
the most to bring the service back.

81
00:05:32,989 --> 00:05:39,049
And the element here is elements
can assist in thinking, but

82
00:05:39,049 --> 00:05:40,344
they don't replace the judgment.

83
00:05:40,789 --> 00:05:45,469
We still, after the pattern recognition,
after synthesis and reasoning.

84
00:05:45,969 --> 00:05:49,779
Everything is presented in front
of you, and then we have the final

85
00:05:50,109 --> 00:05:53,829
judgment based on what has been
already pre-compiled for you.

86
00:05:53,829 --> 00:05:53,859
Okay.

87
00:05:54,359 --> 00:05:57,689
So the clear message here is
LM would reduce the cognitive

88
00:05:57,689 --> 00:06:01,109
load, not human responsibility.

89
00:06:01,619 --> 00:06:05,129
So that's what the LLM for SRE is.

90
00:06:05,189 --> 00:06:08,689
And then we're going to also talk about
how critical challenge and the production

91
00:06:08,689 --> 00:06:10,519
safety problem exists with this.

92
00:06:10,519 --> 00:06:11,630
And then how do we solve those?

93
00:06:12,604 --> 00:06:16,465
So the critical challenge, the
production safety problem, right?

94
00:06:17,124 --> 00:06:22,525
One would ask if elements are so useful,
why aren't they everywhere in production?

95
00:06:22,525 --> 00:06:24,205
SRE, right?

96
00:06:24,544 --> 00:06:27,215
There are definitely some risks
and we'll walk through those risks.

97
00:06:27,784 --> 00:06:30,215
One, health nation, right?

98
00:06:30,814 --> 00:06:33,574
Confident but wrong suggestions.

99
00:06:34,054 --> 00:06:34,684
We have.

100
00:06:35,284 --> 00:06:37,294
Privileged escalations, right?

101
00:06:37,564 --> 00:06:41,645
Tools, which means tools that
can mutate a production state.

102
00:06:42,145 --> 00:06:43,794
And we have a trust erosion, right?

103
00:06:44,155 --> 00:06:49,435
Once a engineer stop trusting the
system, adoption dies down, right?

104
00:06:50,104 --> 00:06:54,304
And think from these perspective,
these risks are operational.

105
00:06:54,304 --> 00:06:58,024
These are not theoretical risks.

106
00:06:58,354 --> 00:07:00,215
These are actual production.

107
00:07:01,039 --> 00:07:03,259
Operation risk, right?

108
00:07:03,769 --> 00:07:07,159
So if you think from a production
safety problem or this critical

109
00:07:07,159 --> 00:07:10,639
challenge perspective, the
biggest blocker is not accuracy.

110
00:07:10,639 --> 00:07:12,049
It's not the model accuracy.

111
00:07:12,049 --> 00:07:15,239
Hey, whether the model can
predict 99% of the pattern or

112
00:07:15,239 --> 00:07:21,239
synthesis, or it can reason, it's
the safety and the trust, right?

113
00:07:21,570 --> 00:07:27,270
So the LLM where they excel, why we would
say that they do not exist everywhere,

114
00:07:27,299 --> 00:07:29,039
which is basically safety and trust.

115
00:07:30,015 --> 00:07:32,775
Where accuracy is not the problem,
but the safety in the draft.

116
00:07:33,224 --> 00:07:37,175
Now, with that, we're going to also
talk about what would the production

117
00:07:37,175 --> 00:07:39,435
tested architecture would look like.

118
00:07:39,855 --> 00:07:42,375
So let's talk about the
production tested architecture.

119
00:07:42,585 --> 00:07:47,005
So if we had to explain the end
pipeline, this goes more like we'll

120
00:07:47,005 --> 00:07:52,915
do data ingestion, we will build the
context, we will do the l and m analysis.

121
00:07:53,455 --> 00:07:56,815
Human would review it, and then
decision logging can happen.

122
00:07:57,275 --> 00:08:01,325
And one point I would like to emphasize
here is there is a separation of concerns.

123
00:08:01,925 --> 00:08:06,185
So if you look at this pipeline, the
core theme is the AI would analyze.

124
00:08:06,685 --> 00:08:08,425
And proposes the solution.

125
00:08:08,635 --> 00:08:14,435
Human decides and act and key thing
to highlight would be nothing executes

126
00:08:14,735 --> 00:08:17,555
automatically in the production system.

127
00:08:17,675 --> 00:08:18,340
And if you have to.

128
00:08:18,840 --> 00:08:24,600
Correlate this to how it aligns with the
SRE principles, which talks about change

129
00:08:24,600 --> 00:08:27,960
control, auditability and postmortems.

130
00:08:28,590 --> 00:08:35,770
So the key takeaway from this slide is
AI suggestions are inputs, not commands.

131
00:08:36,270 --> 00:08:41,040
Now let's talk about the
foundation of multi-source data

132
00:08:41,040 --> 00:08:42,420
ingestion and why we need it.

133
00:08:42,600 --> 00:08:46,050
So if you look at a single
source of data, right?

134
00:08:46,530 --> 00:08:52,890
In incidents or think from any incident
response perspective, a single source,

135
00:08:53,250 --> 00:08:54,270
we don't look at single source.

136
00:08:54,500 --> 00:08:58,010
We look at logs, we look at traces, we
look at dashboards, we look at n number

137
00:08:58,010 --> 00:09:02,030
of things because we understand the
single source of data is insufficient.

138
00:09:02,405 --> 00:09:03,574
When it comes to incidents.

139
00:09:04,295 --> 00:09:09,004
And if we have to build a very
comprehensive context for an LLM

140
00:09:09,004 --> 00:09:12,665
to analyze, to give us patterns,
synthesize, and reason, we need to

141
00:09:12,665 --> 00:09:18,105
give the vast variety of data, which
means a different context, right?

142
00:09:18,675 --> 00:09:23,535
So traces for a request
flow and latency, right?

143
00:09:23,805 --> 00:09:28,545
Metrics for health
signals, logs for detailed.

144
00:09:29,045 --> 00:09:34,385
Even context, we have to also be to
correlate what kind of deployments

145
00:09:34,385 --> 00:09:36,185
were happening in our systems.

146
00:09:36,755 --> 00:09:42,905
We have our SLOs, SLAs, and error
budgets to frame the impact, right?

147
00:09:43,835 --> 00:09:50,615
And as, as it is as is, as it is very
well known, LLM is only as good as the

148
00:09:50,615 --> 00:09:55,745
context or the data it received, so
the key theme I would want you to take

149
00:09:55,745 --> 00:10:01,175
away from this is a better context in
the incident response, would beat a

150
00:10:01,175 --> 00:10:05,315
bigger model any single day because
with a bigger, with a better context.

151
00:10:05,815 --> 00:10:09,205
Your reasoning, your pattern
recognition, your synthesis, everything

152
00:10:09,205 --> 00:10:13,865
would get met in quality and quality
methods rather than the accuracy

153
00:10:14,055 --> 00:10:15,435
here or the bigger model here.

154
00:10:15,555 --> 00:10:19,195
So the multi-source data ation
is the core element when it

155
00:10:19,195 --> 00:10:21,620
comes to the better context.

156
00:10:22,550 --> 00:10:27,830
So now let's talk about LLM generated
insights and what insights can we expect.

157
00:10:28,624 --> 00:10:30,305
From lms, right?

158
00:10:30,634 --> 00:10:35,344
So at the code level there are
three outcome types, situational

159
00:10:35,344 --> 00:10:39,484
summaries, root cause hypothesis,
and suggested mitigation.

160
00:10:40,084 --> 00:10:41,795
And when we talk about
situational summaries.

161
00:10:42,680 --> 00:10:45,890
It's shared understanding
across responders.

162
00:10:46,220 --> 00:10:50,330
So whenever there is an incident, you
would understand there's an on-call team.

163
00:10:50,630 --> 00:10:53,600
There are multiple team members,
some are application teams, some are

164
00:10:53,600 --> 00:10:55,940
infrastructure teams or SRE teams.

165
00:10:55,940 --> 00:10:58,700
Some are from data side,
data engineering side.

166
00:10:58,700 --> 00:11:01,940
Some are AI teams and product
and so on and so forth.

167
00:11:02,300 --> 00:11:06,620
So everyone has an insight
and context of what the.

168
00:11:07,340 --> 00:11:10,340
What the root cause of the issue can be.

169
00:11:10,620 --> 00:11:14,310
And situation summaries would
basically share the understanding

170
00:11:14,310 --> 00:11:15,720
across different responders.

171
00:11:16,050 --> 00:11:20,490
So it, it, the, based on the situation,
those summaries can be generated.

172
00:11:21,330 --> 00:11:24,870
It can also come up with a root
cause hypothesis, which is a ranked.

173
00:11:24,920 --> 00:11:26,690
Explain explainable guesses.

174
00:11:27,305 --> 00:11:28,385
But not facts.

175
00:11:28,485 --> 00:11:32,085
And then we have suggested
mitigation, which is a runbook

176
00:11:32,085 --> 00:11:35,235
style, human reviewable step.

177
00:11:36,105 --> 00:11:40,755
If this happens next time, these are the
number of steps I would want you to take.

178
00:11:41,065 --> 00:11:45,055
And the core piece here, which we
need to understand is mitigations

179
00:11:45,055 --> 00:11:46,945
are never auto executed.

180
00:11:47,485 --> 00:11:50,005
The core idea is.

181
00:11:50,425 --> 00:11:54,925
To reduce the meantime to understanding,
which is M-T-T-U-I would like to call it

182
00:11:54,925 --> 00:12:01,675
as we have MT TR, which is for response,
but idea here is to reduce the time, which

183
00:12:01,675 --> 00:12:03,835
is meantime, to meantime to understanding.

184
00:12:04,285 --> 00:12:06,685
So the goal would be.

185
00:12:07,185 --> 00:12:11,745
To get faster clarity, not
autonomous radiation in this case.

186
00:12:11,745 --> 00:12:15,765
So these are the LLM generated insight,
which we can definitely tap into.

187
00:12:15,855 --> 00:12:20,425
So now let's talk about what is safety
first guard rail design principle, right?

188
00:12:20,965 --> 00:12:25,495
So there's a difference between an
experiment and a production system.

189
00:12:25,705 --> 00:12:28,240
So we just wanted to make sure
our production system are.

190
00:12:28,975 --> 00:12:31,725
Safe, secure and guarded in that way.

191
00:12:31,825 --> 00:12:34,045
So three core components, data hygiene.

192
00:12:34,345 --> 00:12:37,165
You have privilege, boundaries,
and verification gates, right?

193
00:12:37,615 --> 00:12:41,845
When it talks to, when we talk about
data hygiene, it's around, we have a

194
00:12:41,845 --> 00:12:44,725
clean, bounded and compliant input.

195
00:12:45,625 --> 00:12:50,575
When we talk about privileged boundaries,
it's least privileged by default.

196
00:12:50,905 --> 00:12:54,505
And when we talk about verification
gates, specifically saying,

197
00:12:54,505 --> 00:12:56,425
validate before you trust anything.

198
00:12:56,495 --> 00:12:57,515
And guardrails.

199
00:12:57,545 --> 00:13:01,355
If you think from a mental
mapping perspective.

200
00:13:01,800 --> 00:13:04,860
These are architecturals,
these are not prompt tricks.

201
00:13:04,860 --> 00:13:08,670
Or I will try to do another
prompt and then see what happens.

202
00:13:08,970 --> 00:13:10,890
So these are architectural,
not prompt tricks.

203
00:13:11,160 --> 00:13:15,660
So I think the core principle
is the safety must be designed

204
00:13:15,930 --> 00:13:18,300
in rather than the afterthought.

205
00:13:18,300 --> 00:13:21,750
So that's why the data hygiene, the
privilege ies, and verification becomes

206
00:13:21,970 --> 00:13:23,750
very important in, in this world.

207
00:13:24,560 --> 00:13:28,010
Now let's talk about data
hygiene and context control.

208
00:13:28,510 --> 00:13:32,810
So we all know, we all operate in
a in a world where we have a lot of

209
00:13:32,810 --> 00:13:34,925
VII data, some of those examples.

210
00:13:35,695 --> 00:13:41,215
Our email addresses, phone numbers,
authentication keys, customer identifier.

211
00:13:41,935 --> 00:13:47,785
So when it comes to data hygiene, we want
to make sure we have an automated PIA

212
00:13:47,785 --> 00:13:50,575
reduction before data reaches the model.

213
00:13:50,705 --> 00:13:55,065
And there are some of these listed on the
slide like I just mentioned, these are

214
00:13:55,065 --> 00:13:56,730
sensitive types, which you want to red.

215
00:13:57,230 --> 00:14:03,410
And when we talk about being a context
window to the model so that you don't give

216
00:14:03,410 --> 00:14:10,730
it extreme amount of very broader context,
but you give it a very narrow context like

217
00:14:10,940 --> 00:14:16,610
focus on last two arts because most of
the incidents happen in, and you're mostly

218
00:14:16,610 --> 00:14:19,850
proactive working on something which
happened in last one or two hours, right?

219
00:14:20,400 --> 00:14:20,970
Prioritize.

220
00:14:21,470 --> 00:14:23,900
Allow metrics and recent deploys, right?

221
00:14:23,900 --> 00:14:27,920
So something recently was deployed and
that triggered, which we have also seen

222
00:14:28,140 --> 00:14:30,420
in many of our organizations where.

223
00:14:31,230 --> 00:14:34,920
Recent deploy had triggered a code change
and that had triggered an incident.

224
00:14:34,920 --> 00:14:36,570
So look for those patterns, right?

225
00:14:36,630 --> 00:14:39,780
And then there can be many
more context you can set.

226
00:14:40,050 --> 00:14:43,710
But these are a few examples of
some key metrics, some recent last

227
00:14:43,860 --> 00:14:46,050
timeframes and some recent deploys.

228
00:14:46,090 --> 00:14:47,170
You can add, right?

229
00:14:47,590 --> 00:14:52,090
So if you look at the flow, the
raw data flows in a reduction and

230
00:14:52,090 --> 00:14:56,830
filtering happens, and then we
have a re refined context, then you

231
00:14:56,830 --> 00:14:58,210
have a way to analyze it, right?

232
00:14:58,550 --> 00:15:04,490
And the key message here is less
cleaner data produces better reasoning

233
00:15:04,490 --> 00:15:06,590
than dumping everything in, right?

234
00:15:06,650 --> 00:15:10,040
We can dump everything in,
but that would also means our

235
00:15:10,040 --> 00:15:12,470
results would not be as great.

236
00:15:12,770 --> 00:15:16,610
So as we make our data hygiene
and context control narrower.

237
00:15:17,110 --> 00:15:21,390
We will see better reasoning, better
results, and better actions and better

238
00:15:21,390 --> 00:15:23,400
decisions for the human to act on.

239
00:15:24,000 --> 00:15:27,570
So let's talk about the privileged
boundaries and tools integration.

240
00:15:28,170 --> 00:15:32,430
So we have couple of components
here, which is read only defaults

241
00:15:32,460 --> 00:15:34,170
allow list tools and audit trail.

242
00:15:35,130 --> 00:15:38,150
So when we say donate
default, it's basically.

243
00:15:39,095 --> 00:15:43,265
We always start with read only default
read only defaults, which means this

244
00:15:43,265 --> 00:15:46,985
prevents an accidental production impact
because if you have a reads, not an

245
00:15:46,985 --> 00:15:51,575
execute or elevated privileges, there is
no way the models or a limbs or agents can

246
00:15:51,905 --> 00:15:54,125
perform actions on the production side.

247
00:15:54,755 --> 00:15:56,405
So only approved tools.

248
00:15:56,405 --> 00:16:00,725
When we say allowed list tools, only
approved tools with explicit scope

249
00:16:01,055 --> 00:16:03,965
are added, and then every tool.

250
00:16:04,465 --> 00:16:09,985
Is logged and verifiable, which is
an audit trail, has to be added.

251
00:16:10,085 --> 00:16:14,135
And if you look from an architecture's
PERS perspective or auditability

252
00:16:14,135 --> 00:16:18,415
perspective, it all ties together
with compliance, with security review

253
00:16:18,415 --> 00:16:20,845
and incident retrospective, right?

254
00:16:21,455 --> 00:16:26,015
And the core key message is
if an action isn't auditable.

255
00:16:26,420 --> 00:16:28,340
It does not belong in, in production.

256
00:16:28,340 --> 00:16:31,010
So that's how we set our
privileged boundaries and tool

257
00:16:31,010 --> 00:16:32,270
integration in production.

258
00:16:32,770 --> 00:16:34,480
So let's talk about verification.

259
00:16:34,660 --> 00:16:38,350
How do we verify through
verification gates and validation?

260
00:16:39,220 --> 00:16:43,700
So there are three core components
shadow execution, counterfactual

261
00:16:43,700 --> 00:16:45,620
validation, and decision letter, right?

262
00:16:46,080 --> 00:16:50,640
Shadow execution would be test
any recommendation through LLMs.

263
00:16:51,140 --> 00:16:55,550
Isolation or in an isolated environment
where it can be executed independently

264
00:16:55,550 --> 00:16:56,870
without any production impact.

265
00:16:57,800 --> 00:17:00,290
We would also do a
counterfactual validation.

266
00:17:00,950 --> 00:17:04,670
Ask yourself a question, would
this have helped last time?

267
00:17:05,090 --> 00:17:07,430
Or if a similar problem
has happened before?

268
00:17:07,930 --> 00:17:12,670
Would the suggestion sound
in the direction of which we,

269
00:17:12,670 --> 00:17:14,020
what we did last time, right?

270
00:17:14,650 --> 00:17:18,610
So counterfactual or counter
questioning those validation.

271
00:17:18,880 --> 00:17:23,230
And then also make a decision ledger,
which is your immutable logs of

272
00:17:23,230 --> 00:17:28,990
AI suggesting and suggestions and
human responses enable audits and

273
00:17:28,990 --> 00:17:30,730
future model deployments, right?

274
00:17:30,790 --> 00:17:33,460
That would enable some of the
model improvements for us.

275
00:17:34,260 --> 00:17:38,670
Then also feed that to, into your learning
system and continuance improvement

276
00:17:38,670 --> 00:17:43,620
systems in the organization, which would
then make your model better and better.

277
00:17:43,740 --> 00:17:44,905
Like what was our runbook?

278
00:17:44,905 --> 00:17:46,040
What was a decision?

279
00:17:46,340 --> 00:17:51,290
So that way you're not only getting
help from LL but also giving it

280
00:17:51,290 --> 00:17:54,920
more data to continuously improve.

281
00:17:55,040 --> 00:17:59,840
The these verification gates and
validation would start building trust into

282
00:17:59,840 --> 00:18:05,420
these systems because the core component
of trust is evidence versus promises.

283
00:18:05,420 --> 00:18:09,170
So this will help us to
gather all of those evidences.

284
00:18:10,160 --> 00:18:18,910
Now our core idea here is how do we
shift from a reactive to proactive usage?

285
00:18:19,290 --> 00:18:24,510
A couple of patterns is first
line responders or first line

286
00:18:24,510 --> 00:18:30,400
response, cross service correlation
and e evidence assembly, right?

287
00:18:30,610 --> 00:18:36,490
So when we say fast line response,
it's most mostly around low

288
00:18:36,490 --> 00:18:38,915
threshold alerts, filtering noise.

289
00:18:39,685 --> 00:18:43,765
Escalating only when the genuine
issue to human responder.

290
00:18:43,765 --> 00:18:44,575
So escalate only.

291
00:18:44,785 --> 00:18:49,015
So core theme of first line response
would be noise reduction, right?

292
00:18:49,195 --> 00:18:53,225
Because we understood from the problem
statement itself the noise or the too many

293
00:18:53,225 --> 00:18:54,995
alert is basically overwhelming human.

294
00:18:54,995 --> 00:19:00,635
So make LLM as a first line responder
for the noise induction, right?

295
00:19:01,055 --> 00:19:03,430
We also start building cross
service correlation like.

296
00:19:03,930 --> 00:19:06,840
Early detection of
systematic issue, right?

297
00:19:07,080 --> 00:19:11,520
So if everyone service is going down
and then there's a dependent service,

298
00:19:11,790 --> 00:19:17,210
it means that dependent service
is at a risk of a systematic issue

299
00:19:17,480 --> 00:19:19,250
and early detection along those.

300
00:19:19,250 --> 00:19:22,495
So that would start trickling down to
your dependencies, your upstreams, your

301
00:19:22,495 --> 00:19:24,290
downstreams, and so on and so forth.

302
00:19:24,830 --> 00:19:28,340
And then also build an
evidence assembly, right?

303
00:19:28,430 --> 00:19:31,130
So pre-compiled evidence, which is ready.

304
00:19:31,505 --> 00:19:35,025
Before a formal incident
declaration, it would basically

305
00:19:35,085 --> 00:19:37,405
accelerate your incident response.

306
00:19:37,915 --> 00:19:41,375
It is to prebuilt your incident context.

307
00:19:41,655 --> 00:19:45,495
And core focus to test all
of these proactive deployment

308
00:19:45,495 --> 00:19:49,815
patterns would be to target very
low risk, high return use cases.

309
00:19:49,895 --> 00:19:54,335
And the win of these proactive
deployment patterns would be.

310
00:19:54,835 --> 00:20:00,615
If we can know an incident before
an incident is declared, that's

311
00:20:00,615 --> 00:20:02,925
the safest win which can come.

312
00:20:03,405 --> 00:20:06,135
So if the, if we can get to the.

313
00:20:06,635 --> 00:20:10,535
Point where now we know that
incidence is about to happen.

314
00:20:11,255 --> 00:20:14,030
That's the goal of a proactive
deployment pattern for us.

315
00:20:14,035 --> 00:20:19,025
So that would be a very proactive and
very u amazing utilization of LLM in the

316
00:20:19,025 --> 00:20:20,825
context of proactive deployment pattern.

317
00:20:21,425 --> 00:20:26,610
So now I wanted to I wanted to leave
you with practical pilot blueprint.

318
00:20:27,360 --> 00:20:28,950
How do we implement this right?

319
00:20:29,760 --> 00:20:34,680
As we all know in, in a software or
infrastructure system, always start small,

320
00:20:35,100 --> 00:20:40,470
but you don't use cases, noncritical
services, which can be tested without

321
00:20:40,770 --> 00:20:43,020
high impact on the production systems.

322
00:20:43,800 --> 00:20:45,570
That would be your very first step, right?

323
00:20:46,230 --> 00:20:50,080
Then you start measuring
trust, not just accuracy.

324
00:20:50,160 --> 00:20:53,820
So do not just go by understanding
of, hey, there was a 90%

325
00:20:53,820 --> 00:20:55,890
accuracy or 95% accuracy.

326
00:20:56,475 --> 00:21:02,175
But start measuring trust in terms of
would I start trusting the system to make

327
00:21:02,175 --> 00:21:04,155
these decision in an auto autonomous way?

328
00:21:04,255 --> 00:21:08,575
And then also you have an, you
should have an iterative guardrail

329
00:21:08,575 --> 00:21:10,405
tuning based on the real behavior.

330
00:21:10,405 --> 00:21:16,385
So there has to be guardrail protection
based on how your system is responding.

331
00:21:16,775 --> 00:21:20,435
So that not only your suggestions
are getting improvement, but your

332
00:21:20,435 --> 00:21:24,335
guardrail around protecting your
system is also getting along with that.

333
00:21:24,695 --> 00:21:31,285
And then you your kind of gradual
expansion would happen and scale

334
00:21:31,285 --> 00:21:34,735
would come for these systems
as the confidence grows, right?

335
00:21:35,155 --> 00:21:39,160
So as in the SRE world or in
the reliability world says.

336
00:21:39,660 --> 00:21:43,950
Your production AI maturity would
grow the same way that reality re

337
00:21:43,950 --> 00:21:46,590
reliability grows incrementally.

338
00:21:46,650 --> 00:21:50,700
So that's the core way of hybrid
would start implementing start small,

339
00:21:51,300 --> 00:21:53,790
measure the trust, not accuracy.

340
00:21:54,450 --> 00:21:57,360
It rate over the guardrail so
that your guardrail can scale with

341
00:21:57,360 --> 00:22:01,900
your system and expand your scope
to scale even to a bigger scale.

342
00:22:01,900 --> 00:22:02,170
Also.

343
00:22:03,160 --> 00:22:08,270
So as we start closing I would wanted
to leave you with some key takeaways,

344
00:22:08,330 --> 00:22:18,420
which are four pillars of our talk today,
which is AI augments human cognition.

345
00:22:18,920 --> 00:22:26,420
Guardrails would define the core
success of your system, and then trust

346
00:22:26,450 --> 00:22:28,700
is always built through transparency.

347
00:22:29,570 --> 00:22:35,210
And or another way of saying transfer
transparency can build trust and

348
00:22:35,570 --> 00:22:39,860
proactive use delivers early value.

349
00:22:39,920 --> 00:22:40,610
Early value.

350
00:22:40,670 --> 00:22:45,560
So as we start building these systems,
as we talked about the proactive

351
00:22:45,560 --> 00:22:49,550
use of building a system, which can.

352
00:22:50,000 --> 00:22:53,420
Identify an incident even
before an incident is declared,

353
00:22:53,510 --> 00:22:55,550
is a huge success for you.

354
00:22:55,650 --> 00:23:01,780
So this is a the core problem
is this is a problem as much

355
00:23:01,870 --> 00:23:04,090
as it is an ML problem, right?

356
00:23:04,360 --> 00:23:08,290
So we all know a reliable AI
systems follow the same principle

357
00:23:08,410 --> 00:23:10,030
as a reliable infrastructure.

358
00:23:10,390 --> 00:23:16,470
So these four pillars would define the
key takeaways of my talk, which was.

359
00:23:16,970 --> 00:23:20,570
Make sure your AI augments
human, you put your guardrails

360
00:23:20,570 --> 00:23:23,060
in, you have a transparency.

361
00:23:23,090 --> 00:23:26,405
And then transparency would
build that trust or you can call

362
00:23:26,405 --> 00:23:27,575
it trust through transparency.

363
00:23:27,575 --> 00:23:32,225
And then proactive uses use, delivers
the early value of an incident as well.

364
00:23:32,725 --> 00:23:36,445
So I wanted to thank
everyone for listening in.

365
00:23:36,585 --> 00:23:39,195
And I'm more than happy.

366
00:23:40,065 --> 00:23:44,085
In my discussions around real world
constraints and failures, right?

367
00:23:44,415 --> 00:23:49,935
I would love if you have any questions
on implementation in your organization

368
00:23:49,965 --> 00:23:54,475
or across across our field whether
you're in from whichever industry.

369
00:23:54,965 --> 00:23:57,995
Not just the theory, but the
implementation detail around this.

370
00:23:58,235 --> 00:23:59,915
Feel free to reach out to you.

371
00:24:00,005 --> 00:24:04,505
I would love to help on these
implementations because I was looking

372
00:24:04,505 --> 00:24:07,345
at some of the latest products
which are coming up including.

373
00:24:08,220 --> 00:24:11,550
Datadog Pops and other, which are
some of the wrappers around it.

374
00:24:11,600 --> 00:24:14,860
I think there are huge
tooling, which is available.

375
00:24:14,890 --> 00:24:18,000
There are multiple agents you
can build through Lang Chain

376
00:24:18,000 --> 00:24:20,310
and other technology as well.

377
00:24:20,360 --> 00:24:24,440
We can help us to build these systems
agents along this way as well.

378
00:24:24,650 --> 00:24:28,340
And I have built out, I shared
my LinkedIn linked here as well.

379
00:24:28,940 --> 00:24:34,520
Feel free to reach out to me for
any follow up conversation, and as I

380
00:24:34,520 --> 00:24:37,490
said, this is just a starting point.

381
00:24:37,820 --> 00:24:40,640
The real work happens in production.

382
00:24:40,730 --> 00:24:43,520
So that's the key pieces,
which I'm looking forward to.

383
00:24:43,910 --> 00:24:48,390
With that, I would love to
congratulate everyone in the Con

384
00:24:48,390 --> 00:24:53,780
42 machine learning conference for
2026 for attending for the talk.

385
00:24:54,080 --> 00:24:59,040
And I would love to have more discussions
with the other panelists around

386
00:24:59,110 --> 00:25:01,480
some of the topics and some of the
exciting talks which are happening.

387
00:25:01,480 --> 00:25:02,320
So I look forward for.

388
00:25:02,840 --> 00:25:06,010
Many more collaborations with
everyone and the panelist and

389
00:25:06,010 --> 00:25:07,030
from the audience as well.

390
00:25:07,390 --> 00:25:08,710
With that, thank you.

391
00:25:08,710 --> 00:25:13,600
That was my talk in Con 42
Machine Learning Conference 2026.

392
00:25:14,110 --> 00:25:15,550
Have a wonderful rest of the day.

393
00:25:15,910 --> 00:25:16,150
Bye

394
00:25:16,345 --> 00:25:16,565
bye.

