1
00:00:00,500 --> 00:00:01,069
Hi.

2
00:00:02,029 --> 00:00:05,270
Welcome to the session of OB Observ 2.0.

3
00:00:05,690 --> 00:00:09,770
Much more than just logs,
mattress and crisis.

4
00:00:10,270 --> 00:00:13,959
Hi, this is Neil Sha,
a DevOps community guy.

5
00:00:14,410 --> 00:00:19,300
I work as a Toyota advocate and
middleware, and I also organize

6
00:00:19,750 --> 00:00:23,145
a lot of communities around
DevOps in my local region like.

7
00:00:23,645 --> 00:00:27,544
Google Cloud, CNCF, HashiCorp,
another, more than Encrypt plus

8
00:00:27,544 --> 00:00:32,375
hackathons and have given talks
in 10 plus conferences, including

9
00:00:32,375 --> 00:00:36,244
platform con, KCDQ, con Linux phase.

10
00:00:36,745 --> 00:00:38,635
So coming to our student's
agenda is evolution.

11
00:00:39,135 --> 00:00:42,495
Open tele role, reducing
downtime with order collector.

12
00:00:42,995 --> 00:00:48,214
ML in already optimizing already
cost AI driven future and takeaways.

13
00:00:48,714 --> 00:00:53,814
So seeing the ation observ already
around 2015 people were also

14
00:00:53,814 --> 00:00:55,875
using the monolith applications.

15
00:00:56,375 --> 00:01:02,265
So they were using tools like
material and Grafana, and then it

16
00:01:02,805 --> 00:01:05,505
came the era of tele explosion.

17
00:01:05,805 --> 00:01:13,005
A lot of data coming out from
application infra and things, and there

18
00:01:13,155 --> 00:01:15,585
are a lot of chaos, leaks of data.

19
00:01:16,085 --> 00:01:22,495
So it created a scenario where a
lot of data is there, but it becomes

20
00:01:22,495 --> 00:01:24,685
very challenging to tackle that.

21
00:01:25,185 --> 00:01:27,151
Annie, around 20 30, 20 23.

22
00:01:27,651 --> 00:01:35,360
The era of AI came and people are using
AI to improve their observing spectrum.

23
00:01:35,860 --> 00:01:38,320
So from firefighting to force it.

24
00:01:38,820 --> 00:01:43,981
Imagine a scenario where it's 2:00 AM
you and your application production

25
00:01:43,981 --> 00:01:49,110
is down and there are tons of data
floating out, and you don't know

26
00:01:49,200 --> 00:01:52,500
actually what you need to dig in and.

27
00:01:53,000 --> 00:01:56,890
It takes a lot of manual effort
for that to understand the problem

28
00:01:57,390 --> 00:01:59,100
and meanwhile there is downtime.

29
00:01:59,970 --> 00:02:02,070
It also affects the business cable.

30
00:02:02,570 --> 00:02:08,290
Now we in a better world, we also
already system NetX software is early.

31
00:02:09,070 --> 00:02:12,850
You have AI flag the growing
risk before any outrageous.

32
00:02:13,555 --> 00:02:16,945
You act proactively avoiding
the incident altogether.

33
00:02:17,445 --> 00:02:22,200
Firefighting to force availability
0.0 from, we have shifted from

34
00:02:22,320 --> 00:02:25,295
DIQ to predictive just like this.

35
00:02:26,105 --> 00:02:27,545
We don't want manual troubleshooting.

36
00:02:27,695 --> 00:02:33,065
We want auto explain incidents so that
we don't need to hurry at the last

37
00:02:33,065 --> 00:02:34,715
time and do a lot of things manually.

38
00:02:35,215 --> 00:02:38,275
Next comes the enhanced capabilities.

39
00:02:38,275 --> 00:02:43,905
There are correlation contextualization,
so there is automatic data

40
00:02:43,905 --> 00:02:45,435
correlation from front end to back.

41
00:02:45,945 --> 00:02:53,235
There is AI ion, there is extra root cause
because coming with a lot of automation

42
00:02:53,265 --> 00:02:56,775
of ai, we have required a lot of kids.

43
00:02:57,465 --> 00:03:01,470
That helps us to understand
and have a better root cause.

44
00:03:01,970 --> 00:03:07,365
In today's journey and 2.0 we have, or
we'll covering the evaluation already.

45
00:03:07,965 --> 00:03:08,605
Actually we covered.

46
00:03:09,335 --> 00:03:13,525
We'll cover the rise of open
Telemetry ml, LLM, already

47
00:03:13,825 --> 00:03:16,525
reason downtime with collector.

48
00:03:16,720 --> 00:03:18,580
We understand how the
order connector works.

49
00:03:19,000 --> 00:03:23,980
We'll also see the cutting costs
with smarter ingestion and that

50
00:03:23,980 --> 00:03:26,230
will be AI driven future already.

51
00:03:26,730 --> 00:03:28,355
So here's the evaluation.

52
00:03:28,585 --> 00:03:29,235
Also already.

53
00:03:29,735 --> 00:03:32,705
So there are a lot of sea load
data sources, sea load being

54
00:03:33,025 --> 00:03:34,015
there are a bunch of data.

55
00:03:34,435 --> 00:03:38,605
We don't know actually what to do
that with that we are performing

56
00:03:38,605 --> 00:03:40,105
annual root cross analysis.

57
00:03:40,105 --> 00:03:43,345
A lot of time we are growing
telemetry volume leading to

58
00:03:43,345 --> 00:03:44,785
higher cost and complexities.

59
00:03:45,285 --> 00:03:46,395
Then come the evolution.

60
00:03:46,895 --> 00:03:49,295
Unified Telemetry with Open Telemetry.

61
00:03:49,795 --> 00:03:55,315
It comes with proactive anoma
reduction with insights and

62
00:03:55,375 --> 00:03:57,295
also optimize data pipelines.

63
00:03:57,795 --> 00:04:00,040
Then it came the rise of Open Telemetry.

64
00:04:00,540 --> 00:04:02,625
Open tele before Open Telemetry.

65
00:04:03,165 --> 00:04:08,265
The thing was people were using
and collecting a lot of data,

66
00:04:08,805 --> 00:04:10,335
but it was in different format.

67
00:04:10,905 --> 00:04:16,785
So if a person A wants to share their
data with person B, it was not compatible

68
00:04:17,285 --> 00:04:18,725
and it was creating a lot of chaos.

69
00:04:19,225 --> 00:04:26,425
Then the Rise Open came, it created
a vendor over those framework

70
00:04:26,425 --> 00:04:28,435
for collecting telemetry data.

71
00:04:28,935 --> 00:04:30,705
Next is Rise Open.

72
00:04:31,205 --> 00:04:36,260
Open Telemetry is one of the biggest
project of ENCF after Kubernetes.

73
00:04:36,720 --> 00:04:41,945
So if you go and search on CNCF, a
lot of people are contributing to Open

74
00:04:41,945 --> 00:04:46,835
Telemetry and a lot of people are using
open Telemetry in their application back

75
00:04:46,835 --> 00:04:48,925
ends backs for observative purposes.

76
00:04:49,425 --> 00:04:50,595
Why open telemetry?

77
00:04:51,095 --> 00:04:52,955
Because it created a unified standardized.

78
00:04:53,455 --> 00:04:55,350
Way for instrumentation.

79
00:04:56,190 --> 00:04:59,340
It created a easier correlation
between different signals.

80
00:04:59,790 --> 00:05:03,900
It created a freedom to route data to
multiple backends and automated platforms.

81
00:05:04,400 --> 00:05:08,840
Next was extensible
processor and exporters.

82
00:05:09,340 --> 00:05:14,500
What it created an impact on is introduced
a lot of engineering from monitoring.

83
00:05:15,000 --> 00:05:17,910
It enable a smarter pipeline
for data processing.

84
00:05:18,410 --> 00:05:24,730
IT form the foundation of AI driven
platforms or systems or collector.

85
00:05:25,180 --> 00:05:29,400
So collector is the core function or
collective core component of open.

86
00:05:29,900 --> 00:05:31,820
It consists of three major things.

87
00:05:31,820 --> 00:05:35,990
That is receiver, processor,
exporter, receivers, patches,

88
00:05:35,990 --> 00:05:37,370
the data from your application.

89
00:05:37,700 --> 00:05:42,170
Processors transform the data in
which manner you need to feed to expo.

90
00:05:42,170 --> 00:05:43,400
You need to feed to some SaaS.

91
00:05:43,400 --> 00:05:45,650
Stacking an exporter.

92
00:05:45,680 --> 00:05:50,170
Does that expose the data to
some of the SaaS platform?

93
00:05:50,200 --> 00:05:56,710
Like middleware is a full stack platform
and if you want to understand more on

94
00:05:56,710 --> 00:06:01,990
hotel collector, you can scan this code,
you can, there will be a blog on that and

95
00:06:01,990 --> 00:06:04,210
it's detailed blog on hotel collector.

96
00:06:04,710 --> 00:06:10,415
Next ml and there are a lot of
anomalies that are detected by ML model.

97
00:06:11,285 --> 00:06:15,635
They also detect a lot of in
incident patterns when before

98
00:06:16,135 --> 00:06:17,795
we were not able to do that.

99
00:06:18,295 --> 00:06:22,405
And it also prioritize in an incident
based on severity and impact production.

100
00:06:22,905 --> 00:06:26,870
LLM also have a summarized incidents
in human readable language.

101
00:06:27,370 --> 00:06:31,390
Also predict the potential future
issue based on the historical patterns.

102
00:06:32,140 --> 00:06:37,725
If you see LLM was already then, some of
the major things are monitoring mattresses

103
00:06:37,865 --> 00:06:42,345
or performance and also improving
that actually issue identification.

104
00:06:42,845 --> 00:06:47,635
And also health and reliability
enhancement MLS are ransom,

105
00:06:48,235 --> 00:06:52,855
the from passive monitoring to
intelligent problem solving.

106
00:06:53,355 --> 00:06:55,395
So what are the key
data types in other 2.0?

107
00:06:55,895 --> 00:06:56,945
What are the events?

108
00:06:57,426 --> 00:06:57,635
Then?

109
00:06:57,635 --> 00:07:00,875
Its profiles, dependencies and EVPF.

110
00:07:01,375 --> 00:07:04,195
The EVPF changed a lot of things because.

111
00:07:04,600 --> 00:07:09,520
It can fetch the data from the ker
level, all the request data, and it

112
00:07:09,520 --> 00:07:15,645
can also figure out a lot of things,
which posts are open on, which

113
00:07:15,955 --> 00:07:20,835
packets are transforming which roots,
all the things are triggered by it,

114
00:07:21,015 --> 00:07:25,085
and all the things can be tracked
from the, is a level of security.

115
00:07:25,585 --> 00:07:30,241
So comparing the features already
2.0 with the traditional already.

116
00:07:31,201 --> 00:07:35,551
So I just wanted to have overview
for you to better understand

117
00:07:35,551 --> 00:07:37,260
how the different things works.

118
00:07:37,831 --> 00:07:40,501
And the first thing is log collection.

119
00:07:40,501 --> 00:07:42,121
I will just go on one of two.

120
00:07:42,121 --> 00:07:42,811
Example.

121
00:07:43,231 --> 00:07:47,371
So log collection was manual,
it was fragmented by 2.0.

122
00:07:47,371 --> 00:07:50,400
It created automated pipelines
and it was centralized.

123
00:07:51,135 --> 00:07:54,376
And similar manner, the feature,
different features have been

124
00:07:54,376 --> 00:07:57,845
changed, so already cost explosion.

125
00:07:58,295 --> 00:08:02,105
So there are a lot of data, massive
telemetry growth due to a lot of

126
00:08:02,105 --> 00:08:09,025
microservices, which needs a use
storage, and it also costs a lot storing

127
00:08:09,025 --> 00:08:13,675
everything, every log, but mattress
traces even when much of it is irrelevant.

128
00:08:14,175 --> 00:08:18,405
Traditional already models are not
designed for cloud native scalability.

129
00:08:19,395 --> 00:08:24,845
Key cost drivers are high, currently
trics logs and over assembling of crisis.

130
00:08:25,345 --> 00:08:30,945
It resulted in increase infrastructure
spend, slow, a query analysis,

131
00:08:30,945 --> 00:08:35,350
performance and noise, unnecessary
noise, unmanageable noise.

132
00:08:35,850 --> 00:08:37,710
Then comes smarter data pipelines.

133
00:08:38,250 --> 00:08:40,920
So they have to reduce the noise.

134
00:08:41,730 --> 00:08:46,170
In middleware, we have a feature
called ingestion control pipeline that

135
00:08:46,170 --> 00:08:50,910
helps to exclude the unnecessary logs
from being stored to storage, like

136
00:08:50,910 --> 00:08:53,010
S3 bucket or some external storage.

137
00:08:53,510 --> 00:08:54,800
So what are the key strategies?

138
00:08:54,980 --> 00:08:59,360
Some of the key strategies are dynamic
sampling, filtering, and deduplication.

139
00:08:59,990 --> 00:09:02,330
Aggregation edge processing.

140
00:09:02,630 --> 00:09:06,140
So these are some of the techniques
and strategies that help to reduce the

141
00:09:06,590 --> 00:09:09,200
noise and also helps reduce the cost.

142
00:09:09,700 --> 00:09:12,600
These are the benefits that I
already covered and what are

143
00:09:12,600 --> 00:09:15,120
the AI and the future already.

144
00:09:15,620 --> 00:09:19,720
So one of the main major thing
that I love over here is to

145
00:09:19,720 --> 00:09:21,580
consider a scenario where you have.

146
00:09:22,250 --> 00:09:25,940
Alert that your CPU is increased.

147
00:09:26,170 --> 00:09:31,000
CPU utilization is more than 80%,
and the thing is you need to increase

148
00:09:31,000 --> 00:09:33,490
the storage or double down the CPU.

149
00:09:34,300 --> 00:09:40,440
So the alert will be, you have
80% plus CP utilization, and if

150
00:09:40,440 --> 00:09:45,500
you want to double down the CPU,
you just press you just click yes.

151
00:09:45,680 --> 00:09:46,700
The CP should double down.

152
00:09:47,200 --> 00:09:50,950
Double down, so it helps a lot of things.

153
00:09:51,850 --> 00:09:53,350
Next is AI assist root cause.

154
00:09:53,830 --> 00:09:59,040
AI can also help a lot of things to
understand better and to do a thing.

155
00:09:59,040 --> 00:09:59,526
A lot of automated things.

156
00:10:00,026 --> 00:10:04,331
And it also nowadays, people are
also using LLM already to improve the

157
00:10:04,331 --> 00:10:06,821
performance of LLMs that they are using.

158
00:10:07,321 --> 00:10:09,091
This is a future, A already trends.

159
00:10:09,091 --> 00:10:13,801
In 2025, we have covered
almost major of the themes, AI

160
00:10:13,801 --> 00:10:18,381
integration, open telemetry, ai,
workload, automations and all.

161
00:10:18,881 --> 00:10:20,516
So what are the outcomes from ai?

162
00:10:20,516 --> 00:10:25,391
And future already is faster recovery
from incidents, greater reliability

163
00:10:25,391 --> 00:10:28,591
and engineer focus on innovation
instead of manual monitoring.

164
00:10:29,091 --> 00:10:34,041
If you want a detailed log on
2.0, you can scan this code.

165
00:10:34,041 --> 00:10:38,781
There is a detailed blog
covering all the spec on of 2.0.

166
00:10:39,281 --> 00:10:43,481
You can connect with me on LinkedIn or
any platform if you have some queries.

167
00:10:43,871 --> 00:10:45,671
Thank you for listening to me.

168
00:10:46,031 --> 00:10:47,861
Hope you have a great day here.

