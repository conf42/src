1
00:00:01,000 --> 00:00:03,070
Hello everyone, and
thank you for joining me.

2
00:00:03,579 --> 00:00:09,610
My name is Loh Kumar Ma and I'm a senior
Azure data engineer with over 15 years

3
00:00:09,610 --> 00:00:13,990
of experience in designing advanced
data solutions for large enterprises.

4
00:00:14,530 --> 00:00:17,440
Today, I will walk you through
a transformative architecture

5
00:00:17,440 --> 00:00:19,000
I design that combines.

6
00:00:19,610 --> 00:00:24,890
Data factory with the predictor, modeling
and real time analytics to manage risk

7
00:00:25,160 --> 00:00:29,180
more intelligently, especially during
times of financial and market and ity.

8
00:00:30,410 --> 00:00:34,970
This framework not only process, reduce
this processing time and cost, but

9
00:00:34,970 --> 00:00:39,470
also improve pipeline reliability and
give leadership teams the ability to

10
00:00:40,010 --> 00:00:42,770
act quickly on risk signals with data.

11
00:00:43,770 --> 00:00:45,300
Let's begin with the problem.

12
00:00:45,390 --> 00:00:48,330
So today's data challenge, let's begin.

13
00:00:48,760 --> 00:00:53,260
Modern organizations are joining in
data spread, so we are talking about

14
00:00:53,260 --> 00:00:57,905
the petabytes of data structure,
unstructured, scattered across cloud,

15
00:00:57,905 --> 00:01:02,740
enter around, and legacy systems, and
third party APAs and internal database.

16
00:01:03,850 --> 00:01:04,810
This creates.

17
00:01:05,695 --> 00:01:10,865
Challenges, and there are three major
challenges explosive data growth.

18
00:01:11,345 --> 00:01:18,905
So constant increase in data from web,
iot, traditional transactional systems and

19
00:01:18,905 --> 00:01:21,845
third parties and integration complexity.

20
00:01:22,205 --> 00:01:24,515
So data comes from the many formats.

21
00:01:25,115 --> 00:01:26,585
At a different speeds.

22
00:01:27,035 --> 00:01:29,555
And with the varying
schema and business rules.

23
00:01:30,005 --> 00:01:30,995
So maintenance burdens.

24
00:01:31,205 --> 00:01:36,065
So traditional ETL frameworks
are code heavy, error prone,

25
00:01:36,215 --> 00:01:38,255
and time consuming to maintain.

26
00:01:38,855 --> 00:01:43,355
So when risk is dynamic and data
is in timely, and your business

27
00:01:43,505 --> 00:01:45,515
decisions are all already outdated,

28
00:01:46,515 --> 00:01:50,505
So the solution is Azure Data Factory.

29
00:01:50,655 --> 00:01:51,645
It's a game changer.

30
00:01:52,395 --> 00:01:56,655
So we needed something that could scale
with the data or respond to change

31
00:01:56,655 --> 00:01:58,964
quickly and reduce manual effort.

32
00:01:59,744 --> 00:02:01,964
So that's where Data Factory came in.

33
00:02:02,115 --> 00:02:08,774
So my solution leverage A DF is low-code
pipeline design with visual interfaces

34
00:02:08,865 --> 00:02:14,685
and pre-built connectors to a hundred
plus data sources and reusable components

35
00:02:14,925 --> 00:02:16,695
for consistency across projects.

36
00:02:17,560 --> 00:02:20,380
Template driven workflows
to register them to market.

37
00:02:21,159 --> 00:02:25,000
So we developed limited data
driven architecture where business

38
00:02:25,000 --> 00:02:28,899
logic could be modified without
changing a single line of code.

39
00:02:29,440 --> 00:02:34,180
This meant faster iterations, better
testing, and Jira deployment overheads.

40
00:02:35,289 --> 00:02:36,310
Let's move on to the next.

41
00:02:37,310 --> 00:02:39,290
And measurable business impacts.

42
00:02:39,359 --> 00:02:42,420
Let's talk about the numbers
because measurable outcomes.

43
00:02:43,420 --> 00:02:48,579
By creating Azure Data Factory
pipelines and we reduced operational

44
00:02:48,579 --> 00:02:53,619
cross dropped 40% thanks to dynamic
scaling and optimized scheduling

45
00:02:54,160 --> 00:02:58,750
pipeline success rates improved
in 99.9%, virtually eliminating

46
00:02:58,750 --> 00:03:00,730
manually restarts and broken chains.

47
00:03:01,730 --> 00:03:03,710
And these aren't just stats.

48
00:03:03,769 --> 00:03:10,250
They really reflect agility, resilience,
and especially when making high stake.

49
00:03:11,250 --> 00:03:14,130
So let's talk about the
architecture best practices.

50
00:03:14,160 --> 00:03:17,850
That's, we followed the backbone
of this success was carefully

51
00:03:17,850 --> 00:03:22,810
architecture framework, built on
four best practices, so model design.

52
00:03:23,200 --> 00:03:27,049
So breaking pipeline into
functional units like ingestions

53
00:03:27,949 --> 00:03:32,059
and clinging transformations
and loading pipeline template.

54
00:03:32,719 --> 00:03:38,029
Templates using parameterize and templates
to quickly roll out new use cases.

55
00:03:38,899 --> 00:03:42,170
Metadata driven executions
to control everything.

56
00:03:42,410 --> 00:03:48,769
Source file types, schema mapping
from a tables incremental loading, so

57
00:03:48,920 --> 00:03:51,739
load only delta, and registering both.

58
00:03:52,660 --> 00:03:54,760
Compute this usage and pipeline runtime.

59
00:03:55,179 --> 00:04:01,030
So this allowed us to scale seamlessly,
reuse proven logic, and adapt rapidly

60
00:04:01,030 --> 00:04:03,640
to new data sources or business roles.

61
00:04:04,640 --> 00:04:10,290
And the governance and line monitoring
that we followed a robust data system

62
00:04:10,350 --> 00:04:13,049
is incompatible without governance.

63
00:04:13,230 --> 00:04:16,350
Here is what we implemented track.

64
00:04:17,380 --> 00:04:17,860
Thing.

65
00:04:17,890 --> 00:04:20,169
Every transformations from how to refine.

66
00:04:20,260 --> 00:04:26,980
Along with the timestamped logs, audit
frameworks ev Every activity was logged

67
00:04:27,189 --> 00:04:31,990
researchable, searchable and tied
to user credentials and operational

68
00:04:31,990 --> 00:04:36,610
dashboards built using Power BI and Log
Analytics to monitor pipeline health

69
00:04:37,610 --> 00:04:40,970
alerting systems, alerts for performance.

70
00:04:41,855 --> 00:04:43,895
Threshold to notifications.

71
00:04:45,755 --> 00:04:49,595
This not only supported complaints
but us proactively detect

72
00:04:52,145 --> 00:04:52,655
before.

73
00:04:53,655 --> 00:04:57,150
Once the pap was stable and we focused on.

74
00:04:58,750 --> 00:05:04,150
So using Power BI and Azure Analysis
Services, we enable self-service analysis

75
00:05:04,150 --> 00:05:10,210
for business teams, reduce report
generation time by up to 70% and connected

76
00:05:10,210 --> 00:05:14,650
directly to streaming sources for real
time dashboards for financial services.

77
00:05:14,890 --> 00:05:19,540
This meant instant visibility into
key metrics like exposure, risk,

78
00:05:19,930 --> 00:05:22,780
operational losses, and market trends.

79
00:05:23,780 --> 00:05:32,360
Let's compliance and security,
so financial end compliance,

80
00:05:33,260 --> 00:05:36,950
encryption and transit using Azure.

81
00:05:38,070 --> 00:05:41,070
So we were stored all the
keys into the Azure Q keyword.

82
00:05:41,550 --> 00:05:45,000
So role robust access controls
to enforce least privileges.

83
00:05:45,120 --> 00:05:48,420
So audit trail generations
and version controlled and

84
00:05:48,420 --> 00:05:49,950
configuration for every change.

85
00:05:50,490 --> 00:05:54,420
Thatm masking right to be
forgotten and constant tracking

86
00:05:54,420 --> 00:05:56,730
for privacy laws like GDPR.

87
00:05:57,180 --> 00:06:00,840
So this control ensure we passed
multiple compliance audits with.

88
00:06:01,840 --> 00:06:06,400
So the solution I built is not just a
pipeline, it's a strategic platform.

89
00:06:06,550 --> 00:06:10,510
We have already started enterprise
rollouts across departments.

90
00:06:10,870 --> 00:06:14,980
We're also training internal teams
to manage the platform, build

91
00:06:14,980 --> 00:06:19,120
new use cases, and improve data
maturity levels across the board.

92
00:06:19,405 --> 00:06:25,255
So looking ahead, I see this evolving
into see self-learning, AI-driven risk

93
00:06:25,255 --> 00:06:30,625
intelligence engine with auto-tuned
pipelines ready to failure alerts and

94
00:06:30,625 --> 00:06:35,785
embedded models that forecast financial
risks based on real data patterns.

95
00:06:36,325 --> 00:06:40,675
This is the future of data-driven risk
management, and we are ready on the path.

96
00:06:41,335 --> 00:06:42,625
We are already on the path.

97
00:06:43,625 --> 00:06:47,535
So to wrap up I invite you assess
your current data architecture.

98
00:06:48,405 --> 00:06:51,945
Identify areas where intelligent
automation could reduce cost and

99
00:06:51,945 --> 00:06:57,915
complexity, and consider how scalable
frameworks like the one I have presented,

100
00:06:57,915 --> 00:07:01,155
can help your organization make
faster, safer, and smarter relations.

101
00:07:01,920 --> 00:07:03,060
Thank you for attending.

102
00:07:03,120 --> 00:07:06,570
I'm excited to share this
journey with the Conference 42

103
00:07:06,570 --> 00:07:10,290
Community and I, and with thank.

