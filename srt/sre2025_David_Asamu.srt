1
00:00:00,985 --> 00:00:02,190
Hi, I am David.

2
00:00:02,250 --> 00:00:06,120
Welcome to this session, AI
in SRE, unlocking permits

3
00:00:06,390 --> 00:00:08,160
Insights with Natural Language.

4
00:00:08,580 --> 00:00:13,020
In this session, I'll be discussing
a project I worked on that try to

5
00:00:13,020 --> 00:00:16,590
answer the question, is it possible
to chat with your mandatory metrics?

6
00:00:16,710 --> 00:00:20,100
I'm excited to share what I've
learned and I ask her if pay

7
00:00:20,100 --> 00:00:21,600
attention as we go through it.

8
00:00:22,300 --> 00:00:25,200
So the talk today is divided
into five sections, we do

9
00:00:25,200 --> 00:00:26,490
like a general introduction.

10
00:00:26,820 --> 00:00:30,090
We look at the approach by
which the tool was built.

11
00:00:30,090 --> 00:00:33,930
The tool itself is com called
Prompt Chart, so we'll spend some

12
00:00:33,930 --> 00:00:36,270
time reviewing the tool itself.

13
00:00:36,540 --> 00:00:41,160
Lastly, we'll discuss the result
and then a closing out section.

14
00:00:41,430 --> 00:00:43,860
Okay, moving straight
on to the first section.

15
00:00:44,250 --> 00:00:46,530
Introduction was a problem model solution.

16
00:00:47,080 --> 00:00:50,590
So as SRE, we understand the
importance of speed when it comes

17
00:00:50,590 --> 00:00:54,220
to incident, and if you have a tool
that allows you to be able to chat

18
00:00:54,340 --> 00:00:56,050
with your metrics, it can speed up.

19
00:00:56,110 --> 00:00:58,630
Your incident responds significantly.

20
00:00:59,090 --> 00:01:00,920
For me, I think there are
two ways to think about it.

21
00:01:00,920 --> 00:01:05,660
The first is that it reduces cognitive
load while responding to an incident where

22
00:01:05,660 --> 00:01:10,400
you know that there is pressure to restore
user experience as fast as possible.

23
00:01:10,610 --> 00:01:11,600
So having a tool that.

24
00:01:12,150 --> 00:01:15,660
Helps you write prompt care without you
having to think about it, reduces the

25
00:01:15,660 --> 00:01:20,370
cognitive load required with responding
to incident on the other side as well.

26
00:01:20,370 --> 00:01:24,000
You're able to get quick insight without
having to go through multiple dashboards,

27
00:01:24,210 --> 00:01:28,200
which is another way in which it can
lead to faster incident response.

28
00:01:28,560 --> 00:01:32,670
The order usefulness of having a
tool like this is that it makes

29
00:01:32,670 --> 00:01:34,590
data accessible to everybody.

30
00:01:34,910 --> 00:01:37,910
Traditionally to expose data from your Es.

31
00:01:39,340 --> 00:01:43,270
Instances, you'll build dashboards
for people that are not capable of

32
00:01:43,540 --> 00:01:45,910
writing like their own prom qls.

33
00:01:46,150 --> 00:01:50,410
So you would build graph dashboard on
things like graph, which they can go.

34
00:01:50,770 --> 00:01:53,410
But of course that means for each type of.

35
00:01:53,770 --> 00:01:56,200
Data, or each question that they
have, you need to create the

36
00:01:56,200 --> 00:01:58,090
dashboard, which they can go and see.

37
00:01:58,420 --> 00:02:02,380
However, if they have a tool that
they can basically ask questions,

38
00:02:02,740 --> 00:02:06,520
then you don't need to keep
coming up with the new dashboard.

39
00:02:06,520 --> 00:02:10,300
Each time that there's a new query
and a new question, they can basically

40
00:02:10,300 --> 00:02:12,520
just interact with the tool and get.

41
00:02:13,405 --> 00:02:14,995
It start answers back.

42
00:02:15,335 --> 00:02:19,415
The other advantage of having a
tool like this is that it uses

43
00:02:19,415 --> 00:02:20,735
the learning curve for Prometheus.

44
00:02:21,405 --> 00:02:26,385
Starting up it, it's easy to write
simple promeus queries as you need

45
00:02:26,385 --> 00:02:28,755
to get, write more completed queries.

46
00:02:28,755 --> 00:02:34,395
Things can get a bit harder, especially
we are just learning d. Language.

47
00:02:34,395 --> 00:02:39,515
So having something like a tool like
this that can essentially cooperate

48
00:02:39,515 --> 00:02:43,384
with you, like copilot when you're
trying to write ProQ is good 'cause

49
00:02:43,384 --> 00:02:47,464
it helps you, is learning cover,
especially via getting started.

50
00:02:47,764 --> 00:02:49,054
And then I think one of the major.

51
00:02:49,684 --> 00:02:54,924
Motivations for me to actually work on
the project is I see a lot of projects

52
00:02:55,164 --> 00:02:58,885
in the data space that allows you
to be able to chat with your data.

53
00:02:58,885 --> 00:03:03,055
So it terms Hey, ask questions about
your database and things like that.

54
00:03:03,055 --> 00:03:07,584
So I started wondering, is it possible
to do the same thing for metrics,

55
00:03:07,584 --> 00:03:09,535
especially in the monitoring sense.

56
00:03:09,774 --> 00:03:15,265
So that's like the motivation in trying
to solve the problem and answering the.

57
00:03:15,640 --> 00:03:18,760
Question, is it possible to chat
with your monitoring metrics?

58
00:03:19,280 --> 00:03:25,880
So the solution that came up with that is
discussed in this project works this way.

59
00:03:25,880 --> 00:03:31,220
So the flow is the u We are building it
too, that allows users to essentially ask

60
00:03:31,220 --> 00:03:33,950
questions in natural language and then.

61
00:03:34,340 --> 00:03:37,850
The AI agents take the natural
language and generates a

62
00:03:37,850 --> 00:03:40,340
corresponding prompt QL query.

63
00:03:40,760 --> 00:03:45,380
The prompt QL query is then run on
Prometheus to get the actual results

64
00:03:45,380 --> 00:03:49,610
back from Prometheus, but then that
result before it gets back to the user

65
00:03:49,610 --> 00:03:52,340
is converted back into natural language.

66
00:03:52,340 --> 00:03:55,610
So the flow is that it starts
with natural language and it

67
00:03:55,610 --> 00:03:56,900
ends with natural language.

68
00:03:56,900 --> 00:03:59,960
The user asks questions in
natural language, and then they

69
00:03:59,960 --> 00:04:03,470
get a result presented back
to them in natural language.

70
00:04:04,400 --> 00:04:07,550
So the second section of the
talk talks about the approach.

71
00:04:07,550 --> 00:04:10,610
At this point, we'll dive deep into what.

72
00:04:11,210 --> 00:04:15,179
How we implemented it the architecture,
and then we review a couple of the

73
00:04:15,179 --> 00:04:17,219
key components of the architecture.

74
00:04:17,649 --> 00:04:21,250
So this diagram shows
architecture for the system.

75
00:04:21,339 --> 00:04:24,640
So on the far left side of it,
we have the front end, which is

76
00:04:24,640 --> 00:04:26,469
how you interact with the system.

77
00:04:26,800 --> 00:04:30,760
So it is either you interacting
with the tool via a web app or a

78
00:04:30,760 --> 00:04:35,890
Slack app basically, which allows
you to enter your user query.

79
00:04:36,109 --> 00:04:39,259
And then that user query
gets sent to a backend.

80
00:04:39,319 --> 00:04:44,679
The backend has rest API that makes
it easy for the front end to interact

81
00:04:44,679 --> 00:04:46,939
and send questions down to it.

82
00:04:47,419 --> 00:04:50,089
As part of the backend as
well, we have the AI agent.

83
00:04:50,419 --> 00:04:54,889
The AI agent basically handles
the coordination with the LLM.

84
00:04:55,249 --> 00:05:00,679
So when the user query comes, it basically
sends the request to the LLM and then

85
00:05:00,679 --> 00:05:03,399
the LLM as well uses what is called.

86
00:05:03,974 --> 00:05:07,634
Two, calling a function, callings
to be able to talk to Prometheus.

87
00:05:07,844 --> 00:05:12,584
So on the Prometheus aspect, we need to
be able to fetch metadata from Prometheus

88
00:05:12,584 --> 00:05:16,154
and run queries from Prometheus as well.

89
00:05:16,394 --> 00:05:20,684
As we go later into the talk, we'll talk
more about the metadata such that you

90
00:05:20,684 --> 00:05:22,639
have better understanding of what entails.

91
00:05:23,139 --> 00:05:24,559
But this gives.

92
00:05:25,304 --> 00:05:30,074
I an eye level overview of what happens
when you're trying to answer questions.

93
00:05:30,074 --> 00:05:32,804
So the flow is, the request
comes in, let's say from the

94
00:05:32,804 --> 00:05:34,844
web app, it goes to the backend.

95
00:05:35,174 --> 00:05:39,194
The backend takes the question,
sends it to the AI agent.

96
00:05:39,194 --> 00:05:44,144
The AI agent takes the question,
sends it to the LLM, so the user

97
00:05:44,384 --> 00:05:45,824
query together with the metadata.

98
00:05:45,824 --> 00:05:50,294
So the metadata in this case is you can
think of it like the scheme of your data.

99
00:05:50,294 --> 00:05:53,504
So it basically just contains
information regarding.

100
00:05:53,909 --> 00:05:57,629
What kind of metrics are available
in your Promeus instance and

101
00:05:57,629 --> 00:05:58,799
the descriptions for them.

102
00:05:58,799 --> 00:06:02,369
So you send that together with
the user query to the LLM.

103
00:06:02,819 --> 00:06:07,409
The LLM then thinks about a
prompt QL that would be able to

104
00:06:07,409 --> 00:06:09,389
answer that question correctly.

105
00:06:09,719 --> 00:06:15,599
Then when they ask the prompt, QL, send
it back to the AI agent, which then runs

106
00:06:15,599 --> 00:06:19,589
that query on Prometheus to get the.

107
00:06:20,069 --> 00:06:21,090
Actual result.

108
00:06:21,090 --> 00:06:25,079
And then once the result comes back
from Prometheus, it sparks back to the

109
00:06:25,079 --> 00:06:29,999
LLM such that the LLM can interpret
the result in natural language.

110
00:06:29,999 --> 00:06:34,319
And then the result goes all the way back
to the front end for the user to see.

111
00:06:34,659 --> 00:06:35,469
So that's what the.

112
00:06:36,219 --> 00:06:37,389
Overview of the system.

113
00:06:37,389 --> 00:06:41,319
Looks like we'll be looking at a
couple of tools that allows, or the key

114
00:06:41,319 --> 00:06:43,389
components of the architecture as well.

115
00:06:43,669 --> 00:06:45,319
So the first one is Prometheus.

116
00:06:45,649 --> 00:06:49,439
Again, if you're not familiar with
Prometheus I won't spend too much time

117
00:06:50,019 --> 00:06:51,819
diving into it, but you can check it out.

118
00:06:51,969 --> 00:06:54,969
What you need to know is that
it's a time series data store.

119
00:06:55,359 --> 00:07:01,189
For Matrix and typically ma matrix
stored in material have these

120
00:07:01,189 --> 00:07:04,639
formats where you have the matrix
name and then you have the label.

121
00:07:04,879 --> 00:07:09,049
So in this case, the labels are method
and handler, and then they have the

122
00:07:09,049 --> 00:07:11,209
corresponding value of post and messages.

123
00:07:11,209 --> 00:07:17,289
So this is an entry of Prometheus
of a metric in Prometheus.

124
00:07:17,319 --> 00:07:19,659
So this metric is then
script by Prometheus.

125
00:07:20,424 --> 00:07:23,524
Quite often based on whatever
script interval you have.

126
00:07:23,534 --> 00:07:26,984
As the time series data is going to
recall the time that it was script

127
00:07:26,984 --> 00:07:32,424
and then the value at that point in
time, and that's how this part works.

128
00:07:32,604 --> 00:07:36,744
So part of the things that make this
architecture works is that premises

129
00:07:36,804 --> 00:07:41,154
as this metadata API that allows
you to get, again, as I described

130
00:07:41,274 --> 00:07:43,404
earlier, like a scheme of your data.

131
00:07:43,914 --> 00:07:45,444
In the protal systems.

132
00:07:45,654 --> 00:07:49,674
So what that entails is that it shows
you the list of all the metrics that are

133
00:07:49,674 --> 00:07:53,665
available inside of your protal system.

134
00:07:53,669 --> 00:07:59,074
So in our case, for example, you can
see C device or this device usage to CPU

135
00:07:59,074 --> 00:08:01,564
load average superior U system sequence.

136
00:08:01,804 --> 00:08:06,994
These are examples of metrics that are
being scripted by Prometheus and is

137
00:08:06,994 --> 00:08:09,634
available in the Prometheus instance.

138
00:08:09,634 --> 00:08:13,054
So that means you can write queries or
ask questions regarding this metrics.

139
00:08:14,449 --> 00:08:18,169
But not only is the name return,
they also returns information,

140
00:08:18,379 --> 00:08:22,249
helpful information about them, which
is the type of the metric itself

141
00:08:22,619 --> 00:08:24,089
based on the type of the metric.

142
00:08:24,089 --> 00:08:29,199
Again, it depends the kind of operations
that you can perform on The metric

143
00:08:29,199 --> 00:08:31,059
depends on the type of the metric, right?

144
00:08:31,059 --> 00:08:34,734
So that's something to take up
that needs to be taken into account

145
00:08:34,734 --> 00:08:36,254
while writing your Pro Creole.

146
00:08:36,554 --> 00:08:40,359
And then the help also gives
information, additional

147
00:08:40,359 --> 00:08:42,280
information regarding each type.

148
00:08:42,879 --> 00:08:48,050
Of metric, which allows you to be able
to interpret the metric correctly.

149
00:08:48,360 --> 00:08:51,900
Additionally, sometimes you can also
add information about the labels.

150
00:08:52,170 --> 00:08:56,790
So for example, for this 80 TTP
request total, the description can be

151
00:08:56,790 --> 00:08:59,701
something like, oh, you let you know
the number of 80 TP requests that

152
00:08:59,706 --> 00:09:01,985
has happened within a time period.

153
00:09:02,565 --> 00:09:04,825
And he has following label like method.

154
00:09:04,885 --> 00:09:13,320
And so this metadata is the, additional
bits that is sent as context along

155
00:09:13,320 --> 00:09:16,890
with the user query itself to the LLM.

156
00:09:17,190 --> 00:09:18,990
So that's the first major component.

157
00:09:19,230 --> 00:09:22,920
The second major component,
as we explained, is the LLM.

158
00:09:22,920 --> 00:09:28,925
And then he makes use of a. Technical
tools calling or function calling, which

159
00:09:28,925 --> 00:09:34,445
is basically he extends the LLM such that
it's able to interact with the environment

160
00:09:34,805 --> 00:09:38,705
that is outside of the LLM itself
using tools and to make that happen,

161
00:09:39,005 --> 00:09:42,275
we have built two tools in our example.

162
00:09:42,275 --> 00:09:46,955
So we have the query promises tool,
which basically allows the LLM to.

163
00:09:47,350 --> 00:09:52,820
Execute Prometheus query on an
instance and get response back before

164
00:09:52,820 --> 00:09:55,580
returning a response back to the user.

165
00:09:55,980 --> 00:10:02,310
The other tool also allows the LLM to be
able to query Prometheus for the metadata

166
00:10:02,640 --> 00:10:07,020
information similar to the metadata
information shown in the previous.

167
00:10:07,560 --> 00:10:08,730
In the previous page here.

168
00:10:09,030 --> 00:10:13,740
So basically the first two allows
Prometheus to be able to run it

169
00:10:13,740 --> 00:10:18,300
degenerated from Q and get responses
back from Prometheus while the second

170
00:10:18,300 --> 00:10:26,250
two allows Prometheus to be able to fetch
metadata results from Prometheus itself.

171
00:10:26,250 --> 00:10:29,920
Metadata regarding the metrics available.

172
00:10:30,220 --> 00:10:31,565
So that's the two major.

173
00:10:32,260 --> 00:10:34,390
Component in our architecture.

174
00:10:34,390 --> 00:10:40,010
So moving on to the third part, which is
we are looking at the tool that was built.

175
00:10:40,010 --> 00:10:44,180
So the tool itself is called Prompt
chart, and basically we'll be exploring

176
00:10:44,180 --> 00:10:46,010
the tool from two points of view.

177
00:10:46,260 --> 00:10:51,570
So based on the data inside of
the Prometheus instance, we've

178
00:10:51,570 --> 00:10:54,340
classified it into two examples.

179
00:10:54,370 --> 00:10:58,990
The first one uses data from no
exporter, and then the second.

180
00:10:59,620 --> 00:11:05,565
Example of pro chats uses data
from custom exporter to Promeus.

181
00:11:05,570 --> 00:11:07,430
So both of them are chat.

182
00:11:07,460 --> 00:11:08,630
You're chatting with Promeus.

183
00:11:08,630 --> 00:11:13,280
So what's just different is the source
of data in the Promeus instance and

184
00:11:13,550 --> 00:11:17,360
which basically affects the type of
queries that you can, or questions you

185
00:11:17,360 --> 00:11:22,790
can ask, and the type of queries that
you, that will be generated as well.

186
00:11:22,820 --> 00:11:24,560
A little bit more about the node exporter.

187
00:11:24,560 --> 00:11:26,120
So the node exporter is essentially.

188
00:11:27,065 --> 00:11:29,975
Like a plugin that you can run on a vm.

189
00:11:30,155 --> 00:11:35,885
So you can run it on a single vm, you can
run it on your euca machine, for example.

190
00:11:36,165 --> 00:11:39,675
Pretty much any like machine
VM node, you can run it.

191
00:11:39,675 --> 00:11:43,845
So what does is you expose system
metrics to permit your such a protus

192
00:11:44,475 --> 00:11:46,425
scan, script them at intervals.

193
00:11:46,425 --> 00:11:52,195
So for example, things like the
CPU seconds, the available storage.

194
00:11:52,520 --> 00:11:55,910
Memory utilization, network
traffic, all of that can be

195
00:11:55,910 --> 00:11:58,850
exposed by the node exporter.

196
00:11:59,360 --> 00:12:02,780
So there are standard system metrics and
you don't need to configure anything.

197
00:12:02,780 --> 00:12:06,650
The node exporter basically just
makes those available and then those

198
00:12:06,650 --> 00:12:09,890
data can be injected into premises.

199
00:12:09,890 --> 00:12:13,740
So these are sample of some of
the metrics that are available.

200
00:12:13,740 --> 00:12:19,060
So as you can see, note CPU seconds
to the the system available.

201
00:12:19,330 --> 00:12:21,250
Five system available in bys.

202
00:12:21,850 --> 00:12:28,190
And then this is on the number
of network requests in bys that

203
00:12:28,190 --> 00:12:29,870
have been received as well.

204
00:12:29,900 --> 00:12:32,960
The average network traffic
invites over the last minute.

205
00:12:32,960 --> 00:12:36,620
So examples of the metrics and
then some of the cray so you can

206
00:12:36,620 --> 00:12:42,050
see, using like rates to get like
averages over a time period of time.

207
00:12:42,050 --> 00:12:47,420
So this is what a Prometheus
query looks like as well.

208
00:12:47,420 --> 00:12:48,380
So moving on.

209
00:12:48,720 --> 00:12:55,020
So this is showing the actual tool
called pro chart and then responses

210
00:12:55,450 --> 00:12:58,480
that was generated by the system
when it was asked certain questions.

211
00:12:58,570 --> 00:13:01,570
So in this example, the prompt
chart is connected to the

212
00:13:01,570 --> 00:13:03,940
premises instance that has data.

213
00:13:04,490 --> 00:13:06,080
From a note exporter.

214
00:13:06,710 --> 00:13:10,880
So the information available or the
kind of questions you can ask is

215
00:13:11,000 --> 00:13:14,360
based on the kind of metrics that
is expected by the note exporter.

216
00:13:14,570 --> 00:13:21,030
So in this case we can see four exchanges
or five questions into the, actually, so

217
00:13:21,030 --> 00:13:23,990
the first talks about DCP utilization.

218
00:13:24,260 --> 00:13:25,340
So this is a question.

219
00:13:25,340 --> 00:13:29,000
So in black here, you have
the user question itself.

220
00:13:29,390 --> 00:13:32,180
While in white it is a response from the.

221
00:13:32,705 --> 00:13:35,375
From chart AI system is itself.

222
00:13:35,375 --> 00:13:39,215
So it looks first asking about the
currency periodization on the node.

223
00:13:39,545 --> 00:13:40,265
We get this.

224
00:13:40,265 --> 00:13:44,385
He asked about the battery percentage
on the node more around information

225
00:13:44,385 --> 00:13:45,495
always running on the node.

226
00:13:45,495 --> 00:13:51,545
So I did run this particular example
on my laptop using the macros node

227
00:13:51,545 --> 00:13:54,155
exporter, so you can see gets that.

228
00:13:54,495 --> 00:13:56,955
The node is running macros, the digs.

229
00:13:57,510 --> 00:14:00,840
Space that is available across
all the instances of the nodes.

230
00:14:00,840 --> 00:14:07,050
And then finally the memory utilization
of the node is shown as well.

231
00:14:08,190 --> 00:14:12,580
Alright, so we'll look at, in, in
follow up, would look at later,

232
00:14:12,820 --> 00:14:17,470
would actually see from the backend
work queries are generated for these

233
00:14:17,470 --> 00:14:19,180
examples and what they look like.

234
00:14:19,520 --> 00:14:23,660
The second Prometheus
instance, I'll be using.

235
00:14:24,110 --> 00:14:28,790
Is thanks to our, the
friends@promlabs.com.

236
00:14:28,790 --> 00:14:32,510
So they have this Prometheus
instance that is publicly accessible.

237
00:14:32,880 --> 00:14:38,400
So is from the team that wrote
Prometheus and they make it available

238
00:14:38,400 --> 00:14:42,540
so that you can use it to experiment,
to learn and interact with Prometheus.

239
00:14:43,260 --> 00:14:46,530
On that Promis instance, we
have some custom metrics.

240
00:14:46,530 --> 00:14:52,250
So if you look at the first set of metrics
that was exposed by, in our first example

241
00:14:52,250 --> 00:14:56,960
using the node exporter, those metrics
are system metrics, meaning that there are

242
00:14:56,960 --> 00:15:00,080
metrics from regarding the machine itself.

243
00:15:00,440 --> 00:15:03,500
But more often than not, when we
are trying to set up monitoring,

244
00:15:03,530 --> 00:15:06,740
we want to collect metrics about
the state of our service or the

245
00:15:06,740 --> 00:15:08,330
application we are running as well.

246
00:15:08,600 --> 00:15:09,830
So in those cases.

247
00:15:09,945 --> 00:15:15,395
You would come up with like your own
custom metrics describing the different

248
00:15:15,395 --> 00:15:18,335
states or events in your service.

249
00:15:18,515 --> 00:15:23,555
So the Prometheus on prom lab,
the demo Prometheus on prom lab.

250
00:15:23,615 --> 00:15:27,005
So he runs a service called the
demo service, and then the demo

251
00:15:27,005 --> 00:15:29,585
service exposes the following.

252
00:15:30,055 --> 00:15:34,835
Custom metrics which is what we
based in a set of questions and

253
00:15:34,835 --> 00:15:36,995
interactions with from chat on.

254
00:15:37,305 --> 00:15:42,165
So a couple of interesting ones that you
see is one is that there are batch jobs.

255
00:15:42,165 --> 00:15:46,765
So you can essentially ask questions
around success rate of batch jobs

256
00:15:46,765 --> 00:15:48,805
that was done by the demo service.

257
00:15:49,075 --> 00:15:53,305
Can see questions around the
HCTP request duration as well.

258
00:15:53,785 --> 00:15:59,185
Another interesting one is that the
service itself as a matrix lets you

259
00:15:59,185 --> 00:16:01,195
know whether today is an holiday or not.

260
00:16:01,195 --> 00:16:06,295
So it's basically called demo is
holiday and from the help information.

261
00:16:06,295 --> 00:16:10,765
So again, if you look at the metadata
information, this is GUI, not from

262
00:16:10,945 --> 00:16:15,365
the API, but basically it's going
to be similar to the metadata.

263
00:16:16,625 --> 00:16:20,795
EPI response showed earlier where
you can see the name of the metric.

264
00:16:21,135 --> 00:16:24,545
The type of it, which is in
this case for his early day.

265
00:16:24,545 --> 00:16:29,345
We can see the type of set as gauge,
and then the description takes that when

266
00:16:29,345 --> 00:16:33,335
the value return is one, it means that
the current days and early day, and when

267
00:16:33,695 --> 00:16:36,965
the value rate return is zero, it means
that the current days one early days.

268
00:16:36,965 --> 00:16:41,345
So these are examples of custom
metrics that has been made available

269
00:16:41,345 --> 00:16:43,415
by the demo service on this instance.

270
00:16:43,715 --> 00:16:44,825
So all of this.

271
00:16:45,335 --> 00:16:49,475
Metrics has been script and is available
inside of the Promeus instance.

272
00:16:49,625 --> 00:16:54,665
So that means we can connect our pro chart
tool to this Prometheus instance and ask

273
00:16:54,665 --> 00:17:00,665
it questions and let the AI generate the
corresponding queries and shows answers.

274
00:17:01,025 --> 00:17:04,745
So let's see how that looks like.

275
00:17:04,745 --> 00:17:10,505
So in the next chat here we can see
is, the prom chart two is connected to

276
00:17:10,505 --> 00:17:15,165
the prom labs protal instance, and the
first question here is today an holiday?

277
00:17:15,495 --> 00:17:19,215
So the AI system response are
based on metrics that I was able

278
00:17:19,215 --> 00:17:20,985
to find from demo service Two.

279
00:17:21,165 --> 00:17:23,025
It reports that today is an holiday.

280
00:17:23,325 --> 00:17:26,810
Again, just in the next couple
of slides, we'll be looking

281
00:17:26,810 --> 00:17:28,790
at the actual query and area.

282
00:17:29,360 --> 00:17:33,650
Got to this answer, but basically
this just shows an overview

283
00:17:33,650 --> 00:17:36,260
of what the interaction from
the web interface looks like.

284
00:17:36,620 --> 00:17:41,030
The second question we ask is how
many items have been shipped to this?

285
00:17:41,030 --> 00:17:45,770
So if a go back cop would see that
there is a custom metric exposed

286
00:17:45,770 --> 00:17:49,910
called items shipped to the, which
is a counter that keeps track of the

287
00:17:49,910 --> 00:17:51,650
number of items that have been shipped.

288
00:17:51,800 --> 00:17:55,400
So making use of this, we expect
that the AI will make use of this.

289
00:17:55,775 --> 00:18:00,815
Counter to be able to answer the question
of how many items have been shipped today.

290
00:18:01,085 --> 00:18:03,515
And then lastly, there's a third
question here where we are trying

291
00:18:03,515 --> 00:18:09,535
to ask questions around the demo API
and if it's taken longer than usual

292
00:18:09,905 --> 00:18:14,885
unfortunately we realize, oh, as you
can see from the screenshots, the ICM

293
00:18:14,885 --> 00:18:17,165
response, that there is no data fund.

294
00:18:17,555 --> 00:18:22,715
So we look at that and try to figure
out, okay, why did that happen as well.

295
00:18:24,600 --> 00:18:25,980
Behind the scenes as promised.

296
00:18:25,980 --> 00:18:30,090
So this is basically the logs
from the backend system letting

297
00:18:30,090 --> 00:18:31,980
us know exactly what transpired.

298
00:18:32,250 --> 00:18:36,180
So in the case of the first
example message, the user

299
00:18:36,180 --> 00:18:38,130
query is a sedan holiday.

300
00:18:38,580 --> 00:18:43,530
Then the AI agents generated this
corresponding prompt Q query,

301
00:18:43,530 --> 00:18:45,900
which is demo is holiday, right?

302
00:18:45,900 --> 00:18:46,470
And then.

303
00:18:46,870 --> 00:18:51,200
This generated prom QR was run
by the AI agent using function,

304
00:18:51,590 --> 00:18:55,400
calling on the permitter sensor
and permit to return this response.

305
00:18:55,400 --> 00:19:00,560
So if you pay attention to this, you'll
see the instance name is demo service two,

306
00:19:00,590 --> 00:19:05,905
which is why the response talks about demo
service two, or the most important stuff

307
00:19:05,905 --> 00:19:08,240
to pay attention to is the value here.

308
00:19:08,465 --> 00:19:12,725
So you see the value here is one,
because this value is one that means

309
00:19:13,145 --> 00:19:16,175
based on the information that we
are able to get from the metadata,

310
00:19:16,445 --> 00:19:18,185
we know that one means only day.

311
00:19:18,365 --> 00:19:22,745
So you can see in this example,
the AI agent is able to both write

312
00:19:22,805 --> 00:19:27,065
the right corresponding query
based on the metadata, but also

313
00:19:27,065 --> 00:19:29,675
based on the information provided.

314
00:19:30,275 --> 00:19:33,305
In the metadata, it's
able to interpret that.

315
00:19:33,305 --> 00:19:39,395
One means that today is an holiday, and as
such, based on that interpretation is able

316
00:19:39,395 --> 00:19:44,915
to reply the user query back in natural
language saying yes based on the data that

317
00:19:44,915 --> 00:19:47,375
we can see from demo service to values.

318
00:19:47,645 --> 00:19:49,445
And that means that today it's an holiday.

319
00:19:49,685 --> 00:19:52,115
So this is essentially what's
going on behind the scene.

320
00:19:52,535 --> 00:19:54,485
In the case of the first question.

321
00:19:54,815 --> 00:19:59,255
In the case of the second question,
this is a way more interesting.

322
00:19:59,465 --> 00:20:02,075
So we asked about how many
items has been shipped today.

323
00:20:02,075 --> 00:20:05,285
This is the user query
again generated from ql.

324
00:20:05,375 --> 00:20:08,915
As expected, it makes use of the
metrics demo item shipped total.

325
00:20:09,665 --> 00:20:11,495
So it sets the time period.

326
00:20:11,495 --> 00:20:14,765
So one day this is right, and
then it looks at the increase

327
00:20:14,765 --> 00:20:16,025
over the period of one day.

328
00:20:16,415 --> 00:20:18,755
So this is generated from QL query.

329
00:20:19,325 --> 00:20:20,285
Which makes sense.

330
00:20:20,285 --> 00:20:25,415
So again, using two scrolling that
this query is executed against the

331
00:20:25,415 --> 00:20:31,115
Prometheus instance and Promeus returns,
this results back to the AI agent.

332
00:20:31,385 --> 00:20:37,865
So now the AI needs to figure out how to
give, return this in natural language.

333
00:20:37,865 --> 00:20:40,145
And this, I think this is interesting.

334
00:20:40,460 --> 00:20:45,440
But this an a particularly interesting
example because we would see that the demo

335
00:20:45,440 --> 00:20:47,750
service actually runs three copies of it.

336
00:20:47,750 --> 00:20:50,840
So as you can see here, there are
three instances of the demo service.

337
00:20:51,110 --> 00:20:53,690
There's demo service zero,
which is the first instance.

338
00:20:54,140 --> 00:20:57,980
There is demo service one, which is the
second instance here, and then there is.

339
00:20:58,310 --> 00:21:00,920
Demo service two, which
is the third instance.

340
00:21:01,160 --> 00:21:04,760
So for each of these instance, they have
been processing orders through the day

341
00:21:05,000 --> 00:21:09,230
and each of them maintain a count of the
number of orders that they've processed.

342
00:21:09,410 --> 00:21:17,030
So if you look at the demo service zero,
it returns around five, 455,000 orders

343
00:21:17,090 --> 00:21:19,220
have been processed by demo service one.

344
00:21:19,550 --> 00:21:21,770
And then if you look at demo service.

345
00:21:22,760 --> 00:21:27,200
No, the demo service zero
rather, has processed 455,000.

346
00:21:27,290 --> 00:21:31,610
If you look at demo service one,
the value four eight is 453,000.

347
00:21:31,910 --> 00:21:36,350
And then if you look at demo service
two, the value process, so five a day

348
00:21:36,710 --> 00:21:40,610
is 453,000, close to 454,000 as well.

349
00:21:40,970 --> 00:21:41,240
Now.

350
00:21:41,735 --> 00:21:46,235
The LLM does something interesting
because it gets this results back.

351
00:21:46,325 --> 00:21:51,395
It is intelligent enough to know that
the value is interested in, is a,

352
00:21:51,845 --> 00:21:57,995
an aggregation of the value across
each of these instances, and also

353
00:21:57,995 --> 00:22:01,745
applying the right grouping so it
was able to figure out that it needs

354
00:22:01,745 --> 00:22:04,355
to sum the value from demo service.

355
00:22:04,355 --> 00:22:06,275
Zero demo service.

356
00:22:06,785 --> 00:22:08,915
One and demo service two.

357
00:22:08,915 --> 00:22:13,335
So if you look at this value, so you
can do it later, but I've confirmed it.

358
00:22:13,335 --> 00:22:18,225
If you sum this value return
for demo service one, I plus the

359
00:22:18,225 --> 00:22:20,835
value sum there, demo service.

360
00:22:21,375 --> 00:22:26,815
One this is the value return from
demo service two plus the value

361
00:22:26,815 --> 00:22:31,075
return from demo service, one plus
value return from demo service zero.

362
00:22:31,405 --> 00:22:32,140
You're going to get.

363
00:22:32,755 --> 00:22:33,715
The total value.

364
00:22:33,715 --> 00:22:37,705
Yeah, so the LLM was actually
able to figure out the right

365
00:22:37,765 --> 00:22:41,875
aggregation for the list of data
that was returned in the results.

366
00:22:41,875 --> 00:22:45,595
Sum it appropriately and give
the right number back in.

367
00:22:46,520 --> 00:22:48,290
Natural language back to the user.

368
00:22:48,290 --> 00:22:53,850
So this is another example that shows
how it really shines to get instant.

369
00:22:54,030 --> 00:22:55,320
And this happens that can stand.

370
00:22:55,470 --> 00:22:58,710
So again, you're able to
get instant insightful data

371
00:22:59,040 --> 00:23:00,300
while making use of the two.

372
00:23:00,660 --> 00:23:04,260
Now looking at the third example
in the case, which we are

373
00:23:04,260 --> 00:23:06,240
unable to get any response back.

374
00:23:06,510 --> 00:23:09,820
So again, this is the user
request or the user query.

375
00:23:11,335 --> 00:23:13,255
The user asks a request to the demo.

376
00:23:13,255 --> 00:23:14,995
With PI taken longer than usual.

377
00:23:15,415 --> 00:23:20,145
Then the AI agents generated this query.

378
00:23:20,385 --> 00:23:24,315
Now the query in terms of the ax is valid.

379
00:23:24,315 --> 00:23:25,220
There's nothing wrong with it.

380
00:23:25,420 --> 00:23:28,865
I returns and mt data set.

381
00:23:28,955 --> 00:23:35,345
And that's not because there is no data
available for the demo API, but how?

382
00:23:35,840 --> 00:23:40,430
The LLM has interpreted the question
and has tried to go about writing

383
00:23:40,430 --> 00:23:44,720
the queries incorrect, and as
such, we get an empty data back.

384
00:23:44,720 --> 00:23:49,500
Although it's a valid ProQ
query, it doesn't answer the

385
00:23:49,500 --> 00:23:52,560
question that the user has asked.

386
00:23:53,130 --> 00:23:55,350
And doesn't give us any response back.

387
00:23:55,350 --> 00:23:58,890
So that's one of the limitations
that we've discovered here.

388
00:23:59,400 --> 00:24:02,510
But again, looking at it there,
where to get around this.

389
00:24:02,660 --> 00:24:04,790
So we'll discuss that in the next session.

390
00:24:04,790 --> 00:24:08,390
So in the next example, or as
you can see here, so this is the

391
00:24:08,390 --> 00:24:10,190
prompt chart application itself.

392
00:24:10,430 --> 00:24:14,010
When you come here,
you're able to modify the.

393
00:24:15,220 --> 00:24:18,825
LLM configurations where you can
set what provider you want to use,

394
00:24:19,065 --> 00:24:21,615
what model you want to use as well.

395
00:24:21,795 --> 00:24:26,355
So in this case scenario is currently
all the questions that we backed so far.

396
00:24:26,625 --> 00:24:32,900
We are using Google's Gemini model and
then we are using the Google Gemini 1.5

397
00:24:32,950 --> 00:24:35,860
flash, which is their first free model.

398
00:24:36,585 --> 00:24:40,005
So that's what we have used so
far to answer a, our questions.

399
00:24:40,195 --> 00:24:45,175
But we see that Gemini 1.5 in this
example was unable to generate the

400
00:24:45,175 --> 00:24:48,115
correct prompt, clear query for us.

401
00:24:48,385 --> 00:24:51,895
So then by changing the model
type to the thinking model.

402
00:24:52,255 --> 00:24:56,395
So if you check here based on
the confirmation prompt chat

403
00:24:56,825 --> 00:24:59,645
so I flipped the model to.

404
00:25:00,275 --> 00:25:03,425
Use the thinking model provided
by Google, which is the Gemini

405
00:25:03,425 --> 00:25:06,425
2.0 flash thinking model instead.

406
00:25:06,665 --> 00:25:09,905
So by making use of this model
and asking exactly the same

407
00:25:09,905 --> 00:25:12,605
question, we are able to see that.

408
00:25:12,605 --> 00:25:18,035
Now if you look at this example,
the AI agent is actually able to

409
00:25:18,035 --> 00:25:19,985
answer the question correctly.

410
00:25:19,985 --> 00:25:22,325
So we'll look at the backend
and see what has changed.

411
00:25:22,565 --> 00:25:27,815
But in this case scenario, you basically
ask the same question based on the newer.

412
00:25:28,235 --> 00:25:32,855
Thinking mod, he was able to provide
this insight and say, oh, we can see

413
00:25:32,855 --> 00:25:38,875
that the slash API slash part seems
to have a lot of 500 errors, and as

414
00:25:38,875 --> 00:25:43,285
such, latency is also significantly
higher than the other part as well.

415
00:25:43,285 --> 00:25:47,245
Which suggest that the part has an issue.

416
00:25:47,245 --> 00:25:54,800
So as anr, you can imagine how
insightful such a. And inside like

417
00:25:54,800 --> 00:25:58,730
this is when you are currently trying
to debug an incident and you're

418
00:25:58,730 --> 00:26:00,260
trying to figure out what's going on.

419
00:26:00,260 --> 00:26:07,010
Maybe you get like a latency
a lot or basically user start

420
00:26:07,010 --> 00:26:08,450
complain that your CM is slow.

421
00:26:08,450 --> 00:26:09,470
You can basically just.

422
00:26:10,025 --> 00:26:14,225
File the tool, ask and get
insights for information like this.

423
00:26:14,475 --> 00:26:18,105
Now going back to the backend, we
can see essentially what happened.

424
00:26:18,435 --> 00:26:23,055
So from the backend there, you would see
pretty much the same thing, the same user

425
00:26:23,385 --> 00:26:26,445
request, but if you pay attention Yeah.

426
00:26:26,715 --> 00:26:27,735
To look at the.

427
00:26:28,485 --> 00:26:30,045
Generated prom ql.

428
00:26:30,045 --> 00:26:33,975
That this generated a different
prom QL that is more appropriate

429
00:26:33,975 --> 00:26:35,445
is considering the rate now.

430
00:26:35,775 --> 00:26:41,215
And also based on the type of the data
is able to use the Instagram counter.

431
00:26:41,815 --> 00:26:45,805
And as such, it was able to
actually generate meaningful

432
00:26:45,805 --> 00:26:47,305
results this time around.

433
00:26:47,305 --> 00:26:52,245
So this is the full responses way
much longer than this because there

434
00:26:52,245 --> 00:26:54,605
are a lot of metrics that match this.

435
00:26:54,815 --> 00:27:00,905
Essentially, based on all of this,
you are able to get for each part you

436
00:27:00,905 --> 00:27:05,850
can get the status and then you can
get the value, so based on that, the

437
00:27:05,850 --> 00:27:11,590
LLM was able to then go over all of
this results similar to the first one.

438
00:27:11,860 --> 00:27:17,290
He was able to group them appropriately
and then also identify the outlier

439
00:27:17,590 --> 00:27:18,970
in the list after the group.

440
00:27:19,120 --> 00:27:21,940
And as such was able to figure
out that compared to the other

441
00:27:21,940 --> 00:27:27,550
guys, we can see that the slash
a p slash par has 500 arrows.

442
00:27:27,835 --> 00:27:32,485
More, and then the viral latency
as well is higher compared

443
00:27:32,485 --> 00:27:33,685
to the rest of the parts.

444
00:27:33,805 --> 00:27:41,525
So this shows basically how by changing
the LLM model that is used, using a

445
00:27:41,525 --> 00:27:46,865
more powerful LM model, you're able to
get a better result for queries that,

446
00:27:47,585 --> 00:27:52,805
or in cases where the simpler models
were unable to write valid queries and

447
00:27:52,805 --> 00:27:55,715
compare the results that was written.

448
00:27:56,310 --> 00:28:00,300
So moving on to the fourth section where
we basically just discuss a summary

449
00:28:00,300 --> 00:28:04,030
of everything that we have learned
from interacting with Prompt Chat.

450
00:28:04,400 --> 00:28:09,960
So this is the first part basically
talks about what are the lessons

451
00:28:09,960 --> 00:28:11,580
that we have learned, right?

452
00:28:11,580 --> 00:28:14,820
You would see throughout the,
from the architecture and the rest

453
00:28:14,820 --> 00:28:16,470
of the presentation, there's no.

454
00:28:17,220 --> 00:28:21,240
Not at any point did we attempt
to retrain any of the models

455
00:28:21,240 --> 00:28:22,950
or do any sort of fine tuning.

456
00:28:22,950 --> 00:28:27,540
So what that lets us know is that
I'm sure that they are LLM models

457
00:28:27,540 --> 00:28:32,310
today are actually capable of writing
prompt QL queries on their own.

458
00:28:33,570 --> 00:28:39,240
The other thing to note as well is that,
the only change we had to do actually

459
00:28:39,240 --> 00:28:44,580
was to use one shot prompting, which is
basically adding an example to ensure

460
00:28:44,580 --> 00:28:49,710
that the output that we get from the
LLM is formatted exactly how we want it.

461
00:28:50,040 --> 00:28:54,660
And that's important because you
initially were on IT project.

462
00:28:54,720 --> 00:28:59,250
We are getting into issues where
the response coming from the LLM

463
00:28:59,250 --> 00:29:02,730
model, it would add additional
like characters or talking to it.

464
00:29:02,730 --> 00:29:04,890
And then once you pass it to.

465
00:29:05,535 --> 00:29:08,795
The Prometheus instance,
it won't be valid.

466
00:29:09,845 --> 00:29:13,745
It would no longer be valid from
cure, and that will lead to like

467
00:29:13,745 --> 00:29:17,915
crashes or issues because Prometheus
cannot interpret the query.

468
00:29:18,275 --> 00:29:23,075
But after using one shot prompting
where we are basically able to show.

469
00:29:23,435 --> 00:29:26,135
DNLM exactly the format of the response.

470
00:29:26,435 --> 00:29:31,415
He started returning exactly just
the prompt QR required without any

471
00:29:31,415 --> 00:29:35,915
additional characters or tokens
around it, and that we basically were

472
00:29:35,915 --> 00:29:38,015
able to avoid having to manually.

473
00:29:38,505 --> 00:29:42,975
Try to extract out the prom
cure out of the LLM response.

474
00:29:43,255 --> 00:29:48,135
Based on the last example that you see
as well would realize that obviously the

475
00:29:48,135 --> 00:29:52,185
thinking models are better when it comes
to trying to write complicated ProQ.

476
00:29:52,515 --> 00:29:55,155
So the lighter models work.

477
00:29:55,515 --> 00:29:57,105
For most cases as well.

478
00:29:57,165 --> 00:30:01,545
But when you want to do, ask more
completed questions, it's more useful to,

479
00:30:01,725 --> 00:30:05,835
the more powerful the models are and the
more time you spend thinking, the better

480
00:30:06,165 --> 00:30:08,835
the prompt cure that they write as well.

481
00:30:08,835 --> 00:30:11,805
So we've seen that again
in the last example.

482
00:30:12,085 --> 00:30:16,345
So some of the limitations that we are
so observed in the course of working on

483
00:30:16,345 --> 00:30:18,855
this project is that as you would see.

484
00:30:19,255 --> 00:30:23,365
The majority of being able
to pull this off depends on

485
00:30:23,365 --> 00:30:26,245
the quality of documentation
that you add to your metadata.

486
00:30:26,875 --> 00:30:31,710
So Promeus would always have the metadata,
API available and it will tell you,

487
00:30:31,715 --> 00:30:33,775
okay, these are the metrics that I have.

488
00:30:34,165 --> 00:30:37,765
And then these are the
types of those metrics.

489
00:30:37,765 --> 00:30:41,965
But if you don't add any help or
information to interpret, for example,

490
00:30:41,965 --> 00:30:45,265
if, look at the case of when we had demo.

491
00:30:46,165 --> 00:30:49,085
Service is today all data metric.

492
00:30:49,175 --> 00:30:54,935
If the documentation do not contain
information, saying A one means today's

493
00:30:54,935 --> 00:31:00,155
holiday, zero means today's an all day,
then there would have not been any way for

494
00:31:00,215 --> 00:31:05,730
premature for the L to interpret correctly
the premature response that I got.

495
00:31:06,155 --> 00:31:10,295
The other challenging bit as
well is even for cases where

496
00:31:10,295 --> 00:31:13,175
you have the documentation.

497
00:31:13,710 --> 00:31:16,770
All the, all partex included
as part of the documentation.

498
00:31:16,770 --> 00:31:18,630
Most of the time the labels are missing.

499
00:31:18,900 --> 00:31:23,220
Labels in this case are like, if you
compare it to traditional databases,

500
00:31:23,250 --> 00:31:26,490
so maybe like the colons or the fields.

501
00:31:26,490 --> 00:31:31,680
So because you don't know what fields
or what labels are available in your,

502
00:31:32,280 --> 00:31:38,160
in that particular matrix, it makes it
harder for DLLM to be able to write.

503
00:31:38,580 --> 00:31:41,730
Correct queries, especially when
you need to filter by things

504
00:31:42,120 --> 00:31:44,790
like the actual bill value.

505
00:31:45,280 --> 00:31:48,250
So that's one of the things where,
again, that can easily be solved

506
00:31:48,460 --> 00:31:52,540
as is essentially going over the
documentation and adding as much

507
00:31:52,540 --> 00:31:54,970
useful information there as possible.

508
00:31:55,300 --> 00:32:00,100
The other limitation notice as well is
that sometimes you might feel you might

509
00:32:00,160 --> 00:32:04,785
having consistency in the results that
you get because the query is generated.

510
00:32:05,440 --> 00:32:06,490
Differ define slightly.

511
00:32:06,760 --> 00:32:10,960
And that's important because if
you frame your question slightly

512
00:32:10,960 --> 00:32:15,310
differently, then the LLM can
interpret it differently and generate

513
00:32:15,400 --> 00:32:19,120
a correspondent different query, which
would then give you a different result.

514
00:32:19,450 --> 00:32:25,650
But that can be eliminated by having
more exact descriptions in your question.

515
00:32:25,650 --> 00:32:31,590
So for example, you're saying a, is
there any endpoint currently between 500?

516
00:32:31,910 --> 00:32:34,640
If you don't put, a timeframe
the first time, maybe you might

517
00:32:34,640 --> 00:32:38,270
do it over an hour or like maybe
five minutes or maybe one minute.

518
00:32:38,540 --> 00:32:42,380
But if for example, you ask specifically
with the timeframe, then generated

519
00:32:43,070 --> 00:32:47,600
prompt cure by the AI agents would
contain that exact timeframe and

520
00:32:47,600 --> 00:32:49,340
then you get the same response back.

521
00:32:49,680 --> 00:32:51,120
That's another limitation.

522
00:32:51,120 --> 00:32:54,660
But again, how that can be improved
upon is basically just puts in.

523
00:32:55,285 --> 00:33:00,235
More, the more exact your question is,
the better the answers that you get.

524
00:33:00,235 --> 00:33:04,865
So in terms of the future improvement
so better support for complex queries.

525
00:33:05,085 --> 00:33:10,005
So this better support includes things
like being able to undo more completed

526
00:33:10,005 --> 00:33:13,395
queries, being able to even supply.

527
00:33:14,075 --> 00:33:18,245
Right now all the answers are coming
back in texts in natural language, but of

528
00:33:18,245 --> 00:33:22,055
course it might be useful to maybe have
a graph to look at from time to time.

529
00:33:22,355 --> 00:33:26,795
So for more competitive queries, for
example, it might be useful to return

530
00:33:26,795 --> 00:33:31,355
both the natural language, but also
some form of visualization for it.

531
00:33:31,775 --> 00:33:36,005
Also, right now the project is
limited to just Prometheus, so

532
00:33:36,005 --> 00:33:37,625
that's the only matrix source that.

533
00:33:38,360 --> 00:33:39,140
It works with.

534
00:33:39,140 --> 00:33:42,560
So in terms of next steps, you're
looking at expanding the project

535
00:33:42,560 --> 00:33:46,430
such that it supports more than
just Prometheus as the source.

536
00:33:46,640 --> 00:33:50,420
And then lastly, I think another
interesting improvement would

537
00:33:50,420 --> 00:33:53,800
be around the system being able
to learn from user interactions.

538
00:33:53,800 --> 00:33:59,350
So imagine you ask a question and maybe
for example, the, you didn't use the

539
00:33:59,350 --> 00:34:04,555
right labels, you and you, or it doesn't
know the labels are available for that.

540
00:34:05,565 --> 00:34:06,465
Particular metrics.

541
00:34:06,465 --> 00:34:09,675
So he asks that, okay, I
can answer this, right?

542
00:34:09,675 --> 00:34:11,565
And then you provide those labels.

543
00:34:11,835 --> 00:34:17,115
Now it would be useful if the, right now
there is no memory in the system, so he

544
00:34:17,115 --> 00:34:19,095
actually doesn't store the information.

545
00:34:19,395 --> 00:34:22,905
So an extension would be such that
next time you actually don't have to

546
00:34:22,905 --> 00:34:28,280
go back and get supply the same labels
back to the system for you to answer.

547
00:34:29,000 --> 00:34:29,720
Correctly.

548
00:34:30,030 --> 00:34:33,720
So that's like a future improvement
when it comes to, it's actually

549
00:34:33,720 --> 00:34:35,280
learning from user interaction.

550
00:34:35,490 --> 00:34:40,730
Another way that can go is when queries,
for example, let's say the queries

551
00:34:40,730 --> 00:34:45,955
are wrong or like the wrong metrics
was used and you correct it again,

552
00:34:46,415 --> 00:34:50,835
all those kind of interactions can be
stored such that his user's context.

553
00:34:51,065 --> 00:34:55,500
Next time he's trying to answer
questions, and as such, he can make use.

554
00:34:55,905 --> 00:34:59,265
Of that, and then the system gets
better over time because it's

555
00:34:59,265 --> 00:35:01,245
learning from user interactions.

556
00:35:01,615 --> 00:35:03,685
So lastly, how can you
contribute and join?

557
00:35:03,685 --> 00:35:07,569
The source code is available
on our GitHub profile.

558
00:35:07,569 --> 00:35:09,369
So it's open the projects open source.

559
00:35:09,369 --> 00:35:14,539
If you've go to next I HQ on GitHub you
see the source code for the prom chat app.

560
00:35:14,899 --> 00:35:21,499
So issues pls are welcomed to, as a
means of contributing to the project.

561
00:35:21,744 --> 00:35:26,364
Also the web interface that I was playing
with or that I was shown in the sites

562
00:35:26,424 --> 00:35:33,124
is available from chat do co so you
can basically visit that is available.

563
00:35:33,124 --> 00:35:36,484
You don't need like an
identification payment or anything.

564
00:35:36,484 --> 00:35:41,294
The only issues might be because
it's using my own personal, aPI key.

565
00:35:41,294 --> 00:35:45,534
So there are limits to the number of
like daily requests or sometimes maybe

566
00:35:45,534 --> 00:35:48,564
even a lot of people have been playing
around with it earlier in the day.

567
00:35:48,874 --> 00:35:54,344
You might get, you might not be
able to get any responses back

568
00:35:54,344 --> 00:35:58,474
from the API it would tell you
that the LLM credits is exhausted.

569
00:35:58,874 --> 00:35:59,324
But yeah.

570
00:35:59,774 --> 00:36:01,844
But you can clone the project locally.

571
00:36:01,844 --> 00:36:03,704
Put your own EPI keys and run it if.

572
00:36:04,774 --> 00:36:08,459
You want to, or if you want to use
the web interface, you can visit from

573
00:36:08,459 --> 00:36:13,409
chat, do nest slide.co as well, and
you would be able to interact with it.

574
00:36:13,989 --> 00:36:15,009
So that's it.

575
00:36:15,939 --> 00:36:16,899
That's it from me.

576
00:36:16,899 --> 00:36:19,509
So thank you very much for
listening to the session.

577
00:36:19,869 --> 00:36:24,129
I hope you've learned a lot and
you have a better understanding.

578
00:36:24,819 --> 00:36:27,159
As regards how to implement
something like this.

579
00:36:27,399 --> 00:36:31,569
And yes, the answer to the
question is it is possible to

580
00:36:31,569 --> 00:36:33,549
chat with your mandatory metrics.

581
00:36:33,789 --> 00:36:38,439
And I do hope you enjoy the rest
of the conference, but I thank you.

582
00:36:40,214 --> 00:36:40,334
I.

