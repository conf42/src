1
00:00:00,600 --> 00:00:05,580
I'm Krista Barde and I'm excited to be
here today to talk about how we transform

2
00:00:06,120 --> 00:00:11,630
our PLM product Life settlement Systems
using GO programming language, modern

3
00:00:11,630 --> 00:00:17,570
product development, demand a lot from PLM
systems, the need to handle massive amount

4
00:00:17,570 --> 00:00:20,210
of data and provide insights in real time.

5
00:00:20,930 --> 00:00:21,935
Our journey volution.

6
00:00:22,435 --> 00:00:25,975
Concurrency and a
microservices architecture.

7
00:00:26,515 --> 00:00:31,165
In this presentation, I'll walk
you through how we built a system

8
00:00:31,795 --> 00:00:37,345
that processes or hundred thousand
product data points for second,

9
00:00:37,645 --> 00:00:39,145
across the distributed services.

10
00:00:39,645 --> 00:00:42,985
This resulted induction in
product launch timelines.

11
00:00:43,485 --> 00:00:43,905
Processing.

12
00:00:44,745 --> 00:00:49,725
We'll cover everything from
the custom schedulers, circuit

13
00:00:49,725 --> 00:00:53,925
breakers to memory optimization
and high throughput pipelines.

14
00:00:54,615 --> 00:00:59,640
I'll share the architectural
decisions that helped us transform

15
00:00:59,640 --> 00:01:04,120
our PL infrastructure into a high
performance AI driven platform.

16
00:01:04,620 --> 00:01:09,725
So let's start by understanding
the challenges that we, as you

17
00:01:09,725 --> 00:01:15,435
can see here, we dealing with some
real of all massive data volume.

18
00:01:15,935 --> 00:01:22,385
PLM Systems struggle with the sheer
volume of product data generated in

19
00:01:22,625 --> 00:01:24,755
today's development environments.

20
00:01:25,445 --> 00:01:32,315
Think about ca files, specifications, test
reserves, and a regulated documentation.

21
00:01:32,675 --> 00:01:35,285
It creates a huge processing demand.

22
00:01:35,785 --> 00:01:42,445
Legacy PLM architecture often
rely on monolithic databases and

23
00:01:42,745 --> 00:01:44,575
sequential processing pipelines.

24
00:01:45,265 --> 00:01:50,485
These simply cannot scale to meet
the demands of increased product

25
00:01:50,485 --> 00:01:53,335
complexity and data requirements.

26
00:01:53,335 --> 00:01:54,075
And third.

27
00:01:54,480 --> 00:01:59,700
The cross team communication intermission
silos between different teams.

28
00:01:59,700 --> 00:02:05,070
Engineering, manufacturing, quality
causes significant dealer critical data

29
00:02:05,610 --> 00:02:11,940
is often trapped in dispar systems and
formats, making it more and more difficult

30
00:02:12,240 --> 00:02:15,030
to share and collaborate effectively.

31
00:02:15,530 --> 00:02:19,900
To address these challeng,
we built a engine met.

32
00:02:20,400 --> 00:02:23,640
This engine is really the found.

33
00:02:24,140 --> 00:02:29,330
Injection and transformation
that feeds all our systems.

34
00:02:30,200 --> 00:02:31,760
It has three key components.

35
00:02:32,480 --> 00:02:39,710
First is the MET extraction,
specialized parcel converts proprietary

36
00:02:39,710 --> 00:02:42,050
CAD format into standardized data.

37
00:02:42,550 --> 00:02:48,490
For seamless integration with analysis
systems and visualization tools.

38
00:02:49,330 --> 00:02:51,460
Secondly, the parallel processing.

39
00:02:52,120 --> 00:02:57,310
We use go routines to process different
sections of complex product hierarchies.

40
00:02:58,090 --> 00:03:00,700
At the same time, also design.

41
00:03:00,950 --> 00:03:04,790
of primitives to maintain the consistency.

42
00:03:05,780 --> 00:03:09,020
And the third one is zero strategies.

43
00:03:09,020 --> 00:03:13,310
That is we implemented a custom
memory management to process

44
00:03:13,610 --> 00:03:17,900
data without allocating new
objects for each transformation.

45
00:03:18,400 --> 00:03:22,240
This significantly reduced the
garbage collection pressure

46
00:03:22,240 --> 00:03:25,360
and improved throughput by 85%.

47
00:03:25,860 --> 00:03:26,190
Now.

48
00:03:26,550 --> 00:03:30,810
Next, let's talk about our
event driven notification

49
00:03:30,810 --> 00:03:33,740
system built with Go Channels.

50
00:03:34,360 --> 00:03:37,960
this system significantly improved
the cross trend communication.

51
00:03:38,680 --> 00:03:41,830
It works through four main steps.

52
00:03:42,175 --> 00:03:43,125
First one is.

53
00:03:43,625 --> 00:03:50,255
Significant data change generates events
to a non-blocking channel based system.

54
00:03:50,765 --> 00:03:56,260
This ensures that the producers never
have to wait for consumer processing.

55
00:03:56,640 --> 00:04:04,900
Second intelligence routing, central
dispatcher route event based on payload.

56
00:04:05,400 --> 00:04:07,680
That enables robust
acknowledgement mechanism.

57
00:04:08,310 --> 00:04:12,990
And the last one, the final one
is aggregated event streams.

58
00:04:13,170 --> 00:04:15,959
The real time, the real feed, dashboards.

59
00:04:16,459 --> 00:04:21,200
This provides immediate
visibility into process status.

60
00:04:21,709 --> 00:04:26,600
It reduces the cross team
communication deal by 45%.

61
00:04:27,100 --> 00:04:31,450
Our performance, we implemented a
distributed caching architecture.

62
00:04:31,990 --> 00:04:34,540
This system uses multi tier structure.

63
00:04:34,960 --> 00:04:41,015
We use locality cache backed by
distributed radius clusters, both

64
00:04:41,575 --> 00:04:48,930
efficient serialization enabled 80%
faster data access across our services.

65
00:04:49,430 --> 00:04:55,084
We prefetching machine learning model
predicts likely data access patterns.

66
00:04:55,654 --> 00:05:01,059
It triggers background prefetching
operation that populate the C

67
00:05:01,759 --> 00:05:03,819
before request even arrives.

68
00:05:04,319 --> 00:05:07,559
To maintain data integrity,
we have cash invalidation.

69
00:05:08,159 --> 00:05:13,799
A sophisticated, published subscribed
mechanism that ensures cash

70
00:05:13,799 --> 00:05:18,770
coherence across the distributed
notes at operations, prevent risk

71
00:05:18,770 --> 00:05:20,060
conditions during the updates.

72
00:05:20,840 --> 00:05:23,755
And finally, the system uses sizing.

73
00:05:24,255 --> 00:05:25,995
Location between the.

74
00:05:26,495 --> 00:05:31,849
Now, let's go to integrating
AI into our PLM systems.

75
00:05:32,179 --> 00:05:37,269
It required advanced currency patterns,
GoPro to be very helpful here.

76
00:05:37,929 --> 00:05:39,879
We used custom sync primitives.

77
00:05:40,209 --> 00:05:45,279
We developed specialized synchronized
primitives that called it ai, AI pipeline

78
00:05:45,279 --> 00:05:47,919
execution across distributed services.

79
00:05:48,419 --> 00:05:54,059
While maintaining data consistency
throughout the locking mechanism, we

80
00:05:54,059 --> 00:05:59,089
also implemented efficient memory pools,
custom memory pools to be allocated

81
00:05:59,089 --> 00:06:02,299
buffers for large scale data processing.

82
00:06:02,989 --> 00:06:07,424
It's significantly reduced garbage
collection and enabled a stable

83
00:06:07,424 --> 00:06:10,309
performance even under the peak.

84
00:06:10,809 --> 00:06:16,979
Or lock free data structures, implementing
atomic operations and, carefully

85
00:06:17,009 --> 00:06:19,349
designed lock free data structures.

86
00:06:19,979 --> 00:06:24,989
It enabled us the concurrent access
patterns and linear scalability.

87
00:06:25,949 --> 00:06:29,009
These patterns were crucial for
integrating machine learning.

88
00:06:29,509 --> 00:06:33,754
Now let's talk about the G ps.
Efficient communication between

89
00:06:33,754 --> 00:06:35,614
our microservices is critical.

90
00:06:36,334 --> 00:06:41,464
We used GPRS, sorry, GRPS for
high performance communication.

91
00:06:42,004 --> 00:06:45,924
Our GRPS implementation is
based on, service definitions.

92
00:06:46,344 --> 00:06:49,559
We use strong type protocol.

93
00:06:50,059 --> 00:06:53,569
Node generation directly from
those interface definitions.

94
00:06:54,379 --> 00:07:00,369
GR PS supports bidirectional
streaming, enabling the efficient data

95
00:07:00,369 --> 00:07:03,609
transfer with multiplex connection.

96
00:07:03,969 --> 00:07:06,760
It also provides client
side load balancing.

97
00:07:07,239 --> 00:07:15,170
And finally it has performance monitoring
with detailed metrics on, Let's talk the.

98
00:07:15,420 --> 00:07:19,230
that we implemented, for the
performance optimization.

99
00:07:19,830 --> 00:07:21,930
The first one is GC tuning.

100
00:07:22,350 --> 00:07:28,810
Then the, second one is the back
pressure mechanism, the flow.

101
00:07:29,275 --> 00:07:31,075
That prevented the system overload.

102
00:07:31,645 --> 00:07:38,185
And then the last one is the real
time monitoring we use for continuous

103
00:07:38,185 --> 00:07:43,725
profiling and to enable the, automatic
resource allocation, our custom

104
00:07:43,725 --> 00:07:45,585
monitoring system, leverage goes.

105
00:07:46,445 --> 00:07:53,165
Built in profiling tools to provide
realtime insight into the system and

106
00:07:53,405 --> 00:07:57,485
it, I just set the resource location
to maintain the optimal response

107
00:07:57,485 --> 00:07:59,135
times across all the services.

108
00:07:59,635 --> 00:08:04,505
The next one here is the, realtime
M transformation examples.

109
00:08:05,325 --> 00:08:06,855
A validation.

110
00:08:07,355 --> 00:08:11,525
To identify and resolve the issues much
earlier in the development process.

111
00:08:12,025 --> 00:08:17,305
The integration of machine learning
model with our GO services enable the

112
00:08:17,305 --> 00:08:21,595
predictive maintenance capabilities
and reduce the system performance

113
00:08:21,595 --> 00:08:24,835
by our system downtown by 60%.

114
00:08:25,090 --> 00:08:29,185
Even driven notification
system, the communication.

115
00:08:29,685 --> 00:08:34,095
And now, the most important thing
is Robert, the robust security

116
00:08:34,095 --> 00:08:39,285
and scalability architecture for
authentication and authorization.

117
00:08:39,285 --> 00:08:44,850
We implemented a service to service
authentication using our TLS.

118
00:08:45,695 --> 00:08:53,515
We use Rate Ling and, we used various
data part, data parting approaches here.

119
00:08:54,015 --> 00:09:01,010
So let's recap the key takeaways, from
the, our business impact was significant

120
00:09:01,460 --> 00:09:04,100
percent faster product launch, 65%.

121
00:09:04,600 --> 00:09:08,260
Technology implementation was
centered around go microservices,

122
00:09:09,130 --> 00:09:13,930
AI integration, and advanced
congruency architecture principles.

123
00:09:13,930 --> 00:09:19,800
Were focusing on event design, distributed
caching, and efficient communication, and

124
00:09:19,800 --> 00:09:22,770
our journey demonstrates goals elegant.

125
00:09:23,270 --> 00:09:25,220
A foundation for.

