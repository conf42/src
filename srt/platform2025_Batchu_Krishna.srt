1
00:00:00,500 --> 00:00:00,680
Hi.

2
00:00:01,040 --> 00:00:01,910
Good morning everyone.

3
00:00:02,240 --> 00:00:02,750
Good evening.

4
00:00:03,410 --> 00:00:04,970
I'm Christian Chu.

5
00:00:05,000 --> 00:00:10,030
I have 20 years of experience in it,
so most of the, my experience belongs

6
00:00:10,030 --> 00:00:12,420
to data migrations and data handling.

7
00:00:13,260 --> 00:00:17,695
And I different have plenty of
expertise in different areas like

8
00:00:18,295 --> 00:00:23,065
financial, automobile, energy,
healthcare, different platforms.

9
00:00:23,065 --> 00:00:23,700
I worked on it.

10
00:00:24,355 --> 00:00:30,135
I have gained a lot of good experience
on data migrations and data handling

11
00:00:30,135 --> 00:00:35,074
projects like heterogeneous data and very
large quantity of data used to handle.

12
00:00:35,614 --> 00:00:40,974
And I work with the data governance and
it supports and controls and everything.

13
00:00:40,979 --> 00:00:41,550
I worked on it.

14
00:00:41,550 --> 00:00:47,870
They like ET tools I worked on now
data stage, Informatica, SSIS, talent,

15
00:00:48,370 --> 00:00:52,149
these, the tools, which they are
developer and new to me is to use.

16
00:00:52,329 --> 00:00:53,679
And I used those.

17
00:00:53,729 --> 00:00:57,724
I do have good knowledge on those
tools and what are the data?

18
00:00:57,884 --> 00:01:01,579
I work on it so generated by the tools
and as per the business requirements.

19
00:01:02,529 --> 00:01:04,709
So I handle a lot of big teams.

20
00:01:04,709 --> 00:01:07,759
Like I used to manage the
team around the 12 people on

21
00:01:07,759 --> 00:01:09,679
onshore and put people offshore.

22
00:01:10,169 --> 00:01:11,909
We have a onshore offshore model.

23
00:01:12,359 --> 00:01:15,899
And so most of the area I used
to work on data handling side

24
00:01:15,909 --> 00:01:21,619
ETLE projects, most of things and
that too in migration and banking

25
00:01:21,619 --> 00:01:25,169
projects like JP Morgan and US Bank.

26
00:01:25,799 --> 00:01:27,639
Those banking side, I work around it.

27
00:01:27,699 --> 00:01:32,759
Most of these migration projects like
MSP two BS promotions and those areas

28
00:01:33,449 --> 00:01:35,979
and coming back to investment sites.

29
00:01:35,979 --> 00:01:39,579
I work with the, currently I'm working
with Fidelity Investments that,

30
00:01:39,999 --> 00:01:44,359
where it's also cloud-based creation
project, which I'm handling right now.

31
00:01:44,719 --> 00:01:46,729
So it's dealing with all four Okay.

32
00:01:46,729 --> 00:01:47,809
Plans and those stuff.

33
00:01:48,444 --> 00:01:48,864
Okay.

34
00:01:49,554 --> 00:01:56,254
And in my career I used different
BM databases like sql Teradata, DB

35
00:01:56,254 --> 00:02:03,354
two and Cloud basis databases like
Databricks Snowflakes and Azure sql.

36
00:02:03,714 --> 00:02:06,384
So these are areas I
work on different tools.

37
00:02:06,804 --> 00:02:06,909
I work on it.

38
00:02:07,409 --> 00:02:12,329
Most of the tools I used to upgrade myself
with new technologies and to furnish

39
00:02:12,329 --> 00:02:14,179
the new requirement to the projects.

40
00:02:15,139 --> 00:02:16,879
And beside that is automation side.

41
00:02:16,999 --> 00:02:21,819
I use the PI spark to get this
good quality of data by using the

42
00:02:21,819 --> 00:02:27,970
PI spark automation tools and set
the turn project I'm using and jc.

43
00:02:28,390 --> 00:02:31,029
So with those two also,
we are doing automation.

44
00:02:31,075 --> 00:02:32,875
Where we can do the declaration.

45
00:02:32,975 --> 00:02:35,885
There are a lot of validations out
there, so many checks out there.

46
00:02:36,365 --> 00:02:40,625
So on capturing screenshots and
validating those logs and everything

47
00:02:40,625 --> 00:02:43,915
is done by the JC automation
tool means automation script.

48
00:02:44,415 --> 00:02:44,715
Yeah.

49
00:02:44,715 --> 00:02:47,015
And beside that I work around on data.

50
00:02:47,015 --> 00:02:47,645
As you mentioned.

51
00:02:47,645 --> 00:02:49,895
I work on lot of times and cloud outside.

52
00:02:50,195 --> 00:02:52,775
So here I'm going to give a presentation.

53
00:02:53,275 --> 00:02:58,285
Building self-service, data platform and
engineering schedule infrastructure for

54
00:02:58,845 --> 00:03:01,575
developers, our companies and those stuff.

55
00:03:02,144 --> 00:03:05,705
And coming to the outline of this
project is the evaluation of platform

56
00:03:05,705 --> 00:03:09,545
engineering has fundamentally
transformed how organization of

57
00:03:09,545 --> 00:03:10,985
projects, data infrastructure.

58
00:03:11,940 --> 00:03:15,765
Slope step, how does infrastructure
is developing in that, in

59
00:03:15,859 --> 00:03:17,299
terms of data approaches?

60
00:03:17,400 --> 00:03:21,280
So day by day there are new challenges
new challenges we have to face and we need

61
00:03:21,280 --> 00:03:25,990
to know, incorporate those new challenges
and we need to come up some solutions.

62
00:03:25,990 --> 00:03:29,800
So to do that, we have good
infrastructure on that.

63
00:03:30,415 --> 00:03:34,915
And the traditional ETL operations
are being reimagined and self-service

64
00:03:34,915 --> 00:03:39,085
platforms that empower, develop developers
while maintaining the enterprise

65
00:03:39,085 --> 00:03:40,915
grade reliability and governances.

66
00:03:41,515 --> 00:03:41,725
Yeah.

67
00:03:42,260 --> 00:03:45,770
And the same time, we need to maintain
the traditional et operations as

68
00:03:45,770 --> 00:03:50,065
well being we need to take as a
foundation of those and we need to.

69
00:03:50,675 --> 00:03:54,225
Based we need to build a new
infrastructure and new ideologies

70
00:03:54,375 --> 00:03:57,705
and we need to transform from the
traditional ETL new ETL cloud-based

71
00:03:57,705 --> 00:03:59,875
or self-service data platforms.

72
00:04:00,375 --> 00:04:04,060
Yeah, this is, this slide tells you
about the platform engineering ERs.

73
00:04:04,560 --> 00:04:08,250
Like product oriented thinking
and developer integration and

74
00:04:08,250 --> 00:04:09,899
balancing cable and complexity.

75
00:04:10,399 --> 00:04:12,739
The first point is product
orientation training.

76
00:04:13,039 --> 00:04:16,880
The platform engineering represents a
chip from the traditional infrastructure

77
00:04:16,880 --> 00:04:18,529
management to the product oriented.

78
00:04:18,529 --> 00:04:21,260
Thinking about internal capabilities.

79
00:04:21,965 --> 00:04:24,415
And and the second one,
develop integration.

80
00:04:24,415 --> 00:04:29,965
The most effective data platform feels
natural to application and developers and

81
00:04:29,965 --> 00:04:35,545
integrating with familiar CACD and process
and established pattern sec, continuous

82
00:04:35,545 --> 00:04:37,375
integration and condition deployment.

83
00:04:37,375 --> 00:04:40,825
Third process will be there whenever
the development is done with a

84
00:04:40,825 --> 00:04:42,515
code deployment and code built.

85
00:04:43,150 --> 00:04:48,620
They'll push this code into the CACD
pipelines where it'll take auto it'll go

86
00:04:48,620 --> 00:04:52,930
for the pull request and where we then
the code will apply into the code and

87
00:04:52,930 --> 00:04:55,125
then seamlessly the process will develop.

88
00:04:55,625 --> 00:05:00,594
Coming to the other point is balancing
complexity and it's modern platform must

89
00:05:00,594 --> 00:05:04,705
be sophisticated enough to enterprise
scale processing while reminding simple

90
00:05:04,705 --> 00:05:06,775
enough to general purpose developers.

91
00:05:07,275 --> 00:05:11,344
And see it is modern data
platform is handing it, the

92
00:05:11,344 --> 00:05:13,594
database, huge data is coming out.

93
00:05:14,074 --> 00:05:18,244
So like more transactions, more
legacy data that the tool that

94
00:05:18,814 --> 00:05:23,114
the selfless server infrastructure
should be sophisticated enough

95
00:05:23,114 --> 00:05:24,944
too, handle those situations.

96
00:05:24,974 --> 00:05:27,854
First thing, while reminding is
simple enough to general purpose

97
00:05:27,854 --> 00:05:31,254
of develop this transformation
is from solid data engineering to

98
00:05:31,254 --> 00:05:34,554
platform driven approaches reflects.

99
00:05:35,054 --> 00:05:38,534
Border industry trends towards
DevOp integration and self

100
00:05:38,744 --> 00:05:42,614
service capabilities, eliminating
traditional handoff between teams.

101
00:05:43,454 --> 00:05:49,094
So by doing, once we did the automation
of data moving, or it's a CS, CD and other

102
00:05:49,094 --> 00:05:55,454
stuff, so as of now, we are depending in
each team, like hand up between the teams

103
00:05:55,499 --> 00:05:58,664
one, like TLC one, LC in the process.

104
00:05:58,664 --> 00:06:01,064
Once we one step is done,
we'll go to the second step.

105
00:06:01,064 --> 00:06:02,624
The parallel also, people work.

106
00:06:03,029 --> 00:06:07,789
But going forward by using this
list transformation to the latest

107
00:06:07,839 --> 00:06:09,744
technologies or data handling things.

108
00:06:09,984 --> 00:06:13,374
So those kind of dependencies we
are going to eliminate actually in

109
00:06:13,374 --> 00:06:18,384
between the teams dependencies, so
the architectural foundations for TL.

110
00:06:18,954 --> 00:06:22,164
So what could be the
architecture foundations for

111
00:06:22,164 --> 00:06:24,084
extract transformation loading.

112
00:06:24,459 --> 00:06:24,849
Okay.

113
00:06:25,299 --> 00:06:28,599
Building effective self-service
data platform requires architecture

114
00:06:28,599 --> 00:06:32,019
platform that accommodate both current
requirements and feature growth.

115
00:06:32,529 --> 00:06:36,129
The m and the foundation
typically centers on below points.

116
00:06:36,489 --> 00:06:40,624
So first thing is that building
effect to self service data platforms.

117
00:06:40,624 --> 00:06:45,094
It should handle the current requirement
as well as the previous requirements, sir.

118
00:06:45,814 --> 00:06:51,014
So migrating the legacy data or current
handling the current transactions that.

119
00:06:51,499 --> 00:06:53,799
It should good enough to
handle the previous answer

120
00:06:53,859 --> 00:06:55,349
feature requirement as well.

121
00:06:55,859 --> 00:06:59,030
The foundation typically is
microservices architecture that

122
00:06:59,030 --> 00:07:05,030
decompose the data processor into
discrete composed components and

123
00:07:05,030 --> 00:07:09,700
containers orchestration platform
like Bernet, providing the Runtime

124
00:07:09,700 --> 00:07:15,070
Foundation and carefully selected data
processing engines like Apache Airflow.

125
00:07:15,490 --> 00:07:20,540
These control schedulers and security
and make sure that you know the usage

126
00:07:20,600 --> 00:07:22,940
of data and security point of view.

127
00:07:23,240 --> 00:07:26,529
Make sure that, we are a
secured security level.

128
00:07:26,529 --> 00:07:29,369
Also, we need to give you a lot
of permissions as to provide

129
00:07:29,369 --> 00:07:31,590
to the customer and people.

130
00:07:32,399 --> 00:07:36,960
The challenges lies in the abstracting
bernet complexity from end users

131
00:07:36,960 --> 00:07:39,269
while processing access to that.

132
00:07:39,705 --> 00:07:43,965
It's the powerful scheduling and
resourcing management capabilities.

133
00:07:43,965 --> 00:07:45,974
Sir, that's the main challenging.

134
00:07:45,974 --> 00:07:51,014
What current industry is facing, starting
Bernet complexity from end user side

135
00:07:51,614 --> 00:07:55,815
and while processing, reserving access
to the powerful scheduling and resource

136
00:07:55,815 --> 00:08:01,114
management capabilities, sir? So if you
give the wrong wrong means the person who

137
00:08:01,114 --> 00:08:05,499
is not into the team or is not part of the
project, if you give the access to him.

138
00:08:05,784 --> 00:08:09,570
Their issues will come at the same
time if you give access to end user.

139
00:08:09,779 --> 00:08:13,420
So there're going to be tremendous
data leakage will be there if you don't

140
00:08:13,420 --> 00:08:15,340
know if you go to the wrong hands.

141
00:08:15,609 --> 00:08:19,089
There is a issues will be there and
there's more come, comes under the

142
00:08:19,089 --> 00:08:20,669
resource management capabilities.

143
00:08:21,169 --> 00:08:25,709
And here this slide talks about
metadata driven pipeline architecture.

144
00:08:26,284 --> 00:08:30,604
So the pipeline architecture means
before going insert slide, as I would

145
00:08:30,604 --> 00:08:32,704
like to give you I insights of this.

146
00:08:32,704 --> 00:08:33,544
So pipeline.

147
00:08:33,724 --> 00:08:36,904
So how that design, the Azure
pipeline is designed the, my

148
00:08:36,934 --> 00:08:40,384
previous projects, so there have
the data from the different sources.

149
00:08:41,329 --> 00:08:44,979
They'll pull it to the, and
they'll put it to the landing

150
00:08:44,979 --> 00:08:49,149
zone, landing two integration
zone integration to the target.

151
00:08:49,779 --> 00:08:53,829
So for each from sources to,
they have their one pipeline

152
00:08:54,339 --> 00:08:56,009
and the landing two integrate.

153
00:08:56,109 --> 00:08:57,804
Raw two landing zone, they have one.

154
00:08:58,304 --> 00:09:01,544
And planning to integration tool
curated and there is a different

155
00:09:01,544 --> 00:09:02,504
pipeline should be there.

156
00:09:02,984 --> 00:09:05,584
So these are all integrated by each level.

157
00:09:06,004 --> 00:09:08,944
So in integration, our planning
to integration, they have some

158
00:09:09,104 --> 00:09:12,434
ETL transactions will be performed
and integration tool curated.

159
00:09:12,434 --> 00:09:15,014
There is different level of
transactions will be performed.

160
00:09:15,614 --> 00:09:17,654
Okay, coming back to this slide.

161
00:09:17,744 --> 00:09:21,854
Metadata, data driven architecture,
the concept of metadata driven,

162
00:09:21,854 --> 00:09:23,474
ETL represents foundation.

163
00:09:23,984 --> 00:09:28,974
Fundamentally shifted from the impressive
programming models to they declared

164
00:09:28,974 --> 00:09:32,484
to approach us that separate business
logic from the implementation details.

165
00:09:32,534 --> 00:09:34,164
So that's what I just now mentioned.

166
00:09:34,194 --> 00:09:38,124
So traditionally we used manually,
we need to load the data and

167
00:09:38,124 --> 00:09:39,904
we need to welfare manually.

168
00:09:39,904 --> 00:09:41,494
We need to place the data.

169
00:09:41,494 --> 00:09:46,594
So then we need to process from the
implementation details, like from

170
00:09:46,594 --> 00:09:50,554
business logics, from implementation
details means like metadata.

171
00:09:51,364 --> 00:09:54,154
And running the interpretations
and optimizations.

172
00:09:54,844 --> 00:09:56,674
So we need to follow these approaches.

173
00:09:57,154 --> 00:09:57,964
Like a structure.

174
00:09:57,964 --> 00:10:01,894
Metadata means that pipelines defined
through the structure metadata describing

175
00:10:01,894 --> 00:10:05,374
the transformation data called the
requirement and operational parameters.

176
00:10:05,374 --> 00:10:10,379
So as mentioned, the data is transformed
one and John two other drone and

177
00:10:11,314 --> 00:10:13,084
the data quality requirements also.

178
00:10:13,324 --> 00:10:16,979
So when you, the site is moving
from the rock integration, the data

179
00:10:16,979 --> 00:10:20,519
quality, also we need to verify
the data quality is coming up.

180
00:10:21,119 --> 00:10:23,039
As per the requirement is coming or not.

181
00:10:23,429 --> 00:10:27,439
These kind of checks we need to
perform and operational parameters.

182
00:10:27,589 --> 00:10:32,444
Also, we need to give proper settings
like if any realtime data is coming out.

183
00:10:32,739 --> 00:10:37,149
So those transactions, how to handle
bulk data handling, and that design

184
00:10:37,329 --> 00:10:40,479
is you need such a way that you
know, if any sharing loads coming.

185
00:10:40,539 --> 00:10:44,089
So you need to a new transformation, new
approaches has to be handled on that.

186
00:10:44,589 --> 00:10:48,854
And runtime, interpretations platform
and the runtime, interpretation

187
00:10:48,854 --> 00:10:52,904
metadata to generate and execute
appropriate programming workflows.

188
00:10:53,864 --> 00:10:58,554
So the runtime interpretation means
generate metadata to generate and execute

189
00:10:58,554 --> 00:11:00,624
the appropriate process workflows.

190
00:11:01,254 --> 00:11:04,554
So I mentioned that the different
pipelines will be there and

191
00:11:04,554 --> 00:11:07,719
we have a airflow deck where
we need to go and trigger it.

192
00:11:08,349 --> 00:11:08,829
Manual.

193
00:11:08,829 --> 00:11:12,039
Also, we can trigger the
pipelines, just this workflows.

194
00:11:12,039 --> 00:11:16,179
Also, we need to, once all
development is done and QA is done,

195
00:11:16,509 --> 00:11:20,829
then make sure that once is moved
to production is appropriately.

196
00:11:21,474 --> 00:11:26,854
Those are workflows running continuously
without any issues, sir. Okay.

197
00:11:27,394 --> 00:11:29,724
Opportunity for the parallel
parallelization coaching

198
00:11:29,724 --> 00:11:31,164
and resource optimization.

199
00:11:31,164 --> 00:11:36,364
Sir. Here if 20 people close are
working, this should not any breakage

200
00:11:36,364 --> 00:11:40,894
or any wall or things like performance
issues did not come and coaching.

201
00:11:41,004 --> 00:11:44,664
Any, process every time we have to
if any newcomers coming we need to

202
00:11:44,664 --> 00:11:48,484
make sure that, documentation is
pro correctly maintained on each and

203
00:11:48,484 --> 00:11:52,264
every step so that if any new changes
comes so we make sure that we take it

204
00:11:52,264 --> 00:11:54,244
from there, wherever the last step.

205
00:11:54,299 --> 00:11:59,219
So that documentation ing we should
maintain and if newcomers are coming,

206
00:11:59,219 --> 00:12:01,059
if new people are joining the team.

207
00:12:01,119 --> 00:12:03,189
So we need to use the coaching on them.

208
00:12:03,894 --> 00:12:06,984
Means we need to mentoring
them and resource optimization.

209
00:12:06,984 --> 00:12:11,034
So make sure that who are using the
project and they should have correct

210
00:12:11,034 --> 00:12:14,924
issues and we should have correct
combination of people who is working

211
00:12:14,924 --> 00:12:16,459
on the access and those stuff.

212
00:12:17,389 --> 00:12:21,499
So this, we need to do the resource
optimizations and make sure that the

213
00:12:21,499 --> 00:12:24,919
current confirmation of the systems
and infrastructure we are using those.

214
00:12:24,919 --> 00:12:25,159
Also.

215
00:12:25,164 --> 00:12:27,994
Also, we need to make sure that it's
part of implement and transformation,

216
00:12:27,994 --> 00:12:29,394
implementation of architecture.

217
00:12:29,894 --> 00:12:34,724
And this approach makes pipelines
more maintainable and less prone to

218
00:12:35,024 --> 00:12:38,414
implementation errors since their
focus on business requirements

219
00:12:38,624 --> 00:12:40,664
rather than the technical details.

220
00:12:41,444 --> 00:12:45,254
So if you maintain all these steps
like structural metadata, runtime,

221
00:12:45,254 --> 00:12:51,224
integration, ation, by doing this more,
there is no issues with the maintenance

222
00:12:51,314 --> 00:12:55,844
and no error errors, no implementation
errors, and only focus on the business

223
00:12:55,844 --> 00:12:57,614
requirements rather than the technical.

224
00:12:58,114 --> 00:13:00,784
Integration with the
CACD and DevOps workflow.

225
00:13:01,084 --> 00:13:05,794
So Mo, most of the time nowadays so
what are the development is done?

226
00:13:06,094 --> 00:13:09,694
So directly straight away is going
for, they're using the CACD integration

227
00:13:09,694 --> 00:13:12,239
tool and DevOps support of devs.

228
00:13:13,199 --> 00:13:16,619
So once a developer extract the
data pipeline, deploy into the

229
00:13:16,619 --> 00:13:20,399
flow, similar patterns, establish
application deployment, including.

230
00:13:20,789 --> 00:13:24,689
Portion control automation
testing and staging deployments.

231
00:13:25,469 --> 00:13:25,889
Road steel.

232
00:13:26,639 --> 00:13:30,209
So by using this is continuous
integration deployment.

233
00:13:30,209 --> 00:13:34,839
We can control this we can do this
like portion controlling and from our

234
00:13:34,839 --> 00:13:38,649
previous version to current version
and latest version, those we can do the

235
00:13:38,649 --> 00:13:40,839
washer controlling automation testing.

236
00:13:41,079 --> 00:13:43,719
So nowadays, no manual
testing is happening.

237
00:13:43,929 --> 00:13:47,229
So most of everywhere,
once the code is deployed.

238
00:13:47,574 --> 00:13:52,304
So we have existing test scenarios and
test code source there where when our

239
00:13:52,304 --> 00:13:56,374
code is available to give the pre request
test pre request test data will be there.

240
00:13:56,864 --> 00:14:01,334
Insert the data and on the test route,
we'll get automation testing will be done.

241
00:14:01,334 --> 00:14:07,654
And based on based scenarios and less
than manual effects and stage deployments,

242
00:14:07,804 --> 00:14:09,994
whenever, a deployment is done.

243
00:14:10,024 --> 00:14:12,724
So they'll wait for approvals
and they'll waiting.

244
00:14:12,754 --> 00:14:18,124
Stating means they waiting for
deployments and means one by one, one

245
00:14:18,124 --> 00:14:19,594
by one, and deployments will happen.

246
00:14:19,594 --> 00:14:24,424
So roll back bilities If something goes
wrong in indication part, there is a

247
00:14:24,424 --> 00:14:28,354
capacity, we can roll back those latest
versions and we will keep it as a world

248
00:14:28,354 --> 00:14:32,784
version to make sure that business is
not impacting, get based workflows have

249
00:14:32,784 --> 00:14:35,724
become this become the defecto standard.

250
00:14:36,359 --> 00:14:40,349
For managing infrastructure and
application code, the platforms must

251
00:14:40,349 --> 00:14:44,929
extend these patterns to pipeline
definition and related to articles.

252
00:14:45,499 --> 00:14:45,709
Yeah.

253
00:14:46,489 --> 00:14:49,009
And continuous in integration
pipelines for data processing.

254
00:14:49,189 --> 00:14:53,419
Present unique challenges captured
to the traditional application to CAC

255
00:14:53,869 --> 00:14:56,149
and extending execution timelines.

256
00:14:56,899 --> 00:14:57,169
Yeah.

257
00:14:58,049 --> 00:15:01,049
Cloud native AP integration patterns.

258
00:15:01,049 --> 00:15:05,154
That's also one of the data platforms
servicing AP integration as well.

259
00:15:05,679 --> 00:15:09,699
Modern data platforms master seamlessly
integrated with the cloud native

260
00:15:09,699 --> 00:15:15,699
services while providing consistent
abstraction that provide vendor locking.

261
00:15:16,239 --> 00:15:19,359
Okay, here we have three topics
I would like to cover here.

262
00:15:19,689 --> 00:15:25,259
One is Azure Data Factory Integration data
Databricks Integration, A P Principles.

263
00:15:26,129 --> 00:15:29,409
So first one is the Azure
Data Factory integration.

264
00:15:29,859 --> 00:15:34,719
Platform should be simplify data
factory pipelines creation while

265
00:15:34,719 --> 00:15:38,769
representing access to the advanced
features when needed, typically

266
00:15:38,769 --> 00:15:41,169
through the template based approaches.

267
00:15:42,039 --> 00:15:46,379
Yeah, and Databricks integration
is here, requires careful

268
00:15:46,379 --> 00:15:48,359
consideration of a cluster management.

269
00:15:48,704 --> 00:15:53,984
Notebook deployment and job orchestration
pattern and automatic automated

270
00:15:53,984 --> 00:15:56,384
pro make carefully consideration.

271
00:15:56,384 --> 00:16:00,164
So we should not create, make sure that
the cluster management, what cluster we

272
00:16:00,164 --> 00:16:05,414
are using and job orchestrations we are
using, like what notebook we are using.

273
00:16:05,684 --> 00:16:09,344
So those kind of cloning while doing
the cloning should do the correct

274
00:16:09,344 --> 00:16:12,224
cloning and cement correctly.

275
00:16:12,989 --> 00:16:17,189
Coming to a PA design principles
a restful intersection interfaces

276
00:16:17,219 --> 00:16:21,839
with clear resources model and
consist handling error handling.

277
00:16:22,109 --> 00:16:26,819
Provide the predictive
tive experience graph.

278
00:16:26,939 --> 00:16:30,959
QR implementations can offer
more flexible query capabilities.

279
00:16:31,459 --> 00:16:36,034
Here, the AP design, also interests
will be there with a clear resource

280
00:16:36,124 --> 00:16:39,094
model and consistent error handling.

281
00:16:39,574 --> 00:16:45,754
So if we use, we used to use the a p
services right there most of the times,

282
00:16:46,324 --> 00:16:51,654
the error fee issues since it is a cloud
native integration to make sure that

283
00:16:51,714 --> 00:16:55,684
whatever the configurations notes we
are giving those request should be, make

284
00:16:55,684 --> 00:16:57,964
sure that it's and clearly is defined.

285
00:16:58,279 --> 00:17:00,289
So that will provide any errors.

286
00:17:00,789 --> 00:17:01,059
Yeah.

287
00:17:01,389 --> 00:17:04,119
Apache Airflow Accession
Foundation, this is one of the

288
00:17:04,659 --> 00:17:07,359
kind of where airflow is placed.

289
00:17:07,359 --> 00:17:13,149
Size, road awareness and Apache Airflow
Service has a physician backbone for

290
00:17:13,149 --> 00:17:18,564
many modern data platforms, as you told
the IT and crucial role, Apache Airflow.

291
00:17:19,064 --> 00:17:19,394
Duty.

292
00:17:19,394 --> 00:17:24,044
It's a Python native two approach
and extensive ecosystem integrations.

293
00:17:24,464 --> 00:17:25,394
So airflow.

294
00:17:25,844 --> 00:17:26,384
Airflow.

295
00:17:26,444 --> 00:17:29,454
So we can integrate it with
the latest latest tools.

296
00:17:29,484 --> 00:17:30,924
So environments we can do it.

297
00:17:31,344 --> 00:17:35,254
And it have Python based
native, it's a Python based

298
00:17:35,534 --> 00:17:37,184
it's a Python is more emerging.

299
00:17:37,844 --> 00:17:42,404
So the airflow also, that's the
Python native approach, right?

300
00:17:42,704 --> 00:17:44,544
So that's easy to use.

301
00:17:44,599 --> 00:17:44,639
Actually.

302
00:17:44,964 --> 00:17:50,024
And the key implementation concern
inclusion is multi-tenancy and resource

303
00:17:50,024 --> 00:17:56,144
isolation and DAG generation that
deploy pattern deployment patterns

304
00:17:56,894 --> 00:18:01,604
and custom operator development
and monitoring and integration.

305
00:18:02,294 --> 00:18:07,244
So the DAG generation deployment
patterns and their DS jobs.

306
00:18:07,739 --> 00:18:11,009
So our generations and the
deployment patterns, development

307
00:18:11,009 --> 00:18:14,959
patterns so based on the development
and how many Ds are required.

308
00:18:15,319 --> 00:18:18,709
So we need to identify, we need
to schedule that pattern also to

309
00:18:18,709 --> 00:18:22,399
make sure that in correct order
and monitoring and directing

310
00:18:22,399 --> 00:18:25,029
integration for one job is triggered.

311
00:18:25,419 --> 00:18:27,969
Then the monitoring will be
there, and if there's any

312
00:18:27,969 --> 00:18:29,499
failure, there could be any locks.

313
00:18:29,964 --> 00:18:30,444
While running.

314
00:18:30,444 --> 00:18:34,834
Also, we can see the logs and make
sure that what is issue and for this

315
00:18:34,834 --> 00:18:39,659
completed and what stage it is, those
we can monitor them as well, rather

316
00:18:39,659 --> 00:18:43,589
than requiring users to write Python
deck definition directly successfully.

317
00:18:43,689 --> 00:18:48,069
Platforms typically provides high
level abstractions that generate

318
00:18:48,069 --> 00:18:50,589
air for d from metadata definition.

319
00:18:51,009 --> 00:18:55,349
So metadata definition itself there's a
generated air four tax will be created.

320
00:18:55,709 --> 00:19:00,144
And we can use those patterns and we can
start using those into our development

321
00:19:00,239 --> 00:19:02,189
and we can schedule it actually

322
00:19:02,689 --> 00:19:06,589
infrastructure as a code for ETL
development and infrastructure as a

323
00:19:06,589 --> 00:19:11,509
code and principle transformation,
ETL development for manual error

324
00:19:11,689 --> 00:19:16,499
from process to automated repeated
procedures to integrate it naturally

325
00:19:16,559 --> 00:19:18,089
with the software development workflows.

326
00:19:18,589 --> 00:19:19,219
Tag workflow.

327
00:19:20,089 --> 00:19:23,014
And here there's three three
points I like to discuss here.

328
00:19:23,314 --> 00:19:27,954
One is Terraform modules and
helm charts, GI Tops Flow.

329
00:19:28,824 --> 00:19:34,514
The Terraform modules are encapsulated
common infrastructure patterns for

330
00:19:34,514 --> 00:19:38,384
data processing workflows, enabling
teams to deploy the complex,

331
00:19:38,384 --> 00:19:42,364
multiple, multi-service through the
simple configuration declarations.

332
00:19:43,204 --> 00:19:48,454
And by, by using helm charts, provide the
Bernet native approaches to infrastructure

333
00:19:48,514 --> 00:19:53,664
as code that align with containers on
the data processing or pressures and

334
00:19:53,664 --> 00:20:00,194
through the deployments and coming to
top our workflows, infrastructure changes

335
00:20:00,194 --> 00:20:06,044
triggered by the commit kit repositories,
creating audit trials and enable

336
00:20:06,224 --> 00:20:08,504
sophisticated approval workflow through.

337
00:20:09,269 --> 00:20:11,189
Tools like ango, cd, and Flux.

338
00:20:11,689 --> 00:20:12,769
Work for data quality.

339
00:20:13,159 --> 00:20:16,599
That's this is the one of the
critical role is the data assurance.

340
00:20:16,989 --> 00:20:19,599
Data Quality testing represents
fundamental requirement for

341
00:20:19,599 --> 00:20:21,009
the enterprise data platforms.

342
00:20:21,009 --> 00:20:23,469
Such traditional testing
approaches often through.

343
00:20:23,969 --> 00:20:27,299
In inadequate for modern
data processing workflows.

344
00:20:27,419 --> 00:20:31,619
Yes, comprehensive testing strategies
includes, testing with data

345
00:20:31,619 --> 00:20:35,879
generation and integrion testing
in container based environments.

346
00:20:36,059 --> 00:20:40,019
In my validation testing for back
backward compatibility, performance

347
00:20:40,019 --> 00:20:42,509
testing, performance regression
testing with the baseline.

348
00:20:42,844 --> 00:20:43,444
Comparison.

349
00:20:43,684 --> 00:20:48,994
So by using these four levels of testing
and make sure that whatever the data is

350
00:20:49,474 --> 00:20:54,754
handling and processing as data quality
would be data quality will bev perfect.

351
00:20:54,754 --> 00:20:58,664
And unlike traditional software
testing where the mock object can

352
00:20:58,664 --> 00:21:02,354
simulate external dependencies,
data crossing tests often required

353
00:21:02,964 --> 00:21:07,344
representative to data sets that
capture the complexity and each cases.

354
00:21:07,674 --> 00:21:09,354
Present in the production data?

355
00:21:09,624 --> 00:21:09,954
Yeah.

356
00:21:10,254 --> 00:21:13,574
Production data means production
data is final data, so it should

357
00:21:13,574 --> 00:21:15,119
not any issues in errors in data.

358
00:21:15,974 --> 00:21:20,384
So by doing the good quality of checks, we
can avoid errors in the production, data

359
00:21:20,884 --> 00:21:22,804
monitoring and observability of standards.

360
00:21:23,434 --> 00:21:27,524
There are four ways of we are doing
the monitoring and distributed testing

361
00:21:27,614 --> 00:21:31,304
and mattress collections and log
aggregations in digital directing.

362
00:21:31,724 --> 00:21:35,759
By using these four we are monitoring
and observability standards.

363
00:21:36,509 --> 00:21:40,079
And here comprehensive monitoring and
observability capitals are essential for

364
00:21:40,079 --> 00:21:41,999
the maintaining reliable data platform.

365
00:21:41,999 --> 00:21:44,489
Operating it enterprise scale.

366
00:21:44,639 --> 00:21:48,949
Enterprise level, and distributing
pricing is a critical for understanding

367
00:21:48,949 --> 00:21:53,179
complex data processing workflows
that span multi multiple service

368
00:21:53,229 --> 00:21:55,029
services and processing stages.

369
00:21:55,464 --> 00:22:01,554
System like Gear and Kin provides
a detailed execution visibility and

370
00:22:01,554 --> 00:22:07,114
went to mattress corrections time
series and databases like influx db

371
00:22:07,184 --> 00:22:11,684
provider efficient storage and querying
cables of both technical performance

372
00:22:11,984 --> 00:22:16,594
in indicators and business relevant
process outcomes coming to the log

373
00:22:16,594 --> 00:22:21,004
aggregations tool, like Elastic Research
and Splunk, and high volume of data.

374
00:22:21,504 --> 00:22:27,464
And log output with sophisticated such and
analysis capabilities for rapid problem

375
00:22:27,794 --> 00:22:30,824
identification coming to intelligent Yeah.

376
00:22:31,004 --> 00:22:36,474
By aggregations, by using this bunk
research is a very quick we can come

377
00:22:36,574 --> 00:22:40,434
issue where exactly is happening, and
intelligent alerting, sophisticated

378
00:22:40,434 --> 00:22:46,084
alert correlation escalations policies
like ML based anomaly detecting and.

379
00:22:46,924 --> 00:22:52,624
Subject issues are reducing the, yeah
real world implementation experience

380
00:22:52,754 --> 00:22:56,684
practice implementation of self-service
data platform across the, and diversity

381
00:22:56,684 --> 00:23:00,734
enterprise and environment provides
the valuable insights c to both

382
00:23:00,854 --> 00:23:04,304
architectural pattern and organizational
changes management strategies.

383
00:23:05,114 --> 00:23:10,034
So here the industry specific challenges
are included financial services, like

384
00:23:10,664 --> 00:23:16,094
appliances, data governance and risk
management requirements, and healthcare,

385
00:23:16,154 --> 00:23:20,864
coming to healthcare like HIPAA
complaints, patient privacy and Alexis

386
00:23:20,924 --> 00:23:25,904
and banking fraud detections, realtime
processing core system integrations.

387
00:23:26,354 --> 00:23:31,389
And those we need to see alien
operational s and disaster require.

388
00:23:31,739 --> 00:23:33,299
And airlines, sorry.

389
00:23:34,289 --> 00:23:37,529
Operational alliances and the
disaster require capabilties.

390
00:23:37,529 --> 00:23:37,589
Yeah.

391
00:23:38,089 --> 00:23:41,284
And developer experience
and load connections.

392
00:23:41,929 --> 00:23:46,569
The source of self-service data platform
ultimately depends on their ability

393
00:23:46,569 --> 00:23:48,624
to reduce the loads for development.

394
00:23:49,299 --> 00:23:55,029
Teams while providing the powerful data
processing capabilities like lead tool

395
00:23:55,029 --> 00:24:00,229
design, like well designated CLA tools
for flow establishment conventions

396
00:24:00,229 --> 00:24:04,989
for pat parameter handling, output
formatting and error reporting with the

397
00:24:05,169 --> 00:24:08,709
compress help system and conversions.

398
00:24:09,249 --> 00:24:13,509
So by using this CLA digital tool
so we can perform all this on.

399
00:24:14,349 --> 00:24:17,979
Like handling output, formatting,
error, reporting, all this

400
00:24:17,979 --> 00:24:18,999
things, we can handle this.

401
00:24:19,599 --> 00:24:23,749
And web interfaces must provide
ation of complex data processing

402
00:24:23,749 --> 00:24:28,279
workflow while enabling sophisticated
ations and monitoring ties.

403
00:24:28,279 --> 00:24:34,879
Sir documentation interactive document is
system that combines explanatory content.

404
00:24:35,239 --> 00:24:39,349
That executable example provides
effective learning experience.

405
00:24:39,849 --> 00:24:43,389
Governance and compliance and
platform architecture, so data

406
00:24:43,389 --> 00:24:48,129
lineage, audit, logging, and access
control and privacy production.

407
00:24:48,179 --> 00:24:52,464
So as I mentioned, so data governance and
compliance is very key to the company.

408
00:24:53,189 --> 00:24:57,129
Based on that, only if you speak
openly, the stock market and all

409
00:24:57,129 --> 00:24:58,509
the stuff will be based on that.

410
00:24:58,899 --> 00:25:04,149
This ambulance only, so there is no
data leak or they maintain all standards

411
00:25:04,419 --> 00:25:06,339
so that all the business is doing.

412
00:25:06,399 --> 00:25:10,229
All standards will be on, accounting
will be there year and the data

413
00:25:10,229 --> 00:25:14,249
line automatically capture the data
flow information as a pipeline and

414
00:25:14,309 --> 00:25:19,629
execute, maintain detailed records
of source submissions and designation

415
00:25:19,629 --> 00:25:20,889
for the compliance reporting.

416
00:25:21,389 --> 00:25:25,409
And access controls integrated with
the enterprise entity system while

417
00:25:25,679 --> 00:25:31,069
providing the fine permissions through
VAC systems for dynamic ations.

418
00:25:31,569 --> 00:25:36,279
Audit logging capture the compress
records for all user actions, data

419
00:25:36,279 --> 00:25:41,559
access and configuration changes
with sufficient detail for complaint

420
00:25:41,619 --> 00:25:44,909
reporting, privacy protection, sorry.

421
00:25:45,409 --> 00:25:49,824
Regulatory requirement like
D-P-R-C-C-P-A through the authorization

422
00:25:49,824 --> 00:25:54,054
data discovery classification,
an normalizations and delete

423
00:25:54,264 --> 00:25:56,214
capabilities, deletion capabilities.

424
00:25:56,664 --> 00:26:01,114
So go data governance and complaints
are the key for the company.

425
00:26:01,624 --> 00:26:05,779
So by using these four things that,
that these architecture office, like

426
00:26:05,779 --> 00:26:10,614
the data line, access control, audit,
logging, and privacy protections.

427
00:26:11,114 --> 00:26:15,004
And this is the feature ions
and conclusion of this task and

428
00:26:15,104 --> 00:26:17,064
transformations of aging here.

429
00:26:17,064 --> 00:26:20,034
The emerging patterns of our
wireless computing for more

430
00:26:20,034 --> 00:26:24,974
efficient resource affiliation one
and machine learning integration

431
00:26:24,974 --> 00:26:26,804
with model electric management.

432
00:26:27,254 --> 00:26:27,614
Yes.

433
00:26:28,114 --> 00:26:34,014
And real time processing and ties with
the steam processing fire frameworks and

434
00:26:34,014 --> 00:26:39,564
edge computing scenarios spanning cloud
and edge environment and a port platform

435
00:26:39,564 --> 00:26:41,574
operations for optimization monitoring.

436
00:26:42,504 --> 00:26:48,534
And these are all patterns and new
technologies and new to do the data

437
00:26:48,534 --> 00:26:51,564
handlings and data transformations
from the legacy to the new one.

438
00:26:52,064 --> 00:26:52,814
To success.

439
00:26:53,444 --> 00:26:53,804
Yeah.

440
00:26:54,194 --> 00:26:58,004
Building the effective self
service data plan is required, is a

441
00:26:58,244 --> 00:27:02,354
holistic approach that balance the
technical, sophisticated, and user

442
00:27:02,354 --> 00:27:05,474
experience considerations the most.

443
00:27:05,524 --> 00:27:09,344
Successful implementation treat
platform development as a product

444
00:27:09,344 --> 00:27:14,194
development with the clear user
persons, interactive implement cycles,

445
00:27:14,284 --> 00:27:16,234
and comprehensive feedback mechanism.

446
00:27:16,734 --> 00:27:20,184
And operations organizations
that successfully implement

447
00:27:20,634 --> 00:27:22,254
comprehensive data platform.

448
00:27:22,254 --> 00:27:27,414
That strategies, again, significant
competitive advantage through the improved

449
00:27:27,474 --> 00:27:32,364
developer productivity, faster time to
market, and more reliable operations.

450
00:27:32,754 --> 00:27:35,574
These are all key to
success directions actually.

451
00:27:35,994 --> 00:27:36,264
Yeah.

452
00:27:37,014 --> 00:27:37,284
Thank you.

453
00:27:37,494 --> 00:27:42,914
And thank you for giving opportunity to
present myself and my work and giving

454
00:27:42,914 --> 00:27:47,844
thoughts on it and I'm happy to share
my experience and presentation here.

455
00:27:48,144 --> 00:27:49,014
Thank you very much.

