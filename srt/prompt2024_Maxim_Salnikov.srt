1
00:00:00,000 --> 00:00:00,820
hello everyone!

2
00:00:00,930 --> 00:00:03,000
Let's talk about prompt engineering.

3
00:00:03,400 --> 00:00:04,130
What is it?

4
00:00:04,670 --> 00:00:08,050
Art or science or maybe
your next job title?

5
00:00:08,550 --> 00:00:09,409
I'm Maxim Selnikov.

6
00:00:09,959 --> 00:00:14,280
I'm based in Oslo, Norway, where
I work in Microsoft, where I help

7
00:00:14,329 --> 00:00:19,349
developers to succeed with our cloud
technologies, tools for developers.

8
00:00:19,575 --> 00:00:22,275
And, everything related AI.

9
00:00:22,795 --> 00:00:26,535
I personally, a developer
myself, and I build applications

10
00:00:26,545 --> 00:00:30,195
since, late 90s of last century.

11
00:00:31,025 --> 00:00:33,084
And I'm big fan of the
developer communities.

12
00:00:33,495 --> 00:00:38,664
In Oslo, where I based, I founded a couple
of conferences and, run a few meetups.

13
00:00:39,164 --> 00:00:43,804
My favorite topics to present about
are web development, all kinds of

14
00:00:43,854 --> 00:00:45,604
cloud development, and of course, AI.

15
00:00:45,894 --> 00:00:50,644
Everything related to generative
AI, including prompt engineering.

16
00:00:51,144 --> 00:00:55,514
I found multiple estimations
on, the number of people who

17
00:00:55,514 --> 00:00:57,864
use generative AI on our planet.

18
00:00:58,364 --> 00:01:05,374
And, the most conservative ones say
that it's well over 1 billion people.

19
00:01:05,464 --> 00:01:06,914
That's a number, right?

20
00:01:07,414 --> 00:01:10,494
What do these people
use generative AI for?

21
00:01:10,994 --> 00:01:12,734
just for us to start the discussion.

22
00:01:13,034 --> 00:01:16,664
I identified three
large application areas.

23
00:01:17,484 --> 00:01:21,044
First of all, everything related
to productivity, your personal

24
00:01:21,044 --> 00:01:25,734
productivity, your business productivity,
everything related to usage of

25
00:01:25,754 --> 00:01:30,464
generative AI in education and science,
and many other areas where we can

26
00:01:30,574 --> 00:01:33,474
really get super powered by gen AI.

27
00:01:33,974 --> 00:01:35,444
Next creativity.

28
00:01:35,914 --> 00:01:38,184
After all, it's called
generative AI, right?

29
00:01:38,184 --> 00:01:41,364
So it can generate for us
many, very interesting things.

30
00:01:41,414 --> 00:01:46,514
so we can really feel ourselves super
creative, but, there should be someone

31
00:01:46,544 --> 00:01:49,484
who is building these applications for us.

32
00:01:49,494 --> 00:01:54,434
Someone who is, actually, Constructing
this UIs that we can use both

33
00:01:54,444 --> 00:01:56,634
for productivity and creativity.

34
00:01:56,844 --> 00:02:02,234
And yeah, here I'm talking about people
who are actually building these AI

35
00:02:02,274 --> 00:02:05,124
infused or intelligent applications.

36
00:02:05,314 --> 00:02:09,994
And I'll do my best to make this
particular session the most useful

37
00:02:10,004 --> 00:02:12,314
for exactly this category of people.

38
00:02:12,534 --> 00:02:18,074
people, but at the same time, I'm sure
that even if you don't have any intention

39
00:02:18,124 --> 00:02:23,674
to build your, application, you can
still get lots of useful information from

40
00:02:23,694 --> 00:02:28,434
this session because, it contains some,
general recommendations, some general,

41
00:02:28,494 --> 00:02:31,884
practices for your improved Prompting.

42
00:02:31,984 --> 00:02:36,384
And, after this session, when you
open any kind of generative AI powered

43
00:02:36,384 --> 00:02:42,104
service like ChatGPT or similar, you
will know better how to communicate.

44
00:02:42,594 --> 00:02:47,504
what is common thing
in all these scenarios?

45
00:02:48,004 --> 00:02:54,354
Let's look how we, for example, ask Meet
Journey to create a new image for us, or

46
00:02:54,374 --> 00:03:01,024
how we start conversation with Microsoft
365 Word, on, creating some very smart

47
00:03:01,084 --> 00:03:04,454
and solid template for our document.

48
00:03:04,954 --> 00:03:11,154
Or let's look how we communicate
with a GitHub copilot for improving

49
00:03:11,234 --> 00:03:16,994
our code, not even saying about
the chat GPT as a service itself,

50
00:03:17,014 --> 00:03:19,944
where we literally chat with this.

51
00:03:20,444 --> 00:03:24,254
Of course, one common thing
is the way we interact.

52
00:03:24,534 --> 00:03:31,054
And, the fact that it all starts with
the prompt with, some text that we write.

53
00:03:31,554 --> 00:03:37,814
And, even though model landscape is
emerging and there is, next amazing

54
00:03:37,894 --> 00:03:43,144
model is, released while I speak, while
you watch this session, and there are

55
00:03:43,144 --> 00:03:45,364
already, multiple models available.

56
00:03:45,694 --> 00:03:48,424
And, It's becoming more and more complex.

57
00:03:48,434 --> 00:03:50,204
There are large language models.

58
00:03:50,254 --> 00:03:57,354
There is new generation of small language
models also multi modality is really

59
00:03:57,854 --> 00:04:04,114
available now even though There are
different kinds and types of models.

60
00:04:04,124 --> 00:04:07,664
Some of them are general some
of them specialized Still

61
00:04:07,864 --> 00:04:11,044
we all go back to prompts

62
00:04:11,544 --> 00:04:18,544
This is why I consider prompt engineering
as a separate discipline or at least as

63
00:04:18,544 --> 00:04:23,754
an essential skill of many people, not
only developers, but, many technical

64
00:04:23,754 --> 00:04:26,944
people, many people on business positions.

65
00:04:27,294 --> 00:04:31,334
and, what is this after all,
this is a process of, designing

66
00:04:31,364 --> 00:04:33,844
prompts of, tuning these prompts.

67
00:04:34,049 --> 00:04:41,149
And the further optimization, while
we keep satisfaction on a satisfaction

68
00:04:41,149 --> 00:04:45,919
level on the results that we get
back from the large language models.

69
00:04:45,999 --> 00:04:51,039
And of course, it's also important to,
to follow cost efficiency, because in

70
00:04:51,039 --> 00:04:57,329
many cases, we talk about some paid
services when we talk specifically about

71
00:04:57,369 --> 00:04:59,769
like high and large language models.

72
00:05:00,269 --> 00:05:05,429
Let's look closer at the prompt components
or if you wish we can call it prompt

73
00:05:05,549 --> 00:05:12,819
anatomy in many cases Again, at least
when we are the users who chat with

74
00:05:12,829 --> 00:05:19,934
chat GPT We don't really think too
much about How we structure our prompt.

75
00:05:19,944 --> 00:05:24,734
We just communicate and as long
as we are happy with the results.

76
00:05:24,754 --> 00:05:30,664
We don't want to Dive deeper into all
these nitty gritty details That's fine.

77
00:05:30,724 --> 00:05:34,634
But when you build your AI infused
application It's really good idea

78
00:05:34,684 --> 00:05:40,224
for you to know how that works from
inside and again the same ideas and

79
00:05:40,224 --> 00:05:44,464
the techniques are still applicable
to your like day to day conversations

80
00:05:44,464 --> 00:05:47,214
with any generative AI powered service.

81
00:05:47,214 --> 00:05:51,104
So yeah, let me, introduce how that works.

82
00:05:51,544 --> 00:05:59,074
in many cases we have very clear and,
concise and straightforward instruction.

83
00:05:59,104 --> 00:06:04,314
What we want to get from this
particular call to our, generative AI

84
00:06:04,314 --> 00:06:06,384
service or, more technically precise.

85
00:06:06,769 --> 00:06:10,459
The two large language model
exposed by one of these services.

86
00:06:10,809 --> 00:06:14,369
and yeah, let me illustrate it by
this example what on the screen.

87
00:06:14,629 --> 00:06:19,589
Let's imagine that we are building
applications for marketing automation

88
00:06:19,609 --> 00:06:25,399
that gives us Nice, at least drafts,
maybe, or maybe ready to go emails

89
00:06:25,439 --> 00:06:30,379
that, share details on some new
products that we either produce or sell,

90
00:06:30,619 --> 00:06:34,889
or, it's again, it's not obligatory
as in role of developers, you can,

91
00:06:34,969 --> 00:06:39,549
use the same, ideas and techniques
when you just, use ready to ready

92
00:06:39,549 --> 00:06:42,059
products, with, some prompts available.

93
00:06:42,689 --> 00:06:43,429
Instruction.

94
00:06:43,679 --> 00:06:48,089
Of course, we have to also provide some
primary data about this product itself.

95
00:06:48,449 --> 00:06:51,859
Also, we can provide some context
or we can call it secondary

96
00:06:51,859 --> 00:06:54,329
data about tone we expect.

97
00:06:54,399 --> 00:06:59,529
In this particular situation, we want
to be friendly and exciting, but for

98
00:06:59,529 --> 00:07:05,259
different scenarios, there could be
different ways of, exact, narrative

99
00:07:05,309 --> 00:07:06,639
that we expect from the model.

100
00:07:07,139 --> 00:07:12,809
Also, we, identify, we set the
format, we define the format,

101
00:07:12,889 --> 00:07:15,059
we want to get, answer back.

102
00:07:15,439 --> 00:07:19,109
And in this particular situation, it's
definitely something that, that is

103
00:07:19,149 --> 00:07:23,729
in the middle of, overall, developer
chain of, of this product, because

104
00:07:24,109 --> 00:07:28,319
you see that we expect not just text,
but, Jason, definitely this will

105
00:07:28,319 --> 00:07:32,489
be somehow processed by the next
steps of, our, for example, backend.

106
00:07:32,639 --> 00:07:35,219
So we say that we want to get back.

107
00:07:35,394 --> 00:07:40,484
Not just subject and the body,
but JSON object with these fields.

108
00:07:40,844 --> 00:07:44,404
Also, we provide an example and
in this particular situation,

109
00:07:44,464 --> 00:07:46,714
it plays at least two roles.

110
00:07:46,904 --> 00:07:54,764
First of all, yeah, it demonstrates the
model, that kind of text, maybe length

111
00:07:54,804 --> 00:08:00,404
of the text and again tone and old
structure That might be a good fit for us.

112
00:08:00,744 --> 00:08:03,194
And also we double down on the format.

113
00:08:03,254 --> 00:08:08,594
So this is why, we, put example
exactly in the format that we

114
00:08:08,794 --> 00:08:10,564
described on the line above.

115
00:08:10,854 --> 00:08:12,774
Why do we need this duplication?

116
00:08:13,124 --> 00:08:13,774
Stay tuned.

117
00:08:13,824 --> 00:08:16,184
I will explain why that might be useful.

118
00:08:16,484 --> 00:08:21,184
So this is how we, human or
developers look at the prompt.

119
00:08:21,684 --> 00:08:26,404
And this is how large language
model or LLM understands

120
00:08:26,444 --> 00:08:28,844
the same prompt on its end.

121
00:08:29,164 --> 00:08:32,124
And that it's split into tokens.

122
00:08:32,424 --> 00:08:34,549
And actually, tokenization
is the first step.

123
00:08:34,799 --> 00:08:39,049
procedure that happens when
you send prompt to LLM.

124
00:08:39,269 --> 00:08:45,059
So large language model understands
your, your prompt in form of tokens.

125
00:08:45,109 --> 00:08:50,079
And, when time has come to generate
answer for you, it also generates

126
00:08:50,109 --> 00:08:52,509
this recursively token by token.

127
00:08:53,119 --> 00:08:55,579
On this example, that, many words.

128
00:08:55,969 --> 00:08:59,999
are equal to one token, right?

129
00:09:00,019 --> 00:09:01,399
One word equals one token.

130
00:09:01,399 --> 00:09:06,059
But of course, the real situation
is much more, complex, right?

131
00:09:06,059 --> 00:09:08,759
So you see that, some words, take.

132
00:09:09,259 --> 00:09:14,379
Multiple tokens and exact
implementation of how that split

133
00:09:14,429 --> 00:09:20,499
is up to implementation of a large
language model, or it's a tokenizer.

134
00:09:20,669 --> 00:09:26,369
There is no any kind of strict rule how
many, Words equals to how many tokens

135
00:09:26,409 --> 00:09:31,289
or how many characters equals to how
many tokens we can very Approximately

136
00:09:31,289 --> 00:09:39,429
say that at least for this generation
of LLMs and for English texts 100 tokens

137
00:09:39,430 --> 00:09:43,929
is Approximately, equal to 75 words.

138
00:09:44,309 --> 00:09:49,999
so that means like one token is maybe
around four characters in English text.

139
00:09:50,009 --> 00:09:53,099
For different languages, the
ratio is completely different.

140
00:09:53,599 --> 00:09:58,809
You might ask me, why do we ever need
to have this knowledge about tokens?

141
00:09:58,809 --> 00:10:02,899
Because Definitely when we are
in user role, this is hidden

142
00:10:02,899 --> 00:10:04,729
completely under the hood for us.

143
00:10:04,749 --> 00:10:08,609
We communicate with
sentences in, sentences back.

144
00:10:09,059 --> 00:10:15,209
when we are in developer role, Also,
this is not that visible at the

145
00:10:15,209 --> 00:10:20,299
beginning of, when you start building
your AI application, but very soon as

146
00:10:20,299 --> 00:10:24,989
a developer, you will understand super
important meaning of tokenization.

147
00:10:25,489 --> 00:10:31,009
And this is why, because number
of tokens in your input and

148
00:10:31,039 --> 00:10:33,039
expected output, first of

149
00:10:33,539 --> 00:10:37,579
Cost of, this particular call,
if we talk about some hosted,

150
00:10:38,109 --> 00:10:39,959
LLMs hosted by some vendors.

151
00:10:40,219 --> 00:10:47,479
And also there is technical limitation on
number of tokens you can send and receive.

152
00:10:48,029 --> 00:10:53,589
on this slide, there is some
screenshot from, pricing for

153
00:10:53,669 --> 00:10:57,599
Azure OpenAI, services for GPT 4.

154
00:10:57,599 --> 00:10:58,839
0 model and 0.

155
00:10:58,909 --> 00:11:00,059
1 preview model.

156
00:11:00,269 --> 00:11:04,279
And for legacy purposes, I
also listed price for GPT 4.

157
00:11:04,639 --> 00:11:09,649
And, that, we are in good situation
as developers because prices.

158
00:11:09,894 --> 00:11:11,584
are going down.

159
00:11:11,634 --> 00:11:15,014
And first of all, yeah, that
it's priced by 1 million tokens.

160
00:11:15,034 --> 00:11:18,034
So every single token for a few
number of calls, maybe this is

161
00:11:18,054 --> 00:11:19,904
not something really crucial.

162
00:11:20,614 --> 00:11:25,252
The price becomes really different when
we talk about some scale usage, but, Yeah.

163
00:11:25,252 --> 00:11:25,279
Yeah.

164
00:11:25,919 --> 00:11:28,319
Still very important to keep eye on it.

165
00:11:28,549 --> 00:11:36,359
And that in GPT 4 it started for
60 per million and suddenly GPT 4.

166
00:11:36,429 --> 00:11:38,859
0 that is more capable,
more performant model.

167
00:11:39,209 --> 00:11:44,519
It's many times more Cheaper
and this is general trend.

168
00:11:44,639 --> 00:11:52,059
there is newer and newer technologies from
outside of providers of LLM services and

169
00:11:52,289 --> 00:11:56,039
yeah, so it's it's very good, especially
for example startups they have to

170
00:11:56,049 --> 00:12:02,589
recalculate their economy and recalculate
in a positive way and next column I want

171
00:12:02,599 --> 00:12:09,764
to emphasize on this screenshot is context
and this is exactly the Number of tokens

172
00:12:09,834 --> 00:12:12,744
we can send in one particular request.

173
00:12:12,754 --> 00:12:16,934
So we see it's not number of characters
or number of words or bytes or whatever.

174
00:12:16,984 --> 00:12:19,144
No, it's calculated in tokens.

175
00:12:19,174 --> 00:12:23,914
This is why it's very important to
understand, at least roughly estimate

176
00:12:23,944 --> 00:12:25,394
number of tokens in your request.

177
00:12:25,504 --> 00:12:25,914
prompts.

178
00:12:26,194 --> 00:12:31,724
as you see, modern models are quite
capable and we talk about a hundred

179
00:12:31,824 --> 00:12:34,464
plus thousands of, of the token.

180
00:12:34,464 --> 00:12:38,924
So we are not talking about every single,
word or white space or, not even about

181
00:12:38,924 --> 00:12:41,144
sentences, not even about paragraphs.

182
00:12:41,484 --> 00:12:43,544
Maybe you can, currently send.

183
00:12:43,964 --> 00:12:49,894
Pages of text as a prompt for these
models and again trend is work

184
00:12:49,954 --> 00:12:56,514
good for us models are Capable to
accept more and more tokens, but

185
00:12:56,514 --> 00:12:58,114
still there is a limitation, right?

186
00:12:58,114 --> 00:13:02,444
So you cannot send maybe full book at
least currently at least at this moment

187
00:13:03,259 --> 00:13:07,889
So this is why, again, tokenization
concept is extremely important and, we'll,

188
00:13:07,909 --> 00:13:11,599
have a few more slides on this topic.

189
00:13:11,839 --> 00:13:15,479
And of course, different providers
of, these LLM services provide

190
00:13:15,499 --> 00:13:20,069
different ways for you to save on,
on this usage, all kinds of, caching.

191
00:13:20,424 --> 00:13:24,984
All kinds of, batch calculations when
you don't need output, here now, but

192
00:13:24,984 --> 00:13:29,364
you can wait a bit and then, price for
this call will be cheaper than regular.

193
00:13:29,654 --> 00:13:34,244
All kinds of, some dedicated, capacities,
that really depends vendor from vendor.

194
00:13:34,744 --> 00:13:39,754
And you also noticed that, model
selection makes real difference.

195
00:13:40,049 --> 00:13:45,539
First of all on capabilities, on quality
of the output, and second on the price.

196
00:13:45,939 --> 00:13:50,569
And, I offer you to use this
simple strategy on model selection.

197
00:13:50,769 --> 00:13:55,339
First of all, try with the most
capable, the most performant,

198
00:13:55,369 --> 00:13:58,879
and, in many cases also the most
expensive model on the market.

199
00:13:58,979 --> 00:14:01,249
maybe you can, try different providers.

200
00:14:01,679 --> 00:14:06,419
And, Identify your best prompt that
gives you the best possible results

201
00:14:06,809 --> 00:14:12,609
Then you can try to downgrade to the
next cheapest one Next cheapest model

202
00:14:13,359 --> 00:14:17,569
in some cases you might to might need
to fine tune the prompt slightly but

203
00:14:17,689 --> 00:14:23,204
check the results if Result or completion
in technical terms when we talk about

204
00:14:23,384 --> 00:14:26,304
prompt engineering is the same or better.

205
00:14:26,374 --> 00:14:33,044
Maybe you can Do next step in
this downgrade and try even

206
00:14:33,074 --> 00:14:36,644
cheaper model Maybe again results
will be the same or even better.

207
00:14:36,654 --> 00:14:40,604
Sometimes that's also possible and it
will give you a chance to save Some

208
00:14:40,834 --> 00:14:47,674
dollars if no Then you just go up to the
previous step where you downgraded from.

209
00:14:47,744 --> 00:14:51,434
It's a simple and efficient
model selection strategy.

210
00:14:51,934 --> 00:14:58,404
Also, you can use multiple models and in
many cases, if we talk about, not hello

211
00:14:58,404 --> 00:15:04,524
world AI infused application, this is not
just one call to the, LLM, it's a chain

212
00:15:04,564 --> 00:15:09,524
of the calls, maybe to multiple models,
maybe to even to multiple providers, maybe

213
00:15:09,804 --> 00:15:14,624
it's mix between, some hosted services,
model hosted by external vendor, model

214
00:15:14,634 --> 00:15:20,324
hosted by yourself, and maybe even a
model that is running straight on your

215
00:15:20,344 --> 00:15:22,254
device or on your customer device.

216
00:15:22,284 --> 00:15:24,574
So that, really depends
on business scenario.

217
00:15:24,614 --> 00:15:31,404
So if you have a chance to use multiple
models in this particular feature

218
00:15:31,405 --> 00:15:35,384
of your AI infused application,
you can follow a simple rule.

219
00:15:35,694 --> 00:15:36,284
for.

220
00:15:36,784 --> 00:15:41,234
Let's say, complex tasks like
generation, you can use expensive ones.

221
00:15:41,284 --> 00:15:43,094
This is, where they really shine.

222
00:15:43,124 --> 00:15:46,264
And, yeah, every new generation
of the model provides better and

223
00:15:46,274 --> 00:15:47,524
better results in generation.

224
00:15:47,844 --> 00:15:51,184
While summarization,
classification, categorization.

225
00:15:51,194 --> 00:15:56,814
This is, again, our days, not
that complex tasks anymore, at

226
00:15:56,814 --> 00:15:58,554
least for a large language models.

227
00:15:58,604 --> 00:16:01,884
And the cheap ones do them pretty good.

228
00:16:02,564 --> 00:16:03,434
another strategy.

229
00:16:04,254 --> 00:16:05,294
is chaining.

230
00:16:05,344 --> 00:16:13,134
for example, you want to send to a large,
expensive model some large amount of text.

231
00:16:13,164 --> 00:16:15,284
This is how your prompt works, right?

232
00:16:15,614 --> 00:16:22,764
but you can leverage cheaper and maybe,
Like very fast one, maybe your, your

233
00:16:22,814 --> 00:16:28,184
own fine tuned model to summarize this,
large amount of text you are going to

234
00:16:28,184 --> 00:16:34,134
send to expensive one also might work
good for you with a minimal performance,

235
00:16:34,174 --> 00:16:35,719
decrease, you will save a lot of time.

236
00:16:35,779 --> 00:16:36,869
some, some budget,

237
00:16:37,369 --> 00:16:39,909
let's go back to talking conversation.

238
00:16:39,939 --> 00:16:45,309
So I hope that I convinced you that,
keeping eye on number of tokens

239
00:16:45,339 --> 00:16:50,179
you send is crucial for, economy
of your application, not only for

240
00:16:50,179 --> 00:16:53,939
economy, but also you remember that,
there was just technical limitation

241
00:16:53,939 --> 00:16:55,829
of a number of tokens you send.

242
00:16:55,859 --> 00:17:01,764
And after all the shorter prompt,
normally the faster you get completion.

243
00:17:01,764 --> 00:17:07,474
So it's, like multiple reasons
why you want to, minimize your

244
00:17:07,484 --> 00:17:09,864
prompts and how to do this exactly.

245
00:17:10,074 --> 00:17:11,424
First of all, very simple rule.

246
00:17:11,474 --> 00:17:12,454
have a closer look at.

247
00:17:12,709 --> 00:17:14,459
White spaces in your prompt.

248
00:17:14,869 --> 00:17:19,229
this is something that we can
easily overlook because this is not

249
00:17:19,239 --> 00:17:21,189
something that is very visible, right?

250
00:17:21,289 --> 00:17:23,359
A couple of extra white spaces.

251
00:17:23,389 --> 00:17:25,119
We can just, okay, ignore this.

252
00:17:25,179 --> 00:17:29,594
But in reality, Some large
language models treat every single

253
00:17:29,634 --> 00:17:32,044
whitespace as one extra token.

254
00:17:32,294 --> 00:17:32,984
Not a big deal.

255
00:17:32,984 --> 00:17:36,784
If you talk about one short request, but
if we talk about, I don't know, hundreds,

256
00:17:36,784 --> 00:17:41,334
thousands, millions of requests that
might bring some, some difference to

257
00:17:41,334 --> 00:17:43,409
your final bill and the end of the month.

258
00:17:43,909 --> 00:17:49,049
Next for, different data formats,
try different, different ways

259
00:17:49,089 --> 00:17:51,739
to like, implement this data.

260
00:17:52,109 --> 00:17:56,849
What I mean exactly can be easily
illustrated by example of, how

261
00:17:56,859 --> 00:18:00,159
we supply, Date in our prompts.

262
00:18:00,559 --> 00:18:05,939
I'm technical mind and my first impression
was okay The shorter string with the

263
00:18:05,939 --> 00:18:11,894
date the better chance that it will take
fewer tokens In reality, not at all.

264
00:18:11,904 --> 00:18:18,064
So you see on the bottom line, short
format of the date takes six tokens, while

265
00:18:18,114 --> 00:18:20,644
on the top line, it only three tokens.

266
00:18:20,654 --> 00:18:23,144
So sometimes it's counter intuitive.

267
00:18:23,264 --> 00:18:27,434
Again, this is example for one
particular large language model.

268
00:18:27,454 --> 00:18:31,594
I don't even remember which one
different models can do it differently.

269
00:18:32,134 --> 00:18:37,724
experimentation is the only way to really
identify what's your optimal format

270
00:18:37,724 --> 00:18:40,484
for this or that type of, your data.

271
00:18:40,984 --> 00:18:46,384
if you talk about tabular data, this,
straightforward tabular format is,

272
00:18:46,434 --> 00:18:50,764
pretty much, space efficient, and what's
very important, understood by LLM.

273
00:18:50,854 --> 00:18:56,444
no need for you to, always reproduce
JSON like format where you supply

274
00:18:56,494 --> 00:18:58,394
caption for every piece of the data.

275
00:18:58,424 --> 00:19:03,124
No, just provide some table headers,
separated by pipes, or type, tabs, or

276
00:19:03,124 --> 00:19:08,159
you'll find your Personal separator and
then rows with the data in vast majority

277
00:19:08,179 --> 00:19:14,599
of situations LLM will understand what you
mean also Language makes real difference.

278
00:19:14,639 --> 00:19:20,969
I already mentioned that English is
the most straightforward ways for way

279
00:19:20,969 --> 00:19:26,189
for Prompt engineering because it's
the most efficient while we talk about

280
00:19:26,509 --> 00:19:32,214
tokenization at least again for the
mainstream Large language models I

281
00:19:32,714 --> 00:19:37,924
suggest this because vast amount of
data, all this, Wikipedia, public books,

282
00:19:37,924 --> 00:19:42,354
et cetera, et cetera, data that was
used for LLM training was in English.

283
00:19:42,644 --> 00:19:46,754
it still understands other languages
perfectly well, but if we talk about

284
00:19:46,804 --> 00:19:50,794
tokenization, the same, let's say,
Concept same sentence in different

285
00:19:50,824 --> 00:19:55,964
language might take more tokens than
the one in English i'm not saying

286
00:19:55,964 --> 00:19:59,904
that you have to translate everything
all single time But again experiment

287
00:20:00,404 --> 00:20:06,494
and I want to start introducing some
tools and yeah in On next slides, you

288
00:20:06,494 --> 00:20:10,344
will see more and more very useful
tools frameworks libraries that can

289
00:20:10,344 --> 00:20:12,729
simplify your life as a prompt engineer.

290
00:20:13,359 --> 00:20:18,139
the first one called LLM Lingua, and
this is nothing by but prompt compressor.

291
00:20:18,359 --> 00:20:22,049
It's created by my colleagues from
Microsoft, and it's open source.

292
00:20:22,249 --> 00:20:26,939
So you can take this tool and host
it locally or on your own server.

293
00:20:27,229 --> 00:20:33,339
And yeah, as the description says,
it takes your prompt and compresses,

294
00:20:33,879 --> 00:20:36,609
but it does it in a very smart way.

295
00:20:36,839 --> 00:20:39,529
of course We can try to
do it ourselves, right?

296
00:20:39,539 --> 00:20:46,559
But, we do not, not always understand
which parts of our prompt are crucial

297
00:20:46,589 --> 00:20:52,419
for, LLM to really understand what
we mean and which parts we can skip.

298
00:20:52,789 --> 00:20:53,599
who can help us?

299
00:20:54,054 --> 00:20:56,344
Of course, another large
language model, right?

300
00:20:56,764 --> 00:21:00,474
And in this particular case,
this is compact one, maybe we can

301
00:21:00,474 --> 00:21:02,474
call it a small language model.

302
00:21:02,974 --> 00:21:07,974
And yeah, so this one is used
to identify and remove non

303
00:21:08,054 --> 00:21:10,584
essential tokens in your prompt.

304
00:21:10,954 --> 00:21:17,564
And with some playing, some fine tuning,
you can get up to 20 times compression

305
00:21:17,624 --> 00:21:21,084
with either zero or minimum performance.

306
00:21:21,374 --> 00:21:26,704
loss because definitely, of course, some
time is needed for this, first step,

307
00:21:27,074 --> 00:21:29,784
LLM or SLM to compress your prompt.

308
00:21:29,824 --> 00:21:35,154
But on the other hand, prompt will be
shorter and, it might happen that you save

309
00:21:35,154 --> 00:21:37,324
some milliseconds just because of that.

310
00:21:37,874 --> 00:21:42,584
if we take prompt from our
first example about, email.

311
00:21:42,999 --> 00:21:47,239
about, new headphones and run
it through this LLM Lingua.

312
00:21:47,459 --> 00:21:50,309
I did it without any
fine tuning, just as it.

313
00:21:50,309 --> 00:21:53,389
So it took literally a couple of
minutes for me to set everything

314
00:21:53,459 --> 00:21:54,639
up locally on my machine.

315
00:21:55,119 --> 00:21:59,959
I immediately got 17% Compression
of this prompt and you see a prompt

316
00:22:00,179 --> 00:22:02,179
looks approximately the same, right?

317
00:22:02,179 --> 00:22:07,349
but definitely Something is removed,
but the devil is in the details.

318
00:22:07,649 --> 00:22:13,999
LLM lingua knows exactly what is
non essential for like other LLMs

319
00:22:14,779 --> 00:22:19,659
Yeah, so I really encourage you
to try this tool Now, some general

320
00:22:19,709 --> 00:22:21,819
recommendations about prompts.

321
00:22:22,339 --> 00:22:24,689
Be specific and clear.

322
00:22:25,059 --> 00:22:30,139
The more concrete your order, your
request, your ask to the prompt, the

323
00:22:30,149 --> 00:22:32,359
better chance you get nice results.

324
00:22:32,889 --> 00:22:37,769
At the same time, be descriptive
and if possible, use examples.

325
00:22:37,819 --> 00:22:44,639
Again, you might need to educate LLM a
bit on what, you expect as a completion.

326
00:22:45,139 --> 00:22:46,699
Order of the components.

327
00:22:47,164 --> 00:22:49,264
Of your prompt matters.

328
00:22:49,914 --> 00:22:53,994
There is no any kind of,
strict rule or rules on that.

329
00:22:54,044 --> 00:22:56,924
only some recommendations,
and it's on the next slide.

330
00:22:57,394 --> 00:22:59,884
Sometimes you need to double down.

331
00:22:59,884 --> 00:23:04,184
Sometimes you need to repeat
either instruction or format, or

332
00:23:04,184 --> 00:23:05,834
any other component of the prompt.

333
00:23:05,884 --> 00:23:06,874
again, this is not a.

334
00:23:07,374 --> 00:23:12,524
Mandatory, absolutely, but if
you are unhappy with, the results

335
00:23:12,524 --> 00:23:15,034
you get back, try it, experiment.

336
00:23:15,764 --> 00:23:20,434
And for the cases when we talk about
classification and categorization, you

337
00:23:20,484 --> 00:23:23,684
better explain in your prompt explicitly.

338
00:23:23,894 --> 00:23:24,144
Like this.

339
00:23:24,604 --> 00:23:29,224
If you don't know which category to
put this text in, you better say,

340
00:23:29,304 --> 00:23:32,994
I don't know, rather than force put
it into the, one or another bucket

341
00:23:33,034 --> 00:23:34,584
that you provided in the prompt.

342
00:23:34,604 --> 00:23:38,144
It will save you time on,
validation of the results.

343
00:23:38,644 --> 00:23:41,784
Based on this general recommendation
some a bit more technical

344
00:23:41,784 --> 00:23:43,864
ones or more concrete ones.

345
00:23:43,864 --> 00:23:49,244
We can say like this Back to order
matters Normally for your optimal

346
00:23:49,254 --> 00:23:53,724
prompt you start with clear instructions
in some cases You might want to

347
00:23:53,724 --> 00:23:55,244
repeat instructions at the end.

348
00:23:55,274 --> 00:23:57,024
Again, this is not a requirement.

349
00:23:57,084 --> 00:24:01,269
You just need to experiment Don't be
shy to provide very clear instructions

350
00:24:01,329 --> 00:24:06,259
Syntax, in your prompt, just, provide some
separators between different, sections.

351
00:24:06,279 --> 00:24:07,839
You can even call these sections.

352
00:24:07,899 --> 00:24:11,629
the better chance you explain
LLM what is what in your prompt,

353
00:24:11,679 --> 00:24:13,769
the better result you get back.

354
00:24:14,489 --> 00:24:15,969
Don't try to multitask.

355
00:24:16,174 --> 00:24:21,034
One prompt for one task, you
better organize set of, the calls,

356
00:24:21,054 --> 00:24:24,554
like chain the calls if you need
to, perform something complex.

357
00:24:25,014 --> 00:24:29,904
And also, many providers of the models
and many models are capable to accept,

358
00:24:29,954 --> 00:24:34,614
some extra parameters, not only prompt
itself, but some, parameters for,

359
00:24:34,644 --> 00:24:40,844
for C in case of GPD family these two
parameters called temperature and Top

360
00:24:40,854 --> 00:24:47,344
probabilities and they both affect how
creative this model is, how deterministic,

361
00:24:47,404 --> 00:24:50,654
you can, you want your answers to be.

362
00:24:50,694 --> 00:24:55,564
Of course, Models, model output
are, non deterministic, right?

363
00:24:55,624 --> 00:24:59,494
but if you, for example, put temperature
to zero and top probabilities, to zero,

364
00:24:59,954 --> 00:25:04,964
the better chance that you will get
the same result for, the same prompt.

365
00:25:05,114 --> 00:25:07,814
Otherwise, if you put everything
to the maximum, it will be

366
00:25:07,874 --> 00:25:08,969
as creative as possible.

367
00:25:09,129 --> 00:25:09,559
possible.

368
00:25:10,059 --> 00:25:15,859
Let me list a couple of techniques that
is widely used in prompt engineering.

369
00:25:15,949 --> 00:25:21,119
And again, it might be very useful in
your, in career of prompt engineer or

370
00:25:21,159 --> 00:25:27,219
AI engineer, or just a developer and,
zero shot versus few short prompts by

371
00:25:27,499 --> 00:25:30,099
saying, short, we mean example here.

372
00:25:30,429 --> 00:25:30,909
and let's.

373
00:25:31,239 --> 00:25:36,659
Let's imagine that we are building,
some automation tool for insurance

374
00:25:36,659 --> 00:25:38,499
company for first line of support.

375
00:25:38,549 --> 00:25:43,589
And we gather it, question from,
Our customer via email automation or

376
00:25:43,589 --> 00:25:47,979
maybe a transcribed phone conversation
and we want to pass it to this or

377
00:25:47,979 --> 00:25:49,909
that department of our company.

378
00:25:50,099 --> 00:25:52,739
It's either auto insurance
or flat insurance.

379
00:25:52,829 --> 00:25:54,779
the prompt is pretty straightforward.

380
00:25:54,919 --> 00:25:59,379
We ask to categorize one, two, three,
and this prompt illustrates one of the

381
00:25:59,379 --> 00:26:05,404
techniques I mentioned giving model and
So if the question is not relevant, you

382
00:26:05,404 --> 00:26:09,614
better, say it's not relevant, just,
mark it as three in this particular

383
00:26:09,614 --> 00:26:14,024
case, rather than force push it to
either auto or home flood insurance.

384
00:26:14,104 --> 00:26:17,584
Again, we'll save some time
on validation of the results.

385
00:26:18,044 --> 00:26:19,994
This prompt might work good.

386
00:26:20,334 --> 00:26:25,374
If it still fails in some, situations,
you can educate this a bit.

387
00:26:25,874 --> 00:26:31,514
Use the same prompt, but somewhere in
the middle, inject a couple of examples,

388
00:26:31,514 --> 00:26:36,844
maybe examples that use specific words
from this specific, field, maybe it's

389
00:26:36,844 --> 00:26:40,974
like from your specific geo area, or
I don't know, something that is from

390
00:26:41,044 --> 00:26:46,274
a real use cases, and, where a model,
for example, failed last time, you

391
00:26:46,274 --> 00:26:50,579
can supply this as an example and
you'll get definitely better results.

392
00:26:50,769 --> 00:26:52,589
Also, you have to find the balance.

393
00:26:52,969 --> 00:26:54,269
your prompt becomes a bit.

394
00:26:54,769 --> 00:26:55,629
Longer, right?

395
00:26:55,629 --> 00:27:00,159
That means a bit more expensive,
maybe, slightly, longer time to

396
00:27:00,209 --> 00:27:04,809
process completion, but, content or
like result is the king after all.

397
00:27:04,819 --> 00:27:06,469
We want, perfect, output.

398
00:27:06,969 --> 00:27:10,199
Another super powerful technique
called chain of output.

399
00:27:10,374 --> 00:27:13,464
this particular example
is solving math problem.

400
00:27:13,684 --> 00:27:19,774
First of all, maybe, I'd say that LLMs
are not perfect mathematicians at all.

401
00:27:19,834 --> 00:27:24,424
and, normally you don't use this for
solving any kind of, math problems.

402
00:27:24,604 --> 00:27:28,454
But, I really want to illustrate
this technique, by its, simpler

403
00:27:28,474 --> 00:27:31,194
of all to illustrate it by
exactly this math problem.

404
00:27:31,254 --> 00:27:31,634
story.

405
00:27:31,734 --> 00:27:38,034
So just imagine that we, want to ask, for
an answer on some simple math operation.

406
00:27:38,034 --> 00:27:42,154
So in the white, prompt in the
light blue, there is, output or

407
00:27:42,154 --> 00:27:46,714
completion, more technical, term
result is here and it's wrong.

408
00:27:46,944 --> 00:27:49,874
So 8 million liters per year
is completely incorrect.

409
00:27:50,084 --> 00:27:51,924
I don't want to dive too deep.

410
00:27:51,994 --> 00:27:54,844
Why that happens in
very simple words, LLMs.

411
00:27:55,129 --> 00:27:58,939
are trying to find the lowest
hanging fruit in, in all

412
00:27:58,939 --> 00:28:00,299
this, kind of calculations.

413
00:28:00,299 --> 00:28:02,139
Of course, it's not real calculations.

414
00:28:02,569 --> 00:28:07,779
but I want to explain to you how to
fix this and, possibly many other

415
00:28:07,799 --> 00:28:17,369
situations, not all, not, limited to any
kind of, math or If we add one simple

416
00:28:17,429 --> 00:28:21,309
sentence, let's think step by step
and explain calculations step by step.

417
00:28:21,789 --> 00:28:25,349
As a result, our completion will
be a bit longer, that's fine.

418
00:28:25,769 --> 00:28:28,229
But the most important thing is the time.

419
00:28:28,754 --> 00:28:30,124
The answer is correct.

420
00:28:30,394 --> 00:28:33,714
And, also, by the way,
this is not, exact wording.

421
00:28:33,744 --> 00:28:35,284
You have to use all the time.

422
00:28:35,284 --> 00:28:36,594
This let's think step by step.

423
00:28:36,594 --> 00:28:41,574
No, you can experiment with a longer,
shorter description, something like

424
00:28:41,574 --> 00:28:48,104
customized description, but you have
to, force model to literally think step

425
00:28:48,124 --> 00:28:52,934
by step or provide reasoning behind
every step and results will be better.

426
00:28:52,984 --> 00:28:53,984
you will clearly see this.

427
00:28:54,484 --> 00:29:00,914
Another technique is not exactly about
prompting itself, but how we construct

428
00:29:00,994 --> 00:29:03,674
overall communication with LLMs.

429
00:29:04,314 --> 00:29:09,754
And in many cases, we need
to send multiple calls to

430
00:29:09,794 --> 00:29:11,074
complete one particular task.

431
00:29:11,614 --> 00:29:12,634
I already told you that.

432
00:29:13,024 --> 00:29:20,054
Not good idea to multitask one prompt
for one operation, but, you can easily

433
00:29:20,054 --> 00:29:24,304
organize prompt chaining by orchestrating
some your, some, backend tooling.

434
00:29:24,594 --> 00:29:29,504
And, you can use again, multiple
models for multiple tasks, hosted

435
00:29:29,534 --> 00:29:33,224
model from one provider, another
provider, your own model, local model,

436
00:29:33,614 --> 00:29:35,944
fine tuned model, general model.

437
00:29:36,014 --> 00:29:37,144
you decide, right?

438
00:29:37,144 --> 00:29:37,734
What works.

439
00:29:37,889 --> 00:29:39,389
Best for yourself.

440
00:29:39,619 --> 00:29:41,069
And the whole idea is very simple.

441
00:29:41,239 --> 00:29:46,919
You can use output of a previous call
to the model or part of this output as

442
00:29:46,919 --> 00:29:49,439
a part of the input for your next call.

443
00:29:49,439 --> 00:29:55,179
So this way you achieve a very good,
situation when you, literally, get

444
00:29:55,209 --> 00:29:57,825
what you want with, minimal efforts.

445
00:29:57,825 --> 00:30:03,477
Now I want to introduce Cambridge
dictionary word of the year 2023.

446
00:30:03,477 --> 00:30:03,948
2023.

447
00:30:04,448 --> 00:30:06,158
And this is hallucination.

448
00:30:07,018 --> 00:30:09,988
Hallucination in context
of our interactions with

449
00:30:09,998 --> 00:30:11,618
the large language models.

450
00:30:11,748 --> 00:30:17,438
And this is something that is really
annoying in all kinds of AI engineering,

451
00:30:17,498 --> 00:30:22,478
in all kinds of prompt engineering, all
kinds of building AI infused applications.

452
00:30:22,978 --> 00:30:28,178
So this is again, outcome
of, how LMS were, designed,

453
00:30:28,178 --> 00:30:30,068
invented, and how they work.

454
00:30:30,178 --> 00:30:34,748
in, in summary, they are very
good in making up facts and,

455
00:30:34,748 --> 00:30:37,118
making this in a very trusty way.

456
00:30:37,118 --> 00:30:42,878
So it's very, complex sometimes to
identify what is, correct in this

457
00:30:42,878 --> 00:30:48,218
output and what is wrong if we talk
about some, as some facts we requested.

458
00:30:48,718 --> 00:30:53,438
Fortunately, there are multiple ways
for, if not removing completely,

459
00:30:53,468 --> 00:30:57,828
but reducing hallucination,
mitigating its consequences.

460
00:30:58,648 --> 00:31:03,308
For example, you can explain model not
only what you want, but also what you

461
00:31:03,308 --> 00:31:05,298
don't want to receive back from this.

462
00:31:05,298 --> 00:31:08,683
So you can limit number of use cases.

463
00:31:09,143 --> 00:31:13,793
Also, this is a recommendation that I
already introduced, give, model and out.

464
00:31:13,883 --> 00:31:16,543
If you're not sure, say, I don't know.

465
00:31:16,573 --> 00:31:20,643
This is, this could, this simple statement
could be your part of the prompt.

466
00:31:21,113 --> 00:31:25,343
As well as this one, sometimes it's, of
course, it's super naive technique, right?

467
00:31:25,403 --> 00:31:27,033
But sometimes that, that works.

468
00:31:27,063 --> 00:31:28,123
Don't make up facts.

469
00:31:28,513 --> 00:31:32,993
As a part of your prompt, if there's
a chance for you to organize, multiple

470
00:31:33,053 --> 00:31:38,053
short conversation with, your LLM, like
chat, like maybe you can, ask every

471
00:31:38,053 --> 00:31:41,663
time, are you sure that you have all
information to answer this question?

472
00:31:41,723 --> 00:31:44,183
If not, you better
request some extra months.

473
00:31:44,693 --> 00:31:49,943
step by step reasoning and asking
model to explain along with the answer.

474
00:31:49,983 --> 00:31:53,813
Basically, this is a chain of
thoughts technique also helps,

475
00:31:54,313 --> 00:32:00,333
but all these points, what on the
picture are nothing compared to this.

476
00:32:00,623 --> 00:32:06,043
You can dynamically find and inject
relevant context in information

477
00:32:06,093 --> 00:32:07,813
straight into your prompt.

478
00:32:07,843 --> 00:32:12,173
And you can say, Use only
this information for answer.

479
00:32:12,213 --> 00:32:15,973
I mean forget all your knowledge
from wikipedia and the public books

480
00:32:16,623 --> 00:32:21,753
Discard this we only need you to use
this data in its trade in the prompt.

481
00:32:22,253 --> 00:32:30,513
I can illustrate this by Example
of us in the role of developers who

482
00:32:30,523 --> 00:32:37,068
build internal application for Say our
employees to investigate what's possible

483
00:32:37,128 --> 00:32:39,578
in medical insurance coverage for them.

484
00:32:40,038 --> 00:32:44,858
And let's imagine that we got a question
from employee, via chat bot, or again,

485
00:32:44,898 --> 00:32:46,588
via some automated email engine.

486
00:32:47,028 --> 00:32:49,758
Does my health plan
cover annual eye exams?

487
00:32:50,258 --> 00:32:53,698
And at the top of the prompt, that
we explain all the, prerequisites.

488
00:32:53,708 --> 00:32:59,928
We are intelligent assistant and, yeah,
you can answer, questions about health

489
00:32:59,958 --> 00:33:08,278
care and use Only sources below for the
answer and we also directly inject into

490
00:33:08,278 --> 00:33:13,358
this prompt These three sources that are
relevant for answering this question.

491
00:33:13,698 --> 00:33:15,048
Sounds very simple, right?

492
00:33:15,258 --> 00:33:20,098
But of course we have to and
yeah completion in that case

493
00:33:20,178 --> 00:33:22,498
will be nice and 100% Correct.

494
00:33:22,808 --> 00:33:26,208
But how do we exactly
identify these sources?

495
00:33:26,218 --> 00:33:28,198
How do we find these sources?

496
00:33:28,508 --> 00:33:33,078
These few sentences in, for example,
dozens of, PDFs or documents

497
00:33:33,108 --> 00:33:35,278
or somewhere in, in database.

498
00:33:35,978 --> 00:33:37,618
How we identify these sources?

499
00:33:37,808 --> 00:33:39,608
How we, shorten these sources?

500
00:33:39,608 --> 00:33:43,508
How we rank them after all
to provide top 3 or 5 or 10,

501
00:33:43,718 --> 00:33:45,708
depending on your use case, items.

502
00:33:46,238 --> 00:33:52,498
And Answer to this question is Retrieval
Augmented Generation Pattern or REG.

503
00:33:52,958 --> 00:33:58,618
as name says, it's about retrieve these
sources, these data sources, augment

504
00:33:58,678 --> 00:34:02,448
them and, augment your promptery
and after all generate completion.

505
00:34:02,698 --> 00:34:04,188
Generate completion is trivial.

506
00:34:04,188 --> 00:34:08,818
This is after, about, sending
your final, Call to, final prompt

507
00:34:08,858 --> 00:34:11,728
to LLM augment is even simpler.

508
00:34:11,738 --> 00:34:13,548
This is just a string operation.

509
00:34:13,558 --> 00:34:18,378
This is, literally inserting sources
of information into your prompt.

510
00:34:18,408 --> 00:34:22,518
Like we've seen on this previous
example, retrieve is the real magic.

511
00:34:22,838 --> 00:34:23,418
How?

512
00:34:23,658 --> 00:34:28,608
based on this particular question or on,
question plus previous conversation, we

513
00:34:28,608 --> 00:34:34,668
really identify data we need from, again,
back to, use case with, dozens of PDFs

514
00:34:34,698 --> 00:34:39,268
with all this insurance, information,
and of course, here we can leverage.

515
00:34:39,523 --> 00:34:43,773
one more large language model, this
one, say one that we use for the final

516
00:34:43,773 --> 00:34:47,473
call, or most likely in vast majority
of situations, this will be completely

517
00:34:47,473 --> 00:34:53,363
different one, specialized, and also,
you might want to, vectorize your

518
00:34:53,363 --> 00:34:57,773
request and, those you have to send
this request to vectorize database.

519
00:34:58,153 --> 00:35:00,468
Unfortunately, it's all,
out of our today's schedule.

520
00:35:00,598 --> 00:35:05,528
Scope this could be your homework to
learn more about retrieve component of

521
00:35:05,848 --> 00:35:11,738
rag and in the real world It's even more
complex because it could be multiple

522
00:35:11,748 --> 00:35:13,788
sources of this information, right?

523
00:35:13,788 --> 00:35:18,728
So before Constructing your final prompt
that contains everything you might

524
00:35:18,748 --> 00:35:25,168
want to send requests to your Internal
database to some external API and who

525
00:35:25,168 --> 00:35:32,068
knows where else so maybe you might
Want to have some orchestration support.

526
00:35:32,568 --> 00:35:33,588
Surprise, surprise.

527
00:35:33,618 --> 00:35:37,698
In real world, it's even more complex
than just, this orchestration.

528
00:35:38,018 --> 00:35:41,718
and if you talk not about this
particular call to or chain of

529
00:35:41,718 --> 00:35:43,658
calls to, LLM, but also about.

530
00:35:43,928 --> 00:35:47,668
Full life cycle of our AI
infused application or like

531
00:35:47,898 --> 00:35:49,998
a generative AI part of this.

532
00:35:50,268 --> 00:35:55,528
we want to get some tools, some,
frameworks for ideation, for

533
00:35:55,528 --> 00:36:00,763
building, for making everything,
real, how we deploy, how we, monitor.

534
00:36:00,763 --> 00:36:05,683
it's, let's say very specialized
part, DevOps, but specifically

535
00:36:05,703 --> 00:36:08,073
for your LLM interactions.

536
00:36:08,073 --> 00:36:09,603
We can call this LLMOps.

537
00:36:10,473 --> 00:36:15,153
So let me introduce more tools
that can help us both with, this,

538
00:36:15,203 --> 00:36:20,423
orchestration of our calls and the
operationalization of all flow.

539
00:36:20,923 --> 00:36:24,603
First couple of tools I want to mention
is LangChain and Semantic Kernel.

540
00:36:24,653 --> 00:36:29,673
And these are amazing orchestrators
of everything you need to

541
00:36:29,703 --> 00:36:32,133
interact with any kind of LLM.

542
00:36:32,353 --> 00:36:36,113
So these two libraries or frameworks,
you name it, are very similar.

543
00:36:36,373 --> 00:36:39,643
They support slightly different,
Set of programming language slightly

544
00:36:39,643 --> 00:36:46,223
different set of Platforms but in a
nutshell they are helping you to Get

545
00:36:46,233 --> 00:36:52,583
very nice abstraction over all your
interactions with LLMs so you don't

546
00:36:52,603 --> 00:36:58,233
need to write all these calls all this
ping pong from scratch also There are

547
00:36:58,233 --> 00:37:03,993
many more very interesting features
included in both of the framework.

548
00:37:03,993 --> 00:37:08,348
So When you start your new AI
infused application, don't ever

549
00:37:08,358 --> 00:37:12,318
start, like your, low level,
communication with LLM from scratch.

550
00:37:12,838 --> 00:37:17,858
Find your perfect, level of abstraction,
in these two or any other frameworks.

551
00:37:18,358 --> 00:37:23,908
And for overall orchestration or
personalization of your LLM application,

552
00:37:23,968 --> 00:37:28,818
I recommend you to have a look at
PromptFlow, another open source tool.

553
00:37:28,858 --> 00:37:34,118
this one also created by my colleagues
from Microsoft that in code first

554
00:37:34,348 --> 00:37:42,888
way can help you again to keep a full
control over full cycle of your LLM,

555
00:37:43,158 --> 00:37:45,488
development starting from experimentation.

556
00:37:45,813 --> 00:37:47,273
Up to monitoring

557
00:37:47,773 --> 00:37:48,303
from flow.

558
00:37:48,303 --> 00:37:49,493
You I could look like this.

559
00:37:49,703 --> 00:37:53,293
This particular short video
is from, the hosted version.

560
00:37:53,333 --> 00:37:59,063
There is one on Azure, but you
can get the same UI straight in

561
00:37:59,063 --> 00:38:01,943
your, VS code ID, via respective.

562
00:38:02,103 --> 00:38:04,043
extension called PromptFlow.

563
00:38:04,203 --> 00:38:12,573
basically it reproduces your interaction
with LLMs as a graph where some

564
00:38:12,573 --> 00:38:14,813
nodes are pieces of Python code.

565
00:38:14,973 --> 00:38:17,843
Some nodes are calling of LLMs.

566
00:38:17,843 --> 00:38:23,403
Some nodes represent prompts where you
can iterate through multiple variants,

567
00:38:23,403 --> 00:38:25,413
for example, to identify what is the best.

568
00:38:25,693 --> 00:38:28,833
And after all, you can host everything
on the cloud of your choice.

569
00:38:29,333 --> 00:38:34,303
There are many learning resources
available about prompt engineering,

570
00:38:34,763 --> 00:38:40,613
and, I hope that you will get PDF of,
this session with all active links.

571
00:38:41,113 --> 00:38:43,353
What is the future of prompt engineering?

572
00:38:43,753 --> 00:38:45,273
I have set up the questions.

573
00:38:45,383 --> 00:38:47,233
I will not give you answers on.

574
00:38:47,403 --> 00:38:50,963
I encourage you, yourself find the answer.

575
00:38:51,193 --> 00:38:51,803
For example.

576
00:38:52,283 --> 00:38:55,553
Will it be a separate job
title or just essential skill?

577
00:38:56,013 --> 00:39:01,263
Will it become simpler with tools like
Longchain, Semantic Kernel, PromptFlow?

578
00:39:01,483 --> 00:39:04,833
Or they become more complex because
there is multi modality, you

579
00:39:04,833 --> 00:39:09,613
have to closely work with vectors
if you talk about React, etc.

580
00:39:10,113 --> 00:39:15,543
Doos, will it be democratized and pretty
much everyone who ever sent a request

581
00:39:15,553 --> 00:39:17,178
to ChatGPT will call themselves Doos?

582
00:39:17,318 --> 00:39:22,618
Prompt engineer those, there'll be title
inflation or it will be more gated,

583
00:39:22,818 --> 00:39:28,868
discipline and who will benefit the most
people who understand how our language

584
00:39:28,888 --> 00:39:33,308
constructed linguists or people who
understand how code works technologies.

585
00:39:33,788 --> 00:39:38,428
Or maybe people who know everything
about this particular domain,

586
00:39:38,778 --> 00:39:43,458
where this application comes from
and able to formulate the problem.

587
00:39:43,928 --> 00:39:48,528
And do we survive competition
with LLM based prompt engineers?

588
00:39:48,528 --> 00:39:51,328
Because LLMs are also
perfect in creating prompts.

589
00:39:51,828 --> 00:39:56,908
I invite you all to the Prompt Engineering
Conference I organize, in November

590
00:39:56,938 --> 00:39:58,508
and this will be second edition.

591
00:39:58,558 --> 00:40:01,638
First of edition was,
last year, super popular.

592
00:40:02,128 --> 00:40:06,988
and, if you watch this session
before November 20, just go

593
00:40:07,038 --> 00:40:08,058
and register your ticket.

594
00:40:08,058 --> 00:40:11,898
If you watch this after,
still go to the same URL.

595
00:40:11,918 --> 00:40:15,818
You will find all sessions
recorded straight there.

596
00:40:16,418 --> 00:40:17,668
And it's online, it's free.

597
00:40:17,748 --> 00:40:19,378
It's free, open for everyone.

598
00:40:19,878 --> 00:40:26,188
Thank you very much for watching this
session and my last prompt for everyone.

599
00:40:26,398 --> 00:40:27,438
Let's stay in touch.

600
00:40:27,518 --> 00:40:28,968
This is my LinkedIn profile.

601
00:40:28,988 --> 00:40:33,928
Find me, just scan this QR code or find
Maxim Salnikov Microsoft on LinkedIn.

602
00:40:34,148 --> 00:40:36,778
And it's my great pleasure
to stay connected with you.

603
00:40:37,088 --> 00:40:41,018
Ask me about, prompt engineering,
AI engineering, any questions

604
00:40:41,068 --> 00:40:44,698
or, any questions about web
development and cloud in general.

605
00:40:45,368 --> 00:40:46,558
Thank you very much.

