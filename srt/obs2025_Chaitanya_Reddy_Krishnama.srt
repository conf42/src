1
00:00:01,440 --> 00:00:02,220
Hello everyone.

2
00:00:02,700 --> 00:00:06,690
I'm Chidi and I'm here to talk
about something that's haunted.

3
00:00:06,690 --> 00:00:11,580
Many data teams like broken dashboards
that look fine, but silently lie.

4
00:00:12,840 --> 00:00:17,580
In this talk, I will walk you through
our journey from those chaotic moments

5
00:00:17,880 --> 00:00:22,680
to building a mature observability
framework for data quality assurance.

6
00:00:23,100 --> 00:00:28,440
This isn't just about testing, it's
all about regaining trust in our data.

7
00:00:28,830 --> 00:00:31,860
By building an observability QA culture

8
00:00:35,730 --> 00:00:40,710
problem statement, our dashboards
looked polished, but under the hood

9
00:00:40,920 --> 00:00:46,560
we have the data issues like silent
pipeline failures and schema drift,

10
00:00:47,129 --> 00:00:49,470
which leads to misleading KPIs.

11
00:00:50,280 --> 00:00:54,330
By the time stakeholders flagged
inconsistencies, it was often too late.

12
00:00:54,900 --> 00:00:58,320
This created confusion, delayed decisions.

13
00:00:58,665 --> 00:01:01,275
And Erod a trust in analytics.

14
00:01:02,445 --> 00:01:04,575
Shiny dashboards, shaky truth.

15
00:01:05,535 --> 00:01:11,115
We had sleek analytic dashboards that
suddenly began giving incorrect insights.

16
00:01:11,655 --> 00:01:13,575
What you see isn't always what you get.

17
00:01:13,965 --> 00:01:18,735
Our data looked fine on the surface,
but was rotten underneath leading

18
00:01:18,735 --> 00:01:21,015
to bad decisions and lost trust.

19
00:01:22,155 --> 00:01:23,835
There were hidden data issues.

20
00:01:24,420 --> 00:01:28,979
Undetected schema changes and
silent pipeline failures introduced

21
00:01:29,009 --> 00:01:30,509
errors into these metrics.

22
00:01:30,869 --> 00:01:35,490
These issues were hidden behind the
scenes with no immediate visible signs.

23
00:01:36,149 --> 00:01:39,869
By the time someone noticed
damage was already done, and

24
00:01:39,869 --> 00:01:41,340
the last is eroding trust.

25
00:01:42,060 --> 00:01:44,580
Stakeholders grew vary of the data.

26
00:01:44,850 --> 00:01:48,449
Once a dashboard tells a lie,
every report becomes a suspect.

27
00:01:48,899 --> 00:01:51,119
Data teams morale clenched.

28
00:01:51,510 --> 00:01:55,680
As more time went into fight
fighting than delivering value.

29
00:01:56,220 --> 00:02:00,840
In fact, industry surveys show
data downtime, which can consume

30
00:02:00,840 --> 00:02:06,120
over 30% of their data teams time
and erode confidence in analytics.

31
00:02:08,580 --> 00:02:13,650
And here comes see real production pain
points, delayed detection and firefights.

32
00:02:14,055 --> 00:02:17,894
Because issues weren't quarterly,
we often operated in reactive mode

33
00:02:17,954 --> 00:02:22,035
by discovering errors only when a
stakeholder raised a concern, this

34
00:02:22,035 --> 00:02:26,475
meant frantic late night troubleshooting
sessions fixed on the fly patches

35
00:02:26,625 --> 00:02:28,605
and data fire drills that burned out.

36
00:02:28,605 --> 00:02:31,635
The team eventually brittle
validations at scale.

37
00:02:32,295 --> 00:02:36,945
The few data quality rules we had
would often break whenever data volume

38
00:02:36,945 --> 00:02:38,685
spiked or new fields were added.

39
00:02:39,165 --> 00:02:40,125
They weren't scalable at all.

40
00:02:41,490 --> 00:02:45,990
Writing exhaustive tests for thousands
of data assets was impractical

41
00:02:46,109 --> 00:02:47,340
with a traditional approach.

42
00:02:47,609 --> 00:02:51,780
The result, blind spots where
bad data could slip in unchecked,

43
00:02:52,560 --> 00:02:54,359
silent pipeline failures.

44
00:02:54,780 --> 00:02:59,565
In some cases, jobs failed or only
partially succeeded without load errors.

45
00:03:00,450 --> 00:03:04,710
Data stopped updating our certain
records didn't load, and we didn't

46
00:03:04,710 --> 00:03:08,730
find out until days later when
someone noticed a bi bizarre trend.

47
00:03:09,120 --> 00:03:11,430
And then next comes POO test coverage.

48
00:03:11,790 --> 00:03:16,920
Our existing data checks were minimal
and brittle, so we might test a couple

49
00:03:16,920 --> 00:03:21,300
of critical tables or use basic QA
scripts, but many assumptions went

50
00:03:21,300 --> 00:03:23,615
untested with hundreds of tables.

51
00:03:23,880 --> 00:03:25,650
We simply didn't cover everything.

52
00:03:26,460 --> 00:03:28,620
And the last is undetected schema drift.

53
00:03:29,685 --> 00:03:33,704
Small upstream changes like a
renamed column or change data

54
00:03:33,704 --> 00:03:35,984
format broke our data silently.

55
00:03:36,525 --> 00:03:39,795
Pipelines kept running, but
key fields got misaligned.

56
00:03:40,125 --> 00:03:44,894
We had no immediate alerts for schema
changes, so reports quietly went wrong.

57
00:03:46,095 --> 00:03:50,204
Moving on to wide traditional qf,
failed pipeline, not equal to software.

58
00:03:51,255 --> 00:03:55,005
Traditionally, data QF was
treated as a one-time checkbox.

59
00:03:55,484 --> 00:04:00,825
We would often test queries or ETLs during
development and assume if the pipeline

60
00:04:00,825 --> 00:04:03,015
ran without error, the data must be fine.

61
00:04:03,525 --> 00:04:07,484
Unlike application qa, there is
little continuous testing once a

62
00:04:07,484 --> 00:04:10,635
code has been moved into production,
but that's a grail mistake.

63
00:04:11,445 --> 00:04:16,575
Incomplete testing data is fluid and
evolving, but our tests were static.

64
00:04:16,844 --> 00:04:20,565
We checked a few sample outputs
or verified logic on day one,

65
00:04:20,805 --> 00:04:24,225
but as data changed over months,
those tests didn't adapt.

66
00:04:24,570 --> 00:04:27,960
We lacked coverage for new
edge cases and evolving data

67
00:04:27,960 --> 00:04:31,230
patterns not built for scale.

68
00:04:31,740 --> 00:04:35,670
Maintain data quality by
manually scripting tests for

69
00:04:35,670 --> 00:04:37,770
every table was unsustainable.

70
00:04:38,820 --> 00:04:44,040
If you have thousand plus tables,
writing even five simple checks per table

71
00:04:44,040 --> 00:04:49,680
means thousands of tasks to code and
update, which is clearly unrealistic.

72
00:04:50,055 --> 00:04:54,405
Traditional QA approaches buckled
under the volume and complexity of

73
00:04:54,405 --> 00:04:57,675
modern data QA processes in silos.

74
00:04:58,425 --> 00:05:02,745
Our QA happened mostly before
deployment, separated from monitoring.

75
00:05:03,015 --> 00:05:05,475
Once pipelines were live, there was a gap.

76
00:05:05,895 --> 00:05:08,745
We had no built-in feedback
loop from production.

77
00:05:08,835 --> 00:05:09,615
Data behavior.

78
00:05:09,615 --> 00:05:10,455
Back to qa.

79
00:05:11,025 --> 00:05:16,950
The first indicator of trouble was often
a user complaint that read it Monday, 8:00

80
00:05:16,950 --> 00:05:19,125
AM Slack message about the broken data.

81
00:05:20,175 --> 00:05:25,965
In practice, a lot of QA relied on manual
checks, eyeballing a report, or running

82
00:05:25,965 --> 00:05:28,725
a SQL snippet to spot check the results.

83
00:05:29,385 --> 00:05:34,755
This doesn't scale our cat settle issues
because human eyeballs can't watch

84
00:05:34,845 --> 00:05:36,675
thousands of metrics in real time.

85
00:05:37,335 --> 00:05:41,805
In summary, our old QA approach
was too shallow and too slow.

86
00:05:42,180 --> 00:05:47,070
It failed to catch the very issues that
mattered because it wasn't continuous

87
00:05:47,250 --> 00:05:49,469
automated or intelligent enough.

88
00:05:49,650 --> 00:05:51,090
We needed a new start.

89
00:05:51,960 --> 00:05:56,430
Moving on to Pvo two observability,
talking about the light bulb movement.

90
00:05:56,760 --> 00:06:00,240
Our turning point was realizing
that we had to treat the data

91
00:06:00,240 --> 00:06:03,719
pipelines like production
software in software engineering.

92
00:06:03,750 --> 00:06:06,270
You wouldn't deploy an app
without monitoring and logging.

93
00:06:07,050 --> 00:06:09,090
Why were we running data
systems in the dark?

94
00:06:09,510 --> 00:06:13,169
This lead us to adopt observability
principles for data qa.

95
00:06:13,770 --> 00:06:15,780
But what actually is observability?

96
00:06:16,169 --> 00:06:21,210
It's our ability to fully understand
what's happening inside a system by

97
00:06:21,210 --> 00:06:27,900
looking at its outputs, be it logs,
metrics, and traces in a data context.

98
00:06:28,080 --> 00:06:33,270
Observability means instrumenting
pipelines, so we can monitor data health

99
00:06:33,390 --> 00:06:38,370
end to end instead of assuming the
data is fine, we actively watch for.

100
00:06:38,880 --> 00:06:39,840
Signs of trouble.

101
00:06:40,830 --> 00:06:45,419
And in the new approach, we treat
data as a code where we began adding

102
00:06:45,419 --> 00:06:51,299
telemetry everywhere, logging each
stage of the ETL, measuring row counts

103
00:06:51,390 --> 00:06:56,669
and crossing times and tracing data
flows from source to the dashboard.

104
00:06:57,179 --> 00:06:59,849
Now, when something went
wrong, it wouldn't be silent.

105
00:07:00,179 --> 00:07:01,799
We would have breadcrumbs to follow.

106
00:07:02,220 --> 00:07:06,690
And the real time alerts where
observability lettuce detect failures

107
00:07:06,690 --> 00:07:08,580
and anomalies before users do.

108
00:07:09,180 --> 00:07:15,870
For example, we set up automated alerts
for when a daily load dint run, or when a

109
00:07:15,870 --> 00:07:18,570
metric deviated beyond the normal range.

110
00:07:19,140 --> 00:07:21,330
No more waiting weeks
to discard a problem.

111
00:07:21,600 --> 00:07:26,190
The system would tell us as an
article, put it without observability.

112
00:07:26,370 --> 00:07:31,530
You're essentially blind to the data
failures, proactive QA via monitoring.

113
00:07:32,010 --> 00:07:36,090
This P vote was essentially
blending QA with operations.

114
00:07:36,210 --> 00:07:41,340
Instead of QA being a one-off gate, it
became an ongoing process of monitoring

115
00:07:41,340 --> 00:07:43,229
data quality in the production.

116
00:07:43,679 --> 00:07:48,299
Our data quality checks now ran
continuously, and any violation

117
00:07:48,450 --> 00:07:51,835
triggered the same kind of response
as a site outage would be in DevOps.

118
00:07:52,349 --> 00:07:56,310
We moved from firefighting to
early detection and prevention.

119
00:07:57,210 --> 00:08:02,130
Now the key principles of observability
in data qa, we added pervasive visibility

120
00:08:02,130 --> 00:08:07,830
to instrument everything, be it locks,
metrics, traces, every pipeline step.

121
00:08:08,010 --> 00:08:12,419
Now emits locks and metrics, which
are quantitative measures like row

122
00:08:12,544 --> 00:08:14,969
counts, latencies, or error dates.

123
00:08:15,120 --> 00:08:19,385
We also implemented tracing to
track data flow across services.

124
00:08:19,935 --> 00:08:24,075
These three pillars, locks,
metrics, and traces are the

125
00:08:24,164 --> 00:08:25,815
foundation of observability.

126
00:08:26,145 --> 00:08:29,265
They let us ask what's happening
inside our data pipelines

127
00:08:29,325 --> 00:08:30,585
at any given point in time?

128
00:08:31,245 --> 00:08:32,085
Fail fast.

129
00:08:32,145 --> 00:08:32,895
Fail safe.

130
00:08:33,105 --> 00:08:37,635
Finally, we embraced a philosophy
from software testing Fail fast.

131
00:08:37,995 --> 00:08:42,074
If the data is bad, better to stop
the pipeline or flag it immediately

132
00:08:42,074 --> 00:08:44,925
than to quietly push 40 data onward.

133
00:08:45,224 --> 00:08:48,885
By halting and errors and
rerouting to a recovery process.

134
00:08:49,005 --> 00:08:51,704
We prevent bad data
from polluting reports.

135
00:08:51,974 --> 00:08:56,594
In practice, this meant designating
certain tests as critical.

136
00:08:56,834 --> 00:09:00,795
If they fail, they stop the line and
alert preventing the downstream harm.

137
00:09:01,995 --> 00:09:04,035
Data lineage and impact analysis.

138
00:09:04,185 --> 00:09:08,625
A key principle we adopted is
maintaining data lineage visibility.

139
00:09:09,435 --> 00:09:12,675
Lineage is a map of how data
flows from source to the target.

140
00:09:12,945 --> 00:09:16,995
When a problem arises in a data
set, lineage tells us what upstream

141
00:09:16,995 --> 00:09:20,415
source might have caused it and which
downstream reports might be affected.

142
00:09:20,685 --> 00:09:24,675
This is critical for rapid root
cause analysis and for deciding

143
00:09:24,704 --> 00:09:25,875
who needs to be involved.

144
00:09:26,324 --> 00:09:31,425
We built lineage tracking so that
no data issue remains a mystery as

145
00:09:31,425 --> 00:09:33,255
we can trace it across the pipeline.

146
00:09:33,944 --> 00:09:35,385
Data quality checks as code.

147
00:09:36,074 --> 00:09:40,245
We embedded automated tests directly
into our pipelines using tools

148
00:09:40,245 --> 00:09:42,980
like DBT or great expectations.

149
00:09:43,365 --> 00:09:44,775
We define the expectations.

150
00:09:44,954 --> 00:09:47,594
Example, no one else in critical columns.

151
00:09:47,925 --> 00:09:52,485
And the transaction amount should
be between dollar and $10,000

152
00:09:52,695 --> 00:09:54,705
that run whenever data is updated.

153
00:09:55,005 --> 00:09:59,505
If data violates an expectation,
the pipeline can fail fast rather

154
00:09:59,505 --> 00:10:01,485
than propagate the bad data.

155
00:10:01,815 --> 00:10:04,095
QA isn't a separate fees anymore.

156
00:10:04,275 --> 00:10:06,435
It's baked into each data transformation.

157
00:10:08,145 --> 00:10:10,005
Continuous monitoring of the data health.

158
00:10:10,365 --> 00:10:13,665
We monitor key data, health
indicators 24 by seven.

159
00:10:13,875 --> 00:10:15,585
This includes freshness.

160
00:10:15,960 --> 00:10:18,840
Like, is data up to date the volume?

161
00:10:19,140 --> 00:10:23,310
Did we get all the records or
some are missing distribution.

162
00:10:23,490 --> 00:10:27,360
Our values within the normal
ranges and schema changes.

163
00:10:27,480 --> 00:10:29,670
Did someone add or remove a column?

164
00:10:30,060 --> 00:10:35,070
By tracking these, we catch anomalies
like sudden drops in row counts or

165
00:10:35,070 --> 00:10:37,140
unexpected null spikes immediately.

166
00:10:37,740 --> 00:10:40,110
Alerting and incident response.

167
00:10:40,530 --> 00:10:44,460
Observability isn't just about passive
monitoring, it's active alerting.

168
00:10:44,939 --> 00:10:49,199
We configured threshold based and
anomaly based alerts that pays

169
00:10:49,199 --> 00:10:51,060
the team when something goes off.

170
00:10:51,270 --> 00:10:56,040
For instance, if a daily sales table
usually has a hundred K rows and

171
00:10:56,040 --> 00:11:01,140
today has zero and alert fires, if an
ETL job doesn't run by its scheduled

172
00:11:01,140 --> 00:11:03,780
time, we get notified this way.

173
00:11:04,380 --> 00:11:08,460
Data issues trigger a response, just
like application downtime would.

174
00:11:08,790 --> 00:11:13,319
Our goal is to be the first to know
about the data issues, not at the last.

175
00:11:14,745 --> 00:11:18,915
Moving on to the tools and techniques,
used, great expectations, csar go-to

176
00:11:18,915 --> 00:11:21,135
framework for the data validation.

177
00:11:21,495 --> 00:11:21,795
Great.

178
00:11:21,795 --> 00:11:25,725
Expectations let's us define the
expectations about our data, like

179
00:11:25,725 --> 00:11:27,855
constraints or quality rules.

180
00:11:27,915 --> 00:11:32,385
In a read WA, we implemented great
expectation checkpoints in our pipelines.

181
00:11:32,625 --> 00:11:36,915
Say for example, after data load, we
automatically verify the row counts.

182
00:11:37,260 --> 00:11:39,720
Valid ranges, NU percentages.

183
00:11:40,020 --> 00:11:44,760
This catches bad data pretty much
early rather than at the latest stage.

184
00:11:45,660 --> 00:11:48,540
And the next is DBT, the data build tool.

185
00:11:49,020 --> 00:11:52,560
We leverage DBT not only for
transforming data, but also

186
00:11:52,560 --> 00:11:54,480
for its testing capabilities.

187
00:11:54,840 --> 00:11:57,329
DBT allows SQ SQL tests on your models.

188
00:11:57,510 --> 00:12:02,610
We wrote tests to ensure dimension
tables aren't missing keys, or that

189
00:12:02,610 --> 00:12:05,069
reference data matches expected values.

190
00:12:05,535 --> 00:12:10,575
DBTs integration with version control
meant our data tests are code reviewed and

191
00:12:10,575 --> 00:12:12,855
versioned, just like our transformations.

192
00:12:13,170 --> 00:12:16,980
It promotes the philosophy of
treating data pipeline code with

193
00:12:16,980 --> 00:12:18,930
the same rigor as application code.

194
00:12:19,590 --> 00:12:21,330
And the next is open telemetry.

195
00:12:21,690 --> 00:12:25,350
We instrumented our custom data
pipeline code with open telemetry

196
00:12:25,350 --> 00:12:27,870
for unified tracing and metrics.

197
00:12:28,050 --> 00:12:32,160
Open telemetry is an open standard
for collecting telemetry data.

198
00:12:32,400 --> 00:12:36,690
By using it, we could export
pipeline runtime metrics, like

199
00:12:36,810 --> 00:12:40,440
how long each stage took, how
many records have been processed.

200
00:12:40,724 --> 00:12:46,035
And distributed traces that followed a
data record from injection to output.

201
00:12:46,245 --> 00:12:47,805
This was huge for debugging.

202
00:12:48,045 --> 00:12:52,665
If a job was slow or failed, we could
trace exactly where it has happened.

203
00:12:53,834 --> 00:12:57,435
And for monitoring and visualization,
we use Prometheus and Grafana.

204
00:12:57,795 --> 00:13:00,555
All those metrics and
locks had to go somewhere.

205
00:13:00,765 --> 00:13:04,275
We set up Prometheus to scrape
and store pipeline metrics

206
00:13:04,275 --> 00:13:06,344
like row counts durations.

207
00:13:06,675 --> 00:13:10,995
Or error counts and Grafana dashboards
to visualize them in real time.

208
00:13:11,355 --> 00:13:16,215
Our Grafana dashboards became our data
pipeline control center showing the health

209
00:13:16,215 --> 00:13:18,795
of each data flow at a glance green.

210
00:13:18,795 --> 00:13:23,865
If all good, ready anomalies, for
example, we have chats for daily record

211
00:13:23,865 --> 00:13:28,605
counts per table with alert thresholds
if they deviate from historical norms.

212
00:13:29,415 --> 00:13:33,285
Our new process can be visualized
as a pipeline with built-in quality

213
00:13:33,285 --> 00:13:34,875
gates and monitoring at every step.

214
00:13:35,250 --> 00:13:38,850
Here is a high level architecture,
data sources, and ingestion.

215
00:13:39,060 --> 00:13:44,790
Data flows in from various sources,
be databases, APIs, or files.

216
00:13:45,060 --> 00:13:48,449
As it's ingested, we
generate logs and metrics.

217
00:13:48,569 --> 00:13:52,830
Example, source record counts,
staging and validation checks.

218
00:13:53,100 --> 00:13:57,090
In staging areas, we run great
expectation suites On raw data.

219
00:13:57,240 --> 00:14:01,470
Example, check that all expected
columns are present and values.

220
00:14:01,470 --> 00:14:02,189
Make sense?

221
00:14:02,370 --> 00:14:04,140
If a critical expectation fails.

222
00:14:04,410 --> 00:14:06,750
Say a primary key column, nus.

223
00:14:06,810 --> 00:14:09,479
We flag it immediately and
hold processing for that.

224
00:14:11,130 --> 00:14:13,439
Moving on to transformation
and linear tracking.

225
00:14:13,770 --> 00:14:20,400
As data moves through transformation jobs,
be it ETL or ELT, EJA is instrumented

226
00:14:20,400 --> 00:14:22,410
with tracing via open telemetry.

227
00:14:22,950 --> 00:14:27,960
We propagate a trace ID so that all events
for a particular pipeline run are being

228
00:14:27,960 --> 00:14:30,090
linked at key transformation points.

229
00:14:30,180 --> 00:14:34,650
We include assertion tests via
DBT tests or custom scripts

230
00:14:34,830 --> 00:14:36,750
to ensure business rules hold.

231
00:14:36,990 --> 00:14:40,170
For example, after joining
tables with test, that row

232
00:14:40,170 --> 00:14:41,760
counts didn't unexpectedly drop.

233
00:14:41,760 --> 00:14:46,650
We also log metadata about schema
versions and data statistics at

234
00:14:46,650 --> 00:14:50,340
each step, monitoring and alerting.

235
00:14:50,895 --> 00:14:53,655
All pipelines push their
metrics to a monitoring system.

236
00:14:53,685 --> 00:14:57,224
For instance, after each load, a
pipeline sends the row count and

237
00:14:57,224 --> 00:14:59,895
execution duration metrics to promeus.

238
00:15:00,045 --> 00:15:04,275
Similarly, any warnings or errors
are locked our central log index with

239
00:15:04,275 --> 00:15:09,285
context, say for example, pipeline
name or the data asset affected.

240
00:15:09,584 --> 00:15:13,365
This layer acts an observability
hub aggregating signals

241
00:15:13,454 --> 00:15:14,625
from all the pipelines.

242
00:15:16,080 --> 00:15:20,340
BI layer and dashboards, the
transform data lands in our warehouse.

243
00:15:20,340 --> 00:15:21,990
Example, BigQuery.

244
00:15:22,290 --> 00:15:25,350
Even here, we leverage
built-in auditing features.

245
00:15:25,350 --> 00:15:30,480
For example, using BigQuery information
schema to detect if any schema changes

246
00:15:30,480 --> 00:15:34,320
occurred or the number of bytes
processed by each query, feeding

247
00:15:34,320 --> 00:15:36,000
those stats back into our monitor.

248
00:15:36,330 --> 00:15:40,500
When the BA dashboard finally queries
this data, we have confidence that it

249
00:15:40,500 --> 00:15:42,330
passed all previous quality checks.

250
00:15:42,705 --> 00:15:45,765
The output dashboards now
have an implicit trust score.

251
00:15:46,005 --> 00:15:50,415
If a dashboard is live, it means
all the upstream data quality tests

252
00:15:50,415 --> 00:15:53,775
have been passed and no anomalies
were detected in the pipeline.

253
00:15:54,075 --> 00:15:58,365
If something was wrong, either the data
never made it here, or the dashboard

254
00:15:58,365 --> 00:16:00,525
is showing an alert annotation.

255
00:16:01,335 --> 00:16:05,295
We also built features to annotate
reports if upstream data is

256
00:16:05,295 --> 00:16:07,125
stale or under investigation.

257
00:16:08,685 --> 00:16:12,314
Lessons learned and best
practices, trust, but verify.

258
00:16:12,675 --> 00:16:15,944
We learned that assuming data
is correct is very dangerous.

259
00:16:16,064 --> 00:16:19,245
Even if a pipeline doesn't
crash, the data can be wrong.

260
00:16:19,454 --> 00:16:23,895
Now our motto is trust but
verify always at every step.

261
00:16:24,194 --> 00:16:27,675
Build verification into your process
so you are never blindly trusting

262
00:16:27,824 --> 00:16:29,355
a perfect looking dashboard.

263
00:16:30,255 --> 00:16:31,755
Observability is a team sport.

264
00:16:32,250 --> 00:16:35,880
Getting to observability maturity
wasn't just a tech implementation,

265
00:16:36,030 --> 00:16:37,290
it was a cultural shift.

266
00:16:37,620 --> 00:16:42,660
We aligned data engineers, Q analysts,
and even analysis on the idea that data

267
00:16:42,660 --> 00:16:44,729
qualities everyone's responsibility.

268
00:16:44,729 --> 00:16:48,975
I. Sharing observability dashboards
with a wider team helped everyone

269
00:16:49,035 --> 00:16:50,535
rally around the data trust.

270
00:16:51,285 --> 00:16:52,755
Start with your pain points.

271
00:16:52,845 --> 00:16:54,855
We didn't implement everything overnight.

272
00:16:54,944 --> 00:16:56,685
We targeted the worst pain points.

273
00:16:56,685 --> 00:17:01,755
First example, a critical dashboard that
kept breaking or a frequently changing

274
00:17:01,755 --> 00:17:04,605
source schema and put observability there.

275
00:17:04,815 --> 00:17:07,635
This yielded quick wins
that built momentum.

276
00:17:08,145 --> 00:17:11,204
For instance, catching a major
schema change in real time prevented

277
00:17:11,204 --> 00:17:14,595
a KPA outage immediately proving
the value of this approach.

278
00:17:14,865 --> 00:17:20,805
The best practice is to iteratively build
your observability, focusing on areas of

279
00:17:20,865 --> 00:17:24,855
highest risk, automate and standardize.

280
00:17:25,305 --> 00:17:29,235
Humans can scale to check hundreds
of tables daily, but machine

281
00:17:29,235 --> 00:17:33,105
scan, we invested in automation
for testing and monitoring.

282
00:17:33,180 --> 00:17:33,210
Okay.

283
00:17:33,780 --> 00:17:37,410
Templates and frameworks like
reusable Great Expectation

284
00:17:37,410 --> 00:17:39,690
Suites and DBT test macros.

285
00:17:39,810 --> 00:17:42,060
That helped standardize RQA.

286
00:17:42,360 --> 00:17:47,610
This way, every new data sets get, say
based on quality checklist out of the box.

287
00:17:47,820 --> 00:17:51,090
Standardization also
means clear expectations.

288
00:17:51,480 --> 00:17:55,110
Everyone knows what past QA
means in measurable terms.

289
00:17:56,040 --> 00:18:00,480
Three data incidents like software
incidents, a big lesson respond to

290
00:18:00,480 --> 00:18:05,460
data problems with the same urgency
and process as application outages.

291
00:18:05,760 --> 00:18:10,350
That means having oncall rotations,
incident tracking, root cause

292
00:18:10,350 --> 00:18:12,690
analysis, and preventative follow-ups.

293
00:18:12,810 --> 00:18:15,330
For us, this led to systematic fixes.

294
00:18:15,420 --> 00:18:19,740
Example, after a data incident, we
would add a new test or improving

295
00:18:19,740 --> 00:18:21,300
monitoring to prevent a repeat.

296
00:18:21,825 --> 00:18:25,635
Over time, this proactive
stance greatly reduce the fight.

297
00:18:25,635 --> 00:18:29,955
Fighting observability is
not a one and done project.

298
00:18:29,985 --> 00:18:32,505
We continuously refine
our checks and alerts.

299
00:18:32,685 --> 00:18:36,105
We review false alarms to
adjust thresholds and analyze

300
00:18:36,225 --> 00:18:37,755
misses to add new tasks.

301
00:18:37,935 --> 00:18:42,525
Our data and systems evolve, so our
observability must evolve to you.

302
00:18:42,915 --> 00:18:46,845
One best practice is to hold regular
data quality review meetings.

303
00:18:47,325 --> 00:18:51,015
To reliability review meetings
to assess what's working and what

304
00:18:51,045 --> 00:18:52,605
isn't in our observability setup.

305
00:18:53,445 --> 00:18:58,035
Finally, we made it to a point to
celebrate when our system catches issues.

306
00:18:58,275 --> 00:19:03,015
Every prevented bad dashboard or
saved hours of troubleshooting is a

307
00:19:03,015 --> 00:19:04,575
win for the team and the business.

308
00:19:04,875 --> 00:19:08,535
This positive reinforcement kept
the team motivated to maintain

309
00:19:08,535 --> 00:19:10,425
a high bar for the data quality.

310
00:19:10,665 --> 00:19:14,475
It also helped in getting buy-in
from the leadership to continue

311
00:19:14,475 --> 00:19:16,995
investing in data observability.

312
00:19:17,115 --> 00:19:22,125
Nothing speaks louder than
prevented crisis and call to action.

313
00:19:22,935 --> 00:19:24,405
Assess your data QA today.

314
00:19:24,675 --> 00:19:27,375
Take a hard look at your
current data quality processes.

315
00:19:27,765 --> 00:19:31,815
Are you catching issues early or are you
relying on end users to spa the mistakes?

316
00:19:32,355 --> 00:19:35,715
Identify one nightmare
scenario in our organizations.

317
00:19:36,105 --> 00:19:39,855
Be it a dashboard that broke or a
critical pipeline that failed silently.

318
00:19:40,095 --> 00:19:42,615
That's your candidate
for an observ makeover.

319
00:19:43,995 --> 00:19:47,535
Embed and embrace observability
if you haven't already.

320
00:19:47,535 --> 00:19:49,395
Start instrumenting your data pipelines.

321
00:19:49,575 --> 00:19:54,855
Even simple steps can pay off enabling
logging if, if it's not on set up.

322
00:19:54,855 --> 00:19:57,735
The basic alerts for data
freshness or volume changes.

323
00:19:58,095 --> 00:20:01,395
Write a few great expectation
tasks for a key table.

324
00:20:01,845 --> 00:20:05,445
Treat your data pipelines with
the same care as live products.

325
00:20:05,595 --> 00:20:07,605
Remember, data is a new software.

326
00:20:07,785 --> 00:20:13,125
It deserves the same level of monitoring
and qa, and leverage the available tools.

327
00:20:13,215 --> 00:20:15,525
You don't have to build
everything from scratch.

328
00:20:15,735 --> 00:20:18,735
Open source tools and
frameworks are at your disposal.

329
00:20:19,095 --> 00:20:21,915
Try out great expectations to
kickstart your data testing.

330
00:20:22,215 --> 00:20:25,635
Use your existing a PM monitoring
tools or cloud monitoring

331
00:20:25,635 --> 00:20:27,165
tool, track pipeline jobs.

332
00:20:27,465 --> 00:20:29,895
Explore what your data warehouse offers.

333
00:20:29,955 --> 00:20:34,365
Many have information schema tables
or built-in logging you can tap

334
00:20:34,365 --> 00:20:36,794
into The ecosystem is pretty rich.

335
00:20:36,975 --> 00:20:39,405
Pick a pain point and
apply a tool to address it.

336
00:20:40,665 --> 00:20:42,014
Make it a team effort.

337
00:20:42,195 --> 00:20:45,764
Encourage a culture where data quality
is part of the definition of done.

338
00:20:45,975 --> 00:20:49,544
Involve QA engineers in
pipeline design and involve data

339
00:20:49,544 --> 00:20:50,804
engineers in quality monitoring.

340
00:20:51,285 --> 00:20:55,035
Perhaps create a data observ
task force, RNA Pilot project.

341
00:20:55,215 --> 00:20:57,555
Share the improvements and
insights with stakeholders.

342
00:20:57,675 --> 00:21:00,285
When people see more relatable
dashboards, they'll become

343
00:21:00,285 --> 00:21:01,545
advocates for this approach.

344
00:21:01,875 --> 00:21:03,615
Thank you for giving me this opportunity.

345
00:21:04,640 --> 00:21:04,760
I.

