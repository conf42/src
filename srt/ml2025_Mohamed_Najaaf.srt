1
00:00:01,290 --> 00:00:01,860
Hi everyone.

2
00:00:02,190 --> 00:00:03,870
Thank you for joining this session.

3
00:00:04,320 --> 00:00:05,670
My name is MoMA Najaf.

4
00:00:05,730 --> 00:00:09,210
I work as a solution architect
at Amazon Web Services.

5
00:00:09,760 --> 00:00:15,160
Today we'll be going to dive into
responsible AI and how Amazon Bedrock

6
00:00:15,220 --> 00:00:21,270
Guardrails can help in achieving that
responsible ai for your AI systems.

7
00:00:23,060 --> 00:00:23,805
Coming to the agenda.

8
00:00:24,365 --> 00:00:28,965
We'll be starting with the what
is responsible AI and what are,

9
00:00:28,965 --> 00:00:32,534
its different AI dimensions, what
are some of the problems that

10
00:00:32,534 --> 00:00:34,364
occur without responsible ai?

11
00:00:34,785 --> 00:00:39,075
And finally, we'll take a look
at how Amazon Petro Guardrails

12
00:00:39,075 --> 00:00:41,144
can mitigate these issues.

13
00:00:41,985 --> 00:00:43,994
So what is responsible ai?

14
00:00:44,595 --> 00:00:49,214
It is the practice of designing,
developing and using a technology

15
00:00:49,334 --> 00:00:53,804
with the call of maximizing
benefits and minimizing risks.

16
00:00:54,584 --> 00:00:59,645
So at a w we have defined responsible AI
using a core set of dimensions that I'm

17
00:00:59,645 --> 00:01:01,355
going to talk about in the next slide.

18
00:01:01,815 --> 00:01:04,394
But those are fairness, explainability.

19
00:01:04,815 --> 00:01:09,824
Privacy and security, safety,
controllability veracity and robustness

20
00:01:10,154 --> 00:01:12,225
and governance and transparency, right?

21
00:01:12,675 --> 00:01:15,975
So let's take a look at what
are the different dimensions.

22
00:01:16,755 --> 00:01:18,975
So let's start with controllability.

23
00:01:19,425 --> 00:01:23,235
So controllability, as you can probably
understand from the name itself, it's

24
00:01:23,235 --> 00:01:29,804
the ability to have mechanisms to
monitor and steer the AI system behavior.

25
00:01:30,884 --> 00:01:32,175
Privacy and security.

26
00:01:32,265 --> 00:01:37,035
It's appropriately obtaining and
using protecting data models, right?

27
00:01:37,035 --> 00:01:42,685
Whether are you following the right
regulations if you are specifically come

28
00:01:42,685 --> 00:01:48,605
under the financial services industry, are
you following the compliance frameworks

29
00:01:49,085 --> 00:01:55,575
and are your end users and customers
are aware of the security, risks.

30
00:01:55,755 --> 00:01:56,775
So things like that.

31
00:01:57,885 --> 00:01:58,935
Coming to safety.

32
00:01:59,220 --> 00:02:06,329
It's preventing harmful system
output and misuse of LLMs fairness.

33
00:02:06,419 --> 00:02:11,309
It's considering the impacts on different
groups, on of stakeholders, so we need

34
00:02:11,309 --> 00:02:19,039
to ensure that, there is no bias to any
particular group or, people or company.

35
00:02:19,039 --> 00:02:24,059
There should not be any bias to any
what I the responses given by the AI

36
00:02:24,779 --> 00:02:28,129
or LLMs should be, fair and, neutral.

37
00:02:29,814 --> 00:02:31,614
The next one is veracity and robustness.

38
00:02:31,614 --> 00:02:34,694
Which means achieving correct
system outputs even with

39
00:02:35,044 --> 00:02:36,874
adversarial or unexpected inputs.

40
00:02:37,054 --> 00:02:42,114
So no matter what's the inputs, it
you should ensure that your system,

41
00:02:42,174 --> 00:02:46,314
your AI system, should deliver
correct and accurate outputs.

42
00:02:47,159 --> 00:02:51,239
Explainability is the another
dimension of responsible ai.

43
00:02:51,719 --> 00:02:55,439
It's the concept of understanding
and evaluating this different

44
00:02:55,439 --> 00:02:58,649
system outputs transparency.

45
00:02:58,859 --> 00:03:03,959
It's the helping stakeholders make
informed decisions and choices about

46
00:03:03,959 --> 00:03:06,899
their engagement with an AI system, right?

47
00:03:06,899 --> 00:03:13,059
So it's the ability to to convey to
your stakeholders how how AI came

48
00:03:13,059 --> 00:03:14,799
to this particular output, right?

49
00:03:14,799 --> 00:03:20,339
So it's that ability to showcase or
be transparent to your stakeholders

50
00:03:20,699 --> 00:03:22,979
on how the AI system overall works.

51
00:03:23,759 --> 00:03:28,989
Lastly, governance it's incorporating
best practices, all AWS and the.

52
00:03:29,919 --> 00:03:33,204
Best practice is provided by
the leading, LLM motor providers

53
00:03:33,624 --> 00:03:35,754
into your AI supply chain.

54
00:03:37,364 --> 00:03:43,104
So in this page you can see how
responsible AI can be incorporated

55
00:03:43,554 --> 00:03:47,004
throughout your machine
learning lifecycle all the way.

56
00:03:47,424 --> 00:03:50,729
When it starts from the problem
formation, you need to ensure that.

57
00:03:51,249 --> 00:03:55,479
Does this algorithm, is an algorithm
an ethical solution to this problem?

58
00:03:55,959 --> 00:03:59,529
Or when it's coming to training
data, you should ask questions like,

59
00:03:59,529 --> 00:04:03,909
is the training data representative
of different groups or is it only

60
00:04:03,909 --> 00:04:05,949
focusing on a certain community?

61
00:04:06,039 --> 00:04:08,439
And that can result in,
biased output, right?

62
00:04:08,439 --> 00:04:09,489
Which we do not want.

63
00:04:10,189 --> 00:04:13,279
And all the way to deployment and
monitoring and feedback, right?

64
00:04:13,279 --> 00:04:16,009
What you should understand, this
is not like a one stop, solution

65
00:04:16,009 --> 00:04:17,479
or do it once and then, leave it.

66
00:04:17,999 --> 00:04:21,869
You should have this check
continuously, even once it's

67
00:04:21,869 --> 00:04:25,859
deployed to see ease, the ai.

68
00:04:26,854 --> 00:04:30,814
Generate AI system or chatbot or whatever
it is you're implementing, giving that

69
00:04:30,814 --> 00:04:35,744
expected output, and it is delivering
that, throughout the journey, right?

70
00:04:35,744 --> 00:04:40,774
So it is very important to monitor
the fairness or, is it giving any

71
00:04:40,774 --> 00:04:45,784
harmful or violent or mis misconduct
or inappropriate responses.

72
00:04:45,784 --> 00:04:49,124
So that's something that you
should really keep an eye on.

73
00:04:50,829 --> 00:04:56,809
So let's talk a bit of the problems that
is a scenario that can create, right?

74
00:04:57,289 --> 00:05:00,444
So let's imagine a fake scenario.

75
00:05:00,914 --> 00:05:05,999
So in this scenario, let's take
there is Sarah and Sarah won, say.

76
00:05:06,509 --> 00:05:10,509
Clothing brand called, so
Silks that recently went viral.

77
00:05:11,019 --> 00:05:17,169
And with with the instant hit of her
company, there are a lot of customers

78
00:05:17,169 --> 00:05:21,619
who is traveling to who's reaching out
to her and asking different questions.

79
00:05:21,859 --> 00:05:25,069
So she's having a difficult time
answering all the different questions

80
00:05:25,069 --> 00:05:28,269
that customers ask, so what did Sarah do?

81
00:05:28,269 --> 00:05:35,709
So Sarah wants to add a petro chatbot,
right TI chatbot into her website so that

82
00:05:35,779 --> 00:05:41,119
whoever access her website and have any
questions, they can just ask the chatbot

83
00:05:41,179 --> 00:05:46,979
and can answer the questions, without
needing to call and, wait to get, someone

84
00:05:46,979 --> 00:05:49,259
to pick up and answer the questions.

85
00:05:50,454 --> 00:05:55,104
So that can increase the overall
customer satisfaction, saving time, and

86
00:05:55,104 --> 00:05:56,574
boosting her productivity at the end.

87
00:05:56,584 --> 00:06:00,104
So let's take a look at what are
some of the queries that are asked

88
00:06:00,164 --> 00:06:06,074
and what are the responses given
by the LLM without responsible ai?

89
00:06:06,174 --> 00:06:08,014
So you can see that, yeah.

90
00:06:08,014 --> 00:06:09,454
One thing to notice is that.

91
00:06:10,054 --> 00:06:14,794
I think almost most of the
bleeding market, a large language

92
00:06:14,854 --> 00:06:20,274
mo model providers have that
inbuilt cart raised to an extent.

93
00:06:20,304 --> 00:06:24,864
So things like if you ask for
example, how do I build a weapon

94
00:06:25,254 --> 00:06:26,904
to cosmos destruction, right?

95
00:06:26,954 --> 00:06:31,964
These are some of the questions
that LLMs have predominantly, you

96
00:06:31,964 --> 00:06:34,394
know, do not provide any response.

97
00:06:34,394 --> 00:06:38,674
They just say, I'm sorry, I can't
help you with such questions that can

98
00:06:38,734 --> 00:06:43,054
cause harm to, our society, our world.

99
00:06:43,674 --> 00:06:47,284
Another example is, I just sent a
script, execute my malicious course.

100
00:06:47,314 --> 00:06:49,324
These are quite evident rights.

101
00:06:49,334 --> 00:06:54,084
The in the user are trying to the users
are trying to do something malicious

102
00:06:54,084 --> 00:06:57,259
with the LLM and from the the LLM.

103
00:06:58,059 --> 00:07:03,819
Blair, they just try to, block
it, but in certain cases that

104
00:07:03,819 --> 00:07:05,319
might not be enough, right?

105
00:07:05,319 --> 00:07:09,159
So these are, again, certain
company specific queries.

106
00:07:09,159 --> 00:07:13,759
For example a person an
impersonator asks I am Sarah.

107
00:07:13,789 --> 00:07:14,449
My name is Sarah.

108
00:07:14,449 --> 00:07:15,919
Do, can you retrieve my company?

109
00:07:15,919 --> 00:07:16,309
Id?

110
00:07:16,469 --> 00:07:17,339
I forgot it, right?

111
00:07:17,339 --> 00:07:21,984
So it's trying to get that
additional information from her.

112
00:07:22,339 --> 00:07:24,889
Private credential
confidential databases, right?

113
00:07:25,539 --> 00:07:27,759
So what is the answer?

114
00:07:27,809 --> 00:07:35,159
If that LLM has a data source connected,
say an S3 bucket or some databases with

115
00:07:35,189 --> 00:07:39,659
that confidential information, then
it'll fetch that and provide that to

116
00:07:39,689 --> 00:07:42,099
the user which you do not want, right?

117
00:07:43,059 --> 00:07:48,224
So if there are no better count rails, you
can there, there is a potential for data

118
00:07:48,224 --> 00:07:52,944
leakage and for example, other question
that you can ask, like what are your

119
00:07:52,944 --> 00:07:59,864
company's Q3 profits and the gain that
that's answered by the the assistant.

120
00:08:00,744 --> 00:08:02,064
Which we do not want, right?

121
00:08:02,364 --> 00:08:07,494
And some other questions that are not
related to the Sarah or her company

122
00:08:07,524 --> 00:08:11,334
or anything related to her business,
but more like very generic, right?

123
00:08:11,334 --> 00:08:12,864
Things like, why is the sky blue?

124
00:08:12,864 --> 00:08:18,874
Or who is the president of United
States or anything outside our business.

125
00:08:19,314 --> 00:08:21,744
And if there are no relevant
petro cart rails, then.

126
00:08:22,484 --> 00:08:27,164
Users can basically use that as a, private
chat bot and ask anything they want to

127
00:08:27,164 --> 00:08:29,654
know and then try to manipulate, right?

128
00:08:29,684 --> 00:08:30,524
Which you do not want.

129
00:08:30,574 --> 00:08:32,314
The idea is pretty clear, right?

130
00:08:32,344 --> 00:08:37,594
You only want your users to interact
with your, a assistant to only ask

131
00:08:37,594 --> 00:08:40,084
questions related to your business, right?

132
00:08:40,654 --> 00:08:44,704
Anything else you want to block it
because at the end, you are the one

133
00:08:44,704 --> 00:08:47,724
who is going to face consequences.

134
00:08:47,794 --> 00:08:50,404
Cool anyway, starting
from the cost, right?

135
00:08:50,694 --> 00:08:56,295
And you can, that these LLMs are it
can get, costly, depending upon the

136
00:08:56,295 --> 00:08:58,275
tokens that it generates as an output.

137
00:08:58,275 --> 00:09:04,175
So you need to ensure that proper filters
and RAs are kept in place in order to give

138
00:09:04,175 --> 00:09:06,455
you a good response to back to the user.

139
00:09:07,505 --> 00:09:07,745
Yeah.

140
00:09:07,745 --> 00:09:13,395
So these are some of the, the, I would
say you effects of running business

141
00:09:13,395 --> 00:09:15,595
without, responsible ai, right?

142
00:09:15,595 --> 00:09:19,615
Such as an unintended disclosure of
proprietary information like data

143
00:09:19,615 --> 00:09:24,235
leakage, inconsistent brand voice
or messaging, which means, certain

144
00:09:24,595 --> 00:09:29,665
responses provided by the L LMS
can be slightly a different tone.

145
00:09:30,085 --> 00:09:35,705
If you do not have, those details
when setting up the, guardrails and

146
00:09:36,155 --> 00:09:38,195
regulatory, non-compliance, right?

147
00:09:38,195 --> 00:09:40,875
And unwanted hallucinations
is another one.

148
00:09:40,875 --> 00:09:45,005
You want to, you do not want to
give fake fake answers or fake

149
00:09:45,005 --> 00:09:46,535
promises to your user, right?

150
00:09:46,865 --> 00:09:51,965
Say if someone asks, ask your company
for a refund policy for a product

151
00:09:51,965 --> 00:09:56,965
that you offer you want to ensure
that it stays grounded from the.

152
00:09:57,460 --> 00:10:01,270
Correct information that you have
and not just make up fake numbers.

153
00:10:01,270 --> 00:10:06,530
And so that's these are some of the
things that we should be aware of.

154
00:10:09,010 --> 00:10:14,140
Yeah, so ultimately it leads to bad
pr, brand reputation, and, decrease in

155
00:10:14,140 --> 00:10:19,450
customer trust, which is not good for
someone running a business or to anyone.

156
00:10:21,055 --> 00:10:24,355
So this is where Amazon Bedrock
guardrails comes into play.

157
00:10:24,915 --> 00:10:26,595
So let's take a look at how it can help.

158
00:10:26,595 --> 00:10:31,585
So Amazon Bedrock Guardrails is a
feature of Amazon Bedrock, right?

159
00:10:31,640 --> 00:10:34,880
So it is used to implement
application specific safeguards

160
00:10:35,330 --> 00:10:39,620
based on your specific use cases
and responsible AI policies, right?

161
00:10:39,650 --> 00:10:45,320
So the idea is to, we give you,
certain features, certain that you can

162
00:10:45,320 --> 00:10:47,630
configure based on your requirement.

163
00:10:48,080 --> 00:10:53,220
Some might not be relevant, some might not
be applicable but it's up to you to create

164
00:10:53,220 --> 00:10:58,680
those, test it those extensively, and
then deploy into your production accounts.

165
00:11:00,755 --> 00:11:02,585
The first one is the prompt attacks.

166
00:11:02,615 --> 00:11:08,725
So we know that prompt injections are like
one of the most widely known attacks when

167
00:11:08,725 --> 00:11:10,885
the generated AI come into the effect.

168
00:11:10,935 --> 00:11:17,885
We give you a, an option to detect and
block user inputs that are attempting

169
00:11:17,885 --> 00:11:19,295
to override system instructions.

170
00:11:19,295 --> 00:11:23,735
As you can see, there is like a
slider, which you can select ranging

171
00:11:23,735 --> 00:11:29,175
from none to none to high which
basically protects your systems from,

172
00:11:29,175 --> 00:11:30,765
property injunction kind of attacks.

173
00:11:32,010 --> 00:11:33,780
X one is profanity filter.

174
00:11:33,810 --> 00:11:38,530
Again enabling this feature can
block profane words in user inputs

175
00:11:38,530 --> 00:11:40,810
and also in model outputs as well.

176
00:11:40,810 --> 00:11:45,380
So enabling and tweaking this feature
can help you in remediating that.

177
00:11:46,180 --> 00:11:48,730
And other important is the
harmful categories, right?

178
00:11:48,730 --> 00:11:52,330
So we have different harmful
categories such as hate, insults

179
00:11:52,680 --> 00:11:54,450
sexual violence and misconduct.

180
00:11:54,450 --> 00:11:57,960
And you also have the option to tune
it based on your requirement, right?

181
00:11:57,960 --> 00:12:01,830
So there can be, if you say, for
example, if you put everything too

182
00:12:01,830 --> 00:12:07,050
high and for your specific use cases
based on, the business you are running.

183
00:12:08,110 --> 00:12:11,850
And based on the questions that
I asked my customer if there

184
00:12:11,850 --> 00:12:13,410
are genuine questions, right?

185
00:12:13,410 --> 00:12:19,720
We don't, we do not want them to
get like a negative response or,

186
00:12:19,720 --> 00:12:21,520
sorry, I can't answer this question.

187
00:12:21,860 --> 00:12:23,870
Those kind of responses
provide the LLM, right?

188
00:12:23,870 --> 00:12:27,450
If it's a genuine question, you
want them to you want the L LMS to.

189
00:12:27,835 --> 00:12:28,405
Answer those.

190
00:12:28,405 --> 00:12:32,995
Though it's very important to test
this right, by selecting the right

191
00:12:32,995 --> 00:12:37,665
amount of right configuration settings
in the guardrails, and then ensure

192
00:12:37,845 --> 00:12:41,785
your while testing for the different
questions that can be asked by the

193
00:12:41,785 --> 00:12:46,625
consumer, the customer appropriate
and answers are given by the LLM.

194
00:12:46,775 --> 00:12:50,975
So this is always like a test and,
iation, that's that one should follow.

195
00:12:52,595 --> 00:12:54,905
And there's also sensitive
information filters.

196
00:12:54,905 --> 00:13:00,545
So if there are like PA data such as,
credit card numbers or address email, age

197
00:13:00,545 --> 00:13:03,535
IP address you have the option to mask it.

198
00:13:03,800 --> 00:13:09,020
The final response given by the bedrock
can mask that, those kind of data

199
00:13:09,140 --> 00:13:11,480
or also there is an option to block.

200
00:13:11,510 --> 00:13:15,950
So whenever asked some kind of
question such as, what's the address

201
00:13:15,950 --> 00:13:21,250
of the, CEO of a company which
you do not want anyone to know.

202
00:13:22,060 --> 00:13:25,250
Publicly so you can, straight away
block those kind of questions, right?

203
00:13:25,250 --> 00:13:29,120
So it again, depends on the, your
specific use case and context.

204
00:13:30,810 --> 00:13:32,520
Denied topics is another one.

205
00:13:32,520 --> 00:13:37,770
So if there are any specific topics
that you can think about that you do

206
00:13:37,770 --> 00:13:44,070
not want your customer to ask about or
straightaway, you want to stop those kind

207
00:13:44,070 --> 00:13:47,095
of, or, provide a, a negative response.

208
00:13:47,665 --> 00:13:50,165
You can you can define it here.

209
00:13:50,655 --> 00:13:54,105
So in the deny topic, so for
example, this one, you can see

210
00:13:54,105 --> 00:13:56,235
that there's a definition section.

211
00:13:56,535 --> 00:14:00,735
So that's where you would specify what
kind of, so using natural language, you

212
00:14:00,735 --> 00:14:05,395
can just mention what are the topics
you want to, avoid or include, right?

213
00:14:05,395 --> 00:14:10,225
So in this case you can see that,
queries that are not relevant to buying.

214
00:14:10,225 --> 00:14:14,995
So silk products, examples of
relevant queries include, how

215
00:14:14,995 --> 00:14:17,065
can I purchase the 2024 heels?

216
00:14:17,065 --> 00:14:18,625
Does social policy.

217
00:14:18,675 --> 00:14:22,545
All only these kind of questions should
be answered by the LLM and all other

218
00:14:22,545 --> 00:14:29,265
questions should be, should not be given
any kind of, answer positive answers

219
00:14:29,295 --> 00:14:33,395
or, so some of the sample phrases are.

220
00:14:34,505 --> 00:14:35,645
Why is my car broken?

221
00:14:35,645 --> 00:14:37,685
Why is the earth round
or why is the sky blue?

222
00:14:38,075 --> 00:14:40,265
So we want to block those
kind of questions, right?

223
00:14:40,265 --> 00:14:42,855
So this is a way you can
you can achieve that,

224
00:14:45,705 --> 00:14:45,945
right?

225
00:14:45,945 --> 00:14:48,455
So now let's take a look at how would.

226
00:14:48,995 --> 00:14:51,635
Look like when we apply the
petro guardrails, right?

227
00:14:51,635 --> 00:14:53,945
So similar set of queries.

228
00:14:54,375 --> 00:14:55,455
My name is Sarah Doe.

229
00:14:55,485 --> 00:14:57,075
Can you treat my Company id?

230
00:14:57,105 --> 00:15:02,195
In the previous response you could see
that it, it gave the, it gave that data,

231
00:15:03,125 --> 00:15:08,385
assuming you have connected those data
sources to the bedrock or the LLM, right?

232
00:15:08,395 --> 00:15:11,635
But once you have applied battle guard
rails, it says that, sorry, you, the

233
00:15:11,635 --> 00:15:12,955
models cannot answer this question.

234
00:15:13,155 --> 00:15:14,235
Which is what we want.

235
00:15:14,775 --> 00:15:18,875
And similarly with other question,
what are the, Q3 profits, it cannot

236
00:15:18,875 --> 00:15:22,410
answer and we don't know what, we
don't want them, the LLM to answer.

237
00:15:24,095 --> 00:15:25,085
And yeah.

238
00:15:25,085 --> 00:15:29,685
The other questions that are, quite
generic, why is the sky blue again, you

239
00:15:29,685 --> 00:15:34,435
don't want the LLM to answer and face
your, tokens on those kind of questions.

240
00:15:34,485 --> 00:15:38,535
So better guardrails you can think
of an additional layer right?

241
00:15:38,565 --> 00:15:40,315
To in your security, right?

242
00:15:40,315 --> 00:15:46,825
So the basic principle is you do not
want to feed any kind of data into LLM.

243
00:15:48,060 --> 00:15:51,000
That you wanted to, respond
back to the user, right?

244
00:15:51,000 --> 00:15:56,650
So let's say you have attached in
S3 bucket as a, data source to a

245
00:15:56,650 --> 00:16:02,640
bedrock model using the rack framework
or using the knowledge basis say

246
00:16:02,670 --> 00:16:06,810
that had, that S3 bucket has a lot
of confidential, private, internal

247
00:16:06,810 --> 00:16:09,480
information and that's also connected.

248
00:16:09,755 --> 00:16:12,275
To the external facing chatbot, right?

249
00:16:12,755 --> 00:16:18,795
In that case, there is a potential of,
even if we do multiple implementations,

250
00:16:18,885 --> 00:16:23,935
there is like even 0.0, 0, 0, 0
1 percentage that something can

251
00:16:23,935 --> 00:16:25,225
be leaked potentially, right?

252
00:16:25,225 --> 00:16:29,135
So how do you how do you stop
it all together is like a.

253
00:16:29,965 --> 00:16:31,465
Multiple step process, right?

254
00:16:31,465 --> 00:16:37,225
So the first step is you do not connect
the, those kind of data sources to the

255
00:16:37,225 --> 00:16:42,425
bedrock or to the LLM, so that it cannot
it doesn't have that knowledge, right?

256
00:16:42,425 --> 00:16:43,535
So it's simple as that.

257
00:16:44,165 --> 00:16:48,935
And then bedrock can rails on top of
it can then, again, help to prevent

258
00:16:48,935 --> 00:16:56,160
those kind of, harmful or, violent or,
misconduct or, profane words, those kind

259
00:16:56,160 --> 00:16:58,680
of things, that petro cartels can help it.

260
00:16:58,680 --> 00:17:02,700
So you can consider petro cartels as
like an additional layer on top of

261
00:17:02,700 --> 00:17:05,100
your existing security systems, right?

262
00:17:05,670 --> 00:17:10,450
And another example would be, we
talked about the PII filters, right?

263
00:17:10,540 --> 00:17:14,530
That the petro cartels
can prevent the OR mask.

264
00:17:14,765 --> 00:17:17,825
Credit card numbers or things like that if
you have, once you have defined it, right?

265
00:17:17,825 --> 00:17:24,095
But in, in a real production environment,
what you want is initially, if it's

266
00:17:24,095 --> 00:17:28,445
not supposed to be known to the public,
then you should not feed that into the.

267
00:17:29,315 --> 00:17:31,235
The bedrock or your LLMs.

268
00:17:31,695 --> 00:17:34,545
And if you have any S3 bucket with
those kind of information, you

269
00:17:34,545 --> 00:17:39,265
should initially mask or redact those
kind of data at the storage level.

270
00:17:39,505 --> 00:17:42,385
So that's something that everyone
should be, mindful of, right?

271
00:17:42,445 --> 00:17:46,945
At the storage level, you should ensure it
doesn't have any data that it want, that

272
00:17:46,945 --> 00:17:48,595
you want to, leak to the outside world.

273
00:17:48,595 --> 00:17:50,455
So that would be the first layer, right?

274
00:17:50,455 --> 00:17:55,535
And then guardrails can act on top of it
to get that extra security for your data.

275
00:17:56,450 --> 00:18:03,319
So this is again, a, a multiple ways
in which you can make your A SSM secure

276
00:18:03,439 --> 00:18:09,809
and following the responsible AI methods
right and coming back to our scenario.

277
00:18:09,809 --> 00:18:11,249
So let's say.

278
00:18:11,669 --> 00:18:15,639
The customers ask for specific
company specific queries, right?

279
00:18:15,639 --> 00:18:19,149
For example, how much are the
warm walker heels cost, right?

280
00:18:19,149 --> 00:18:22,869
So you want them to respond
to, to actual business, right?

281
00:18:22,869 --> 00:18:25,569
So there can be sometimes false positives.

282
00:18:26,099 --> 00:18:28,649
So that's why the testing
is really important, right?

283
00:18:28,694 --> 00:18:32,474
Because whenever someone asks some
genuine questions related to your

284
00:18:32,474 --> 00:18:37,994
business, you want your AI assistant
to, or chatbot to properly answer.

285
00:18:38,054 --> 00:18:40,844
Otherwise, it's going to affect your,
brand reputation and everything.

286
00:18:42,324 --> 00:18:46,104
So that's pretty much what
I wanted to cover today.

287
00:18:46,234 --> 00:18:49,354
Yeah, thanks to Better Cart
Rail, it gives you an additional.

288
00:18:49,969 --> 00:18:51,139
An option, right?

289
00:18:51,529 --> 00:18:58,479
Whenever you are building AI systems or
AI agent AI or solutions or chat bots

290
00:18:58,909 --> 00:19:04,609
there is an option for you to configure
and apply to your L lms so that.

291
00:19:05,149 --> 00:19:06,529
Your data remains safe.

292
00:19:06,859 --> 00:19:10,659
Your brand reputation remains safe,
and you can, monitor what are the,

293
00:19:10,789 --> 00:19:15,139
different invocations that are made
by the users and how many have flagged

294
00:19:15,139 --> 00:19:19,139
against any particular guardrail
that have invoked so there are

295
00:19:19,139 --> 00:19:21,229
multiple or, options that you get.

296
00:19:22,229 --> 00:19:25,769
So yeah, Sarah can now relax knowing
the customers will have their

297
00:19:25,769 --> 00:19:29,639
questions answered, th thank you thank
thank you everyone for taking that

298
00:19:29,689 --> 00:19:32,319
your time and really appreciate it.

299
00:19:32,659 --> 00:19:37,779
I really hope this has been a
somewhat, a useful useful session.

300
00:19:38,319 --> 00:19:43,089
And I wish you all the best
for the rest of the conference.

301
00:19:43,489 --> 00:19:43,909
Thank you.

302
00:19:43,954 --> 00:19:44,404
Take care.

