1
00:00:00,500 --> 00:00:01,220
Hi everyone.

2
00:00:01,280 --> 00:00:01,999
I'm bna.

3
00:00:02,030 --> 00:00:03,620
Thanks for joining me for the session.

4
00:00:03,950 --> 00:00:06,680
Today I'm going to talk about how
to accelerate your data science

5
00:00:06,680 --> 00:00:08,180
career via experimentation.

6
00:00:08,600 --> 00:00:10,310
Think of the session as a field guide.

7
00:00:10,820 --> 00:00:14,540
We'll hit experiment design patterns
that work in the wild and the platform

8
00:00:14,540 --> 00:00:18,740
pieces that make experimentation fast
and trustworthy and the career moves

9
00:00:18,740 --> 00:00:20,640
that turn your work into impact.

10
00:00:21,030 --> 00:00:24,810
My goal is to leave you with a playbook
that you can start using this week.

11
00:00:25,435 --> 00:00:31,164
Before we jump into agenda let me
introduce myself and take a minute to

12
00:00:31,164 --> 00:00:33,444
talk about why this topic matters to me.

13
00:00:33,894 --> 00:00:38,724
I've spent over a decade helping teams
design and scale experimentation using

14
00:00:38,724 --> 00:00:43,045
data to build products and solve business
problems, not just to report on them.

15
00:00:43,435 --> 00:00:47,724
My core craft is experimentation
and casual inference, choosing

16
00:00:47,724 --> 00:00:51,765
right design for the constraint,
me measuring impact, credibility,

17
00:00:52,035 --> 00:00:53,895
translating results into decisions.

18
00:00:54,315 --> 00:00:57,705
On systems side, I design and
operationalize cloud native

19
00:00:57,705 --> 00:00:59,145
experimentation platforms.

20
00:00:59,475 --> 00:01:02,655
So Tessa, reproducible,
observable, and cheap to run.

21
00:01:03,015 --> 00:01:06,795
And strategically, I'm obsessed with
connecting insights to outcomes,

22
00:01:07,065 --> 00:01:11,745
turning analysis into decisions leaders
can trust, and building a culture

23
00:01:12,015 --> 00:01:14,175
where being data informed is default.

24
00:01:14,535 --> 00:01:18,405
That combination, methodology,
platform, and business impact is

25
00:01:18,405 --> 00:01:19,970
what I'll share with you today.

26
00:01:20,470 --> 00:01:22,210
Let's jump into the agenda.

27
00:01:22,700 --> 00:01:26,300
We will start by looking at why AB
testing matters for your career,

28
00:01:27,170 --> 00:01:31,940
where experimentation can create
value, and some basic concepts about

29
00:01:32,310 --> 00:01:35,160
experimentation and some common pitfalls.

30
00:01:35,550 --> 00:01:39,520
After that, we will jump into how
can you leverage this concepts

31
00:01:39,579 --> 00:01:41,020
to accelerate your career?

32
00:01:41,289 --> 00:01:44,049
And few tips on interview strategy.

33
00:01:44,679 --> 00:01:46,304
Then we will end with an action plan.

34
00:01:46,804 --> 00:01:48,154
Okay, let's get started.

35
00:01:48,615 --> 00:01:51,195
Why AB testing matters for your career?

36
00:01:51,884 --> 00:01:53,085
It's a high demand skill.

37
00:01:53,535 --> 00:01:55,154
It's highly sought after.

38
00:01:55,544 --> 00:01:58,365
Companies across industries
are desperately seeking for

39
00:01:58,365 --> 00:02:02,235
professionals who can design,
implement, interpret experiments

40
00:02:02,414 --> 00:02:04,605
that drive real business outcomes.

41
00:02:05,054 --> 00:02:09,504
And this is not only valuable for
somebody who is a data analyst.

42
00:02:09,804 --> 00:02:13,534
This skill understanding of the
skill can help engineers build

43
00:02:13,534 --> 00:02:15,754
smarter, experiment ready systems.

44
00:02:16,114 --> 00:02:19,774
Product managers can make
evidence-based features and roadmaps.

45
00:02:20,434 --> 00:02:26,554
Also, it'll bridge gap between data,
engineering, design, and product.

46
00:02:27,139 --> 00:02:30,919
Helping you communicate insights
effectively and influence decision

47
00:02:30,919 --> 00:02:32,569
making across organization.

48
00:02:32,899 --> 00:02:37,099
This skill makes you indispensable
as a translator between technical

49
00:02:37,099 --> 00:02:38,689
complexity and business value.

50
00:02:39,589 --> 00:02:43,419
Now let's look at where
experimentation can create value.

51
00:02:43,809 --> 00:02:48,589
The short answer is every organization
and every department every

52
00:02:48,589 --> 00:02:51,619
department in modern organization
can leverage experimentation

53
00:02:51,619 --> 00:02:53,509
to make data-driven decisions.

54
00:02:53,659 --> 00:02:56,269
You can see some examples
here on this slide.

55
00:02:56,689 --> 00:03:00,969
This is just like maybe 50 percentage
of examples that we covered here.

56
00:03:01,559 --> 00:03:06,569
The key is understanding which
experiment approach work best for

57
00:03:06,569 --> 00:03:08,999
each use case and business context.

58
00:03:09,044 --> 00:03:09,074
Okay.

59
00:03:09,674 --> 00:03:12,194
And how do we choose
right experiment design.

60
00:03:12,704 --> 00:03:16,424
Now let's look at commonly
used experiment designs.

61
00:03:16,754 --> 00:03:19,324
Let's start with AB
testing between subjects.

62
00:03:19,444 --> 00:03:20,734
This is a classic approach.

63
00:03:21,064 --> 00:03:23,674
This compares variance simultaneously.

64
00:03:23,914 --> 00:03:27,274
This is great for feature
releases and UI changes.

65
00:03:27,709 --> 00:03:32,729
Multivariate and factorial is used to
test multiple elements and multiple

66
00:03:32,729 --> 00:03:36,869
variations where we can learn
and capture interaction effects.

67
00:03:37,409 --> 00:03:42,579
Holdout studies are used in long-term
experiments to measure effects

68
00:03:42,579 --> 00:03:44,769
that AC occur over weeks and years.

69
00:03:45,269 --> 00:03:49,049
Business constraints should pick the
design, not the other way around.

70
00:03:49,319 --> 00:03:53,939
When traffic pattern or platform
complicates things, switch designs.

71
00:03:54,269 --> 00:03:57,149
And now let's go to
switchback experiments.

72
00:03:57,449 --> 00:04:03,024
Switchback experiments rotate treatment
by time, so the unit cycles through.

73
00:04:03,654 --> 00:04:07,584
A and B ideal for marketplaces
and two-sided platforms.

74
00:04:07,704 --> 00:04:12,654
This is mostly leveraged by companies like
DoorDash, Uber and Lyft, geo experiments

75
00:04:12,804 --> 00:04:19,224
randomized by region when user level
randomization risks spillovers useful for

76
00:04:19,224 --> 00:04:21,504
location based features and marketing.

77
00:04:21,884 --> 00:04:25,754
If true randomization isn't
possible, use credible alternatives.

78
00:04:26,114 --> 00:04:29,054
For example, something like
Difference in Difference Interrupted

79
00:04:29,054 --> 00:04:30,644
Time Series and Synthetic Control.

80
00:04:31,244 --> 00:04:33,074
These are called cultural
inference methods.

81
00:04:33,074 --> 00:04:34,364
I have three of them here.

82
00:04:34,784 --> 00:04:38,084
Difference in Difference compares
before and after changes in the

83
00:04:38,084 --> 00:04:42,224
treated group against similar control
group that didn't get the change.

84
00:04:42,614 --> 00:04:45,319
Interrupted time series
look for structured.

85
00:04:45,929 --> 00:04:50,669
Structure break right when the inter
intervention happens, if the level

86
00:04:50,699 --> 00:04:54,689
or the slope jumps at that point and
not anywhere else, that's the signal.

87
00:04:55,499 --> 00:04:56,579
Synthetic control.

88
00:04:56,939 --> 00:04:57,359
When you.

89
00:04:57,734 --> 00:05:02,114
Change something in one big
unit, say a state or city build

90
00:05:02,114 --> 00:05:05,684
weighted virtual control from the
other units that didn't change.

91
00:05:06,014 --> 00:05:09,134
So the pre pret trend matches yours.

92
00:05:09,484 --> 00:05:11,164
Expert judgment matters.

93
00:05:11,404 --> 00:05:13,354
Choose design based on constraint.

94
00:05:13,804 --> 00:05:16,054
Technical capabilities,
intervention nature.

95
00:05:16,299 --> 00:05:18,754
Know when to deviate
from pure randomization.

96
00:05:19,254 --> 00:05:23,984
Now let's look at some basic
concepts of experimentation.

97
00:05:24,494 --> 00:05:28,574
I wanna take a different direction in
talking about basic statistical concepts.

98
00:05:28,904 --> 00:05:32,324
There are a lot of excellent
resources online if you wanna

99
00:05:32,324 --> 00:05:34,794
learn about the topics here.

100
00:05:35,064 --> 00:05:39,214
But today I wanna spend some
time talking about trying tying

101
00:05:39,214 --> 00:05:41,114
these concepts to business.

102
00:05:41,474 --> 00:05:44,624
And why it's important for business
and how you can think in those lines.

103
00:05:45,014 --> 00:05:46,544
First hypothesis testing.

104
00:05:46,964 --> 00:05:48,554
This drives clarity.

105
00:05:48,854 --> 00:05:52,664
Knowing how to form good hypothesis
teaches structured thinking.

106
00:05:52,934 --> 00:05:56,714
It's not about is it better,
but what outcome and why.

107
00:05:57,074 --> 00:06:02,354
The mindset helps align experiments with
business strategy and ensures TE has

108
00:06:02,534 --> 00:06:05,144
clear success criteria before the launch.

109
00:06:05,564 --> 00:06:07,664
Confidence uncertainty builds.

110
00:06:08,024 --> 00:06:08,534
Trust.

111
00:06:08,744 --> 00:06:12,854
Communicating uncertainty transparently
builds stakeholder confidence.

112
00:06:13,094 --> 00:06:14,684
Leaders don't expect perfection.

113
00:06:14,984 --> 00:06:17,534
They expect honest ranges
and risk awareness.

114
00:06:17,714 --> 00:06:21,044
This scale separates a strategic
thinker from report generator.

115
00:06:21,094 --> 00:06:21,994
F, exercise and power.

116
00:06:22,594 --> 00:06:26,284
Understanding these concepts helps
you prioritize impactful changes,

117
00:06:26,494 --> 00:06:32,114
not statistically trivial ones four,
which is multiple testing disciplines.

118
00:06:32,174 --> 00:06:35,714
Today we run hundreds and thousands
of experiments every month.

119
00:06:36,104 --> 00:06:39,584
With the number of experiments and
variance we test, there comes a need

120
00:06:39,584 --> 00:06:42,434
to control a false positive rate.

121
00:06:42,734 --> 00:06:45,914
This protects the business from
false winds and bad rollouts.

122
00:06:46,319 --> 00:06:50,629
The last but not least, is
sequential and patient methods.

123
00:06:50,809 --> 00:06:53,839
In today's world, we see the
need for faster experimentation.

124
00:06:54,619 --> 00:06:58,489
To improve the velocity of experimentation
and get continuous learnings.

125
00:06:58,759 --> 00:07:04,339
We need modern techniques like sequential
and patient to support adaptive learnings.

126
00:07:04,579 --> 00:07:08,679
We have looked now that we have
looked at sequential aviation, which

127
00:07:08,679 --> 00:07:10,599
drive experimentation velocity.

128
00:07:10,869 --> 00:07:12,669
Let's also look at other techniques.

129
00:07:13,089 --> 00:07:18,589
In a traditional experiment setup,
we have to wait for larger samples.

130
00:07:19,009 --> 00:07:23,209
This delay is the experiment time and the
insight, and thereby product development.

131
00:07:24,019 --> 00:07:28,999
But if you look at the formula here
for calculating the sample size is

132
00:07:28,999 --> 00:07:30,234
directly proportional to the vari.

133
00:07:30,734 --> 00:07:35,174
If you reduce the variance by half, you
effectively have the required samples.

134
00:07:35,624 --> 00:07:39,894
By implementing variance reduction
techniques, you have you can

135
00:07:39,894 --> 00:07:43,634
achieve statistical significance
faster with fewer users.

136
00:07:43,754 --> 00:07:48,044
This translates to QCA decision
making, accelerated product iterations

137
00:07:48,044 --> 00:07:52,494
and more experiments run overall
driving rapid innovation and growth.

138
00:07:52,929 --> 00:07:57,039
Now that we have looked at the concept
of the variance reduction, let's look

139
00:07:57,039 --> 00:07:58,899
at some variance reduction techniques.

140
00:07:59,619 --> 00:08:03,499
Variance reduction techniques broadly
fall into five different buckets.

141
00:08:03,739 --> 00:08:08,119
The first one is regression
adjusted with pre period covariates.

142
00:08:08,489 --> 00:08:11,369
This is the umbrella for Cupid and coa.

143
00:08:11,469 --> 00:08:12,649
Pre, pre-post we.

144
00:08:13,314 --> 00:08:18,284
Pick one or two strong pre-treatment
predictors, adjust the outcome.

145
00:08:18,764 --> 00:08:20,474
This will help lower the variance.

146
00:08:20,654 --> 00:08:24,924
We will touch Cupid from this
umbrella in detail in the next slide.

147
00:08:25,824 --> 00:08:28,914
The second one is balanced
assignment at launch.

148
00:08:29,334 --> 00:08:33,744
This can be achieved by blocking,
stratify, randomization,

149
00:08:34,074 --> 00:08:35,814
or pay matching for geos.

150
00:08:36,145 --> 00:08:40,754
So treatment and control start
equal on the stuff that matters.

151
00:08:41,265 --> 00:08:42,914
And the third one is metric engineering.

152
00:08:43,364 --> 00:08:47,295
Noisy numerators and denominators
and heavy tails blow up variance.

153
00:08:48,135 --> 00:08:52,004
So instead, aggregate per
user or cluster level.

154
00:08:52,530 --> 00:08:58,830
Ize the extreme outliers log, transform
the right skewed metrics Cluster.

155
00:08:58,890 --> 00:09:00,820
Cluster aware estimations.

156
00:09:00,970 --> 00:09:04,840
Cluster aware estimations are mainly
used for geo and switchback testing.

157
00:09:05,290 --> 00:09:10,790
Analyze at the cluster or block level and
use the cluster robust standard errors.

158
00:09:11,030 --> 00:09:15,090
The last one is more of
operational thing where you.

159
00:09:15,900 --> 00:09:21,980
Make sure only count truly eligible
and actually exposed users and

160
00:09:22,220 --> 00:09:26,690
freeze the definition of the events
before the experiment starts.

161
00:09:27,020 --> 00:09:29,270
These are few variance
reduction techniques.

162
00:09:29,510 --> 00:09:31,980
Now let's look at QPI in detail.

163
00:09:32,480 --> 00:09:37,170
Cupid is one of the vastly used
variance reduction techniques In

164
00:09:37,170 --> 00:09:42,390
Cupid, we reduce the variance by using
pre-ex experiment data that can help

165
00:09:42,390 --> 00:09:44,130
explain the variance post experiment.

166
00:09:44,490 --> 00:09:45,660
Let's look at an example.

167
00:09:46,050 --> 00:09:51,750
Say we wanna run a test to see if people
run slower with weights attached to them.

168
00:09:52,360 --> 00:09:53,605
The experiment, mild time.

169
00:09:54,250 --> 00:09:59,150
It's a metric that we get when we run
the experiment and corresponding row will

170
00:09:59,150 --> 00:10:01,220
tell us if the weights were added or no.

171
00:10:01,730 --> 00:10:05,110
By looking at the data on the right
side, you can see you can see that the

172
00:10:05,110 --> 00:10:10,220
experiment mile time is already influenced
by how fast of a runner they are.

173
00:10:10,580 --> 00:10:16,920
And you can adjust the experiment mile
time by using average base mile time.

174
00:10:17,790 --> 00:10:22,560
And the average means base mile time
from the pre-ex experiment period

175
00:10:22,860 --> 00:10:25,260
can explain some of the variance.

176
00:10:25,680 --> 00:10:30,330
If you use the change column, which is a
difference between experiment mile time

177
00:10:30,330 --> 00:10:35,430
and base mile time, that can help you
estimate the impact of adding weights.

178
00:10:35,640 --> 00:10:40,470
So what we are doing here is using
pre-ex experiment data to reduce

179
00:10:40,470 --> 00:10:42,940
the variance that was explainable.

180
00:10:43,060 --> 00:10:43,090
Okay.

181
00:10:43,590 --> 00:10:48,720
Some of the best practices for Cupid are
using variables that has high correlation

182
00:10:48,720 --> 00:10:50,100
to the variable of the interest.

183
00:10:50,490 --> 00:10:55,080
When we are applying Cupid, there is
no need to stick with just one variable

184
00:10:55,080 --> 00:10:56,490
from the pre-ex experiment period.

185
00:10:56,700 --> 00:10:59,520
We can use multiple variables if
we believe that could reduce the

186
00:10:59,520 --> 00:11:03,660
variance, effectively, use baseline
that reflects the normal behavior.

187
00:11:04,635 --> 00:11:08,985
Make sure to use the data only
prior to the experiment not

188
00:11:09,015 --> 00:11:10,515
impacted by the experiment.

189
00:11:10,845 --> 00:11:15,245
You can handle the missing values
by imputing the values a color

190
00:11:15,245 --> 00:11:16,565
for people who are interested.

191
00:11:16,845 --> 00:11:22,245
There are some advancements for QPI
methodology called QA where in place of

192
00:11:22,245 --> 00:11:26,535
regression model and single or multiple
covariate from the pre-ex experiment

193
00:11:26,535 --> 00:11:30,525
period, PAC uses machine learning
models to predict the baseline metrics.

194
00:11:30,915 --> 00:11:34,215
By leveraging multiple variables
from the P experiment period.

195
00:11:34,425 --> 00:11:38,175
For those interested, I've
attached a attached link to this

196
00:11:38,175 --> 00:11:39,495
at the end of the presentation.

197
00:11:39,865 --> 00:11:41,365
Please feel free to take a look.

198
00:11:41,765 --> 00:11:43,535
Now let's look at common pitfalls.

199
00:11:44,465 --> 00:11:47,795
If something seems too good
to be true, it probably is.

200
00:11:48,125 --> 00:11:52,025
Whenever you find results that
are extreme, it's always good to

201
00:11:52,025 --> 00:11:54,005
check against common pitfalls.

202
00:11:54,545 --> 00:11:59,585
This can be broadly divided into four
sections technical implementation issues.

203
00:12:00,080 --> 00:12:04,460
Technical implementation issues,
cover sample ratio, mismatch logging

204
00:12:04,460 --> 00:12:07,050
gaps units of randomization mismatch.

205
00:12:07,050 --> 00:12:10,260
When the unit of randomization is
different from unit of analysis,

206
00:12:10,500 --> 00:12:11,790
it inflates the variance.

207
00:12:12,030 --> 00:12:14,940
The second bucket is
experiment design flaws.

208
00:12:15,300 --> 00:12:19,370
This is mainly cost because of
interference and contamination

209
00:12:19,920 --> 00:12:23,670
between the units in the control and
treatment, having impact on one other.

210
00:12:24,190 --> 00:12:26,420
Statistical AL analysis, if you have.

211
00:12:26,755 --> 00:12:32,685
If you need to peak, adjust false, a
positive rate so that there is no room

212
00:12:32,685 --> 00:12:35,565
for p hacking contextual challenges.

213
00:12:35,855 --> 00:12:39,005
There are some additional challenges
in the contextual challenges

214
00:12:39,215 --> 00:12:41,105
like meta drift and seasonality.

215
00:12:42,045 --> 00:12:47,095
Now that we've covered some basic
concepts of AB testing let's look at how

216
00:12:47,095 --> 00:12:49,675
to use this to accelerate your career.

217
00:12:50,175 --> 00:12:53,535
Understanding and practicing
these concepts will enhance

218
00:12:53,535 --> 00:12:55,185
your analytical maturity.

219
00:12:55,575 --> 00:13:00,555
You shift your mindset from reactive
to proactive experimentation strategy.

220
00:13:01,185 --> 00:13:05,835
This will help you challenge
opinions of leaders with evidence

221
00:13:06,045 --> 00:13:07,605
and guide strategic decisions.

222
00:13:07,815 --> 00:13:13,245
You will become a person bridging the
gap between cross-functional teams,

223
00:13:13,575 --> 00:13:17,445
translating stats into business impact,
and these will differentiate you in

224
00:13:17,445 --> 00:13:18,945
the market and make you invaluable.

225
00:13:19,815 --> 00:13:22,575
To do this repeatedly, you
need a platform support.

226
00:13:22,620 --> 00:13:26,520
Okay, let's walk through end-to-end
Cloud native experimentation.

227
00:13:26,520 --> 00:13:31,160
Architecture, a successful a
testing strategy relies on robust.

228
00:13:31,775 --> 00:13:35,945
Scalable cloud native architecture
that supports entire experimentation

229
00:13:35,945 --> 00:13:40,025
lifecycle from initial idea
to validation and iteration,

230
00:13:40,355 --> 00:13:42,095
enabling continuous learning path.

231
00:13:42,665 --> 00:13:47,045
A general cloud native architecture
of end-to-end experimentation flow

232
00:13:47,135 --> 00:13:48,995
can be categorized into few layers.

233
00:13:49,235 --> 00:13:49,715
First.

234
00:13:50,255 --> 00:13:54,305
We could have an infrastructure
which acts like a bedrock for

235
00:13:54,305 --> 00:13:56,315
getting compute building services.

236
00:13:56,495 --> 00:14:01,565
The second layer, we could categorize
this as data layer, where typical

237
00:14:01,565 --> 00:14:04,595
processing, along with the checks
and bounds are set in place.

238
00:14:05,375 --> 00:14:09,605
And the final on the top, we would
have experimentation platform, which

239
00:14:09,605 --> 00:14:14,615
would enable us to control the test
design routing, feature flagging.

240
00:14:14,915 --> 00:14:19,545
And this could run hypothesis testing
to enable us to make informed decisions.

241
00:14:19,545 --> 00:14:19,815
And itrate.

242
00:14:20,315 --> 00:14:24,865
Now let's jump into tips on
building experiment portfolio.

243
00:14:25,365 --> 00:14:30,625
The key three key steps in building
experimentation portfolio, working and

244
00:14:30,625 --> 00:14:32,875
documenting on realistic scenarios.

245
00:14:33,145 --> 00:14:37,445
Document the process showing problem
identification, hypothesis development,

246
00:14:37,475 --> 00:14:40,955
experimental design choice, and
statistical methodologies with

247
00:14:41,045 --> 00:14:45,370
clear business rationale contribute
to open source, contribute to.

248
00:14:46,130 --> 00:14:52,250
Experimentation frameworks and statistical
libraries like STA Model PMCQ, flow Show

249
00:14:52,250 --> 00:14:57,320
Business Impact and recommendation about
next steps to complete end-to-end flow.

250
00:14:57,710 --> 00:15:02,390
Develop a mindset to iterate on the ideas
instead of one and done experiments.

251
00:15:02,840 --> 00:15:07,040
I also want to share some success
strategies for interviews.

252
00:15:07,685 --> 00:15:08,135
Interview.

253
00:15:08,135 --> 00:15:12,525
Success in experimentation fields
requires not only technical expertise,

254
00:15:12,525 --> 00:15:15,735
but also mastery of the concepts
and being able to explain it in

255
00:15:15,735 --> 00:15:17,865
simple terms to the stakeholders.

256
00:15:18,225 --> 00:15:22,455
Prepare specific examples where
your experimentation work drove

257
00:15:22,545 --> 00:15:24,135
measurable business outcomes.

258
00:15:24,405 --> 00:15:28,005
End-to-end experimentation
knowledge on how experimentation

259
00:15:28,245 --> 00:15:30,315
fits larger ecosystem and how.

260
00:15:30,555 --> 00:15:33,765
How to run experiments at scale
will definitely add to this.

261
00:15:34,005 --> 00:15:39,375
A combination of these three would
help you be successful in interviews.

262
00:15:40,275 --> 00:15:43,715
Now that we have a look, now that
we have spoken about the interview

263
00:15:43,805 --> 00:15:47,855
success strategies, let's look at how
the career trajectory of an IC in the

264
00:15:47,855 --> 00:15:51,425
field of data science specializing
in experimentation looks like.

265
00:15:51,895 --> 00:15:57,385
An entry level candidate is expected
to have rock solid EBIT s fundamentals.

266
00:15:57,900 --> 00:16:01,830
Clean hypothesis, reproducible
analysis, and clear writing.

267
00:16:02,520 --> 00:16:06,780
A mid-level candidate is
expected to lead multiple tests.

268
00:16:06,840 --> 00:16:10,570
Mentor analysts, have knowledge
about sequential testing and patient

269
00:16:10,600 --> 00:16:12,670
testing, standardized guardrails.

270
00:16:13,060 --> 00:16:16,660
A senior would be setting
experimentation, strategy,

271
00:16:16,660 --> 00:16:19,000
partnering with engineering to drive.

272
00:16:19,225 --> 00:16:23,655
The platform experimentation platform
drive cross org decisions with a

273
00:16:23,655 --> 00:16:28,875
quantified impact, a principal or
staff would shape the system by

274
00:16:29,225 --> 00:16:33,815
platform roadmaps contributing to open
source conference docs, teaching the

275
00:16:33,815 --> 00:16:35,675
organization on how to learn faster.

276
00:16:36,175 --> 00:16:39,245
To summarize there are
three key takeaways.

277
00:16:39,395 --> 00:16:44,045
One is develop the technical
expertise, pair it with strong

278
00:16:44,045 --> 00:16:48,305
business acumen, master end-to-end
experimentation ecosystem.

279
00:16:48,725 --> 00:16:52,055
Together these form foundation
for a successful career.

280
00:16:52,555 --> 00:16:55,815
I will end my talk with the action plan.

281
00:16:56,415 --> 00:16:57,255
Start today.

282
00:16:57,765 --> 00:17:00,075
Start today by building something small.

283
00:17:00,495 --> 00:17:04,925
Engage with the community, share the
learnings, and learn from the community.

284
00:17:05,265 --> 00:17:08,565
Stay current with emerging
trends and experimentation.

285
00:17:08,895 --> 00:17:11,325
These three will differentiate
you from the rest.

286
00:17:11,745 --> 00:17:14,105
Okay, that's all I have for today.

287
00:17:14,135 --> 00:17:15,395
Thanks for joining me.

288
00:17:15,705 --> 00:17:19,305
Please reach out to me on LinkedIn
if you have any questions or simply

289
00:17:19,305 --> 00:17:23,655
wanna chat about experimentation
or any advice on your career,

290
00:17:23,655 --> 00:17:25,005
I'll be happy to connect there.

291
00:17:25,455 --> 00:17:27,765
All the best to everyone
for all the future yours.

292
00:17:27,825 --> 00:17:28,585
Thank you.

