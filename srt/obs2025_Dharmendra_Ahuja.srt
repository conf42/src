1
00:00:00,500 --> 00:00:01,880
Everyone, thanks for joining.

2
00:00:02,090 --> 00:00:06,260
My name is Der Hoja and
I'm a DevOps lead in IBM.

3
00:00:06,890 --> 00:00:10,760
So today I'm gonna talk about unwilling
the power of observability using

4
00:00:10,940 --> 00:00:13,910
fluent be in a WS E case environment.

5
00:00:14,410 --> 00:00:19,840
So as part of this topic, the
we are going to talk about.

6
00:00:20,090 --> 00:00:23,810
The power of observability
in our daily life.

7
00:00:24,200 --> 00:00:29,279
So as the cloud applications scale
across the cluster guiding gaining

8
00:00:29,369 --> 00:00:33,669
visibility into into our cloud
environment becomes extremely difficult

9
00:00:33,759 --> 00:00:35,979
as the application loads scale.

10
00:00:36,640 --> 00:00:40,449
And when the application complexity
increases because of the scalability.

11
00:00:40,990 --> 00:00:45,109
It is getting very difficult
to get into the of.

12
00:00:45,749 --> 00:00:46,619
Of the application.

13
00:00:47,159 --> 00:00:53,459
So it is so in the session, like we will
unveil how to harness the true power of

14
00:00:53,459 --> 00:00:58,869
possibility in our a WC case environment
using fuel and weight which is like a,

15
00:00:59,679 --> 00:01:03,289
which is like a lightweight processor
which is built for modern architecture.

16
00:01:03,589 --> 00:01:07,449
And also we will learn how fluent
bit can actually efficiently.

17
00:01:07,909 --> 00:01:13,319
Efficiently collect power filter and
and analyze and route the logs from

18
00:01:13,319 --> 00:01:18,999
our containers platform and and like
how it routes to like how it routes

19
00:01:18,999 --> 00:01:24,509
the Kubernetes metadata also and stream
them into into restorations like Amazon

20
00:01:24,509 --> 00:01:29,649
CloudWatch S3 or open search or any
observability backend of a choice.

21
00:01:30,149 --> 00:01:35,279
And so we'll break down also the real
world use cases in this session and how

22
00:01:35,279 --> 00:01:38,549
we deploy in in the real world scenario.

23
00:01:38,819 --> 00:01:42,549
So we'll be covering the demo
also as part of this session.

24
00:01:43,049 --> 00:01:45,329
So now we'll talk about the observability.

25
00:01:45,779 --> 00:01:46,829
What is observability?

26
00:01:46,834 --> 00:01:50,539
Observability is like ability
to measure the internal state

27
00:01:50,789 --> 00:01:52,319
by examining the outputs.

28
00:01:52,769 --> 00:01:56,684
So the key pillar of
observability like a log.

29
00:01:57,624 --> 00:01:59,394
Mattresses and events, right?

30
00:01:59,794 --> 00:02:01,054
And the traces basically.

31
00:02:01,524 --> 00:02:05,124
And observability is very important
in our modern architecture because

32
00:02:05,154 --> 00:02:09,894
EKS in, in that today's presentation,
we are talking in the context of

33
00:02:09,894 --> 00:02:14,554
the EKS cluster and EKS cluster in
AWS around the complex workload.

34
00:02:15,004 --> 00:02:20,279
So observability will help to
track the performance, health, and

35
00:02:20,279 --> 00:02:22,659
usage patterns across the services.

36
00:02:23,124 --> 00:02:26,655
And also the, also across
the notes and the pods.

37
00:02:27,165 --> 00:02:31,665
And also it also provides the visibility
into the control plane and also the

38
00:02:31,665 --> 00:02:33,805
data plane notes data plane events.

39
00:02:34,105 --> 00:02:39,215
And also it helps to detect the pro
pod crashes and any node pressure.

40
00:02:39,335 --> 00:02:40,625
It'll help us detect that.

41
00:02:41,075 --> 00:02:46,165
And also it helps to monitor the CPO
memory disc related mattresses also.

42
00:02:46,665 --> 00:02:48,135
Observability in EKS.

43
00:02:48,225 --> 00:02:49,185
We'll talk about it now.

44
00:02:49,325 --> 00:02:54,105
EKS is like a fully managed Kubernetes
service by AWS and it supports native

45
00:02:54,105 --> 00:02:55,710
tools for, monitoring and logging.

46
00:02:56,110 --> 00:02:58,420
So a case s logs, everything.

47
00:02:58,520 --> 00:03:02,030
Basically it logs application logs
and systems and Kubernetes logs.

48
00:03:02,360 --> 00:03:03,740
So we have these kind of logs.

49
00:03:03,740 --> 00:03:09,090
We have any any case environment, and
we have mattresses like CPU Ma and

50
00:03:09,090 --> 00:03:10,890
also we can have custom metrics also.

51
00:03:10,890 --> 00:03:13,320
Also and everything is like traceable.

52
00:03:13,410 --> 00:03:15,750
So we have whatever the
requests are coming in.

53
00:03:16,080 --> 00:03:18,750
And going out, like request
flows and performance monitoring,

54
00:03:18,840 --> 00:03:19,770
like performance tracing.

55
00:03:19,770 --> 00:03:24,250
We have everything we can trace
and it is and basically this fluent

56
00:03:24,250 --> 00:03:28,230
bit help us to trace all the logs
and and performance mattresses.

57
00:03:28,410 --> 00:03:33,030
So that is the advantage of having
fluent bit as a log processor

58
00:03:33,060 --> 00:03:34,410
in a WC case environment.

59
00:03:34,740 --> 00:03:37,810
So now we'll talk about
the next fluent bit.

60
00:03:38,200 --> 00:03:40,000
So what does fluent bit.

61
00:03:40,930 --> 00:03:45,180
Mean, so fluent Bit is like a
lightweight log processor, and it's like

62
00:03:45,185 --> 00:03:50,240
a forwarder, which allows to collect
the data and logs from our different

63
00:03:50,240 --> 00:03:55,980
sources and it'll enrich them and
filter them and send it to the multiple

64
00:03:55,980 --> 00:04:02,060
destinations like CloudWatch and like
a Kinesis data fire hose S3 bucket.

65
00:04:02,520 --> 00:04:06,030
Or it can also route it to
Amazon Open set service as well.

66
00:04:06,540 --> 00:04:11,220
And be, and like it can be used to
ship logs to various destinations.

67
00:04:11,520 --> 00:04:15,790
But in, in this presentation, we'll be
focusing on forwarding the logs to to

68
00:04:15,795 --> 00:04:17,345
do CloudWatch, with the, in the demo.

69
00:04:18,335 --> 00:04:22,595
So now we'll talk about why
fluent bit is important.

70
00:04:22,595 --> 00:04:27,449
So fluent bit is like a. Critical
collection layer, and it performs

71
00:04:27,449 --> 00:04:29,069
several essential functions.

72
00:04:29,519 --> 00:04:31,464
So what Fluent width does?

73
00:04:31,884 --> 00:04:33,804
Fluent width does the collection for us.

74
00:04:34,164 --> 00:04:37,254
So when we say collection, so
it'll capture the logs from a

75
00:04:37,254 --> 00:04:43,044
different containers and and also
the endpoints and it'll enrich them.

76
00:04:43,294 --> 00:04:47,064
Enrich stem means like we can augment
the logs augment the log data with the

77
00:04:47,064 --> 00:04:51,844
Kubernetes metadata, we can append we can
enrich it and we can append additional

78
00:04:51,844 --> 00:04:56,604
information to the logs, and we can
add custom tags, we can add contextual

79
00:04:56,604 --> 00:04:59,099
information also to, to the logs.

80
00:04:59,459 --> 00:05:02,069
And also we can transform
the locks, right?

81
00:05:02,309 --> 00:05:05,999
So we like, we can do structuring,
we can do filtering, and we can

82
00:05:05,999 --> 00:05:08,929
also mask sensitive information
from the logs as well.

83
00:05:09,499 --> 00:05:10,459
That is what it does.

84
00:05:10,459 --> 00:05:11,969
And the fourth one is routing.

85
00:05:12,239 --> 00:05:17,929
So we can direct the logs to appropriate
destination based on the content namespace

86
00:05:18,379 --> 00:05:20,209
and any other attribute which we specify.

87
00:05:20,209 --> 00:05:23,119
So we can route it based
on that, those attributes.

88
00:05:23,619 --> 00:05:27,279
So how the fluent weight
works, that is the next topic.

89
00:05:27,329 --> 00:05:29,819
So Fluent Weight works.

90
00:05:30,159 --> 00:05:34,289
So it, it has a log sources
locks, basically originate from

91
00:05:34,289 --> 00:05:40,469
different sources like containers
and log files or system D, right?

92
00:05:40,469 --> 00:05:43,139
So these are the log sources from
where it will collect the locks.

93
00:05:43,499 --> 00:05:44,909
And we have a input plugin.

94
00:05:45,509 --> 00:05:50,099
So input plugin, basically these
input plugins collect the locks.

95
00:05:50,579 --> 00:05:52,489
And and then we have a parsers.

96
00:05:52,729 --> 00:05:55,399
So parsers are basically the transformers.

97
00:05:55,399 --> 00:06:00,479
So they transform the logs into a
structure Metadata, basically we use

98
00:06:00,479 --> 00:06:03,419
JSON passing, and then we have filters.

99
00:06:03,719 --> 00:06:09,459
So filters will enrich the logs and it
will, or it'll remove the specify specific

100
00:06:09,459 --> 00:06:12,429
logs from the specific data from the logs.

101
00:06:12,859 --> 00:06:14,149
And then we have buffers.

102
00:06:14,389 --> 00:06:19,379
So buffers is basically a temporary
storage where we are storing the locks in

103
00:06:19,379 --> 00:06:21,869
a ma or disk in case of a high throughput.

104
00:06:22,364 --> 00:06:23,244
Or network delay.

105
00:06:23,934 --> 00:06:26,034
And then we have output plugin.

106
00:06:26,304 --> 00:06:30,444
So output plugin is basically to push
process logs to external location

107
00:06:30,774 --> 00:06:35,804
like Elasticsearch or Amazon S3,
or Kafka or Kinesis Fire hose.

108
00:06:36,304 --> 00:06:41,504
So now we will talk about the setup
to set up the fluent bit we need to

109
00:06:41,504 --> 00:06:43,334
collect the logs from containers.

110
00:06:43,764 --> 00:06:46,344
So to do that we need an IAM roll.

111
00:06:46,749 --> 00:06:50,119
So I role we need which will be
attached to our cluster notes.

112
00:06:50,419 --> 00:06:54,399
So we will be using a service account for
that to achieve rollback access control.

113
00:06:54,899 --> 00:06:57,539
And also we will be in the demo.

114
00:06:57,539 --> 00:07:03,099
We'll see how we will set up the flow
bit as a demo set to send the logs to

115
00:07:03,099 --> 00:07:05,379
the CloudWatch to the CloudWatch logs.

116
00:07:05,780 --> 00:07:07,490
So we'll be creating the log group.

117
00:07:07,905 --> 00:07:12,255
And also we'll be having log stream,
so we'll be configuring these

118
00:07:12,255 --> 00:07:13,935
things as part of the help chart.

119
00:07:14,395 --> 00:07:17,545
So we'll see that as part of the
demo that we are, will be using

120
00:07:17,545 --> 00:07:21,295
fluent help chart to, to deploy
to do install the fluent bit.

121
00:07:21,835 --> 00:07:27,125
So we'll see that and, and also
we'll be using YDC connector.

122
00:07:27,305 --> 00:07:27,575
Alright.

123
00:07:27,575 --> 00:07:30,455
OIDC provider to which
needs to be created.

124
00:07:31,035 --> 00:07:34,815
And also we will need, for I am role,
we need to create the IM policy.

125
00:07:34,875 --> 00:07:36,315
So it is part of a prerequisite.

126
00:07:36,615 --> 00:07:39,315
So we will be creating
the IAM policy upfront.

127
00:07:39,770 --> 00:07:43,050
And also I'll be using
vs code for the demo.

128
00:07:43,050 --> 00:07:48,600
So I'm already installing E-K-C-T-L,
cube, CTL, and helm and GI B on

129
00:07:48,600 --> 00:07:54,309
my machine so that I can make
the demo more more, more doable.

130
00:07:54,310 --> 00:07:57,879
Just wanted to make sure that we,
once we have a prerequisites in

131
00:07:57,879 --> 00:08:01,419
place, it'll be easier to follow the
instructions, which I mention here.

132
00:08:01,919 --> 00:08:03,569
So now we'll come to the demo.

133
00:08:04,069 --> 00:08:10,259
So as part of the demo setup we will
we will also need the log server.

134
00:08:10,649 --> 00:08:14,699
So apart from the pre prerequisites
we discussed, so we need to, once

135
00:08:14,699 --> 00:08:19,104
the prerequisite setup is completed,
we need to have a application

136
00:08:19,104 --> 00:08:22,979
which needs, which will, which
we'll be using to route the logs.

137
00:08:23,579 --> 00:08:26,959
So to do that we will we'll be
using the log server, like an

138
00:08:26,959 --> 00:08:31,269
Ionix server to generate the
logs, and then we will forward the

139
00:08:31,269 --> 00:08:32,799
service to our local environment.

140
00:08:33,159 --> 00:08:35,739
And we will use call command to do that.

141
00:08:36,099 --> 00:08:38,169
So to, to make the request.

142
00:08:38,539 --> 00:08:42,320
And also what we'll do is we
will be watching the logs locally

143
00:08:42,590 --> 00:08:45,620
and also we'll be routing those
logs too to the CloudWatch logs.

144
00:08:46,355 --> 00:08:47,345
To the cloud log group.

145
00:08:47,645 --> 00:08:50,555
So we will do that in a three terminal.

146
00:08:50,555 --> 00:08:54,465
So in the terminal one, we'll be
using we'll be using, we will be

147
00:08:54,465 --> 00:08:57,885
using the terminal to forward the
logger server traffic locally.

148
00:08:58,335 --> 00:09:03,115
And then and we'll be route, I mean
making a port forwarding to that.

149
00:09:03,405 --> 00:09:06,255
And then in the second terminal,
we'll be watching the logs locally.

150
00:09:06,940 --> 00:09:09,600
And the third time we will
be making the call request.

151
00:09:09,850 --> 00:09:14,560
So we, and then we will be seeing like
whatever the logs we are seeing in a

152
00:09:14,560 --> 00:09:17,200
local machine, we will be also forwarding
it to the CloudWatch locker room.

153
00:09:17,740 --> 00:09:19,240
So this is what we are going to see.

154
00:09:19,750 --> 00:09:23,470
And then once everything is
completed we'll see all the logs

155
00:09:23,470 --> 00:09:26,660
going to the CloudWatch log group
as mentioned here in the screen.

156
00:09:27,050 --> 00:09:28,100
So it is like a sample.

157
00:09:28,350 --> 00:09:30,560
But we will see the
similar thing in our demo.

158
00:09:30,980 --> 00:09:32,870
So now let's get to the demo.

159
00:09:33,370 --> 00:09:34,990
We will start the demo now.

160
00:09:35,350 --> 00:09:39,150
So as part of the demo, I'm
going to further instructions.

161
00:09:39,810 --> 00:09:41,400
There are several things
we are going to do.

162
00:09:41,760 --> 00:09:44,730
So first we are going to create
the Kubernetes cluster first.

163
00:09:45,150 --> 00:09:49,225
So for creating the Kubernetes
cluster, we have a EKS config

164
00:09:49,230 --> 00:09:52,130
YAML file, which we can see here.

165
00:09:53,040 --> 00:09:56,860
And as part of this yaml it
is a cluster configuration.

166
00:09:57,190 --> 00:10:00,520
So we are going to create a ees
cluster with the name ees cluster demo.

167
00:10:01,020 --> 00:10:05,150
And and it is a configuration, so I'm
not going to go into the detail of that.

168
00:10:05,690 --> 00:10:08,150
So I'm going to create
the ees cluster first.

169
00:10:08,930 --> 00:10:16,020
And then so I'm going to, run e case CT
command to, to create the EGA cluster.

170
00:10:16,860 --> 00:10:17,910
So here's the point.

171
00:10:18,410 --> 00:10:22,610
So as soon as I execute this command, you
can see that it is creating a cluster.

172
00:10:23,225 --> 00:10:27,545
So we will look into the
cloud formation stack.

173
00:10:28,465 --> 00:10:29,875
I'll just go move to that.

174
00:10:30,375 --> 00:10:36,085
So it is creating all the dependencies,
for example, net gateway subnet routes and

175
00:10:36,115 --> 00:10:38,665
route tables, control plane, et cetera.

176
00:10:39,245 --> 00:10:42,785
It seems like there are several components
being created, so it is almost done.

177
00:10:43,285 --> 00:10:45,895
So we'll just wait for a
couple of more minutes.

178
00:10:46,395 --> 00:10:50,835
We can see that the creation of the
cluster is completed and we can see

179
00:10:50,835 --> 00:10:56,025
the associated sub components as part
of the cluster is also completed.

180
00:10:56,535 --> 00:11:02,110
So here all the sub components,
you can see like route tables,

181
00:11:02,440 --> 00:11:07,140
internet gateway, and and control,
plane n Gateway, et cetera.

182
00:11:07,770 --> 00:11:09,510
All the dependent components
have been created now.

183
00:11:09,930 --> 00:11:13,920
So we are good with the cluster
creation process, which is

184
00:11:13,920 --> 00:11:15,090
a prerequisite for the demo.

185
00:11:15,240 --> 00:11:22,250
So I did that as an extra step so that
I can demonstrate end-to-end demo here.

186
00:11:22,750 --> 00:11:25,420
So now we will go to the next step.

187
00:11:25,960 --> 00:11:28,410
The next step is we'll
update the q config.

188
00:11:28,910 --> 00:11:30,260
So that we can access the cluster.

189
00:11:31,010 --> 00:11:34,690
So I will just, just execute
the command for that.

190
00:11:35,190 --> 00:11:35,550
Okay.

191
00:11:36,050 --> 00:11:37,090
I will just go here.

192
00:11:37,590 --> 00:11:42,305
So we'll see here, I think a few
things happening behind the scene.

193
00:11:42,785 --> 00:11:46,700
So I think we need to watch
cluster again, maybe three.

194
00:11:47,200 --> 00:11:52,430
Let's give a minute and let me look
at what's happening at the code.

195
00:11:52,930 --> 00:11:58,210
Yeah, so now it is trying to create,
it's trying to create the add-ons

196
00:11:58,270 --> 00:12:00,330
that is VPCC and I and Q proxy.

197
00:12:00,450 --> 00:12:02,970
So it is trying to create that.

198
00:12:03,470 --> 00:12:06,310
So we may have one-on-one
more stack coming in.

199
00:12:06,400 --> 00:12:10,740
So we, before we access the
cluster using, cLI, we need

200
00:12:10,740 --> 00:12:12,570
to have this process finished.

201
00:12:12,750 --> 00:12:14,460
Then we can continue on that.

202
00:12:14,460 --> 00:12:18,380
So as we say, as we can see, it is it
escalated on the cloud formation stack.

203
00:12:18,380 --> 00:12:23,695
So we need to wait for a few more
minutes until that is completed.

204
00:12:24,355 --> 00:12:25,775
So I'll just show you.

205
00:12:26,275 --> 00:12:30,595
We can see that the cluster dependent
dependencies is also being created.

206
00:12:30,985 --> 00:12:33,925
So we can, we seen the load
group is also created now,

207
00:12:34,675 --> 00:12:36,145
which we can see on the screen.

208
00:12:36,445 --> 00:12:39,145
That stack is also completed,
which is a dependency here.

209
00:12:39,685 --> 00:12:40,795
So that is good.

210
00:12:41,435 --> 00:12:42,645
Now we will, we can see that.

211
00:12:42,645 --> 00:12:44,955
So I, hopefully we should be
able to access the cluster now.

212
00:12:45,745 --> 00:12:48,175
So I'll just switch to VS.

213
00:12:48,175 --> 00:12:48,475
Code.

214
00:12:48,775 --> 00:12:48,995
Yes.

215
00:12:49,105 --> 00:12:49,395
Code

216
00:12:49,895 --> 00:12:50,875
so we can

217
00:12:51,375 --> 00:12:52,750
Yeah, we can see here, right?

218
00:12:53,470 --> 00:12:54,020
That, the.

219
00:12:54,520 --> 00:12:57,100
It is showing that cluster
dependency is being created.

220
00:12:57,640 --> 00:13:04,630
So we we will go to the this point and
now we will just execute the command

221
00:13:04,630 --> 00:13:07,980
to access the cluster and here go.

222
00:13:08,480 --> 00:13:09,375
So let's say.

223
00:13:09,875 --> 00:13:12,455
You can see that plus was created
and we can, we are able to exit

224
00:13:12,455 --> 00:13:14,375
the pos which are already created.

225
00:13:15,065 --> 00:13:16,445
So that's a good step.

226
00:13:16,895 --> 00:13:22,465
Now, the next step would be we will
create the namespace fluent bit as part

227
00:13:22,465 --> 00:13:27,235
of this demo, as we are going to deploy
all the resources in the fluent namespace.

228
00:13:27,735 --> 00:13:30,545
So we will go here and execute.

229
00:13:30,815 --> 00:13:33,125
The one cube, it'll create
namespace fluid bit.

230
00:13:33,625 --> 00:13:36,825
Oh, it seems fluent namespace is created.

231
00:13:37,305 --> 00:13:38,415
So that's great.

232
00:13:39,015 --> 00:13:45,125
Now we will go ahead and we will
create the YDC provider now so that

233
00:13:45,125 --> 00:13:49,745
we can associate the service account
with the YDC provider later on.

234
00:13:50,105 --> 00:13:54,195
So first we'll create our YDC
provider as that we can associate

235
00:13:54,195 --> 00:13:55,905
the cluster to the YDC provider.

236
00:13:56,775 --> 00:13:59,205
So I'm going to execute
the EK CT L command.

237
00:13:59,625 --> 00:14:03,045
So as part of this process,
EK CT is a prerequisite.

238
00:14:03,435 --> 00:14:07,185
So we need to have the EK
CT installed on machine.

239
00:14:07,215 --> 00:14:08,745
I have already taken care of that.

240
00:14:09,245 --> 00:14:13,605
Now I'm going to execute EK
CT command to to associate.

241
00:14:13,605 --> 00:14:17,705
I am ODC provider with the
cluster, and this is Dan.

242
00:14:18,410 --> 00:14:22,250
So it has created an I am Open 80
connector provider for the cluster.

243
00:14:23,060 --> 00:14:23,900
That's great.

244
00:14:24,740 --> 00:14:26,390
I will go to the next step.

245
00:14:26,900 --> 00:14:32,140
So next step is we'll create a am
service account so that we can have a

246
00:14:32,140 --> 00:14:33,880
little back access control in place.

247
00:14:34,360 --> 00:14:37,980
So I'm going to execute
the command for that.

248
00:14:38,650 --> 00:14:44,560
So in order to do that, we need to
have a, IM policy created in place.

249
00:14:44,625 --> 00:14:45,795
Which is a prerequisite.

250
00:14:46,435 --> 00:14:51,205
And I have already taken care of that
as part of the prerequisite earlier.

251
00:14:51,205 --> 00:14:54,995
So I'm going to just execute
the command and I will walk

252
00:14:54,995 --> 00:14:56,625
through the, policy later on.

253
00:14:57,125 --> 00:14:58,685
So here we can see.

254
00:14:59,185 --> 00:15:02,945
So here is I'm creating the AM
service account in the fluent with

255
00:15:02,945 --> 00:15:07,900
the Fluent Withs, a name, and I'm
attaching the policy which is EESS

256
00:15:07,900 --> 00:15:12,490
Cluster Demo Policy which I'll show
it to you here how I created it.

257
00:15:13,060 --> 00:15:15,155
So I'm executing this comment now.

258
00:15:16,075 --> 00:15:18,460
And it'll create a stack
here in cloud formation.

259
00:15:19,060 --> 00:15:21,400
And I will show you how it looks like.

260
00:15:21,900 --> 00:15:23,970
We can see that there's
another stack is created.

261
00:15:24,310 --> 00:15:27,720
There is a, in case IT cluster
demo IM service account.

262
00:15:28,020 --> 00:15:29,010
So that is the.

263
00:15:29,260 --> 00:15:32,650
That is a new new cloud commission
stack is being created right

264
00:15:32,650 --> 00:15:34,690
now, and it is in progress.

265
00:15:35,020 --> 00:15:38,840
I will just quickly show you where
it where I've created the policy.

266
00:15:39,290 --> 00:15:42,310
So I go to IN and go to, I am here.

267
00:15:42,810 --> 00:15:44,580
And will go to the policies.

268
00:15:45,080 --> 00:15:50,800
So as part of the policy, I'll just, so I
have created this EKS cluster demo policy

269
00:15:51,670 --> 00:15:56,140
and I'm giving the full access to this
CloudWatch logs as part of this demo.

270
00:15:56,230 --> 00:16:00,100
So that is what I did and I created
this in advance so that we can,

271
00:16:01,090 --> 00:16:03,040
attach this policy to the IM role.

272
00:16:03,790 --> 00:16:08,160
So I'll just go back to cloud
formation stack, as so that

273
00:16:08,160 --> 00:16:10,310
I can show the next steps.

274
00:16:10,810 --> 00:16:16,465
So we'll go back here and you can see
that AM service account is also created.

275
00:16:17,140 --> 00:16:18,910
So the cloud formation sticks.

276
00:16:18,960 --> 00:16:20,130
It says create complete.

277
00:16:20,130 --> 00:16:20,970
So we are good now.

278
00:16:21,570 --> 00:16:28,560
So I'll just go back to my V code and
to execute further the commands here,

279
00:16:29,060 --> 00:16:33,830
go to the V code and I'm excluding next.

280
00:16:33,950 --> 00:16:34,760
Next commands now.

281
00:16:35,060 --> 00:16:38,690
So now since we have IM service account
created, so I'm going to just describe

282
00:16:38,690 --> 00:16:42,620
it and see whether AM service's
account is created properly or not.

283
00:16:43,390 --> 00:16:44,440
And I'm going to access it.

284
00:16:44,470 --> 00:16:45,700
So I'm going to,

285
00:16:46,200 --> 00:16:49,650
to describe my service account here.

286
00:16:49,900 --> 00:16:53,800
So I'm saying yes, it is
created so we can see that.

287
00:16:54,570 --> 00:16:58,560
So it is, it has created this,
it is associated with this role.

288
00:16:59,070 --> 00:17:04,590
So the role is also created as part
of this process and, the name is

289
00:17:04,590 --> 00:17:06,090
fluent with essay, so that's good.

290
00:17:06,590 --> 00:17:10,640
Now as part of the next step
so I just wanted to highlight

291
00:17:10,640 --> 00:17:12,580
that I executed Cube CT Command.

292
00:17:12,670 --> 00:17:17,180
So Cube, CTLE xe also needs to
be installed similar to E-K-C-T-L

293
00:17:17,230 --> 00:17:18,640
as a part of the prerequisites.

294
00:17:19,040 --> 00:17:20,150
So we need to take care of that.

295
00:17:20,440 --> 00:17:25,600
Now as part of the next step I will need
to install hand charts for fluent bit.

296
00:17:26,150 --> 00:17:31,280
To the helm report so that we can
and so once the helm was really

297
00:17:31,360 --> 00:17:35,270
add the helm report for one bit
and then we'll update it so that

298
00:17:35,270 --> 00:17:37,250
we can access helm charts for that.

299
00:17:37,640 --> 00:17:39,560
So I'm going to execute
helm commands for that.

300
00:17:39,980 --> 00:17:43,400
So as part of the Prerequisites,
prerequisites, helm also should

301
00:17:43,400 --> 00:17:45,380
be installed on the machine.

302
00:17:45,880 --> 00:17:51,235
So I'll go here and, i'll just go here and

303
00:17:51,735 --> 00:17:57,515
I'm just adding ham repo for the
fluent bit, so my, that is good.

304
00:17:58,175 --> 00:18:01,225
Is now I'm going to update the.

305
00:18:01,725 --> 00:18:04,000
So we'll be going here and updating.

306
00:18:04,250 --> 00:18:05,390
So update is completed.

307
00:18:05,390 --> 00:18:05,990
That's great.

308
00:18:06,920 --> 00:18:08,330
So we go to the next step.

309
00:18:08,580 --> 00:18:14,540
Now since PU is updated now, we'll install
the flowing bit using home commands.

310
00:18:14,690 --> 00:18:16,970
So we have.

311
00:18:17,470 --> 00:18:19,050
I am going to for that.

312
00:18:19,550 --> 00:18:21,500
So here is the helm command.

313
00:18:21,500 --> 00:18:23,540
So it says Helm upgrade, install one bit.

314
00:18:24,140 --> 00:18:28,815
So it is creating installing the fluent
bit release and then it is space flow bit.

315
00:18:29,205 --> 00:18:32,265
Service account we are not creating
because it's all, I already took care of

316
00:18:32,265 --> 00:18:37,095
that and we are just giving the service
account name, which is already there.

317
00:18:37,695 --> 00:18:41,115
And CloudWatch is going to be
enabled through, and we are going

318
00:18:41,115 --> 00:18:47,055
to create this in US East one region
and it is going to check if the

319
00:18:47,555 --> 00:18:48,755
log group name is there or not.

320
00:18:48,755 --> 00:18:53,135
So it's going to set the log
group name if it is not created.

321
00:18:53,795 --> 00:18:58,245
And then it is going to, we
are setting the stream for from

322
00:18:58,245 --> 00:18:59,895
flu and with, from fluent demo.

323
00:18:59,935 --> 00:19:04,110
So all that cloud watch logs will be
will be with this stream prefix name.

324
00:19:04,970 --> 00:19:07,040
And then we are seeing the
region should be assist one.

325
00:19:07,130 --> 00:19:08,095
So this execute is command.

326
00:19:08,595 --> 00:19:10,515
So it says affluent bit does not exist.

327
00:19:10,515 --> 00:19:14,505
So installing it now, so
we have to wait few minutes

328
00:19:15,005 --> 00:19:18,370
right now it is executed, so that's great.

329
00:19:19,070 --> 00:19:22,530
Now as part of the next step,
for the demo purposes, we need

330
00:19:22,530 --> 00:19:23,670
to create a demo namespace.

331
00:19:24,170 --> 00:19:31,980
So I'm going to create a new namespace
demo where we will be installing our app.

332
00:19:32,250 --> 00:19:36,175
So we will just, first, we are going
to create the demo namespace space.

333
00:19:36,675 --> 00:19:38,595
So demo namespace is created.

334
00:19:38,925 --> 00:19:39,615
That's good.

335
00:19:40,115 --> 00:19:40,475
Now.

336
00:19:40,975 --> 00:19:44,605
What we need to do is so since we
executed health commands earlier,

337
00:19:44,605 --> 00:19:49,465
so as part of the health command,
it has also created the config maps.

338
00:19:49,965 --> 00:19:53,475
So we already discussed like
what the config map does.

339
00:19:53,475 --> 00:19:56,135
So we have fluent bit
configuration stored there.

340
00:19:56,585 --> 00:19:59,090
So we need to update
fluent bid configuration.

341
00:19:59,740 --> 00:20:05,330
So that we are just telling fluent bit
what to output and where it has to go.

342
00:20:05,480 --> 00:20:08,390
So I'll just quickly show
you what needs to be done.

343
00:20:09,080 --> 00:20:11,510
So here I'll just execute this command.

344
00:20:12,350 --> 00:20:16,630
Fluent added cm config map, fluent
bit hyphen fluent bit namespace.

345
00:20:17,050 --> 00:20:19,000
So now we can see

346
00:20:19,500 --> 00:20:21,530
it has created.

347
00:20:22,030 --> 00:20:27,070
I just click issue here since
it does, I'll put it in not pad.

348
00:20:27,570 --> 00:20:30,180
I just see how we can do it here now.

349
00:20:30,480 --> 00:20:30,700
In

350
00:20:31,000 --> 00:20:33,130
so we can see the the fluent bit lock.

351
00:20:33,460 --> 00:20:34,360
This is the.

352
00:20:34,610 --> 00:20:37,280
The fluent bit configuration,
config map configuration here.

353
00:20:37,700 --> 00:20:39,800
So this is what we see.

354
00:20:40,280 --> 00:20:44,300
So as part of this process, I need
to so this is the configuration

355
00:20:44,300 --> 00:20:49,260
looks like so this is a config map
configuration and fluent for fluent bit.

356
00:20:49,560 --> 00:20:55,460
So I'm going to update the, the outputs
because I don't need Elasticsearch here.

357
00:20:55,850 --> 00:20:59,200
So I'm going to add outputs
for, because we are going to

358
00:20:59,200 --> 00:21:01,510
output our logs to car watch.

359
00:21:01,780 --> 00:21:06,000
So I'm going to add, add
output condition there.

360
00:21:06,780 --> 00:21:10,910
So I'm just adding my open
condition there is to make sure.

361
00:21:11,410 --> 00:21:12,400
So here we go.

362
00:21:13,000 --> 00:21:13,295
I am.

363
00:21:13,795 --> 00:21:19,390
So we can see that I have
updated the configuration as to

364
00:21:19,390 --> 00:21:20,830
output the logs to CLO locks.

365
00:21:20,830 --> 00:21:21,275
So it is going to.

366
00:21:22,150 --> 00:21:27,340
I put it in US S two, and this is going
to the log drop name and log stream.

367
00:21:27,340 --> 00:21:32,170
Name prefix is going to be from and
auto grid to true if it is not created.

368
00:21:33,000 --> 00:21:33,660
So that is good.

369
00:21:34,160 --> 00:21:34,985
Just check again.

370
00:21:35,285 --> 00:21:36,245
How it is looking.

371
00:21:36,745 --> 00:21:39,835
So my output is good.

372
00:21:40,705 --> 00:21:41,455
That's good.

373
00:21:41,955 --> 00:21:42,585
So I'm good.

374
00:21:42,825 --> 00:21:44,055
So I just checked it quickly.

375
00:21:44,275 --> 00:21:45,985
Now I'm export of the next step.

376
00:21:46,135 --> 00:21:48,785
I'm going to I'm going to deploy.

377
00:21:49,395 --> 00:21:49,905
My,

378
00:21:50,405 --> 00:21:54,855
so as part of the next step we will
be restarting the dam set for the

379
00:21:54,855 --> 00:21:57,535
fluent bit to check to check on it.

380
00:21:58,025 --> 00:21:59,220
First we need to restart.

381
00:21:59,220 --> 00:22:03,015
First we have to, we are restarting
because we updated the confi map.

382
00:22:03,365 --> 00:22:07,735
So it is important to restart it so
that it gets the updated configuration.

383
00:22:08,455 --> 00:22:13,825
Now, after I restarted the po I'm going
to check the pods associated with that

384
00:22:14,325 --> 00:22:16,875
and making sure if they're created it,

385
00:22:17,375 --> 00:22:17,725
sorry.

386
00:22:18,225 --> 00:22:19,005
I am checking.

387
00:22:19,505 --> 00:22:20,525
Yes, there are.

388
00:22:20,525 --> 00:22:22,415
The new bots are created.

389
00:22:22,415 --> 00:22:24,615
Now I'm going to check the logs for that

390
00:22:25,115 --> 00:22:25,925
just to make sure.

391
00:22:26,175 --> 00:22:29,145
That it is outputting, so
making sure there are no errors.

392
00:22:29,805 --> 00:22:31,005
So I think it is good.

393
00:22:31,305 --> 00:22:36,985
So we can see that log streaming
is creating and everything

394
00:22:36,985 --> 00:22:38,150
is looking good and no error.

395
00:22:38,395 --> 00:22:43,245
So it is outputting it to the
appropriate, cloud wash law group.

396
00:22:43,635 --> 00:22:44,805
And it is creating the log stream.

397
00:22:44,805 --> 00:22:45,555
So that's great.

398
00:22:46,215 --> 00:22:50,885
So as, so I'm going to click the screen,
and now as part of the next step,

399
00:22:50,975 --> 00:22:59,035
I'm going to I'm going to deploy the
AIX server as a sample application.

400
00:22:59,535 --> 00:23:04,035
So I'm to execute the command
for that, and I already have the,

401
00:23:04,335 --> 00:23:05,955
an server configuration here.

402
00:23:06,555 --> 00:23:08,115
Which I'm going to show here.

403
00:23:08,175 --> 00:23:14,725
That is a log server if I show here,
so that is a log server and it is ionix

404
00:23:14,845 --> 00:23:17,095
type, like match levels app ionix.

405
00:23:17,095 --> 00:23:22,625
So I'm going to deploy this so
that we can forward the logs and

406
00:23:22,625 --> 00:23:23,645
then we can follow the locks.

407
00:23:23,645 --> 00:23:25,925
So I'm going to apply this.

408
00:23:26,425 --> 00:23:27,250
So that's great.

409
00:23:27,760 --> 00:23:31,880
So we, and we have a exposed server
service for log server as well.

410
00:23:32,180 --> 00:23:33,140
So that is great.

411
00:23:33,940 --> 00:23:38,770
Now as part of the next step, we
will open another terminal to do

412
00:23:38,800 --> 00:23:41,520
port to do port forwarding the locks.

413
00:23:42,030 --> 00:23:43,650
So I'll be executing the command for that.

414
00:23:44,150 --> 00:23:45,920
I am executing the command here.

415
00:23:46,730 --> 00:23:52,950
So here I'm doing a port port port
forwarding for this hour to port 84.

416
00:23:53,450 --> 00:23:55,420
So this is done.

417
00:23:55,570 --> 00:23:58,800
It is forwarding other
logs from 80, 80 to 80.

418
00:23:59,100 --> 00:23:59,820
So that's good.

419
00:24:00,720 --> 00:24:02,195
Now we'll do the next step.

420
00:24:02,695 --> 00:24:06,885
Now we'll open one more
terminal to check the logs.

421
00:24:06,945 --> 00:24:10,875
So I've done, I create another
terminal here and I am executing

422
00:24:10,875 --> 00:24:12,465
a command to check the logs.

423
00:24:12,855 --> 00:24:19,135
So on this terminal, we will be checking
the locks whenever we make a request.

424
00:24:19,885 --> 00:24:20,815
So that is good.

425
00:24:21,235 --> 00:24:22,825
Now we'll go to the next step.

426
00:24:23,325 --> 00:24:26,850
And as part of the next step, we
are going to make a call request.

427
00:24:28,025 --> 00:24:32,165
To code 80 80 and we will see if
if it is able to, follow the locks.

428
00:24:32,665 --> 00:24:35,005
So I'll be executing the command for that.

429
00:24:35,505 --> 00:24:37,245
So now I'm executing the command.

430
00:24:37,915 --> 00:24:42,715
So for that, I opened another vice
gentleman and I'm making a request.

431
00:24:43,495 --> 00:24:47,085
So here we can see that,
this is welcome to Ionix.

432
00:24:47,085 --> 00:24:52,155
So we are able to hit Endpoint
local Host 80 80, and we can see

433
00:24:52,155 --> 00:24:54,635
here and it is forwarding the logs.

434
00:24:54,935 --> 00:24:55,835
So that is great.

435
00:24:56,335 --> 00:25:02,565
Now we will go to CloudWatch and
see if it is forwarding the locks

436
00:25:02,565 --> 00:25:04,775
there or not to check on that.

437
00:25:05,275 --> 00:25:07,435
So I will be showing you
the Clovis Locks now.

438
00:25:07,935 --> 00:25:10,105
Now we can see that CLA logs here.

439
00:25:10,735 --> 00:25:14,835
Here we can see in US West two, there
is a output location we mentioned.

440
00:25:15,135 --> 00:25:18,315
So it is creating the log
group EKS application logs.

441
00:25:18,375 --> 00:25:23,525
I think that is the log group name we
gave and is created that with that name.

442
00:25:24,015 --> 00:25:25,985
And we specified the log stream.

443
00:25:25,985 --> 00:25:26,795
So it was from.

444
00:25:27,770 --> 00:25:30,650
And we can see that it is
outputting the logs here.

445
00:25:31,040 --> 00:25:36,900
So today it is 27th in
UTC time, so 27th May.

446
00:25:36,990 --> 00:25:42,240
So it is outputting the logs here and
we can see that we see all the log logs

447
00:25:42,240 --> 00:25:44,460
here, coming here, so that it's good.

448
00:25:44,460 --> 00:25:47,820
So now, whatever, whenever we
are making the call request,

449
00:25:48,480 --> 00:25:49,650
it is outputting the log here.

450
00:25:50,640 --> 00:25:53,430
So that is a great great success here.

451
00:25:53,970 --> 00:25:59,400
As part of the next step what we can
do is we can see these CloudWatch

452
00:25:59,400 --> 00:26:01,020
logs in a cloud rush dashboard.

453
00:26:01,980 --> 00:26:05,180
So I have I'll walk you
through the process as well.

454
00:26:05,660 --> 00:26:09,950
So here we go to the CloudWatch
dashboard, and I have already created

455
00:26:10,430 --> 00:26:13,670
CloudWatch Dashboard as part of this demo.

456
00:26:13,670 --> 00:26:16,190
So ES Locks is a dashboard name.

457
00:26:16,580 --> 00:26:19,760
So if I click on it, it is going to.

458
00:26:20,225 --> 00:26:22,065
Show us the logs.

459
00:26:22,115 --> 00:26:24,985
We can see so it is, we are
streaming the log for this

460
00:26:24,985 --> 00:26:26,765
log web name application logs.

461
00:26:26,765 --> 00:26:31,865
So this is the log web name we gave
and it is showing the logs, other,

462
00:26:31,865 --> 00:26:34,535
the logs coming to the CloudWatch.

463
00:26:34,595 --> 00:26:41,165
It is just display it here so that we can
make the meaningful information out of it.

464
00:26:41,810 --> 00:26:47,390
So similarly, like it is just one, but
we can have multiple filters and multiple

465
00:26:47,390 --> 00:26:51,680
log groups we can create and so that
we can output the data accordingly.

466
00:26:52,310 --> 00:26:58,190
And we can also customize this logs
based on the different metrics type,

467
00:26:58,190 --> 00:27:03,360
for example, CPO, memory desk like
several parameters are there so we

468
00:27:03,360 --> 00:27:06,040
can create different mattresses.

469
00:27:06,475 --> 00:27:10,895
And we can output to the different
logs and we can display it

470
00:27:10,895 --> 00:27:14,824
accordingly so that we can make the
meaningful information out of it.

471
00:27:15,364 --> 00:27:20,064
So we can see, for example, like
we have, we are at the different

472
00:27:20,064 --> 00:27:24,115
point of time, we have a different
type of logs like different and

473
00:27:24,115 --> 00:27:26,004
like number of requests hitting.

474
00:27:26,485 --> 00:27:30,535
So that is the information we can
make out of it as part of this demo.

475
00:27:31,035 --> 00:27:32,869
So now this is the end of the demo.

476
00:27:33,369 --> 00:27:35,584
Now we'll talk about
the real world examples.

477
00:27:36,134 --> 00:27:38,804
So let's look at them one by one.

478
00:27:39,164 --> 00:27:41,844
So first, now we'll talk
about the real world examples.

479
00:27:42,825 --> 00:27:45,530
So let's look at them one by one.

480
00:27:45,950 --> 00:27:47,990
So first one we will talk about.

481
00:27:48,770 --> 00:27:49,790
Is the FinTech client.

482
00:27:50,270 --> 00:27:54,840
So FinTech client needed a centralized
logging system to, so that the challenge

483
00:27:54,840 --> 00:28:00,420
was to there were microservices running
in a e case environment and the bank

484
00:28:00,420 --> 00:28:03,360
needed a, like a log aggregation solution.

485
00:28:03,610 --> 00:28:05,319
To comply with the financial audits.

486
00:28:05,765 --> 00:28:10,055
And also audit trails and also,
and so that they can access the

487
00:28:10,055 --> 00:28:11,825
log, access these logs easily.

488
00:28:12,245 --> 00:28:16,545
So the solution was given in
terms of fluent bit, can be

489
00:28:16,575 --> 00:28:17,625
deployed as a diamond set.

490
00:28:18,125 --> 00:28:23,035
On e case notes and we'll configure the
ENT bit to to tail the container logs

491
00:28:23,320 --> 00:28:26,260
and them with the Kubernetes metadata.

492
00:28:26,650 --> 00:28:29,840
And then forwarding all the logs
to to Amazon CloudWatch logs.

493
00:28:30,110 --> 00:28:33,580
So this was a solution given
and the result was amazing.

494
00:28:33,790 --> 00:28:38,380
So the, the bank got the centralized
logging system and it got access

495
00:28:38,380 --> 00:28:41,980
to, to all the logs, which helped
them to improve troubleshooting.

496
00:28:42,010 --> 00:28:44,140
And it increased the
efficiency for the bank.

497
00:28:44,480 --> 00:28:46,460
And they got the operational
visibility as well.

498
00:28:46,865 --> 00:28:49,775
And they were getting automated
alerts for for everything.

499
00:28:49,925 --> 00:28:52,505
And it gave them no fast,
faster incident response.

500
00:28:52,805 --> 00:28:54,785
So that was a success story for the bank.

501
00:28:55,235 --> 00:28:59,665
So the second one second one use,
second use case is about, steaming

502
00:28:59,665 --> 00:29:01,645
logs to Amazon Open Search company.

503
00:29:01,855 --> 00:29:04,765
So there was a SaaS monitoring
provider the customer.

504
00:29:04,975 --> 00:29:10,185
So the company needed like a scalable
low latency solution to, to index the

505
00:29:10,185 --> 00:29:14,815
logs from the Kubernetes environment
and so that they can basically allow,

506
00:29:15,315 --> 00:29:17,060
so that they can query the locks.

507
00:29:17,385 --> 00:29:18,345
In, in real time.

508
00:29:18,435 --> 00:29:20,325
That was the main challenge they had.

509
00:29:20,715 --> 00:29:24,505
So the solution was like they use
fluent bit as a to collect the logs

510
00:29:24,895 --> 00:29:28,585
and then they will increase the, all
the logs with the Kubernetes metadata.

511
00:29:28,945 --> 00:29:32,965
And then as part of the streaming, like
outputting the logs, they output the logs

512
00:29:32,965 --> 00:29:38,075
to Amazon open set service and then build
the Kibana dashboards on top of it so that

513
00:29:38,075 --> 00:29:40,825
they can query these logs in real time.

514
00:29:41,325 --> 00:29:44,215
So the result the output was amazing.

515
00:29:44,255 --> 00:29:48,905
They were able to get near realtime
log availability and which improved the

516
00:29:48,905 --> 00:29:50,525
monitoring and troubleshooting for them.

517
00:29:50,925 --> 00:29:54,995
And also customer gained like in
like very good realtime experience

518
00:29:55,305 --> 00:29:57,225
and and the analytics capabilities.

519
00:29:57,565 --> 00:30:01,295
And also the operational overhead
was reduced with this process.

520
00:30:01,775 --> 00:30:06,005
The third use case is about about
online, online gaming platform client.

521
00:30:06,395 --> 00:30:09,895
So they were having they wanted to forward
the logs to a third party service company.

522
00:30:10,135 --> 00:30:13,125
So they were using Datadog
service initially to route

523
00:30:13,125 --> 00:30:14,590
the logs from ES to Datadog.

524
00:30:15,075 --> 00:30:17,265
But they wanted to make it more efficient.

525
00:30:17,635 --> 00:30:22,945
So the, and they wanted to find a
way so that e case container logs can

526
00:30:22,945 --> 00:30:25,625
be can be sent to Datadog without.

527
00:30:25,970 --> 00:30:27,500
Impacting the cluster performance.

528
00:30:27,500 --> 00:30:28,580
That was the main challenge.

529
00:30:28,980 --> 00:30:33,540
So the solution given was like they
deployed the fluent bit as a with the

530
00:30:33,540 --> 00:30:39,210
SGTP plugin which was configured for
data for Datadog API, and then filters

531
00:30:39,210 --> 00:30:41,820
for applied to reduce ansl log volume.

532
00:30:42,210 --> 00:30:43,650
And the production.

533
00:30:43,980 --> 00:30:48,140
And also they use Kubernetes metadata
to, in to to enhance the locks.

534
00:30:48,540 --> 00:30:52,600
So the result was they use the log
ingestion cost through the selective

535
00:30:52,600 --> 00:30:57,460
filtering, and then also it helped them
to improve the debugging experience.

536
00:30:57,730 --> 00:30:59,470
So that was a very good benefit they got.

537
00:30:59,770 --> 00:31:03,490
And also they got the seamless
integration with this, with the Datadog.

538
00:31:03,790 --> 00:31:05,260
So that was a very good.

539
00:31:05,575 --> 00:31:06,565
Success story for them.

540
00:31:07,255 --> 00:31:11,495
Now, the fourth one is about is a
success story of a healthcare provider.

541
00:31:11,855 --> 00:31:17,445
So this is a real world example where they
were having the challenge was like for

542
00:31:17,445 --> 00:31:21,230
healthcare client they have to maintain
hipaa compliance, which is required

543
00:31:21,260 --> 00:31:26,780
for collecting and retention, retaining
the logs from the ized workloads.

544
00:31:27,050 --> 00:31:28,220
So that was the.

545
00:31:28,720 --> 00:31:29,440
A challenge.

546
00:31:29,560 --> 00:31:32,930
So the solution was to, they
deployed fluent bit as a diamond set.

547
00:31:33,365 --> 00:31:34,625
To collect the container logs.

548
00:31:34,955 --> 00:31:38,915
And then and also then they needed
Linux audit logs as well via

549
00:31:38,915 --> 00:31:43,475
system input plugin, and then logs
were forwarded to a dedicated S3

550
00:31:43,475 --> 00:31:44,945
bucket for the long-term retention.

551
00:31:45,305 --> 00:31:50,585
And also security team also used
like Splunk to ingest from S3

552
00:31:50,585 --> 00:31:52,565
for foreign forensic analysis.

553
00:31:53,080 --> 00:31:58,110
So this was the solution, given the
benefit of this was like they ensure

554
00:31:58,410 --> 00:32:02,430
compliance with regulatory requirements,
the HPA requirements they had,

555
00:32:02,430 --> 00:32:03,830
they were able to comply with them.

556
00:32:03,950 --> 00:32:08,850
And also it was a very cost effective
and very effective for investing the,

557
00:32:08,850 --> 00:32:11,460
investigating the incidents also.

558
00:32:11,640 --> 00:32:13,050
So that worked really.

559
00:32:13,790 --> 00:32:14,240
For them.

560
00:32:14,600 --> 00:32:18,540
So the fifth one is multi-tenant
logging in a, in in a company.

561
00:32:18,870 --> 00:32:21,655
So they had they had a
they had a multi-tenant

562
00:32:22,205 --> 00:32:23,795
multi-tenant Kubernetes cluster.

563
00:32:24,095 --> 00:32:29,045
So they had multiple name spaces and
parts were running in multiple namespace.

564
00:32:29,045 --> 00:32:34,155
So they wanted to, they wanted to ship
the locks in an isolated way and making

565
00:32:34,155 --> 00:32:38,495
sure they are shipped in a secure they
shipped in a secure manner, part tenant.

566
00:32:38,975 --> 00:32:43,815
So the solution was fluent bit was
deployed as a demonstr with the filters

567
00:32:44,115 --> 00:32:49,105
to add tenant labels based on namespace
and then routing rules for configured.

568
00:32:49,475 --> 00:32:52,275
To send the logs to a separate
na to separate Grafana low key

569
00:32:52,275 --> 00:32:56,615
tenants using the lock output plugin
with the tenant specific URLs.

570
00:32:56,945 --> 00:32:57,695
That was done.

571
00:32:58,025 --> 00:33:00,005
And then the result was amazing.

572
00:33:00,054 --> 00:33:04,875
They were able to get the multi-tenant
lock isolation without the need

573
00:33:04,875 --> 00:33:07,304
for separating the cluster.

574
00:33:07,615 --> 00:33:11,455
And then and it simplified
the platform operations with

575
00:33:11,455 --> 00:33:12,595
centralized logging system.

576
00:33:13,115 --> 00:33:14,825
And the centralized logging
management for them.

577
00:33:15,455 --> 00:33:19,595
So these were the real world
examples you wanted to discuss here.

578
00:33:20,075 --> 00:33:21,945
And now we'll talk about the references.

579
00:33:21,945 --> 00:33:26,205
So have referred to AWS official
documentation to prepare this.

580
00:33:26,535 --> 00:33:30,404
And and thank you very much
for being part of this session.

581
00:33:30,855 --> 00:33:34,185
And and please let me know if you
need, if you have any questions.

582
00:33:34,455 --> 00:33:34,725
Thank you.

