1
00:00:00,500 --> 00:00:01,219
Hi everyone.

2
00:00:01,270 --> 00:00:02,230
Thank you for joining.

3
00:00:02,260 --> 00:00:04,570
My talk today is on ML Ops at scale.

4
00:00:05,199 --> 00:00:08,590
Before we dive into ML ops let
me quickly introduce myself.

5
00:00:09,220 --> 00:00:12,100
I've been in the data analytics
and machine learning space for

6
00:00:12,100 --> 00:00:16,720
over 15 years, building large scale
platforms, predictive models, and

7
00:00:16,720 --> 00:00:19,090
intelligent system across industries.

8
00:00:19,780 --> 00:00:23,514
Over the years, I've seen data evolve
from simple dashboard to mission critical.

9
00:00:24,154 --> 00:00:27,034
AI systems that influence
billions of decisions.

10
00:00:27,084 --> 00:00:31,974
Every single day in my career, I have
led teams that designed end-to-end ML

11
00:00:31,974 --> 00:00:37,855
pipelines, architected the petabyte scale
data warehouse, and deployed models,

12
00:00:37,855 --> 00:00:40,224
powering finance, hr, and other use cases.

13
00:00:40,724 --> 00:00:46,245
And more recently, my focus has expanded
to world of agent AI building autonomous

14
00:00:46,245 --> 00:00:51,285
AI agents that don't just predict but
can act, collaborate, and make decision

15
00:00:51,285 --> 00:00:54,154
with other systems across all of this.

16
00:00:54,154 --> 00:00:58,545
One lesson that has stayed constant
is building a model is easy.

17
00:00:58,855 --> 00:01:02,095
But operationalizing it at
scale is the real challenge.

18
00:01:02,575 --> 00:01:07,925
That's why ML ops and and now ML ops
and agent DKI ops are so critical.

19
00:01:08,615 --> 00:01:13,285
They ensure models are reliable,
auditable, and scalable turning

20
00:01:13,285 --> 00:01:16,165
experimental prototypes into
real business solutions.

21
00:01:16,915 --> 00:01:18,685
That's why I'm passionate about ML Ops.

22
00:01:18,685 --> 00:01:23,365
This is the foundation that allows
enterprise to move from experimentation

23
00:01:23,645 --> 00:01:27,065
to impact safely, reliably, and at scale.

24
00:01:27,565 --> 00:01:29,125
Let me move on to the next slide.

25
00:01:29,225 --> 00:01:31,235
I wanna make sure that
I give some disclaimer.

26
00:01:31,285 --> 00:01:35,605
All the views and opinions shared in
this presentation are my own perspective.

27
00:01:35,975 --> 00:01:40,505
They don't represent or reflect
the views of Amazon or AWS I'm

28
00:01:40,505 --> 00:01:45,045
just sharing this purely out of
my passion for ML ops and ai.

29
00:01:45,545 --> 00:01:45,900
All right.

30
00:01:46,400 --> 00:01:48,080
I wanna start with a simple idea.

31
00:01:48,785 --> 00:01:52,295
A model in a Jupyter Notebook
is an experiment, a model in

32
00:01:52,295 --> 00:01:56,165
production, monitored and gone
at scale is a business solution.

33
00:01:56,675 --> 00:02:00,965
Think about how critical this
is at companies like Amazon,

34
00:02:00,965 --> 00:02:02,435
Netflix, or major banks.

35
00:02:02,435 --> 00:02:06,905
Machine learning system drive billions
of decisions, recommendations fraud

36
00:02:06,905 --> 00:02:11,055
detection, supply chain optimization
or even workforce planning.

37
00:02:11,925 --> 00:02:15,380
If these model fail the business
feels it's very instantly.

38
00:02:16,045 --> 00:02:20,094
Now we are also entering the
era of large language models.

39
00:02:20,154 --> 00:02:22,954
They're powerful, but
they're also unpredictable.

40
00:02:22,984 --> 00:02:27,004
ML ops is what keeps the system
reliable and trustworthy.

41
00:02:27,634 --> 00:02:32,324
My goal today is to show how
automation CICD, monitoring and

42
00:02:32,324 --> 00:02:37,124
governance ensure just that let's
talk a little bit on the challenges.

43
00:02:37,624 --> 00:02:41,134
Traditional ML project often fail
because pipelines are brittle.

44
00:02:41,734 --> 00:02:45,664
Maybe a column name changes in
the source data, and suddenly

45
00:02:45,664 --> 00:02:47,044
the feature store breaks.

46
00:02:47,384 --> 00:02:49,784
Deployments are often manual.

47
00:02:50,124 --> 00:02:55,444
Someone copies of model file pushes
it into production and then sprays.

48
00:02:55,449 --> 00:02:55,699
It works.

49
00:02:56,199 --> 00:02:59,559
And then there is monitoring,
or rather the lack of it.

50
00:03:00,129 --> 00:03:04,319
Many enterprise have models running
in production right now that haven't

51
00:03:04,319 --> 00:03:06,839
been retrained in months or even years.

52
00:03:07,499 --> 00:03:08,459
That's a big problem.

53
00:03:08,459 --> 00:03:13,599
Let me give you an example, A credit
risk model at a financial firm that was

54
00:03:13,599 --> 00:03:19,479
trained on pre 2020 data when COVID hit
consumer behavior, changed overnight,

55
00:03:19,539 --> 00:03:23,009
job loss, losses loan defaults.

56
00:03:23,364 --> 00:03:25,014
Government relief checks and so on.

57
00:03:25,734 --> 00:03:30,654
But the model was sent, monitored,
and for months, it misclassified risk

58
00:03:30,654 --> 00:03:32,814
leading to massive financial losses.

59
00:03:33,444 --> 00:03:37,604
Now brings in LLM, or now
let's bring in the LLM.

60
00:03:38,134 --> 00:03:43,754
They add even bigger risk cost explosion,
hallucination, or compliance gaps.

61
00:03:43,964 --> 00:03:50,904
So if ML ops was important before in LLM
world it's absolutely mission critical.

62
00:03:51,404 --> 00:03:52,059
All right.

63
00:03:52,159 --> 00:03:54,439
Let's begin with what is ML ops?

64
00:03:54,799 --> 00:03:59,209
ML Ops is about taking the best of dev
ops, continuous integration, continuous

65
00:03:59,209 --> 00:04:03,039
delivery, infrastructure automation,
and applying it to machine learning.

66
00:04:03,489 --> 00:04:04,789
But there's one more.

67
00:04:04,899 --> 00:04:11,839
DevOp DevOps deals with code while ML
ops deals with code data and models.

68
00:04:12,339 --> 00:04:13,119
Let's break it down.

69
00:04:13,329 --> 00:04:16,139
Data pipelines feed
features into the model.

70
00:04:16,619 --> 00:04:19,289
The model itself is trained,
validated, and washed.

71
00:04:19,959 --> 00:04:25,919
Deployment must be automate automated
across environments and monitoring keeps

72
00:04:25,919 --> 00:04:28,659
track of the model performance over time.

73
00:04:29,319 --> 00:04:31,389
And now with large language models.

74
00:04:31,389 --> 00:04:36,249
We need to extend this thinking
to what some call LLM Ops.

75
00:04:36,309 --> 00:04:41,949
Where we manage not just training and
deployment, but also prompt pipelines,

76
00:04:41,949 --> 00:04:44,019
retrieval, augmentation, and fine tuning.

77
00:04:44,769 --> 00:04:46,419
The principles remains the same.

78
00:04:46,419 --> 00:04:48,459
Automate, monitor and govern.

79
00:04:48,959 --> 00:04:52,924
I wanna talk about ML ops in
the large language model era.

80
00:04:53,254 --> 00:04:56,884
As we shift now into the
age of large language model.

81
00:04:57,194 --> 00:04:58,244
Some people ask.

82
00:04:58,589 --> 00:05:00,449
Do we still need ML labs?

83
00:05:00,509 --> 00:05:05,819
And I think the answer is, and
in fact it's, this is a big yes.

84
00:05:06,809 --> 00:05:09,419
If anything, we need it even more.

85
00:05:10,319 --> 00:05:10,859
Why?

86
00:05:10,909 --> 00:05:15,469
Because the complexity of operating
large language model is even higher

87
00:05:15,469 --> 00:05:16,909
than classical machine learning.

88
00:05:17,539 --> 00:05:20,599
Just like we manage feature
pipelines for machine learning

89
00:05:20,869 --> 00:05:22,459
in large language, we need.

90
00:05:22,829 --> 00:05:26,309
Are we now manage prompt
pipelines prompts evolve.

91
00:05:26,309 --> 00:05:29,979
Chains of prompts need testing
and they must be version control.

92
00:05:30,429 --> 00:05:33,369
Fine tuning and large language
model is very expensive.

93
00:05:33,419 --> 00:05:37,919
Without automated evaluation
pipeline, you risk wasting millions

94
00:05:38,189 --> 00:05:39,839
retraining models unnecessarily.

95
00:05:40,339 --> 00:05:42,709
Monitoring also becomes bo broader.

96
00:05:43,229 --> 00:05:46,069
We don't just watch accuracy or latency.

97
00:05:46,069 --> 00:05:50,609
We track hallucination,
toxicity, bias and grounding.

98
00:05:51,119 --> 00:05:55,029
For example, a large language
model chat bot in healthcare starts

99
00:05:55,029 --> 00:05:59,609
producing producing advice, not
grounded in medical solutions.

100
00:05:59,639 --> 00:06:02,659
We need guardrails to
catch that instantly.

101
00:06:03,379 --> 00:06:04,699
Finally, governance.

102
00:06:04,759 --> 00:06:06,769
With machine learning, we tracked.

103
00:06:07,244 --> 00:06:12,794
Data sets and model versions
with LLM, we must also track data

104
00:06:13,184 --> 00:06:15,974
compliance and human feedback loops.

105
00:06:16,724 --> 00:06:21,794
ML ops gives us the foundation to
make lms reliable, responsible,

106
00:06:21,794 --> 00:06:23,894
and scalable in enterprise context.

107
00:06:24,134 --> 00:06:28,954
Without it large language model adoption
will stall because enterprise won't

108
00:06:28,954 --> 00:06:31,384
trust them in mission critical use cases.

109
00:06:31,884 --> 00:06:35,744
Let's move on to ML lops lifecycle.

110
00:06:36,244 --> 00:06:39,664
The ML of lifecycle looks like
a loop not a straight line.

111
00:06:39,934 --> 00:06:44,434
It begins with data ingesting in
pre-processing, validating data,

112
00:06:44,464 --> 00:06:48,364
detecting schema changes, applying
feature engineering, and so on.

113
00:06:49,249 --> 00:06:51,709
Then comes model training and fine tuning.

114
00:06:51,759 --> 00:06:57,639
Often tracked in ML flow or SageMaker
with hyper parameters and metrics.

115
00:06:58,009 --> 00:06:59,149
Next is validation.

116
00:06:59,149 --> 00:07:02,269
This isn't just accuracy,
it's fairness, robustness, and

117
00:07:02,319 --> 00:07:05,199
interpretability then comes deployment.

118
00:07:05,229 --> 00:07:11,169
This could mean exposing an
API containerizing in model or

119
00:07:11,169 --> 00:07:12,879
serving it via a feature store.

120
00:07:13,584 --> 00:07:17,364
Finally monitoring and feedback
loop closes the entire cycle.

121
00:07:17,644 --> 00:07:20,614
We monitor accuracy, drift, and latency.

122
00:07:20,974 --> 00:07:25,679
If something falls out of the bound
retraining triggers automatically.

123
00:07:26,609 --> 00:07:31,589
A real world example a retail store
recommendation models retrain daily

124
00:07:31,589 --> 00:07:34,439
with new browsing and purchase data.

125
00:07:34,859 --> 00:07:38,719
They're validated against test
sets, rolled out with shadow

126
00:07:38,719 --> 00:07:43,319
deployments and monitored for
drift for large language models.

127
00:07:43,319 --> 00:07:47,879
This lifecycle also includes prompt
chain testing, ground checks, and

128
00:07:47,879 --> 00:07:52,189
human in the loop review to ensure
the output remains useful and safe.

129
00:07:52,689 --> 00:07:53,019
All right.

130
00:07:53,229 --> 00:07:57,069
Automation I think automation is the
first pillar without it, ma a machine

131
00:07:57,069 --> 00:08:01,479
learning model becomes artisanal
slow and consistent and fragile.

132
00:08:02,049 --> 00:08:04,719
Take a retail company
running pricing optimization.

133
00:08:04,719 --> 00:08:09,769
If a data engineer has to manually rerun
the pre-processing scripts, every time

134
00:08:10,069 --> 00:08:14,869
new sales data arrives, they'll never keep
up with the changing market condition.

135
00:08:15,844 --> 00:08:22,044
By automating the pipelines with
airflow or QB flow retraining happens

136
00:08:22,044 --> 00:08:25,104
daily and reproducibly for LLMs.

137
00:08:25,104 --> 00:08:28,424
Automatic automation
is equally cr crucial.

138
00:08:28,544 --> 00:08:33,404
As I've mentioned before, imagine a chat
bot trained on enterprise documentation.

139
00:08:33,699 --> 00:08:39,609
Automation ensures that when new docs are
published the retrieval index is refreshed

140
00:08:39,929 --> 00:08:42,089
prompts are validated and evaluation.

141
00:08:42,469 --> 00:08:43,339
Test are triggered.

142
00:08:43,969 --> 00:08:46,309
No one has to manually intervene.

143
00:08:47,209 --> 00:08:51,199
Automation ensures that models are
not one off science experiments.

144
00:08:51,199 --> 00:08:53,119
They le their living systems.

145
00:08:53,619 --> 00:08:58,279
CICD for ML and large language models.

146
00:08:58,279 --> 00:09:01,999
CICD bring agility and
discipline in machine learning.

147
00:09:01,999 --> 00:09:06,059
We work in not just code,
but data sets and models.

148
00:09:06,629 --> 00:09:11,319
Tools like DVC or ML
Flow make this possible.

149
00:09:11,919 --> 00:09:15,879
Every new data set goes through
regression test, bias test,

150
00:09:15,879 --> 00:09:17,165
and unit test for features.

151
00:09:17,665 --> 00:09:22,714
Example let's say at Netflix, thousands of
models are deployed using CICD pipelines.

152
00:09:23,014 --> 00:09:28,174
If one experiment fails, it can be rolled
back instantly for large language models.

153
00:09:28,174 --> 00:09:32,284
CICD includes prompt worsening, so
you have a customer service chat bot.

154
00:09:32,344 --> 00:09:36,375
Each new prompts template gets
tested against historical customer

155
00:09:36,375 --> 00:09:42,074
interaction, shadow deployment let
you compare the new version against

156
00:09:42,074 --> 00:09:44,379
the old before fully rolling it out.

157
00:09:45,239 --> 00:09:48,839
And just like microservices,
rollback strategies are essential

158
00:09:49,199 --> 00:09:54,139
because the cost of a bad model or
prompt in production can be huge.

159
00:09:54,639 --> 00:09:59,339
Moving on to governance and
responsible ai governance is

160
00:09:59,339 --> 00:10:01,139
all about trust and compliance.

161
00:10:01,169 --> 00:10:04,529
Enterprise needs to answer
which model made this decision?

162
00:10:04,889 --> 00:10:06,569
What data was it trained on?

163
00:10:06,569 --> 00:10:08,579
Can we prove it wasn't biased?

164
00:10:08,909 --> 00:10:14,029
And financials services regulators may
demand proof of why a loan was denied.

165
00:10:14,509 --> 00:10:18,859
With ML ops governance, you
can show the dataset, the model

166
00:10:18,859 --> 00:10:20,149
version, even the code used.

167
00:10:20,969 --> 00:10:22,050
LLMs take it.

168
00:10:22,079 --> 00:10:23,939
The, take this a little bit further.

169
00:10:24,000 --> 00:10:30,580
We must track data where training or fine
tuning data came from red teaming results.

170
00:10:30,580 --> 00:10:35,600
What failure modes were tested
and regulatory compliances

171
00:10:35,600 --> 00:10:41,245
like GDPR or eu AI Act are very
explicit about AI transparency.

172
00:10:41,905 --> 00:10:44,485
Responsible AI is not optional.

173
00:10:44,535 --> 00:10:49,335
It's the price of doing business
with with ai model and LLM training.

174
00:10:49,335 --> 00:10:54,015
So monitoring is where
the battle is won or lost.

175
00:10:54,435 --> 00:10:59,655
For classical ml, we monitor accuracy,
latency, drift, and infrastructure cost.

176
00:10:59,985 --> 00:11:05,485
For example, a fraud detection at
a bank looked great at launch, but

177
00:11:05,485 --> 00:11:07,985
started missing new fraud patterns.

178
00:11:07,985 --> 00:11:12,985
After six months, continuous monitoring
detected drift earlier early and

179
00:11:12,985 --> 00:11:15,145
triggered retraining, saving millions.

180
00:11:15,415 --> 00:11:16,655
LM introduced another.

181
00:11:17,385 --> 00:11:22,135
Monitoring challenge the hallucination
rate the toxicity grounding our

182
00:11:22,135 --> 00:11:25,165
answers tied back to real data sources.

183
00:11:25,615 --> 00:11:31,165
Some enterprise now run red teaming
in production monitoring outputs for

184
00:11:31,225 --> 00:11:35,895
unsafe responses, and automatically
flagging them for human review.

185
00:11:36,465 --> 00:11:40,695
Without monitoring, large language
model can quickly become liabilities.

186
00:11:41,195 --> 00:11:45,445
I wanna talk about, some of the best
practices in the large language model era.

187
00:11:46,345 --> 00:11:49,915
So what works well use
infrastructure as a code.

188
00:11:50,405 --> 00:11:53,135
So every environment is reproducible.

189
00:11:53,495 --> 00:11:58,835
Adopt feature stores for ML and emerging
prompt stores for LLMs, automate

190
00:11:58,835 --> 00:12:04,255
bias testing, fairness checks and
safety evaluation as part of CICD,

191
00:12:04,675 --> 00:12:06,985
build cross-functional teams because.

192
00:12:07,360 --> 00:12:13,090
Reliable AI requests, not just data
scientists, but engineers, compliance

193
00:12:13,090 --> 00:12:15,160
offices, and business leaders.

194
00:12:15,660 --> 00:12:18,620
Moving on to my final slide.

195
00:12:19,260 --> 00:12:23,820
This is just my closing and
call to action to close.

196
00:12:23,820 --> 00:12:25,500
Let me emphasize this.

197
00:12:25,500 --> 00:12:27,450
ML OPS is not outdated.

198
00:12:27,870 --> 00:12:28,890
It is evolving.

199
00:12:29,760 --> 00:12:33,990
It is the foundation that allows
enterprise to scale both classical

200
00:12:33,990 --> 00:12:39,150
and machine learning and modern
large language models responsibly.

201
00:12:39,750 --> 00:12:47,875
Without automation, CICD, monitoring
and governance models will fall or

202
00:12:48,180 --> 00:12:54,990
fail silently cost too much and expose
you to regulatory risk with them.

203
00:12:55,490 --> 00:12:59,600
AI becomes reliable,
trustworthy, and scalable.

204
00:13:00,410 --> 00:13:05,700
So whether you're deploying a
custom churn model or recommendation

205
00:13:05,700 --> 00:13:11,460
engine, or a generative AI assistant,
remember ML ops is the bridge

206
00:13:11,460 --> 00:13:13,500
between innovation and trust.

207
00:13:14,000 --> 00:13:19,710
With that, I want to thank everyone for
taking the time to listen into my talk.

208
00:13:19,760 --> 00:13:20,870
And thank you very much.

