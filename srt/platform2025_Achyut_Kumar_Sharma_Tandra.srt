1
00:00:00,500 --> 00:00:01,160
Good morning.

2
00:00:01,160 --> 00:00:02,150
Good afternoon, everyone.

3
00:00:02,420 --> 00:00:03,460
My name is Ra.

4
00:00:03,460 --> 00:00:05,260
I'm excited to be with you today.

5
00:00:05,455 --> 00:00:08,154
I'm currently working for
Amazon as a data engineer.

6
00:00:08,675 --> 00:00:08,915
I am.

7
00:00:09,880 --> 00:00:13,299
My main focus is on building
next generation of data platform.

8
00:00:13,630 --> 00:00:16,810
Today I want to talk about a topic
that quickly becoming crucial in

9
00:00:16,810 --> 00:00:20,680
our industry, how generative AI
is fundamentally cha changing the

10
00:00:20,680 --> 00:00:22,420
way we approach data integration.

11
00:00:22,720 --> 00:00:26,830
This isn't about just new tool
or incremental change, it's

12
00:00:26,830 --> 00:00:28,630
our complete paradigm shift.

13
00:00:29,080 --> 00:00:33,339
The title of my talk is Scalable,
code Free, ETL, how Generative AI

14
00:00:33,339 --> 00:00:35,140
is Redefining Data Integration.

15
00:00:35,775 --> 00:00:39,645
And it's a topic that's very
relevant to anyone involved in data

16
00:00:39,645 --> 00:00:42,375
platforms, engineering, or analytics.

17
00:00:42,645 --> 00:00:46,184
I'm looking forward to walking you
through how this technology works,

18
00:00:46,184 --> 00:00:50,595
showing you some real world results,
and discussing what it means for

19
00:00:50,595 --> 00:00:52,214
the future of platform engineering.

20
00:00:52,714 --> 00:00:56,434
So before we dive into the solution,
let's acknowledge the problems

21
00:00:56,434 --> 00:00:57,725
we are all too familiar with.

22
00:00:58,145 --> 00:01:01,714
The image of tire looking
developer says It solved.

23
00:01:02,165 --> 00:01:04,625
Organizations today face
significant hurdles with

24
00:01:04,625 --> 00:01:06,125
traditional E detailed processes.

25
00:01:06,484 --> 00:01:11,315
The first major problem is the manual
coding burden, creating a new data

26
00:01:11,315 --> 00:01:13,440
pipeline from scratch request deep.

27
00:01:13,755 --> 00:01:18,795
Technical expertise and specialized
engineering skills for each one, which

28
00:01:18,795 --> 00:01:20,865
creates a body link in the workflow.

29
00:01:20,895 --> 00:01:24,435
The number of new pipelines a
business can create is limited by

30
00:01:24,435 --> 00:01:26,025
the number of engineers available.

31
00:01:26,384 --> 00:01:28,005
This is a major friction point.

32
00:01:28,395 --> 00:01:30,645
This leads directly to the second problem.

33
00:01:30,759 --> 00:01:33,009
A high technical learning curve.

34
00:01:33,250 --> 00:01:37,210
The people who need the data most,
the data analyst and business users

35
00:01:37,419 --> 00:01:40,870
often can't get it themselves because
they lack the technical skills.

36
00:01:41,139 --> 00:01:44,440
They must rely on a small,
specialized team of engineers,

37
00:01:44,740 --> 00:01:49,869
which delays insights and slowly
it slows down the entire business.

38
00:01:50,260 --> 00:01:54,520
This dependency creates a communication
gap and significant time lag.

39
00:01:54,820 --> 00:01:56,830
And finally, there is
the operational overhead.

40
00:01:57,570 --> 00:02:00,870
A data pipeline isn't a
one on one on done project.

41
00:02:00,900 --> 00:02:05,310
It requires continuous maintenance,
troubleshooting, and optimization,

42
00:02:05,820 --> 00:02:10,710
which drains valuable resources and
time from your engineering team.

43
00:02:11,190 --> 00:02:16,260
This constant upkeep prevents teams
from focusing on innovation, so we

44
00:02:16,260 --> 00:02:21,269
are stuck in a cycle of building and
maintaining which limits our ability to

45
00:02:21,269 --> 00:02:23,490
respond to new business needs quickly.

46
00:02:23,990 --> 00:02:27,560
Generative AI is the key to
solving these challenges.

47
00:02:27,950 --> 00:02:30,980
It's a fundamental shift
moving us from a code centric

48
00:02:30,980 --> 00:02:32,660
world to a conversational one.

49
00:02:33,050 --> 00:02:37,640
Imagine telling the system in plain
English exactly what you need.

50
00:02:37,640 --> 00:02:38,355
That's the first step.

51
00:02:38,855 --> 00:02:41,615
Translating natural language to
your pipeline, you can express

52
00:02:41,615 --> 00:02:44,765
your data needs without any SQL
or transformation code required.

53
00:02:44,825 --> 00:02:49,055
This empowers a much broader group
of users to interact with data.

54
00:02:49,760 --> 00:02:52,760
The magic happens with intent recognition.

55
00:02:52,760 --> 00:02:54,890
The AI doesn't just look for keywords.

56
00:02:54,890 --> 00:02:57,470
It understands the business
intent behind your request.

57
00:02:57,830 --> 00:03:02,510
For example, if you say, show me sales
strength, the AI understands you need

58
00:03:02,510 --> 00:03:08,030
a time series analysis with specific
aggregations, not just a simple query.

59
00:03:08,090 --> 00:03:11,330
It translates the business goal into
correct technical specification.

60
00:03:11,975 --> 00:03:15,785
The system handles the entire
workflow automatically, which is

61
00:03:15,785 --> 00:03:17,675
what we call automatic execution.

62
00:03:18,005 --> 00:03:22,295
It doesn't just generate the code, it
handles the deployment and continuous

63
00:03:22,295 --> 00:03:26,945
monitoring of the pipeline without any
manual intervention from an engineer.

64
00:03:27,335 --> 00:03:32,255
This is a huge leap in efficiency
and it gets smarter with every use.

65
00:03:32,525 --> 00:03:35,795
This is the concept of
continuous learning over time.

66
00:03:35,795 --> 00:03:39,305
The system builds a knowledge
base unique to your organization,

67
00:03:39,755 --> 00:03:43,355
learning your specific data assets,
and common integration pattern.

68
00:03:43,685 --> 00:03:46,385
This makes it more efficient
and accurate over time, and

69
00:03:46,385 --> 00:03:48,455
it's a self-improving system.

70
00:03:49,060 --> 00:03:52,390
This entire paradigm shift
is about democratizing data.

71
00:03:52,630 --> 00:03:55,390
It puts data integration
capability directly in the

72
00:03:55,390 --> 00:03:57,190
hands of who need insights.

73
00:03:57,690 --> 00:04:00,930
So let's take a closer look at
what's happening behind the scenes.

74
00:04:00,960 --> 00:04:05,040
This is the architectural
breakdown of AI driven ETL system.

75
00:04:05,430 --> 00:04:08,730
It all starts with the
natural language interface.

76
00:04:09,000 --> 00:04:10,320
This is the user phrasing.

77
00:04:10,320 --> 00:04:15,690
Front end users can simply type or speak
their data request just as they would.

78
00:04:15,925 --> 00:04:17,635
Do a search engine or a chat bot.

79
00:04:18,024 --> 00:04:22,045
This is where the initial intent is
captured, and the system can even

80
00:04:22,045 --> 00:04:26,125
prove contextual assistance to help
the user formulate their request

81
00:04:26,125 --> 00:04:30,415
more clearly, the request is then
passed to the semantic password.

82
00:04:30,895 --> 00:04:32,755
This is the core intelligence layer.

83
00:04:33,145 --> 00:04:36,510
It breaks down the request
interpreting and mapping it to this.

84
00:04:37,200 --> 00:04:40,289
Specific data entities,
their relationships, and the

85
00:04:40,289 --> 00:04:42,030
transformation we have asked for.

86
00:04:42,390 --> 00:04:46,320
For example, if you ask for sales
by product, the passer knows to

87
00:04:46,320 --> 00:04:50,610
map sales to specific sales table
and product to a product table.

88
00:04:51,150 --> 00:04:54,150
It understands the relationship
between these entities and

89
00:04:54,150 --> 00:04:55,065
what you want to do with them.

90
00:04:55,925 --> 00:04:58,925
The output from the passer
goes to the execution planner.

91
00:04:58,925 --> 00:05:00,605
This component is a strategist.

92
00:05:01,115 --> 00:05:05,135
It takes the past information and
figures out the most efficient

93
00:05:05,135 --> 00:05:06,755
way to execute the pipeline.

94
00:05:07,085 --> 00:05:11,135
It considers factors like the size
of the data, available, resources,

95
00:05:11,465 --> 00:05:14,825
and your company's governance
policies to create an optimized plan.

96
00:05:15,245 --> 00:05:18,305
This step is critical for ensuring
performance and scalability.

97
00:05:19,065 --> 00:05:22,184
Finally, the runtime engine
puts the plan into action.

98
00:05:22,485 --> 00:05:28,485
It takes the optimized plan and translates
it into actual executable code for

99
00:05:28,485 --> 00:05:30,195
a variety of processing frameworks.

100
00:05:30,434 --> 00:05:35,174
Whether that says part job, a series
of SQL queries, or something else, the

101
00:05:35,174 --> 00:05:39,315
runtime engine ensures the pipeline
runs correctly and efficiently

102
00:05:39,674 --> 00:05:43,210
translating the plan into the correct
syntax for the underlying technology.

103
00:05:43,710 --> 00:05:48,690
Let's make this concept even more concrete
by walking through a practical example.

104
00:05:49,020 --> 00:05:52,560
A business user, perhaps from the
marketing team, needs a specific report.

105
00:05:53,010 --> 00:05:56,790
Instead of submitting a ticket to the
data team and waiting for weeks, they

106
00:05:56,790 --> 00:05:58,740
simply type the request into the system.

107
00:05:58,740 --> 00:06:01,170
The request is clear
plain English sentence.

108
00:06:01,500 --> 00:06:04,050
The AI understands the full
context of the request.

109
00:06:04,380 --> 00:06:08,179
It recognizes that total sales
needs a sum function that.

110
00:06:08,630 --> 00:06:12,229
Product category is a group by
class, and the top 10 customers

111
00:06:12,229 --> 00:06:14,030
require a rank or limit class.

112
00:06:14,630 --> 00:06:17,719
It also understands the time
dimension for the comparison logic.

113
00:06:18,455 --> 00:06:21,425
This is far more sophisticated
than a simple search function.

114
00:06:21,844 --> 00:06:26,675
This deep understanding allows it to
generate complete, optimized logic.

115
00:06:27,034 --> 00:06:30,844
This isn't just a simple query, it's
a full fledged operational pipeline,

116
00:06:30,844 --> 00:06:36,395
which scheduling transformation rules
and output formatting built in this.

117
00:06:36,425 --> 00:06:41,104
This system creates all the necessary code
and logic to make this request a reality.

118
00:06:41,969 --> 00:06:45,449
The final product is a production
ready workflow that is automatically

119
00:06:45,449 --> 00:06:46,739
deployed and ready to run.

120
00:06:47,039 --> 00:06:50,099
This pipeline is ready to
execute, and it's even set up with

121
00:06:50,099 --> 00:06:54,209
monitoring and notification, so
the user is informed of its status.

122
00:06:54,419 --> 00:06:59,399
The best part, the entire process takes
minutes instead of days, and the users

123
00:06:59,399 --> 00:07:01,499
never have to write a single line of code.

124
00:07:01,999 --> 00:07:06,259
To move from a cool demo to a robust
enterprise ready platform, a few

125
00:07:06,259 --> 00:07:07,759
critical, comprehensive needed.

126
00:07:08,150 --> 00:07:12,859
It's a combination of a solid
foundation, powerful processing,

127
00:07:12,889 --> 00:07:15,019
and strong operational capabilities.

128
00:07:15,469 --> 00:07:18,439
In the foundation layer, the
system must have a comprehensive

129
00:07:18,530 --> 00:07:20,239
understanding of your data landscape.

130
00:07:20,599 --> 00:07:24,229
This requires metadata discovery,
which automatically scans and

131
00:07:24,229 --> 00:07:25,789
catalogs all data sources.

132
00:07:26,139 --> 00:07:29,199
So the AI knows what's available
and how it's structured.

133
00:07:29,559 --> 00:07:33,519
We also need a strong credential
management system to securely handle

134
00:07:33,519 --> 00:07:37,894
authentication and maintain zero trust
principles, the processing layers.

135
00:07:38,869 --> 00:07:41,599
Ensure the system is
efficient and powerful.

136
00:07:42,109 --> 00:07:45,534
Intelligent catching prevents the
same work from being done over

137
00:07:45,554 --> 00:07:49,339
and over by catching results and
minimizing redundant processing.

138
00:07:49,639 --> 00:07:53,089
This is especially important
for frequently run reports.

139
00:07:53,359 --> 00:07:58,249
It also supports real-time processing with
stream based pipelines from time sensitive

140
00:07:58,249 --> 00:08:02,989
applications like fraud detection or
live dashboards on the operational layer.

141
00:08:02,989 --> 00:08:07,280
Data lineage tracking is a key component
for governance and troubleshooting.

142
00:08:07,659 --> 00:08:12,129
The system automatically creates a
trail of how data moves, which is vital

143
00:08:12,129 --> 00:08:16,809
for audits, debugging, and ensuring
data quality and API orchestration

144
00:08:16,809 --> 00:08:20,649
is what allows the system to connect
and coordinate with all the various

145
00:08:20,649 --> 00:08:24,999
systems and services in a modern
enterprise, ensuring seamless data

146
00:08:24,999 --> 00:08:27,069
movement across disparate systems.

147
00:08:27,429 --> 00:08:31,299
These components work together to
create a platform that balances

148
00:08:31,299 --> 00:08:33,039
flexibility with governance.

149
00:08:33,539 --> 00:08:37,889
Let's examine a real world case study
from a financial services company.

150
00:08:37,889 --> 00:08:39,059
This is a great example.

151
00:08:39,059 --> 00:08:41,849
Because of the complexity of
the data environment and strict

152
00:08:42,359 --> 00:08:45,599
regulatory requirements, they
face significant challenges.

153
00:08:45,779 --> 00:08:48,419
Their data landscape was
immense and fragment.

154
00:08:48,469 --> 00:08:51,829
Fragmented with over 300 data
sources across legacy systems

155
00:08:51,829 --> 00:08:53,119
and modern cloud platform.

156
00:08:53,479 --> 00:08:57,709
The industry's regulatory reporting
requirements meant that any new

157
00:08:57,709 --> 00:09:01,279
data pipeline needed to handle
high complex transformations.

158
00:09:01,549 --> 00:09:05,539
The result was that the average time
to implement a new data pipeline was

159
00:09:05,539 --> 00:09:09,069
a staggering three weeks, and the
limited data engineering resources

160
00:09:09,309 --> 00:09:13,419
were constantly a bottleneck,
unable to keep up with the demand.

161
00:09:14,329 --> 00:09:17,929
After implementing an AI driven EL
system, the results were transformative.

162
00:09:18,169 --> 00:09:22,459
The time to create a new pipeline was
reduced to a matter of hours, not weeks.

163
00:09:22,729 --> 00:09:27,559
This meant the business could get
new reports and insights much faster.

164
00:09:27,919 --> 00:09:32,659
They found that 85% of their common
integration tasks could be completed

165
00:09:32,659 --> 00:09:34,014
without a single line of coding.

166
00:09:34,829 --> 00:09:36,839
The impact on the team was huge.

167
00:09:37,169 --> 00:09:41,609
Data analysts who were previously reliant
on engineers were able to create their

168
00:09:41,609 --> 00:09:43,229
own pipelines with natural language.

169
00:09:43,559 --> 00:09:47,519
This led to a 40% reduction in
the data engineering backlog as

170
00:09:47,519 --> 00:09:51,209
the team was freed up to focus on
more complex strategic projects.

171
00:09:51,959 --> 00:09:55,559
And for a regulatory industry like
financial services, the automatic

172
00:09:55,559 --> 00:10:00,809
lineage documentation was a critical
benefit, ensuring improved compliance

173
00:10:00,809 --> 00:10:02,219
and making orders much easier.

174
00:10:02,719 --> 00:10:06,199
Here is another powerful case study from
the e-commerce sector, which highlights

175
00:10:06,199 --> 00:10:08,899
this value of speed and agility.

176
00:10:09,289 --> 00:10:14,989
The company's goal was to reduce the time
it took to analyze customer behavior with.

177
00:10:15,169 --> 00:10:16,309
Traditional methods.

178
00:10:16,339 --> 00:10:20,749
It took five days to build a pipeline,
which was a huge delay in a fast moving

179
00:10:20,749 --> 00:10:25,759
market with the AI driven system, that
time dropped to a staggering 30 minutes,

180
00:10:26,029 --> 00:10:30,049
allowing the marketing team to react
to market changes almost in real time.

181
00:10:30,349 --> 00:10:32,809
This is a massive competitive advantage.

182
00:10:33,079 --> 00:10:35,749
This led to a huge leap
in user empowerment.

183
00:10:36,079 --> 00:10:38,389
The marketing team, a
group of business users.

184
00:10:38,929 --> 00:10:43,489
Created over 75 pipelines on their own,
completely reminding the dependency

185
00:10:43,489 --> 00:10:44,809
on the data engineering team.

186
00:10:45,079 --> 00:10:49,039
This is a perfect example of what
it means to democratize data.

187
00:10:49,369 --> 00:10:51,679
The system proved to
be incredibly scalable.

188
00:10:51,979 --> 00:10:58,699
It handles over 500 daily tasks and
processes, a massive amount of data, 12

189
00:10:58,699 --> 00:11:00,739
terabytes across 30 different system.

190
00:11:00,739 --> 00:11:03,019
This shows that technology
can handle enterprise level,

191
00:11:03,289 --> 00:11:05,869
scale data and complexity.

192
00:11:06,369 --> 00:11:13,119
I, many leaders from the company have
also significantly saw the improvement.

193
00:11:13,399 --> 00:11:18,229
They went from waiting weeks for
data to be being self-sufficient.

194
00:11:18,529 --> 00:11:22,129
They were able to respond to market
changes in hours instead of weeks

195
00:11:22,339 --> 00:11:25,879
because they had the power to create
and modify pipeline themselves.

196
00:11:26,379 --> 00:11:28,809
These aren't just
isolated success stories.

197
00:11:28,809 --> 00:11:31,029
The data shows a clear
pattern of improvement.

198
00:11:31,359 --> 00:11:33,939
The first bar on the graph
shows a dramatic reduction

199
00:11:33,939 --> 00:11:35,259
in pipeline creation time.

200
00:11:35,769 --> 00:11:40,449
The average time was reduced by
96% from days, two hours, which

201
00:11:40,449 --> 00:11:42,069
is a massive leap in agility.

202
00:11:42,579 --> 00:11:45,879
The second bar shows a similar
trend for engineering hours.

203
00:11:46,209 --> 00:11:51,454
There was an 89, 80 7% reduction
in the number of hours required.

204
00:11:51,789 --> 00:11:52,869
Per pipeline.

205
00:11:53,049 --> 00:11:57,489
This frees up your most skilled resources
to work on more complex strategic

206
00:11:57,489 --> 00:12:01,749
projects, rather than spending their
time on manual repetitive tasks.

207
00:12:02,259 --> 00:12:07,059
We also see a significant decrease
in pipeline errors, a 33% reduction.

208
00:12:07,329 --> 00:12:09,369
This is a direct result
of the system's ability.

209
00:12:10,094 --> 00:12:14,474
To generate optimized and consistent
code with built-in validation, which

210
00:12:14,474 --> 00:12:17,024
is far less error than manual coding.

211
00:12:17,535 --> 00:12:22,964
The most exciting data point for me is
that 85% of business users could create

212
00:12:22,964 --> 00:12:24,884
simple pipelines with no training.

213
00:12:25,214 --> 00:12:27,764
This is a powerful testament
to the systems user

214
00:12:27,764 --> 00:12:29,864
friendliness and accessibility.

215
00:12:30,224 --> 00:12:34,035
The data shows that the promise of
democratizing data is not just a

216
00:12:34,035 --> 00:12:36,074
theory, it's a measurable reality.

217
00:12:36,474 --> 00:12:41,424
These benchmarks are based on aggregated
data from 12 enterprise implementations

218
00:12:41,694 --> 00:12:43,344
across a variety of sectors.

219
00:12:43,584 --> 00:12:45,564
So we know this is a consistent finding.

220
00:12:46,064 --> 00:12:49,805
Let's look beyond the numbers at
the broader organizational impact.

221
00:12:50,505 --> 00:12:53,985
This technology directly impacts
your a accelerated time to

222
00:12:54,045 --> 00:12:55,620
insight, the time it takes.

223
00:12:56,295 --> 00:13:00,315
To create a new data workflow is reduced
from weeks to minutes, which enables

224
00:13:00,315 --> 00:13:02,595
faster, more agile business decisions.

225
00:13:03,045 --> 00:13:05,415
This is crucial for staying
competitive in today's market.

226
00:13:05,915 --> 00:13:08,135
It also leads to
cross-functional empowerment.

227
00:13:08,165 --> 00:13:11,855
When business users can create their
own data pipelines, they are no longer

228
00:13:11,855 --> 00:13:13,595
bottlenecked by a single department.

229
00:13:13,925 --> 00:13:18,845
This frees up your data engineers to focus
on more complex, high value tasks like

230
00:13:18,845 --> 00:13:20,825
building the underlying platform itself.

231
00:13:21,400 --> 00:13:25,360
From a financial perspective, you can
expect significant cost reduction.

232
00:13:25,810 --> 00:13:29,170
The technology offers elastic
resource utilization and automatic

233
00:13:29,170 --> 00:13:34,990
optimization, which leads to a 30
to 50% lower total cost of ownership

234
00:13:34,990 --> 00:13:36,699
compared to traditional ETL solution.

235
00:13:37,150 --> 00:13:39,730
And finally, you get a significant
reduction in technical rate.

236
00:13:40,120 --> 00:13:43,060
The automatically generated
pipelines are consistent.

237
00:13:43,420 --> 00:13:48,129
Documented and have governance built
in, which makes them much easier to

238
00:13:48,129 --> 00:13:50,139
manage and maintain in the long run.

239
00:13:50,469 --> 00:13:55,359
The system also adapts to a new data
source and transformation needs without

240
00:13:55,359 --> 00:13:56,979
requiring you to record everything.

241
00:13:57,479 --> 00:14:00,629
So how do you get started on this journey?

242
00:14:00,989 --> 00:14:04,169
It's important to approach this
strategically with a clear roadmap.

243
00:14:04,499 --> 00:14:06,719
The first phase is
discovery and assessment.

244
00:14:07,109 --> 00:14:10,259
You need to take full inventory
of your existing data sources

245
00:14:10,349 --> 00:14:11,669
and integration points.

246
00:14:11,999 --> 00:14:14,039
Don't try to tackle everything at once.

247
00:14:14,279 --> 00:14:19,049
Instead, identify a few high value,
low complexity use cases that

248
00:14:19,049 --> 00:14:21,060
are perfect for an initial pilot.

249
00:14:21,490 --> 00:14:26,890
You also need to establish clear success
metrics and baseline measurements to

250
00:14:26,890 --> 00:14:28,840
show the value of the new approach.

251
00:14:29,110 --> 00:14:31,750
Next, a pilot deployment is critical.

252
00:14:31,780 --> 00:14:36,700
Implement the AI ETL system for those
two to three selected use cases.

253
00:14:37,120 --> 00:14:40,570
Train a small initial group of
both technical and business users

254
00:14:40,810 --> 00:14:43,960
and make sure you validate the
results against your traditional

255
00:14:43,960 --> 00:14:46,540
methods to prove its effectiveness.

256
00:14:46,900 --> 00:14:49,120
This phase is all about
building confidence and buying.

257
00:14:49,620 --> 00:14:53,040
Once the pilot proves its value,
you can scale and optimize.

258
00:14:53,550 --> 00:14:57,045
You will expand the system to
more data domains and use cases.

259
00:14:57,545 --> 00:15:00,725
This is also where you integrate
with your existing governance

260
00:15:00,725 --> 00:15:04,685
frameworks and establish a center
of excellence for knowledge sharing

261
00:15:05,015 --> 00:15:06,635
to ensure widespread adoption.

262
00:15:07,115 --> 00:15:11,795
The final stage is full enterprise
integration, where the ETL AI

263
00:15:11,795 --> 00:15:15,275
ETL system becomes your standard
approach for data integration.

264
00:15:15,725 --> 00:15:19,025
You can then progressively migrate
your legacy pipelines to the

265
00:15:19,025 --> 00:15:23,855
new system using usage analytics
for continuous improvement.

266
00:15:24,355 --> 00:15:27,895
It is important to be realistic about
the challenges and consideration.

267
00:15:27,925 --> 00:15:30,745
This is a new technology, and
there are a few hurdles to keep

268
00:15:30,745 --> 00:15:32,755
in mind on the technical side.

269
00:15:32,755 --> 00:15:36,475
While the system handles most
tasks, a few highly specialized

270
00:15:36,475 --> 00:15:39,985
or complex transformation must
still require a coding extension.

271
00:15:40,375 --> 00:15:43,485
You can't expect AI to
handle every edge case.

272
00:15:43,985 --> 00:15:48,155
Perfectly from day one and for extremely
large data volumes, some performance

273
00:15:48,155 --> 00:15:52,445
tuning of the AI generated pipeline
might be needed to achieve optimal speed.

274
00:15:52,775 --> 00:15:55,775
Additionally, legacy system
integration can be a challenge.

275
00:15:56,045 --> 00:15:59,405
Older systems without modern
a PS may require you to build

276
00:15:59,495 --> 00:16:02,915
additional con connectors to get
them to work with the platform.

277
00:16:03,425 --> 00:16:07,685
On the organizational side, your data
governance processes will need to evolve.

278
00:16:08,674 --> 00:16:12,094
To support this new self-service
model, you will need to think

279
00:16:12,094 --> 00:16:16,174
about how to manage a high volume
of high user generator pipelines.

280
00:16:16,564 --> 00:16:19,805
The roles of your data needs
will also significantly change.

281
00:16:20,074 --> 00:16:22,805
They'll shift from writing code
to focusing on architecture,

282
00:16:22,805 --> 00:16:26,074
governance, and oversight
becoming true platform specialist.

283
00:16:26,314 --> 00:16:30,154
Finally, while the system is core free,
you'll still need to provide training

284
00:16:30,154 --> 00:16:34,535
and adoption guidance for users on how
to effectively communicate in their

285
00:16:34,564 --> 00:16:36,874
data requirements to the AI systems.

286
00:16:37,374 --> 00:16:42,129
To summarize, generative AI is not
just an incremental improvement to ETL.

287
00:16:42,734 --> 00:16:44,444
It's a fundamental transformation.

288
00:16:44,474 --> 00:16:48,674
The ability to use natural language to
create data pipelines is a game changer.

289
00:16:49,094 --> 00:16:51,254
The business impact is
real and measurable.

290
00:16:51,494 --> 00:16:55,214
We have seen significant cost savings
and huge reductions in development

291
00:16:55,214 --> 00:16:59,294
time with some organizations seeing
over 90% reductions in development

292
00:16:59,294 --> 00:17:01,544
time and significant cost savings.

293
00:17:01,869 --> 00:17:05,024
This technology democratizes
data access, which is one of

294
00:17:05,024 --> 00:17:06,494
the most powerful outcomes.

295
00:17:07,244 --> 00:17:10,754
It empowers business users to be
self-sufficient and get the insight they

296
00:17:10,754 --> 00:17:12,824
need without waiting on a technical team.

297
00:17:13,634 --> 00:17:18,055
My final advice is to is to start
small and scale strategically.

298
00:17:18,595 --> 00:17:20,035
Don't try to boil ocean.

299
00:17:20,035 --> 00:17:22,405
Begin with a few well-defined use cases.

300
00:17:22,735 --> 00:17:25,645
Prove the value and then expand
your implementation as your team's

301
00:17:25,645 --> 00:17:27,385
confidence and capabilities grow.

302
00:17:27,865 --> 00:17:29,695
This is the future of data integration.

303
00:17:29,905 --> 00:17:34,885
It's about making data accessible to
everyone in your organization, and I hope

304
00:17:34,885 --> 00:17:38,965
this presentation has given you a solid
understanding of how that can be achieved.

305
00:17:39,415 --> 00:17:39,895
Thank you.

