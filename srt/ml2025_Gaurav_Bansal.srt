1
00:00:01,274 --> 00:00:01,904
Hi guys.

2
00:00:01,904 --> 00:00:03,104
My name is Gar Benzel.

3
00:00:03,344 --> 00:00:07,723
I graduated in 2009 in
information Technology from India.

4
00:00:07,994 --> 00:00:12,014
I have around 15 years of experience
working in information technology,

5
00:00:12,014 --> 00:00:16,524
like building services, web
applications test infrastructure.

6
00:00:16,854 --> 00:00:23,454
I am currently working with Amazon for
around 2015, almost like 10 years now.

7
00:00:23,823 --> 00:00:25,744
So yeah, that's all me.

8
00:00:26,398 --> 00:00:28,529
Let's talk about why we are here today.

9
00:00:28,529 --> 00:00:31,948
So today we are here for
the LLM applications.

10
00:00:31,948 --> 00:00:34,109
The industry is moving fast.

11
00:00:34,379 --> 00:00:35,339
Everyone is learning.

12
00:00:35,339 --> 00:00:40,169
LLM, everyone is talking about large
language models and how to use in

13
00:00:40,169 --> 00:00:44,579
their day-to-day testing, how to
use it in their day-to-day life

14
00:00:44,669 --> 00:00:50,129
changing events like finding the
flights, finding the code solution, or

15
00:00:50,129 --> 00:00:52,289
finding anything at this moment like.

16
00:00:52,889 --> 00:00:55,229
But how do we test LLM application?

17
00:00:55,229 --> 00:00:56,429
That's the question today, right?

18
00:00:56,429 --> 00:01:00,419
We are talking about, so
testing LLM applications.

19
00:01:00,479 --> 00:01:06,119
Before we can talk about that, let's talk
about how do we test a normal surveys

20
00:01:06,119 --> 00:01:08,399
or how do we test a simple method.

21
00:01:08,399 --> 00:01:12,299
So for an example, let's talk about a
method that test the sum two plus two

22
00:01:12,299 --> 00:01:14,699
equal to four, two, and two as an input.

23
00:01:15,194 --> 00:01:15,944
Four is an output.

24
00:01:16,004 --> 00:01:20,514
So you evaluate sorry, you assert
equal with input, which is two and

25
00:01:20,514 --> 00:01:24,454
two asset output like four comma four.

26
00:01:24,874 --> 00:01:25,984
I am equal.

27
00:01:26,014 --> 00:01:26,614
I'm done.

28
00:01:26,674 --> 00:01:28,504
How to test an LLM application.

29
00:01:28,534 --> 00:01:29,074
LLMR.

30
00:01:29,074 --> 00:01:32,104
Like kids, like they answer different.

31
00:01:32,134 --> 00:01:38,464
So like for an example, I ask an LLM
question, how is the weather today?

32
00:01:38,884 --> 00:01:41,314
Is his answer is the weather is awesome.

33
00:01:41,314 --> 00:01:42,664
It's 62 degrees.

34
00:01:43,339 --> 00:01:47,569
Okay, I have an expected, the same
weather is also is 62 degrees.

35
00:01:47,569 --> 00:01:50,239
I miss it a surgical, it worked fine.

36
00:01:50,539 --> 00:01:51,409
My test passed.

37
00:01:51,769 --> 00:01:52,429
Same thing.

38
00:01:52,429 --> 00:01:53,689
I asked the question again.

39
00:01:53,689 --> 00:01:54,559
How's the weather today?

40
00:01:54,589 --> 00:01:55,809
It's weather is wonderful.

41
00:01:55,809 --> 00:01:57,189
It's 74 degrees today.

42
00:01:57,729 --> 00:02:01,659
My God, the answer I got first is
different in the answer I got second,

43
00:02:01,689 --> 00:02:03,429
so I'm not going to assert it.

44
00:02:04,869 --> 00:02:05,769
What do we do then?

45
00:02:05,859 --> 00:02:06,909
How do we test it?

46
00:02:06,909 --> 00:02:11,979
This is a horrible situation being a
testing industry or a software developer.

47
00:02:11,979 --> 00:02:13,989
How do you test your LM application?

48
00:02:13,989 --> 00:02:14,859
So how do we test it?

49
00:02:15,459 --> 00:02:17,499
So the thing is like we don't test it.

50
00:02:17,679 --> 00:02:19,329
We evaluate it for an example.

51
00:02:19,329 --> 00:02:20,709
Like you go to the doctor, right?

52
00:02:20,979 --> 00:02:22,209
Doctor ask you like.

53
00:02:22,494 --> 00:02:25,164
How are you feeling on
a scale from one to 10?

54
00:02:25,254 --> 00:02:26,514
And you say I'm feeling nine.

55
00:02:26,514 --> 00:02:28,344
That means you're good, great, wonderful.

56
00:02:28,854 --> 00:02:30,564
And two, I'm bad.

57
00:02:30,774 --> 00:02:31,644
I'm not doing good.

58
00:02:31,674 --> 00:02:32,694
I just want to lay down.

59
00:02:32,724 --> 00:02:34,044
Same thing happens with LLM.

60
00:02:34,554 --> 00:02:36,834
You evaluate them, you don't test them.

61
00:02:36,834 --> 00:02:40,614
So there are very many metrics
available into the market that

62
00:02:40,614 --> 00:02:41,994
tells you how to evaluate it.

63
00:02:42,264 --> 00:02:43,764
Like what the text was, input.

64
00:02:44,394 --> 00:02:45,804
Did you get the correct response?

65
00:02:45,834 --> 00:02:46,974
Is the similarity is good?

66
00:02:46,974 --> 00:02:48,114
Is the evolution is good?

67
00:02:48,454 --> 00:02:51,744
Is there so context
relevant or other things.

68
00:02:51,744 --> 00:02:55,914
So you evaluate them, you don't
assert a call onto them, you just

69
00:02:55,914 --> 00:02:57,804
them based on a scale from zero to 10.

70
00:02:58,434 --> 00:03:02,274
And then let's talk about the first
slide that we are talking today,

71
00:03:02,274 --> 00:03:03,984
the enterprises scale challenge.

72
00:03:03,984 --> 00:03:05,634
What is the challenge we are facing?

73
00:03:05,724 --> 00:03:10,424
The challenges like, like everywhere
we have building LLM applications

74
00:03:10,424 --> 00:03:12,224
and LLM applications are everywhere.

75
00:03:12,224 --> 00:03:17,354
It is in pharmacy, it is in education,
it is in sports everywhere, and

76
00:03:17,354 --> 00:03:18,734
they are growing every single day.

77
00:03:18,734 --> 00:03:20,474
They're learning so many things.

78
00:03:20,574 --> 00:03:22,224
They're getting complex and complex.

79
00:03:22,224 --> 00:03:24,714
Everyone wants to use
LLM for their work today.

80
00:03:24,714 --> 00:03:26,934
No one wants to write
the code for services.

81
00:03:27,234 --> 00:03:28,194
They're getting complex.

82
00:03:29,364 --> 00:03:32,754
Operational performance variability,
like they are huge, right?

83
00:03:32,994 --> 00:03:35,904
They, you have huge data centers
over there, so you need to work

84
00:03:35,904 --> 00:03:37,404
to them to understand them.

85
00:03:37,674 --> 00:03:42,024
You need to understand like,
how do I manage these huge data

86
00:03:42,024 --> 00:03:46,944
structures and huge data storage,
excluding stakeholder expectations.

87
00:03:46,944 --> 00:03:49,444
So LM is going big, right?

88
00:03:49,444 --> 00:03:52,714
So everyone wants to get their things
done within seconds, within minutes,

89
00:03:53,044 --> 00:03:54,904
or within microsecond actually.

90
00:03:55,174 --> 00:03:56,854
LLM today is slow.

91
00:03:57,034 --> 00:04:00,424
Slow means like they
process a lot, ton of data.

92
00:04:00,514 --> 00:04:04,564
Like they, they talk about ton
of data, like you put an input,

93
00:04:04,804 --> 00:04:06,274
there is a large data set.

94
00:04:06,334 --> 00:04:07,414
They have to go through it.

95
00:04:07,414 --> 00:04:09,004
They think about it and they respond.

96
00:04:09,004 --> 00:04:09,124
It.

97
00:04:09,124 --> 00:04:11,734
How can you make it more quicker?

98
00:04:12,094 --> 00:04:15,139
Those are the challenges
we are currently facing.

99
00:04:15,869 --> 00:04:17,164
Let's talk about.

100
00:04:18,034 --> 00:04:21,244
Traditional metrics, like
how do we handle that?

101
00:04:21,244 --> 00:04:25,174
Like accuracy, safety,
coherence, adherence.

102
00:04:25,714 --> 00:04:28,954
So accuracy is one thing that
we always talk about, right?

103
00:04:29,313 --> 00:04:33,904
I want my LLM to give me the correct
answer, not the wrong answer, and

104
00:04:33,904 --> 00:04:36,664
how many times is gonna give me
the correct answer, and what is

105
00:04:36,664 --> 00:04:38,464
the criteria of the correct answer?

106
00:04:38,724 --> 00:04:42,944
If the answer is matching with an
expected results, 80% is it correct?

107
00:04:43,514 --> 00:04:44,324
It is accurate.

108
00:04:44,534 --> 00:04:46,574
So we need to always
check on those metrics.

109
00:04:46,574 --> 00:04:51,434
Safety, like I want my LLM not
to provide wrong answers, right?

110
00:04:52,064 --> 00:04:55,724
Anybody can use LLM to
do any, anything bad.

111
00:04:56,294 --> 00:05:01,904
So I want my LLM when someone ask a
question to hurt something, to destroy

112
00:05:01,904 --> 00:05:03,884
something, my LLM should respond.

113
00:05:04,094 --> 00:05:05,204
I don't know the answer.

114
00:05:05,744 --> 00:05:07,304
Coherence, logical coherence.

115
00:05:07,544 --> 00:05:11,984
Every time I ask a question, I. LLM
has to give me the correct answer.

116
00:05:12,194 --> 00:05:13,304
It should be logical.

117
00:05:13,454 --> 00:05:15,404
I it should not be like,
how's the weather today?

118
00:05:15,404 --> 00:05:21,394
And LLM answering is today
is I'm going to buy a car.

119
00:05:21,634 --> 00:05:22,654
That's not logical.

120
00:05:22,654 --> 00:05:24,274
So you need to understand that thing.

121
00:05:24,829 --> 00:05:28,684
Instruction, like how precise is that?

122
00:05:28,684 --> 00:05:30,844
How easy is to interpret like.

123
00:05:31,279 --> 00:05:33,619
Can it support multiple
step instructions or not?

124
00:05:33,619 --> 00:05:37,159
So those things, l, LM has to
know those are the beyond the

125
00:05:37,249 --> 00:05:39,049
additional metrics, business impact.

126
00:05:39,049 --> 00:05:40,369
Let's talk about the third slide.

127
00:05:40,649 --> 00:05:44,729
Business impact of robust
evaluation very well.

128
00:05:44,829 --> 00:05:47,244
LLM currently utilizing so many things.

129
00:05:47,244 --> 00:05:50,244
So for an example is writing code earlier.

130
00:05:50,708 --> 00:05:53,979
We used to do simple Java
upgrades and all those things.

131
00:05:54,068 --> 00:05:58,268
Everyone has to spend time on it,
like hours and hours writing code.

132
00:05:58,268 --> 00:06:00,878
Even though it was just one line
of code, you still need to test it.

133
00:06:00,878 --> 00:06:02,049
You still need to deploy it.

134
00:06:02,049 --> 00:06:05,429
You still need to make sure that
it's not breaking things for a normal

135
00:06:05,429 --> 00:06:09,179
developer to make a Java upgrade,
it was a at least a one day of work.

136
00:06:09,179 --> 00:06:11,429
With LLM, it is like a
five minutes of work.

137
00:06:11,728 --> 00:06:13,498
It just put up the CR for you.

138
00:06:13,498 --> 00:06:16,378
It put up in your pipelines, it runs
the test case and all those things.

139
00:06:17,443 --> 00:06:21,703
So it's getting more, providing
more business impact is failure

140
00:06:21,703 --> 00:06:23,114
reduction is one of the categories.

141
00:06:23,114 --> 00:06:27,043
So how many failure reductions we are
facing like 30% failure reduction.

142
00:06:27,253 --> 00:06:28,933
So what a human could catch it.

143
00:06:28,933 --> 00:06:32,503
Like where I used to do, if we
are people as used to do manual

144
00:06:32,503 --> 00:06:35,563
testing by reading some of the test
cases, but we are humans, right?

145
00:06:35,563 --> 00:06:38,614
We get tired after 10 hour
shift, nine hour shift.

146
00:06:38,614 --> 00:06:39,693
My eye doesn't work.

147
00:06:39,693 --> 00:06:42,273
I am not able to figure out
what test cases on the left.

148
00:06:42,373 --> 00:06:45,313
How is, how many is passed, how many
is relevant, how many is failed.

149
00:06:45,614 --> 00:06:47,953
With LLM, they don't get tired.

150
00:06:48,073 --> 00:06:50,713
They can always execute your test cases.

151
00:06:50,713 --> 00:06:53,803
They can always run
your integration tests.

152
00:06:53,803 --> 00:06:54,628
They never get tired.

153
00:06:55,258 --> 00:06:55,948
Reliability.

154
00:06:56,328 --> 00:06:57,148
It has gone up.

155
00:06:57,598 --> 00:07:00,388
Because of, we are training LLM every day.

156
00:07:00,388 --> 00:07:03,929
We are providing a lot of
data, so it's always going up.

157
00:07:04,228 --> 00:07:08,078
Foster ations, like as I said,
like it never gets fired.

158
00:07:08,078 --> 00:07:10,508
So you can always done as
many times as you want.

159
00:07:11,828 --> 00:07:13,898
Distributed evolution architecture.

160
00:07:13,978 --> 00:07:16,318
Now let's talk about real time monitoring.

161
00:07:16,498 --> 00:07:20,908
Every time that LLM is going into
that, like we are doing the real

162
00:07:20,908 --> 00:07:22,708
time monitoring, it has to be there.

163
00:07:22,739 --> 00:07:26,728
Every time the model is changing, we
are changing the model on the backend.

164
00:07:26,728 --> 00:07:27,748
New model is coming.

165
00:07:27,748 --> 00:07:31,348
We want to assess it, we want to see that
it is good doing the right thing or not.

166
00:07:31,679 --> 00:07:35,548
How do you do it like you get
the evaluation results before?

167
00:07:35,758 --> 00:07:39,718
Then you put a new model and then
you do the evaluation results after,

168
00:07:39,718 --> 00:07:42,878
and then you match it and then you
say oh my God, now I can see my

169
00:07:42,878 --> 00:07:45,818
model is working good before or now.

170
00:07:45,908 --> 00:07:48,428
And if it is degrading the
performance, it alerts.

171
00:07:48,458 --> 00:07:49,448
So that's how it works.

172
00:07:49,478 --> 00:07:50,948
It's scalable infrastructure.

173
00:07:52,088 --> 00:07:53,348
It is really scalable.

174
00:07:53,348 --> 00:07:58,088
Like you can just like run thousands
of utterances towards your LLM and it

175
00:07:58,088 --> 00:07:59,588
always give you the precise resource.

176
00:07:59,588 --> 00:08:02,618
It cannot, but the
infrastructure is scalable.

177
00:08:02,888 --> 00:08:05,858
Hub and spoke testing
specialized evolution models

178
00:08:05,858 --> 00:08:07,178
for different capabilities.

179
00:08:07,178 --> 00:08:10,418
Enabling target assessment for reasoning
knowledge to driver and creating

180
00:08:10,418 --> 00:08:12,368
generation is the one of the key part.

181
00:08:13,958 --> 00:08:16,298
Let's go back to the human element.

182
00:08:16,298 --> 00:08:18,068
Humans are really very important.

183
00:08:18,638 --> 00:08:21,908
It has to provide right sometime.

184
00:08:22,088 --> 00:08:22,868
LLM.

185
00:08:24,203 --> 00:08:25,553
Needs to be test by a human.

186
00:08:25,853 --> 00:08:29,713
It doesn't, has to be like like LLM
can test itself, but human has to

187
00:08:29,713 --> 00:08:31,183
know what is right, what is wrong.

188
00:08:31,183 --> 00:08:32,263
At least at this moment.

189
00:08:32,263 --> 00:08:36,833
Once LLM grew up, maybe we don't
need humans, but currently an expert

190
00:08:36,833 --> 00:08:40,253
reviews needed for all the test
cases or all the data that we are

191
00:08:40,253 --> 00:08:42,863
using to test an LLM for accuracy.

192
00:08:43,193 --> 00:08:45,714
We need to make sure that it
has been reviewed by a human.

193
00:08:46,959 --> 00:08:48,759
To expert review feedback loops.

194
00:08:48,759 --> 00:08:51,849
What are the problems we are
seeing with an output of the aams?

195
00:08:51,849 --> 00:08:55,089
We need to make sure that it also has
to go through the human so that they can

196
00:08:55,089 --> 00:08:57,699
provide the feedbacks that is good or not.

197
00:08:57,969 --> 00:08:58,899
Bias detection.

198
00:08:59,139 --> 00:09:01,629
If human is finding there is
a problem in the output, there

199
00:09:01,629 --> 00:09:02,769
is a problem in the in input.

200
00:09:02,769 --> 00:09:05,649
We need to solve those
problems before sending it out.

201
00:09:06,429 --> 00:09:07,479
Versatile testing.

202
00:09:07,539 --> 00:09:08,679
H Case generation.

203
00:09:09,399 --> 00:09:11,049
Next slide that we are talking about.

204
00:09:11,199 --> 00:09:14,529
So SK generation is strategically
check, create challenging in

205
00:09:14,529 --> 00:09:16,629
unexpected scenario, right?

206
00:09:16,809 --> 00:09:21,669
So those are like very, like LLM
is able to test where you need

207
00:09:21,669 --> 00:09:27,168
to prove is running, like you can
send any kind of, entrances to it.

208
00:09:27,468 --> 00:09:28,608
There should be guardrails.

209
00:09:29,178 --> 00:09:30,888
You cannot ask a bad
question to it, right?

210
00:09:30,918 --> 00:09:32,568
The guard guardrail should save it.

211
00:09:32,568 --> 00:09:34,988
And you say oh, this is the
question that I don't support it.

212
00:09:35,258 --> 00:09:37,268
Rat team exercises every time.

213
00:09:37,268 --> 00:09:41,408
We need to make sure that we mislead
it, we break it, we do some of the

214
00:09:41,408 --> 00:09:43,058
security vulnerability testing on it.

215
00:09:43,388 --> 00:09:45,008
So like you need to make sure that.

216
00:09:45,193 --> 00:09:49,123
Views and like whatever the data
you are sending to your LLM is not

217
00:09:49,123 --> 00:09:50,413
gonna be open up to the public.

218
00:09:50,593 --> 00:09:54,943
Some of the times you are working on the
project that is like critical projects

219
00:09:55,333 --> 00:09:57,373
that is not gonna open for the public.

220
00:09:57,553 --> 00:10:02,843
That's not, that's going to be published
maybe two months from now, but it's gonna

221
00:10:02,843 --> 00:10:06,383
change how your industry is working.

222
00:10:06,383 --> 00:10:11,033
So you need to make sure that those
things is not open to anyone else.

223
00:10:11,083 --> 00:10:11,833
That is very important.

224
00:10:11,893 --> 00:10:12,853
Stress testing.

225
00:10:13,213 --> 00:10:15,973
Stress testing is also really
important because you need to make

226
00:10:15,973 --> 00:10:19,783
sure that your LLM can support
thousands and millions of requests.

227
00:10:20,083 --> 00:10:23,503
Everybody is growing up right,
so PR service should be able

228
00:10:23,503 --> 00:10:24,943
to support all those testing.

229
00:10:25,363 --> 00:10:29,173
You should be able to do the stress
testing before to make sure that

230
00:10:29,173 --> 00:10:33,943
it is able to do support, the high
volume of queries and complex queries.

231
00:10:34,153 --> 00:10:36,313
That is really important
in the testing as well.

232
00:10:37,718 --> 00:10:42,218
Then we talk about the testing,
compliance integration, documentation.

233
00:10:42,248 --> 00:10:45,668
So every time you are doing
anything, you document it.

234
00:10:45,668 --> 00:10:48,728
You write test reserves, you
write evolution methodologies,

235
00:10:48,728 --> 00:10:52,058
you write limitation actions,
everything to be documented.

236
00:10:52,058 --> 00:10:55,958
That's a key part of any testing,
transparency, implement clear reporting

237
00:10:55,958 --> 00:10:59,108
mechanism that articulate how AI decides.

238
00:10:59,558 --> 00:11:02,708
On evaluated right AI
decisions are evaluated.

239
00:11:02,768 --> 00:11:05,348
Make sure that you have
the evolution in place.

240
00:11:06,188 --> 00:11:12,998
And so always keep your back evaluation
results and your new evaluation results.

241
00:11:13,358 --> 00:11:16,078
Regular audits, as I said, keep auditing.

242
00:11:16,138 --> 00:11:17,278
Keep auditing your data.

243
00:11:17,308 --> 00:11:18,778
Make sure that you are adding new data.

244
00:11:18,808 --> 00:11:20,338
Make sure you are removing the data.

245
00:11:20,338 --> 00:11:22,618
That is a STA geographic adoption.

246
00:11:22,618 --> 00:11:25,608
Make sure that wherever you are you
need to make sure, if you're part of

247
00:11:25,668 --> 00:11:28,788
India, then you need to make sure that
the standards of India are followed.

248
00:11:28,788 --> 00:11:31,698
If you're part of us, then the
standards of us as follows.

249
00:11:33,033 --> 00:11:37,053
Evolution trends, simulation, driving
multimodal evaluation, continuous

250
00:11:37,053 --> 00:11:38,943
info evaluation has to be there.

251
00:11:39,153 --> 00:11:41,643
Any service that is using LLM, right?

252
00:11:41,913 --> 00:11:44,943
So all the services at
this moment is using LLM.

253
00:11:44,973 --> 00:11:49,593
You need to make sure you put a
enough testing in your CDCI pipeline,

254
00:11:49,593 --> 00:11:51,213
so you should be able to test it.

255
00:11:51,573 --> 00:11:55,173
It could be related to the input,
it could be related to the output.

256
00:11:55,173 --> 00:11:59,043
L and M is not only sir testing
your services, there is like

257
00:11:59,043 --> 00:12:00,813
model that can be evaluated like.

258
00:12:02,028 --> 00:12:05,538
And without even going through your
business logics, it can be right.

259
00:12:05,868 --> 00:12:07,098
Simply be evaluated.

260
00:12:07,098 --> 00:12:08,868
Continuous evaluation is very important.

261
00:12:08,868 --> 00:12:10,188
Keep continue evaluating.

262
00:12:10,278 --> 00:12:14,028
Learn from your mistakes, understand
what the old one was that, and

263
00:12:14,028 --> 00:12:14,898
then make sure the new one.

264
00:12:16,263 --> 00:12:17,463
Implementation roadmap.

265
00:12:17,463 --> 00:12:19,053
Oh, this is very important to learn.

266
00:12:19,053 --> 00:12:24,453
So before you are going to evaluate
an LLM or evaluate an application that

267
00:12:24,453 --> 00:12:28,953
uses l and m on the backend, first you
need to define this evolution criteria.

268
00:12:28,953 --> 00:12:32,533
Like what are the metrics you
are gonna achieve from this?

269
00:12:33,148 --> 00:12:37,078
Evaluation like it is similarity
metrics, it's large rock metrics.

270
00:12:37,078 --> 00:12:41,798
It's like any metrics is that is important
for evaluation for an LLM infrastructure.

271
00:12:42,128 --> 00:12:44,168
Next comes to the infrastructure,
like once you have the

272
00:12:44,168 --> 00:12:45,248
documented data, the end.

273
00:12:45,348 --> 00:12:45,888
Metrics.

274
00:12:45,888 --> 00:12:47,088
You build infrastructure, right?

275
00:12:47,178 --> 00:12:49,188
You create service, you create ui.

276
00:12:49,368 --> 00:12:53,328
You make sure that your service has
access to your LLM, your service

277
00:12:53,328 --> 00:12:55,068
has access to your knowledge basis.

278
00:12:55,068 --> 00:13:01,048
You use the UI to show how the previous
Golden Data set was executed towards

279
00:13:01,048 --> 00:13:03,398
x your LLM, and what was the result?

280
00:13:03,428 --> 00:13:05,768
What was the evolution
criteria for met or not?

281
00:13:07,043 --> 00:13:08,243
Intricate human therapy.

282
00:13:08,343 --> 00:13:12,823
Always be there because you don't know
that test data is a stale the golden data

283
00:13:12,823 --> 00:13:15,373
set that we are evaluating an LLM is gone.

284
00:13:15,373 --> 00:13:16,873
It's bad, it's new.

285
00:13:16,903 --> 00:13:19,213
Even the l and m response
is correct or not.

286
00:13:19,213 --> 00:13:21,153
So always have a human in there.

287
00:13:21,483 --> 00:13:22,683
Continuous improvement.

288
00:13:22,743 --> 00:13:24,003
Learn from your mistakes.

289
00:13:24,033 --> 00:13:25,413
Whatever the bad thing happens.

290
00:13:25,413 --> 00:13:27,843
If the LLM was bad
previously, run from it.

291
00:13:28,353 --> 00:13:29,943
Automate evolution process.

292
00:13:29,943 --> 00:13:30,903
Try to automate it.

293
00:13:31,503 --> 00:13:33,723
Make it part of your CDCI pipelines.

294
00:13:33,723 --> 00:13:36,513
Make it part of automated deployment.

295
00:13:36,543 --> 00:13:39,053
Don't make sure because
every time you have to run it

296
00:13:39,053 --> 00:13:40,223
manually, it's not gonna work.

297
00:13:40,403 --> 00:13:42,443
So you always want to put
it in your automation.

298
00:13:43,493 --> 00:13:46,163
Implement compound lines monitoring.

299
00:13:46,163 --> 00:13:49,253
Make sure that you are not using
any production data, you're not

300
00:13:49,253 --> 00:13:50,903
using any red data in there.

301
00:13:52,223 --> 00:13:54,863
Same thing, driving
innovation with confidence.

302
00:13:54,863 --> 00:13:58,133
Yes, evaluate, improve,
deploy, monitor and learn.

303
00:13:58,193 --> 00:13:59,153
Keep repeating the same.

304
00:13:59,153 --> 00:14:03,803
Look, evaluate deploy, improve,
monitor, evaluate, improve,

305
00:14:03,803 --> 00:14:05,243
deploy, monitor, same thing.

306
00:14:05,393 --> 00:14:10,163
Keep on moving the same circle and make
sure that you are in the confidence.

307
00:14:10,383 --> 00:14:13,323
Yep, that is all about the
LLM that I can talk about.

308
00:14:14,698 --> 00:14:18,928
Anything that, yeah, it was really
great talking to you and then I hope

309
00:14:18,928 --> 00:14:24,418
I would be able to provide some more
presentations in near future talk.

310
00:14:24,508 --> 00:14:26,578
Thank you so much and we'll
talk to you later then.

311
00:14:26,668 --> 00:14:27,268
Bye-Bye guys.

312
00:14:27,328 --> 00:14:27,508
Bye.

