1
00:00:00,000 --> 00:00:00,810
Hey everyone.

2
00:00:01,410 --> 00:00:04,590
Welcome to the session on evolution
of Natural Language Processing.

3
00:00:05,250 --> 00:00:09,270
Here I would like to cover how we
have progressed over the years from

4
00:00:09,270 --> 00:00:11,400
counting words to conversing with ai.

5
00:00:12,300 --> 00:00:15,180
The ability to communicate
through language is one of the key

6
00:00:15,180 --> 00:00:18,120
characteristics of humans for decades.

7
00:00:18,150 --> 00:00:22,260
The dream of training machines with a
similar understanding has fueled the

8
00:00:22,260 --> 00:00:23,940
field of natural language processing.

9
00:00:24,825 --> 00:00:28,455
What began as rudimentary attempts
to analyze text has blossomed into a

10
00:00:28,455 --> 00:00:33,495
sophisticated discipline, culminating
in generative AI and multimodal models

11
00:00:33,765 --> 00:00:35,535
that capture our imagination today.

12
00:00:36,495 --> 00:00:40,935
This journey marked by breakthroughs
and paradigm shifts is a testament to

13
00:00:40,935 --> 00:00:45,165
human creativity and the relentless
pursuit of artificial intelligence

14
00:00:45,165 --> 00:00:46,575
that truly understands us.

15
00:00:47,575 --> 00:00:50,214
Before we begin, a quick
introduction about myself.

16
00:00:50,785 --> 00:00:54,055
I'm the Director of IP Strategy
and Technology at United Lake.

17
00:00:54,655 --> 00:00:56,214
I'm based in Austin, Texas.

18
00:00:56,275 --> 00:00:57,865
In United States of America.

19
00:00:58,734 --> 00:01:02,095
I lead the IP data science
team where we create innovative

20
00:01:02,095 --> 00:01:05,755
solutions at the intersection of
intellectual property and technology.

21
00:01:06,655 --> 00:01:09,865
One of the products that our team
has developed is Vantage for ip.

22
00:01:10,674 --> 00:01:15,084
It is a patent intelligence platform
that processes hundreds of thousands of

23
00:01:15,084 --> 00:01:17,334
patents using natural language processing.

24
00:01:18,204 --> 00:01:21,354
It provides competitive
intelligence, evaluates patent

25
00:01:21,354 --> 00:01:25,824
strength, and uncovers licensing
opportunities amongst other things.

26
00:01:27,024 --> 00:01:31,164
Two of my key focus areas are developing
deep learning models to evaluate

27
00:01:31,345 --> 00:01:35,604
patents and designing user-friendly
platforms to convey the results.

28
00:01:36,604 --> 00:01:41,520
NLP being the hot field it is at the
moment can be attributed to the surge in

29
00:01:41,520 --> 00:01:46,289
investment in this domain, and also the
improvement in computer hardware, thus

30
00:01:46,289 --> 00:01:48,330
increasing the compute capabilities.

31
00:01:49,170 --> 00:01:53,670
According to the 2023 AI index
report by the Stanford Institute of

32
00:01:53,670 --> 00:01:59,669
Human-Centered ai, global AI private
investment was about 92 billion in 2022.

33
00:02:00,554 --> 00:02:05,174
This amount was 18 times greater
than it was 10 years back in 2013.

34
00:02:06,975 --> 00:02:10,394
The amount of computational power
used by significant AI machine

35
00:02:10,394 --> 00:02:14,625
learning systems had increased
exponentially in the last half decade.

36
00:02:15,765 --> 00:02:19,545
As pre-trained large language models
have gotten bigger and more capable

37
00:02:19,549 --> 00:02:23,774
over the years, they have required
bigger and more elaborate training sets.

38
00:02:24,315 --> 00:02:29,655
For example, when OpenAI released
its original G PT one model in 2018.

39
00:02:30,000 --> 00:02:32,970
The model was trained on Book
Corpus, which is a collection of

40
00:02:32,970 --> 00:02:37,950
about 7,000 unpublished books,
comprising about five GB of text.

41
00:02:39,244 --> 00:02:43,739
GT two Launched next year in
2019, was trained on about 40

42
00:02:43,739 --> 00:02:47,640
GB training set created based on
scrap links from Reddit users.

43
00:02:48,959 --> 00:02:52,200
Next year, G PT three was released,
which was pre-trained on more than

44
00:02:52,200 --> 00:02:56,910
500 GB of text from open sources,
including book Corpus, common

45
00:02:56,910 --> 00:02:59,579
Crawl, Wikipedia, and Web Text Two.

46
00:03:00,959 --> 00:03:04,890
While officially the training set
details are scanned for G PT four, which

47
00:03:04,890 --> 00:03:10,049
launched in 2023, the training set has
been reported to be 13 trillion tokens,

48
00:03:10,350 --> 00:03:12,329
which would be a few terabytes of data.

49
00:03:13,329 --> 00:03:18,159
The early days of NLP were characterized
by statistical models, techniques like

50
00:03:18,159 --> 00:03:23,260
bag of words, prevalent in the early,
late 20th and early 21st centuries

51
00:03:23,290 --> 00:03:28,089
treated text as an unordered collection
of words, disregarding, grammar and

52
00:03:28,089 --> 00:03:31,054
syntax, while simple to implement.

53
00:03:31,434 --> 00:03:36,010
Bag of words, enabled basic tasks like
text classification and information

54
00:03:36,010 --> 00:03:38,649
retrieval by analyzing word frequencies.

55
00:03:39,804 --> 00:03:44,394
Term frequency in words, document
frequency built upon this by weighing

56
00:03:44,394 --> 00:03:48,954
words based on their importance
within a document and across a corpus.

57
00:03:49,465 --> 00:03:52,704
These models though limited in
their understanding of context

58
00:03:52,704 --> 00:03:56,575
and relationship between words
laid the foundational statistical

59
00:03:56,575 --> 00:03:58,765
groundwork for future advancements.

60
00:03:59,905 --> 00:04:04,225
The limitations of pure statistical
approaches became increasingly apparent

61
00:04:04,614 --> 00:04:06,624
when tackling more complex NLP tasks.

62
00:04:07,720 --> 00:04:12,609
The inability of compute, the
inability to capture semantic

63
00:04:12,609 --> 00:04:16,809
meaning and the sensitivity to
word order led to the development

64
00:04:16,809 --> 00:04:18,940
of more sophisticated techniques.

65
00:04:19,510 --> 00:04:24,310
Engram models represented a step
forward by considering sequences of N

66
00:04:24,310 --> 00:04:28,325
consecutive words, thus incorporating
some contextual information.

67
00:04:29,325 --> 00:04:32,655
Perplexity is a common metric
for evaluating language models.

68
00:04:32,775 --> 00:04:36,555
It estimates how well a model can
predict the next word in a sequence.

69
00:04:37,035 --> 00:04:41,295
Lower perplexity scores generally indicate
a better model, meaning the model is

70
00:04:41,295 --> 00:04:43,365
more confident in its predictions.

71
00:04:44,085 --> 00:04:48,135
Our perplexity score of 120 plus
for engram models highlights

72
00:04:48,135 --> 00:04:49,635
the limitations of early models.

73
00:04:50,550 --> 00:04:55,410
These models also suffered from the
curse of dimensionality as vocabulary

74
00:04:55,410 --> 00:05:01,170
size increased leading to data
sparsity shoes, for example, with a

75
00:05:01,170 --> 00:05:03,180
vocabulary of 10,000 unique words.

76
00:05:03,330 --> 00:05:07,380
Each document will be represented
by a vector of 10,000 dimensions.

77
00:05:07,950 --> 00:05:11,970
If we have only few, hundreds of few
thousand documents, each document

78
00:05:11,970 --> 00:05:13,500
vector will mostly be zeros.

79
00:05:14,100 --> 00:05:18,570
Trying to find similarities or train
a classifier on such spars, high

80
00:05:18,570 --> 00:05:20,670
dimensional data can be very challenging.

81
00:05:21,480 --> 00:05:26,010
With sparse data in a high dimensional
space, the data points or document

82
00:05:26,010 --> 00:05:28,920
vectors become increasingly
far apart from each other.

83
00:05:29,460 --> 00:05:33,930
This makes it difficult for distant
based algorithms to find meaningful

84
00:05:33,930 --> 00:05:35,700
similarities between documents.

85
00:05:36,090 --> 00:05:38,670
The concept of neighborhood
becomes less reliable.

86
00:05:40,095 --> 00:05:44,835
Further storing and manipulating
these large spars vectors can consume

87
00:05:44,835 --> 00:05:48,915
vast amounts of memory, potentially
exceeding the available resources.

88
00:05:49,915 --> 00:05:55,285
The late 2000 and early 2010s witnessed
a paradigm shift towards machine

89
00:05:55,285 --> 00:05:59,665
learning, particularly with the rise
of deep learning word embedding,

90
00:05:59,665 --> 00:06:04,225
such as what to work and globe
revolutionize, how words were represented.

91
00:06:05,050 --> 00:06:08,860
These techniques learned
dense, low dimensional vector

92
00:06:08,860 --> 00:06:13,900
representations of words based on
their ence in large text corpora.

93
00:06:14,830 --> 00:06:19,480
Crucially, these embeddings capture
semantic relationships, allowing models to

94
00:06:19,480 --> 00:06:24,970
understand that king is similar to queen
in a way that simple statistical models

95
00:06:24,970 --> 00:06:28,750
could not design for sequential data.

96
00:06:29,140 --> 00:06:31,510
Recurrent neural networks are an ins.

97
00:06:31,885 --> 00:06:36,115
Utilize a unique lube architecture
that allowed them to remember past

98
00:06:36,115 --> 00:06:40,105
information, making them useful for
tasks like language modeling and

99
00:06:40,105 --> 00:06:42,805
predictive text where context matters.

100
00:06:43,765 --> 00:06:47,605
A significant limitation of RNN
is their struggle with long range

101
00:06:47,605 --> 00:06:51,085
dependencies, primarily due to the
vanishing radiant problem, which

102
00:06:51,985 --> 00:06:56,005
causes them to lose information from
earlier parts of longer sequences.

103
00:06:57,715 --> 00:07:03,325
Long short-term memory or LSGM networks
were developed as an advanced RNN

104
00:07:03,325 --> 00:07:07,165
architecture, specifically to overcome
the vanishing gradient problem.

105
00:07:08,005 --> 00:07:11,725
Their introduction of memory cells
capable of retaining information for

106
00:07:11,725 --> 00:07:16,495
extended durations and intelligent gating
mechanisms that control information

107
00:07:16,495 --> 00:07:20,575
flow, enable them to effectively
learn long-term relationships.

108
00:07:21,880 --> 00:07:26,350
The best per reported perplexity
score of RN models on pen tree

109
00:07:26,350 --> 00:07:29,530
Bank dataset was around 73.4.

110
00:07:30,490 --> 00:07:35,530
And few LSTM models have also achieved
perplexities in the forties, such as 44.9.

111
00:07:37,720 --> 00:07:42,850
While LSGM offer a substantial improvement
over RNAs in handling long sequences, they

112
00:07:42,850 --> 00:07:44,920
come with a higher computational cost.

113
00:07:45,250 --> 00:07:48,190
Their complex skating mechanism
require more computational

114
00:07:48,190 --> 00:07:49,780
resources and training time.

115
00:07:50,170 --> 00:07:53,800
Moreover, their inherent
sequential processing still

116
00:07:53,800 --> 00:07:57,580
hinders parallel computation and
can limit their effectiveness

117
00:07:57,580 --> 00:08:00,160
with extremely long dependencies.

118
00:08:01,160 --> 00:08:06,050
The transformer architecture introduced
in 2017 with the groundbreaking paper.

119
00:08:06,140 --> 00:08:09,530
Attention is all you need
marked another significant leap.

120
00:08:09,740 --> 00:08:13,910
By leveraging the attention mechanism,
transformers allowed the model to

121
00:08:13,915 --> 00:08:18,470
weigh the importance of different
words in a sequence when processing

122
00:08:18,470 --> 00:08:21,590
it regardless of their distance.

123
00:08:22,280 --> 00:08:26,090
This overcame the limitation of RNs
and enabled parallel processing.

124
00:08:26,630 --> 00:08:29,780
Leading to significantly improved
performance and scalability.

125
00:08:31,940 --> 00:08:36,230
Blue, which stands for bilingual
evaluation UNDERST study.

126
00:08:36,620 --> 00:08:41,330
It's a metric used to evaluate the
quality of machine translated text.

127
00:08:41,900 --> 00:08:46,280
It measures how similar the machine
translated text is to a set of

128
00:08:46,280 --> 00:08:48,410
high quality human translations.

129
00:08:49,070 --> 00:08:52,880
Transformer architecture
achieved a blue score of 41.8 on

130
00:08:52,880 --> 00:08:54,320
English to French translation.

131
00:08:55,320 --> 00:08:59,130
Here is an example of the attention
mechanism following long distance

132
00:08:59,130 --> 00:09:00,630
dependencies from the paper.

133
00:09:01,530 --> 00:09:05,550
Different colors represent different
heads of the multi-head attention.

134
00:09:06,150 --> 00:09:08,610
Attention shown only for the word making.

135
00:09:09,300 --> 00:09:13,170
As we can see, many of the attention
heads attend to the distant

136
00:09:13,170 --> 00:09:15,360
dependency of the word making.

137
00:09:16,425 --> 00:09:19,755
Completing the phrase
Making more difficult.

138
00:09:20,755 --> 00:09:25,525
Here is another example of two attention
heads involved in N four resolution.

139
00:09:26,545 --> 00:09:31,735
On the left side, we see full attentions
for one of the heads, head five, and

140
00:09:31,735 --> 00:09:35,845
on the right side we see isolated
attentions from just the word.

141
00:09:35,875 --> 00:09:37,405
It's for attention.

142
00:09:37,405 --> 00:09:38,515
Heads five and six.

143
00:09:39,370 --> 00:09:45,160
While Head five relates, its to only the
word law Head six relates it to both the

144
00:09:45,215 --> 00:09:48,400
words law and application fairly strongly,

145
00:09:49,400 --> 00:09:53,535
but or bidirectional encoder
representations from transformers.

146
00:09:55,155 --> 00:09:59,685
This architecture achieved 94.9%
accuracy on sentiment analysis,

147
00:09:59,685 --> 00:10:03,555
benchmarks, surpassing previous
models by a significant margin.

148
00:10:04,365 --> 00:10:08,955
Bert introduced several key innovations
that significantly advance the field

149
00:10:08,955 --> 00:10:10,940
of natural language, understanding the.

150
00:10:12,540 --> 00:10:13,830
Bidirectional training.

151
00:10:14,550 --> 00:10:18,600
Unlike previous language models
that process texts, unidirectionally

152
00:10:18,660 --> 00:10:22,920
either left to right or right to
left, bird is designed to learn

153
00:10:22,920 --> 00:10:25,260
deep bidirectional representations.

154
00:10:25,770 --> 00:10:30,570
This means it considered the context
of each word from both the word

155
00:10:30,660 --> 00:10:34,500
that preceded and the words that
follow it simultaneously in all

156
00:10:34,500 --> 00:10:36,150
layers of the transformer network.

157
00:10:37,260 --> 00:10:42,120
This bidirectional understanding allows
Bert to grasp the meaning of words

158
00:10:42,120 --> 00:10:47,940
based on their full context within a
sequence leading to a more nuanced and

159
00:10:47,940 --> 00:10:49,770
accurate understanding of language.

160
00:10:50,310 --> 00:10:51,150
For example, the.

161
00:10:51,585 --> 00:10:54,945
Bert can better differentiate
the meaning of bank in river

162
00:10:54,945 --> 00:10:56,865
bank versus financial bank

163
00:10:57,865 --> 00:10:59,305
mask language modeling.

164
00:10:59,845 --> 00:11:03,655
To enable bidirectional training,
Bert employs a novel pre-training

165
00:11:03,655 --> 00:11:05,575
task called Mask Language modeling.

166
00:11:06,350 --> 00:11:11,480
During pre-training a certain percentage,
typically 15 percents of the word in

167
00:11:11,480 --> 00:11:13,610
the input sequence are randomly massed.

168
00:11:13,700 --> 00:11:17,720
The model's objective is to predict
the original mass words based

169
00:11:17,720 --> 00:11:21,650
on the context provided by the
unmasked words in the sentence.

170
00:11:22,430 --> 00:11:26,930
This forces the model to understand the
meaning of each word by considering its

171
00:11:27,110 --> 00:11:29,480
surrounding context from both directions.

172
00:11:29,900 --> 00:11:33,260
This is crucial for learning deep
bidirectional representations.

173
00:11:35,125 --> 00:11:36,565
Pre-training and fine tuning.

174
00:11:37,225 --> 00:11:40,735
Bird established a powerful
pre-training and fine tuning pattern.

175
00:11:41,605 --> 00:11:45,595
The model is first pre-trained on a
massive amount of unlabeled text data

176
00:11:45,655 --> 00:11:47,845
using the mass language modeling task.

177
00:11:48,325 --> 00:11:52,165
This allows the model to learn general
purpose language representations.

178
00:11:52,735 --> 00:11:57,175
Then the pre-trained bird model
can be fine tuned on smaller tasks,

179
00:11:57,565 --> 00:12:01,495
task specific labeled data sets
for various downstream NLP tasks.

180
00:12:02,080 --> 00:12:07,540
Example, text classification question
answering named entity recognition, et

181
00:12:07,540 --> 00:12:13,450
cetera, by adding a simple task specific
output layer, this approach significantly

182
00:12:13,540 --> 00:12:18,700
reduced the need for large label dataset
for each specific task, and led to

183
00:12:18,700 --> 00:12:23,455
substantial performance improvement
across a wide range of NLP application.

184
00:12:25,680 --> 00:12:30,150
The GPT series developed by OpenAI
has shown a remarkable evolution in

185
00:12:30,150 --> 00:12:35,100
its capabilities, primarily driven by
scaling, model size, and training data.

186
00:12:35,910 --> 00:12:41,160
G PT one released in 2018
included 125 million parameters.

187
00:12:41,700 --> 00:12:45,840
It demonstrated the effectiveness of
pre-training a transformer model for

188
00:12:45,840 --> 00:12:50,280
language understanding and generation,
followed by task specific fine tuning.

189
00:12:51,825 --> 00:12:56,355
GT two released in 2019
included 1.5 billion parameters.

190
00:12:56,775 --> 00:13:01,065
It showed significant improvements in
text generation, quality coherence,

191
00:13:01,065 --> 00:13:05,715
and the ability to perform various
downstream tasks with little to

192
00:13:05,715 --> 00:13:07,545
no task specific fine tuning.

193
00:13:08,505 --> 00:13:14,205
GPT-3 released next year in 2020
included 175 billion parameters.

194
00:13:15,375 --> 00:13:19,455
It showed remarkable few short and
even zero short learning capabilities.

195
00:13:19,875 --> 00:13:24,825
It could perform a wide range of NLP
tasks, including translation, question

196
00:13:24,825 --> 00:13:26,595
answering, and code generation.

197
00:13:26,895 --> 00:13:31,695
With a few examples or even natural
language instructions, it showed

198
00:13:31,695 --> 00:13:36,225
improved coherence and context
understanding compared to its producers.

199
00:13:38,325 --> 00:13:42,855
Superglue is a benchmark data set for
evaluating natural language understanding.

200
00:13:43,965 --> 00:13:47,955
It focuses on tasks that require
deeper reasoning and understanding of

201
00:13:47,955 --> 00:13:52,875
language, including tasks like question
answering, core reference resolution,

202
00:13:53,145 --> 00:13:54,945
and natural language inference.

203
00:13:55,515 --> 00:14:02,835
While GPT one had a super glue score of
45.2 GPT-3 had super glue score of 71.8.

204
00:14:03,615 --> 00:14:08,385
This scaling trend in the GPT series has
been a major driving force behind the

205
00:14:08,385 --> 00:14:13,095
advancements in generative AI and has
paved the way for even larger and more

206
00:14:13,095 --> 00:14:16,065
capable models like GPT-4 and beyond.

207
00:14:17,065 --> 00:14:21,085
The latest frontier in NLP is
the rise of multimodal models.

208
00:14:21,880 --> 00:14:26,380
These models go beyond processing, just
text, and can understand and generate

209
00:14:26,380 --> 00:14:31,450
content across different modalities,
such as images, audio, and video.

210
00:14:32,170 --> 00:14:37,240
By jointly learning representations
from diverse data sources, multimodal

211
00:14:37,240 --> 00:14:41,350
models can achieve a more holistic
understanding of the world and

212
00:14:41,350 --> 00:14:43,510
enable exciting new applications.

213
00:14:44,950 --> 00:14:49,060
Combining text and image data has
improved healthcare diagnostic accuracy.

214
00:14:50,005 --> 00:14:54,835
In a recent study, multimodal models were
used to generate assessments based on

215
00:14:54,835 --> 00:14:57,445
medical images and clinical observations.

216
00:14:58,015 --> 00:15:01,375
Those were evaluated against
physician authored diagnosis.

217
00:15:01,855 --> 00:15:07,375
In 80 plus person cases, the multimodal
assessments outperformed human

218
00:15:07,375 --> 00:15:14,005
diagnosis, AI-driven diagnostics minimize
misdiagnosis, leading to fewer malpractice

219
00:15:14,005 --> 00:15:15,865
claims and unnecessary treatment.

220
00:15:16,840 --> 00:15:21,250
Automated data processing speeds
up disease detection, decreasing

221
00:15:21,250 --> 00:15:23,770
patient wait times and hospital stays.

222
00:15:24,400 --> 00:15:29,950
AI enabled hospitals report 30 to 40%
improvement in workflow efficiency,

223
00:15:30,430 --> 00:15:34,780
allowing providers to treat more patients
without increasing staff workload.

224
00:15:36,835 --> 00:15:41,875
Visual question answering or view
QA model, which understand images

225
00:15:41,875 --> 00:15:46,615
and answer questions about them
have achieved 84.3% accuracy.

226
00:15:47,155 --> 00:15:51,715
This has wide application in
education, accessibility tools, and

227
00:15:51,745 --> 00:15:53,875
even in visual impairment community.

228
00:15:56,725 --> 00:16:02,725
Video chat two achieved 51.1% MV
bench surprising the previous date

229
00:16:02,725 --> 00:16:05,215
of the model by over 15% in accuracy.

230
00:16:05,905 --> 00:16:10,015
This benchmark evaluates complex
temporal reasoning in videos.

231
00:16:10,765 --> 00:16:17,444
L MSS or large multimodal models show
promise in generating accurate and

232
00:16:17,444 --> 00:16:19,665
contextually relevant closed captions.

233
00:16:20,280 --> 00:16:24,150
Their ability to understand the
video context can lead to better

234
00:16:24,150 --> 00:16:28,170
handling of similar sounding
words, speaker identification

235
00:16:28,170 --> 00:16:29,880
and overall caption quality.

236
00:16:31,350 --> 00:16:36,630
Multimodal retrieval enhanced
by LMS aims to retrieve relevant

237
00:16:36,630 --> 00:16:41,970
content, images, or videos based
on complex multifaceted curies,

238
00:16:42,420 --> 00:16:44,245
including text and visual elements.

239
00:16:45,245 --> 00:16:49,955
Large language models, while powerful
are computationally expensive and

240
00:16:49,955 --> 00:16:54,425
memory intensive to make them more
practical for various applications

241
00:16:54,485 --> 00:16:58,805
and deployments, researchers have
developed several efficiency techniques.

242
00:16:59,165 --> 00:17:01,505
Here is an elaboration on
some of these techniques.

243
00:17:02,585 --> 00:17:04,175
Model compression or pruning.

244
00:17:05,175 --> 00:17:09,555
Pruning aims to reduce the number of
parameters in a model by identifying

245
00:17:09,555 --> 00:17:14,925
and removing unimportant weights
or even entire neurons and layers.

246
00:17:15,375 --> 00:17:20,265
This leads to reduced model size, faster
inference, and low memory footprint,

247
00:17:22,905 --> 00:17:24,105
knowledge distillation.

248
00:17:24,870 --> 00:17:28,740
Knowledge distillation involves
training a smaller, more efficient

249
00:17:28,740 --> 00:17:34,080
student model to mimic the behavior
of larger, more complex teacher model.

250
00:17:34,710 --> 00:17:38,790
The teacher model, having learned
from a vast amount of data, transfers

251
00:17:38,790 --> 00:17:40,140
his knowledge to the student.

252
00:17:40,830 --> 00:17:42,810
This leads to smaller model size.

253
00:17:43,020 --> 00:17:47,700
The student model has significantly
fewer parameters, faster inference,

254
00:17:48,090 --> 00:17:49,560
and improved performance.

255
00:17:50,100 --> 00:17:52,105
The student can sometimes
generalize better.

256
00:17:52,800 --> 00:17:57,720
Then if trained directly on the target
task, limited data as it benefits

257
00:17:57,720 --> 00:17:59,730
from the teacher's broader knowledge.

258
00:18:01,770 --> 00:18:05,970
Quantization reduces the precision of
the numerical representations used for

259
00:18:05,970 --> 00:18:08,100
the models, weights and activations.

260
00:18:08,580 --> 00:18:13,140
Instead of using floating point
numbers like 32 bit or 16 bit, the

261
00:18:13,140 --> 00:18:17,310
model uses lower precision numbers
like eight bit or even four bit.

262
00:18:18,090 --> 00:18:22,980
This leads to reduced model size, faster
inference, and lower power consumptions,

263
00:18:23,250 --> 00:18:25,530
which is crucial for edge devices.

264
00:18:27,330 --> 00:18:28,410
Sparse attention.

265
00:18:29,130 --> 00:18:32,730
It is an efficiency technique, used
large language models to reduce

266
00:18:32,730 --> 00:18:37,560
the computational cost of the self
attention mechanism, especially when

267
00:18:37,560 --> 00:18:39,265
dealing with long input sequences.

268
00:18:39,900 --> 00:18:43,470
The standard self attention
mechanism in transformers has a

269
00:18:43,470 --> 00:18:45,840
quadratic time and memory complexity.

270
00:18:46,500 --> 00:18:51,720
Sparse attentions aims to elevate this
quadratic complexity by only computing

271
00:18:51,720 --> 00:18:57,480
attention scores for a subset of possible
query key pairs rather than all of them.

272
00:18:58,260 --> 00:19:02,820
The selection of these pairs is based on
a specific pattern of learned criteria,

273
00:19:03,420 --> 00:19:08,580
aiming to retain the most important
contextual information while significantly

274
00:19:08,580 --> 00:19:10,650
reducing the number of computations.

275
00:19:11,650 --> 00:19:15,670
Throughout this session, I have
mentioned various metrics or scores like

276
00:19:15,670 --> 00:19:18,610
perplexity, blue glue, or super glue.

277
00:19:19,300 --> 00:19:22,750
As the models have evolved,
the metrics have evolved too to

278
00:19:22,810 --> 00:19:24,850
effectively compare different models.

279
00:19:25,330 --> 00:19:29,980
Here are a few metrics which will be in
focus for comparing future and LP models.

280
00:19:32,139 --> 00:19:34,210
Training efficiency and sustainability.

281
00:19:34,870 --> 00:19:37,540
Energy consumption will
be a key metric to watch.

282
00:19:38,005 --> 00:19:41,785
Studies have shown that training
large language models can have

283
00:19:41,785 --> 00:19:43,525
significant carbon footprint.

284
00:19:44,065 --> 00:19:48,385
Researchers at the University of
Massachusetts amhurst estimated that

285
00:19:48,385 --> 00:19:53,095
training a single large language model
could emit around 300 tons of CO2

286
00:19:53,095 --> 00:19:56,245
equivalent reasoning capabilities.

287
00:19:56,965 --> 00:20:01,435
New benchmarks will emerge to assess a
model's ability to perform multi-step

288
00:20:01,735 --> 00:20:05,905
casual reasoning, which involves
understanding how different actions

289
00:20:05,965 --> 00:20:09,115
of or events cause specific outcomes.

290
00:20:09,775 --> 00:20:12,805
Current evaluations often miss
these capabilities, which make

291
00:20:12,805 --> 00:20:17,005
up a large portion of human
reasoning, ethical alignment.

292
00:20:17,905 --> 00:20:22,135
Future metrics will evaluate models
on their ethical considerations.

293
00:20:22,554 --> 00:20:25,375
Including fairness,
transparency, and safety.

294
00:20:25,975 --> 00:20:30,175
This should lead to creating responsible
AI systems that align with human

295
00:20:30,175 --> 00:20:33,145
values across diverse cultural context.

296
00:20:34,145 --> 00:20:36,065
This brings us to the end of this session.

297
00:20:36,335 --> 00:20:38,740
I would like to thank every
one of you for attending.

298
00:20:39,455 --> 00:20:42,815
From the humble beginnings of the
counting words to sophisticated

299
00:20:42,815 --> 00:20:47,254
capabilities of generative and
multimodal ai, the evolution of

300
00:20:47,254 --> 00:20:49,445
NLP has been a remarkable journey.

301
00:20:50,075 --> 00:20:55,535
Each stage built upon the limitations
of it producers driven by the desire

302
00:20:55,535 --> 00:20:59,260
to create machines that can truly
understand and interact with humans.

303
00:21:00,120 --> 00:21:05,225
As we continue to push the boundaries of
ai, the future of NLP promises even more

304
00:21:05,225 --> 00:21:07,895
exciting developments bringing us closer.

305
00:21:09,560 --> 00:21:11,885
And human interaction.

306
00:21:12,815 --> 00:21:13,325
Thank you.

