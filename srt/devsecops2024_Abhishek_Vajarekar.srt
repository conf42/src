1
00:00:00,210 --> 00:00:00,919
Hello, everyone.

2
00:00:01,510 --> 00:00:02,340
Thank you for joining.

3
00:00:02,960 --> 00:00:07,290
Today, we are diving into a topic that
is foundational for any organization

4
00:00:07,630 --> 00:00:09,650
operating modern distributed systems.

5
00:00:10,160 --> 00:00:11,320
Designing for failure.

6
00:00:12,090 --> 00:00:14,740
Now, I know failure isn't
the most glamorous subject.

7
00:00:15,280 --> 00:00:18,150
It's something we often want
to avoid, brush under the rug,

8
00:00:18,510 --> 00:00:20,040
or treat as a one off event.

9
00:00:20,550 --> 00:00:24,480
But the reality is, in complex
systems, failure is unavoidable.

10
00:00:25,090 --> 00:00:28,639
The question is not if your
system will fail, but when,

11
00:00:28,990 --> 00:00:30,880
how, and what happens next.

12
00:00:30,880 --> 00:00:32,750
Why is this such a big deal?

13
00:00:33,420 --> 00:00:34,360
Let's set the scene.

14
00:00:34,800 --> 00:00:38,029
Imagine you're running an e commerce
platform and it's Black Friday.

15
00:00:38,680 --> 00:00:41,760
Millions of users are browsing,
adding items to their cards,

16
00:00:41,880 --> 00:00:43,460
and checking out simultaneously.

17
00:00:44,200 --> 00:00:45,360
Then suddenly, boom!

18
00:00:46,010 --> 00:00:48,769
A core component of your
payment processing goes down.

19
00:00:49,759 --> 00:00:51,349
What happens next is critical.

20
00:00:51,950 --> 00:00:55,130
Does the entire system crash,
blocking customers out and making

21
00:00:55,130 --> 00:00:56,700
headlines for all the wrong reasons?

22
00:00:57,180 --> 00:01:00,620
Or does your platform gracefully
handle the failure, keeping users

23
00:01:00,620 --> 00:01:03,610
shopping while retrying the affected
functionality in the background?

24
00:01:04,539 --> 00:01:08,149
The difference between disaster
and seamless recovery lies in

25
00:01:08,160 --> 00:01:09,639
how your system was designed.

26
00:01:10,139 --> 00:01:12,499
Failure aware design
is no longer optional.

27
00:01:13,119 --> 00:01:17,069
Systems today are massively
distributed, spanning multiple data

28
00:01:17,069 --> 00:01:19,039
centers, clouds, or even continents.

29
00:01:19,499 --> 00:01:23,269
They are highly integrated,
relying on third party APIs, cloud

30
00:01:23,269 --> 00:01:24,969
providers, and external services.

31
00:01:25,699 --> 00:01:29,859
And they are also under constant
demand, with users expecting near

32
00:01:29,909 --> 00:01:32,269
instant responses, even during outages.

33
00:01:33,079 --> 00:01:37,249
With that context, our goal in this
talk is to arm you with the strategies,

34
00:01:37,299 --> 00:01:41,389
tools, and mindset needed to build
systems that not only withstand

35
00:01:41,389 --> 00:01:43,449
failure, but thrive in spite of it.

36
00:01:44,069 --> 00:01:47,949
We'll cover redundancy, failover
mechanisms, graceful degradation,

37
00:01:47,950 --> 00:01:52,829
graceful shutdown, chaos
engineering, circuit breakers,

38
00:01:53,169 --> 00:01:54,929
and automated recovery mechanisms.

39
00:01:55,624 --> 00:02:00,054
By the end of this talk, you'll not only
see failures in a new light, but you'll

40
00:02:00,054 --> 00:02:02,324
also feel empowered to prepare for it.

41
00:02:02,824 --> 00:02:06,134
Redundancy is the foundation
of any resilient system design.

42
00:02:06,584 --> 00:02:11,654
At its core, redundancy means having
backups, spares that can take over

43
00:02:11,724 --> 00:02:12,804
when something critical fails.

44
00:02:13,669 --> 00:02:16,599
Think of it as the safety
net beneath a trapeze artist.

45
00:02:17,099 --> 00:02:19,529
The idea isn't to
prevent falls altogether.

46
00:02:20,029 --> 00:02:21,189
Those are inevitable.

47
00:02:21,659 --> 00:02:26,269
But to ensure that when a fall does
happen, the consequences are minimized.

48
00:02:26,789 --> 00:02:30,129
Now, let's explore why
redundancy is so important.

49
00:02:30,679 --> 00:02:33,819
Imagine you're on a road trip,
driving through a remote area,

50
00:02:34,049 --> 00:02:35,119
and your car breaks down.

51
00:02:35,619 --> 00:02:39,469
If you have a spare tire in the trunk,
you can swap it out and continue

52
00:02:39,479 --> 00:02:41,209
your journey with just a minor delay.

53
00:02:41,809 --> 00:02:43,589
But if you don't, you're stranded.

54
00:02:44,109 --> 00:02:46,519
In systems, redundancy works the same way.

55
00:02:46,989 --> 00:02:49,739
It's not about preventing
every possible breakdown.

56
00:02:50,089 --> 00:02:53,349
It's about ensuring the journey
doesn't stop when a failure occurs.

57
00:02:54,309 --> 00:02:57,839
Redundancy eliminates single point
of failure and ensures that critical

58
00:02:57,839 --> 00:02:59,729
operations continue without interruption.

59
00:03:00,399 --> 00:03:04,569
To fully understand how redundancy
works in practice, it helps to break

60
00:03:04,569 --> 00:03:06,269
it into three broad categories.

61
00:03:06,839 --> 00:03:11,249
Infrastructure Redundancy, Data
Redundancy, and Service Redundancy.

62
00:03:12,199 --> 00:03:15,729
Infrastructure redundancy focuses
on ensuring that your underlying

63
00:03:15,739 --> 00:03:19,829
hardware, networks, and facilities
are not single points of failure.

64
00:03:20,419 --> 00:03:24,439
In modern systems, this often means
deploying applications and services

65
00:03:24,439 --> 00:03:29,169
across multiple physical locations such
as data centers or availability zones.

66
00:03:29,809 --> 00:03:34,299
If one location goes offline due to
a power outage, natural disaster, or

67
00:03:34,299 --> 00:03:39,199
hardware failure, traffic is automatically
redirected to other operational locations.

68
00:03:39,689 --> 00:03:40,829
Consider this example.

69
00:03:41,454 --> 00:03:44,384
Suppose you're running an online
platform with millions of users.

70
00:03:44,814 --> 00:03:47,654
Hosting everything in a single
data center might seem cost

71
00:03:47,654 --> 00:03:49,614
effective, but it's also risky.

72
00:03:50,184 --> 00:03:53,944
If that data center experiences a
power outage or network failure, your

73
00:03:53,994 --> 00:03:55,774
entire platform could go offline.

74
00:03:56,414 --> 00:04:00,574
By contrast, deploying your systems
across multiple data centers ensures

75
00:04:00,864 --> 00:04:04,454
that even if one fails, users
can still access your platform

76
00:04:04,474 --> 00:04:06,114
without any noticeable disruption.

77
00:04:06,874 --> 00:04:11,054
Modern cloud platforms make infrastructure
redundancy more accessible than ever.

78
00:04:11,739 --> 00:04:16,319
Many providers offer availability zones,
which are isolated regions within a

79
00:04:16,319 --> 00:04:20,509
data center network that allow you to
distribute your services geographically.

80
00:04:21,089 --> 00:04:24,379
This means a failure in one zone
doesn't affect the others, keeping

81
00:04:24,379 --> 00:04:25,879
your services always available.

82
00:04:26,379 --> 00:04:30,859
Data redundancy is all about ensuring that
critical information remains accessible.

83
00:04:31,379 --> 00:04:35,959
Even in the face of hardware or system
failure, losing data is often more

84
00:04:35,959 --> 00:04:39,709
damaging than temporary downtime,
particularly for applications that

85
00:04:39,719 --> 00:04:43,429
handle sensitive information like
financial transactions, health

86
00:04:43,429 --> 00:04:45,079
records, or user credentials.

87
00:04:46,059 --> 00:04:50,089
The most common approach to data
redundancy is replication, which

88
00:04:50,089 --> 00:04:53,359
is storing multiple copies of your
data across different locations.

89
00:04:54,004 --> 00:04:58,784
For instance, a financial service might
replicate transactional data to two or

90
00:04:58,804 --> 00:05:00,794
more geographically separated locations.

91
00:05:01,374 --> 00:05:05,174
Even if one region fails, customer
account balances, transaction

92
00:05:05,174 --> 00:05:07,774
histories, and payment records
remain safe and accessible.

93
00:05:08,584 --> 00:05:13,604
Data redundancy can also take forms
depending on your system's needs.

94
00:05:14,144 --> 00:05:18,204
For example, synchronous replication
ensures that updates to data are

95
00:05:18,304 --> 00:05:21,574
written to multiple locations
simultaneously, maintaining

96
00:05:21,574 --> 00:05:23,444
consistency across all replicas.

97
00:05:23,964 --> 00:05:28,204
This approach is often used for mission
critical systems where even the slightest

98
00:05:28,214 --> 00:05:30,114
inconsistency could lead to issues.

99
00:05:30,664 --> 00:05:34,564
On the other hand, asynchronous
replication, which introduces a

100
00:05:34,564 --> 00:05:39,154
slight lag between writes, can be more
efficient for less sensitive use cases.

101
00:05:39,654 --> 00:05:42,804
Service redundancy focuses on
ensuring that your application

102
00:05:42,834 --> 00:05:46,884
components remain available even
when individual instances fail.

103
00:05:47,464 --> 00:05:52,024
In a microservice architecture, this
often means deploying multiple instances

104
00:05:52,064 --> 00:05:54,164
of each service behind a load balancer.

105
00:05:54,774 --> 00:05:59,454
The load balancer distributes incoming
traffic evenly among these instances,

106
00:05:59,834 --> 00:06:03,784
and if one instance becomes unhealthy,
it stops sending traffic to it.

107
00:06:04,624 --> 00:06:05,424
Here's an example.

108
00:06:06,139 --> 00:06:09,559
Imagine an online marketplace
with a search service that users

109
00:06:09,559 --> 00:06:10,999
depend on to find products.

110
00:06:11,569 --> 00:06:15,469
By running three instances of the
search service and using a load

111
00:06:15,499 --> 00:06:19,929
balancer to manage traffic, you ensure
that even if one instance crashes,

112
00:06:20,269 --> 00:06:23,649
the remaining two can continue
to handle requests seamlessly.

113
00:06:24,409 --> 00:06:27,329
Users are unlikely to notice
anything that has gone wrong.

114
00:06:28,064 --> 00:06:32,104
Service redundancy is particularly
important for high traffic systems,

115
00:06:32,474 --> 00:06:36,484
where a single failure can cause a
ripple effect across dependent services.

116
00:06:37,314 --> 00:06:40,714
Load balancing, health checks,
and failover mechanisms work

117
00:06:40,714 --> 00:06:44,284
together to ensure that service
redundancy is effective.

118
00:06:44,784 --> 00:06:49,254
While redundancy is essential for
resilience, it comes with a cost.

119
00:06:49,729 --> 00:06:53,229
Maintaining backups, replicas,
and redundant infrastructure

120
00:06:53,329 --> 00:06:56,869
means additional hardware,
storage, and operational expenses.

121
00:06:57,489 --> 00:07:01,929
However, the cost of not having
redundancy can be far greater.

122
00:07:02,449 --> 00:07:06,349
Prolonged downtime, lost customer
trust, and financial penalties.

123
00:07:06,849 --> 00:07:11,469
The key is to optimize redundancy for
critical systems and balance the level

124
00:07:11,469 --> 00:07:16,039
of redundancy with your organization's
risk tolerance and budget constraints.

125
00:07:16,834 --> 00:07:21,464
For example, some businesses opt
for hot backups, which are fully

126
00:07:21,464 --> 00:07:25,214
operational replicas ready to take
over instantly during a failure.

127
00:07:25,914 --> 00:07:29,454
Others may use cold backups,
which are cheaper to maintain

128
00:07:29,454 --> 00:07:31,164
but require more time to restore.

129
00:07:31,794 --> 00:07:35,584
Understanding your system's requirements
helps you decide the best approach.

130
00:07:36,554 --> 00:07:38,764
Let's look at redundancy in action.

131
00:07:39,354 --> 00:07:42,484
Imagine an e commerce platform
that serves users globally.

132
00:07:43,124 --> 00:07:47,214
To ensure resilience, the platform is
deployed across multiple cloud providers.

133
00:07:47,604 --> 00:07:51,964
If one provider experiences a region
wide outage, the platform can seamlessly

134
00:07:51,964 --> 00:07:53,674
continue operations on the other.

135
00:07:54,324 --> 00:07:58,744
Meanwhile, user data is replicated
across regions, ensuring that no

136
00:07:58,744 --> 00:08:00,784
single failure results in data loss.

137
00:08:01,574 --> 00:08:05,174
When implementing redundancy,
keep these best practices in mind.

138
00:08:05,774 --> 00:08:08,314
First, distribute services across regions.

139
00:08:08,914 --> 00:08:13,474
Geo redundancy ensures that localized
failures such as natural disasters

140
00:08:13,874 --> 00:08:15,674
don't take down your entire system.

141
00:08:16,314 --> 00:08:19,234
Second, use load balancers
and health checks.

142
00:08:19,774 --> 00:08:23,934
These tools monitor the health of your
services and redirect traffic away

143
00:08:23,934 --> 00:08:25,644
from failing instances automatically.

144
00:08:26,384 --> 00:08:28,814
And third, test your redundant systems.

145
00:08:29,474 --> 00:08:32,554
Backups and replicas are
only useful if they work.

146
00:08:33,224 --> 00:08:37,364
Regularly simulate failures to
validate your redundancy mechanisms.

147
00:08:37,864 --> 00:08:41,254
Redundancy ensures that backups
are available when something fails.

148
00:08:41,424 --> 00:08:43,504
But backups aren't alone enough.

149
00:08:43,884 --> 00:08:47,979
When a failure occurs, You need a way
to switch seamlessly to those backups.

150
00:08:48,429 --> 00:08:50,599
That's where failover mechanisms come in.

151
00:08:51,399 --> 00:08:54,329
Failover is the process of
switching operations from a failing

152
00:08:54,329 --> 00:08:58,459
component to a healthy one, ensuring
uninterrupted service for users.

153
00:08:58,959 --> 00:09:03,029
A seamless failover means users might not
even notice that anything has gone wrong.

154
00:09:03,919 --> 00:09:06,929
Failover is critical for
maintaining uptime and reliability,

155
00:09:07,309 --> 00:09:10,309
especially in systems with
high availability requirements.

156
00:09:11,139 --> 00:09:15,719
To better understand how Failover works,
let's explore its two main categories,

157
00:09:16,199 --> 00:09:18,489
Automatic Failover and Manual Failover.

158
00:09:19,049 --> 00:09:22,459
Along the way, we'll look at
examples of these mechanisms in

159
00:09:22,459 --> 00:09:26,439
action, including how DNS Failover
fits into the broader landscape.

160
00:09:26,939 --> 00:09:31,189
Automatic Failover is designed to handle
failures without human intervention.

161
00:09:31,769 --> 00:09:35,429
This approach relies on continuous
monitoring and predefined rules

162
00:09:35,429 --> 00:09:39,019
to detect failures and trigger
recovery processes instantly.

163
00:09:39,879 --> 00:09:44,079
It's particularly valuable for mission
critical systems where every second of

164
00:09:44,099 --> 00:09:48,359
downtime has significant repercussions,
such as real time communication

165
00:09:48,369 --> 00:09:52,099
platforms, financial transaction
systems, and e commerce platforms.

166
00:09:52,609 --> 00:09:56,379
For instance, consider a database
cluster with a single primary

167
00:09:56,379 --> 00:09:57,829
node and multiple replicas.

168
00:09:58,439 --> 00:10:02,019
If the primary node crashes,
the system's failover mechanism

169
00:10:02,039 --> 00:10:05,589
automatically promotes one of the
replicas to act as the new primary.

170
00:10:06,229 --> 00:10:10,999
This transition is nearly instantaneous,
ensuring that applications dependent

171
00:10:11,069 --> 00:10:14,129
on the database can continue to
function without interruption.

172
00:10:14,629 --> 00:10:19,404
Other examples of automatic failover
include Service level failure.

173
00:10:19,904 --> 00:10:23,004
Load balancer detect unhealthy
application instances and

174
00:10:23,004 --> 00:10:24,494
redirect traffic to healthy ones.

175
00:10:25,164 --> 00:10:29,214
This is common in microservice
architecture, where distributed services

176
00:10:29,264 --> 00:10:34,014
often have multiple replicas to ensure
availability and infrastructure failure.

177
00:10:34,604 --> 00:10:38,384
In cloud environments, automatic
failover can move workloads between

178
00:10:38,384 --> 00:10:42,304
available zones or regions if a server
or a data center becomes unavailable.

179
00:10:43,124 --> 00:10:46,474
By eliminating the need of
human intervention, automatic

180
00:10:46,494 --> 00:10:49,864
failover reduces downtime and
minimizes the risk of human error.

181
00:10:50,374 --> 00:10:54,444
However, it requires meticulous
configuration and regular testing

182
00:10:54,444 --> 00:10:56,084
to ensure it works as intended.

183
00:10:56,584 --> 00:10:59,944
Manual failover, on the other
hand, requires human involvement

184
00:10:59,954 --> 00:11:03,174
to detect failures and
initiate the switch to backups.

185
00:11:03,784 --> 00:11:08,244
While it offers more control, this
approach is slower and less reliable for

186
00:11:08,244 --> 00:11:10,364
systems with strict uptime requirements.

187
00:11:11,124 --> 00:11:15,334
Manual failover is often seen in
legacy systems or scenarios where

188
00:11:15,374 --> 00:11:19,084
automated processes might not
account for complex dependencies.

189
00:11:19,659 --> 00:11:23,499
For example, in an older database
system, administrators might need

190
00:11:23,529 --> 00:11:27,579
to manually update DNS entries
or application configurations to

191
00:11:27,639 --> 00:11:29,469
redirect traffic to a backup server.

192
00:11:29,969 --> 00:11:34,679
This approach has its place, particularly
for non critical systems where immediate

193
00:11:34,679 --> 00:11:39,569
recovery isn't essential, or in cases
where a control switch is preferable

194
00:11:39,919 --> 00:11:41,909
to avoid introducing further issues.

195
00:11:42,714 --> 00:11:46,834
However, the increased recovery
time and risk of human error make it

196
00:11:46,874 --> 00:11:49,134
unsuitable for high demand applications.

197
00:11:49,634 --> 00:11:53,985
Let's take an example of failover
in action with DNS failovers.

198
00:11:53,985 --> 00:11:59,544
DNS failover serves as an useful example
of how failure mechanism can operate

199
00:11:59,574 --> 00:12:04,294
in both automatic and manual contexts,
depending on how they are implemented.

200
00:12:05,024 --> 00:12:08,624
When a service hosted in one
region becomes unavailable, DNS

201
00:12:08,724 --> 00:12:11,814
records can be updated to redirect
traffic to a healthy region.

202
00:12:12,699 --> 00:12:16,389
In an automatic failover scenario,
health checks integrated with a

203
00:12:16,390 --> 00:12:19,599
DNS provider continuously monitor
the availability of services.

204
00:12:20,169 --> 00:12:24,669
If an outage is detected, the provider
automatically updates DNS records to

205
00:12:24,679 --> 00:12:26,529
reroute users to backup endpoints.

206
00:12:27,049 --> 00:12:30,969
This process ensures minimal
disruption, but DNS propagation

207
00:12:30,969 --> 00:12:34,484
delays can sometimes impact how
quickly all users are redirected.

208
00:12:35,334 --> 00:12:40,714
In contrast, a manual DNS failover
requires system administrators to identify

209
00:12:40,724 --> 00:12:45,764
the failure, modify DNS records manually,
and wait for those changes to propagate.

210
00:12:46,444 --> 00:12:51,124
While less ideal for highly available
systems, this approach might

211
00:12:51,124 --> 00:12:55,144
still be useful for environments
where outages are rare or control

212
00:12:55,145 --> 00:12:56,984
over redirection is critical.

213
00:12:57,484 --> 00:13:01,924
By positioning DNS failover as an
example, we emphasize that it's

214
00:13:01,934 --> 00:13:05,394
not a distinct type of failure, but
rather a mechanism that supports

215
00:13:05,434 --> 00:13:07,634
either automatic or manual processes.

216
00:13:08,134 --> 00:13:11,944
While failover mechanisms are
indispensable for system resilience,

217
00:13:12,274 --> 00:13:15,984
implementing them effectively involves
overcoming several challenges.

218
00:13:16,759 --> 00:13:20,239
Addressing these challenges requires
a combination of thoughtful design,

219
00:13:20,659 --> 00:13:22,949
proactive testing, and robust automation.

220
00:13:23,499 --> 00:13:24,949
Here's how you can tackle them.

221
00:13:25,689 --> 00:13:27,079
Latency during failover.

222
00:13:27,849 --> 00:13:31,899
Failover transitions, especially in
global systems, can introduce latency.

223
00:13:32,349 --> 00:13:35,799
For instance, DNS propagation
delays or database replication

224
00:13:35,809 --> 00:13:37,419
lags may slow down the process.

225
00:13:38,094 --> 00:13:42,504
To minimize latency, consider optimizing
your DNS settings with low time to

226
00:13:42,504 --> 00:13:46,674
live values and using synchronous
replication for critical data.

227
00:13:47,644 --> 00:13:52,174
Data Consistency Switching to backup
components, such as replicas, can

228
00:13:52,204 --> 00:13:56,444
create temporary inconsistencies,
especially in real time systems.

229
00:13:57,174 --> 00:14:01,044
To mitigate this, use replication
strategies that balance performance

230
00:14:01,044 --> 00:14:04,554
and consistency, such as synchronous
replication for sensitive

231
00:14:04,554 --> 00:14:08,314
operations and asynchronous
replications for non critical data.

232
00:14:08,814 --> 00:14:13,364
Testing Failover Scenarios A failover
mechanism that hasn't been tested

233
00:14:13,364 --> 00:14:15,164
might fail when you need it the most.

234
00:14:15,964 --> 00:14:20,274
Regularly simulate failover events to
validate that backups work as intended.

235
00:14:20,834 --> 00:14:25,774
For example, you could simulate the
failover of a primary database to observe

236
00:14:25,774 --> 00:14:27,264
how quickly and seamlessly it works.

237
00:14:27,974 --> 00:14:31,424
Seamlessly replicas take over
human error in manual failure.

238
00:14:32,374 --> 00:14:36,484
Manual failure processes are prone to
mistakes, especially when under pressure

239
00:14:36,984 --> 00:14:41,494
to reduce the risk of human error, clearly
document failure procedures, and provide

240
00:14:41,524 --> 00:14:45,964
training for operations teams better
yet automate as much of the failure

241
00:14:46,004 --> 00:14:48,904
process as possible false positives.

242
00:14:49,404 --> 00:14:53,774
Sometimes, failover mechanisms might
trigger unnecessarily, redirecting

243
00:14:53,814 --> 00:14:56,854
traffic even when the primary
component is still functional.

244
00:14:57,494 --> 00:15:00,834
Use robust health checks and
monitoring tools to reduce false

245
00:15:00,834 --> 00:15:04,914
positives, ensuring that failover
occurs only when genuinely needed.

246
00:15:05,904 --> 00:15:07,594
Observability and monitoring.

247
00:15:08,339 --> 00:15:10,639
Monitoring is crucial
for effective failover.

248
00:15:11,519 --> 00:15:17,709
Use observability tools to track component
health, detect failures early, and

249
00:15:17,719 --> 00:15:19,889
gain visibility into failover events.

250
00:15:20,469 --> 00:15:23,899
Real time dashboards and alerting
systems can help teams respond

251
00:15:23,909 --> 00:15:27,999
faster and refine failover
processes based on historical data.

252
00:15:28,954 --> 00:15:32,424
By combining these practices with
proactive monitoring and regular

253
00:15:32,454 --> 00:15:36,594
validation, you can build failure
mechanisms that are not only reliable,

254
00:15:36,974 --> 00:15:40,804
but also agile enough to handle
the complexities of modern systems.

255
00:15:41,304 --> 00:15:45,564
The next strategy focuses on mitigating
the impact of failures when they occur.

256
00:15:46,154 --> 00:15:49,314
Not every failure needs to
result in catastrophic outage.

257
00:15:49,804 --> 00:15:53,444
With graceful degradation, your
system continues to function,

258
00:15:53,874 --> 00:15:55,904
albeit with reduced capabilities.

259
00:15:56,604 --> 00:15:59,954
Not every failure has to result
in an all or nothing scenario.

260
00:16:00,364 --> 00:16:05,054
In fact, one of the hallmarks of a
well designed system is its ability to

261
00:16:05,054 --> 00:16:09,324
continue operating even when some of
its parts aren't working as expected.

262
00:16:10,039 --> 00:16:14,609
Graceful degradation is a strategy
that makes this possible by focusing

263
00:16:14,609 --> 00:16:18,719
on preserving core functionality while
temporarily limiting or disabling

264
00:16:18,719 --> 00:16:22,849
non essential features, you ensure
that users can still rely on your

265
00:16:22,849 --> 00:16:25,319
service even during challenging times.

266
00:16:26,079 --> 00:16:29,349
Think of it as a dimmer switch
rather than an on off button.

267
00:16:30,119 --> 00:16:33,699
Instead of plunging users into
complete darkness when something

268
00:16:33,699 --> 00:16:36,089
goes wrong, you reduce the intensity.

269
00:16:36,429 --> 00:16:40,239
Keeping things functional while working
behind the scenes to restore full service.

270
00:16:40,779 --> 00:16:44,929
This approach helps maintain trust
and usability, even when resources

271
00:16:44,989 --> 00:16:46,759
are constrained or dependencies fail.

272
00:16:47,259 --> 00:16:50,899
Graceful degradation allows the
system to adapt dynamically to

273
00:16:50,899 --> 00:16:54,959
disruptions, prioritizing essential
functionality while scaling back

274
00:16:55,279 --> 00:16:57,599
or disabling non critical features.

275
00:16:58,369 --> 00:17:02,869
This ensures that users can continue
interacting with the system, albeit

276
00:17:02,869 --> 00:17:04,619
with a slightly diminished experience.

277
00:17:05,569 --> 00:17:07,719
Take, for instance, a
video streaming platform.

278
00:17:08,339 --> 00:17:12,449
If its infrastructure faces bandwidth
constraints, it might automatically

279
00:17:12,569 --> 00:17:16,009
reduce the streaming resolution
to standard definition instead of

280
00:17:16,049 --> 00:17:17,859
cutting off the stream entirely.

281
00:17:18,729 --> 00:17:22,819
Users can still watch their content,
even if it's not in full high definition.

282
00:17:23,319 --> 00:17:28,969
Similarly, an online retailer might handle
an outage in its recommendation engine by

283
00:17:28,969 --> 00:17:33,249
continuing to display the product catalog,
but without personalized suggestions.

284
00:17:33,849 --> 00:17:37,619
Customers can still browse and
shop without realizing that part

285
00:17:37,619 --> 00:17:39,009
of the system is under strain.

286
00:17:39,699 --> 00:17:44,329
At its core, graceful degradation
is about designing systems that can

287
00:17:44,359 --> 00:17:49,129
tolerate partial failures and deliver
value in less than perfect conditions.

288
00:17:49,629 --> 00:17:53,489
To implement graceful degradation
effectively, you need to decide how

289
00:17:53,489 --> 00:17:57,559
individual components of your system
should behave when they encounter issues.

290
00:17:58,169 --> 00:18:02,939
This boils down to two main
approaches, fail open and fail close.

291
00:18:03,439 --> 00:18:06,349
For fail open, the system
continues operating.

292
00:18:06,879 --> 00:18:08,689
Even if some parts are degraded.

293
00:18:09,159 --> 00:18:13,819
For example, a ride sharing app might
allow users to book rides even if its

294
00:18:13,829 --> 00:18:15,919
real time pricing engine is unavailable.

295
00:18:16,479 --> 00:18:20,879
In such cases, fallback mechanisms
could display default or cash

296
00:18:20,909 --> 00:18:23,299
pricing data and fail close.

297
00:18:24,009 --> 00:18:28,349
The system halts certain operations
entirely to prevent errors or risks.

298
00:18:28,839 --> 00:18:33,409
For example, a banking system might
stop processing payments if its fraud

299
00:18:33,409 --> 00:18:37,419
detection module fails, avoiding
potential financial liabilities.

300
00:18:37,429 --> 00:18:37,489
Thank you.

301
00:18:38,309 --> 00:18:41,509
These decisions depend on the
criticality of the features.

302
00:18:42,169 --> 00:18:46,279
Non critical components should typically
fail open to preserve user experience,

303
00:18:46,629 --> 00:18:50,349
while mission critical or sensitive
components might need to fail close

304
00:18:50,599 --> 00:18:52,769
to avoid unintended consequences.

305
00:18:53,699 --> 00:18:57,119
From a user's perspective,
graceful degradation is often

306
00:18:57,289 --> 00:19:01,279
indistinguishable from normal
operation, provided it's executed well.

307
00:19:01,779 --> 00:19:05,569
Users are more likely to accept
limitations if they are presented with

308
00:19:05,609 --> 00:19:10,849
clear, actionable communication about
what's happening and what to expect.

309
00:19:11,349 --> 00:19:16,539
For instance, a document editing platform
might display a message saying, Real time

310
00:19:16,539 --> 00:19:20,549
collaboration is currently unavailable,
but you can continue editing offline.

311
00:19:21,019 --> 00:19:24,999
Changes will sync when the service
is restored, or an online food

312
00:19:25,009 --> 00:19:28,979
delivery app might notify users that
live order tracking is temporarily

313
00:19:29,009 --> 00:19:32,669
disabled, but provide regular
SMS updates as an alternative.

314
00:19:33,169 --> 00:19:37,379
This transparency fosters trust,
showing users that while the system is

315
00:19:37,379 --> 00:19:39,549
under stress, it remains dependable.

316
00:19:40,049 --> 00:19:45,319
Designing systems for graceful degradation
comes with its own set of challenges, but

317
00:19:45,329 --> 00:19:49,499
these can be addressed effectively through
careful planning and best practices.

318
00:19:49,999 --> 00:19:51,559
First, Dependency Mapping.

319
00:19:52,199 --> 00:19:56,449
Complex systems often have hidden
dependencies, and a failure in one

320
00:19:56,449 --> 00:19:58,359
service can cascade into others.

321
00:19:59,019 --> 00:20:02,349
Regularly analyze and map
these dependencies to identify

322
00:20:02,379 --> 00:20:03,569
potential weak points.

323
00:20:04,099 --> 00:20:08,519
This enables you to design fallback
mechanisms for critical services, and

324
00:20:08,519 --> 00:20:12,969
ensure that non critical features degrade
without affecting the entire system.

325
00:20:13,469 --> 00:20:15,299
Second, performance optimization.

326
00:20:15,949 --> 00:20:18,879
Degraded modes may consume
additional resources.

327
00:20:19,409 --> 00:20:23,179
For example, serving cached
responses or processing fallback

328
00:20:23,179 --> 00:20:25,349
operations can increase system load.

329
00:20:25,879 --> 00:20:30,099
Optimizing these processes to ensure that
the act of degradation doesn't create

330
00:20:30,119 --> 00:20:32,549
new bottlenecks or performance issues.

331
00:20:33,329 --> 00:20:34,959
Third, user communication.

332
00:20:35,459 --> 00:20:39,849
Poor communication during degradation
can leave users confused and frustrated.

333
00:20:40,479 --> 00:20:44,329
Clear, actionable messaging
helps users understand what's

334
00:20:44,329 --> 00:20:46,149
happening and how it impacts them.

335
00:20:46,859 --> 00:20:51,759
For example, instead of a generic error
message, a notification like this feature

336
00:20:51,759 --> 00:20:57,469
is temporarily unavailable, core services
remain unaffected, reassures users and

337
00:20:57,469 --> 00:20:59,379
maintains their confidence in your system.

338
00:21:00,159 --> 00:21:02,489
For testing degraded states.

339
00:21:03,269 --> 00:21:07,979
Systems often fail to perform well in
degraded states simply because those

340
00:21:07,989 --> 00:21:10,229
states haven't been adequately tested.

341
00:21:10,929 --> 00:21:15,329
Use chaos engineering techniques
to simulate failures and validate

342
00:21:15,329 --> 00:21:17,019
your system's fallback mechanisms.

343
00:21:17,809 --> 00:21:21,849
Testing under controlled conditions
ensures that degraded modes function

344
00:21:21,909 --> 00:21:24,219
as intended when real failures occur.

345
00:21:24,719 --> 00:21:25,999
Fifth, plan degradation early.

346
00:21:26,499 --> 00:21:29,899
Incorporate graceful degradation
into the system's design phase.

347
00:21:30,399 --> 00:21:33,909
Identify which features are
critical and which can be de

348
00:21:33,909 --> 00:21:35,289
prioritized during failures.

349
00:21:36,019 --> 00:21:40,439
Establish clear rules for what should
fail open and what should fail close.

350
00:21:40,939 --> 00:21:42,839
Sixth, leverage observability.

351
00:21:43,339 --> 00:21:46,599
Observability tools, such as
monitoring dashboards and alerting

352
00:21:46,599 --> 00:21:50,519
systems, play a critical role in
enabling graceful degradation.

353
00:21:51,139 --> 00:21:54,469
By detecting failures early,
they allow systems to trigger

354
00:21:54,479 --> 00:21:58,549
fallback mechanisms proactively,
minimizing the impact on users.

355
00:21:59,169 --> 00:22:01,409
And seventh, iterate and improve.

356
00:22:02,209 --> 00:22:05,759
Systems evolve, and so should
their degradation strategies.

357
00:22:06,429 --> 00:22:10,779
Continuously refine these mechanisms
based on real world incidents, user

358
00:22:10,779 --> 00:22:13,179
feedback, and evolving business needs.

359
00:22:13,964 --> 00:22:17,974
Regular iteration ensures that your system
remains prepared for new challenges.

360
00:22:18,474 --> 00:22:22,734
Sometimes, systems need to be
taken offline, whether it's planned

361
00:22:22,744 --> 00:22:26,704
maintenance, software upgrades,
or to address an unexpected issue.

362
00:22:27,504 --> 00:22:31,724
When this happens, the way a system
shuts down can make all the difference.

363
00:22:32,264 --> 00:22:33,304
An abrupt shutdown?

364
00:22:33,774 --> 00:22:35,044
That's a recipe for trouble.

365
00:22:35,824 --> 00:22:39,569
It can lead to data loss,
Interrupted transactions and a

366
00:22:39,569 --> 00:22:41,169
whole lot of frustrated users.

367
00:22:41,759 --> 00:22:45,799
In some cases, it can even cause
more damage than the problem you were

368
00:22:45,799 --> 00:22:47,299
trying to fix in the first place.

369
00:22:48,109 --> 00:22:50,609
Now, compare that with graceful shutdown.

370
00:22:51,029 --> 00:22:55,049
A graceful shutdown ensures that the
system transitions apply smoothly.

371
00:22:55,979 --> 00:23:00,449
Ongoing processes are completed, data
is saved, and dependent systems are

372
00:23:00,449 --> 00:23:02,769
notified before the services go offline.

373
00:23:03,269 --> 00:23:06,939
It's about taking control of the
shutdown process to protect system's

374
00:23:06,939 --> 00:23:08,369
integrity and maintain trust.

375
00:23:08,869 --> 00:23:10,699
Think of it like parking a car.

376
00:23:11,349 --> 00:23:14,619
You wouldn't just jump out of the
car while car's still moving, right?

377
00:23:15,289 --> 00:23:19,179
You'd slow down, park it properly,
and make sure everything is

378
00:23:19,179 --> 00:23:20,529
secure before walking away.

379
00:23:21,159 --> 00:23:25,079
A well executed, graceful shutdown
does exactly that for your system.

380
00:23:25,579 --> 00:23:26,999
So why is this such a big deal?

381
00:23:27,939 --> 00:23:32,844
At its core, it's A graceful shutdown
is about protecting data and maintaining

382
00:23:32,844 --> 00:23:34,154
the integrity of your system.

383
00:23:34,934 --> 00:23:35,984
Let's take an example.

384
00:23:36,884 --> 00:23:41,514
Imagine a payment processing service that
needs to be temporarily taken offline.

385
00:23:42,444 --> 00:23:45,804
If the shutdown isn't handled
properly, you could end up with

386
00:23:45,814 --> 00:23:47,444
transactions left incomplete.

387
00:23:48,094 --> 00:23:52,324
Customers might get charged multiple
times, or worse, not at all,

388
00:23:52,934 --> 00:23:56,354
leading to financial discrepancies
and user dissatisfaction.

389
00:23:56,854 --> 00:24:00,564
A graceful shutdown ensures that all
ongoing transactions are completed,

390
00:24:01,014 --> 00:24:05,784
session data is saved, and pending
requests are properly queued or deferred.

391
00:24:06,574 --> 00:24:09,834
It's about maintaining
order, even during downtime.

392
00:24:10,334 --> 00:24:13,404
And in distributed systems,
the stakes are even higher.

393
00:24:14,139 --> 00:24:18,809
Components are often dependent
on one or another to function.

394
00:24:19,809 --> 00:24:23,829
If one service suddenly stops accepting
requests without notifying the rest of

395
00:24:23,829 --> 00:24:26,619
the system, it can cause cascading issues.

396
00:24:27,199 --> 00:24:30,379
Think timeouts, errors, or
even system wide failures.

397
00:24:31,049 --> 00:24:34,169
A graceful shutdown prevents
this by coordinating with all

398
00:24:34,169 --> 00:24:35,599
the relevant parts of the system.

399
00:24:36,099 --> 00:24:39,559
Now, how do you actually
perform a graceful shutdown?

400
00:24:40,259 --> 00:24:45,189
It's all about implementing strategies
that prioritize ongoing processes, notify

401
00:24:45,189 --> 00:24:47,349
dependencies, and protect your data.

402
00:24:48,139 --> 00:24:50,159
Let's walk through a few key techniques.

403
00:24:50,859 --> 00:24:52,829
Firstly, there's connection training.

404
00:24:53,744 --> 00:24:55,884
This is one of the most
important techniques.

405
00:24:56,364 --> 00:25:00,734
It means allowing active requests to
complete while rejecting new ones.

406
00:25:01,234 --> 00:25:04,724
For instance, imagine a web
server that's about to shut down.

407
00:25:05,464 --> 00:25:09,794
Instead of cutting off all traffic
immediately, it stops accepting new

408
00:25:09,794 --> 00:25:14,004
HTTP requests, but continues processing
the ones that are already in progress.

409
00:25:14,774 --> 00:25:17,844
Once those active connections
are resolved, the server

410
00:25:17,844 --> 00:25:19,274
can safely go offline.

411
00:25:19,834 --> 00:25:22,374
This ensures no requests are left hanging.

412
00:25:23,089 --> 00:25:25,589
Next, we have Health Check Signaling.

413
00:25:26,309 --> 00:25:29,829
This is where your system lets other
components know it's shutting down.

414
00:25:30,379 --> 00:25:34,529
For example, when a service begins
a shutdown process, it can update

415
00:25:34,599 --> 00:25:38,989
its health status to tell upstream
load balancers or dependent services

416
00:25:39,289 --> 00:25:40,689
that it's no longer available.

417
00:25:41,339 --> 00:25:46,889
In a Kubernetes environment, readiness
proofs are commonly used for this purpose.

418
00:25:47,389 --> 00:25:51,379
These probes signal to the load
balancer to stop routing traffic to

419
00:25:51,599 --> 00:25:55,719
the shutting down port, ensuring that
no new requests are sent its way.

420
00:25:56,639 --> 00:25:58,769
Then, there's state persistence.

421
00:25:59,479 --> 00:26:04,039
Before shutting down, a service must
ensure that all of its data, whether it's

422
00:26:04,069 --> 00:26:06,554
session data, logs, transactions, etc.

423
00:26:07,054 --> 00:26:10,584
This is especially important
for systems that handle critical

424
00:26:10,674 --> 00:26:11,994
or sensitive information.

425
00:26:12,664 --> 00:26:17,044
For example, a messaging service
should persist any unsent messages to a

426
00:26:17,044 --> 00:26:19,784
database or queue before going offline.

427
00:26:20,344 --> 00:26:23,074
This way, no data gets
lost in the process.

428
00:26:23,744 --> 00:26:26,574
Finally, We have timeouts
and grace periods.

429
00:26:27,424 --> 00:26:30,394
Sometimes tasks need a bit
of extra time to finish.

430
00:26:30,934 --> 00:26:35,534
By configuring reasonable timeouts,
you can ensure that processes don't

431
00:26:35,564 --> 00:26:37,644
terminate prematurely during a shutdown.

432
00:26:38,374 --> 00:26:41,084
However, it's important
to strike a balance.

433
00:26:41,704 --> 00:26:46,424
Too long a timeout and you risk delaying
the shutdown process unnecessarily.

434
00:26:47,174 --> 00:26:51,184
Analyze your typical request completion
times to set reasonable time limits.

435
00:26:51,684 --> 00:26:55,574
Of course, graceful shutdown comes
with its own set of challenges.

436
00:26:56,234 --> 00:26:59,144
Let's talk about those and how
to address them effectively.

437
00:26:59,644 --> 00:27:02,574
Firstly, there's the issue
of managing dependencies.

438
00:27:03,454 --> 00:27:07,684
In distributed systems, services
often rely on one another to function.

439
00:27:08,254 --> 00:27:12,404
When one service shuts down, it must
notify all upstream and downstream

440
00:27:12,404 --> 00:27:14,304
components to ensure smooth transition.

441
00:27:14,854 --> 00:27:18,694
For instance, if a database is shutting
down for maintenance, it should signal

442
00:27:18,714 --> 00:27:23,144
the application servers to pause their
queues or switch to a backup database.

443
00:27:23,814 --> 00:27:26,354
This coordination prevents
errors and interruptions.

444
00:27:27,129 --> 00:27:30,389
Next, we have the challenge
of testing shutdown scenarios.

445
00:27:31,199 --> 00:27:34,269
You can't just assume your shutdown
process will work perfectly.

446
00:27:34,799 --> 00:27:35,959
It needs to be tested.

447
00:27:36,559 --> 00:27:40,569
Simulate shutdowns in staging environments
to ensure that techniques like connection

448
00:27:40,569 --> 00:27:44,929
draining, health check signaling, and
state persistence work as intended.

449
00:27:45,629 --> 00:27:48,799
Think of this as running a dress
rehearsal before the real show.

450
00:27:49,729 --> 00:27:51,989
Then, there's timeouts.

451
00:27:52,609 --> 00:27:55,349
While it's critical to give
tasks enough time to complete,

452
00:27:55,849 --> 00:27:59,419
Excessively long timeouts can
unnecessarily delay the shutdown.

453
00:28:00,059 --> 00:28:04,419
To address this, analyze typical
request durations and set timeouts

454
00:28:04,439 --> 00:28:06,569
that balance efficiency with safety.

455
00:28:07,069 --> 00:28:11,309
Automation tools can also play a big
role in overcoming these challenges.

456
00:28:11,809 --> 00:28:14,599
Finally, You need to
plan for the unexpected.

457
00:28:15,289 --> 00:28:19,109
Even the best late plans can
encounter issues during a shutdown.

458
00:28:19,819 --> 00:28:22,939
That's why it's important to
implement safeguards, like

459
00:28:22,939 --> 00:28:26,799
retries for failed operations or
fallbacks to prevent data loss.

460
00:28:27,479 --> 00:28:31,529
For instance, if a service fails to
save session data during a shutdown,

461
00:28:31,879 --> 00:28:35,899
it could retry the operation or
log the issue for later resolution.

462
00:28:36,399 --> 00:28:39,649
Let's bring life to this with
some real world examples.

463
00:28:40,149 --> 00:28:43,669
Take a ride hailing platform
that processes millions of

464
00:28:43,699 --> 00:28:45,139
payment transactions every day.

465
00:28:46,129 --> 00:28:50,399
If the payment service needs to be
taken offline for maintenance, a

466
00:28:50,399 --> 00:28:55,139
graceful shutdown ensures that ongoing
transactions are completed before the

467
00:28:55,139 --> 00:28:59,649
service stops, load balances stop sending
new traffic to the payment service,

468
00:29:00,199 --> 00:29:04,739
any unprocessed payments are saved to
persistent queue for later handling.

469
00:29:05,239 --> 00:29:08,919
Another example is a video streaming
platform performing a rolling

470
00:29:08,919 --> 00:29:10,539
update to its back end service.

471
00:29:11,349 --> 00:29:14,959
During the update, each server
undergoing maintenance stops

472
00:29:14,959 --> 00:29:19,469
accepting new requests, but processes
any active stream to completion.

473
00:29:20,079 --> 00:29:24,619
It also saves session data, so users
don't lose their place in the video.

474
00:29:25,399 --> 00:29:29,329
This allows the platform to update
its infrastructure seamlessly

475
00:29:29,619 --> 00:29:31,009
without disrupting viewers.

476
00:29:31,509 --> 00:29:35,669
Building a resilient system isn't
just about adding redundancy or

477
00:29:35,679 --> 00:29:37,389
implementing failover mechanisms.

478
00:29:38,139 --> 00:29:42,049
It's about making sure those mechanisms
actually work when they are needed.

479
00:29:42,849 --> 00:29:47,029
And let's be honest, there's a big
difference between theory and practice.

480
00:29:47,909 --> 00:29:50,399
This is where chaos
engineering comes into play.

481
00:29:50,899 --> 00:29:55,299
Chaos engineering is a discipline that
helps uncover vulnerabilities in your

482
00:29:55,299 --> 00:29:59,439
system by intentionally introducing
failures in a controlled manner.

483
00:30:00,259 --> 00:30:04,129
The idea is to identify weaknesses
and address them before they

484
00:30:04,129 --> 00:30:05,279
happen in the real world.

485
00:30:05,979 --> 00:30:08,759
It's a proactive approach
that prepares your system to

486
00:30:08,759 --> 00:30:10,399
handle disruptions gracefully.

487
00:30:10,899 --> 00:30:14,459
Think of chaos engineering as the
fire drill for your infrastructure.

488
00:30:15,209 --> 00:30:19,099
Just like fire drills train
people to respond to emergencies,

489
00:30:19,589 --> 00:30:23,289
chaos experiments train your
systems to handle the unexpected.

490
00:30:24,109 --> 00:30:28,319
The goal isn't to create failure for
failure's sake, but to understand

491
00:30:28,329 --> 00:30:32,719
how your system behaves under stress
and ensure it recovers gracefully,

492
00:30:33,069 --> 00:30:35,029
minimizing any impact on users.

493
00:30:35,529 --> 00:30:37,749
Let's take a closer look
at chaos engineering.

494
00:30:38,249 --> 00:30:42,189
At its core, chaos engineering
is the practice of deliberately

495
00:30:42,219 --> 00:30:45,779
injecting failures into your
system to test its resilience.

496
00:30:46,279 --> 00:30:48,149
It operates on a simple principle.

497
00:30:48,769 --> 00:30:52,249
If you don't test for failure,
you'll only discover weaknesses

498
00:30:52,279 --> 00:30:56,049
when it's too late, when the system
has already failed in production.

499
00:30:56,549 --> 00:31:01,159
By simulating real world disruptions,
Chaos Engineering allows teams to

500
00:31:01,159 --> 00:31:05,109
proactively identify and address
vulnerabilities, improving the

501
00:31:05,139 --> 00:31:07,069
overall reliability of your systems.

502
00:31:07,679 --> 00:31:11,429
For example, you might simulate
the failure of a database server,

503
00:31:11,759 --> 00:31:16,189
introduce network latency between
the services, or randomly terminate

504
00:31:16,189 --> 00:31:17,869
instances in a cloud environment.

505
00:31:18,649 --> 00:31:23,059
These experiments reveal how your system
responds to stress, whether failure

506
00:31:23,269 --> 00:31:28,914
mechanisms work as intended, Or where
bottlenecks or weaknesses might exist.

507
00:31:29,414 --> 00:31:33,234
Chaos engineering involves three
main techniques that help you design

508
00:31:33,284 --> 00:31:35,424
and execute experiments effectively.

509
00:31:35,924 --> 00:31:37,384
First, failure injection.

510
00:31:38,094 --> 00:31:41,104
Failure injection is the
cornerstone of QoS engineering.

511
00:31:41,964 --> 00:31:46,224
This is where you introduce controlled
disruption into your system to observe

512
00:31:46,224 --> 00:31:48,674
its behavior and evaluate its resilience.

513
00:31:49,354 --> 00:31:55,144
These disruptions can take various forms
like service termination, You might

514
00:31:55,144 --> 00:32:00,034
simulate the sudden failure of a service
or service instance network partitioning.

515
00:32:00,604 --> 00:32:04,154
You can create artificial communication
breakdowns between services to

516
00:32:04,154 --> 00:32:08,054
simulate scenarios like isolated
data centers or connectivity

517
00:32:08,054 --> 00:32:10,594
issues or resource exhaustion.

518
00:32:11,144 --> 00:32:15,844
This involves simulating high CPU or
memory usage to understand how your

519
00:32:15,844 --> 00:32:17,584
system handles capacity constraints.

520
00:32:18,084 --> 00:32:20,064
Second, controlled experiments.

521
00:32:20,564 --> 00:32:24,774
Chaos engineering isn't about randomly
breaking things and hoping for the best.

522
00:32:25,364 --> 00:32:28,254
It's about conducting
carefully designed experiments.

523
00:32:29,004 --> 00:32:32,584
Controlled experiments ensure
that failures are introduced in

524
00:32:32,584 --> 00:32:36,524
a way that minimizes risk while
providing valuable insights.

525
00:32:37,024 --> 00:32:38,754
Third, resilience metrics.

526
00:32:39,254 --> 00:32:42,454
Measuring the outcomes of
chaos experiments is just as

527
00:32:42,454 --> 00:32:43,884
important as running them.

528
00:32:44,664 --> 00:32:48,924
Resilience metrics help you to assess how
well your system can recover from failures

529
00:32:49,224 --> 00:32:51,314
and identify areas of improvement.

530
00:32:51,814 --> 00:32:54,904
Key metrics include mean time to recovery.

531
00:32:55,574 --> 00:32:59,334
This measures how quickly your system
returns to normal after a failure.

532
00:32:59,884 --> 00:33:02,924
A lower MTTR indicates better resilience.

533
00:33:03,834 --> 00:33:04,784
Second, error rate.

534
00:33:05,284 --> 00:33:08,134
Track what percentage of requests
fail during the experiment.

535
00:33:08,614 --> 00:33:12,414
Understanding these error patterns helps
to refine your recovery strategies.

536
00:33:13,194 --> 00:33:14,334
And third, user impact.

537
00:33:15,004 --> 00:33:17,754
How many users are
affected and how severely?

538
00:33:18,464 --> 00:33:21,114
This metric helps balance
technical resilience with

539
00:33:21,144 --> 00:33:22,674
the overall user experience.

540
00:33:23,174 --> 00:33:24,824
So why go through all these efforts?

541
00:33:25,324 --> 00:33:28,244
Chaos engineering isn't
just about finding problems.

542
00:33:28,804 --> 00:33:32,744
It's about building confidence in your
system's ability to handle adversity.

543
00:33:33,589 --> 00:33:37,749
When done right, Chaos Engineering
offers several important benefits.

544
00:33:38,729 --> 00:33:40,229
Proactive risk mitigation.

545
00:33:40,729 --> 00:33:44,469
By identifying and addressing
vulnerabilities early, you reduce the

546
00:33:44,479 --> 00:33:46,689
likelihood of user facing disruptions.

547
00:33:47,499 --> 00:33:48,939
Faster incident response.

548
00:33:49,649 --> 00:33:54,529
Your team gains deeper understanding of
system behavior during failures, enabling

549
00:33:54,529 --> 00:33:56,489
quicker and more effective responses.

550
00:33:56,989 --> 00:33:58,429
And improved system design.

551
00:33:59,279 --> 00:34:02,829
Insights from Kiosk experiments
often lead to better architecture,

552
00:34:03,229 --> 00:34:06,749
more robust recovery processes
and stronger failover mechanisms.

553
00:34:07,249 --> 00:34:10,589
Of course, Kiosk engineering
comes with its own challenges.

554
00:34:11,289 --> 00:34:14,559
Let's talk about some of the common
hurdles and how to address them.

555
00:34:15,499 --> 00:34:17,359
First, managing risk.

556
00:34:18,019 --> 00:34:21,569
Introducing failure into a
production system can feel risky.

557
00:34:22,349 --> 00:34:26,299
Poorly designed experiments might
even cause outages, which defeats

558
00:34:26,299 --> 00:34:27,769
the purpose of chaos engineering.

559
00:34:28,449 --> 00:34:29,159
The solution?

560
00:34:29,629 --> 00:34:30,289
Start small.

561
00:34:31,259 --> 00:34:34,169
Begin with experiments in
staging environments or on

562
00:34:34,179 --> 00:34:35,449
non critical components.

563
00:34:36,419 --> 00:34:40,019
And when you move to production,
always limit the blast radius to

564
00:34:40,019 --> 00:34:41,349
minimize the potential impact.

565
00:34:42,079 --> 00:34:44,969
Next, lack of observability.

566
00:34:45,449 --> 00:34:48,949
Chaos experiments are only as
effective as the insights they provide.

567
00:34:49,719 --> 00:34:53,359
Without robust monitoring, it's hard
to measure the impact of failures or

568
00:34:53,359 --> 00:34:55,339
determine how well your system recovered.

569
00:34:56,179 --> 00:34:58,989
To fix this, invest in
observability tools.

570
00:34:59,419 --> 00:35:04,069
These tools provide detailed metrics,
logs, and traces so you can track

571
00:35:04,099 --> 00:35:07,739
recovery time, error rates, and
user impact during experiments.

572
00:35:08,389 --> 00:35:10,839
Finally, Scaling kiosk engineering.

573
00:35:11,649 --> 00:35:16,329
As your systems grow, running kiosk
experiments across multiple services and

574
00:35:16,329 --> 00:35:18,189
environments can become more challenging.

575
00:35:19,079 --> 00:35:20,439
The solution is automation.

576
00:35:21,029 --> 00:35:25,359
Tools and frameworks like kiosk mesh
can help scale failure injection

577
00:35:25,389 --> 00:35:27,389
consistently across your infrastructure.

578
00:35:27,889 --> 00:35:31,809
In distributed systems, One small
failure can quickly snowball

579
00:35:31,809 --> 00:35:33,229
into a much larger problem.

580
00:35:33,879 --> 00:35:37,619
Imagine a situation where one
service becomes unresponsive.

581
00:35:38,319 --> 00:35:42,209
Its dependent services might keep
sending requests, overwhelming the

582
00:35:42,209 --> 00:35:46,279
failing service and creating a chain
reaction that spreads across the system.

583
00:35:47,279 --> 00:35:51,809
This is what we call a cascading failure,
and it's something every resilient

584
00:35:51,809 --> 00:35:53,479
system must be designed to prevent.

585
00:35:53,979 --> 00:35:55,929
This is where circuit breakers come in.

586
00:35:56,819 --> 00:36:02,729
Circuit breakers are the critical design
pattern that act as safeguards, isolating

587
00:36:02,729 --> 00:36:06,239
failing components and protecting the
rest of the system from their impact.

588
00:36:07,059 --> 00:36:10,379
If you're wondering what this
looks like, think of a circuit

589
00:36:10,379 --> 00:36:12,249
breaker in an electrical system.

590
00:36:13,139 --> 00:36:14,859
When the circuit gets overloaded.

591
00:36:15,489 --> 00:36:19,889
The breaker trips and cuts off the flow
of electricity to prevent further damage.

592
00:36:20,579 --> 00:36:24,039
In distributed systems, circuit
breakers work the same way.

593
00:36:24,759 --> 00:36:29,189
They temporarily break the connection
to failing services, allowing those

594
00:36:29,189 --> 00:36:32,969
services to recover while ensuring
the overall system stays operational.

595
00:36:33,469 --> 00:36:36,199
Let's look at how circuit
breakers actually work.

596
00:36:36,899 --> 00:36:40,579
They monitor the interactions
between services and take action

597
00:36:40,589 --> 00:36:44,479
when they detect repeated failures
or signs of degraded performance.

598
00:36:45,139 --> 00:36:47,899
This behavior can be broken
down into three states.

599
00:36:48,509 --> 00:36:49,859
First, closed state.

600
00:36:50,529 --> 00:36:53,849
In the closed state, the circuit
breakers allows all requests

601
00:36:53,849 --> 00:36:55,049
to flow through normally.

602
00:36:55,694 --> 00:36:59,934
It keeps an eye on the success and
failure rates of requests, constantly

603
00:36:59,944 --> 00:37:01,194
checking for signs of trouble.

604
00:37:01,884 --> 00:37:04,044
Everything functions
as usual in this state.

605
00:37:04,544 --> 00:37:05,814
Second, open state.

606
00:37:06,614 --> 00:37:11,414
If the failure rate exceeds the predefined
threshold, maybe too many requests are

607
00:37:11,414 --> 00:37:16,094
timing out or returning errors, the
circuit breaker trips into the open state.

608
00:37:16,744 --> 00:37:20,064
At this point, it blocks any
further requests to the failing

609
00:37:20,064 --> 00:37:23,964
service, preventing additional
strain and allowing the service

610
00:37:23,974 --> 00:37:25,364
some breathing room to recover.

611
00:37:25,374 --> 00:37:28,094
Thank you Third, half open state.

612
00:37:28,944 --> 00:37:32,254
After a cooldown period, the
circuit breaker transitions

613
00:37:32,264 --> 00:37:33,454
to the half open state.

614
00:37:34,014 --> 00:37:38,014
Here, it allows a limited number
of test requests to go through.

615
00:37:38,914 --> 00:37:43,994
If these test requests are successful,
the circuit breaker resets to the closed

616
00:37:44,004 --> 00:37:46,354
state, resuming normal operations.

617
00:37:46,904 --> 00:37:50,694
But if failures persist, it goes
right back to the open state.

618
00:37:51,194 --> 00:37:55,534
This dynamic behavior is what makes
circuit breakers so effective.

619
00:37:56,164 --> 00:38:01,024
They isolate failing components,
prevent cascading issues, and help

620
00:38:01,044 --> 00:38:02,644
the system recover gracefully.

621
00:38:03,144 --> 00:38:05,864
Now, where do circuit breakers
make the biggest impact?

622
00:38:06,674 --> 00:38:08,114
Let me give you a few scenarios.

623
00:38:08,859 --> 00:38:11,679
First, consider third party dependencies.

624
00:38:12,369 --> 00:38:16,639
If your system relies on an external
API that becomes unresponsive,

625
00:38:17,169 --> 00:38:21,159
circuit breakers can cut off requests
to the API and prevent it from

626
00:38:21,159 --> 00:38:22,879
slowing down your entire system.

627
00:38:23,764 --> 00:38:27,834
For example, if a payment gateway
starts timing out, the circuit

628
00:38:27,834 --> 00:38:31,644
breaker can stop further requests
and provide fallback responses

629
00:38:31,674 --> 00:38:33,414
like a queue transaction message.

630
00:38:34,374 --> 00:38:36,984
Next, think about database overloads.

631
00:38:37,654 --> 00:38:42,184
If a database is overwhelmed with too
many requests, a circuit breaker can

632
00:38:42,234 --> 00:38:46,924
temporarily block new requests, giving
the database time to recover before

633
00:38:46,934 --> 00:38:48,634
it becomes a single point of failure.

634
00:38:49,494 --> 00:38:54,234
And finally, There are latency
sensitive applications in services where

635
00:38:54,234 --> 00:38:58,754
performance is critical, like streaming
platforms or real time communication

636
00:38:58,754 --> 00:39:03,314
apps, circuit breakers can prevent slow
or failing components from dragging

637
00:39:03,314 --> 00:39:05,014
down the entire user experience.

638
00:39:05,514 --> 00:39:07,844
So what do circuit breakers
bring to the table?

639
00:39:08,634 --> 00:39:10,334
The benefits are pretty significant.

640
00:39:11,314 --> 00:39:12,384
Failure isolation.

641
00:39:13,184 --> 00:39:17,614
Circuit breakers isolate failing
components, preventing a single issue

642
00:39:17,854 --> 00:39:20,284
from cascading into a system wide outage.

643
00:39:20,784 --> 00:39:21,794
Improved recovery.

644
00:39:22,514 --> 00:39:27,264
By reducing the load on failing services,
circuit breakers give these services

645
00:39:27,564 --> 00:39:29,404
breathing room they need to recover.

646
00:39:29,904 --> 00:39:31,224
Better user experience.

647
00:39:31,969 --> 00:39:36,089
Instead of timing out or serving
errors, systems can provide fallback

648
00:39:36,159 --> 00:39:38,419
responses when a circuit breaker trips.

649
00:39:39,279 --> 00:39:43,679
This maintains partial functionality
for users, even during failures.

650
00:39:44,179 --> 00:39:45,009
Enhanced observability.

651
00:39:45,509 --> 00:39:49,609
Circuit breakers generate valuable
data about failure rates and recovery

652
00:39:49,779 --> 00:39:53,919
trends, helping teams identify
vulnerabilities and improve resilience.

653
00:39:53,939 --> 00:39:54,069
Of

654
00:39:54,569 --> 00:39:58,559
course, circuit breakers also come
with their own set of challenges.

655
00:39:59,369 --> 00:40:01,749
Let's talk about those and
how to address some of them.

656
00:40:02,249 --> 00:40:04,949
One challenge is setting
the right thresholds.

657
00:40:05,869 --> 00:40:10,064
If the failure threshold is too low,
The breaker might trip unnecessarily,

658
00:40:10,484 --> 00:40:11,914
disrupting healthy service.

659
00:40:12,864 --> 00:40:16,394
But if it's too high, the circuit
breaker might not respond in time

660
00:40:16,434 --> 00:40:17,934
to prevent cascading failures.

661
00:40:18,534 --> 00:40:19,264
The solution?

662
00:40:19,744 --> 00:40:23,584
Use historical performance data
to set realistic thresholds and

663
00:40:23,614 --> 00:40:25,454
adjust them as your system evolves.

664
00:40:26,434 --> 00:40:29,044
Another issue is balancing
failures and recovery.

665
00:40:29,904 --> 00:40:33,429
When a circuit breaker trips, It
can disrupt service if there's

666
00:40:33,429 --> 00:40:35,079
no fallback mechanism in place.

667
00:40:35,579 --> 00:40:39,839
At the same time, retrying too many
requests too quickly during the

668
00:40:39,839 --> 00:40:42,439
recovery can overwhelm the system again.

669
00:40:43,259 --> 00:40:47,349
The fix here is to combine circuit
breakers with retry strategies and

670
00:40:47,349 --> 00:40:52,099
exponential back off, where retries
are spaced out and progressively

671
00:40:52,819 --> 00:40:54,839
give the system time to stabilize.

672
00:40:55,339 --> 00:40:57,639
You might also encounter false positives.

673
00:40:58,374 --> 00:41:01,444
Cases where the circuit breaker
trips because of temporary

674
00:41:01,474 --> 00:41:05,404
network issues or traffic spikes
rather than genuine failures.

675
00:41:06,254 --> 00:41:10,044
To avoid this, use rolling
averages or sliding windows

676
00:41:10,324 --> 00:41:11,984
when calculating failure rates.

677
00:41:12,694 --> 00:41:16,884
This smooths out short term fluctuations
and ensures the circuit breaker

678
00:41:16,944 --> 00:41:18,734
only trips when it's truly needed.

679
00:41:18,734 --> 00:41:21,224
And then there's testing
and observability.

680
00:41:21,724 --> 00:41:24,614
Circuit breakers need to be
rigorously tested to make

681
00:41:24,614 --> 00:41:26,744
sure they behave as expected.

682
00:41:27,244 --> 00:41:30,724
Without proper monitoring, it's
hard to tell if they're functioning

683
00:41:30,724 --> 00:41:33,204
correctly or introducing new problems.

684
00:41:33,844 --> 00:41:34,544
The solution?

685
00:41:35,114 --> 00:41:38,824
Use observability tools to
monitor circuit breakers activity.

686
00:41:39,484 --> 00:41:43,544
Track trip counts, recovery
attempts, and fallback responses.

687
00:41:44,044 --> 00:41:47,714
Simulate failures in staging environments
to validate your configuration

688
00:41:48,114 --> 00:41:49,494
before deploying to production.

689
00:41:49,994 --> 00:41:53,004
Finally, let's not forget
about fallback mechanisms.

690
00:41:53,584 --> 00:41:57,624
A circuit breaker without a fallback
plan can leave users with no

691
00:41:57,684 --> 00:42:01,884
response at all, which is almost
as bad as the failure itself.

692
00:42:02,634 --> 00:42:06,794
The answer is to provide fallback
responses, such as cache data

693
00:42:06,874 --> 00:42:10,574
or default messages, to maintain
some level of functionality

694
00:42:10,634 --> 00:42:11,834
when the circuit breaker trips.

695
00:42:12,334 --> 00:42:16,924
In today's always on world, users
expect services to recover from

696
00:42:16,934 --> 00:42:19,174
failures without them even noticing.

697
00:42:20,034 --> 00:42:23,854
Automated recovery mechanisms
make this expectation a reality.

698
00:42:24,754 --> 00:42:29,604
These systems enable software to
detect issues, respond autonomously,

699
00:42:29,914 --> 00:42:33,344
and restore normal operations
without requiring human intervention.

700
00:42:34,264 --> 00:42:37,584
You can think of automated
recovery mechanisms as the immune

701
00:42:37,584 --> 00:42:39,014
system of your infrastructure.

702
00:42:39,894 --> 00:42:43,894
Just as your body fights off illnesses
to keep you healthy, self healing

703
00:42:43,894 --> 00:42:48,794
systems identify failures, isolate the
affected components, and fix the problem.

704
00:42:49,554 --> 00:42:52,734
All while ensuring the rest of
the system continues to function.

705
00:42:53,344 --> 00:42:54,184
The result?

706
00:42:54,514 --> 00:42:58,094
Minimize downtime, fewer
manual intervention, and a

707
00:42:58,094 --> 00:42:59,564
smoother user experience.

708
00:43:00,384 --> 00:43:05,224
Let's dive into how automated recovery
works and explore three core strategies

709
00:43:05,254 --> 00:43:06,974
that make all of these possible.

710
00:43:07,474 --> 00:43:11,334
Firstly, let's talk about
autoscaling, a fundamental

711
00:43:11,344 --> 00:43:13,124
strategy for automated recovery.

712
00:43:13,124 --> 00:43:18,274
Autoscaling dynamically adjusts the
number of running instances in your system

713
00:43:18,634 --> 00:43:20,704
based on demand or performance issues.

714
00:43:21,604 --> 00:43:24,234
Imagine an e commerce
platform during a flash sale.

715
00:43:24,469 --> 00:43:28,259
Traffic skyrockets as
users rush to grab deals.

716
00:43:28,909 --> 00:43:33,109
With autoscaling, the system can
automatically spin up new application

717
00:43:33,109 --> 00:43:37,099
servers to handle the search, ensuring
smooth performance for everyone.

718
00:43:37,599 --> 00:43:40,509
But autoscaling isn't just
for handling traffic spikes.

719
00:43:41,129 --> 00:43:43,159
It's also a key recovery mechanism.

720
00:43:43,829 --> 00:43:47,939
If an instance becomes unresponsive
due to resource exhaustion, the

721
00:43:47,939 --> 00:43:52,319
autoscaling system can remove it and
replace it with a healthy instance.

722
00:43:52,949 --> 00:43:56,649
Combined with load balancers,
this ensures your service remains

723
00:43:56,649 --> 00:43:58,649
available even during failures.

724
00:43:59,579 --> 00:44:05,239
For example, think about a video streaming
platform experiencing a sudden spike

725
00:44:05,239 --> 00:44:07,209
in viewership during a live event.

726
00:44:08,019 --> 00:44:12,829
Within minutes, autoscaling provisions
new servers to manage the increased load.

727
00:44:13,704 --> 00:44:18,004
If one of these servers fail, a
replacement is spun up almost instantly.

728
00:44:18,999 --> 00:44:23,259
The users, it feels seamless,
but behind the scenes, automated

729
00:44:24,139 --> 00:44:25,609
recovery is hard at work.

730
00:44:26,109 --> 00:44:31,459
Next, let's look at self healing systems,
the heartbeat of automated recovery.

731
00:44:32,209 --> 00:44:35,639
These systems constantly monitor
the health of components and take

732
00:44:35,639 --> 00:44:37,049
action when something goes wrong.

733
00:44:37,709 --> 00:44:40,334
The goal is simple, detect failures early.

734
00:44:40,684 --> 00:44:43,844
fix them autonomously, and
keep disruptions to a minimum.

735
00:44:44,344 --> 00:44:47,464
Self healing systems rely on
health checks to identify issues.

736
00:44:47,904 --> 00:44:52,914
For instance, in a Kubernetes environment,
liveness and readiness probes can

737
00:44:52,924 --> 00:44:54,944
detect when a pod becomes unhealthy.

738
00:44:55,574 --> 00:44:59,614
The orchestrator might restart the
pod, moving it to another node, or

739
00:44:59,614 --> 00:45:01,444
spin up a replacement automatically.

740
00:45:02,014 --> 00:45:04,334
All of this happens
without human intervention.

741
00:45:04,834 --> 00:45:07,904
Self healing also extends
to managing dependencies.

742
00:45:08,604 --> 00:45:12,674
Imagine a microservice architecture
where one service becomes unresponsive.

743
00:45:13,304 --> 00:45:17,654
A self healing system might isolate
that service, reroute requests to

744
00:45:17,654 --> 00:45:21,704
a fallback, or notify dependent
services to rely on a cache data.

745
00:45:22,204 --> 00:45:25,664
This isolation prevents the failure
from affecting the rest of the system.

746
00:45:26,164 --> 00:45:28,594
And let's not forget data
replication and failover.

747
00:45:29,314 --> 00:45:34,174
In distributed systems, if a primary
node goes offline, self healing

748
00:45:34,184 --> 00:45:36,514
mechanisms redirect traffic to replicas.

749
00:45:37,184 --> 00:45:38,854
Users never notice the change.

750
00:45:39,099 --> 00:45:43,619
But the system is busy recovering in
the background picture a messaging

751
00:45:43,619 --> 00:45:48,739
platform where the notification service
crashes, the orchestrator detects the

752
00:45:48,739 --> 00:45:53,899
issue, restart the service and queue
spending notifications while all this

753
00:45:53,909 --> 00:45:58,179
is happening, users continue to send
and receive messages without disruption.

754
00:45:58,679 --> 00:46:00,719
Now let's talk about deployment failures.

755
00:46:01,219 --> 00:46:05,769
Sometimes failures aren't caused by
hardware or system overloads, but by

756
00:46:05,769 --> 00:46:07,449
the introduction of buggy software.

757
00:46:07,949 --> 00:46:11,339
A poorly tested release
can crash services, degrade

758
00:46:11,339 --> 00:46:13,019
performance, or generate errors.

759
00:46:13,829 --> 00:46:16,199
That's where rollback
strategies come into play.

760
00:46:16,699 --> 00:46:19,469
Rollback strategies detect
when a deployment is causing

761
00:46:19,479 --> 00:46:22,779
issues and automatically revert
to the last stable version.

762
00:46:23,709 --> 00:46:27,899
This minimizes downtime and ensures
that users are largely unaffected.

763
00:46:28,729 --> 00:46:32,039
For example, consider a
continuous deployment pipeline

764
00:46:32,059 --> 00:46:33,339
for an e commerce platform.

765
00:46:34,239 --> 00:46:38,419
A new feature is rolled out, but
almost immediately, error rates spike.

766
00:46:39,289 --> 00:46:44,279
The system detects an anomaly, rolls back
the deployment, and restores the previous

767
00:46:44,289 --> 00:46:46,559
version, all in a matter of minutes.

768
00:46:47,309 --> 00:46:50,859
The engineering team can investigate
the problem while the platform

769
00:46:50,859 --> 00:46:51,959
continues to run smoothly.

770
00:46:52,459 --> 00:46:55,209
Rollback strategies are
particularly valuable in an

771
00:46:55,209 --> 00:46:56,969
environment with frequent updates.

772
00:46:57,649 --> 00:47:01,109
By integrating them into your
deployment pipeline, you can make

773
00:47:01,149 --> 00:47:05,819
rapid changes confidently, knowing
that any issues can be quickly undone.

774
00:47:06,319 --> 00:47:10,859
Of course, automated recovery
doesn't come without its challenges.

775
00:47:11,649 --> 00:47:15,129
Let's explore some of these hurdles and
how you can address them effectively.

776
00:47:15,629 --> 00:47:18,929
One major challenge is false
positive in health checks.

777
00:47:19,609 --> 00:47:23,019
Overly sensitive health checks
might interpret a temporary spike

778
00:47:23,029 --> 00:47:27,919
in resource outage leading to
unnecessary restarts or replacements.

779
00:47:28,419 --> 00:47:31,489
The solution here is to design
your health checks carefully.

780
00:47:32,244 --> 00:47:37,404
Instead of focusing solely on metrics
like CPU or memory usage, prioritize

781
00:47:37,404 --> 00:47:42,184
functional indicators, such as whether the
service is processing requests correctly.

782
00:47:42,684 --> 00:47:44,714
Another challenge is recovery loops.

783
00:47:45,424 --> 00:47:49,604
Have you ever seen a system that keeps
restarting a component over and over

784
00:47:49,894 --> 00:47:51,484
without solving the underlying issue?

785
00:47:52,474 --> 00:47:55,664
This happens when recovery
mechanisms aren't properly throttled.

786
00:47:56,434 --> 00:47:59,969
To avoid this, You can limit
recovery attempts or escalate

787
00:47:59,999 --> 00:48:02,139
persistent issues to human operators.

788
00:48:02,639 --> 00:48:04,179
There's also the question of cost.

789
00:48:04,849 --> 00:48:09,339
Automated recovery often requires spare
capacity or additional infrastructure,

790
00:48:10,009 --> 00:48:12,159
which can increase operational expenses.

791
00:48:12,879 --> 00:48:16,239
To manage this, balance
redundancy with efficiency.

792
00:48:16,849 --> 00:48:20,939
For instance, use predictive scaling
to ensure you are prepared for demand

793
00:48:20,939 --> 00:48:22,889
spikes without over provisioning.

794
00:48:23,389 --> 00:48:25,419
Testing is another critical
piece of the puzzle.

795
00:48:26,319 --> 00:48:29,749
Recovery mechanisms that
haven't been tested might fail

796
00:48:29,789 --> 00:48:31,049
when you need them the most.

797
00:48:31,549 --> 00:48:35,609
Regularly simulate failures in staging
environments and use chaos engineering

798
00:48:35,629 --> 00:48:37,189
to evaluate recovery processes.

799
00:48:37,689 --> 00:48:42,369
The more you test, the more confident you
will be in your system's availability and

800
00:48:42,369 --> 00:48:45,249
ability to handle real world disruptions.

801
00:48:45,749 --> 00:48:49,539
As we wrap up today, I want to
leave you with one key idea.

802
00:48:50,309 --> 00:48:51,709
Failures are not the enemy.

803
00:48:52,184 --> 00:48:56,714
In fact, they are an opportunity
to learn, to grow, and to build

804
00:48:56,714 --> 00:49:00,754
systems that are stronger, more
reliable, and always available.

805
00:49:01,564 --> 00:49:05,914
By designing for failure, you can
transform your systems into resilient,

806
00:49:06,134 --> 00:49:11,374
always on services that don't just survive
disruptions, but thrive through them.

807
00:49:12,264 --> 00:49:14,934
And resilience isn't just
about avoiding failure.

808
00:49:15,434 --> 00:49:16,994
It's about being prepared for it.

809
00:49:17,974 --> 00:49:22,864
It's about ensuring that when something
does go wrong, your system can handle

810
00:49:22,864 --> 00:49:25,054
it gracefully and keep moving forward.

811
00:49:26,044 --> 00:49:29,034
Let's take a moment to reflect
on what we have covered today.

812
00:49:29,704 --> 00:49:32,154
We have worked through some of
the most important strategies

813
00:49:32,154 --> 00:49:33,604
for building resilient systems.

814
00:49:34,604 --> 00:49:35,824
First, redundancy.

815
00:49:36,644 --> 00:49:41,854
This gives your system the safety nets
it needs when things go wrong, ensuring

816
00:49:41,854 --> 00:49:43,794
that backups are always ready to step in.

817
00:49:44,294 --> 00:49:48,564
Failover mechanisms make those
transitions seamless, minimizing

818
00:49:48,564 --> 00:49:50,744
downtime and keeping users connected.

819
00:49:51,244 --> 00:49:55,274
Graceful degradation allows your
system to continue serving users,

820
00:49:55,614 --> 00:49:59,564
even if some components fail, by
prioritizing core functionality.

821
00:50:00,544 --> 00:50:04,664
Graceful shutdown protects data
and ensures that when services

822
00:50:04,664 --> 00:50:09,094
go offline, whether planned or
unexpected, is done in an orderly way.

823
00:50:09,594 --> 00:50:13,254
Chaos engineering teaches your system
how to handle real world failures

824
00:50:13,554 --> 00:50:16,754
by uncovering vulnerabilities
before they become problems.

825
00:50:17,254 --> 00:50:20,374
Circuit breakers isolate
failures to prevent them from

826
00:50:20,374 --> 00:50:21,994
cascading across your system.

827
00:50:22,864 --> 00:50:27,364
And finally, automated recovery
mechanisms give your system the power

828
00:50:27,364 --> 00:50:32,364
to heal itself, minimizing disruptions
without requiring human intervention.

829
00:50:32,864 --> 00:50:35,114
These strategies aren't just theoretical.

830
00:50:35,704 --> 00:50:36,694
They are practical.

831
00:50:37,144 --> 00:50:40,584
actionable approaches you can
implement to build resilient systems.

832
00:50:41,084 --> 00:50:42,014
Thank you for your time.

833
00:50:42,984 --> 00:50:47,104
If you have any questions or want
to discuss specific challenges, feel

834
00:50:47,104 --> 00:50:48,304
free to connect with me on LinkedIn.

835
00:50:49,034 --> 00:50:50,704
You can find my profile on the screen.

836
00:50:51,394 --> 00:50:54,224
I'd love to continue the
conversation and hear your thoughts.

