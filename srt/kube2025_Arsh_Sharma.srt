1
00:00:00,570 --> 00:00:01,470
Hey everyone.

2
00:00:01,620 --> 00:00:03,840
Welcome to Con 42 Q Native.

3
00:00:04,350 --> 00:00:08,010
I'm Rs and I work as a senior
Dere engineer at Metal Bill.

4
00:00:08,940 --> 00:00:13,200
In this talk, I'm going to be covering
a very important problem, and that

5
00:00:13,200 --> 00:00:17,850
is the rising number of Kubernetes
clusters in an organization once

6
00:00:17,850 --> 00:00:19,799
they decide to adopt Kubernetes.

7
00:00:20,790 --> 00:00:24,960
I'm going to show you that how when you
as an organization make a decision to

8
00:00:24,960 --> 00:00:29,009
move to Kubernetes, you start with a
couple of clusters, and then the number

9
00:00:29,009 --> 00:00:31,200
of clusters just exceed exponentially.

10
00:00:31,259 --> 00:00:34,500
Don't worry, I'm not just going to be
talking about the problem in this talk.

11
00:00:34,740 --> 00:00:38,700
But I'm also going to give you a solution
using which you can reduce the number of

12
00:00:38,700 --> 00:00:44,100
Kubernetes clusters in your organization,
thereby reducing your cloud costs, and

13
00:00:44,100 --> 00:00:48,450
just making the lives of your DevOps
teams and developers a lot easier.

14
00:00:48,870 --> 00:00:50,670
So let's just get right into it.

15
00:00:51,260 --> 00:00:54,860
First, I wanna walk you through
how things happen when you as an

16
00:00:54,860 --> 00:00:57,050
organization decide to adopt Kubernetes.

17
00:00:57,170 --> 00:00:59,450
So it starts out with a
lot of excitement, right?

18
00:00:59,450 --> 00:01:03,019
You have heard good things about
Kubernetes, and you realize the benefits

19
00:01:03,019 --> 00:01:07,730
you will get, and you decide to adopt
Kubernetes for your production workloads.

20
00:01:08,000 --> 00:01:11,270
So this usually means creating
one cluster, which you'll use to

21
00:01:11,270 --> 00:01:12,530
deploy your production workloads.

22
00:01:13,030 --> 00:01:16,720
Then you realize that you cannot
ship changes directly to that

23
00:01:16,720 --> 00:01:17,860
production cluster, right?

24
00:01:17,860 --> 00:01:22,870
Because sometimes your changes might
break that cluster, and because of this,

25
00:01:22,870 --> 00:01:27,970
you add one or two staging clusters,
which help you, you know, test changes.

26
00:01:27,970 --> 00:01:28,630
And then.

27
00:01:29,015 --> 00:01:30,995
You ship them to the
production cluster too.

28
00:01:31,505 --> 00:01:34,475
So at this point things are
still manageable and I feel like

29
00:01:34,475 --> 00:01:35,945
it's, we are at an okay point.

30
00:01:36,275 --> 00:01:39,935
Then you realize that deploying to
staging actually takes a lot of time

31
00:01:39,935 --> 00:01:43,775
and developers don't want to wait
that long just to get that validation.

32
00:01:43,775 --> 00:01:46,145
If there are changes in a poll
request are working or not.

33
00:01:47,030 --> 00:01:50,899
At this point, you start spinning up
clusters in your CI setup because you

34
00:01:50,899 --> 00:01:55,100
want to run tests and give feedback
to developers early on instead of them

35
00:01:55,100 --> 00:01:57,170
having to wait for staging deployments.

36
00:01:57,590 --> 00:02:01,670
Also, when one team is deploying to
staging, it becomes unusable for the other

37
00:02:01,670 --> 00:02:06,800
team, which is also another reason why
the demand for CI clusters start growing.

38
00:02:07,279 --> 00:02:11,300
At this point, things have, you
know, started to get a little.

39
00:02:12,130 --> 00:02:12,940
Out of control.

40
00:02:12,940 --> 00:02:16,690
You are realizing that your cloud bill
is slowly rising, but it's still okay

41
00:02:16,690 --> 00:02:19,990
because you recognize the benefits
Kubernetes is giving you and your team.

42
00:02:20,350 --> 00:02:23,769
Then one day you hear a complaint
from your developers that even CI

43
00:02:23,769 --> 00:02:29,230
is so slow that staging and CI both
are inaccessible because of how long

44
00:02:29,230 --> 00:02:32,485
they take, and your developers want
a faster way to test their changes.

45
00:02:33,339 --> 00:02:35,800
At this point, you start thinking
about personal environments and

46
00:02:35,800 --> 00:02:39,040
personal clusters, which mirror your
production setup so that developers

47
00:02:39,040 --> 00:02:42,370
can test out immediately, and
you are still able to ship at the

48
00:02:42,370 --> 00:02:44,079
speed that your business demands.

49
00:02:44,769 --> 00:02:48,399
Now you can clearly see
your cloud bills rising.

50
00:02:48,459 --> 00:02:52,120
Now you're thinking about finops and
how to optimize cloud costs because

51
00:02:52,149 --> 00:02:56,500
you are hearing pressure from everyone
in your organization that we really

52
00:02:56,500 --> 00:02:57,880
need to cut down a cloud bill.

53
00:02:58,469 --> 00:03:01,589
But the story doesn't stop here
because personal environments

54
00:03:01,589 --> 00:03:03,390
also now seem insufficient.

55
00:03:03,420 --> 00:03:07,920
Your developers want ephemeral on
demand environments, so you either

56
00:03:07,920 --> 00:03:12,510
resort to provisioning a lot of clusters
or start provisioning a huge cluster

57
00:03:12,510 --> 00:03:17,250
and partitioning it by different
namespace, but the outcome is the same.

58
00:03:17,279 --> 00:03:20,159
You now have way too many
clusters or namespace.

59
00:03:20,159 --> 00:03:24,900
Then your DevOps team can manage,
and your cloud bill is just enormous.

60
00:03:25,080 --> 00:03:29,160
Anyone who has, you know, dealt with
personal dev environments on Kubernetes,

61
00:03:29,369 --> 00:03:33,929
I'm sure you will relate to this meme
where your cloud bill has reached a

62
00:03:33,929 --> 00:03:38,489
point where it's just not sustainable to
continue with personal dev environments.

63
00:03:38,520 --> 00:03:41,220
It might have been obvious from
the stuff we talked about earlier,

64
00:03:41,220 --> 00:03:44,609
but let's still take a detailed
look at some of the downsides of

65
00:03:44,609 --> 00:03:46,710
having multiple Kubernetes clusters.

66
00:03:47,130 --> 00:03:51,119
First of all, your cloud bill is just
enormous and your costs are rising.

67
00:03:51,619 --> 00:03:56,749
Secondly, managing all these environments
takes a huge chunk of time and effort

68
00:03:56,749 --> 00:04:00,950
from your DevOps team time and effort,
which could be better spent on other

69
00:04:00,950 --> 00:04:03,469
things had they had that flexibility.

70
00:04:04,040 --> 00:04:07,369
It also sometimes means you need
to expand the size of your team

71
00:04:07,369 --> 00:04:10,820
and and hire more people to manage
these clusters, which further adds

72
00:04:10,820 --> 00:04:12,890
to your costs as an organization.

73
00:04:13,579 --> 00:04:17,299
The other downside is that if you decide
to do ephemeral environments, these

74
00:04:17,299 --> 00:04:19,219
environments still take time to provision.

75
00:04:19,549 --> 00:04:24,439
So even when a developer wants to, you
know, test a code change or you spin

76
00:04:24,439 --> 00:04:29,360
up a cluster in ci, the developers end
up waiting like 15 to 20 minutes each

77
00:04:29,360 --> 00:04:32,990
time before they're able to get that
feedback and validation that the latest

78
00:04:32,990 --> 00:04:34,729
code they have written is working or not.

79
00:04:34,849 --> 00:04:39,530
And lastly, at one point you just have too
many yams and too many clusters to manage.

80
00:04:39,804 --> 00:04:44,155
That you start to think why you even
went down this path in the first place.

81
00:04:44,215 --> 00:04:44,485
Okay?

82
00:04:44,485 --> 00:04:47,034
I know at this point you might
be thinking that you're doomed

83
00:04:47,034 --> 00:04:48,564
and there is no way out of this.

84
00:04:48,919 --> 00:04:50,390
But there actually is.

85
00:04:51,110 --> 00:04:55,159
What if you could use your
staging environment for your

86
00:04:55,280 --> 00:04:57,350
CI and your dev environments?

87
00:04:57,350 --> 00:05:00,589
What if you could safely share that
environment instead of having to

88
00:05:00,589 --> 00:05:04,880
provision individual environments or
having to provision more clusters for

89
00:05:05,059 --> 00:05:07,640
each developer or each CI runs needs?

90
00:05:07,834 --> 00:05:11,704
This is where our open source project
called Medi comes into picture.

91
00:05:11,794 --> 00:05:16,414
Medi lets you run your local process in
the context of a Kubernetes environment.

92
00:05:16,505 --> 00:05:20,225
So while your process is running on
the local machine, you actually get all

93
00:05:20,225 --> 00:05:24,034
the benefits you would had you deploy
to staging without going through the

94
00:05:24,034 --> 00:05:26,225
pain of actually deploying to staging.

95
00:05:26,640 --> 00:05:29,429
This also means that you don't
have to provision more clusters

96
00:05:29,429 --> 00:05:33,600
because your developers can safely
run in the context of one staging

97
00:05:33,600 --> 00:05:37,799
cluster instead of requiring their
own personal clusters, ephemeral

98
00:05:37,799 --> 00:05:39,270
environments, or things like that.

99
00:05:40,159 --> 00:05:43,100
Let's see how this works behind
the scenes so that you start to

100
00:05:43,100 --> 00:05:44,539
understand what I'm saying here.

101
00:05:44,840 --> 00:05:49,850
With Medi, your application runs locally,
but the traffic, the file system, the

102
00:05:49,850 --> 00:05:54,140
environment variables, all of that gets
mirrored from the Kubernetes cluster.

103
00:05:54,200 --> 00:05:58,340
So your local process thinks that it's
running on the cluster, whereas actually

104
00:05:58,340 --> 00:05:59,600
it's still running on your local.

105
00:06:00,530 --> 00:06:03,590
This way you're able to test in
a realistic environment without

106
00:06:03,590 --> 00:06:07,370
provisioning more environments or
without having to, you know, wait 15,

107
00:06:07,370 --> 00:06:11,240
20 minutes for your code to get deployed
to a staging or dev environment.

108
00:06:11,740 --> 00:06:14,830
This diagram here should give you
a better idea of how this works.

109
00:06:15,040 --> 00:06:20,050
So you see that the local process is
running on your laptop, but medi mirrors

110
00:06:20,110 --> 00:06:24,040
incoming and outgoing traffic between
your local machine and the cluster.

111
00:06:24,460 --> 00:06:28,000
So let's suppose your process needs
to talk to some other process or

112
00:06:28,000 --> 00:06:32,110
microservice, which is deployed on the
cluster, but not on your local machine.

113
00:06:32,620 --> 00:06:36,430
Then any outgoing requests which your
process makes, they get mirrored.

114
00:06:36,930 --> 00:06:40,050
To the target service, which
is deployed in the cluster for

115
00:06:40,050 --> 00:06:41,670
that process and other services.

116
00:06:41,670 --> 00:06:44,520
See those requests as if
they were coming from the.

117
00:06:44,950 --> 00:06:46,150
Target service.

118
00:06:46,300 --> 00:06:50,320
Similarly, any traffic you send to
the target service in the cluster gets

119
00:06:50,320 --> 00:06:54,070
also mirrored to your local machine, so
you are able to test out the behavior

120
00:06:54,070 --> 00:06:57,370
of the code you have just written
without having to actually deploy it.

121
00:06:57,640 --> 00:07:01,420
If things aren't super clear from this
diagram right now, trust me, when we

122
00:07:01,420 --> 00:07:03,220
see the demo, everything will connect.

123
00:07:03,310 --> 00:07:07,390
Before we go to the demo, I just wanna
sum up the things Medi enables for you

124
00:07:07,390 --> 00:07:10,720
so that when you see the demo, you're
able to recognize what is happening.

125
00:07:10,990 --> 00:07:14,560
With medi incoming traffic to
your cluster is mirrored to

126
00:07:14,560 --> 00:07:16,180
the locally running process.

127
00:07:16,420 --> 00:07:20,740
Similarly, outgoing traffic from the
locally running process is routed

128
00:07:20,740 --> 00:07:24,760
via the cluster, so it appears as if
it's coming from within the cluster.

129
00:07:24,850 --> 00:07:28,420
Finally, instead of mirroring
requests, you can also choose to

130
00:07:28,420 --> 00:07:32,320
steal certain targeted requests, so
you can set filters that you know,

131
00:07:32,320 --> 00:07:36,760
if this request matches a particular
header, you want that request stolen.

132
00:07:37,080 --> 00:07:41,130
And just send to your local process
instead of being mirrored between the

133
00:07:41,130 --> 00:07:42,810
cluster and the locally running process.

134
00:07:43,310 --> 00:07:47,150
And you're also able to read and
write to the files in the cluster in

135
00:07:47,150 --> 00:07:48,830
case your application requires that.

136
00:07:49,250 --> 00:07:53,240
And at the same time, you can mirror
environment variables, which are

137
00:07:53,240 --> 00:07:57,140
there in the cluster to your local
machine so that the configuration in

138
00:07:57,140 --> 00:08:01,460
which your application runs remains
entirely the same, allowing you to test

139
00:08:01,460 --> 00:08:03,020
it in a production-like environment.

140
00:08:03,080 --> 00:08:07,130
Okay, it's almost time for the demo,
but let's look at the architecture

141
00:08:07,130 --> 00:08:10,700
of the demo application so that you
understand when we actually see the demo.

142
00:08:11,445 --> 00:08:15,405
So we will be working with the IP
visit counter service, which when you

143
00:08:15,405 --> 00:08:20,565
send it a request, it returns back
the IP the request was from, and the

144
00:08:20,565 --> 00:08:25,065
number of times the request has been
sent from that IP behind the scenes.

145
00:08:25,065 --> 00:08:28,875
How it works is that anytime a
request comes, it saves that request

146
00:08:28,875 --> 00:08:31,875
to a Reds database, and then it.

147
00:08:31,940 --> 00:08:35,930
Talks to another service, which is the IP
Info Service, and it gets some information

148
00:08:35,930 --> 00:08:38,720
about the IP from that service.

149
00:08:38,960 --> 00:08:43,730
And then finally, it sends messages to
Kafka, which get consumed by IP visit

150
00:08:43,730 --> 00:08:45,620
Consumer Service, which is for login.

151
00:08:46,580 --> 00:08:50,000
We won't be seeing the logging part in
this demo, but we will be working with

152
00:08:50,000 --> 00:08:55,040
the IP visit counter service, making some
changes to that and seeing how we are able

153
00:08:55,040 --> 00:08:59,870
to test out those changes without having
to wait for our code to get deployed or

154
00:08:59,870 --> 00:09:03,590
without having to provision a cluster
or a separate environment for this.

155
00:09:03,650 --> 00:09:06,200
So the setup will work with
would look like this, that.

156
00:09:06,275 --> 00:09:10,595
You know, we assume that this application
is deployed to a staging cluster by

157
00:09:10,595 --> 00:09:13,415
the DevOps team, like the standard
staging cluster where you used to

158
00:09:13,415 --> 00:09:18,365
test, but we use the staging cluster
to actually test a locally written

159
00:09:18,365 --> 00:09:21,995
code without having to commit the code
or without having to, you know, go

160
00:09:21,995 --> 00:09:23,735
through pipelines and all of that stuff.

161
00:09:23,795 --> 00:09:27,965
Now, when I usually tell people this,
these are some of the responses I have.

162
00:09:28,190 --> 00:09:32,810
I'm often met with a sort of disbelief
or just straight up, you're lying,

163
00:09:33,230 --> 00:09:36,230
but a lot of people have questions
about, you know, is this safe?

164
00:09:36,230 --> 00:09:38,540
Can you actually safely share the cluster?

165
00:09:38,600 --> 00:09:42,500
What happens if my process accidentally
writes to a sensitive database?

166
00:09:42,590 --> 00:09:46,160
I'll answer all of these questions
after the demo so that once you've seen

167
00:09:46,160 --> 00:09:49,550
Meridian action, a lot of things would
just start making sense themselves.

168
00:09:49,615 --> 00:09:49,945
Cool.

169
00:09:49,975 --> 00:09:51,325
So now let's see the demo.

170
00:09:51,385 --> 00:09:56,275
Okay, so I have my application cloned
here, and we are in the IP visit counter

171
00:09:56,275 --> 00:09:58,075
service, the main dot go file there.

172
00:09:58,705 --> 00:10:03,985
And the way this application works
is that if you, it has been deployed,

173
00:10:03,985 --> 00:10:08,035
first of all on the Kubernetes cluster,
and this is like a staging cluster.

174
00:10:08,305 --> 00:10:12,385
So if we get a list of all the deployments
on that cluster, you'll see that the

175
00:10:12,775 --> 00:10:17,845
IP visit, counter service, the consumer
service, it talks to Kafka, Redis.

176
00:10:18,220 --> 00:10:21,220
Everything has been deployed on
the cluster, and none of this

177
00:10:21,220 --> 00:10:22,900
is running on my local machine.

178
00:10:23,200 --> 00:10:26,740
So this solves the first problem where,
you know, developers need to set up their

179
00:10:26,740 --> 00:10:31,510
dev environment or wait for a ephemeral
dev environment to get provisioned where

180
00:10:31,510 --> 00:10:33,095
all of these things take time to deploy.

181
00:10:33,839 --> 00:10:38,069
If you use a staging cluster for
development with medi, like I've been

182
00:10:38,069 --> 00:10:43,410
suggesting, then you skip all of that
because all the dependencies, databases,

183
00:10:43,410 --> 00:10:47,970
queues, whatnot, are already deployed
on the staging cluster and you can

184
00:10:48,000 --> 00:10:50,310
directly talk to them using this method.

185
00:10:50,810 --> 00:10:55,010
So to give you an overview of the
application, the way it works is

186
00:10:55,010 --> 00:10:59,000
that you can send a request to the
endpoint and then it will give a

187
00:10:59,000 --> 00:11:00,980
response showing the IP address.

188
00:11:00,980 --> 00:11:05,120
The request was sent from the number
of times we have sent a request and

189
00:11:05,120 --> 00:11:07,189
some text, which says, remote is fun.

190
00:11:07,195 --> 00:11:09,310
A. Plus high appended to it.

191
00:11:09,700 --> 00:11:13,690
This text remote is fun, is not
hardcoded in the application.

192
00:11:13,960 --> 00:11:18,640
It is being read from a file which
is on the cluster, and this high is

193
00:11:18,640 --> 00:11:20,530
hard there in the application code.

194
00:11:20,800 --> 00:11:24,475
So if I Google here, so
searcher you'll see that.

195
00:11:25,410 --> 00:11:28,800
We get this response string and
then append a high to it, but this

196
00:11:28,800 --> 00:11:32,670
response string is being picked
up from a file which is on the

197
00:11:32,670 --> 00:11:34,560
cluster, and we will soon see that.

198
00:11:35,070 --> 00:11:38,220
But if I were to send another
request here, the account should

199
00:11:38,220 --> 00:11:40,320
get updated to two like we see here.

200
00:11:40,820 --> 00:11:41,060
Cool.

201
00:11:41,060 --> 00:11:45,320
So now we are ready to test Medi out
in the context of this application.

202
00:11:45,590 --> 00:11:51,200
And the easiest way to test Medi
Out, so I'm in Cursor, but Medi has

203
00:11:51,200 --> 00:11:56,300
a code editor extension for Cursor
via code Windsor JetBrains IDs.

204
00:11:57,040 --> 00:12:00,430
So you can search for medi
extension and install it here.

205
00:12:00,835 --> 00:12:03,955
But if you don't want to use the
extension for some reason, we also

206
00:12:03,955 --> 00:12:08,455
have a CLI available, which is which,
which lets you achieve the same

207
00:12:08,455 --> 00:12:09,595
functionality you'll see me do here.

208
00:12:10,095 --> 00:12:10,275
Cool.

209
00:12:10,305 --> 00:12:14,595
So once you have the extension
installed, you should see written here.

210
00:12:14,895 --> 00:12:18,075
And now we want to select
an active configuration.

211
00:12:18,075 --> 00:12:21,165
So configuration for the service
we are going to be working on.

212
00:12:21,555 --> 00:12:23,835
I'll walk you through the
configuration later on.

213
00:12:24,165 --> 00:12:26,925
But since we are working for the
IP visit counter service, I'm

214
00:12:26,925 --> 00:12:28,455
gonna select that configuration.

215
00:12:28,875 --> 00:12:31,125
So I go here, select active configuration.

216
00:12:31,125 --> 00:12:32,235
I choose IP visit counter.

217
00:12:32,735 --> 00:12:36,245
After that, I just simply enable
my extension by clicking this dot.

218
00:12:37,205 --> 00:12:37,445
Cool.

219
00:12:37,475 --> 00:12:42,005
So once it's enabled, what we are
going to do is set a break point.

220
00:12:42,005 --> 00:12:46,445
So now we saw that, you know, when
we send a request to the end point of

221
00:12:46,445 --> 00:12:48,275
the cluster, nothing was happening.

222
00:12:48,845 --> 00:12:52,595
Now what we are going to do is we are
going to use Medi and see that when

223
00:12:52,595 --> 00:12:56,555
we send a request to the endpoint,
that request does get mirrored.

224
00:12:56,895 --> 00:12:58,814
To our locally running process.

225
00:12:59,415 --> 00:13:01,935
So to see that, let's add a break point.

226
00:13:01,935 --> 00:13:05,025
I've added a break point in the
main function where we load some

227
00:13:05,025 --> 00:13:11,295
config and then I go to the run
and debug section of my corridor.

228
00:13:11,324 --> 00:13:12,584
I click run and debug.

229
00:13:12,584 --> 00:13:16,334
You see, it says that using
medi binary found and.

230
00:13:17,090 --> 00:13:20,540
Shortly it should ask us
for a target on the cluster.

231
00:13:21,080 --> 00:13:25,580
So this target is the target
whose requests we want to mirror

232
00:13:25,580 --> 00:13:27,200
to the locally running process.

233
00:13:27,470 --> 00:13:30,980
So basically we wanna establish a
sort of connection between our IP

234
00:13:30,980 --> 00:13:34,730
visit counter service, which would be
running locally on the machine and the

235
00:13:34,730 --> 00:13:37,040
IP visit, counter services deployment.

236
00:13:37,400 --> 00:13:42,290
So I come here and I'll choose the
IP visit counter deployment, and.

237
00:13:42,680 --> 00:13:46,520
After that, ity would basically
establish that connection for us.

238
00:13:46,880 --> 00:13:51,830
Okay, now things are running and we
can go to a debug console and see that

239
00:13:51,830 --> 00:13:54,680
like this connection is established.

240
00:13:55,189 --> 00:14:00,350
And now when we send a request to the
same end point, we'll see that our break

241
00:14:00,350 --> 00:14:02,240
point here on the coordinator gets hit.

242
00:14:02,689 --> 00:14:06,110
So I'm sending this request
and we should get a response as

243
00:14:06,110 --> 00:14:10,490
expected, but here you'll see
that our breakpoint would get hit.

244
00:14:10,990 --> 00:14:15,970
Once a break point gets hit, we can
actually step into this function and.

245
00:14:16,970 --> 00:14:19,610
Here you see that we load
some environment variables.

246
00:14:19,610 --> 00:14:24,320
So once these environment variables
get loaded, the thing I want to

247
00:14:24,320 --> 00:14:28,890
show you is that you know, the con
final conflict which we return.

248
00:14:28,890 --> 00:14:31,980
So I'm gonna like just continue
till we hit this break point.

249
00:14:32,400 --> 00:14:35,520
And here in the final configuration,
you see that we load all of these

250
00:14:35,520 --> 00:14:39,150
environment variables and they
get loaded from the cluster.

251
00:14:39,150 --> 00:14:41,670
So I have not set any of these locally.

252
00:14:41,970 --> 00:14:45,450
And here you see that we are reading
a response file, which is at app.

253
00:14:45,765 --> 00:14:48,075
App slash response txt.

254
00:14:48,405 --> 00:14:51,015
And this file does not
exist on my local machine.

255
00:14:51,195 --> 00:14:52,695
It exists on the cluster.

256
00:14:52,995 --> 00:14:56,805
And this is a file which has the
remote and remotes fund text I

257
00:14:56,805 --> 00:14:58,185
was telling you about earlier.

258
00:14:58,685 --> 00:15:04,555
So if we return from this function, go
here and we, you know step down here,

259
00:15:04,555 --> 00:15:06,475
you'll see that we are reading this file.

260
00:15:07,045 --> 00:15:12,445
The file is app slash response txt, and
from here we are loading the file content.

261
00:15:13,285 --> 00:15:19,455
Which when we passe here in a response
string, which is empty right now, but

262
00:15:19,505 --> 00:15:26,255
like if we go here, then it gets populated
and response string is now remote is fun.

263
00:15:26,255 --> 00:15:31,205
So you saw how our locally running
process is actually able to communicate

264
00:15:31,205 --> 00:15:35,765
with the cluster, read that file,
and see the contents of that file

265
00:15:36,005 --> 00:15:40,295
without us having to, you know,
deploy this particular code like.

266
00:15:40,850 --> 00:15:44,150
We have not made any changes to it,
but without us having to go through

267
00:15:44,150 --> 00:15:48,290
pipelines or set up our own environment,
we are able to leverage the environment

268
00:15:48,319 --> 00:15:49,910
of the existing staging cluster.

269
00:15:50,410 --> 00:15:56,290
So here we saw how we can mirror
requests and basically test out our code.

270
00:15:56,290 --> 00:15:59,980
So assume you had made some change
and you wanted to test it out, you

271
00:15:59,980 --> 00:16:03,520
could just mirror the request and
then that request would get forward to

272
00:16:03,520 --> 00:16:07,180
your locally running process and you
can test it out, but that's not it.

273
00:16:07,540 --> 00:16:10,960
With mirror D, you can even
test the response of your code.

274
00:16:11,319 --> 00:16:15,640
So here you saw that the response we
were receiving, this was being sent

275
00:16:15,640 --> 00:16:18,819
by the counterpart of the service,
which is in the cluster already.

276
00:16:19,560 --> 00:16:23,670
But if we want a locally running
code to, you know, respond to

277
00:16:23,670 --> 00:16:27,509
requests, we can do that as well
with the steel mode in mirror team.

278
00:16:27,989 --> 00:16:33,209
So the way to enable steel mode is by
editing your ity dot con jason file, which

279
00:16:33,209 --> 00:16:35,189
is what I was talking about earlier here.

280
00:16:35,400 --> 00:16:38,579
Our mode was set to mirror and
I'm gonna change it to steel and.

281
00:16:39,079 --> 00:16:43,189
I don't wanna steal all requests
because one of the main benefits

282
00:16:43,189 --> 00:16:47,420
of MERIDIA is, is that it allows
multiple developers to share the same

283
00:16:47,420 --> 00:16:49,400
Kubernetes cluster for development.

284
00:16:49,520 --> 00:16:54,105
So what I can do here is add an
H two TP filter with my name.

285
00:16:55,010 --> 00:16:59,720
And now basically all the requests which
have this filter set would get stolen

286
00:16:59,720 --> 00:17:04,340
by medi to my local machine, and all
other requests would remain untouched.

287
00:17:04,580 --> 00:17:08,870
Now, this is really important because
it ensures that other people or other

288
00:17:08,870 --> 00:17:13,880
developers or even AI agents can
continue using that staging cluster,

289
00:17:14,150 --> 00:17:17,720
and I do not, you know start stealing
all the requests through my locally

290
00:17:17,720 --> 00:17:21,290
running process, thereby breaking
the cluster for everyone else.

291
00:17:21,605 --> 00:17:26,465
This way, I'm able to work independently
while sharing the same staging cluster

292
00:17:26,465 --> 00:17:28,445
for development as everybody else.

293
00:17:29,015 --> 00:17:33,335
So once we have this filter set, I'm
also gonna make some small code chain.

294
00:17:33,335 --> 00:17:36,035
So we saw the text high earlier, right?

295
00:17:36,455 --> 00:17:40,314
I'm gonna change this to high people from.

296
00:17:40,814 --> 00:17:46,574
Gone 42 and now this is part of
our response, and this is really

297
00:17:46,574 --> 00:17:50,564
important because we want to test
out that if Medi is allowing our.

298
00:17:51,209 --> 00:17:55,320
Locally running process to respond to
requests and thereby allowing us to test

299
00:17:55,379 --> 00:18:00,209
our locally running code without us having
to deploy it to the cluster or provision

300
00:18:00,209 --> 00:18:02,429
another cluster where this gets deployed.

301
00:18:03,270 --> 00:18:07,669
So with that said, I've set a breakpoint
as well in this GetCount function.

302
00:18:07,749 --> 00:18:12,069
And now I am going to run
and debug again with Medi.

303
00:18:12,569 --> 00:18:15,899
I will choose the IP visit
counter service again.

304
00:18:16,399 --> 00:18:18,659
And I forgot to remove this break point.

305
00:18:18,659 --> 00:18:23,129
So this first break point was hit
immediately, but we can continue.

306
00:18:23,219 --> 00:18:29,519
And now when we go back here, right,
and we send a request to this end point.

307
00:18:30,389 --> 00:18:31,469
Nothing would happen.

308
00:18:31,469 --> 00:18:33,149
And that is as expected.

309
00:18:33,149 --> 00:18:36,929
So our remote process is still
replying and giving us a response.

310
00:18:36,929 --> 00:18:41,279
We see the text, which is not showing
the change we made, and that's

311
00:18:41,279 --> 00:18:45,239
expected because we are not using
the HGDP filter, which we had set.

312
00:18:45,719 --> 00:18:50,969
But if I sent a request which
has that HT HTT B filter, we will

313
00:18:50,969 --> 00:18:52,649
see that a break point gets hit.

314
00:18:52,649 --> 00:18:55,979
So you see the break point here
gets hit and we are able to, you

315
00:18:56,384 --> 00:19:00,284
know, check the same ip, which
is the IPFR machine, and now.

316
00:19:01,159 --> 00:19:05,299
We are also not getting a response here
because the request has been paused by

317
00:19:05,299 --> 00:19:11,119
our locally running code, and now if I
let go of this request and go back to

318
00:19:11,119 --> 00:19:16,339
my terminal, I should see a response,
which SH shows our updated string.

319
00:19:16,569 --> 00:19:19,929
So you see here we got our
response and it says, remote is

320
00:19:19,929 --> 00:19:22,689
fun plus high people from Goth 42.

321
00:19:22,899 --> 00:19:26,349
So this is very important to
understand how Medi allows you to,

322
00:19:26,349 --> 00:19:30,429
you know, test out your code changes,
including the response of your code

323
00:19:30,429 --> 00:19:35,499
or anything else in an isolated
manner, but in a shared environment.

324
00:19:35,499 --> 00:19:40,329
So you share the same cluster as
everyone else, but you're able to safely

325
00:19:40,329 --> 00:19:42,279
and independently test out your code.

326
00:19:42,429 --> 00:19:44,529
Okay, so that was a demo which showed.

327
00:19:44,914 --> 00:19:49,594
Some features of Medi, but I also wanna
touch upon other features which make

328
00:19:49,594 --> 00:19:52,594
sharing a cluster really easy with Medi.

329
00:19:53,224 --> 00:19:58,474
So first thing which we saw is that
you can set HDP filters, which ensure

330
00:19:58,474 --> 00:20:03,634
that you know each request is handled
once and is routed to the right

331
00:20:03,639 --> 00:20:06,874
process, the right developer who's
working on that particular thing.

332
00:20:06,904 --> 00:20:11,764
If a request hits the remote service
in the cluster, DY will look for the

333
00:20:11,764 --> 00:20:14,014
right HGDP filter and then forwarded it.

334
00:20:14,019 --> 00:20:15,369
To the right developer.

335
00:20:15,639 --> 00:20:19,089
This way, developers don't end up
interfering with each other's work

336
00:20:19,269 --> 00:20:21,039
and are able to work independently.

337
00:20:21,099 --> 00:20:25,179
Secondly, if your application, you
know, consumes messages from a Q service

338
00:20:25,179 --> 00:20:30,669
like Kafka or SQS, Medi supports q
splitting, which enables concurrency.

339
00:20:30,669 --> 00:20:34,329
So Q Splitting is a feature in Medi,
which allows multiple developer.

340
00:20:34,479 --> 00:20:38,769
Purse to safely consume messages from
the same queue without disrupting

341
00:20:38,769 --> 00:20:40,329
the environment for other people.

342
00:20:40,629 --> 00:20:43,509
Now, I won't be going into the details
of this, but you can check out our

343
00:20:43,509 --> 00:20:47,079
documentation or our YouTube channel
as well where we have created short

344
00:20:47,079 --> 00:20:48,729
videos explaining each feature.

345
00:20:48,789 --> 00:20:52,839
Third feature melody has are things
called policies and me Policies help.

346
00:20:53,284 --> 00:20:54,634
Prevent forbidden action.

347
00:20:54,634 --> 00:20:57,004
So if you have a sensitive
database, which you don't want your

348
00:20:57,004 --> 00:21:00,784
developers accidentally writing
to, you can set up a mey policy,

349
00:21:00,784 --> 00:21:02,494
which prevents that from happening.

350
00:21:02,524 --> 00:21:06,844
So this gives you the peace of mind that
your shared staging environment will not

351
00:21:06,844 --> 00:21:11,014
be broken down by any single developers
changes or when they are, you know,

352
00:21:11,014 --> 00:21:12,599
using that environment for development.

353
00:21:13,004 --> 00:21:17,414
Lastly, if you do not wanna use some
staging resources, let's suppose

354
00:21:17,414 --> 00:21:20,984
a database, which is in staging
for any reason you can choose

355
00:21:20,984 --> 00:21:22,544
to run those services locally.

356
00:21:22,544 --> 00:21:26,564
So Medi gives you the flexibility of,
you know, using some services from the

357
00:21:26,564 --> 00:21:31,214
cluster or running some on your local
machine, like databases you might want

358
00:21:31,214 --> 00:21:35,234
to use because you're testing out a
migration or you just don't wanna affect

359
00:21:35,234 --> 00:21:36,809
the staging database or talk to that.

360
00:21:37,119 --> 00:21:40,149
So these are some of the features
which make sharing a staging

361
00:21:40,149 --> 00:21:44,439
cluster for development using
medi completely safe and possible.

362
00:21:44,469 --> 00:21:48,909
And this is how you can avoid creating
ephemeral dev environments or personal

363
00:21:48,909 --> 00:21:52,749
environments, or even spinning up
clusters for your CI process because

364
00:21:52,749 --> 00:21:55,869
you can use Medi CLI in the CI itself.

365
00:21:56,709 --> 00:21:57,489
So.

366
00:21:57,744 --> 00:22:00,504
This brings me to the conclusion
of my talk, and I just wanna sum

367
00:22:00,504 --> 00:22:02,274
up everything we have seen so far.

368
00:22:02,304 --> 00:22:05,934
The gist is that once you as an
organization adopt Kubernetes, if you

369
00:22:05,934 --> 00:22:09,924
are not careful the amount of Kubernetes
clusters, you start provisioning,

370
00:22:09,924 --> 00:22:13,674
just go out of hand because you're
provisioning clusters for ci, you're

371
00:22:13,674 --> 00:22:17,724
provisioning clusters for devs, you're
provisioning on demand clusters for

372
00:22:17,724 --> 00:22:19,704
future branches and all of that stuff.

373
00:22:20,659 --> 00:22:25,069
Medi helps you avoid that by letting
multiple developers and your CI

374
00:22:25,069 --> 00:22:29,059
use the same staging cluster for
development instead of provisioning

375
00:22:29,059 --> 00:22:30,474
clusters for each and every one.

376
00:22:30,974 --> 00:22:34,754
This gives your developers and
your CI tests a production-like

377
00:22:34,754 --> 00:22:35,774
environment to run in.

378
00:22:35,774 --> 00:22:39,374
Whereas it also saves your DevOps
team the time and hassle of

379
00:22:39,374 --> 00:22:43,034
provisioning and maintaining multiple
clusters which look like production.

380
00:22:43,424 --> 00:22:47,444
So with this, you have one production
cluster and you can have another staging

381
00:22:47,444 --> 00:22:51,164
cluster, which looks exactly like
production and that staging cluster.

382
00:22:51,439 --> 00:22:55,459
Is the only thing the DevOps or the
platform engineering team has to

383
00:22:55,459 --> 00:22:58,279
just take care of, you know, making
sure that looks like production

384
00:22:58,609 --> 00:23:02,689
because all your developers and your
CI can just then use that cluster

385
00:23:02,689 --> 00:23:04,729
for development or testing purposes.

386
00:23:04,759 --> 00:23:07,879
It's much better than having multiple
clusters, which you have to manage and

387
00:23:07,879 --> 00:23:09,469
which increase your cloud bail and.

388
00:23:10,079 --> 00:23:14,459
Make for a really bad developer experience
because of how slow they are, because

389
00:23:14,459 --> 00:23:16,049
of the time they take to provision.

390
00:23:16,379 --> 00:23:19,349
When you go with this approach, you
have one cluster which is already

391
00:23:19,349 --> 00:23:22,859
ready and nobody has to wait for
things to provision and they can

392
00:23:22,859 --> 00:23:24,539
instantly access and test out.

393
00:23:24,539 --> 00:23:26,849
The code changes as we
saw in the demo as well.

394
00:23:27,059 --> 00:23:32,489
So to sum up with medi, your cloud costs
are lower, you ship way faster, and most

395
00:23:32,489 --> 00:23:34,319
importantly, your developers are happier.

396
00:23:34,634 --> 00:23:38,384
Thank you for watching this talk, and
I hope you really found this useful and

397
00:23:38,384 --> 00:23:42,464
helps you reduce the number of Kubernetes
clusters you have in your organization

398
00:23:42,734 --> 00:23:44,594
for development and invest purposes.

399
00:23:44,624 --> 00:23:47,624
If you want to try that out, you
can check out our website, metal

400
00:23:47,624 --> 00:23:52,934
bear.com/medi to learn more about me,
or if you want to get hands on and try

401
00:23:52,934 --> 00:23:56,744
out Medi for yourself, you can check
out the open source project on GitHub,

402
00:23:56,744 --> 00:23:58,574
the link you can see on the slide.

403
00:23:59,049 --> 00:24:02,589
And if you have any questions about
the talk, if you have any feedback

404
00:24:02,589 --> 00:24:06,459
or if you wanna set up a call to
understand how Medi works, my calendar

405
00:24:06,459 --> 00:24:08,409
is always available to talk about Medi.

406
00:24:08,409 --> 00:24:12,429
So you can send me an email
at at the red metal bear.com.

407
00:24:12,909 --> 00:24:15,459
Thank you for watching and I
hope you all have a great day.

