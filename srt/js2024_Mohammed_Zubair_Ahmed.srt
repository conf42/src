1
00:00:00,150 --> 00:00:04,560
Hello everyone Welcome to
Conf 42 JavaScript track.

2
00:00:05,560 --> 00:00:08,890
I'm really excited to be
speaking about this topic.

3
00:00:09,390 --> 00:00:12,419
It's called mission
performance, impossible real

4
00:00:12,460 --> 00:00:14,500
time chat web app performance.

5
00:00:15,320 --> 00:00:16,190
My name is Zubair.

6
00:00:16,650 --> 00:00:18,369
I'm a software engineer at Mattermost.

7
00:00:18,869 --> 00:00:20,349
So a little bit about Mattermost.

8
00:00:21,009 --> 00:00:29,629
It's an open source collaboration platform
for communities, enterprises, teams.

9
00:00:30,129 --> 00:00:37,859
We have been, very focused on Mattermost
being an entire suit of platform tools

10
00:00:37,900 --> 00:00:39,780
for entire software development cycle.

11
00:00:40,280 --> 00:00:44,050
Now, before we begin with the,
with the topic, let's first

12
00:00:44,050 --> 00:00:46,250
understand the architecture.

13
00:00:46,670 --> 00:00:52,090
of the web app matter was web
app uses react as it's, front end

14
00:00:52,100 --> 00:00:54,350
library to make the web application.

15
00:00:55,130 --> 00:00:59,320
So as everybody's already aware, react
is more efficient when we have a lot

16
00:00:59,320 --> 00:01:03,140
of updates and indeed learning the
components which have changed only,

17
00:01:03,209 --> 00:01:05,589
change only have the difference.

18
00:01:06,315 --> 00:01:10,604
in visualization of what components
have actually are supposed to re render

19
00:01:10,815 --> 00:01:12,544
instead of re rendering the complete.

20
00:01:13,044 --> 00:01:17,454
And the state manager which we
are using is Redux, Redux with

21
00:01:17,494 --> 00:01:18,824
Thung's library separately.

22
00:01:18,824 --> 00:01:22,344
It's, and it serves, as I've
said, it serves as a state

23
00:01:22,344 --> 00:01:23,624
manager of the application.

24
00:01:24,454 --> 00:01:26,754
And, I think everybody is aware of Redux.

25
00:01:26,784 --> 00:01:31,695
It's a very predictable state
container library for React.

26
00:01:32,195 --> 00:01:37,614
It also answers, handles asynchronous
logic for interaction with networks

27
00:01:38,074 --> 00:01:40,894
and all the other core features.

28
00:01:41,394 --> 00:01:45,564
The next is technology which
actually makes the web application of

29
00:01:45,594 --> 00:01:49,454
MatterMost real time is WebSocket API.

30
00:01:49,954 --> 00:01:54,314
This WebSocket API enables us
to have real time communication

31
00:01:54,864 --> 00:01:56,404
between the client and the server.

32
00:01:56,794 --> 00:01:59,724
And this obviously is very crucial
for the chat application to

33
00:01:59,744 --> 00:02:02,404
be able to serve in real time.

34
00:02:02,904 --> 00:02:07,954
And, yes, all of that, the application
is a single bit application, which

35
00:02:07,954 --> 00:02:12,154
enhances the user experiences by reducing,
frequent page loads and all that.

36
00:02:12,684 --> 00:02:20,854
It's it's entirely modular design, and
if it's open source, feel free to just

37
00:02:20,854 --> 00:02:27,094
check out the repository and the code
behind that, everything is modularized

38
00:02:27,094 --> 00:02:33,144
into components, actions, selectors
and the UI component library, all that.

39
00:02:33,644 --> 00:02:38,054
And as I've already mentioned, real
time updates with AppSocket, that

40
00:02:38,214 --> 00:02:42,214
maintains a persistent connection
to the server, allowing for the

41
00:02:42,214 --> 00:02:47,814
instant data push and updates without
requiring, much user interaction.

42
00:02:48,314 --> 00:02:52,514
With this, let's try to understand
what the, what are the challenges

43
00:02:52,514 --> 00:02:53,624
of real time application.

44
00:02:53,624 --> 00:02:58,804
Now, when I say challenges real time
data, I'm purely talking about in

45
00:02:58,804 --> 00:03:01,564
respective of the client performance.

46
00:03:02,064 --> 00:03:05,074
The real time application on the server
side is again a completely different

47
00:03:05,094 --> 00:03:11,164
story, which needs its own talk,
but this talk is concerned about the

48
00:03:11,304 --> 00:03:15,524
client performance, by client the web
application running on the browser.

49
00:03:16,024 --> 00:03:21,704
Now when things are real time, there
is a high volume of incoming network

50
00:03:21,704 --> 00:03:24,214
requests with WebSocket events.

51
00:03:24,714 --> 00:03:29,804
And most of the times, this
incoming network events also must

52
00:03:30,154 --> 00:03:35,794
be followed up by more by the
client by more network request.

53
00:03:36,124 --> 00:03:41,824
For example, if you have a post coming in,
we usually don't send the user's complete

54
00:03:41,824 --> 00:03:43,324
information who's sending the post.

55
00:03:43,754 --> 00:03:47,474
Now it's up to the client if it's, if
it has that user ID particularly stored

56
00:03:47,474 --> 00:03:49,094
in the Redux, it's well and good.

57
00:03:49,454 --> 00:03:52,774
If it's not, then again, it
has to go back to the server to

58
00:03:52,774 --> 00:03:54,864
fetch the user details for you.

59
00:03:55,364 --> 00:04:02,164
Now, in high elevated environment,
this could lead to a lot of, network

60
00:04:02,164 --> 00:04:09,439
traffic when your client is constantly
responding to the new incoming, WebSocket

61
00:04:09,439 --> 00:04:12,269
events with more network events.

62
00:04:12,269 --> 00:04:15,449
We get to this one of the problems, which
this is one of the problems we have faced.

63
00:04:16,149 --> 00:04:18,069
we'll get to that problem in a bit.

64
00:04:18,569 --> 00:04:24,069
Now there's always a constant battle
between what to fetch and what not

65
00:04:24,089 --> 00:04:26,449
to fetch or what to fetch later on.

66
00:04:27,319 --> 00:04:31,699
So with the real time data, there
is always, you have to may have this

67
00:04:31,729 --> 00:04:34,649
balance of overfetching or underfetching.

68
00:04:35,149 --> 00:04:39,719
Keeping all of this There, your
UI has to be responsive, has to be

69
00:04:40,319 --> 00:04:47,349
interactive, by the user, because user is
concerned about using the, application.

70
00:04:47,849 --> 00:04:50,989
It has, it just, your
app has to support that.

71
00:04:51,489 --> 00:04:56,139
Now, there were a lot of performance,
optimizations which we had, which

72
00:04:56,139 --> 00:05:02,789
had to wet in to, elevate the load
bearing capacity of the client.

73
00:05:03,519 --> 00:05:07,039
We'll get to some of the PRs,
not all of them, in a while.

74
00:05:07,199 --> 00:05:14,729
But before that, let's try to understand
how can we simulate this, right?

75
00:05:15,059 --> 00:05:19,014
Because you, if you are
testing on a local environment.

76
00:05:19,644 --> 00:05:22,994
You won't have this many users
signing up, these are sending

77
00:05:23,004 --> 00:05:27,454
the, these many users sending the
post, people joining the channel,

78
00:05:28,194 --> 00:05:32,594
people reacting to the post, people
commenting on the post and all that.

79
00:05:33,094 --> 00:05:37,474
Matomos has an inbuilt tool called
Matomos load testing engine.

80
00:05:37,974 --> 00:05:42,734
which it uses to test, test,
test the server environment.

81
00:05:43,234 --> 00:05:47,004
A little bit about the architecture
of this particular tool.

82
00:05:47,314 --> 00:05:50,264
It's an in house tool
developed by Metamast.

83
00:05:50,764 --> 00:05:53,874
We have, I think you're already
seeing the architecture over here.

84
00:05:54,264 --> 00:05:56,754
Just a very big, very high level view.

85
00:05:57,254 --> 00:06:01,414
There are two main components that
we should be, we should look out for.

86
00:06:01,914 --> 00:06:03,774
That is, the first one
is a user controller.

87
00:06:04,274 --> 00:06:09,194
Now, this component is actually in
charge of driving or controlling a user.

88
00:06:09,914 --> 00:06:12,454
By user, a simulated user, right?

89
00:06:13,184 --> 00:06:18,414
It is, its implementation is what defines,
what exactly the behavior of the user is.

90
00:06:19,044 --> 00:06:23,474
Its action could include, what the user
might do when it opens the Matterport.

91
00:06:24,024 --> 00:06:28,064
It might go to type in post,
switch channels, switch teams,

92
00:06:28,654 --> 00:06:30,654
open a new thread, a reactor post.

93
00:06:31,394 --> 00:06:35,314
So all of this is implemented
in the user controller module

94
00:06:36,294 --> 00:06:37,424
which you can see over there.

95
00:06:37,924 --> 00:06:44,244
And then another thing which we should
also be aware of is the coordinator.

96
00:06:44,744 --> 00:06:50,674
Now this coordinator is in charge of
managing the cluster of load test agents

97
00:06:50,704 --> 00:06:54,824
by load test agents and then it's trying
to figure out what is the maximum number

98
00:06:54,824 --> 00:07:00,544
of concurrent users the particular
server or active server can have.

99
00:07:01,044 --> 00:07:04,954
So again, this is, if you can see there
is a looping, there's a loop going on

100
00:07:04,954 --> 00:07:07,314
from coordinator directly to Prometheus.

101
00:07:07,314 --> 00:07:10,274
It's a loop back, a simple feedback loop.

102
00:07:10,774 --> 00:07:14,674
So when the load test starts, the
coordinator We'll begin increasing the

103
00:07:14,674 --> 00:07:20,784
number of users and at the same time,
we'll also start monitoring, or acquiring

104
00:07:20,794 --> 00:07:26,564
the Prometheus server by collecting
its server metrics and seeing are

105
00:07:26,564 --> 00:07:28,294
we reaching our target limit or not?

106
00:07:28,794 --> 00:07:31,334
Then based on that, it will
start decreasing the active

107
00:07:31,334 --> 00:07:32,544
number of users slowly.

108
00:07:32,844 --> 00:07:36,014
And once the equilibrium
point is reached, it'll stop.

109
00:07:36,899 --> 00:07:40,799
By equilibrium, when no more
active users can be added.

110
00:07:41,299 --> 00:07:46,809
And that would indicate that these are
the maximum number of supported users

111
00:07:47,159 --> 00:07:49,289
this particular Matomo server can have.

112
00:07:49,789 --> 00:07:53,469
With this, these are all
happening on the server side.

113
00:07:53,969 --> 00:07:57,899
What we would like to do is have an
extension of this for the client.

114
00:07:58,399 --> 00:08:04,589
So into the one of this app instances,
we are going to connect our client and

115
00:08:04,589 --> 00:08:10,999
see how the client performs when all of
these user controllers are working in

116
00:08:11,039 --> 00:08:16,949
tandem, doing all sorts of things, all
expected user behavior, because that is

117
00:08:16,959 --> 00:08:21,249
how we would, that is how the application
would be used eventually, right?

118
00:08:21,749 --> 00:08:27,499
So for this, what we did is we
have, automated test running on

119
00:08:27,499 --> 00:08:32,579
the client, which will open the
web application when the load test

120
00:08:32,589 --> 00:08:34,679
is being run on the background.

121
00:08:35,629 --> 00:08:40,329
And again, the front end, load testing
tool, which start interacting with the

122
00:08:40,329 --> 00:08:42,319
application, the real time application.

123
00:08:42,819 --> 00:08:47,593
Now you must be confusing how you said the
user controller is doing the interaction,

124
00:08:47,593 --> 00:08:49,534
but yes, user controller is doing.

125
00:08:50,034 --> 00:08:55,744
On the server side, but we need
a client side metrics, right?

126
00:08:55,744 --> 00:09:01,174
So that's the reason we need to connect a
client and then run an automation, loading

127
00:09:01,174 --> 00:09:06,674
tool to get the actual browser metrics
and what the actual user is seeing.

128
00:09:07,454 --> 00:09:09,394
And for that, we need to
connect it to a browser.

129
00:09:09,894 --> 00:09:13,194
So as you can see, once we
start doing that, it runs whole

130
00:09:13,204 --> 00:09:14,804
suits, it runs for some time.

131
00:09:14,804 --> 00:09:16,824
It runs whole suits of tests.

132
00:09:17,324 --> 00:09:21,944
And with this, we are, getting,
we are piping all the data to the

133
00:09:21,944 --> 00:09:24,784
Prometheus, which is already there
in our load testing instance.

134
00:09:25,284 --> 00:09:30,584
And, with that Prometheus, we are
scraping, Grafana is scraping for

135
00:09:30,584 --> 00:09:34,304
us to, get us the useful analytics,
which we would like to see.

136
00:09:34,804 --> 00:09:40,684
Now, I told about this, like
analysis, like what the metrics are.

137
00:09:40,684 --> 00:09:43,304
Let's try to understand
what these metrics are.

138
00:09:43,304 --> 00:09:45,654
Now, before going to the metrics.

139
00:09:46,154 --> 00:09:51,154
How can you compare two versions
of a metadata application, right?

140
00:09:51,654 --> 00:09:56,674
So what we do is, within the, for
the version number A, we run the

141
00:09:56,674 --> 00:10:00,274
load test, wait for the equilibrium
to set in, get the app node,

142
00:10:00,324 --> 00:10:02,234
run the client automation test.

143
00:10:02,734 --> 00:10:07,114
Run it for some time, see the graph
dashboard, take the metrics from the

144
00:10:07,114 --> 00:10:12,364
browser, and then we run it again
with the equivalent or exactly the

145
00:10:12,364 --> 00:10:15,874
same environment variables, which
we have done for the past test.

146
00:10:16,354 --> 00:10:19,714
Now with the new, with the
different client version.

147
00:10:20,214 --> 00:10:23,874
And then that is how, after again,
running the same steps, then we

148
00:10:23,874 --> 00:10:26,494
compare, have we improved or not?

149
00:10:26,584 --> 00:10:30,414
And that is how we analyze,
okay, are we improving the.

150
00:10:31,184 --> 00:10:34,604
performance of web app or
are actually degrading that.

151
00:10:35,104 --> 00:10:36,724
Now, with this,

152
00:10:37,224 --> 00:10:41,164
let's, as I've mentioned that the
metrics which we are talking about,

153
00:10:41,174 --> 00:10:44,254
let's try to understand what are
these, what are a few of the metrics

154
00:10:44,254 --> 00:10:46,814
that we track in the client side.

155
00:10:47,314 --> 00:10:49,214
Now, this is important.

156
00:10:49,474 --> 00:10:53,154
Now, even before going to the
actual metrics, we need to, the

157
00:10:53,154 --> 00:10:56,664
reason there are a few things
that we first need to understand.

158
00:10:56,664 --> 00:11:00,534
This client side metrics are very
much dependent on the hardware.

159
00:11:01,034 --> 00:11:04,124
And because the hardware is
constantly changing with any

160
00:11:04,124 --> 00:11:05,184
number of clients, right?

161
00:11:05,184 --> 00:11:11,484
Because each one of them have a different
browser or have a different hardware.

162
00:11:12,094 --> 00:11:16,019
All the users, even them, Although
they all of them are in the same

163
00:11:16,019 --> 00:11:21,679
Matamoros instance, so we need to
be, that's the reason I've said that

164
00:11:21,679 --> 00:11:26,289
comparison is the key over here, not
the absolute values this metrics.

165
00:11:26,789 --> 00:11:33,369
So with that, the first thing which we
do is we, use the Chrome dev tools to

166
00:11:33,369 --> 00:11:36,229
see if there are any memory leaks or not.

167
00:11:36,279 --> 00:11:38,319
So we do the memory profiling.

168
00:11:38,819 --> 00:11:42,759
And with this, what we are basically
trying to see is, are there any

169
00:11:42,759 --> 00:11:47,819
memory, leakages or are there
any inefficient memory users?

170
00:11:48,319 --> 00:11:53,099
With this, what we can see is, will
this memory excess, excessive memory

171
00:11:53,109 --> 00:11:55,799
leads to crash or slow performance.

172
00:11:56,299 --> 00:12:00,189
once we do find out, okay, there are a few
things that we need to do, then we have to

173
00:12:00,629 --> 00:12:06,389
follow up with efficient data structures,
if you would like to see the structures

174
00:12:06,509 --> 00:12:12,409
or efficient calling of functions so
that our memory allocation optimization,

175
00:12:12,419 --> 00:12:14,969
if we Optimized memory allocation.

176
00:12:15,469 --> 00:12:17,169
We also monitor CPU usages.

177
00:12:17,989 --> 00:12:23,179
So this identifies us if there are
any processes bottlenecks because

178
00:12:23,179 --> 00:12:27,489
high CPU usage can slow down the
complete client affecting response

179
00:12:27,489 --> 00:12:29,859
times or application performance.

180
00:12:30,489 --> 00:12:34,549
Not only batteries application
performance, it also degrades

181
00:12:34,609 --> 00:12:35,959
other applications which are open.

182
00:12:36,899 --> 00:12:38,979
under the user's computer.

183
00:12:39,479 --> 00:12:44,849
So we profile, the CPU, and see what
cause, if there are any spikes, then we

184
00:12:44,849 --> 00:12:49,809
can go back to it and see what caused
this additional computational overheads.

185
00:12:50,309 --> 00:12:54,459
Another metric which we also see is what
is the frames per second when the user

186
00:12:54,539 --> 00:12:56,239
is interacting with the application.

187
00:12:56,799 --> 00:13:01,459
Now that is on the top left, as
you can see, Again, available in

188
00:13:01,459 --> 00:13:07,399
the Chrome DevTools, it's, it's a
measure of how smooth the animations

189
00:13:07,649 --> 00:13:12,419
or interactions are being happening,
are taking place in your application.

190
00:13:13,354 --> 00:13:19,914
Obviously, low FPS leads to
choppy visuals, delayed responses,

191
00:13:20,414 --> 00:13:22,494
very bad user experience.

192
00:13:22,994 --> 00:13:27,814
So we identify which process
is causing lower frame rate per

193
00:13:27,814 --> 00:13:32,364
second and concentrate on that to
see how can we improve that, how

194
00:13:32,364 --> 00:13:33,764
can the re renders be improved.

195
00:13:34,624 --> 00:13:35,134
And all that.

196
00:13:35,634 --> 00:13:40,464
Another useful tool, which we
also take a snapshot of is the

197
00:13:40,964 --> 00:13:45,174
network, HTTP archives or apps, hr.

198
00:13:45,674 --> 00:13:50,094
What is, this is what this will,
allow us to see is are there any,

199
00:13:50,744 --> 00:13:55,124
long pending network requests which
are being carried on by the client,

200
00:13:56,424 --> 00:13:59,504
which we might not be aware of.

201
00:13:59,684 --> 00:14:00,974
That client is making that.

202
00:14:01,524 --> 00:14:04,034
and see how other web assets are loading.

203
00:14:04,434 --> 00:14:05,914
Are there any network bottlenecks?

204
00:14:05,934 --> 00:14:09,774
Because what happens is, although the
browser does not have any limitations

205
00:14:09,784 --> 00:14:17,074
on number of requests being sent, but
all, but this network request, when the

206
00:14:17,074 --> 00:14:23,364
response from this network request comes
in, the client has to get this data, do,

207
00:14:23,464 --> 00:14:27,354
understand where to, where in the place
of redirect that has to place this data,

208
00:14:27,874 --> 00:14:29,774
do a lot of pre rendering and all that.

209
00:14:29,784 --> 00:14:37,394
So although network request doesn't
cost us the memory usage, but the

210
00:14:37,394 --> 00:14:41,464
following processes after we get
the response is, are the heavy ones.

211
00:14:42,014 --> 00:14:45,824
optimized, network loading
is, very much appreciated.

212
00:14:46,324 --> 00:14:51,974
And, the next thing is we have the Web
Vitals, library from Google as well,

213
00:14:52,004 --> 00:14:58,724
which helps us to understand various,
stable, industry recognized metrics.

214
00:14:58,734 --> 00:15:06,584
For example, FCPs, LCPs like FCP is the
first content and contentful paints,

215
00:15:07,084 --> 00:15:09,774
cumulative layout shifts and all that.

216
00:15:10,394 --> 00:15:13,934
there's a amazing library by
made by the Google, which you can

217
00:15:13,934 --> 00:15:17,654
incorporate to, and which would
track all these numbers for you.

218
00:15:18,154 --> 00:15:21,744
So measurement of these vitals
and also comparison is one of

219
00:15:21,744 --> 00:15:25,834
the vital, performance tool
which we have in our arsenal.

220
00:15:26,334 --> 00:15:29,834
Running Lighthouse is another,
process which we have.

221
00:15:30,164 --> 00:15:34,924
it gives us an holistic view,
said by, Google Lighthouse tool.

222
00:15:35,424 --> 00:15:41,204
It has, again, not only the metrics, which
we talked about earlier, but also other,

223
00:15:41,854 --> 00:15:43,684
other metrics, which are very useful.

224
00:15:43,694 --> 00:15:47,964
For example, the accessibilities
and, it also gives a very helpful

225
00:15:48,424 --> 00:15:52,834
recommendation on how to solve
this performance, bottlenecks.

226
00:15:53,684 --> 00:15:54,974
So we have.

227
00:15:55,459 --> 00:15:58,499
the FCP, which is the
first contentful paint.

228
00:15:59,079 --> 00:16:06,389
It's the measurement of time when the user
first navigate, have navigated the page.

229
00:16:07,119 --> 00:16:13,379
And, and if the part is rendered on the
screen, that's the first contentful paint.

230
00:16:14,139 --> 00:16:19,199
And the largest, LCP is the largest
contentful paint is basically the

231
00:16:19,239 --> 00:16:24,009
time between the, one of the largest
element on the page, which appears.

232
00:16:24,509 --> 00:16:27,459
And then we also have interaction
to NextPaint, again, which is

233
00:16:27,459 --> 00:16:30,639
a very core, web vital for us.

234
00:16:31,049 --> 00:16:36,749
It basically measures how much of the
latency we have with the user interaction

235
00:16:36,929 --> 00:16:39,809
when the user first lands on the page.

236
00:16:40,309 --> 00:16:43,169
And then we also have other
things like total blocking time,

237
00:16:43,179 --> 00:16:45,479
speed index, all of that here.

238
00:16:45,979 --> 00:16:51,589
Then we also have a specific set of
custom Matomost metrics, for example,

239
00:16:52,089 --> 00:16:57,159
channel switch time, like how much time
does it take for, on an average, for

240
00:16:57,159 --> 00:17:02,549
the user to change from one channel to
another, or from one team to another,

241
00:17:03,049 --> 00:17:07,259
all this, these are custom metrics
which we have defined, in Matomost.

242
00:17:07,759 --> 00:17:14,369
And yes, we are, we keep on defining
or developing this custom metrics

243
00:17:14,509 --> 00:17:17,329
as a part of performance analysis.

244
00:17:17,829 --> 00:17:21,709
Now with that, I think now
we have, understood like what

245
00:17:21,709 --> 00:17:22,999
all metrics which we can use.

246
00:17:23,209 --> 00:17:26,689
look out for and how do we do the
comparison and what is the process that

247
00:17:26,709 --> 00:17:29,779
we follow to the load test on the client.

248
00:17:29,819 --> 00:17:38,699
Let's look, quickly look at a few example
PRs just to get an understanding of how

249
00:17:38,739 --> 00:17:41,059
the actual process of work is being done.

250
00:17:41,559 --> 00:17:48,429
So this is one, one of the things which
we have seen, when we ran the load test on

251
00:17:48,429 --> 00:17:54,199
the client is there was a very high memory
usage of one of the, one of the reducers.

252
00:17:54,699 --> 00:18:00,039
And we have seen that it was like,
what, 34 percent of the memory usage,

253
00:18:00,199 --> 00:18:01,869
in that particular interaction.

254
00:18:02,369 --> 00:18:05,639
We had to reduce that, but
why is that even happening?

255
00:18:05,639 --> 00:18:09,919
It turned out that we were not correct.

256
00:18:09,919 --> 00:18:15,219
to say we were not actually,
getting, we were getting objects.

257
00:18:15,219 --> 00:18:18,999
We were not, which were not being
recycled by Chrome and they were

258
00:18:19,019 --> 00:18:22,999
having some dangling, objects, which
were not being garbage collected.

259
00:18:23,359 --> 00:18:28,009
And if they were dangling objects,
We had to, usually what happens is

260
00:18:28,019 --> 00:18:32,609
when you run a garbage collector,
it stops the whole application

261
00:18:32,729 --> 00:18:34,599
and then runs garbage collector.

262
00:18:35,009 --> 00:18:40,259
So GC run by the browser
is intensive operation.

263
00:18:40,659 --> 00:18:45,789
I need to avoid that as much as possible,
not completely avoided, but yeah.

264
00:18:45,929 --> 00:18:47,419
as much as possible.

265
00:18:47,919 --> 00:18:52,849
So this was a one of the thing which
we, which we had and we were using few

266
00:18:52,849 --> 00:18:58,369
of the arrays, patterns, which solved
the issue and we were able to get down a

267
00:18:58,539 --> 00:19:01,189
significant usage by this particularity.

268
00:19:01,209 --> 00:19:01,439
So

269
00:19:01,939 --> 00:19:05,299
there was also another one which
was causing from the issues was

270
00:19:05,309 --> 00:19:11,039
that we were using a library for the
profiles, profile popover and all that.

271
00:19:11,659 --> 00:19:15,569
So which were causing a significant memory
leak, which we were able to find out.

272
00:19:16,069 --> 00:19:19,459
So one of the methods was to
either, either have it custom

273
00:19:19,459 --> 00:19:24,089
built by ourselves or replace the
complete library, an existing one.

274
00:19:24,589 --> 00:19:31,209
So we had the base library changed,
to a more, new and supported

275
00:19:31,209 --> 00:19:33,049
library without these memory leaks.

276
00:19:33,049 --> 00:19:40,309
And yes, we did had a lot of improvement
when we actually changed the popover

277
00:19:40,319 --> 00:19:42,419
library and performance that.

278
00:19:42,919 --> 00:19:47,339
And what another, another,
investigation, which we also did was

279
00:19:47,839 --> 00:19:54,049
the, we had a lot of, network calls
been sent to get statuses and user

280
00:19:54,049 --> 00:19:57,179
IDs, when the new message was posted.

281
00:19:58,109 --> 00:20:03,579
Now, agreed, there is a little
bit of, what you can say, we

282
00:20:03,579 --> 00:20:08,319
don't have a web socket event
when someone changes the statuses

283
00:20:08,819 --> 00:20:13,989
because there are some performance,
bottlenecks we have from the server.

284
00:20:14,029 --> 00:20:16,619
And that is one of the,
limitations which we have.

285
00:20:16,669 --> 00:20:20,689
That's the reason we don't have a,
we don't send out a WebSocket event

286
00:20:20,699 --> 00:20:22,539
when the user status is changed.

287
00:20:22,889 --> 00:20:27,459
So what we had to resort for is manual
fetching after some amount of time,

288
00:20:27,879 --> 00:20:29,379
fetching again the user statuses.

289
00:20:29,739 --> 00:20:35,369
Now that was repeatedly hammering the
Redux store, updating that, the whole

290
00:20:35,369 --> 00:20:38,009
application was very much freezing.

291
00:20:38,019 --> 00:20:39,259
We had to do something about that.

292
00:20:39,759 --> 00:20:46,959
what we ended up doing was we created
a simple, data manager for statuses

293
00:20:46,959 --> 00:20:52,979
and also varieties, which, which would,
keep a queue of the statuses and then

294
00:20:52,979 --> 00:20:54,829
execute after a certain amount of time.

295
00:20:55,329 --> 00:20:56,369
that solved the issue.

296
00:20:56,399 --> 00:20:59,999
And yeah, that, after how much
interval that this particular

297
00:21:00,169 --> 00:21:03,069
queue has to get cleared, that
is again, configurable better.

298
00:21:03,714 --> 00:21:04,334
application.

299
00:21:04,834 --> 00:21:08,444
And there was also another, I'm
sorry about the network request.

300
00:21:08,454 --> 00:21:12,204
So we also saw like an unwanted
network request, right before.

301
00:21:12,534 --> 00:21:19,764
So the, again, there was one of the thing
which we saw is, we, it was unnecessarily

302
00:21:19,774 --> 00:21:25,024
fetching complete thread when we are
not seeing that particular channel.

303
00:21:25,524 --> 00:21:32,189
So as you can see, We, if you have noticed
all of these PRs and not only this PRs,

304
00:21:32,209 --> 00:21:38,359
but any of the performance PRs have, a
very good backing of reasoning of why

305
00:21:38,399 --> 00:21:44,529
this particular performance improvement
is important by having a sound comparison

306
00:21:44,549 --> 00:21:51,209
between before and after effects of
the PR so that we can see visually.

307
00:21:51,469 --> 00:21:55,899
Okay, this has improved by this particular
percentage or by this particular metric.

308
00:21:56,399 --> 00:22:01,499
and, not only, it helps for the,
but also it helps in the future as

309
00:22:01,509 --> 00:22:04,599
well, when we go back to this PR
and see, okay, why did we do that?

310
00:22:04,949 --> 00:22:07,519
At that particular point of
time, we saw, okay, this was the

311
00:22:07,519 --> 00:22:08,879
improvement which we have seen.

312
00:22:09,379 --> 00:22:11,219
Now, let's get back to

313
00:22:11,719 --> 00:22:15,989
now, by, we have seen a few of
the examples, work, which was done

314
00:22:16,759 --> 00:22:18,329
on Matamoros, the client side.

315
00:22:18,829 --> 00:22:22,799
Now, obviously there are a few caveats
with this, no matter how much performance

316
00:22:22,799 --> 00:22:27,469
improvement you do, network latency
will eat away all your hard work,

317
00:22:28,209 --> 00:22:31,909
Because that's just a hard reality.

318
00:22:32,509 --> 00:22:36,739
So even if you have improved
like 30, 30 milliseconds of your

319
00:22:36,739 --> 00:22:41,119
improvement, if you're a server,
if you have a large latency, all

320
00:22:41,119 --> 00:22:42,939
of that performance goes to waste.

321
00:22:43,439 --> 00:22:44,019
And then.

322
00:22:44,344 --> 00:22:47,324
client, as I've also mentioned
earlier, client side performance

323
00:22:47,364 --> 00:22:52,904
is, has a lot of dependent factors.

324
00:22:53,424 --> 00:22:55,454
Now, for example, what kind
of hardware are they using?

325
00:22:56,044 --> 00:22:57,344
What kind of browser they have?

326
00:22:57,354 --> 00:23:00,464
What kind of other application
they have running alongside

327
00:23:00,464 --> 00:23:01,514
the Matterport application?

328
00:23:01,524 --> 00:23:02,024
So yeah.

329
00:23:02,524 --> 00:23:08,954
So that's the reason I've said comparison
is the key in load testing client.

330
00:23:09,454 --> 00:23:14,754
And it's, it so happens that, the
performance, for example, it so happens

331
00:23:14,754 --> 00:23:19,014
that when the user loads the application
and the first time it is slow, the user

332
00:23:19,014 --> 00:23:21,454
perception is that application is bad.

333
00:23:21,954 --> 00:23:27,394
So user perception regarding
performance is very much different.

334
00:23:27,914 --> 00:23:31,594
That's the reason that you have to do
your own load testing on the client,

335
00:23:31,954 --> 00:23:35,764
or not rely on server's feedback,
or you know what, channel switching

336
00:23:35,764 --> 00:23:39,694
is slow, channel team is slow.

337
00:23:40,474 --> 00:23:43,874
We cannot rely on that information solely.

338
00:23:44,374 --> 00:23:48,214
And then we have to just replicate
exactly what exactly is happening.

339
00:23:48,714 --> 00:23:53,084
with our load testing tool to understand
where and when the problem is arising.

340
00:23:53,584 --> 00:23:57,874
And not all your performances
improvements will be linear.

341
00:23:58,034 --> 00:24:01,334
It might be some might
give you a lot of gain.

342
00:24:01,344 --> 00:24:02,914
Some might give a very less gain.

343
00:24:03,114 --> 00:24:04,924
So that's always there.

344
00:24:05,424 --> 00:24:08,374
Now, with this, I'd like
to conclude my talk.

345
00:24:09,014 --> 00:24:13,674
If you have any questions, please,
reach out to me and I'm available

346
00:24:13,674 --> 00:24:15,754
at my, website over there.

347
00:24:16,044 --> 00:24:17,354
Thank you so much for listening.

348
00:24:18,114 --> 00:24:20,584
And once again, it was lovely.

349
00:24:21,264 --> 00:24:24,444
It was lovely talking,
presenting all of you.

350
00:24:24,584 --> 00:24:25,094
Thank you.

