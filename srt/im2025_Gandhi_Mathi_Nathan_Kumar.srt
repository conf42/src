1
00:00:00,500 --> 00:00:01,489
Hello listeners.

2
00:00:01,589 --> 00:00:02,880
My name is Gandhi Kumar.

3
00:00:03,030 --> 00:00:07,260
I am a principal incident
commander working for Twilio

4
00:00:07,319 --> 00:00:08,760
based out of Seattle, Washington.

5
00:00:09,600 --> 00:00:16,320
And I took interest in participating
in Con 42, 20 25 to take, to talk

6
00:00:16,320 --> 00:00:21,525
about how incident management's
evolving with the advent of AI and how.

7
00:00:22,319 --> 00:00:26,960
There are so many amazing solutions
available readily out of box in

8
00:00:26,960 --> 00:00:31,220
the tools and also things that,
we can build to make life easier.

9
00:00:31,699 --> 00:00:34,480
And today I'm gonna be
talking specifically about.

10
00:00:35,310 --> 00:00:40,050
How AI is changing incident management,
speci spec specifically on incident

11
00:00:40,050 --> 00:00:44,680
response communications at Twilio
and the reason I chose this specific

12
00:00:44,680 --> 00:00:52,010
topic is because there is so much
about detection, resiliency, SRE,

13
00:00:52,010 --> 00:00:56,870
DevOps, but there's very little
content I personally have noticed on.

14
00:00:57,665 --> 00:01:00,245
How incident response
communication happens.

15
00:01:00,245 --> 00:01:05,525
You gotta remember, this is an absolutely
critical aspect that's often overlooked.

16
00:01:06,025 --> 00:01:10,695
So if you know what incidents are it's
essentially a disruption to service.

17
00:01:10,725 --> 00:01:14,625
And at any point in time whenever
something like that happens,

18
00:01:14,625 --> 00:01:16,185
customers are directly impacted.

19
00:01:16,995 --> 00:01:20,285
And if you have been a
vendor or a customer.

20
00:01:20,785 --> 00:01:25,795
You know that communication is
absolutely critical during an incident.

21
00:01:26,545 --> 00:01:28,915
You want to be notified
of what's going on.

22
00:01:28,915 --> 00:01:30,535
You want to see progressive updates.

23
00:01:31,015 --> 00:01:35,665
You want to have updates on the status
page for that specific product that's

24
00:01:35,665 --> 00:01:40,555
impacted because you probably have end
users who are asking you questions on.

25
00:01:41,335 --> 00:01:43,835
To when the product's gonna
be, up and running once again.

26
00:01:44,555 --> 00:01:48,035
And this is true, regardless if it's
like a degradation or if a product's are

27
00:01:48,035 --> 00:01:53,415
down, communication during incidents is
absolutely crucial because that's pretty

28
00:01:53,415 --> 00:01:56,765
much what builds trust between parties.

29
00:01:57,265 --> 00:02:02,320
So I'm gonna walk us through a few
slides, but majority of this is gonna be.

30
00:02:02,820 --> 00:02:07,900
Me talking because I really love to
share, what we have done and how we've

31
00:02:07,900 --> 00:02:09,730
solved this problem to a large scale.

32
00:02:09,730 --> 00:02:14,410
And then towards the end, I'm gonna
probably, try to do like a deep demo and

33
00:02:14,470 --> 00:02:17,839
address some of the potential questions
that I think, people may be having.

34
00:02:18,529 --> 00:02:21,329
So I'll start off by this,
what is incident management?

35
00:02:21,329 --> 00:02:24,480
CQ probably here you probably
know this, but for folks who don't

36
00:02:24,480 --> 00:02:28,689
know, a lot of this started with
fema, implementing nims, which is

37
00:02:28,689 --> 00:02:30,219
National Incident Management System.

38
00:02:30,719 --> 00:02:34,529
And then eventually as technology
companies picked up, everyone realized

39
00:02:34,529 --> 00:02:40,709
the need for a dedicated team of
incident responders whose job essentially

40
00:02:40,709 --> 00:02:47,009
is nothing but getting service back
up and running and that are so many

41
00:02:47,009 --> 00:02:48,779
other teams that collaborate today.

42
00:02:48,829 --> 00:02:53,230
You have your SRE, you have your DevOps,
you have your product owners incident.

43
00:02:53,730 --> 00:02:58,499
Technology owners and depending on
which product's impacted and all of,

44
00:02:58,559 --> 00:03:02,889
a lot of these teams sit together,
and collaborate on an incident given,

45
00:03:03,559 --> 00:03:05,209
what's impacted and who's required.

46
00:03:06,079 --> 00:03:09,679
And the landscape essentially
is changing because I'll give

47
00:03:09,679 --> 00:03:11,119
one specific example here.

48
00:03:11,869 --> 00:03:15,269
If you have worked with most big tech
companies or if you have at least

49
00:03:15,269 --> 00:03:19,619
spoken to people there, you will see
the common theme is a lot of tools.

50
00:03:20,319 --> 00:03:21,279
It's unavoidable.

51
00:03:21,799 --> 00:03:27,409
Every tool provides something that the
other cannot, and you have to work with

52
00:03:27,409 --> 00:03:28,999
all of these tools during incidents.

53
00:03:28,999 --> 00:03:31,459
That's it's absolutely, you can
cut down the number of tools,

54
00:03:31,489 --> 00:03:34,579
but you can't consolidate all
of that to one single tool.

55
00:03:35,059 --> 00:03:36,949
So what I put there is
like a good example of.

56
00:03:37,449 --> 00:03:41,529
The way incidents run at Twilio,
like we use the vendor fire hydrant

57
00:03:41,679 --> 00:03:43,449
to declare incidents, run incidents.

58
00:03:43,729 --> 00:03:48,049
We have PagerDuty to alert and
notify us during incidents and.

59
00:03:48,729 --> 00:03:52,119
We use Zoom to actually
run an incident bridge.

60
00:03:52,179 --> 00:03:56,499
We use Slack for a lot of Async
Chat, and it's 90% of the incidents

61
00:03:56,529 --> 00:03:59,729
essentially are run on Slack because
you need Async communications with

62
00:03:59,729 --> 00:04:00,659
all the different parties that.

63
00:04:01,334 --> 00:04:03,224
That have to be present use.

64
00:04:03,284 --> 00:04:08,155
We use Datadog for dashboards and,
notifying what product service status is.

65
00:04:08,665 --> 00:04:12,504
Atlassian status page for status page
updates to customers Google Docs for

66
00:04:12,504 --> 00:04:14,334
running our post incident retrospectives.

67
00:04:14,884 --> 00:04:17,089
We use Confluence where
we have a ton and tons.

68
00:04:17,589 --> 00:04:23,369
Theoretical knowledge, inner workings
of products saved over, 15, 18 years.

69
00:04:23,829 --> 00:04:27,189
And we use slack canvas for
tracking, action items that

70
00:04:27,189 --> 00:04:27,999
happened during incident.

71
00:04:27,999 --> 00:04:31,959
So essentially you're seeing it's,
there's a lot of tools that are

72
00:04:31,959 --> 00:04:37,399
working simultaneously at play
during incidents and it's unavoidable

73
00:04:37,789 --> 00:04:40,069
and the challenge starts when.

74
00:04:40,569 --> 00:04:43,549
You end up working the tools
instead of working the incident?

75
00:04:43,889 --> 00:04:47,249
I'll repeat that and I'll let
the listeners think about it.

76
00:04:48,029 --> 00:04:54,379
As an incident responder, your prime
focus is mitigation, bringing the

77
00:04:54,379 --> 00:04:56,780
customer services back up and running.

78
00:04:56,959 --> 00:04:58,939
The impact of products
back up and running.

79
00:04:59,439 --> 00:05:03,159
The challenge starts when you spend
too much time working some of these

80
00:05:03,159 --> 00:05:07,209
tools, trying to gather information,
understand impact, trying to gather

81
00:05:07,329 --> 00:05:11,439
data, start documenting things, which
is, don't get me wrong, absolutely

82
00:05:11,439 --> 00:05:16,709
critical, but when you spend too much
time doing that, as an incident commander

83
00:05:16,709 --> 00:05:23,849
or incident manager, your primary
goal or my primary goal is making sure

84
00:05:23,849 --> 00:05:25,799
teams are working on what is required.

85
00:05:26,299 --> 00:05:29,499
Asking the right questions
ensuring that the conversation or

86
00:05:29,499 --> 00:05:31,029
discussion doesn't go off topic.

87
00:05:31,229 --> 00:05:37,849
And we are stay focused on mitigation
ensuring that if someone's too

88
00:05:37,849 --> 00:05:41,479
opinionated, pushing back on them and.

89
00:05:42,304 --> 00:05:46,804
Having everyone focus on the single
goal, which is mitigation, and all of

90
00:05:46,804 --> 00:05:52,364
this needs a ton of focus attention,
which, if diverted from, it's extremely

91
00:05:52,364 --> 00:05:55,024
hard to drive the incident and
take it all the way to mitigation.

92
00:05:55,524 --> 00:05:56,574
What is the solution?

93
00:05:56,624 --> 00:06:01,004
How can AI help with internal external
incident response communications before,

94
00:06:01,004 --> 00:06:02,444
during, and after incidents like.

95
00:06:02,799 --> 00:06:07,079
That's, as I said, that's the primary
focus that I wanted to talk about today.

96
00:06:07,579 --> 00:06:10,879
Here's like a good snippet that
I found on Atlassian's website.

97
00:06:11,659 --> 00:06:14,889
And the top, what is,
oh, something's wrong.

98
00:06:15,219 --> 00:06:16,989
Monitoring tool alerts and escalates.

99
00:06:17,139 --> 00:06:19,059
The on-call team joins everyone.

100
00:06:19,059 --> 00:06:21,909
Swarms back to normal document.

101
00:06:22,824 --> 00:06:24,804
What failed, remediate, move on, right?

102
00:06:24,834 --> 00:06:28,014
That's what we see, but
here's what our customers see.

103
00:06:28,254 --> 00:06:29,154
Something's wrong.

104
00:06:29,934 --> 00:06:30,864
They don't know about it.

105
00:06:31,364 --> 00:06:32,294
What's going on?

106
00:06:32,774 --> 00:06:34,364
We haven't gotten communications.

107
00:06:34,414 --> 00:06:35,974
The status page is still green.

108
00:06:36,584 --> 00:06:37,364
They start panicking.

109
00:06:37,364 --> 00:06:38,984
They start calling their account managers.

110
00:06:38,984 --> 00:06:42,534
They start filing in those
Zendesk, tickets or any support

111
00:06:42,534 --> 00:06:43,704
tickets that, that they have.

112
00:06:43,804 --> 00:06:46,414
And most companies, seem
to use Zendesk, but.

113
00:06:46,939 --> 00:06:49,879
Just to give an example, and
they start emailing, creating a

114
00:06:49,879 --> 00:06:51,590
ton of, like a flood of support.

115
00:06:51,590 --> 00:06:55,429
Tickets start to come in and
they assume the worst they get

116
00:06:55,429 --> 00:06:57,530
angry, which is very normal.

117
00:06:57,530 --> 00:07:01,449
You are paying a huge amount of
money for those products and services

118
00:07:01,449 --> 00:07:04,599
and for them to be down and you
are end users to, to feel the pain.

119
00:07:04,650 --> 00:07:05,010
It's.

120
00:07:05,655 --> 00:07:06,165
Frustrating.

121
00:07:07,125 --> 00:07:10,435
And they're back to normal, but
less trust in this company, right?

122
00:07:10,675 --> 00:07:14,795
Because here is, I've been working
across different industries

123
00:07:14,795 --> 00:07:16,145
over the course of 15 years.

124
00:07:16,245 --> 00:07:22,125
I worked in carrier avionics banking,
finance and now communications platform as

125
00:07:22,125 --> 00:07:24,105
a service, which is which is with Twilio.

126
00:07:24,885 --> 00:07:28,845
One thing that's common is if
you do not communicate well

127
00:07:28,845 --> 00:07:30,165
enough with your customers.

128
00:07:30,730 --> 00:07:35,530
From the time an incident starts
all the way till it ends, what

129
00:07:35,530 --> 00:07:36,640
you're gonna do to fix it?

130
00:07:36,880 --> 00:07:39,040
Are you making sure this
doesn't happen again?

131
00:07:39,040 --> 00:07:42,790
Future prevention, your
classic idle terminology.

132
00:07:43,390 --> 00:07:47,110
If you don't do that,
customers lose trust.

133
00:07:47,540 --> 00:07:51,260
One incident, two incidents,
three incidents, four incidents.

134
00:07:52,085 --> 00:07:56,685
As time progresses, trust is broken
and once broken it's very hard to men.

135
00:07:56,685 --> 00:07:59,655
So communication during
incidents, absolutely critical.

136
00:08:00,155 --> 00:08:04,885
So one of the hackathon projects
this year that I chose to work

137
00:08:04,935 --> 00:08:08,100
and build for Twilio was a bot.

138
00:08:08,700 --> 00:08:14,640
A bot that can gather information
real time and present it.

139
00:08:15,140 --> 00:08:20,370
With the simplicity of a slack command
since one of the, one of the principles

140
00:08:20,370 --> 00:08:24,190
that I really like to stick with
is when you're running an incident,

141
00:08:24,190 --> 00:08:27,770
as I said at the start, when you're
juggling different tool juggling between

142
00:08:27,770 --> 00:08:29,330
different tools, you lose a lot of time.

143
00:08:29,900 --> 00:08:34,020
What if you could stay in a single
tool, let's say Slack and get

144
00:08:34,020 --> 00:08:37,540
all the answers you need without
having to juggle and bounce around?

145
00:08:38,260 --> 00:08:42,510
I'm not saying the other tools are not
important, but it's the frequency of

146
00:08:42,510 --> 00:08:48,030
how often you juggle and move around is
what determines how quickly and how much

147
00:08:48,030 --> 00:08:50,115
more present you are on an incident.

148
00:08:50,505 --> 00:08:53,595
As the incident commander,
as the incident manager.

149
00:08:54,195 --> 00:08:58,040
So I'll not dive into details, but
I'll try to keep it high level here.

150
00:08:58,090 --> 00:09:02,120
And then if you are curious and
I'll leave a link to my LinkedIn.

151
00:09:02,210 --> 00:09:03,080
We can chat more.

152
00:09:03,080 --> 00:09:05,920
I'm happy to share, how we did
this but for the purposes of,

153
00:09:06,000 --> 00:09:07,650
con 42, I'm gonna share it.

154
00:09:08,445 --> 00:09:11,085
Idea, and I'll give as
much details as I can.

155
00:09:11,325 --> 00:09:14,515
But we used a tool called Windmill
and we ended up building a Slack

156
00:09:14,545 --> 00:09:17,615
app, which makes a call to open ai.

157
00:09:17,615 --> 00:09:22,875
And then, which is again, integrated
to every single tool that we have that

158
00:09:22,875 --> 00:09:24,255
I showed you on the previous slide.

159
00:09:24,405 --> 00:09:28,875
Zoom voice, zoom chat, slack,
chat, everything's in the chat.

160
00:09:29,055 --> 00:09:30,225
Fire had our incident.

161
00:09:30,615 --> 00:09:31,335
A management tool.

162
00:09:32,160 --> 00:09:36,180
Wiki pages because all the theoretical
knowledge about how customers use the

163
00:09:36,180 --> 00:09:41,600
products are on our Confluence and Wiki
pages, which is like a ton of information,

164
00:09:41,960 --> 00:09:46,440
but accessibility, on exactly what
you need when you need in an incident.

165
00:09:46,470 --> 00:09:47,040
That's hard.

166
00:09:47,850 --> 00:09:53,370
And we've integrated it in such a way
where we can tap into that information.

167
00:09:53,870 --> 00:09:54,680
20 seconds.

168
00:09:54,860 --> 00:09:56,540
So simple commands in slack.

169
00:09:56,840 --> 00:10:02,200
Ask what you want and it'll give you
real time contextual information.

170
00:10:02,980 --> 00:10:06,020
And we can even one can even give a
persona, which is what we ended up

171
00:10:06,020 --> 00:10:12,890
doing, where we can give each command
a persona and the persona essentially.

172
00:10:13,655 --> 00:10:16,445
Describes who you are
asking that information for.

173
00:10:16,445 --> 00:10:21,725
So what I did this over here is
created a command saying when I

174
00:10:21,725 --> 00:10:24,905
type this command with an incident
number, I want you to give me.

175
00:10:25,405 --> 00:10:29,285
What a technical customer would want
to know about this incident as a,

176
00:10:29,285 --> 00:10:32,585
somebody, as a somebody and another
command saying, give me a high

177
00:10:32,585 --> 00:10:37,405
level status page post that I can
integrate and post on, status page.io.

178
00:10:38,155 --> 00:10:41,515
And then another command dedicated
to asking questions that I can

179
00:10:41,515 --> 00:10:44,335
literally go in and just ask
the dumbest of dumb questions.

180
00:10:44,335 --> 00:10:47,785
Silliest of silly questions that
people are often reluctant to

181
00:10:47,785 --> 00:10:49,675
interrupt the triage bridge.

182
00:10:50,080 --> 00:10:50,950
And ask, right?

183
00:10:50,950 --> 00:10:56,710
Because if the people who are fixing
it start getting distracted and

184
00:10:56,710 --> 00:11:00,680
start answering you on what's broken
it's a slippery slope because you

185
00:11:00,680 --> 00:11:04,620
are getting updates now, but you're
also losing precious time where

186
00:11:04,620 --> 00:11:06,125
actually something could be fixed.

187
00:11:06,815 --> 00:11:09,550
I'll go ahead and give you a
quick demo, and I'll try to speak.

188
00:11:09,850 --> 00:11:13,940
About, what we thought about and
what was the ideology behind it it

189
00:11:13,940 --> 00:11:15,200
and how we are actually using it.

190
00:11:15,800 --> 00:11:21,350
So for the demos right now, if you see
here, I've created like a public Slack

191
00:11:21,350 --> 00:11:23,870
channel, only for this demo Con 42 demo.

192
00:11:24,380 --> 00:11:27,340
And in this channel and
this is something we.

193
00:11:27,730 --> 00:11:31,100
People also run event, people
run in incident channels as well.

194
00:11:31,100 --> 00:11:33,665
But just for the sake of
this demo I'll show you this.

195
00:11:34,055 --> 00:11:38,845
So right here the three commands that
I mentioned that we ended up, creating

196
00:11:38,845 --> 00:11:42,295
for this, and I keep saying we, because
there's like a team of six, seven

197
00:11:42,295 --> 00:11:46,165
people who worked on it within Twilio,
just for the hackathon to solve the

198
00:11:46,165 --> 00:11:50,105
common problem that, that, that was
in front of us every single incident.

199
00:11:50,965 --> 00:11:55,025
Windmill HVC is for generating
technical summary for customers.

200
00:11:55,245 --> 00:12:01,625
Windmill sp is for status page summary
or, simpler motor genetic explanation.

201
00:12:01,985 --> 00:12:06,155
And then windmill Gen is our gen
command where we can essentially.

202
00:12:06,605 --> 00:12:08,535
Ask questions on what we want.

203
00:12:08,775 --> 00:12:11,775
So for the purpose of this I'll
show you the Vin Gen command.

204
00:12:11,775 --> 00:12:12,645
Let's take an example.

205
00:12:12,645 --> 00:12:17,415
I can simply go ahead and say,
Hey, VIN Gen what are Twilio's core

206
00:12:17,415 --> 00:12:21,280
products and how do customers use them?

207
00:12:21,780 --> 00:12:25,380
Now if you look at it, all
this knowledge and info.

208
00:12:26,340 --> 00:12:28,920
It's present internally
in our help center.

209
00:12:28,920 --> 00:12:32,140
It's tons and tons to hundreds,
thousands of documents.

210
00:12:32,830 --> 00:12:35,040
The accessibility is what this was about.

211
00:12:35,620 --> 00:12:39,940
And in, in the next, I'd say about few
seconds, if you see here, just gave me

212
00:12:39,940 --> 00:12:46,600
a response and it started explaining
for every product what it means, what

213
00:12:46,600 --> 00:12:49,600
customers use about, yeah, not impressed.

214
00:12:49,900 --> 00:12:50,830
I'll show you one more.

215
00:12:51,250 --> 00:12:59,210
What if we go to a partner where, say
email Jan my customer is reporting a 5 0

216
00:12:59,210 --> 00:13:04,055
3 adder on all of their Twilio services.

217
00:13:04,555 --> 00:13:06,520
What could this mean?

218
00:13:07,020 --> 00:13:12,330
Again the reason such features were
important is we have tons and tons

219
00:13:12,330 --> 00:13:16,350
of different teams who talk with
different parties, like customers,

220
00:13:16,410 --> 00:13:20,670
account executives and all of them
have different varieties of questions.

221
00:13:21,120 --> 00:13:22,215
And it's very important.

222
00:13:22,715 --> 00:13:26,155
It was very important to, to be to have
a channel that anyone can literally

223
00:13:26,155 --> 00:13:27,625
ask anything and get answers they need.

224
00:13:27,895 --> 00:13:31,675
So if you see here, it was able to
describe why the 5 0 3 error means service

225
00:13:31,675 --> 00:13:36,735
unavailable common causes, and then it was
also able to point some troubleshooting

226
00:13:36,815 --> 00:13:40,805
articles in our internal product docs
that we have for customers on how they can

227
00:13:40,805 --> 00:13:44,385
solve such an issue or what logs can they
gather when they encounter such an issue.

228
00:13:44,865 --> 00:13:46,275
Now I have a few.

229
00:13:46,770 --> 00:13:49,020
I have a few test incidents right here.

230
00:13:49,100 --> 00:13:52,130
These are incidents we have created on our
incident tool, so I'm gonna be using them

231
00:13:52,130 --> 00:13:58,790
to show you what the and again just to
remind, when I generate these, it's gonna

232
00:13:58,790 --> 00:14:01,760
be static, but during an actual incident.

233
00:14:02,260 --> 00:14:04,060
The information keeps evolving.

234
00:14:04,330 --> 00:14:07,630
So for an example, the way I'm talking
right now, so if this was an actual

235
00:14:07,630 --> 00:14:13,660
incident, everything I'm speaking real
time right here, gets transcribed, is

236
00:14:13,660 --> 00:14:19,300
made available to our AI model, and it
can contextualize between what is being

237
00:14:19,300 --> 00:14:24,790
said, real time on Zoom, what is being
available, what's available in our wiki

238
00:14:24,790 --> 00:14:28,990
pages, in our theoretical knowledge
base, and understand what exactly.

239
00:14:29,695 --> 00:14:32,515
Is the user asking for and
come up with that app summary.

240
00:14:32,755 --> 00:14:39,815
So if I say four slash windmill
HVC 1, 5, 0 8, 9, now it's

241
00:14:39,815 --> 00:14:41,225
actually like a test incident.

242
00:14:41,665 --> 00:14:46,720
And then when I mention, 1, 5, 0, 8,
9, give me a technical verbiage that

243
00:14:46,720 --> 00:14:48,490
I can go speak to my customers with.

244
00:14:49,060 --> 00:14:50,350
And it'll instantly say.

245
00:14:50,850 --> 00:14:53,870
This was a test incident
initiated and quickly resolved

246
00:14:53,870 --> 00:14:55,010
with just over two minutes.

247
00:14:55,010 --> 00:14:56,210
There was no customer impacts.

248
00:14:56,310 --> 00:14:59,610
And then, the best part
is I can quiz it further.

249
00:15:00,205 --> 00:15:04,180
Email Jen, what time did the incident?

250
00:15:05,170 --> 00:15:07,450
1, 5 0 8, 9 start.

251
00:15:07,950 --> 00:15:11,240
And not just the time there's,
there are layers of questions.

252
00:15:11,290 --> 00:15:15,660
One can ask just dive de into extreme
details to what this person did.

253
00:15:16,020 --> 00:15:18,830
When did we find out about this
first, what was the anomaly?

254
00:15:19,400 --> 00:15:22,130
And it'll give exact details of real time.

255
00:15:22,130 --> 00:15:27,230
And this is a lifesaver because
if you have been in incidents and

256
00:15:27,230 --> 00:15:31,070
you've seen incident chats, there are
hundreds if not thousands of messages

257
00:15:31,070 --> 00:15:32,390
in a matter of minutes and hours.

258
00:15:33,020 --> 00:15:35,300
And it's hard to keep track of what's.

259
00:15:35,800 --> 00:15:38,820
It's being said, and what exactly
is the info that you are looking

260
00:15:38,820 --> 00:15:42,910
for, especially when the incident
starts and you are it's chaos, right?

261
00:15:42,910 --> 00:15:46,150
The first few minutes are often chaos
because you're trying to wrap your

262
00:15:46,150 --> 00:15:48,070
head around where the issue is, right?

263
00:15:48,070 --> 00:15:48,880
Who caused this?

264
00:15:48,880 --> 00:15:49,660
Was it a vendor?

265
00:15:49,690 --> 00:15:50,410
Is it upstream?

266
00:15:50,410 --> 00:15:51,100
Is it downstream?

267
00:15:51,370 --> 00:15:52,420
Is it us?

268
00:15:52,720 --> 00:15:54,140
Is it, a carrier?

269
00:15:54,190 --> 00:15:55,840
It's absolutely chaotic.

270
00:15:56,120 --> 00:15:59,420
And the other thing that we found,
this model is super real time.

271
00:15:59,420 --> 00:16:04,900
So it generates responses like in, within
20 seconds, but even like right one minute

272
00:16:04,900 --> 00:16:08,600
into an incident run this model and it
generates answers super, super quickly.

273
00:16:09,040 --> 00:16:10,040
So let's create another one.

274
00:16:10,370 --> 00:16:13,380
I'll say in sp now this
time I'm gonna ask it.

275
00:16:13,980 --> 00:16:17,310
Different example to create
like a status page post for me.

276
00:16:17,770 --> 00:16:21,630
This is one of the things we are
actively exploding and doing,

277
00:16:21,660 --> 00:16:24,420
a prototype testing on to see.

278
00:16:24,830 --> 00:16:29,270
Because the eventual goal is to automate
this directly to our status page.

279
00:16:29,690 --> 00:16:33,360
So when we have an incident we can be
confident enough that we have trained

280
00:16:33,360 --> 00:16:39,270
the model to take the info, filter out
internal stuff versus external, and then

281
00:16:39,270 --> 00:16:42,000
automatically post on the status page.

282
00:16:42,090 --> 00:16:45,600
Imagine how impactful and.

283
00:16:46,100 --> 00:16:49,130
Efficient that would be for
customers to get notifications.

284
00:16:49,760 --> 00:16:54,050
So if you see here, it literally creates
it in a title and body format, which

285
00:16:54,050 --> 00:16:56,030
is very standard and status page.

286
00:16:56,030 --> 00:16:59,120
It'll be a title and there'll be a body
that describes what the incident is.

287
00:16:59,620 --> 00:17:00,420
And yeah.

288
00:17:00,480 --> 00:17:08,060
And it's super, super useful to take away
all the focus, and all the noise out of,

289
00:17:08,160 --> 00:17:12,460
people were actually like troubleshooting
and just put all that all that in a

290
00:17:12,460 --> 00:17:16,670
summarized systemic way for people
to be able to like, question and get

291
00:17:16,670 --> 00:17:20,060
answers to hey, it's, it super simple.

292
00:17:20,370 --> 00:17:22,990
Tool, but it is absolutely effective.

293
00:17:22,990 --> 00:17:27,065
And then the best part is I won't
be able to do it here, but we can

294
00:17:27,065 --> 00:17:29,245
generate post incident retrospective.

295
00:17:29,245 --> 00:17:30,715
We can generate out of four drafts.

296
00:17:30,745 --> 00:17:34,745
We can literally ask what we want and
the model will generate a lot of these

297
00:17:34,745 --> 00:17:38,945
for us to be able to like, use it for
communications internally and externally.

298
00:17:39,445 --> 00:17:40,955
And so coming back.

299
00:17:41,455 --> 00:17:44,515
Yeah, that's how quickly this
model's been responding so far.

300
00:17:44,515 --> 00:17:45,835
It's less than 30 seconds.

301
00:17:45,935 --> 00:17:48,235
And, getting answers
in a real time manner.

302
00:17:48,805 --> 00:17:49,635
And yes.

303
00:17:49,695 --> 00:17:50,235
That's me.

304
00:17:50,625 --> 00:17:53,475
You can scan that QR code,
it'll take you to my LinkedIn.

305
00:17:53,535 --> 00:17:56,295
I am based outta Seattle, so if
you are in the area, let me know.

306
00:17:56,775 --> 00:17:59,715
Would love to catch up for
a coffee, lunch, whatever.

307
00:18:00,195 --> 00:18:02,925
And yeah, I'm always
open to collaborating.

308
00:18:03,900 --> 00:18:08,330
Learning sharing what I know on
how to make incidents better.

309
00:18:08,360 --> 00:18:11,820
Incident response is something
super near and dear to me.

310
00:18:11,820 --> 00:18:16,350
I've been doing this for, as I
said, like 15 years at this point.

311
00:18:16,350 --> 00:18:20,760
And communication during incident
responses is something that I

312
00:18:20,790 --> 00:18:23,080
started exploring, late last year.

313
00:18:23,080 --> 00:18:27,965
And when I realized how important
that is and how prudent it is to keep.

314
00:18:28,430 --> 00:18:31,460
Customers come and earn that
trust or retain that trust

315
00:18:31,910 --> 00:18:33,590
even in the toughest of times.

316
00:18:33,830 --> 00:18:37,170
So once again thank you con
42 for this opportunity and

317
00:18:37,170 --> 00:18:38,310
thank you all for listening.

318
00:18:38,780 --> 00:18:39,970
Have a great rest of your day.

