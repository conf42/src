1
00:00:00,690 --> 00:00:03,300
Let's talk about multi hop,
big data pipelines today.

2
00:00:03,900 --> 00:00:07,860
In this day and age of J and AI and
machine learning, it is inevitable for

3
00:00:07,860 --> 00:00:12,450
businesses not only to maintain their own
big data ecosystem, but also to ensure

4
00:00:12,480 --> 00:00:13,830
that it is scalable and performant.

5
00:00:14,330 --> 00:00:15,950
Hi, I'm Hardik Patel.

6
00:00:16,219 --> 00:00:19,310
I have around 20 years of experience
in tech industry working for both

7
00:00:19,310 --> 00:00:21,245
small startups and large enterprises.

8
00:00:21,845 --> 00:00:25,294
I hold Master's degree in software
engineering from Bits, Ani, and I have

9
00:00:25,294 --> 00:00:29,225
held various tech leadership roles at
high tech places like Yahoo, Symantec,

10
00:00:29,345 --> 00:00:31,174
Groupon, PayPal, to name a few.

11
00:00:31,625 --> 00:00:36,365
I had opportunity to lead teams
ranging from five plus people to

12
00:00:36,365 --> 00:00:40,985
30 plus organization, comprising of
engineers, tech leads and managers.

13
00:00:41,285 --> 00:00:44,794
I have over a decade of experience in
designing and developing distributed

14
00:00:44,794 --> 00:00:46,565
systems and big data architecture.

15
00:00:47,065 --> 00:00:50,965
So before we dive deep into big
data pipelines, let us first

16
00:00:50,965 --> 00:00:55,105
understand what are typical hops
involved in such big data pipelines?

17
00:00:55,885 --> 00:01:01,255
It typically starts with data extraction,
so as the name suggests, this is usually

18
00:01:01,285 --> 00:01:03,205
towards the intake side of data pipeline.

19
00:01:03,235 --> 00:01:08,545
And this stage is mostly involved in
grading data from variety of sources,

20
00:01:08,545 --> 00:01:12,955
ranging from structure to semi-structured,
to completely unstructured data.

21
00:01:13,750 --> 00:01:19,210
The most essential aspect of this stage
is to ensure connectors to disparate input

22
00:01:19,210 --> 00:01:24,730
data sources are resilient and there are
sufficient data quality checks at and to

23
00:01:24,730 --> 00:01:29,260
ensure no invalid data enters into the
system, into the pipeline from the get-go.

24
00:01:29,920 --> 00:01:34,630
The second aspect, or second hop in such
data pipeline is data transformation.

25
00:01:35,230 --> 00:01:40,060
Besides cleansing the data, the stage
is designed to format, normalize, and

26
00:01:40,060 --> 00:01:42,190
enrich data for faster processing.

27
00:01:42,520 --> 00:01:46,090
Thereby reducing overall data
availability, lag through the

28
00:01:46,090 --> 00:01:47,770
subsequent stages of data pipeline.

29
00:01:48,270 --> 00:01:53,790
Data analysis is another important aspect
of stages within the data pipeline.

30
00:01:54,120 --> 00:01:58,890
So once the data is cleansed and prepped
up for faster processing, subsequent

31
00:01:58,890 --> 00:02:03,840
stages take up the responsibility of
applying complex event processing and

32
00:02:03,840 --> 00:02:07,560
relevant machine learning algorithms
to further extract meaningful

33
00:02:07,560 --> 00:02:09,630
insights in near real time manner.

34
00:02:10,410 --> 00:02:15,000
The last aspect of a typical data
pipeline is action and storage.

35
00:02:15,240 --> 00:02:19,500
So such data pipelines are essentially
in orchestrating automated responses to

36
00:02:19,500 --> 00:02:25,980
key events while also persisting process
data with optimized storage footprint.

37
00:02:26,480 --> 00:02:32,240
Now that we have looked at usual hops
in a data pipeline, let us dive into

38
00:02:32,690 --> 00:02:37,220
various core design principles that are
employed in building such pipelines.

39
00:02:37,955 --> 00:02:41,725
We will start with, fundamental
parallelism principle.

40
00:02:41,995 --> 00:02:44,905
So one of the key principles in
such pipelines is parallelism.

41
00:02:45,295 --> 00:02:48,835
It is one of the fundamental tenant
of such pipelines that help enable

42
00:02:49,135 --> 00:02:53,245
concurrent execution through means
of effectively partitioning workloads

43
00:02:53,515 --> 00:02:57,625
across available computational nodes,
thereby achieving maximum throughput.

44
00:02:58,125 --> 00:03:03,855
Scalability is another design goal is to
ensure we are able to elastically expand

45
00:03:03,855 --> 00:03:08,865
both horizontally and vertically in
response to fluctuating and ever evolving

46
00:03:09,135 --> 00:03:11,715
workloads of data volume and velocity.

47
00:03:12,435 --> 00:03:16,935
Fault tolerance is also inevitable in
such complex environment of distributed

48
00:03:16,940 --> 00:03:18,584
systems and parallel processing.

49
00:03:18,975 --> 00:03:24,105
It helps us ensure we are meaningfully
handling errors and also in some cases

50
00:03:24,225 --> 00:03:26,565
have automated recovery and self-healing.

51
00:03:27,495 --> 00:03:30,855
Mechanisms to ensure overall
continuity of such data.

52
00:03:30,855 --> 00:03:31,575
Pipelines.

53
00:03:31,965 --> 00:03:36,735
Throughput optimization implies we
are able to craft efficient data parts

54
00:03:36,765 --> 00:03:41,685
with minimal overhead of serialization,
optimal memory utilization, and effective

55
00:03:41,685 --> 00:03:46,935
data partitioning to balance overall
volume and velocity of incoming data.

56
00:03:47,895 --> 00:03:52,125
So now that we have looked at
typical design principles involved in

57
00:03:52,270 --> 00:03:53,685
building, building this data pipelines.

58
00:03:54,090 --> 00:03:58,680
Let us look at some of the very popular
industry technology stack and tools and

59
00:03:58,680 --> 00:04:01,110
frameworks used to build such pipelines.

60
00:04:01,740 --> 00:04:04,950
Apache Kafka is an industry
leading messaging system.

61
00:04:05,250 --> 00:04:09,000
It is calibrated to handle more
than a hundred thousand messages

62
00:04:09,000 --> 00:04:13,280
per second, throughput with built-in
capabilities of partitioning,

63
00:04:13,520 --> 00:04:15,410
replication, and fault tolerance.

64
00:04:15,890 --> 00:04:20,269
Spark is another very popular dis
distributed data processing framework.

65
00:04:20,720 --> 00:04:21,230
It is.

66
00:04:21,485 --> 00:04:25,684
Hundred times faster than its
in-memory Hadoop counterpart.

67
00:04:26,194 --> 00:04:30,725
Besides that, it natively supports SQL
machine learning models, deployment

68
00:04:30,814 --> 00:04:32,944
and graph data processing workloads.

69
00:04:33,395 --> 00:04:39,215
AWS Lambda happens to be the perfect
technology that exhibits elasticity and is

70
00:04:39,215 --> 00:04:44,975
adaptable to ever evolving and fluctuating
data volume, velocity, and variety.

71
00:04:45,710 --> 00:04:50,870
It helps automatically scales from
few requests per day to thousands

72
00:04:50,870 --> 00:04:56,329
per second, and has a very attractive
pay only for use pricing model.

73
00:04:56,659 --> 00:04:59,720
It is very popular amongst
the companies and businesses

74
00:05:00,230 --> 00:05:02,840
operating in Amazon's AWS Cloud.

75
00:05:03,710 --> 00:05:08,539
Apache Fling is yet another distributed
data processing framework that supports

76
00:05:08,569 --> 00:05:11,330
constructs, like exactly one semantics.

77
00:05:11,720 --> 00:05:15,350
Event time processing
sophisticated windowing operations

78
00:05:15,380 --> 00:05:17,090
with subsequent latency.

79
00:05:17,590 --> 00:05:22,410
Now that we have looked at various
technologies, at, at use and different

80
00:05:22,410 --> 00:05:27,300
designing and developing big data
pipelines, let us touch upon how, what

81
00:05:27,300 --> 00:05:35,010
are some usual performance optimization
techniques in maintaining and ensuring

82
00:05:35,010 --> 00:05:36,575
that these pipelines are performed.

83
00:05:37,110 --> 00:05:39,720
So first strategy is
caching strategy idea.

84
00:05:40,080 --> 00:05:44,880
In case of caching strategy is to store
frequently access data elements to

85
00:05:44,880 --> 00:05:49,470
further minimize redundant computation
and DIS and database IO calls.

86
00:05:49,920 --> 00:05:53,790
Data partitioning is another performance
improvement techniques, which is

87
00:05:53,850 --> 00:05:58,500
used to strategically segment data
sets to enable parallel processing

88
00:05:58,530 --> 00:06:00,360
and reduce overall processing Time.

89
00:06:01,020 --> 00:06:02,640
Load balancing is a means.

90
00:06:02,655 --> 00:06:07,905
To dynamically distribute computational
workloads to prevent resource bottlenecks,

91
00:06:07,965 --> 00:06:10,725
and in turn, optimize overall throughput.

92
00:06:11,415 --> 00:06:15,345
Resource allocation is a way to
precisely allocate resources like

93
00:06:15,345 --> 00:06:20,025
CPU Memory Network based on workload
characteristics and business priorities.

94
00:06:20,475 --> 00:06:24,155
The last one is optimized data
formats, and it is a very popular

95
00:06:24,155 --> 00:06:28,415
technique to choose formats like
Columnal format binary formats.

96
00:06:28,715 --> 00:06:32,405
Which further helps us reduce
IU operations and minimize

97
00:06:32,465 --> 00:06:34,025
data transfer overheads.

98
00:06:34,625 --> 00:06:40,685
So with that, now let's dive deeper into
various scaling challenges and potential

99
00:06:40,685 --> 00:06:43,385
solutions involved in those challenges.

100
00:06:43,885 --> 00:06:47,415
So the first, scale popular
scaling challenges, data skewness.

101
00:06:47,845 --> 00:06:48,775
what is data skew?

102
00:06:49,165 --> 00:06:52,015
So it implies to uneven
distribution of data.

103
00:06:52,360 --> 00:06:56,620
It le leading to performance and scale
bottlenecks and resource utilization

104
00:06:56,650 --> 00:06:59,080
inefficiencies across clusters of nodes.

105
00:06:59,950 --> 00:07:04,600
A typical solution that we employ
in order to tackle data skewness is

106
00:07:04,600 --> 00:07:08,560
to implement adaptive partitioning
with real time workload monitoring

107
00:07:08,590 --> 00:07:13,900
and dynamic re rebalancing to ensure
optimal processing across compute nodes.

108
00:07:14,410 --> 00:07:16,690
State management is
another scaling challenge.

109
00:07:17,065 --> 00:07:20,485
What happens in case of state
management is that with maintaining

110
00:07:20,485 --> 00:07:24,535
stateful operations, it significantly
limits our ability to horizontally

111
00:07:24,535 --> 00:07:25,945
scale distributed system.

112
00:07:26,605 --> 00:07:31,975
So typical solution that we employ in
this case is we actually deploy something

113
00:07:31,975 --> 00:07:37,165
like fault tolerant distributed state
stores with tiered caching architecture

114
00:07:37,585 --> 00:07:39,535
and configurable eviction policies.

115
00:07:39,745 --> 00:07:43,885
It helps us to preserve consistency
while maximizing overall.

116
00:07:44,200 --> 00:07:47,560
Throughput at scale across
various stages of data.

117
00:07:47,560 --> 00:07:51,400
Pipeline back pressure handling
is another scaling challenge.

118
00:07:52,200 --> 00:07:58,780
this occurs when a typical, high velocity
data producers ends up overwhelming

119
00:07:58,990 --> 00:08:01,870
a relatively slower consumer system.

120
00:08:02,440 --> 00:08:06,130
It actually leads to system
instability, and in some of the cases

121
00:08:06,130 --> 00:08:07,960
also leads to potential data loss.

122
00:08:08,890 --> 00:08:13,330
What a typical solution that can be
employed in this case is to implement

123
00:08:13,360 --> 00:08:18,160
intelligent rate limiting algorithms,
priority based buffering cues and

124
00:08:18,160 --> 00:08:22,660
adaptive throttling mechanism that
will, this will certainly helps us

125
00:08:22,720 --> 00:08:28,420
maintain system illiquid equilibrium and
of at the time of wearing data loops.

126
00:08:29,290 --> 00:08:32,870
Now let us look, so whenever
we are talking about big data

127
00:08:32,870 --> 00:08:35,000
pipelines, you cannot avoid.

128
00:08:35,465 --> 00:08:36,965
Talking about observability.

129
00:08:37,235 --> 00:08:40,155
So let's touch upon a little bit
of monitoring and observability.

130
00:08:40,365 --> 00:08:45,375
So how do we ensure that all of this
complex ecosystem and distributed

131
00:08:45,375 --> 00:08:49,515
compute nodes and storage nodes
are always observable and we always

132
00:08:49,755 --> 00:08:53,805
are able to stay on top of anything
that is happening under the hood?

133
00:08:54,305 --> 00:08:56,945
So what are various ways to
achieve effective observability?

134
00:08:57,395 --> 00:09:00,305
So one of the popular mechanism
is metrics collection.

135
00:09:01,025 --> 00:09:02,915
Metrics collection is inevitable.

136
00:09:02,990 --> 00:09:07,760
To track mission critical performance
indicators like end-to-end latency,

137
00:09:08,030 --> 00:09:12,350
throughput at each and every processing
stage, and resource computation trends.

138
00:09:12,680 --> 00:09:18,020
So when in case of metrics collection,
it not only helps us track the

139
00:09:18,170 --> 00:09:22,970
effective and metrics in a meaningful
way, but it also helps us stay

140
00:09:22,970 --> 00:09:25,790
proactive in case something goes south.

141
00:09:26,570 --> 00:09:29,270
The second aspect of
observability is log analysis.

142
00:09:29,390 --> 00:09:31,610
So we capture detailed event logs.

143
00:09:32,000 --> 00:09:37,280
as another aspect of keeping systems
observable, it helps us identify error

144
00:09:37,280 --> 00:09:42,590
trends and improves our overall time
required to conduct root cause analysis.

145
00:09:42,740 --> 00:09:48,080
In the event of various performance
issues, DTrace helps engineering

146
00:09:48,080 --> 00:09:53,690
teams to create correlation amongst
distributed computing notes and helps us

147
00:09:53,690 --> 00:09:59,870
ensure in pre taking preemptive action
before performance degrades of this.

148
00:10:00,350 --> 00:10:02,090
Typical big data pipelines.

149
00:10:02,690 --> 00:10:06,440
So now with that, let us take
a look at what are some of the

150
00:10:06,440 --> 00:10:08,060
key takeaways of this session.

151
00:10:08,560 --> 00:10:10,600
So architecture matters.

152
00:10:11,020 --> 00:10:14,650
To employ architectures that
facilitates this performance criteria

153
00:10:14,740 --> 00:10:18,700
is inevitable in case of distributor
systems and big data pipeline.

154
00:10:19,510 --> 00:10:24,580
Technology selection is another important
aspect, which helps us ensure that we

155
00:10:24,580 --> 00:10:26,345
are leveraging the right framework.

156
00:10:26,875 --> 00:10:29,185
Our tool for business problem at hand.

157
00:10:29,905 --> 00:10:33,805
Continuous optimization is also
very important, which helps us

158
00:10:33,805 --> 00:10:37,975
regularly calibrate and benchmark
key performance indicators to stay

159
00:10:37,975 --> 00:10:43,015
proactive with ever evolving data
volume, velocity, and variety.

160
00:10:43,795 --> 00:10:47,315
Business alignment, this goes
without saying whatever we

161
00:10:47,315 --> 00:10:50,645
build must be in alignment with
impactful business outcomes.

162
00:10:50,825 --> 00:10:55,385
There is no technology solution without
a meaningful business associated to it.

163
00:10:55,885 --> 00:10:59,395
I hope you enjoyed this talk as
much as I enjoyed preparing for it.

164
00:10:59,755 --> 00:11:00,835
Thank you for your time today.

