1
00:00:00,500 --> 00:00:01,039
Hi everyone.

2
00:00:01,340 --> 00:00:01,880
Good morning.

3
00:00:02,030 --> 00:00:02,570
Good evening.

4
00:00:03,319 --> 00:00:05,180
This is Rajkumar Supermar.

5
00:00:05,930 --> 00:00:09,130
I have close to 21 years
of industry experience.

6
00:00:09,700 --> 00:00:14,920
I'm currently working as a lead data and
AI engineering for at t services, right?

7
00:00:15,629 --> 00:00:19,709
Today I want to talk about, the
critical or origin challenges that every

8
00:00:19,949 --> 00:00:22,409
organization facing every day, right?

9
00:00:22,589 --> 00:00:23,129
While building.

10
00:00:23,629 --> 00:00:27,229
The AI are implementing AI under
machine learning pipelines.

11
00:00:27,529 --> 00:00:33,289
So before I get into, my topic, I
would like to thank Dev SecOps for

12
00:00:33,619 --> 00:00:37,129
providing me opportunity to speak
in this conference in front of you.

13
00:00:37,609 --> 00:00:37,909
Okay.

14
00:00:37,989 --> 00:00:41,949
Okay, let me begin my topic with
the simple the recent example.

15
00:00:42,219 --> 00:00:42,459
Alright.

16
00:00:42,959 --> 00:00:44,909
Couple of months before, right?

17
00:00:45,059 --> 00:00:47,759
You're a large retail
company in the US right?

18
00:00:48,269 --> 00:00:52,559
They deployed an automated visual
quality inspection model, right?

19
00:00:53,279 --> 00:00:55,079
And once after the deployment, right?

20
00:00:55,169 --> 00:00:58,689
Everything works fine and
whatever they expect at, right?

21
00:00:58,744 --> 00:01:00,724
They got the outcome,
everything works fine.

22
00:01:00,804 --> 00:01:01,404
People are happy.

23
00:01:01,944 --> 00:01:04,284
But after several weeks, right?

24
00:01:04,384 --> 00:01:06,904
The product defects are
skyrocketing, right?

25
00:01:06,964 --> 00:01:08,554
The customers are not Hannah.

26
00:01:08,614 --> 00:01:09,914
The customers are, complaining.

27
00:01:09,914 --> 00:01:11,264
Lot of defects, right?

28
00:01:11,264 --> 00:01:13,244
The business leaders are
getting investigation.

29
00:01:13,244 --> 00:01:17,504
They start the investigation into that,
and then the team discovered, right?

30
00:01:17,564 --> 00:01:21,584
Someone has intentionally
uploaded you, manipulated images

31
00:01:21,674 --> 00:01:23,474
into the training data, right?

32
00:01:23,924 --> 00:01:27,354
It's a very small poisoning, right?

33
00:01:27,354 --> 00:01:32,074
I can say very small poisoning into the
training data, but that compromised, yeah.

34
00:01:32,074 --> 00:01:33,064
Multimillionaire dollar system.

35
00:01:33,564 --> 00:01:36,954
But the problem is nobody
noticed that is a worst part.

36
00:01:36,984 --> 00:01:40,234
Nobody noticed this, once, once
they, deployed this model into

37
00:01:40,234 --> 00:01:42,634
the C-D-C-C-A-C-D pipeline, right?

38
00:01:43,144 --> 00:01:47,374
Because the pipeline doesn't have any
control over the, a security, right?

39
00:01:47,884 --> 00:01:50,604
And this story is not
only for retail, right?

40
00:01:50,604 --> 00:01:52,074
It's for everywhere, right?

41
00:01:52,074 --> 00:01:56,514
It's, it starts from telecom, it starts
from insurance, or, it starts from.

42
00:01:57,014 --> 00:01:58,094
Banking, right?

43
00:01:58,214 --> 00:02:01,904
The DevOps pipeline have no
control over the security, right?

44
00:02:01,964 --> 00:02:05,084
We need to have some kind
of strong DevOps, right?

45
00:02:05,084 --> 00:02:06,224
That's what we're going to discuss.

46
00:02:06,724 --> 00:02:06,964
Okay.

47
00:02:07,065 --> 00:02:08,715
Let me move to the next slide.

48
00:02:09,215 --> 00:02:09,505
Okay.

49
00:02:09,740 --> 00:02:13,490
Nowadays deploying EA systems
into the production is very fast.

50
00:02:13,950 --> 00:02:16,170
It's not even taking
weeks or month, right?

51
00:02:16,170 --> 00:02:17,130
It's days, right?

52
00:02:17,239 --> 00:02:19,910
They plant, they implement
in a days, right?

53
00:02:20,690 --> 00:02:23,769
But the speed comes
with a couple of issues.

54
00:02:23,799 --> 00:02:25,029
One is data poisoning.

55
00:02:25,029 --> 00:02:27,009
That's what we discussed
in the previous slide.

56
00:02:27,729 --> 00:02:29,819
And then, we have and very still attacks.

57
00:02:30,089 --> 00:02:32,429
We have a prompt injection.

58
00:02:32,609 --> 00:02:35,789
We have ethical risk,
we have privacy leak.

59
00:02:36,164 --> 00:02:39,994
So these are, some of the issues
it's come bundled with the first

60
00:02:39,994 --> 00:02:41,854
deployment of the ai, okay?

61
00:02:42,454 --> 00:02:44,614
So in 2023, right?

62
00:02:44,664 --> 00:02:49,644
You have big financial institution
right past its loan approval system.

63
00:02:49,834 --> 00:02:52,644
They implemented they
deployed using ai right?

64
00:02:52,764 --> 00:02:53,634
For the loan approval.

65
00:02:53,904 --> 00:02:56,984
But they stopped within few
weeks or within few weeks, right?

66
00:02:57,584 --> 00:03:02,109
Because, it is not a it is, it's keep
on, rejecting the loan application for

67
00:03:02,290 --> 00:03:04,000
a different sector of people, right?

68
00:03:04,209 --> 00:03:07,920
When they tried to investigate the
issue they found, there was no issues.

69
00:03:07,920 --> 00:03:11,820
But the problem is the model was trained
with a particular set of people, right?

70
00:03:12,149 --> 00:03:15,749
So it is not able to recognize the
different set of peoples, right?

71
00:03:15,869 --> 00:03:17,915
So that's what it's like a bias, right?

72
00:03:18,415 --> 00:03:19,855
That's the issue, right?

73
00:03:20,185 --> 00:03:23,065
So again, that's a need,
we need to build, yeah.

74
00:03:23,305 --> 00:03:25,735
EA native dev SecOps framework.

75
00:03:26,235 --> 00:03:29,355
Okay let me talk about, the EA
native dev SecOps framework, right?

76
00:03:29,405 --> 00:03:31,785
It's, it comes with the,
different layers, right?

77
00:03:32,305 --> 00:03:36,885
We have data injection we have
privacy preserving training.

78
00:03:37,290 --> 00:03:42,550
We have automated security testing,
we have governance, ethical governance

79
00:03:42,550 --> 00:03:45,190
case, we have producted interference.

80
00:03:45,520 --> 00:03:48,870
And finally, we have to do
continuous monitoring, right?

81
00:03:48,870 --> 00:03:50,940
We should not we should not
lift off the deployment.

82
00:03:50,940 --> 00:03:52,620
We should monitor completely, right?

83
00:03:52,620 --> 00:03:57,550
Why it's, for example, why it's
approval, why it's denying the loan.

84
00:03:57,610 --> 00:03:59,140
For example, loan application, right?

85
00:03:59,140 --> 00:04:02,770
We need to do monitoring for
several months at least, right?

86
00:04:02,775 --> 00:04:02,865
Until.

87
00:04:03,670 --> 00:04:04,810
The product become very stable.

88
00:04:05,170 --> 00:04:09,690
Okay, so let me break into this, into let
me break this layer into small chunks.

89
00:04:09,800 --> 00:04:10,010
Okay.

90
00:04:10,330 --> 00:04:12,640
Secure data injection, right?

91
00:04:12,830 --> 00:04:14,630
How do we implement this, right?

92
00:04:14,730 --> 00:04:17,850
The data, and the trust starts
with the data injection, right?

93
00:04:17,850 --> 00:04:21,960
We should properly inject the
the data, whichever we trusted.

94
00:04:22,770 --> 00:04:25,830
And then we need to implement
the cryptographic verification

95
00:04:25,980 --> 00:04:26,970
of the data sources.

96
00:04:27,540 --> 00:04:32,650
And then we need to detect the real
time anomaly and the automatic p

97
00:04:32,780 --> 00:04:35,220
P two i detection and redaction.

98
00:04:35,670 --> 00:04:40,310
If it is having any, P two I P two
I values, for example, email id,

99
00:04:40,310 --> 00:04:43,365
personal information that that we need
to redact, that we need to mask that.

100
00:04:43,865 --> 00:04:47,685
We need to enable our a or in
audit audit logs we need to

101
00:04:47,685 --> 00:04:49,705
track the lineage, data lineage.

102
00:04:49,915 --> 00:04:52,855
Again, just wanted to underline,
the what is data lineage, one of the

103
00:04:52,855 --> 00:04:55,435
enterprise again happened in us, right?

104
00:04:55,935 --> 00:05:00,435
They traced a model failure back to
a single character CSV file, right?

105
00:05:00,790 --> 00:05:02,020
The issue is right.

106
00:05:02,020 --> 00:05:03,010
Because of the lineage.

107
00:05:03,100 --> 00:05:03,595
Lineage, right?

108
00:05:03,710 --> 00:05:04,070
Nobody knows.

109
00:05:04,715 --> 00:05:06,965
Where exactly the CS
file comes from, right?

110
00:05:06,965 --> 00:05:10,825
So it should come from a trusted source,
but because of the corrupt corrupted

111
00:05:10,825 --> 00:05:12,255
file, they had a model failure.

112
00:05:12,765 --> 00:05:14,175
Okay, so let me go to the next one.

113
00:05:14,205 --> 00:05:16,075
That's privacy preserving training.

114
00:05:16,575 --> 00:05:20,345
So nowadays companies, social
media companies are, telecom

115
00:05:20,645 --> 00:05:21,965
or financial companies, right?

116
00:05:22,385 --> 00:05:24,515
They're dealing with
sensitive data, right?

117
00:05:24,755 --> 00:05:28,165
So in, in terms of in terms of
adding the privacy we need to

118
00:05:28,165 --> 00:05:30,165
apply multiple privacy techniques.

119
00:05:30,405 --> 00:05:36,220
For example, we need to apply differential
privacy which is something like, adding a

120
00:05:36,220 --> 00:05:40,210
nice during the training so that the model
cannot memorize the individuals, right?

121
00:05:40,210 --> 00:05:43,355
It's like a collaboration, not
individual kind of training.

122
00:05:43,855 --> 00:05:46,260
Then it's like a, and
then second, second level.

123
00:05:46,405 --> 00:05:49,765
Second level will be like federated
training, federated learning.

124
00:05:50,455 --> 00:05:52,435
So the data stays at the source.

125
00:05:52,555 --> 00:05:54,505
Only the model updates the travel, right?

126
00:05:54,505 --> 00:05:59,775
So for example, think of it as a
learning from 10 hospitals, without

127
00:05:59,775 --> 00:06:01,825
ever moving patient data, right?

128
00:06:02,815 --> 00:06:05,565
And then third one will be
like, homomorphic, encryption

129
00:06:05,875 --> 00:06:07,165
training and encrypted data.

130
00:06:07,475 --> 00:06:08,045
Nobody sees.

131
00:06:08,475 --> 00:06:10,005
That's a plain text, right?

132
00:06:10,105 --> 00:06:13,465
So for example I can say,
usually the banks, right?

133
00:06:13,525 --> 00:06:15,505
They used to do the fraud detection.

134
00:06:15,745 --> 00:06:20,165
When once they do the fraud detection,
they cannot share the real data, right?

135
00:06:20,315 --> 00:06:23,715
They also encrypted data and
then they help each other to

136
00:06:23,715 --> 00:06:25,125
do the fraud detection, right?

137
00:06:25,545 --> 00:06:28,620
Only they need to share the,
the patents right in, in stuff.

138
00:06:29,450 --> 00:06:32,270
Sharing the real or the
raw customer data, right?

139
00:06:32,750 --> 00:06:34,190
So that's the power of privacy.

140
00:06:34,240 --> 00:06:36,380
Privacy of the preserving ai.

141
00:06:36,880 --> 00:06:38,260
Okay, let me move to the next one.

142
00:06:38,440 --> 00:06:38,710
Okay.

143
00:06:38,740 --> 00:06:42,600
And in terms of security testing,
the machine learning pipelines we

144
00:06:42,600 --> 00:06:45,150
will we bring CACD automation, right?

145
00:06:45,700 --> 00:06:47,690
With multiple layers of testing.

146
00:06:47,810 --> 00:06:52,720
For example, advers testing that
will generate the host inputs.

147
00:06:53,220 --> 00:06:56,060
After that, it'll do the
static code analysis and then

148
00:06:56,060 --> 00:06:57,770
the model hardening, right?

149
00:06:57,830 --> 00:07:00,875
If there was any differential
things it'll validate the, it'll

150
00:07:00,875 --> 00:07:02,330
do the input validation, right?

151
00:07:02,900 --> 00:07:08,000
And further it'll do we have implement
dependency scanning for the ML frameworks.

152
00:07:08,750 --> 00:07:14,605
So this these stages ensures the the
issues and if there was any issues.

153
00:07:15,425 --> 00:07:17,465
That will be caught before
the deployment, right?

154
00:07:17,495 --> 00:07:19,925
We can able to cut the issues,
we can able to fix it, right?

155
00:07:20,375 --> 00:07:24,670
So we cannot able, we, so we
will avoid those breaches, right?

156
00:07:24,670 --> 00:07:26,080
Or the customer complaints.

157
00:07:26,580 --> 00:07:27,240
Move to the next one.

158
00:07:27,740 --> 00:07:32,070
Okay, next one is, producting
interference inference endpoints, right?

159
00:07:32,450 --> 00:07:34,440
Defense against real time threats.

160
00:07:34,940 --> 00:07:37,860
So once after we apply the model, right?

161
00:07:37,860 --> 00:07:43,030
We should secure by, by the
model extraction or the, bots in

162
00:07:43,030 --> 00:07:45,430
inference or the prompt injection.

163
00:07:46,090 --> 00:07:47,820
And, there are multiple ways.

164
00:07:47,880 --> 00:07:51,460
One of the common ways is, setting
up the valve security protocol

165
00:07:51,640 --> 00:07:53,295
valves can be used to call us.

166
00:07:53,795 --> 00:07:55,355
And security protocol, right?

167
00:07:55,355 --> 00:08:00,575
So that will detect the bots that detect
the injection like that some of which

168
00:08:00,635 --> 00:08:05,665
like, it'll it'll it'll pass each request
coming from the outside and then it'll

169
00:08:06,065 --> 00:08:10,250
pass the request and understand, you
said, coming from the trust source.

170
00:08:10,750 --> 00:08:14,310
It's not from the, it should not
be coming from, the bars, right?

171
00:08:14,850 --> 00:08:18,540
And then it'll filter those
requests, try to understand

172
00:08:18,540 --> 00:08:20,970
what is a request header, right?

173
00:08:21,420 --> 00:08:24,570
It should not be any SQ injection or
there should not be any bad request.

174
00:08:24,720 --> 00:08:24,930
Yeah.

175
00:08:24,960 --> 00:08:27,640
So that's, that's how we'll
product the the endpoints.

176
00:08:28,180 --> 00:08:28,480
Okay.

177
00:08:28,780 --> 00:08:29,590
Let go to the next one.

178
00:08:30,090 --> 00:08:33,550
Alright, so next one is
governance by design, right?

179
00:08:33,620 --> 00:08:34,820
Ethical AI checkpoints.

180
00:08:35,600 --> 00:08:40,415
So now, security is not, now, not only
security is enough, we must ethics and

181
00:08:40,415 --> 00:08:42,365
accountability into the pipelines, right?

182
00:08:42,905 --> 00:08:46,975
We need to add the governance gets
the pre-training ethical review

183
00:08:47,665 --> 00:08:49,585
and we should do the bias testing.

184
00:08:50,005 --> 00:08:51,475
Across demographic groups, right?

185
00:08:51,475 --> 00:08:56,975
We should not do the testing in a in
a, in a isolated group of demographics.

186
00:08:56,975 --> 00:08:59,075
We should group across
demographics, right?

187
00:08:59,765 --> 00:09:02,455
And then for sometimes at
least we should have the human

188
00:09:02,455 --> 00:09:03,895
in the loop approval, right?

189
00:09:04,735 --> 00:09:08,205
Yeah, again, I can tell, one of
the one of the example in one

190
00:09:08,205 --> 00:09:10,295
of the recent deployment, right?

191
00:09:10,825 --> 00:09:13,285
The bias test revealed
the model performed.

192
00:09:13,285 --> 00:09:16,495
20 percentage was for
minority group, right?

193
00:09:17,035 --> 00:09:20,335
The data scientist who developed
the model does it, did not

194
00:09:20,335 --> 00:09:22,225
know why it was went wrong.

195
00:09:22,765 --> 00:09:23,785
The business doesn't know.

196
00:09:24,085 --> 00:09:27,655
But the problem is, it's all
about the training data, right?

197
00:09:27,715 --> 00:09:31,255
The reason is they did not provide
the enough training data for

198
00:09:31,315 --> 00:09:33,625
those minority, minority groups.

199
00:09:33,940 --> 00:09:34,540
That's the issue.

200
00:09:35,040 --> 00:09:35,330
Okay.

201
00:09:35,830 --> 00:09:36,130
Okay.

202
00:09:36,160 --> 00:09:40,485
Now, we can have policy as a code
or we can have the blockchain

203
00:09:40,605 --> 00:09:42,885
audit trials to scale governance.

204
00:09:43,155 --> 00:09:48,515
We can qualify policies, we can have
version control rules, and then we should

205
00:09:48,515 --> 00:09:50,795
implement automated compliance validation.

206
00:09:51,260 --> 00:09:53,930
Like enforcement across
every EML pipeline.

207
00:09:54,860 --> 00:09:59,500
And once we implement the blockchain it'll
it'll extend the immutable audit logs.

208
00:09:59,500 --> 00:10:04,360
No one can able to temper that and
it's other transparent lineage, right?

209
00:10:04,360 --> 00:10:08,470
So some where exactly the source that
you know it's coming from, right?

210
00:10:08,470 --> 00:10:12,370
It's another transparent lineage
and then regulator ready evidence

211
00:10:12,370 --> 00:10:13,810
without the proper evidence, right?

212
00:10:14,230 --> 00:10:17,330
So that's what we build the
trust across the company.

213
00:10:17,610 --> 00:10:20,340
We know from where exactly
the data is coming.

214
00:10:20,410 --> 00:10:24,810
The the training data is coming and
we should have, the strong nobody

215
00:10:24,810 --> 00:10:26,160
can able to temper the audit logs.

216
00:10:26,250 --> 00:10:28,590
Who is going to modify
everything will be recorded.

217
00:10:28,800 --> 00:10:30,390
And then regulator ready evidence.

218
00:10:30,400 --> 00:10:33,520
Whenever there was a question from
the federal, we know how to answer.

219
00:10:33,890 --> 00:10:37,530
We should have the the answer is
ready, okay what the next one?

220
00:10:38,030 --> 00:10:42,130
Okay because of this recently
healthcare, a healthcare company

221
00:10:42,190 --> 00:10:45,050
implemented the, the above strategy.

222
00:10:45,650 --> 00:10:49,590
And after that so the implement
is DevOps framework, what we

223
00:10:49,590 --> 00:10:51,570
discussed the previous slide, right?

224
00:10:52,380 --> 00:10:56,770
So now the results, they don't have they
have zero security incidents and they

225
00:10:56,770 --> 00:10:58,900
have much faster deployments, right?

226
00:10:58,960 --> 00:11:01,790
Through automated CSCD or
automated compliance pipelines.

227
00:11:02,570 --> 00:11:05,740
And then they can successfully
complete the audits, like in

228
00:11:05,740 --> 00:11:08,000
a HIPAA or SOC the process.

229
00:11:08,600 --> 00:11:14,750
And then they do have a trust, higher
trust in among the healthcare systems.

230
00:11:15,600 --> 00:11:21,070
This explains once we deploy on the
dev sec of ea AI innovation, right?

231
00:11:21,600 --> 00:11:25,390
With the multiple safeguard so
we can able to achieve the fruit.

232
00:11:25,890 --> 00:11:26,970
Okay, next one.

233
00:11:27,470 --> 00:11:32,090
So now, what are the strategies we can
do for every organization based on, the

234
00:11:32,090 --> 00:11:34,305
couple of the real time success stories?

235
00:11:34,935 --> 00:11:37,505
We should start with
the, yay risk assessment.

236
00:11:38,045 --> 00:11:41,925
And then we should integrate
address and bias testing into CSCD.

237
00:11:42,405 --> 00:11:46,775
And we should have human in the loop for
at least the model became very stable.

238
00:11:47,495 --> 00:11:52,075
And we need to implement policy
as a code enable cross-functional

239
00:11:52,075 --> 00:11:56,835
collaboration deploy runtime
monitoring for inference for, and then

240
00:11:56,895 --> 00:12:00,945
maintain immutable audit trials so
nobody can tamper the audit trials.

241
00:12:01,445 --> 00:12:02,855
Okay, go to the next one.

242
00:12:03,815 --> 00:12:05,675
Okay, so what are the key takeaways?

243
00:12:05,885 --> 00:12:06,125
Right?

244
00:12:06,335 --> 00:12:11,035
This security and ethics cannot
be bolted onto the AI systems.

245
00:12:11,275 --> 00:12:13,255
They must be embedded, right?

246
00:12:13,315 --> 00:12:14,485
They must be extended right.

247
00:12:14,985 --> 00:12:18,635
This is how we build the ai that
is, it should be secure, should

248
00:12:18,635 --> 00:12:23,615
be private, should be fair,
trustworthy, transparency, and then

249
00:12:23,615 --> 00:12:24,905
ready for the production at scale.

250
00:12:25,595 --> 00:12:25,955
Okay.

251
00:12:26,195 --> 00:12:27,845
I think that's all I have.

252
00:12:28,475 --> 00:12:30,475
Thank you so much for, joining me today.

253
00:12:31,465 --> 00:12:33,605
If you have any questions,
you can let me know.

254
00:12:33,700 --> 00:12:33,970
Thank you.

