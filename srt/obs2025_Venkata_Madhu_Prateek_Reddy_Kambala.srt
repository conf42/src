1
00:00:00,500 --> 00:00:01,279
Hello everyone.

2
00:00:01,310 --> 00:00:04,220
Welcome to Con 42 Observability 2025.

3
00:00:04,340 --> 00:00:09,410
I am Vanga PR Kambala, a senior
DevOps engineer with 15 plus

4
00:00:09,410 --> 00:00:10,580
years of experience in it.

5
00:00:10,910 --> 00:00:15,079
I started working as a network engineer
in both enterprise and data center

6
00:00:15,079 --> 00:00:19,280
domains, and then moved to cloud as
a DevOps engineer and now working as

7
00:00:19,280 --> 00:00:23,120
a senior DevOps engineer implementing
DevOps and DevSecOps methodologies.

8
00:00:24,085 --> 00:00:28,855
For deploying infrastructure into
AWS using services like compute,

9
00:00:29,185 --> 00:00:33,745
storage databases, serverless
infrastructure, and as well as contain

10
00:00:33,745 --> 00:00:35,215
based application infrastructure.

11
00:00:35,715 --> 00:00:39,775
And I also have experience across
domains like multiage finance,

12
00:00:39,835 --> 00:00:41,755
healthcare insurance, and e-commerce.

13
00:00:42,380 --> 00:00:46,100
The topic I chose today is logging
in the age of cost, cutting

14
00:00:46,280 --> 00:00:48,020
smart strategies to reduce bills.

15
00:00:48,410 --> 00:00:52,690
Over the last decade, companies migrated
a lot of their workloads to cloud because

16
00:00:52,690 --> 00:00:56,650
of ease of use, high availability,
fault tolerance, and scalability.

17
00:00:57,040 --> 00:01:00,990
And also cloud provided mean
exponentially fast path for moving

18
00:01:00,990 --> 00:01:05,220
from ideation to like implementation
compared to traditional data centers.

19
00:01:06,035 --> 00:01:09,365
All this moving to cloud, like
there is a lot of cost incurred.

20
00:01:09,395 --> 00:01:12,785
So the latest buzzword is
finops, where cost optimization

21
00:01:12,785 --> 00:01:14,515
is like a top most strategy.

22
00:01:14,965 --> 00:01:19,125
And logging is one such a aspects
where cost optimization is looked at.

23
00:01:19,545 --> 00:01:23,415
So today we will dive into how
like industry leaders like Uber

24
00:01:23,465 --> 00:01:27,035
Airbnb and Slack implemented,
like their cost implementation,

25
00:01:27,125 --> 00:01:28,535
cost optimization strategies.

26
00:01:29,190 --> 00:01:33,540
Reducing 70% of their bills
while retaining the same level of

27
00:01:33,540 --> 00:01:36,870
debugging capabilities and as well
as like monitoring capabilities.

28
00:01:37,590 --> 00:01:39,060
So let's dive into that.

29
00:01:39,970 --> 00:01:40,420
Let's see.

30
00:01:40,420 --> 00:01:44,260
Like I'm in what like causes all
this cost crisis for logging.

31
00:01:44,690 --> 00:01:49,800
As per the research like available openly
Airbnb, like spent like 1.2 millions

32
00:01:49,800 --> 00:01:52,740
a year for stale, unused debug data.

33
00:01:53,340 --> 00:01:56,550
So there is a research that state
states that, like for all the logging

34
00:01:56,550 --> 00:02:01,949
that is generated, only 30% is used for
all the queries and like monitoring.

35
00:02:01,970 --> 00:02:05,990
Any meaningful operations like
an enterprise can do, and rest

36
00:02:05,990 --> 00:02:09,380
of the 70% is sitting there in
the storage costing enterprises

37
00:02:09,380 --> 00:02:11,850
like a premium without any use.

38
00:02:12,390 --> 00:02:16,770
So this problem even compounds with
microservice architecture because each

39
00:02:16,770 --> 00:02:18,870
microservice generates its own logs.

40
00:02:18,970 --> 00:02:22,570
Yeah, we will look into how Airbnb
mitigated and how like we'll

41
00:02:22,570 --> 00:02:26,020
also look into other companies
on different strategies of

42
00:02:26,070 --> 00:02:27,540
cost optimization for logging.

43
00:02:27,970 --> 00:02:32,020
But yeah, this is what causes
like a costing for logging a lot.

44
00:02:32,080 --> 00:02:33,750
So that's going to the next slide.

45
00:02:34,250 --> 00:02:38,800
So here we will look at why why the costs
are spiraling out of control for logging.

46
00:02:38,850 --> 00:02:42,000
So mainly the first one would
be microservice explosion.

47
00:02:42,480 --> 00:02:46,250
So we moved from monolith architecture
to microservices architecture which

48
00:02:46,300 --> 00:02:49,930
created like, a lot of services and
those services, like independent

49
00:02:49,930 --> 00:02:51,790
leads generate lot of log streams.

50
00:02:52,120 --> 00:02:56,220
So creating the, all those log streams
and like I'm maintaining them paid

51
00:02:56,220 --> 00:02:59,520
for exponential log generation,
log creation and log storage.

52
00:02:59,820 --> 00:03:02,100
Which added to our exponential costs.

53
00:03:02,700 --> 00:03:06,280
And then cloud vendors like
have a the pricing at both

54
00:03:06,280 --> 00:03:07,690
ingestion and storage level.

55
00:03:07,960 --> 00:03:11,320
So you're getting double charge
so you'll be charged for per GB

56
00:03:11,350 --> 00:03:13,180
while you're ingesting the data.

57
00:03:13,180 --> 00:03:17,380
And as well as while you're storing
that data for later retrieval purpose.

58
00:03:18,220 --> 00:03:22,170
And then there are some development
practices where some applications

59
00:03:22,170 --> 00:03:27,170
or some enterprises like users or
developers still have debug level kind

60
00:03:27,220 --> 00:03:31,150
logs enabled in production, which is
not recommended because debug generates

61
00:03:31,150 --> 00:03:35,810
a lot of log data, which is not used
most of the times unless there is.

62
00:03:36,090 --> 00:03:41,360
An alert or there is something that
needs to be troubleshoot, enabling

63
00:03:41,360 --> 00:03:46,220
debug log level is not advised because
that increases the sheer volume of like

64
00:03:46,400 --> 00:03:51,630
logs, it generates and 90% of the time
or 99% of the time, it is not needed.

65
00:03:52,449 --> 00:03:54,519
And then there is lack
of governance, right?

66
00:03:54,519 --> 00:03:57,429
Like where there are no clear
policies on how log collection should

67
00:03:57,429 --> 00:04:01,550
work, and what are the optimization
strategies and inconsistencies caused

68
00:04:01,550 --> 00:04:05,539
to excessive logging and excessive
logging caused to an excessive storage.

69
00:04:05,539 --> 00:04:09,169
So it concurs again, cost at both
levels, like for ingestion and storage.

70
00:04:09,669 --> 00:04:13,089
So we'll look into the first strategy,
which is implementation sampling.

71
00:04:13,569 --> 00:04:18,819
So Uber had a big breakthrough, like Uber
we mentioned 70% of the logs generated

72
00:04:18,819 --> 00:04:20,649
are unused with respect to debug.

73
00:04:20,649 --> 00:04:23,519
On it's almost like 95% that is unused.

74
00:04:23,919 --> 00:04:27,899
So what Uber did was like, it
implemented a sampling mechanism and

75
00:04:27,899 --> 00:04:31,979
using that sampling mechanism, it was
only retaining 5% of the debug logs

76
00:04:31,979 --> 00:04:37,019
that is generated discarding 95% of
like 95% of them, which helped them

77
00:04:37,019 --> 00:04:39,839
like, save around $8 million annually.

78
00:04:40,339 --> 00:04:42,619
This was published in s Econ 2022.

79
00:04:42,929 --> 00:04:47,539
So yeah, while they were doing that,
they still retained their logs, like they

80
00:04:47,539 --> 00:04:49,579
were only sampling like M and DBA blogs.

81
00:04:49,579 --> 00:04:53,439
So they were reducing the noise while
keeping the effective logs that are needed

82
00:04:53,799 --> 00:04:55,269
for maintenance of the infrastructure.

83
00:04:56,029 --> 00:04:59,639
So there are three different kinds of
implementer implementation approaches

84
00:04:59,639 --> 00:05:01,169
for this head-based sampling.

85
00:05:01,169 --> 00:05:04,999
So where the decision is
made at the log generation as

86
00:05:05,059 --> 00:05:06,799
itself, so like source, right?

87
00:05:06,799 --> 00:05:10,039
Like wherever, like the microservices,
our infrastructure components

88
00:05:10,039 --> 00:05:12,299
are used at that place itself.

89
00:05:12,329 --> 00:05:16,714
There is a sampling mechanism used where
it drops 95% of like other debug data.

90
00:05:17,214 --> 00:05:21,424
Whereas there is another approach to
it, like tail based sampling mechanism

91
00:05:21,424 --> 00:05:25,234
where the decision is made after
the trace is completed and ingested

92
00:05:25,234 --> 00:05:26,914
in like at the ingestion process.

93
00:05:27,214 --> 00:05:33,084
So Uber implemented this and saved like
all saved 8 million in cost for lobbying.

94
00:05:34,064 --> 00:05:38,429
Then there is level based sampling
mechanism where like it can use like

95
00:05:38,429 --> 00:05:42,319
at different rates at different log
levels, which is also one of the widely

96
00:05:42,319 --> 00:05:44,899
used approaches for best practices.

97
00:05:44,899 --> 00:05:48,589
Like we should always preserve
error and like fatal log levels.

98
00:05:48,639 --> 00:05:51,609
That is what helps us for effective
troubleshooting and creeping track

99
00:05:51,609 --> 00:05:54,939
of what services went down and
alerting and other stuff and all.

100
00:05:55,239 --> 00:05:57,939
So we should never be
sampling error and fatal logs.

101
00:05:58,439 --> 00:06:02,129
You can start, like sampling can
be done from one to 10% for debugs.

102
00:06:02,179 --> 00:06:04,379
And info levels info level of logs.

103
00:06:04,439 --> 00:06:08,849
But like you can always start at 10%
and then build your way up to 1%.

104
00:06:09,689 --> 00:06:13,329
And also we have to use like I
inconsistent sampling consistent

105
00:06:13,329 --> 00:06:16,659
sampling methodologies and also service
boundaries for effective sampling.

106
00:06:17,159 --> 00:06:20,429
Here we look into a second strategy,
which is called noise filtering.

107
00:06:20,489 --> 00:06:23,340
Like we said 70, like out of
like a hundred percent of the

108
00:06:23,340 --> 00:06:27,150
logs, 70% is just noise that is
generated that is never used.

109
00:06:27,210 --> 00:06:32,330
So slack and counted similar kind of
situation where there were like health

110
00:06:32,330 --> 00:06:36,630
check logs that were not meaningful
to them, but they're still kept on,

111
00:06:36,630 --> 00:06:41,489
accumulated by the logging like in methods
they had, which incurred both ingestion

112
00:06:41,489 --> 00:06:43,200
cost and as well as storage cost.

113
00:06:43,695 --> 00:06:48,495
So Slack implemented the fluid fluent
bit filtering mechanism where like they

114
00:06:48,495 --> 00:06:52,295
dropped all the health check data that
was not necessary, which dramatically

115
00:06:52,295 --> 00:06:56,665
reduced their cost by 15% with zero
impact in their day-to-day operations.

116
00:06:57,145 --> 00:07:00,985
So generally the common nice sources
would be health check endpoints,

117
00:07:00,985 --> 00:07:03,415
ping health or status like endpoints.

118
00:07:03,415 --> 00:07:07,255
And then there are repeated connection
timeouts that are not critical.

119
00:07:07,675 --> 00:07:11,065
And also we don't need to maintain all
the successful authentication logs.

120
00:07:11,065 --> 00:07:14,995
And there are some service to service
communications between the services

121
00:07:14,995 --> 00:07:16,735
that you can drop and not retain.

122
00:07:17,545 --> 00:07:20,935
So filtering is one of the most
effective mechanisms because you can

123
00:07:20,935 --> 00:07:25,034
just drop the logs that you don't need
effectively helping in cost optimization.

124
00:07:25,604 --> 00:07:30,124
And also it's effective when you're able
to identify systematic noise sources

125
00:07:30,124 --> 00:07:33,874
that you can filter out at ingestion
layer itself, so you're not ingesting

126
00:07:33,874 --> 00:07:37,624
them as well as like I'm installing
them, saving a lot of the cost there.

127
00:07:38,124 --> 00:07:41,984
There is another strategy like that we
can use, like this is the earlier too we

128
00:07:41,984 --> 00:07:45,804
were looking at like generation like log
generation level and ingestion level.

129
00:07:45,804 --> 00:07:47,394
This comes at storage level.

130
00:07:47,394 --> 00:07:50,454
So there are there is a storage
strategy the three tier storage

131
00:07:50,454 --> 00:07:51,834
strategy that you can implement.

132
00:07:52,359 --> 00:07:56,469
So it can be classified into hot
storage, warm storage, and closed cloud.

133
00:07:56,519 --> 00:07:57,419
Sorry, cold storage.

134
00:07:57,959 --> 00:08:00,959
So hot storage is something
like for a week of logs, right?

135
00:08:00,959 --> 00:08:04,679
So anything from today to last seven
days can be a hot storage, like

136
00:08:04,679 --> 00:08:06,709
stored in elastic search which helps.

137
00:08:07,664 --> 00:08:11,174
To query, which helps for a better
query rates and retrieval rates.

138
00:08:11,264 --> 00:08:15,854
And for anything between seven to 30
days is less queried compared to the

139
00:08:15,974 --> 00:08:17,984
ones that are queried in hot storage.

140
00:08:17,984 --> 00:08:22,784
So we store them under a warm storage
using standard S3 buckets, which has

141
00:08:22,784 --> 00:08:25,074
balanced cost and performance ratio.

142
00:08:25,679 --> 00:08:29,879
After 30 days most of the applications
are like an enterprises won't need to

143
00:08:29,879 --> 00:08:34,869
like, have logging data that is like
readily variable every for day-to-day use.

144
00:08:35,349 --> 00:08:39,189
So we can move that to glacier
storage where the cost would

145
00:08:39,189 --> 00:08:40,659
be very less for retaining.

146
00:08:40,659 --> 00:08:42,599
But you should be careful
while doing that because.

147
00:08:43,099 --> 00:08:47,599
Grier data is really tough because of
like less retrieval speeds for data.

148
00:08:47,969 --> 00:08:51,089
But Shopify implemented one of
these this approach, and they

149
00:08:51,089 --> 00:08:55,299
were able to save 60% in their
retention costs and, they were yeah.

150
00:08:55,399 --> 00:08:58,294
Like generally, like they needed
like the com anything logs for

151
00:08:58,294 --> 00:09:00,034
like beyond 30 days for compliance.

152
00:09:00,034 --> 00:09:01,954
So that is the reason
they're retaining it.

153
00:09:02,524 --> 00:09:06,824
But yeah, this reduced around dramatically
their retention cost, like for logging,

154
00:09:06,874 --> 00:09:12,294
and there is the fourth strategy, which
shifts your logging cost to personal cost.

155
00:09:12,394 --> 00:09:17,704
So in a way razor pay, one of India's
major payment gateway was able to like,

156
00:09:17,744 --> 00:09:19,884
opt for an open source solution like.

157
00:09:19,914 --> 00:09:25,634
Grafana Loki instead of Splunk, and
thereby they saved around 200 K dollars.

158
00:09:26,064 --> 00:09:29,454
So like Splunk was costing
them like, $250,000.

159
00:09:29,454 --> 00:09:34,234
Whereas graph Grafana and Loki
implementation just costed them $50,000

160
00:09:34,624 --> 00:09:39,634
because of how like Loki's architecture is
cost effective while using S3 for storage.

161
00:09:40,339 --> 00:09:44,609
But this comes at a cost like from from
a operations perspective where you will

162
00:09:44,609 --> 00:09:49,469
have to have a subject matter experts
implementing the solutions to maintain

163
00:09:49,469 --> 00:09:53,719
the infrastructure for open source
where you'll not have enter application

164
00:09:53,719 --> 00:09:55,759
support or like I vendor support.

165
00:09:55,819 --> 00:09:58,729
So that is the cost that you'll
be incurring, but definitely

166
00:09:58,729 --> 00:10:00,349
reducing your logging costs.

167
00:10:00,849 --> 00:10:04,329
So we will look at like a real world
case study that Uber implemented.

168
00:10:04,359 --> 00:10:10,129
So Uber had a challenge where there
are like 250,000 spot jobs that run

169
00:10:10,129 --> 00:10:12,649
daily generated around 200 TB data.

170
00:10:13,159 --> 00:10:14,239
So that.

171
00:10:14,999 --> 00:10:16,919
Storing that data is very costly.

172
00:10:16,919 --> 00:10:22,539
So they came up with the compressor,
CLP, which is called compressor log com,

173
00:10:22,629 --> 00:10:28,599
compressor Log Pro, like process where
they achieved a compression of 1 69 ish

174
00:10:28,599 --> 00:10:34,769
to one ratio, thereby reducing their
costs from 1.8 million to 10 k. Annually,

175
00:10:35,069 --> 00:10:37,319
so that is 99% production in cost.

176
00:10:37,319 --> 00:10:41,279
So they implemented like an compression
mechanism of compression logs and

177
00:10:41,279 --> 00:10:45,469
were able to like and achieve this
remarkable feat of reducing their cost.

178
00:10:45,569 --> 00:10:50,409
And while like this also helped so
this compression compress log processor

179
00:10:50,654 --> 00:10:54,539
solution also helped them query the
logs without decompressing them.

180
00:10:54,899 --> 00:10:58,849
So there was no, there was no
deterioration in their querying

181
00:10:58,849 --> 00:11:00,169
and like monitoring needs.

182
00:11:00,669 --> 00:11:04,359
So another best practices like for
logging is like I'm in structured logging.

183
00:11:04,359 --> 00:11:08,109
So if you see the example here on the
left side like it is an unstructured

184
00:11:08,109 --> 00:11:13,719
logging where an user is trying to create
or sign up for a new account, but using

185
00:11:13,809 --> 00:11:17,409
like an email address that is already
signed up for that particular service.

186
00:11:17,939 --> 00:11:20,869
On the right side, like the same
thing for a structured log using

187
00:11:20,869 --> 00:11:25,219
A-J-S-O-N, which contains all the
metadata and timing information.

188
00:11:25,269 --> 00:11:28,509
So the right side structured data
helps us like come into query better.

189
00:11:28,909 --> 00:11:33,559
And so like the more we have like I
structured logs like the better for

190
00:11:33,559 --> 00:11:37,269
us I mean from operations perspective
and as well as storage perspective.

191
00:11:37,794 --> 00:11:39,024
Because it helps in compression.

192
00:11:39,024 --> 00:11:42,054
So we can look at at the bottom
we see all the things that

193
00:11:42,054 --> 00:11:43,674
structured logging data can provide.

194
00:11:43,674 --> 00:11:47,244
So it enables precise filtering
because you can target, based on the

195
00:11:47,244 --> 00:11:50,964
metadata, you can select a few fields
and target filtering based on that.

196
00:11:51,489 --> 00:11:54,639
Also it improves better compression
mechanism of just how structurally

197
00:11:54,639 --> 00:11:58,689
it organized and then it enhances
your query performance as well.

198
00:11:58,689 --> 00:12:02,949
Like apart from like your regular queries,
you can use JQ and other queries as well.

199
00:12:03,279 --> 00:12:06,519
And there is more metadata and more
fields that you can query on and

200
00:12:06,519 --> 00:12:08,019
retrieve the data in efficient way.

201
00:12:08,559 --> 00:12:11,649
And also, which also paves
ways for better automation.

202
00:12:12,149 --> 00:12:15,329
So for optimization success you'll
have to have financial metrics,

203
00:12:15,329 --> 00:12:18,689
operational metrics and volume
metrics optimized properly.

204
00:12:19,019 --> 00:12:21,989
Whereas financial metrics will
have your monthly ingestion costs,

205
00:12:21,989 --> 00:12:24,239
storage, like cost per tier.

206
00:12:24,574 --> 00:12:28,984
Also total GB ingested because as
like we discussed earlier, clouds

207
00:12:28,984 --> 00:12:31,324
charge at GB ingestion level as well.

208
00:12:31,864 --> 00:12:35,764
And like also I mean at the storage level
coming to operations you have to see

209
00:12:35,764 --> 00:12:38,734
like how fast so when we are implementing
all these strategies, you'll have to

210
00:12:38,734 --> 00:12:40,684
always make sure how fast like your.

211
00:12:40,714 --> 00:12:44,134
Query responses because that
is what matters at the time of

212
00:12:44,134 --> 00:12:47,614
troubleshooting or I'm looking into
services on what is happening there.

213
00:12:48,034 --> 00:12:52,424
If you if something went wrong there,
and then and you'll have to also make

214
00:12:52,424 --> 00:12:56,434
sure that log is available and with all
the sampling mechanism you're doing, how

215
00:12:56,434 --> 00:13:00,514
effectively you're able to detect that
or, and then the volume metrics like.

216
00:13:00,534 --> 00:13:04,404
You have to look at like daily volume
trends and as well as like how much

217
00:13:04,404 --> 00:13:08,544
is filtered versus retained, and
from all this, like how much like

218
00:13:08,544 --> 00:13:10,104
storage costs like you're incurring.

219
00:13:10,604 --> 00:13:13,514
So the key is actually establishing
baselines before implementing

220
00:13:13,514 --> 00:13:16,244
these changes and then tracking
improvements over the time.

221
00:13:16,744 --> 00:13:19,794
So with all this strategies
like noise I mean filtering

222
00:13:19,854 --> 00:13:21,504
and sampling and everything.

223
00:13:21,504 --> 00:13:24,804
There are com some common pitfalls
and we have to be like, let's

224
00:13:24,804 --> 00:13:26,304
look at how we can avoid them.

225
00:13:26,804 --> 00:13:30,584
So anything like, that is aggressive or
like I'm an overdoing will cause an issue.

226
00:13:30,644 --> 00:13:34,539
So over an aggressive filtering
will always have some repercussions,

227
00:13:34,569 --> 00:13:37,659
so you might be losing some
critical debug information.

228
00:13:38,289 --> 00:13:42,249
Also, always while doing sampling and
filtering always start conservatively,

229
00:13:42,249 --> 00:13:46,389
then increase gradually based
on and what you're seeing and

230
00:13:46,389 --> 00:13:48,099
always and make sure that you.

231
00:13:48,194 --> 00:13:52,034
Preserve error and like fatal level
logs, you are not filtering them

232
00:13:52,034 --> 00:13:53,894
out or you're not sampling them.

233
00:13:53,894 --> 00:13:55,664
Make sure you retain them completely.

234
00:13:56,354 --> 00:14:01,124
And then while sampling, you'll have
sampling bias because just because of how

235
00:14:01,124 --> 00:14:03,464
we are sampling mechanism is implemented.

236
00:14:03,464 --> 00:14:07,124
Like you might miss some of the
infrequent like events that come up.

237
00:14:07,129 --> 00:14:07,999
In your logs?

238
00:14:08,479 --> 00:14:12,499
The solution would be like, using some
trace aware sampling mechanisms, and also

239
00:14:12,499 --> 00:14:17,685
you can implement a safeguard like by
triggering like I'm in full tri triggering

240
00:14:17,685 --> 00:14:19,785
stopping of sampling by using an alert.

241
00:14:19,785 --> 00:14:22,905
So if you see a alert,
like then stop sampling.

242
00:14:22,905 --> 00:14:25,615
Like you can implement
certain triggers and do that.

243
00:14:26,334 --> 00:14:30,714
And then like I'm in a cold storage
query performance retrieving data from

244
00:14:30,764 --> 00:14:34,714
cold storage is really painful, so you
can't write a query it's not easy to

245
00:14:34,714 --> 00:14:39,035
write querying mechanism to pull data
or retrieve data from cold storage.

246
00:14:39,634 --> 00:14:43,525
So design your queries in such
a way that you are not accessing

247
00:14:43,525 --> 00:14:44,814
cold storage for your queries.

248
00:14:44,814 --> 00:14:47,744
Mostly you're using hot and
like your warm storages.

249
00:14:48,510 --> 00:14:52,230
Also you can implement some of
the rehydration workflows of

250
00:14:52,230 --> 00:14:55,800
moving the cold storage data
into one or standby for better.

251
00:14:55,800 --> 00:14:57,780
Like I'm querying like, speeds.

252
00:14:58,280 --> 00:15:00,890
So now we will look at a
cost optimization roadmap.

253
00:15:00,940 --> 00:15:04,030
From looking at all these strategies
what can be done for this to be

254
00:15:04,030 --> 00:15:05,590
implemented in your organization?

255
00:15:05,980 --> 00:15:07,810
So first you'll have
to assess and analyze.

256
00:15:07,810 --> 00:15:11,710
So you'll have to look at immediately you
can look at your log volume and audit,

257
00:15:11,710 --> 00:15:13,030
like how much volume you're seeing.

258
00:15:13,525 --> 00:15:17,035
And then isolate five to 10 log
streams that are like our log sources

259
00:15:17,035 --> 00:15:22,645
that are causing like really high
volume of logs and then calculate how

260
00:15:22,645 --> 00:15:24,325
much you're spending on those logs.

261
00:15:24,325 --> 00:15:27,605
And set up like, dashboards
see what are your quick wins?

262
00:15:27,605 --> 00:15:30,395
And implement some kind of like
the strategies like we discussed

263
00:15:30,605 --> 00:15:34,255
earlier, either filtering or
like an sampling, et cetera.

264
00:15:34,745 --> 00:15:38,405
And like after while you monitor that,
like I'm gonna get to a conclusion

265
00:15:38,405 --> 00:15:41,475
after 30 days, you can start
implementing like we discussed, right?

266
00:15:41,475 --> 00:15:44,585
Like you can start implementing
health check filtering, log sampling,

267
00:15:44,585 --> 00:15:48,145
and then for storage needs, you can
do three tier, three tier storage.

268
00:15:48,535 --> 00:15:51,055
And also we looked at I'm
insert an open source logs.

269
00:15:51,590 --> 00:15:55,950
And in long run and you can always have
some strategic strategic initiatives like

270
00:15:55,980 --> 00:15:59,510
we discussed open source alternatives
and advanced compression techniques

271
00:15:59,510 --> 00:16:04,490
like UBA did with CLP and, or you can
always have like a log-based metric

272
00:16:04,490 --> 00:16:08,670
alerting and you can bring most
training for cost optimization and

273
00:16:08,670 --> 00:16:10,500
culture around that in your enterprise.

274
00:16:11,000 --> 00:16:13,740
So that concludes how
we can save in logging.

275
00:16:13,790 --> 00:16:16,010
So there are other essential
tools you can look at.

276
00:16:16,070 --> 00:16:19,400
For example, data Datadog management
cost management site, and as well

277
00:16:19,400 --> 00:16:23,960
as all the open source applications
like Grafana, ELK, et cetera.

278
00:16:24,020 --> 00:16:27,430
And there is CLP Open Source if
you want to implement it at the

279
00:16:27,430 --> 00:16:30,610
same time, like on vendor specific
stuff you can see in Datadog.

280
00:16:30,610 --> 00:16:34,660
And Uber also maintains a good
treasure of a CLP technical papers.

281
00:16:35,420 --> 00:16:38,120
Which is a blog and that has
all the technical papers where

282
00:16:38,120 --> 00:16:40,880
you can read through on how they
implemented that successfully.

283
00:16:41,270 --> 00:16:44,600
And the same way cloud service
providers also have best practices

284
00:16:44,600 --> 00:16:46,880
of white papers, like AWS and GCP.

285
00:16:47,240 --> 00:16:51,770
And then there are always conferences
like SR Econ and Lisa conferences and

286
00:16:51,770 --> 00:16:56,120
even our con 42 like we are doing as
of today, where they talk about these

287
00:16:56,120 --> 00:16:58,080
situations and have make us aware.

288
00:16:58,580 --> 00:17:01,460
Then there are like, cloud native
foundations communities or your

289
00:17:01,460 --> 00:17:05,030
internal communities that you can
discuss and get like better approaches.

290
00:17:05,530 --> 00:17:07,450
That's it for today, and thank you.

