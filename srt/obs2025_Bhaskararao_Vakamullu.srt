1
00:00:00,500 --> 00:00:00,799
Hi.

2
00:00:01,299 --> 00:00:02,170
Welcome everybody.

3
00:00:03,130 --> 00:00:07,270
Today I'm going to discuss about
observability in the evolution of AI

4
00:00:07,840 --> 00:00:12,550
and error network infrastructure, how
the network observability powers the

5
00:00:12,550 --> 00:00:17,080
backbone of artificial LI systems,
and why it matters of the future

6
00:00:17,080 --> 00:00:18,370
of hyper performance computing.

7
00:00:18,870 --> 00:00:19,410
My name is Basra.

8
00:00:19,910 --> 00:00:22,610
I was graduated from University India.

9
00:00:23,110 --> 00:00:27,570
I worked in a couple of networking and
observability companies where I have seen

10
00:00:27,600 --> 00:00:33,385
firsthand how this matters, starting from
the enterprise internet to all the way to

11
00:00:33,405 --> 00:00:36,465
AI networks, which are prevalent nowadays.

12
00:00:36,965 --> 00:00:40,625
In 1960s, aquanet, that was a
packet switching network, which

13
00:00:40,625 --> 00:00:43,675
was formed to facilitate research.

14
00:00:44,175 --> 00:00:47,175
Internet in research
institutions and universities.

15
00:00:47,595 --> 00:00:51,375
So that was the foundation for
of modern internet with very

16
00:00:51,375 --> 00:00:53,025
rudimentary monitoring tools.

17
00:00:53,275 --> 00:00:59,565
It has interface message processing,
console networking, control center

18
00:00:59,565 --> 00:01:04,335
tools, and CP debugging tools,
ping like utilities and so on.

19
00:01:04,835 --> 00:01:10,595
In 1980 to 2000, the evolution of s
network management protocol had happened.

20
00:01:11,345 --> 00:01:15,874
That was a net for, to, it was
a standard tool for monitoring

21
00:01:15,874 --> 00:01:21,835
the network performance and
health metrics in 2002, 2015.

22
00:01:22,115 --> 00:01:25,655
That's where most cloud related.

23
00:01:26,155 --> 00:01:27,115
Revolution happened.

24
00:01:27,595 --> 00:01:33,625
So in 2002 to 2015, the agent based
monitoring which is installed on

25
00:01:33,655 --> 00:01:39,895
VMs or containers like JX data, but
Datadog agents software as service

26
00:01:39,895 --> 00:01:41,695
delivery, reduce infrastructure.

27
00:01:41,905 --> 00:01:42,415
But then.

28
00:01:42,915 --> 00:01:47,835
Like New Relic p Datadog
dashboard and visualizations

29
00:01:47,835 --> 00:01:49,305
have become a common practice.

30
00:01:49,725 --> 00:01:53,925
Real time dashboards become very
standard like Datadog and Stack Driver.

31
00:01:54,764 --> 00:01:58,785
Lifting systems like Threshold, threshold
based, an anomaly detection alerts

32
00:01:58,785 --> 00:02:03,884
like CloudWatch, Jix, integration with
DevOps, tools like PagerDuty, slack,

33
00:02:03,884 --> 00:02:07,845
CICD, and Systems Multi-Cloud monitoring.

34
00:02:08,535 --> 00:02:15,415
2012 to 15 tools like Datadog and Stack
Driver 2015 to now new AI era tools.

35
00:02:15,415 --> 00:02:15,505
There.

36
00:02:16,005 --> 00:02:19,725
Develop on top of whatever the
cloud environments have it.

37
00:02:20,225 --> 00:02:25,084
Pretty much like they're advanced
in terms of network networking.

38
00:02:25,089 --> 00:02:30,484
Observability like Datadog to collect
unified metrics logs, security with the

39
00:02:30,484 --> 00:02:33,025
AI based directing Grafana Pros, low key.

40
00:02:33,085 --> 00:02:36,295
These are open source stack for
metrics, logs, and visualization.

41
00:02:37,015 --> 00:02:37,915
Open telemetry.

42
00:02:37,915 --> 00:02:41,845
Open standard for collecting
crisis metrics and logs arise.

43
00:02:41,995 --> 00:02:47,305
AI fi, AA production model monitoring,
drift detection, explainability.

44
00:02:47,605 --> 00:02:50,275
Dynatrace full stack
observability with determinist.

45
00:02:50,275 --> 00:02:55,945
K again, the cloud providers like
A-W-S-G-C-P Azure, they have native

46
00:02:55,945 --> 00:02:58,705
tools integration integrated in their
cloud environments for observability.

47
00:02:59,205 --> 00:03:02,785
So this is the history of
the observability tools.

48
00:03:02,785 --> 00:03:05,275
Those have happened from the old internet.

49
00:03:05,725 --> 00:03:07,315
Still our.

50
00:03:07,815 --> 00:03:12,846
Why network observability matters for
ea, there are very, some of the very

51
00:03:13,446 --> 00:03:17,496
common reasons are like performance
guarantee, pay failure, domain

52
00:03:17,496 --> 00:03:20,946
isolation, resource optimization,
security assurance, et cetera.

53
00:03:21,196 --> 00:03:25,391
Performance guarantees for EAA
involves setting and enforcing.

54
00:03:26,071 --> 00:03:31,411
Measurable benchmarks for model accuracy,
response time, scalability, and resource

55
00:03:31,411 --> 00:03:33,511
utilization throughout the AI life cycle.

56
00:03:34,261 --> 00:03:38,551
This guarantee ensures that AI
systems may define service level

57
00:03:38,551 --> 00:03:42,181
objectives under varying workloads
and real world conditions.

58
00:03:42,601 --> 00:03:47,071
And establishing such guarantees
is critical for building trust,

59
00:03:47,371 --> 00:03:51,511
ensuring reliability, and aligning a
outputs with business or regulatory

60
00:03:51,511 --> 00:03:54,081
expectations, a failure domain.

61
00:03:54,486 --> 00:03:56,586
Isolation failure domain isolation.

62
00:03:56,586 --> 00:04:01,596
AI refers to the practice of designing
AI systems so that failures in one

63
00:04:01,596 --> 00:04:06,576
component, such as data ingestion,
model training, or inference do not

64
00:04:06,576 --> 00:04:08,406
cascade and impact the entire pipeline.

65
00:04:09,036 --> 00:04:11,741
By segmenting the system into
independent fault tolerant

66
00:04:11,771 --> 00:04:14,381
domains, teams can contain issues.

67
00:04:15,146 --> 00:04:18,686
Simplify, debugging and maintain
overall system resilience.

68
00:04:19,046 --> 00:04:22,716
This approach is essential for ensuring
high availability and durability in

69
00:04:22,716 --> 00:04:24,751
complex and distributed environments.

70
00:04:25,251 --> 00:04:26,481
Resource optimization.

71
00:04:26,836 --> 00:04:30,766
This source optimization AI
focuses on efficiently managing

72
00:04:30,766 --> 00:04:34,846
computational storage and manage memory
resources across the AI life cycle.

73
00:04:35,386 --> 00:04:40,666
From data processing and model
training to deployment and inference

74
00:04:41,146 --> 00:04:45,016
techniques such as model pruning,
quantization hardware acceleration,

75
00:04:45,046 --> 00:04:49,306
and all cloud scheduling help reduce
costs while maintaining performance

76
00:04:49,846 --> 00:04:51,466
effective resource optimization.

77
00:04:51,466 --> 00:04:52,636
Ensure scalability.

78
00:04:53,351 --> 00:04:59,161
Lowers environmental impact and
maximizes investments security which is

79
00:04:59,161 --> 00:05:03,891
very important in AA networks because
AA networks are always susceptible.

80
00:05:03,941 --> 00:05:06,771
They're vulnerable to security threats.

81
00:05:07,431 --> 00:05:11,121
Security assurance in AA involves
implementing measures to protect

82
00:05:11,361 --> 00:05:15,616
systems from threats such as
data breaches, model theft.

83
00:05:16,116 --> 00:05:18,546
Serial effects and unauthorized access.

84
00:05:18,966 --> 00:05:22,716
It includes securing training
data, ensuring model integrity,

85
00:05:23,136 --> 00:05:27,006
enforcing access controls, and
regularly editing system behavior.

86
00:05:27,546 --> 00:05:32,366
Strong security assurance is critical
for maintaining trust complaints and

87
00:05:32,366 --> 00:05:37,646
the safety deployment of a AI in since
two and high stakes environments.

88
00:05:38,146 --> 00:05:38,836
Is not good.

89
00:05:39,226 --> 00:05:43,966
Visual, there is a visual scale
challenge, in networking observability.

90
00:05:44,056 --> 00:05:49,156
The full stack visibility in AA re
refers to a comprehensive monitoring and

91
00:05:49,186 --> 00:05:51,646
understanding of the entire AA pipeline.

92
00:05:52,450 --> 00:05:56,111
From data ingestion and pre-processing
to model training, deployment, and

93
00:05:56,111 --> 00:06:02,080
inference, it allows teams to track
data quality, model performance, and

94
00:06:02,111 --> 00:06:06,491
infrastructure usage in real time,
ensuring transparency and accountability.

95
00:06:06,991 --> 00:06:12,061
This visibility is crucial for
detecting biases, managing drift,

96
00:06:12,541 --> 00:06:16,231
and maintaining the reliability and
ethical integrity of AA systems.

97
00:06:16,731 --> 00:06:17,181
Excuse me.

98
00:06:17,681 --> 00:06:18,791
Distributed tracing.

99
00:06:19,241 --> 00:06:22,721
Distributed tracing in the AI
provides visibility into the flow of

100
00:06:22,721 --> 00:06:28,210
data and execution across different
components of an AI system, such

101
00:06:28,210 --> 00:06:33,760
as data pipelines, model training
services, APIs, and inference engines.

102
00:06:34,361 --> 00:06:40,510
It enables teams to track requests
and processing steps and end to end

103
00:06:41,171 --> 00:06:45,671
help helping identifying latency,
bottlenecks, or failures across.

104
00:06:46,496 --> 00:06:47,756
Distributed environments.

105
00:06:47,905 --> 00:06:51,955
This traceability is crucial for
debugging, optimizing performance

106
00:06:51,955 --> 00:06:54,865
and ensuring accountability
in complex AI workflows.

107
00:06:55,615 --> 00:07:02,245
Realtime telemetry this is an advanced
version of monitoring this in AI

108
00:07:02,305 --> 00:07:06,355
involves the continuous collection
and monitoring of metrics, logs and

109
00:07:06,355 --> 00:07:08,545
events from AI systems as they operate.

110
00:07:09,045 --> 00:07:13,575
It provides immediate insights into
model performance, data, quality,

111
00:07:13,575 --> 00:07:15,376
system health, and user interactions.

112
00:07:15,825 --> 00:07:19,845
This enables the rapid detection
of anomalies, supports proactive

113
00:07:19,845 --> 00:07:25,155
maintenance, and ensures a systems remain
reliable, efficient, and aligned with

114
00:07:25,155 --> 00:07:29,226
expectations In dynamic environments,
hardware level metrics in EA refers

115
00:07:29,226 --> 00:07:34,116
to the monitoring of low level system
indicators such as G-P-U-C-P utilization.

116
00:07:34,611 --> 00:07:39,501
Memory, bandwidth, power consumption,
temperature, and disk input.

117
00:07:39,501 --> 00:07:39,921
Output.

118
00:07:40,431 --> 00:07:43,860
These metrics are critical for
understanding the resource demands

119
00:07:43,920 --> 00:07:47,400
of EA workloads, optimizing
performance and preventing

120
00:07:47,400 --> 00:07:49,140
hardware bottleneck or failures.

121
00:07:49,860 --> 00:07:55,110
By analyzing hardware level data,
teams can fine tune model deployments,

122
00:07:55,230 --> 00:07:59,160
improve efficiency, and ensure
sustainable scaling of AI operations.

123
00:07:59,660 --> 00:08:02,540
Inside the modern network
like the port speed.

124
00:08:02,540 --> 00:08:05,720
Once upon a time, if it was a
hundred gig, it was a revolution.

125
00:08:05,720 --> 00:08:10,430
But nowadays I speak, there are
systems which are being developed

126
00:08:10,430 --> 00:08:17,300
with 800 gig port speed, a single
box in networking as 51.2 terabytes

127
00:08:17,305 --> 00:08:20,815
per second processing capability,
data processing capability such as

128
00:08:20,820 --> 00:08:23,810
the speed, because that is the speed.

129
00:08:24,621 --> 00:08:30,290
Required by AA systems extraordinary
volume of data packets, processes daily

130
00:08:30,350 --> 00:08:32,030
with the large scale AI infrastructure.

131
00:08:32,530 --> 00:08:36,400
Ultra low latency, near
instantaneous response are very

132
00:08:36,400 --> 00:08:37,871
critical for synchronizing.

133
00:08:38,290 --> 00:08:42,190
Network training, reliability, of
course, the finance availability,

134
00:08:42,190 --> 00:08:46,300
essential for uninterrupted model
training and inference operations.

135
00:08:46,631 --> 00:08:52,540
So the network has become a very
crucial comprising of speed, latency.

136
00:08:52,540 --> 00:08:54,891
Reliability, and throughput.

137
00:08:55,391 --> 00:08:56,240
Observability.

138
00:08:56,740 --> 00:08:59,550
So metrics with metrics.

139
00:09:00,045 --> 00:09:02,985
They provide quantitative data
on system performance and health.

140
00:09:03,825 --> 00:09:10,215
A specific metrics include model accuracy,
latency, throughput, data drift, and

141
00:09:10,305 --> 00:09:15,136
resource utilization, which help track how
will the AI systems meet its objectives.

142
00:09:15,526 --> 00:09:19,845
These metrics enable teams to
quickly detect anomalies, optimize

143
00:09:19,875 --> 00:09:24,136
model behavior, and maintain
reliable, scalable AI deployments.

144
00:09:24,636 --> 00:09:29,591
Logs provide detailed timestamp records
of discre events within a system.

145
00:09:30,091 --> 00:09:34,351
Such as data processing steps, model
training, isolation address, and user

146
00:09:34,351 --> 00:09:38,791
interactions, logs of rich, contextual
information that helps diagnose

147
00:09:38,791 --> 00:09:43,111
issues, press failures, and understand
the sequence of actions leading to

148
00:09:43,111 --> 00:09:48,271
a problem in a comprehensive logging
is variation for editing, debugging

149
00:09:48,271 --> 00:09:51,961
complex workflows, and ensuring
transparency throughout the AI life cycle.

150
00:09:52,461 --> 00:09:57,021
Processing AA captured the detailed
end to engineer requests or data as

151
00:09:57,081 --> 00:10:01,261
they move through various components
of an AA system such as data inges

152
00:10:01,921 --> 00:10:04,921
model training and inference services.

153
00:10:05,431 --> 00:10:09,961
They help map the sequence and timing
of operations, revealing dependency

154
00:10:09,961 --> 00:10:13,731
dependencies and pinpointing
where delays are errors occur.

155
00:10:14,451 --> 00:10:18,651
Expressing is crucial for diagnosing
performance issues, understanding

156
00:10:18,651 --> 00:10:23,451
system behavior, and ensuring a
smooth, relatable, a workflows

157
00:10:23,451 --> 00:10:25,581
in distributed AI environments.

158
00:10:26,081 --> 00:10:28,651
Programmable telemetry revolution.

159
00:10:28,651 --> 00:10:34,281
Telemetry has become a rich
monitoring tools like Google,

160
00:10:34,281 --> 00:10:37,396
RPC, and various other products.

161
00:10:37,896 --> 00:10:43,266
Lot of companies are developing telemetry
tools, but traditional monitoring

162
00:10:43,266 --> 00:10:48,276
focuses on correcting and analyzing
predefined system metrics like CPU use

163
00:10:48,276 --> 00:10:52,966
a memory consumption dis, and network
traffic to ensure infrastructure and

164
00:10:52,966 --> 00:10:54,376
applications surrounding smoothly.

165
00:10:55,126 --> 00:10:58,936
It often delays on threshold based
alerts and periodic checks to detect

166
00:10:58,936 --> 00:11:04,066
failures or performance issues, while
effective for basic operational health.

167
00:11:04,756 --> 00:11:08,626
Traditional monitoring likes the deep
contextual insights needed for complex

168
00:11:08,656 --> 00:11:10,606
modern AI or distributor systems.

169
00:11:11,386 --> 00:11:17,136
Modern telemetry such as GRPC goes beyond
traditional monitoring by continuously

170
00:11:17,136 --> 00:11:21,966
correcting rich high resolution data,
including metrics, logs, and events.

171
00:11:22,466 --> 00:11:25,916
Traces, of course, from all
layers of a system in real time.

172
00:11:26,036 --> 00:11:29,786
It leverages advanced analytics,
machine learning and automation to

173
00:11:29,786 --> 00:11:35,936
provide deeper insights into system
behavior, detect anomalies, and predict

174
00:11:35,936 --> 00:11:37,856
issues before they impact users.

175
00:11:38,306 --> 00:11:41,996
This approach is essential for
managing complex distributed AI

176
00:11:41,996 --> 00:11:46,531
systems and dynamic cloud environments
with greater accuracy and agility.

177
00:11:47,031 --> 00:11:49,131
Self-healing network architectures.

178
00:11:49,631 --> 00:11:57,371
So the networks no longer should no longer
need to wait for healing for a human.

179
00:11:57,641 --> 00:12:01,661
They have to self-heal so that
they will be resilient, they will

180
00:12:01,661 --> 00:12:03,816
be available they'll be hype.

181
00:12:04,316 --> 00:12:05,906
They will be crossing high throughput.

182
00:12:06,536 --> 00:12:10,376
So detect self-healing network
architectures, detect operational

183
00:12:10,376 --> 00:12:15,836
efficient issues by continuously
monitoring network health through

184
00:12:15,836 --> 00:12:19,826
real time telemetry, including
metrics, logs, and distributed traces.

185
00:12:20,516 --> 00:12:22,646
They use automated anomaly detection.

186
00:12:23,146 --> 00:12:26,926
A analytics and predefined rules
to identify faults, performance,

187
00:12:26,926 --> 00:12:30,976
degradations, or security threats
without human intervention.

188
00:12:31,366 --> 00:12:36,106
Once detected, these systems can trigger
automatic remediation actions, like

189
00:12:36,136 --> 00:12:41,056
re rerouting the traffic, restarting
services, or adjusting configurations, or

190
00:12:41,146 --> 00:12:45,906
to maintain optimal network performance
and minimize downtime in self-feeding.

191
00:12:45,906 --> 00:12:48,546
Network architecture analysis
plays a critical role.

192
00:12:48,951 --> 00:12:52,671
By processing the continuous
stream of telemetry data to

193
00:12:52,671 --> 00:12:56,061
identify patterns, anomalies, and
root causes of network issues.

194
00:12:56,871 --> 00:13:00,771
This involves yay and machine learning
algorithms to correlate metrics,

195
00:13:00,771 --> 00:13:05,751
logs, and traces across distributed
components, enabling real-time

196
00:13:05,751 --> 00:13:10,491
diagnosis, failures, diagnosis of
failures, or performance degradations.

197
00:13:11,376 --> 00:13:14,766
Effective analysis allows the
system to prioritize incidents,

198
00:13:15,066 --> 00:13:19,416
predict potential faults, and make
informations for automated recovery,

199
00:13:19,986 --> 00:13:24,001
thereby minimizing human intervention
and ensuring network resilience.

200
00:13:24,501 --> 00:13:30,201
The execute phase involves automatically
carrying out corrective actions once an

201
00:13:30,206 --> 00:13:32,061
issue has been detected and analyzed.

202
00:13:32,481 --> 00:13:35,286
This can include tasks
like traffic around fault.

203
00:13:35,466 --> 00:13:35,686
No.

204
00:13:36,681 --> 00:13:40,371
Restarting the failed services,
adjusting configurations, or

205
00:13:40,701 --> 00:13:42,171
scaling resources dynamically.

206
00:13:42,861 --> 00:13:45,201
Education is driven by prefin playbooks.

207
00:13:45,351 --> 00:13:49,281
A power decision engines are
orchestration tools enabling the

208
00:13:49,281 --> 00:13:53,571
network to quickly recover from fault
and maintain continuous reliable

209
00:13:53,571 --> 00:13:55,551
operation without manual intervention.

210
00:13:56,316 --> 00:14:00,351
In healing networks, the decide
phase involves evaluating the analyze

211
00:14:00,351 --> 00:14:05,541
data to determine the most effective
remediation action, the decision making.

212
00:14:06,111 --> 00:14:12,481
Process liberates serial GOs policy
rules and historical incident knowledge

213
00:14:12,481 --> 00:14:16,621
to select the best course of action,
whether to isolate a faulty component,

214
00:14:16,861 --> 00:14:21,211
trigger, create failover, or a play
configuration change, accurate in time,

215
00:14:21,211 --> 00:14:26,731
limitations, or crucial for minimizing
downtime, preventing cascading failures,

216
00:14:26,731 --> 00:14:28,561
and maintaining overall network stability.

217
00:14:29,061 --> 00:14:31,941
These are various different
flows that happen.

218
00:14:32,301 --> 00:14:38,101
AA networks model training in AI is the
process of teaching a machine learning

219
00:14:38,101 --> 00:14:42,961
algorithm to recognize patterns in
data by adjusting its parameters to

220
00:14:42,961 --> 00:14:45,031
improve performance on a specific task.

221
00:14:45,531 --> 00:14:50,271
Parameters serving in EAA is the process
of effectively delivering trained model

222
00:14:50,271 --> 00:14:54,711
parameters to influence systems to
enable fast and scalable predictions.

223
00:14:55,131 --> 00:14:58,551
Data ingestion is the process
of collecting, importing, and

224
00:14:58,551 --> 00:15:02,571
processing raw data from various
sources into your system.

225
00:15:02,631 --> 00:15:04,431
For analysis are a model.

226
00:15:04,431 --> 00:15:08,781
Training inference in AI is the
process where a trained model makes

227
00:15:08,781 --> 00:15:12,441
predictions orions based on new RNC data.

228
00:15:12,941 --> 00:15:16,866
Of course visibility is a very
challenge because to observe the

229
00:15:16,866 --> 00:15:19,266
networks, we need to have visibility.

230
00:15:19,656 --> 00:15:24,006
So that is a still challenge in
some areas, like hardware opacity.

231
00:15:24,006 --> 00:15:26,136
Challenges in AA systems refers to the.

232
00:15:26,636 --> 00:15:29,996
Difficulty of understanding and
optimizing how underlying hardware

233
00:15:29,996 --> 00:15:36,086
like GPU TPU or Specialist Accelerators
affect AI model performance due to

234
00:15:36,086 --> 00:15:41,996
limited visibility in the into hardware
operations and complex interactions

235
00:15:41,996 --> 00:15:43,796
between software and hardware layers.

236
00:15:44,616 --> 00:15:46,326
Hardware is made from made up.

237
00:15:46,956 --> 00:15:48,846
Hardware is made by different vendors.

238
00:15:48,846 --> 00:15:51,696
They follow different
protocols, different procedures.

239
00:15:52,056 --> 00:15:54,666
So it's a common standard to.

240
00:15:55,446 --> 00:15:57,706
Digging into the hardware is a challenge.

241
00:15:58,276 --> 00:16:02,836
Cross-domain correlation in AI system
refers to identifying and analyzing

242
00:16:02,836 --> 00:16:07,876
relationships between data events or
behaviors across different domains or

243
00:16:07,876 --> 00:16:13,036
components, such as combining in insights
from user behavior system logs, network

244
00:16:13,036 --> 00:16:18,136
metrics to improve model accuracy, detect
anomalies, or enhance decision making.

245
00:16:19,051 --> 00:16:21,331
Scale limitations in observability.

246
00:16:21,331 --> 00:16:25,991
Challenges refer to the difficulties in
collecting, processing, and analyzing

247
00:16:25,991 --> 00:16:30,311
massive volumes of telemetry data
generated by large distributed AI systems.

248
00:16:30,701 --> 00:16:34,841
As systems grow our traditional tools
administered with data storage, real-time

249
00:16:34,841 --> 00:16:39,251
processing, and maintaining low latency
insights, making it harder to achieve.

250
00:16:40,121 --> 00:16:43,361
Comprehensive visibility and timely
troubleshooting power coming.

251
00:16:43,361 --> 00:16:46,841
These limitations require scalable
architectures and intelligent data

252
00:16:46,841 --> 00:16:49,451
sampling or aggregation techniques.

253
00:16:49,951 --> 00:16:54,691
How the future of observable networks,
so a powered observability leverages

254
00:16:55,191 --> 00:16:59,811
artificial intelligence and machine
learning to automatically collect.

255
00:16:59,856 --> 00:17:05,616
To analyze and interpret vast amounts of
telemetry data, such as metrics, logs,

256
00:17:05,616 --> 00:17:10,806
and traces to detect anomalies, predict
failures, and provide actionable insights.

257
00:17:11,646 --> 00:17:15,866
This approach enhances traditional
monitoring by enabling faster root

258
00:17:15,866 --> 00:17:20,036
cause analysis, proactive issue
resolution, and smart addition making

259
00:17:20,036 --> 00:17:22,316
in complex AI and distributed systems.

260
00:17:22,816 --> 00:17:26,836
AL level telemetry refer to the
monitoring and collection of detailed

261
00:17:26,836 --> 00:17:30,736
performance and health data directly
from the hardware chip, such as

262
00:17:30,971 --> 00:17:33,136
CPUs, GPUs, or AI accelerators.

263
00:17:33,646 --> 00:17:38,186
This includes metrics like power,
uses, temperature clock speeds.

264
00:17:39,041 --> 00:17:42,731
Cache hits or misses and hardware
interrupts, providing deep visibility

265
00:17:42,731 --> 00:17:46,781
into how the physical hardware
operates and impacts a workloads.

266
00:17:47,171 --> 00:17:51,941
Such fine-grain telemetry is
essential for optimizing performance.

267
00:17:52,031 --> 00:17:55,811
Detecting hardware falls early and
improving overall system reliability.

268
00:17:56,311 --> 00:17:56,791
Excuse me.

269
00:17:57,291 --> 00:18:02,241
Intent based observability in AI focuses
on aligning monitoring and analysis

270
00:18:02,241 --> 00:18:05,481
tools with the desired business or
operational outcomes By interpreting

271
00:18:05,481 --> 00:18:11,031
the system's intent behaviors,
instead of just collecting raw data,

272
00:18:11,061 --> 00:18:15,771
it uses AI to understand whether the
system actions meet pre-define goals,

273
00:18:16,011 --> 00:18:20,631
enabling a prior to detection of
deviations and automated adjustments.

274
00:18:20,961 --> 00:18:24,861
This approach helps ensure
AI systems remain aligned.

275
00:18:25,611 --> 00:18:29,811
With user expectations and business
objectives in dynamic environments.

276
00:18:29,811 --> 00:18:35,211
For example, instead of checking how
the CPU sales is, it's a business based

277
00:18:35,211 --> 00:18:42,551
policy, are we serving the 4K quality
to the important users in Euro region?

278
00:18:42,731 --> 00:18:44,201
So that is the business objective.

279
00:18:44,201 --> 00:18:46,841
Based on that, we can take
corrective actions in the observable.

280
00:18:47,341 --> 00:18:52,381
Digital twin networks are virtual replicas
of physical network infrastructure that

281
00:18:52,381 --> 00:18:56,341
simulate their behavior, performance,
and interactions in real time.

282
00:18:57,121 --> 00:19:02,701
They enable a testing, monitoring,
and optimizing network operations

283
00:19:02,731 --> 00:19:05,251
by mirroring live conditions
without impacting the actual system.

284
00:19:05,251 --> 00:19:09,166
So this technology helps in predictive
maintenance, capacity planning,

285
00:19:09,166 --> 00:19:12,431
and accelerating troubleshooting
in complex networking environments.

286
00:19:12,931 --> 00:19:18,301
So in a closing note, our network
observability has become a, it plays a

287
00:19:18,301 --> 00:19:22,901
key role in planning, maintaining, and

288
00:19:23,401 --> 00:19:28,141
maintaining networks for high
resiliency by processing of throughput.

289
00:19:29,071 --> 00:19:30,961
Ultra latency and so on.

290
00:19:31,021 --> 00:19:38,071
So this network durability is very
important in the coming years of a based

291
00:19:38,071 --> 00:19:44,531
networks, which are very low latency
and high performance computing networks.

292
00:19:44,681 --> 00:19:44,952
Thank you.

