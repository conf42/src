1
00:00:02,760 --> 00:00:03,420
Hi folks.

2
00:00:03,480 --> 00:00:04,050
I'm Santos.

3
00:00:05,010 --> 00:00:06,600
I'm a senior database administrator.

4
00:00:06,990 --> 00:00:08,550
As a senior database administrator.

5
00:00:08,550 --> 00:00:12,660
I spent so long talked to service,
started to understand my jokes,

6
00:00:12,660 --> 00:00:14,010
mostly the s ql o and stuff.

7
00:00:14,700 --> 00:00:18,600
So today I'm gonna talk about
AI driven anomaly detection

8
00:00:18,600 --> 00:00:20,009
for cloud data pipelines.

9
00:00:20,550 --> 00:00:23,600
So picture this you got
all this amazing data.

10
00:00:23,710 --> 00:00:25,300
Flowing into your company, right?

11
00:00:25,600 --> 00:00:29,080
It's like a Jane Digital
River powering everything from

12
00:00:29,130 --> 00:00:31,290
showing you relevant cat videos.

13
00:00:31,320 --> 00:00:31,740
Okay?

14
00:00:32,230 --> 00:00:36,220
Actually running the business now, keeping
that river clean and flowing smoothly,

15
00:00:36,550 --> 00:00:38,440
that's where the things get tricky.

16
00:00:38,770 --> 00:00:39,550
So think about it.

17
00:00:39,610 --> 00:00:43,510
We are talking about tons of data coming
from everywhere, going through all sort

18
00:00:43,510 --> 00:00:45,430
of twists and turns into the cloud.

19
00:00:45,820 --> 00:00:49,390
Is that trying to manage the plumbing
in a city like City of Texas?

20
00:00:49,930 --> 00:00:53,470
One little clog, one tiny leak, and
suddenly your reports are garbage.

21
00:00:53,530 --> 00:00:57,940
So your decisions are based on who knows
what, and you're potentially facing some

22
00:00:57,940 --> 00:01:01,150
C ds Oops moments with the folks in suits.

23
00:01:01,630 --> 00:01:04,510
Traditional ways of checking
if things are going wrong.

24
00:01:04,540 --> 00:01:07,810
They're like having a guy with
checklists standing by the river,

25
00:01:07,870 --> 00:01:10,060
looking for a specific types of trash.

26
00:01:10,510 --> 00:01:12,790
But what if the problem
isn't obviously trash?

27
00:01:13,150 --> 00:01:16,750
What if it's just a water turning
a weird shade off green, slowly

28
00:01:16,750 --> 00:01:18,160
poisoning everything downstream?

29
00:01:19,230 --> 00:01:21,600
So that's where our
superhero comes in, ai.

30
00:01:21,600 --> 00:01:25,675
Over next 20, 25 to 30 minutes,
I'm going to show you how we have

31
00:01:25,675 --> 00:01:29,304
built this smart system that's like
a superpower water quality expert

32
00:01:29,304 --> 00:01:30,865
for your driver, data drivers.

33
00:01:31,345 --> 00:01:35,154
It use all the fancy AI stuff,
machine learning that actually learns.

34
00:01:35,390 --> 00:01:36,920
What normal water looks like.

35
00:01:37,310 --> 00:01:40,820
Deep learning that can spot even
the subtle changes in the flow, and

36
00:01:40,820 --> 00:01:44,990
even some brainy stuff that tries
to understand what the data means.

37
00:01:45,050 --> 00:01:48,980
This isn't just about finding big,
obvious errors, it's about getting a real

38
00:01:48,980 --> 00:01:53,450
healthy report on your data so we can
fix things before they become a disaster.

39
00:01:53,840 --> 00:01:56,570
Think of it as a preventative
medicine for your data.

40
00:01:56,880 --> 00:01:59,160
We are better than waiting
for a data to get sick, right?

41
00:01:59,280 --> 00:01:59,880
Hey.

42
00:02:00,229 --> 00:02:04,360
Again, my name is Santos and I'm
still chatting with you all day today.

43
00:02:05,770 --> 00:02:09,370
I'm gonna talk about the data
changes of modern data pipelines.

44
00:02:09,470 --> 00:02:14,920
Where why AI superheroes so necessary in
the first place of modern data pipelines.

45
00:02:15,220 --> 00:02:17,140
They're not your grandpa Grand hose.

46
00:02:18,040 --> 00:02:21,130
They're more like a massive,
interconnected networks of pipes,

47
00:02:21,190 --> 00:02:25,060
pumps, and filters spread across
and entire digital landscape.

48
00:02:25,450 --> 00:02:28,210
And that complexity, it brings
a whole heap of headaches.

49
00:02:28,480 --> 00:02:32,740
So complex architecture, the web of
interconnections, think of it as a game.

50
00:02:33,570 --> 00:02:37,230
Digital Spa Monster data comes in
from a million different places.

51
00:02:37,230 --> 00:02:40,680
Maybe it's just customers like a
website, sunsets in a factory, sales

52
00:02:40,680 --> 00:02:43,140
figures from a dozen different systems.

53
00:02:43,140 --> 00:02:46,830
Then it gets tossed around between
all these different cloud services.

54
00:02:47,340 --> 00:02:49,680
One thing transforms is another store.

55
00:02:50,125 --> 00:02:50,365
It.

56
00:02:50,875 --> 00:02:54,595
And another analyze it, and then
it gets sent to even more places.

57
00:02:54,595 --> 00:02:57,445
Each time it jumps from
one system to other.

58
00:02:57,775 --> 00:02:59,665
There's a chance to think to go wrong.

59
00:03:00,055 --> 00:03:02,545
Maybe the data gets transferred
incorrectly, maybe the little

60
00:03:03,295 --> 00:03:06,535
bit gets lost in the transit,
or maybe the system just have a

61
00:03:06,535 --> 00:03:08,515
different idea about what a data.

62
00:03:08,875 --> 00:03:13,195
Date or a number even means this
tangled web of connections is just

63
00:03:14,304 --> 00:03:18,475
begging for subtle errors to sneak it
and trying to track them down the old

64
00:03:18,475 --> 00:03:22,945
school methods is like trying to find a
specific knowledge in what whole mess I.

65
00:03:23,320 --> 00:03:24,130
Good luck with that.

66
00:03:24,610 --> 00:03:27,850
So difficult detection, finding
the needle in a haystack.

67
00:03:27,850 --> 00:03:32,980
So you've got this crazy complex system
with tons of data whizzing around how do

68
00:03:32,980 --> 00:03:34,990
you even know when something goes wrong?

69
00:03:35,590 --> 00:03:39,340
Traditionally monitoring is
setting up a few basic alarms.

70
00:03:39,400 --> 00:03:42,580
If the water pressure drops
below this point, some the other.

71
00:03:42,640 --> 00:03:46,990
But if a problem isn't a certain drop,
what if the gradual decrease over time?

72
00:03:47,709 --> 00:03:53,410
Or a slight change in our color that only
trained, I would notice this subtle shift.

73
00:03:53,469 --> 00:03:57,790
These tiny deviations from the usual
flow might not trigger any of those

74
00:03:57,790 --> 00:04:02,950
basic alarms, but they could be early
signs of much bigger problem brewing.

75
00:04:03,850 --> 00:04:08,695
And with the sheer volume of data we are
dealing with manually, looking for these

76
00:04:09,820 --> 00:04:10,854
little clues is basically impossible.

77
00:04:11,485 --> 00:04:15,295
You need any army of, you
need army of data detection.

78
00:04:15,295 --> 00:04:19,045
Starting at endless streams of
numbers made a smart system that

79
00:04:19,045 --> 00:04:24,175
can learn what normal look like, so
it can automatically flag even the

80
00:04:24,175 --> 00:04:26,965
quietest something, not right signals.

81
00:04:27,474 --> 00:04:32,655
So it can automatically flag those
without, with those, a little bit of

82
00:04:32,655 --> 00:04:34,844
tiny minute, something not right signals.

83
00:04:35,414 --> 00:04:38,505
So financial impact, the
hidden cost of bad data.

84
00:04:38,685 --> 00:04:43,484
Now we are, we might be thinking,
okay, so some data gets a little wonky.

85
00:04:43,784 --> 00:04:44,265
Big deal.

86
00:04:44,315 --> 00:04:44,675
Wrong.

87
00:04:45,185 --> 00:04:47,585
So bad data ist just a technical problem.

88
00:04:47,585 --> 00:04:50,015
It hits the company where it really hurts.

89
00:04:50,075 --> 00:04:53,135
The valid and the numbers
are actually pretty scary.

90
00:04:53,525 --> 00:04:57,515
Studies shows the companies
can lose around 20% of their

91
00:04:57,515 --> 00:04:59,375
revenue due to poor data quality.

92
00:04:59,975 --> 00:05:01,565
Let that sink in for a second.

93
00:05:02,285 --> 00:05:08,165
That's like throwing one out of every
$5 straight into a digital garbage care.

94
00:05:09,335 --> 00:05:10,475
And where does this money go?

95
00:05:11,255 --> 00:05:16,085
Wasted time, compliance, nightmares,
dumb decisions, and a angry

96
00:05:16,085 --> 00:05:18,005
customer and a missed opportunity.

97
00:05:18,195 --> 00:05:18,961
It all adds up.

98
00:05:19,125 --> 00:05:21,795
So yeah, good data is
in just a tech thing.

99
00:05:21,795 --> 00:05:23,895
It's a straight up business necessity.

100
00:05:26,775 --> 00:05:30,135
So few of the key
features of our solution.

101
00:05:30,885 --> 00:05:34,755
So how does our AI power system
actually step in and save the day?

102
00:05:35,715 --> 00:05:39,465
It's, it got a whole toolbox of
clever features designed to be

103
00:05:39,525 --> 00:05:44,304
ultimate data quality guardians
and advanced detection algorithm.

104
00:05:44,785 --> 00:05:46,705
The multilayered approach to intelligent.

105
00:05:46,945 --> 00:05:50,965
We don't just throw one type of AI at
this problem and help hope for the best.

106
00:05:50,965 --> 00:05:55,495
We got a whole team of digital
detectives each with their own specific

107
00:05:55,555 --> 00:06:00,175
skills, the old school smarties, using
statistics to spot obvious awareness

108
00:06:00,175 --> 00:06:04,675
and the modern brain using deep
learning to catch the real subtle stuff.

109
00:06:05,365 --> 00:06:08,905
Adaptive learning, continuous
implement to experience this

110
00:06:08,905 --> 00:06:11,335
system is in just one trick pony.

111
00:06:11,784 --> 00:06:15,715
It actually learns from data, it sees
the feedback it gets from your smart

112
00:06:15,715 --> 00:06:20,455
humans consistently getting better at
knowing what normal and what's not.

113
00:06:21,505 --> 00:06:25,495
So real time processing, immediate
insight and actionability in the cloud.

114
00:06:25,495 --> 00:06:28,885
Things move fast and
then so does our system.

115
00:06:29,305 --> 00:06:33,775
It's spots problem as they happen,
letting us jump in and then things fix

116
00:06:33,775 --> 00:06:37,015
things before they cause real chaos.

117
00:06:37,115 --> 00:06:37,955
Our AI driven.

118
00:06:38,540 --> 00:06:41,780
Anomaly detection system tackles
these challenges head on with a

119
00:06:41,780 --> 00:06:44,810
suit of sophisticated features
designed for proactive and

120
00:06:45,110 --> 00:06:47,300
intelligence, data quality management.

121
00:06:48,170 --> 00:06:51,304
So when it comes to advanced
data detection, I. Advanced

122
00:06:51,304 --> 00:06:54,244
detection algorithm and multiple
layered approach to intelligence.

123
00:06:54,244 --> 00:06:58,534
Our system doesn't rely on single
magic bullet algorithm Instead.

124
00:07:00,005 --> 00:07:03,065
Instead, it employs a careful
curated combination of techniques

125
00:07:03,575 --> 00:07:06,455
to provide comprehensive and
accurate anomaly detection.

126
00:07:07,114 --> 00:07:08,645
Traditional statistics techniques.

127
00:07:08,650 --> 00:07:13,310
We leverage a. Establish statistics
methods like control charts for

128
00:07:13,310 --> 00:07:15,020
monitoring data distribution over time.

129
00:07:15,560 --> 00:07:19,370
I identifying out outliners
based on standard deviations and

130
00:07:19,370 --> 00:07:23,120
employing time series analysis to
forecast expected values and detect

131
00:07:24,050 --> 00:07:25,670
deviations from these forecasts.

132
00:07:25,730 --> 00:07:30,290
These provides a strong foundation for
identifying well-defined anomalies,

133
00:07:32,990 --> 00:07:36,525
modern deep learning models to
capture more complex and new.

134
00:07:37,744 --> 00:07:38,854
Nuisance patterns.

135
00:07:38,854 --> 00:07:44,614
We integrated cutting edge deep learning
models, variational auto encoders,

136
00:07:44,674 --> 00:07:47,974
learning the underlying structure
of normal data, and can identify

137
00:07:48,064 --> 00:07:51,814
subtle anomalies by detect instance
that are difficult to reconstruct.

138
00:07:53,075 --> 00:07:57,544
Long short term memory networks
are crucial for understanding

139
00:07:57,664 --> 00:07:58,804
temporal dependencies.

140
00:07:58,804 --> 00:07:59,614
In sequence.

141
00:08:00,304 --> 00:08:04,054
Networks are crucial for understanding
temporal dependencies in sequential

142
00:08:04,054 --> 00:08:06,030
data, allowing us to detect anomalies.

143
00:08:06,635 --> 00:08:10,955
That manifest as usual sequence of
events or shift in trend over time.

144
00:08:13,475 --> 00:08:17,525
Assembly methods, the wisdom
of crowd to further enhance the

145
00:08:17,525 --> 00:08:20,975
accuracy and robust offer detection,
be utilize assembly methods.

146
00:08:21,544 --> 00:08:25,355
This involves combination of predictions
from multiple different algorithms.

147
00:08:25,955 --> 00:08:29,645
By leveraging the strength of each
individual model, we can achieve

148
00:08:29,645 --> 00:08:33,815
higher oral accuracy and reduce
the currency of false posture.

149
00:08:33,875 --> 00:08:38,255
For example, a statistical model
might quickly flag a larger outliner,

150
00:08:38,315 --> 00:08:42,784
while a deep learning model might
detect subtle but persistent drift in

151
00:08:42,784 --> 00:08:48,695
a complex data relationship, so real
time processing, immediate insight.

152
00:08:49,695 --> 00:08:53,205
In immediate insight in the dynamics
of the cloud data pipelines speed

153
00:08:53,205 --> 00:08:57,135
in the Paramount, our system is
engineered for realtime processing as

154
00:08:57,135 --> 00:08:58,695
data streams through the pipelines.

155
00:08:58,695 --> 00:09:02,265
It is con, continuously analyzed
by our detection algorithm.

156
00:09:03,630 --> 00:09:08,040
When anomalies identified, an alert
is generated or almost instantly.

157
00:09:08,250 --> 00:09:11,190
This is realtime capability is
crucial for several reasons.

158
00:09:11,250 --> 00:09:15,930
Preventing error propagation, faster
incident response, enabling realtime

159
00:09:16,530 --> 00:09:21,045
decision making for applications that
rely on realtime data immediately.

160
00:09:21,155 --> 00:09:24,285
Anomaly detection ensures that
decisions are based on most

161
00:09:24,565 --> 00:09:25,805
accurate and accurate information.

162
00:09:26,805 --> 00:09:29,205
Adaptive learning, continuous
improvements through experience.

163
00:09:29,370 --> 00:09:29,640
Data.

164
00:09:29,640 --> 00:09:30,510
Pipelines are leaving.

165
00:09:30,510 --> 00:09:31,890
Breathing systems.

166
00:09:32,265 --> 00:09:33,795
That consistently evolve.

167
00:09:34,485 --> 00:09:37,935
Our system is designed to adopt
these changes through continuous

168
00:09:37,935 --> 00:09:43,095
learning, learning from historical
data, incorporating user feedback,

169
00:09:43,365 --> 00:09:47,030
implicit learning through remediations
dynamic threshold adjustments.

170
00:09:47,790 --> 00:09:51,300
So based on continuous feedback,
observation, data patterns,

171
00:09:51,330 --> 00:09:54,360
the system dynamically adjusts
its detection thresholds.

172
00:09:54,840 --> 00:10:00,240
This ensures that system remains s due
to genuine anomalies while minimizing

173
00:10:00,240 --> 00:10:03,955
false alarm as underlying data
characteristics exchange over time.

174
00:10:04,955 --> 00:10:06,815
Now, system design and architecture.

175
00:10:08,765 --> 00:10:10,415
Alright, let's speak.

176
00:10:10,745 --> 00:10:14,255
Under the hood and see how we are
actually build the brainy system.

177
00:10:14,345 --> 00:10:18,335
We designed it specifically for the
cloud, thinking about making it flexible,

178
00:10:18,395 --> 00:10:24,215
able to handle huge amount of data and
easy to maintain, flexible and scalable

179
00:10:24,995 --> 00:10:28,325
adapting to diverse cloud landscapes.

180
00:10:28,355 --> 00:10:32,530
We know you folks are in all using
the same brand of cloud, so you may.

181
00:10:33,440 --> 00:10:37,430
We made sure our system plays
well with all the major players

182
00:10:37,430 --> 00:10:38,690
and can handle your growth.

183
00:10:39,470 --> 00:10:42,800
Growing data needs, it's container,
containerizing, microservices,

184
00:10:42,860 --> 00:10:47,870
independent unit of functionality
instead of one big complicated thing.

185
00:10:47,930 --> 00:10:51,380
We got a bunch of smaller
independent parts like Legos,

186
00:10:51,590 --> 00:10:52,730
each doing a specific job.

187
00:10:53,135 --> 00:10:56,165
This, make it easy to deploy,
scale the right parts, and makes

188
00:10:56,165 --> 00:10:57,665
the whole system more reliable.

189
00:10:58,745 --> 00:11:01,085
Modular components,
adaptability and evolution.

190
00:11:01,115 --> 00:11:05,105
This Lego like approach means we
can easily update, replace, or

191
00:11:05,105 --> 00:11:08,525
even customize the parts of systems
without bringing everything down.

192
00:11:09,995 --> 00:11:12,445
So in depth in the cloud.

193
00:11:13,020 --> 00:11:16,110
The challenges and opportunities
of cloud environment in mind,

194
00:11:16,110 --> 00:11:19,200
emphasizing flexibility,
scalability, and maintainability.

195
00:11:19,920 --> 00:11:23,820
So adapting to the diverse landscape,
recognizing that our users operate

196
00:11:23,820 --> 00:11:25,200
in diverse cloud environments.

197
00:11:25,200 --> 00:11:28,320
We are architecting our system to
be highly flexible and adaptable.

198
00:11:28,920 --> 00:11:33,900
It's designed to integrate seamlessly with
major cloud provider like AWS, Azure and

199
00:11:34,200 --> 00:11:37,140
GCP, as well as hybrid cloud deployments.

200
00:11:38,070 --> 00:11:38,490
This flex.

201
00:11:39,450 --> 00:11:43,920
This flexibility extends to the
type of data sources and pipeline

202
00:11:43,920 --> 00:11:45,210
components it can monitor.

203
00:11:45,930 --> 00:11:49,170
Furthermore, scalability is
a core tenant of Artisan.

204
00:11:49,500 --> 00:11:53,580
As your data volume and the processing
demand grow in the cloud, our system can

205
00:11:53,580 --> 00:11:59,700
scalable scale horizontally by adding more
instance of its microservices, enduring

206
00:11:59,700 --> 00:12:01,675
consistent performance without bottleneck.

207
00:12:02,500 --> 00:12:04,545
Containerizing microservices.

208
00:12:04,635 --> 00:12:06,435
Independent unit of functionality.

209
00:12:06,495 --> 00:12:10,575
We embrace a microservices architecture
where system is broken down into

210
00:12:10,575 --> 00:12:15,615
collection of small independent services,
each responsible for a specific function.

211
00:12:15,645 --> 00:12:20,565
Data ingestions features engineering,
statistical analysis, deep learning

212
00:12:20,565 --> 00:12:24,135
interference, alert management,
and a feedback processing.

213
00:12:24,195 --> 00:12:25,320
Those microservices.

214
00:12:25,530 --> 00:12:30,300
Are a package into a lead lightweight
container using technology like Docker.

215
00:12:31,740 --> 00:12:37,470
This container offers several key
advantages, simplified deployment,

216
00:12:37,710 --> 00:12:42,300
independent scaling fault,
isolation, technology agnostic.

217
00:12:42,960 --> 00:12:47,220
So microservices allows us to
choose the best technology stack

218
00:12:47,220 --> 00:12:52,530
of each specific function promoting
innovative innovation and efficiency.

219
00:12:53,880 --> 00:12:58,770
Modular components, so the modular
design of our system goes hand in

220
00:12:58,770 --> 00:13:00,330
hand with the microservices approach.

221
00:13:00,675 --> 00:13:03,615
Each microservices represents
a distinct replaceable module.

222
00:13:04,275 --> 00:13:07,755
This modularity provides significant
benefits for a long time.

223
00:13:07,805 --> 00:13:11,615
Maintainability, and
evaluation simplified updates.

224
00:13:11,645 --> 00:13:15,425
Individual module can be updated or
passed without requiring a complete

225
00:13:15,425 --> 00:13:19,895
system redeployment, minimizing
downtime and risk technology upgrades.

226
00:13:19,925 --> 00:13:23,645
As a new and more efficient algorithms
are technologies become available,

227
00:13:23,735 --> 00:13:26,855
we can seamlessly integrate them
by replacing existing audios.

228
00:13:29,075 --> 00:13:33,755
Customizing and extensibility The
modeler design allows for potential

229
00:13:34,085 --> 00:13:37,775
customization and extensions of system
to meet specific user requirement.

230
00:13:37,835 --> 00:13:38,465
In the future,

231
00:13:41,255 --> 00:13:45,485
new modules for specialized data
types or detection techniques

232
00:13:45,845 --> 00:13:48,845
could be added without affecting
the more functionalities,

233
00:13:49,845 --> 00:13:50,985
data ingestion, and traffic.

234
00:13:52,170 --> 00:13:56,910
Okay, so before our AI brains
can do their magic, they need

235
00:13:56,910 --> 00:13:58,410
good clean data to work with.

236
00:13:58,710 --> 00:14:01,350
Think of it like a chef
needing good ingredients.

237
00:14:02,280 --> 00:14:06,250
This slide is about how we grab the data
and then get it ready for the analysis.

238
00:14:06,850 --> 00:14:09,760
So data capture, comprehensive
monitoring coverage.

239
00:14:09,790 --> 00:14:14,380
We like, we are like thorough
detective gathering all the source

240
00:14:14,380 --> 00:14:18,460
of clue, not just like a main data,
but also the data about the data.

241
00:14:19,000 --> 00:14:23,620
How the pipeline is behaving and even
little snapshots of data itself, and

242
00:14:23,670 --> 00:14:28,800
you can tell us what clue are most
important to collect Data captures

243
00:14:28,800 --> 00:14:30,360
comprehensive monitoring coverage.

244
00:14:30,390 --> 00:14:33,060
Our system is designed to a
holistic monitoring solution

245
00:14:33,090 --> 00:14:34,140
for your data pipelines.

246
00:14:34,170 --> 00:14:40,085
This means we don't just focus on raw
transactional data, we also collect

247
00:14:40,110 --> 00:14:42,120
a rich set of contextual information.

248
00:14:42,615 --> 00:14:47,775
Metadata, operational metrics, sample
data, configuration collection.

249
00:14:48,435 --> 00:14:52,215
So this includes information about
the data such as source schema,

250
00:14:52,245 --> 00:14:54,225
data types, timestamps and lineages.

251
00:14:54,855 --> 00:14:59,265
Changes in metadata can often be early
indicators of the problem, so we also

252
00:14:59,265 --> 00:15:02,925
gather performance metrics from various
components of data pipeline itself,

253
00:15:02,925 --> 00:15:05,540
such as processing time, CPU, memory.

254
00:15:06,690 --> 00:15:08,820
Network latency and error rates.

255
00:15:08,940 --> 00:15:11,970
Anomalies in these operational
metrics can often correlate with

256
00:15:12,150 --> 00:15:14,940
even precise data called tissues.

257
00:15:15,660 --> 00:15:18,450
So to understand the actual
content and structure of the data,

258
00:15:18,450 --> 00:15:21,530
we strategically sample data at
various points in the pipeline.

259
00:15:22,160 --> 00:15:26,000
This allows our algorithm to learn
the normal patterns and identify

260
00:15:26,000 --> 00:15:27,740
deviations in the data values themselves.

261
00:15:29,060 --> 00:15:32,900
The specific data points and metrics we
collect are configurable, allowing you

262
00:15:32,900 --> 00:15:38,450
to tailor the monitoring to the most cri
critical aspect of your data pipeline.

263
00:15:39,620 --> 00:15:45,090
So feature engineering, e extracting
meaning the raw data is often messy.

264
00:15:45,240 --> 00:15:50,130
Feature engineering is like our expert
chef, taking those raw ingredients and

265
00:15:50,130 --> 00:15:53,940
prepping them just right, so our AI
models can actually understand them.

266
00:15:55,050 --> 00:15:58,890
The raw data is just a native format,
is often not directly suitable

267
00:15:58,890 --> 00:16:00,210
for machine learning analysis.

268
00:16:00,840 --> 00:16:03,720
Feature engineering is a crucial
process of transforming this raw

269
00:16:03,720 --> 00:16:07,200
data into a set of meaningful
features that are anomaly detects.

270
00:16:07,230 --> 00:16:09,750
Models can effectively learn from.

271
00:16:10,560 --> 00:16:14,580
This involves several techniques, data
cleaning, handling missing values,

272
00:16:14,760 --> 00:16:18,870
correcting, inconsistency, and removing
noise from the data transformation.

273
00:16:18,870 --> 00:16:23,190
Scaling numeric features ENC coding,
categorial variables, and applying

274
00:16:23,190 --> 00:16:26,850
mathematical transformations to
make the data more suitable for the

275
00:16:26,850 --> 00:16:31,590
algorithm creation of new features,
deriving new features from the

276
00:16:31,590 --> 00:16:34,620
existing data from the high highlights.

277
00:16:35,370 --> 00:16:36,480
Potential anomalies.

278
00:16:36,480 --> 00:16:40,620
For example, calculating the rate of
change of a metric over time, or creating

279
00:16:40,620 --> 00:16:45,210
interaction features between different
data points, dimensional direction

280
00:16:45,330 --> 00:16:46,770
in the high dimensional dataset.

281
00:16:46,770 --> 00:16:49,530
The techniques like
principle component analysis.

282
00:16:50,520 --> 00:16:55,980
Also, CALS, PCA, can be used to reduce
the number of features while preserving

283
00:16:55,980 --> 00:17:00,570
the most important information, improving
model efficiency and reducing noise.

284
00:17:02,220 --> 00:17:04,290
Next, coming to the format optimization.

285
00:17:05,820 --> 00:17:12,035
So different AM models are picky, so we
make sure that the data is in the perfect

286
00:17:12,035 --> 00:17:14,135
format for each one of its best work.

287
00:17:15,255 --> 00:17:19,215
It's all about making sure it's
good data, good insights out.

288
00:17:21,045 --> 00:17:24,315
Our pre-processing pipelines ensure
that the engineering features are in

289
00:17:24,315 --> 00:17:28,695
the I format for each of the anomaly
detection algorithm we employ.

290
00:17:30,375 --> 00:17:32,415
This might involve
normalization and scaling.

291
00:17:32,655 --> 00:17:33,105
Enduring.

292
00:17:33,135 --> 00:17:37,095
All numerical features are on a similar
scale to prevent certain features from

293
00:17:37,095 --> 00:17:41,415
dominating the CER learning process,
data type conversions, converting data

294
00:17:41,415 --> 00:17:43,575
into appropriate data types except.

295
00:17:44,430 --> 00:17:48,450
Expected by the models structuring
data for specific models.

296
00:17:48,450 --> 00:17:53,890
For example, formatting time series
data into a sequence for LSGM networks,

297
00:17:54,130 --> 00:17:56,170
or creating input vector for VAEs.

298
00:17:56,920 --> 00:18:01,270
This meticulous data tion and tree
processing data is fundamental to

299
00:18:01,270 --> 00:18:04,810
overall accuracy and efficiency
of our anomaly detection system.

300
00:18:05,560 --> 00:18:07,000
Garbage in, garbage out.

301
00:18:07,270 --> 00:18:12,910
We ensure that our AI are working with
clean, relevant, and well formatted data.

302
00:18:13,910 --> 00:18:16,670
Anomaly detect detection algorithms.

303
00:18:16,730 --> 00:18:19,070
Alright, let's get a
little more technical and.

304
00:18:19,760 --> 00:18:23,570
Talk about the actual brains behind
our anomaly detection system.

305
00:18:23,660 --> 00:18:29,060
The algorithms, we got a whole stable of
different mod models working together.

306
00:18:29,360 --> 00:18:33,260
Statistical model, there are more
reliable workhorses using classic

307
00:18:33,260 --> 00:18:35,060
methods to spot unusual stuff.

308
00:18:36,200 --> 00:18:37,910
Variational auto encoders.

309
00:18:38,000 --> 00:18:39,260
These are the deep learning art.

310
00:18:40,445 --> 00:18:43,385
Artist learning to
recognize complex patterns.

311
00:18:43,445 --> 00:18:46,685
LSGM networks, these
are the memory experts.

312
00:18:46,685 --> 00:18:49,235
Great for spotting weird
sequences in the data.

313
00:18:49,325 --> 00:18:49,835
Over time.

314
00:18:50,915 --> 00:18:52,685
Assembly assemble methods.

315
00:18:53,075 --> 00:18:56,165
We combine the strength of
all these different brains to

316
00:18:56,165 --> 00:18:58,085
get the most accurate results.

317
00:18:58,145 --> 00:19:03,185
It's having a team of specialists all
working together to solve the case.

318
00:19:04,355 --> 00:19:06,005
It's a time series analysis.

319
00:19:07,085 --> 00:19:11,705
For time dependent data like pipeline
metrics, these models forecast

320
00:19:11,705 --> 00:19:16,445
expected features, values based on
historical trends and seasonality.

321
00:19:17,375 --> 00:19:21,395
Deviations from these forecasts
are flagged as potential anomalies.

322
00:19:21,725 --> 00:19:23,255
Con control chats.

323
00:19:23,885 --> 00:19:26,735
These visually tracked
data points over time.

324
00:19:26,885 --> 00:19:30,245
Agne, statistically calculated
upper and lower control limits.

325
00:19:31,010 --> 00:19:35,570
Points failing outside these limits are
considered statistically significant

326
00:19:35,630 --> 00:19:38,990
anomalies, distribution based methods.

327
00:19:39,980 --> 00:19:42,280
Example gas and mixture models.

328
00:19:42,340 --> 00:19:46,350
These models learn the underlying
probability distribution of the data

329
00:19:46,350 --> 00:19:49,950
points with the lower probability
of belonging of the learning

330
00:19:49,950 --> 00:19:52,020
distributions are flagged as anomalies.

331
00:19:53,129 --> 00:19:55,230
Proximity based methods.

332
00:19:55,290 --> 00:20:00,270
These methods identify anomalies
based on the distance to their nearest

333
00:20:00,450 --> 00:20:04,860
neighbors in the data space, the data
points that are significantly far

334
00:20:04,860 --> 00:20:09,870
from other data points are considered
unusual assembly methods, combining

335
00:20:09,870 --> 00:20:12,060
strength for superior performance.

336
00:20:12,060 --> 00:20:16,800
Our assembly approach strategically
combines the output of these diverse

337
00:20:16,800 --> 00:20:19,710
statistical models and our approach.

338
00:20:21,465 --> 00:20:25,185
Advanced deep learning models to achieve
more accurate and reliable anomaly.

339
00:20:25,185 --> 00:20:30,225
Detections weighted averaging, assigning
different weights to the predictions

340
00:20:30,225 --> 00:20:34,245
of individual models based on their
historical performance and expertise in

341
00:20:34,245 --> 00:20:39,105
detecting specific type of anomalies,
working, combining the predictions

342
00:20:39,105 --> 00:20:41,025
of multiple models through major.

343
00:20:41,385 --> 00:20:44,505
Majority voting and other
aggregation techniques.

344
00:20:45,105 --> 00:20:49,635
Stacking using a metal learner model
to learn how to best combine the

345
00:20:49,635 --> 00:20:51,525
predictions of base level models.

346
00:20:52,035 --> 00:20:56,355
So these strategies helps to mitigate the
weakness of individual models and leverage

347
00:20:56,355 --> 00:21:00,675
their strengths leading to a more robust
and accurate overall detection system.

348
00:21:01,575 --> 00:21:06,855
So variational auto encoders, unsupervised
learning for complex anomalies are

349
00:21:07,125 --> 00:21:08,895
powerful class of deep learning models.

350
00:21:09,254 --> 00:21:13,095
Particularly well suited for
unsupervised anomaly detection in

351
00:21:13,095 --> 00:21:18,044
complex, high dimensional data,
learning latent representations.

352
00:21:18,225 --> 00:21:23,415
So VAEs learns a compressed, low
dimensional representation and latent

353
00:21:23,415 --> 00:21:25,745
space for normal data distributions.

354
00:21:26,745 --> 00:21:28,965
So continuous feedback and adaptions.

355
00:21:29,025 --> 00:21:31,575
So now our AI system isn't just a set.

356
00:21:31,980 --> 00:21:35,190
Set it and forget it kind
of thing to keep it sharp.

357
00:21:35,220 --> 00:21:35,940
We are built.

358
00:21:36,600 --> 00:21:40,080
We have built in a way for it to
consistently learn and improve

359
00:21:40,560 --> 00:21:42,180
detection, the system flag.

360
00:21:42,420 --> 00:21:46,950
Potentially the system flags
for potential weird feedback.

361
00:21:46,980 --> 00:21:48,240
You are smart humans.

362
00:21:48,285 --> 00:21:51,735
Tells it is a right or wrong adaption.

363
00:21:51,795 --> 00:21:56,175
The system uses the that feedback
to get back over time, adjust

364
00:21:56,175 --> 00:21:59,535
its rules, and getting smarter
at what that normal looks like.

365
00:22:00,015 --> 00:22:00,885
What the normal looks like.

366
00:22:00,915 --> 00:22:02,210
It's like a training, a puppy

367
00:22:03,210 --> 00:22:04,740
deployment, scaling strategies.

368
00:22:04,770 --> 00:22:09,540
When we rolled this out, we didn't
just throw it at everything at once.

369
00:22:10,155 --> 00:22:13,725
We looked at careful
approach, isolated testing.

370
00:22:14,415 --> 00:22:19,035
We started small, testing it
out on less critical pipelines.

371
00:22:19,035 --> 00:22:22,485
First, education, educate, identification.

372
00:22:23,145 --> 00:22:27,435
We use these initial tests to figure
out any weird edge cases, like

373
00:22:27,435 --> 00:22:31,575
sudden changes in data structures
or very new pipeline startups.

374
00:22:32,070 --> 00:22:33,320
Gradual expansion.

375
00:22:33,590 --> 00:22:38,175
Once we are confident, we started rolling
out to the bigger and complex systems.

376
00:22:38,990 --> 00:22:39,980
Full implementation.

377
00:22:40,520 --> 00:22:44,600
The goal is to have this smart
monitoring across the entire companies,

378
00:22:44,600 --> 00:22:47,480
keeping all our data reverse clean.

379
00:22:48,480 --> 00:22:52,764
Alright, so all that fancy AI stuff
and cloud engineering, it's actually

380
00:22:52,764 --> 00:22:54,475
delivered some pretty sweet results.

381
00:22:55,240 --> 00:22:57,190
So we are not just talking theory here.

382
00:22:57,340 --> 00:23:01,420
You have seen some real world wins, the
40% reduction in data quality issues.

383
00:23:01,480 --> 00:23:02,110
Think about it.

384
00:23:02,830 --> 00:23:07,060
That's like a cutting down the number
of oops movements with your, with our

385
00:23:07,060 --> 00:23:14,470
data by almost half fewer corrupted
files, fewer inconsistencies, fewer time.

386
00:23:14,590 --> 00:23:17,500
Someone says, wait, is
this data even right?

387
00:23:18,190 --> 00:23:22,030
It means our reports are cleaner,
our analysis are more reliable.

388
00:23:22,675 --> 00:23:27,235
And we spend less time chasing
down data governance, basically

389
00:23:27,235 --> 00:23:28,465
less headaches for everyone.

390
00:23:29,215 --> 00:23:34,315
So 97, 99 0.9, 99.7%,
reduction in reduction times.

391
00:23:35,395 --> 00:23:37,765
So this one mind, this
one is mind blowing.

392
00:23:37,975 --> 00:23:38,515
Mind blowing.

393
00:23:38,545 --> 00:23:41,395
Before, when someone went
wrong, something went wrong.

394
00:23:41,755 --> 00:23:46,430
It could take hours, sometimes even
days or manual digging through logs

395
00:23:46,525 --> 00:23:48,265
and trying to figure out what happened.

396
00:23:49,000 --> 00:23:54,670
Now our AI spots is where things
almost instantly, it's like going

397
00:23:54,670 --> 00:23:59,820
from snail mail to instantly may
messaging for finding problems.

398
00:24:00,450 --> 00:24:03,780
The speed means we can jump on
issues before they can cause

399
00:24:03,780 --> 00:24:09,690
bigger problems downstream, and
84, 80 4% fewer false postures.

400
00:24:09,690 --> 00:24:13,805
Nobody likes getting a bunch of
alerts that turns out to be nothing.

401
00:24:15,105 --> 00:24:18,195
It's like a car alarm that
keeps going off for no reason.

402
00:24:18,525 --> 00:24:19,545
You start to ignore it.

403
00:24:20,145 --> 00:24:22,635
Our system is much better at
telling the difference between

404
00:24:22,635 --> 00:24:27,165
a real time problem and a just a
normal, slightly unusual data point.

405
00:24:28,305 --> 00:24:32,475
That means our team can trust the
alerts they get an then focus on real

406
00:24:32,475 --> 00:24:34,545
fires instead of chasing shadows.

407
00:24:35,715 --> 00:24:40,185
Overall, this AI system has really made
our data environments way more stable.

408
00:24:40,890 --> 00:24:44,760
It's like we have got dedicated data
health monitor that always on the lookout

409
00:24:44,970 --> 00:24:49,380
catching the problems early and letting
our talented teams focus on building cool

410
00:24:49,380 --> 00:24:54,570
stuff and getting real insights from data
instead of just constantly firefighting.

411
00:24:54,570 --> 00:24:54,630
I.

412
00:24:55,630 --> 00:24:59,530
So to give you a taste of how this works
in the real world, let's talk about big

413
00:24:59,530 --> 00:25:01,300
financial institutions we've worked with.

414
00:25:01,330 --> 00:25:05,560
You can imagine in the world of real time
trading, even the tiniest data glitch

415
00:25:05,560 --> 00:25:08,440
can have massive financial consequences.

416
00:25:08,950 --> 00:25:11,260
Think millions of dollars
in the blink of ice.

417
00:25:11,950 --> 00:25:15,160
They needed a way to monitor
this increasingly fasting moving.

418
00:25:16,060 --> 00:25:18,400
Data streams and catching
more anomalies immediately.

419
00:25:19,390 --> 00:25:21,760
So we plugged our AI
driven system into their.

420
00:25:22,375 --> 00:25:25,015
Trading data pipelines, the
results are pretty dramatic.

421
00:25:25,105 --> 00:25:27,895
43.7 reduction in data issues.

422
00:25:28,645 --> 00:25:33,475
They say a huge drop in number of data
quality incidents that meant fewer errors

423
00:25:33,475 --> 00:25:38,575
in the data, trading data leading to more
reliable operations and fewer moments.

424
00:25:39,235 --> 00:25:43,430
So this freed up their highly skilled
expert from spending time on tedious

425
00:25:43,555 --> 00:25:47,365
data cleanups and let them focus
on what strategic high value task

426
00:25:48,265 --> 00:25:51,205
3.27 mil million annual savings.

427
00:25:52,015 --> 00:25:52,915
And here is a kicker.

428
00:25:53,335 --> 00:25:56,905
All that early detection and
fewer data problems translated

429
00:25:57,055 --> 00:25:58,765
into some serious cost saving.

430
00:25:59,365 --> 00:26:05,695
This came from a few different areas,
less time spent on manual monitoring

431
00:26:05,755 --> 00:26:11,335
and fixing errors, avoiding potentially
huge losses from bad trading decisions

432
00:26:11,335 --> 00:26:14,935
based on faulty data, and even
reducing the risk of complicated

433
00:26:15,025 --> 00:26:16,765
analysis due to inaccurate reporting.

434
00:26:17,605 --> 00:26:20,125
That's real money back in their pockets.

435
00:26:20,275 --> 00:26:23,605
All thanks to our smart data watchdogs.

436
00:26:24,605 --> 00:26:28,895
Now we are pretty proud of what we have
built, but we are not planning on just

437
00:26:28,895 --> 00:26:30,815
kicking back and watching the data flow.

438
00:26:31,265 --> 00:26:35,975
We are always looking for a way to make
our system even better and smarter.

439
00:26:36,950 --> 00:26:40,490
Here's a sneak peek of what we are
working on, concept drift management.

440
00:26:41,000 --> 00:26:44,330
Imagine the normal of our
data slowly changing our time,

441
00:26:44,960 --> 00:26:46,190
like the season changing.

442
00:26:46,700 --> 00:26:50,360
We are working on making our AI even
better, recognizing these long-term

443
00:26:50,360 --> 00:26:54,350
shifts so it doesn't start flagging
the new normal as an anomaly.

444
00:26:55,010 --> 00:26:59,270
It's like teaching it to understand the
changing weather patterns of our data.

445
00:27:01,055 --> 00:27:02,915
Enhancing computational efficiency.

446
00:27:02,945 --> 00:27:06,455
We are always looking for a
way to make our algorithms run

447
00:27:06,455 --> 00:27:08,435
faster and use fewer resources.

448
00:27:08,825 --> 00:27:13,055
Think of it as masking our AI brain
even more efficient so it can process

449
00:27:13,055 --> 00:27:14,765
more data without getting a headache.

450
00:27:15,575 --> 00:27:19,385
This might involve smart algorithms
or even using specialized computer

451
00:27:19,385 --> 00:27:21,965
hardware real time remediation.

452
00:27:22,325 --> 00:27:27,635
This is the holy grail, the feature
where you, our system doesn't just

453
00:27:27,635 --> 00:27:29,045
tell you that there's a problem.

454
00:27:29,435 --> 00:27:33,545
But actually fixes it automatically
without needing a human to step in.

455
00:27:33,545 --> 00:27:35,315
Imagine a self feeling data pipeline.

456
00:27:35,525 --> 00:27:36,815
That's the direction we are heading.

457
00:27:37,865 --> 00:27:41,885
Creating a closed loop system that
keeps your data flowing smoothly

458
00:27:41,945 --> 00:27:43,565
and minimal human intervention.

459
00:27:44,915 --> 00:27:47,675
So to wrap things up, are AI driven.

460
00:27:47,915 --> 00:27:51,845
Anomaly detection system is a
significant leap forward in keeping

461
00:27:52,295 --> 00:27:54,395
your cloud data healthy and reliable.

462
00:27:54,545 --> 00:27:55,295
Just smart.

463
00:27:55,445 --> 00:27:57,125
It's, it learns, it scales.

464
00:27:57,695 --> 00:27:59,050
It delivers critical results.

465
00:28:00,365 --> 00:28:02,345
It gives business a powerful tool.

466
00:28:02,975 --> 00:28:08,255
Not only catch problems early, but
also build more trustworthy data

467
00:28:08,255 --> 00:28:13,955
foundations improve how they operate
and ultimately make better decisions.

468
00:28:14,735 --> 00:28:17,375
We are excited about the future
of this technology and the

469
00:28:17,375 --> 00:28:22,265
potential to make managing complex
cloud data environments a whole.

470
00:28:22,700 --> 00:28:24,110
A lot less chaotic.

471
00:28:24,890 --> 00:28:25,820
Thanks for your time.

472
00:28:25,910 --> 00:28:28,100
I'm happy to answer any
questions you might have.

473
00:28:28,880 --> 00:28:29,150
Thank you.

