1
00:00:01,920 --> 00:00:02,700
Hello everyone.

2
00:00:02,910 --> 00:00:03,870
Thank you for joining.

3
00:00:04,050 --> 00:00:08,520
My name is Ari Koli and I'm excited
to talk take you on a journey through

4
00:00:08,520 --> 00:00:10,320
the next evolution of data management.

5
00:00:10,830 --> 00:00:15,300
In today's talk, AI-driven data
platforms, the transition from manual

6
00:00:15,300 --> 00:00:19,935
management to self-optimizing systems
will explore how artificial intelligence

7
00:00:20,190 --> 00:00:24,330
is transforming data platforms
from manual reactive setups into

8
00:00:24,330 --> 00:00:26,610
autonomous self-optimizing ecosystems.

9
00:00:27,180 --> 00:00:31,169
In other words, we are looking at how
to build a self-driving data platform.

10
00:00:31,680 --> 00:00:35,850
We will discuss why this change is
necessary, walk through the four key

11
00:00:35,850 --> 00:00:40,800
pillars that make the, make it possible,
see some real world use cases and metrics

12
00:00:40,800 --> 00:00:45,540
for each, and then envision the future
where data systems manage themselves.

13
00:00:46,080 --> 00:00:49,500
My goal is to keep this clear,
conversational and practical.

14
00:00:49,860 --> 00:00:54,360
By the end, you should see how these
AI driven approaches can free us from

15
00:00:54,360 --> 00:00:58,200
a lot of data grunt work, and let
us focus on getting value from data.

16
00:00:59,010 --> 00:00:59,820
Let's dive in

17
00:01:00,820 --> 00:01:02,589
the data explosion challenge.

18
00:01:02,890 --> 00:01:07,210
First, let's talk about why we need AI
driven automation in the first place.

19
00:01:07,390 --> 00:01:09,970
We are living in an era
of explosive data growth.

20
00:01:10,555 --> 00:01:14,905
Consider these numbers by 2025,
the world's data is projected

21
00:01:14,905 --> 00:01:20,185
to reach 1 75 zetabytes up
from only 33 terabytes in 2018.

22
00:01:20,785 --> 00:01:25,165
That's an annual growth rate of
about 61%, which is astounding.

23
00:01:25,615 --> 00:01:28,525
It's hard to even imagine 1 75 terabytes.

24
00:01:29,185 --> 00:01:34,135
Some put it in perspective by saying if
you stored art on Blu-ray disks, the stack

25
00:01:34,135 --> 00:01:35,935
would reach the moon multiple times over.

26
00:01:37,000 --> 00:01:42,250
What's more, a huge portion of this data
will need to be handled in real time.

27
00:01:42,850 --> 00:01:49,480
By 2025, 30% of all data generated
will require immediate processing.

28
00:01:49,810 --> 00:01:54,340
Think of sensors and iot streams where
you can't wait hours or days to react.

29
00:01:55,060 --> 00:01:59,140
In fact, IOT devices alone are
expected to generate about 90

30
00:01:59,140 --> 00:02:01,690
terabytes of data annually by 2025.

31
00:02:02,290 --> 00:02:05,800
So not only is data bigger, it's.

32
00:02:06,475 --> 00:02:08,604
Also faster and more complex.

33
00:02:09,025 --> 00:02:12,655
The takeaway is that traditional
manual data management can't keep

34
00:02:12,655 --> 00:02:14,905
up with this volume and velocity.

35
00:02:15,415 --> 00:02:18,745
We simply can't hire enough
people or write enough static

36
00:02:18,745 --> 00:02:21,954
strips to manage 1 75 terabytes.

37
00:02:22,345 --> 00:02:24,115
Much of it's streaming in real time.

38
00:02:24,715 --> 00:02:26,575
Automation is no longer a luxury.

39
00:02:26,845 --> 00:02:27,805
It's a necessity.

40
00:02:28,405 --> 00:02:32,485
We need smarter AI driven
systems to handle data growth,

41
00:02:32,995 --> 00:02:35,035
or we risk in data we can't use.

42
00:02:36,520 --> 00:02:40,630
This sets the stage for why we're
explore exploring AI driven data

43
00:02:40,630 --> 00:02:44,710
platforms to tackle the scale
and speed in a sustainable way.

44
00:02:45,710 --> 00:02:49,490
Now let's talk about the four
pillars of AI driven data platforms.

45
00:02:49,940 --> 00:02:52,220
So how do we actually transform our data

46
00:02:53,220 --> 00:02:53,700
first?

47
00:02:53,970 --> 00:02:56,280
The first pillar is metadata intelligence.

48
00:02:56,835 --> 00:03:00,615
This is about making data
about our data work for us.

49
00:03:00,915 --> 00:03:05,805
The platform uses AI to understand
and organize metadata, essentially

50
00:03:05,805 --> 00:03:10,515
knowing what data we have, what
it means, and how it's connected.

51
00:03:10,935 --> 00:03:14,265
Think of it like a brain and
nervous system that senses where

52
00:03:14,265 --> 00:03:17,745
everything is and helps the whole
platform respond intelligently.

53
00:03:19,305 --> 00:03:21,675
Second one is self-healing data pipelines.

54
00:03:22,500 --> 00:03:25,530
These are your data workflows
that can fix themselves.

55
00:03:25,740 --> 00:03:29,370
If something breaks or an anomaly
occurs, the system detects it

56
00:03:29,400 --> 00:03:31,200
and resolves it automatically.

57
00:03:31,620 --> 00:03:35,579
It's like an immune system for your
data operations, handling problems

58
00:03:35,579 --> 00:03:37,290
without waiting for human intervention.

59
00:03:38,430 --> 00:03:40,680
The third is predictive optimization.

60
00:03:41,655 --> 00:03:45,675
Here, the platform for casts,
what resources like compute,

61
00:03:45,675 --> 00:03:49,245
storage, et cetera, will be
needed and adjusts in advance.

62
00:03:49,755 --> 00:03:55,035
Instead of reacting after a slowdown or
outage, it proactively tunes performance.

63
00:03:55,575 --> 00:03:59,985
This is akin to a smart autopilot
that adjusts scores before turbulence

64
00:03:59,985 --> 00:04:02,475
hits, keeps things running smoothly.

65
00:04:03,180 --> 00:04:05,625
And the final one is
the embedded governance.

66
00:04:06,375 --> 00:04:07,454
The platforms.

67
00:04:07,545 --> 00:04:12,285
The platform has governance, security,
and compliance driven in from the start.

68
00:04:12,584 --> 00:04:17,505
AI helps enforce policies like privacy
rules or data quality standards

69
00:04:17,774 --> 00:04:19,575
continuously and transparently.

70
00:04:20,175 --> 00:04:25,035
Think of this as having a diligent
guardian or compass inside the

71
00:04:25,035 --> 00:04:28,485
system, ensuring everything stays
on the ethical and compliant

72
00:04:28,485 --> 00:04:30,345
part without manual checklists.

73
00:04:31,185 --> 00:04:34,395
Each of these pillars address a
different aspect of the data platform.

74
00:04:34,950 --> 00:04:35,640
Together.

75
00:04:35,880 --> 00:04:39,600
They create a system that can
adapt, optimize, and govern

76
00:04:39,600 --> 00:04:42,120
itself with minimum manual effort.

77
00:04:42,990 --> 00:04:47,310
Next, we'll dive into each pillar
one by one with examples and results

78
00:04:47,310 --> 00:04:49,075
to show how they work in practice

79
00:04:50,075 --> 00:04:51,515
metadata intelligence.

80
00:04:51,575 --> 00:04:52,415
Let's start with that.

81
00:04:53,075 --> 00:04:55,865
Often call the nervous system
of modern data platforms.

82
00:04:56,045 --> 00:04:58,385
Metadata is essentially data about data.

83
00:04:59,030 --> 00:05:02,930
Descriptions of what each data set
contains, where it came from, who

84
00:05:02,930 --> 00:05:05,270
owns it, quality stats, and so on.

85
00:05:05,750 --> 00:05:10,520
In traditional setups, metadata is
often passive documentation, but

86
00:05:10,520 --> 00:05:14,960
with AI driven metadata intelligence,
it becomes an active component

87
00:05:15,350 --> 00:05:18,965
that helps the whole platform
understand and organize information.

88
00:05:19,820 --> 00:05:24,170
In plain language, metadata intelligence
means the system can automatically

89
00:05:24,170 --> 00:05:27,200
catalog, tag and discover that data sets.

90
00:05:27,650 --> 00:05:31,820
It's like having a smart librarian
for your data, who not only catalogs

91
00:05:31,820 --> 00:05:36,620
everything, but also knows in context
how it's used and can instantly

92
00:05:36,620 --> 00:05:38,000
direct you to what you need.

93
00:05:38,540 --> 00:05:41,750
This dramatically improves
how we find and trust data.

94
00:05:42,290 --> 00:05:43,520
Here are some outcomes.

95
00:05:43,520 --> 00:05:47,360
Organizations have seen
62% faster discovery.

96
00:05:47,885 --> 00:05:53,045
Teams can find relevant data more than
twice As fast as before, imagine searching

97
00:05:53,045 --> 00:05:55,655
for a dataset with AI curated metadata.

98
00:05:55,925 --> 00:05:57,605
It's like a supercharged search engine.

99
00:05:58,085 --> 00:06:01,745
So what used to take hours of
digging is done in minutes.

100
00:06:02,165 --> 00:06:05,585
People spend less time hunting
for data and more time using it.

101
00:06:06,935 --> 00:06:09,575
72% fewer data quality issues.

102
00:06:09,965 --> 00:06:12,965
The platform catches
many data errors upfront.

103
00:06:13,265 --> 00:06:17,285
AI tags data with quality
metrics and flags anomalies.

104
00:06:17,615 --> 00:06:21,635
Leading to a dramatic reduction in
data incidents in practice, this

105
00:06:21,635 --> 00:06:26,225
means far less time spent cleaning
data or dealing with broken reports.

106
00:06:27,815 --> 00:06:30,425
43% higher trust in data.

107
00:06:31,085 --> 00:06:34,475
When data is well-documented
and consistently reliable,

108
00:06:35,105 --> 00:06:36,875
business users gain confidence.

109
00:06:36,965 --> 00:06:41,555
In fact, organizations reported a
43% increase in business stakeholder

110
00:06:41,555 --> 00:06:43,115
confidence in their data file.

111
00:06:43,625 --> 00:06:47,465
People trust the dashboards and insights
because they trust the underlying data.

112
00:06:47,945 --> 00:06:50,285
Thanks to that intelligent
metadata management.

113
00:06:51,095 --> 00:06:55,445
And lastly, 2.7 times faster
project delivery with quick

114
00:06:55,445 --> 00:06:57,185
discovery and fewer issues.

115
00:06:57,635 --> 00:07:00,395
Analytics and data
science projects finish.

116
00:07:00,425 --> 00:07:02,765
Projects finish almost three times faster.

117
00:07:03,155 --> 00:07:05,375
Teams are in bo down by data angling.

118
00:07:05,615 --> 00:07:08,435
The data is ready to go so
they can deliver insights to

119
00:07:08,435 --> 00:07:10,475
stakeholders much more rapidly.

120
00:07:11,240 --> 00:07:15,950
In short, metadata intelligence
makes the platform data aware

121
00:07:16,190 --> 00:07:17,990
and accelerates everything.

122
00:07:18,200 --> 00:07:21,380
It's the foundational layer
that ensures the right data is

123
00:07:21,380 --> 00:07:25,075
easily found, understood, and
trusted across the organization.

124
00:07:26,075 --> 00:07:30,665
Now to make this concrete,
let's look at its case study of

125
00:07:30,665 --> 00:07:32,405
metadata intelligence in action.

126
00:07:32,675 --> 00:07:36,545
Consider a large e-commerce
retailer that implemented AI driven

127
00:07:36,545 --> 00:07:38,705
metadata for their product database.

128
00:07:39,395 --> 00:07:42,335
All the descriptions, attributes,
customer reviews, the rich

129
00:07:42,335 --> 00:07:43,655
information of our products.

130
00:07:43,835 --> 00:07:45,215
The results were impressive.

131
00:07:46,145 --> 00:07:50,225
They saw 37% better search relevancy.

132
00:07:50,585 --> 00:07:55,415
Customers found that they were looking
for much more what customers found, what

133
00:07:55,415 --> 00:07:57,125
they were looking for much more easily.

134
00:07:57,485 --> 00:08:01,505
By automatically tagging products
with detailed attributes using AI

135
00:08:01,505 --> 00:08:05,975
to generate and refine metadata, the
site search engine could deliver more

136
00:08:05,975 --> 00:08:11,795
relevant results, which resulted in a
37% improvement in search accuracy and

137
00:08:11,795 --> 00:08:16,655
relevancy In practice, if you search for
waterproof jacket, the system understands

138
00:08:16,655 --> 00:08:20,900
context like synonyms related features,
and surface as the best matches.

139
00:08:22,820 --> 00:08:27,200
And they saw 24% higher conversion
rate because shoppers were

140
00:08:27,200 --> 00:08:29,120
finding the right products faster.

141
00:08:29,300 --> 00:08:31,340
More of them ended up making purchases.

142
00:08:31,640 --> 00:08:36,080
This retailer saw a 24% increase
in conversion rate, meaning a lot

143
00:08:36,080 --> 00:08:38,270
more browsers turned into buyers.

144
00:08:38,630 --> 00:08:42,169
And people can quickly
find what fits their needs.

145
00:08:42,230 --> 00:08:44,150
They're more likely to hit add to cart.

146
00:08:45,530 --> 00:08:48,740
Third, 19% fewer product returns.

147
00:08:48,980 --> 00:08:53,569
Rich metadata meant product pages
had better information, accurate

148
00:08:53,569 --> 00:08:59,000
specs, usage details, comparable
items, leading customers to buy items

149
00:08:59,000 --> 00:09:01,040
that truly met their expectations.

150
00:09:01,400 --> 00:09:04,970
The result was 19% drop in return rates.

151
00:09:05,360 --> 00:09:08,810
Fewer returns indicate customers
are happier with what they bought.

152
00:09:08,960 --> 00:09:11,060
They got what they
thought they were getting.

153
00:09:12,200 --> 00:09:14,900
Lastly, 29% larger auto value.

154
00:09:15,530 --> 00:09:18,170
Enhanced discovery didn't
just help find one item.

155
00:09:18,170 --> 00:09:18,710
It helped.

156
00:09:18,860 --> 00:09:22,250
It often helped customers find
additional or related items.

157
00:09:22,730 --> 00:09:26,300
The retailer saw the average
order size grow by 29%.

158
00:09:26,780 --> 00:09:31,130
This might be because AI driven
recommendations powered by metadata,

159
00:09:31,550 --> 00:09:35,780
exposed more relevant add-ons or
complementary products, and people

160
00:09:35,780 --> 00:09:37,695
felt confident adding them to the cart.

161
00:09:38,630 --> 00:09:40,760
This case study shows that.

162
00:09:41,074 --> 00:09:44,225
Metadata intelligence isn't
just an IT improvement.

163
00:09:44,345 --> 00:09:49,295
It directly boosts business outcomes
better search and data understanding

164
00:09:49,355 --> 00:09:51,515
led to more sales and happier customers.

165
00:09:51,845 --> 00:09:56,735
It's a great example of how making data
more intelligent pays off intangible ways.

166
00:09:57,735 --> 00:10:00,555
Self hailing data pipelines
Now onto the second pillar.

167
00:10:01,095 --> 00:10:04,995
Data pipelines are the processes
that move and transform data.

168
00:10:05,295 --> 00:10:09,675
For example, an ETL job that pulls
sales records from a database and

169
00:10:09,675 --> 00:10:13,605
loads them into a data warehouse
nightly or a streaming pipeline that

170
00:10:13,605 --> 00:10:15,345
collects sensor data in real time.

171
00:10:16,064 --> 00:10:17,775
Traditionally, pipelines are brittle.

172
00:10:18,584 --> 00:10:23,865
If there is a slight change in input or an
error, same as in file or schema change or

173
00:10:23,865 --> 00:10:26,084
a server outage, the pipeline might break.

174
00:10:26,655 --> 00:10:30,165
Then data engineers get paged
at 2:00 AM to fix it manually.

175
00:10:30,165 --> 00:10:31,845
Troubleshooting and patching the issue.

176
00:10:32,835 --> 00:10:36,945
With self-healing pipelines, we
add AI and automation to make these

177
00:10:36,945 --> 00:10:39,705
workflows resilient and autonomous.

178
00:10:40,515 --> 00:10:45,795
The platforms will detect issues and fix
many of them automatically without needing

179
00:10:45,795 --> 00:10:48,285
a human to intervene for common problems.

180
00:10:48,645 --> 00:10:53,000
Here is how it works in plain
terms, detect anomalies.

181
00:10:53,865 --> 00:10:58,245
The system continuously monitors
pipeline behavior and data quality.

182
00:10:58,665 --> 00:11:00,975
It knows what normal looks like.

183
00:11:01,440 --> 00:11:05,790
If something odd happens, example,
a data patch is 50% smaller than

184
00:11:05,790 --> 00:11:10,170
usual, or an unexpected file doesn't
arrive, it flags it immediately.

185
00:11:10,920 --> 00:11:13,530
Early detection means we can
address issues before they

186
00:11:13,530 --> 00:11:14,970
cascade into bigger failures.

187
00:11:16,800 --> 00:11:18,360
Second, adapt to changes.

188
00:11:18,690 --> 00:11:22,830
Suppose an upstream system adds a new
column or changes the data format.

189
00:11:23,190 --> 00:11:27,960
Normally that might break the pipeline
because the code wasn't expecting it.

190
00:11:27,960 --> 00:11:29,370
A self-healing pipeline.

191
00:11:29,880 --> 00:11:36,660
Can automatically adjust to certain
changes, for instance, by using metadata

192
00:11:36,660 --> 00:11:41,250
to map the new schema or by ignoring extra
fields until it learns how to handle them.

193
00:11:41,730 --> 00:11:44,700
It's as if the pipeline can
evolve on the fly to stay

194
00:11:44,700 --> 00:11:48,300
compatible, remediate problems.

195
00:11:48,750 --> 00:11:53,580
For issues that have known issue success,
the system can apply them instantly.

196
00:11:53,790 --> 00:11:58,050
For example, if a pipeline fails
due to a timeout, the platform might

197
00:11:58,050 --> 00:12:02,699
automatically retry it on a backup
server, or if data arrives corrupted,

198
00:12:02,819 --> 00:12:04,680
it could switch to a secondary source.

199
00:12:05,130 --> 00:12:09,480
These automated corrections handle
mini routine failure patterns

200
00:12:09,689 --> 00:12:10,995
without human involvement.

201
00:12:12,480 --> 00:12:13,530
Learn and improve.

202
00:12:14,130 --> 00:12:18,360
Every time the system encounters a
new issue, it learns from it over

203
00:12:18,360 --> 00:12:22,740
time, it builds up knowledge of how
to handle various failure modes.

204
00:12:23,100 --> 00:12:26,940
This means the more it runs,
the smarter and more efficient

205
00:12:26,940 --> 00:12:28,170
it becomes self-healing.

206
00:12:28,650 --> 00:12:32,460
It's similar to how our immune
system learns to fight new pathogens.

207
00:12:32,850 --> 00:12:35,250
Each incident makes it
stronger for next time.

208
00:12:36,569 --> 00:12:39,959
In practice, self-healing
pipelines drastically reduce

209
00:12:39,959 --> 00:12:41,400
downtime and firefighting.

210
00:12:41,910 --> 00:12:45,360
Imagine far fewer nightly
alerts and emergency fixes.

211
00:12:45,660 --> 00:12:46,860
The data keeps flowing.

212
00:12:46,890 --> 00:12:51,569
E, even when hiccups occur, because
the platform resolves them, or at

213
00:12:51,569 --> 00:12:56,010
least contains them until a permanent
fixes applied, it turns a fragile

214
00:12:56,250 --> 00:13:00,360
data pipeline into a robust immune
system that keeps your data moving.

215
00:13:01,360 --> 00:13:05,020
So what is the impact of self
hailing pipelines on operations?

216
00:13:05,440 --> 00:13:10,030
In short, significantly higher
reliability and lower maintenance effort.

217
00:13:10,660 --> 00:13:14,800
When your data pipelines can fix
themselves or at least handle issues

218
00:13:14,800 --> 00:13:19,480
gracefully, you can see benefits like
increased uptime and reliability,

219
00:13:20,170 --> 00:13:24,130
fewer pipeline failures mean data
is available when people need it.

220
00:13:24,580 --> 00:13:27,400
If your dashboards are
updated by 6:00 AM daily, they

221
00:13:27,400 --> 00:13:29,170
continue to be updated on time.

222
00:13:29,500 --> 00:13:33,730
Because the system has handled any
overnight glitches automatically,

223
00:13:34,330 --> 00:13:38,650
this consistency builds trust
with users who rely on timely

224
00:13:38,650 --> 00:13:41,680
data, lower maintenance burden.

225
00:13:43,030 --> 00:13:47,170
Your engineers and DA data ops
team spend a lot time, a lot

226
00:13:47,170 --> 00:13:48,940
less time reacting to problems.

227
00:13:49,330 --> 00:13:53,200
One study by McKensey found that
advanced self failing systems can

228
00:13:53,200 --> 00:13:55,420
cut operational costs by up to 30%.

229
00:13:56,155 --> 00:13:59,395
Through reduced downtime and
more efficient fall management.

230
00:13:59,845 --> 00:14:00,625
Think about that.

231
00:14:00,745 --> 00:14:04,945
Nearly a third of the effort and
cost that used to go into manual

232
00:14:04,945 --> 00:14:06,415
troubleshooting can be saved.

233
00:14:07,015 --> 00:14:11,125
Your team can redirect that time to
more strategic work, like improving the

234
00:14:11,125 --> 00:14:17,030
platform or building new data features
faster recovery when issues occur.

235
00:14:17,995 --> 00:14:22,315
Even when the problem happens, the
systems can't fix fully by itself.

236
00:14:22,435 --> 00:14:27,925
The automated detection and
preliminary action means the eventual

237
00:14:27,925 --> 00:14:29,605
human intervention is quicker.

238
00:14:31,255 --> 00:14:36,145
The system might already have isolated
the issue or tried common fixes, so the

239
00:14:36,145 --> 00:14:38,395
meantime to recovery drops dramatically.

240
00:14:38,485 --> 00:14:41,635
You're looking at minutes instead
of hours for many incidents.

241
00:14:43,015 --> 00:14:45,235
Greater scalability of operations.

242
00:14:46,360 --> 00:14:51,520
As your data volume and complexity
grow, a self-healing system scales

243
00:14:51,520 --> 00:14:53,110
much better than a manual one.

244
00:14:53,440 --> 00:14:56,080
You don't need to grow your
ops team linearly with data.

245
00:14:56,470 --> 00:14:59,230
The AI copes with the growth
by learning and automating.

246
00:14:59,560 --> 00:15:05,320
This makes the platform more scalable
from an operation standpoint in an, in a

247
00:15:05,320 --> 00:15:10,090
sense, we move from a situation where data
engineering teams are constantly putting

248
00:15:10,090 --> 00:15:15,310
out fires to one where the platform
autonomously handles routine issues.

249
00:15:16,315 --> 00:15:21,175
That shift not only saves time and
money, but it also gives developers

250
00:15:21,205 --> 00:15:25,285
and analysts confidence that the data
pipeline won't be the bottleneck or a

251
00:15:25,285 --> 00:15:27,625
single point of failure in their projects.

252
00:15:28,625 --> 00:15:31,925
Our third pillar is predictive
resource optimization.

253
00:15:32,675 --> 00:15:36,575
This is about managing the computational
resources of the platform, things

254
00:15:36,575 --> 00:15:41,555
like CPU Memory Storage Network in an
intelligent, proactive way using ai.

255
00:15:42,860 --> 00:15:47,600
Traditional systems often either
overprovision resources just in

256
00:15:47,600 --> 00:15:51,890
case, or you reactively scale up
after you notice performance lag or

257
00:15:51,890 --> 00:15:55,400
high load, which can be too late.

258
00:15:55,790 --> 00:15:56,840
Neither is ideal.

259
00:15:57,500 --> 00:16:02,240
Predictive optimization uses machine
learning to anticipate demand and

260
00:16:02,240 --> 00:16:04,190
edges resources ahead of time.

261
00:16:04,580 --> 00:16:06,590
Here is the breakdown of how it works.

262
00:16:07,730 --> 00:16:08,510
Learn patterns.

263
00:16:09,875 --> 00:16:12,605
The platform looks at
historical usage data.

264
00:16:12,755 --> 00:16:16,385
For example, it learns that every
weekday at 9:00 AM there's a spike

265
00:16:16,385 --> 00:16:20,795
in dashboard quiz, or that end of
month processing uses a lot of CPU.

266
00:16:21,425 --> 00:16:26,465
By analyzing these patterns over
weeks and months, ML models learn

267
00:16:26,465 --> 00:16:28,085
the typical workload cycles.

268
00:16:28,505 --> 00:16:34,445
It's similar to how a smart assistant
might learn your routine forecast demand.

269
00:16:35,195 --> 00:16:38,255
With these learn patterns,
the AI can predict future.

270
00:16:38,750 --> 00:16:42,560
For instance, it might forecast
that tomorrow's web traffic will

271
00:16:42,560 --> 00:16:47,030
be 20% higher due to a marketing
event, or that in December you'll

272
00:16:47,030 --> 00:16:49,100
need extra storage for air end data.

273
00:16:49,610 --> 00:16:53,480
It anticipates both regular
cycles and unusual events.

274
00:16:53,840 --> 00:16:59,810
If it is fed the right signals,
optimize resources based on the

275
00:16:59,810 --> 00:17:05,090
forecast, the platform proactively
allocates or adjusts resources.

276
00:17:05,750 --> 00:17:09,350
This could mean spinning up
additional server instances at 8:50

277
00:17:09,350 --> 00:17:11,480
AM just before the users login.

278
00:17:11,870 --> 00:17:16,250
So performance stays smooth or
automatically scaling down some

279
00:17:16,250 --> 00:17:19,640
services during weekends to
save cost when demand is low.

280
00:17:20,390 --> 00:17:24,860
Essentially, it tunes capacity in
advance as instead of waiting to

281
00:17:24,860 --> 00:17:28,460
react, it's like packing an umbrella
because the weather forecast said

282
00:17:28,460 --> 00:17:29,780
there is a high chance of rain.

283
00:17:30,230 --> 00:17:31,820
You were prepared ahead of time.

284
00:17:32,810 --> 00:17:33,830
Measure results.

285
00:17:34,970 --> 00:17:38,240
After acting the system watches
what actually happens and

286
00:17:38,240 --> 00:17:39,950
compares it to the prediction.

287
00:17:40,550 --> 00:17:43,790
Maybe it's predicted a
20% spike, but it was 25%.

288
00:17:44,210 --> 00:17:47,630
It then learns from the discrepancy
and fine tunes its model.

289
00:17:48,020 --> 00:17:48,980
This is a feedback loop.

290
00:17:49,520 --> 00:17:52,610
Over time, the predictions
get more accurate and the

291
00:17:52,610 --> 00:17:54,410
optimization strategies improve.

292
00:17:54,770 --> 00:17:59,510
The system continuously learns and
adapts to changing usage patterns.

293
00:18:00,455 --> 00:18:04,055
In non-technical terms, predictive
resource optimization makes your

294
00:18:04,055 --> 00:18:08,435
platform run like a well oil machine,
always sufficiently power for

295
00:18:08,435 --> 00:18:10,175
what's coming, but not excessive.

296
00:18:10,775 --> 00:18:14,615
It's the difference between always
driving your car with the pedal to the

297
00:18:14,615 --> 00:18:20,225
metal just in case, versus using cruise
control and predictive GPS that knows

298
00:18:20,225 --> 00:18:24,695
a hill is coming and accelerates a bit
beforehand than eases off and coasting.

299
00:18:25,505 --> 00:18:28,985
The result is smoother performance
and far better efficiency.

300
00:18:29,985 --> 00:18:34,035
What benefits can we actually get from
this kind of predictive optimization?

301
00:18:34,305 --> 00:18:37,935
Let's look at some real results achieved
with AI driven resource management.

302
00:18:38,895 --> 00:18:44,475
Utilization improvement systems
have seen around 43.8% improvement

303
00:18:44,475 --> 00:18:45,795
in resource utilization.

304
00:18:46,425 --> 00:18:51,285
In one comparison, using ML based
workload forecasting, outperform

305
00:18:51,285 --> 00:18:52,600
traditional threshold based.

306
00:18:53,010 --> 00:18:58,110
Auto scaling by that margin Concretely,
this means the hardware and cloud

307
00:18:58,110 --> 00:19:01,950
instances you're paying for are
being used much more efficiently.

308
00:19:02,820 --> 00:19:07,650
Before maybe servers sat idle 60%
of the time as a safety buffer.

309
00:19:08,100 --> 00:19:12,030
After implementing AI predictions
over provisioning dropped from

310
00:19:12,030 --> 00:19:18,485
about 61% to just 12.3% while
still maintaining performance.

311
00:19:19,395 --> 00:19:23,115
The platform isn't holding a bunch of
extra capacity that never gets used.

312
00:19:23,595 --> 00:19:28,905
It's closer to a just in time model For
compute, this efficiency is a direct

313
00:19:28,905 --> 00:19:36,585
cost saver, cost reduction, because of
better utilization, companies saved a

314
00:19:36,585 --> 00:19:43,635
lot of money on our, on average midsize
deployments, saved about 476 K per

315
00:19:43,635 --> 00:19:46,935
year by using AI to optimize resources.

316
00:19:47,519 --> 00:19:52,289
In fact, the investment in the, in
these optimization algorithms paid

317
00:19:52,289 --> 00:19:54,479
back three 80% in the first year.

318
00:19:57,389 --> 00:20:00,239
A three 80% return on investment is huge.

319
00:20:00,419 --> 00:20:06,090
Essentially, for every dollar put into
implementing the solution, they got back

320
00:20:06,149 --> 00:20:09,090
three 80 back in savings within that year.

321
00:20:09,540 --> 00:20:13,439
This kind of ROI indicates that
the optimization not only covers

322
00:20:13,439 --> 00:20:17,340
its own costs, but generates
significant net savings very quickly.

323
00:20:18,720 --> 00:20:23,909
Performance enhancement, it's
not about cost, performance,

324
00:20:23,909 --> 00:20:25,290
reliability improved too.

325
00:20:27,840 --> 00:20:30,895
SLE violations drop by 76.2%.

326
00:20:31,620 --> 00:20:35,820
That means far fewer incidents where
the system's performance fell below.

327
00:20:35,820 --> 00:20:36,659
Agreed targets.

328
00:20:37,050 --> 00:20:39,210
Like slow response time or downtime.

329
00:20:39,600 --> 00:20:42,300
By predicting and preventing
resource bottlenecks, the

330
00:20:42,300 --> 00:20:45,930
system can meet its performance
targets much more consistently.

331
00:20:46,440 --> 00:20:50,910
Over three quarters of the issues that
used to break as is were eliminated

332
00:20:52,500 --> 00:20:56,520
handling 3.2 times workload variability.

333
00:20:56,850 --> 00:21:00,450
The platform became much more
robust against traffic spikes

334
00:21:00,450 --> 00:21:02,520
and variability in tests.

335
00:21:02,580 --> 00:21:05,760
It could handle 3.2 times
more variations in workload.

336
00:21:06,314 --> 00:21:09,104
Without performance
degradation compared to before.

337
00:21:09,554 --> 00:21:14,475
So if your user load suddenly triples,
think flash sale or a wire event,

338
00:21:14,804 --> 00:21:16,814
the system can absorb it gracefully.

339
00:21:17,445 --> 00:21:19,185
This is a big deal for scalability.

340
00:21:19,695 --> 00:21:24,824
It means fewer emergencies when load
searches unexpectedly because the

341
00:21:24,824 --> 00:21:29,894
system likely already scaled itself in
anticipation and can react fast enough

342
00:21:29,985 --> 00:21:31,574
thanks to the predictive head start.

343
00:21:32,655 --> 00:21:34,695
Overall, these results show a win-win.

344
00:21:35,385 --> 00:21:37,695
Greater efficiency and better performance.

345
00:21:38,235 --> 00:21:38,685
Often.

346
00:21:38,685 --> 00:21:41,415
In the past we had a trade
off cost versus speed.

347
00:21:41,865 --> 00:21:46,754
You'd overprovision to ensure speed
or save cost, but risk slowdowns AI

348
00:21:46,754 --> 00:21:51,764
lets you get both lower cloud builds
and a faster, more reliable system.

349
00:21:52,455 --> 00:21:55,395
It's a great example of
optimization done right

350
00:21:56,395 --> 00:21:59,995
now let's talk about the fourth pillar
embedded governance and ethical ai.

351
00:22:01,060 --> 00:22:06,610
This is all about trust, security, and
compliance built into the data platform.

352
00:22:07,300 --> 00:22:10,840
Traditionally, data governance,
ensuring the right people

353
00:22:10,840 --> 00:22:12,250
have access to the right data.

354
00:22:12,490 --> 00:22:16,570
Data is issues used ethically,
privacy is protected, et cetera, has

355
00:22:16,570 --> 00:22:18,640
been a very manual set of processes.

356
00:22:19,090 --> 00:22:22,120
It often feels like a gatekeeper
that slows things down.

357
00:22:22,450 --> 00:22:28,060
Example, waiting weeks for approval to
access a data set or big audits to ensure

358
00:22:28,060 --> 00:22:30,370
compliance with regulations like GDPR.

359
00:22:31,690 --> 00:22:36,040
In a AI driven platform, we turn
governance from a hurdle into an

360
00:22:36,040 --> 00:22:38,110
integrated automated safeguard.

361
00:22:38,680 --> 00:22:45,130
The platform itself helps enforce policies
and ethical guidelines continuously so

362
00:22:45,130 --> 00:22:47,500
that it's not all manual checkpoints.

363
00:22:48,070 --> 00:22:49,000
Let's break it down.

364
00:22:49,780 --> 00:22:50,620
What the entails.

365
00:22:51,550 --> 00:22:52,570
Automatic detection.

366
00:22:52,570 --> 00:22:57,490
The platform automatically identifies
and classifies sensitive data.

367
00:22:58,120 --> 00:23:02,650
For example, it can scan incoming
data and recognize this column looks

368
00:23:02,650 --> 00:23:04,510
like social security numbers are.

369
00:23:04,630 --> 00:23:08,830
This dataset contains personal
health information using ai.

370
00:23:08,890 --> 00:23:13,600
It can even detect things that aren't
just based on simple rules, say it

371
00:23:13,600 --> 00:23:17,620
learns to flag a combination of data
fields that together are sensitive.

372
00:23:18,250 --> 00:23:22,150
By knowing where all the sensitive
or regulated data is, we have the

373
00:23:22,150 --> 00:23:24,040
foundation to handle it properly.

374
00:23:25,150 --> 00:23:26,260
Policy enforcement.

375
00:23:26,965 --> 00:23:30,505
Once data is classified, the
system can apply the appropriate

376
00:23:30,505 --> 00:23:32,395
protections automatically.

377
00:23:32,995 --> 00:23:36,475
If something is tagged as confidential,
the platform might automatically

378
00:23:36,475 --> 00:23:40,435
encrypt it, mask it, and analytics
or restrict who can query it.

379
00:23:41,065 --> 00:23:45,475
The idea is that rules like financial
data X can only be seen by those

380
00:23:45,475 --> 00:23:49,735
roles or mask out customer names
when data is used for analysis, are

381
00:23:49,735 --> 00:23:54,175
enforced by the system at runtime,
not just written in a policy document.

382
00:23:54,790 --> 00:23:58,660
This reduces human error and
makes compliance consistent.

383
00:24:00,100 --> 00:24:03,880
Continuous monitoring governance
isn't a one-time thing.

384
00:24:04,300 --> 00:24:08,530
The platform continuously monitors
data usage for compliance risks.

385
00:24:09,070 --> 00:24:11,410
It watches for unusual access patterns.

386
00:24:11,800 --> 00:24:14,830
For instance, if someone is s querying
an abnormal amount of sensitive

387
00:24:14,865 --> 00:24:19,990
data at 2:00 AM it might flag or
block that as potential breach.

388
00:24:20,500 --> 00:24:23,680
It also ensures that as new data comes in.

389
00:24:24,175 --> 00:24:28,225
As regulations change, it keeps
everything in check in real time.

390
00:24:28,885 --> 00:24:32,215
This is like having a security
guard on duty 24 by seven

391
00:24:32,245 --> 00:24:33,685
inside your data platform.

392
00:24:33,955 --> 00:24:37,705
Always alert, transparent decisions.

393
00:24:38,935 --> 00:24:41,665
A critical part of ethical
AI is transparency.

394
00:24:41,995 --> 00:24:45,625
The system provides explanations
for automated actions.

395
00:24:46,375 --> 00:24:49,915
If it blocked the user's access
or encrypted a data set, it'll

396
00:24:50,065 --> 00:24:52,105
log or even inform you why.

397
00:24:52,495 --> 00:24:56,185
This day-to-day set contains
HIPAA protected data, or so

398
00:24:56,185 --> 00:24:57,745
direct access was restricted.

399
00:24:58,405 --> 00:25:04,045
The transparency is key to trust both
internally, so data teams understand

400
00:25:04,045 --> 00:25:06,505
system behavior and externally.

401
00:25:06,715 --> 00:25:12,685
Imagine how regulators or customers
clear audit trail of how data is handled.

402
00:25:13,915 --> 00:25:16,225
We don't want a black box governance, ai.

403
00:25:16,495 --> 00:25:19,495
We want a glass box that
shows its reasoning.

404
00:25:20,485 --> 00:25:23,965
Ultimately embedded governance
means compliance and ethics

405
00:25:23,965 --> 00:25:25,135
are not an afterthought.

406
00:25:25,795 --> 00:25:29,005
They're bake into the daily
operations of the data platform.

407
00:25:29,515 --> 00:25:33,865
It ensures that as we
automate more, we are doing so

408
00:25:33,865 --> 00:25:36,475
responsibly for the organization.

409
00:25:36,475 --> 00:25:41,395
This means safer data, fewer compliance
nightmares, and a strong ethical stance

410
00:25:41,725 --> 00:25:46,765
by default, rather than depending
solely on periodic manual checks.

411
00:25:47,765 --> 00:25:51,305
It turns out doing governance
right with AI and automation

412
00:25:51,305 --> 00:25:52,895
doesn't just avoid problems.

413
00:25:53,135 --> 00:25:56,765
It can actually speed up your
organization and build trust

414
00:25:57,125 --> 00:25:59,195
becoming a competitive advantage.

415
00:26:00,005 --> 00:26:03,605
Let's look at some metrics that
companies have reported after

416
00:26:03,785 --> 00:26:08,375
embedding governance into their data
platforms, operational efficiency.

417
00:26:09,350 --> 00:26:14,030
Automated governance dramatically cuts
down the bus, busy work and compliance.

418
00:26:14,330 --> 00:26:20,690
For example, companies saw a 67% reduction
in audit prep time, preparing for audits,

419
00:26:20,690 --> 00:26:25,640
used to take months of gathering records,
permissions, data, lineage information.

420
00:26:26,360 --> 00:26:31,580
Now, a lot of that is available on demand
via the platform's, logs and catalogs.

421
00:26:32,360 --> 00:26:36,140
They also saw 43% improvement
in audit success rates.

422
00:26:36,605 --> 00:26:40,355
Meaning far fewer findings or
issues since the system was

423
00:26:40,355 --> 00:26:41,765
already keeping things in line.

424
00:26:42,425 --> 00:26:43,595
Another impressive figure.

425
00:26:44,195 --> 00:26:48,455
94.8% sensitive data
classification accuracy.

426
00:26:50,615 --> 00:26:55,985
The AI is identifying sensitive data,
almost 95% correctly, which is better

427
00:26:55,985 --> 00:26:57,695
than most humans could do at scale.

428
00:26:58,265 --> 00:27:03,100
High accuracy here means you really mis
tagging something that needs protection.

429
00:27:03,635 --> 00:27:06,875
Giving you confidence that nothing
is slipping through the cracks.

430
00:27:08,165 --> 00:27:09,755
Agility and compliance

431
00:27:12,005 --> 00:27:16,325
embed, embedding governance actually
makes the organization more agile

432
00:27:16,445 --> 00:27:21,065
in how it can use data because
the guardrails are automated.

433
00:27:21,875 --> 00:27:26,345
One metric data access requests
that used to take nearly nine

434
00:27:26,345 --> 00:27:27,815
and a half days on average.

435
00:27:28,355 --> 00:27:33,845
All the approval sticker checks now
can be fulfilled about 12 minutes, in

436
00:27:33,845 --> 00:27:38,525
about 12 minutes, basically in near real
time, once the proper criteria are met.

437
00:27:39,095 --> 00:27:43,865
That's a game changer for analytics
or scientists who need data now

438
00:27:43,955 --> 00:27:45,335
rather than two weeks later.

439
00:27:45,965 --> 00:27:50,975
Another metric, adapting to new
regulations went from 97 days to 26

440
00:27:50,975 --> 00:27:54,335
days when a new law or policy comes out.

441
00:27:54,665 --> 00:27:58,655
Because much of the governance logic
is in software, you can update rules

442
00:27:58,655 --> 00:28:02,735
in the platform and achieve compliance
in a few weeks instead of a quarter.

443
00:28:03,575 --> 00:28:08,795
And importantly, unauthorized
access incidents went down 82%.

444
00:28:09,665 --> 00:28:14,525
The continuous monitoring and automated
controls shut down a lot of opportunities

445
00:28:14,525 --> 00:28:16,235
for mistakes or malicious access.

446
00:28:16,805 --> 00:28:19,685
So there are far fewer security incidents.

447
00:28:20,555 --> 00:28:24,035
That's a huge risk reduction market trust.

448
00:28:24,695 --> 00:28:28,055
Strong governance boosts
your external reputation.

449
00:28:28,685 --> 00:28:31,625
Companies with this autonomous
governance measures got

450
00:28:31,625 --> 00:28:34,775
regulatory approvals 28% faster.

451
00:28:35,345 --> 00:28:37,985
For things like new product
launches or certifications,

452
00:28:38,615 --> 00:28:42,365
regulators trust them because they
can see robust controls in place.

453
00:28:42,545 --> 00:28:47,855
So the approval process, speed up customer
confidence in how the data is handled,

454
00:28:47,975 --> 00:28:51,785
jumped from about 51 to 74% in surveys.

455
00:28:51,935 --> 00:28:52,745
That's a big leap.

456
00:28:53,420 --> 00:28:56,810
When customers know you take
their data seriously and have the

457
00:28:56,810 --> 00:29:00,380
systems to protect it, they're more
comfortable doing business with you.

458
00:29:00,980 --> 00:29:05,270
And notably, legal challenges and
disputes related to data dropped

459
00:29:05,270 --> 00:29:11,570
by 76% fewer lawsuits, fines, or
public relation issues occur when

460
00:29:11,570 --> 00:29:13,460
you are on top of privacy and ethics.

461
00:29:13,940 --> 00:29:17,870
In summary, good data governance
isn't just about avoiding penalties,

462
00:29:18,200 --> 00:29:21,560
it's about it builds trust
with customers and regulators.

463
00:29:22,055 --> 00:29:25,745
Which is invaluable for business
continuity and brand reputation.

464
00:29:26,915 --> 00:29:32,375
So this pillar turns what used to be seen
as a check boss or drag on productivity

465
00:29:32,375 --> 00:29:34,325
into a source of speed and trust.

466
00:29:34,955 --> 00:29:40,175
It empowers the organization to use data
more freely and to confidently show the

467
00:29:40,175 --> 00:29:42,665
world that it can be trusted with data.

468
00:29:43,355 --> 00:29:47,045
It's absolutely a competitive advantage
in today's data-driven market.

469
00:29:48,045 --> 00:29:52,485
We have talked through the four pillars,
metadata intelligence, self-healing,

470
00:29:52,485 --> 00:29:57,165
pipelines, predictive optimization, and
embedded governance, and seen how each

471
00:29:57,165 --> 00:29:59,085
improves a part of the data platform.

472
00:29:59,745 --> 00:30:03,645
Now let's zoom out and conclude by
painting the picture of the future, the

473
00:30:03,645 --> 00:30:07,165
truly autonomous data platforms the.

474
00:30:07,884 --> 00:30:13,044
The theme across all these pillars
is moving from manual reactive

475
00:30:13,044 --> 00:30:17,334
ways of working to automated,
proactive, and intelligent systems.

476
00:30:18,054 --> 00:30:23,215
On this slide, we sum it up as
a series of transformations from

477
00:30:23,245 --> 00:30:25,705
documentation to active intelligence.

478
00:30:27,114 --> 00:30:30,715
Metadata is no longer just passive
documentation sitting in a catalog.

479
00:30:31,104 --> 00:30:33,715
It becomes an active
living part of the system.

480
00:30:34,330 --> 00:30:38,200
In the future when new data comes in,
the platform doesn't just note it down.

481
00:30:38,590 --> 00:30:42,490
It actively understands it,
organizes it, and even uses that

482
00:30:42,490 --> 00:30:44,470
knowledge to automate processes.

483
00:30:45,670 --> 00:30:50,080
It's as if your documentation comes alive
and start doing some of the work for you.

484
00:30:51,610 --> 00:30:55,720
From brittle to self hailing
pipelines, data pipelines today can be

485
00:30:55,720 --> 00:30:57,550
fragile, breaking with small changes.

486
00:30:57,910 --> 00:31:02,560
We are moving to a world where pipelines
are self-healing and resilient by design.

487
00:31:03,100 --> 00:31:06,970
This means a minor schema change or
a spike in data volume won't bring

488
00:31:06,970 --> 00:31:11,139
things crashing down The pipeline
adapts or recovers on its own.

489
00:31:11,740 --> 00:31:15,580
The future pipeline is like a
robust organism that can heal wounds

490
00:31:15,580 --> 00:31:17,740
quickly rather than a house of guard.

491
00:31:18,220 --> 00:31:23,320
This drastically reduces failures
and maintenance effort from

492
00:31:23,470 --> 00:31:28,389
reactive to predictive resource
management instead of the ops team

493
00:31:28,419 --> 00:31:30,669
reacting to outages or slowdowns.

494
00:31:31,000 --> 00:31:36,100
Chasing after issues, the platform will
predict and prevent many of those issues.

495
00:31:36,940 --> 00:31:40,720
Resource management and tuning
will largely be on autopilot.

496
00:31:41,440 --> 00:31:45,400
The systems will know, for example,
that end of quarter is coming, let's

497
00:31:45,400 --> 00:31:49,330
allocate more capacity now, or,
this query pattern looks like it

498
00:31:49,330 --> 00:31:50,860
could cause a bottleneck tomorrow.

499
00:31:50,950 --> 00:31:52,665
Let's opt optimize it now.

500
00:31:53,514 --> 00:31:57,115
We move from always being on the
back foot to being a step ahead.

501
00:31:57,655 --> 00:32:02,695
The result is smoother performance and
happy users who don't even realize how

502
00:32:02,695 --> 00:32:08,544
much foresight is happening behind the
scenes from control to empowerment.

503
00:32:09,235 --> 00:32:14,905
Governance shifts from a strict control
mindset to an empowerment mindset.

504
00:32:16,854 --> 00:32:20,995
Yes, you can use this data because
the controls to do it safely are

505
00:32:20,995 --> 00:32:22,675
already in place and automated.

506
00:32:23,635 --> 00:32:27,070
In the future, governance is
an enabler, not a roadblock.

507
00:32:27,430 --> 00:32:33,190
It gives people confidence to explore and
use data freely within safe boundaries.

508
00:32:33,760 --> 00:32:36,880
This means data innovation
can flourish because the

509
00:32:36,880 --> 00:32:39,100
platform is both open and safe.

510
00:32:39,700 --> 00:32:44,410
People get to work with data faster,
and the system widely ensures it all.

511
00:32:44,440 --> 00:32:48,550
It's all compliant and ethical
when you put it all together.

512
00:32:49,045 --> 00:32:52,945
Future autonomous data platform is
one where a lot of the heavy lifting,

513
00:32:53,245 --> 00:32:57,355
like finding data, fixing errors,
tuning performance, checking compliance

514
00:32:57,355 --> 00:32:59,215
is handled by the platform itself.

515
00:32:59,875 --> 00:33:04,255
It's analogous to how modern
cars app lane assist, automatic

516
00:33:04,255 --> 00:33:05,635
braking and self parking.

517
00:33:06,235 --> 00:33:09,805
We are adding those kind of
intelligent features to data systems.

518
00:33:10,495 --> 00:33:16,465
We, as data professionals get to focus
more on strategy, analysis and innovation.

519
00:33:16,750 --> 00:33:21,699
Rather than babysitting pipelines, or
chasing now issues, this transition

520
00:33:21,699 --> 00:33:25,240
from manual to self-optimizing
systems is already underway.

521
00:33:25,840 --> 00:33:31,060
As these pillars mature, we will see
data platforms that can run with minimal

522
00:33:31,060 --> 00:33:35,560
human intervention With us, providing
high level guidance and handling the

523
00:33:35,560 --> 00:33:40,570
exceptions, it's an exciting future,
one that turns the growing data uch

524
00:33:40,570 --> 00:33:42,670
from a challenge into an opportunity.

525
00:33:43,345 --> 00:33:46,855
Because our AI driven platforms
will be ready to handle it.

526
00:33:47,695 --> 00:33:49,375
Thank you for listening.

527
00:33:49,855 --> 00:33:55,435
I hope this gave you a clear
picture of how AI can fundamentally

528
00:33:55,435 --> 00:33:57,295
improve data platform management.

529
00:33:57,805 --> 00:34:02,095
With that, I wrap up and I'm happy to
take any questions or discuss further.

530
00:34:02,575 --> 00:34:02,965
Thank you.

