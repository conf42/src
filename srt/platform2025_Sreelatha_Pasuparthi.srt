1
00:00:00,500 --> 00:00:02,195
This is ti.

2
00:00:02,480 --> 00:00:07,760
I have a total of 18 plus years of
experience in information technology.

3
00:00:08,240 --> 00:00:11,840
Right now I am working as a
principal software engineer

4
00:00:11,899 --> 00:00:13,555
in Liberty Mutual Insurance.

5
00:00:14,540 --> 00:00:19,099
Today I'm going to show the
presentation about how we can build

6
00:00:19,130 --> 00:00:24,439
a bulletproof error detection, a
middleware fast approach to dramatically

7
00:00:24,560 --> 00:00:30,380
reducing meantime to resolution
when your application crashes at.

8
00:00:30,799 --> 00:00:33,290
2:00 AM every second counts.

9
00:00:33,589 --> 00:00:38,269
This guide shows how a strategic
middleware implementation transformed

10
00:00:38,330 --> 00:00:43,489
error response across multiple
enterprise applications, reducing

11
00:00:43,550 --> 00:00:47,810
alert noise, improving detection
accuracy, and dramatically

12
00:00:47,810 --> 00:00:50,269
cutting mean time to resolution.

13
00:00:50,930 --> 00:00:55,280
Next, we are talking about the
crisis that changed everything.

14
00:00:55,780 --> 00:00:58,870
It Tuesday morning brought
a stark realization.

15
00:00:58,930 --> 00:01:04,780
Our flagship e-commerce platform had
been slightly failing for over two

16
00:01:04,780 --> 00:01:09,880
hours incorrectly, processing orders
and corrupting critical customer data.

17
00:01:10,270 --> 00:01:16,420
Our discovery came not from our extensive
monitoring systems, but from an escalating

18
00:01:16,420 --> 00:01:22,180
flood of support tickets and an endra
call from our largest enterprise client.

19
00:01:23,055 --> 00:01:27,285
Despite significant investments
in infrastructure monitoring,

20
00:01:27,615 --> 00:01:32,205
application performance management,
and synthetic testing, our first

21
00:01:32,205 --> 00:01:37,335
indication of critical issues continue
to be direct to customer compliance.

22
00:01:37,395 --> 00:01:43,575
This incident exposed a fundamental flaw
in our existing error detection strategy.

23
00:01:44,075 --> 00:01:46,625
Next we are talking
about the state of alert.

24
00:01:47,125 --> 00:01:52,925
The average developer received over
40 alerts daily with the vast majority

25
00:01:52,925 --> 00:01:58,245
being false Positives are low priority
issues, sir. More than three quarters

26
00:01:58,245 --> 00:02:03,525
of all alerts required no action
or were duplicate of known issues.

27
00:02:03,525 --> 00:02:08,235
Sir. Operations team
turnover froze as ongoing.

28
00:02:08,265 --> 00:02:12,165
Engineers reported high stress
levels and sleep disruption.

29
00:02:12,615 --> 00:02:16,575
Engineers were becoming
desensitized to alerts, often

30
00:02:16,575 --> 00:02:21,855
ignoring or delaying responses
to what might be critical issues.

31
00:02:22,355 --> 00:02:25,054
Next, we are talking
about the hidden costs.

32
00:02:25,659 --> 00:02:26,829
On the left hand.

33
00:02:26,920 --> 00:02:30,939
On the left hand side, we, I'm showing
that four different categories.

34
00:02:31,119 --> 00:02:35,619
The engineering productivity teams
spent excessive time investigating

35
00:02:35,619 --> 00:02:41,499
false positives and manually correlating
data from multiple monitoring tools.

36
00:02:41,920 --> 00:02:46,719
The customer tasked eroded with
each incident that went undetected

37
00:02:46,719 --> 00:02:49,209
leading to contract renewals at risk.

38
00:02:49,774 --> 00:02:54,515
The support burden teams fee fielded
calls about issues that should

39
00:02:54,515 --> 00:02:58,414
have been detected and resolved
before customers ever ripped.

40
00:02:58,414 --> 00:03:03,094
Notice the business impact
sales provided credits.

41
00:03:03,635 --> 00:03:08,854
Marketing dealt with the negative social
media and the company reputation suffered.

42
00:03:09,849 --> 00:03:14,739
We had dozens of monitoring tools and
hundreds of dashboards, yet lacked

43
00:03:14,949 --> 00:03:20,139
the fundamental capability to detect
when things were actually broken From

44
00:03:20,139 --> 00:03:25,299
the user's perspective, the catalyst,
a database corruption issue went

45
00:03:25,299 --> 00:03:30,639
undetected for nearly three hours
affecting thousands of customers.

46
00:03:31,139 --> 00:03:36,809
Next, we are going to see the rethinking
of error detection at the source.

47
00:03:37,409 --> 00:03:38,729
The breakthrough insight.

48
00:03:39,059 --> 00:03:43,709
Real errors happen in code,
not in infrastructure metrics.

49
00:03:44,069 --> 00:03:48,569
Traditional monitoring approaches place
sensors at the infrastructure level.

50
00:03:48,799 --> 00:03:52,849
Monitoring CPU Memory Network
and database performance.

51
00:03:53,329 --> 00:03:57,079
These are lagging indicators
of user impacting issues.

52
00:03:57,469 --> 00:04:01,729
By moving our detection logic into
the application layer, through

53
00:04:01,729 --> 00:04:05,929
strategic middleware implementation,
we could capture errors.

54
00:04:05,929 --> 00:04:09,889
At the moment, they occur with
full context about what the

55
00:04:10,009 --> 00:04:11,704
user was trying to accomplish.

56
00:04:12,494 --> 00:04:17,324
Our middleware captures rich contextual
information about each error.

57
00:04:17,774 --> 00:04:23,114
The user's sessions need the specific
operation being performed, the input

58
00:04:23,114 --> 00:04:25,724
data, and the complete execution path.

59
00:04:26,224 --> 00:04:29,314
Next, we are talking about
the architecture philosophy.

60
00:04:29,814 --> 00:04:32,754
Here we are representing
four different categories.

61
00:04:32,814 --> 00:04:34,015
The request pipeline.

62
00:04:34,015 --> 00:04:38,844
Every request flows through a chain of
middleware, components for authentication,

63
00:04:38,874 --> 00:04:43,525
validation, business logic, data
persistence, and response formatting.

64
00:04:44,114 --> 00:04:48,014
The error capture middleware
operates with surgical precision

65
00:04:48,014 --> 00:04:52,964
capturing with contextual information
about each error as it occurs.

66
00:04:53,444 --> 00:04:58,844
The next category is the classification
ML algorithm determines and

67
00:04:58,844 --> 00:05:02,744
category of the error in real
time based on historical patterns.

68
00:05:03,244 --> 00:05:05,765
The next category is intelligent routing.

69
00:05:06,124 --> 00:05:09,484
Alerts are sent to appropriate
teams through their preferred

70
00:05:09,484 --> 00:05:11,645
channels with complete context.

71
00:05:12,569 --> 00:05:15,239
The middleware operates
in multiple phases.

72
00:05:15,929 --> 00:05:20,099
Immediate capture with minimal
processing, enrichment with additional

73
00:05:20,099 --> 00:05:25,200
context gathered synchronously
classification with ML algorithms

74
00:05:25,259 --> 00:05:26,969
and routing to appropriate teams.

75
00:05:26,969 --> 00:05:30,060
Sir. Each phase is
designed to fail safely.

76
00:05:30,120 --> 00:05:34,229
If the error reporting system
itself encounters problems, the

77
00:05:34,380 --> 00:05:37,770
original application request
continues processing normally.

78
00:05:38,520 --> 00:05:41,849
Next, we are talking about
machine learning for intelligent

79
00:05:41,880 --> 00:05:46,860
alerting, traditional alerting
systems, relay on static thresholds

80
00:05:46,890 --> 00:05:49,200
and simple rule-based logic.

81
00:05:49,289 --> 00:05:54,240
Our machine learning approach analyzes
per patterns across multiple dimensions.

82
00:05:54,905 --> 00:06:01,565
Temporal patterns that identify unusual
behavior at specific times are days

83
00:06:01,955 --> 00:06:06,724
correlation analysis that identifies
relationships between different

84
00:06:06,724 --> 00:06:09,094
types of errors and system events.

85
00:06:09,594 --> 00:06:14,695
User coherent analysis that
detects when errors affect

86
00:06:14,695 --> 00:06:16,675
particular segments differently.

87
00:06:17,005 --> 00:06:22,645
Continuous learning from historical
incident data and engineering feedback.

88
00:06:23,145 --> 00:06:26,145
Next we are talking about
dynamic priority assignment.

89
00:06:26,505 --> 00:06:28,845
Not all errors are created equal.

90
00:06:28,905 --> 00:06:33,375
A database timeout affecting one
user might be a minor play, but

91
00:06:33,375 --> 00:06:38,325
the same error affecting hundreds
of users simultaneously indicates

92
00:06:38,325 --> 00:06:40,305
a serious infrastructure problem.

93
00:06:40,875 --> 00:06:45,435
Our intelligent classification
system considers multiple factors

94
00:06:45,435 --> 00:06:47,505
when assigning priority levels.

95
00:06:47,805 --> 00:06:52,005
On the right hand side, we are
showing four different factors.

96
00:06:52,440 --> 00:06:54,000
In as a category.

97
00:06:54,090 --> 00:06:58,350
The number one is an error
patterns, frequency distribution,

98
00:06:58,380 --> 00:07:00,420
and correlation with other errors.

99
00:07:00,450 --> 00:07:05,040
The next category is business
context, affected user segments

100
00:07:05,040 --> 00:07:06,390
and their business value.

101
00:07:06,450 --> 00:07:11,010
The next category is historical
data, similar past errors

102
00:07:11,070 --> 00:07:12,690
and their resolution parts.

103
00:07:12,780 --> 00:07:17,340
The next category is system
state, current load performance

104
00:07:17,340 --> 00:07:19,530
metrics and scheduled maintenance.

105
00:07:20,400 --> 00:07:26,900
This dynamic approach transformed our
alert quality high priority Alerts now

106
00:07:26,990 --> 00:07:32,990
consistently represent genuine emergencies
while lower priority issues are

107
00:07:33,020 --> 00:07:36,140
appropriately batched for business hours.

108
00:07:36,640 --> 00:07:40,630
Next, we are talking about
multi-platform notification strategies.

109
00:07:40,659 --> 00:07:44,650
Different teams have diff distinct
communication preferences.

110
00:07:45,150 --> 00:07:50,610
Often varying by role, time of day,
and incident severity developers,

111
00:07:50,700 --> 00:07:52,860
slack channels for team coordination.

112
00:07:53,040 --> 00:07:55,290
GitHub, issues for tracking resolution.

113
00:07:55,440 --> 00:08:00,510
IDE, plugins for immediate visibility
coming to the operations, integration

114
00:08:00,510 --> 00:08:04,770
with runbooks incident management
platforms, PagerDuty escalations

115
00:08:05,525 --> 00:08:08,075
next to the management is executed.

116
00:08:08,075 --> 00:08:11,225
Dashboards, email
summaries, schedule reports.

117
00:08:11,684 --> 00:08:15,825
Our system learns which channels
generate the fastest response

118
00:08:15,825 --> 00:08:20,234
times for different types of alerts
and adjust routing accordingly.

119
00:08:20,734 --> 00:08:24,724
Intelligent noise reduction,
the signs of signal detection.

120
00:08:25,504 --> 00:08:31,354
Our approach combines multiple techniques
to separate signal from noise, baselines

121
00:08:31,354 --> 00:08:36,484
that learn normal behavior patterns
for each application and service.

122
00:08:36,909 --> 00:08:38,229
Temporal patterns.

123
00:08:38,289 --> 00:08:42,039
Accounting for diary, weekly
and seasonal variations.

124
00:08:42,699 --> 00:08:47,500
Load based patterns, understanding
how error rates change with traffic,

125
00:08:47,560 --> 00:08:52,599
contextual awareness of deployments,
maintenance windows, and business events.

126
00:08:53,139 --> 00:08:57,194
These dynamic baselines enable
the system to detect anomalies.

127
00:08:57,879 --> 00:09:01,990
That would be invisible to static
threshold based approaches.

128
00:09:02,619 --> 00:09:07,030
Next, we are talking about cascade
prevention and root cause identification.

129
00:09:07,089 --> 00:09:12,429
When systems fail, they often fail
in cascading patterns where one issue

130
00:09:12,429 --> 00:09:14,829
triggers multiple downstream problems.

131
00:09:15,219 --> 00:09:19,510
Traditional monitoring treats each
symptom as an independent issue,

132
00:09:19,599 --> 00:09:23,949
generating dozens of alerts for what
is fundamentally a single problem.

133
00:09:24,449 --> 00:09:28,259
Let's take a example of as shown
below the database failure.

134
00:09:28,319 --> 00:09:33,299
When connection pool exertion causes
primary alert, it'll cause the service

135
00:09:33,299 --> 00:09:38,429
timeouts related alerts, supposed and
linked to root cause system maintenance

136
00:09:38,429 --> 00:09:40,529
service dependency map in real time.

137
00:09:40,949 --> 00:09:45,689
And it'll cause the UI frontend
exceptions related with backend issue.

138
00:09:46,199 --> 00:09:50,909
Our middleware captures sufficient context
to identify these relationships in real

139
00:09:50,909 --> 00:09:57,289
time, dramatically reducing alert noise
during major incidents, real production

140
00:09:57,289 --> 00:09:59,509
monitoring metrics and lessons learned.

141
00:10:00,009 --> 00:10:03,999
Issues that previously took covers
to identify are now detected

142
00:10:03,999 --> 00:10:06,069
with minutes of occurrence.

143
00:10:06,549 --> 00:10:10,659
The machine learning classification
system has virtually eliminated

144
00:10:10,659 --> 00:10:12,189
false positive alerts.

145
00:10:12,699 --> 00:10:17,649
Customer reported errors decreased as
issues were caught before customer impact.

146
00:10:18,534 --> 00:10:22,824
The key success factors, several
FA factors provided critical to our

147
00:10:22,824 --> 00:10:27,834
medieval wear implementation and success
execute to support through initial

148
00:10:27,834 --> 00:10:31,944
investment period when development
velocity temporarily decreased,

149
00:10:32,544 --> 00:10:38,124
minimal performance overhead, allowing
instrumentation of even performance

150
00:10:38,124 --> 00:10:43,824
sensitive code parts middle weight that
fails safely to ensure monitoring does

151
00:10:43,824 --> 00:10:46,134
not introduce new points of failure.

152
00:10:46,719 --> 00:10:51,969
Cultural shift to treating error detection
as integral to application design.

153
00:10:52,029 --> 00:10:53,409
Not an afterthought.

154
00:10:53,909 --> 00:10:55,259
Implementation roadmap.

155
00:10:56,129 --> 00:11:00,749
During the foundation phase, select a
single well understood application with

156
00:11:00,749 --> 00:11:05,579
moderate complexity and clear success
metrics for your pilot implementation.

157
00:11:06,089 --> 00:11:09,629
Basic error captured with
rich context collection,

158
00:11:09,749 --> 00:11:11,489
comprehensive logging of sense.

159
00:11:11,694 --> 00:11:14,454
Session info and request parameters.

160
00:11:14,844 --> 00:11:20,814
Synchronous processing with manual alert
routing, scaling across applications.

161
00:11:20,875 --> 00:11:26,275
Develop standardized approaches for
rollout across your application portfolio.

162
00:11:26,995 --> 00:11:29,694
Create reusable middleware components.

163
00:11:29,905 --> 00:11:33,055
Establish standards for
classification and routing.

164
00:11:33,444 --> 00:11:36,055
Provide training and documentation.

165
00:11:36,884 --> 00:11:41,175
During advanced futures, after
establishing basic error detection,

166
00:11:41,234 --> 00:11:46,364
focus on advanced capabilities, machine
learning classification, intelligent

167
00:11:46,364 --> 00:11:52,124
notification routing, cascade detection
and suppression, performance optimization.

168
00:11:52,624 --> 00:11:56,374
Next we are talking about the
future of operational excellence

169
00:11:56,794 --> 00:11:58,474
beyond error detection.

170
00:11:58,564 --> 00:11:59,914
Predict two operations.

171
00:11:59,944 --> 00:12:03,514
The Middleweight first approach
represents just the beginning

172
00:12:03,514 --> 00:12:07,864
of a broader transformation
toward predict two operations.

173
00:12:08,524 --> 00:12:13,204
Rich contextual data provides the
foundation for advanced analytics.

174
00:12:13,454 --> 00:12:16,394
That can predict problems
before they occur.

175
00:12:16,994 --> 00:12:22,634
Machine learning algorithms identify
patterns that proceed system failures,

176
00:12:22,814 --> 00:12:27,374
enabling proactive intervention
before customers are affected.

177
00:12:28,064 --> 00:12:32,339
The question is not whether to begin
this transformation, but how quickly

178
00:12:32,639 --> 00:12:37,724
you can start building bulletproof
error detection for your organization.

179
00:12:38,224 --> 00:12:42,964
Start small, measure everything
and prepare to be amazed by the

180
00:12:42,964 --> 00:12:48,934
transformation your future self
and your on-call engineers will.

181
00:12:48,934 --> 00:12:54,244
Thank you for taking the first step
toward operational excellence today.

182
00:12:54,744 --> 00:12:54,924
Thank you.

