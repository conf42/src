1
00:00:02,370 --> 00:00:03,210
Hello everyone.

2
00:00:03,210 --> 00:00:04,920
This is Gina Abraham.

3
00:00:05,790 --> 00:00:09,390
Today I would be presenting
modern reliability framework

4
00:00:09,390 --> 00:00:11,190
for high speed infrastructure.

5
00:00:12,090 --> 00:00:16,685
This is a comprehensive approach to
reduce service disruptions and improve

6
00:00:17,405 --> 00:00:21,405
incident detection and resolution
in complex distributed systems.

7
00:00:29,040 --> 00:00:31,710
So key takeaways on the
infrastructure growth.

8
00:00:33,195 --> 00:00:37,155
Over the past few years, we have
seen an unprecedented growth

9
00:00:37,185 --> 00:00:39,045
in infrastructure components.

10
00:00:39,735 --> 00:00:44,655
Systems are scaling at 10 x the previous
rate, creating both opportunities

11
00:00:44,655 --> 00:00:46,245
and significant challenges.

12
00:00:47,505 --> 00:00:52,845
As infrastructure grows, it becomes
increasingly difficult to manage this

13
00:00:52,845 --> 00:00:55,269
scaling means that we need to adopt.

14
00:00:55,545 --> 00:01:00,585
Sophisticated management solutions
to ensure that systems continue to

15
00:01:00,585 --> 00:01:02,985
operate smoothly without disruptions.

16
00:01:04,425 --> 00:01:07,935
One of the biggest challenges
here is monitoring these systems.

17
00:01:08,985 --> 00:01:14,745
Distributed systems create visibility
gaps, essentially, blind spots that

18
00:01:14,745 --> 00:01:20,265
mask potential failure points, making
it harder to detect and address

19
00:01:20,265 --> 00:01:23,175
issues before they impact operations.

20
00:01:31,400 --> 00:01:35,505
So let's just look at
some key SRE challenges.

21
00:01:36,615 --> 00:01:42,315
Site reliability engineering teams or
SRE teams are often faced with, uh,

22
00:01:42,345 --> 00:01:47,054
several critical challenges in maintaining
the reliability of modern systems.

23
00:01:47,985 --> 00:01:51,735
First, monitoring gaps
are a significant issue.

24
00:01:52,634 --> 00:01:55,304
In fact, incomplete visibility is.

25
00:01:55,979 --> 00:01:59,940
Responsible for 68% of
extended service outages.

26
00:02:01,020 --> 00:02:05,160
When we can't see everything happening
in the system, failures can go

27
00:02:05,160 --> 00:02:09,030
undetected, leading to longer down times.

28
00:02:09,690 --> 00:02:13,165
Second, service degradation
is another challenge.

29
00:02:14,670 --> 00:02:19,260
This refers to a gradual performance
decline that is often undetected

30
00:02:19,260 --> 00:02:21,330
until it reaches a critical point.

31
00:02:22,290 --> 00:02:25,890
These small issues accumulate
over time and cause major

32
00:02:25,890 --> 00:02:28,590
disruptions, if not address early.

33
00:02:29,850 --> 00:02:32,370
Lastly, detection delays are common.

34
00:02:33,420 --> 00:02:38,730
On average, there is 42 minute delay
between when an incident occurs

35
00:02:38,730 --> 00:02:40,500
and when an alert is triggered.

36
00:02:41,520 --> 00:02:44,040
That's a lot of time for
the issue to escalate.

37
00:02:52,695 --> 00:02:57,315
Now let's dive into the core components
of our modern reliability framework.

38
00:02:58,665 --> 00:03:02,475
The framework aims to address
the challenges mentioned earlier

39
00:03:02,505 --> 00:03:06,465
and provides powerful tools
to improve system reliability.

40
00:03:07,485 --> 00:03:09,945
One key benefit is 85%.

41
00:03:10,484 --> 00:03:11,715
Faster detection.

42
00:03:12,495 --> 00:03:17,715
By using advanced monitoring and
analytics, we can identify issues

43
00:03:17,715 --> 00:03:19,995
significantly faster than before.

44
00:03:21,075 --> 00:03:25,515
Next, the framework also
reduces false alerts by 65%.

45
00:03:26,385 --> 00:03:30,946
This is important because false
positives can overwhelm SRE

46
00:03:30,946 --> 00:03:33,165
teams and lead to alert fatigue.

47
00:03:33,600 --> 00:03:36,630
Reducing the effectiveness
of response efforts.

48
00:03:38,040 --> 00:03:45,240
Additionally, 93% co coverage of
system components with service level

49
00:03:45,240 --> 00:03:50,700
objectives ensures that we can define
measurable reliability goals for nearly

50
00:03:50,700 --> 00:03:52,800
all aspects of the infrastructure to.

51
00:03:58,960 --> 00:04:03,735
Now let's look at how automated
anomaly detection works.

52
00:04:04,995 --> 00:04:09,615
This section focuses on one of the
most powerful tools of our framework,

53
00:04:10,095 --> 00:04:13,005
the automated anomaly detection.

54
00:04:14,160 --> 00:04:18,000
Dynamic thresholds are used
to adapt to evolving traffic

55
00:04:18,000 --> 00:04:19,680
patterns and system behavior.

56
00:04:20,790 --> 00:04:25,770
This allows the system to automatically
adjust its detection criteria based on

57
00:04:25,770 --> 00:04:32,280
current load, significantly reducing the
occurrence of unnecessary alerts during

58
00:04:32,280 --> 00:04:34,680
expected fluctuations in system behavior.

59
00:04:36,150 --> 00:04:42,420
Machine learning powered analysis plays
a critical role in identifying subtle

60
00:04:42,420 --> 00:04:49,350
deviations in system performance often
before they evolve into major incidents.

61
00:04:50,280 --> 00:04:55,230
This proactive detection of anomalies
allows us to react to issues

62
00:04:55,230 --> 00:04:57,630
earlier and with more precision.

63
00:04:58,920 --> 00:05:00,645
The framework also includes.

64
00:05:01,560 --> 00:05:06,900
Predictive alerts which provide early
warnings based on emerging patterns.

65
00:05:07,830 --> 00:05:12,450
This allows teams to intervene before
systems reach critical thresholds,

66
00:05:12,480 --> 00:05:14,940
potentially avoiding system-wide failures.

67
00:05:23,775 --> 00:05:28,635
System level objectives are a
cornerstone of this framework,

68
00:05:28,665 --> 00:05:31,185
providing a clear set of goals for the.

69
00:05:31,695 --> 00:05:34,335
Reliability and
performance of our systems.

70
00:05:35,325 --> 00:05:42,525
For instance, we are able to track core
resources such as CPU, memory and IO with

71
00:05:42,525 --> 00:05:49,365
99.9% accuracy, ensuring that we have a
clear picture of how our infrastructure

72
00:05:49,365 --> 00:05:51,345
is performing at any given time.

73
00:05:54,045 --> 00:05:58,005
Additionally, the framework is
capable of pre-production bottleneck

74
00:05:58,005 --> 00:06:04,395
detection, which allows us to identify
95% of potential issues before

75
00:06:04,395 --> 00:06:06,225
they even make it to production.

76
00:06:07,515 --> 00:06:12,585
This reduces the chances of performance
degradation once a system is live.

77
00:06:13,739 --> 00:06:18,510
Another benefit is capacity planning,
which allows teams to forecast

78
00:06:18,570 --> 00:06:20,669
resource needs 30 days ahead.

79
00:06:21,299 --> 00:06:26,135
Preventing unexpected performance
degradation due to resource shortages.

80
00:06:34,380 --> 00:06:41,039
Managing complex systems for organizations
that rely on multi-service architectures.

81
00:06:41,580 --> 00:06:42,390
It's important.

82
00:06:42,960 --> 00:06:49,140
To have tools that streamline
incident detection and response alert

83
00:06:49,140 --> 00:06:56,430
consolidation reduces duplicate new
notifications by 75% when multiple

84
00:06:56,430 --> 00:06:59,130
services are impacted by the same issue.

85
00:07:00,330 --> 00:07:06,360
We can avoid overwhelming SRE teams
with redundant alerts, intelligent

86
00:07:06,360 --> 00:07:09,180
filtering, prioritizes alerts based.

87
00:07:09,720 --> 00:07:16,110
On context, ensuring that critical
issues are addressed first and that

88
00:07:16,110 --> 00:07:18,150
resources are used efficiently.

89
00:07:19,710 --> 00:07:26,970
Dependency mapping automatically discovers
and visualizes service relationships,

90
00:07:27,420 --> 00:07:32,100
making it easier to understand how
different services are interconnected.

91
00:07:33,060 --> 00:07:35,640
This insight is crucial for identifying.

92
00:07:36,525 --> 00:07:40,455
And resolving issues that affect
multiple parts of the system.

93
00:07:41,474 --> 00:07:47,414
The correlation engine connects incidents
across services providing a unified view

94
00:07:47,414 --> 00:07:52,875
of service health, and making it easier
to pinpoint root causes of problems.

95
00:08:02,400 --> 00:08:07,170
Cascading failures where one failure
triggers a series of problems across

96
00:08:07,170 --> 00:08:12,060
interconnected systems are among
the most damaging types of outages.

97
00:08:12,900 --> 00:08:18,360
One of the way ways we mitigate these
failures is through early detection.

98
00:08:18,840 --> 00:08:18,930
These.

99
00:08:20,250 --> 00:08:24,210
Real time anomaly detection
to catch deviations before

100
00:08:24,210 --> 00:08:26,010
they spread across the system.

101
00:08:27,060 --> 00:08:29,789
Circuit breaker is another
critical mechanism.

102
00:08:30,479 --> 00:08:35,069
By deploying intelligent boundaries
between services, we can isolate

103
00:08:35,189 --> 00:08:40,260
failing components and prevent them
from contaminating the entire system.

104
00:08:41,159 --> 00:08:46,650
Load shedding algorithms dynamically
route traffic to preserve mission

105
00:08:46,650 --> 00:08:51,150
critical services during periods of
high load or degraded performance.

106
00:08:51,900 --> 00:08:53,490
This ensures that even when.

107
00:08:53,984 --> 00:08:56,265
A part of the system is under strain.

108
00:08:56,685 --> 00:08:58,755
Core services remain operational.

109
00:08:59,805 --> 00:09:04,905
Graceful recovery involves orchestrating
service restoration in a way that

110
00:09:04,905 --> 00:09:10,484
respects dependency, minimizing
the impact on system stability

111
00:09:10,484 --> 00:09:12,435
as services come back online.

112
00:09:19,680 --> 00:09:21,900
Now, let's look at how AI can.

113
00:09:22,845 --> 00:09:24,555
Help with observability.

114
00:09:24,560 --> 00:09:25,935
Observability here.

115
00:09:27,165 --> 00:09:31,575
Artificial inte intelligence
plays a critical role in improving

116
00:09:32,145 --> 00:09:36,040
observability and helping teams
stay on top of system health.

117
00:09:37,320 --> 00:09:38,940
Anomaly classification.

118
00:09:38,940 --> 00:09:45,150
Using AI offers 90% accuracy in
identifying type of issue, reducing

119
00:09:45,150 --> 00:09:49,710
manual in investigation efforts,
and speeding up the response time.

120
00:09:50,730 --> 00:09:56,310
Predictive maintenance can forecast
failures 24 to 48 hours in advance,

121
00:09:56,850 --> 00:10:00,780
giving teams a head start on
addressing potential issues before

122
00:10:00,780 --> 00:10:04,470
they cause real major disruptions.

123
00:10:05,655 --> 00:10:11,925
AI also aids in root cause analysis,
reducing investigation time by 70%.

124
00:10:12,825 --> 00:10:18,165
Instead of manually correlating
fail logs and metrics, AI tools can

125
00:10:18,165 --> 00:10:22,545
quickly pinpoint the root cause,
allowing for faster resolution.

126
00:10:23,595 --> 00:10:28,275
Finally, AI can improve on-call
efficiency, reducing unnecessary

127
00:10:28,275 --> 00:10:32,265
escalations by 60%, which minimizes.

128
00:10:33,345 --> 00:10:37,575
Fatigue and allows SRE teams
to focus on critical tasks.

129
00:10:45,825 --> 00:10:50,990
Now let's walk through the implementation
phases of the framework assessment phase.

130
00:10:52,005 --> 00:10:56,084
During this phase, we conduct a
gap analysis to assess current

131
00:10:56,084 --> 00:11:00,375
monitoring capabilities and
identify areas of improvement.

132
00:11:01,064 --> 00:11:05,745
We also organize workshops to
define SLOs and for and form the

133
00:11:05,745 --> 00:11:11,025
implementation team deployment phase
in the, in the deployment phase.

134
00:11:11,025 --> 00:11:16,155
We start, we set up the core
observability platform and begin service.

135
00:11:16,365 --> 00:11:17,445
Instrumentation.

136
00:11:18,045 --> 00:11:21,735
This is also when we start training
our machine learning models to

137
00:11:22,095 --> 00:11:24,225
recognize patterns in the system.

138
00:11:25,905 --> 00:11:28,065
Lastly, the optimization phase.

139
00:11:29,025 --> 00:11:32,085
The final phase focuses
on refining the system.

140
00:11:32,625 --> 00:11:37,365
This involves tuning alerts to ensure
they are both precise and actionable.

141
00:11:38,955 --> 00:11:45,375
Automating run books to streamline
incident response and con and.

142
00:11:46,005 --> 00:11:49,740
Continuously improving the platform
based on feedback and new data.

143
00:11:59,430 --> 00:12:03,360
In conclusion, thank you
for your time and attention.

144
00:12:04,170 --> 00:12:09,360
This framework is designed to tackle
the modern challenges of distributed

145
00:12:09,360 --> 00:12:14,010
systems and provide proactive
solutions for ensuring reliability

146
00:12:14,550 --> 00:12:17,490
and minimizing service disruptions.

