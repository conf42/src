1
00:00:00,120 --> 00:00:00,810
Hey everyone.

2
00:00:01,080 --> 00:00:04,140
I'm a senior software engineer
in the FinTech industry.

3
00:00:04,740 --> 00:00:08,100
Today we are gonna talk about
revolutionizing FinTech with a WSH

4
00:00:08,100 --> 00:00:10,320
maker and using go in machine learning.

5
00:00:10,605 --> 00:00:14,865
The FinTech industry is undergoing
a dramatic transformations powered

6
00:00:14,865 --> 00:00:19,005
by machine learning and artificial
intelligence integration with global

7
00:00:19,005 --> 00:00:23,145
FinTech markets projected to reach
thousand billion dollars by 2033,

8
00:00:23,595 --> 00:00:27,795
the financial institutions must
adopt to scalable ML solutions to

9
00:00:27,795 --> 00:00:29,535
maintain competitive advantage.

10
00:00:30,015 --> 00:00:34,215
This presentation explores how A WSH
makers simplifies the implementation

11
00:00:34,215 --> 00:00:35,775
of machine learning models.

12
00:00:36,255 --> 00:00:40,425
We'll examine how sage makers integrated
environment streamlines the machine

13
00:00:40,425 --> 00:00:44,825
learnings lifecycle while maintaining
security and regulatory compliance.

14
00:00:45,275 --> 00:00:49,640
We'll also see how Golan can be used,
for ML development and integrate,

15
00:00:49,700 --> 00:00:53,600
and how we can integrate it with
AWS SageMaker for machine learning

16
00:00:53,600 --> 00:00:55,490
model development and deployment.

17
00:00:55,990 --> 00:01:00,280
Using SageMaker has given a boost for
model development by 40% by taking

18
00:01:00,280 --> 00:01:04,690
advantage of its features, simplifying,
automating various stages in ML lifecycle.

19
00:01:05,260 --> 00:01:08,950
This leads to better focus on model
development rather than focusing on

20
00:01:08,950 --> 00:01:13,300
managing the lifecycle development,
deployment, experimentation of the models.

21
00:01:13,440 --> 00:01:15,930
Which ultimately leads
to improved accuracy.

22
00:01:16,430 --> 00:01:19,970
Financial institution face increasing
pressure to innovate while maintaining

23
00:01:19,970 --> 00:01:21,440
strict compliance standards.

24
00:01:21,800 --> 00:01:25,700
The adoption of our advanced machine
learning solutions like A WSH Maker

25
00:01:26,000 --> 00:01:30,620
represents a critical co advantage
in this rapidly evolving marketplace.

26
00:01:30,620 --> 00:01:34,414
There are some key bottlenecks in
the FinTech ML implementations.

27
00:01:34,755 --> 00:01:38,565
The first one is the talent bottleneck
shortage of ML specialists with the

28
00:01:38,565 --> 00:01:42,795
knowledge and requiring the engineers
to be fully knowledgeable of models and

29
00:01:42,795 --> 00:01:44,685
separate infrastructure management teams.

30
00:01:44,685 --> 00:01:47,175
For ML models, deployment and maintenance.

31
00:01:47,835 --> 00:01:50,985
There's infrastructure complexity,
model development, training,

32
00:01:51,015 --> 00:01:54,345
nest states, the use of large
and powerful computing resources.

33
00:01:54,734 --> 00:01:58,544
Such as high performance GPUs,
distributed competitive systems to

34
00:01:58,544 --> 00:02:03,024
handle the massive data sets and,
computational intensive algorithms

35
00:02:03,205 --> 00:02:04,825
they need to be handed efficiently.

36
00:02:04,825 --> 00:02:09,714
With scalability, there are compliance
requirements, data security concerns.

37
00:02:10,014 --> 00:02:14,154
Protecting sensitive customer
financial data remains the highest

38
00:02:14,154 --> 00:02:17,575
priority, while adhering to the
stringent regulatory frameworks

39
00:02:17,844 --> 00:02:19,579
as additional layer of complexity.

40
00:02:20,079 --> 00:02:21,789
There are model deployment hurdles too.

41
00:02:21,849 --> 00:02:25,390
Once the development is done, there's
a gap and extensive overhead of

42
00:02:25,390 --> 00:02:29,820
infrastructure management, management for
training and deployment into production

43
00:02:30,060 --> 00:02:32,160
for inference and batch inferences.

44
00:02:32,660 --> 00:02:36,290
Here comes where we
introduce AWS SageMaker.

45
00:02:36,710 --> 00:02:38,960
So let's see how it solves these problems.

46
00:02:39,320 --> 00:02:42,829
The integrated ML
workflow, it provides a in.

47
00:02:42,829 --> 00:02:46,160
It provides an intuitive to environment
for building, training and deploying

48
00:02:46,160 --> 00:02:49,459
the machine learning models with
minimal infrastructure management.

49
00:02:50,329 --> 00:02:54,799
It has prebuilt algorithms, it gives
access to production ready algorithms

50
00:02:54,799 --> 00:02:58,400
for common FinTech use cases,
including fraud detection, credit

51
00:02:58,400 --> 00:03:00,740
scoring, and customer segmentation.

52
00:03:01,355 --> 00:03:04,595
It also provides several
state-of-the-art foundational models

53
00:03:04,595 --> 00:03:06,875
to build your own gen AI solutions.

54
00:03:07,375 --> 00:03:08,934
It has framework flexibility.

55
00:03:09,355 --> 00:03:12,774
It supports popular M
ML frameworks, including

56
00:03:12,774 --> 00:03:15,265
TensorFlow, pipe touch and MXNet.

57
00:03:15,565 --> 00:03:20,815
It also has multiple kernels, and it
also enables usage of Golan for model

58
00:03:20,815 --> 00:03:25,045
development and deployment, allowing
teams to work with familiar tools

59
00:03:25,255 --> 00:03:27,059
while leveraging AWS infrastructure.

60
00:03:27,559 --> 00:03:29,689
It has automated
infrastructure management.

61
00:03:29,869 --> 00:03:33,469
It eliminates the complexity of
provisioning and managing specialized

62
00:03:33,739 --> 00:03:38,239
ML infrastructure while ensuring
optimal performance and cost efficiency.

63
00:03:39,049 --> 00:03:43,009
The A-W-S-H-H-S-H Maker democratizes
machine learning by removing

64
00:03:43,009 --> 00:03:45,199
infrastructure barriers and providing.

65
00:03:45,224 --> 00:03:48,764
Tools that support the entire
machine learning model lifecycle.

66
00:03:49,484 --> 00:03:53,114
This em empowers FinTech organizations
to focus on building sophisticated

67
00:03:53,114 --> 00:03:57,254
models rather than managing complex
infrastructure and adhering to,

68
00:03:57,444 --> 00:03:59,424
stringent rules and compliances.

69
00:03:59,924 --> 00:04:03,944
Now let's deep dive deep into the features
of SageMaker that help in achieving these

70
00:04:03,944 --> 00:04:07,064
advantages and how they are ease of use.

71
00:04:07,564 --> 00:04:12,030
AWS SageMaker has something called
SageMaker Studio, and most recently

72
00:04:12,030 --> 00:04:13,980
they have added Unified Studio.

73
00:04:14,850 --> 00:04:19,079
It provides a platform which has a
fully managed service that enables

74
00:04:19,079 --> 00:04:22,230
data scientists and developers to
seamlessly build, train, and deploy

75
00:04:22,230 --> 00:04:26,970
machine learning models with Unified
Studio or advanced project management and

76
00:04:26,970 --> 00:04:28,770
collaboration tools are also provided.

77
00:04:29,459 --> 00:04:32,909
SageMaker Studio provides an
integrated development environment.

78
00:04:33,240 --> 00:04:36,600
Through which you can perform
all machine learning development

79
00:04:36,600 --> 00:04:38,280
steps and deploy at scale two.

80
00:04:38,549 --> 00:04:43,260
It's a web-based interface program,
which you can access directly from

81
00:04:43,260 --> 00:04:45,659
the browser or the AWS console.

82
00:04:46,159 --> 00:04:49,700
The studio provides environments
like Jupiter Lab Code Editor, which

83
00:04:49,700 --> 00:04:53,179
is based on Visual Studio Code,
open source, and even our studio.

84
00:04:54,034 --> 00:04:59,194
It has various kernels and infra
development in ones can be spun

85
00:04:59,194 --> 00:05:02,614
up quickly with just few clicks,
and they come pre-configured

86
00:05:02,614 --> 00:05:06,844
with necessary machine learning
libraries and kernels and frameworks.

87
00:05:07,234 --> 00:05:12,304
Users can also spec select specific
instance types to optimize performance

88
00:05:12,304 --> 00:05:16,654
and cost and instance types can
all can be updated and modified at

89
00:05:16,654 --> 00:05:18,694
different stages of the ML lifecycle.

90
00:05:19,309 --> 00:05:23,299
They also have large selection of
models optimized for AWS through

91
00:05:23,299 --> 00:05:27,709
which you can select any model, train
it, deploy it, and also evaluate

92
00:05:27,709 --> 00:05:29,449
the model based on your data set.

93
00:05:30,439 --> 00:05:33,679
You can also quickly fine tune
the models with hyper parameters

94
00:05:33,709 --> 00:05:35,329
that it feature that it provides.

95
00:05:35,929 --> 00:05:39,409
So it SageMaker provides
an end-to-end flow.

96
00:05:40,219 --> 00:05:45,409
It offers the capability for model tuning,
monitoring, debugging, even experimenting

97
00:05:45,409 --> 00:05:46,999
with your morals and updating them.

98
00:05:47,914 --> 00:05:53,074
It provides a platform to check all the
training jobs, deployed endpoints for

99
00:05:53,074 --> 00:05:57,874
inferences, build and maintain pipelines,
unifying the tools needed for machine

100
00:05:57,874 --> 00:06:00,154
learning development into one interface.

101
00:06:00,654 --> 00:06:04,014
This is the SageMaker UI
interface with your IM rules.

102
00:06:04,014 --> 00:06:07,524
You can set up environment, select
an ID, and choose an instant

103
00:06:07,524 --> 00:06:11,184
type and an image and start model
development instantaneously.

104
00:06:11,664 --> 00:06:14,854
You can choose, you can change the
instance type at different, stages

105
00:06:14,854 --> 00:06:19,034
of the ML life cycle, like data
prep training, and the data will

106
00:06:19,034 --> 00:06:20,744
be persisted across the instance.

107
00:06:20,744 --> 00:06:21,824
Start and stop.

108
00:06:22,324 --> 00:06:26,794
Once the, instance has been ready,
you can just start and stop or

109
00:06:26,794 --> 00:06:29,074
open the AWS SageMaker Studio.

110
00:06:29,124 --> 00:06:31,374
ID by clicking on the open here.

111
00:06:31,874 --> 00:06:35,384
As I was previously mentioned, we have
a wide range of kernels to choose from,

112
00:06:35,384 --> 00:06:40,554
and here's a sample of the web based
UI interface, for a model development.

113
00:06:41,054 --> 00:06:45,319
We can use a jumpstart feature in
SageMaker where you can select a wide

114
00:06:45,319 --> 00:06:50,609
range of open source models with just
a click train the model, against your

115
00:06:50,609 --> 00:06:55,199
data, deploy it, and even evaluate
your data against the train model.

116
00:06:55,699 --> 00:06:59,204
SageMaker has its seamless
integration with various other, AWS

117
00:06:59,204 --> 00:07:01,424
services, which is a big advantage.

118
00:07:01,814 --> 00:07:07,274
For example, a we can ha use AWS S3 to
store and retrieve training data models.

119
00:07:07,554 --> 00:07:14,044
data models securely leverage compute
power of EC2 instances avail available.

120
00:07:14,644 --> 00:07:18,724
Use A-W-S-E-M-R and a AWS
glue for data preparations.

121
00:07:18,844 --> 00:07:24,594
In case of large data sets, AWS Lambda
can be used for event driven ML pipelines.

122
00:07:25,149 --> 00:07:30,429
A-W-S-I-M helps in fine grain access
management to models, data and resources.

123
00:07:31,389 --> 00:07:36,639
With A-W-S-V-P-C, it enables our
users to seek securely, train and

124
00:07:36,639 --> 00:07:39,819
deploy machine learning models
within a private network environment.

125
00:07:40,359 --> 00:07:44,829
This isolation helps protect sensitive
data and ensures that model training

126
00:07:44,829 --> 00:07:48,909
and inference process are not
exposed to external threats, thereby

127
00:07:48,909 --> 00:07:50,469
hand answering the data security.

128
00:07:50,969 --> 00:07:54,359
Now let's go through the different
stages in ML model development and see

129
00:07:54,359 --> 00:07:57,089
how A-A-W-S-H maker helps in each stage.

130
00:07:57,809 --> 00:08:00,419
The first stage would be
obviously data preparation.

131
00:08:00,449 --> 00:08:06,274
Once we have the, the id, set up
using SageMaker Studio or locally.

132
00:08:06,774 --> 00:08:07,254
Data.

133
00:08:07,404 --> 00:08:09,624
Data provision can be
done in different ways.

134
00:08:09,714 --> 00:08:11,454
The first one is Juta Lab.

135
00:08:11,904 --> 00:08:14,154
This is the traditional way of data prep.

136
00:08:14,184 --> 00:08:18,234
In a W SageMaker, you use your,
its provided by SageMaker.

137
00:08:18,564 --> 00:08:21,624
It works for manageable data
size, and when individual

138
00:08:21,624 --> 00:08:22,974
has experience with coding.

139
00:08:23,474 --> 00:08:25,274
SageMaker also provides data Wrangler.

140
00:08:25,514 --> 00:08:27,734
It's a visual interface
for data preparation.

141
00:08:28,094 --> 00:08:33,344
We can import data from various sources
like S3 Athena, upload your own file, use,

142
00:08:33,395 --> 00:08:38,554
import data from Redshift or Salesforce
Data Cloud, or even from A-W-S-E-M-R.

143
00:08:39,155 --> 00:08:41,765
We can do visual data
analysis of your data.

144
00:08:41,799 --> 00:08:46,300
Find duplicates, missing data, and get
a general overview of the data with

145
00:08:46,359 --> 00:08:50,979
pre-built functionalities before the
data prep as well, we can perform data

146
00:08:50,979 --> 00:08:55,390
per perform data transformations like
dropping columns, splitting the training

147
00:08:55,420 --> 00:08:57,939
and test data, and the validation data.

148
00:08:58,209 --> 00:09:02,525
It's a user friendly work environment,
which works for non coders as well.

149
00:09:02,979 --> 00:09:06,449
Data Wrangler is more, a UI
based data preparation tool.

150
00:09:06,949 --> 00:09:09,900
We can use, SageMaker
integration with A-W-S-E-M-R.

151
00:09:10,575 --> 00:09:15,555
Elastic Map produce, we can leverage emr,
big data processing capabilities directly

152
00:09:15,555 --> 00:09:20,584
from SageMaker studio or notebooks to
efficiently handle large scale data sets

153
00:09:20,824 --> 00:09:22,834
in the range of terabytes and petabytes.

154
00:09:23,194 --> 00:09:25,924
Eliminating the need to switch
between tools and platforms.

155
00:09:26,164 --> 00:09:30,364
We can create and view EMR clusters
directly from SageMaker Studio

156
00:09:30,364 --> 00:09:34,714
and connect with Autogenerated
code in Eds and studios.

157
00:09:35,214 --> 00:09:39,180
We can also use AWS Glue, which is
primarily used by data engineers.

158
00:09:39,209 --> 00:09:43,280
However, with integration with AWS
SageMaker, AWS Glue Interactive

159
00:09:43,280 --> 00:09:47,630
Sessions provide an on demand serverless
Apache Spark run type environment that

160
00:09:47,630 --> 00:09:51,949
you can initialize in seconds on a
dedicated processing unit that you can

161
00:09:52,250 --> 00:09:55,970
enlist to collect, transform, clean,
and prepare the data for storage in

162
00:09:55,970 --> 00:09:57,949
your data lakes and data pipelines.

163
00:09:58,449 --> 00:10:02,560
Once the data prep is done, we move on
to the model development and training.

164
00:10:03,459 --> 00:10:08,220
SageMaker provides various model
development, methods for all stages of

165
00:10:08,220 --> 00:10:12,780
doubles from novice to expert, and you,
we can also utilize predefined models or

166
00:10:12,780 --> 00:10:16,740
develop your own models with SageMaker
or bring in custom models from elsewhere.

167
00:10:17,640 --> 00:10:21,760
Let's first discuss about the no code
or the low code, which is focused

168
00:10:21,760 --> 00:10:23,045
on using the built-in algorithms.

169
00:10:23,635 --> 00:10:26,005
It requires no or less coding.

170
00:10:26,485 --> 00:10:30,955
The only inputs you need to provide
are the data, hyper parameter

171
00:10:30,955 --> 00:10:34,225
tuning and the compute resources
that you would like to use.

172
00:10:35,065 --> 00:10:39,215
This allows you to run, experiments
more quickly with less overhead of

173
00:10:39,215 --> 00:10:41,285
tracking the results and code changes.

174
00:10:41,735 --> 00:10:45,605
You can utilize wide range of models
provided from SageMaker, from fundamental

175
00:10:45,605 --> 00:10:48,725
models to foundational models with Gen ai.

176
00:10:49,225 --> 00:10:52,885
You provide the data to your model,
train data and deploy your model.

177
00:10:53,425 --> 00:10:56,785
We'll get into the more details about
the deployment in the next slides.

178
00:10:57,285 --> 00:11:00,255
Then it's a, we can go into
the next, a bit more advanced

179
00:11:00,255 --> 00:11:01,485
version called the script mode.

180
00:11:02,325 --> 00:11:05,685
If the algorithm required for your
model isn't available as a built-in

181
00:11:05,685 --> 00:11:10,605
more option, your A and you're adapt at
coding your own solution, you might want

182
00:11:10,605 --> 00:11:12,705
to use SageMaker supported frameworks.

183
00:11:13,095 --> 00:11:14,505
This is often called a script mode.

184
00:11:14,505 --> 00:11:18,075
In this mode, you create your
custom core or a script in a dot,

185
00:11:18,075 --> 00:11:20,835
PIY, or a do R or a do spark file.

186
00:11:21,285 --> 00:11:24,825
You can utilize the various machine
learning frameworks that come built in,

187
00:11:24,825 --> 00:11:27,525
like the TensorFlows psyche, learn MXNet.

188
00:11:28,005 --> 00:11:31,635
We can add custom parameters
and do other modifications too.

189
00:11:32,135 --> 00:11:35,875
Then we can use the bi, if the
built-in algorithms and the supportive

190
00:11:35,875 --> 00:11:39,925
frameworks, kernels language
should cover the most use cases.

191
00:11:40,105 --> 00:11:43,285
But there are times where you
might need to use algorithms from

192
00:11:43,285 --> 00:11:46,285
a package not included in any
of the supportive frameworks.

193
00:11:46,660 --> 00:11:49,780
Perhaps you're not comfortable
with Python or R and you want to

194
00:11:49,780 --> 00:11:51,460
use your preferred language as go.

195
00:11:51,700 --> 00:11:55,630
In such cases, you can write your
own models externally and create

196
00:11:55,630 --> 00:11:59,710
a custom docker image with model
code and necessary dependencies.

197
00:12:00,210 --> 00:12:05,880
Containerizations on models of models
is a fundamental aspect of a WSH maker.

198
00:12:06,465 --> 00:12:09,855
SageMaker relies heavily on
container images for both

199
00:12:09,855 --> 00:12:11,475
model deployment and training.

200
00:12:12,255 --> 00:12:16,155
This allows for efficient and scalable
management of models, ensuring

201
00:12:16,155 --> 00:12:21,964
portability, scalability, and consistency
and reproducibility across environments.

202
00:12:22,464 --> 00:12:26,944
Space SageMaker uses, docker images
and containers as a standard format for

203
00:12:27,155 --> 00:12:29,344
packaging, training and deploying models.

204
00:12:29,645 --> 00:12:33,275
These containers contain all the necessary
dependencies, libraries, and core

205
00:12:33,275 --> 00:12:37,564
requirements to run the model, isolating
it from the underlying infrastructure.

206
00:12:38,135 --> 00:12:40,775
In the further slides, we, I
will explain the steps to use.

207
00:12:40,775 --> 00:12:45,525
Go for creating a model and train
and deploy, the data using SageMaker.

208
00:12:46,425 --> 00:12:48,984
You can train your model
through Studio, ID.

209
00:12:49,870 --> 00:12:55,575
Jumpstart or through SageMaker, training
job for custom model AI in images

210
00:12:56,145 --> 00:13:01,810
SageMaker save the train model in, in as
an artifact in the S3 bucket specified

211
00:13:01,810 --> 00:13:06,190
in your training job configuration
mentioned through the UI or through code.

212
00:13:06,790 --> 00:13:11,020
This bucket can be in your own
account or in an AWS service catalog.

213
00:13:11,520 --> 00:13:14,760
This is an example of, using
a built model builtin model.

214
00:13:15,090 --> 00:13:20,030
As you can see in these two examples,
we are importing SageMaker URIs, from

215
00:13:20,195 --> 00:13:25,555
of linear and XCG boost, and passing
in our data to train our model.

216
00:13:25,885 --> 00:13:28,875
We can also use the, the using Jumpstart.

217
00:13:28,875 --> 00:13:33,245
We can use the foundational models,
directly, instead of even going into

218
00:13:33,245 --> 00:13:38,575
the ID and, selecting a model, train
the pass, when you click, passing

219
00:13:38,575 --> 00:13:40,705
your data and you can deploy it.

220
00:13:40,755 --> 00:13:45,895
Two, you can also do fine tuning,
which will again open an IDE in

221
00:13:45,895 --> 00:13:50,485
SageMaker studio, and you can even
fine tune the fine foundational models.

222
00:13:51,470 --> 00:13:55,250
Going back, the next, method
is using the script mode.

223
00:13:55,610 --> 00:13:59,180
This is an example of a script mode
where we write our own algorithm using

224
00:13:59,180 --> 00:14:02,810
libraries and framework, provided
by SageMaker to build your model.

225
00:14:03,350 --> 00:14:04,575
Here we have more control.

226
00:14:05,205 --> 00:14:10,675
On the model and have the ability to use
libraries like LEA and TensorFlow here.

227
00:14:10,675 --> 00:14:14,785
In this case, we are using the
TensorFlow, DDNN classifier.

228
00:14:15,175 --> 00:14:20,270
We mentioned the script as the entry point
and the remaining parameters are similar

229
00:14:20,270 --> 00:14:23,100
to using your, using a built in, model.

230
00:14:23,600 --> 00:14:29,310
This is an example of building our own
model and models and using them, in

231
00:14:29,310 --> 00:14:31,650
SageMaker with the help of Docker images.

232
00:14:32,150 --> 00:14:34,990
This is a mo this is the case
where a model that you have built

233
00:14:34,990 --> 00:14:38,740
locally using custom LI libraries
not available in SageMaker.

234
00:14:38,740 --> 00:14:42,040
For example, an ML model
written in goal language.

235
00:14:42,540 --> 00:14:46,980
So assume like these are the,
go language, files, of where you

236
00:14:46,980 --> 00:14:48,720
have written your, your model.

237
00:14:49,500 --> 00:14:52,790
Along with the model we create,
a Docker file containing the

238
00:14:52,790 --> 00:14:57,180
information about the necessary
dependencies, libraries and kernels.

239
00:14:57,680 --> 00:15:01,220
We created a Dockery image
and push it into A-W-S-E-C-R

240
00:15:01,310 --> 00:15:02,840
Elastic Container Registry.

241
00:15:03,770 --> 00:15:08,310
Now, using the image id, we
train our model in SageMaker.

242
00:15:09,090 --> 00:15:10,440
There are two ways to do it.

243
00:15:10,830 --> 00:15:15,870
One is in a Jupiter Studio, lab studio,
Jupiter lab, as we see in the image here

244
00:15:15,870 --> 00:15:17,700
where we are mentioning the image name.

245
00:15:18,000 --> 00:15:21,860
And NCE in the models model
programmatically, or or through

246
00:15:21,860 --> 00:15:23,900
the console using training job.

247
00:15:24,400 --> 00:15:24,610
Here.

248
00:15:24,610 --> 00:15:28,390
Also, we give the image information and
training data where we want to store

249
00:15:28,390 --> 00:15:33,580
the, train model artifact After the
training is done here, in this case,

250
00:15:33,580 --> 00:15:37,710
you can see we are, man, we are creating
a training job, YouTube hyphen one.

251
00:15:37,870 --> 00:15:43,410
we are also, we selecting an option to
use your own algorithm container in ECR.

252
00:15:43,740 --> 00:15:50,620
Here we pass in our image name, the, which
has, containerized, our model code and all

253
00:15:50,620 --> 00:15:52,720
the libraries and kernels that we need.

254
00:15:53,050 --> 00:15:56,920
We specify the data source and
we specify the output path where

255
00:15:56,920 --> 00:16:00,070
the, machine learning model,
artifact needs to be stored.

256
00:16:00,925 --> 00:16:05,965
In the code format, we spare, we
use the code to pass in the image,

257
00:16:06,235 --> 00:16:08,035
and we pass in the I am role.

258
00:16:08,285 --> 00:16:13,595
and again, the remaining parameters
are similar to how we use the script

259
00:16:13,595 --> 00:16:16,595
mode or, or as in the built in models

260
00:16:17,095 --> 00:16:18,505
now coming to deployment.

261
00:16:19,195 --> 00:16:23,215
Once you've trained your model
using the IDE or through SageMaker

262
00:16:23,215 --> 00:16:25,645
training jobs, you can deploy it.

263
00:16:25,705 --> 00:16:29,245
You can deploy your model with
AWS SageMaker to make predictions.

264
00:16:29,745 --> 00:16:32,595
Amazon SageMaker provides
several deployment options

265
00:16:32,595 --> 00:16:34,065
to suit different needs.

266
00:16:34,565 --> 00:16:37,825
The first use case is
realtime hosting service.

267
00:16:38,155 --> 00:16:41,935
This can be used for continuous
realtime inferences against your model.

268
00:16:42,520 --> 00:16:45,550
This is best in the case where
there is continuous traffic.

269
00:16:46,000 --> 00:16:47,890
The play payload is of smaller site.

270
00:16:48,370 --> 00:16:51,070
It's deployed in our
preferred instance type.

271
00:16:51,400 --> 00:16:56,090
This is best suited for the cases where,
the machine learning model is for ad

272
00:16:56,090 --> 00:17:00,020
serving, personalized recommendation
or immediate fraud detection.

273
00:17:00,770 --> 00:17:03,680
The second case is of
using serverless inference.

274
00:17:04,010 --> 00:17:05,440
This is, this option.

275
00:17:05,440 --> 00:17:10,420
We can use it for intermittent traffic
where we see bursts of traffic maybe

276
00:17:10,420 --> 00:17:14,260
during daytime or during office
hours or during end of the day.

277
00:17:14,650 --> 00:17:18,280
This is ideal for workloads
that can tolerate cold starts.

278
00:17:18,780 --> 00:17:21,750
It automatically launches computer
sources and scales automatically

279
00:17:22,260 --> 00:17:25,530
removing the need to choose the
instant type based on the traffic.

280
00:17:26,130 --> 00:17:31,520
This best works in a case where
we're doing workload tests, where we

281
00:17:31,550 --> 00:17:36,440
extract, analyze data from documents,
after a certain, end of the day.

282
00:17:36,800 --> 00:17:40,040
And also sometimes we see
chat bots with search traffic.

283
00:17:40,370 --> 00:17:44,390
This serverless inference would
be the best for these scenarios.

284
00:17:44,840 --> 00:17:47,610
There are other cases where we
can use, asynchronous inference.

285
00:17:48,110 --> 00:17:51,170
Here the inferences where we
have to have large payloads,

286
00:17:51,230 --> 00:17:55,420
maybe up to one gb and getting an
inference takes longer processing.

287
00:17:56,110 --> 00:18:00,150
In this case, we have the
flexibility of waiting for the

288
00:18:00,540 --> 00:18:02,190
inference, and we are okay with it.

289
00:18:02,690 --> 00:18:08,390
There are other cases where we are not,
okay with one, one or two inferences.

290
00:18:08,690 --> 00:18:12,890
We need to do a batch of them and
usually this is the case for offline.

291
00:18:13,250 --> 00:18:15,670
The so here where we
can use batch transform.

292
00:18:16,375 --> 00:18:18,655
This is for large amounts of data sets.

293
00:18:18,955 --> 00:18:20,185
It has higher throughput.

294
00:18:20,555 --> 00:18:24,365
it can support large data sets in
the gigabit size, and processing

295
00:18:24,365 --> 00:18:26,285
times can go up to days.

296
00:18:26,915 --> 00:18:30,845
So this works for the specific case,
like churn prediction, predictive

297
00:18:30,845 --> 00:18:37,180
maintenance, and historical analysis of
customers to identify any, scope of risk.

298
00:18:37,680 --> 00:18:38,760
These are the cases.

299
00:18:38,810 --> 00:18:41,530
we, these are the two examples
that we have where we are

300
00:18:41,530 --> 00:18:44,440
making real time inferences,
calling the endpoint to predict.

301
00:18:44,740 --> 00:18:48,510
So we are passing in a sentence which
we want to do a prediction on, we

302
00:18:48,510 --> 00:18:52,190
call, a classifier, which has, which
we've used previously to deploy.

303
00:18:52,395 --> 00:18:56,535
And we call the predict function
with our input, and we get back

304
00:18:56,535 --> 00:19:01,255
a response with the, probability
of saying like this is 99%.

305
00:19:01,525 --> 00:19:06,915
correct or wrong, this, however, is
an example of batch transform where

306
00:19:07,245 --> 00:19:12,425
we have created hugging face model,
and we are, passing a job, a bad job

307
00:19:12,455 --> 00:19:17,985
transformer to do inferences on a
bunch of data from an NS three file.

308
00:19:18,485 --> 00:19:22,625
This is the overview of SageMaker and how
it can be used in entire ML lifecycle.

309
00:19:23,135 --> 00:19:26,885
Financial institutions benefit from
the streamlined data labeling with

310
00:19:26,885 --> 00:19:31,295
SageMaker ground truth, comprehensive
exploit analysis through SageMaker

311
00:19:31,295 --> 00:19:36,155
studio notebooks, and sophisticated hyper
parameter optimization that eliminates the

312
00:19:36,155 --> 00:19:38,925
guesswork, framework in model refinement.

313
00:19:39,425 --> 00:19:43,805
Model operation directly translates to
financial outcomes and risk management.

314
00:19:44,105 --> 00:19:47,915
SageMaker has advanced monitoring
capabilities by integration with AWS.

315
00:19:47,915 --> 00:19:50,285
CloudWatch provide essential safeguards.

316
00:19:50,785 --> 00:19:53,545
SageMaker is continuously
evolving with new features.

317
00:19:54,025 --> 00:19:56,785
These are some of them
SageMaker experiments.

318
00:19:56,995 --> 00:19:59,290
It's used to run experiments
during model building.

319
00:19:59,770 --> 00:20:04,180
Monitoring the inputs and outputs
enhance the repeatability and tracking.

320
00:20:04,600 --> 00:20:08,060
It helps you keep track of your metrics
and collaborate your, with your teams

321
00:20:08,060 --> 00:20:13,190
in these experiments, it also provides a
new feature called SageMaker Pipelines.

322
00:20:13,580 --> 00:20:18,350
It provides a fully managed C-S-E-R-D
service for machine learning workflows.

323
00:20:18,560 --> 00:20:22,800
Enabling automated, scalable, and
reproduce ML model development.

324
00:20:23,340 --> 00:20:28,860
It helps to define end-to-end ML
workflows with a UI based with drag drop

325
00:20:28,860 --> 00:20:31,830
connect directed cycle graph structure.

326
00:20:32,220 --> 00:20:35,870
This is a UI interface where you
can just drag and drop pipelines.

327
00:20:36,665 --> 00:20:42,885
And AI handle the ML lifecycle from,
data preparation to model deployment.

328
00:20:43,815 --> 00:20:45,885
It also has SageMaker model monitor.

329
00:20:46,305 --> 00:20:50,205
It helps in oversee the quality of AI
machine learning models in production.

330
00:20:50,565 --> 00:20:55,305
It continuously monitors across various
deployment methods and also alerts

331
00:20:55,335 --> 00:20:58,245
any deviations in model quality.

332
00:20:58,830 --> 00:21:02,310
That could compromise the accuracy in
mission critical operations such as

333
00:21:02,310 --> 00:21:06,375
fraud prevention, credit assessment,
and algorithmic trading conditions.

334
00:21:06,945 --> 00:21:12,105
Fa this facilitates an early direction
of wrong predicts and take early

335
00:21:12,135 --> 00:21:16,835
action With Amazon Bedrock, and
Sage and Unified Studio and AWS

336
00:21:16,835 --> 00:21:22,175
SageMaker, cap, capabilities, it helps
us to quickly build and customize

337
00:21:22,175 --> 00:21:23,405
your generative AI applications.

338
00:21:23,905 --> 00:21:27,085
It lets you work with high
performing foundational models,

339
00:21:27,385 --> 00:21:28,675
customize the Foundation.

340
00:21:28,985 --> 00:21:33,455
Foundational models to match your
requirements, data workflows, and

341
00:21:33,725 --> 00:21:36,125
also meet responsible AI standards.

342
00:21:36,665 --> 00:21:40,625
It gives an access to wide range of
foundational models from leading AI

343
00:21:40,625 --> 00:21:42,995
companies through the Gen I Playground.

344
00:21:43,865 --> 00:21:46,865
You can compare different
models and configurations to

345
00:21:46,985 --> 00:21:48,570
evaluate their performances Easy.

346
00:21:49,070 --> 00:21:53,750
Now coming to the usage of go for
ML model development while go might

347
00:21:53,750 --> 00:21:56,720
not be the first language that
comes to mind for machine learning,

348
00:21:56,990 --> 00:22:00,710
it offers several advantages for
specific tasks and situation.

349
00:22:01,430 --> 00:22:05,980
It has the benefits of performance
efficiency, critical for real time

350
00:22:05,980 --> 00:22:08,760
applications and large data sets.

351
00:22:08,940 --> 00:22:11,940
We can take advantage of ghost
concurrency and parallelism to

352
00:22:11,940 --> 00:22:13,770
improve training and inference.

353
00:22:14,385 --> 00:22:17,505
It has the advantage of
performance and concurrency.

354
00:22:17,805 --> 00:22:21,615
Goal line is a compiled language,
which means it generally runs faster

355
00:22:21,615 --> 00:22:23,745
than interpreted language like Python.

356
00:22:24,285 --> 00:22:28,275
This can be significant and advantage when
working with large data sets or complex

357
00:22:28,275 --> 00:22:34,835
models with concurrency in goal language,
it has a built in support of go routines,

358
00:22:35,015 --> 00:22:39,725
making it easier to paralyze tasks, which
can be beneficial in training large models

359
00:22:39,725 --> 00:22:41,945
or handling real-time data processing.

360
00:22:42,445 --> 00:22:47,915
Go Bindings are available for many popular
machine learning libraries like TENS

361
00:22:48,005 --> 00:22:50,555
four Pipe Touch and Sky Psychic Learn.

362
00:22:51,305 --> 00:22:56,345
This enables leveraging existing
code and algorithms with GO Projects.

363
00:22:56,845 --> 00:22:58,915
Golan also has the ease of deployment.

364
00:22:59,005 --> 00:23:04,235
Golang Static Library libraries simplify,
deployment, especially in environments

365
00:23:04,235 --> 00:23:06,575
where containerization, it is used.

366
00:23:06,935 --> 00:23:10,505
This can be particularly useful when
deploying ML models in production.

367
00:23:11,315 --> 00:23:15,645
Also go language has strong
typing, which helps catch errors in

368
00:23:15,645 --> 00:23:19,395
development process, reducing the
risk of runtime in ML applications.

369
00:23:20,205 --> 00:23:23,645
There are also going number of, libraries
for machine learning development.

370
00:23:23,645 --> 00:23:27,725
Go facilitating data transformations
and using ML capabilities

371
00:23:27,725 --> 00:23:28,985
in go programming language.

372
00:23:29,790 --> 00:23:33,630
we have gunna, which is like
a numerical computing library,

373
00:23:33,630 --> 00:23:36,240
similar to mpi, we have goia.

374
00:23:36,480 --> 00:23:40,410
It's a modern, high performance
deep learning library, specifically

375
00:23:40,410 --> 00:23:42,390
designed for go programming language.

376
00:23:42,750 --> 00:23:47,190
It provides a clean and expressive API
for building and training neural networks,

377
00:23:47,190 --> 00:23:50,040
making a suitable choice for ML tasks.

378
00:23:50,540 --> 00:23:55,080
We have also have Go Learn and Go ml,
which are versatile and comprehensive

379
00:23:55,080 --> 00:23:56,310
machine learning libraries.

380
00:23:56,430 --> 00:23:58,890
Again, specifically built
for go programming language.

381
00:23:59,310 --> 00:24:03,060
They provide a wide range of
algorithms and tools for tasks, making

382
00:24:03,060 --> 00:24:07,630
it as a valuable resource for ML
practitioners, who work with Go Language.

383
00:24:08,200 --> 00:24:12,535
We also have Go cv, which is a
computer vision library, specifically

384
00:24:12,535 --> 00:24:16,835
designed, to provide a comprehensive
set of tools and algorithms for image,

385
00:24:16,835 --> 00:24:21,465
video, image and video processing
and analysis and manipulation.

386
00:24:21,965 --> 00:24:23,465
This is the ML part of it.

387
00:24:23,855 --> 00:24:27,425
There are some scenarios where we can
use, go along with SageMaker as well.

388
00:24:28,125 --> 00:24:31,365
the first case is the data handling
and the pre and the post processing.

389
00:24:31,865 --> 00:24:35,385
Although the act, we can go ahead
with an approach of, actually

390
00:24:35,385 --> 00:24:37,215
building the machine learning models.

391
00:24:37,315 --> 00:24:42,135
In the more, wide approached
languages like Python Go can be

392
00:24:42,135 --> 00:24:46,305
used for other tasks that involve
data gathering, pre-processing,

393
00:24:46,965 --> 00:24:48,465
or post processing results.

394
00:24:48,735 --> 00:24:53,205
Go efficiency and speed makes it a
great choice For these tasks that

395
00:24:53,205 --> 00:24:57,240
require high performance, such as
processing large volumes of data or

396
00:24:57,240 --> 00:25:02,655
streaming data, we can use, libraries
like A-W-S-S-D-K Go SageMaker, go.

397
00:25:02,845 --> 00:25:07,165
To interact with SageMaker, APIs
and services from your GO code to

398
00:25:07,165 --> 00:25:11,305
interact with SageMaker to automate
workflow jobs and life cycles.

399
00:25:11,875 --> 00:25:16,705
We can deploy a go based Lambda function
to build an event driven ML life cycle

400
00:25:17,275 --> 00:25:22,855
events, like a new training data,
a new inference data, a new docker

401
00:25:22,855 --> 00:25:24,325
image of the machine learning model.

402
00:25:24,715 --> 00:25:27,355
And once these events
get triggered, we could.

403
00:25:27,460 --> 00:25:31,210
Automate of the training
and deploying the models.

404
00:25:31,710 --> 00:25:35,820
Finally, from our previous
slides, utilize ML libraries or go

405
00:25:35,820 --> 00:25:37,890
bindings with Python ML libraries.

406
00:25:37,890 --> 00:25:38,520
In Go.

407
00:25:38,970 --> 00:25:40,980
We can write your own model container.

408
00:25:40,980 --> 00:25:43,410
I containerize it and create an image.

409
00:25:43,830 --> 00:25:47,280
Use this image in SageMaker to
train and deploy your model.

410
00:25:47,590 --> 00:25:48,310
written and go.

411
00:25:48,810 --> 00:25:54,300
SageMaker enables to overcome the
barrier of language by using no code or

412
00:25:54,300 --> 00:26:00,060
low code solutions, providing a large
range of models, intuitive interfaces to

413
00:26:00,060 --> 00:26:02,280
handle various stages in ML lifecycle.

414
00:26:02,670 --> 00:26:07,860
It also provides ML model Docker image
solution to go beyond and use tools

415
00:26:07,860 --> 00:26:10,140
and languages provided by SageMaker.

416
00:26:10,710 --> 00:26:12,180
For example, we can use GO.

417
00:26:12,680 --> 00:26:17,210
Here are some of the use cases where
ML is used in the FinTech world.

418
00:26:17,710 --> 00:26:18,850
For fraud detection.

419
00:26:18,850 --> 00:26:22,180
We can use XG Boost for
credit risk assessment.

420
00:26:22,210 --> 00:26:27,010
Linear and neural networks can be used
to evaluate real borrower's risk profile.

421
00:26:27,610 --> 00:26:32,350
For customer intelligence, we can use
clustering algorithms to identify customer

422
00:26:32,350 --> 00:26:34,690
categories and tailor financial products.

423
00:26:35,410 --> 00:26:40,460
We can also do portfolio management using,
algorithmic trading using reinforcement

424
00:26:40,460 --> 00:26:42,200
learning and time series forecasting.

425
00:26:42,740 --> 00:26:47,480
As you can see, all these models are
provided and can be built and fine tuned.

426
00:26:47,480 --> 00:26:52,005
Using SageMaker architecture ensures
scalability to handle transactions, of

427
00:26:52,195 --> 00:26:57,245
volumes from millions of customers while
maintaining low latency response necessary

428
00:26:57,265 --> 00:26:59,565
for critical financial operations.

429
00:27:00,065 --> 00:27:05,555
We can achieve this level of performance
with data encryption in transit or at

430
00:27:05,555 --> 00:27:12,005
rest meeting stringent requirements for
FinTech data using A-W-S-V-P-C, we never

431
00:27:12,005 --> 00:27:16,055
have to process sensitive financial
data in public internet and provide

432
00:27:16,055 --> 00:27:18,725
isolation for all the ML workloads.

433
00:27:19,475 --> 00:27:24,635
Again, using AWS I, we can find grain,
have fine grain access to data and models

434
00:27:24,965 --> 00:27:27,305
in aligned with regulatory requirements.

435
00:27:27,805 --> 00:27:32,945
Now, let's discuss about a case where
the Fortune 500 bank utilized SageMaker

436
00:27:33,305 --> 00:27:38,105
for fraud detection, which led to a
false positive redu false positive

437
00:27:38,105 --> 00:27:44,920
rates, reduced by 69% detection
rates, improved by 94%, and with the

438
00:27:45,040 --> 00:27:48,175
leveraging A WSH Makers various features.

439
00:27:48,460 --> 00:27:54,340
The model development improved by 60%
and also, you, a better alert processing

440
00:27:54,340 --> 00:27:59,540
time was reduced, by making it, faster
processing speeds by five times.

441
00:28:00,040 --> 00:28:03,480
So going out of this, presentations,
what are the steps ahead to begin

442
00:28:04,230 --> 00:28:07,590
your journey of using SageMaker
or into machine learning?

443
00:28:08,040 --> 00:28:08,820
First try.

444
00:28:08,850 --> 00:28:11,820
first identify your problem
you're looking to solve with

445
00:28:11,820 --> 00:28:15,030
defined KPIs and success metrics.

446
00:28:15,690 --> 00:28:20,750
Start off leveraging the prebuilt models
and advanced, and advanced forward to use

447
00:28:20,750 --> 00:28:25,820
foundational models and develop your own
models for more fine tuning and control.

448
00:28:26,555 --> 00:28:32,395
Utilize AWS Lambda SageMaker pipelines to
create automated workflow, workflows for

449
00:28:32,395 --> 00:28:34,795
model training, evaluation and deployment.

450
00:28:35,215 --> 00:28:40,615
This ensures consistent quality and
enables rapid interaction attritions

451
00:28:40,615 --> 00:28:44,835
on financial models, SageMaker model
monitor, and CloudWatch logs to

452
00:28:44,835 --> 00:28:49,215
continuously track the model performance
and identify any drifts causing

453
00:28:49,215 --> 00:28:51,645
customer impact or compliance issue.

454
00:28:52,145 --> 00:28:56,045
I had mentioned previously start
with assessment phase, move to

455
00:28:56,045 --> 00:29:00,665
proof of concept and production
implementation later, and then further

456
00:29:00,725 --> 00:29:04,325
move across to cross-functional
implementation across organizations.

457
00:29:05,105 --> 00:29:09,245
There are many resources for
understanding A WSH maker and how

458
00:29:09,305 --> 00:29:11,765
it can be used in various use cases.

459
00:29:12,095 --> 00:29:15,065
In this presentation, we have
seen the case of FinTech.

460
00:29:15,565 --> 00:29:19,075
We can explore the different ML
libraries that are available for grow.

461
00:29:19,575 --> 00:29:23,385
And explore more into the
presentation that I've just made.

462
00:29:24,295 --> 00:29:25,855
one more final conclusion.

463
00:29:25,915 --> 00:29:26,845
Let's make it short.

464
00:29:27,265 --> 00:29:29,395
There are two main points to all this.

465
00:29:29,755 --> 00:29:32,965
If you're new to ml, start
with Python and SageMaker.

466
00:29:32,995 --> 00:29:35,745
It's easier to learn as a
bigger community and all of the

467
00:29:35,745 --> 00:29:37,155
features implemented for you.

468
00:29:37,995 --> 00:29:43,515
If you're more comfortable with Go and as
you advance and you, as you become a pro.

469
00:29:43,810 --> 00:29:46,930
there are more efficient ways and
you want to have a more efficient

470
00:29:46,930 --> 00:29:48,370
way to improve your model.

471
00:29:48,790 --> 00:29:50,080
Welcome to Goal Language.

472
00:29:50,110 --> 00:29:55,100
Here you can write an optimal model
by your own hands, of your knowledge,

473
00:29:55,130 --> 00:29:58,940
and utilize features of SageMaker
for better ML lifecycle management.

474
00:29:59,440 --> 00:29:59,830
Thank you.

