1
00:00:00,120 --> 00:00:01,710
Okay, thanks for.

2
00:00:02,220 --> 00:00:06,160
Joining my session this is
Tackling Alert Fatigue with SLOs.

3
00:00:06,490 --> 00:00:08,020
My name is Mandy Walls.

4
00:00:08,020 --> 00:00:10,629
I am a developer advocate for PagerDuty.

5
00:00:11,140 --> 00:00:14,290
And if you'd like to get in touch
with me, I'm happy to chat anytime.

6
00:00:14,540 --> 00:00:15,320
You can email me.

7
00:00:15,320 --> 00:00:17,090
I'm mWalls@pagerduty.com.

8
00:00:17,150 --> 00:00:20,630
You can reach out on Blue Sky, you can
hit me up on LinkedIn if you'd like.

9
00:00:20,910 --> 00:00:22,500
You can reach out to our whole team.

10
00:00:22,560 --> 00:00:25,740
We're community team@pager.com.

11
00:00:26,169 --> 00:00:31,450
Or you can scan that QR code and join our
community forums if you are a PagerDuty

12
00:00:31,450 --> 00:00:36,670
customer or maybe in the future, you
can join us there and hang out, ask

13
00:00:36,670 --> 00:00:39,050
questions, answer questions share fun.

14
00:00:39,170 --> 00:00:39,890
It's all good stuff.

15
00:00:39,980 --> 00:00:44,570
So that's all I'm really gonna talk
about as far as PagerDuty goes.

16
00:00:44,570 --> 00:00:47,750
So if you have other questions
about our product, please

17
00:00:47,750 --> 00:00:49,190
join us in those other forms.

18
00:00:49,690 --> 00:00:52,790
So I'm gonna be talking about
alert fatigue and using SLOs

19
00:00:52,790 --> 00:00:53,870
to help you deal with that.

20
00:00:53,870 --> 00:00:59,080
So let's let's set a baseline for
what alert fatigue actually is.

21
00:00:59,450 --> 00:01:03,410
A lot of you out there may have actually
experienced alert fatigue, maybe even

22
00:01:03,410 --> 00:01:05,510
if you didn't really know that term.

23
00:01:06,020 --> 00:01:09,110
But basically alert fatigue
can happen when your team is.

24
00:01:09,679 --> 00:01:15,950
Constantly inundated with alerts, events,
alarms, notifications, sirens, claxons,

25
00:01:15,980 --> 00:01:20,600
blinking lights whatever the case might
be from your various monitoring systems.

26
00:01:20,600 --> 00:01:24,440
And we know folks have a lot of different
monitoring systems depending on the

27
00:01:24,440 --> 00:01:27,680
solutions that they need for all of
the services in their environment.

28
00:01:27,680 --> 00:01:31,460
So there's lots of stuff out
there that can be sending you.

29
00:01:31,865 --> 00:01:39,245
Noise notifications, and it doesn't really
matter if the alert is a positive event,

30
00:01:39,245 --> 00:01:44,705
a negative event, high priority, low
priority, false positive, whatever it is.

31
00:01:45,365 --> 00:01:50,825
We're really just dealing with the,
just the sheer volume of alerts,

32
00:01:50,825 --> 00:01:54,950
distractions, things that pull you
out of your work, that contribute

33
00:01:55,020 --> 00:01:57,515
then to what we call alert fatigue.

34
00:01:58,415 --> 00:01:59,495
Unfortunately.

35
00:02:00,410 --> 00:02:04,910
Our entire working life is full
of things that alert at us, right?

36
00:02:05,180 --> 00:02:07,490
Your chat application,
your email, your phone.

37
00:02:07,490 --> 00:02:11,870
Like I turn a lot of notifications
off, but I know lots of folks don't

38
00:02:11,870 --> 00:02:15,430
like, they fear they're gonna miss an
important notification or whatever it is.

39
00:02:15,430 --> 00:02:17,860
But like I'm in 15 different slacks.

40
00:02:17,950 --> 00:02:20,440
None of them get to play any sounds.

41
00:02:20,710 --> 00:02:23,320
So like when I hear someone
else's slack noises.

42
00:02:23,740 --> 00:02:24,580
On a call.

43
00:02:24,580 --> 00:02:26,830
I'm like, do you really
need to know the noise?

44
00:02:26,830 --> 00:02:28,390
Oh my gosh, that would drive me crazy.

45
00:02:28,670 --> 00:02:33,830
So you're already in a place
where we're set up to be

46
00:02:33,830 --> 00:02:36,860
distracted all the time anyway.

47
00:02:36,950 --> 00:02:41,360
And then if your team has production
responsibilities, you have all this

48
00:02:41,360 --> 00:02:46,280
other stuff that then piles on top of
that and it really can just drive people

49
00:02:46,280 --> 00:02:53,230
crazy, when we talk to, folks who are,
who maybe called pre on-call folks who

50
00:02:53,730 --> 00:02:57,840
aren't going to, aren't on call yet,
but are going to be on call, right?

51
00:02:57,970 --> 00:03:00,490
They don't currently have on-call
responsibilities, but it's coming.

52
00:03:00,880 --> 00:03:03,490
That's one of the things they
worry about is you know, how many

53
00:03:03,580 --> 00:03:04,750
alerts are there going to be?

54
00:03:04,750 --> 00:03:08,980
How often am I going to be pulled
out of the other things that I'm

55
00:03:08,980 --> 00:03:13,180
doing, the other work that I'm
doing, or even, my personal time, my

56
00:03:13,180 --> 00:03:15,040
sleep, and all that kind of stuff.

57
00:03:15,520 --> 00:03:19,695
It's the biggest part of,
dealing with being on call.

58
00:03:19,880 --> 00:03:22,490
It's just the volume of
things to come through.

59
00:03:22,570 --> 00:03:27,100
What we want then is to make
sure that the stuff that.

60
00:03:27,505 --> 00:03:32,305
Comes through to notifying human beings
about the production environment is

61
00:03:32,305 --> 00:03:35,575
gonna be the right stuff, going to
the right people at the right time,

62
00:03:35,935 --> 00:03:37,375
and it's going to be important.

63
00:03:37,885 --> 00:03:40,925
And we find that too many teams.

64
00:03:41,510 --> 00:03:45,320
Alert on stuff that doesn't
really meet that criteria.

65
00:03:45,350 --> 00:03:50,270
So there are lots of concerns about
the volume of alerts that teams

66
00:03:50,270 --> 00:03:52,670
get and how they deal with them.

67
00:03:52,670 --> 00:03:57,260
For the rest of this talk, I'm gonna be
referring to alerts as specifically the

68
00:03:57,260 --> 00:04:01,850
real time events that send notifications
to some human responder, right?

69
00:04:01,850 --> 00:04:04,790
So there's lots of
different terms for that.

70
00:04:04,790 --> 00:04:06,680
Different products, call
them different things.

71
00:04:07,085 --> 00:04:10,425
And we're just gonna be referring
to those just generically as alerts,

72
00:04:10,425 --> 00:04:14,115
but specifically the things that
need to be actioned in real time.

73
00:04:14,445 --> 00:04:17,985
They represent some kind of, or
hopefully represent some kind of

74
00:04:17,985 --> 00:04:20,534
issue in your production environments.

75
00:04:20,585 --> 00:04:22,905
We're gonna focus on
that sort of definition.

76
00:04:23,865 --> 00:04:27,705
So there's plenty of signs of alert
fatigue, some of the main ones.

77
00:04:28,205 --> 00:04:31,835
Maybe you've experienced these, maybe
you see them on your team right now.

78
00:04:31,915 --> 00:04:34,045
The big one is going to
be delayed response time.

79
00:04:34,045 --> 00:04:37,045
That's going to impact your
customer experience, right?

80
00:04:37,285 --> 00:04:39,775
Your potentially your bottom line as well.

81
00:04:40,225 --> 00:04:44,155
If there's a lot of alerts, it
takes longer to deal with the volume

82
00:04:44,275 --> 00:04:46,015
and find the real problems, right?

83
00:04:46,015 --> 00:04:48,525
Because you're constantly
shifting through all this stuff

84
00:04:48,525 --> 00:04:50,235
that's maybe junk, not helpful.

85
00:04:50,880 --> 00:04:53,400
Not informational enough
to do anything about.

86
00:04:53,880 --> 00:04:56,400
We also find that folks
just simply miss alerts.

87
00:04:56,430 --> 00:04:59,250
It looks like all of these 10 things
are the same, but it turns out that

88
00:04:59,250 --> 00:05:02,640
the ninth one was just a little bit
different and actually a problem.

89
00:05:03,390 --> 00:05:07,410
But again, if you're drowning under a
huge volume of alerts, it's a lot more

90
00:05:07,410 --> 00:05:08,760
likely you're gonna miss something.

91
00:05:09,120 --> 00:05:11,850
Folks also ignore false
positives for the same reason.

92
00:05:11,880 --> 00:05:13,950
Everything looks the same,
and you just assume that.

93
00:05:14,000 --> 00:05:16,700
It's the same as those other
ones and maybe it wasn't, right.

94
00:05:16,750 --> 00:05:20,130
New alerts may look like they're
not a problem when they are.

95
00:05:21,060 --> 00:05:23,880
All of this then leads to
increased stress and burnout.

96
00:05:23,880 --> 00:05:30,060
We see this across the board for very
busy teams with very rate high rates of

97
00:05:30,600 --> 00:05:35,940
incidents and notifications during their
on-call shifts anyway, and the volume of

98
00:05:35,940 --> 00:05:37,800
alerts is definitely contributor to that.

99
00:05:38,020 --> 00:05:39,570
Based on market conditions.

100
00:05:39,570 --> 00:05:41,970
Sometimes folks will leave,
sometimes they feel like they can't.

101
00:05:42,270 --> 00:05:46,200
There's a bit of that, but at the
same time, like job satisfaction,

102
00:05:46,200 --> 00:05:50,760
stress, burnout, if you are especially
a team manager, team lead, those are

103
00:05:50,760 --> 00:05:52,200
things that should concern you, right?

104
00:05:52,200 --> 00:05:56,670
Like folks are going to their job
quality will decrease if they are

105
00:05:56,670 --> 00:06:00,540
under a lot of stress and maybe not
sleeping, or they're feeling burned out.

106
00:06:01,040 --> 00:06:05,270
So that's something to keep an eye
on overall, then it all contributes

107
00:06:05,270 --> 00:06:06,560
to decreased productivity.

108
00:06:06,960 --> 00:06:08,730
That's probably the
most obvious one, right?

109
00:06:08,730 --> 00:06:11,790
There's lots of alerts coming in,
there's more distractions, there's

110
00:06:11,790 --> 00:06:13,290
less useful work being done.

111
00:06:13,650 --> 00:06:16,530
And while one of our primary
recommendations for folks who are going

112
00:06:16,530 --> 00:06:20,790
to be on call is that they don't carry
a regular workload during their on-call

113
00:06:20,790 --> 00:06:22,500
shift for that week or whatever it is.

114
00:06:23,000 --> 00:06:25,160
They still should be able
to do something, right?

115
00:06:25,160 --> 00:06:29,930
We don't want them constantly inundated,
just buried under a flood of alerts 24

116
00:06:29,930 --> 00:06:33,715
by seven while they're on call, because
there's always work that can be done as

117
00:06:33,715 --> 00:06:37,405
far as like documentation or updating
monitors and those kinds of things.

118
00:06:37,885 --> 00:06:43,825
But overall, what we end up seeing on
teams that have too many alerts is a great

119
00:06:43,825 --> 00:06:46,315
decrease in their productivity overall.

120
00:06:46,315 --> 00:06:47,965
Then that gives you.

121
00:06:48,675 --> 00:06:52,455
The less ability to learn from all
the stuff that is coming in, right?

122
00:06:52,485 --> 00:06:55,335
You have lower quality, instant
reviews and documentation.

123
00:06:55,575 --> 00:06:59,715
'cause there simply isn't
enough time to deal with all

124
00:06:59,715 --> 00:07:01,155
of the stuff that's coming in.

125
00:07:01,245 --> 00:07:04,935
Sift through it, find the things are
important, find the things that you

126
00:07:04,935 --> 00:07:10,335
haven't seen before, document those update
monitors, whatever the case might be.

127
00:07:10,335 --> 00:07:14,775
They folks just never catch up
to do that kind of work that.

128
00:07:15,090 --> 00:07:18,480
Is the stuff that we really
want to get out of having

129
00:07:18,480 --> 00:07:20,040
incidents in the first place.

130
00:07:20,370 --> 00:07:24,990
We bring these alerts into our systems
because there is something wrong and

131
00:07:25,020 --> 00:07:30,330
we want to get to a place where we
can shore up the systems so that we

132
00:07:30,330 --> 00:07:34,380
don't see those problems the same way
in another time when we run outta time

133
00:07:34,380 --> 00:07:37,960
to do that if folks are too buried.

134
00:07:38,590 --> 00:07:41,950
So some basic things we can
be doing to improve your alert

135
00:07:41,950 --> 00:07:43,550
volume and overall alert health.

136
00:07:44,050 --> 00:07:47,950
Are going to help us before
we can get to using SLOs.

137
00:07:48,730 --> 00:07:52,000
Even if you don't have access to SLOs,
these things are gonna help you, right?

138
00:07:52,000 --> 00:07:55,270
So if you haven't gotten to a place
yet where you're going to deploy those,

139
00:07:55,270 --> 00:07:59,260
or think about SLOs, this stuff is
still gonna help you if you feel like

140
00:07:59,260 --> 00:08:00,700
you have too many alerts coming in.

141
00:08:01,000 --> 00:08:05,130
So we'll set a good baseline for
things all teams can really be

142
00:08:05,130 --> 00:08:08,520
doing or thinking about to improve
their overall alert volume.

143
00:08:09,480 --> 00:08:10,590
The first one's a big one.

144
00:08:10,590 --> 00:08:14,280
Like just get rid of something that's
alerting you when something succeeds.

145
00:08:14,700 --> 00:08:19,890
There are very few cases where
this additional noise is helpful.

146
00:08:19,890 --> 00:08:21,930
Like I, we're all dopa fanatics.

147
00:08:21,930 --> 00:08:25,890
But get your dopamine hit on Instagram
or social media or somewhere else

148
00:08:25,940 --> 00:08:29,510
don't have your systems like constantly
pinging you with success, right?

149
00:08:29,900 --> 00:08:34,070
Unless you absolutely need to know in
real time that something succeeded.

150
00:08:34,070 --> 00:08:37,730
Maybe you've got like a long running
pipeline, or you've got some ETLs

151
00:08:37,730 --> 00:08:39,230
that are important or whatever.

152
00:08:39,230 --> 00:08:42,850
Some reports or something that runs
none of that stuff really should

153
00:08:42,850 --> 00:08:45,970
come to your real time notifications.

154
00:08:45,970 --> 00:08:47,880
And by that comes to your phone pages.

155
00:08:47,880 --> 00:08:50,220
You in the middle of the night,
sends a text message, like

156
00:08:50,220 --> 00:08:51,510
that kind of immediate stuff.

157
00:08:51,990 --> 00:08:57,689
Things that go to chat channels that can
be responded to asynchronously that are

158
00:08:58,000 --> 00:09:01,270
deferred for not necessarily real time.

159
00:09:01,540 --> 00:09:02,080
That's great.

160
00:09:02,080 --> 00:09:04,510
That's a great place to put
those kinds of notifications.

161
00:09:04,840 --> 00:09:07,360
Where they don't need to be
coming to is basically what you

162
00:09:07,360 --> 00:09:08,740
think of as your pager, right?

163
00:09:08,920 --> 00:09:10,210
For your emergencies.

164
00:09:10,750 --> 00:09:13,870
Number two, get rid of
anything that isn't actionable.

165
00:09:13,970 --> 00:09:18,560
I know there's stuff out there that
I was assisted men for a long time.

166
00:09:18,560 --> 00:09:19,670
There's chronic issues.

167
00:09:20,000 --> 00:09:25,280
Let's sneak into the production
systems, and if it's stuff that happens

168
00:09:25,280 --> 00:09:29,240
all the time and can't be fixed and
the people that you're notifying

169
00:09:29,330 --> 00:09:33,410
about it can't do anything about it
anyway, don't notify them about it.

170
00:09:33,800 --> 00:09:36,830
Track it as a metric to say,
oh yeah, last week this thing

171
00:09:36,830 --> 00:09:39,080
happened 17 times this week.

172
00:09:39,170 --> 00:09:41,450
It's looking like it's
gonna happen 25 times.

173
00:09:41,950 --> 00:09:47,080
Use that as leverage to get things
fixed with your product managers.

174
00:09:47,160 --> 00:09:51,630
Have that discussion, especially if you're
the manager or team lead advocate for

175
00:09:51,630 --> 00:09:56,010
time to fix these chronic issues, but
don't notify people who can't fix them.

176
00:09:56,110 --> 00:09:59,860
Part of that too is making sure
alerts go to the right people.

177
00:09:59,860 --> 00:10:03,250
So if you have a bunch of stuff that
comes into your team that's like.

178
00:10:03,880 --> 00:10:05,739
This isn't really our responsibility.

179
00:10:05,739 --> 00:10:07,510
We don't have access to fix this stuff.

180
00:10:07,510 --> 00:10:09,639
Maybe it should go to
somebody else, have a talk.

181
00:10:10,104 --> 00:10:12,894
With that other team that should
be getting those notifications.

182
00:10:13,224 --> 00:10:17,334
I know that's challenging in lots of
environments because you've got like lots

183
00:10:17,334 --> 00:10:21,144
of different platform teams and someone's
responsible for the cloud account.

184
00:10:21,414 --> 00:10:24,174
Maybe somebody else is running
your Kubernetes and then you

185
00:10:24,174 --> 00:10:26,604
have your application teams
and they know about the code.

186
00:10:26,844 --> 00:10:30,084
There's a lot of division of labor
and you wanna make sure all those

187
00:10:30,084 --> 00:10:31,584
alerts go to the right places.

188
00:10:31,674 --> 00:10:34,574
But if stuff is coming into your
team can't do anything about,

189
00:10:34,814 --> 00:10:36,164
you don't want it, turn it off.

190
00:10:36,664 --> 00:10:39,634
Anything else that's low urgency
that's coming in is just an

191
00:10:39,634 --> 00:10:41,284
informational notification.

192
00:10:41,284 --> 00:10:44,914
Shouldn't be going to real
time notifications to humans

193
00:10:45,724 --> 00:10:47,374
again, somewhere else.

194
00:10:47,404 --> 00:10:49,924
Put it to Slack or Teams or whatever.

195
00:10:50,494 --> 00:10:53,914
Something that's asynchronous that's
just there for information, right?

196
00:10:54,034 --> 00:10:57,844
Have a channel for your bots
on your chat and pipe it there.

197
00:10:58,144 --> 00:11:02,104
Not to a place that is going to notify
someone at three o'clock in the morning.

198
00:11:02,164 --> 00:11:02,524
Right?

199
00:11:03,024 --> 00:11:04,074
Low urgency.

200
00:11:04,254 --> 00:11:08,334
Maybe it comes into your regular
system, but it doesn't notify people.

201
00:11:08,334 --> 00:11:12,699
You can set in PagerDuty in particular,
you can set the notifications to not.

202
00:11:13,454 --> 00:11:18,064
Notify people via SMS or app
push for low urgency alerts and

203
00:11:18,064 --> 00:11:19,684
they just will hang out there.

204
00:11:19,954 --> 00:11:23,674
You could have a goalie or
triage time in the morning to

205
00:11:23,674 --> 00:11:25,954
say, okay, nine 30 we're here.

206
00:11:25,954 --> 00:11:28,534
We're gonna go through everything
that came in overnight that was low

207
00:11:28,534 --> 00:11:30,694
urgency and deal with it that way.

208
00:11:30,894 --> 00:11:34,194
You can also push things maybe
to a ticketing system instead

209
00:11:34,224 --> 00:11:35,994
of your real time notifications.

210
00:11:36,264 --> 00:11:40,374
If it's stuff that can really be
deferred and be decided on later, you

211
00:11:40,374 --> 00:11:41,694
can be super aggressive with these.

212
00:11:41,694 --> 00:11:46,224
I know everyone thinks their
baby is beautiful, but if you put

213
00:11:46,224 --> 00:11:49,284
stuff out into production and your
users aren't engaging with it.

214
00:11:49,334 --> 00:11:52,514
Maybe you can think about lowering
the urgency of your response to those

215
00:11:52,514 --> 00:11:56,834
parts of the service and reevaluating
those on a regular basis to say, has

216
00:11:56,834 --> 00:12:01,244
this reached a threshold where we need
to up it to 24 by seven urgency, high

217
00:12:01,244 --> 00:12:02,834
urgency alerts, or things like that.

218
00:12:03,334 --> 00:12:05,674
Be aggressive with all of those, right?

219
00:12:06,469 --> 00:12:09,409
And then finally delete
or suspend broken alerts.

220
00:12:09,409 --> 00:12:14,129
Like it happens code changes somewhere
and there's, a mess and like the monitors

221
00:12:14,129 --> 00:12:17,159
don't quite work, but you haven't
had time to go back and really dis

222
00:12:17,339 --> 00:12:19,529
distinguish like why they don't work.

223
00:12:20,029 --> 00:12:23,839
Good practice means all the alerts get
updated when the code changes, but if

224
00:12:23,839 --> 00:12:25,489
you've got janky stuff sitting around.

225
00:12:25,804 --> 00:12:29,404
You might have some of these old alerts
that aren't being super helpful, don't

226
00:12:29,404 --> 00:12:34,364
necessarily reflect the state of the
current code and, just dump them.

227
00:12:34,604 --> 00:12:38,354
If you wanna save them for a while just to
make sure that maybe you can go back and

228
00:12:38,354 --> 00:12:40,454
fix them, that's fine, but disable them.

229
00:12:40,454 --> 00:12:44,294
You don't have to turn them on to
keep them ping people in the middle

230
00:12:44,294 --> 00:12:45,434
of the night if they don't need to.

231
00:12:45,914 --> 00:12:48,194
What you're doing with
this whole practice is.

232
00:12:49,139 --> 00:12:52,499
Helping with alert fatigue by
creating mental space, right?

233
00:12:52,769 --> 00:12:57,334
You're increasing the cognitive capacity
of your team to deal with the things

234
00:12:57,334 --> 00:13:01,294
that are real and urgent and impactful
by getting rid of all the stuff that

235
00:13:01,294 --> 00:13:02,834
doesn't meet that barrier, right?

236
00:13:03,134 --> 00:13:05,564
Everything that is extraneous.

237
00:13:05,624 --> 00:13:08,504
So if the alert isn't real, dump it.

238
00:13:08,774 --> 00:13:11,264
If it's not urgent, create a record.

239
00:13:11,264 --> 00:13:12,074
Make a ticket.

240
00:13:12,554 --> 00:13:15,314
When you're left with just
the stuff you care about, then

241
00:13:15,314 --> 00:13:16,664
you can be more productive.

242
00:13:16,949 --> 00:13:19,889
So how can you go about doing
this if you feel like your team

243
00:13:19,889 --> 00:13:24,449
is really drowning under a lot of
alerts that aren't useful to you?

244
00:13:24,949 --> 00:13:28,459
Try and declare an alert, cleanup,
sprint, whatever it takes.

245
00:13:28,459 --> 00:13:32,069
You are the impacts of alert
fatigue are real, right?

246
00:13:32,069 --> 00:13:33,389
We talked about them in the beginning.

247
00:13:33,509 --> 00:13:38,369
They are going to be impactful for the
reliability of your services and systems.

248
00:13:38,804 --> 00:13:44,234
So if you are in danger of experiencing
alert fatigue, it is worth investing the

249
00:13:44,234 --> 00:13:46,454
time, and it is an investment, right?

250
00:13:46,454 --> 00:13:52,964
It does take resources to fix the things
that are going to impact your reliability.

251
00:13:53,384 --> 00:13:56,414
That means you cannot be
missing important alerts.

252
00:13:56,444 --> 00:13:59,054
You cannot be ignoring the
things that are coming in.

253
00:13:59,564 --> 00:14:02,324
But if you have too many alerts.

254
00:14:02,729 --> 00:14:03,719
You are gonna have to deal with that.

255
00:14:03,719 --> 00:14:08,009
So give your team some time to take
stock of the alerts you are getting if

256
00:14:08,009 --> 00:14:09,809
you feel like you have too many, right?

257
00:14:10,139 --> 00:14:13,349
If you start with, say, the 10
most noisy alerts, you walk through

258
00:14:13,349 --> 00:14:18,479
the list by frequency, you deal
with the noisiest first and a

259
00:14:18,479 --> 00:14:20,009
kind of snowball effect, right?

260
00:14:20,249 --> 00:14:24,389
So we deal with teams that have tens
of thousands of alerts in a week, and

261
00:14:24,389 --> 00:14:28,499
like they use AIOps and some of the
other tools to group things together

262
00:14:28,499 --> 00:14:30,119
into a limited number of incidents.

263
00:14:30,119 --> 00:14:31,979
But like taking stock of all those.

264
00:14:32,264 --> 00:14:35,534
Alerts and making sure that
they are meeting your criteria

265
00:14:35,774 --> 00:14:36,974
is going to be super helpful.

266
00:14:36,974 --> 00:14:40,694
And just going through them
list by list for every alert.

267
00:14:41,144 --> 00:14:42,824
Marie Kondo, that thing, right?

268
00:14:42,824 --> 00:14:43,874
Is it actionable?

269
00:14:44,099 --> 00:14:47,549
If not, we're not gonna alert like
we will maybe track it as a metric,

270
00:14:47,759 --> 00:14:50,669
leave it in the dashboard, but it
doesn't have to go to somebody's

271
00:14:50,669 --> 00:14:51,959
pager in the middle of the night.

272
00:14:52,709 --> 00:14:53,639
Is it urgent?

273
00:14:53,819 --> 00:14:55,379
If yes, fine.

274
00:14:55,409 --> 00:14:56,789
Get it in front of a responder.

275
00:14:57,059 --> 00:14:59,249
If not, log it to future work.

276
00:14:59,249 --> 00:15:00,629
Send it to the Slack channel.

277
00:15:00,869 --> 00:15:05,159
Create a low urgency queue, whatever
it takes, but get it out of the

278
00:15:05,159 --> 00:15:07,889
flow that's gonna wake someone
up in the middle of the night.

279
00:15:08,789 --> 00:15:12,689
Is it telling you something important
about the performance of the service?

280
00:15:13,394 --> 00:15:15,734
That is a big one, and we're
gonna talk about that more.

281
00:15:16,634 --> 00:15:21,644
If it doesn't turn it off, set it
aside for discussion, maybe revisit

282
00:15:21,644 --> 00:15:23,804
it if it might be helpful, right?

283
00:15:24,194 --> 00:15:27,734
You want to take a good look
at what you're alerting on and

284
00:15:27,734 --> 00:15:29,804
how you're alerting about it.

285
00:15:30,284 --> 00:15:33,824
If you have a bunch of baseline
alerts that come in all the time.

286
00:15:34,364 --> 00:15:37,364
It's maybe time to have another
sprint about best practices

287
00:15:37,364 --> 00:15:38,594
for your services, right?

288
00:15:38,594 --> 00:15:43,524
Stuff like running out of capacity have
a discussion about how to set up the

289
00:15:43,524 --> 00:15:50,054
system so they rightsize automatically
how to see the trends that would lead to.

290
00:15:50,729 --> 00:15:52,349
Services being outta capacity.

291
00:15:52,589 --> 00:15:55,049
If you're running in Kubernetes
environments, all that

292
00:15:55,049 --> 00:15:56,309
stuff is available, right?

293
00:15:56,309 --> 00:15:59,489
You just need to be able to make
sure you're deploying it correctly.

294
00:15:59,799 --> 00:16:02,169
Things like memory leaks,
other chronic issues.

295
00:16:02,859 --> 00:16:05,409
You might have to have a discussion
with your product managers.

296
00:16:05,559 --> 00:16:08,679
Put that stuff in the backlog
for engineering investigation

297
00:16:09,099 --> 00:16:10,929
to get it fixed, right?

298
00:16:10,989 --> 00:16:15,309
I used to work in Java applications
and like memory leaks are par for

299
00:16:15,309 --> 00:16:19,359
the course at that time, deal with
those in a more productive way.

300
00:16:19,419 --> 00:16:23,069
Just don't it's easy to get to
feel like you're not gonna make any

301
00:16:23,069 --> 00:16:26,669
progress with these if they have
been alerting for a very long time.

302
00:16:27,479 --> 00:16:30,209
But you can push back on your product
managers and have that discussion.

303
00:16:30,539 --> 00:16:33,539
'cause again, if it's gonna
impact your reliability, sometimes

304
00:16:33,539 --> 00:16:34,829
these things get hidden, right?

305
00:16:34,829 --> 00:16:36,959
If you've got a service that has.

306
00:16:37,734 --> 00:16:41,394
A thousand instances across however
many availability zones, like it

307
00:16:41,394 --> 00:16:46,014
is easy to hide that one of them is
restarting for a memory leak every so

308
00:16:46,014 --> 00:16:48,404
often, but can still be a chronic issue.

309
00:16:48,904 --> 00:16:52,754
You may have a lot of legacy alerts
that no longer make sense, right?

310
00:16:52,754 --> 00:16:56,174
Depending on the mix
of newer versus legacy.

311
00:16:56,469 --> 00:16:57,819
Services in your environment.

312
00:16:57,819 --> 00:17:00,459
You may still have stuff that's
like alerting on disc full and

313
00:17:00,459 --> 00:17:03,759
things like that really take a
hard look at whether the or not.

314
00:17:03,759 --> 00:17:05,349
Those things are super helpful to you.

315
00:17:05,849 --> 00:17:07,049
They feel comfortable, right?

316
00:17:07,049 --> 00:17:11,379
We, they're well understood if nothing
else but they may not be anything

317
00:17:11,379 --> 00:17:15,579
that actually gives you a hint
of the performance of the system.

318
00:17:16,179 --> 00:17:19,629
There may be some that are there that
might be helpful, but there's gonna

319
00:17:19,629 --> 00:17:21,279
be a whole lot more that maybe aren't.

320
00:17:21,789 --> 00:17:25,989
So once you've called these alerts,
take a hard look at the ones that

321
00:17:25,989 --> 00:17:27,549
you've determined are important.

322
00:17:28,179 --> 00:17:33,549
Part of the problem we see folks deal with
is a lot of their alerts look the same.

323
00:17:34,329 --> 00:17:37,689
You look through a list of incidents
that have been created, you cannot

324
00:17:37,689 --> 00:17:41,439
tell from one incident to the next
what exactly why they're different.

325
00:17:41,509 --> 00:17:44,089
Because the first couple of
lines are all exactly the same.

326
00:17:44,915 --> 00:17:47,524
Are your alerts giving
you enough information?

327
00:17:47,854 --> 00:17:53,195
Can responders tell that something
important is going on from the

328
00:17:53,195 --> 00:17:54,874
first initial line of the report?

329
00:17:55,235 --> 00:17:58,475
This is easier to do in some
monitoring systems than others.

330
00:17:58,834 --> 00:18:00,215
Some of them are very verbose.

331
00:18:00,215 --> 00:18:01,894
They give you a lot of stuff, but.

332
00:18:02,584 --> 00:18:06,034
The stuff that's important and indicates
what's going on, if something's

333
00:18:06,034 --> 00:18:10,894
hidden or hard to find right, or hard
to surface in a way that makes it

334
00:18:11,114 --> 00:18:13,544
immediately apparent to your responders.

335
00:18:13,994 --> 00:18:17,714
And we get a lot of questions about
this because folks are relying on

336
00:18:17,714 --> 00:18:22,604
SMS in particular, and the questions
always are can we add more to the SMS?

337
00:18:22,604 --> 00:18:24,225
Can we add links to the SMS?

338
00:18:24,225 --> 00:18:24,824
Can we do this?

339
00:18:24,824 --> 00:18:25,334
Can we do that?

340
00:18:25,834 --> 00:18:30,334
The long answer is a whole other talk,
but the short answer unfortunately is no.

341
00:18:30,684 --> 00:18:37,044
Because SMS is governed basically by
each country in the world, like everybody

342
00:18:37,044 --> 00:18:39,054
has their own different regulations.

343
00:18:39,354 --> 00:18:41,274
Some places you're not
allowed to send a link at all.

344
00:18:42,264 --> 00:18:43,764
And the US we get a lot of spam.

345
00:18:43,764 --> 00:18:47,244
Suddenly there's other things that
are allowed and disallowed and

346
00:18:47,244 --> 00:18:48,624
there's all these rules about it.

347
00:18:48,624 --> 00:18:50,574
So we've dialed back our SMS.

348
00:18:51,039 --> 00:18:53,969
On our delivery systems to
be lowest common denominator.

349
00:18:54,299 --> 00:18:57,429
So you're very limited in
what things can go through.

350
00:18:57,459 --> 00:18:58,599
'cause we're a global company.

351
00:18:58,599 --> 00:19:00,069
We have customers all over the world.

352
00:19:00,069 --> 00:19:05,079
To avoid those kinds of undeliver ability
errors, we've cut things way back.

353
00:19:05,109 --> 00:19:09,909
So thinking about how your
notifications will get to your humans.

354
00:19:10,614 --> 00:19:14,424
Is also part of making sure that
your alerts are working correctly.

355
00:19:14,844 --> 00:19:17,124
The other part, like I mentioned
earlier, is making sure they're

356
00:19:17,124 --> 00:19:18,594
getting to the right team.

357
00:19:19,094 --> 00:19:22,634
If you have division of
responsibility across the

358
00:19:22,634 --> 00:19:24,314
different layers of your platforms.

359
00:19:24,704 --> 00:19:28,544
Also making sure that all those
actionable things are going to the

360
00:19:28,544 --> 00:19:30,674
team that can actually do the action.

361
00:19:31,094 --> 00:19:32,924
Also super important, right?

362
00:19:33,164 --> 00:19:36,464
If your application development
team knows nothing about the

363
00:19:36,464 --> 00:19:38,444
Kubernetes other than it packages up.

364
00:19:38,904 --> 00:19:41,574
Their CICD tool packages
things up into a container.

365
00:19:41,934 --> 00:19:45,474
You can't ask them to respond to
errors in the Kubernetes layer.

366
00:19:45,774 --> 00:19:49,344
Same with the virtualization
layer, whatever it is, right?

367
00:19:49,344 --> 00:19:53,064
However many different teams you
might have responsible for things,

368
00:19:53,364 --> 00:19:57,954
making sure that when that alert comes
in, the responders that receive it,

369
00:19:58,254 --> 00:20:02,814
have the access and knowledge to fix
it, is part of all of this as well.

370
00:20:02,964 --> 00:20:05,604
And if you need to ship
things off to another team.

371
00:20:05,949 --> 00:20:11,079
That's also helpful if you are in a
more sort of advisory SRE kind of role,

372
00:20:11,709 --> 00:20:16,269
putting together good guidelines about
that sort of separation of duties and

373
00:20:16,269 --> 00:20:20,019
separation of alerts and what alerts
should look like, should be part of

374
00:20:20,019 --> 00:20:21,489
the things you provide for folks.

375
00:20:21,819 --> 00:20:22,899
It's part of your.

376
00:20:23,184 --> 00:20:24,294
Golden path, right?

377
00:20:24,294 --> 00:20:28,524
It's part of your best practices, your
recommendations for getting things into

378
00:20:28,524 --> 00:20:32,754
production, what these alerts should look
like, where they should go, what kind of

379
00:20:32,754 --> 00:20:34,554
information they should convey, right?

380
00:20:34,644 --> 00:20:38,934
It's all part of that advisory role,
if that is how your SRE team is set up.

381
00:20:38,934 --> 00:20:39,804
So there's a lot of.

382
00:20:40,194 --> 00:20:42,564
Things to think about there
and things to work on.

383
00:20:42,864 --> 00:20:46,554
If you are coming into this from a place
where you have a lot of noise in your

384
00:20:46,554 --> 00:20:50,784
alerts, maybe a lot of legacy systems
to deal with that maybe are being

385
00:20:51,094 --> 00:20:53,284
modernized or reworked or whatever.

386
00:20:53,284 --> 00:20:57,004
So there's a lot of, it feels
like it should be super simple and

387
00:20:57,004 --> 00:20:58,714
unfortunately it never is, right?

388
00:20:58,714 --> 00:21:01,594
Because this systems themselves are
complex, making sure they're running

389
00:21:01,594 --> 00:21:06,694
correctly is also going to be complex,
and that's where SLOs come into the play.

390
00:21:06,774 --> 00:21:10,524
Once we've cleaned up our junk alerts,
it's time to talk about improving

391
00:21:10,524 --> 00:21:12,414
the alerts that aren't junk, right?

392
00:21:12,744 --> 00:21:17,364
So if you're not currently working with
SLOs and SLIs, or maybe you're planning on

393
00:21:17,364 --> 00:21:21,654
looking at those to improve your position,
your reliability, there's lots of.

394
00:21:22,129 --> 00:21:22,849
Resources.

395
00:21:22,849 --> 00:21:27,059
I'll mention a couple at the end
that you can take a look at, but SLOs

396
00:21:27,059 --> 00:21:31,229
essentially are the team goals that
you set for your production metrics.

397
00:21:31,559 --> 00:21:35,849
They are usually inside any
contractually obligated SLA, so like SLA.

398
00:21:36,349 --> 00:21:37,849
Often has lawyers involved.

399
00:21:37,849 --> 00:21:40,189
There's contracts involved, like
that's the legal part, right?

400
00:21:40,339 --> 00:21:43,789
Like service level agreements between
your company and your customers.

401
00:21:44,149 --> 00:21:47,269
Your SLIs, your SLOs sit inside of that.

402
00:21:47,269 --> 00:21:52,279
And what I mean by that is if you
are a promising 99.9% reliability

403
00:21:52,279 --> 00:21:57,779
on some system, you are setting your
personal goals to like 99.99, right?

404
00:21:57,839 --> 00:21:58,649
Whatever that is.

405
00:21:59,149 --> 00:22:02,899
And they may be focused on certain
parts of your customer workflow.

406
00:22:03,149 --> 00:22:06,389
You may have other components
that are fall outside of SLA

407
00:22:06,389 --> 00:22:08,969
because maybe they are not GA yet.

408
00:22:08,999 --> 00:22:10,529
Maybe they aren't used as much.

409
00:22:10,529 --> 00:22:13,559
Maybe they're premium features and
there's other things going on there

410
00:22:13,799 --> 00:22:14,969
that's a whole other discussion.

411
00:22:15,359 --> 00:22:19,859
But SLOs, like I said, your are your
team's goals for your production metrics.

412
00:22:20,359 --> 00:22:24,109
They are the level of
reliability your team commits

413
00:22:24,109 --> 00:22:25,639
to delivering for that service.

414
00:22:25,999 --> 00:22:28,759
They can encompass anything
that you think is important.

415
00:22:28,759 --> 00:22:31,999
Anything that your customers
determine is important, right?

416
00:22:32,499 --> 00:22:37,329
Service, speed, payload size, any
metrics that matter to your users,

417
00:22:37,419 --> 00:22:41,019
and they may be different for
every component in the ecosystem.

418
00:22:41,379 --> 00:22:46,704
There's no O universal best set of
indicators to set your objectives to.

419
00:22:47,439 --> 00:22:51,159
It's going to vary depending on
what your users do, what things

420
00:22:51,159 --> 00:22:55,839
they interact with, what they expect
performance wise of your services.

421
00:22:56,049 --> 00:23:00,939
So they can be very individualized
and in that way take some work and

422
00:23:00,969 --> 00:23:02,679
may require some tooling, right?

423
00:23:02,929 --> 00:23:05,299
Others, you can back into
a system data, right?

424
00:23:05,299 --> 00:23:08,214
If you have an application
that slows down.

425
00:23:08,914 --> 00:23:13,534
When some system resource threshold
is met, you can monitor that

426
00:23:13,534 --> 00:23:17,829
system resource as a proxy for the
behavior of that service, right?

427
00:23:18,759 --> 00:23:24,159
Some of your SLOs are going to be
around metrics that are easy to count,

428
00:23:24,249 --> 00:23:26,469
but also transient, noisy, right?

429
00:23:26,739 --> 00:23:32,029
Especially if you have a system that has a
very large number of hits or interactions.

430
00:23:32,884 --> 00:23:36,484
One or two little errors pop
up occasionally anyway, because

431
00:23:36,484 --> 00:23:39,304
that's just the nature of
packet switch systems, right?

432
00:23:39,754 --> 00:23:43,234
So if you've ever been called by an
executive because they couldn't access

433
00:23:43,234 --> 00:23:47,024
a service that's working for someone
else, and you're like all our monitors

434
00:23:47,024 --> 00:23:51,224
say it's fine, and I have no idea why
it's not working for your, from you, for

435
00:23:51,224 --> 00:23:55,064
you from your pineapple farm in Hawaii
when everything's hosted in Dulles.

436
00:23:55,414 --> 00:23:57,724
The Internet's in the way,
there's ocean in the way.

437
00:23:57,784 --> 00:24:01,994
There's lots of things in the middle that
are, preventing that, that performance.

438
00:24:02,094 --> 00:24:07,544
You, the magic of SLOs though is that
the, it allows you to account for

439
00:24:07,544 --> 00:24:12,104
some percentage of those errors before
blowing up everybody's phone, right?

440
00:24:12,854 --> 00:24:15,094
You have everybody gets 500 errors.

441
00:24:15,814 --> 00:24:17,554
You wanna alert on them at some point.

442
00:24:17,974 --> 00:24:19,114
Do you have.

443
00:24:19,614 --> 00:24:22,044
Do you have to alert on
every single 500 error?

444
00:24:22,544 --> 00:24:23,864
That's the question, right?

445
00:24:24,494 --> 00:24:30,164
If you have a site with a very large
capacity, very large traffic numbers,

446
00:24:30,664 --> 00:24:35,074
one 500 error out of 10,000 hits,
probably not something to worry about

447
00:24:35,074 --> 00:24:36,694
unless it keeps recurring right?

448
00:24:36,724 --> 00:24:37,984
In a small timeframe.

449
00:24:38,404 --> 00:24:39,604
So when you're combating.

450
00:24:40,324 --> 00:24:41,224
Alert fatigue.

451
00:24:41,614 --> 00:24:43,774
SLOs are gonna help you with two things.

452
00:24:43,864 --> 00:24:47,314
They're gonna help you determine urgency,
and they're gonna help you reduce the

453
00:24:47,314 --> 00:24:52,024
overall volume of notifications and alerts
that are getting to your human responders.

454
00:24:52,524 --> 00:24:56,394
Having these SLOs attached to your key
metrics means that you know how many

455
00:24:56,394 --> 00:25:01,794
failures you have before they're, you can
really say, oh yes, this is a problem.

456
00:25:02,094 --> 00:25:06,534
There's no need in a lot of cases
to alert on every single failure.

457
00:25:07,029 --> 00:25:10,359
You don't wanna burn through your
entire air budget on a single instance

458
00:25:10,359 --> 00:25:16,059
of alert, but you be able to better
manage the importance of a single alert

459
00:25:16,089 --> 00:25:22,179
versus a group of them in a given time
period, one every couple of hours versus.

460
00:25:22,644 --> 00:25:24,834
10 in the next five minutes, right?

461
00:25:24,864 --> 00:25:31,784
Like those are extreme cases, but example
for, how varied the situation can be.

462
00:25:32,144 --> 00:25:37,634
So if your SLO is like a 95% success
on some key metric, you can alert your

463
00:25:37,634 --> 00:25:43,514
humans starting maybe at 97% instead
of 100% success minus one error, right?

464
00:25:44,084 --> 00:25:46,994
That's going to lower the volume
of overall alerts your team is

465
00:25:46,994 --> 00:25:49,814
receiving on these key indicators.

466
00:25:50,314 --> 00:25:51,844
So what an error budget looks like.

467
00:25:52,384 --> 00:25:54,244
It's really your wiggle room, right?

468
00:25:54,744 --> 00:25:58,674
There are the places where you can
be a little bit unreliable, right?

469
00:25:59,004 --> 00:26:01,494
And your customers are
okay with it, right?

470
00:26:01,884 --> 00:26:04,194
Some of these are gonna be a
discussion you need to have

471
00:26:04,194 --> 00:26:05,304
with your product managers.

472
00:26:05,334 --> 00:26:10,074
'cause they're gonna know, hopefully,
what the data looks like for how

473
00:26:10,414 --> 00:26:15,244
customers engage with your systems,
where customers abandon stuff, right?

474
00:26:15,749 --> 00:26:19,949
But the error budget is where the
magic of SLOs meets the annoyance

475
00:26:19,979 --> 00:26:21,599
of realtime alerting, right?

476
00:26:22,559 --> 00:26:26,909
Once you decide on your SLOs for a
particular service, you can determine how

477
00:26:26,909 --> 00:26:31,679
much of your error budget you wanna spend
on any given series of errors, right?

478
00:26:32,009 --> 00:26:37,649
A single isolated error on a very
busy service shouldn't wake anybody

479
00:26:37,649 --> 00:26:38,819
up in the middle of the night.

480
00:26:39,399 --> 00:26:41,829
You want a cluster of them
before you wake someone up,

481
00:26:42,159 --> 00:26:43,719
you might wanna note it, right?

482
00:26:43,749 --> 00:26:48,339
And it comes in maybe as low urgency until
there's a whole bunch of 'em, and then

483
00:26:48,339 --> 00:26:50,679
it jumps to escalated to a high urgency.

484
00:26:50,919 --> 00:26:55,149
So you have some record that it
happened, but you don't necessarily need

485
00:26:55,149 --> 00:26:56,949
to wake some up at 3:00 AM about it.

486
00:26:57,449 --> 00:27:00,059
So your users really are
the point on this, right?

487
00:27:00,059 --> 00:27:03,689
The entire reason we have all these alerts
and notifications and all this stuff

488
00:27:03,689 --> 00:27:05,699
that comes in is because we have users.

489
00:27:06,254 --> 00:27:09,494
They wanna serve them a good
experience, so they keep coming

490
00:27:09,494 --> 00:27:12,644
back and engaging with their stuff
and maybe paying us for it, right?

491
00:27:13,154 --> 00:27:16,424
So we're not going through this
exercise simply because it's fun.

492
00:27:17,184 --> 00:27:22,344
We're saving our human team
members some sanity, right?

493
00:27:22,434 --> 00:27:26,784
By cleaning up all the alerts and
giving our customers, our users

494
00:27:26,784 --> 00:27:30,024
the best experience by focusing
on what they care about, right?

495
00:27:30,104 --> 00:27:31,454
You might have to talk to users.

496
00:27:31,454 --> 00:27:33,464
You definitely wanna talk
to your product managers.

497
00:27:33,554 --> 00:27:35,114
You might wanna do some experiments.

498
00:27:35,234 --> 00:27:39,734
The good thing about using SLOs in
particular is that they're movable, right?

499
00:27:39,734 --> 00:27:41,954
And like I said, they're, as
long as they're inside your,

500
00:27:42,014 --> 00:27:43,394
the boundaries of your SLA.

501
00:27:43,774 --> 00:27:47,734
Like you can move them around and
do what you need to do with them in

502
00:27:47,734 --> 00:27:52,464
order to, flex around how the system
is performing, the goals that you can

503
00:27:52,464 --> 00:27:56,704
actually meet and how it's impacting
your cus your actual human team.

504
00:27:57,304 --> 00:28:00,664
Knowing what your users are expecting
is gonna help you determine,

505
00:28:01,024 --> 00:28:02,674
is future work necessary here?

506
00:28:02,714 --> 00:28:05,114
Are we slipping out of our tolerances?

507
00:28:05,439 --> 00:28:07,479
For these particular metrics.

508
00:28:07,719 --> 00:28:12,009
'cause maybe there's no amount of
notifications of people overnight

509
00:28:12,039 --> 00:28:15,159
that's gonna fix the thing that
is giving you a chronic issue.

510
00:28:15,519 --> 00:28:17,889
And it's time to go back to
the product manager and have a

511
00:28:17,889 --> 00:28:20,949
discussion about reliability and
how it's impacting the system.

512
00:28:21,429 --> 00:28:24,039
So there's a lot of flexibility here.

513
00:28:24,954 --> 00:28:28,299
These tools can be fairly
sophisticated, so you might wanna

514
00:28:28,299 --> 00:28:29,319
talk to a vendor about them.

515
00:28:29,819 --> 00:28:32,729
So overall, our goals are number one.

516
00:28:33,104 --> 00:28:36,494
We want to reduce our overall
alert load that is going to

517
00:28:36,494 --> 00:28:39,154
help our alert fatigue greatly.

518
00:28:40,084 --> 00:28:44,304
Even if we aren't in a place to
deploy to SLOs, right off the bat,

519
00:28:45,234 --> 00:28:49,914
we'll have an lower volume of high
urgency alerts and they will be

520
00:28:50,209 --> 00:28:52,674
legitimately be important, right?

521
00:28:52,674 --> 00:28:57,894
They're going to things that we need to
see in real time in order to preserve the

522
00:28:57,894 --> 00:29:00,264
reliability of the system for the users.

523
00:29:00,364 --> 00:29:05,974
Every alert that comes to a human
responder should be actionable and

524
00:29:05,974 --> 00:29:08,224
it should require human intervention.

525
00:29:08,584 --> 00:29:12,514
There are lots of other things
that we can de to deploy in this

526
00:29:13,444 --> 00:29:18,034
workflow like auto remediation, other
points of automation, maybe some

527
00:29:18,034 --> 00:29:19,564
machine learning to help us out.

528
00:29:20,064 --> 00:29:21,564
You can also make use of.

529
00:29:22,064 --> 00:29:26,534
But everything that comes to our humans
needs to be actionable over time.

530
00:29:26,564 --> 00:29:31,064
As you clean this up and keep things
rolling in a good with good hygiene,

531
00:29:32,054 --> 00:29:35,654
mostly alerts that come to your
responders are going to be novel.

532
00:29:35,714 --> 00:29:37,724
They're gonna be stuff
you haven't seen before.

533
00:29:37,724 --> 00:29:40,304
In the same way they're
going to be things that are.

534
00:29:40,844 --> 00:29:47,444
New to you or are completely dependent on
the sort of climate in your environment

535
00:29:47,444 --> 00:29:51,974
at that time and not chronic issues
that you can't fix or can't deal with.

536
00:29:52,474 --> 00:29:55,954
So in summary number one, clean
up your alerts even if you have

537
00:29:55,954 --> 00:29:59,854
no intention of deploying SLOs,
making use of those as a tool.

538
00:30:00,304 --> 00:30:05,224
Helping your team out by making sure
your alerts are good and coming into

539
00:30:05,534 --> 00:30:09,314
people that can actually work on them
and actually make sense and are helpful

540
00:30:09,644 --> 00:30:11,264
is going to help everyone out anyway.

541
00:30:11,294 --> 00:30:13,394
Like we, like I mentioned
at the beginning, we all

542
00:30:13,394 --> 00:30:14,894
have enough notifications.

543
00:30:15,284 --> 00:30:17,504
Without production responsibilities.

544
00:30:17,744 --> 00:30:20,984
So if you do have production
responsibilities, you're already shoving

545
00:30:20,984 --> 00:30:25,424
all this information into people who
are already maybe overwhelmed by how

546
00:30:25,424 --> 00:30:26,804
many notifications that they get.

547
00:30:27,284 --> 00:30:31,634
So we're gonna focus on the users with
SLOs and we're gonna prioritize the

548
00:30:31,634 --> 00:30:34,274
things that are important to the users.

549
00:30:34,274 --> 00:30:37,664
And it doesn't matter if your
users are internal or external

550
00:30:37,664 --> 00:30:39,494
or somewhere in between, right?

551
00:30:39,919 --> 00:30:42,829
Everyone has things that they care
about and the systems that we run.

552
00:30:42,859 --> 00:30:44,449
So we're work towards that.

553
00:30:45,049 --> 00:30:48,109
You can then think about getting
the junk out of your workflow with

554
00:30:48,109 --> 00:30:51,389
automation and machine learning.

555
00:30:51,449 --> 00:30:54,629
If those things are in your tool
set and you can deploy them,

556
00:30:54,929 --> 00:30:56,369
you're gonna get more out of them.

557
00:30:56,459 --> 00:30:59,969
After you've cleaned up your alerts,
then you would, if you just throw

558
00:30:59,969 --> 00:31:03,269
the fire hose of all your alerts at
them, it'd be more helpful that way.

559
00:31:03,839 --> 00:31:08,789
If you get to a place where your manager
is not bought in on this, remind them of.

560
00:31:09,444 --> 00:31:13,704
The six points I mentioned at the
beginning about missing alerts and that

561
00:31:13,704 --> 00:31:17,694
impacting your reliability and burnout
and all those kinds of things, right?

562
00:31:18,024 --> 00:31:20,784
Negative impacts, especially if
you're already seeing them on your

563
00:31:20,784 --> 00:31:24,294
team, and you might be right, you
might have folks who are burned out.

564
00:31:24,534 --> 00:31:26,514
You might have folks that
feel like they're drowning

565
00:31:26,754 --> 00:31:28,014
during their on-call shift.

566
00:31:28,284 --> 00:31:32,114
If you're getting more than a
handful of major alerts during a,

567
00:31:32,219 --> 00:31:36,509
like a week long shift, like you're
basically drowning people, right?

568
00:31:36,599 --> 00:31:37,889
There should be a limit.

569
00:31:38,389 --> 00:31:41,899
So make sure you're focusing
on the language around your

570
00:31:41,899 --> 00:31:46,289
reliability your customer
experience, your bottom line, right?

571
00:31:46,349 --> 00:31:48,959
You are running these things
to make money in some way.

572
00:31:49,694 --> 00:31:52,304
And if they aren't running
well, then that's impacting

573
00:31:52,454 --> 00:31:53,894
your potential bottom line.

574
00:31:54,524 --> 00:31:55,904
So some resources for you.

575
00:31:56,084 --> 00:32:00,114
We have plenty of stuff on
our blog about alert fatigue.

576
00:32:00,354 --> 00:32:01,884
You can check all those out.

577
00:32:02,244 --> 00:32:07,859
If you are new to SLOs and haven't
encountered those before, the Google SRE

578
00:32:07,889 --> 00:32:10,049
workbook has an entire chapter about them.

579
00:32:10,349 --> 00:32:12,269
You can also reach out to
our friends Noble nine.

580
00:32:12,864 --> 00:32:16,104
We use their stuff to manage our own SLOs.

581
00:32:16,314 --> 00:32:18,474
They have lots of
documentation on their site.

582
00:32:18,474 --> 00:32:22,864
They wrote a O'Reilly book about
SLOs and can get you going there.

583
00:32:22,864 --> 00:32:25,324
So if you're totally new to this
and you're like, I don't understand

584
00:32:25,324 --> 00:32:28,444
what you're talking about, plenty
of information for you out there.

585
00:32:28,624 --> 00:32:31,324
Lots of resources to, to take a look at.

586
00:32:31,354 --> 00:32:35,734
So I hope that was helpful and I hope you
enjoy the rest of the sessions at com.

587
00:32:35,734 --> 00:32:36,364
42 SRE.

