1
00:00:02,220 --> 00:00:03,090
Hello everyone.

2
00:00:03,630 --> 00:00:04,530
I am ESH mdi.

3
00:00:04,650 --> 00:00:08,460
I'm a principal engineer at
Microsoft, and I am going to talk

4
00:00:08,460 --> 00:00:12,120
to you about removing hallucinations
from your Gen I applications

5
00:00:12,840 --> 00:00:15,000
using the embeddings perspective.

6
00:00:16,650 --> 00:00:18,270
But what are embeddings?

7
00:00:18,420 --> 00:00:23,010
So embeddings is simply your
computers don't understand the

8
00:00:23,010 --> 00:00:26,910
natural language that we speak and
type it, understand numbers and.

9
00:00:27,735 --> 00:00:34,095
Embeds are a process of converting
sentences, words into numerical values.

10
00:00:35,025 --> 00:00:40,695
But what fun those numerical values
have is that they have semantic meaning.

11
00:00:41,805 --> 00:00:45,975
If you can see on the screen here,
there is a man and a woman queen,

12
00:00:45,975 --> 00:00:47,535
and a king, and a dog and a puppy.

13
00:00:49,095 --> 00:00:53,235
Interestingly, the dog and the puppy
within this X and y coordinate, you

14
00:00:53,235 --> 00:00:56,055
would see are closer compared to a dog.

15
00:00:56,865 --> 00:01:01,065
And a woman, but a man and a woman
are more closer compared to a king and

16
00:01:01,065 --> 00:01:05,805
a queen, while king and man are more
closer compared to queen and woman.

17
00:01:06,765 --> 00:01:13,065
So converting into numbers from
a sentence, but having semantic

18
00:01:13,065 --> 00:01:17,290
meaning in those numbers is
what embedded em is all about.

19
00:01:18,830 --> 00:01:19,210
But then.

20
00:01:20,835 --> 00:01:24,434
The topic today is that we shouldn't be
trusting, embedding Strom Foundation model

21
00:01:24,494 --> 00:01:26,414
without experimentation and evaluation.

22
00:01:27,104 --> 00:01:31,514
So we have been using large language
models and small language models

23
00:01:31,514 --> 00:01:37,105
from various different vendors like
Hugging Face and other O Open and other

24
00:01:37,315 --> 00:01:39,505
providers like OpenAI, for example.

25
00:01:39,955 --> 00:01:44,935
And we have been converting our sentences
into numbers and should we trust them.

26
00:01:45,445 --> 00:01:47,065
This topic is all about that.

27
00:01:48,730 --> 00:01:52,930
But before we even get into the details
about why, what we should do to remove

28
00:01:52,930 --> 00:01:58,540
hallucinations into a semantic, meaning
that numbers provide after converting from

29
00:01:58,540 --> 00:02:02,350
a sentences, using the embedding process,
let's understand the embedding process.

30
00:02:02,740 --> 00:02:04,900
So as you can see here,
there are two sentences.

31
00:02:04,990 --> 00:02:08,920
The CAD jumped over fence
and the cat jumped well.

32
00:02:10,389 --> 00:02:13,025
So each word in the embedding process.

33
00:02:13,400 --> 00:02:16,700
Has a mapping to a number,
which is also called input id.

34
00:02:17,359 --> 00:02:18,560
So the CAD jumped over.

35
00:02:18,560 --> 00:02:23,400
Fence has input IDs 1, 2, 3, 4,
5, and which is an additional

36
00:02:23,400 --> 00:02:25,260
word, has a input ID six.

37
00:02:26,160 --> 00:02:29,280
So they get mapped when you
send a sentence to a model.

38
00:02:29,715 --> 00:02:32,655
That gets mapped into,
converted into those numbers.

39
00:02:32,685 --> 00:02:37,035
As you can see here, that we have
1, 2, 3, 4, 5, whereas you have in

40
00:02:37,035 --> 00:02:38,895
the second sentence, 1, 2, 3, 6.

41
00:02:39,585 --> 00:02:43,095
Then it goes through a embedding process
and it generates some numbers for you.

42
00:02:43,755 --> 00:02:49,065
Interestingly, you would notice that
there are five rows over here, one for

43
00:02:49,065 --> 00:02:51,465
each word in the corresponding sentence.

44
00:02:52,365 --> 00:02:55,695
While you have four rows over
here, one for each word in

45
00:02:55,695 --> 00:02:56,895
the corresponding sentence.

46
00:02:58,605 --> 00:03:01,485
The another interesting fact
you would notice is each row has

47
00:03:01,515 --> 00:03:03,405
four values and why four values?

48
00:03:03,405 --> 00:03:08,205
Because this embedding model
generates an output of four

49
00:03:08,355 --> 00:03:14,175
numerical value per word, which is
also known as that dimensionality.

50
00:03:15,120 --> 00:03:19,890
So the dimension dimensionality is
nothing but the output for each word.

51
00:03:20,010 --> 00:03:24,930
The number of numbers in each row is what
the dimensionality is, and they can range

52
00:03:24,930 --> 00:03:27,720
from anything between 32 to 4 0 9 6.

53
00:03:28,650 --> 00:03:32,910
And the higher is better because the more
the number of dimension, which means the

54
00:03:32,910 --> 00:03:36,630
more the nuances, the more the semantic
meaning for that word can be captured.

55
00:03:36,630 --> 00:03:41,460
Within that embedding lower the
dimensions, which means there

56
00:03:41,460 --> 00:03:43,620
are lower semantic value into it.

57
00:03:43,980 --> 00:03:44,400
But.

58
00:03:45,990 --> 00:03:47,010
They are more faster.

59
00:03:47,090 --> 00:03:52,190
They're more faster, and they can easily
be stored with less storage requirements.

60
00:03:53,930 --> 00:03:58,040
Another important concept in understanding
embeddings is the max sequence length.

61
00:03:58,550 --> 00:04:02,450
So if you remember, we had,
we send the CAD jumped over

62
00:04:02,450 --> 00:04:05,360
French, the cat jumped well here.

63
00:04:06,110 --> 00:04:12,230
If our max sequin lens is 10, then
we can send 10 words at a time.

64
00:04:13,370 --> 00:04:18,935
To this embedding model over here,
and it'll generate the output based

65
00:04:18,995 --> 00:04:22,385
on what the dimensionality is.

66
00:04:22,835 --> 00:04:28,235
But if we send 12 from as a, as the
number of words in a sentence, then

67
00:04:28,235 --> 00:04:32,914
it'll actually reduce, it'll only use
a first 10, but it'll not use a, it'll

68
00:04:32,914 --> 00:04:34,565
just discard the rest of two of them.

69
00:04:36,485 --> 00:04:40,105
So if we have to know what is
the max sequence lens of the

70
00:04:40,105 --> 00:04:41,095
model that we're using and.

71
00:04:42,220 --> 00:04:46,360
If, and basically the numbers get
converted into tokens, and the tokens

72
00:04:46,360 --> 00:04:50,710
are what then gets evaluated in
terms of the max sequence length.

73
00:04:51,670 --> 00:04:55,120
Now, they can range between, as
you can see here from sentence

74
00:04:55,120 --> 00:04:59,110
models can have 1 28 to 512 tokens,
whereas document models can have

75
00:04:59,110 --> 00:05:01,990
from 1,002, maybe close to 10,000.

76
00:05:02,890 --> 00:05:04,960
And they get cut if you go beyond that.

77
00:05:05,050 --> 00:05:07,210
But if they're less than that,
they get patterned and they

78
00:05:07,210 --> 00:05:09,160
are well used in the semantic.

79
00:05:09,160 --> 00:05:09,490
Meaning.

80
00:05:10,540 --> 00:05:13,240
Another very important is the
vocabulary and the size when

81
00:05:13,240 --> 00:05:14,530
we are generating embeddings.

82
00:05:15,310 --> 00:05:16,240
Why is this important?

83
00:05:16,600 --> 00:05:24,060
This is important because if we are
providing words like cat jump and

84
00:05:24,060 --> 00:05:25,260
they are part of the vocabulary.

85
00:05:25,695 --> 00:05:29,765
Then the model has been well trained
on those words and they preserve

86
00:05:29,765 --> 00:05:31,325
the somatic meaning for those words.

87
00:05:32,015 --> 00:05:36,665
But if you have an out of vocabulary
word which has not even been ever been

88
00:05:36,665 --> 00:05:42,110
seen by the model, then I. The model
cannot have a semantic meaning for it.

89
00:05:43,070 --> 00:05:47,770
So it's always good to have a vocabulary
within a model which is quite rich, which

90
00:05:47,770 --> 00:05:52,330
means the model has been trained on data,
which has seen a variety of content.

91
00:05:52,330 --> 00:05:55,900
From a data perspective, the
size is also important because

92
00:05:56,350 --> 00:05:57,580
the bigger the vocabulary.

93
00:05:58,914 --> 00:06:02,664
The more training time you need, the
more it might, the model might be

94
00:06:02,664 --> 00:06:06,594
slower because it has to find those
number of input IDs, convert them into

95
00:06:06,594 --> 00:06:09,864
tokens, and then generate the embeddings
and all of those kind of things.

96
00:06:10,284 --> 00:06:15,505
Smaller the size, faster the model,
the faster the embedding process and

97
00:06:15,984 --> 00:06:17,635
richer the vocabulary, the better.

98
00:06:17,635 --> 00:06:21,164
The semantic meaning preserved,
lower the number of lower the

99
00:06:21,164 --> 00:06:22,484
number of words in vocabulary.

100
00:06:23,234 --> 00:06:26,655
You have less semantic meaning, but then.

101
00:06:27,015 --> 00:06:28,724
What are the use cases for embeddings?

102
00:06:29,965 --> 00:06:33,684
Embeddings are very, become very
popular for gene AI based applications,

103
00:06:33,684 --> 00:06:38,294
especially after the invent of
retrieval documented generation, which

104
00:06:38,294 --> 00:06:40,135
is also known as the rag pattern.

105
00:06:40,885 --> 00:06:46,314
And in which basically we try to
search, and when we try to search

106
00:06:46,525 --> 00:06:49,885
at times, you want to search with
keywords, but at different times you

107
00:06:49,885 --> 00:06:54,174
want to search using semantic meaning,
which means we might ask for a thing.

108
00:06:55,270 --> 00:06:59,659
That not, might not exactly
available within our data store.

109
00:07:00,859 --> 00:07:04,969
And we might have to infer the intent,
the meaning of the sentence or the

110
00:07:04,969 --> 00:07:09,530
utterance or the query the user is sending
to find out what relevant results we

111
00:07:09,530 --> 00:07:14,349
want to give it as part of the search
results, which is very important because

112
00:07:14,349 --> 00:07:20,679
these days, especially every Gen AI
application is using such technology.

113
00:07:21,145 --> 00:07:21,754
Most of it.

114
00:07:22,780 --> 00:07:26,799
Then we have recommendation systems,
which again, you want to, if somebody's

115
00:07:26,799 --> 00:07:30,440
searching for a product like a cellular
phone, then would you like to have

116
00:07:30,440 --> 00:07:34,610
a mobile phone displayed as part
of recommendation system or maybe

117
00:07:34,700 --> 00:07:40,640
other electronical device, which
might make similar choices, but you

118
00:07:40,640 --> 00:07:45,670
have to know that a cellular phone
might mean also a automotive device.

119
00:07:46,510 --> 00:07:49,690
Similarly, natural language processing,
and there are other use cases as well.

120
00:07:51,640 --> 00:07:53,860
So how do we compare embeddings?

121
00:07:54,010 --> 00:07:56,860
Now if we have embeddings, which is
great, we have numbers stored in our

122
00:07:56,860 --> 00:07:59,020
database and we have numbers coming up.

123
00:07:59,530 --> 00:08:02,740
How do we compare numbers to
number the way to compare numbers?

124
00:08:03,295 --> 00:08:07,345
Is using various different matrix and
some of them have been shown over here.

125
00:08:07,825 --> 00:08:11,525
They the most famous one is the
cosign similarity in which we try to

126
00:08:11,525 --> 00:08:14,745
find you saw, you remember the man
and the woman in the first slide.

127
00:08:15,285 --> 00:08:20,725
Then if we draw straight lines from the
point where the 0.0 starch, then what's

128
00:08:20,725 --> 00:08:22,165
the angle between those two lines?

129
00:08:22,645 --> 00:08:26,155
The lower the angle, the most
similar they are, the higher the

130
00:08:26,155 --> 00:08:29,965
angle, they're not similar, and it
could range from minus one to one.

131
00:08:30,670 --> 00:08:35,260
Similarly, you have Qadium distance,
Manhattan distance and dot product, and

132
00:08:35,260 --> 00:08:37,210
I will leave it on you to explore them.

133
00:08:37,270 --> 00:08:41,410
But IGN similarity is the
leading way of doing comparisons.

134
00:08:42,280 --> 00:08:46,510
So let's see a small example of how
we can find embeddings using open ai.

135
00:08:46,540 --> 00:08:47,230
And its, and.

136
00:08:47,530 --> 00:08:48,670
Sentence Transformers.

137
00:08:48,910 --> 00:08:52,290
So sentence transformer is basically
a package that you can, in Python,

138
00:08:52,290 --> 00:08:56,350
you can download or install it
and use it to find embeddings.

139
00:08:56,650 --> 00:09:01,280
As you can see here, I have some
code and I'm, I am going to use a

140
00:09:01,330 --> 00:09:05,170
simple model called MP net, which
is available in hugging face.

141
00:09:05,290 --> 00:09:06,520
And I have two sentences.

142
00:09:06,550 --> 00:09:07,990
What's the weather like in New York today?

143
00:09:08,020 --> 00:09:09,970
And can you tell me the
current weather in New York?

144
00:09:10,540 --> 00:09:12,880
And I'm trying to find
embeddings for both of them.

145
00:09:13,675 --> 00:09:15,775
And to find the embeddings
for both of them.

146
00:09:15,985 --> 00:09:22,435
I could also, for example, come here
and I could show you that in this case.

147
00:09:22,495 --> 00:09:26,185
I have another example where I am
just trying to find the embeddings

148
00:09:26,185 --> 00:09:29,155
for American Pizza is one of the
nation's greatest cultural export.

149
00:09:30,385 --> 00:09:31,495
Just provide me the embeddings.

150
00:09:31,615 --> 00:09:36,775
The results for this you would see here
is an embedding over here with quite

151
00:09:36,775 --> 00:09:38,755
a few numbers and why these numbers.

152
00:09:38,755 --> 00:09:41,965
Now you might know that these are the
dimensions, so this embedding model.

153
00:09:42,850 --> 00:09:46,480
M Ms. Marco provides us
with 7 68 dimensions.

154
00:09:46,480 --> 00:09:49,000
If you remember the row I was
talking about, how many numbers,

155
00:09:49,000 --> 00:09:51,320
there were four in that example.

156
00:09:51,380 --> 00:09:52,280
In the first example.

157
00:09:52,340 --> 00:09:56,120
Here they are 7 68, which is quite rich
in semantic meaning, and it's, as you

158
00:09:56,120 --> 00:10:02,120
can see here, those numbers semantically
mean something about this sentence.

159
00:10:02,120 --> 00:10:05,840
So it might not be exact keyword
match, but semantically, what

160
00:10:05,840 --> 00:10:09,470
does it mean now if we have to?

161
00:10:10,640 --> 00:10:14,870
Find and compare embeddings, which is
what I was trying to show you before.

162
00:10:15,410 --> 00:10:19,850
If we want to compare two sentences
and find how close those two sentences

163
00:10:19,910 --> 00:10:24,110
are from an embeds perspective, I have
those two sentences I was talking about.

164
00:10:24,530 --> 00:10:28,820
I can use the package sentence
transformers and code method, and

165
00:10:28,850 --> 00:10:33,110
once I do that, I use the sign
similarity, that measure that I

166
00:10:33,110 --> 00:10:35,090
was talking in the beginning and.

167
00:10:35,585 --> 00:10:38,375
Once I do that, you can
see the results of it.

168
00:10:38,885 --> 00:10:44,015
So if I use cosign similarity, I get 88%
similarity between these two sentences.

169
00:10:44,325 --> 00:10:46,815
The words are not in the same order.

170
00:10:46,905 --> 00:10:52,845
They might not even have the same words
in them, but I am able to find a lot of

171
00:10:52,845 --> 00:10:54,615
similarity between these two sentences.

172
00:10:54,620 --> 00:10:58,335
And that's the beauty of embeddings, that
you don't have to be doing an exact search

173
00:10:58,335 --> 00:11:00,435
keyword search, entity-based search.

174
00:11:00,795 --> 00:11:02,955
You can do a simple search.

175
00:11:03,855 --> 00:11:07,755
And the semantic meaning of that
is what is being shown as being

176
00:11:07,755 --> 00:11:09,405
compared with the other sentence.

177
00:11:10,665 --> 00:11:14,565
There are other metrics used over here
and we can ignore it for now and we

178
00:11:14,565 --> 00:11:15,885
can just concentrate on cosigning.

179
00:11:17,025 --> 00:11:19,785
So that's how you compare
embeddings, right?

180
00:11:19,935 --> 00:11:24,405
So now I can find out
embeddings, but let's see how

181
00:11:24,405 --> 00:11:25,575
well we understand embeddings.

182
00:11:25,965 --> 00:11:30,825
So I have a small quiz for you and
let's you try it in mentally or you

183
00:11:30,825 --> 00:11:31,740
want to write it down somewhere.

184
00:11:32,445 --> 00:11:33,135
Answer this.

185
00:11:33,315 --> 00:11:35,805
You saw the cosign similarity
ranges from minus one to one.

186
00:11:36,465 --> 00:11:40,305
Now there are four different sentences,
pairs of sentences over here.

187
00:11:40,995 --> 00:11:44,925
You tell me or you find out
just by reading it, you don't

188
00:11:44,925 --> 00:11:46,245
have to write any code for this.

189
00:11:46,605 --> 00:11:49,875
How closely these two sentences
would be, or what would be the

190
00:11:49,875 --> 00:11:52,305
cosign similarity answers for this?

191
00:11:52,575 --> 00:11:55,395
The treatment was completely
ineffective against the disease

192
00:11:56,115 --> 00:11:59,775
versus the treatment was absolutely
effective against the disease.

193
00:11:59,775 --> 00:12:00,015
What does.

194
00:12:00,555 --> 00:12:03,495
The human mind says, are
these two sentences similar?

195
00:12:03,945 --> 00:12:04,425
No way.

196
00:12:04,665 --> 00:12:05,985
They're completely opposites.

197
00:12:06,285 --> 00:12:09,225
So the IGN similarity should
be quite low because the angle

198
00:12:09,225 --> 00:12:10,815
between them should be quite high.

199
00:12:13,215 --> 00:12:16,725
Similarly, placed the specimen in the
refrigerator at exact four degrees

200
00:12:16,725 --> 00:12:20,835
centigrade, the sample must be stored at
precisely four degrees centigrade, and

201
00:12:20,845 --> 00:12:25,255
that there is a degree word over here
rather than a symbol in the cooling unit.

202
00:12:26,440 --> 00:12:28,690
Results showed statistical significance.

203
00:12:28,690 --> 00:12:32,230
The findings indicated
a significant effect.

204
00:12:32,950 --> 00:12:36,640
The patient shows hypertension,
the patient shows hypotension.

205
00:12:38,050 --> 00:12:42,460
Feel free to think about how
closely they are and, but let me

206
00:12:42,460 --> 00:12:43,840
show you what they eventually mean.

207
00:12:44,410 --> 00:12:47,890
So if you look at the quiz for
this, I already had a run, you

208
00:12:47,890 --> 00:12:54,220
would see that quiz three had an 83%
similarity, which is great because

209
00:12:54,220 --> 00:12:55,480
they almost mean the same thing.

210
00:12:56,695 --> 00:13:02,425
The finding that absolutely puzzled
me was quiz one or is quiz one?

211
00:13:04,315 --> 00:13:10,225
This sentence is completely opposite of
this sentence, but I got a 96% similarity,

212
00:13:10,285 --> 00:13:15,805
which means if I am building a system
where I have stored some data that I

213
00:13:15,805 --> 00:13:19,435
have in my documents, and those data
could be looked like this, the content

214
00:13:19,435 --> 00:13:20,785
in my documents could be like this.

215
00:13:21,115 --> 00:13:24,115
And if a user is sending
me something like this.

216
00:13:24,670 --> 00:13:27,460
It is telling me completely
opposite as an answer.

217
00:13:27,970 --> 00:13:31,840
So when we are implementing rack
based solutions, or we are using

218
00:13:32,110 --> 00:13:36,010
building our own copilot, or when we
are building things based on search,

219
00:13:36,790 --> 00:13:40,945
how can we rely on an embedding
which is outta the box like MP net.

220
00:13:42,235 --> 00:13:46,715
With such a scenario, and that's what
the crux of the whole topic today is

221
00:13:46,865 --> 00:13:52,025
that how do we remove such hallucinations
so that we can even work with such

222
00:13:52,025 --> 00:13:53,525
kind of sentences within our data?

223
00:13:54,845 --> 00:13:55,595
Why is this happening?

224
00:13:56,285 --> 00:13:57,125
So think about it.

225
00:13:57,125 --> 00:13:57,785
Why is this happening?

226
00:13:57,785 --> 00:14:01,115
So we have a embedding model
like MP net that you see here.

227
00:14:01,685 --> 00:14:06,155
Now, why does MP net capture the
difference between these two and

228
00:14:06,155 --> 00:14:08,940
say that the embed and the cosign
similarity should be quite low?

229
00:14:09,785 --> 00:14:11,675
Because of some of the reasons
I've been mentioned over.

230
00:14:11,725 --> 00:14:18,955
And the top of them is that some of the
models provide more weightage to words

231
00:14:19,495 --> 00:14:21,565
that it thinks should be is important.

232
00:14:21,805 --> 00:14:25,315
And if those words are available on them,
in both of them, they think, oh it's a

233
00:14:25,375 --> 00:14:28,165
good ign, it's a higher cosign similarity.

234
00:14:28,705 --> 00:14:31,525
Similarly, it might try to compare.

235
00:14:32,185 --> 00:14:36,145
The available words in both the
sentences and see if the overlap is high.

236
00:14:36,235 --> 00:14:40,675
It might just say that the cosign
similarity is high, irrespective it'll

237
00:14:40,675 --> 00:14:44,515
not take the order of the words that's
available within these two sentences.

238
00:14:45,835 --> 00:14:51,085
It might also see that if it has a
positive sentence sentiment, for example,

239
00:14:51,085 --> 00:14:55,135
then it'll provide more weightage
to it compared to others, so that if

240
00:14:55,135 --> 00:15:00,095
enjoyed is there in both the sentences
it just thinks it is very similar.

241
00:15:00,725 --> 00:15:03,905
It does not see that there
might be a knot word not enjoyed

242
00:15:03,905 --> 00:15:05,375
as well in the next sentence.

243
00:15:05,765 --> 00:15:08,825
So there are quite a few reasons why
this happens, and it's our job to make

244
00:15:08,825 --> 00:15:12,785
sure that we experiment the base model,
find out if they are hallucinating,

245
00:15:13,325 --> 00:15:16,595
and if they're hallucinating,
then make sure we remove them.

246
00:15:17,705 --> 00:15:20,945
What are the various scenarios I've
found out for embedding models?

247
00:15:20,945 --> 00:15:24,665
Hallucinating, and there are quite a
few, as you can see, there are 38 of

248
00:15:24,665 --> 00:15:28,025
them starting from capitalization,
and I'll show you all of those.

249
00:15:28,025 --> 00:15:31,415
Some of these examples, wide space,
variations between sentences,

250
00:15:31,415 --> 00:15:32,615
does it impact embeddings?

251
00:15:32,615 --> 00:15:33,245
Negation?

252
00:15:33,245 --> 00:15:37,235
We just saw an example, not
versus not special characters.

253
00:15:37,235 --> 00:15:39,725
Word, order, date, time,
and all of those things.

254
00:15:39,725 --> 00:15:42,935
So we will go and look into
this, how that actually happens.

255
00:15:42,935 --> 00:15:44,765
So if I come to my code here, back again.

256
00:15:46,340 --> 00:15:49,730
I have quite a few sentences over
here, and let me show you some

257
00:15:49,730 --> 00:15:53,950
of the ones that you would be
able to immediately capture here.

258
00:15:54,005 --> 00:15:54,885
You would be able to.

259
00:15:55,530 --> 00:15:59,310
Think clearly whether they
should sound similar or not.

260
00:15:59,400 --> 00:16:00,270
Now let me show you.

261
00:16:00,540 --> 00:16:03,930
See, she completed her degree
before starting the job.

262
00:16:04,470 --> 00:16:07,410
She started her job before
completing her degree.

263
00:16:07,530 --> 00:16:09,030
How similar are they?

264
00:16:09,960 --> 00:16:13,290
The company barely exceeded
earnings expectations.

265
00:16:13,320 --> 00:16:17,850
The company significantly
missed earning expectations.

266
00:16:17,910 --> 00:16:20,580
Think about it, how closely
they are related to each other.

267
00:16:21,825 --> 00:16:24,855
If the treatment works,
symptoms should improve.

268
00:16:25,035 --> 00:16:28,965
The treatment works and
symptoms have improved.

269
00:16:29,985 --> 00:16:31,845
One is hypothetical,
one is actual factual.

270
00:16:33,195 --> 00:16:35,505
The meeting ran significantly
shorter than planned.

271
00:16:35,505 --> 00:16:38,505
The meeting ran significantly
longer than planned, and

272
00:16:38,505 --> 00:16:39,705
there are quite a few of them.

273
00:16:39,825 --> 00:16:44,235
And as you can see, they all have
been different types of sentences.

274
00:16:44,595 --> 00:16:46,815
Legal domain, medical domain.

275
00:16:47,250 --> 00:16:50,700
Attribution unit of
time, quite interesting.

276
00:16:50,700 --> 00:16:52,500
The procedure takes about five minutes.

277
00:16:52,530 --> 00:16:54,390
The procedure takes about five hours.

278
00:16:54,510 --> 00:16:57,570
There's a ton of a difference between
these two sentences, and we'll

279
00:16:57,570 --> 00:16:58,950
see how closely similar they are.

280
00:17:00,270 --> 00:17:04,620
So same for speed, kilometer
per hour, miles per hour.

281
00:17:05,160 --> 00:17:09,480
The product costs ranges between $50
to a hundred dollars versus $101.

282
00:17:10,200 --> 00:17:12,935
The patient's fever was
101 degree Fahrenheit.

283
00:17:13,080 --> 00:17:15,870
The fever was 104 degree
faite and things like that.

284
00:17:16,560 --> 00:17:24,000
If I execute this, you see a result like
this and you can see that date time.

285
00:17:24,000 --> 00:17:30,990
For example, if I say her interview was on
10th of December, 10th, 12, 20 25 versus.

286
00:17:31,590 --> 00:17:34,950
12, 10, 20, 25. The similarity is 99%.

287
00:17:34,950 --> 00:17:37,800
It's a two month difference,
but from a language perspective,

288
00:17:38,460 --> 00:17:39,420
it is two month different.

289
00:17:39,570 --> 00:17:42,900
From a numbers perspective, it is not a
two month difference because our embedding

290
00:17:42,900 --> 00:17:47,340
model thinks they're exactly the same
thing from a taxonomic per percentages.

291
00:17:47,760 --> 00:17:50,700
And all of these are
different combination.

292
00:17:50,700 --> 00:17:54,720
As you can see, the minimum I'm
getting is 80 for spelling and typos,

293
00:17:55,530 --> 00:17:59,520
whereas they should all have been
quite low in terms of embeddings.

294
00:18:00,120 --> 00:18:01,830
So it's a big challenge, right?

295
00:18:02,220 --> 00:18:03,300
How do we solve this?

296
00:18:04,500 --> 00:18:07,170
So the way to solve this, there are
quite a few ways we can solve it.

297
00:18:07,380 --> 00:18:10,230
One of, and I've listed down some
of them over here, and you should

298
00:18:10,230 --> 00:18:15,510
think about using them one or the
other, or in combination of multiple

299
00:18:15,510 --> 00:18:16,930
of them to solve this problem.

300
00:18:17,470 --> 00:18:22,480
So you can use a domain that is
very specific to your use case.

301
00:18:22,480 --> 00:18:24,850
So if you are building
a medical based system.

302
00:18:25,315 --> 00:18:30,085
And you using a rag or a copilot building
your active chatbot you're building, then

303
00:18:30,085 --> 00:18:34,315
you could use a medical based embedding
model and you might find a ton of them

304
00:18:34,315 --> 00:18:39,235
within, say for example, in hugging face,
if you're doing a legal based system, then

305
00:18:39,235 --> 00:18:41,815
you can use a model based on legal words.

306
00:18:41,815 --> 00:18:44,575
It's been trained on legal semantic
meanings and things like that.

307
00:18:45,715 --> 00:18:47,065
So that's one way to look at it.

308
00:18:47,065 --> 00:18:50,725
So it's try to find a model that
discloses to the work that you're doing.

309
00:18:51,265 --> 00:18:53,965
The second thing is pre-processor data.

310
00:18:54,910 --> 00:18:58,150
If you have numbers,
expand them into words.

311
00:18:58,210 --> 00:19:00,220
If you have dates, expand them into words.

312
00:19:00,430 --> 00:19:07,480
If you have words like entities
like an apple, that apple means a

313
00:19:07,480 --> 00:19:12,310
fruit or a brand, provide additional
metadata within the content itself.

314
00:19:12,460 --> 00:19:15,970
And those are all pre-processing steps,
which means when you are ingesting

315
00:19:15,970 --> 00:19:17,440
these documents within your database.

316
00:19:17,815 --> 00:19:20,185
Or a data store, you take
them through a process.

317
00:19:20,185 --> 00:19:23,905
Each chunk goes through this process
and add more metadata if it needed,

318
00:19:24,205 --> 00:19:28,285
expand the abbreviations numbers
into words, data into sentences,

319
00:19:28,285 --> 00:19:29,725
and all of those things you do.

320
00:19:29,725 --> 00:19:34,345
And then you store them in your
database and then eventually you fine

321
00:19:34,345 --> 00:19:38,635
tune an existing foundation model
on the data that you already have.

322
00:19:39,175 --> 00:19:42,175
If you do that, and a
combination of these.

323
00:19:42,745 --> 00:19:47,605
You would find that the embeddings assign
similarity between the same sentences that

324
00:19:47,605 --> 00:19:49,495
we've seen before falls down consider.

325
00:19:50,065 --> 00:19:50,695
So let's see that.

326
00:19:52,045 --> 00:19:55,735
I'm not going to fine tune because
fine tuning is a pretty exhaustive

327
00:19:55,735 --> 00:19:57,145
process and it takes time.

328
00:19:57,625 --> 00:20:00,985
But I'll talk about the
process of fine tuning.

329
00:20:01,525 --> 00:20:04,075
So for fine tuning,
basically what you do is.

330
00:20:04,915 --> 00:20:06,865
You need data first, right?

331
00:20:06,985 --> 00:20:08,395
So the, I already have data.

332
00:20:08,575 --> 00:20:12,295
I have the test data, train data, and the
validation data, and they're very simple.

333
00:20:12,775 --> 00:20:15,775
I need to provide them in a format
that is acceptable by sentence

334
00:20:15,775 --> 00:20:18,565
transformers, by loss functions,
and the evaluation function.

335
00:20:19,075 --> 00:20:22,285
So for example, I have sentence
one, sentence two, and what is this

336
00:20:22,285 --> 00:20:23,605
level of similarity between them?

337
00:20:24,115 --> 00:20:27,655
So for example, if I see this,
the software includes new feature.

338
00:20:28,285 --> 00:20:30,445
Technology continues to evolve rapidly.

339
00:20:30,985 --> 00:20:34,435
There might be some similarities, so
I'm giving you a 0.5 as similarity.

340
00:20:35,035 --> 00:20:38,545
Only 15% students fail the exam up to 85%.

341
00:20:38,725 --> 00:20:39,925
Students fail the exam.

342
00:20:40,135 --> 00:20:40,795
Big difference.

343
00:20:40,825 --> 00:20:42,235
The similarity is zero over.

344
00:20:42,985 --> 00:20:46,285
So this kind of a data, either you can
generate using another large language

345
00:20:46,285 --> 00:20:48,895
model or and curate it after that.

346
00:20:48,985 --> 00:20:50,650
Or you can bring your own data like this.

347
00:20:51,160 --> 00:20:52,450
And then use it for fine tuning.

348
00:20:52,450 --> 00:20:56,410
So I have trained data on which I will
fine tune the model, and then I have

349
00:20:56,440 --> 00:21:00,430
test data on which I will test the
model and see how good the results are.

350
00:21:00,910 --> 00:21:04,240
So the test data is something the
model has never seen back, ever.

351
00:21:04,660 --> 00:21:09,325
And the validation is I use the
validation data while training and

352
00:21:09,325 --> 00:21:12,805
evaluating at the same time to check
whether things are getting overfitted

353
00:21:12,835 --> 00:21:16,405
or unfitted, whether my model is
going in the right direction or not.

354
00:21:17,905 --> 00:21:21,205
So I have data now and
how do I do fine tuning?

355
00:21:21,865 --> 00:21:26,425
So what I do is I load
up an existing model.

356
00:21:26,515 --> 00:21:27,325
I load up my data.

357
00:21:27,355 --> 00:21:33,085
So I load my data, I load my, I convert
the data into format that sentence,

358
00:21:33,085 --> 00:21:34,765
transformers understand for fine tuning.

359
00:21:34,765 --> 00:21:36,955
So which means sentence one,
sentence two, and the similarity,

360
00:21:36,955 --> 00:21:38,245
which you saw in the CSE file.

361
00:21:38,755 --> 00:21:39,655
I load that data.

362
00:21:40,600 --> 00:21:44,650
And then I have a embedding similarity
evaluator because when I fine tune,

363
00:21:44,650 --> 00:21:48,520
I will want to evaluate on the new
data how the model is behaving.

364
00:21:49,030 --> 00:21:52,570
I have a loss because if I find
the embedding for the same sentence

365
00:21:52,570 --> 00:21:58,240
to be 0.8 versus I expect them
to be 0.0, then that loss is 0.8.

366
00:21:58,480 --> 00:21:59,860
And so I want to reduce that loss.

367
00:21:59,860 --> 00:22:03,490
So what kind of a loss I'm trying
to use and the host of other.

368
00:22:04,030 --> 00:22:05,320
Attributes to make it run.

369
00:22:05,440 --> 00:22:08,680
And you can, obviously these are all a
well document and available within hugging

370
00:22:08,680 --> 00:22:10,610
face and sentence transformers package.

371
00:22:11,360 --> 00:22:15,110
And then we try to fit, which means this
fit is what starts training the model.

372
00:22:15,750 --> 00:22:18,810
Fine tuning the model it uses
the data that we have given.

373
00:22:18,810 --> 00:22:20,040
It uses a loss function.

374
00:22:20,130 --> 00:22:23,970
We are given an evaluator, and the number
of times you want to loop through is the

375
00:22:23,970 --> 00:22:28,830
number of epochs and eventually the output
is where you get your fine tuned model.

376
00:22:28,860 --> 00:22:30,840
So I have named my model as.

377
00:22:31,590 --> 00:22:34,680
Fine tune, semantic model that,
and that's why you see a new

378
00:22:34,680 --> 00:22:35,820
model that's created over here.

379
00:22:36,870 --> 00:22:43,620
And now if I want to evaluate my new
model after I've trained the model, I

380
00:22:43,620 --> 00:22:48,750
can load that model and then I can load
my test data as well, and then can.

381
00:22:49,245 --> 00:22:53,145
Execute the same encoding because
I'm generating embeddings now.

382
00:22:53,445 --> 00:22:57,345
I can send sentence one, sentence
two from a test data, get both the

383
00:22:57,345 --> 00:23:01,035
embeddings, and then use codesign
similarity to see how closely they are

384
00:23:01,335 --> 00:23:03,585
and then can list them down over here.

385
00:23:04,335 --> 00:23:06,735
So that is what the entire
process of fine tuning.

386
00:23:06,765 --> 00:23:13,155
And after fine tune, what I can do is come
back over here and I will run the same

387
00:23:13,155 --> 00:23:16,815
thing, same code that you see here, which
is giving me such high embedding model.

388
00:23:17,400 --> 00:23:18,540
Using a fine tune model.

389
00:23:18,570 --> 00:23:24,900
So instead of using, if you
see here, I had used Ms. Marco,

390
00:23:25,260 --> 00:23:26,550
and now I'm not using Ms.

391
00:23:26,550 --> 00:23:30,480
Marco, but what I'm using is a fine
tune semantic model, which is going

392
00:23:30,570 --> 00:23:35,520
to fine embeddings between the same
two pairs of sentences, all the 38

393
00:23:35,790 --> 00:23:39,690
or ones stuff that I've shown you
before, and see what the results are.

394
00:23:40,050 --> 00:23:42,090
So if you see here, I've
executed this already.

395
00:23:42,510 --> 00:23:47,250
And now you will see that the embedding
model is even giving me negative returns.

396
00:23:47,670 --> 00:23:51,690
It was the minimum was 80%, the
previous one with the foundation model.

397
00:23:52,110 --> 00:23:57,030
Now for negation, for example, if I
see the negation over here, I get 46%.

398
00:23:57,360 --> 00:24:02,100
And anything less than 60 or 70% in
embeddings, you can obviously discard it.

399
00:24:02,550 --> 00:24:08,610
But you see now for a majority of
them, I have very low embedding.

400
00:24:08,610 --> 00:24:11,220
So I have removed hallucination
to a very large extent.

401
00:24:12,015 --> 00:24:16,605
From my system, if I use the new
embedding model, which I've just fine

402
00:24:16,605 --> 00:24:22,245
tuned, obviously you will find some of
them quite high even now, which means

403
00:24:22,245 --> 00:24:23,490
you might have to either fine tune.

404
00:24:24,120 --> 00:24:27,600
Further with more data might be
data slash or maybe I haven't used

405
00:24:27,690 --> 00:24:31,230
finetune appropriately with changing
some parameters of the number

406
00:24:31,230 --> 00:24:32,370
of epochs and things like that.

407
00:24:32,880 --> 00:24:36,030
Or maybe capitalization
does not matter to me.

408
00:24:36,160 --> 00:24:39,760
If the capitalization, whether
it's slow or big, it matters,

409
00:24:39,760 --> 00:24:40,510
then I have to fine tune.

410
00:24:40,510 --> 00:24:42,910
But if it does not matter, and
having one is great for me.

411
00:24:43,270 --> 00:24:44,770
So it's also use case dependent.

412
00:24:45,160 --> 00:24:45,945
So you have to.

413
00:24:47,050 --> 00:24:51,470
Think about how you want to fine
tune, get your data and the entire

414
00:24:51,470 --> 00:24:53,570
process to make this thing happen.

415
00:24:55,340 --> 00:24:56,510
So what are the steps of fine tuning?

416
00:24:56,510 --> 00:24:59,420
Just as a recap, you set up an
environment, which means you have,

417
00:24:59,450 --> 00:25:03,030
you need maybe a environment, like
a virtual environment conduct or a

418
00:25:03,390 --> 00:25:05,550
uv, and you set up that environment.

419
00:25:05,550 --> 00:25:07,620
You download all the packages,
like sentence transformers.

420
00:25:08,255 --> 00:25:10,235
And you obviously need some code to write.

421
00:25:10,655 --> 00:25:14,165
You prepare your training data, validation
data, test data for fine tuning the model.

422
00:25:14,225 --> 00:25:17,765
You pick up a model to fine
tune, and it could be MP net,

423
00:25:17,885 --> 00:25:19,325
Ms. Marco, a couple of examples.

424
00:25:19,325 --> 00:25:20,165
There are a ton of them.

425
00:25:20,165 --> 00:25:23,675
You can use any of them, but you need
to find which model works best for you.

426
00:25:23,885 --> 00:25:25,805
And then you can go for
fine tuning as well.

427
00:25:26,195 --> 00:25:29,195
Provide configuration of hyper
parameters, like which kind of a loss

428
00:25:29,195 --> 00:25:32,285
function, what kind of an evaluator,
how many epochs, what should be your.

429
00:25:33,285 --> 00:25:34,965
How much data and things like that.

430
00:25:35,085 --> 00:25:39,285
Train your model, save it, and then
evaluate it to see how better it is

431
00:25:39,285 --> 00:25:41,505
performing based on the foundation model.

432
00:25:42,885 --> 00:25:43,785
How do you choose a model?

433
00:25:43,845 --> 00:25:46,425
It could be based on a domain size.

434
00:25:46,480 --> 00:25:50,385
You, the bigger, the size, the most space,
it needs more storage, it needs, and then

435
00:25:50,385 --> 00:25:54,735
you need to also less performance because
it takes more time to revert back on

436
00:25:54,735 --> 00:25:57,105
the, from a request response perspective.

437
00:25:58,620 --> 00:26:03,390
But it's a legal based, if you can't
use a medical based domain model

438
00:26:03,480 --> 00:26:05,160
on a legal based use case, right?

439
00:26:05,340 --> 00:26:09,210
So it, and you can't then expect
it to have a good response.

440
00:26:09,240 --> 00:26:11,400
So you have to think about
the domain alignment as well.

441
00:26:11,700 --> 00:26:13,590
The speed at which you
want the response back.

442
00:26:13,590 --> 00:26:15,930
Some needs quick response
come, some does not.

443
00:26:15,930 --> 00:26:20,880
And so if you need higher speed in terms
of inference, then you need relatively

444
00:26:20,880 --> 00:26:22,500
smaller model compared to higher models.

445
00:26:23,265 --> 00:26:26,115
And there are various other things,
but these are some of the top ones,

446
00:26:26,115 --> 00:26:29,745
how you choose a model, foundation
model, and then you go for fine tuning.

447
00:26:31,365 --> 00:26:36,555
And this is the same thing, but showing
some of the top models or some of the

448
00:26:36,555 --> 00:26:39,855
models that are quite widely used.

449
00:26:39,975 --> 00:26:44,415
What's the sizes, strengths, and the best,
how are they used in different capacities?

450
00:26:45,795 --> 00:26:46,935
I won't go into those details.

451
00:26:46,935 --> 00:26:50,625
I already talked about the evaluators
and other epochs, but you can

452
00:26:50,625 --> 00:26:52,395
obviously return into details into.

453
00:26:53,415 --> 00:26:54,435
A hugging first document.

454
00:26:54,465 --> 00:26:58,605
But these all impact the quality
of the new fine-tune model.

455
00:26:59,175 --> 00:27:04,245
So if you think that the quality is not
meeting your standards, you have to come

456
00:27:04,245 --> 00:27:08,415
and tube them to see whether and generate
a new model, and obviously compare with

457
00:27:08,415 --> 00:27:12,405
the opposite, the previous model to find
whether you're improving or not improving.

458
00:27:12,405 --> 00:27:13,935
So that's what the
experimentation is all for.

459
00:27:15,420 --> 00:27:17,190
So that's all I had.

460
00:27:17,190 --> 00:27:18,570
So what are the key takeaway?

461
00:27:18,570 --> 00:27:20,010
So first, understand your data.

462
00:27:20,050 --> 00:27:24,070
If you don't know your data well, you
won't be able to know whether, what kind

463
00:27:24,070 --> 00:27:28,030
of a model I want to use, what kind of
a foundational model I should bring in.

464
00:27:28,420 --> 00:27:31,900
How should I fine tune if I have to
fine tune what is my training data?

465
00:27:31,900 --> 00:27:32,920
What is my validation data?

466
00:27:32,920 --> 00:27:33,760
What is my testing data?

467
00:27:34,060 --> 00:27:38,140
And whether they meet the quality of the
data or not identify the model to use.

468
00:27:38,380 --> 00:27:39,070
Very important.

469
00:27:39,460 --> 00:27:42,040
And then embedding models are not magic.

470
00:27:42,140 --> 00:27:48,160
You have to first use the embedding model
using some of the examples or samples of

471
00:27:48,160 --> 00:27:52,600
data that you have and see whether, how
are they performing and if they're not

472
00:27:52,600 --> 00:27:56,705
performing well, then you think about the
various ways to remove that hallucination.

473
00:27:57,895 --> 00:28:01,375
Very important that the data quality
is super important, and if that's

474
00:28:01,375 --> 00:28:04,405
not important, no matter what you
do, you'll not get the right results,

475
00:28:05,005 --> 00:28:07,315
but only fine tune if it's required.

476
00:28:07,345 --> 00:28:09,115
It's not the, it's not the end.

477
00:28:09,115 --> 00:28:12,115
It's not the starting point
of removing the hallucination.

478
00:28:12,505 --> 00:28:15,925
It is one of the, towards the end side
of removing the hallucination, there

479
00:28:15,925 --> 00:28:20,335
are other steps that I mentioned before,
like pre-processing, finding a good

480
00:28:20,335 --> 00:28:24,025
model that already does not do all
this, or somebody might have already

481
00:28:24,085 --> 00:28:27,085
done a fine tuning, then you can use it
instead of you doing the fine tuning.

482
00:28:28,570 --> 00:28:30,280
Systematic evaluation is quite important.

483
00:28:30,280 --> 00:28:33,580
So you should evaluate your
models well and before using

484
00:28:33,580 --> 00:28:34,990
them in your Gen I applications.

485
00:28:34,990 --> 00:28:40,240
And then finally, keep iterating,
keep exploring and find out whether

486
00:28:40,300 --> 00:28:41,560
things can be further improved.

487
00:28:41,980 --> 00:28:42,970
Have a baseline.

488
00:28:43,000 --> 00:28:45,850
If you are crossing that threshold,
you should be good to go.

489
00:28:46,210 --> 00:28:50,710
But Dr. Han does not let you stop the
experimentation further because who knows,

490
00:28:50,710 --> 00:28:52,420
you might find better results as well.

491
00:28:53,560 --> 00:29:01,885
So with that, I. Pause or I'll stop my
presentation here and obviously if you

492
00:29:01,885 --> 00:29:07,145
have any questions, feel free to post them
out in any of these contacts that I have,

493
00:29:07,235 --> 00:29:09,785
you can get reach out to me on LinkedIn.

494
00:29:10,385 --> 00:29:13,170
I have the code that you saw
here is available in GitHub.

495
00:29:14,585 --> 00:29:18,215
You can obviously go and check out
this code, and that's my automation.

496
00:29:18,335 --> 00:29:20,145
Next is my account in Twitter.

497
00:29:20,925 --> 00:29:21,705
So thank you very much.

498
00:29:21,705 --> 00:29:24,465
That was my time and I hope to
see you in the conference and

499
00:29:24,465 --> 00:29:25,395
I hope you're enjoying a lot.

500
00:29:25,605 --> 00:29:26,355
Thank you very much.

