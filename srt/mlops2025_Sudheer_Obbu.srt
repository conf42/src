1
00:00:00,500 --> 00:00:01,130
Hello everyone.

2
00:00:01,189 --> 00:00:02,299
My name is bu.

3
00:00:02,599 --> 00:00:07,189
As a DevSecOps and multi-cloud
architect, I have spent the last 19 years

4
00:00:07,189 --> 00:00:11,700
helping companies architect secure and
optimize their cloud infrastructures.

5
00:00:12,200 --> 00:00:15,770
I have had opportunity to lead
some incredible projects from

6
00:00:15,770 --> 00:00:21,850
innovating gene based solutions
for AIOps to re-engineering high

7
00:00:21,850 --> 00:00:23,470
performance payment systems.

8
00:00:23,970 --> 00:00:29,709
The topic I'm here to discuss today is one
of the topic I'm deeply passionate about,

9
00:00:30,309 --> 00:00:36,609
securing our, a pipelines from development
all the way to production as we.

10
00:00:37,569 --> 00:00:39,969
Op operationalize machine learning.

11
00:00:40,719 --> 00:00:44,249
We will creating we are creating
a new kind of battlefield.

12
00:00:44,749 --> 00:00:50,800
The challenge is that our traditional
DevSecOps practices, which we have relayed

13
00:00:50,800 --> 00:00:57,044
on for years simply fail to address the
novel threats that ML ops introduces.

14
00:00:57,545 --> 00:01:01,745
We are facing an attack
surface that extend beyond

15
00:01:01,804 --> 00:01:04,475
code to our data, our models.

16
00:01:04,700 --> 00:01:09,950
And our entire infrastructure
today, we are going on a journey.

17
00:01:10,910 --> 00:01:15,820
We'll start with the reality
that you have vulnerable ML

18
00:01:15,820 --> 00:01:18,130
models, but we won't stay there.

19
00:01:18,850 --> 00:01:23,329
We are going to move from a state
of vulnerability to position

20
00:01:23,334 --> 00:01:28,839
of strength and creating what
I call unbreachable pipelines.

21
00:01:29,339 --> 00:01:35,840
We will do this by looking at real proven
security frameworks, seeing a security in

22
00:01:35,840 --> 00:01:41,870
action and leveraging battlefield tested
strategies used by Fortune 500 companies.

23
00:01:42,590 --> 00:01:47,400
My goal is simple to show you
how to stop playing defense and

24
00:01:47,400 --> 00:01:49,950
make your pipelines more robust.

25
00:01:50,450 --> 00:01:51,690
So how will.

26
00:01:52,325 --> 00:01:53,075
We get there.

27
00:01:53,575 --> 00:01:55,225
Here are our agenda for today.

28
00:01:55,415 --> 00:02:00,305
First we will define the problem
by looking at the expanding

29
00:02:00,305 --> 00:02:01,835
ML apps attack surface.

30
00:02:02,375 --> 00:02:06,965
Then we will introduce the solution,
adopting the zero trust mindset.

31
00:02:07,465 --> 00:02:10,645
The core idea of never
trust, always verify.

32
00:02:11,155 --> 00:02:15,705
Then we will get the practical
with the framework applying these

33
00:02:15,705 --> 00:02:17,535
principles across ML lifecycle.

34
00:02:18,465 --> 00:02:23,715
I will share the impact with the proven
results and real world case studies.

35
00:02:23,984 --> 00:02:29,614
And finally I will leave you with
the, with your roadmap actionable

36
00:02:29,614 --> 00:02:35,104
steps to get started on building
truly secure MOPS environment.

37
00:02:35,604 --> 00:02:37,909
So let's start with the core problem.

38
00:02:38,599 --> 00:02:42,799
Machine learning pipelines have created
a brand new security battlefield.

39
00:02:43,609 --> 00:02:49,649
The traditional DevSecOps practices
we have relied on for years simply

40
00:02:50,099 --> 00:02:52,559
failed to address these challenges.

41
00:02:53,059 --> 00:02:55,809
Think of your ML ops pipelines.

42
00:02:55,869 --> 00:03:00,459
Like this hose in a practic,
in perfect world clean.

43
00:03:00,835 --> 00:03:06,075
One valuable data flows through
it, through, through it smoothly.

44
00:03:06,575 --> 00:03:10,464
But in reality this pipelines
has multiple weak points.

45
00:03:10,914 --> 00:03:17,724
An attacker can target your training data,
poisoning it stealing, seal it outright.

46
00:03:18,224 --> 00:03:20,674
They can go after the model itself.

47
00:03:20,825 --> 00:03:23,405
Tampering the tampering with its logic.

48
00:03:23,905 --> 00:03:29,035
And stealing intellectual property,
they can compromise the underlying

49
00:03:29,035 --> 00:03:34,734
infrastructure where your model
is trained, or they can attack the

50
00:03:34,734 --> 00:03:41,965
interface endpoints where your model
makes live decisions leading to envision

51
00:03:41,965 --> 00:03:47,894
attacks, data leakage, the attack
surface has expanded beyond just code.

52
00:03:48,719 --> 00:03:52,350
And we, we need a new
strategy to defend it.

53
00:03:52,850 --> 00:03:54,799
So let's zoom in on this.

54
00:03:54,859 --> 00:03:55,730
Hidden threats.

55
00:03:56,230 --> 00:04:01,900
While we still worry about traditional
code, vulnerables ML Labs brings

56
00:04:02,410 --> 00:04:06,970
a new and dangerous class of high
risk threats to the forefront.

57
00:04:07,470 --> 00:04:11,280
Let's look at three major
ML lab specific threats.

58
00:04:12,090 --> 00:04:18,890
First data poisoning, which has a
staging sta staging 95 risk score.

59
00:04:19,390 --> 00:04:24,830
Imagine an attacker stop subscale
feeds your fraud detection model.

60
00:04:24,830 --> 00:04:29,090
Bad data, turning your fraud
detector into fraud enabler.

61
00:04:29,590 --> 00:04:34,409
Next is model theft with
an 85 5% of risk score.

62
00:04:34,909 --> 00:04:38,299
This is a direct attack on
your company, core business

63
00:04:38,299 --> 00:04:40,700
asset, your proprietary models.

64
00:04:40,969 --> 00:04:47,299
Finally, we have interface interference
attacks, carrying 90% risk score.

65
00:04:48,260 --> 00:04:56,059
This is where an attacker manipulate
a live model to get desired outputs or

66
00:04:56,690 --> 00:04:59,239
to extract confidential information.

67
00:04:59,659 --> 00:05:00,349
It was trained on.

68
00:05:00,849 --> 00:05:03,459
So how do we fight back?

69
00:05:03,959 --> 00:05:08,359
The solution is a fundamental
shift in mindset called zero trust.

70
00:05:08,859 --> 00:05:12,429
The core principle is simple but powerful.

71
00:05:12,699 --> 00:05:14,349
Never trust, always verify.

72
00:05:15,339 --> 00:05:20,769
It's a security model built
on maintaining strict access.

73
00:05:20,829 --> 00:05:23,739
Access controls, not trusting
anyone by de default.

74
00:05:24,384 --> 00:05:27,594
Even if they are already
inside your network.

75
00:05:28,094 --> 00:05:30,494
What does this means for ML apps?

76
00:05:30,554 --> 00:05:34,334
It means every single
action is scrutinized.

77
00:05:34,664 --> 00:05:40,184
Every request to access data or
deploy a model must be authenticated

78
00:05:40,684 --> 00:05:43,234
authorized and continuously verified.

79
00:05:43,734 --> 00:05:48,549
And the benefits is not
just a theoretical adopting.

80
00:05:48,699 --> 00:05:50,029
This approach has.

81
00:05:50,859 --> 00:05:55,749
Shown a cause of 70 by 73%
reduction in security instruments.

82
00:05:56,249 --> 00:06:02,520
This is not just about better security,
it's about building a more resilient and

83
00:06:02,520 --> 00:06:06,299
trustworthy AI systems from ground up.

84
00:06:06,799 --> 00:06:11,330
So it's important to understand
that applying zero trust

85
00:06:11,984 --> 00:06:14,824
is not a. Single action.

86
00:06:14,884 --> 00:06:19,734
It's a continuous practical
practice applied at every

87
00:06:19,734 --> 00:06:21,564
stage of ML Labs pipelines.

88
00:06:22,064 --> 00:06:26,394
We are going to break down the
implementation of zero trust into four

89
00:06:26,394 --> 00:06:29,424
key stages of machine learning life cycle.

90
00:06:30,264 --> 00:06:36,004
We'll start with the data and
preparation where we securely

91
00:06:36,004 --> 00:06:37,774
gather and prepare our data.

92
00:06:37,984 --> 00:06:41,284
Then we will move to model
training and validation.

93
00:06:41,784 --> 00:06:46,735
Ensure we train our models with
a strict action access controls.

94
00:06:47,005 --> 00:06:48,145
After that, we will cover

95
00:06:48,645 --> 00:06:54,554
deployment and survey serving where we
deploy models into secure environments.

96
00:06:54,824 --> 00:06:58,044
And then finally, we look at
monitoring and governance.

97
00:06:58,099 --> 00:07:04,469
Where we continuously watch watch over our
models to keep them in secure production.

98
00:07:04,969 --> 00:07:08,510
Let's start the start at
the foundation Data inion.

99
00:07:09,169 --> 00:07:14,480
The goal here is to fortify your
data foundation in machine learning.

100
00:07:15,080 --> 00:07:21,390
We all know the phrase, garbage in,
garbage out, but from a security

101
00:07:21,390 --> 00:07:23,040
perspective, it's far worse.

102
00:07:24,000 --> 00:07:28,650
It's the poisoning and poison
out, how do we apply zero trust?

103
00:07:28,650 --> 00:07:32,270
Here we build a SEC data security pyramid.

104
00:07:32,770 --> 00:07:39,260
At the base we'll use a strict identity
and access management to enforce

105
00:07:39,260 --> 00:07:41,929
authentication for data sources.

106
00:07:42,289 --> 00:07:49,240
Next, we enforce data encryption for
all data, both in transit addressed.

107
00:07:49,495 --> 00:07:50,275
Under trust.

108
00:07:50,424 --> 00:07:55,794
Finally, at the top we ensure
data, lineage and integrity.

109
00:07:56,454 --> 00:08:02,194
We need to be able to track data
origins and transformations using

110
00:08:02,554 --> 00:08:08,525
checksums to verify that the
data has not been tampered with

111
00:08:09,025 --> 00:08:11,544
stage two model training and validation.

112
00:08:12,505 --> 00:08:12,895
So now.

113
00:08:13,840 --> 00:08:16,659
We move to stage two model training.

114
00:08:17,159 --> 00:08:24,780
This is where your secret SaaS is
created, and our goal is to secure

115
00:08:25,380 --> 00:08:27,569
this environment at all cost.

116
00:08:28,069 --> 00:08:30,770
We do this in three key ways.

117
00:08:31,099 --> 00:08:34,309
First, we created
fortified training zones.

118
00:08:35,089 --> 00:08:40,429
Think of this as a isolated every
training job in a sandbox container.

119
00:08:40,924 --> 00:08:43,804
To prevent unauthorized access.

120
00:08:44,305 --> 00:08:51,575
We then server all, we serve all necessary
network access and scan every dependency.

121
00:08:52,355 --> 00:08:56,905
Second, we enforce the
principles of least p release.

122
00:08:57,405 --> 00:09:04,395
The A training script should only have
absolute minimum permissions if needed.

123
00:09:04,895 --> 00:09:09,265
To function, read access to
necessary data and right access to

124
00:09:09,265 --> 00:09:12,715
specific model artifact location.

125
00:09:13,215 --> 00:09:17,625
And the third, we focus on
mold model artifact security.

126
00:09:18,125 --> 00:09:22,925
Every model that is trained should
be digitally signed and versioned.

127
00:09:23,425 --> 00:09:28,555
Those models are then stored in a
secure and audited model registry.

128
00:09:29,305 --> 00:09:32,575
With its own strict access controls

129
00:09:33,075 --> 00:09:35,955
stage three deployment and serving.

130
00:09:36,455 --> 00:09:39,965
In stage three, we focus
on deployment and serving.

131
00:09:40,865 --> 00:09:45,345
This is where the model meets
the real world, interfacing

132
00:09:45,570 --> 00:09:48,000
with users and other systems.

133
00:09:48,510 --> 00:09:51,600
So our goal is to hard
these live endpoints.

134
00:09:52,100 --> 00:09:58,450
We start with the runtime protection model
should be deployed on hardened minimal

135
00:09:58,450 --> 00:10:03,820
container images using network policies
to restrict all unnecessary traffic.

136
00:10:04,750 --> 00:10:08,980
Then we secure the A
PA endpoint endpoints.

137
00:10:09,610 --> 00:10:14,855
Every single interface request must
require authentication and authorization.

138
00:10:15,640 --> 00:10:20,050
We also need to implement a
practical defense like rate limiting.

139
00:10:20,785 --> 00:10:23,365
And input validation.

140
00:10:23,995 --> 00:10:28,395
Finally, we need a secure
configuration management, things

141
00:10:28,395 --> 00:10:34,845
like a PA key keys and database
credentials should never be hardcoded.

142
00:10:35,345 --> 00:10:40,714
They must be stored in a managed,
in a dedicated secret manager like

143
00:10:40,714 --> 00:10:43,150
HashiCorp Walt or a W Secret manager.

144
00:10:43,650 --> 00:10:45,000
Monitoring and governance.

145
00:10:45,030 --> 00:10:47,790
Our final stage is
monitoring and governance.

146
00:10:48,290 --> 00:10:52,010
Here, our mindset has to shift.

147
00:10:52,510 --> 00:10:59,469
Our goal is to assume breach and to be
continuously look looking for threats.

148
00:11:00,099 --> 00:11:03,219
Security is not a static,
it's a living cycle.

149
00:11:03,719 --> 00:11:06,060
It start with the continuous monitoring.

150
00:11:06,719 --> 00:11:12,149
We need to log every request,
response, and access event

151
00:11:12,389 --> 00:11:15,239
across entire pipeline analysis.

152
00:11:15,739 --> 00:11:19,489
With that data, we perform
behavioral analysis.

153
00:11:20,425 --> 00:11:28,790
We actu, we actively look for anomalies
like model performance drift, and data

154
00:11:28,790 --> 00:11:33,579
skew, which can indicate a subtile attack.

155
00:11:34,079 --> 00:11:37,979
When anomaly detected, we
trigger automated response.

156
00:11:38,759 --> 00:11:43,559
Instead of waiting for a human,
we have automated workflows to

157
00:11:43,559 --> 00:11:47,999
mitigate threats such as review,
taking access, alerting the security

158
00:11:47,999 --> 00:11:50,429
team for further investigation.

159
00:11:50,929 --> 00:11:55,609
So what is the ultimate
impact of all this work?

160
00:11:56,104 --> 00:12:00,534
Adopting zero trust framework
does not, does more than

161
00:12:00,534 --> 00:12:03,084
just, we just reduce the risk.

162
00:12:03,474 --> 00:12:08,594
It's a fundamental build resiliency
by design, you shift your

163
00:12:08,594 --> 00:12:11,324
entire secure security posture.

164
00:12:11,824 --> 00:12:17,219
You move from reacting to threats
to a building, a pipeline, secure

165
00:12:17,459 --> 00:12:20,299
pipeline that expects and ies them.

166
00:12:20,799 --> 00:12:25,409
By verifying every action and enforcing
the principles of least privileged

167
00:12:25,409 --> 00:12:31,029
access, you harden your systems
agonist, both external internal threats.

168
00:12:31,574 --> 00:12:37,274
The result is a dramatically lowering of
your organization overall risk profile,

169
00:12:38,204 --> 00:12:45,884
and leading to that proven 73% reduction
in breaches we talked about earlier.

170
00:12:46,384 --> 00:12:49,464
So let's look at how
this works in real world.

171
00:12:50,004 --> 00:12:55,284
Let first FinTech leader, so
they challenge was immense.

172
00:12:56,214 --> 00:13:00,024
They need to be protected
and sensitive financial data,

173
00:13:00,804 --> 00:13:05,724
proprietary fraud detection models by
implementing a zero trust strategy.

174
00:13:06,054 --> 00:13:07,029
Their solution was.

175
00:13:07,404 --> 00:13:15,984
To implement a strict IEM and sign
all model artifacts, the result they

176
00:13:15,984 --> 00:13:22,614
eliminate, they eliminated all major
audit findings on data access controls

177
00:13:22,704 --> 00:13:30,114
and reduce the time they team spent on
manual com compliance reporting by 40%.

178
00:13:30,614 --> 00:13:32,664
Next consider a healthcare platform.

179
00:13:33,164 --> 00:13:37,474
The challenge was complying
with hipaa while using a

180
00:13:37,474 --> 00:13:39,994
patient data for a diagnostics.

181
00:13:40,494 --> 00:13:45,654
Their solution was to deploy models
in isolated environments with their

182
00:13:45,654 --> 00:13:47,454
token based a PA authentication.

183
00:13:47,954 --> 00:13:50,149
The outcome was a huge success.

184
00:13:51,019 --> 00:13:55,399
They achieved a HIPAA compliance, and
most importantly, secure sensitive

185
00:13:55,399 --> 00:13:56,929
patient healthcare information.

186
00:13:57,429 --> 00:14:03,029
So this might seem like a lot,
but you can begin with journey

187
00:14:03,029 --> 00:14:07,109
today with four actionable steps.

188
00:14:07,609 --> 00:14:13,059
This is your roadmap to zero
test ML Labs, the first assess

189
00:14:13,059 --> 00:14:15,459
and baseline identify all.

190
00:14:15,959 --> 00:14:21,779
Components in your ML pipeline
and map current access controls

191
00:14:21,839 --> 00:14:24,389
to find high risk areas.

192
00:14:25,079 --> 00:14:31,159
Second, implement a strong
identity, assign unique identity

193
00:14:31,159 --> 00:14:36,679
to all users and services and
enforce multifactor authentication.

194
00:14:37,489 --> 00:14:43,339
Third, enforce least privileged
access grant only minimal.

195
00:14:44,104 --> 00:14:46,924
Permissions required for each task.

196
00:14:47,104 --> 00:14:50,854
Use just in time access
where it is possible.

197
00:14:51,354 --> 00:14:54,654
And fourth, monitor and respond.

198
00:14:55,044 --> 00:15:00,114
Implement continuous logging and
anom anomaly detection to identify

199
00:15:00,654 --> 00:15:03,314
and contain threats in real time.

200
00:15:03,814 --> 00:15:04,650
The good news is.

201
00:15:05,629 --> 00:15:10,069
You don't have to build all
the, all this from scratch.

202
00:15:10,669 --> 00:15:14,899
There is a rich ecosystem of
security tools you can leverage

203
00:15:14,899 --> 00:15:19,699
to implement this framework for
identity and access management.

204
00:15:19,759 --> 00:15:24,144
You have tools like Okta, Azure
active Directory, HashiCorp Vault,

205
00:15:24,644 --> 00:15:30,124
and AWS or GCP, secret Manager
for code and container security.

206
00:15:30,484 --> 00:15:32,644
You can use scanners like sync.

207
00:15:33,484 --> 00:15:40,575
Three V and dependent bot for networks
and runtime security services.

208
00:15:40,625 --> 00:15:48,035
Service mesh O are linker can handle
encryption and policies where while

209
00:15:48,215 --> 00:15:50,375
Falco can detect runtime threats.

210
00:15:50,765 --> 00:15:58,204
And finally, for data model governance
tools like DVC can track data lineage.

211
00:15:59,089 --> 00:16:05,060
While ML flow or clear ML
can provide secure model,

212
00:16:05,560 --> 00:16:10,659
as you start on this path, there
are like, there are a few common

213
00:16:10,780 --> 00:16:14,020
pitfalls I wanted to avoid first.

214
00:16:14,050 --> 00:16:15,795
Don't ignore the human element.

215
00:16:16,295 --> 00:16:20,155
If you create so much friction,
your data scientist will.

216
00:16:20,655 --> 00:16:24,825
Build insecure shadow IT workflows.

217
00:16:25,755 --> 00:16:32,115
The solution is to automate security
within the CACD pipeline and prove

218
00:16:32,175 --> 00:16:34,905
secure by default project templates.

219
00:16:35,865 --> 00:16:37,140
Second, the set.

220
00:16:37,140 --> 00:16:39,245
The set, set and forget mentality.

221
00:16:39,745 --> 00:16:39,865
Zero.

222
00:16:39,865 --> 00:16:43,675
Trust is continuous process,
not a one time setup.

223
00:16:44,215 --> 00:16:49,165
You must regularly logs,
rotate credentials, and perform

224
00:16:49,225 --> 00:16:51,445
automated security drills.

225
00:16:52,075 --> 00:16:58,045
Finally, remember to treat models
differently than regular code.

226
00:16:58,545 --> 00:17:04,275
A signed container is good,
but it doesn't prevent a model

227
00:17:04,275 --> 00:17:06,105
from being stolen and poisoned.

228
00:17:06,605 --> 00:17:10,685
You need model specific controls
like digital signature for.

229
00:17:11,195 --> 00:17:17,255
The artifact themself and actively
monitor for performance drift.

230
00:17:17,755 --> 00:17:25,285
So as we wrap up, I wanted to leave
you with the core four takeaways,

231
00:17:25,885 --> 00:17:31,125
which you can think of as the
MLF security pyramid at the base.

232
00:17:31,225 --> 00:17:35,005
I understand that ML maps
attack surface is unique.

233
00:17:35,505 --> 00:17:41,985
It extends beyond code to
data models, infrastructure

234
00:17:42,705 --> 00:17:44,805
requiring a specialized approach.

235
00:17:45,465 --> 00:17:50,115
The next layer is embracing
the zero trust mindset.

236
00:17:51,075 --> 00:17:55,930
Never trust, always verify is the
guiding principle to effectively

237
00:17:56,070 --> 00:17:58,170
secure modern AA systems.

238
00:17:58,670 --> 00:18:05,060
Building on that, remember that securities
security is a life cycle, not checkbox.

239
00:18:05,560 --> 00:18:10,780
You must apply zero trust controls
at every stage from data ingestion to

240
00:18:10,780 --> 00:18:17,740
production monitoring, and finally, at
the top know the results are proven.

241
00:18:18,520 --> 00:18:23,140
Adopting this framework is
a strategic investment that.

242
00:18:23,640 --> 00:18:28,650
Secure and reduce the secure
and reduce security incidents.

243
00:18:29,150 --> 00:18:31,790
So that brings to me
end of my presentation.

244
00:18:31,840 --> 00:18:36,435
We have covered unique threats
facing ML lops and laid out a

245
00:18:36,435 --> 00:18:42,135
comprehensive proven framework to
make your a pipelines unbreachable.

246
00:18:42,635 --> 00:18:43,715
Thanks for your time.

247
00:18:44,215 --> 00:18:47,245
I would like to be happy
to answer any questions.

248
00:18:47,295 --> 00:18:50,355
You can reach out to me in
my LinkedIn or like my email.

249
00:18:50,355 --> 00:18:50,595
Id.

250
00:18:51,195 --> 00:18:51,585
Thank you guys.

251
00:18:52,085 --> 00:18:52,375
Okay.

