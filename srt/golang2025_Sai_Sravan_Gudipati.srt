1
00:00:00,120 --> 00:00:00,690
Hello everyone.

2
00:00:01,110 --> 00:00:04,880
Today we're gonna go through some of
the cloud data engineering, aspects.

3
00:00:05,300 --> 00:00:08,120
and I'm gonna walk you through
a couple of challenges.

4
00:00:08,189 --> 00:00:12,779
couple of latest innovations and how
cloud data engineering is transforming,

5
00:00:12,999 --> 00:00:18,519
businesses across globally, and
what are some of the benefits that

6
00:00:18,519 --> 00:00:23,070
typically, businesses would see,
in terms of, moving into the cloud.

7
00:00:23,535 --> 00:00:27,345
Data engineering side of things,
Mainly the cloud data engineering

8
00:00:27,345 --> 00:00:31,245
is it's also driving innovation by
enabling organizations to manage data

9
00:00:31,305 --> 00:00:33,405
efficiently and cost effectively.

10
00:00:33,885 --> 00:00:37,065
So let's take multiple
platforms that we have today.

11
00:00:37,125 --> 00:00:40,650
So we have AWS, we have
GCP, we have Azure.

12
00:00:41,280 --> 00:00:46,210
we also have several platforms that are
built on top of, these different cloud

13
00:00:46,240 --> 00:00:49,330
platforms like, snowflake, Databricks.

14
00:00:49,780 --> 00:00:52,300
One of the very good examples
that we could also consider is

15
00:00:52,300 --> 00:00:55,050
Netflix, as a, very good use case.

16
00:00:55,380 --> 00:00:59,320
So the kind of, so organizations
like Netflix, they basically,

17
00:00:59,370 --> 00:01:03,280
exemplify the cloud data engineering
potential by delivering, highly

18
00:01:03,280 --> 00:01:07,450
personalized content experiences
to millions of users worldwide.

19
00:01:07,840 --> 00:01:11,440
And they also very efficiently
manage the data streams.

20
00:01:11,770 --> 00:01:13,630
by leveraging the cloud technologies.

21
00:01:13,630 --> 00:01:18,500
So you would've already known that
architecture for Netflix is, it basically

22
00:01:18,500 --> 00:01:23,020
runs on a. That's, that, that is
something that kind of helped Netflix

23
00:01:23,020 --> 00:01:27,509
to maintain that industry leadership
by quickly adapting to the, consumer

24
00:01:27,509 --> 00:01:32,115
trends, latest technology, maintaining,
a serverless kind of an architecture.

25
00:01:32,545 --> 00:01:35,934
so they were able to reduce the
amount of investment that has to be

26
00:01:35,934 --> 00:01:40,195
done from a physical hardware and
then just have everything to cloud.

27
00:01:40,544 --> 00:01:46,994
so that is, That is one very good
example of how an organization has really

28
00:01:46,994 --> 00:01:52,354
transformed, by moving away from the
on-premises, hardware and then having

29
00:01:52,354 --> 00:01:55,034
the data completely, residing in cloud.

30
00:01:55,394 --> 00:01:58,074
So without any delay,
let's get started with.

31
00:01:58,454 --> 00:02:03,614
some of the, the challenges, what
we typically see with the data on

32
00:02:03,614 --> 00:02:07,194
a day-to-day basis, what is the
whole cloud data engineering about?

33
00:02:07,194 --> 00:02:09,444
And, what do we, Where do we stand today?

34
00:02:09,744 --> 00:02:10,944
Right across globally.

35
00:02:11,514 --> 00:02:15,704
So if you look at some of the
metrics here, that I have, so

36
00:02:15,764 --> 00:02:19,664
approximately 463 million terabytes
of data generated globally.

37
00:02:19,984 --> 00:02:22,684
and this data could be from
various sources like video,

38
00:02:22,684 --> 00:02:25,534
text, audio, streaming content.

39
00:02:25,594 --> 00:02:26,119
It could be.

40
00:02:26,684 --> 00:02:28,264
various number of, sources.

41
00:02:28,564 --> 00:02:32,704
As you can see, it's four 63
million terabytes and this continues

42
00:02:32,704 --> 00:02:34,324
to grow exponentially, right?

43
00:02:34,834 --> 00:02:39,424
So this data explosion challenges racial
data management methods, prompting

44
00:02:39,424 --> 00:02:43,744
businesses to seek advanced cloud
solutions for effective data handling.

45
00:02:43,884 --> 00:02:47,844
the global cloud computing market
is predicted to reach around 2.84

46
00:02:47,844 --> 00:02:52,984
trillion by 2030, which is Less than
a decade, like half of a decade.

47
00:02:53,294 --> 00:02:56,924
to be, to put it precisely, this
kind of highlights, this urgent

48
00:02:56,924 --> 00:02:59,144
demand for robust data solutions.

49
00:02:59,424 --> 00:03:05,014
we, with the recent, trend or with
the recent, increase in the usage of

50
00:03:05,014 --> 00:03:09,874
social media like, Twitter, Facebook,
Instagram, TikTok, all of these

51
00:03:09,874 --> 00:03:13,734
different platforms, they they widely
demonstrate this, data exposure.

52
00:03:14,154 --> 00:03:16,504
They generate and process
vast amounts of data.

53
00:03:16,764 --> 00:03:20,704
and it could be on a daily basis, on
an hourly basis, it would basically

54
00:03:20,704 --> 00:03:22,594
go on what's basically trending.

55
00:03:22,814 --> 00:03:26,504
the data would exponentially increase
within an hour or within a day.

56
00:03:26,814 --> 00:03:31,504
and also, that is just one,
given use case of how, the social

57
00:03:31,504 --> 00:03:32,914
media platforms react to it.

58
00:03:33,184 --> 00:03:34,804
But we do also have like several.

59
00:03:35,534 --> 00:03:39,194
healthcare industries, that kind of
heavily rely on the cloud technologies

60
00:03:39,194 --> 00:03:43,974
to manage, the different patient
records, apply some AI on top of

61
00:03:43,974 --> 00:03:46,404
it to, AI and as well as big data.

62
00:03:46,754 --> 00:03:50,964
AI has recently started, but I wanted
to emphasize both on the AI and the

63
00:03:50,964 --> 00:03:56,124
big data side of things where most of
the patient records are managed very

64
00:03:56,124 --> 00:04:01,314
efficiently and also, there are very,
There are many predictive analytics or

65
00:04:01,314 --> 00:04:05,774
predictive, applications that kind of,
help to improve the patient outcomes

66
00:04:05,774 --> 00:04:07,304
and also the swim and the operations.

67
00:04:07,964 --> 00:04:11,264
Now, the other business plan that
we're gonna talk about is also

68
00:04:11,264 --> 00:04:12,764
on the financial institutions.

69
00:04:13,064 --> 00:04:16,474
they also depend on several cloud
solutions to detect the, real

70
00:04:16,474 --> 00:04:20,124
time, fraud, risk management, and
also the transaction processing.

71
00:04:20,544 --> 00:04:24,524
So let's take an example for one of the
large scale organization like Capital

72
00:04:24,524 --> 00:04:32,384
One, which has a huge, very huge, base or
AWS as we went through three different,

73
00:04:32,434 --> 00:04:36,764
business lines, healthcare, financial,
and as well as social media, this kind

74
00:04:36,764 --> 00:04:39,074
of ex extends across multiple sectors.

75
00:04:39,074 --> 00:04:41,714
What I have given is only a
bunch of examples and sample.

76
00:04:42,214 --> 00:04:44,824
Kind of bunch of examples, but
it extends to multiple sectors.

77
00:04:45,184 --> 00:04:47,854
this would basically import
the organizations to turn

78
00:04:47,854 --> 00:04:49,924
data into actionable insights.

79
00:04:50,144 --> 00:04:51,824
maybe they could build some dashboards.

80
00:04:51,824 --> 00:04:56,704
They could, build some analytics, or they
could, build an application such a way

81
00:04:56,704 --> 00:05:01,354
that takes a preventive action against
it, Making the decision effectively

82
00:05:01,444 --> 00:05:05,854
and driving the innovation as well,
in wherever, areas as applicable.

83
00:05:06,154 --> 00:05:11,444
So that is the main reason why embracing
the cloud technologies becomes like really

84
00:05:11,444 --> 00:05:16,924
critical for businesses that kind of
seek, sustainable growth and operation

85
00:05:16,924 --> 00:05:18,874
efficiency in a data driven organization.

86
00:05:19,314 --> 00:05:24,054
so most of the organizations, I
would say, In the recent times,

87
00:05:24,114 --> 00:05:28,234
even if you take the large Fortune
500, most of them are data-driven.

88
00:05:28,524 --> 00:05:33,394
I think that's how, most of the,
large scale enterprises are expected

89
00:05:33,394 --> 00:05:34,924
to be seen, that they're data driven.

90
00:05:35,204 --> 00:05:35,944
and that's.

91
00:05:36,924 --> 00:05:40,494
That's where the whole operational
efficiency and everything comes into play.

92
00:05:41,244 --> 00:05:47,734
Now, let's also, take, go through
in the next slide on how, we wanted

93
00:05:47,734 --> 00:05:49,744
to see infrastructure aspects of it.

94
00:05:50,074 --> 00:05:53,074
Like how this whole
thing had started, right?

95
00:05:53,374 --> 00:05:53,974
So we.

96
00:05:54,474 --> 00:06:00,664
When we go back to the old days, it was
basically when, even within mar, within

97
00:06:00,664 --> 00:06:06,584
modernization, if I were to procure
a Linux server, someone has to build

98
00:06:06,584 --> 00:06:11,484
a vblock for it, allocate some memory
for it, there's a human effort that is

99
00:06:11,484 --> 00:06:14,154
technically involved in such a situation.

100
00:06:14,654 --> 00:06:19,394
it could take anywhere from,
four weeks to eight weeks, right?

101
00:06:19,394 --> 00:06:21,194
For a fiscal infrastructure to happen.

102
00:06:21,704 --> 00:06:25,684
Now, when this whole cloud computing
has started, the infrastructure

103
00:06:25,684 --> 00:06:29,844
as a code, can, revolutionized
how, infrastructure can be.

104
00:06:30,514 --> 00:06:35,349
so it the traditional infrastructure
management by providing, automated

105
00:06:35,349 --> 00:06:36,669
infrastructure provisioning.

106
00:06:37,019 --> 00:06:40,169
Through programmable scripts
like Terraform for an example.

107
00:06:40,199 --> 00:06:44,349
So Terraform primarily focuses on,
having the infrastructure as a code,

108
00:06:44,679 --> 00:06:46,569
automating the builds and everything.

109
00:06:46,869 --> 00:06:54,609
So it, the main good part or the,
the revolution that infrastructure

110
00:06:54,609 --> 00:06:56,349
as a code has really brought in.

111
00:06:56,849 --> 00:07:02,629
It would really transform how, businesses
manage it, resources, implementing.

112
00:07:02,939 --> 00:07:07,309
ISC would also reduce the need to
physically deploy something, the amount

113
00:07:07,309 --> 00:07:11,479
of time that is needed to deploy and
also minimize the number of configuration

114
00:07:11,539 --> 00:07:14,339
errors or, human error from mistakes.

115
00:07:14,579 --> 00:07:16,469
Now, there could be some situations where.

116
00:07:17,014 --> 00:07:21,234
When someone is doing the ISE
code, basically, someone could

117
00:07:21,234 --> 00:07:24,484
make a programmatic mistake, but
then that's where we, we could

118
00:07:24,484 --> 00:07:26,014
basically do some peer reviews.

119
00:07:26,204 --> 00:07:30,824
we can basically have some checks
against what we are trying to do

120
00:07:31,064 --> 00:07:34,294
and then basically determine, detect
all of these issues beforehand.

121
00:07:34,704 --> 00:07:37,524
another example is also
the A Ws cloud formation.

122
00:07:37,644 --> 00:07:41,844
enabling enterprises worldwide to automate
consistent infrastructure deployments

123
00:07:42,114 --> 00:07:43,974
effectively across multiple environments.

124
00:07:44,244 --> 00:07:47,904
when, if we go back to the previous
slide where I was talking about Netflix.

125
00:07:48,114 --> 00:07:51,694
So Netflix similarly utilizes
ISC to manage, thousands of cloud

126
00:07:51,694 --> 00:07:53,804
servers globally, because it's not.

127
00:07:54,184 --> 00:07:57,514
It's not truly possible when you
have a bunch of servers that are

128
00:07:57,514 --> 00:08:00,844
lying across globally in different
regions, in different zones.

129
00:08:01,174 --> 00:08:06,394
and, keeping a tab of them unless
it is truly automated and is truly,

130
00:08:06,424 --> 00:08:08,494
code driven, which is the IAC.

131
00:08:09,184 --> 00:08:14,184
Now, it also helps to significantly,
enhance the disaster, recover

132
00:08:14,184 --> 00:08:16,764
capabilities, reduces the system downtime.

133
00:08:17,089 --> 00:08:20,649
And fosters better collaboration
among, technical teams.

134
00:08:20,859 --> 00:08:24,689
So one good example that I can
provide is, in our organization

135
00:08:24,809 --> 00:08:28,899
at least, we have basically seen
a use case where, the most of the

136
00:08:28,899 --> 00:08:30,909
infrastructure code is already ready.

137
00:08:31,129 --> 00:08:34,279
We would just prepare a J and
submit the J to the code and it

138
00:08:34,369 --> 00:08:35,959
would build out a server for us.

139
00:08:35,989 --> 00:08:41,179
so that's truly the innovation of how
infrastructure as code has brought in.

140
00:08:41,629 --> 00:08:46,129
That's truly a revolution that really
happened when we go back in time and look

141
00:08:46,129 --> 00:08:50,089
at how traditionally things were done
versus how infrastructure as a code has

142
00:08:50,179 --> 00:08:53,749
evolved over a period of time right now.

143
00:08:54,139 --> 00:08:56,859
Let's talk about, modern
data pipeline architecture.

144
00:08:57,399 --> 00:09:00,669
so for the Modern Data Pipeline
architecture, we know we have

145
00:09:00,669 --> 00:09:02,569
like different forms of, data.

146
00:09:02,819 --> 00:09:04,910
we have different forms of, data formats.

147
00:09:05,100 --> 00:09:11,420
e either it could be CSV, it could be,
xml, it could be Jason, The very beginning

148
00:09:11,420 --> 00:09:17,830
need, to look at the data pipeline, side
of aspects is, to consider that, we would

149
00:09:17,830 --> 00:09:23,140
have to efficiently process different
types of, different types of data formats,

150
00:09:23,220 --> 00:09:25,110
getting in this set on a continuous basis.

151
00:09:25,500 --> 00:09:29,850
So let's go through a couple of
examples where, I can walk you through.

152
00:09:30,350 --> 00:09:35,660
Excuse me, I can walk you through a very
good example of, data tion and as well

153
00:09:35,660 --> 00:09:39,890
as, the processing, the transformation,
the storage, the storage optimization,

154
00:09:40,430 --> 00:09:41,720
the analytics under consumption.

155
00:09:42,170 --> 00:09:45,550
So let's take one single
platform as an example.

156
00:09:45,970 --> 00:09:50,870
So I wanna pick Snowflake for the purpose
of our, When we start developing, a

157
00:09:50,870 --> 00:09:55,160
Snowflake application, we would need
an underlying cloud, storage platform.

158
00:09:55,640 --> 00:09:58,970
Let it be A-W-S-G-C-P or Azure.

159
00:09:59,150 --> 00:10:00,080
It doesn't really matter.

160
00:10:00,390 --> 00:10:02,580
but for our discussion, we
are gonna consider a Ws.

161
00:10:03,120 --> 00:10:07,600
If I have five different, applications
that are dumping data into an SL

162
00:10:07,660 --> 00:10:13,130
bucket, which are of different data
types, and I want to build something

163
00:10:13,490 --> 00:10:17,740
that would technically, processes the
data in a real time format, right?

164
00:10:17,980 --> 00:10:21,370
So that is where all of this
data would essentially come in,

165
00:10:21,600 --> 00:10:23,490
as real time as possible from.

166
00:10:23,540 --> 00:10:27,120
diverse sources, with scaling
opportunities and capabilities as well.

167
00:10:27,560 --> 00:10:28,620
now, the.

168
00:10:29,120 --> 00:10:33,020
the data that has coming through as
part of the data inges that has to be

169
00:10:33,050 --> 00:10:34,730
processed, that has to be transformed.

170
00:10:35,150 --> 00:10:39,760
so we would apply whatever, business
logic or implementation that is applicable

171
00:10:40,060 --> 00:10:43,720
and we would basically go about,
running transformations accordingly.

172
00:10:44,170 --> 00:10:47,800
Now, that is one very
good advantage of how.

173
00:10:48,570 --> 00:10:51,975
cloud data or cloud data engineering
solutions will help us as we are

174
00:10:51,975 --> 00:10:53,535
not restricted to any compute.

175
00:10:53,535 --> 00:10:57,815
We would, basically have a lot of
autoscaling, autoscaling of the warehouse

176
00:10:57,815 --> 00:11:00,305
size, autoscaling of the instant size.

177
00:11:00,515 --> 00:11:05,805
So we are not worried truly about,
okay, I'm gonna hit about a, 50

178
00:11:05,805 --> 00:11:07,425
million users within the next minute.

179
00:11:07,815 --> 00:11:11,485
Now I have an on-prem infrastructure,
I don't think my on-prem

180
00:11:11,515 --> 00:11:12,775
infrastructure is gonna handle.

181
00:11:13,690 --> 00:11:17,800
So we would technically procure another
three or four different servers and then

182
00:11:17,800 --> 00:11:19,630
just have them idle setting out there.

183
00:11:19,960 --> 00:11:24,730
But in the case of cloud, the instances
would autoscale to whatever size it needs

184
00:11:24,730 --> 00:11:29,620
to be scaled, like from extra small to
an extra large in the event the demand

185
00:11:29,680 --> 00:11:31,540
increases and then scale back down.

186
00:11:32,395 --> 00:11:36,965
So end of today, the main goal, is
to bring in business efficiency in

187
00:11:36,965 --> 00:11:40,785
terms of how we are operating it
and if the scalability brings, the

188
00:11:40,785 --> 00:11:44,275
bridge between, the user demand
and the business efficiency.

189
00:11:44,335 --> 00:11:48,800
I think that itself, the cloud data
engineering, from Ws as an example.

190
00:11:49,345 --> 00:11:50,455
It does a great job.

191
00:11:50,645 --> 00:11:54,145
that's a real benefit and advantage of,
the modern data architecture pipeline.

192
00:11:54,775 --> 00:11:57,835
Now we are also gonna talk
about storage optimization.

193
00:11:58,225 --> 00:12:02,185
So storage optimization, technically,
there is no retention period.

194
00:12:02,185 --> 00:12:05,005
There is no cleanup that
automatically happens on an on-prem.

195
00:12:05,505 --> 00:12:06,405
storage systems.

196
00:12:06,705 --> 00:12:10,905
So typically we would end up doing
like just store the stale data over

197
00:12:10,905 --> 00:12:14,265
and over a period of time and there's
no controls or restriction around it.

198
00:12:14,625 --> 00:12:18,905
So that is one big drawback of,
having an, having an on-prem

199
00:12:18,905 --> 00:12:22,785
system, but then with the cloud
data engineering, with a multi-tier

200
00:12:22,815 --> 00:12:27,370
architecture, we could have that
automated data movement, by looking at.

201
00:12:27,740 --> 00:12:30,660
some of the patterns that
are really impacted, and.

202
00:12:31,160 --> 00:12:34,880
the most important thing is the analytics
and the consumption of the data.

203
00:12:35,270 --> 00:12:39,740
So we could get like different sources
of data, kind of data transform,

204
00:12:39,960 --> 00:12:42,250
have, business logic applied to it.

205
00:12:42,480 --> 00:12:47,800
feed the data into a Power BI dashboard
or any analytics dashboard, that we, that

206
00:12:47,860 --> 00:12:50,120
whatever the enterprise, plans to use it.

207
00:12:50,620 --> 00:12:52,810
And then technically
generate analytics out of it.

208
00:12:53,260 --> 00:12:58,140
So this would also reduce, the need for
writing custom scripts or custom data.

209
00:12:58,470 --> 00:13:03,230
So in, if I were to go back in time
and basically see, someone would have

210
00:13:03,230 --> 00:13:07,580
to write a custom script to basically
see, to generate those charts and

211
00:13:07,580 --> 00:13:11,770
for analytics and to run SQL queries
or, run any kind of data operations.

212
00:13:11,770 --> 00:13:16,720
The data analytics is gonna really play
important and crucial role, especially

213
00:13:16,720 --> 00:13:18,190
for the financial organizations.

214
00:13:18,220 --> 00:13:20,080
As I mentioned, a use case earlier.

215
00:13:20,290 --> 00:13:22,610
So it's gonna truly bring
out a lot of, value add.

216
00:13:23,375 --> 00:13:25,835
No, elastic scalability benefits.

217
00:13:26,195 --> 00:13:28,745
as I was talking about the scalability
in the previous slide as well, I'm

218
00:13:28,745 --> 00:13:33,115
gonna, cover this quickly because, went
through a little bit deep previously.

219
00:13:33,365 --> 00:13:37,095
resource optimization, having the
dynamic allocation, ensuring the

220
00:13:37,125 --> 00:13:38,655
perfect resource to workload matching.

221
00:13:38,655 --> 00:13:40,935
There is the Netflix use
case I was talking about.

222
00:13:41,125 --> 00:13:43,735
the users can increase
in any minute going from.

223
00:13:44,035 --> 00:13:45,985
A hundred users to a million users.

224
00:13:46,225 --> 00:13:49,565
So the resource optimization is
the prime benefit of, having the

225
00:13:49,565 --> 00:13:51,965
scalability within a cloud data pipeline.

226
00:13:52,225 --> 00:13:56,775
and there's also a significant
reduction in cost because the

227
00:13:56,835 --> 00:13:57,930
most of the instances are all.

228
00:13:58,430 --> 00:13:59,030
On demand.

229
00:13:59,340 --> 00:14:02,880
so technically we could scale back
to the lowest server as possible.

230
00:14:03,150 --> 00:14:07,020
And for most part, all of the servers
are like very, pretty high available.

231
00:14:07,250 --> 00:14:10,840
they have a very good availability,
because, most of the cloud platforms

232
00:14:10,840 --> 00:14:13,100
have a dual zone, a deployment, a setup.

233
00:14:13,470 --> 00:14:18,510
like for example, if I have a server in
US East, I would also have AA in West.

234
00:14:18,990 --> 00:14:23,405
If the, in the US East goes down
a, that is really and readily.

235
00:14:23,905 --> 00:14:27,435
To, to switch over the region
and then give that seamless

236
00:14:27,435 --> 00:14:29,235
experience to the user, right?

237
00:14:29,955 --> 00:14:33,195
So those are mainly the
elastic scalability benefits.

238
00:14:33,495 --> 00:14:37,585
Now, we're gonna go through some of
the, mission learning, operations,

239
00:14:37,745 --> 00:14:42,035
and how ML lops acceleration can be
achieved through the cloud data pipeline.

240
00:14:42,565 --> 00:14:47,525
in a typical machine learning, application
or, the lifecycle, we would need to

241
00:14:47,525 --> 00:14:50,185
have, a model that we would develop.

242
00:14:50,575 --> 00:14:53,985
the model would be continuously,
improved upon based on user

243
00:14:53,985 --> 00:14:55,335
feedbacks and user inputs.

244
00:14:56,230 --> 00:15:00,810
We would also have to do some, monitoring
around it, to track if the model is

245
00:15:00,810 --> 00:15:02,700
deviating from any kind of responses.

246
00:15:03,120 --> 00:15:05,010
ability to do the continuous integration.

247
00:15:05,060 --> 00:15:09,970
let's say, let's take a use case
where I just want to get, some data

248
00:15:09,970 --> 00:15:11,500
generated through machine learning.

249
00:15:11,940 --> 00:15:14,620
technically what are the,
different steps that we go through.

250
00:15:14,620 --> 00:15:15,400
We need a model.

251
00:15:15,690 --> 00:15:20,450
it has, the model has to be developed
and it, it has to be developed, it has

252
00:15:20,450 --> 00:15:25,480
to evolve, it has to grow, and it has
to get the feedback that it needs to get

253
00:15:25,480 --> 00:15:27,070
for the model to operate efficiently.

254
00:15:27,550 --> 00:15:30,880
Then we would go into the
continuation, sorry, continuous

255
00:15:30,880 --> 00:15:34,860
integration aspect where, it'll be
integrated into multiple sources.

256
00:15:35,160 --> 00:15:38,960
And also we would also enable like
automated testing, which will basically

257
00:15:38,960 --> 00:15:44,940
improve the model sufficiency and
quality and the, reproducibility Then.

258
00:15:45,360 --> 00:15:47,340
The deployment of the model itself.

259
00:15:47,490 --> 00:15:51,280
Now, typically, as with any other
software lifecycle, we would have a test

260
00:15:51,280 --> 00:15:52,720
environment and a deploy environment.

261
00:15:53,020 --> 00:15:56,530
And the deployment deploy environment or
the production environment and production

262
00:15:56,530 --> 00:15:59,480
environment is something that we truly
really, roll it out to the users.

263
00:15:59,480 --> 00:16:00,860
So that's, that's a deployment.

264
00:16:00,950 --> 00:16:04,750
And the monitoring, of course, how
the model is behaving, how does

265
00:16:04,750 --> 00:16:08,180
it act in terms of the different,
data flows that are coming in.

266
00:16:08,570 --> 00:16:12,090
So how does this all tie
up to the, cloud data?

267
00:16:12,590 --> 00:16:16,430
Engineering as a whole is basically
most of the cloud platforms today.

268
00:16:16,430 --> 00:16:20,950
Right now, they have inbuilt
integrated, ML ops capabilities.

269
00:16:21,190 --> 00:16:25,240
So ML lops is an area where it is
combining the machine learning and

270
00:16:25,240 --> 00:16:26,950
the operation software together.

271
00:16:27,310 --> 00:16:31,290
So it gives a great deal of,
advantage, for folks who are truly

272
00:16:31,290 --> 00:16:35,630
interested with the lops, development
or, defining the morals as such.

273
00:16:36,130 --> 00:16:40,420
Now, as, as much as everyone
would be interested in moving to a

274
00:16:40,420 --> 00:16:43,790
serverless architecture with less
maintenance and everything, the other

275
00:16:43,790 --> 00:16:49,040
main aspect that also truly really
plays a role is how secure is, are

276
00:16:49,040 --> 00:16:50,510
these cloud environments, right?

277
00:16:50,730 --> 00:16:55,180
because we are truly relying on, based
on the consumer versus a producer

278
00:16:55,540 --> 00:16:59,590
or if something's developed inhouse
as well, we depending upon, not

279
00:16:59,650 --> 00:17:03,030
in-house, which is, which could be
the case for an on-prem environment,

280
00:17:03,270 --> 00:17:05,970
but we depending over the cloud,
which could be a third party provider.

281
00:17:06,480 --> 00:17:12,400
Now that, that kind of brings in,
a very good, aspect of discussion,

282
00:17:12,400 --> 00:17:13,660
which is about the security.

283
00:17:14,500 --> 00:17:19,430
Now we could basically have, A
couple of options here, that we

284
00:17:19,430 --> 00:17:24,150
could expand upon how the cloud
security can be, enforced, can be

285
00:17:24,150 --> 00:17:26,190
made resilient and can be made better.

286
00:17:26,280 --> 00:17:30,030
So one of them is being, is the
AI powered threat detection.

287
00:17:30,330 --> 00:17:34,680
we could develop like machine learning
algorithms that basically identify and

288
00:17:34,680 --> 00:17:40,090
as well as neutralize the accuracy,
reducing, the security breaches.

289
00:17:40,445 --> 00:17:46,735
So typically, the model or the machine
learning algorithm gets fed of all the

290
00:17:46,735 --> 00:17:51,505
different possible, scenarios around the
different threats that are really for,

291
00:17:51,605 --> 00:17:53,975
the different threats that could occur.

292
00:17:54,415 --> 00:18:01,575
and the model is continuously monitoring
the security, The model or the application

293
00:18:01,575 --> 00:18:06,105
would continuously monitor the threats
and the application with as much

294
00:18:06,105 --> 00:18:10,605
maximum accuracy as possible, thereby
reducing the security breach, right?

295
00:18:11,115 --> 00:18:14,955
And the second option is we could
follow a zero trust architecture.

296
00:18:15,285 --> 00:18:19,885
this is basically for a strict, very,
strict identity verification for every

297
00:18:19,885 --> 00:18:24,025
user and device, regardless of network
permission or resource location.

298
00:18:24,305 --> 00:18:29,115
we have, two form factor authentication,
multi-form factor authentication.

299
00:18:29,305 --> 00:18:32,905
we have pink fed rate authentication
where users would get an immediate

300
00:18:32,905 --> 00:18:37,225
notification whenever they're trying
to access any, any network objects

301
00:18:37,225 --> 00:18:39,145
or any different application.

302
00:18:39,505 --> 00:18:41,475
and every time.

303
00:18:41,975 --> 00:18:46,065
Whenever user to login, it is
always, continuously authenticating

304
00:18:46,065 --> 00:18:49,215
based on Based on the requirements
set by the organization.

305
00:18:49,545 --> 00:18:52,405
So that is where the zero trust,
architecture comes into play.

306
00:18:52,945 --> 00:18:55,265
And then the third aspect is
the compliance automation.

307
00:18:55,545 --> 00:18:58,395
these are the intelligent systems
that kind of continuously monitor,

308
00:18:58,815 --> 00:19:04,125
document, and also enforce regulatory,
requirements across multiple jurisdictions

309
00:19:04,125 --> 00:19:05,835
with minimal human intervention.

310
00:19:06,195 --> 00:19:11,655
So I'll give a classic example of how this
gets played in a financial institution.

311
00:19:12,155 --> 00:19:16,055
Institutions such as, post
trade brokerages or, post

312
00:19:16,355 --> 00:19:18,245
event reporting organizations.

313
00:19:18,545 --> 00:19:23,165
They basically have a very huge
need to comply with the regulator

314
00:19:23,285 --> 00:19:25,205
laws and the regulator requirements.

315
00:19:25,585 --> 00:19:29,995
often when we are sending the data
back, to the regulators, it has to

316
00:19:29,995 --> 00:19:35,405
be continuously, the, sent back in
a, continuous feedback loop and.

317
00:19:36,245 --> 00:19:40,705
For most part, the regulators would
ask for, how strong the security of

318
00:19:40,705 --> 00:19:43,945
the application is, how strong the
network is, and things like that.

319
00:19:44,275 --> 00:19:48,105
So this also falls under that compliance
automation thing where we could

320
00:19:48,105 --> 00:19:52,805
build some intelligent systems that
kind of take care, of, identifying,

321
00:19:53,125 --> 00:19:55,345
the, the cloud data pipeline.

322
00:19:55,845 --> 00:19:58,545
Now moving on to the next one,
we're gonna talk about the

323
00:19:58,545 --> 00:19:59,985
inclusion storage management.

324
00:20:00,315 --> 00:20:05,925
So with the storage management, for a
traditional storage, we would just, have

325
00:20:05,925 --> 00:20:10,575
a solid state device sitting somewhere
on a rack, on a V block or something.

326
00:20:11,175 --> 00:20:15,753
But, in the case of the
cloud platforms, it.

327
00:20:16,253 --> 00:20:19,613
Falls under different
brackets or different ways.

328
00:20:20,033 --> 00:20:23,093
So there could be auto
tiering of the data.

329
00:20:23,433 --> 00:20:26,613
the data can be classified into different
deals, something that would change

330
00:20:26,613 --> 00:20:30,213
frequently, something that would be
stagnant for a longer period of time.

331
00:20:30,633 --> 00:20:33,063
or something that is like a hybrid.

332
00:20:33,628 --> 00:20:36,478
most of the modern cloud
platforms leverage the storage

333
00:20:36,478 --> 00:20:39,998
optimization techniques, that
significantly reduce the cost while

334
00:20:39,998 --> 00:20:41,348
improving the performance, right?

335
00:20:41,538 --> 00:20:46,648
as I was saying, if some data is prone to
change pretty frequently and we don't need

336
00:20:46,648 --> 00:20:50,668
the data or a period of time, we could
set up retention policies against that.

337
00:20:51,168 --> 00:20:54,558
if we need some data to be available
for a very long extended period of time,

338
00:20:54,558 --> 00:20:58,333
let's say two years or three years,
like a classic example of Netflix,

339
00:20:58,603 --> 00:21:02,983
that is where we could use, glaciers
Glacier type of storage, where, the

340
00:21:02,983 --> 00:21:07,713
costs are significantly, low, but it
is not highly performant or anything.

341
00:21:07,783 --> 00:21:10,153
that's one of the catch that
typically that we'll have to

342
00:21:10,153 --> 00:21:13,793
keep in mind while dealing with,
long retention data timeframes.

343
00:21:14,333 --> 00:21:17,873
Now, if you look at the whole graph
here, that I'm showing, the data

344
00:21:17,873 --> 00:21:22,373
basically demonstrates how implementing
a comprehensive, optimization strategy,

345
00:21:22,373 --> 00:21:26,663
combining audit, hearing compression, and
as well as intelligent data placement.

346
00:21:27,353 --> 00:21:32,033
It would effectively reduce 30% of
the cost reduction when compared

347
00:21:32,033 --> 00:21:33,563
rotation storage approaches.

348
00:21:33,643 --> 00:21:37,603
So this would also increase the data
accessibility and the performance as

349
00:21:37,603 --> 00:21:40,473
I was stating earlier, based on the
use case, what we're trying to do.

350
00:21:40,753 --> 00:21:45,453
it, it is really important on, how
we structure, how we tier it and

351
00:21:45,453 --> 00:21:48,213
how we, so that all comes into play.

352
00:21:48,713 --> 00:21:52,123
We're gonna look at some of the
computing advancements, especially

353
00:21:52,123 --> 00:21:53,983
the edge computing advancements.

354
00:21:54,463 --> 00:21:56,143
Now let's look at a
couple of aspects here.

355
00:21:56,603 --> 00:21:59,693
before the edge computing the
data, travel long distances to

356
00:21:59,693 --> 00:22:01,403
centralized cloud infrastructure.

357
00:22:01,763 --> 00:22:05,193
there used to be a very high
latency, that compromise real-time

358
00:22:05,193 --> 00:22:08,373
applications, so bandwidth
constraints, limited scalability,

359
00:22:08,733 --> 00:22:11,103
and transmission costs primarily.

360
00:22:11,553 --> 00:22:17,913
Now after the Edge implementation, every
single data processing that is happening,

361
00:22:18,333 --> 00:22:20,013
it is happening at the local network.

362
00:22:20,343 --> 00:22:25,198
So let's say if my data center is
sitting in, London, which is Cent

363
00:22:25,318 --> 00:22:27,798
typically if we go by the areas, world.

364
00:22:28,638 --> 00:22:32,178
Now, if I have something as a data
center that is sitting in London,

365
00:22:32,208 --> 00:22:36,738
my data has to transfer the network
hub from the US to over the internet

366
00:22:36,738 --> 00:22:40,578
to the Londons network, which could
basically bring in a lot of latency.

367
00:22:40,998 --> 00:22:45,378
But if I have a, a data center that
is located like in the same state,

368
00:22:45,378 --> 00:22:49,568
in the same region as well, then
it's gonna basically, get the local

369
00:22:49,568 --> 00:22:54,538
network parameter, the data accesses
faster now, in, in correlation.

370
00:22:54,538 --> 00:22:59,588
And to attest to the fact it would
have an ultra response, 15 milliseconds

371
00:22:59,588 --> 00:23:01,418
latency, which is like super low.

372
00:23:01,748 --> 00:23:04,258
That enables the time critical operations.

373
00:23:04,258 --> 00:23:06,898
So let's say, I wanna do
a trade card transaction.

374
00:23:06,943 --> 00:23:10,363
Now create card transactions
cannot run for five or six seconds.

375
00:23:10,463 --> 00:23:12,173
So it has to happen instantly.

376
00:23:12,173 --> 00:23:15,703
So that is where, having the edge
computing, advancement will help.

377
00:23:16,303 --> 00:23:19,003
And the transmission costs are reduced.

378
00:23:19,063 --> 00:23:22,213
it has an optimized bandwidth
utilization that improves

379
00:23:22,543 --> 00:23:23,713
network efficiency as aspect.

380
00:23:24,043 --> 00:23:24,343
Yep.

381
00:23:24,843 --> 00:23:28,763
Now the technology, convergence, impact.

382
00:23:29,453 --> 00:23:32,703
So let's look at three
different, aspects here.

383
00:23:33,103 --> 00:23:36,403
we're gonna look at the operational
excellence, the enhanced compliance,

384
00:23:36,433 --> 00:23:38,713
and as well as the innovation catalyst.

385
00:23:39,163 --> 00:23:42,613
So 90% of the faster data
processing through seamlessly

386
00:23:42,613 --> 00:23:44,233
integrated cloud technologies.

387
00:23:44,493 --> 00:23:48,033
it would enable real time decision
making capabilities, right?

388
00:23:48,273 --> 00:23:51,393
So this was the data pipeline
use case I was talking about

389
00:23:51,483 --> 00:23:54,913
where, you can get the data from
different sources combined together.

390
00:23:55,243 --> 00:23:58,393
have a centralized data
transformation, data application,

391
00:23:58,393 --> 00:24:01,863
whatever we're trying to do, and
then basically improve your business

392
00:24:01,863 --> 00:24:03,633
operations and business efficiency.

393
00:24:04,338 --> 00:24:08,128
Now the second part of it is the
enhanced compliance, which kind of

394
00:24:08,128 --> 00:24:12,448
falls under the automated governance
and the regulatory, events, throughout

395
00:24:12,448 --> 00:24:14,198
the, through the entire data cycle.

396
00:24:14,708 --> 00:24:17,048
And the third one is
the innovation catalyst.

397
00:24:17,218 --> 00:24:20,558
acceleration, the time to market
for data driven products, creating

398
00:24:20,558 --> 00:24:22,518
sustainable, competitor advantages.

399
00:24:22,568 --> 00:24:25,948
So let's take an example of,
the social media platforms.

400
00:24:25,948 --> 00:24:29,808
So let's say there are two or three of
them, no, I wanna build a. There are,

401
00:24:30,258 --> 00:24:32,718
we could use a mix of, machine learning.

402
00:24:32,878 --> 00:24:37,403
we could use a mix of something,
as well as architecture, and have

403
00:24:37,463 --> 00:24:39,923
everything deployed in no time, right?

404
00:24:40,253 --> 00:24:45,523
So it accelerates what the business
really needs, in a very short span.

405
00:24:45,943 --> 00:24:50,053
And also, as far as the costs are
concerned, we're not paying anything

406
00:24:50,113 --> 00:24:52,103
upfront, to procure the infrastructure.

407
00:24:52,268 --> 00:24:54,248
And we're also saving a
lot of time for a fact.

408
00:24:54,638 --> 00:24:59,518
So let's say I need like about 20
different B blocks or, and a whole bunch

409
00:24:59,518 --> 00:25:02,238
of network, routers and everything.

410
00:25:02,568 --> 00:25:06,748
I would end up dealing with
a lot of infrastructure teams

411
00:25:06,748 --> 00:25:08,938
waiting and paying for the cost.

412
00:25:09,458 --> 00:25:10,958
Even when I'm not really using them.

413
00:25:11,008 --> 00:25:14,098
so that is where the innovation
catalyst comes into play.

414
00:25:14,338 --> 00:25:17,448
Where, most of the acceleration
in time to market for data driven

415
00:25:17,448 --> 00:25:18,978
products are data driven applications.

416
00:25:18,978 --> 00:25:22,978
It really speeds it up because it's
all, cloud driven, cloud data driven

417
00:25:23,098 --> 00:25:24,358
and cloud infrastructure driven.

418
00:25:24,858 --> 00:25:25,078
Now.

419
00:25:25,578 --> 00:25:29,948
Someone could question back saying,
how do I really make a move, for

420
00:25:29,948 --> 00:25:33,908
an application that is sitting
in a legacy environment to cloud?

421
00:25:34,178 --> 00:25:40,668
So I think it is really important
to assess, do an assessment of,

422
00:25:41,058 --> 00:25:44,768
A very, comprehensive analysis of
the existing data architecture.

423
00:25:45,068 --> 00:25:46,718
How is the data flowing?

424
00:25:46,718 --> 00:25:47,918
Is a database on-prem?

425
00:25:47,918 --> 00:25:48,908
Are you using a database?

426
00:25:48,908 --> 00:25:50,048
Do you need some local storage?

427
00:25:50,048 --> 00:25:51,008
Do you need the cache?

428
00:25:51,328 --> 00:25:55,408
how is the performance looking and what
exactly are we trying to achieve with the

429
00:25:55,408 --> 00:25:56,788
current application that we really have?

430
00:25:57,358 --> 00:26:01,258
That comprehensive analysis of
the data architecture underlying

431
00:26:01,258 --> 00:26:05,568
performance gaps, and the high value
optimization opportunities through

432
00:26:05,568 --> 00:26:09,458
the stakeholder interviews and system
audits is, is something that really

433
00:26:09,458 --> 00:26:14,528
needs to happen now that serves as a
foundation as a basement to get started.

434
00:26:15,433 --> 00:26:19,623
Based on that, we would go about, moving
into the strategy development aspects.

435
00:26:19,873 --> 00:26:21,643
get some strategy flowing around it.

436
00:26:21,943 --> 00:26:24,133
What type of cloud
platform do you want use?

437
00:26:24,133 --> 00:26:26,203
do you wanna go with A-W-S-G-C-P?

438
00:26:26,573 --> 00:26:29,073
do you want to go with the
cloud, driven database?

439
00:26:29,203 --> 00:26:30,088
how do we deal with the data?

440
00:26:30,793 --> 00:26:33,133
how do, how would your
data pipeline be built?

441
00:26:33,553 --> 00:26:37,363
get that strategy going, and get
an estimate of the cost that might

442
00:26:37,363 --> 00:26:39,073
incur as part of your strategy.

443
00:26:39,403 --> 00:26:40,723
And also look for.

444
00:26:41,223 --> 00:26:44,253
what is the net value realization
that you're getting outta the cost?

445
00:26:44,493 --> 00:26:48,343
So let's say if you're spending a
million bucks today, that's, for an year.

446
00:26:48,583 --> 00:26:52,253
And then if you see that, your
cost could be reduced to 150

447
00:26:52,253 --> 00:26:54,783
grand, for every year, then yes.

448
00:26:55,293 --> 00:26:58,233
Yeah, that, that would
be an absolute use case.

449
00:26:58,233 --> 00:27:00,033
Why we would wanna move to cloud, right?

450
00:27:00,333 --> 00:27:04,693
So getting the strategy development,
design a cloud, customized cloud

451
00:27:04,693 --> 00:27:08,053
migration roadmap, aligned with the
business objectives, including the

452
00:27:08,053 --> 00:27:09,703
technology, stakeholders, government.

453
00:27:10,288 --> 00:27:13,708
Governance policies and also
the regulatory compliance

454
00:27:13,708 --> 00:27:15,288
and, implementation timeline.

455
00:27:15,948 --> 00:27:18,768
And with the actual implementation
itself, we would need multiple

456
00:27:18,768 --> 00:27:22,778
teams, to get involved, get the
cross-functional teams leverage DevOps.

457
00:27:22,988 --> 00:27:25,348
How, how do we do infrastructure as code?

458
00:27:25,588 --> 00:27:30,298
How do we code deploy to AWS, how do we,
bring up, bring down the instances or,

459
00:27:30,518 --> 00:27:34,438
get the regulatory, sorry, get the regular
infrastructure support aspects as well.

460
00:27:34,988 --> 00:27:38,858
The DevOps methodologies and
also establishing the feedback

461
00:27:38,858 --> 00:27:41,498
loops for continuous refinement
and capability building.

462
00:27:42,338 --> 00:27:46,348
And as I said earlier, the value
realization, what is the cost

463
00:27:46,558 --> 00:27:50,428
effectiveness that this is gonna
bring us in terms of, moving to cloud.

464
00:27:50,818 --> 00:27:55,328
So that basically summarizes the,
the data transformation journey.

465
00:27:55,828 --> 00:27:58,898
So that's pretty, that's
pretty much I had.

466
00:27:59,118 --> 00:28:03,638
I hope you all felt interested
in what was, spoken so far.

467
00:28:03,918 --> 00:28:08,098
please feel free to reach out, if you
have any questions or any clarifications.

468
00:28:08,878 --> 00:28:09,148
All right.

469
00:28:09,208 --> 00:28:09,628
Thank you.

