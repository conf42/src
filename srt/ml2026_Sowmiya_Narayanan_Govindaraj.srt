1
00:00:00,500 --> 00:00:01,010
Speaker 28: Hi everyone.

2
00:00:01,310 --> 00:00:01,880
I'm Simon Ach.

3
00:00:02,380 --> 00:00:04,810
I'm a senior machine learning
engineer working on perception

4
00:00:04,810 --> 00:00:06,280
systems for autonomous vehicles.

5
00:00:06,550 --> 00:00:09,640
My work centers on taking models
from research ideas all the way

6
00:00:09,640 --> 00:00:12,219
through real world deployment
in safety critical environments.

7
00:00:12,579 --> 00:00:15,040
In this talk, I'll share how the
autonomous vehicle industry is

8
00:00:15,040 --> 00:00:18,580
moving from fragmented perception
pipelines to unified end-to-end

9
00:00:18,640 --> 00:00:22,540
models, and what we have learned
while scaling the systems in practice.

10
00:00:23,040 --> 00:00:24,630
Before we dive in, here's a quick roadmap.

11
00:00:25,305 --> 00:00:28,065
We'll start with the fragmentation
problem in traditional perception

12
00:00:28,065 --> 00:00:31,485
stacks, and discuss why separate
models for each task create

13
00:00:31,485 --> 00:00:33,705
inefficiencies and hidden failure modes.

14
00:00:33,915 --> 00:00:37,215
Then we'll look at the unified
perception paradigm and how a single

15
00:00:37,215 --> 00:00:41,144
integrated system can learn, share
representations across task consensus.

16
00:00:41,415 --> 00:00:44,745
From there, I'll introduce Multisensor
Fusion transformers, which are

17
00:00:44,745 --> 00:00:47,415
a key architectural enabler for
making this approach practical.

18
00:00:47,985 --> 00:00:51,285
After that, I'll share some real world
deployment challenges and lessons

19
00:00:51,285 --> 00:00:54,945
from production because scaling
perception outside the lab is where

20
00:00:54,945 --> 00:00:56,745
most of the complexity really shows up.

21
00:00:57,195 --> 00:01:00,645
And finally, we'll touch on foundation
models for autonomous driving, which I

22
00:01:00,645 --> 00:01:04,605
see as the next frontier for making the
systems more scalable and data efficient.

23
00:01:05,084 --> 00:01:08,265
By the end, you should have both the
technical intuition and the practical

24
00:01:08,265 --> 00:01:10,035
perspective behind unified perception.

25
00:01:10,535 --> 00:01:11,735
Let's start with the core problem.

26
00:01:12,035 --> 00:01:14,345
Most traditional perception
systems are highly fragmented.

27
00:01:14,675 --> 00:01:18,155
We typically train one model for
detection, another for segmentation,

28
00:01:18,335 --> 00:01:21,335
and one more for depth, and
sometimes even separate models.

29
00:01:21,425 --> 00:01:25,835
Each sensor, each of those modules
processes the same inputs independently,

30
00:01:26,195 --> 00:01:29,855
which leads to duplicated computation,
high latency, and errors that

31
00:01:29,855 --> 00:01:31,655
cascade from one stage to the next.

32
00:01:31,925 --> 00:01:36,245
But beyond efficiency, the bigger issue
is that these models never develop

33
00:01:36,245 --> 00:01:37,835
a cohesive understanding of the sea.

34
00:01:38,570 --> 00:01:40,970
They're optimized locally
rather than globally.

35
00:01:41,420 --> 00:01:45,380
So over time, the system becomes
complex, brittle, and harder to maintain.

36
00:01:46,370 --> 00:01:48,650
The alternative approach
is unified perception.

37
00:01:49,280 --> 00:01:52,730
Instead of isolated models, we train
a single end-to-end system that

38
00:01:52,730 --> 00:01:56,660
jointly learns spatial semantic,
and temporal representations.

39
00:01:57,140 --> 00:02:01,160
All sensors feed into SHA encoders
and lightweight task heads

40
00:02:01,220 --> 00:02:03,020
branch out for specific outputs.

41
00:02:03,590 --> 00:02:07,430
Because those features are shared,
improvements in one task naturally

42
00:02:07,430 --> 00:02:09,380
benefit the others detection.

43
00:02:09,380 --> 00:02:13,180
L segmentation informs depth,
and everything improves together.

44
00:02:13,840 --> 00:02:17,230
So we reduce redundancy while
actually increasing accuracy.

45
00:02:17,710 --> 00:02:20,710
The system becomes both
simpler, architecturally, and

46
00:02:20,710 --> 00:02:21,580
stronger performance base.

47
00:02:22,080 --> 00:02:25,205
One of the biggest enablers of
this shift has been transformers.

48
00:02:26,145 --> 00:02:29,685
Self attention lets us model long
grade relationships across the

49
00:02:29,685 --> 00:02:33,015
scene, which is critical in driving
where context really matters.

50
00:02:33,555 --> 00:02:37,005
Cross model attention allows the
network to align camera appearance

51
00:02:37,065 --> 00:02:41,475
with lidar geometry automatically
instead of relying on and design fusion

52
00:02:41,505 --> 00:02:45,255
Rules and token based representations
make it easier to handle different

53
00:02:45,255 --> 00:02:46,965
sensor resolutions and modalities.

54
00:02:47,535 --> 00:02:51,075
Together these properties make
transformers a very natural fit

55
00:02:51,135 --> 00:02:52,845
for unified end-to-end perception.

56
00:02:53,345 --> 00:02:56,135
If we break the architecture down,
there are three main components.

57
00:02:56,375 --> 00:02:59,285
First, a shared backbone
that extracts general purpose

58
00:02:59,285 --> 00:03:00,845
features across all sensors.

59
00:03:01,265 --> 00:03:05,674
Second cross model alignment networks
that project lidar and camera information

60
00:03:05,704 --> 00:03:09,845
into a unified latent space, and
third temporal fusion modules that

61
00:03:09,845 --> 00:03:11,285
aggregate information over time.

62
00:03:11,915 --> 00:03:15,395
This temporal component is especially
important because a single frame

63
00:03:15,424 --> 00:03:19,505
can be ambiguous, but motion across
frames reveals intent and continuity.

64
00:03:20,005 --> 00:03:24,054
When we train these models, we also
train the 2D and 3D elements of

65
00:03:24,054 --> 00:03:25,825
it jointly rather than separately.

66
00:03:26,325 --> 00:03:30,015
Images gives us rich semantics like
texture and appearance, while point

67
00:03:30,015 --> 00:03:32,024
clouds provide precise geometry and depth.

68
00:03:32,385 --> 00:03:35,804
By combining both these modalities,
we get the best of each.

69
00:03:36,135 --> 00:03:40,004
This leads to better 3D boxes,
more stable depth estimates,

70
00:03:40,274 --> 00:03:41,714
and a unified representation.

71
00:03:41,774 --> 00:03:44,984
The downstream modules like tracking
and planning can depend upon.

72
00:03:45,315 --> 00:03:49,304
In practice this joint learning
significantly improves robustness.

73
00:03:49,804 --> 00:03:53,165
One key lesson from real world
deployment is that sensors don't fail.

74
00:03:53,165 --> 00:03:53,765
Uniformly.

75
00:03:54,454 --> 00:03:58,054
Rain or darkness might degrade
cameras while fall, or sparse

76
00:03:58,265 --> 00:04:02,495
returns can affect lidars, so
static fusion simply isn't enough.

77
00:04:03,125 --> 00:04:06,875
Instead, we use adaptive attention
mechanisms and dynamically weight

78
00:04:06,964 --> 00:04:08,855
each sensor based on reliability.

79
00:04:09,425 --> 00:04:12,875
When one degrades, the system
automatically leans more on the others.

80
00:04:13,295 --> 00:04:17,105
This gives us graceful degradation,
extra sudden failures, which

81
00:04:17,105 --> 00:04:18,575
is critical in terms of safety.

82
00:04:19,075 --> 00:04:23,784
As soon as we move outside control
settings, new challenges appear, weather

83
00:04:23,784 --> 00:04:27,805
changes, geographic differences and
domain shifts, all impact performance.

84
00:04:28,344 --> 00:04:31,645
And then there are long tail scenarios
like construction zones, emergency

85
00:04:31,645 --> 00:04:33,775
vehicles, or unusual pedestrian behavior.

86
00:04:34,344 --> 00:04:37,585
These rare events matter the
most for safety, and they show

87
00:04:37,585 --> 00:04:39,085
up the least containing data.

88
00:04:39,625 --> 00:04:43,405
So we rely heavily on targeted
data collection, balance sampling,

89
00:04:43,495 --> 00:04:47,005
and continual learning strategies
to cover those edge cases.

90
00:04:47,505 --> 00:04:49,575
There's also a big
engineering benefit here.

91
00:04:50,445 --> 00:04:54,465
Traditional stacks acquire multiple
models, multiple inference passes,

92
00:04:54,555 --> 00:04:56,385
and complex interfaces between them.

93
00:04:57,104 --> 00:05:01,935
Whereas unified perception simplifies all
of that into a single forward pass that

94
00:05:02,025 --> 00:05:06,645
reduces latency, makes debugging easier
and significantly streamlines deployment.

95
00:05:07,469 --> 00:05:10,799
From an infrastructure standpoint,
this is a huge operational bit.

96
00:05:11,299 --> 00:05:15,349
Another important trend is supervised
rather self supervised learning

97
00:05:16,190 --> 00:05:20,719
Labeling, autonomous driving data at
scale is extremely expensive, so instead

98
00:05:20,749 --> 00:05:24,590
we use techniques like contrastive
alignment, temporal consistency,

99
00:05:24,619 --> 00:05:28,010
and cross sensor reconstruction to
learn directly from unlabel data.

100
00:05:28,700 --> 00:05:31,849
This lets us scale training
to massive data sets without

101
00:05:31,849 --> 00:05:33,380
proportional annotation costs.

102
00:05:33,710 --> 00:05:37,520
Helps capture the long tail scenarios
that are the hardest to label manually.

103
00:05:38,020 --> 00:05:41,169
Even with large scale training
data distribution shift is

104
00:05:41,169 --> 00:05:42,700
unavoidable in autonomous driving.

105
00:05:43,299 --> 00:05:47,739
Every new city looks different,
lighting changes, road layouts change,

106
00:05:47,799 --> 00:05:49,659
and even driving behavior changes.

107
00:05:49,840 --> 00:05:53,034
So models that work well in one
place can negate somewhere else.

108
00:05:53,304 --> 00:05:57,864
To handle this, we use domain adaptation
techniques that allow the system

109
00:05:58,014 --> 00:06:02,604
to adjust continuously, including
adversarial, future alignment, test

110
00:06:02,604 --> 00:06:04,644
time adaptation, and rapid fine tuning.

111
00:06:05,034 --> 00:06:09,174
The key idea is that perception isn't
something you train once and freeze.

112
00:06:09,384 --> 00:06:12,294
It's something that keeps learning
and adapting as the vehicle

113
00:06:12,294 --> 00:06:13,374
encounters the real world.

114
00:06:13,874 --> 00:06:17,385
Now, let's come to the new and hot
topic in the industry right now.

115
00:06:17,414 --> 00:06:18,734
Those are foundational models.

116
00:06:19,234 --> 00:06:22,504
By pre-training on massive
driving data sets, we learn strong

117
00:06:22,534 --> 00:06:25,624
general purpose representations
that transfer well across tasks.

118
00:06:26,135 --> 00:06:30,124
Then we only need lightweight heads
for detection, tracking, or prediction.

119
00:06:30,814 --> 00:06:33,455
What's especially exciting
is zero shot behavior.

120
00:06:33,784 --> 00:06:38,854
These models often generalize to unseen
scenarios without explicitly training,

121
00:06:39,354 --> 00:06:43,289
which is incredibly valuable when you
can't label every possible edge case.

122
00:06:43,789 --> 00:06:46,729
So stepping back, what do we gain overall?

123
00:06:47,329 --> 00:06:50,449
We get lower latency because
everything runs in a single pass.

124
00:06:50,509 --> 00:06:53,629
Better accuracy through shared
learning and simpler deployment

125
00:06:53,749 --> 00:06:54,949
with fewer moving parts.

126
00:06:55,639 --> 00:06:58,909
It's rare to find an approach that
improves both performance and engineering

127
00:06:58,909 --> 00:07:02,569
complexity at the same time, but
unified perception really does both.

128
00:07:03,069 --> 00:07:04,934
Ultimately, all of this
ties back to safety.

129
00:07:05,434 --> 00:07:07,984
Consistency and understanding
reduces error propagation.

130
00:07:08,224 --> 00:07:11,434
Temporal reasoning improves
predictions of dynamic agents and

131
00:07:11,434 --> 00:07:13,474
adaptive fusion maintains robustness.

132
00:07:13,534 --> 00:07:17,584
When sensors degrade together, these
improvements translate directly

133
00:07:17,584 --> 00:07:21,184
into fewer disengagements and more
reliable behavior in the real world.

134
00:07:21,784 --> 00:07:24,964
So unified perception ist
just an architectural choice.

135
00:07:25,264 --> 00:07:26,974
It has a measurable safety impact.

136
00:07:27,474 --> 00:07:28,134
To summarize.

137
00:07:28,539 --> 00:07:31,329
Unified architectures
streamlined perception.

138
00:07:31,629 --> 00:07:36,339
Multisensor fusion is essential, and
foundation models combined with self

139
00:07:36,339 --> 00:07:40,929
supervision will continue accelerating
progress as these systems mature

140
00:07:40,989 --> 00:07:45,789
autonomous vehicles become safer, more
scalable and better acute to handle

141
00:07:45,789 --> 00:07:47,649
the complexity, the real world driving.

142
00:07:48,149 --> 00:07:48,869
Thank you for your time.

143
00:07:49,134 --> 00:07:51,869
If you'd like to continue the
conversation, feel free to stand the Q

144
00:07:51,869 --> 00:07:53,699
are code and connect with me on LinkedIn.

