1
00:00:00,500 --> 00:00:01,220
Speaker 9: Hello and welcome.

2
00:00:01,430 --> 00:00:02,510
My name is Gareth Hemsley.

3
00:00:02,690 --> 00:00:04,069
I'm a product manager at Dynatrace.

4
00:00:04,760 --> 00:00:07,489
I'm pleased to be here today to
talk to you about how you can

5
00:00:07,489 --> 00:00:12,290
go from Zero to DevSecOps Hero
by extending Microsoft Center.

6
00:00:12,790 --> 00:00:15,850
So the information presented in my
session today is based on our personal

7
00:00:15,850 --> 00:00:20,800
experience and my own opinions,
not those of my employer mention

8
00:00:20,800 --> 00:00:23,140
of any specific product or service.

9
00:00:23,140 --> 00:00:24,760
Does not imply endorsement.

10
00:00:25,750 --> 00:00:29,769
I also don't claim to be an
expert in all or in fact any of

11
00:00:29,769 --> 00:00:31,209
the areas we'll talk about today.

12
00:00:31,709 --> 00:00:34,830
An article published by the
Economist in 2017 highlighted

13
00:00:34,830 --> 00:00:39,449
how data had surpassed oil as the
driving force of the global economy.

14
00:00:40,109 --> 00:00:44,279
Just as oil fueled industrial growth
data now powers the digital area,

15
00:00:45,059 --> 00:00:48,809
enabling companies like Google, Amazon,
and Facebook to dominate the market.

16
00:00:49,309 --> 00:00:52,694
Of course, the value of data has
not gone unnoticed by bad actors.

17
00:00:53,194 --> 00:00:57,094
According to the 2023 data breach
report published by Identity Theft

18
00:00:57,334 --> 00:01:02,014
Resource Center, the number of data
breaches stayed fairly constant over

19
00:01:02,014 --> 00:01:05,525
2021 and 2022 at 1800 public incidents.

20
00:01:06,215 --> 00:01:12,935
But in 2023, data breaches reached
an all time high with 3,205 publicly

21
00:01:12,935 --> 00:01:18,275
reported incidents affecting over 353
million individuals, marking a staggering

22
00:01:18,305 --> 00:01:20,539
78% increase from the previous year.

23
00:01:21,039 --> 00:01:25,449
This surge highlights our data has
become a prime target for actors,

24
00:01:25,570 --> 00:01:30,009
as in organizations and individuals
increasingly rely on digital platforms.

25
00:01:30,509 --> 00:01:33,929
According to the data breach report,
social engineering led the pack

26
00:01:34,230 --> 00:01:40,320
accounting for 18.5% of breaches
followed by ransomware at 10.4%, malware

27
00:01:40,320 --> 00:01:43,565
at 4.9%, and zero day tax at 4.6%.

28
00:01:44,065 --> 00:01:47,905
What is unfortunately not
surprising is that 74% of all

29
00:01:47,905 --> 00:01:49,675
breaches involve the human element.

30
00:01:50,515 --> 00:01:53,696
This continues to underscore
the importance of addressing

31
00:01:53,696 --> 00:01:56,636
human vulnerabilities such
as phishing, pretexting, and

32
00:01:56,636 --> 00:01:57,985
other manipulative tactics.

33
00:01:58,485 --> 00:02:03,196
Ransomware continues to be a lucrative
strategy for cyber criminals as

34
00:02:03,196 --> 00:02:06,770
organizations are often forced to
pay to regain access to their data.

35
00:02:07,270 --> 00:02:11,561
Unfortunately, today's operation teams
are not only challenged by attackers,

36
00:02:12,061 --> 00:02:13,651
they're also challenged by complexity.

37
00:02:14,071 --> 00:02:17,191
The digital landscape is growing
more intricate every day.

38
00:02:17,701 --> 00:02:21,091
Organizations rely on multiple
tools and systems across on-premise,

39
00:02:21,091 --> 00:02:22,651
cloud and hybrid environments.

40
00:02:23,221 --> 00:02:26,371
This complexity makes it difficult
for operation teams to manage

41
00:02:26,521 --> 00:02:28,831
oversight, leading to gaps in security.

42
00:02:29,331 --> 00:02:32,301
Of course, the sheer volume
of data generated by modern

43
00:02:32,301 --> 00:02:34,191
IT systems is also staggering.

44
00:02:34,731 --> 00:02:39,111
With data flowing in from endpoints,
networks, cloud services, and IOT devices,

45
00:02:39,951 --> 00:02:43,881
operation teams are inundated with
alerts, many of which are false positives.

46
00:02:44,601 --> 00:02:48,791
This alert fatigue makes it harder
to detect real threats in real time.

47
00:02:49,291 --> 00:02:51,841
Managing security operations
is also expensive.

48
00:02:52,261 --> 00:02:55,951
The need for advanced tools,
skilled personnel, and continuous

49
00:02:55,951 --> 00:02:57,421
training drives up cost.

50
00:02:58,201 --> 00:03:02,311
Moreover, the financial impact of
breaches and compliance requirements

51
00:03:02,311 --> 00:03:05,341
adds to the burden, making it
challenging for organizations to

52
00:03:05,341 --> 00:03:06,811
allocate resources effectively.

53
00:03:07,311 --> 00:03:11,781
Correlating events across multiple
sources in a world of fragmented data

54
00:03:11,871 --> 00:03:16,461
is a monumental task without a proper
correlation, without proper correlation.

55
00:03:17,046 --> 00:03:20,706
Critical insights are missed, and
teams struggle to connect the dots

56
00:03:20,706 --> 00:03:22,776
between seemingly unrelated events.

57
00:03:23,276 --> 00:03:28,406
According to the threat stack SecOps
report, 52% of companies admitted to

58
00:03:28,406 --> 00:03:32,101
sacrificing cybersecurity measures
to meet business deadlines or

59
00:03:32,101 --> 00:03:33,941
achieve faster product releases.

60
00:03:34,441 --> 00:03:39,186
This statistic underscores a common
tension in the DevOps workflow where

61
00:03:39,186 --> 00:03:43,146
the pressure to deliver quickly often
leads to cutting corners on security.

62
00:03:43,646 --> 00:03:49,316
Organizations must recognize that speed
and security are not mutually exclusive.

63
00:03:50,126 --> 00:03:54,866
By embedding security practices into the
DevOps pipeline, transforming it into

64
00:03:54,866 --> 00:04:01,366
the DevSecOps pipeline teams can achieve
both agility and robust protection.

65
00:04:01,866 --> 00:04:04,866
This, of course, requires fostering
a culture of collaboration between

66
00:04:04,866 --> 00:04:07,086
development operations and security teams.

67
00:04:07,776 --> 00:04:11,616
Alongside leveraging automation and
advanced tools to streamline the

68
00:04:11,616 --> 00:04:16,176
processes without compromising safety,
the ultimate goal is to deliver

69
00:04:16,236 --> 00:04:20,556
innovation at speed while safeguarding
against evolving cyber threats.

70
00:04:21,056 --> 00:04:24,866
Microsoft Sentinel empowers SOC
teams by combining advanced threat

71
00:04:24,866 --> 00:04:28,636
hunting, security orchestration,
automation, and response.

72
00:04:28,696 --> 00:04:30,196
So saw capabilities.

73
00:04:30,586 --> 00:04:33,556
Integration with external systems
to streamline the incident

74
00:04:33,556 --> 00:04:35,446
lifecycle and remediation process.

75
00:04:35,946 --> 00:04:41,256
Sentinel enables SOC teams to define
complex analytics rules to detect threats.

76
00:04:41,916 --> 00:04:45,786
These rules can trigger automated
incident creation, ensuring that threats

77
00:04:45,786 --> 00:04:47,316
are flagged and addressed immediately.

78
00:04:47,676 --> 00:04:50,076
Reducing the time to
detection and response.

79
00:04:50,576 --> 00:04:53,696
Central saw capabilities, leverage
playbooks and automation rules

80
00:04:53,786 --> 00:04:55,196
to manage incidents centrally.

81
00:04:55,976 --> 00:04:59,976
These tools allow teams to standardize
incident handling, automated re

82
00:05:00,036 --> 00:05:04,566
repetitive tasks, and orchestrate
responses across multiple tools.

83
00:05:05,556 --> 00:05:10,206
Automation reduces the manual effort
required for incident response, enabling

84
00:05:10,206 --> 00:05:15,066
faster revolution resolution, and allowing
analysts to focus on more critical tasks.

85
00:05:15,966 --> 00:05:18,606
The MATA Attack framework is a
globally recognized knowledge

86
00:05:18,606 --> 00:05:21,666
base that maps adversary
behavior into structured tactics.

87
00:05:22,166 --> 00:05:25,736
So what attack attackers aim
to achieve, and techniques

88
00:05:26,036 --> 00:05:27,476
how they want to achieve it?

89
00:05:27,976 --> 00:05:31,036
Microsoft Sentinel integrates
this framework to help SOC teams

90
00:05:31,036 --> 00:05:33,796
identify and classify threats
within their environments.

91
00:05:34,636 --> 00:05:39,376
Bylining hunting queries and
analytics rules to Mitre tactics.

92
00:05:39,946 --> 00:05:43,786
Sentinel provides a clear and standardized
view of potential attack parts.

93
00:05:44,286 --> 00:05:47,256
Sentinel's hunting queries are
grouped by a MA attack tactics

94
00:05:47,256 --> 00:05:51,156
such as initial access, privilege,
acceleration, and data exfiltration.

95
00:05:51,936 --> 00:05:56,046
This allows SOC teams to focus their
investigations on specific stages

96
00:05:56,046 --> 00:06:00,246
of an attack, making it easier
to identify and mitigate threats.

97
00:06:01,236 --> 00:06:05,526
Sentinel includes pre-built queries
designed to uncover potential threats.

98
00:06:05,676 --> 00:06:08,645
These queries are mapped to
specific tactics and techniques.

99
00:06:09,231 --> 00:06:11,841
Guiding analysts towards the
right data and helping them

100
00:06:11,841 --> 00:06:13,551
uncover issues in the environment.

101
00:06:14,051 --> 00:06:19,181
Analysts can sort and filter queries
by data source tactics or techniques.

102
00:06:20,171 --> 00:06:23,561
This flexibility allows teams to
prioritize high risk areas and

103
00:06:23,561 --> 00:06:25,181
focus on the most relevant alerts.

104
00:06:25,681 --> 00:06:29,851
Microsoft Sentinel's content hub is a
game changer for SOC teams, providing

105
00:06:29,851 --> 00:06:33,660
them with the tools and integrations they
need to protect their organizations in

106
00:06:33,660 --> 00:06:35,611
an increasingly complex threat landscape.

107
00:06:36,510 --> 00:06:40,171
By leveraging the centralized
marketplace, operations teams can

108
00:06:40,171 --> 00:06:43,770
enhance their security posture while
reducing the time and efforts spent

109
00:06:43,770 --> 00:06:44,941
on configuration and management.

110
00:06:45,441 --> 00:06:48,621
Microsoft Partners can publish
their own connectors and solutions

111
00:06:48,831 --> 00:06:51,741
directly to the content hub, making
it easier for their customers to

112
00:06:51,741 --> 00:06:53,271
adopt and deploy these integrations.

113
00:06:54,200 --> 00:06:58,131
This open ecosystem fosters
collaboration and innovation, allowing

114
00:06:58,131 --> 00:07:01,791
organizations to benefit from a
constantly expanding library of tools.

115
00:07:02,291 --> 00:07:05,320
So does Sentinel have the
vendor integration, which you're

116
00:07:05,320 --> 00:07:06,701
looking for in the content hub?

117
00:07:07,151 --> 00:07:11,051
If not, we can create it in
a couple of simple steps.

118
00:07:11,551 --> 00:07:15,721
For example, take Dynatrace Runtime
application protection, which is

119
00:07:15,721 --> 00:07:18,510
a security solution that detects
and mitigates sophisticated

120
00:07:18,510 --> 00:07:19,801
threats like SQL injection.

121
00:07:20,010 --> 00:07:26,041
JNDI Injection Command Injection
S-S-R-R-F attacks in real time.

122
00:07:26,541 --> 00:07:30,411
It provides code level visibility through
its one agent technology, and enables

123
00:07:30,411 --> 00:07:32,181
automatic blocking of detected attacks.

124
00:07:32,681 --> 00:07:36,881
In this presentation, we're looking
for a vendor which exposes rest

125
00:07:36,881 --> 00:07:40,871
APIs, which can be leveraged to bring
the vendors' insights into sentel.

126
00:07:41,371 --> 00:07:45,960
In this case, Dynatrace provides APIs
to query all attacks as well as to

127
00:07:45,960 --> 00:07:48,030
get the details of a specific attack.

128
00:07:48,530 --> 00:07:53,481
So Microsoft Sentinel provides a couple of
options for building connectors to bring.

129
00:07:53,731 --> 00:07:57,810
To query these rest APIs and bring
the relevant insights into centel.

130
00:07:58,310 --> 00:07:59,810
First option is Logic Apps.

131
00:08:00,310 --> 00:08:02,080
So these can be acquired
through the content hub.

132
00:08:02,580 --> 00:08:07,360
They have fairly intermediate rest
API support, right when in a what

133
00:08:07,360 --> 00:08:09,430
is what you get style interface.

134
00:08:09,930 --> 00:08:13,890
But unfortunately, they require
additional Azure resources deployed

135
00:08:13,890 --> 00:08:15,090
to their customer environment.

136
00:08:15,590 --> 00:08:17,180
Another option is Azure functions.

137
00:08:17,960 --> 00:08:20,030
These also can be acquired
through the Content Hub.

138
00:08:20,600 --> 00:08:25,600
They have far superior rest API support
right in code, but unfortunately, they

139
00:08:25,600 --> 00:08:30,220
also require additional Azure resources
deployed in the customer environment.

140
00:08:30,555 --> 00:08:30,775
The

141
00:08:31,275 --> 00:08:33,495
final option is Codeless
Connector framework.

142
00:08:33,995 --> 00:08:36,125
These can also be acquired
through the content hub.

143
00:08:36,845 --> 00:08:41,915
They have fairly limited rest a I
support, but no additional Azure

144
00:08:41,915 --> 00:08:44,975
resources are required in the environment,
in the customer's environment.

145
00:08:45,475 --> 00:08:49,885
So CCF is recommended as Microsoft
hosts and runs these connectors and

146
00:08:49,885 --> 00:08:53,995
the end user does not have to deploy
resources in the Azure environments.

147
00:08:54,495 --> 00:08:58,545
Our data connector will ingest the
raw JS N responses from our rest.

148
00:08:58,580 --> 00:08:58,820
API.

149
00:08:59,320 --> 00:09:02,950
Now when this happens, the
table schema, at least with

150
00:09:02,950 --> 00:09:04,750
the Htt B data collector, API.

151
00:09:04,870 --> 00:09:08,530
Previously the table schema was
automatically defined on ingest

152
00:09:08,920 --> 00:09:12,160
fields were suffixed with their
type, for example, underscore

153
00:09:12,160 --> 00:09:17,210
S for string and tables with
underscore cl for custom tables.

154
00:09:17,710 --> 00:09:21,400
Today Microsoft Sentinel leverages
data transformation and the

155
00:09:21,400 --> 00:09:22,810
advanced security information model.

156
00:09:23,310 --> 00:09:25,980
For normalizing and
enriching ingested data.

157
00:09:26,760 --> 00:09:29,730
This enables efficient threat
detection and investigation.

158
00:09:30,230 --> 00:09:36,710
So now at ingest time, data collection
rules are applied during ingestion

159
00:09:36,800 --> 00:09:40,250
to normalize and transform raw
telemetry, which we saw in the previous

160
00:09:40,250 --> 00:09:41,940
slide into standardized formats.

161
00:09:42,570 --> 00:09:45,570
This process ensures that data
is ready for immediate use,

162
00:09:45,600 --> 00:09:49,470
reducing the overhead of processing
large data sets during analysis.

163
00:09:49,970 --> 00:09:54,170
KQL transformations are applied
at this stage to filter, enrich,

164
00:09:55,010 --> 00:09:58,520
and or structure data before it is
stored in log in links workspace.

165
00:09:59,020 --> 00:10:01,720
At query time, the asim
parser comes into play.

166
00:10:02,380 --> 00:10:07,325
It uses KQL functions to transform and
normalize data dynamically at runtime.

167
00:10:07,825 --> 00:10:11,665
This allows SOC teams to analyze
data from multiple sources using a

168
00:10:11,665 --> 00:10:15,445
consistent schema without needing
to manage the underlying complexity.

169
00:10:15,945 --> 00:10:19,595
Another option is leveraging a custom
parr, which is a source specific

170
00:10:19,595 --> 00:10:21,665
parr, which uses KQL functions.

171
00:10:21,955 --> 00:10:23,125
To do something very similar.

172
00:10:23,625 --> 00:10:26,690
In the slide, we can see an
example of data which has been

173
00:10:26,690 --> 00:10:28,580
ingested via data collection rules.

174
00:10:29,080 --> 00:10:33,730
We can see that the custom table still has
a suffix of under underscore cl, right?

175
00:10:34,230 --> 00:10:39,380
We have the same data type support, so
things like string into long, we were able

176
00:10:39,380 --> 00:10:44,701
to transform the ingested data, adding a
time generated field, which is required.

177
00:10:45,201 --> 00:10:48,891
And it's important to note that billing
is based on the volume of data analyzed,

178
00:10:49,371 --> 00:10:50,841
and of course the storage of that data.

179
00:10:51,341 --> 00:10:54,040
So as we saw, data collection
rules allow us to define the

180
00:10:54,040 --> 00:10:55,721
ingestion pipeline upfront.

181
00:10:56,221 --> 00:11:00,511
We can define the table schema,
we can apply transformations

182
00:11:00,601 --> 00:11:01,591
to the ingested data.

183
00:11:01,831 --> 00:11:02,881
For example, filtering.

184
00:11:03,091 --> 00:11:09,021
Adding fields, removing fields, and we can
also send data to multiple destinations.

185
00:11:09,891 --> 00:11:13,251
Once you ingested your data, you
can use custom query language to

186
00:11:13,251 --> 00:11:17,330
perform advanced analytics across the
entire data set as well as produce.

187
00:11:17,721 --> 00:11:19,401
Visualizations of these results.

188
00:11:20,151 --> 00:11:23,631
In the next slide, we'll see how you
can also leverage KQL For runtime

189
00:11:23,631 --> 00:11:28,001
analytics, we can reduce human error
and increase accuracy by automating

190
00:11:28,001 --> 00:11:29,741
threat detection in Sentinel.

191
00:11:29,771 --> 00:11:31,601
We can do this by using analytics rules.

192
00:11:32,351 --> 00:11:36,040
We can schedule KQL queries to
be evaluated continuously and

193
00:11:36,040 --> 00:11:37,631
trigger creation of alerts.

194
00:11:38,560 --> 00:11:41,680
We can also automatically create
incidents to be triaged and

195
00:11:41,680 --> 00:11:43,450
remediated by operations teams.

196
00:11:43,950 --> 00:11:45,031
Sentinel Connects.

197
00:11:45,526 --> 00:11:49,305
Raw telemetry from various sources
such as logs, network activity,

198
00:11:49,365 --> 00:11:50,805
and user authentication events.

199
00:11:51,735 --> 00:11:55,545
We could, for example, query failed
login attempts across systems to

200
00:11:55,545 --> 00:11:57,495
identify potential brute force attacks.

201
00:11:57,995 --> 00:12:01,415
We would create a scheduled
analytics rule using our KQL query.

202
00:12:01,915 --> 00:12:05,935
In this example, a rule is configured
to detect 10 failed login attempts

203
00:12:05,995 --> 00:12:07,225
within a five minute window.

204
00:12:07,725 --> 00:12:09,580
These rules are run at regular intervals.

205
00:12:10,080 --> 00:12:12,840
Analyzing raw data from a
defined lookback period.

206
00:12:13,340 --> 00:12:18,080
If the query results exceed a defined
threshold, the rule generates an alert.

207
00:12:18,580 --> 00:12:22,700
When the rule detects suspicious activity,
such as a repeated fail login from

208
00:12:22,700 --> 00:12:25,241
the same IP address alert is created.

209
00:12:25,960 --> 00:12:29,531
These alerts can be aggregated
into an incident that SOC

210
00:12:29,531 --> 00:12:30,715
teams can investigate further.

211
00:12:31,215 --> 00:12:34,325
There are some additional rule
types, which you could, which are

212
00:12:34,325 --> 00:12:36,536
available the near realtime rules.

213
00:12:36,566 --> 00:12:39,446
These are prior faster detection
by running queries every minute.

214
00:12:39,836 --> 00:12:43,866
These are ideal for time
sensitive threats, anomaly rules.

215
00:12:44,436 --> 00:12:47,286
These use advanced machine learning
to identify unusual patterns in the

216
00:12:47,286 --> 00:12:50,046
data and Microsoft security rules.

217
00:12:50,616 --> 00:12:53,796
These automatically create incidents
in Sentinel based on alerts from

218
00:12:53,796 --> 00:12:56,646
other Microsoft security solutions
such as Microsoft Defense.

219
00:12:57,146 --> 00:13:00,536
So Sentinel enables organizations
to monitor the health of their data

220
00:13:00,536 --> 00:13:03,956
ingestion processes using tools
like the Data Collection, health

221
00:13:03,956 --> 00:13:07,396
Monitoring workbook, and the sen
the Sentinel Health Data Table.

222
00:13:07,896 --> 00:13:11,586
The Health Monitoring workbook is
basically an out of box workbook,

223
00:13:11,586 --> 00:13:14,616
provides comprehensive view
of the data ingestion process.

224
00:13:15,066 --> 00:13:17,586
It allows teams to monitor
ingestion, size, latency, and

225
00:13:17,586 --> 00:13:18,726
the number of logs per source.

226
00:13:19,550 --> 00:13:22,941
The workbook includes three key
tabs, the overview tab, which

227
00:13:22,941 --> 00:13:24,681
displays overall ingestion status.

228
00:13:25,181 --> 00:13:28,811
The data collection anomalies
tab highlights, irregularities or

229
00:13:28,811 --> 00:13:33,241
interruptions in data ingestion,
and the agent's information tab

230
00:13:33,541 --> 00:13:36,811
monitors the health and connectivity
of logger analytics agents, ensuring

231
00:13:36,811 --> 00:13:38,371
that is being collected as expected.

232
00:13:38,871 --> 00:13:42,561
The Sentinel Health Data table
provides granular insights into

233
00:13:42,561 --> 00:13:43,981
the status of the data connectors.

234
00:13:44,461 --> 00:13:45,331
Ingestion health.

235
00:13:45,451 --> 00:13:48,841
It tracks success and failure events
for each connector, enabling teams to

236
00:13:48,841 --> 00:13:50,551
identify and resolve issues quickly.

237
00:13:51,051 --> 00:13:54,441
The table supports querying for specific
health drifts, such as connectors

238
00:13:54,441 --> 00:13:58,851
transitioning from success to failure,
or identifying the latest failure events.

239
00:13:59,351 --> 00:13:59,711
Okay.

240
00:13:59,981 --> 00:14:02,201
Let's take a look at how you
would build your first connector.

241
00:14:02,701 --> 00:14:06,301
As I mentioned previously in, in
the session, Microsoft maintains

242
00:14:06,301 --> 00:14:09,971
a GitHub repository where all the.

243
00:14:10,601 --> 00:14:13,601
Sentinel Solutions are,
source code is available.

244
00:14:14,101 --> 00:14:16,731
So you can see here all the
instructions basically for

245
00:14:16,731 --> 00:14:18,621
contributing to this repo are here.

246
00:14:19,051 --> 00:14:21,001
If we look, there's a solutions folder.

247
00:14:21,091 --> 00:14:23,641
This is where we will
primarily be working.

248
00:14:23,961 --> 00:14:29,261
Today in this demo, I will, yeah,
I'll show you exactly what you,

249
00:14:29,291 --> 00:14:31,161
what steps you need to execute.

250
00:14:31,941 --> 00:14:35,201
To create a new solution which
will be available in Sentinel.

251
00:14:35,701 --> 00:14:39,681
Okay, so let's open up
this repository locally.

252
00:14:40,181 --> 00:14:40,541
Alright.

253
00:14:40,901 --> 00:14:45,731
If we're go into the solutions folder,
we go and look for an existing solution,

254
00:14:45,731 --> 00:14:47,436
which I created, which is Dynatrace.

255
00:14:47,936 --> 00:14:50,606
So we have to follow the convention.

256
00:14:51,506 --> 00:14:56,876
Of basically the vendor name in the
solution folder, and then within

257
00:14:56,876 --> 00:15:01,216
that vendor's folder, analytics
rules data connectors package.

258
00:15:01,286 --> 00:15:04,291
This is produced when you actually
when you compile your solution

259
00:15:04,781 --> 00:15:06,911
pauses, playbooks, and workbooks.

260
00:15:07,901 --> 00:15:08,261
Okay.

261
00:15:09,251 --> 00:15:12,681
So an important thing that you need
to create is the metadata file.

262
00:15:13,086 --> 00:15:18,856
So the metadata file describes all the
artifacts which are part of your solution.

263
00:15:19,096 --> 00:15:23,636
It also describes, how your
solution is supported and you're

264
00:15:23,636 --> 00:15:27,701
able to provide description,
logo, et cetera for your solution.

265
00:15:28,201 --> 00:15:28,491
Okay.

266
00:15:28,791 --> 00:15:34,111
What we saw in the presentation is that
the very first thing you know, how you get

267
00:15:34,111 --> 00:15:39,211
data into Sentinel is via data connectors,
or one of the ways is via data connectors.

268
00:15:39,661 --> 00:15:45,490
And in this demo we're gonna look
at how you would create a data

269
00:15:45,490 --> 00:15:47,190
collection, rule based data connector.

270
00:15:47,460 --> 00:15:47,820
Okay.

271
00:15:48,320 --> 00:15:50,930
There are a number of files
that we need to create, right?

272
00:15:50,930 --> 00:15:54,030
We can see here we need to
define essentially the data

273
00:15:54,030 --> 00:15:55,920
collection rule itself, right?

274
00:15:56,640 --> 00:16:01,500
And so this basically we need to
define the schema for the table, right?

275
00:16:01,590 --> 00:16:07,300
When we ingesting this data, we need to
define the destinations where we want

276
00:16:07,300 --> 00:16:09,350
to actually send the incoming telemetry.

277
00:16:09,850 --> 00:16:14,600
And then another very important
step is we need to define the

278
00:16:14,650 --> 00:16:17,550
transformation query, which we'll use.

279
00:16:17,550 --> 00:16:21,900
So the custom query, which we use to
transform the data as it, it's ingested.

280
00:16:22,400 --> 00:16:22,700
Okay?

281
00:16:23,200 --> 00:16:26,090
So another very important
file, which we need to.

282
00:16:26,590 --> 00:16:31,410
We need to create, is we need to actually
set up the polling configuration, right?

283
00:16:31,410 --> 00:16:35,920
So we are gonna build a
codeless a CCF connector, right?

284
00:16:36,310 --> 00:16:39,670
So we don't want the customer
to have to manage any sort

285
00:16:39,670 --> 00:16:40,600
of infrastructure in Azure.

286
00:16:40,600 --> 00:16:45,120
We want Microsoft to basically manage
the polling and the the running of the

287
00:16:45,120 --> 00:16:46,650
connector within their infrastructure.

288
00:16:47,505 --> 00:16:50,765
Okay, so of course we need
to give it a a useful name.

289
00:16:51,065 --> 00:16:55,005
And basically in this file, what
you can do is we can say we can

290
00:16:55,005 --> 00:16:58,215
define things like what specific.

291
00:16:59,115 --> 00:17:02,765
Parameters are required right
when polling the rest endpoint.

292
00:17:03,095 --> 00:17:06,605
So of course we need to know
the address of the endpoint, and

293
00:17:06,605 --> 00:17:08,195
this can be parameterized, right?

294
00:17:08,195 --> 00:17:13,805
So we will see in the UI how we actually
provide the Dynatrace environment, URL.

295
00:17:14,195 --> 00:17:17,945
But basically we concatenate that
with the actual rest endpoint,

296
00:17:17,945 --> 00:17:19,325
which we are gonna connect to.

297
00:17:19,865 --> 00:17:21,475
We can define, the HTT P method.

298
00:17:21,995 --> 00:17:25,515
We can define also how retries happen.

299
00:17:25,965 --> 00:17:27,495
We can define time format.

300
00:17:27,495 --> 00:17:30,715
So this is for basically
for the query string, right?

301
00:17:30,765 --> 00:17:36,325
We ask the API to return, attacks
in this case between two different

302
00:17:36,425 --> 00:17:38,045
two a start time and an end time.

303
00:17:38,545 --> 00:17:40,375
We can also define some HTP headers.

304
00:17:41,335 --> 00:17:44,825
And any additional query parameters
which we would have to which

305
00:17:44,825 --> 00:17:46,275
are needed by the rest API.

306
00:17:46,726 --> 00:17:52,605
In the response we can actually tell
the connector where to look for the

307
00:17:52,605 --> 00:17:54,105
data that's important to us, right?

308
00:17:54,105 --> 00:17:54,645
In the json.

309
00:17:55,315 --> 00:17:57,265
In this case, we're
looking at their tax array.

310
00:17:57,765 --> 00:18:01,236
Finally, we can actually
specify which DCR to use.

311
00:18:01,686 --> 00:18:04,086
And so this was the file
that we saw previously.

312
00:18:04,366 --> 00:18:06,075
We referenced this from
the polling config.

313
00:18:06,575 --> 00:18:10,456
And of course very important
is how paging happens, right?

314
00:18:10,456 --> 00:18:13,805
In this case we're looking to
use next page token paging type.

315
00:18:14,165 --> 00:18:19,635
And we're looking at basically
we we specify, what, where the

316
00:18:19,635 --> 00:18:21,905
page token is in the response.

317
00:18:22,865 --> 00:18:24,605
So in this case it's called next page key.

318
00:18:25,445 --> 00:18:30,165
And we can also, it's redundant in
this case, but we can also rename,

319
00:18:30,215 --> 00:18:32,145
the next page query string parameter.

320
00:18:32,445 --> 00:18:32,775
Okay.

321
00:18:33,275 --> 00:18:36,965
So another file we need to
define is the table file.

322
00:18:37,025 --> 00:18:38,135
So workspace table.

323
00:18:38,555 --> 00:18:43,305
So in this case we define the schema
for this table which is used in the DCR.

324
00:18:44,045 --> 00:18:46,675
As I mentioned time generators
is a required field.

325
00:18:46,975 --> 00:18:52,915
Here we can, we define what the schema
will look like after transformation.

326
00:18:52,975 --> 00:18:57,935
So you can see there's no suffixes
on, on field names, which indicate

327
00:18:57,935 --> 00:19:02,405
types on an underscore s in this case,
we also define then the type of each

328
00:19:02,405 --> 00:19:04,175
of those fields right in the table.

329
00:19:04,425 --> 00:19:08,995
Okay, now once we have those three
other files, we can now tie 'em

330
00:19:08,995 --> 00:19:10,465
together with the definition file.

331
00:19:11,245 --> 00:19:15,405
So here what we need to do is we
basically, we can define the connector ui.

332
00:19:15,905 --> 00:19:19,730
I can show you in a little bit what
that looks like, but basically we

333
00:19:19,730 --> 00:19:24,600
can we can define, what the publisher
name, description, all those things

334
00:19:24,600 --> 00:19:28,410
that, will need to be displayed in
the Azure portal to the end user.

335
00:19:28,860 --> 00:19:33,520
We can also define some example
queries as well as graph

336
00:19:33,520 --> 00:19:34,630
queries for example, for the.

337
00:19:34,910 --> 00:19:38,810
How much data has been received over,
over time, and which table to look at.

338
00:19:39,110 --> 00:19:44,940
We can, this availability section
basically we use that to mark

339
00:19:45,000 --> 00:19:48,510
a connector as preview in this
case, I'm using it in the demo.

340
00:19:48,790 --> 00:19:50,650
And you would set this to false, right?

341
00:19:50,650 --> 00:19:52,940
When it a connector would
be generally available.

342
00:19:52,990 --> 00:19:53,770
Generally available.

343
00:19:54,270 --> 00:19:57,690
We can define what permissions are
required, right on the workspace itself.

344
00:19:57,690 --> 00:20:00,561
So we need to, of course right
permissions are required on

345
00:20:00,561 --> 00:20:02,271
the workspace to ingest data.

346
00:20:02,771 --> 00:20:06,941
And then we can define some custom
elements which which are displayed, right?

347
00:20:06,941 --> 00:20:10,681
So in this case, I specify, the
requirements on the vendor side.

348
00:20:10,681 --> 00:20:14,431
So in this case, we need a Dynatrace
tenant with a specific feature activated.

349
00:20:14,806 --> 00:20:19,816
And of course we need an access
token which allows us to read attacks

350
00:20:19,816 --> 00:20:22,266
from, the best endpoint in this case.

351
00:20:22,336 --> 00:20:25,236
An access token is used for
authentication, but obviously

352
00:20:25,416 --> 00:20:27,966
the connector framework supports
multiple types of authentication.

353
00:20:28,466 --> 00:20:31,906
We also need to define instructions
for the end user, right?

354
00:20:32,156 --> 00:20:36,406
In this case, the customer has the
su supplier, us with the URL right?

355
00:20:36,646 --> 00:20:41,156
For their tenant, which they want, or the
environment that they want to connect to.

356
00:20:41,656 --> 00:20:45,046
Then, as I mentioned before,
an access token is required.

357
00:20:45,046 --> 00:20:48,916
So this is something that the the
end user has to provide us as well.

358
00:20:49,636 --> 00:20:54,706
Finally we set up a connection,
connect toggle button which

359
00:20:54,706 --> 00:20:55,846
you'll see later in the ui.

360
00:20:56,346 --> 00:20:58,266
Okay, so that's the data connector itself.

361
00:20:58,766 --> 00:21:02,276
I'll show you in a little bit
how we actually what that looks

362
00:21:02,276 --> 00:21:03,506
like right in the Azure portal.

363
00:21:04,376 --> 00:21:07,196
So some other interesting
things to show you, I think in

364
00:21:07,196 --> 00:21:09,296
this demo are analytics rules.

365
00:21:09,796 --> 00:21:13,876
So we can define analytics rules
for our solution as well, so we

366
00:21:13,876 --> 00:21:18,106
can, this essentially creates a
analytics rule template right in

367
00:21:18,266 --> 00:21:23,066
your Sentel workspace, which you
can then create instances of that.

368
00:21:23,126 --> 00:21:24,206
You can create an instance of that.

369
00:21:24,751 --> 00:21:26,911
And run it, to raise alerts, right?

370
00:21:26,911 --> 00:21:28,891
As I mentioned in, in the presentation.

371
00:21:29,311 --> 00:21:32,911
So in this case, what we're doing
is we're, it's defined in yaml, in

372
00:21:32,911 --> 00:21:39,211
this, in, in my demo, but we give
it a name description status which

373
00:21:39,211 --> 00:21:43,821
connector it's actually, dependent on,
as well as, the query frequency, the

374
00:21:44,151 --> 00:21:46,721
query period and basically tactics.

375
00:21:46,721 --> 00:21:52,301
So the matter framework tactics and
techniques, how we group the alerts.

376
00:21:52,721 --> 00:21:56,071
And then very important, of course,
is the query that we want to run.

377
00:21:56,611 --> 00:21:59,851
In this case, we, we're looking for
attacks which are not allowed listed.

378
00:22:00,286 --> 00:22:01,216
Okay.

379
00:22:01,936 --> 00:22:04,846
And we're giving it a custom name.

380
00:22:04,846 --> 00:22:08,856
So based on basically a format, so
we're looking to call it Dynatrace

381
00:22:08,856 --> 00:22:12,366
attack with this current state display
id, which is from Dynatrace, and

382
00:22:12,366 --> 00:22:14,406
then the display name from Dynatrace.

383
00:22:14,476 --> 00:22:15,526
From the Dynatrace attack.

384
00:22:16,026 --> 00:22:16,316
Okay.

385
00:22:16,816 --> 00:22:20,596
We are also configuring that we should
create incidents based on these at alerts.

386
00:22:21,436 --> 00:22:26,566
And we group the alerts over the
last seven days and we have to match.

387
00:22:26,566 --> 00:22:28,156
So we're doing entity mapping here.

388
00:22:28,336 --> 00:22:32,406
We need to match, all entities within
the alert for that to be grouped.

389
00:22:32,906 --> 00:22:35,666
Okay, I won't go into a lot of detail.

390
00:22:35,716 --> 00:22:40,866
But we also have some pauses included
in the solution as well as playbooks

391
00:22:40,876 --> 00:22:45,586
which essentially, are automation allow
you to automate processes based on

392
00:22:45,586 --> 00:22:48,726
like alerts or events sorry, incidents.

393
00:22:49,226 --> 00:22:52,286
And we can also include
workbooks in our solution.

394
00:22:52,376 --> 00:22:54,596
So workbooks, like
dashboards, essentially.

395
00:22:55,451 --> 00:22:58,721
Okay, so what does that look
like in the Azure portal?

396
00:22:58,821 --> 00:23:05,991
So we can see, we can come to Content Hub
in this case I have we can see Dynatrace

397
00:23:06,051 --> 00:23:08,571
has a solution published to Content Hub.

398
00:23:08,831 --> 00:23:12,891
We can see all the content, which
is included in the solution, right?

399
00:23:12,891 --> 00:23:19,451
So things like analytics, rule templates
data connectors, workbooks pauses,

400
00:23:20,041 --> 00:23:22,551
that's all available in this solution.

401
00:23:23,151 --> 00:23:27,691
So in this case, I've already installed
the connector, but basically if

402
00:23:27,691 --> 00:23:30,031
it was not installed, you'd have
the option to install it here.

403
00:23:30,531 --> 00:23:30,711
Okay.

404
00:23:31,211 --> 00:23:34,941
So once it's installed once it's
installed, you will see that

405
00:23:34,941 --> 00:23:37,871
there'll be a number of, data
Connect is available to you.

406
00:23:37,921 --> 00:23:41,701
So for the purposes of our
demo, we're looking at attacks.

407
00:23:41,761 --> 00:23:45,391
So in this case, I've created
a V two, which is a preview

408
00:23:45,391 --> 00:23:50,471
version of the connector which is
leverages data collection rules.

409
00:23:51,311 --> 00:23:53,371
And the V one was basically
the prior version, right?

410
00:23:53,371 --> 00:23:55,861
Which used prior technology from CCF.

411
00:23:56,161 --> 00:23:56,521
Okay.

412
00:23:56,581 --> 00:23:59,576
So if we open that V two connector page.

413
00:24:00,076 --> 00:24:02,441
We will see it's already connected, right?

414
00:24:02,441 --> 00:24:06,171
So right now attacks are being
ingested into te into sentinel.

415
00:24:06,411 --> 00:24:11,691
We can see over time, how many attacks
have or events have been received.

416
00:24:12,631 --> 00:24:16,981
Could, we can see here what we
defined in the JSON files, right?

417
00:24:17,411 --> 00:24:19,971
In the UI config, we can see the the.

418
00:24:20,471 --> 00:24:24,281
The customer or the end user
needs a Dynatrace tenant with

419
00:24:24,281 --> 00:24:25,691
specific feature enabled.

420
00:24:25,691 --> 00:24:29,541
We can see that an access token
is required and we can provide,

421
00:24:29,551 --> 00:24:32,531
documentation to the end user on how to.

422
00:24:33,116 --> 00:24:34,016
Just hit those things up.

423
00:24:34,516 --> 00:24:40,806
We also need to get, of course the URL of
the environment that the end user wants

424
00:24:40,806 --> 00:24:46,636
to monitor and access token, in this case,
on, to authenticate those risk calls.

425
00:24:46,666 --> 00:24:46,696
Okay.

426
00:24:47,196 --> 00:24:51,786
So the other thing which
we can do is we can create.

427
00:24:52,701 --> 00:24:55,821
So we can come in and look at the
rule templates, which were provided.

428
00:24:55,821 --> 00:24:58,491
So in, in this case, there are
a number of different templates

429
00:24:58,491 --> 00:25:00,111
provided out the box by the solution.

430
00:25:00,631 --> 00:25:02,761
In our case, we're interested
in attack detection.

431
00:25:03,261 --> 00:25:07,171
So here we can say, okay, create
a new rule based on this template.

432
00:25:07,801 --> 00:25:12,701
By default the arguments or the
parameters that you see the inputs

433
00:25:13,211 --> 00:25:15,341
are set to default values, right?

434
00:25:15,341 --> 00:25:18,521
Which we defined in the solution
in the YAML that you saw earlier.

435
00:25:18,771 --> 00:25:21,931
We can change this if we like, we
can change, alter the severity, we

436
00:25:21,931 --> 00:25:24,461
can alter the Mitre characteristics.

437
00:25:24,961 --> 00:25:28,896
We can also come through and say, okay we
wish to alter the KQL query, which is run.

438
00:25:29,396 --> 00:25:30,116
Which is scheduled.

439
00:25:30,776 --> 00:25:33,176
We can define different entity mapping.

440
00:25:33,176 --> 00:25:36,266
In this case, we're
mapping ips and host names.

441
00:25:36,766 --> 00:25:39,396
We can also change the query scheduling.

442
00:25:39,786 --> 00:25:45,746
So currently it's run every one
day and we look back one day.

443
00:25:46,526 --> 00:25:50,576
We can change the threshold
right where alert is generated in

444
00:25:50,576 --> 00:25:52,356
our case, if there's more than.

445
00:25:52,856 --> 00:25:56,876
Zero items in a return, then
we want to generate an alert.

446
00:25:57,136 --> 00:26:00,136
In our case we also trigger an alert
for each event, but you could also

447
00:26:00,136 --> 00:26:01,906
group these into a single alert.

448
00:26:02,406 --> 00:26:02,616
Okay.

449
00:26:03,116 --> 00:26:06,716
We can configure incident creation,
so in this case we are by default.

450
00:26:06,956 --> 00:26:10,136
And then we are also
configuring alert grouping.

451
00:26:10,256 --> 00:26:13,596
So we are grouping alerts, from
the last seven days into a single

452
00:26:13,596 --> 00:26:15,036
incident if the entities match.

453
00:26:15,296 --> 00:26:18,786
It's possible also to trigger
automation rules or playbooks, right?

454
00:26:18,886 --> 00:26:21,066
Using a, analytics rule.

455
00:26:21,566 --> 00:26:22,856
In this case, we'll leave it like it is.

456
00:26:23,356 --> 00:26:23,746
Click save.

457
00:26:24,246 --> 00:26:24,536
Okay.

458
00:26:25,036 --> 00:26:29,006
So now what we'll see is that alerts
will start to be generated based

459
00:26:29,006 --> 00:26:30,466
on this new rule that we've set up.

460
00:26:30,966 --> 00:26:31,296
Okay.

461
00:26:31,796 --> 00:26:31,976
Okay.

462
00:26:31,976 --> 00:26:35,966
We've seen what what our solution
looks like in the Azure portal.

463
00:26:36,716 --> 00:26:42,186
Now something I haven't shown you yet is
how you actually package your solution.

464
00:26:42,236 --> 00:26:44,966
Your, the content for your solution
into a package, which you can then,

465
00:26:45,276 --> 00:26:49,396
submit to the partner, Microsoft Partner
Center and publish right publicly.

466
00:26:49,896 --> 00:26:52,231
So let's take a look at how we do that.

467
00:26:52,731 --> 00:26:57,171
So what we need is basically to copy the
path to the data folder in your solution.

468
00:26:57,671 --> 00:27:01,011
We go into the tools folder
within the repository, right?

469
00:27:01,071 --> 00:27:06,896
So if we look at, I'm here currently
at that path within the repo I

470
00:27:06,956 --> 00:27:13,196
run sorry, I run the PowerShell
script, create solution V three.

471
00:27:13,696 --> 00:27:16,216
I paste in my solution
data folder path, right?

472
00:27:16,466 --> 00:27:20,546
This goes through a number of steps,
preparing basically the new package

473
00:27:20,996 --> 00:27:22,966
which we'll be able to publish.

474
00:27:23,466 --> 00:27:23,756
Okay?

475
00:27:23,951 --> 00:27:28,846
Once that's completed it produces a
number of output files, number of output.

476
00:27:29,346 --> 00:27:31,686
And also a package file, right?

477
00:27:31,686 --> 00:27:35,436
So if we go and look at that, so we
go into our package folder, right?

478
00:27:35,976 --> 00:27:38,886
You'll see there's a
new version, a zip file.

479
00:27:38,886 --> 00:27:44,616
So 3.0 0.2 which we would use to
publish our solution publicly.

480
00:27:45,306 --> 00:27:49,926
But what is also interesting is
that we have, the UI definition

481
00:27:49,956 --> 00:27:52,506
file as well as the arm template.

482
00:27:52,506 --> 00:27:56,076
So the Azure Resource Manager
template, which we could.

483
00:27:56,976 --> 00:27:57,996
Potentially use.

484
00:27:58,176 --> 00:28:01,646
So you can actually go into the
Azure portal and you can just,

485
00:28:01,736 --> 00:28:04,916
you can say deploy custom template

486
00:28:05,416 --> 00:28:07,126
and we can say, build our own template.

487
00:28:07,396 --> 00:28:08,686
We can load the file.

488
00:28:08,746 --> 00:28:11,661
So in this case let's get
the path to that fall.

489
00:28:12,161 --> 00:28:13,316
Okay, we click save.

490
00:28:13,816 --> 00:28:18,916
We then can provide in this case,
we can create a new resource group.

491
00:28:19,416 --> 00:28:21,961
We provide all the parameters, right?

492
00:28:22,571 --> 00:28:24,721
In this case, the analytics workspace.

493
00:28:25,226 --> 00:28:29,566
Resource group, the location for the
workspace, the name of the workbook that

494
00:28:29,566 --> 00:28:31,666
we wanna create, resource group name.

495
00:28:31,716 --> 00:28:34,416
You can leave these values as
they are, they'll take whatever's

496
00:28:34,416 --> 00:28:36,286
set at the deployment level.

497
00:28:36,786 --> 00:28:40,176
And once you've entered in all
these parameters, you click next.

498
00:28:40,676 --> 00:28:44,886
And this will essentially deploy
the solu, the side load the solution

499
00:28:44,886 --> 00:28:46,086
right into your subscription.

500
00:28:46,536 --> 00:28:49,726
So you won't have to publish it
by a partner center, for example.

501
00:28:49,776 --> 00:28:54,736
You can actually deploy this way to
test your solution and the content that

502
00:28:54,736 --> 00:28:55,786
you're deploying with your solution.

503
00:28:56,626 --> 00:28:58,086
This is what I did for this demo.

504
00:28:58,586 --> 00:29:02,936
Our goal as an enterprise should be
to limit threat impact by triggering

505
00:29:02,936 --> 00:29:05,156
immediate response on threat detection.

506
00:29:05,656 --> 00:29:09,016
Manual triage and remediation
is often slow prone to human

507
00:29:09,016 --> 00:29:10,456
error and resource intensive.

508
00:29:11,176 --> 00:29:14,566
Security teams face challenges such
as delayed responses to threats,

509
00:29:14,866 --> 00:29:17,026
high operational costs due to 24 7.

510
00:29:17,026 --> 00:29:20,416
Staffing needs fatigue and
burnout from handling repetitive

511
00:29:20,416 --> 00:29:22,006
tasks and high alert volume.

512
00:29:22,506 --> 00:29:27,636
Automated triage and remediation Sentinel
replaces manual workflows with automated

513
00:29:27,636 --> 00:29:33,966
triage and remediation processes, ensuring
threats are addressed in real time.

514
00:29:34,446 --> 00:29:37,716
Automation rules and playbooks are
used to evaluate alerts continuously,

515
00:29:37,986 --> 00:29:41,376
trigger immediate responses, and
reduce the cost of operations.

516
00:29:41,876 --> 00:29:43,736
Thank you so much for
watching my session today.

517
00:29:44,516 --> 00:29:46,436
I hope to see you again really soon.

