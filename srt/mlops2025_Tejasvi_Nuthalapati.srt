1
00:00:03,140 --> 00:00:04,019
Hello everyone.

2
00:00:04,080 --> 00:00:08,610
My name is Tejasvi Nuthalapati and I work as a
lead software development engineer at

3
00:00:08,610 --> 00:00:13,230
Amazon, and I'm super excited to be
here with you today presenting my topic.

4
00:00:14,400 --> 00:00:15,390
So let's get started.

5
00:00:15,630 --> 00:00:18,930
The title of my talk is
Human in the Loop, ML Ops

6
00:00:22,440 --> 00:00:26,010
Production Patterns That Boost
Model Performance by 40% while

7
00:00:26,305 --> 00:00:27,225
maintaining human Control.

8
00:00:27,960 --> 00:00:28,830
Now it's now.

9
00:00:28,830 --> 00:00:32,640
That's definitely a how a mouthful,
but I want to set the stage.

10
00:00:32,640 --> 00:00:36,060
Clearly, the session is not
going to be abstract theory.

11
00:00:36,300 --> 00:00:40,140
It is about real architectures,
measurable outcomes and patterns that

12
00:00:40,140 --> 00:00:42,330
you can apply in your own environments.

13
00:00:42,780 --> 00:00:47,280
My goal is simple to show you that
when we design ML LOP systems in a

14
00:00:47,280 --> 00:00:52,050
way that balances automation with
human judgment, we cannot only

15
00:00:52,050 --> 00:00:57,390
improve performance, but also build
systems that teams actually trust.

16
00:01:02,190 --> 00:01:04,830
So if you've worked with machine
learning and production, you

17
00:01:04,830 --> 00:01:06,270
know the pain of extremes.

18
00:01:06,750 --> 00:01:09,000
On one side we have full automation.

19
00:01:09,090 --> 00:01:13,080
Everything is left to the model and
the pipelines look clean and fast.

20
00:01:13,860 --> 00:01:17,280
But underneath that Polish
drift starts creeping in.

21
00:01:19,170 --> 00:01:21,510
Something like false
positives should show up.

22
00:01:22,050 --> 00:01:24,240
Each case pile up and B long.

23
00:01:24,240 --> 00:01:27,240
No one is really sure why the
system is making the decisions.

24
00:01:27,240 --> 00:01:30,690
It is making trust evaporates.

25
00:01:31,530 --> 00:01:35,970
So on the other side we have a, we
have like multiple manual bottlenecks.

26
00:01:36,210 --> 00:01:38,339
Every decision requires a human check.

27
00:01:38,759 --> 00:01:42,750
Accuracy might look good, but the
system becomes slow and expensive.

28
00:01:43,050 --> 00:01:46,859
CU burn out on repetitive
work and throughput collapses.

29
00:01:47,639 --> 00:01:48,359
You heard that right?

30
00:01:50,055 --> 00:01:55,335
Both of these extremes fail us, and yet
too often organizations think the solution

31
00:01:55,335 --> 00:01:59,235
is to just double down on whichever
extreme they have already chosen.

32
00:01:59,805 --> 00:02:03,345
More automation or like more humans.

33
00:02:03,705 --> 00:02:06,045
But the truth is, may
the path is sustainable.

34
00:02:08,205 --> 00:02:09,780
So let me talk to you about the third.

35
00:02:11,850 --> 00:02:13,019
This is a new concept.

36
00:02:13,140 --> 00:02:15,390
Um, so instead, humans versus machines.

37
00:02:15,390 --> 00:02:16,950
It is humans with machines.

38
00:02:17,010 --> 00:02:21,870
That's the third way thinking of it as
designing systems where AI brings scale,

39
00:02:21,870 --> 00:02:27,989
speed, and pattern recognition, and humans
bringing judgment, context and ethics.

40
00:02:28,290 --> 00:02:32,820
Each is doing what they do best at
what they do, and the pipeline treats

41
00:02:32,820 --> 00:02:34,950
both as first class participants.

42
00:02:35,609 --> 00:02:38,370
Here's a story that
illustrates a simple point.

43
00:02:39,570 --> 00:02:45,300
I was researching about a FinTech com, uh,
company, and, um, it was drowning in false

44
00:02:45,300 --> 00:02:47,400
positives in their fraud detection system.

45
00:02:47,640 --> 00:02:51,300
Transactions were consistently
being flagged and estimate.

46
00:02:51,300 --> 00:02:52,650
Customers were furious.

47
00:02:52,710 --> 00:02:56,040
Initially, they tried tuning
the models, but the improvements

48
00:02:56,040 --> 00:02:57,180
were a bit teeny tiny.

49
00:02:57,900 --> 00:03:01,680
What made the real difference
was embedding structured human

50
00:03:01,680 --> 00:03:02,910
review into the pipeline.

51
00:03:04,575 --> 00:03:08,805
Analysts weren't just fixing mistakes
on the site, their decision were

52
00:03:08,805 --> 00:03:13,155
logged, versioned, and directly
tied to the retraining cycles.

53
00:03:14,985 --> 00:03:16,425
So finally, what's the result?

54
00:03:17,325 --> 00:03:19,695
It's the false positives dropped by 40%.

55
00:03:21,615 --> 00:03:25,095
It essentially means there's an
uplift that that just didn't come

56
00:03:25,095 --> 00:03:26,745
from some breakthrough algorithm.

57
00:03:26,745 --> 00:03:30,285
It came from rearchitecting the
system so that humans and machines

58
00:03:30,285 --> 00:03:34,575
were truly collaborative and
collaborating at the same time.

59
00:03:36,225 --> 00:03:40,035
I'm going to talk to you about a
few patterns that have identified,

60
00:03:40,035 --> 00:03:41,625
and let's go one after the other.

61
00:03:41,625 --> 00:03:43,425
First one is the tiered autonomy.

62
00:03:44,400 --> 00:03:47,490
This essentially enables this
collaboration that is something,

63
00:03:47,550 --> 00:03:50,760
um, I call it as a tiered autonomy.

64
00:03:51,030 --> 00:03:53,490
So let's go back to the
fraud detection example.

65
00:03:53,580 --> 00:03:57,990
Low value, low risk transactions
where the model could have like

66
00:03:58,080 --> 00:03:59,610
high, very high confidence.

67
00:03:59,940 --> 00:04:01,320
Those could be auto approved.

68
00:04:01,620 --> 00:04:06,030
Medium risk cases where the model
is a little bit uncertain should

69
00:04:06,030 --> 00:04:10,565
be routed to a human analyst, which
can bring in context to the problem.

70
00:04:12,030 --> 00:04:15,239
And high risk cases involving
large sums of money should be

71
00:04:15,239 --> 00:04:17,040
escalated to full human review.

72
00:04:17,370 --> 00:04:21,510
But with AI generated insights and
supporting data attached to speed

73
00:04:21,510 --> 00:04:26,370
of decision making, from a technical
standpoint, this can be implemented

74
00:04:26,370 --> 00:04:27,900
in multiple different waves.

75
00:04:28,409 --> 00:04:32,085
So let's start with service
measures that, uh, handle routing.

76
00:04:32,685 --> 00:04:37,485
Uh, CICD branching that responds to
confidence thresholds or even, uh,

77
00:04:37,515 --> 00:04:41,505
Kubernetes operators that govern,
which requests require intervention.

78
00:04:42,015 --> 00:04:44,175
But the technology is not
the heart of the pattern.

79
00:04:44,565 --> 00:04:49,215
The heart of it is, um, essentially
designing autonomy as a spectrum in stuff.

80
00:04:49,215 --> 00:04:54,795
A binary choice in one production
deployment, um, the tier autonomy

81
00:04:54,795 --> 00:04:57,495
reduced analyst workload by 60%.

82
00:04:57,945 --> 00:05:01,960
Analysts were no longer wasting time
on trivial cases and their expertise.

83
00:05:02,895 --> 00:05:07,875
Which was essentially, um, must be
concentrated towards what matters the

84
00:05:07,875 --> 00:05:10,395
most and the throughput increased.

85
00:05:10,425 --> 00:05:13,485
Um, and finally the
customer trust also went up.

86
00:05:13,785 --> 00:05:16,485
So the system as a whole
became much more reliable.

87
00:05:19,665 --> 00:05:22,875
So I'm also talk, going to talk about
the pattern and the second pattern,

88
00:05:22,875 --> 00:05:24,760
which is the observable ML model.

89
00:05:27,210 --> 00:05:29,430
Which is essentially
absorbability in a fancy way.

90
00:05:29,669 --> 00:05:32,460
So if you think about it, in
traditional software engineering,

91
00:05:32,460 --> 00:05:36,359
we would never deploy a service
without logs, metrics and traces.

92
00:05:36,690 --> 00:05:40,469
Yet with machine learning models, too
many organizations throw models into

93
00:05:40,469 --> 00:05:43,799
production as opaque black boxes too bad.

94
00:05:44,159 --> 00:05:47,429
So imagine a recommendation system
that suddenly starts suggesting

95
00:05:47,429 --> 00:05:49,740
irrelevant or outdated products.

96
00:05:49,919 --> 00:05:50,669
How would you see that?

97
00:05:51,210 --> 00:05:51,900
Just imagine.

98
00:05:51,900 --> 00:05:54,330
So without visibility
into confidence scores.

99
00:05:54,900 --> 00:05:58,620
Feature drift or reasoning traces,
you won't realize there's a problem

100
00:05:58,620 --> 00:06:00,300
until customers start complaining.

101
00:06:00,990 --> 00:06:03,030
By then, trust has already eroded.

102
00:06:04,710 --> 00:06:10,320
The solution is to bring absorbability
patterns or the principles into ML system.

103
00:06:10,320 --> 00:06:12,659
That means structured decision logging.

104
00:06:14,460 --> 00:06:18,600
And that also means, it means, uh,
building dashboards that show confidence

105
00:06:18,600 --> 00:06:24,150
distributions, um, and also drift
indicators along with feature stability.

106
00:06:24,390 --> 00:06:29,460
That means explanations that are layered
so that engineers, business stakeholders,

107
00:06:29,460 --> 00:06:32,220
and even end users can understand
decisions at the level they need.

108
00:06:33,630 --> 00:06:36,360
When models become
observable, two things happen.

109
00:06:36,660 --> 00:06:41,970
First, operators can catch drift and
bias early before it becomes a crisis.

110
00:06:42,210 --> 00:06:46,830
And second, trust grows stakeholders
who might otherwise rest a state

111
00:06:46,830 --> 00:06:50,640
adoption, start to see the system
as something transparent and

112
00:06:50,640 --> 00:06:52,800
manageable, but not as a black box.

113
00:06:53,010 --> 00:06:55,435
This is a huge change in
perspective if you think about it.

114
00:06:58,560 --> 00:07:03,330
The last and the most, much more important
one is the human in the loop pipeline.

115
00:07:03,330 --> 00:07:07,349
So the, this is the one that I'm, um,
more interested in driving the point.

116
00:07:07,710 --> 00:07:12,000
So right now, in many organizations, human
feedback is treated as an afterthought.

117
00:07:12,780 --> 00:07:17,729
Yeah, a human spot's a mistake,
files a ticket, and maybe that

118
00:07:17,729 --> 00:07:21,630
data makes it way back into
retraining month, many months later.

119
00:07:22,770 --> 00:07:26,215
So that process is far too slow,
and by the time it completes the,

120
00:07:26,340 --> 00:07:29,640
the system might have already
drifted far, far farther apart.

121
00:07:30,120 --> 00:07:33,780
So instead, we should treat human
feedback as first class data.

122
00:07:34,050 --> 00:07:37,440
Every human correction
should be logged, tied to the

123
00:07:37,440 --> 00:07:39,130
specific model version that it.

124
00:07:39,720 --> 00:07:44,940
Um, like it began and was in the pipeline,
that feedback should be automatically

125
00:07:44,940 --> 00:07:49,020
incorporated into retraining, so
the system continuously learns.

126
00:07:49,830 --> 00:07:52,170
Let's look at content
moderation as an example.

127
00:07:52,200 --> 00:07:57,720
When a moderator flags post that model
misclassified, that feedback is not

128
00:07:57,720 --> 00:08:02,670
just noted, it is captured, version,
and fed right back into the pipeline.

129
00:08:04,020 --> 00:08:07,260
When you see this over time,
the model becomes sharper, more

130
00:08:07,320 --> 00:08:09,360
aligned with real human judgment.

131
00:08:10,034 --> 00:08:14,864
In one system, I observed this loop
raised the production F1 score from 0.72

132
00:08:15,465 --> 00:08:22,034
to 0.84 in just a few three training
cycles, a 16% lift in measurable accuracy.

133
00:08:22,424 --> 00:08:26,804
That improvement didn't just come from
new features or maybe new algorithms.

134
00:08:27,195 --> 00:08:32,025
It just came from designing the pipeline
so that human judgment flowed directly

135
00:08:32,025 --> 00:08:33,525
into the model's learning process.

136
00:08:34,829 --> 00:08:36,270
And here's the few key points.

137
00:08:36,299 --> 00:08:41,010
As models get stronger, human input,
bigger doesn't just become less valuable,

138
00:08:41,309 --> 00:08:45,420
it becomes more valuable because
humans are the only ones who can guide

139
00:08:45,420 --> 00:08:50,040
systems through the messy, ambiguous,
ethically sensitive cases that no

140
00:08:50,040 --> 00:08:55,890
model, sorry, that no model will have
a fully master, which is what I said.

141
00:08:57,480 --> 00:09:00,569
So the final one is
the scalable oversight.

142
00:09:01,980 --> 00:09:03,719
Which deals with the reality of scale.

143
00:09:03,810 --> 00:09:09,089
As systems grow, you cannot have humans
reviewing every single decision of yours.

144
00:09:09,420 --> 00:09:10,860
That simply doesn't scale.

145
00:09:11,730 --> 00:09:16,469
But if you reduce human oversight too
much, you expose yourself to risk.

146
00:09:16,920 --> 00:09:19,439
The answer is adaptable.

147
00:09:19,469 --> 00:09:21,209
Oversight, you guessed it.

148
00:09:21,209 --> 00:09:21,600
Right?

149
00:09:22,020 --> 00:09:24,180
Uh, so you stratify risk levels.

150
00:09:24,185 --> 00:09:28,410
You, you use, you just use intelligent
sampling and you design escalation parts.

151
00:09:29,265 --> 00:09:32,685
Take supply chain, um, anomaly
detection as an example.

152
00:09:32,685 --> 00:09:36,975
Instead of reviewing every alert,
the team over sample low conference

153
00:09:36,975 --> 00:09:39,405
cases and routed them to humans.

154
00:09:39,705 --> 00:09:44,385
By doing so, they caught 90% of
novel anomalies, thinks the model

155
00:09:44,385 --> 00:09:48,915
had never seen before, while cutting
overall human workload in half.

156
00:09:49,215 --> 00:09:50,890
That's what scalable oversight looks.

157
00:09:52,770 --> 00:09:54,920
Human attention grows, subline.

158
00:09:55,620 --> 00:09:56,880
Subline yearly.

159
00:09:57,300 --> 00:10:02,040
With system growth, you still have the
quality and the trust that human review

160
00:10:02,040 --> 00:10:05,940
provides, but without overwhelming
your people, just what I said.

161
00:10:09,600 --> 00:10:12,840
So how do you measure success
in human, in the loop systems?

162
00:10:14,984 --> 00:10:16,635
You don't stop at model accuracy.

163
00:10:16,665 --> 00:10:17,834
You just don't stop there.

164
00:10:18,104 --> 00:10:22,364
You have to measure the system,
humans as well as machines together.

165
00:10:22,875 --> 00:10:25,995
That means tracking the
reduction in false positives.

166
00:10:27,675 --> 00:10:30,795
It also means measuring
time to drift detection.

167
00:10:30,854 --> 00:10:35,630
It means calculating how quickly human
feedback makes its way into retraining.

168
00:10:37,110 --> 00:10:42,210
It even means surveying operator
confidence because a system that humans

169
00:10:42,210 --> 00:10:47,700
trust is one that they actually use
effectively across multiple organizations.

170
00:10:49,410 --> 00:10:52,260
Um, also, uh, let me
forget, uh, not forget this.

171
00:10:52,260 --> 00:10:56,580
So these patterns have consistently
driven around a 40% improvement

172
00:10:56,580 --> 00:10:57,780
in effective performance.

173
00:10:58,050 --> 00:11:01,350
Again, not because the model
themselves got radically better, but

174
00:11:01,350 --> 00:11:04,710
because the system just not just.

175
00:11:05,444 --> 00:11:06,314
It's the system.

176
00:11:06,314 --> 00:11:09,704
The collaboration between
humans and machines were, was

177
00:11:09,704 --> 00:11:11,505
architected more intelligently.

178
00:11:18,074 --> 00:11:18,869
Let's look for.

179
00:11:19,589 --> 00:11:20,400
Farther ahead.

180
00:11:20,430 --> 00:11:24,150
The future of ML lops is not about
squeezing humans out of the loop.

181
00:11:24,449 --> 00:11:26,579
It's just about amplifying them.

182
00:11:26,670 --> 00:11:31,380
Machines will continue to handle
scale, speed and repetitive pattern

183
00:11:31,380 --> 00:11:34,770
recognition, but humans will
continue to provide oversight,

184
00:11:34,770 --> 00:11:36,449
creativity, and ethical grounding.

185
00:11:37,170 --> 00:11:41,939
When we design systems with that balance,
we don't just get models that are a

186
00:11:41,939 --> 00:11:43,920
few percentage points more accurate.

187
00:11:44,280 --> 00:11:47,250
We get systems that
organizations actually trust.

188
00:11:47,640 --> 00:11:51,750
We get pipelines that operators
can debug as well as improve.

189
00:11:53,430 --> 00:11:58,050
And also let's not forget, and we get gen
AI that end users feel confident depending

190
00:11:58,050 --> 00:12:04,200
on, so my closing thought is this, stop
thinking of humans and AI as substitutes.

191
00:12:04,290 --> 00:12:06,810
Start thinking of them
as co-collaborators.

192
00:12:07,185 --> 00:12:12,075
This is where the real 40% uplift
comes from, and that is how we build

193
00:12:12,075 --> 00:12:15,675
the next generation of production
ml, lops systems, systems that are

194
00:12:15,675 --> 00:12:19,185
powerful, scalable, and trustworthy.

195
00:12:20,025 --> 00:12:20,775
Trustworthy.

196
00:12:21,015 --> 00:12:21,465
Thank you.

197
00:12:25,500 --> 00:12:25,620
I.

