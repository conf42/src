1
00:00:01,260 --> 00:00:03,330
Hello everyone, and thank
you for joining me today.

2
00:00:03,990 --> 00:00:04,845
My name is Garish.

3
00:00:06,030 --> 00:00:09,960
Today we'll go beyond SLOs and
see how AI is revolutionizing

4
00:00:10,050 --> 00:00:11,310
site reliability engineering.

5
00:00:12,570 --> 00:00:16,260
So before we dive into specifics, let's
take a moment to look at the roadmap

6
00:00:16,260 --> 00:00:19,230
for our discussion Today we're gonna
cover a lot of important doubt related

7
00:00:19,230 --> 00:00:21,030
to SRE and how AI is transforming it.

8
00:00:22,620 --> 00:00:27,270
We'll start by talking about the core
challenges that Sari faces today.

9
00:00:27,985 --> 00:00:31,345
That is set the stage for talking
about why we need some of these

10
00:00:31,345 --> 00:00:32,725
advanced solutions for them.

11
00:00:33,985 --> 00:00:36,805
We'll talk about the what and
the how what are the different

12
00:00:36,805 --> 00:00:39,845
solutions and how they're applied
and how they're applicable.

13
00:00:41,165 --> 00:00:43,595
We look at anomaly detection
and little bit of detail.

14
00:00:44,145 --> 00:00:47,714
We break down what anomalies are the
different types, how we detect them.

15
00:00:48,235 --> 00:00:50,875
This is crucial for understanding
how we can start to predict them.

16
00:00:52,735 --> 00:00:57,755
And then we look at some of the case
studies for big companies out there

17
00:00:57,755 --> 00:00:59,195
who use some of these techniques.

18
00:00:59,265 --> 00:01:02,895
Effectively we look at
tools and technologies.

19
00:01:03,164 --> 00:01:08,294
We'll discuss the major open source
and commercial options available for

20
00:01:08,294 --> 00:01:09,585
applying some of these solutions.

21
00:01:11,024 --> 00:01:12,854
We'll look at the future of AI and SRE.

22
00:01:13,395 --> 00:01:14,955
What the trends in this field are.

23
00:01:16,094 --> 00:01:18,614
And finally we'll wrap up with some
practical recommendations for getting

24
00:01:18,614 --> 00:01:21,024
started and applying these practices.

25
00:01:23,755 --> 00:01:31,294
Alright, so what are the key metrics
that we are going to rely upon as

26
00:01:31,294 --> 00:01:32,704
we start building these systems?

27
00:01:33,874 --> 00:01:37,934
In the world of SRE metrics
like availability, latency,

28
00:01:38,205 --> 00:01:40,245
throughput and error rates.

29
00:01:42,630 --> 00:01:45,480
From the foundation for doing a lot.

30
00:01:45,480 --> 00:01:50,300
The analysis that we do, these metrics
are the North stop and they help

31
00:01:50,300 --> 00:01:52,520
us build reliable user experiences.

32
00:01:54,740 --> 00:01:58,250
We'll also talk about the monitoring
explosion that is happening.

33
00:01:59,300 --> 00:02:05,290
So it's almost as if we have become,
too good at collecting data and

34
00:02:05,290 --> 00:02:06,640
now there's an explosion of it.

35
00:02:06,790 --> 00:02:08,170
There's too many signals.

36
00:02:08,530 --> 00:02:09,520
And too much noise.

37
00:02:09,550 --> 00:02:13,990
So that makes it difficult for
us to pinpoint problems and

38
00:02:13,990 --> 00:02:15,190
understand what's really going on.

39
00:02:16,420 --> 00:02:19,570
It also causes alert fatigue, right?

40
00:02:19,900 --> 00:02:25,319
If there are just too many, signals and
noise coming in, we don't know what the

41
00:02:25,319 --> 00:02:30,989
false positives are, what significant or
insignificant pieces of data are, and this

42
00:02:30,989 --> 00:02:36,929
can be problematic because it can cause
us to, have a delay and how quickly we

43
00:02:36,929 --> 00:02:39,489
can implement solutions to these problems.

44
00:02:42,339 --> 00:02:49,369
So then we'll talk about the
shift in predictive reliability.

45
00:02:50,449 --> 00:02:52,689
So traditionally SRE has been reactive.

46
00:02:53,019 --> 00:02:55,089
We wait for things to
break and then we fix them.

47
00:02:56,380 --> 00:02:59,589
But today we are moving more
towards being able to predict.

48
00:03:00,835 --> 00:03:04,345
And be ready for problems to
happen before they actually happen.

49
00:03:05,245 --> 00:03:09,124
And this is where AI and ML
also come in and we'll dive

50
00:03:09,124 --> 00:03:10,265
into these in the next section.

51
00:03:11,794 --> 00:03:12,154
Alright

52
00:03:14,164 --> 00:03:20,329
firstly let's talk about the,
what, what can AI do for us?

53
00:03:20,329 --> 00:03:22,909
So one of the things it can
do is intelligent learning.

54
00:03:24,004 --> 00:03:31,774
So what that means is AI can be used to
reign in the flood of alerts that we can

55
00:03:31,774 --> 00:03:37,744
get if we are gathering too many metrics
because AI can filter out the noise

56
00:03:37,864 --> 00:03:42,474
and identify the really crucial bits
of data that we should be focusing on.

57
00:03:43,404 --> 00:03:47,394
This reduces alert fatigue and
helps us focus on what is important.

58
00:03:49,374 --> 00:03:51,354
So the next what.

59
00:03:52,299 --> 00:03:54,489
We should look at as
predictive maintenance.

60
00:03:54,549 --> 00:04:00,839
So this is one of the major benefits
of AI, is its ability to look at

61
00:04:00,839 --> 00:04:05,189
past data and help us identify
patterns that we can use to forecast

62
00:04:05,759 --> 00:04:07,169
our problems before they happen.

63
00:04:08,279 --> 00:04:12,329
So this proactive approach helps
us avoid outages altogether.

64
00:04:13,329 --> 00:04:16,639
The next thing we should look
at is automated remediation.

65
00:04:18,109 --> 00:04:20,659
So this is an exciting topic.

66
00:04:20,809 --> 00:04:25,489
So AI can also remediate many of the
problems that we face in an automated way.

67
00:04:26,239 --> 00:04:30,334
So when an issue is detected, AI can
trigger corrective actions automatically.

68
00:04:31,769 --> 00:04:35,459
This significantly reduces the
time it takes to solve problems,

69
00:04:35,459 --> 00:04:36,539
and it minimizes doubting.

70
00:04:38,159 --> 00:04:39,780
And systems can become self-healing.

71
00:04:41,969 --> 00:04:44,009
AI also helps us in capacity planning.

72
00:04:44,909 --> 00:04:47,519
So this is another crucial area.

73
00:04:48,469 --> 00:04:52,310
AI can predict future demand
and help us optimize a resource

74
00:04:52,310 --> 00:04:53,360
allocation accordingly.

75
00:04:54,080 --> 00:04:58,844
This ensures that we have enough resources
to handle peak loads without over

76
00:04:58,844 --> 00:05:01,125
provisioning and without wasting money.

77
00:05:02,835 --> 00:05:05,685
It also helps us be
efficient and cost effective.

78
00:05:07,424 --> 00:05:13,034
We can use AI to do root cause
analysis, so AI can assist us in quickly

79
00:05:13,034 --> 00:05:14,474
identify the root cause of problems.

80
00:05:14,474 --> 00:05:19,494
So it can do this by analyzing various
data points and correlations from

81
00:05:19,494 --> 00:05:26,335
the past, and it can identify and
point to source of issues, saving

82
00:05:26,335 --> 00:05:29,034
a lot of time and effort that you
would've had to otherwise spend.

83
00:05:29,604 --> 00:05:31,315
Do this using traditional approaches.

84
00:05:32,934 --> 00:05:35,504
We can also use AI for
incident management.

85
00:05:36,075 --> 00:05:39,744
So AI can automate various incident
management tasks statistical

86
00:05:39,744 --> 00:05:41,034
creation and prioritization.

87
00:05:42,384 --> 00:05:46,704
This streamlines the incident response
process, and that ensures that

88
00:05:46,704 --> 00:05:48,294
critical issues are less promptly.

89
00:05:50,784 --> 00:05:53,364
And let's talk about ChatOps enhancement.

90
00:05:53,724 --> 00:05:55,464
So finally, AI can enhance ChatOps.

91
00:05:56,829 --> 00:05:59,679
By using natural language
processing to interact with systems

92
00:05:59,679 --> 00:06:01,179
and run diagnostics via chat.

93
00:06:02,139 --> 00:06:04,719
So this makes it easier for
teams to troubleshoot and

94
00:06:04,719 --> 00:06:06,429
manage systems in real time.

95
00:06:06,979 --> 00:06:07,369
Excellent.

96
00:06:08,689 --> 00:06:16,274
Alright now let's talk about the how as
we've seen what AI can do in the field of

97
00:06:16,894 --> 00:06:18,794
now let's talk a little bit about the how.

98
00:06:20,144 --> 00:06:22,994
Let's look at the different
machine learning algorithms and

99
00:06:22,994 --> 00:06:24,164
how they're applicable here.

100
00:06:24,554 --> 00:06:26,834
So there's of course, supervised learning.

101
00:06:27,914 --> 00:06:33,414
So think of this as AI learning
by looking at past examples.

102
00:06:33,414 --> 00:06:40,224
So we provide label data, excuse me data
where we've had the correct outcome or

103
00:06:40,224 --> 00:06:43,824
data about incidents from the past and.

104
00:06:44,229 --> 00:06:49,609
AI can use that to identify, what
issues look like and what the resolution

105
00:06:49,699 --> 00:06:51,049
resolutions have been in the past.

106
00:06:52,049 --> 00:06:56,849
There's also unsupervised learning,
so this is where AI can find patterns

107
00:06:56,909 --> 00:06:58,829
in its on its own without label data.

108
00:06:59,129 --> 00:07:02,129
So this is incredibly useful
for anomaly detection.

109
00:07:03,089 --> 00:07:05,909
AI can look for unusual behavior
and deviations from the norm.

110
00:07:06,509 --> 00:07:10,019
It's like saying hey does
something look off here?

111
00:07:10,109 --> 00:07:12,329
Let me know if something looks
off so I can investigate.

112
00:07:13,349 --> 00:07:19,049
So for example, if there is certain
spike in traffic, then you know, non

113
00:07:19,049 --> 00:07:22,829
detection algorithms can identify
it because, it looks different from

114
00:07:22,949 --> 00:07:25,109
what the non spike behavior is.

115
00:07:26,589 --> 00:07:27,819
There's also reinforcement.

116
00:07:29,179 --> 00:07:32,509
This is where we are training
our AI through trial and error.

117
00:07:32,609 --> 00:07:38,809
AI takes an action and receives feedback
from us, and it uses that to improve how

118
00:07:38,809 --> 00:07:40,729
it does remediation the next time around.

119
00:07:41,519 --> 00:07:45,039
For example it can, using this,
it can find optimal ways to

120
00:07:45,039 --> 00:07:46,329
respond to different situations.

121
00:07:46,339 --> 00:07:52,039
For example AI can try different ways to
restart a service if one method fails.

122
00:07:52,609 --> 00:07:55,899
It can realize that and, look
for a different approach.

123
00:07:58,629 --> 00:08:03,099
Let's talk about the different
anomaly detection techniques.

124
00:08:05,079 --> 00:08:07,869
So there can be statistical methods.

125
00:08:08,809 --> 00:08:13,739
These are based on statistical
techniques where we are looking at.

126
00:08:14,340 --> 00:08:18,569
Our data and calculating various scores
and using that to identify anomaly.

127
00:08:18,569 --> 00:08:22,629
So for example with the Z score, we
are measuring how a specific data

128
00:08:22,629 --> 00:08:24,620
point deviates from the average.

129
00:08:25,040 --> 00:08:28,760
With the MAD, we are measuring
how it deviates from the media.

130
00:08:29,960 --> 00:08:33,620
And this can help us identify outliers
like individual data points that

131
00:08:33,710 --> 00:08:35,540
significantly differ from the rest.

132
00:08:36,785 --> 00:08:41,465
For example, if the average response time
is a hundred milliseconds and we see a

133
00:08:41,465 --> 00:08:46,385
certain spike of 500, we know this is an
outlier because this will significantly

134
00:08:46,385 --> 00:08:48,855
deviate from the average or median.

135
00:08:51,675 --> 00:08:58,195
We can also use clustering techniques
so we can look at groups of data points

136
00:08:58,465 --> 00:08:59,575
and we can cluster them together.

137
00:09:00,235 --> 00:09:01,885
And see what doesn't fit in the cluster.

138
00:09:01,885 --> 00:09:03,145
And then, that becomes a layer.

139
00:09:04,955 --> 00:09:05,765
It's like finding an

140
00:09:07,805 --> 00:09:13,505
so an example would be if CP utilization
falls outside a range we are seeing

141
00:09:13,505 --> 00:09:18,335
consistently some utilization, and then
suddenly there's a different utilization

142
00:09:18,335 --> 00:09:22,925
number, then, it falls outside the cluster
and we know, we should focus on that.

143
00:09:23,475 --> 00:09:28,155
We also have different AI ops
approaches that we can use.

144
00:09:29,055 --> 00:09:34,605
So these are different ways in which AI
can be incorporated in our SRE practices.

145
00:09:34,605 --> 00:09:39,915
For example, we can have the overlay
model, which is pretty much about

146
00:09:40,665 --> 00:09:44,505
taking our AI capabilities and running
them alongside our existing tools.

147
00:09:45,765 --> 00:09:50,805
So our current monitoring systems and
alerting systems stay in place, but we

148
00:09:50,805 --> 00:09:55,875
add an AI layer that gives us additional
sort of signals, additional insights

149
00:09:55,875 --> 00:09:58,245
that we can use to make decisions.

150
00:10:00,375 --> 00:10:06,165
This is a good on ramp to AI and SRE
because it doesn't require replacing

151
00:10:06,165 --> 00:10:08,775
a lot of existing infrastructure
and we can go step by step.

152
00:10:11,185 --> 00:10:13,825
The other approach is
with embedded models.

153
00:10:14,665 --> 00:10:20,605
So think of this as, a set of
monitoring platforms that can have AI

154
00:10:20,605 --> 00:10:22,435
capabilities already built into them.

155
00:10:23,455 --> 00:10:29,875
So many modern platforms now offer AI
driven anomaly detection or predictive

156
00:10:29,880 --> 00:10:31,460
analytics, or automated learning.

157
00:10:33,205 --> 00:10:38,815
So these help us create,
streamlined workflows.

158
00:10:39,325 --> 00:10:41,185
And give some more integrated experience.

159
00:10:43,435 --> 00:10:49,235
And then one very crucial aspect of how AI
works in SREs with automated remediation.

160
00:10:50,465 --> 00:10:54,635
So I can go beyond just
detecting problems and can also

161
00:10:54,635 --> 00:10:56,465
take corrective action, right?

162
00:10:56,735 --> 00:11:00,630
So this could involve things like
restarting services, scaling resources

163
00:11:01,505 --> 00:11:03,275
triggering other automated workflows.

164
00:11:05,855 --> 00:11:06,275
Alright.

165
00:11:07,145 --> 00:11:10,485
Now what are some of the challenges
that we, face implementing?

166
00:11:10,485 --> 00:11:14,175
This one is, of course, with data
quality and data requirements.

167
00:11:14,225 --> 00:11:19,695
For implementing any of these approaches,
we need access to, a lot of high quality

168
00:11:19,695 --> 00:11:22,585
data which may or may not be available.

169
00:11:23,735 --> 00:11:27,535
AI is very data intensive.

170
00:11:27,745 --> 00:11:28,955
Set of approaches.

171
00:11:29,855 --> 00:11:31,925
If data's incomplete,
inaccurate, or noisy.

172
00:11:32,195 --> 00:11:33,455
AI performance suffers.

173
00:11:34,565 --> 00:11:39,195
So we need to invest in, robust
data collection and cleaning

174
00:11:39,195 --> 00:11:40,625
and management practices.

175
00:11:42,245 --> 00:11:45,425
Another challenge we run into
implementing this is with just

176
00:11:45,455 --> 00:11:47,995
the training and tuning of yeah.

177
00:11:48,205 --> 00:11:49,555
So this is an ongoing thing.

178
00:11:50,915 --> 00:11:54,305
We can put systems in place, but we
also need to invest in making sure that.

179
00:11:54,915 --> 00:11:59,395
These systems continuously
trained with data as it, evolves.

180
00:12:01,405 --> 00:12:07,005
The data that a system sees evolves
over time and, so our systems also

181
00:12:07,005 --> 00:12:09,285
need to be aware of the new normal.

182
00:12:10,285 --> 00:12:13,885
Alright, let's look at normal
detection in a little more detail.

183
00:12:15,670 --> 00:12:21,530
So our detection at its core
is just identifying deviation

184
00:12:22,040 --> 00:12:23,900
from the expected, right?

185
00:12:24,920 --> 00:12:29,330
So we first need to identify what
expected behavior is, and we can do

186
00:12:29,330 --> 00:12:35,330
that by learning from strong data so
we understand what normal looks like

187
00:12:36,270 --> 00:12:39,160
so we can, spot what is not normal.

188
00:12:41,260 --> 00:12:42,820
Here are some types of anomalies.

189
00:12:42,820 --> 00:12:48,340
For example, a point anomaly is a single
data point That air is an outlier.

190
00:12:48,920 --> 00:12:49,940
It stands on its own.

191
00:12:50,300 --> 00:12:54,410
So for example, a certain spike
in CPU utilization on one server

192
00:12:54,530 --> 00:12:55,880
is an example of a point anomaly.

193
00:12:57,110 --> 00:13:03,810
We could also have contextual anomalies
that are unusual in a given context.

194
00:13:04,170 --> 00:13:06,570
So some system behavior
might be normal in one.

195
00:13:07,765 --> 00:13:11,695
And become anomalous in a
different situation, right?

196
00:13:12,565 --> 00:13:19,235
So for example, if we see various spikes
in our system utilization during a normal

197
00:13:19,235 --> 00:13:20,795
workday, that might not be unusual.

198
00:13:20,795 --> 00:13:23,565
But if there is a spike in CP
activity, for example, at 3:00

199
00:13:23,565 --> 00:13:25,845
AM then that might be abnormal.

200
00:13:25,905 --> 00:13:28,515
And, that's an example
of a contextual anomaly.

201
00:13:30,465 --> 00:13:37,595
Another sort of set of anomalies are
just behaviors that we see only when we

202
00:13:37,595 --> 00:13:39,395
collectively look at various data points.

203
00:13:40,505 --> 00:13:44,355
So for example a graduate increase
in latency across multiple

204
00:13:44,355 --> 00:13:45,975
services over time, right?

205
00:13:45,975 --> 00:13:51,205
So any one of those points by themselves
may not be an example of anomaly,

206
00:13:51,205 --> 00:13:54,730
but taking together they give us a
signal that something might be wrong.

207
00:13:56,440 --> 00:13:58,990
Okay, what are some of the ways
in which we can detect them?

208
00:13:59,770 --> 00:14:01,000
There are statistical methods.

209
00:14:02,070 --> 00:14:05,940
So these involve just analyzing data
distributions and identifying outliers.

210
00:14:07,560 --> 00:14:12,350
And, I can name suggest we apply
this by using various statistical

211
00:14:12,780 --> 00:14:18,080
techniques like moving averages,
exponential smoothing to identify what

212
00:14:18,080 --> 00:14:19,930
an outlier is in a given set of data.

213
00:14:21,385 --> 00:14:23,515
We can also use machine learning methods.

214
00:14:24,055 --> 00:14:25,825
So we talked about clustering just now.

215
00:14:25,825 --> 00:14:32,485
So clustering time series forecasting,
or other ways in which we, use machine

216
00:14:32,485 --> 00:14:38,395
learning methodologies to cluster
information or to what, to create a sense

217
00:14:38,395 --> 00:14:43,135
of what normal is so that we can identify
when something is outside that normal.

218
00:14:45,670 --> 00:14:49,240
So one of the challenges we run into
with knowledge detection, so whereas

219
00:14:49,240 --> 00:14:54,760
of course seasonality many businesses
are, have season has seasonality

220
00:14:54,760 --> 00:14:56,990
in their sort of system usage.

221
00:14:58,070 --> 00:15:02,190
So that needs to be factored in
what constitutes normal changes

222
00:15:02,250 --> 00:15:04,350
at different times of the year
or different times of the day.

223
00:15:06,070 --> 00:15:09,280
Another problem is with,
data that might be noisy.

224
00:15:10,270 --> 00:15:14,660
So we might need some pre-processing to
filter out the noise so we can accurately

225
00:15:14,660 --> 00:15:16,650
identify, what a true anomaly is.

226
00:15:18,600 --> 00:15:25,420
We can also have situations where what
constitutes normal evolves over time, so

227
00:15:25,420 --> 00:15:28,780
as systems grow, as usage patterns change.

228
00:15:29,560 --> 00:15:31,990
What might be normal today
might not be normal tomorrow.

229
00:15:32,890 --> 00:15:37,890
So we need to invest in ongoing
sort of training and tuning of our

230
00:15:38,280 --> 00:15:43,140
algorithms to make sure that they are
current in terms of what is normal.

231
00:15:44,940 --> 00:15:48,930
And there might be, cultural effort
involved in making sure that our

232
00:15:48,930 --> 00:15:53,790
data is labeled correctly so that,
AI systems can use them effectively.

233
00:15:55,225 --> 00:16:00,265
But despite all these challenges,
anomaly detection is crucial and

234
00:16:00,265 --> 00:16:03,495
is a very important technique
for catching issues early.

235
00:16:05,535 --> 00:16:08,855
Alright let's talk about
a few case studies.

236
00:16:09,365 --> 00:16:12,135
So these are all publicly
available and it'll be, if you're

237
00:16:12,135 --> 00:16:13,115
interested to read about them.

238
00:16:14,360 --> 00:16:20,980
So Netflix and how Netflix used anoma
detection to prevent streaming outages.

239
00:16:21,640 --> 00:16:24,310
So Netflix, as we know, is
the massive streaming service.

240
00:16:24,920 --> 00:16:27,320
They have probably the largest
streaming infrastructure, one

241
00:16:27,320 --> 00:16:28,160
of the largest in the world.

242
00:16:29,205 --> 00:16:34,330
And they rely heavily on normal
detection to, identify when something

243
00:16:34,330 --> 00:16:36,350
is going wrong with their system.

244
00:16:36,350 --> 00:16:38,300
So they continuously
monitor their boss network.

245
00:16:39,255 --> 00:16:45,315
They look for anomalies or any unusual
patterns, and by identifying these

246
00:16:45,315 --> 00:16:50,405
things early, they can take proactive
steps and, have been by and large, good

247
00:16:50,405 --> 00:16:55,775
at preventing streaming outages and
ensuring smooth viewing experience.

248
00:16:56,375 --> 00:17:01,595
So wherever they have a new launch of a
Blockbuster movie, they're able to predict

249
00:17:01,655 --> 00:17:05,015
ahead of time the kind of spikes that
they might see, and they plan for it.

250
00:17:06,245 --> 00:17:09,815
So the other system we should
talk about is LinkedIn.

251
00:17:10,685 --> 00:17:14,405
So LinkedIn is of course, the huge
professional network that we perhaps

252
00:17:14,405 --> 00:17:19,175
all of us use, and they face the
challenge of quickly resolving incidents.

253
00:17:20,435 --> 00:17:25,925
So they have an AI powered correlation
system that's able to analyze a lot

254
00:17:25,925 --> 00:17:30,415
of different data sources and identify
difference I'm sorry, identify

255
00:17:30,475 --> 00:17:31,945
relationships between these events.

256
00:17:33,580 --> 00:17:36,880
This helps to pinpoint whose
causes of problems much faster than

257
00:17:36,980 --> 00:17:38,450
manual investigation would have.

258
00:17:39,050 --> 00:17:44,180
So as a result, they significantly
reduced their F TT R and minimized

259
00:17:44,180 --> 00:17:45,770
impact of incidents on their users.

260
00:17:46,370 --> 00:17:51,590
So this is another showcase of
how AI can streamline incident

261
00:17:51,590 --> 00:17:54,140
response and improve efficiency.

262
00:17:56,600 --> 00:18:03,020
Uber, the ride sharing service many of us
use is another case study in how AI was

263
00:18:03,350 --> 00:18:05,420
used successfully in the realm of SRE.

264
00:18:06,680 --> 00:18:10,790
Uber faces a lot of fluctuating demand,
and they use machine learning to predict

265
00:18:10,850 --> 00:18:13,400
future traffic patterns and rider demands.

266
00:18:14,480 --> 00:18:18,540
So with this they're able to automatically
scale that infrastructure up and down as.

267
00:18:20,160 --> 00:18:23,890
By predicting demand they ensure they
have enough resources to handle peak

268
00:18:23,890 --> 00:18:28,450
times, and, they don't waste money over
provisioning during off peak times.

269
00:18:29,620 --> 00:18:34,170
This demonstrates how, AI can be used
for proactive capacity planning for

270
00:18:34,170 --> 00:18:38,590
resource optimization, and, and thereby
saving cost and into the performance.

271
00:18:41,470 --> 00:18:46,010
Okay, let's look at some of
the, most famous and popular

272
00:18:46,010 --> 00:18:48,260
tools in this space, right?

273
00:18:48,410 --> 00:18:51,890
And we have open source as well as
commercial tools and platforms that can

274
00:18:51,890 --> 00:18:54,140
be used depending upon our situation.

275
00:18:55,310 --> 00:18:58,940
So some of the really popular
ones, Prometheus it's a metric

276
00:18:58,940 --> 00:19:01,070
collection and a storage system.

277
00:19:02,870 --> 00:19:04,370
It's excellent for gathering data.

278
00:19:05,075 --> 00:19:07,235
Which is a fourth cliche
for anomaly platform.

279
00:19:08,825 --> 00:19:12,505
Grafana is a visualization
and and dashboarding system.

280
00:19:13,255 --> 00:19:16,585
Using that, we can visualize some
of the metrics that we are capturing

281
00:19:17,185 --> 00:19:19,525
and we can, visualize anomalies.

282
00:19:20,605 --> 00:19:20,965
Excuse me.

283
00:19:22,645 --> 00:19:27,835
There is the elk stack of
tools, the elastic search log.

284
00:19:30,115 --> 00:19:33,985
So with this stack, we can do
log management and analysis.

285
00:19:34,585 --> 00:19:37,225
We can collect and process
and search our log.

286
00:19:38,245 --> 00:19:44,835
And and this is very useful for root
cause analysis, this profit from

287
00:19:44,835 --> 00:19:49,735
Facebook this is a useful tool for,
predicting future trends and patterns.

288
00:19:50,365 --> 00:19:55,045
So using this tool, we can do things
like capacity planning and monitoring.

289
00:19:57,040 --> 00:20:01,260
Then there are of course tools like
psychic intensive flow by watch.

290
00:20:01,450 --> 00:20:08,400
So these tools can be used if we want to
build, our AI ML systems from scratch and

291
00:20:08,400 --> 00:20:09,990
apply some of these algorithms ourselves.

292
00:20:11,910 --> 00:20:16,650
Then there are commercial platform
platforms like Datadog from

293
00:20:16,950 --> 00:20:18,690
anomaly detection and forecasting.

294
00:20:19,815 --> 00:20:24,885
New Relic which has, automated
incident intelligence built into it.

295
00:20:26,265 --> 00:20:32,855
There's Dynatrace for automated monitoring
and Splunk which is log capturing System,

296
00:20:32,855 --> 00:20:36,815
which has very advanced log analysis and
anomaly detection systems built into it.

297
00:20:38,675 --> 00:20:43,905
And depending upon whether we
are on, the Amazon ecosystem

298
00:20:43,905 --> 00:20:45,465
or the Google Cloud ecosystem.

299
00:20:45,990 --> 00:20:51,120
There are AI tools built into these
cloud providers that help us to anomaly

300
00:20:51,120 --> 00:20:55,800
detection, forecasting, log answers
in a variety of these practices.

301
00:20:57,960 --> 00:21:00,030
Alright, so where are we headed?

302
00:21:01,705 --> 00:21:08,020
So we know that there's going to be
increased automation especially in

303
00:21:08,020 --> 00:21:09,790
the field of incident response and.

304
00:21:11,560 --> 00:21:15,770
AI will not just detect problems,
but will also trigger and execute

305
00:21:15,770 --> 00:21:18,480
corrective actions corrective workflows.

306
00:21:19,530 --> 00:21:23,450
And, it can do a variety of things
before human intervention is needed.

307
00:21:23,690 --> 00:21:27,690
Sometimes human intervention is not
even needed and it can speed up recovery

308
00:21:27,690 --> 00:21:33,180
times we know that there, there are gonna
be more and more self-healing systems.

309
00:21:34,050 --> 00:21:37,800
Systems that can detect, diagnose,
and fix problems all on their own.

310
00:21:37,890 --> 00:21:43,230
So without, or with minimal human
intervention self-healing system

311
00:21:43,230 --> 00:21:46,810
is becoming more and more of a
buzzword and AI and ML powering that,

312
00:21:49,490 --> 00:21:53,730
we're gonna have gradually more and
more sophisticated models that have

313
00:21:53,730 --> 00:21:57,570
the ability to detect more complex.

314
00:21:57,895 --> 00:21:58,675
Problems.

315
00:21:59,545 --> 00:22:05,065
And then using deep learning, they
are able to analyze a vast variety

316
00:22:05,115 --> 00:22:08,265
of data sets, sorry, vast of data.

317
00:22:08,835 --> 00:22:11,625
Identify problems that otherwise
may not have been caught.

318
00:22:14,245 --> 00:22:18,005
Another trend is the
rise of explainable ai.

319
00:22:18,055 --> 00:22:21,485
We don't just want AI to, make
a decision or make a prediction.

320
00:22:21,935 --> 00:22:25,065
We also want to understand,
how it got to that point, why

321
00:22:25,065 --> 00:22:26,505
it made a certain decision.

322
00:22:27,435 --> 00:22:33,575
And explainable ai, if I is the,
is the approach to look inside

323
00:22:33,575 --> 00:22:38,155
the black box to see how AI is
working so that, we are able to have

324
00:22:38,155 --> 00:22:40,405
greater confidence in descriptions.

325
00:22:42,610 --> 00:22:46,420
And of course we will have AI
driven observability as well.

326
00:22:47,950 --> 00:22:51,210
So AI will help us gain a
more holistic comprehensive

327
00:22:51,210 --> 00:22:52,590
understanding of our systems.

328
00:22:53,070 --> 00:22:56,760
AI can help us correlate data
from various sources, identify

329
00:22:57,270 --> 00:23:02,100
dependencies, and provide a deeper
level insight into our system behavior.

330
00:23:05,100 --> 00:23:07,440
Alright, so how can you get started?

331
00:23:08,760 --> 00:23:13,350
Step one, focus on specific
high value use cases.

332
00:23:14,010 --> 00:23:18,610
So rather than try to, boil the
ocean and fixing everything on day

333
00:23:18,610 --> 00:23:25,660
one we should identify specific high
value use cases and leverage AI to

334
00:23:25,870 --> 00:23:27,670
make an impact with those use cases.

335
00:23:28,360 --> 00:23:30,580
Think about what the biggest
pain points of our systems are.

336
00:23:31,235 --> 00:23:33,425
Where are we spending the most
amount of money and effort?

337
00:23:33,965 --> 00:23:38,665
And if we start with those areas so for
example if the problem that you're facing

338
00:23:38,665 --> 00:23:45,565
is alert fatigue, then you focus on that
first, then building the right foundation.

339
00:23:46,105 --> 00:23:51,055
So any AI system is only as
good as the data we use it.

340
00:23:51,595 --> 00:23:53,035
We train it on, right?

341
00:23:53,515 --> 00:23:55,315
So they need a solid foundation of data.

342
00:23:56,525 --> 00:23:59,255
This means that, we
need the right metrics.

343
00:23:59,375 --> 00:24:01,055
We have to store them properly.

344
00:24:01,555 --> 00:24:05,485
They should be an available for
analysis, and we have to invest in good

345
00:24:05,485 --> 00:24:06,955
monitoring tools and data pipelines.

346
00:24:08,015 --> 00:24:12,575
Clean, comprehensive, relevant
data is essentially we also need

347
00:24:12,575 --> 00:24:14,045
to invest in skill development.

348
00:24:14,495 --> 00:24:18,890
So implementing AI isn't
just, for tool at it.

349
00:24:20,015 --> 00:24:24,705
But we must understand and
build our own skill and identify

350
00:24:24,705 --> 00:24:27,315
what tool or what technique is
applicable in a given situation.

351
00:24:28,215 --> 00:24:29,715
And this is also an ongoing thing.

352
00:24:30,625 --> 00:24:37,085
We, we train, we hire accordingly
and we keep ourselves updated,

353
00:24:39,815 --> 00:24:40,145
right?

354
00:24:40,145 --> 00:24:45,935
So in essence, we start small, we create
a roadmap, and we follow the roadmap.

355
00:24:46,935 --> 00:24:55,675
All right, so to recap, we saw, how AI,
in particular, how and how detection can

356
00:24:55,675 --> 00:24:57,865
significantly transform SI e practices.

357
00:24:58,405 --> 00:25:01,795
By leveraging these techniques,
we can achieve several benefits.

358
00:25:02,185 --> 00:25:07,285
We can reduce MTTR, we can identify
and address our issues quickly.

359
00:25:07,705 --> 00:25:11,305
We can improve liability and we
can review increase our efficiency.

360
00:25:13,735 --> 00:25:19,225
So I encourage all of us to explore
the basic possibilities that AI

361
00:25:19,255 --> 00:25:23,785
and noal detection in particular
give to our SRE workflows.

362
00:25:24,955 --> 00:25:28,555
This may seem intimidating at first, but
the potential for reward is very high.

363
00:25:30,595 --> 00:25:35,395
And of course, we should start
small focus on specific use cases.

364
00:25:36,445 --> 00:25:41,165
Have a roadmap and then follow the roadmap
and then it's, this is going to be an

365
00:25:41,525 --> 00:25:47,015
IT process, so we're gonna try something
and then we are gonna improve it.

366
00:25:47,555 --> 00:25:48,905
And let's repeat.

367
00:25:51,815 --> 00:25:52,265
Alright.

368
00:25:53,255 --> 00:25:54,425
Thank you so much for joining me today.

