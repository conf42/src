1
00:00:00,210 --> 00:00:02,979
Hey, I'm Femi Arushel.

2
00:00:03,420 --> 00:00:06,510
I've been deeply passionate
about web scraping for a year.

3
00:00:07,050 --> 00:00:10,899
My enthusiasm led me to explore
the fascinating world of

4
00:00:10,930 --> 00:00:12,759
proxy and anti bot systems.

5
00:00:13,259 --> 00:00:15,579
I'm also the creator of Scrapoxy.

6
00:00:16,099 --> 00:00:19,239
Scrapoxy is a free and
open source proxy manager.

7
00:00:19,955 --> 00:00:22,884
It allows you to manage and
route traffic through cloud

8
00:00:22,884 --> 00:00:24,904
providers and proxy services.

9
00:00:25,405 --> 00:00:32,064
It supports major cloud providers
such as AWS, Azure, or GCP, and also

10
00:00:32,064 --> 00:00:37,305
it supports proxy vendors like Bryta,
Railbite, IPRail, and many others.

11
00:00:37,805 --> 00:00:44,735
Since I started the version
4 in February 2024, over 650

12
00:00:44,735 --> 00:00:46,185
users have installed Skopoxy.

13
00:00:46,685 --> 00:00:52,295
They exchange one petabyte of
data, reserve 55 billion requests,

14
00:00:52,355 --> 00:00:54,545
and use 6 million proxies.

15
00:00:55,165 --> 00:00:58,225
And this proxy is written in Node.

16
00:00:58,255 --> 00:01:01,505
js, so fully JavaScript plus Angular.

17
00:01:02,425 --> 00:01:06,715
But before we dive into our discussion,
I'd like to share with you a little story.

18
00:01:07,585 --> 00:01:08,705
Enter Isabella.

19
00:01:09,205 --> 00:01:13,015
Isabella is a brilliant
student in IST school.

20
00:01:13,665 --> 00:01:16,795
She has a lot of energy
and a thirst for traveling.

21
00:01:17,295 --> 00:01:22,545
Every year, she embarks on a one month
backpacking journey to a random country.

22
00:01:22,875 --> 00:01:25,325
But here is the twist.

23
00:01:25,825 --> 00:01:30,995
This level of preparation
consumes an entire year for

24
00:01:31,015 --> 00:01:33,615
just one month of traveling.

25
00:01:34,425 --> 00:01:38,325
Isabella couldn't help but notice
there is a gap in the market.

26
00:01:39,135 --> 00:01:44,794
Why wasn't there such a tool in
a digital era pumped with AI?

27
00:01:44,795 --> 00:01:47,395
This could be her ticket
to a successful business.

28
00:01:47,895 --> 00:01:50,595
She realized she needed
vast amounts of data.

29
00:01:51,095 --> 00:01:55,235
But, this vast amount of data will
be used to train a large language

30
00:01:55,255 --> 00:01:57,785
model to curate her ultimate trip.

31
00:01:58,285 --> 00:02:01,935
And Isabella is very careful
in her approach to business.

32
00:02:02,920 --> 00:02:09,290
Before she starts scraping data, she makes
sure to consider all the legal aspects.

33
00:02:09,950 --> 00:02:14,160
She knows it's important not to
overwhelm the website by making

34
00:02:14,190 --> 00:02:15,720
too many requests too quickly.

35
00:02:16,630 --> 00:02:18,730
She also respects privacy.

36
00:02:19,470 --> 00:02:24,720
She only collects information that is
already public, like reviews and doesn't

37
00:02:24,730 --> 00:02:28,544
take any personal details, like names.

38
00:02:29,035 --> 00:02:33,785
She also doesn't sign any
term and condition easels.

39
00:02:34,205 --> 00:02:36,815
So she is free from every contract.

40
00:02:37,315 --> 00:02:41,415
So now that everything is clear,
she is ready to collect the data.

41
00:02:41,845 --> 00:02:46,765
So let me introduce you the website
she chose to script trickyreview.

42
00:02:46,795 --> 00:02:49,595
com let's open the website.

43
00:02:50,445 --> 00:02:52,595
So what's Trekkie Review all about?

44
00:02:53,005 --> 00:02:58,045
Trekkie Review is your go to spot
for searching any accommodation

45
00:02:58,065 --> 00:03:00,095
in a city that you want to visit.

46
00:03:00,584 --> 00:03:04,225
You just have to click on search
and you will get accommodations.

47
00:03:04,605 --> 00:03:09,725
Imagine that Isabella is living in
Paris, she just click on search and

48
00:03:09,725 --> 00:03:12,465
she will get 50 accommodations here.

49
00:03:12,965 --> 00:03:17,295
And if she click on one accommodation
She will get the name, the

50
00:03:17,335 --> 00:03:22,515
contact, location, descriptions,
and also she will get the review.

51
00:03:23,465 --> 00:03:26,085
Isabella is interested in reviews.

52
00:03:27,045 --> 00:03:33,665
It is all analyzing this review to extract
the main feeling about the accommodations.

53
00:03:33,935 --> 00:03:40,975
And now with large language models, we can
know if it's a bad or good accommodation.

54
00:03:41,475 --> 00:03:43,655
Also, something very important.

55
00:03:44,315 --> 00:03:46,685
The website is super secure.

56
00:03:47,045 --> 00:03:52,395
I've put different levels of protection on
the website, and Isabella needs to bypass

57
00:03:52,395 --> 00:03:55,035
them one by one during these sessions.

58
00:03:56,014 --> 00:03:59,645
So let's get back to the
home page, on level 1.

59
00:04:00,255 --> 00:04:04,554
I will open the Chrome Inspector to
show you the structure of the website.

60
00:04:05,085 --> 00:04:11,855
If I'm clicking on Preserve Log, on Docs,
to avoid Other documents, reloadings, you

61
00:04:11,855 --> 00:04:14,325
can see we're starting with home page.

62
00:04:14,994 --> 00:04:16,894
So we are on level one.

63
00:04:17,005 --> 00:04:21,664
If I'm looking on accommodations,
I will get all accommodations

64
00:04:21,664 --> 00:04:23,414
on the city of Paris.

65
00:04:23,844 --> 00:04:27,694
And if I'm clicking on one
accommodations, I will get the ID of

66
00:04:27,695 --> 00:04:32,765
the accommodations, and if I'm clicking
on that, on the response, I will

67
00:04:32,794 --> 00:04:35,985
get all information inside the HTML.

68
00:04:36,485 --> 00:04:43,705
And if I'm looking down, I will get
descriptions here, and also the review.

69
00:04:44,194 --> 00:04:46,315
That's what Isabella want to extract.

70
00:04:46,815 --> 00:04:50,974
But clearly Isabella doesn't
want to do everything manually.

71
00:04:51,685 --> 00:04:57,445
She doesn't want to make HTTP requests,
doing retry, managing error, and

72
00:04:57,475 --> 00:05:00,365
also doing the CSS extractions.

73
00:05:00,715 --> 00:05:02,865
She wants a framework
to manage everything.

74
00:05:03,835 --> 00:05:08,584
And to do that, she will use one
of the most used frameworks in

75
00:05:08,584 --> 00:05:10,815
the web scraping industry, Scrapy.

76
00:05:11,315 --> 00:05:12,515
let me show you Scrapy.

77
00:05:13,015 --> 00:05:14,435
what is Scrapy?

78
00:05:15,000 --> 00:05:19,789
It's a Python framework maintained
by a large open source community and

79
00:05:19,800 --> 00:05:25,979
it can handle HTML parsing, retry,
errors, concurrency, everything.

80
00:05:25,979 --> 00:05:28,076
how Scrapy works?

81
00:05:28,076 --> 00:05:30,173
Scrapy runs SPIDERS.

82
00:05:30,173 --> 00:05:38,563
SPIDERS is responsible for all the
logic of collecting data on websites.

83
00:05:38,563 --> 00:05:40,660
Scrapy runs SPIDERS.

84
00:05:40,860 --> 00:05:46,580
So we just gave the Python class a cool
name, here, TrickyReviewSpiders, and

85
00:05:46,580 --> 00:05:51,540
a name, TrickyReviews, and we have two
methods, the startRequest and pass method.

86
00:05:52,040 --> 00:05:55,980
So startRequest method is used
to define the URL to collect.

87
00:05:56,510 --> 00:06:01,050
Also, when we get the response, the
Scrapy engine will call the method.

88
00:06:01,920 --> 00:06:06,670
parse here with the response
and we just need to extract the

89
00:06:06,670 --> 00:06:12,370
data with css selectors, xpath
selectors, and we can start again

90
00:06:13,170 --> 00:06:17,190
url scraping, ask new content, etc.

91
00:06:17,680 --> 00:06:19,840
So let's jump to HealSpiders.

92
00:06:20,340 --> 00:06:23,000
So here is the same spiders, basically.

93
00:06:23,440 --> 00:06:28,570
So what we will do all the pagination
of the website to collect the 50 items,

94
00:06:28,880 --> 00:06:33,430
but instead of going to the homepage
and doing page one, page two, page

95
00:06:33,460 --> 00:06:40,080
three, we will start for every page to
start navigation by the first homepage.

96
00:06:40,405 --> 00:06:45,145
So that's what I'm doing
here on start url level 1.

97
00:06:45,145 --> 00:06:52,895
So I will ask the home page 10 times, but
after we are going to page 1, page 2, etc.

98
00:06:53,045 --> 00:06:55,845
So that's what we are doing here.

99
00:06:55,915 --> 00:06:58,705
We are going to the
page 1 on Paris cities.

100
00:06:59,005 --> 00:07:02,555
And when we get the list of
accommodations, we ask to

101
00:07:02,575 --> 00:07:04,525
collect each accommodations.

102
00:07:05,010 --> 00:07:10,090
And when we get the accommodation's
answers, yes, we extract name,

103
00:07:10,320 --> 00:07:13,610
email, and review with CSS selectors.

104
00:07:14,110 --> 00:07:16,990
what we can do is to run the spiders.

105
00:07:17,400 --> 00:07:18,520
let's run it here.

106
00:07:19,020 --> 00:07:21,800
it's very quick, and
I've got the 50 items.

107
00:07:22,400 --> 00:07:27,940
And the cool stuff of the Python
framework here is on Scrapy you

108
00:07:27,940 --> 00:07:30,060
can extract in a structured ways.

109
00:07:30,560 --> 00:07:32,380
You've got everything here.

110
00:07:32,880 --> 00:07:34,460
So name, email, reviews.

111
00:07:34,910 --> 00:07:40,610
Now let's jump to the spiders
and move to the level two.

112
00:07:41,110 --> 00:07:41,420
Here.

113
00:07:42,150 --> 00:07:46,665
So now if I'm starting again
the spider, I've got an another

114
00:07:46,665 --> 00:07:48,585
error, which is unknown browsers.

115
00:07:49,085 --> 00:07:54,475
It's a very common error and this
error just happen when a client

116
00:07:54,475 --> 00:07:59,275
connect to a server, so a browser
or a spider connect to web servers.

117
00:07:59,815 --> 00:08:07,435
The client send a lot of information with
HT TP editor and we are saying who we are.

118
00:08:07,465 --> 00:08:10,495
We are saying we are
scrappy and we are also.

119
00:08:11,190 --> 00:08:15,910
Sending the versions, and the web
server says, Hey, wait, it's a scraper.

120
00:08:15,970 --> 00:08:19,230
I don't want you to collect
data from my website.

121
00:08:19,270 --> 00:08:20,450
the website blocked me.

122
00:08:20,930 --> 00:08:22,510
I need to change this value.

123
00:08:23,110 --> 00:08:25,230
But, which value should I use?

124
00:08:25,980 --> 00:08:27,940
I will use the same as Chrome.

125
00:08:28,060 --> 00:08:29,430
let's get back here.

126
00:08:29,930 --> 00:08:36,100
on the headers, I've got the
response and request headers.

127
00:08:36,455 --> 00:08:40,485
And at the end, I got the user
agent, so I will use the same.

128
00:08:40,985 --> 00:08:46,895
I can do that here, add the user
agent, and Copilot will complete me.

129
00:08:47,135 --> 00:08:48,145
Oh, thanks Copilot!

130
00:08:48,595 --> 00:08:50,305
You say that we are on Windows?

131
00:08:50,445 --> 00:08:50,845
Why not?

132
00:08:50,845 --> 00:08:53,735
Let's run that again.

133
00:08:54,235 --> 00:08:59,855
Now, I just bypassed the user agent
issue, but I've got another one, which

134
00:08:59,855 --> 00:09:03,205
is, the sexiest UI header is missing.

135
00:09:04,095 --> 00:09:06,495
So I need to understand what is it.

136
00:09:06,995 --> 00:09:09,705
Let's get back to the request headers.

137
00:09:10,135 --> 00:09:15,075
As you see, I've got the user agents,
but I've got others headers, additional

138
00:09:15,185 --> 00:09:17,515
headers, which are confirming the request.

139
00:09:18,015 --> 00:09:21,495
It's additional security to confirm that.

140
00:09:21,495 --> 00:09:23,555
And it's requested by the website.

141
00:09:23,565 --> 00:09:27,335
So if I'm missing these headers,
clearly the website will block me.

142
00:09:27,725 --> 00:09:29,575
So I need to add this one.

143
00:09:30,075 --> 00:09:32,075
So let's do that again.

144
00:09:32,575 --> 00:09:40,875
So I'm going to header
and I'm saying unamobiles.

145
00:09:40,875 --> 00:09:42,710
com and platform.

146
00:09:42,710 --> 00:09:48,140
So we are saying that we are on windows
because the user region is on windows.

147
00:09:48,330 --> 00:09:51,600
Okay, so let's run that again.

148
00:09:52,100 --> 00:09:56,800
So now this time I just bypass
protections and I've got my 50 items.

149
00:09:57,140 --> 00:09:57,750
That's perfect.

150
00:09:58,250 --> 00:10:03,260
So now let's say that image
Isabella have more serious troubles.

151
00:10:04,180 --> 00:10:06,610
So let's jump to level three.

152
00:10:07,110 --> 00:10:13,810
As you can see, I quickly collect
few items, but I'm directly blocked

153
00:10:13,840 --> 00:10:20,580
with HTTP error 429, which is too
many requests, and too many requests.

154
00:10:21,210 --> 00:10:27,310
So we are sending too much information,
too much requests from my laptops.

155
00:10:28,270 --> 00:10:31,870
So I need to have multiple IP address.

156
00:10:32,020 --> 00:10:38,140
It's the same if you're using a server or
request will come from this IP address.

157
00:10:38,440 --> 00:10:42,010
So I need to introduce one
new concept, which is a proxy.

158
00:10:42,510 --> 00:10:44,610
So what is a proxy?

159
00:10:45,600 --> 00:10:48,630
So a proxy is a system
running on internet.

160
00:10:49,080 --> 00:10:52,830
It relays a request, were
servers and the server retrieve,

161
00:10:53,180 --> 00:10:55,550
this request from this proxy.

162
00:10:56,255 --> 00:11:00,425
So instead of seeing one IP address
sending millions of requests,

163
00:11:00,425 --> 00:11:04,435
the server will see a lot of IP
addresses sending a few requests.

164
00:11:04,705 --> 00:11:09,215
So with this technique you can bypass
a lot of rate limit protections.

165
00:11:10,145 --> 00:11:12,865
But of course there is
different types of proxy.

166
00:11:13,085 --> 00:11:15,495
So first type is data center proxy.

167
00:11:16,395 --> 00:11:17,885
So what's data center proxy?

168
00:11:18,885 --> 00:11:22,165
It's proxy running on AWS, Azure or GCP.

169
00:11:23,160 --> 00:11:28,910
It is the first type, first serious type
of proxies that you can find on internet.

170
00:11:29,090 --> 00:11:31,080
It's fast, cheap and reliable.

171
00:11:31,580 --> 00:11:35,320
However, they can be easily
identified by Antibot solution.

172
00:11:35,710 --> 00:11:36,850
Let me explain you.

173
00:11:37,830 --> 00:11:43,410
The IP of the proxy belong to an IP
ranch, and this IP ranch is associated

174
00:11:43,410 --> 00:11:48,180
with an autonomous system number a
sales, and the name of the autonomous

175
00:11:48,180 --> 00:11:54,180
system number can be Amazon zero two
can be Microsoft or Google, whatever.

176
00:11:54,750 --> 00:12:02,580
So with this kind of associations, so
we can have this link with IP database.

177
00:12:03,080 --> 00:12:08,460
So it's used by Antibot system to
detect the autonomous system numbers.

178
00:12:09,050 --> 00:12:14,420
And clearly they can block traffic
with this type of IP address.

179
00:12:14,960 --> 00:12:16,840
But there is a trick to get around it.

180
00:12:17,330 --> 00:12:22,330
And this trick is known as ISP proxy,
Internet Service Provider proxy.

181
00:12:22,830 --> 00:12:26,270
So let's talk a little
bit about ISP proxy.

182
00:12:26,770 --> 00:12:31,080
So these proxies are set up in
datacenters, but they don't use

183
00:12:31,170 --> 00:12:33,290
IP addresses from the datacenters.

184
00:12:33,370 --> 00:12:37,583
Instead, they rent IP addresses
from clean autonomous system numbers

185
00:12:37,583 --> 00:12:43,050
like a mobile carrier, internet
box provider like AT& T or Verizon.

186
00:12:43,510 --> 00:12:48,110
And they get a bunch of IP addresses
for a lot of money, and the proxy

187
00:12:48,110 --> 00:12:50,070
clearly will use one of those.

188
00:12:50,570 --> 00:12:55,480
this means, when you're using the proxy,
your activities get mixed with all the

189
00:12:55,490 --> 00:12:57,500
mobile IP addresses, keeping you hidden.

190
00:12:58,000 --> 00:13:00,850
And there is a last type of
proxy, residential proxy.

191
00:13:00,850 --> 00:13:07,420
The IP comes from a real device, which
can be a laptop or a mobile phone.

192
00:13:07,860 --> 00:13:08,920
how does it works?

193
00:13:09,420 --> 00:13:13,220
when the developers want to earn
money from his applications,

194
00:13:13,350 --> 00:13:15,030
he has three solutions.

195
00:13:15,660 --> 00:13:20,190
First solution is to sell
subscriptions, like a monthly or annual

196
00:13:20,200 --> 00:13:22,810
subscription which unlocks features.

197
00:13:23,440 --> 00:13:28,551
Second, he can add advertising,
like having an ad to the bottom

198
00:13:28,551 --> 00:13:32,795
of the application or a video to
watch before unlocking features.

199
00:13:33,295 --> 00:13:37,815
And the last solution is to share
the bandwidth of the device.

200
00:13:38,645 --> 00:13:41,405
Of course, with the user agreements.

201
00:13:42,245 --> 00:13:45,195
And that's where REST
and July become friends.

202
00:13:45,765 --> 00:13:50,265
So this type of proxy is very
powerful because the IP address can

203
00:13:50,265 --> 00:13:52,555
be the same IP from the real users.

204
00:13:52,835 --> 00:13:55,915
And there are millions
of endpoints available.

205
00:13:56,415 --> 00:14:01,165
So now we will use Proxy, the super proxy
aggregator to manage our proxy strategy.

206
00:14:01,935 --> 00:14:04,230
So let me show you how we can do that.

207
00:14:04,730 --> 00:14:11,300
So what we can do is to start
Dockerfiles with Scopoxy, and in a

208
00:14:11,300 --> 00:14:14,110
second I will get Scopoxy up and ready.

209
00:14:15,090 --> 00:14:17,990
So I just need to log to Scopoxy now.

210
00:14:18,490 --> 00:14:25,310
We go on localhost, and I'm already
logged, so I've got one project ready.

211
00:14:26,210 --> 00:14:30,690
So you can see that I've
got, here's one project.

212
00:14:30,720 --> 00:14:31,660
There is no project.

213
00:14:32,040 --> 00:14:33,440
I need to add connectors.

214
00:14:34,035 --> 00:14:38,575
Squareproxy offers you a lot of
connectors, data center connectors,

215
00:14:38,975 --> 00:14:44,535
proxy vendor connectors with ISP
proxy, a residential proxy, also

216
00:14:44,565 --> 00:14:49,705
proxy lists or free proxy on internet,
and you can also use even hardwares.

217
00:14:50,205 --> 00:14:53,355
So let's imagine that you
want to use AWS proxy.

218
00:14:53,485 --> 00:14:57,925
You just click on create, and you
add your access key and secret key.

219
00:14:58,395 --> 00:15:02,275
Just get on ScrapOxy documentation
to get all this information.

220
00:15:02,515 --> 00:15:04,375
In two minutes you are up and ready.

221
00:15:04,875 --> 00:15:09,245
So I already had my credential,
AWS credentials, and we

222
00:15:09,245 --> 00:15:10,935
will start, a connector.

223
00:15:11,095 --> 00:15:13,365
It's very quick, so you will see.

224
00:15:13,365 --> 00:15:14,525
So I just click here.

225
00:15:15,115 --> 00:15:17,785
So let's jump also on AWS consoles.

226
00:15:18,185 --> 00:15:22,225
You can see that I don't have any
instance, but if I'm waiting two

227
00:15:22,225 --> 00:15:27,805
seconds here, refreshing, I've got
already ten instances up and ready.

228
00:15:28,515 --> 00:15:34,645
So Scrapbox, in order to The API of
AWS to start all these instances.

229
00:15:35,335 --> 00:15:39,525
And if I'm going back to Scrapbox
here on the proxy list, you

230
00:15:39,525 --> 00:15:41,205
can see all the proxy here.

231
00:15:41,765 --> 00:15:45,555
I've got a lot of information
on my 10 instances.

232
00:15:45,585 --> 00:15:49,815
I can know the traffic sent,
received, the number of requests,

233
00:15:50,095 --> 00:15:51,625
and also the IP address.

234
00:15:52,615 --> 00:15:57,385
And I've got also very cool
information which is the geolocations.

235
00:15:57,745 --> 00:16:01,165
So all IP are based in the
datacenter of Dublin here.

236
00:16:01,665 --> 00:16:03,895
I can confirm that with a coverage map.

237
00:16:04,405 --> 00:16:04,795
Here's.

238
00:16:05,235 --> 00:16:08,055
So everything is in Ireland.

239
00:16:08,255 --> 00:16:08,885
That's perfect.

240
00:16:09,765 --> 00:16:12,535
So now we need to plug
ScrapOxy to Scrappy.

241
00:16:13,425 --> 00:16:14,845
So that's very easy.

242
00:16:15,365 --> 00:16:18,695
Scrap Proxy can handle multiple projects.

243
00:16:18,775 --> 00:16:20,865
on each project you can
have different strategies.

244
00:16:20,865 --> 00:16:25,280
Strategies by geolocation,
strategies by type of proxy, data

245
00:16:25,280 --> 00:16:27,115
center, ISP, residential, whatever.

246
00:16:27,685 --> 00:16:29,685
So you can set up multiple projects.

247
00:16:29,715 --> 00:16:32,965
And each project has a
username and password.

248
00:16:33,335 --> 00:16:35,495
So let's copy paste this information.

249
00:16:36,205 --> 00:16:37,565
And plug it to Scrapy.

250
00:16:38,065 --> 00:16:43,075
So what I will do is to just copy
past this information and show it.

251
00:16:43,845 --> 00:16:46,335
So I will update my middleware list.

252
00:16:46,725 --> 00:16:50,745
So now Scrappy, instead of
directly sending an HTTP request,

253
00:16:50,875 --> 00:16:52,465
he will use a middleware.

254
00:16:52,545 --> 00:16:55,425
And this middleware will say,
okay, please use Scropoxy.

255
00:16:55,425 --> 00:16:58,195
And here are the credentials of Scropoxy.

256
00:16:58,845 --> 00:17:01,215
So now if I'm running again, the spider.

257
00:17:01,715 --> 00:17:07,375
Everything is routed through nrest
and ScrapOxy and I've got my 50 items.

258
00:17:07,795 --> 00:17:08,535
That's cool!

259
00:17:09,315 --> 00:17:11,375
let's have a look on the proxy list.

260
00:17:12,055 --> 00:17:18,295
ScrapOxy just sent in a round robin
fashion way all the requests and

261
00:17:18,295 --> 00:17:20,315
I've got 100 percent success rate.

262
00:17:20,945 --> 00:17:21,965
That's perfect.

263
00:17:22,465 --> 00:17:24,895
Let's get back to our
spider to help Isabelle.

264
00:17:25,395 --> 00:17:29,555
So if I'm jumping to level
4, running again the spider.

265
00:17:30,055 --> 00:17:32,665
This time, I've got an issue.

266
00:17:33,165 --> 00:17:38,655
The antibody system just detected that
the IP address came from a datacenter.

267
00:17:39,430 --> 00:17:41,050
That's what I explained to you.

268
00:17:41,560 --> 00:17:45,420
So what I can do is to use
another type of IP address.

269
00:17:45,480 --> 00:17:49,370
Here we have the ASN detecting
which is Amazon ZeroTwo.

270
00:17:50,340 --> 00:17:53,530
So let's switch to a more advanced proxy.

271
00:17:54,525 --> 00:17:59,485
So we will use for that, smart
proxy, which is one of the biggest

272
00:17:59,725 --> 00:18:01,295
proxy provider in the world.

273
00:18:01,605 --> 00:18:06,985
So they have ISP proxy, residential
proxy, even sneaky data center proxy.

274
00:18:06,985 --> 00:18:08,345
So that's cool ones.

275
00:18:08,755 --> 00:18:10,755
So I will add that here.

276
00:18:11,255 --> 00:18:16,495
But first what I will do is to
stop the instances on Amazon.

277
00:18:17,185 --> 00:18:22,095
So I just have to click here,
and Scrapbox, it just orders to

278
00:18:22,145 --> 00:18:26,255
Amazon to stop everything, because
I don't want to pay for that.

279
00:18:26,825 --> 00:18:31,415
And that's a cool feature when
you have scraping sessions.

280
00:18:32,150 --> 00:18:36,020
ScrapOxy bring autoscale
up and autoscale down.

281
00:18:36,050 --> 00:18:38,590
It detects the sessions and just scale.

282
00:18:38,850 --> 00:18:44,280
So you just run instances when
you are running scraping sessions.

283
00:18:44,510 --> 00:18:46,150
And you don't pay for nothing.

284
00:18:46,830 --> 00:18:48,410
It's a huge cost saver.

285
00:18:48,910 --> 00:18:50,190
So let's have a look here.

286
00:18:50,290 --> 00:18:54,360
And yes, ScrapOxy just shutting
down all the instances.

287
00:18:54,660 --> 00:18:55,350
That's nice.

288
00:18:55,850 --> 00:18:57,790
So let's get back to Scope Proxy.

289
00:18:58,050 --> 00:19:04,890
I will add a connector from Scope
Proxy using ISP proxy with 10

290
00:19:04,890 --> 00:19:08,070
proxy from the United States.

291
00:19:08,930 --> 00:19:10,230
Perfect, and start it.

292
00:19:11,050 --> 00:19:18,470
So let's jump directly to the coverage map
and wait a little bit to get the instance.

293
00:19:19,450 --> 00:19:23,770
And you can see that I don't
have AWS instance, but now I got.

294
00:19:24,460 --> 00:19:26,230
It's my proxy running in the U.

295
00:19:26,230 --> 00:19:26,650
S.

296
00:19:26,860 --> 00:19:28,790
So all traffic will be route there.

297
00:19:29,560 --> 00:19:34,260
So if I'm running at MySpider, I
didn't touch anything on MySpider,

298
00:19:34,270 --> 00:19:37,330
just modify squad proxy configurations.

299
00:19:37,940 --> 00:19:40,560
Everything is now route to the U.

300
00:19:40,560 --> 00:19:41,120
S.

301
00:19:42,000 --> 00:19:45,110
And I will get all my 50 items.

302
00:19:45,610 --> 00:19:46,190
That's cool.

303
00:19:46,690 --> 00:19:50,380
So here, I've got 50 items.

304
00:19:50,510 --> 00:19:51,080
Perfect.

305
00:19:51,990 --> 00:19:52,730
Thank you very much.

306
00:19:52,730 --> 00:19:57,970
So now let's jump to level six.

307
00:19:58,470 --> 00:19:59,560
I will use that shield.

308
00:19:59,640 --> 00:19:59,870
Yeah.

309
00:20:00,370 --> 00:20:06,090
So now we'll have very serious autobot
systems and you will quickly understand.

310
00:20:06,860 --> 00:20:10,890
So I'm just connecting to the website
and at the first connection the

311
00:20:10,890 --> 00:20:15,680
website say okay you don't have a
fingerprint so I won't let you pass.

312
00:20:16,180 --> 00:20:17,790
So what is a fingerprint?

313
00:20:18,500 --> 00:20:23,800
So let's get back to the Tricky
Review website and jump to level 6.

314
00:20:24,300 --> 00:20:30,160
So if I'm just removing the filtering
here, yeah, I'm having everything.

315
00:20:30,660 --> 00:20:36,030
So when I'm connecting as a browser
to a website, I'm downloading

316
00:20:36,060 --> 00:20:40,210
HTML, images, CSS, JavaScript.

317
00:20:40,210 --> 00:20:42,310
So you have a lot of GET requests.

318
00:20:43,140 --> 00:20:46,420
But why do I have any POST requests?

319
00:20:46,920 --> 00:20:51,280
Post request is that I'm
sending information from

320
00:20:51,290 --> 00:20:53,520
my browser to the website.

321
00:20:53,930 --> 00:20:57,960
I'm sending at that, at regular
interval that you can see.

322
00:20:58,220 --> 00:20:59,290
what am I sending?

323
00:20:59,290 --> 00:21:01,370
Let's have a look on the payload.

324
00:21:02,170 --> 00:21:06,650
Oh, I'm sending the platform, the
time zone, and the real user agent.

325
00:21:06,920 --> 00:21:08,570
So I cannot fake it anymore.

326
00:21:09,070 --> 00:21:11,630
And all this information are
gathered with JavaScript.

327
00:21:12,050 --> 00:21:12,580
That's great.

328
00:21:13,080 --> 00:21:17,860
So what the website is doing is
executing JavaScript and collecting

329
00:21:17,860 --> 00:21:21,990
this information and sending them
to the web server with AJAX request.

330
00:21:22,780 --> 00:21:27,360
So I cannot rely anymore
on only HTTP request.

331
00:21:27,420 --> 00:21:28,870
Now I need real browsers.

332
00:21:29,370 --> 00:21:35,150
So I need a framework to start the
browsers and doing all the request.

333
00:21:35,560 --> 00:21:39,290
And of course, I don't
want to manage that myself.

334
00:21:39,755 --> 00:21:42,315
I want that Scrapy manage this framework.

335
00:21:42,745 --> 00:21:47,435
So let me introduce you a cool
framework called Playwright.

336
00:21:48,105 --> 00:21:51,505
So Playwright is an open source
framework for end to end tests.

337
00:21:51,575 --> 00:21:56,085
There is also Selenium, perhaps you know
them, Ubuntu Puppeteers, but Playwright

338
00:21:56,115 --> 00:21:58,775
is very adapted to use with Scrapy.

339
00:21:59,325 --> 00:22:04,055
So you can start Chrome, Firefox, Edge,
Safari, it's maintained by Microsoft,

340
00:22:04,365 --> 00:22:06,545
and clearly you can execute JavaScript.

341
00:22:06,545 --> 00:22:06,945
It's pretty cool.

342
00:22:07,895 --> 00:22:11,255
So let me show you how you can
adapt your code with Playwright.

343
00:22:12,045 --> 00:22:17,665
So I will open the same spider,
but just adapted for Playwright.

344
00:22:18,035 --> 00:22:19,225
So it's quite the same.

345
00:22:20,040 --> 00:22:25,200
First, we are doing all the requests,
so the 10 requests on the homepage after

346
00:22:25,200 --> 00:22:30,300
you are going doing the ations on the
city of Paris, when we get a list of

347
00:22:30,390 --> 00:22:33,780
hotels, we gathering hotel information.

348
00:22:33,960 --> 00:22:37,800
When we get the response of
hotels, we extracting name

349
00:22:37,890 --> 00:22:39,810
emails, and we quite the same.

350
00:22:40,310 --> 00:22:44,385
The only difference is
how we collect the data.

351
00:22:45,300 --> 00:22:51,350
Instead of using default Scrapy
download handlers by doing HTTP

352
00:22:51,350 --> 00:22:57,250
requests, we're saying, okay Scrapy,
now this time you will use Playwright,

353
00:22:57,320 --> 00:22:59,340
and Playwright will handle that.

354
00:22:59,900 --> 00:23:04,710
And internally, Playwright will open
a browser and doing all the requests.

355
00:23:05,320 --> 00:23:11,360
So if I'm running here the spider,
not this one, but the Playwright one,

356
00:23:11,860 --> 00:23:20,310
What you will see is that Scrapy has
the playwright open browsers, so 10

357
00:23:20,310 --> 00:23:22,790
browsers, 10 sessions, as we did.

358
00:23:23,290 --> 00:23:28,440
And, Connecting on the home page,
gathering all information, executing

359
00:23:28,440 --> 00:23:33,540
JavaScript, sending the payload, the
website says, OK, now you are executing

360
00:23:33,540 --> 00:23:35,400
JavaScript and I've got the payload.

361
00:23:35,690 --> 00:23:38,190
You can download hotel information.

362
00:23:38,440 --> 00:23:41,380
And that's what we are doing now.

363
00:23:42,130 --> 00:23:45,570
We are gathering every hotel information.

364
00:23:46,070 --> 00:23:50,330
As you can see, it's not so fast
because we are starting Chrome.

365
00:23:50,830 --> 00:23:56,290
It can take A little bit time,
but we are executing JavaScript

366
00:23:56,450 --> 00:23:57,600
and that's the cool part.

367
00:23:58,220 --> 00:24:00,830
So you can see I've got my 50 items.

368
00:24:01,070 --> 00:24:01,610
That's perfect.

369
00:24:02,110 --> 00:24:06,140
But of course, Antibot are
not only controlling that

370
00:24:06,140 --> 00:24:07,670
you are executing JavaScript.

371
00:24:08,220 --> 00:24:10,520
They are checking the fingerprint.

372
00:24:10,550 --> 00:24:13,580
You are sending signals,
information, to the website.

373
00:24:13,830 --> 00:24:14,680
Let's check that.

374
00:24:15,410 --> 00:24:18,310
So that's what I want
to show you on level 7.

375
00:24:18,810 --> 00:24:21,020
If I'm starting again, the spider here.

376
00:24:21,520 --> 00:24:24,230
Scrappy has to play right
to open the Chrome browser.

377
00:24:24,740 --> 00:24:26,430
We'll open 10 sessions.

378
00:24:26,930 --> 00:24:28,460
Connecting to the home page.

379
00:24:28,940 --> 00:24:30,750
Executing the JavaScript.

380
00:24:31,360 --> 00:24:37,010
Sending the javascript to the web
server, and now the Antibot checks the

381
00:24:37,030 --> 00:24:41,970
consistency of the payload, and every
time I'm trying to collect accommodations,

382
00:24:42,950 --> 00:24:45,130
I've got clearly a big error.

383
00:24:45,630 --> 00:24:49,840
So you can see that I'm not
collecting anything, and this error

384
00:24:50,140 --> 00:24:51,660
yields inconsistent timezones.

385
00:24:52,160 --> 00:24:56,660
Yes, of course, we are sending
information to the web server,

386
00:24:56,660 --> 00:24:58,890
and the Antibot will use that.

387
00:24:59,290 --> 00:25:02,740
And now it's checking that
the time zones are consistent.

388
00:25:02,750 --> 00:25:06,450
time zone between the browser,
which is in Europe, Paris, and

389
00:25:06,450 --> 00:25:08,540
we are using US IP address.

390
00:25:08,670 --> 00:25:11,250
it's America, Chicago for the time zones.

391
00:25:11,870 --> 00:25:13,170
we need to align that.

392
00:25:14,170 --> 00:25:17,170
I can change the IP address,
but I won't do that.

393
00:25:17,200 --> 00:25:22,430
I will Use more easy solutions, which is
changing the timezone of the browsers.

394
00:25:22,860 --> 00:25:24,250
So I can do that here.

395
00:25:24,690 --> 00:25:25,910
It's very quick.

396
00:25:26,430 --> 00:25:31,850
I just connect here, write
timezone options, and say,

397
00:25:31,900 --> 00:25:33,990
okay, it's America, New York.

398
00:25:34,180 --> 00:25:35,050
That's perfect.

399
00:25:35,050 --> 00:25:37,390
So let's run again the spiders.

400
00:25:37,890 --> 00:25:44,170
So now, Scrappy asked Playwright to
clone with his modified informations.

401
00:25:44,670 --> 00:25:48,940
We are connecting to the website,
gathering the website gather

402
00:25:48,970 --> 00:25:53,680
informations, through the correct
time zones, and in the US.

403
00:25:54,020 --> 00:25:59,410
And I'm sending this informations, and
as you can see, now I can collect hotels.

404
00:25:59,910 --> 00:26:04,040
So yeah, the different
hotels are just collecting.

405
00:26:04,540 --> 00:26:05,380
That's perfect.

406
00:26:05,880 --> 00:26:10,210
I will stop from there because it can be
very slow, but you understand the concept.

407
00:26:10,710 --> 00:26:11,420
Perfect.

408
00:26:12,090 --> 00:26:18,000
So now let me show you what Isabella
can have on very serious websites.

409
00:26:18,500 --> 00:26:23,490
It's protections that you can
find, on websites, commercial

410
00:26:23,490 --> 00:26:25,820
website, social platforms.

411
00:26:26,040 --> 00:26:27,890
It's very advanced protections.

412
00:26:28,575 --> 00:26:31,095
So let's jump to level eight.

413
00:26:31,595 --> 00:26:36,455
So now on these levels, as you
see, I'm gathering information,

414
00:26:36,455 --> 00:26:41,825
HTML, CSS, images, JavaScript, and
also I'm also doing POST requests.

415
00:26:42,005 --> 00:26:43,295
So I'm sending information.

416
00:26:43,895 --> 00:26:46,615
But the URL is not the same.

417
00:26:47,155 --> 00:26:49,415
It's more encrypted sort.

418
00:26:50,235 --> 00:26:52,375
So let's have a look at
the payload this time.

419
00:26:53,280 --> 00:26:55,900
Okay, I cannot understand the payload.

420
00:26:56,460 --> 00:26:58,110
So the payload is encrypted.

421
00:26:58,610 --> 00:27:01,550
And that's how Antibot
systems protect themselves.

422
00:27:01,600 --> 00:27:06,940
They don't want that you know
which signals you are tracking.

423
00:27:07,360 --> 00:27:08,800
So you cannot emulate them.

424
00:27:09,330 --> 00:27:10,280
Okay, that's fine.

425
00:27:10,280 --> 00:27:15,130
So let's have a look on the source
code which generates this payload.

426
00:27:15,600 --> 00:27:18,360
So if I'm opening the
source, let's find that.

427
00:27:18,860 --> 00:27:20,570
I've got level 8.

428
00:27:20,840 --> 00:27:22,290
Should be on JavaScript.

429
00:27:22,350 --> 00:27:23,480
I've got bootstrap.

430
00:27:23,530 --> 00:27:24,980
Okay, that's not Antibot.

431
00:27:25,010 --> 00:27:26,290
JQuery, no.

432
00:27:26,820 --> 00:27:28,310
But it should be this one.

433
00:27:28,390 --> 00:27:29,950
Okay, so let's have a look.

434
00:27:30,160 --> 00:27:33,000
Oh, that's not cool source code.

435
00:27:33,500 --> 00:27:39,240
So there is a lot of variable
declarations, which is encrypted,

436
00:27:39,280 --> 00:27:46,650
yeah, and also I've got functions
with soft, very difficult names.

437
00:27:47,420 --> 00:27:48,520
So I think the code is obfuscative.

438
00:27:49,020 --> 00:27:52,230
So now if I want to understand,
the signal, I need to

439
00:27:52,300 --> 00:27:53,580
deobfuscate this source code.

440
00:27:54,080 --> 00:27:57,910
But before trying to do that,
I want to explain to you how

441
00:27:57,910 --> 00:27:59,900
you can obfuscate source code.

442
00:27:59,910 --> 00:28:03,420
We need to understand obfuscation
before doing deobfuscations.

443
00:28:03,730 --> 00:28:07,330
So let's jump into some
techniques on string obfuscations.

444
00:28:07,830 --> 00:28:10,420
So the first, technique is to obfuscate.

445
00:28:10,920 --> 00:28:12,070
Simplify that.

446
00:28:12,120 --> 00:28:15,170
as you can see, I've got strings.

447
00:28:15,370 --> 00:28:18,950
And these strings are
encrypted with R functions.

448
00:28:19,510 --> 00:28:22,790
the R function is a base64 encoding.

449
00:28:22,790 --> 00:28:25,350
I will apply reverse, functions.

450
00:28:25,670 --> 00:28:29,070
if I'm doing that, which is
named as string containing, I

451
00:28:29,070 --> 00:28:31,980
will get the correct strings.

452
00:28:32,480 --> 00:28:36,890
But what I can do now is to
replace the constant by the value.

453
00:28:37,280 --> 00:28:41,820
So if I'm doing that, doing
constant unfolding, I will get,

454
00:28:42,250 --> 00:28:44,880
the value inside the last lines.

455
00:28:45,390 --> 00:28:49,640
But clearly it's not understandable
because the last, string are split.

456
00:28:50,580 --> 00:28:52,720
So I need to join all strings.

457
00:28:53,340 --> 00:28:57,600
So I will do string splitting,
unsplitting in fact.

458
00:28:58,230 --> 00:29:01,960
And you can see that's
now a little bit readable.

459
00:29:02,470 --> 00:29:09,260
Let's finish that and use and replace
the string notation by the dot notations

460
00:29:09,330 --> 00:29:12,350
and also to correctly avoid h here.

461
00:29:12,850 --> 00:29:15,830
If you're doing that, it's
called string bracketing, you'll

462
00:29:15,830 --> 00:29:18,080
get the correct instructions.

463
00:29:18,510 --> 00:29:22,740
So you can see these four lines are
just windows dot screen dot wise.

464
00:29:22,930 --> 00:29:25,330
So I'm gathering the size of the screen.

465
00:29:25,830 --> 00:29:28,790
So that's how you're doing deobfuscations.

466
00:29:28,790 --> 00:29:31,330
So let's jump to our source code.

467
00:29:32,130 --> 00:29:38,560
So I will copy paste this code
and put it in an empty file here.

468
00:29:38,880 --> 00:29:39,090
Yeah.

469
00:29:39,540 --> 00:29:42,100
So you can see it's an obfuscated code.

470
00:29:42,100 --> 00:29:44,230
You can remember here.

471
00:29:44,600 --> 00:29:44,810
Yeah.

472
00:29:45,310 --> 00:29:53,930
So I've got share a Script, which is a
the for Thisor, and it will do the same.

473
00:29:54,110 --> 00:29:55,610
So I will use Babel.

474
00:29:55,790 --> 00:30:01,210
So Babel is a er, and Babel will
just do all these string speeding,

475
00:30:01,210 --> 00:30:03,130
constant falling operations.

476
00:30:03,430 --> 00:30:05,800
Here I'm doing constant
and falling, et cetera.

477
00:30:06,730 --> 00:30:08,890
So let's run these spiders.

478
00:30:09,390 --> 00:30:15,550
So if I run this tool, it will create
a new file which is the obfuscative.

479
00:30:16,050 --> 00:30:16,890
Let's open it.

480
00:30:17,390 --> 00:30:20,300
Okay, so now it's a little bit readable.

481
00:30:21,215 --> 00:30:24,855
Of course, the deobfuscator cannot
find the name of the functions

482
00:30:24,895 --> 00:30:28,885
because you lost this information
during the obfuscation process.

483
00:30:29,235 --> 00:30:31,835
but we can understand
what the code is doing.

484
00:30:32,025 --> 00:30:34,425
So here I've got the functions.

485
00:30:34,975 --> 00:30:37,485
Here I've got an encryption functions.

486
00:30:37,875 --> 00:30:39,735
I'm doing RSA encryption.

487
00:30:39,735 --> 00:30:42,915
So if it's asymmetric
encryption, I need a key.

488
00:30:43,305 --> 00:30:46,275
So I assuming that this is a key.

489
00:30:46,325 --> 00:30:46,905
Okay.

490
00:30:47,355 --> 00:30:49,755
So now what this function is doing.

491
00:30:50,415 --> 00:30:55,175
Oh, it's the function which sends,
the request, the AJAX request, you

492
00:30:55,195 --> 00:30:57,315
can see we are doing a POST request.

493
00:30:57,635 --> 00:30:59,825
So which signal are we sending?

494
00:30:59,925 --> 00:31:02,205
Oh, it should be WebGL stuff.

495
00:31:02,625 --> 00:31:07,905
Okay, so we are sending the Vendor
and the Renderer of the GP models.

496
00:31:08,125 --> 00:31:08,675
That's perfect.

497
00:31:09,125 --> 00:31:13,995
So we need to create a payload,
a JSON payload, with a Vendor

498
00:31:13,995 --> 00:31:16,335
and Renderer, stringifies it.

499
00:31:16,740 --> 00:31:21,270
Encrypt that with RSA
encryption with this key.

500
00:31:21,600 --> 00:31:23,480
So let's copy paste this key.

501
00:31:23,980 --> 00:31:25,190
I will get this one.

502
00:31:25,280 --> 00:31:25,610
Perfect.

503
00:31:26,520 --> 00:31:29,810
So I've got some script to do that.

504
00:31:30,170 --> 00:31:30,420
Okay.

505
00:31:30,920 --> 00:31:35,780
So it's a kind of, it's the same,
spider, yeah, the Trekkie spiders.

506
00:31:36,200 --> 00:31:39,830
So we're going to the homepage,
so you already know this.

507
00:31:40,200 --> 00:31:44,710
But when we are on the homepage, we
will send the encrypted payload, the

508
00:31:44,890 --> 00:31:48,560
payload that we just forged on this ULNs.

509
00:31:48,600 --> 00:31:52,700
So we create this payload with
the build payload functions.

510
00:31:53,275 --> 00:31:57,785
And after we are doing all the pagination
requests and getting all the information.

511
00:31:58,715 --> 00:32:00,595
So let's get back on the build payload.

512
00:32:01,095 --> 00:32:04,845
The build payload is building
here, a payload with signals.

513
00:32:04,855 --> 00:32:06,665
So we need, the vendor.

514
00:32:06,725 --> 00:32:08,395
So let's say it's Intel.

515
00:32:08,860 --> 00:32:16,310
The handler, let's say it's
Intel 2, we need a public key,

516
00:32:16,520 --> 00:32:18,290
I will copy paste that one.

517
00:32:19,280 --> 00:32:23,860
So this information is signified,
encrypted with RSA encryption,

518
00:32:23,900 --> 00:32:25,050
and sent to the server.

519
00:32:25,510 --> 00:32:32,130
So now if I'm just running this
spider encrypted, you can see I

520
00:32:32,140 --> 00:32:34,300
just quickly have my 50 items.

521
00:32:34,800 --> 00:32:39,840
So doing deobfuscation can be a
very cool way to bypass protections.

522
00:32:39,970 --> 00:32:42,430
it's very interesting,
but only in one case.

523
00:32:42,460 --> 00:32:45,820
When you need to do a million
of requests very quickly.

524
00:32:46,390 --> 00:32:50,800
Otherwise, it's better to use
playwrights or more generic stuff.

525
00:32:51,280 --> 00:32:57,240
Because deobfuscating source code can take
a lot of time and take a lot of energy.

526
00:32:57,270 --> 00:32:58,950
That's the goal of Antivox.

527
00:32:59,450 --> 00:33:00,300
Thank you very much.

528
00:33:00,360 --> 00:33:01,970
I hope that you enjoyed the session.

529
00:33:01,990 --> 00:33:07,160
please download ScrapOxy, and if you can,
that helps me a lot, add GitHub stars.

530
00:33:07,660 --> 00:33:08,090
Bye bye.

