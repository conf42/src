1
00:00:01,500 --> 00:00:02,220
Hi everyone.

2
00:00:02,400 --> 00:00:02,910
I'm Hitesh.

3
00:00:03,280 --> 00:00:08,740
Today I'm going to top on topic on video
compression by using AI based approach.

4
00:00:08,800 --> 00:00:12,090
And my topic is on optimal
video compression using

5
00:00:12,090 --> 00:00:13,349
pixel ship tracking method.

6
00:00:14,040 --> 00:00:17,209
So before getting into the
topic I would like to give a

7
00:00:17,240 --> 00:00:18,589
quick introduction about myself.

8
00:00:19,374 --> 00:00:20,124
I'm ish.

9
00:00:20,234 --> 00:00:24,365
I've been working as a senior machine
learning engineer for Expedia Group.

10
00:00:24,884 --> 00:00:28,724
I have over seven years of experience
in the field of machine learning and ai.

11
00:00:29,154 --> 00:00:33,944
I have worked in several industries
like in InsureTech fin FinTech, and

12
00:00:34,065 --> 00:00:35,964
now I'm working in travel industries.

13
00:00:36,754 --> 00:00:39,535
I did my masters in mission
learning and statistics.

14
00:00:39,625 --> 00:00:42,675
And from that it's been quite
a journey in the field of ai.

15
00:00:42,705 --> 00:00:47,845
And we recently worked on this research
idea on video compression, and that's

16
00:00:47,845 --> 00:00:49,515
what I'm going to talk about today.

17
00:00:50,065 --> 00:00:51,205
Let's get into the topic.

18
00:00:52,345 --> 00:00:55,295
So just to give a quick
introduction in today's world.

19
00:00:56,210 --> 00:01:01,400
Videos, traffic is comprised of almost
85 percentage of the internet traffic.

20
00:01:01,870 --> 00:01:07,850
From social media alone we get around 10
petabytes of data every day gets processed

21
00:01:07,850 --> 00:01:10,075
and stored into the cloud environments.

22
00:01:11,070 --> 00:01:14,050
So there are around 20 different
VDA compression methods, which

23
00:01:14,050 --> 00:01:17,660
has been used, which has been used
and being used in the industry.

24
00:01:17,840 --> 00:01:20,960
And most of them are a
rule-based algorithm approaches.

25
00:01:21,240 --> 00:01:25,550
In recent times there has been a lot
of research happening in the mission

26
00:01:25,550 --> 00:01:29,830
learning field on Computeration side,
on trying to use AI based approach

27
00:01:29,830 --> 00:01:31,740
to do compressions of the videos.

28
00:01:32,110 --> 00:01:33,430
And the main purpose of.

29
00:01:34,020 --> 00:01:37,200
The research is happening in ML
space for video compression that

30
00:01:37,540 --> 00:01:41,860
using ml it can be used across a
diverse video format, irrespective

31
00:01:41,860 --> 00:01:43,730
of what format we are trying to use.

32
00:01:44,090 --> 00:01:47,290
And also it can be tapped
across any ML framework.

33
00:01:47,290 --> 00:01:50,200
Irrespective of trying to just
be possible in one framework.

34
00:01:50,200 --> 00:01:51,120
It is framework.

35
00:01:51,550 --> 00:01:54,640
Independent and also
diverse in video formats.

36
00:01:54,980 --> 00:02:00,400
We are pro, we are proposing an
another approach over here and so

37
00:02:00,400 --> 00:02:01,660
that's what I'm gonna talk about.

38
00:02:01,660 --> 00:02:02,980
So before getting into.

39
00:02:03,615 --> 00:02:05,145
More detail about our idea.

40
00:02:05,145 --> 00:02:09,545
I would just like to give some quick
idea of how the video compressions,

41
00:02:09,635 --> 00:02:12,875
what type of v compressions are
done today, and and then we can

42
00:02:12,875 --> 00:02:16,790
get into the topic of our approach
on the pixel ship tracking method.

43
00:02:18,230 --> 00:02:21,470
So in the current current traditional
compressional algorithm, these are the

44
00:02:21,470 --> 00:02:26,200
formats we have been using, which is
the H 2 64 H 2 65, AB one B, P eight

45
00:02:26,690 --> 00:02:30,650
all these things can, could be for
some, could be familiar or un, but.

46
00:02:31,555 --> 00:02:34,910
All of these traditional algorithm
methods are being used to store all

47
00:02:34,910 --> 00:02:40,450
these MP G four files, MP fours, MP five
for audios and NPG files for videos.

48
00:02:40,810 --> 00:02:44,320
And the algorithm which is being used at
the backend for the compressions of the

49
00:02:44,320 --> 00:02:46,900
videos or these algorithms basically.

50
00:02:47,140 --> 00:02:49,399
So there are basically
two types of compressions.

51
00:02:49,399 --> 00:02:52,219
One is lossless compression
and the loss compressions.

52
00:02:52,369 --> 00:02:57,570
So loss compressions are the, mostly used
format where when we try to compress data

53
00:02:57,570 --> 00:03:02,609
or we, I try to write and store a data
in term of in terms of video, we do see

54
00:03:02,609 --> 00:03:05,119
some decrease in the quality of the video.

55
00:03:05,179 --> 00:03:05,959
And that's the loss.

56
00:03:06,059 --> 00:03:10,190
The loss e compressions unless lossless
compression, which is more of the the high

57
00:03:10,190 --> 00:03:11,810
definition videos, which we talk about.

58
00:03:12,109 --> 00:03:15,500
But, most of the videos which
you write or upload to any social

59
00:03:15,500 --> 00:03:19,730
media or any cloud storages, all
of them are being compressed.

60
00:03:19,730 --> 00:03:21,225
And that's a lossy approach.

61
00:03:21,524 --> 00:03:23,959
And the approach, which we are
going to talk about today is

62
00:03:24,029 --> 00:03:25,410
also a lossy based approach.

63
00:03:25,790 --> 00:03:29,795
Logies are not basically something where
the data will be lost rather it's just

64
00:03:29,795 --> 00:03:32,234
the decrease in the quality of the video.

65
00:03:32,234 --> 00:03:36,754
But from a normal human standpoint
a human, viewpoint you won't, you

66
00:03:36,754 --> 00:03:40,790
wouldn't see a lot of data losses, but
granularly, if you try to see that there

67
00:03:40,790 --> 00:03:42,200
will be a lot of data losses happening.

68
00:03:42,440 --> 00:03:44,450
But that's the only thing which we lose.

69
00:03:44,490 --> 00:03:47,570
But we e eventually we try
to compress these videos and

70
00:03:47,570 --> 00:03:48,560
store in a more optimal way.

71
00:03:49,220 --> 00:03:50,870
And so that's the lossy approach.

72
00:03:50,870 --> 00:03:53,660
And one of the approaches which we
are approaching we are proposing

73
00:03:53,660 --> 00:03:55,430
today is also a based approach.

74
00:03:56,775 --> 00:04:01,215
Today in the ai machine learning space
these are the, some of the areas that

75
00:04:01,215 --> 00:04:05,145
researchers are happening on the video
compressions which is tapped by the

76
00:04:05,145 --> 00:04:10,435
encoders, VAE, which is a variant, auto
encoders and deep con contextual network.

77
00:04:10,855 --> 00:04:16,470
So we are our idea is also based on a deep
contextual network over here, and that's

78
00:04:16,470 --> 00:04:18,879
what I'm going to talk talk about today.

79
00:04:18,979 --> 00:04:21,879
For our approach which is the
pixel point tracking approach

80
00:04:22,129 --> 00:04:23,359
to do the video D compression.

81
00:04:23,929 --> 00:04:25,549
Yeah, so this is a proposed method.

82
00:04:25,549 --> 00:04:30,834
So the way we have approached this problem
is basically trying to avoid the rid

83
00:04:30,834 --> 00:04:33,414
then pixels in a video while storing.

84
00:04:33,924 --> 00:04:38,214
So just to give a quick example, so
videos are basically comprised of frames.

85
00:04:39,069 --> 00:04:42,339
For any given video, let's see, even
a five second video, there would be at

86
00:04:42,339 --> 00:04:46,179
least 20 to 30 frames, which we have,
which are the, basically the images.

87
00:04:46,749 --> 00:04:50,169
So when you, when we see as a
video, we saw a lot of things

88
00:04:50,169 --> 00:04:52,059
moving here and there as a video.

89
00:04:52,059 --> 00:04:53,214
But if you see as a frame.

90
00:04:53,859 --> 00:04:55,809
Frame by frame, image by image.

91
00:04:56,469 --> 00:05:00,239
We basically see the, there's a lot
of redundancy happening from one frame

92
00:05:00,239 --> 00:05:04,409
to another frame, which is basically
storing the same pixels or the same

93
00:05:04,409 --> 00:05:06,209
data from one frame to another frame.

94
00:05:06,539 --> 00:05:10,254
And that's what we are trying to
avoid by not storing those redundant

95
00:05:10,439 --> 00:05:13,739
frames from one frame to another
frame on all the subsequent frames.

96
00:05:14,219 --> 00:05:18,009
And, reduce all those un datas
and we are trying to optimize the

97
00:05:18,009 --> 00:05:21,459
storage when we try to store using
machine learning based approach.

98
00:05:21,759 --> 00:05:26,769
So the way we are approaching, or we way
we are trying to find this redundant pixel

99
00:05:26,769 --> 00:05:29,019
is basically trying to track the pixels.

100
00:05:29,259 --> 00:05:34,239
So for example, when a video has been
taken so that you know that a video

101
00:05:34,239 --> 00:05:35,949
moves in a particular directions, right?

102
00:05:35,949 --> 00:05:39,789
So it can move from left to right or top
to bottom, or it can move from left to

103
00:05:40,089 --> 00:05:42,519
top or right to top or top to bottom.

104
00:05:42,519 --> 00:05:43,809
The video can move any anyways.

105
00:05:44,049 --> 00:05:47,769
So based on the moment of the videos
using the coordinates, we are trying to

106
00:05:47,769 --> 00:05:51,849
understand how far it has moved from a
point A to point B. Based on that, we know

107
00:05:51,849 --> 00:05:55,479
how far the new pixels we are going to
get in the next frame, and what are the

108
00:05:55,479 --> 00:05:57,189
other relevant pixels we are going to.

109
00:05:57,564 --> 00:06:01,904
Get the next frame from the first frame,
and we are trying to nullify those and

110
00:06:01,904 --> 00:06:05,274
trying to put a black spots on those by
doing surveillance already the size of

111
00:06:05,274 --> 00:06:07,944
a frame or size of a image in the video.

112
00:06:08,934 --> 00:06:10,169
So there are two different approaches.

113
00:06:10,974 --> 00:06:14,624
One is single point trajectory method
and the multi-point trajectory method.

114
00:06:15,104 --> 00:06:19,964
So the single point trajectory method is
basically using trying to find arbitrary

115
00:06:19,964 --> 00:06:21,554
point and we are trying to track it.

116
00:06:21,554 --> 00:06:25,754
And multi-point is basically on trying to
use multiple points and trying to track

117
00:06:25,824 --> 00:06:28,374
the video movement or the frames movement.

118
00:06:28,764 --> 00:06:32,784
So I'll get into details now of
starting with a single point trajectory.

119
00:06:34,029 --> 00:06:37,529
So for a single point trajectory,
what we basically do is we try to

120
00:06:37,529 --> 00:06:40,809
find an arbitrary point in a in a
video in the starting first frame.

121
00:06:41,259 --> 00:06:46,569
And from there we try to basically track
the point from one frame to another frame.

122
00:06:46,869 --> 00:06:50,519
And this is where we are using a deep
learning, a computer vision approach.

123
00:06:50,769 --> 00:06:53,934
We're using a a concept called
persistent independent prac

124
00:06:53,984 --> 00:06:55,814
particles, which is called Pips.

125
00:06:56,514 --> 00:07:00,514
It's an a research paper written
by, another researchers and we

126
00:07:00,514 --> 00:07:05,114
made use of that over here and to
optimize that pips concept in a way.

127
00:07:05,384 --> 00:07:08,294
And we are trying to achieve
the video compression over here.

128
00:07:08,384 --> 00:07:09,944
And that's our approach basically.

129
00:07:10,214 --> 00:07:13,244
So we basically try to use a
single point trajectory over here.

130
00:07:13,549 --> 00:07:16,259
Which is basically trying
to find arbitrary point in

131
00:07:16,259 --> 00:07:17,224
the first frame of a video.

132
00:07:17,724 --> 00:07:22,024
And from that we basically try to track
where the particular object or a point

133
00:07:22,024 --> 00:07:25,799
or a pixel is moving from one frame
to another frame to subsequent frame.

134
00:07:25,799 --> 00:07:28,469
Based on that, we know what's the,
how many coordinates it's moving.

135
00:07:28,779 --> 00:07:32,019
Based on that we try to see how
many new pixels we are going to

136
00:07:32,019 --> 00:07:33,669
get, and that's what we store.

137
00:07:33,669 --> 00:07:34,624
We try to avoid the rest.

138
00:07:35,404 --> 00:07:39,419
So the single point
trajectory basically works.

139
00:07:39,519 --> 00:07:43,304
Only in places where the objects are
starting and the video is moving.

140
00:07:43,724 --> 00:07:47,034
So there are three different
ways a video can be seen.

141
00:07:47,154 --> 00:07:51,604
So one is where the objects are static,
where the video is moving or the camera

142
00:07:51,604 --> 00:07:55,704
is moving, or the camera, or the camera
is starting where the objects are moving.

143
00:07:56,799 --> 00:07:57,489
It could be the both.

144
00:07:57,489 --> 00:08:00,069
A camera and object could be
moving at the same time in a video.

145
00:08:00,219 --> 00:08:01,449
So there is three different approaches.

146
00:08:01,449 --> 00:08:05,899
The single point tracking can solve
only in scenarios where the objects are

147
00:08:05,899 --> 00:08:11,069
static, but the but the camera is moving
but Multipoint trajectory can do it.

148
00:08:11,264 --> 00:08:14,779
Way better by trying to have multiple
pixel point tracking trajectories.

149
00:08:14,779 --> 00:08:18,079
And based on, we can try to get
an average movement of it and we

150
00:08:18,079 --> 00:08:20,089
try to avoid the readable pixels.

151
00:08:20,629 --> 00:08:24,559
So before getting into Multipoint
trajectory, I will just show you a quick

152
00:08:24,559 --> 00:08:26,269
example of how this tracking looks like.

153
00:08:27,259 --> 00:08:30,349
You can see here in this
picture, there is a dog running.

154
00:08:30,349 --> 00:08:34,134
This is a video where we are trying
to track the nose of the dog.

155
00:08:34,634 --> 00:08:37,114
So the idea is not to
track an object here.

156
00:08:37,114 --> 00:08:40,804
The idea is basically to track
in particular pixel, and based

157
00:08:40,804 --> 00:08:44,249
upon the pixel movement, we can
identify what is the redundant

158
00:08:44,249 --> 00:08:45,574
and the non-redundant part of it.

159
00:08:46,064 --> 00:08:48,934
As I mentioned, this single
point trajectory is achievable

160
00:08:49,034 --> 00:08:52,694
through an to a not approach.

161
00:08:52,759 --> 00:08:55,999
I would say like through a
method where the objects are

162
00:08:55,999 --> 00:08:57,109
static, where the camera is.

163
00:08:58,219 --> 00:09:00,349
So that's what the single
point tracking looks like.

164
00:09:00,399 --> 00:09:03,429
When we try to track it using mission
learnings and we are trying to use the

165
00:09:03,429 --> 00:09:05,349
frames to see where the objects is moving.

166
00:09:06,009 --> 00:09:07,329
The pixel point is moving.

167
00:09:08,289 --> 00:09:11,929
So to talk about the multipoint
trajectory as I mentioned, so

168
00:09:12,259 --> 00:09:13,729
Multipoint trajectory basically works.

169
00:09:14,029 --> 00:09:18,199
When when you try to put the pixel
points, the trajectory points in

170
00:09:18,199 --> 00:09:21,529
multiple places in like a grid
format, in a 2D grid format.

171
00:09:22,034 --> 00:09:27,744
And when by doing so, so now what it does
is in a given frame let's say we have

172
00:09:27,744 --> 00:09:32,979
a eight by eight frame where you put 64
points or 64 trajectory points, which can

173
00:09:32,979 --> 00:09:37,579
track all the 64 positions within a grid
where it could be a midpoint of every

174
00:09:37,579 --> 00:09:42,649
small grid positions within the eight by
pixel image, so by a by eight image size.

175
00:09:42,649 --> 00:09:44,689
So by doing so, what
happens is can also track.

176
00:09:45,364 --> 00:09:46,114
Object movement.

177
00:09:46,179 --> 00:09:48,609
It can also track the camera movements.

178
00:09:48,609 --> 00:09:53,779
And by by calibrating all of those, we
can try to find what is the only T part,

179
00:09:53,779 --> 00:09:57,014
which is what are the non-redundant
part which can we can expect in the

180
00:09:57,014 --> 00:09:58,424
next frame compared to the first frame.

181
00:09:58,814 --> 00:10:03,274
And this can mostly work in in a very
advanced approaches where the videos

182
00:10:03,274 --> 00:10:06,784
is very complex and that's where the
multi-point trajectory element lead.

183
00:10:06,999 --> 00:10:10,169
Be helpful to just give a quick
visual of how the multipoint

184
00:10:10,919 --> 00:10:13,049
like would, will look like.

185
00:10:13,049 --> 00:10:16,429
Is from the previous video we saw
that there was a dog and the dog

186
00:10:16,429 --> 00:10:20,809
was moving and we were just pointing
one and, I can go back here.

187
00:10:20,809 --> 00:10:23,784
So you can see here, it's just
tracking one trajectory and this tr

188
00:10:24,084 --> 00:10:27,924
looks like it's going like a it looks
like a, some kind of leaf, right?

189
00:10:27,924 --> 00:10:29,274
So that's why, that's how it's moving.

190
00:10:29,694 --> 00:10:34,474
But in this case, in this video, we
can see that the dog the object as

191
00:10:34,474 --> 00:10:35,999
far as the video, both are moving.

192
00:10:36,269 --> 00:10:39,269
So a single point trajectory
would not effectively can say

193
00:10:39,269 --> 00:10:40,559
how the pixels are moving.

194
00:10:40,809 --> 00:10:42,669
But by using a multi-point trajectory.

195
00:10:43,019 --> 00:10:48,269
You can see here actually the how the
entire video from one frame to another

196
00:10:48,269 --> 00:10:51,839
frame, or at least this is like a, let's
say it's a two second video, might be

197
00:10:51,839 --> 00:10:53,669
we have four to five frames in this.

198
00:10:53,969 --> 00:10:57,509
We can see from frame one to frame
five, how far it has been shifted.

199
00:10:57,509 --> 00:11:02,114
So this can give like an
overview of how the pixels are

200
00:11:02,114 --> 00:11:03,614
moving at different positions.

201
00:11:03,914 --> 00:11:07,274
And based on that, every single
positions we can see, what are the

202
00:11:07,544 --> 00:11:10,394
pixels we need to store, what are
the coordinates we need to store?

203
00:11:10,524 --> 00:11:13,944
What are the quad, what are the position
coordinates we, we can avoid storing and

204
00:11:13,944 --> 00:11:15,844
we can retribute from the previous spring.

205
00:11:15,934 --> 00:11:17,794
And that's what we are
trying to do over here.

206
00:11:18,604 --> 00:11:21,514
So let's get into the steps
on how we have achieved it.

207
00:11:22,294 --> 00:11:26,004
So for the compression step just to
give a quick note, we in this approach

208
00:11:26,004 --> 00:11:29,374
we are trying to prove that this is
possible to pixel point tracking by

209
00:11:29,374 --> 00:11:30,724
using a single trajectory method.

210
00:11:30,814 --> 00:11:34,929
So we have used that as a proof of
concept over here to display displayed

211
00:11:34,929 --> 00:11:36,339
and show that this is possible.

212
00:11:36,939 --> 00:11:40,809
And so as a step one, using a
single point trajectory method.

213
00:11:41,529 --> 00:11:43,869
We arbitrarily choose a point in a video.

214
00:11:43,939 --> 00:11:45,169
So we are using a video.

215
00:11:45,169 --> 00:11:47,869
We used a video where the
objects are static, but the

216
00:11:47,869 --> 00:11:49,149
video the camera is moving.

217
00:11:49,149 --> 00:11:49,899
The video is moving.

218
00:11:50,649 --> 00:11:52,629
Basically in the video, the
camera is moving, sorry.

219
00:11:52,969 --> 00:11:55,404
We use like a, let's say a
midpoint, the in the frame.

220
00:11:55,584 --> 00:11:57,404
And and we are tracking
that point over here.

221
00:11:57,434 --> 00:12:02,094
So we choose arbitrary point for
the the pips to track the present

222
00:12:02,094 --> 00:12:04,194
independent particles model to track.

223
00:12:04,529 --> 00:12:07,629
And it basically process
basically eight frames at a time.

224
00:12:07,989 --> 00:12:11,799
So to have, to improve the accuracy,
we are trying to use one frame at a

225
00:12:11,799 --> 00:12:15,739
time to predict what bad the pixels are
moving first, why that point is moving

226
00:12:15,739 --> 00:12:19,339
from one frame to so from first frame to
second frame for the given pixel point.

227
00:12:20,069 --> 00:12:22,284
So we first, we place a random point.

228
00:12:23,119 --> 00:12:27,109
Arbitrary point in the first frame, and
then we try to track the frame from frame

229
00:12:27,109 --> 00:12:28,549
by frame one frame to another frame.

230
00:12:28,919 --> 00:12:30,059
While we are trying to track.

231
00:12:30,209 --> 00:12:35,234
So let's say we were, we had a, we were
at a point in frame one, and eventually

232
00:12:35,234 --> 00:12:39,829
when we try to move from frame one to
frame one, to frame two, let's say mode

233
00:12:39,919 --> 00:12:42,884
four coordinates to the right, we know
that there are, there is going to be.

234
00:12:43,439 --> 00:12:45,953
A with of four pixels.

235
00:12:45,953 --> 00:12:48,863
Four with of four size
pixels going to come inside.

236
00:12:48,913 --> 00:12:52,723
And I know the rest of that part on the
left side of the first frame is gonna

237
00:12:52,723 --> 00:12:54,283
be redundant and should be available.

238
00:12:54,583 --> 00:12:56,393
Same pixels in the second frame.

239
00:12:56,393 --> 00:12:59,103
So that's what we are going to
nullify and direct delete that

240
00:12:59,588 --> 00:13:01,598
and store only the non pixels.

241
00:13:02,368 --> 00:13:06,598
And that's what the compression part
does for one frame to the second frame.

242
00:13:06,598 --> 00:13:09,268
And similarly, from second frame
to the third frame to so on.

243
00:13:09,298 --> 00:13:11,398
That's how we try to compress these.

244
00:13:12,448 --> 00:13:14,658
So just give a quick idea
of how that looks like.

245
00:13:14,658 --> 00:13:19,033
You can see here in the frame one in the
frame one, there's we can see that sorry.

246
00:13:19,363 --> 00:13:22,653
So the framework, we can see
that over here, that the.

247
00:13:23,418 --> 00:13:27,858
This is the complete image and the, in the
frame two, what ha what's happening is it

248
00:13:27,858 --> 00:13:30,258
has mowed like few pixels to the right.

249
00:13:30,828 --> 00:13:33,798
So when we are trying to do the
compression, the second frame won't

250
00:13:33,798 --> 00:13:38,558
look like how we see the second picture
or second picture over here, rather.

251
00:13:38,778 --> 00:13:43,318
It would look like the third picture
in the image where it basically

252
00:13:44,908 --> 00:13:49,088
all of the redundant pixels and it
stores only the non-redundant pixel

253
00:13:49,148 --> 00:13:51,308
in the storage for the second frame.

254
00:13:51,713 --> 00:13:54,563
Similarly, we do it from the third
frame, fourth frame, and so on.

255
00:13:54,833 --> 00:13:58,583
And by doing so, we try as we are
putting a black value of zero in terms

256
00:13:58,583 --> 00:14:02,883
of pixel to all the redundant part we
reduce the size during the compression

257
00:14:03,093 --> 00:14:04,593
or this is how the compression part work.

258
00:14:05,163 --> 00:14:08,153
And now I'll get into how
the decompression part work.

259
00:14:09,278 --> 00:14:11,463
So the decompression part
is an interesting one.

260
00:14:11,463 --> 00:14:12,633
It's a very intuitive approach.

261
00:14:12,633 --> 00:14:16,863
So what we have done is so that in
the process of this compressions, the

262
00:14:16,863 --> 00:14:19,598
first frame will always remain intact.

263
00:14:19,598 --> 00:14:22,718
That basically means like it
could be whatever it is, the

264
00:14:22,718 --> 00:14:24,008
first frame, it remains the same.

265
00:14:24,653 --> 00:14:28,013
We don't because for the first frame
there is nothing called render than pixel.

266
00:14:28,013 --> 00:14:28,778
That's the first frame.

267
00:14:29,118 --> 00:14:33,508
So first frame always stores the
entire data that without with any

268
00:14:33,508 --> 00:14:36,773
of the data points in the pixel data
points, which is so called the pixels.

269
00:14:37,253 --> 00:14:40,283
So now what happens during the
decompression step over here is,

270
00:14:40,733 --> 00:14:44,603
so during the compression, we know
that from frame one to frame two.

271
00:14:45,438 --> 00:14:48,258
Four, four coordinates to
the right on the X axis.

272
00:14:48,588 --> 00:14:52,358
So we, what we basically do is that's
this, that's the only new width, which

273
00:14:52,358 --> 00:14:54,788
we are looking from the second frame,
and that's what we stored from the

274
00:14:54,788 --> 00:14:56,498
second frame and we nullify the wrist.

275
00:14:56,918 --> 00:15:00,008
So those nullified positions
are stored as an array.

276
00:15:00,868 --> 00:15:05,578
On, on, on collecting those coordinates
positions alone, which is basically

277
00:15:05,578 --> 00:15:11,428
the a, the X axis and the yxi points on
basically let's say like a rectangle.

278
00:15:11,428 --> 00:15:15,148
So it takes four coordinate points,
and we store it for a reframe on

279
00:15:15,148 --> 00:15:19,093
what has to be recomposed when we
try to de do the decompressions.

280
00:15:19,578 --> 00:15:22,218
So basically we, when we do
the compression, we store all

281
00:15:22,218 --> 00:15:25,668
the all the frames by storing
only the non-resistant pixels.

282
00:15:26,208 --> 00:15:29,848
And then we also stored a separate
array, which basically holds the

283
00:15:30,068 --> 00:15:34,108
holds the coordinates of the redundant
positions, which can be obtained from

284
00:15:34,108 --> 00:15:35,988
the previous pixel sorry, previous frame.

285
00:15:36,258 --> 00:15:39,688
So how that basically would work is
if you see here in the first image

286
00:15:39,743 --> 00:15:41,753
the data retrieval frame, right?

287
00:15:41,753 --> 00:15:44,123
So this is the first frame from
the first frame to second frame.

288
00:15:44,123 --> 00:15:46,114
If you see the second the.

289
00:15:46,878 --> 00:15:50,748
LA where, wherever that shades,
those are the non pixels.

290
00:15:50,748 --> 00:15:52,458
Wherever it's blank, it's the ENT part.

291
00:15:52,878 --> 00:15:56,478
So what it's basically showing
us is over here from the first

292
00:15:56,478 --> 00:15:58,078
frame to the second frame.

293
00:15:58,078 --> 00:15:58,553
And it moved.

294
00:15:58,648 --> 00:15:59,248
It has moved.

295
00:15:59,298 --> 00:16:01,638
The point has moved like this,
the camera has moved like this.

296
00:16:01,638 --> 00:16:05,538
So it has moved from this position
to move like little bit of, little

297
00:16:05,538 --> 00:16:06,888
bit tilted towards downside.

298
00:16:06,888 --> 00:16:09,438
So those are the new pixels just
coming inside the second frame.

299
00:16:09,708 --> 00:16:12,293
So that's what we are storing
and we remove the rest of it.

300
00:16:14,343 --> 00:16:18,138
To reconstruct those second
frame with all the pixels.

301
00:16:18,138 --> 00:16:22,638
I mean with all the remaining redundant
part, to make it as a video at the end,

302
00:16:22,638 --> 00:16:26,678
what we do is we take the redundant part
from the first pixel because we sorry.

303
00:16:26,678 --> 00:16:27,188
First frame.

304
00:16:27,188 --> 00:16:31,068
We know the first frame is completely,
intact where it has all their pixels from

305
00:16:31,068 --> 00:16:35,648
the first frame, we take the T part and
we which is basically the inverse position

306
00:16:35,648 --> 00:16:37,443
of the second frames redundant position.

307
00:16:37,443 --> 00:16:40,353
So the second frames redundant
coordinates, will be the

308
00:16:40,443 --> 00:16:42,213
reverse of the first frames.

309
00:16:42,313 --> 00:16:45,033
Coordinates positions so that's
what, because that's why the

310
00:16:45,033 --> 00:16:46,293
video is moved like this.

311
00:16:46,293 --> 00:16:48,873
So we try to take the redundant
part of the pixels, try to

312
00:16:49,143 --> 00:16:50,373
reconstruct the second frame.

313
00:16:50,733 --> 00:16:52,173
So second frame will be reconstructed.

314
00:16:52,173 --> 00:16:54,963
Now, similarly, we know that for the
subsequent frames, for frame two,

315
00:16:54,963 --> 00:16:58,173
frame three and until the frame, and
by doing so, we do the decompression.

316
00:16:58,173 --> 00:16:59,403
We bring back everything again.

317
00:16:59,943 --> 00:17:02,853
So that's how we are approaching this
and that's how we do the decompression.

318
00:17:03,243 --> 00:17:04,353
And this is basically work.

319
00:17:04,408 --> 00:17:06,873
We have done the POC on
the single point tracking.

320
00:17:08,463 --> 00:17:11,223
So in terms of the results over here yeah.

321
00:17:11,223 --> 00:17:15,163
So what we were able to achieve
is the compression we try to do is

322
00:17:15,163 --> 00:17:17,773
like a 15 times a 15 time per frame.

323
00:17:18,133 --> 00:17:21,543
So that basically means we are
taking like a 15 milliseconds times

324
00:17:21,548 --> 00:17:25,643
per it takes 15 milliseconds to
do the compression of per frame.

325
00:17:26,513 --> 00:17:30,233
And and it resulting in size
of 36 kilo 36 kilobytes.

326
00:17:30,353 --> 00:17:30,773
That's.

327
00:17:32,723 --> 00:17:35,843
Per frame, which we are trying to
compress over here, and it takes

328
00:17:35,843 --> 00:17:37,343
around 15 milliseconds per frame.

329
00:17:37,583 --> 00:17:41,653
So let's say we have a thousand frames it
would be basically 15,000 milliseconds.

330
00:17:42,023 --> 00:17:45,383
So similarly for decompression,
we are trying we, it's taking

331
00:17:45,383 --> 00:17:47,063
around 15 milliseconds over here.

332
00:17:47,363 --> 00:17:50,208
And as we are decompressing, we
need to reconstruct the frame.

333
00:17:50,208 --> 00:17:54,063
So that's add some extra time per
frame and it reconstructs back the

334
00:17:54,493 --> 00:17:58,093
the images with each frames looks
like around 238 kilobytes over here.

335
00:17:58,783 --> 00:18:00,943
And that's what it has been used.

336
00:18:00,943 --> 00:18:04,213
There could be a question of why the
compression decompression takes a 15

337
00:18:04,213 --> 00:18:08,413
milliseconds and 15 milliseconds is
basically the model when we are trying

338
00:18:08,413 --> 00:18:10,843
to do the compression, the model
has to do the predictions aspect.

339
00:18:11,023 --> 00:18:15,133
So it has to do the predictions of where
this pixel is moving from one point to

340
00:18:15,133 --> 00:18:16,873
the next from one frame to the next frame.

341
00:18:17,353 --> 00:18:18,973
So the model takes some.

342
00:18:19,238 --> 00:18:20,138
Few milliseconds.

343
00:18:20,138 --> 00:18:25,063
And also by, and also we have a builtin
algorithm on top of it, which actually use

344
00:18:25,063 --> 00:18:29,963
those coordinates to nullify the nullify
and store those redundant pixels into an

345
00:18:29,963 --> 00:18:31,763
array pixel coordinates into an array.

346
00:18:31,883 --> 00:18:35,058
And this just store the, and
non-written pixels from the

347
00:18:35,493 --> 00:18:37,053
subsequent frames and so on.

348
00:18:37,443 --> 00:18:42,453
So that's why it takes 15 milliseconds
and 50 like milliseconds per frame.

349
00:18:42,543 --> 00:18:44,643
And eventually this for.

350
00:18:46,068 --> 00:18:50,238
For like a one minute video, it takes
around currently it's taking around close

351
00:18:50,238 --> 00:18:51,898
to a minute for a two minutes video.

352
00:18:51,898 --> 00:18:55,663
It takes close to a minute to do the
compressions and store it and and and

353
00:18:55,663 --> 00:18:58,573
also the decompression takes pretty
much close to two minutes to do that.

354
00:18:58,623 --> 00:19:00,833
So it's quite kind of little slow.

355
00:19:00,833 --> 00:19:02,948
The reason is we are trying
to do the prediction per.

356
00:19:03,848 --> 00:19:06,998
But we can also try to do the
predictions per eight frames.

357
00:19:07,048 --> 00:19:10,408
Let's say we have 64 frames in the
video, and we can just do it in eight

358
00:19:10,408 --> 00:19:14,338
iterations by ha, by trying to predict
each eight frames and trying to store it.

359
00:19:14,858 --> 00:19:19,148
But eventually what happened was the loss
was more when we tried to do eight, eight

360
00:19:19,148 --> 00:19:22,418
frames, the prediction was dropping and
eventually the data loss was increasing.

361
00:19:22,908 --> 00:19:26,688
We had to use per single frame at a
time so that the accuracy is really good

362
00:19:26,688 --> 00:19:31,618
and the loss is less so that we can try
to save lot of, data reduce the data

363
00:19:31,618 --> 00:19:36,788
losses and to trying to benchmark it
against the existing traditional methods.

364
00:19:37,388 --> 00:19:39,038
And you can see that in this graph, right?

365
00:19:39,398 --> 00:19:43,193
So as we try to do the compression
per eight frames, or per seven frames

366
00:19:43,193 --> 00:19:47,208
or per six frames, you can see that
the the duction in the size is.

367
00:19:48,633 --> 00:19:52,293
When we try to do per eight frames,
we are able to reduce the compression.

368
00:19:52,398 --> 00:19:56,773
We reduce the size like by 82%, but 82%.

369
00:19:56,773 --> 00:20:00,933
But compared to one frame, using using
one frame for predictions by doing

370
00:20:00,933 --> 00:20:02,853
so we can reduce a lot of the size.

371
00:20:03,043 --> 00:20:06,723
And also it can be faster, but
the problem is the reason that is

372
00:20:06,723 --> 00:20:10,253
able to reduce in size is because
increasing number of frames to do

373
00:20:10,253 --> 00:20:13,823
the at time to do the prediction
also adds a lot of losses to data.

374
00:20:13,823 --> 00:20:16,643
That's why we see the reduction in the
total size when we do the compression.

375
00:20:16,973 --> 00:20:20,303
And you can see the right hand side
graph where as we increase number

376
00:20:20,303 --> 00:20:23,603
of frames to use at time to do the
prediction for all of the pixel moments.

377
00:20:23,873 --> 00:20:26,093
So if you use like eight
frames, it just tries to.

378
00:20:27,113 --> 00:20:30,683
The arbitrary point and see where
it is moving in the first eight

379
00:20:30,683 --> 00:20:35,523
pixels so that accuracy is going down
because of that the way a working.

380
00:20:38,393 --> 00:20:42,768
The, rather than pixels, it also does
the wrong thing because it eventually

381
00:20:42,768 --> 00:20:44,628
end up having a lot of data losses.

382
00:20:44,988 --> 00:20:47,623
And because of that, we can see
that the number of the compression

383
00:20:47,623 --> 00:20:50,323
percentage is pretty high when
we try this number of frames.

384
00:20:50,323 --> 00:20:51,793
But also the loss is also high.

385
00:20:52,063 --> 00:20:55,388
So then the righthand side graph,
you can see that the loss is going

386
00:20:55,388 --> 00:20:59,463
pretty high when we try to increase
the prediction per eight frames at.

387
00:21:02,393 --> 00:21:02,843
At a time.

388
00:21:02,893 --> 00:21:06,373
So if you try to do one frame, you can
see the loss is very less close to 4%.

389
00:21:06,853 --> 00:21:09,973
Whereas if you try to use eight
frames using pips plus it's

390
00:21:09,973 --> 00:21:11,353
around sound percent of data loss.

391
00:21:11,353 --> 00:21:12,193
So it's pretty high.

392
00:21:12,193 --> 00:21:15,703
Eventually you can see that the quality
would be very not very good actually.

393
00:21:16,023 --> 00:21:19,048
So then you can see there's
the reason for you to see that

394
00:21:19,048 --> 00:21:20,218
there are two lines over here.

395
00:21:20,218 --> 00:21:21,478
One is Pips and Pips Plus, plus.

396
00:21:21,478 --> 00:21:22,998
There are two different models.

397
00:21:23,068 --> 00:21:23,938
One is Pips.

398
00:21:24,733 --> 00:21:28,253
Which is initial version of pixel versus
the version two of it, which tests

399
00:21:28,253 --> 00:21:31,943
better predictions on tracking the
trajectories of the pixel point, which

400
00:21:31,943 --> 00:21:35,138
we are using to the, of the video to the.

401
00:21:37,643 --> 00:21:40,973
And that's why we try to use both the
models to evaluate the performance

402
00:21:41,033 --> 00:21:45,393
and eventually in both the models
using single frame per at a time

403
00:21:45,393 --> 00:21:46,833
to do the prediction does better.

404
00:21:46,923 --> 00:21:50,283
Because single frame instance, like
using two frames at a time, so it

405
00:21:50,283 --> 00:21:53,823
can predict from one frame number
one, to frame number two on how where

406
00:21:53,823 --> 00:21:55,743
the pixel has been actually shifted.

407
00:21:56,743 --> 00:21:58,913
And that's what the
performance graph looks like.

408
00:21:58,913 --> 00:22:03,443
And we were able to achieve a better
performance by using single frame

409
00:22:03,443 --> 00:22:05,433
predictions at the time in the model.

410
00:22:05,433 --> 00:22:07,893
So the loss was very much close to 4%.

411
00:22:07,893 --> 00:22:12,478
So where we were able to, 96 percentage of
the the data and the loss is basically not

412
00:22:12,478 --> 00:22:15,898
like a visible loss where we can, where
you will see black dots here and there.

413
00:22:15,898 --> 00:22:19,138
No, it's not like the visible, you
might still see the video working fine.

414
00:22:19,563 --> 00:22:23,853
But but in terms of the quality,
there's 4% reduction quality, but

415
00:22:23,853 --> 00:22:28,503
still, we were able to do that
compression way better by producing

416
00:22:28,503 --> 00:22:31,113
the size of the storage of by 80%.

417
00:22:31,513 --> 00:22:32,743
That's something really great.

418
00:22:32,748 --> 00:22:35,443
Around 84, 80 4%.

419
00:22:36,038 --> 00:22:37,658
With just 4% loss in data.

420
00:22:37,708 --> 00:22:39,328
That, that's a good cutoff over here.

421
00:22:39,328 --> 00:22:42,358
And this is more of like an
initial approach of using redundant

422
00:22:42,358 --> 00:22:46,358
concept over here to do the
storage using ML based approach.

423
00:22:47,648 --> 00:22:52,150
So just further approaches and
ideas for viewers who are listening

424
00:22:52,150 --> 00:22:55,753
to this we can try to use this
on multiple point trajectories.

425
00:22:56,588 --> 00:23:00,478
That's what we are trying to work on for
our next paper research paper where we

426
00:23:00,478 --> 00:23:04,688
are trying to improve this performance for
more complex videos where we try to put

427
00:23:04,688 --> 00:23:06,918
their trajectories on multiple directions.

428
00:23:07,398 --> 00:23:10,928
And and the another approach is
basically the object direction masking.

429
00:23:10,928 --> 00:23:15,308
This is basically works in places where,
let's say in a given frame there are

430
00:23:15,408 --> 00:23:17,838
a human, a dog, or any kind of object.

431
00:23:18,108 --> 00:23:20,808
It can mask all those objects
and understand the pixels in

432
00:23:20,808 --> 00:23:21,700
a very more very smooth way.

433
00:23:21,705 --> 00:23:26,318
Like it can put a mask on top of it
and I can try to identify the same mask

434
00:23:26,318 --> 00:23:29,678
or the same person, the second pixel,
and can eventually try to avoid those

435
00:23:29,678 --> 00:23:34,488
t pixels in theum frames by masking
those object, masking those objects,

436
00:23:34,908 --> 00:23:36,318
and then other approach similarity.

437
00:23:36,318 --> 00:23:36,558
Such.

438
00:23:36,558 --> 00:23:40,908
Similarity search metrics is basically
can be used where if you see any similar

439
00:23:40,908 --> 00:23:44,318
pixels which already available in the
previous frame compared to the new frame.

440
00:23:44,363 --> 00:23:48,623
At every pixel level using similarity
search metrics, using any kind of cosign

441
00:23:48,623 --> 00:23:51,923
similarities or any kind of dot product
similarities, we can try to see how

442
00:23:51,923 --> 00:23:56,243
close these pixels are and we can try
not to store those pixels, the subsequent

443
00:23:56,243 --> 00:24:00,233
frames so that we can reuse it and map
and reuse these pixels in all those

444
00:24:00,233 --> 00:24:01,823
places when we try the decompression.

445
00:24:02,123 --> 00:24:05,133
So these are some of the approaches
which we can try and open to anyone

446
00:24:05,133 --> 00:24:09,408
can give a try on this and try to
see if we can come up with a better

447
00:24:09,408 --> 00:24:10,878
approaches or better solutions.

448
00:24:11,433 --> 00:24:16,053
Yeah, these are the future approaches and
I'm I'm hoping machine learning and these

449
00:24:16,173 --> 00:24:22,053
AI models not only uses a lot of data to
train themselves, but also I hope that

450
00:24:22,053 --> 00:24:28,663
gives a scope for us to use AI to also
reduce, data storage because a lot of data

451
00:24:28,663 --> 00:24:31,903
storage in today's world is being used
to train these machine learning models.

452
00:24:31,903 --> 00:24:34,693
In return, I hope these machine
learning models can also contribute

453
00:24:34,693 --> 00:24:39,778
in a way where it can store things in
a optimal way and reduce cost for us.

454
00:24:39,878 --> 00:24:44,118
Yeah, so that's a that's a
good takeaway out of this.

455
00:24:44,158 --> 00:24:48,028
Talk that AI not only uses a lot
of data, but it can also help us

456
00:24:48,028 --> 00:24:52,248
optimize these usage of data, a
storage of the data, and this one such

457
00:24:52,248 --> 00:24:54,098
approach, which we tried and yeah.

458
00:24:54,103 --> 00:24:54,263
And.

459
00:24:55,053 --> 00:24:55,563
That's all.

460
00:24:55,563 --> 00:24:57,693
And these are some of the references.

461
00:24:57,693 --> 00:25:01,603
That's the research paper which
you can look into and repose there.

462
00:25:01,603 --> 00:25:05,483
And these are some of the other references
which we looked into to inspire from

463
00:25:05,483 --> 00:25:09,923
them and to work on these optimization
approaches using deep learning methods.

464
00:25:10,113 --> 00:25:13,208
Especially we use the deep
convolutional network methods over here.

465
00:25:13,388 --> 00:25:15,908
And that's what this video is all about.

466
00:25:16,118 --> 00:25:18,248
And I hope you all enjoyed my.

467
00:25:18,623 --> 00:25:21,893
Talk and feel free to reach out
to me if you have any questions.

468
00:25:21,893 --> 00:25:25,373
Would love to answer and thank you.

