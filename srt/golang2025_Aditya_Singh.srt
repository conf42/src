1
00:00:00,210 --> 00:00:00,929
Hi everyone.

2
00:00:01,150 --> 00:00:02,139
thanks for tuning in.

3
00:00:02,530 --> 00:00:07,340
we will talk about, the architectural
evolution that has happened in distributed

4
00:00:07,340 --> 00:00:10,280
training over the past decade, actually.

5
00:00:11,129 --> 00:00:14,324
so today we'll explore how distributed
training has evolved to keep up with

6
00:00:14,324 --> 00:00:16,199
the explosive growth in model size.

7
00:00:16,799 --> 00:00:20,189
The deep learning models have grown
from millions parameters to trillions

8
00:00:20,189 --> 00:00:23,640
of parameters, and as models have
scale, so have the challenges.

9
00:00:24,540 --> 00:00:29,685
So memory limits, communication, resource
usage, all of these have some issues.

10
00:00:30,599 --> 00:00:35,200
And there have been many things
that have been built to, get around

11
00:00:35,200 --> 00:00:36,580
those issues, fix those issues.

12
00:00:36,580 --> 00:00:41,210
We'll walk through some architectural
milestones, from parameter server

13
00:00:41,240 --> 00:00:45,320
to ring all reduce pipeline
parallelism and zero along the way.

14
00:00:45,320 --> 00:00:49,870
We'll see how these advances work
alongside optimizers, like Lars

15
00:00:49,870 --> 00:00:53,320
and Lamb, to train larger models
faster and more efficiently.

16
00:00:54,110 --> 00:00:54,920
let's begin.

17
00:00:55,220 --> 00:00:59,000
between 2012 and 2020
model sizes skyrocketed.

18
00:00:59,240 --> 00:01:01,880
A g single GPU couldn't just keep up.

19
00:01:02,520 --> 00:01:05,520
we needed distributor training,
splitting work across devices,

20
00:01:05,520 --> 00:01:06,900
but that created new problems.

21
00:01:07,400 --> 00:01:10,460
Early systems use data parallelism.

22
00:01:10,510 --> 00:01:13,450
each device persist a batch
and synced with others.

23
00:01:13,540 --> 00:01:18,010
Sounds simple, but syncing meant a lot
of communication, especially for big

24
00:01:18,010 --> 00:01:23,310
models as GPUs waited on each other
or the network efficiency tanked.

25
00:01:23,810 --> 00:01:25,790
And the other issue was memory.

26
00:01:25,820 --> 00:01:29,540
So as models grew, so did the need
to store parameters, gradients,

27
00:01:29,540 --> 00:01:34,370
and optimizer states that meant
memory often became the bottleneck.

28
00:01:34,940 --> 00:01:36,950
Even with multiple devices.

29
00:01:37,450 --> 00:01:42,970
So then came in the parameter server
architecture, and this was done, this

30
00:01:42,970 --> 00:01:45,130
came in to coordinate multiple workers.

31
00:01:45,160 --> 00:01:49,330
The parameter server emerged a central
node holding the model workers who would

32
00:01:49,330 --> 00:01:53,350
compute gradients and send them to the
server, which would then update the model.

33
00:01:53,600 --> 00:01:57,620
it had two sync modes, synchronous
and async in the synchronous when

34
00:01:57,620 --> 00:02:00,020
all workers wait and update together.

35
00:02:00,080 --> 00:02:01,070
Simple but slow.

36
00:02:01,699 --> 00:02:07,409
Asynchronous worker move at their own
PAs faster, but, it doesn't optimize as

37
00:02:07,409 --> 00:02:09,300
good and there are some inconsistencies.

38
00:02:10,149 --> 00:02:13,539
so while useful learning on the
centralized server became a choke

39
00:02:13,539 --> 00:02:17,559
point, and you guessed it because
there was a centralized server, As

40
00:02:17,559 --> 00:02:20,979
you scale to dozens or hundreds of
workers, the server got overloaded.

41
00:02:21,009 --> 00:02:24,279
Much like everything, trying to
exit a stadium through one door.

42
00:02:25,129 --> 00:02:27,769
plus it introduced a
single point of failure.

43
00:02:28,339 --> 00:02:29,299
So we scaled up.

44
00:02:29,329 --> 00:02:31,369
The architecture started to fall apart.

45
00:02:31,619 --> 00:02:34,194
then, early optimization challenges.

46
00:02:34,694 --> 00:02:39,614
Specifically stood out and, these
were, communication overhead.

47
00:02:39,914 --> 00:02:44,264
The frequent, syncing of massive
tensor over tensors overwhelm the

48
00:02:44,264 --> 00:02:46,674
bandwidth, the straggler effect.

49
00:02:46,764 --> 00:02:51,294
So in the synchronous setup, slow
workers held everyone back, like

50
00:02:51,294 --> 00:02:53,064
waiting for the last hiker in the group.

51
00:02:53,564 --> 00:02:55,274
And the third one, idle resources.

52
00:02:55,274 --> 00:02:58,884
So GPU spent time waiting
instead of computing, especially

53
00:02:58,884 --> 00:03:00,714
in mixed hardware setups.

54
00:03:01,404 --> 00:03:05,244
So we needed smart, smarter ways to
share data, memory and manage the

55
00:03:05,244 --> 00:03:07,224
memory and keep the hardware busy.

56
00:03:07,474 --> 00:03:10,919
then came in, ring reduced, which
was basically a better way to sing.

57
00:03:11,419 --> 00:03:15,979
To get the rid of the central server
bottleneck, we turn to ring all.

58
00:03:15,979 --> 00:03:21,649
Reduce and imagine arranging all GPUs in
a ring instead of syncing with a server.

59
00:03:21,649 --> 00:03:26,509
Each GPU talks only to its neighbors,
so gradients are passed around the

60
00:03:26,509 --> 00:03:28,639
ring gradually summed and shared.

61
00:03:29,339 --> 00:03:32,309
this, reduces network
congestion and scales well.

62
00:03:32,309 --> 00:03:34,019
Each node does equal work.

63
00:03:34,019 --> 00:03:35,129
No bottlenecks.

64
00:03:35,834 --> 00:03:39,014
It's like replacing a traffic
jam at one big intersection with

65
00:03:39,014 --> 00:03:40,664
a smooth flowing roundabout.

66
00:03:41,294 --> 00:03:43,214
How's that for an analogy?

67
00:03:43,994 --> 00:03:50,035
So the result, better bandwidth
use and near linear scaling, add

68
00:03:50,035 --> 00:03:53,935
more GPUs and training speeds up
proportionally and up to a point.

69
00:03:54,805 --> 00:03:55,404
Sound good?

70
00:03:56,274 --> 00:03:58,695
Next came in, pipeline parallelism.

71
00:03:58,945 --> 00:04:01,435
this was basically done
to fit bigger models.

72
00:04:01,535 --> 00:04:06,125
even with Ring already fitting huge
models on a single device was still tough.

73
00:04:06,625 --> 00:04:09,215
That's where, pipeline parallelism helps.

74
00:04:09,285 --> 00:04:14,305
we basically here in this approach,
split the model layers across devices.

75
00:04:14,305 --> 00:04:18,790
So let's say GPU one
handles layers, one to 10.

76
00:04:19,585 --> 00:04:23,125
GP U2 handles layers, 11 to 20 and so on.

77
00:04:23,175 --> 00:04:27,955
you take the model, you divide up
the layers and, hand them off to,

78
00:04:28,205 --> 00:04:30,605
different compute nodes or GPUs.

79
00:04:31,345 --> 00:04:32,545
it's like an assembly line.

80
00:04:32,545 --> 00:04:36,585
Data flows from one GPU to the next,
during forward and backward passes.

81
00:04:36,875 --> 00:04:39,395
we use, micro batches
to keep all GPUs busy.

82
00:04:39,795 --> 00:04:43,575
just like feeding the items down
the pipeline so no station is idle.

83
00:04:44,245 --> 00:04:49,175
this dramatically reduced, memory
per device, memory usage per

84
00:04:49,175 --> 00:04:52,565
device, letting a strain, much
larger models without hitting,

85
00:04:52,745 --> 00:04:54,935
memory walls or memory constraints.

86
00:04:55,875 --> 00:05:00,895
next came in the approach of, zero, this
was done to basically kill the redundancy.

87
00:05:01,145 --> 00:05:05,315
so traditional data parallelism
meant each GPU would keep full

88
00:05:05,315 --> 00:05:10,365
copies of the models of the model
gradients and optimizer, states.

89
00:05:10,425 --> 00:05:13,635
And it was, as you can
see, it was wasteful.

90
00:05:13,635 --> 00:05:18,405
I. So zero fixes this by splitting
those elements across devices.

91
00:05:18,495 --> 00:05:21,525
Now each GPU only stores
a slice of model and data.

92
00:05:22,025 --> 00:05:25,295
So it's like dividing a textbook
among students instead of

93
00:05:25,325 --> 00:05:26,855
everyone carrying all of them.

94
00:05:27,105 --> 00:05:32,564
this leads to eight x better memory
efficiency, and making trillion

95
00:05:32,564 --> 00:05:34,244
parameter models more trainable.

96
00:05:34,544 --> 00:05:36,974
So I would just go back to the analogy.

97
00:05:36,974 --> 00:05:38,204
It's like dividing.

98
00:05:38,849 --> 00:05:42,900
The textbooks for a day across students.

99
00:05:42,960 --> 00:05:46,080
So History book is brought by a student.

100
00:05:46,109 --> 00:05:47,909
The maths book is brought
by another student.

101
00:05:47,909 --> 00:05:51,479
Instead of all students carrying
all copies of all the books.

102
00:05:52,254 --> 00:05:56,034
each student copying his
or her copy of each book.

103
00:05:56,064 --> 00:06:00,984
So it led to a significant memory
efficiency and models that could be

104
00:06:00,984 --> 00:06:03,684
trained got significantly bigger in size.

105
00:06:04,404 --> 00:06:06,084
It also didn't hurt performance.

106
00:06:06,114 --> 00:06:09,984
it's smart enough to fetch or share data
as needed while keeping everything stable.

107
00:06:10,374 --> 00:06:12,014
So this was a good thing to come along.

108
00:06:12,514 --> 00:06:18,704
Now next what happened was, we enabled a
large scale batch, large batch training.

109
00:06:19,034 --> 00:06:20,834
Not large scale, but large batch training.

110
00:06:21,134 --> 00:06:26,974
So batch sizes, larger batch sizes,
increase efficiency, and faster training

111
00:06:27,215 --> 00:06:29,565
is achieved through, bigger bat sizes.

112
00:06:30,254 --> 00:06:32,835
but they can hurt convergence
if you're not careful.

113
00:06:32,835 --> 00:06:34,094
So there are two.

114
00:06:35,025 --> 00:06:39,674
Optimizers specifically designed for this,
these are larges and lamb larges are just

115
00:06:39,674 --> 00:06:43,814
learning rates for each layer based on
the weight to gradient ratio, so keeping

116
00:06:43,814 --> 00:06:50,715
layers balanced during training, and then
lamb builds on that with added adaptivity

117
00:06:50,715 --> 00:06:52,934
for deep models like transformers.

118
00:06:52,934 --> 00:06:57,254
So LAMB is specifically there to
help with transformer type models.

119
00:06:57,754 --> 00:07:02,525
Now these models make training
with 32,000 plus path size,

120
00:07:02,614 --> 00:07:08,015
practical without losing accuracy or
spending weeks on tuning, frankly.

121
00:07:08,914 --> 00:07:12,439
So they are key to keeping
training stable at scale.

122
00:07:12,939 --> 00:07:15,039
Next, communication optimization.

123
00:07:15,169 --> 00:07:15,529
basically.

124
00:07:16,029 --> 00:07:17,649
And less talk, more work.

125
00:07:18,279 --> 00:07:21,579
So even with Google Architectures,
communication is still costly.

126
00:07:22,079 --> 00:07:27,879
And so we, what, we did was compress
and quantize the gradients, gradient

127
00:07:27,879 --> 00:07:35,080
compression, was done and it helps, drop
or delays, communicating tiny values.

128
00:07:35,580 --> 00:07:39,749
And quantization uses fewer widths for
each number, like jpeg for gradient.

129
00:07:39,999 --> 00:07:43,149
These, all these, both of these
reduce communication without

130
00:07:43,149 --> 00:07:45,459
hurting accuracy if tuned properly.

131
00:07:46,209 --> 00:07:49,839
And the goal is to spend less time waiting
on data and more time training the model.

132
00:07:50,339 --> 00:07:50,879
Sounds good.

133
00:07:51,309 --> 00:07:51,939
what did we cover?

134
00:07:52,059 --> 00:07:53,679
We covered communication optimization.

135
00:07:53,679 --> 00:07:54,009
Yeah.

136
00:07:54,529 --> 00:07:56,909
putting it all together,
what's the impact?

137
00:07:57,179 --> 00:08:02,794
so the impact is that the training
time for the same size of model I. and

138
00:08:02,794 --> 00:08:07,624
the memory efficiency, the training
time has come down with each one

139
00:08:07,624 --> 00:08:11,554
of these milestones that we covered
starting from parameter servers.

140
00:08:12,404 --> 00:08:16,074
for this reference, the, the
model that has been used to

141
00:08:16,074 --> 00:08:17,814
compare the, is the same model.

142
00:08:17,844 --> 00:08:23,004
So if we just had the earlier milestone
of parameter server, and if this

143
00:08:23,004 --> 00:08:27,664
model is used, is trained using that
approach, it would, The training time

144
00:08:27,664 --> 00:08:31,565
would be basically 30 a month, and
the memory efficiency would be low.

145
00:08:31,565 --> 00:08:37,115
And with each one of these milestones,
the training time drops and the

146
00:08:37,115 --> 00:08:38,585
memory efficiency increases.

147
00:08:38,585 --> 00:08:41,735
These numbers are not hypothetical
thanks to these methods.

148
00:08:41,735 --> 00:08:46,925
Australian parameter models are
now a reality, and they're powering

149
00:08:46,925 --> 00:08:50,225
cutting at science language,
morals, and climate simulations.

150
00:08:50,405 --> 00:08:54,185
So distributor training is
no longer a limiting factor.

151
00:08:54,335 --> 00:08:55,655
It's a launchpad.

