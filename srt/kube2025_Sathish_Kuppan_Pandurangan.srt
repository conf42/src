1
00:00:00,500 --> 00:00:01,070
Hello everyone.

2
00:00:01,789 --> 00:00:03,319
I'm Satish Pangan.

3
00:00:03,950 --> 00:00:08,390
I'm working as a platform administrator
in Zurich North America, USA.

4
00:00:08,890 --> 00:00:13,450
I have overall of 15 years of
experience in data platform engineering

5
00:00:14,110 --> 00:00:18,490
with a specialization in enterprise
data integration on automation.

6
00:00:18,990 --> 00:00:23,670
Today I'm going to present the topic,
a driven data integration at scale,

7
00:00:24,480 --> 00:00:27,330
real time compliant, and cloud native.

8
00:00:27,830 --> 00:00:34,430
This topic, A driven data integration,
transforms enterprise data workflow

9
00:00:34,970 --> 00:00:41,559
across hybrid cloud environments
through this AEA power engine to

10
00:00:41,559 --> 00:00:44,460
perform orchestration and automation.

11
00:00:44,960 --> 00:00:45,440
That's,

12
00:00:45,940 --> 00:00:46,269
let's see.

13
00:00:46,945 --> 00:00:51,144
The evolution of cloud native data
architecture and the challenges, the

14
00:00:51,144 --> 00:00:53,334
traditional method, which we face now.

15
00:00:53,834 --> 00:00:58,435
So as the legacy systems struggle
with the real time operations, manual

16
00:00:58,435 --> 00:01:00,775
processes create several bottlenecks.

17
00:01:01,525 --> 00:01:05,365
And on top of it, the compliance
requirements add complexity to

18
00:01:05,365 --> 00:01:10,515
the existing data and workflows,
which challenging the process time.

19
00:01:11,055 --> 00:01:14,325
Kubernetes reshapes the
cloud NATO architecture.

20
00:01:14,825 --> 00:01:18,955
Which having a, which mounting
pressure to modernize their data.

21
00:01:19,455 --> 00:01:24,795
So this shift to containerized
environments demands a new

22
00:01:24,795 --> 00:01:29,085
approaches to data orchestration,
governance, and scalability.

23
00:01:29,445 --> 00:01:32,655
The traditional integration
platforms cannot scale and

24
00:01:32,655 --> 00:01:33,975
provide the required output.

25
00:01:34,475 --> 00:01:40,685
Let me introduce Claire, the A engine
behind intelligence data management.

26
00:01:41,185 --> 00:01:47,185
So Claire is provided by the
Informatica service provider, which

27
00:01:47,430 --> 00:01:51,955
represents the next generation of
data integration intelligence, which

28
00:01:51,955 --> 00:01:56,605
powers the intelligence data management
cloud IDMC, which is a cloud native

29
00:01:56,605 --> 00:01:58,225
solution provided by Informatica.

30
00:01:59,125 --> 00:02:03,685
This sophisticated platform transforms
how enterprise approaches data

31
00:02:03,685 --> 00:02:09,355
workflows across hybrid environments,
including AWS Azure, Google Cloud,

32
00:02:09,385 --> 00:02:11,215
and on-premises infrastructure.

33
00:02:11,715 --> 00:02:16,355
The SCL a driven approach fundamentally
changes the data integration, which

34
00:02:17,045 --> 00:02:21,725
instead of reactive maintenance
to a proactive optimization and

35
00:02:22,625 --> 00:02:25,625
intelligent automations that add
access to the business request.

36
00:02:25,970 --> 00:02:31,350
Now let's see how multi-cloud
integration can be seamlessly

37
00:02:31,350 --> 00:02:32,520
integrated with the cloud.

38
00:02:33,360 --> 00:02:38,630
Our supports AWS Amazon Web Service
of Azure on this, which supports

39
00:02:38,630 --> 00:02:43,520
native integrations with AWS
services, including S3 relational

40
00:02:43,555 --> 00:02:45,955
databases, Redshift and Lambda.

41
00:02:46,805 --> 00:02:51,065
And it also leverages AWS Native
Security and Compliance Framework

42
00:02:51,245 --> 00:02:54,875
while maintaining data governance
acceler Azure entire cloud stack.

43
00:02:55,375 --> 00:03:02,515
So coming to Microsoft Azure, the IT
integrates with Azure Data Factory for

44
00:03:02,515 --> 00:03:07,465
all the integration needs and Azure
Data Lake storage for all the storage

45
00:03:07,465 --> 00:03:10,375
needs and synapsis for analytics.

46
00:03:10,915 --> 00:03:11,515
WS.

47
00:03:12,015 --> 00:03:16,495
It also supports Azure SQL database
and it also seamlessly connects with

48
00:03:16,495 --> 00:03:20,035
the existing Microsoft ecosystem
while extending capabilities

49
00:03:20,035 --> 00:03:21,325
through a powered automations.

50
00:03:21,825 --> 00:03:23,955
Now let's talk about the on-premises ra.

51
00:03:24,525 --> 00:03:30,405
So it builds the legacy between the
on-premises and the cloud, modern

52
00:03:30,405 --> 00:03:35,265
cloud native infrastructure, and it
provides real time synchronization.

53
00:03:35,765 --> 00:03:40,205
It also seamlessly integrated our data
all across the hybrid environments.

54
00:03:40,705 --> 00:03:47,365
Now about the data integration on the
task automation, revolutionize traditional

55
00:03:47,365 --> 00:03:52,165
data integration by automating the
most time consuming and error pro task

56
00:03:52,945 --> 00:03:54,640
through machine learning algorithms.

57
00:03:55,140 --> 00:03:56,220
Pattern recognition.

58
00:03:56,490 --> 00:04:00,180
The platform identifies optimal
integration path, such as the

59
00:04:00,180 --> 00:04:04,200
transformation logic, automatic and
automatically generates mapping and

60
00:04:04,200 --> 00:04:09,180
workflow recommendations so that
it'll be easy for the developers to

61
00:04:09,180 --> 00:04:14,345
see where they need to look into the
logic, transformation or mappings.

62
00:04:14,845 --> 00:04:19,285
This intelligent automation extends
beyond simple task execution.

63
00:04:19,675 --> 00:04:23,755
To include, prevent predictive
maintenance, anomaly detection, and

64
00:04:23,755 --> 00:04:29,365
self-healing data pipelines that adapts
to ever changing business requirements,

65
00:04:29,755 --> 00:04:31,315
and also without a manual inter.

66
00:04:31,815 --> 00:04:34,515
Metadata discovery and
data classification.

67
00:04:35,475 --> 00:04:40,815
So Class A power metadata discovery
engine automatically scans and

68
00:04:40,815 --> 00:04:45,135
classifies data across all your entire
ecosystem, enterprise ecosystem.

69
00:04:46,035 --> 00:04:49,435
Using that ones the patent
recognition, semantic analysis.

70
00:04:50,255 --> 00:04:53,910
It identifies sensitive data
personally, personal information

71
00:04:53,910 --> 00:04:56,790
like PIA, and regulate.

72
00:04:57,255 --> 00:04:59,235
Content with unprecedented present.

73
00:05:00,045 --> 00:05:03,175
The system continuously
inter learns from the user.

74
00:05:03,175 --> 00:05:09,655
Feedback through a continuous monitoring
and regulate updates, ensuring that

75
00:05:09,655 --> 00:05:14,635
classification accuracy improves over time
while adapting to the new data types and

76
00:05:14,635 --> 00:05:16,675
sources as they imagine your organization.

77
00:05:17,175 --> 00:05:20,505
Now let's see how the
real-time integration and

78
00:05:20,505 --> 00:05:22,725
intelligent routing takes place.

79
00:05:23,225 --> 00:05:28,655
So when stream data ingestion the
process, it process high velocity data

80
00:05:29,645 --> 00:05:33,965
streams from several iot, internet
of things, devices, applications, and

81
00:05:33,965 --> 00:05:38,665
external APAs with minimal latency
and to intelligent parts selection.

82
00:05:38,725 --> 00:05:41,635
AI algorithm automatically
selects optimal route parts

83
00:05:41,640 --> 00:05:44,965
based on the network condition,
data sensitivity and performance

84
00:05:44,965 --> 00:05:47,125
requirement while enting the data.

85
00:05:47,625 --> 00:05:51,795
It also takes the real time
decision making, so enabling instant

86
00:05:51,795 --> 00:05:55,545
insights and rapid response to
the changing business conditions

87
00:05:55,995 --> 00:05:57,915
through continuous data processing.

88
00:05:58,415 --> 00:06:04,115
Let's see how these algorithms self-learn
and performs the anomaly detection.

89
00:06:04,985 --> 00:06:09,555
Claire has a self-learning capabilities
continuously monitor data quality

90
00:06:09,555 --> 00:06:14,835
patterns, identify any deviations before
any of the downstream is getting impacted.

91
00:06:15,465 --> 00:06:17,055
Are the processes getting impacted?

92
00:06:17,805 --> 00:06:22,245
The system builds comprehensive baselines
of normal data behavior and uses

93
00:06:22,245 --> 00:06:27,405
statistical models to flag any potential
issues when anomalies are detected.

94
00:06:28,245 --> 00:06:33,405
The A engine not only alerts
administrator, but also statist

95
00:06:33,410 --> 00:06:37,725
the character actions based on the
historical resolution patterns.

96
00:06:38,565 --> 00:06:41,205
As I mentioned earlier,
it has a continuous.

97
00:06:41,705 --> 00:06:45,395
Monitoring and pattern
identification methods.

98
00:06:46,055 --> 00:06:50,165
So it keeps, it is very easy to
check the historical resolution.

99
00:06:50,665 --> 00:06:55,525
This proactive approach to data
quality ensures consistent, reliable

100
00:06:55,525 --> 00:06:58,855
data across all integration points.

101
00:06:59,355 --> 00:07:02,325
Now, let's see how Kubernetes
native orchestration for

102
00:07:02,325 --> 00:07:04,305
containerized data services.

103
00:07:04,605 --> 00:07:07,515
So container orchestration native.

104
00:07:07,875 --> 00:07:11,535
Kubernetes integrations enables
automatic scaling, loading, BA load

105
00:07:11,535 --> 00:07:14,745
balancing, and resource optimization
for data processing workloads.

106
00:07:15,245 --> 00:07:20,245
So when considering the data processing
workloads, it performs like PO level

107
00:07:20,245 --> 00:07:25,135
scaling based on the data volume, and it
allocates the resources and optimizes it.

108
00:07:25,945 --> 00:07:29,820
And if there is any there of failures it
performs the failure recovery automation.

109
00:07:30,320 --> 00:07:33,020
And when it comes to micro
microservices architecture.

110
00:07:33,830 --> 00:07:38,410
So the data services that can be
independently deployed, scaled and

111
00:07:38,410 --> 00:07:43,119
maintained within container services
can be performed with service mesh

112
00:07:43,119 --> 00:07:47,735
integration, a PA gateway management
and secure breaker patterns.

113
00:07:48,235 --> 00:07:53,290
And it also has a cloud NATO
security integrated security controls

114
00:07:53,320 --> 00:07:56,515
that leverage Kubernetes native
features for comprehensive data

115
00:07:56,515 --> 00:08:00,985
protection, like network policies
and segmentation secret management.

116
00:08:01,705 --> 00:08:04,465
It also supports a
role-based access control.

117
00:08:04,965 --> 00:08:11,055
Now let's see how the A engine
dynamic auto optimizes the

118
00:08:11,055 --> 00:08:15,515
workload and cost management, the
predictive resource allocations.

119
00:08:15,605 --> 00:08:20,075
So Clare has a predictive algorithm
which analyzes the torical usage

120
00:08:20,285 --> 00:08:24,275
patterns, seasonal trends, and
business cycles to optimizes

121
00:08:24,275 --> 00:08:28,685
resource allocation automatically,
the system anticipates demands.

122
00:08:29,090 --> 00:08:31,610
Spikes and scales
infrastructure proactively.

123
00:08:32,300 --> 00:08:37,080
This intelligent approach to
workload management ensures optimal

124
00:08:37,650 --> 00:08:42,150
performance during peak load, while
minimizes the cost during when

125
00:08:42,150 --> 00:08:43,880
there is less much of a demand.

126
00:08:44,720 --> 00:08:49,100
Through it, it'll auto scale and
de provisioning the resources.

127
00:08:49,600 --> 00:08:55,990
Now, compliance automation for regulated
industries in highly regulated industries.

128
00:08:56,410 --> 00:09:00,490
Claire automates the complex processes
of governance, policy enforcement,

129
00:09:00,610 --> 00:09:02,020
and audit trial generation.

130
00:09:02,520 --> 00:09:06,570
The platform maintains comprehensive
lineage tracking automatically generates

131
00:09:06,570 --> 00:09:10,080
compliance reports, and ensures
that the data handling practices

132
00:09:10,080 --> 00:09:11,550
aligns with industry regulations.

133
00:09:11,605 --> 00:09:16,530
Through intelligent policy enforcements,
class monitors, data access

134
00:09:16,530 --> 00:09:20,640
patterns, flags, potential violations
in real time and automatically

135
00:09:20,640 --> 00:09:21,655
applies the remediation measures.

136
00:09:22,350 --> 00:09:26,280
This proactive approach to compliance
management significantly reduces the

137
00:09:27,000 --> 00:09:31,660
risk of regulatory violations while
streamlining audit pro processes.

138
00:09:32,160 --> 00:09:36,030
Now let's see the real world
successes in the regulated industry.

139
00:09:36,530 --> 00:09:41,290
Let's take, let's talk about
financial s services, transformation

140
00:09:41,295 --> 00:09:43,385
like a banking sector where.

141
00:09:43,885 --> 00:09:48,835
Major investment bankings implemented
class to automate a risk data

142
00:09:48,835 --> 00:09:54,205
aggregation across global trading
systems, achieving real-time regulatory

143
00:09:54,205 --> 00:09:56,655
records and reporting capabilities.

144
00:09:57,155 --> 00:10:03,575
So Clay Class A engine uses machine
learning to analyze millions of

145
00:10:03,575 --> 00:10:06,715
transactions across accounts rapidly.

146
00:10:07,105 --> 00:10:08,095
Spotting unusual.

147
00:10:09,040 --> 00:10:14,350
Spendings suspicious transfers or
deviations from typical customer behavior.

148
00:10:14,850 --> 00:10:18,930
This enables banks to flag the
potential of fraud transactions in

149
00:10:18,930 --> 00:10:25,290
real time, sending instant alerts
to investigation, and even trigger

150
00:10:25,350 --> 00:10:29,340
automated interventions like temporarily
placing a hold on the accounts.

151
00:10:29,840 --> 00:10:33,110
So for complaints, cloud
continuously scans activities.

152
00:10:34,100 --> 00:10:39,290
For regulatory breaches adapts to
policies and generate audit ready

153
00:10:39,290 --> 00:10:44,060
reports all in a real time in all
in real time keeping institution

154
00:10:44,060 --> 00:10:46,370
compliant without manual efforts.

155
00:10:46,870 --> 00:10:52,780
Now let's talk about the healthcare in
the large healthcare system change data.

156
00:10:53,185 --> 00:10:56,905
Integration to create unified
electronic health records while

157
00:10:56,905 --> 00:10:59,545
maintaining HIPAA or CG or CPDA.

158
00:10:59,935 --> 00:11:03,415
Compliance now come to
the insurance industry.

159
00:11:03,915 --> 00:11:09,165
So in the insurance industry, they are
leveraging CLA to automate cleaning.

160
00:11:09,735 --> 00:11:15,165
The A can automatically ingest,
classify, and route claims based

161
00:11:15,165 --> 00:11:17,745
on complexity, urgency, and risk.

162
00:11:18,645 --> 00:11:22,635
Enabling foster settlements and
tries on high priority cases.

163
00:11:23,595 --> 00:11:28,585
So this a powered engine analysis,
submitted documents, extract required

164
00:11:28,585 --> 00:11:32,365
information and cross reference it
against the policy historical claims

165
00:11:32,365 --> 00:11:37,285
to validate each submission, reducing
manual intervention and review time.

166
00:11:37,785 --> 00:11:42,705
So this a detects claims
anomalies such as language

167
00:11:42,705 --> 00:11:44,655
pattern associated with the fraud.

168
00:11:45,155 --> 00:11:50,045
Flagging suspicious cases for a deeper
inspection, which result in cost savings

169
00:11:50,045 --> 00:11:52,925
and reduce the payout on fraudulent camps.

170
00:11:53,885 --> 00:11:58,175
So these are some real world success
stories, which is already implemented

171
00:11:58,175 --> 00:12:00,635
in the industry using the cla.

172
00:12:01,135 --> 00:12:07,725
Now, let's talk about how DevOps and data
architecture can use this class A and

173
00:12:07,725 --> 00:12:09,735
how it can be leveraged for the future.

174
00:12:10,235 --> 00:12:12,705
Class A driven streaming pipeline.

175
00:12:12,765 --> 00:12:17,635
Unpredictable planning capabilities,
empower DevOps teams and data architects

176
00:12:17,635 --> 00:12:22,435
to build scalable, secure solutions
that adapts to changing business

177
00:12:22,935 --> 00:12:26,985
learning approach ensures that your data
integration strategy evolves with the

178
00:12:26,985 --> 00:12:31,680
organization, providing the foundation
for sustain the digital transformation

179
00:12:31,900 --> 00:12:34,245
in a containerized cloud Native neurons.

180
00:12:34,745 --> 00:12:36,275
Thank you so much for your time and.

181
00:12:36,775 --> 00:12:37,375
Have a good day.

182
00:12:38,065 --> 00:12:38,515
Thank you.

