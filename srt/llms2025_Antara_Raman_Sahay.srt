1
00:00:00,300 --> 00:00:04,350
With new models being released
regularly, do you ever wonder what

2
00:00:04,350 --> 00:00:06,150
goes into the making of these lum?

3
00:00:06,420 --> 00:00:08,550
What makes these models so powerful?

4
00:00:09,040 --> 00:00:09,729
hello everyone.

5
00:00:10,120 --> 00:00:15,789
This is Antra Zaha, and today we
will be deep diving into the secret

6
00:00:15,789 --> 00:00:17,860
sauce behind making these lums.

7
00:00:18,189 --> 00:00:22,180
We will be asking what refinement
does each new model makes, and

8
00:00:22,180 --> 00:00:26,530
discussing about one of the most common
practice used in all language model

9
00:00:26,949 --> 00:00:29,560
inclusion of code data, including.

10
00:00:29,710 --> 00:00:33,820
Of code data in the pre-training
data mixture, even for the models,

11
00:00:33,820 --> 00:00:38,410
not specifically designed for code,
has become a very common practice.

12
00:00:39,070 --> 00:00:44,050
For example, all the state of
the art model such as palm gofer

13
00:00:44,050 --> 00:00:48,580
bloom, which are not intended to
support code generation, includes

14
00:00:48,580 --> 00:00:50,470
a percentage of code data together.

15
00:00:50,920 --> 00:00:55,810
For instance, LAMA three has more
code data as compared to your LAMA

16
00:00:55,810 --> 00:00:58,570
two, which brings to a question.

17
00:00:59,140 --> 00:01:00,940
To code, hard not to code.

18
00:01:01,705 --> 00:01:06,655
So to analyze the impact of code, this
paper conducts extensive ablations and

19
00:01:06,655 --> 00:01:10,854
evaluates the language models along
different benchmarks, namely your natural

20
00:01:10,854 --> 00:01:15,565
needs, natural language reasoning task,
world knowledge task, and code ation.

21
00:01:16,065 --> 00:01:20,354
Before moving ahead, I want to give
you a concise information about the

22
00:01:20,354 --> 00:01:24,645
phases of training l lm. So it's
all starts with pre-training phase.

23
00:01:25,214 --> 00:01:28,065
In this phase, model learns
the language structure.

24
00:01:28,065 --> 00:01:30,015
Its semantics from the vast text data.

25
00:01:30,705 --> 00:01:34,275
This results in the broad
understanding of our natural language.

26
00:01:34,635 --> 00:01:37,035
Then the second step is
the fine tuning phase.

27
00:01:37,335 --> 00:01:40,665
So here the model adapts
for a specific task.

28
00:01:40,755 --> 00:01:43,455
This helps in specialized
task performance.

29
00:01:44,235 --> 00:01:45,945
Third is our continual pre-training.

30
00:01:46,215 --> 00:01:50,595
So in this new or updated
knowledge is added without losing

31
00:01:50,595 --> 00:01:53,595
the previous knowledge, which we
added in our pre-training phase.

32
00:01:54,315 --> 00:01:57,435
Then we have the fourth stage,
which is the cool down phase.

33
00:01:57,945 --> 00:01:59,775
So during cool down models.

34
00:02:00,015 --> 00:02:03,825
Learning rate is gradually
reduced and high quality data

35
00:02:03,825 --> 00:02:05,685
set has given more priority.

36
00:02:06,045 --> 00:02:09,285
Last, we have the evaluation
and fine tuning phase.

37
00:02:10,154 --> 00:02:15,495
So the best analog to relate to all
this would be the phases of learning

38
00:02:15,495 --> 00:02:20,905
in our own human learning, the lifelong
earned lessons from our parents.

39
00:02:20,905 --> 00:02:25,165
The school life is the pre-training
and the fine tuning the building

40
00:02:25,165 --> 00:02:26,575
block of our learning journey.

41
00:02:27,204 --> 00:02:29,215
College is continual pre-training.

42
00:02:29,605 --> 00:02:34,435
Knowledge is added or updated, but the
previous knowledge, which we have learned

43
00:02:34,435 --> 00:02:36,595
from our parents and school is not lost.

44
00:02:36,924 --> 00:02:41,935
What we further continuing in our pro
profession is the cool down phase here.

45
00:02:42,024 --> 00:02:44,515
The learning rate is gradually reduced.

46
00:02:44,545 --> 00:02:47,905
If we compare to our school
life and high quality data set,

47
00:02:47,905 --> 00:02:51,025
that is our specialization is
given much more preference.

48
00:02:51,595 --> 00:02:57,174
Lastly, we always fine tune and evaluate
our learning based on our experiences.

49
00:02:57,954 --> 00:02:59,575
So the overview of.

50
00:02:59,605 --> 00:03:05,364
This experimental framework is we
evaluate the impact of code by evaluating

51
00:03:05,364 --> 00:03:10,744
code proportions, state as which the
codes has been introduced, code quality

52
00:03:10,744 --> 00:03:13,024
and property, and the model scale.

53
00:03:13,714 --> 00:03:17,794
So in this experiment setup, pre-training
is considered into two phases.

54
00:03:17,854 --> 00:03:22,054
One is the continued pre-training, and
the second is the cool down phase here.

55
00:03:22,114 --> 00:03:26,854
Continued pre-training refers to a model
which is initialized from a pre-train

56
00:03:26,854 --> 00:03:29,224
model and trained for a specific.

57
00:03:29,254 --> 00:03:29,944
Token budget.

58
00:03:30,444 --> 00:03:34,614
So the first is your impact
of initializing using

59
00:03:34,674 --> 00:03:36,325
code pre-trained models.

60
00:03:36,825 --> 00:03:41,234
To understand this better, imagine
four robots whose trainers have

61
00:03:41,234 --> 00:03:43,125
trained them on different ways.

62
00:03:43,394 --> 00:03:47,864
First is your text language model, who
has been trained on text only data.

63
00:03:48,135 --> 00:03:50,445
Second, we have balanced language model.

64
00:03:50,864 --> 00:03:55,545
So here his trainer has trained him
on equal ratio of code and text data.

65
00:03:56,385 --> 00:03:58,785
We have balanced initialized text.

66
00:03:58,995 --> 00:04:02,754
So here the, this model, this
robot has been trained on equal

67
00:04:02,754 --> 00:04:07,314
ratio of code and text data and
continually pre-trained non text data.

68
00:04:07,944 --> 00:04:10,405
Then we have code initialized text.

69
00:04:10,674 --> 00:04:16,555
So what happens is here it is only co
trained on code data and continually

70
00:04:16,555 --> 00:04:18,295
being pre-trained on our text data.

71
00:04:18,845 --> 00:04:20,225
Let's see what happens.

72
00:04:20,285 --> 00:04:25,565
So during exam time for natural
reasoning, code text has the best

73
00:04:25,565 --> 00:04:31,195
performance followed by our balanced
text for no let's, world Knowledge Task

74
00:04:31,195 --> 00:04:33,115
balanced text has the best performance.

75
00:04:33,325 --> 00:04:37,765
And for a code generation balanced
only robot has the best performance.

76
00:04:38,265 --> 00:04:40,110
Then we have impact of scale.

77
00:04:41,070 --> 00:04:42,690
So we are, considering two.

78
00:04:43,395 --> 00:04:46,905
Which is four 70000002.8 billion robot.

79
00:04:47,295 --> 00:04:49,335
Continuing with our robot analog.

80
00:04:49,755 --> 00:04:53,265
Turns out bigger reports did
much better at everything.

81
00:04:53,765 --> 00:04:56,465
Then we have code data
proportion in pre-training.

82
00:04:56,885 --> 00:04:58,490
So here we read six models.

83
00:04:58,865 --> 00:05:05,645
For 200 billion token with increasing
core proportion, such as 0%, 25%,

84
00:05:05,645 --> 00:05:09,905
50%, 75%, 90%, and a hundred percent,

85
00:05:10,405 --> 00:05:13,520
and the remaining proportion
is spelled with the text data.

86
00:05:13,940 --> 00:05:17,960
So for natural reasoning, as the
code increases, the performance

87
00:05:17,960 --> 00:05:20,330
also increases the best performance.

88
00:05:20,630 --> 00:05:26,360
Is from 25% code data and 75%
text data, world knowledge.

89
00:05:26,360 --> 00:05:30,920
We see an inverse relationship with
increasing amount of code data.

90
00:05:31,790 --> 00:05:35,540
this is and reduced for code generation.

91
00:05:36,530 --> 00:05:41,720
Pest is no code model, so for the code
generation, linear increase in performance

92
00:05:41,720 --> 00:05:44,210
as the amount of code increases.

93
00:05:44,450 --> 00:05:47,720
So we get the best is the
one, is the best code model.

94
00:05:48,220 --> 00:05:51,070
Then we have code quality
and code property.

95
00:05:51,610 --> 00:05:55,270
Now we have been judging with different
proportion of code and text data

96
00:05:55,330 --> 00:05:57,100
with their different combination.

97
00:05:57,655 --> 00:05:59,725
But what about the type of code data?

98
00:05:59,785 --> 00:06:04,295
So if we compare four types of
code models, code, code model,

99
00:06:04,325 --> 00:06:08,135
which is trained on web-based code
data, then we have code markup in

100
00:06:08,135 --> 00:06:10,295
this 20% does the markup language.

101
00:06:10,295 --> 00:06:11,015
Markup language.

102
00:06:11,015 --> 00:06:12,074
Your is STML Cs.

103
00:06:12,640 --> 00:06:18,190
Then we have code that, which is in this
15% is code adjust data such as your

104
00:06:18,730 --> 00:06:21,430
GitHub issues stack Jupyter notebook.

105
00:06:21,850 --> 00:06:25,375
Then lastly, we have code
synthetic data in this 10% is.

106
00:06:25,450 --> 00:06:31,110
Synthetically generated code for different
tasks such as natural language reasoning,

107
00:06:31,110 --> 00:06:33,750
world knowledge to code generation.

108
00:06:33,870 --> 00:06:38,660
Code synthetic gives the best
performance code in pre raining code.

109
00:06:38,660 --> 00:06:42,320
On, in this paper, we
evaluate the impact of.

110
00:06:43,070 --> 00:06:47,900
Including the code and cool down,
comparing three model pre-train

111
00:06:47,900 --> 00:06:50,450
before cool down without code data.

112
00:06:50,630 --> 00:06:55,880
Cool down with 20% code data
and we find cool down with code

113
00:06:55,880 --> 00:06:57,890
data is the most beneficial.

114
00:06:58,390 --> 00:07:03,845
So the results for the natural language
reasoning task, adding 25% core data,

115
00:07:03,845 --> 00:07:06,665
boosted the natural reasoning by 8.2%.

116
00:07:07,235 --> 00:07:12,245
So the best word, the balance model
is comparatively best if we are

117
00:07:12,245 --> 00:07:16,235
going to go with natural language
reasoning task for World Knowledge

118
00:07:16,235 --> 00:07:21,365
Task Code and pre-train provide a 10.1
person boost in World Knowledge Task.

119
00:07:21,695 --> 00:07:22,265
So cool.

120
00:07:22,265 --> 00:07:25,685
Down with code was a very
crucial in world knowledge task.

121
00:07:26,185 --> 00:07:30,415
Then for code performance, code
heavy model outperform text model

122
00:07:30,445 --> 00:07:32,875
by 12 times in all the code tasks.

123
00:07:33,115 --> 00:07:37,885
So what balance model gave a strong
performance but lagged in, if we

124
00:07:37,885 --> 00:07:39,925
compare to your code only models?

125
00:07:40,795 --> 00:07:43,345
So the best recipe for code performance.

126
00:07:43,375 --> 00:07:44,545
For code benchmarks.

127
00:07:44,545 --> 00:07:49,705
Code only models were the clear winner
and balance text models were strong

128
00:07:49,705 --> 00:07:54,535
performing in natural language reasoning,
but lagged heavily in code then sens.

129
00:07:54,535 --> 00:07:59,545
The code data was a key differentiator
in boosting the code overall performance.

130
00:08:00,045 --> 00:08:05,505
So the key recommendation for pre-training
with code would be including a balanced

131
00:08:05,505 --> 00:08:08,655
mix of code and text data from the start.

132
00:08:08,985 --> 00:08:14,565
And using synthetic code data to improve
both the code as well as your natural

133
00:08:14,565 --> 00:08:19,575
language task, and prioritizing the
inclusion of coding cooled on phase two.

134
00:08:19,665 --> 00:08:21,615
Maximize the performance game.

135
00:08:22,115 --> 00:08:27,035
So the future research area could
be how we can see if we scale

136
00:08:27,035 --> 00:08:28,595
the synthetic code generation.

137
00:08:28,595 --> 00:08:31,505
Right now we are only
seeing two up to 20%.

138
00:08:31,745 --> 00:08:36,785
What if we increase it and we can
also explore as new models are being

139
00:08:36,785 --> 00:08:39,095
released with different training model.

140
00:08:39,485 --> 00:08:44,095
What if, we are doing any task
specific tuning and we can always

141
00:08:44,095 --> 00:08:46,105
introduce advanced, cool down phases.

142
00:08:47,020 --> 00:08:52,450
So the final takeaways from my side
would be code data significantly

143
00:08:52,450 --> 00:08:57,280
improves AI model across all data,
not just if you are talking about

144
00:08:57,280 --> 00:09:01,780
code specific data, such as your very
common example would be Jag Z pt.

145
00:09:02,165 --> 00:09:08,285
So Chad, GPT is heavily used by
programmers also as well as your general

146
00:09:08,285 --> 00:09:12,815
audience also, but it was not intended
to support your code related task.

147
00:09:12,815 --> 00:09:16,285
It was a general model to be
used by, the general public.

148
00:09:16,645 --> 00:09:22,105
But if we see, and not just Chad G pt,
any other model which has been released.

149
00:09:22,115 --> 00:09:26,195
For your general purpose task,
it includes some amount of

150
00:09:26,195 --> 00:09:27,995
code data in its training data.

151
00:09:28,835 --> 00:09:36,485
So balanced model with both text and code
are best for general task while code heavy

152
00:09:36,485 --> 00:09:38,975
models dominate the coding benchmark.

153
00:09:38,975 --> 00:09:44,040
So if you want to go, for a task
which require heavy codings, let's

154
00:09:44,040 --> 00:09:48,880
say you want in a specific area and
you want, in your coding related

155
00:09:48,880 --> 00:09:51,040
only, let's say you want to replace.

156
00:09:51,140 --> 00:09:56,620
create a software engineer or automate
any development pipelines, then my

157
00:09:56,620 --> 00:10:00,010
recommendation would be to go with
code heavy related models, which are

158
00:10:00,010 --> 00:10:01,750
released for these purposes only.

159
00:10:02,260 --> 00:10:06,220
And the cool down phase,
particularly with code, is critical

160
00:10:06,490 --> 00:10:08,140
for optimal model performance.

161
00:10:08,640 --> 00:10:09,875
That's it from my side.

162
00:10:09,905 --> 00:10:10,505
Thank you everyone.

