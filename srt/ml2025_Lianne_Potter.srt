1
00:00:00,180 --> 00:00:02,880
AI isn't just a tool,
it's a cultural force.

2
00:00:02,940 --> 00:00:05,850
And despite technological
advancements in ai, a constant

3
00:00:05,850 --> 00:00:08,070
remains, and that is the human.

4
00:00:08,580 --> 00:00:11,610
Now, this talk is about those humans
and the cultures they inhabit.

5
00:00:12,150 --> 00:00:15,810
So today, I'm gonna take you back
into the 1980s when the research

6
00:00:15,810 --> 00:00:17,160
in this book was being conducted.

7
00:00:17,370 --> 00:00:19,950
This is Diana e Forsyth and
anthropologists who for about eight

8
00:00:19,950 --> 00:00:23,340
years during the eighties and nineties,
there was a fly on the wall in four US

9
00:00:23,340 --> 00:00:25,710
labs observing AI research in action.

10
00:00:26,189 --> 00:00:29,099
Now these labs specialized in
creating medical AI tools like

11
00:00:29,099 --> 00:00:31,950
building systems to help migraine
sufferers cope with their symptoms.

12
00:00:32,970 --> 00:00:37,589
The question will be, has much changed
about the way we de design our AI systems

13
00:00:37,589 --> 00:00:41,550
40 years later, I. This is what we're
gonna explore through the lens of Diane.

14
00:00:42,360 --> 00:00:47,100
So Diana e exposed the power
dynamics at play in these labs,

15
00:00:47,100 --> 00:00:51,090
and she gave us insights around the
perception of what tech work is.

16
00:00:51,810 --> 00:00:56,100
Now, this talk isn't a historical
C, but it's going to be a warning

17
00:00:56,100 --> 00:01:00,269
that if we don't change the way we
build this technology soon, we may

18
00:01:00,269 --> 00:01:03,900
be doomed to make the same mistakes
that our grandparents of AI did.

19
00:01:04,950 --> 00:01:08,340
So together today, I hope we can
cover the key lessons for building a

20
00:01:08,340 --> 00:01:10,950
more inclusive, more responsible ai.

21
00:01:12,240 --> 00:01:14,820
A little note about anthropology, just
in case you're not familiar with it,

22
00:01:14,820 --> 00:01:17,010
it's not what Ross did from friends.

23
00:01:17,580 --> 00:01:20,970
He's a paleontologist, and it's
not what Indiana Jones did.

24
00:01:21,120 --> 00:01:23,520
He's an archeologist and a
pretty dubious one at that.

25
00:01:24,240 --> 00:01:26,970
Hey, Indy, I hear the British
Museum is still hiring.

26
00:01:28,199 --> 00:01:32,190
Now anthropology is the study of
human experience, so that's culture

27
00:01:32,250 --> 00:01:35,699
and communities, including all
the work and tools that we use.

28
00:01:35,970 --> 00:01:39,479
And then when you add tech
mix, this is called digital

29
00:01:39,479 --> 00:01:41,550
anthropology or techno anthropology.

30
00:01:42,030 --> 00:01:45,750
So digital anthropology is the
study of how we use and live with

31
00:01:45,750 --> 00:01:47,699
technology from a cultural perspective.

32
00:01:48,210 --> 00:01:49,890
Now, most of the work is done.

33
00:01:50,835 --> 00:01:54,494
When conducting anthropology is things
like field work, so observing and

34
00:01:54,494 --> 00:01:57,675
interacting with people to get a better
understanding of a person or a group.

35
00:01:58,005 --> 00:01:59,414
And this is what Diana did.

36
00:01:59,625 --> 00:02:02,445
So she would come into these AI
labs and she would observe them and

37
00:02:02,445 --> 00:02:04,065
interview with 'em while they worked.

38
00:02:04,785 --> 00:02:07,754
Oh, and if you can't cope with
not listening to an ai, talk

39
00:02:07,754 --> 00:02:09,014
about picture of the Terminator.

40
00:02:09,014 --> 00:02:14,415
I present you with an alternative
Lady Terminator, a 1980s Indonesia

41
00:02:14,415 --> 00:02:17,355
film, where the main character
starts off as an a anthropologist.

42
00:02:18,105 --> 00:02:19,665
Before turning into the Terminator.

43
00:02:19,665 --> 00:02:20,745
It's a fantastic film.

44
00:02:20,774 --> 00:02:21,615
Highly recommend it.

45
00:02:21,615 --> 00:02:23,084
It's absolutely crazy.

46
00:02:23,535 --> 00:02:25,725
Iman, in this day and
age, you speak of legends.

47
00:02:26,025 --> 00:02:27,045
I'm an anthropologist.

48
00:02:27,704 --> 00:02:28,005
Huh?

49
00:02:28,005 --> 00:02:29,670
Iman, in this day age,
you speak of legend.

50
00:02:29,850 --> 00:02:31,725
My ai an anthropologist, huh?

51
00:02:32,954 --> 00:02:34,725
In this day and age, you speak of legends.

52
00:02:34,725 --> 00:02:36,045
Anyways, I'm an anthrop anthropologist.

53
00:02:37,604 --> 00:02:41,055
As Diana's research developed through
those eight years in the lab, she

54
00:02:41,055 --> 00:02:44,625
became increasingly concerned about
the power dynamics influencing

55
00:02:44,625 --> 00:02:46,245
the end product of that ai.

56
00:02:46,635 --> 00:02:49,815
Namely that there was a lot of voices
missing from the creation of it.

57
00:02:50,385 --> 00:02:54,495
So she explained that in conducting
anthropological research in these labs,

58
00:02:54,765 --> 00:02:59,894
she was exposing key absent voices, as she
described this as speaking truth to power.

59
00:03:00,765 --> 00:03:05,385
It wasn't always well received as speaking
truth to power, however, so while her

60
00:03:05,385 --> 00:03:08,935
work earned her lots of accolades from
her fellow anthropologists and people

61
00:03:08,935 --> 00:03:13,705
working in academia, it ruffled a lot of
feathers amongst the AI researchers that

62
00:03:13,705 --> 00:03:16,255
she was observing in the lab as we'll.

63
00:03:16,255 --> 00:03:18,955
See, perhaps if they paid a bit
more attention to the observation

64
00:03:18,955 --> 00:03:22,615
she was making, maybe we'd
have better AI capabilities.

65
00:03:24,205 --> 00:03:27,665
Now from one anthropologist
to another I'm Leanne Potter.

66
00:03:27,665 --> 00:03:31,355
I'm your time traveling guide here
to connect Diana's work with the work

67
00:03:31,355 --> 00:03:33,154
we're doing today in AI creation.

68
00:03:34,114 --> 00:03:36,814
Aside from being an anthropologist,
I've also worked in tech for the

69
00:03:36,814 --> 00:03:40,864
last 10 years versus as a software
developer and then pivoted into

70
00:03:40,864 --> 00:03:45,304
cybersecurity, and I'm now currently
doing an MSC in AI and data science.

71
00:03:46,355 --> 00:03:51,515
So Diana was once told that those
who ca code shouldn't be in the Lao.

72
00:03:52,024 --> 00:03:54,964
Unfortunately, I think many people
in text still feel that way today,

73
00:03:55,325 --> 00:03:59,674
but the danger is as soon as you
exclude, you delete the social.

74
00:04:00,874 --> 00:04:03,214
But what does it mean
to delete the social?

75
00:04:03,654 --> 00:04:05,994
Humans are intricate beings.

76
00:04:06,414 --> 00:04:07,254
We're all layered.

77
00:04:07,254 --> 00:04:08,394
We have history and culture.

78
00:04:08,424 --> 00:04:10,015
We belong to multiple communities.

79
00:04:10,264 --> 00:04:12,574
There's the family communities,
our friendship groups that

80
00:04:12,574 --> 00:04:13,984
are local and national groups.

81
00:04:14,104 --> 00:04:15,784
And of course work.

82
00:04:15,934 --> 00:04:19,024
Going to work is a community
and a cultural event.

83
00:04:19,504 --> 00:04:23,194
So whenever we interact with
anything or anyone, we carry all

84
00:04:23,194 --> 00:04:25,145
this cultural tapestry around others.

85
00:04:25,534 --> 00:04:27,784
And sometimes our cultural
experiences align.

86
00:04:27,834 --> 00:04:29,214
And sometimes they clash.

87
00:04:29,694 --> 00:04:32,334
And this also influences how
we engage with technology.

88
00:04:32,904 --> 00:04:36,684
So to delete the social is to
remove what makes us human.

89
00:04:37,014 --> 00:04:40,824
And those creating the tools that are
part of the social fabric too, they

90
00:04:40,824 --> 00:04:43,914
have a significant impact on this,
even if sometimes they don't see it.

91
00:04:44,994 --> 00:04:48,384
So Diana explored how the
researcher's own assumptions were

92
00:04:48,384 --> 00:04:50,004
woven into the tools they created.

93
00:04:50,424 --> 00:04:52,134
But she asked them how
they viewed their work.

94
00:04:52,134 --> 00:04:57,294
Their answers were often very telling
there was doing AI and not doing ai.

95
00:04:58,669 --> 00:05:03,019
So to them, this was doing ai,
which was writing code, problem

96
00:05:03,019 --> 00:05:07,789
solving, building systems, but get an
anthropologist to observe for a while.

97
00:05:07,969 --> 00:05:09,919
Then you are presented with this picture.

98
00:05:11,149 --> 00:05:15,919
So this in order is what the AI
researchers actually did all day.

99
00:05:17,849 --> 00:05:19,414
I'll give you a moment
to look through that.

100
00:05:19,414 --> 00:05:24,764
So you can see it's there like lab
meetings research seminars, more meetings.

101
00:05:24,819 --> 00:05:26,019
It looks like my diary.

102
00:05:26,169 --> 00:05:28,840
At work, going to conferences.

103
00:05:29,169 --> 00:05:31,809
And then on, on the other
side in the black background.

104
00:05:31,809 --> 00:05:37,080
Teaching courses, taking courses,
writing papers doing admin.

105
00:05:37,469 --> 00:05:39,270
This is what they actually did all day.

106
00:05:40,515 --> 00:05:45,434
And as you can see, they were pretty
selective with what they called work

107
00:05:45,434 --> 00:05:49,275
because right at the very bottom, even
though they themselves prioritize,

108
00:05:49,275 --> 00:05:54,344
this is what they mainly do, the
thing they did least was writing code,

109
00:05:54,525 --> 00:05:56,534
building systems and solving problems.

110
00:05:57,885 --> 00:06:01,544
So all the social elements
of work were neglected.

111
00:06:02,895 --> 00:06:07,335
The meetings, the engaging with other
people, they didn't see that as.

112
00:06:08,114 --> 00:06:09,434
Doing real work.

113
00:06:10,244 --> 00:06:13,184
So what does this omission
mean for the social elements

114
00:06:13,184 --> 00:06:14,324
for the systems they've built?

115
00:06:14,924 --> 00:06:18,554
Now, typically when you don't
value something like being social,

116
00:06:18,614 --> 00:06:22,064
then you don't put any F into it,
or you can be really bad at it.

117
00:06:23,654 --> 00:06:28,564
There was no good for the very and
this is like no good for the very

118
00:06:28,564 --> 00:06:35,284
social tools that these technologies
will bring because these researchers.

119
00:06:35,974 --> 00:06:40,504
Became often very shy and introverted
and describing an interaction.

120
00:06:40,504 --> 00:06:45,904
She said, one expert even said, they
have dreadful communication skills

121
00:06:45,904 --> 00:06:51,544
and that the technical skills far
exceed their need to be social.

122
00:06:52,689 --> 00:06:55,599
In my experience, and probably in
yours too, we've all met technologists

123
00:06:55,599 --> 00:06:57,400
who prefer tech over people.

124
00:06:57,790 --> 00:07:01,840
But can you really succeed in delivering
good tech without good communication?

125
00:07:01,840 --> 00:07:05,725
I. Now Diana's insights suggest
that better communication could have

126
00:07:05,725 --> 00:07:09,475
prevented a lot of shelfware, and
we'll go into more of that later.

127
00:07:10,645 --> 00:07:15,895
So if these AI researchers
didn't see meetings as work,

128
00:07:15,895 --> 00:07:18,295
then what actually was work?

129
00:07:18,664 --> 00:07:21,544
It was hard science and
seeking universal truths.

130
00:07:21,755 --> 00:07:26,224
So anything outside the scientific
method wasn't valid for ai 'cause they

131
00:07:26,224 --> 00:07:28,955
were building quantifiable codifiable.

132
00:07:29,244 --> 00:07:32,275
Testable systems, believing only
technical solutions could solve

133
00:07:32,275 --> 00:07:37,794
problems, coding and building were the
only legitimate AI work in their eyes.

134
00:07:38,304 --> 00:07:42,445
With one research going researcher,
going so far to suggest that AI research

135
00:07:42,445 --> 00:07:46,314
is something that you do sitting alone
in front of the computer for our zone.

136
00:07:47,659 --> 00:07:50,509
So I say anything outside that
was considered pseudo work.

137
00:07:51,049 --> 00:07:51,259
Sorry.

138
00:07:51,379 --> 00:07:52,999
Meetings and discussions, for example.

139
00:07:53,489 --> 00:07:59,069
Disciplines like anthropology, psychology
philosophy were all dismissed as soft

140
00:07:59,069 --> 00:08:04,229
science or even unscientific with
no place in the lab's AI development

141
00:08:06,059 --> 00:08:11,069
with this mindset that made Diana an
anthropologist work very challenging.

142
00:08:11,429 --> 00:08:15,684
Despite being a highly educated doctor
herself, her work was seen as lesser.

143
00:08:16,094 --> 00:08:18,974
One lab member even described
her as a Dictaphone with legs.

144
00:08:19,604 --> 00:08:22,544
It wasn't easy being an anthropologist
in a technical space, and from

145
00:08:22,544 --> 00:08:24,104
my experience, it still isn't.

146
00:08:25,215 --> 00:08:29,924
So how does viewing their work as
a purely logical and scientific

147
00:08:29,924 --> 00:08:31,874
endeavor delete the social?

148
00:08:32,895 --> 00:08:33,794
It's twofold.

149
00:08:34,484 --> 00:08:39,164
They fail to recognize that their
work is inherently social and it

150
00:08:39,164 --> 00:08:43,064
takes place in a highly social
setting, which is the institution.

151
00:08:43,905 --> 00:08:44,865
Their worldview is.

152
00:08:45,480 --> 00:08:49,470
Decontextualize and idealize, they only
see their work for the aspects they

153
00:08:49,470 --> 00:08:52,859
enjoy most and ideal not the reality.

154
00:08:53,640 --> 00:08:57,930
Now, we all do this to an extent, but
this was skewing their worldview and in

155
00:08:57,930 --> 00:09:02,339
turn influenced the products they were
building because their approach raised

156
00:09:02,339 --> 00:09:07,440
the nuances of the human experience,
which is crucial for creating meaningful

157
00:09:07,440 --> 00:09:10,740
AI systems, which are supposed to
replicate the human experience.

158
00:09:11,879 --> 00:09:15,480
So the question is, if they don't value
the social elements of their own work,

159
00:09:16,020 --> 00:09:20,010
what's stopping them from raising the
social aspects of other people's work?

160
00:09:21,359 --> 00:09:22,619
Turns out not much.

161
00:09:22,619 --> 00:09:24,270
And that's exactly what happened.

162
00:09:24,720 --> 00:09:27,709
Now this brings up to the
section on power dynamics.

163
00:09:29,060 --> 00:09:33,079
Now, the researchers were mostly male,
white, and from privileged backgrounds.

164
00:09:33,439 --> 00:09:37,219
In the 1980s and nineties, they were
known as knowledge engineers and their

165
00:09:37,219 --> 00:09:39,949
aim was to duplicate human expertise.

166
00:09:41,105 --> 00:09:45,005
They saw knowledge as something to
extract, like a mineral or a disease

167
00:09:45,005 --> 00:09:47,825
tooth and make that machine readable.

168
00:09:48,125 --> 00:09:50,825
But how did this knowledge ac Accusa Accu?

169
00:09:51,485 --> 00:09:53,405
How did this knowledge acquisition work?

170
00:09:54,725 --> 00:09:59,015
Without today's masses amounts of
data, these researchers relied on very

171
00:09:59,015 --> 00:10:05,615
manual data extraction, which often was
delivered through face-to-face views.

172
00:10:07,219 --> 00:10:10,760
So these sessions, these interviews
would last for about one or two hours,

173
00:10:10,760 --> 00:10:12,560
occurring once or twice a fortnight.

174
00:10:12,859 --> 00:10:16,440
And they typically involved
a single expert coming in.

175
00:10:16,440 --> 00:10:19,380
So maybe a heart surgeon, for
example, which was often another

176
00:10:19,380 --> 00:10:23,760
white man in power whose expertise
was then taken at face value.

177
00:10:25,200 --> 00:10:30,450
They relied on one specialist to reflect
the knowledge base of the entire topic.

178
00:10:30,840 --> 00:10:33,900
So as a back to that heart surgeon
example, getting a heart surgery to

179
00:10:33,900 --> 00:10:36,900
tell you everything they know about
heart attacks and building a system

180
00:10:36,900 --> 00:10:40,950
from there, as you can imagine,
this opens up the potential of the

181
00:10:40,950 --> 00:10:43,320
knowledge gaps, but also a lot of bias.

182
00:10:44,370 --> 00:10:46,770
Researchers treated knowledge
as something that could be easy,

183
00:10:46,770 --> 00:10:48,930
accessible with direct questions.

184
00:10:49,020 --> 00:10:51,960
They weren't interested in what
really happened during heart surgery.

185
00:10:52,290 --> 00:10:55,260
They wanted to know what
heart surgery was by the book.

186
00:10:56,040 --> 00:10:59,580
Now Diana observed that even the
warmest of personalities would adopt

187
00:10:59,580 --> 00:11:03,840
a really cold style during this
knowledge of citation or interviews.

188
00:11:04,560 --> 00:11:07,850
And she would ask them, why
was em empathy switched off?

189
00:11:07,850 --> 00:11:08,930
When you're interviewing people?

190
00:11:08,990 --> 00:11:13,700
And one researcher replied, it just
seems like we're doing a technical task.

191
00:11:13,790 --> 00:11:15,195
There isn't room for empathy.

192
00:11:16,970 --> 00:11:20,360
The knowledge gathering process
for them was transactional.

193
00:11:20,540 --> 00:11:21,470
It wasn't collaborative.

194
00:11:21,740 --> 00:11:22,930
It was medical questioning.

195
00:11:23,575 --> 00:11:27,115
It was really task orientated,
treating the expert as a brain to

196
00:11:27,115 --> 00:11:29,095
be mind, not really to be engaged.

197
00:11:30,145 --> 00:11:33,205
As I said, they weren't often very
good at interviewing, and some of the

198
00:11:33,205 --> 00:11:37,435
experts that came in to give their
expertise often felt mistreated.

199
00:11:39,655 --> 00:11:42,025
One, this is a direct quote,
someone saying that they felt

200
00:11:42,025 --> 00:11:44,965
like they'd been treated like a
dog during the interview process.

201
00:11:46,255 --> 00:11:50,185
Using cold methods to gather
data from people was ineffective.

202
00:11:50,785 --> 00:11:53,485
The engineers blamed the experts
when they didn't get the desired

203
00:11:53,485 --> 00:11:57,205
knowledge as well, and they failed to
recognize that a lot of knowledge isn't

204
00:11:57,205 --> 00:11:59,545
universal or rule bound like coding.

205
00:12:00,835 --> 00:12:05,245
Humans are stable entities
to be codified in absolutes.

206
00:12:05,275 --> 00:12:06,115
We're just not.

207
00:12:06,985 --> 00:12:10,135
And today, much of the data is
great from the internet and it's

208
00:12:10,135 --> 00:12:14,115
sought by others and we know how
reliable online information can be.

209
00:12:15,345 --> 00:12:17,035
And then, obviously we
have to make a judgment.

210
00:12:17,035 --> 00:12:19,925
If you've seen the thumbs up, are
you happy with that response or not?

211
00:12:20,975 --> 00:12:25,925
It's very easy to train a model
with reinforcement learning to also

212
00:12:25,985 --> 00:12:32,375
give really rubbish answers because
the whole process of getting a good

213
00:12:32,375 --> 00:12:36,545
response out of AI or a suitable
response is incredibly subjective.

214
00:12:36,995 --> 00:12:40,760
Who am I to say that when I,
that what I've heard about.

215
00:12:41,855 --> 00:12:45,215
Or given information about heart
surgery is good enough or not

216
00:12:45,215 --> 00:12:46,475
good enough to go into the system.

217
00:12:46,475 --> 00:12:48,365
For example, who am I to click?

218
00:12:48,365 --> 00:12:49,445
Yes, well done.

219
00:12:49,445 --> 00:12:50,015
Gemini.

220
00:12:50,195 --> 00:12:51,515
Can you make sure you keep doing this?

221
00:12:52,205 --> 00:12:54,995
And then that influences
everyone else's experience.

222
00:12:55,805 --> 00:12:59,885
These activities in the ones Diana
observes are form, again, of deleting

223
00:12:59,915 --> 00:13:05,015
the social because it's making judgment
calls about whose knowledge counts.

224
00:13:05,630 --> 00:13:10,700
Creating a significant power dynamic,
a small group deciding what's important

225
00:13:10,700 --> 00:13:12,800
for us to know is a form of power.

226
00:13:13,820 --> 00:13:16,790
And these knowledge engineers
were very aware of that power.

227
00:13:17,390 --> 00:13:19,550
But their end users were not.

228
00:13:19,880 --> 00:13:22,460
So the doctor down the line who
was supposed to be using these AI

229
00:13:22,460 --> 00:13:27,410
tools didn't know that this whole
software, this AI software, was

230
00:13:27,410 --> 00:13:29,570
based on just a handful of sources.

231
00:13:29,570 --> 00:13:32,589
And really that was best
endeavors, a handful of sources.

232
00:13:32,829 --> 00:13:35,374
Usually it was, one
person or one textbook.

233
00:13:36,819 --> 00:13:40,089
The knowledge engineer's power
was invisible to the end user.

234
00:13:40,180 --> 00:13:43,359
And that is just like it is
today with black box models.

235
00:13:44,110 --> 00:13:48,850
Imagine building a medical diagnosis
tool today that's based on that kind of

236
00:13:48,850 --> 00:13:51,040
data set and that lack of transparency.

237
00:13:51,910 --> 00:13:53,680
But maybe we're not too far off.

238
00:13:54,189 --> 00:13:59,290
We might have a lot more data points, but
as the old others goes, shit in, shit out.

239
00:13:59,890 --> 00:14:02,470
We don't always know of these
models we're interacting with

240
00:14:02,470 --> 00:14:07,090
today are deleting the social until
there's a scandal, like the Amazon

241
00:14:07,569 --> 00:14:10,665
recruiting tool or many other cases.

242
00:14:11,874 --> 00:14:16,885
Now I think power is at its most insidious
when you can't see it or challenge it or

243
00:14:16,885 --> 00:14:20,545
when we're given a false sense of security
that it's working in our best interests.

244
00:14:21,385 --> 00:14:24,984
The knowledge engineers that Diana worked
with viewed knowledge in absolutes.

245
00:14:25,464 --> 00:14:27,954
They were very comfortable
with sweeping the perspective

246
00:14:27,954 --> 00:14:29,814
of others under the AI rug.

247
00:14:30,775 --> 00:14:34,944
Not only that, actual end users,
clinicians who would eventually be using

248
00:14:34,944 --> 00:14:39,895
these tools day in day out, were not
a part of the data collection process.

249
00:14:40,780 --> 00:14:45,370
Nurses, for example, were not considered
value expert sources for medical

250
00:14:45,370 --> 00:14:50,280
informatics According to these researchers
had absolute power in delineating who

251
00:14:50,280 --> 00:14:55,710
counted in the build process and spoiler,
it usually looked like them, came

252
00:14:55,710 --> 00:14:57,240
from the same schools and backgrounds.

253
00:14:57,240 --> 00:14:57,630
They did.

254
00:14:58,469 --> 00:15:03,060
So in doing so, they raised
various cultures, races, classes,

255
00:15:03,089 --> 00:15:04,829
and genders to name a few.

256
00:15:05,819 --> 00:15:06,329
Muting.

257
00:15:06,329 --> 00:15:09,449
These voices caused both
ethical and practical issues.

258
00:15:09,959 --> 00:15:14,610
Introducing challenges were still
contending with as we build AI today.

259
00:15:16,020 --> 00:15:18,839
They didn't see their power as a problem.

260
00:15:18,839 --> 00:15:24,150
However, the issue was a
problem with user acceptance.

261
00:15:25,515 --> 00:15:30,615
After the 1970s AI winter labs were
thriving with funding and possibilities

262
00:15:30,615 --> 00:15:35,655
build, building medical AI for clinicians
and patients with really few constraints.

263
00:15:35,655 --> 00:15:40,395
And it sounds perfect, but it's
not quite because when you have

264
00:15:40,395 --> 00:15:43,995
no constraints, you have no rails.

265
00:15:45,375 --> 00:15:48,645
You don't have to adhere
to any guidelines.

266
00:15:49,620 --> 00:15:53,220
And when you don't have any guardrails
or guidelines, you don't care about what

267
00:15:53,220 --> 00:15:57,420
you build because if the money's gonna
come in anyways in your head, it's all one

268
00:15:57,600 --> 00:15:59,910
step towards scientific mastery, right?

269
00:16:00,689 --> 00:16:01,890
Just keep breaking things.

270
00:16:01,890 --> 00:16:04,650
If the money's gonna keep flowing,
one, one of these things will land.

271
00:16:05,590 --> 00:16:11,050
The consequence of blue Sky building
was that they didn't care if the

272
00:16:11,050 --> 00:16:15,460
end users liked their products
or thought they were very useful.

273
00:16:15,955 --> 00:16:19,314
Because at the end of the day, their
main goal was that they were doing ai.

274
00:16:20,515 --> 00:16:23,575
As a result, nearly everything
that Diana witnessed in these

275
00:16:23,575 --> 00:16:28,195
labs that these researchers built
ended up completely untouched

276
00:16:30,655 --> 00:16:35,995
AI automation promises
today, as it did back then.

277
00:16:36,205 --> 00:16:41,465
Quicker diagnosis is lower costs,
but nobody want to use these tools.

278
00:16:43,055 --> 00:16:46,594
So when Diana asked about this,
why is all the tech that you know,

279
00:16:46,594 --> 00:16:50,145
this AI tech that they've been
building not being used, where is it?

280
00:16:50,535 --> 00:16:52,574
She was told it's all on the shelf.

281
00:16:53,505 --> 00:16:56,145
They had literally built shelfware.

282
00:16:57,645 --> 00:17:01,515
They were also looking in the wrong
place, is for answers to why this

283
00:17:01,515 --> 00:17:07,244
kept happening, or as Diana said,
they genuinely were baffled by him.

284
00:17:09,194 --> 00:17:12,974
She pointed out, but if the
physicians need these systems so

285
00:17:12,974 --> 00:17:16,005
much, why aren't they using them now?

286
00:17:16,005 --> 00:17:19,904
The problem was, is that these
systems just weren't seen as useful

287
00:17:21,255 --> 00:17:24,884
either the researchers weren't
building the right systems or

288
00:17:24,884 --> 00:17:26,565
not building the systems, right?

289
00:17:26,985 --> 00:17:30,645
So Diana spent a lot of time
trying to figure out why.

290
00:17:31,409 --> 00:17:35,159
She found that instead of
examining their own mistakes, the

291
00:17:35,159 --> 00:17:37,350
AI researchers blamed the user.

292
00:17:37,679 --> 00:17:41,639
They would call them things
like naive or computer phobic.

293
00:17:42,179 --> 00:17:46,259
It was an end user problem, not
a problem with the tool itself.

294
00:17:46,860 --> 00:17:51,750
So they believed because they
believed that these systems work fine

295
00:17:51,779 --> 00:17:53,250
if you just know how to use them.

296
00:17:54,360 --> 00:17:57,810
Diana pointed out to them, however,
that instead of blaming users, maybe

297
00:17:57,810 --> 00:18:00,530
they should, realize the problem
is with the systems themselves,

298
00:18:00,560 --> 00:18:01,969
not the people trying to use it.

299
00:18:03,169 --> 00:18:04,999
That suggestion was dismissed.

300
00:18:06,919 --> 00:18:11,509
So Diana observed that the researchers
blamed user acceptance instead of

301
00:18:11,870 --> 00:18:14,540
recognizing the unusable systems.

302
00:18:15,019 --> 00:18:18,889
And this is because of how they
perceived success and what success

303
00:18:18,889 --> 00:18:21,259
was, because success did for them.

304
00:18:21,664 --> 00:18:24,754
Wasn't tied to real life use usefulness.

305
00:18:25,354 --> 00:18:25,594
One.

306
00:18:25,594 --> 00:18:29,044
Scientists even claimed that
usefulness is not quantifiable.

307
00:18:29,794 --> 00:18:34,084
So seeking feedback from users
was rare and the users weren't

308
00:18:34,084 --> 00:18:35,644
observed using the systems.

309
00:18:37,455 --> 00:18:38,055
What do you have?

310
00:18:38,055 --> 00:18:38,415
Then?

311
00:18:39,315 --> 00:18:42,015
You have a system that's
going to remain on the shelf.

312
00:18:42,615 --> 00:18:46,305
Now, if you recall, I said earlier
that work is a social aspect.

313
00:18:46,335 --> 00:18:48,165
It's a cultural social event.

314
00:18:50,264 --> 00:18:52,004
It really is, and it's very nuanced.

315
00:18:52,004 --> 00:18:53,279
So yes, a Dr.

316
00:18:53,279 --> 00:18:58,215
May do heart surgery, but there is more to
their job than just doing heart surgery.

317
00:18:58,304 --> 00:19:01,215
They have meetings, they
have consultations, they see

318
00:19:01,215 --> 00:19:03,165
patients, they update records.

319
00:19:04,274 --> 00:19:06,885
None of this stuff was any of
interest to these researchers.

320
00:19:06,885 --> 00:19:13,995
However, despite the very activities,
creating barriers to making this actually

321
00:19:13,995 --> 00:19:15,915
a successful soft pace of software.

322
00:19:17,024 --> 00:19:18,495
Now you speak to a cian today.

323
00:19:18,495 --> 00:19:21,835
They don't wanna be bogged down
with steps in an IT process.

324
00:19:21,835 --> 00:19:22,975
They want to help patients.

325
00:19:23,635 --> 00:19:27,595
This is a disconnect between AI
designers and real world users

326
00:19:27,805 --> 00:19:30,265
deleting the social realities of work.

327
00:19:30,835 --> 00:19:36,925
Remember, data gathered was often a
clinical engagement, pun intended,

328
00:19:37,315 --> 00:19:39,965
between experts, medical question out.

329
00:19:41,810 --> 00:19:44,745
And, they rarely set foot
in clinical settings.

330
00:19:44,794 --> 00:19:47,850
One time I read in the book, she
asked a designer of one of these

331
00:19:47,850 --> 00:19:51,500
AI systems and she said, have you
actually visited a hospital site?

332
00:19:52,580 --> 00:19:54,139
'cause they were building
stuff in before this site.

333
00:19:54,139 --> 00:19:55,700
And she said, have you
been to go see them?

334
00:19:56,270 --> 00:20:00,560
And the researcher in astonishment said,
oh no I can't stand the site of blood.

335
00:20:02,614 --> 00:20:07,144
So the clinician's real job processes
weren't part of the design either, and AI

336
00:20:07,144 --> 00:20:11,434
teams rarely observed real life problems.

337
00:20:11,614 --> 00:20:16,444
The AI tools that were built on engineers
knowledge or a single user experts

338
00:20:16,474 --> 00:20:21,994
input, not specific data on in the
individuals who will actually be using

339
00:20:21,994 --> 00:20:23,614
the systems and facing these problems.

340
00:20:24,214 --> 00:20:27,514
Remember, humans aren't
like computer programs.

341
00:20:28,624 --> 00:20:33,364
But I work like, think about if you were
to describe your day job, you'd miss so

342
00:20:33,364 --> 00:20:38,195
many steps, would you be able to remember
that most of your day is meetings?

343
00:20:38,195 --> 00:20:43,425
Would if you're a coder now or someone
working in ai, do you say that you, you

344
00:20:43,425 --> 00:20:47,505
do more coding than going to meetings,
doing admin, little things like that.

345
00:20:48,195 --> 00:20:52,725
Now, remember the AI researchers in this
case, the ones that Diana was observing

346
00:20:52,725 --> 00:20:53,804
during the eighties and nineties.

347
00:20:54,614 --> 00:20:57,854
They can't even grasp the
realities of their own day jobs.

348
00:20:58,304 --> 00:21:02,235
So how are they going to imagine someone
else's that they've not even seen?

349
00:21:03,945 --> 00:21:07,425
Now, work, as we all know, doesn't
follow official procedures.

350
00:21:07,425 --> 00:21:10,334
We, I'm sure HR would love
them to, but they don't.

351
00:21:11,445 --> 00:21:13,034
And these systems.

352
00:21:13,439 --> 00:21:17,189
Miss the tactic knowledge and
like workarounds, and they lack

353
00:21:17,189 --> 00:21:19,620
accommodation for real life complexities.

354
00:21:20,490 --> 00:21:24,120
And as a result, knowledge falls off
a cliff, which is a term that the AI

355
00:21:24,120 --> 00:21:29,399
researchers used when their systems hit
limits and faced unanticipated situations.

356
00:21:29,909 --> 00:21:34,810
For example, one system suggests that a
male patient had an infection of post.

357
00:21:37,435 --> 00:21:41,615
That can only occur if in pregnant
women, so they forgot to code

358
00:21:41,615 --> 00:21:44,435
in that men don't get pregnant.

359
00:21:46,715 --> 00:21:51,875
So Diana's conclusion was that
these engineers could choose what

360
00:21:51,875 --> 00:21:55,685
goes into the knowledge base,
but focused on an ideal scenario.

361
00:21:55,895 --> 00:22:01,835
And this made their systems fragile,
just like they only focused on the

362
00:22:01,835 --> 00:22:03,455
ideal aspects of their own work.

363
00:22:03,515 --> 00:22:05,195
They didn't recognize that humans.

364
00:22:05,855 --> 00:22:06,845
And not typical.

365
00:22:07,355 --> 00:22:12,875
And as such, systems need to be designed
knowing that people will deviate from

366
00:22:12,905 --> 00:22:16,685
the ideal, will deviate from the script,
will deviate from the happy path.

367
00:22:18,335 --> 00:22:24,935
Doing AI in the end really meant
that this powerful group was able

368
00:22:24,935 --> 00:22:28,925
to delete the social and as a
result, limit this tool's potential.

369
00:22:29,435 --> 00:22:31,205
But what else were they deleting?

370
00:22:33,995 --> 00:22:38,595
Now the lab wasn't the labs that data
research weren't just made up of men.

371
00:22:38,645 --> 00:22:42,965
In the 1980s and nineties, women
made up 8% of the academic computer

372
00:22:42,965 --> 00:22:44,555
science profession in America.

373
00:22:45,125 --> 00:22:47,735
Today, it's 15% go progress.

374
00:22:48,965 --> 00:22:53,375
Women in the lab were often treated
as other in this male dominated field.

375
00:22:53,565 --> 00:22:56,685
And I experienced something very
similar on a podcast recently where

376
00:22:56,715 --> 00:23:02,745
they introduced me as Leanne Potter
female cybersecurity expert, to

377
00:23:02,745 --> 00:23:05,865
which I responded, do you refer
to your male guest that way?

378
00:23:06,975 --> 00:23:08,745
They quickly changed the introduction.

379
00:23:09,645 --> 00:23:14,205
Now, Diana observed that women in these
labs were seen as second class citizens

380
00:23:14,505 --> 00:23:16,935
and face teasing and sexual harassment.

381
00:23:17,535 --> 00:23:21,345
In one instance, a file was placed on a
female worker's desktop that played the

382
00:23:21,345 --> 00:23:24,765
orgasm scene from when Harry met Sally
on the loop and could not be stopped.

383
00:23:24,765 --> 00:23:28,155
When Diana asked the woman
about it how she felt, she just

384
00:23:28,155 --> 00:23:30,045
said, it's just male territory.

385
00:23:31,065 --> 00:23:33,135
When Diana highlighted this gender divide.

386
00:23:33,855 --> 00:23:37,185
Male lab workers often denied it or
dismissed their female colleagues

387
00:23:37,185 --> 00:23:38,475
as being overly sensitive.

388
00:23:39,255 --> 00:23:43,875
After all these scientists believe that
science was objective and culture free,

389
00:23:44,085 --> 00:23:46,545
arguing that science is gender neutral.

390
00:23:47,055 --> 00:23:50,475
Ergo, because they're scientists,
they obviously can't be sexist.

391
00:23:51,495 --> 00:23:54,885
Yeah, surely we've made some progress
in recognizing women as significant

392
00:23:54,885 --> 00:23:56,925
contributors to tech and the AI space.

393
00:23:56,975 --> 00:23:59,885
Here is a snippet that really
hit home from the book, and

394
00:23:59,885 --> 00:24:03,155
this is Diana commenting on
gender initiatives at the time.

395
00:24:03,155 --> 00:24:04,875
So this is probably early nineties.

396
00:24:06,135 --> 00:24:08,840
So she says, public efforts to
increase the proportion of women

397
00:24:08,840 --> 00:24:11,625
in computer science is often
treated as a pipeline issue.

398
00:24:12,990 --> 00:24:16,350
The idea is that encouraging more
girls and young women to study

399
00:24:16,350 --> 00:24:17,940
science will solve the problem.

400
00:24:18,540 --> 00:24:21,660
However, this overlooks the fact
that there are already women in this

401
00:24:21,660 --> 00:24:23,700
field who face unequal treatment,

402
00:24:24,700 --> 00:24:29,650
pipeline problem, getting girls
into tech and why women are

403
00:24:30,070 --> 00:24:31,420
leaving tech is the culture.

404
00:24:32,530 --> 00:24:38,170
How is this an issue still after
40 years, I. I would argue it

405
00:24:38,170 --> 00:24:39,550
is more than disappointing.

406
00:24:39,670 --> 00:24:40,990
It's actually dangerous.

407
00:24:42,580 --> 00:24:46,120
Nothing much has changed, and with
AI technology moving so fast, we

408
00:24:46,120 --> 00:24:49,270
don't have the luxury of another
40 years to get this right.

409
00:24:49,270 --> 00:24:50,560
So we need to act now.

410
00:24:52,150 --> 00:24:54,100
Now, Diana.

411
00:24:54,415 --> 00:24:59,215
The anthropologist tragically passed
away in 1997 during a hiking accident,

412
00:24:59,275 --> 00:25:02,455
and I feel like we are definitely
poorer for not seeing her work

413
00:25:02,455 --> 00:25:04,225
continued in today's AI landscape.

414
00:25:04,465 --> 00:25:06,685
I'm sure she would have
so much to say on that.

415
00:25:07,855 --> 00:25:11,935
We need anthropologists to help
AI researchers who focus solely

416
00:25:11,935 --> 00:25:15,655
on theoretical data by immersing
themselves in AI development.

417
00:25:15,655 --> 00:25:19,345
Anthropologists can provide crucial
insights into the social, cultural,

418
00:25:19,345 --> 00:25:20,515
and ethical dimensions of ai.

419
00:25:21,985 --> 00:25:25,825
This bridges the gap between the technical
aspects and human experiences and

420
00:25:25,825 --> 00:25:30,235
preventing harm because an outsider's
perspective is always valuable.

421
00:25:33,085 --> 00:25:37,435
Diana said this about her work as an
outside observer, I have a different

422
00:25:37,435 --> 00:25:39,445
perspective on the source of this problem.

423
00:25:40,105 --> 00:25:43,285
Medical information systems are
not being accepted because they

424
00:25:43,285 --> 00:25:47,065
do not meet the needs of the
consumers, and that the difficulty

425
00:25:47,065 --> 00:25:48,895
in turn results in the way that.

426
00:25:49,360 --> 00:25:53,350
In which the problem formulation,
system design, system building

427
00:25:53,560 --> 00:25:57,070
and evaluation are understood and
carried out in medical informatics.

428
00:25:58,780 --> 00:26:03,970
What I take away as an anthropologist
from Diana's Worth is that the main

429
00:26:03,970 --> 00:26:08,560
problem of user acceptance was that
those creating the AI systems valued

430
00:26:08,560 --> 00:26:13,330
the technology and its potential
more than the humans using it.

431
00:26:15,325 --> 00:26:18,895
Let's not keep making that mistake
because we stand at a crossroads.

432
00:26:19,165 --> 00:26:22,375
We should learn from the AI pioneers
of the eighties and nineties

433
00:26:22,705 --> 00:26:27,025
and embrace this exciting air
landscape with the human in mind.

434
00:26:27,715 --> 00:26:33,010
Otherwise risk not only AI harms and power
alignment, but also a lot of shelfware.

435
00:26:34,045 --> 00:26:36,355
Now I'm hoping, ensuring your insights.

436
00:26:37,245 --> 00:26:39,975
Showing her insights with you
today that you'd be tempted to

437
00:26:39,975 --> 00:26:41,115
pick up a copy of this book.

438
00:26:41,115 --> 00:26:43,515
This book is called
Studying Those Who Study Us.

439
00:26:44,545 --> 00:26:48,145
I was lucky enough to start this
project with the blessing of

440
00:26:48,175 --> 00:26:52,315
Diana's widow, who has been a great
source of support, insight, and

441
00:26:52,315 --> 00:26:54,145
this is a picture of me and him.

442
00:26:54,365 --> 00:26:57,515
He came to go see this talk we
presented for the first time.

443
00:27:00,110 --> 00:27:02,030
And to prove that it's
a really small world.

444
00:27:02,030 --> 00:27:05,770
After all, I was posting about the
work I was doing, do doing a deep

445
00:27:05,770 --> 00:27:09,820
dive into Diana's research and
her niece got in touch with me.

446
00:27:10,190 --> 00:27:14,120
She's a software developer and she's also
doing an AI safety course at the moment.

447
00:27:14,450 --> 00:27:17,240
So it's great to see that AI
family tradition continuing.

448
00:27:17,780 --> 00:27:22,570
So I had a call with Diana's niece,
and she was reading Diana's book at

449
00:27:22,570 --> 00:27:24,580
the time, and I said, you know what?

450
00:27:24,985 --> 00:27:29,875
In Diana's research really resonates
with her as a person working

451
00:27:29,875 --> 00:27:33,025
in tech, but also someone who's
witnessing this new AI revolution.

452
00:27:33,685 --> 00:27:39,595
And what she said was that the hypocrisy
is that a mostly male tech elite are

453
00:27:39,595 --> 00:27:43,825
the ones telling us how dangerous AI is,
but they're also the ones in control.

454
00:27:44,365 --> 00:27:49,615
The real danger then answer is
now isn't AI itself, but that

455
00:27:49,615 --> 00:27:54,205
the people who have the power to
build it have great power indeed.

456
00:27:55,795 --> 00:27:58,345
Diana said her work was about
speaking truth to power.

457
00:27:59,425 --> 00:28:00,385
Now it's your turn.

458
00:28:00,865 --> 00:28:01,975
Go build better.

459
00:28:02,005 --> 00:28:02,425
Ai.

460
00:28:03,415 --> 00:28:04,255
Thank you very much.

