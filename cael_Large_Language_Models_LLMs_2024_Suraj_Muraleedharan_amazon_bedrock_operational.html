<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Operational excellence for your LLMs using Amazon Bedrock</title>
    <meta name="description" content="One model, extra large, please!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Suraj%20Muraleedharan_llm.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Operational excellence for your LLMs using Amazon Bedrock | Conf42"/>
    <meta property="og:description" content="Customers are looking for a turnkey solution to integrate LLMs with their existing applications. This session will provide an overview of the operational considerations, architectural patterns, and governance controls needed to operate LLMs at scale."/>
    <meta property="og:url" content="https://conf42.com/Large_Language_Models_LLMs_2024_Suraj_Muraleedharan_amazon_bedrock_operational"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/QUANTUM2025_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Quantum Computing 2025
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2025-06-19
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/quantum2025" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2025">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2025">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2025">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2025">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2025">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2025">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2025">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2025">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2025">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/robotics2025">
                            Robotics
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2025">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2025">
                            Internet of Things (IoT)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="https://conf42.circle.so/">
                            <b>Community platform login</b>
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Join the community!
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #CCB87B;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Large Language Models (LLMs) 2024 - Online
            </h1>

            <h2 class="text-white">
              
              Content unlocked! Welcome to the community!
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- One model, extra large, please!
 -->
              <script>
                const event_date = new Date("2024-04-11T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2024-04-11T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "GSteZj7Y3MQ"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <a class="btn btn-info lift mb-3" data-bigpicture='{"ytSrc": "TQwxk0c4sh0"}' href="#">
                <i class="fe fe-eye me-2"></i>
                Watch Premiere
              </a>
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrBjR6ZR0g0LRq9Fp8c_4HrI" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Hello everyone, welcome to the session. Large language", "timestamp": "00:00:20,680", "timestamp_s": 20.0}, {"text": "models have captured the imagination for different software developers and customers", "timestamp": "00:00:24,006", "timestamp_s": 24.0}, {"text": "who are interested in now integrating those models into their day to day workflows.", "timestamp": "00:00:28,286", "timestamp_s": 28.0}, {"text": "Today I\u0027ll be talking about Amazon Bedrock, which is a managed service", "timestamp": "00:00:33,204", "timestamp_s": 33.0}, {"text": "via which you can have access to different foundation models", "timestamp": "00:00:37,484", "timestamp_s": 37.0}, {"text": "using a single API. We talk about the operational", "timestamp": "00:00:40,660", "timestamp_s": 40.0}, {"text": "excellence best practices that you would like to consider when", "timestamp": "00:00:44,324", "timestamp_s": 44.0}, {"text": "using Amazon bedrock. Customers are often", "timestamp": "00:00:47,644", "timestamp_s": 47.0}, {"text": "looking for turnkey solution which can help integrate these llms", "timestamp": "00:00:50,828", "timestamp_s": 50.0}, {"text": "into their existing applications. As part of", "timestamp": "00:00:54,644", "timestamp_s": 54.0}, {"text": "the session, we will talk about an introduction to the operational", "timestamp": "00:00:57,958", "timestamp_s": 57.0}, {"text": "excellence term from a well architected review perspective. We will", "timestamp": "00:01:01,430", "timestamp_s": 61.0}, {"text": "talk about the llms and then we will go in depth in the bedrock.", "timestamp": "00:01:05,230", "timestamp_s": 65.0}, {"text": "So let\u0027s start with operational excellence. If you look at the", "timestamp": "00:01:09,734", "timestamp_s": 69.0}, {"text": "well architected review that AWS recommends,", "timestamp": "00:01:13,286", "timestamp_s": 73.0}, {"text": "we have a pillar in there which says optional excellence.", "timestamp": "00:01:16,902", "timestamp_s": 76.0}, {"text": "Operational excellence is basically an ability to support the development", "timestamp": "00:01:20,334", "timestamp_s": 80.0}, {"text": "of your workloads, how to run your workloads, gain insight", "timestamp": "00:01:24,006", "timestamp_s": 84.0}, {"text": "into your workloads, and essentially improve your process and procedures", "timestamp": "00:01:28,200", "timestamp_s": 88.0}, {"text": "to deliver business value. Operational excellence is", "timestamp": "00:01:32,712", "timestamp_s": 92.0}, {"text": "a practice which you develop over the course of time.", "timestamp": "00:01:36,584", "timestamp_s": 96.0}, {"text": "It\u0027s not something which you will be able to get", "timestamp": "00:01:40,128", "timestamp_s": 100.0}, {"text": "done overnight or just by adopting a particular solution.", "timestamp": "00:01:43,824", "timestamp_s": 103.0}, {"text": "This is how your team is structured, this is how your people process and", "timestamp": "00:01:47,624", "timestamp_s": 107.0}, {"text": "the technologies working together. Now,", "timestamp": "00:01:51,586", "timestamp_s": 111.0}, {"text": "within operational excellence, there are different design principles", "timestamp": "00:01:54,522", "timestamp_s": 114.0}, {"text": "that you should be considering. One of the key", "timestamp": "00:01:58,106", "timestamp_s": 118.0}, {"text": "principles is performing operations as a code. We are all aware of infrastructure as", "timestamp": "00:02:01,490", "timestamp_s": 121.0}, {"text": "code and we are aware of different tools and technologies", "timestamp": "00:02:05,442", "timestamp_s": 125.0}, {"text": "which are there in the market. Try to adopt as much as possible", "timestamp": "00:02:08,994", "timestamp_s": 128.0}, {"text": "from an operations perspective so that you can start executing these as code", "timestamp": "00:02:13,010", "timestamp_s": 133.0}, {"text": "snippets, making frequent and small reversible changes.", "timestamp": "00:02:17,186", "timestamp_s": 137.0}, {"text": "That\u0027s another key aspect of the design principle for operational excellence,", "timestamp": "00:02:21,478", "timestamp_s": 141.0}, {"text": "refining your operational procedures, obviously anticipating", "timestamp": "00:02:26,454", "timestamp_s": 146.0}, {"text": "failures and learning from your operational failures,", "timestamp": "00:02:30,638", "timestamp_s": 150.0}, {"text": "and finally observability, which can help you get actionable insights.", "timestamp": "00:02:34,574", "timestamp_s": 154.0}, {"text": "With respect to llms. Let\u0027s say you\u0027re using Amazon Bedrock,", "timestamp": "00:02:39,654", "timestamp_s": 159.0}, {"text": "which is an API, access into different foundation models.", "timestamp": "00:02:43,262", "timestamp_s": 163.0}, {"text": "You still have to follow all of these design principles in", "timestamp": "00:02:46,594", "timestamp_s": 166.0}, {"text": "order to how to deploy the API. How do you start versioning", "timestamp": "00:02:49,858", "timestamp_s": 169.0}, {"text": "the API? How do you have the different operational procedures", "timestamp": "00:02:53,938", "timestamp_s": 173.0}, {"text": "working together? What kind of observability can be put in?", "timestamp": "00:02:57,930", "timestamp_s": 177.0}, {"text": "So these are some of the factors that we will be talking about as we", "timestamp": "00:03:01,378", "timestamp_s": 181.0}, {"text": "go more into the session. Now let\u0027s talk about some of", "timestamp": "00:03:04,898", "timestamp_s": 184.0}, {"text": "the key terms that we keep hearing day in, day out DevOps,", "timestamp": "00:03:08,362", "timestamp_s": 188.0}, {"text": "mlops and llmops.", "timestamp": "00:03:12,946", "timestamp_s": 192.0}, {"text": "DevOps as a term has been around for a pretty long time.", "timestamp": "00:03:15,834", "timestamp_s": 195.0}, {"text": "It\u0027s basically encouraging you to break down the silos,", "timestamp": "00:03:19,522", "timestamp_s": 199.0}, {"text": "start having the organizational and the functional separation removed", "timestamp": "00:03:22,858", "timestamp_s": 202.0}, {"text": "from the different teams, and have an ownership end to end. As to whatever", "timestamp": "00:03:26,362", "timestamp_s": 206.0}, {"text": "you are building, you are also running and supporting that code.", "timestamp": "00:03:30,242", "timestamp_s": 210.0}, {"text": "Mlogs is basically using the same set of processes and", "timestamp": "00:03:33,954", "timestamp_s": 213.0}, {"text": "people and technology best practices within the scheme", "timestamp": "00:03:38,534", "timestamp_s": 218.0}, {"text": "of machine learning solutions. So you consider DevOps", "timestamp": "00:03:41,750", "timestamp_s": 221.0}, {"text": "is something which you would often use for microservices written in Java,", "timestamp": "00:03:45,830", "timestamp_s": 225.0}, {"text": "Python or Golang. When you are trying to", "timestamp": "00:03:49,326", "timestamp_s": 229.0}, {"text": "use the same set of technology stack, but now trying", "timestamp": "00:03:52,534", "timestamp_s": 232.0}, {"text": "to solve a machine learning problem where you suddenly have a model,", "timestamp": "00:03:56,534", "timestamp_s": 236.0}, {"text": "you need to train the model, you need to have the inference of the model,", "timestamp": "00:04:00,158", "timestamp_s": 240.0}, {"text": "you need to have multi model endpoints.", "timestamp": "00:04:03,566", "timestamp_s": 243.0}, {"text": "You want to incorporate these practices into how this model is getting trained,", "timestamp": "00:04:07,582", "timestamp_s": 247.0}, {"text": "how it is getting deployed, how the approval process is going to be there,", "timestamp": "00:04:11,510", "timestamp_s": 251.0}, {"text": "and ultimately how the inference is going to be there, be it a real time", "timestamp": "00:04:15,118", "timestamp_s": 255.0}, {"text": "inference or a batch inference. So that\u0027s the mlops part in there.", "timestamp": "00:04:18,182", "timestamp_s": 258.0}, {"text": "What about llmops? So far, mlops is", "timestamp": "00:04:22,614", "timestamp_s": 262.0}, {"text": "mostly used for specific machine learning models which you have", "timestamp": "00:04:26,694", "timestamp_s": 266.0}, {"text": "created to solve a single task. With large language", "timestamp": "00:04:30,494", "timestamp_s": 270.0}, {"text": "models, you have a capability of using a single", "timestamp": "00:04:34,566", "timestamp_s": 274.0}, {"text": "foundation model to solve different types of tasks.", "timestamp": "00:04:37,686", "timestamp_s": 277.0}, {"text": "For example, a foundation model like something", "timestamp": "00:04:41,166", "timestamp_s": 281.0}, {"text": "which you would be having access via", "timestamp": "00:04:45,046", "timestamp_s": 285.0}, {"text": "the bedrock, you can use it for text summarization,", "timestamp": "00:04:48,198", "timestamp_s": 288.0}, {"text": "you can use it as a chatbot, you can use it for", "timestamp": "00:04:52,166", "timestamp_s": 292.0}, {"text": "question and answers. There are different business scenarios where", "timestamp": "00:04:55,582", "timestamp_s": 295.0}, {"text": "you can use these models. Hence the llmops as a term", "timestamp": "00:04:59,542", "timestamp_s": 299.0}, {"text": "is using that single model. That is", "timestamp": "00:05:03,474", "timestamp_s": 303.0}, {"text": "the foundation model for different aspects of your business.", "timestamp": "00:05:06,546", "timestamp_s": 306.0}, {"text": "So in all your best practices on the operational excellence which we spoke about", "timestamp": "00:05:10,114", "timestamp_s": 310.0}, {"text": "in the previous slide, they remain quite consistent.", "timestamp": "00:05:13,922", "timestamp_s": 313.0}, {"text": "But just the nature of these specific problems which you", "timestamp": "00:05:16,970", "timestamp_s": 316.0}, {"text": "are solving would be differing depending on the machine learning solutions", "timestamp": "00:05:20,138", "timestamp_s": 320.0}, {"text": "or the LLM solutions which you have. At its core,", "timestamp": "00:05:23,834", "timestamp_s": 323.0}, {"text": "every solution that you are creating is going to be talking about", "timestamp": "00:05:27,830", "timestamp_s": 327.0}, {"text": "people, process and technology.", "timestamp": "00:05:31,566", "timestamp_s": 331.0}, {"text": "Now we say MLops is basically productionization", "timestamp": "00:05:34,614", "timestamp_s": 334.0}, {"text": "of your ML solutions effectively. So let\u0027s say I deploy", "timestamp": "00:05:38,206", "timestamp_s": 338.0}, {"text": "a solution into production which has the model go through its own", "timestamp": "00:05:41,974", "timestamp_s": 341.0}, {"text": "set of training. Someone has provided an approval. Now it", "timestamp": "00:05:45,310", "timestamp_s": 345.0}, {"text": "is an inference, be it a batch inference or a real time inference.", "timestamp": "00:05:48,998", "timestamp_s": 348.0}, {"text": "There is a lot of overlap that happens with foundation model operations", "timestamp": "00:05:53,724", "timestamp_s": 353.0}, {"text": "such as generative AI solutions using text, image,", "timestamp": "00:05:58,252", "timestamp_s": 358.0}, {"text": "video and audio. And finally, when you talk about", "timestamp": "00:06:01,532", "timestamp_s": 361.0}, {"text": "llms, these are basically large language models which are using,", "timestamp": "00:06:05,436", "timestamp_s": 365.0}, {"text": "again for productionization. There are some of the attributes which would", "timestamp": "00:06:08,996", "timestamp_s": 368.0}, {"text": "change in terms of the metrics that", "timestamp": "00:06:12,436", "timestamp_s": 372.0}, {"text": "you\u0027re looking at, but the process more or less remains the same.", "timestamp": "00:06:16,028", "timestamp_s": 376.0}, {"text": "And then there are some more additional customizations that you would incorporate", "timestamp": "00:06:19,620", "timestamp_s": 379.0}, {"text": "into llms with say, rag or fine", "timestamp": "00:06:23,454", "timestamp_s": 383.0}, {"text": "tuning, etcetera. We\u0027ll talk about it in a, in a few slides.", "timestamp": "00:06:27,350", "timestamp_s": 387.0}, {"text": "At its core, it\u0027s still going to be people, process and technology,", "timestamp": "00:06:31,374", "timestamp_s": 391.0}, {"text": "and that overlap is going to be consistent,", "timestamp": "00:06:35,614", "timestamp_s": 395.0}, {"text": "irrespective of what kind of operational excellence you\u0027re going for,", "timestamp": "00:06:39,694", "timestamp_s": 399.0}, {"text": "be it you using the best technology that is available in the market.", "timestamp": "00:06:43,614", "timestamp_s": 403.0}, {"text": "You still need to train your people who can effectively use that technology", "timestamp": "00:06:47,182", "timestamp_s": 407.0}, {"text": "to derive the business value. You need to have", "timestamp": "00:06:51,452", "timestamp_s": 411.0}, {"text": "a close correlation between the team which is training your model,", "timestamp": "00:06:54,804", "timestamp_s": 414.0}, {"text": "or maybe fine tuning that model, and ultimately the consumers who", "timestamp": "00:06:58,212", "timestamp_s": 418.0}, {"text": "are going to be using that model. That aspect of people,", "timestamp": "00:07:01,676", "timestamp_s": 421.0}, {"text": "process and technology. And obviously, Conway\u0027s law doesn\u0027t change much", "timestamp": "00:07:05,508", "timestamp_s": 425.0}, {"text": "when it comes to deploying a software, whether you\u0027re deploying it using an", "timestamp": "00:07:09,788", "timestamp_s": 429.0}, {"text": "LLM or you\u0027re deploying it in the traditional sense", "timestamp": "00:07:13,372", "timestamp_s": 433.0}, {"text": "with microservices or even a monolith application.", "timestamp": "00:07:16,882", "timestamp_s": 436.0}, {"text": "Now, let\u0027s talk about foundation models. And the first thing that we want", "timestamp": "00:07:20,194", "timestamp_s": 440.0}, {"text": "to talk about is the model lifecycle. So in a", "timestamp": "00:07:23,466", "timestamp_s": 443.0}, {"text": "typical machine learning use case, you will have a model", "timestamp": "00:07:27,058", "timestamp_s": 447.0}, {"text": "lifecycle where you have a lot of data, and using that data now", "timestamp": "00:07:30,722", "timestamp_s": 450.0}, {"text": "you\u0027re going to go into a processing stage where you\u0027re processing", "timestamp": "00:07:34,130", "timestamp_s": 454.0}, {"text": "all of your information. That data has been labeled, maybe a supervised", "timestamp": "00:07:37,634", "timestamp_s": 457.0}, {"text": "learning or unsupervised learning, whatever is your choice algorithm that you\u0027re", "timestamp": "00:07:41,594", "timestamp_s": 461.0}, {"text": "doing. And then once the training has been done,", "timestamp": "00:07:44,922", "timestamp_s": 464.0}, {"text": "now you have a hyper parameter tuning that you\u0027re doing to ultimately", "timestamp": "00:07:48,006", "timestamp_s": 468.0}, {"text": "create a model. That model is going to go through the model validation,", "timestamp": "00:07:52,198", "timestamp_s": 472.0}, {"text": "testing, and once that model is ready,", "timestamp": "00:07:56,494", "timestamp_s": 476.0}, {"text": "you are going to be using for that specific task. That\u0027s the", "timestamp": "00:07:59,678", "timestamp_s": 479.0}, {"text": "important part here. You\u0027re going to be using for this specific task because", "timestamp": "00:08:02,918", "timestamp_s": 482.0}, {"text": "the model has been tuned and trained for that particular task.", "timestamp": "00:08:06,566", "timestamp_s": 486.0}, {"text": "Tomorrow you have new set of data, you\u0027re going to do an iteration,", "timestamp": "00:08:10,750", "timestamp_s": 490.0}, {"text": "and then you\u0027re going to do the training of the model and ultimately", "timestamp": "00:08:14,430", "timestamp_s": 494.0}, {"text": "redeploy the model once the requisite approvals are available.", "timestamp": "00:08:18,414", "timestamp_s": 498.0}, {"text": "So this is just one project. When it comes to", "timestamp": "00:08:22,294", "timestamp_s": 502.0}, {"text": "foundation models or large language", "timestamp": "00:08:26,294", "timestamp_s": 506.0}, {"text": "models, your data set is no longer just one data", "timestamp": "00:08:30,206", "timestamp_s": 510.0}, {"text": "set. You\u0027re training that model using every possible", "timestamp": "00:08:33,766", "timestamp_s": 513.0}, {"text": "data set. For example, the model which you", "timestamp": "00:08:37,174", "timestamp_s": 517.0}, {"text": "have from meta, the Lama models you are,", "timestamp": "00:08:40,764", "timestamp_s": 520.0}, {"text": "they have been trained, pre trained with large data sets,", "timestamp": "00:08:44,660", "timestamp_s": 524.0}, {"text": "70 billion parameters, etcetera. Once that", "timestamp": "00:08:48,164", "timestamp_s": 528.0}, {"text": "model has been made available. Now, from that foundation model,", "timestamp": "00:08:51,764", "timestamp_s": 531.0}, {"text": "you can either do a fine tuning if you\u0027re interested in doing that.", "timestamp": "00:08:56,428", "timestamp_s": 536.0}, {"text": "So that\u0027s the project b that you\u0027re looking at. But then from the", "timestamp": "00:08:59,812", "timestamp_s": 539.0}, {"text": "same foundation model, you can directly use it for some task specific", "timestamp": "00:09:03,340", "timestamp_s": 543.0}, {"text": "deployments. And then once you do a fine tuning or rag", "timestamp": "00:09:07,060", "timestamp_s": 547.0}, {"text": "or something else, you can use that same model for a different", "timestamp": "00:09:10,194", "timestamp_s": 550.0}, {"text": "use case. So that\u0027s the key difference here. You\u0027re using a single model", "timestamp": "00:09:13,994", "timestamp_s": 553.0}, {"text": "for different projects and different scenarios with some", "timestamp": "00:09:17,922", "timestamp_s": 557.0}, {"text": "alterations. And in the previous case you are having one model", "timestamp": "00:09:21,314", "timestamp_s": 561.0}, {"text": "for each of it. Now, with Llmops,", "timestamp": "00:09:24,874", "timestamp_s": 564.0}, {"text": "there can be different types of users that you\u0027re interacting with.", "timestamp": "00:09:28,034", "timestamp_s": 568.0}, {"text": "And basically I want to talk about the generative AI user", "timestamp": "00:09:31,642", "timestamp_s": 571.0}, {"text": "types and then the skills which are needed. Let\u0027s talk first about", "timestamp": "00:09:35,002", "timestamp_s": 575.0}, {"text": "the providers. You have a provider, let\u0027s say someone is building an", "timestamp": "00:09:38,582", "timestamp_s": 578.0}, {"text": "NLM from scratch. In this case we take a Lama model", "timestamp": "00:09:41,958", "timestamp_s": 581.0}, {"text": "which has been built from scratch, and that model can", "timestamp": "00:09:45,262", "timestamp_s": 585.0}, {"text": "be used for different use cases,", "timestamp": "00:09:49,110", "timestamp_s": 589.0}, {"text": "NLP, data science, model deployment, inference,", "timestamp": "00:09:52,254", "timestamp_s": 592.0}, {"text": "etcetera. So that\u0027s a provider. You have got a model from there internally,", "timestamp": "00:09:55,582", "timestamp_s": 595.0}, {"text": "your team can decide to have a fine tuning on those", "timestamp": "00:09:59,830", "timestamp_s": 599.0}, {"text": "models. So those are the people who are doing a fine tuning on the model", "timestamp": "00:10:03,134", "timestamp_s": 603.0}, {"text": "to fit custom requirements. Maybe you have a business specific data", "timestamp": "00:10:06,518", "timestamp_s": 606.0}, {"text": "which you want the model to be a little bit more aware of.", "timestamp": "00:10:10,008", "timestamp_s": 610.0}, {"text": "So you\u0027re going to be training that model, fine tuning that model,", "timestamp": "00:10:14,200", "timestamp_s": 614.0}, {"text": "using that business particular data, domain specific knowledge that you\u0027re", "timestamp": "00:10:17,744", "timestamp_s": 617.0}, {"text": "having. And then the third group is basically consumers.", "timestamp": "00:10:21,152", "timestamp_s": 621.0}, {"text": "They don\u0027t care about what the model has been trained", "timestamp": "00:10:24,304", "timestamp_s": 624.0}, {"text": "on or how it has been fine tuned. They are more like consumers who are", "timestamp": "00:10:27,608", "timestamp_s": 627.0}, {"text": "going to be just using that model. So consider someone who is using", "timestamp": "00:10:30,368", "timestamp_s": 630.0}, {"text": "your chatbot, someone who has asked a question. They would like to", "timestamp": "00:10:33,456", "timestamp_s": 633.0}, {"text": "get a response. They want to ensure that the response is not", "timestamp": "00:10:36,678", "timestamp_s": 636.0}, {"text": "having any kind of bias, toxicity,", "timestamp": "00:10:40,462", "timestamp_s": 640.0}, {"text": "or unrequired", "timestamp": "00:10:43,350", "timestamp_s": 643.0}, {"text": "responses that you will be getting. So they don\u0027t really have much of the", "timestamp": "00:10:48,134", "timestamp_s": 648.0}, {"text": "ML expertise, but they are basically using prompt engineering for getting", "timestamp": "00:10:51,510", "timestamp_s": 651.0}, {"text": "a response from the model. Be mindful.", "timestamp": "00:10:55,222", "timestamp_s": 655.0}, {"text": "These roles are transferable. So you can always have", "timestamp": "00:10:58,694", "timestamp_s": 658.0}, {"text": "a provider who\u0027s also becoming a tuner, and you can always", "timestamp": "00:11:02,040", "timestamp_s": 662.0}, {"text": "have a consumer who can also become a tuner.", "timestamp": "00:11:05,296", "timestamp_s": 665.0}, {"text": "Essentially, this is the entire spectrum that you\u0027re having,", "timestamp": "00:11:08,176", "timestamp_s": 668.0}, {"text": "where you have more on the MLov side, where the model", "timestamp": "00:11:11,040", "timestamp_s": 671.0}, {"text": "is getting created, and then you have the other end of the spectrum, where people", "timestamp": "00:11:14,248", "timestamp_s": 674.0}, {"text": "are directly incorporating this model into their day", "timestamp": "00:11:17,240", "timestamp_s": 677.0}, {"text": "to day workflows when it comes to LLM selection,", "timestamp": "00:11:20,600", "timestamp_s": 680.0}, {"text": "there are different aspects that you would want to consider.", "timestamp": "00:11:24,448", "timestamp_s": 684.0}, {"text": "The three key ones that we have seen from the field is the speed,", "timestamp": "00:11:27,844", "timestamp_s": 687.0}, {"text": "precision and the cost. Now, let\u0027s say you have three different llms", "timestamp": "00:11:31,388", "timestamp_s": 691.0}, {"text": "and each one of them is good at one particular thing. So let\u0027s say", "timestamp": "00:11:35,348", "timestamp_s": 695.0}, {"text": "we have LLM one, two and three. LLM one is", "timestamp": "00:11:39,172", "timestamp_s": 699.0}, {"text": "the best when it comes to precision. Two is the best when it comes to", "timestamp": "00:11:42,348", "timestamp_s": 702.0}, {"text": "cost, and we have one again", "timestamp": "00:11:45,572", "timestamp_s": 705.0}, {"text": "to be the best when it comes to speed. Depending on", "timestamp": "00:11:48,868", "timestamp_s": 708.0}, {"text": "the business scenario and the priority for a particular customer,", "timestamp": "00:11:52,260", "timestamp_s": 712.0}, {"text": "they can choose one of those llms. Some customers are", "timestamp": "00:11:56,606", "timestamp_s": 716.0}, {"text": "ready to sacrifice a bit of precision in", "timestamp": "00:12:00,702", "timestamp_s": 720.0}, {"text": "order to pick up a low cost LLM", "timestamp": "00:12:03,910", "timestamp_s": 723.0}, {"text": "because of the number of tokens that you\u0027ll be sending across and the large use", "timestamp": "00:12:07,422", "timestamp_s": 727.0}, {"text": "that you\u0027ll be having. You always want to have a cost effective solution in", "timestamp": "00:12:10,894", "timestamp_s": 730.0}, {"text": "terms of any software that you are deploying. Second is", "timestamp": "00:12:15,158", "timestamp_s": 735.0}, {"text": "the response time. There are different ways in which you can surely improve the response", "timestamp": "00:12:19,126", "timestamp_s": 739.0}, {"text": "times. Maybe you\u0027re using an embedded text,", "timestamp": "00:12:22,774", "timestamp_s": 742.0}, {"text": "embeddings or something like that with a vector database by which you can cache it", "timestamp": "00:12:25,946", "timestamp_s": 745.0}, {"text": "or you do something else. But essentially these are some of the key", "timestamp": "00:12:29,330", "timestamp_s": 749.0}, {"text": "factors, the three key factors that I have seen with different customers when", "timestamp": "00:12:32,826", "timestamp_s": 752.0}, {"text": "they are evaluating llms. And obviously this is a summarization of what", "timestamp": "00:12:36,850", "timestamp_s": 756.0}, {"text": "I just spoke about, which is LLM one, two and three,", "timestamp": "00:12:40,698", "timestamp_s": 760.0}, {"text": "how they compare. And then it\u0027s up to the customer how they want to", "timestamp": "00:12:43,714", "timestamp_s": 763.0}, {"text": "pick up a particular LLM and what they want to use it against.", "timestamp": "00:12:47,594", "timestamp_s": 767.0}, {"text": "Let\u0027s talk about customization. When it comes to customization of the llms,", "timestamp": "00:12:51,414", "timestamp_s": 771.0}, {"text": "there are four different ways in which I have seen the customers", "timestamp": "00:12:56,494", "timestamp_s": 776.0}, {"text": "to be using the llms. And one of the most common use", "timestamp": "00:13:00,150", "timestamp_s": 780.0}, {"text": "cases that they have is prompt engineering. So that\u0027s when you are", "timestamp": "00:13:04,134", "timestamp_s": 784.0}, {"text": "sending a request to the llms. For example, you\u0027re using an anthropic cloud", "timestamp": "00:13:07,966", "timestamp_s": 787.0}, {"text": "model on Amazon Bedrock. You are going to be using one of", "timestamp": "00:13:13,566", "timestamp_s": 793.0}, {"text": "the playgrounds and just send a request and ask for.", "timestamp": "00:13:16,806", "timestamp_s": 796.0}, {"text": "Give me details of when was the", "timestamp": "00:13:19,918", "timestamp_s": 799.0}, {"text": "last major incident which has happened in software", "timestamp": "00:13:23,404", "timestamp_s": 803.0}, {"text": "engineering around the best practices or something like that. So that\u0027s", "timestamp": "00:13:27,228", "timestamp_s": 807.0}, {"text": "a problem, engineering. Just asking a question. You\u0027re expecting a response from the LLC.", "timestamp": "00:13:30,588", "timestamp_s": 810.0}, {"text": "A more nuanced one is a retrieval augmented generation, which is", "timestamp": "00:13:35,340", "timestamp_s": 815.0}, {"text": "Rag, where you are able to use", "timestamp": "00:13:39,068", "timestamp_s": 819.0}, {"text": "Rag, which as a better solution and as a better cost", "timestamp": "00:13:43,452", "timestamp_s": 823.0}, {"text": "benefit, and you can use it for customizing your", "timestamp": "00:13:47,300", "timestamp_s": 827.0}, {"text": "llms. Using rag then comes fine tuning,", "timestamp": "00:13:51,274", "timestamp_s": 831.0}, {"text": "which is more time consuming, it is more complex.", "timestamp": "00:13:55,266", "timestamp_s": 835.0}, {"text": "There is a lot of data and other things which would be needed.", "timestamp": "00:13:58,882", "timestamp_s": 838.0}, {"text": "And compared to rag, fine tuning is a special", "timestamp": "00:14:01,866", "timestamp_s": 841.0}, {"text": "case. I would say if you really want to have that level of", "timestamp": "00:14:05,338", "timestamp_s": 845.0}, {"text": "control over the responses, then maybe you can think of fine", "timestamp": "00:14:08,978", "timestamp_s": 848.0}, {"text": "tuning. And the last would be continued pre training where you", "timestamp": "00:14:12,930", "timestamp_s": 852.0}, {"text": "are essentially loading the model and customizing", "timestamp": "00:14:16,334", "timestamp_s": 856.0}, {"text": "it way more. And obviously the complexity increases as you go", "timestamp": "00:14:20,174", "timestamp_s": 860.0}, {"text": "from prompt engineering to rack to fine tuning to", "timestamp": "00:14:24,174", "timestamp_s": 864.0}, {"text": "ultimately continued pre training. One of", "timestamp": "00:14:27,790", "timestamp_s": 867.0}, {"text": "the most common cases of a rush of", "timestamp": "00:14:31,406", "timestamp_s": 871.0}, {"text": "LLMs that has been seen is everyone tries to", "timestamp": "00:14:35,334", "timestamp_s": 875.0}, {"text": "start doing fine tuning, thinking that the LLMs can be made", "timestamp": "00:14:38,774", "timestamp_s": 878.0}, {"text": "aware of specific knowledge and facts about the organization\u0027s", "timestamp": "00:14:42,730", "timestamp_s": 882.0}, {"text": "code base or domain knowledge, etcetera. What has been observed", "timestamp": "00:14:46,706", "timestamp_s": 886.0}, {"text": "is in majority of the cases, rag is good enough.", "timestamp": "00:14:50,802", "timestamp_s": 890.0}, {"text": "It offers a better solution. It is more cost effective", "timestamp": "00:14:54,994", "timestamp_s": 894.0}, {"text": "from in terms of cost benefit ratio between rag", "timestamp": "00:14:59,026", "timestamp_s": 899.0}, {"text": "and fine tuning. And fine tuning requires considerably more computational", "timestamp": "00:15:02,234", "timestamp_s": 902.0}, {"text": "resources and expertise. It introduces even more challenges around", "timestamp": "00:15:06,922", "timestamp_s": 906.0}, {"text": "sensitivity and the proprietary data than rag.", "timestamp": "00:15:11,240", "timestamp_s": 911.0}, {"text": "And there\u0027s obviously the risk of underfitting or overfitting if you", "timestamp": "00:15:15,512", "timestamp_s": 915.0}, {"text": "don\u0027t have enough data which is available for fine", "timestamp": "00:15:19,600", "timestamp_s": 919.0}, {"text": "tuning. So do have a very clear benchmarking", "timestamp": "00:15:22,920", "timestamp_s": 922.0}, {"text": "to see how your model is performing with prompt engineering versus", "timestamp": "00:15:28,840", "timestamp_s": 928.0}, {"text": "rag. And then think about whether fine tuning", "timestamp": "00:15:32,880", "timestamp_s": 932.0}, {"text": "is the right solution that you want to go for without much", "timestamp": "00:15:36,572", "timestamp_s": 936.0}, {"text": "evaluation. You may be jumping into a", "timestamp": "00:15:40,244", "timestamp_s": 940.0}, {"text": "technology solution, but which may be a much more difficult thing to manage", "timestamp": "00:15:43,524", "timestamp_s": 943.0}, {"text": "in the long term. Now customizing, now here we", "timestamp": "00:15:47,348", "timestamp_s": 947.0}, {"text": "talk about customizing the business responses. So what\u0027s really going", "timestamp": "00:15:50,868", "timestamp_s": 950.0}, {"text": "to help drive your business in generative AI is what\u0027s important", "timestamp": "00:15:54,548", "timestamp_s": 954.0}, {"text": "for your customers, what\u0027s important for your products,", "timestamp": "00:15:58,452", "timestamp_s": 958.0}, {"text": "which you\u0027re creating, and how you go about that. And you can", "timestamp": "00:16:01,722", "timestamp_s": 961.0}, {"text": "leverage different mechanisms here. And this is where fine", "timestamp": "00:16:05,514", "timestamp_s": 965.0}, {"text": "tuning and continued pre training comes into picture.", "timestamp": "00:16:09,442", "timestamp_s": 969.0}, {"text": "You talk about the purpose, it\u0027s basically maximizing the accuracy of", "timestamp": "00:16:13,274", "timestamp_s": 973.0}, {"text": "the specific tasks that you\u0027re having.", "timestamp": "00:16:16,858", "timestamp_s": 976.0}, {"text": "And we have comparatively smaller number of", "timestamp": "00:16:19,514", "timestamp_s": 979.0}, {"text": "label data. But then when it comes to continued pre", "timestamp": "00:16:23,114", "timestamp_s": 983.0}, {"text": "training, that\u0027s where you want to maintain the model for", "timestamp": "00:16:26,894", "timestamp_s": 986.0}, {"text": "a longer duration on your specific domain.", "timestamp": "00:16:30,622", "timestamp_s": 990.0}, {"text": "That is hyper customizations and large number of unlabeled data", "timestamp": "00:16:33,686", "timestamp_s": 993.0}, {"text": "sets that you will be using. Now, as I mentioned before,", "timestamp": "00:16:37,574", "timestamp_s": 997.0}, {"text": "Amazon Bedrock can help remove the heavy lifting for these", "timestamp": "00:16:41,494", "timestamp_s": 1001.0}, {"text": "kind of model customization process. But be", "timestamp": "00:16:44,774", "timestamp_s": 1004.0}, {"text": "very clear on your use case as to", "timestamp": "00:16:48,254", "timestamp_s": 1008.0}, {"text": "when you would be using a rag versus a fine tuning or prompt", "timestamp": "00:16:51,902", "timestamp_s": 1011.0}, {"text": "engineering, and why you would want to use a more complex customization", "timestamp": "00:16:55,446", "timestamp_s": 1015.0}, {"text": "than the one that you\u0027re getting. So without that clarity at a business", "timestamp": "00:16:59,704", "timestamp_s": 1019.0}, {"text": "level, it will be quite difficult for you to just adopt the LLM", "timestamp": "00:17:03,272", "timestamp_s": 1023.0}, {"text": "and make sure that it is viable in the long term.", "timestamp": "00:17:06,952", "timestamp_s": 1026.0}, {"text": "Now let\u0027s talk about Amazon Bedrock.", "timestamp": "00:17:10,264", "timestamp_s": 1030.0}, {"text": "Amazon Bedrock is basically a way for simplifying", "timestamp": "00:17:13,224", "timestamp_s": 1033.0}, {"text": "the access to foundation models and providing an integration", "timestamp": "00:17:17,256", "timestamp_s": 1037.0}, {"text": "layer for you via a single API which is an invoke model API.", "timestamp": "00:17:20,888", "timestamp_s": 1040.0}, {"text": "You get access to different models which are available within Amazon bedrock.", "timestamp": "00:17:25,165", "timestamp_s": 1045.0}, {"text": "Some of the models which you have here is the", "timestamp": "00:17:29,797", "timestamp_s": 1049.0}, {"text": "stability AI model. You have the Amazon Titan, you have cloud,", "timestamp": "00:17:34,197", "timestamp_s": 1054.0}, {"text": "you have Lama models, etcetera. Customers have often told us", "timestamp": "00:17:37,677", "timestamp_s": 1057.0}, {"text": "that one of the most important features of bedrock is how easy it", "timestamp": "00:17:41,661", "timestamp_s": 1061.0}, {"text": "makes it to experiment with and select and combine different range of", "timestamp": "00:17:45,093", "timestamp_s": 1065.0}, {"text": "foundation models. It\u0027s still very early days and we are", "timestamp": "00:17:49,157", "timestamp_s": 1069.0}, {"text": "all just getting started and customers are moving extremely fast.", "timestamp": "00:17:52,730", "timestamp_s": 1072.0}, {"text": "And the key aspect is customers want to", "timestamp": "00:17:56,530", "timestamp_s": 1076.0}, {"text": "experiment, they want to deploy, they want to iterate on whatever", "timestamp": "00:18:00,410", "timestamp_s": 1080.0}, {"text": "they have done. And today Bedrock provides access to", "timestamp": "00:18:03,666", "timestamp_s": 1083.0}, {"text": "wide range of foundation models from different organizations", "timestamp": "00:18:07,434", "timestamp_s": 1087.0}, {"text": "and as well as the Amazon Titan models that you have. So once you have", "timestamp": "00:18:11,746", "timestamp_s": 1091.0}, {"text": "access to the bedrock API itself, invoking one", "timestamp": "00:18:15,366", "timestamp_s": 1095.0}, {"text": "of these models is extremely straightforward. I\u0027ll talk about it", "timestamp": "00:18:18,598", "timestamp_s": 1098.0}, {"text": "in a bit. Now let\u0027s talk about the architectural patterns", "timestamp": "00:18:21,950", "timestamp_s": 1101.0}, {"text": "that you have when using Amazon Bedrock.", "timestamp": "00:18:25,430", "timestamp_s": 1105.0}, {"text": "Obviously with Amazon Bedrock you have different knowledge bases", "timestamp": "00:18:29,014", "timestamp_s": 1109.0}, {"text": "for Amazon Bedrock which you will be using. And to", "timestamp": "00:18:33,070", "timestamp_s": 1113.0}, {"text": "equip a foundation model with an up to date proprietary", "timestamp": "00:18:37,102", "timestamp_s": 1117.0}, {"text": "information organization often talk about retrieval augmented", "timestamp": "00:18:40,462", "timestamp_s": 1120.0}, {"text": "generation. We spoke about it a little bit earlier when during", "timestamp": "00:18:44,342", "timestamp_s": 1124.0}, {"text": "the customization slide. It\u0027s basically a technique where", "timestamp": "00:18:48,742", "timestamp_s": 1128.0}, {"text": "you\u0027re fetching the data from the company\u0027s data sources,", "timestamp": "00:18:52,630", "timestamp_s": 1132.0}, {"text": "enriching the prompt with that particular data and delivering", "timestamp": "00:18:56,462", "timestamp_s": 1136.0}, {"text": "more relevant and accurate responses. We have knowledge", "timestamp": "00:19:00,046", "timestamp_s": 1140.0}, {"text": "bases within Amazon Bedrock which helps you", "timestamp": "00:19:04,054", "timestamp_s": 1144.0}, {"text": "in a fully managed rag capability and it allows", "timestamp": "00:19:07,606", "timestamp_s": 1147.0}, {"text": "you to customize the foundation model responses with contextual", "timestamp": "00:19:11,360", "timestamp_s": 1151.0}, {"text": "and relevant company data. So essentially it", "timestamp": "00:19:15,408", "timestamp_s": 1155.0}, {"text": "helps you securely connect to your foundation models. It\u0027s a fully managed", "timestamp": "00:19:19,520", "timestamp_s": 1159.0}, {"text": "rag and it\u0027s a built in session context management for", "timestamp": "00:19:23,736", "timestamp_s": 1163.0}, {"text": "multi tone conversations. And obviously you", "timestamp": "00:19:27,352", "timestamp_s": 1167.0}, {"text": "also have automated citations with retrievals to improve the transparency", "timestamp": "00:19:31,288", "timestamp_s": 1171.0}, {"text": "that you get. So how does it work for you?", "timestamp": "00:19:35,544", "timestamp_s": 1175.0}, {"text": "So let\u0027s say you have a user query someone has asked about", "timestamp": "00:19:38,508", "timestamp_s": 1178.0}, {"text": "how can I get the latest details about my statement or something.", "timestamp": "00:19:43,844", "timestamp_s": 1183.0}, {"text": "Now that information goes into Amazon Bedrock", "timestamp": "00:19:47,652", "timestamp_s": 1187.0}, {"text": "and it has the knowledge basis which is associated", "timestamp": "00:19:51,532", "timestamp_s": 1191.0}, {"text": "with that particular Amazon bedrock.", "timestamp": "00:19:55,236", "timestamp_s": 1195.0}, {"text": "And it\u0027s an iterative process going to look at the knowledge basis for Amazon Bedrock", "timestamp": "00:19:58,372", "timestamp_s": 1198.0}, {"text": "and based on that it\u0027s going to augment the prompt that", "timestamp": "00:20:02,556", "timestamp_s": 1202.0}, {"text": "you have received and ultimately you are going to use one of the models,", "timestamp": "00:20:05,950", "timestamp_s": 1205.0}, {"text": "which is the foundation models, be the Claude Llama,", "timestamp": "00:20:10,574", "timestamp_s": 1210.0}, {"text": "Titan or Jurassic models, and ultimately provide a response", "timestamp": "00:20:14,358", "timestamp_s": 1214.0}, {"text": "to your customer. All the information that you are retrieving", "timestamp": "00:20:17,854", "timestamp_s": 1217.0}, {"text": "as part of this process comes from the source", "timestamp": "00:20:22,414", "timestamp_s": 1222.0}, {"text": "citations and the source which you have within the knowledge base.", "timestamp": "00:20:25,734", "timestamp_s": 1225.0}, {"text": "And ultimately it gives you the citations to the knowledge base. In order to improve", "timestamp": "00:20:29,630", "timestamp_s": 1229.0}, {"text": "the transparency. You also have Amazon Q, which is", "timestamp": "00:20:32,852", "timestamp_s": 1232.0}, {"text": "an which has a similar approach when it comes to integrating with Amazon Connect.", "timestamp": "00:20:36,428", "timestamp_s": 1236.0}, {"text": "Not something which we are covering for this particular session, but it", "timestamp": "00:20:40,732", "timestamp_s": 1240.0}, {"text": "has a similar aspect of being able to use your", "timestamp": "00:20:44,772", "timestamp_s": 1244.0}, {"text": "knowledge bases and then give you customized responses.", "timestamp": "00:20:48,516", "timestamp_s": 1248.0}, {"text": "Another architectural pattern is the fine tuning. We spoke about it", "timestamp": "00:20:52,644", "timestamp_s": 1252.0}, {"text": "earlier. So let\u0027s say you want to have a very specific", "timestamp": "00:20:56,908", "timestamp_s": 1256.0}, {"text": "task for which you need to have fine tuning. Simply point", "timestamp": "00:21:01,068", "timestamp_s": 1261.0}, {"text": "to those examples of that particular data, which is an", "timestamp": "00:21:05,084", "timestamp_s": 1265.0}, {"text": "S three and they have been labeled. And then Amazon", "timestamp": "00:21:08,804", "timestamp_s": 1268.0}, {"text": "bedrock makes a copy of the base model,", "timestamp": "00:21:12,284", "timestamp_s": 1272.0}, {"text": "trains it, and creates a private fine tuned model so you", "timestamp": "00:21:15,964", "timestamp_s": 1275.0}, {"text": "can get tailored responses. So how does that", "timestamp": "00:21:19,668", "timestamp_s": 1279.0}, {"text": "work? Essentially you\u0027re making use of one of", "timestamp": "00:21:23,284", "timestamp_s": 1283.0}, {"text": "the foundation models, be it a Lama two model or a Titan", "timestamp": "00:21:26,966", "timestamp_s": 1286.0}, {"text": "model. For these specific tasks you are keeping all of", "timestamp": "00:21:30,614", "timestamp_s": 1290.0}, {"text": "your specific labeled data sets in Amazon", "timestamp": "00:21:34,334", "timestamp_s": 1294.0}, {"text": "S three, and then you are using that", "timestamp": "00:21:37,798", "timestamp_s": 1297.0}, {"text": "data set in order to make your model", "timestamp": "00:21:41,054", "timestamp_s": 1301.0}, {"text": "better to get tailored responses. So today you will have fine tuning", "timestamp": "00:21:44,486", "timestamp_s": 1304.0}, {"text": "available with Lama models cohere, command,", "timestamp": "00:21:48,510", "timestamp_s": 1308.0}, {"text": "Titan and express Titan multi model and", "timestamp": "00:21:51,874", "timestamp_s": 1311.0}, {"text": "Titan image generator. Fine tuning will be very soon coming", "timestamp": "00:21:55,850", "timestamp_s": 1315.0}, {"text": "into anthropic cloud models, but today it is not available.", "timestamp": "00:21:59,418", "timestamp_s": 1319.0}, {"text": "So this creates a copy. You have the label", "timestamp": "00:22:04,554", "timestamp_s": 1324.0}, {"text": "dataset which is in Amazon S three, and from there you are able", "timestamp": "00:22:08,242", "timestamp_s": 1328.0}, {"text": "to fine tune the model and get the generated responses.", "timestamp": "00:22:11,562", "timestamp_s": 1331.0}, {"text": "Now let\u0027s talk about how do you invoke these models.", "timestamp": "00:22:15,034", "timestamp_s": 1335.0}, {"text": "One of the most common patterns that you have with respect to invoking", "timestamp": "00:22:18,744", "timestamp_s": 1338.0}, {"text": "these models is by using API gateway. So it\u0027s", "timestamp": "00:22:22,184", "timestamp_s": 1342.0}, {"text": "a very well tested serverless pattern which has been there", "timestamp": "00:22:25,888", "timestamp_s": 1345.0}, {"text": "in existence even before Amazon bedrock instead of bedrock. You would be", "timestamp": "00:22:29,832", "timestamp_s": 1349.0}, {"text": "having, I don\u0027t know, ECS or EKS or", "timestamp": "00:22:33,312", "timestamp_s": 1353.0}, {"text": "just something running on a compute somewhere.", "timestamp": "00:22:38,208", "timestamp_s": 1358.0}, {"text": "And you can use Amazon Lambda AWS lambda for doing that invocation", "timestamp": "00:22:41,536", "timestamp_s": 1361.0}, {"text": "with bedrock as well. You are able to use the same pattern and", "timestamp": "00:22:45,712", "timestamp_s": 1365.0}, {"text": "it leverages the event driven architecture that you have", "timestamp": "00:22:49,128", "timestamp_s": 1369.0}, {"text": "been using or maybe using with Amazon API gateway.", "timestamp": "00:22:52,344", "timestamp_s": 1372.0}, {"text": "And it doesn\u0027t always have to be Amazon API gateway. You can use it with", "timestamp": "00:22:55,760", "timestamp_s": 1375.0}, {"text": "any integration layer which can support AWS Lambda to invoke", "timestamp": "00:22:59,184", "timestamp_s": 1379.0}, {"text": "the bedrock APIs. And finally, instead of AWS", "timestamp": "00:23:03,040", "timestamp_s": 1383.0}, {"text": "lambda, you can also have the same behavior which let\u0027s say if", "timestamp": "00:23:07,472", "timestamp_s": 1387.0}, {"text": "you\u0027re having a long running compute and EC,", "timestamp": "00:23:10,896", "timestamp_s": 1390.0}, {"text": "two ecs or eks, and then you can invoke bedrock API", "timestamp": "00:23:13,934", "timestamp_s": 1393.0}, {"text": "in the exact same way. For this particular", "timestamp": "00:23:17,270", "timestamp_s": 1397.0}, {"text": "example, let\u0027s consider that you are having two models which you have created", "timestamp": "00:23:20,710", "timestamp_s": 1400.0}, {"text": "for your request and response. Payload request is saying that", "timestamp": "00:23:24,230", "timestamp_s": 1404.0}, {"text": "you need to have a prompt which is going in and response is saying that", "timestamp": "00:23:27,422", "timestamp_s": 1407.0}, {"text": "you have a response that is coming back and a status code that is coming", "timestamp": "00:23:31,294", "timestamp_s": 1411.0}, {"text": "back. When you want to invoke the Amazon", "timestamp": "00:23:34,598", "timestamp_s": 1414.0}, {"text": "bedrock endpoint, you\u0027re going to be writing a very simple", "timestamp": "00:23:39,132", "timestamp_s": 1419.0}, {"text": "lambda code which is going to be using the boto three API.", "timestamp": "00:23:43,012", "timestamp_s": 1423.0}, {"text": "So let\u0027s walk through this API. So you guys basically", "timestamp": "00:23:46,716", "timestamp_s": 1426.0}, {"text": "creating a client of bedrock using the bedrock runtime.", "timestamp": "00:23:50,156", "timestamp_s": 1430.0}, {"text": "With boto three you are creating the body which", "timestamp": "00:23:54,100", "timestamp_s": 1434.0}, {"text": "is the prompt, the max tokens that you need to get", "timestamp": "00:23:57,636", "timestamp_s": 1437.0}, {"text": "as a sample response, the temperature, etcetera.", "timestamp": "00:24:01,492", "timestamp_s": 1441.0}, {"text": "And then you\u0027re selecting the model id. So here I have", "timestamp": "00:24:04,868", "timestamp_s": 1444.0}, {"text": "selected anthropic cloud model. You can also select any", "timestamp": "00:24:08,282", "timestamp_s": 1448.0}, {"text": "of the other model like a Titan model, or you can select the", "timestamp": "00:24:11,850", "timestamp_s": 1451.0}, {"text": "Lama model, any of the model that you want. And once", "timestamp": "00:24:17,714", "timestamp_s": 1457.0}, {"text": "you select the model id and you select the payload structure", "timestamp": "00:24:21,962", "timestamp_s": 1461.0}, {"text": "that you are sending. So be mindful that this particular payload structure can change", "timestamp": "00:24:25,234", "timestamp_s": 1465.0}, {"text": "depending on the model that you are invoking. And you can just use", "timestamp": "00:24:29,274", "timestamp_s": 1469.0}, {"text": "invoke model and give you a response. And that", "timestamp": "00:24:33,618", "timestamp_s": 1473.0}, {"text": "response is how you would return it back by using the same model", "timestamp": "00:24:37,034", "timestamp_s": 1477.0}, {"text": "structure that you have used earlier. Now this particular response", "timestamp": "00:24:40,834", "timestamp_s": 1480.0}, {"text": "request payload is structure would be differing based on the", "timestamp": "00:24:44,650", "timestamp_s": 1484.0}, {"text": "model that you are using and the model id will also change based", "timestamp": "00:24:49,090", "timestamp_s": 1489.0}, {"text": "on the model that you are intending to use. So that\u0027s one of", "timestamp": "00:24:52,450", "timestamp_s": 1492.0}, {"text": "the way of invoking it if you\u0027re using Lambda and API gateway, and even if", "timestamp": "00:24:55,858", "timestamp_s": 1495.0}, {"text": "you\u0027re not using API gateway, anything else which can integrate with that", "timestamp": "00:24:59,338", "timestamp_s": 1499.0}, {"text": "you can use. Now let\u0027s say you\u0027re not using any lambda you just", "timestamp": "00:25:03,274", "timestamp_s": 1503.0}, {"text": "want to use from a generic application. You can essentially use boto", "timestamp": "00:25:06,650", "timestamp_s": 1506.0}, {"text": "three and you can use a temporary credentials in order to gain access", "timestamp": "00:25:10,402", "timestamp_s": 1510.0}, {"text": "and ultimately invoke the bedrock API. And for any reason", "timestamp": "00:25:13,818", "timestamp_s": 1513.0}, {"text": "if the AWS SDK is not available to you.", "timestamp": "00:25:18,170", "timestamp_s": 1518.0}, {"text": "You can also leverage AWS SIG V four for", "timestamp": "00:25:22,002", "timestamp_s": 1522.0}, {"text": "constructing a valid request payload and invoking the bedrock", "timestamp": "00:25:25,498", "timestamp_s": 1525.0}, {"text": "API. So this is a similar example,", "timestamp": "00:25:29,242", "timestamp_s": 1529.0}, {"text": "quite similar to the one that has been shown earlier.", "timestamp": "00:25:33,562", "timestamp_s": 1533.0}, {"text": "The only difference here is we don\u0027t have the lambda handler with", "timestamp": "00:25:37,874", "timestamp_s": 1537.0}, {"text": "the event context and the event and the context. Here we are directly using the", "timestamp": "00:25:42,010", "timestamp_s": 1542.0}, {"text": "Botox reap and we are getting a response from here.", "timestamp": "00:25:46,058", "timestamp_s": 1546.0}, {"text": "So you can embed it in any of the applications which has", "timestamp": "00:25:48,810", "timestamp_s": 1548.0}, {"text": "access to the temporary credentials and you should be able to access the bedrock API.", "timestamp": "00:25:52,218", "timestamp_s": 1552.0}, {"text": "Talking about operational excellence, one of the things that I had spoken about earlier", "timestamp": "00:25:57,034", "timestamp_s": 1557.0}, {"text": "is having good insight into your application.", "timestamp": "00:26:01,082", "timestamp_s": 1561.0}, {"text": "So we spoke about how do you invoke the application, how do you have the", "timestamp": "00:26:04,386", "timestamp_s": 1564.0}, {"text": "API driven approach so that you\u0027re able to have a versioning,", "timestamp": "00:26:08,250", "timestamp_s": 1568.0}, {"text": "you\u0027re able to have visibility of what is invoking what,", "timestamp": "00:26:11,642", "timestamp_s": 1571.0}, {"text": "and you\u0027re able to have temporary credentials, best practices, etc.", "timestamp": "00:26:15,506", "timestamp_s": 1575.0}, {"text": "Etcetera. Now we talk about observability that you", "timestamp": "00:26:18,618", "timestamp_s": 1578.0}, {"text": "would be getting with Amazon bedrock and that\u0027s the invocation login.", "timestamp": "00:26:22,010", "timestamp_s": 1582.0}, {"text": "So customers want to know what was the invocation, what was the prompt", "timestamp": "00:26:26,010", "timestamp_s": 1586.0}, {"text": "which was sent and what kind of response did I get.", "timestamp": "00:26:30,314", "timestamp_s": 1590.0}, {"text": "You can enable it at the bedrock level and all of these", "timestamp": "00:26:33,890", "timestamp_s": 1593.0}, {"text": "logs can go into Amazon s three or cloudwatch or both.", "timestamp": "00:26:37,546", "timestamp_s": 1597.0}, {"text": "Here is a sample of a log structure where you", "timestamp": "00:26:41,178", "timestamp_s": 1601.0}, {"text": "have the input body which was sent by the requester", "timestamp": "00:26:45,066", "timestamp_s": 1605.0}, {"text": "either via lambda or any other way by which the API has been invoked.", "timestamp": "00:26:49,474", "timestamp_s": 1609.0}, {"text": "And you can see that this is the input someone is asking", "timestamp": "00:26:53,954", "timestamp_s": 1613.0}, {"text": "explain the three body problem. And here the response is coming in", "timestamp": "00:26:57,694", "timestamp_s": 1617.0}, {"text": "in terms of the number of tokens that has been given. So you will notice", "timestamp": "00:27:01,654", "timestamp_s": 1621.0}, {"text": "that because we had given a maximum token of 300, the response", "timestamp": "00:27:05,262", "timestamp_s": 1625.0}, {"text": "token count is 296. For the purpose of the presentation I\u0027ve", "timestamp": "00:27:09,310", "timestamp_s": 1629.0}, {"text": "just truncated what is there in the completion response.", "timestamp": "00:27:12,638", "timestamp_s": 1632.0}, {"text": "But here you will have a response coming from the model. In this case it", "timestamp": "00:27:16,278", "timestamp_s": 1636.0}, {"text": "was a cloud model which had been used for that. So this logging will be", "timestamp": "00:27:19,558", "timestamp_s": 1639.0}, {"text": "available for you directly within Cloudwatch. And then from Cloudwatch", "timestamp": "00:27:23,510", "timestamp_s": 1643.0}, {"text": "onwards you can change it to let\u0027s say s three or", "timestamp": "00:27:26,774", "timestamp_s": 1646.0}, {"text": "maybe use it for any kind of future use. Talking about metrics,", "timestamp": "00:27:30,406", "timestamp_s": 1650.0}, {"text": "you have these metrics available out of the box for Cloudwatch", "timestamp": "00:27:34,758", "timestamp_s": 1654.0}, {"text": "with Amazon Bedrock, which would be your number of invocations, your latency", "timestamp": "00:27:38,478", "timestamp_s": 1658.0}, {"text": "that you\u0027re having, any kind of client and server side errors, any throttling that", "timestamp": "00:27:42,342", "timestamp_s": 1662.0}, {"text": "you\u0027re having, and obviously the token count, the input and output,", "timestamp": "00:27:46,238", "timestamp_s": 1666.0}, {"text": "you saw a sample of it in the previous log structure is going to be", "timestamp": "00:27:49,758", "timestamp_s": 1669.0}, {"text": "the same. Now, talking about the", "timestamp": "00:27:52,718", "timestamp_s": 1672.0}, {"text": "model evaluation bedrock currently has", "timestamp": "00:27:57,006", "timestamp_s": 1677.0}, {"text": "in preview, I believe a way for you to evaluate", "timestamp": "00:28:00,142", "timestamp_s": 1680.0}, {"text": "the models. Now the models can be evaluated for robustness,", "timestamp": "00:28:04,054", "timestamp_s": 1684.0}, {"text": "it can be evaluated for toxicity and accuracy.", "timestamp": "00:28:08,198", "timestamp_s": 1688.0}, {"text": "There is on the AWS console,", "timestamp": "00:28:12,294", "timestamp_s": 1692.0}, {"text": "you you can can essentially evaluate the model using recommended metrics.", "timestamp": "00:28:15,390", "timestamp_s": 1695.0}, {"text": "There\u0027s an automated evaluation, but you can also choose", "timestamp": "00:28:18,702", "timestamp_s": 1698.0}, {"text": "what kind of task you\u0027re evaluating it for. For example,", "timestamp": "00:28:22,718", "timestamp_s": 1702.0}, {"text": "this particular screenshot is from the AWS console, which allows you to", "timestamp": "00:28:26,518", "timestamp_s": 1706.0}, {"text": "evaluate for a question and answer scenario for Amazon bedrock.", "timestamp": "00:28:30,094", "timestamp_s": 1710.0}, {"text": "And we are using the anthropic cloud model and", "timestamp": "00:28:34,582", "timestamp_s": 1714.0}, {"text": "these were the responses which we received on the", "timestamp": "00:28:38,062", "timestamp_s": 1718.0}, {"text": "accuracy and the toxicity that was evaluated", "timestamp": "00:28:41,822", "timestamp_s": 1721.0}, {"text": "against. And you can also bring your own prompt data set", "timestamp": "00:28:45,166", "timestamp_s": 1725.0}, {"text": "or use built and curated prompt data sets for this purpose.", "timestamp": "00:28:48,574", "timestamp_s": 1728.0}, {"text": "So these are some of the observability and insight", "timestamp": "00:28:52,774", "timestamp_s": 1732.0}, {"text": "related details that you can potentially use when you", "timestamp": "00:28:56,678", "timestamp_s": 1736.0}, {"text": "are thinking about using bedrock as your single API for different", "timestamp": "00:28:59,950", "timestamp_s": 1739.0}, {"text": "foundation models. And finally, we want to talk about the", "timestamp": "00:29:03,614", "timestamp_s": 1743.0}, {"text": "guardrails because as we talk about generative", "timestamp": "00:29:06,950", "timestamp_s": 1746.0}, {"text": "AI, there are different challenges around undesirable,", "timestamp": "00:29:10,930", "timestamp_s": 1750.0}, {"text": "irrelevant topic responses or controversial queries or", "timestamp": "00:29:14,418", "timestamp_s": 1754.0}, {"text": "responses which you will be getting, toxicity of your responses,", "timestamp": "00:29:18,114", "timestamp_s": 1758.0}, {"text": "privacy protection bias, stereotyping propagation", "timestamp": "00:29:22,434", "timestamp_s": 1762.0}, {"text": "and all of those things. So as we talk about", "timestamp": "00:29:26,778", "timestamp_s": 1766.0}, {"text": "these new challenges, you also want to talk about what", "timestamp": "00:29:29,978", "timestamp_s": 1769.0}, {"text": "kind of guardrails you will be applying for your models.", "timestamp": "00:29:33,860", "timestamp_s": 1773.0}, {"text": "One open source solution that you have is with Nvidia Nemo", "timestamp": "00:29:37,244", "timestamp_s": 1777.0}, {"text": "guardrails. So this is basically for building", "timestamp": "00:29:41,684", "timestamp_s": 1781.0}, {"text": "trustworthy, safe and secure llms. So you can define the", "timestamp": "00:29:45,756", "timestamp_s": 1785.0}, {"text": "guardrails or rails and to guide and safeguard,", "timestamp": "00:29:49,740", "timestamp_s": 1789.0}, {"text": "guide and have a safeguarded conversation. And you can", "timestamp": "00:29:53,492", "timestamp_s": 1793.0}, {"text": "also choose to define the behavior of your LLM based application", "timestamp": "00:29:56,732", "timestamp_s": 1796.0}, {"text": "for specific topics and prevent it from engaging in any", "timestamp": "00:30:00,452", "timestamp_s": 1800.0}, {"text": "discussions which are in unwanted topics. You can also", "timestamp": "00:30:03,956", "timestamp_s": 1803.0}, {"text": "start connecting different models using LAN chain and", "timestamp": "00:30:07,908", "timestamp_s": 1807.0}, {"text": "other services which you can have. So it\u0027s kind of like a shim", "timestamp": "00:30:11,652", "timestamp_s": 1811.0}, {"text": "layer which is sitting between your application which", "timestamp": "00:30:15,236", "timestamp_s": 1815.0}, {"text": "is going to be invoking an LLM. So here you can define all your", "timestamp": "00:30:18,500", "timestamp_s": 1818.0}, {"text": "programmable guardrails and then you", "timestamp": "00:30:22,364", "timestamp_s": 1822.0}, {"text": "can kind of steer your llms in order to follow a predefined", "timestamp": "00:30:26,876", "timestamp_s": 1826.0}, {"text": "conversation path and enforce standard operating", "timestamp": "00:30:30,452", "timestamp_s": 1830.0}, {"text": "procedures. So these kind of standard operating procedures", "timestamp": "00:30:34,458", "timestamp_s": 1834.0}, {"text": "are some of the core context", "timestamp": "00:30:37,970", "timestamp_s": 1837.0}, {"text": "when it comes to building an operational excellence practice,", "timestamp": "00:30:41,530", "timestamp_s": 1841.0}, {"text": "especially when you are building out the llms. So these are some", "timestamp": "00:30:45,138", "timestamp_s": 1845.0}, {"text": "of the same points which I have mentioned. And you", "timestamp": "00:30:48,498", "timestamp_s": 1848.0}, {"text": "can have a look at GitHub and there is, I\u0027ll give you", "timestamp": "00:30:52,346", "timestamp_s": 1852.0}, {"text": "a link towards the end of the session as well,", "timestamp": "00:30:55,698", "timestamp_s": 1855.0}, {"text": "where within Amazon bedrock samples", "timestamp": "00:30:59,178", "timestamp_s": 1859.0}, {"text": "are available which you can take a look at it and you can see how", "timestamp": "00:31:03,352", "timestamp_s": 1863.0}, {"text": "the guardrails have been incorporated, is basically a config", "timestamp": "00:31:06,480", "timestamp_s": 1866.0}, {"text": "YAML file and you give an", "timestamp": "00:31:10,080", "timestamp_s": 1870.0}, {"text": "input rail and the input rails are basically applied to", "timestamp": "00:31:13,368", "timestamp_s": 1873.0}, {"text": "the inputs from the user and it can reject the input or", "timestamp": "00:31:17,168", "timestamp_s": 1877.0}, {"text": "we can stop any additional processing. Then you have the dialogue", "timestamp": "00:31:21,088", "timestamp_s": 1881.0}, {"text": "rails which is to influence how the LLM is prompted", "timestamp": "00:31:24,552", "timestamp_s": 1884.0}, {"text": "and they operate on the canonical form messages. You have the", "timestamp": "00:31:28,320", "timestamp_s": 1888.0}, {"text": "retrieval rails which are applied on the retrieval chunk. In the case of", "timestamp": "00:31:31,910", "timestamp_s": 1891.0}, {"text": "say rag scenario, a retrieval rail can reject", "timestamp": "00:31:35,622", "timestamp_s": 1895.0}, {"text": "a chunk or prevent it from being used to prompt the LLM. You have", "timestamp": "00:31:38,886", "timestamp_s": 1898.0}, {"text": "the execution rail and finally you have the output rails. So these", "timestamp": "00:31:42,438", "timestamp_s": 1902.0}, {"text": "are like five different levers which you can control", "timestamp": "00:31:45,518", "timestamp_s": 1905.0}, {"text": "and you can write your config in a config yaml.", "timestamp": "00:31:49,214", "timestamp_s": 1909.0}, {"text": "If you go into the GitHub for this particular guardrail,", "timestamp": "00:31:53,054", "timestamp_s": 1913.0}, {"text": "the Nemo guardrails, you will be able to find more details in", "timestamp": "00:31:56,830", "timestamp_s": 1916.0}, {"text": "there. But this is just an introduction of what kind of guardrails you can add", "timestamp": "00:32:00,400", "timestamp_s": 1920.0}, {"text": "into your LLM invocation. So that you are", "timestamp": "00:32:03,832", "timestamp_s": 1923.0}, {"text": "ensuring that it\u0027s safe and you\u0027re ensuring", "timestamp": "00:32:08,784", "timestamp_s": 1928.0}, {"text": "the responsible AI best practices when using llms.", "timestamp": "00:32:12,552", "timestamp_s": 1932.0}, {"text": "And finally, this is how it would look when you are using it with Amazon", "timestamp": "00:32:16,704", "timestamp_s": 1936.0}, {"text": "bedrock where you would have the central layer of all the guardrails", "timestamp": "00:32:20,464", "timestamp_s": 1940.0}, {"text": "that you are having. You have the invoker in here and ultimately the bedrock model", "timestamp": "00:32:24,110", "timestamp_s": 1944.0}, {"text": "coming in here and the Neemo guardrails would apply at the central layer.", "timestamp": "00:32:28,182", "timestamp_s": 1948.0}, {"text": "So that is the shim layer that is sitting between your LLM", "timestamp": "00:32:31,630", "timestamp_s": 1951.0}, {"text": "which is exposed via the bedrock and then the invoker who is", "timestamp": "00:32:35,182", "timestamp_s": 1955.0}, {"text": "giving that. And finally", "timestamp": "00:32:39,262", "timestamp_s": 1959.0}, {"text": "the GitHub handle where you can find the details of Amazon bedrock workshop.", "timestamp": "00:32:42,830", "timestamp_s": 1962.0}, {"text": "And this is a screenshot of the UI that you have that", "timestamp": "00:32:46,286", "timestamp_s": 1966.0}, {"text": "closes every, all the topics that I wanted to cover for this session.", "timestamp": "00:32:50,180", "timestamp_s": 1970.0}, {"text": "Talking all the way from what bedrock is what", "timestamp": "00:32:53,996", "timestamp_s": 1973.0}, {"text": "it is offering. How do you invoke bedrock, what kind", "timestamp": "00:32:57,620", "timestamp_s": 1977.0}, {"text": "of observability you\u0027re getting out of the box. And finally the guardrails", "timestamp": "00:33:00,924", "timestamp_s": 1980.0}, {"text": "which you can apply for bedrock. Hope this helps.", "timestamp": "00:33:05,308", "timestamp_s": 1985.0}, {"text": "Thank you so much for your time. It\u0027s been a pleasure.", "timestamp": "00:33:08,788", "timestamp_s": 1988.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'GSteZj7Y3MQ',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Operational excellence for your LLMs using Amazon Bedrock
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>Customers are looking for a turnkey solution to integrate LLMs with their existing applications. This session will provide an overview of the operational considerations, architectural patterns, and governance controls needed to operate LLMs at scale.</p>
<!-- End Text -->
          </div>

          
          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
              <li>
                Today I'll be talking about Amazon Bedrock, which is a managed service via which you can have access to different foundation models using a single API. We talk about the operational excellence best practices that you would like to consider when using Amazon bedrock.

              </li>
              
              <li>
                Operational excellence is basically an ability to support the development of your workloads. One of the key principles is performing operations as a code. refining your operational procedures, anticipating failures and learning from your operational failures. Finally observability, which can help you get actionable insights.

              </li>
              
              <li>
                Mlogs is basically using the same set of processes and people and technology best practices within the scheme of machine learning solutions. MLops is basically productionization of your ML solutions effectively. At its core, every solution that you are creating is going to be talking about people, process and technology.

              </li>
              
              <li>
                With Llmops, there can be different types of users that you're interacting with. And basically I want to talk about the generative AI user types and then the skills which are needed. The three key aspects that you would want to consider are speed, precision and cost.

              </li>
              
              <li>
                There are four ways in which customers to be using the llms. Prompt engineering, Rag, rack and continued pre training. Using rag then comes fine tuning, which is more time consuming. Do have a clear benchmarking to see how your model is performing with prompt engineering versus rag.

              </li>
              
              <li>
                Now customizing, now here we talk about customizing the business responses. This is where fine tuning and continued pre training comes into picture. Be very clear on your use case as to when you would be using a rag versus a fine tuning or prompt engineering.

              </li>
              
              <li>
                Amazon Bedrock is basically a way for simplifying the access to foundation models. You get access to different models which are available within Amazon bedrock. Once you have access to the bedrock API, invoking one of these models is extremely straightforward. It's still very early days and we are all just getting started.

              </li>
              
              <li>
                Amazon Bedrock allows you to customize the foundation model responses with contextual and relevant company data. Another architectural pattern is the fine tuning. You can use any integration layer which can support AWS Lambda to invoke the bedrock APIs.

              </li>
              
              <li>
                You can enable it at the bedrock level and all of these logs can go into Amazon s three or cloudwatch or both. This logging will be available for you directly within Cloudwatch. And then from Cloudwatch onwards you can change it to let's say s 3 or maybe use it for any kind of future use.

              </li>
              
              <li>
                Talking about metrics, you have these metrics available out of the box for Cloudwatch with Amazon Bedrock. Models can be evaluated for robustness, it can be evaluation for toxicity and accuracy. You can also bring your own prompt data set or use built and curated prompt data sets for this purpose.

              </li>
              
              <li>
                And finally, we want to talk about the guardrails because as we talk about generative AI, there are different challenges around undesirable, irrelevant topic responses. One open source solution that you have is with Nvidia Nemo guardraILS. And finally, how do you invoke bedrock?
              </li>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./assemblyai/GSteZj7Y3MQ.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:20,680'); seek(20.0)">
              Hello everyone, welcome to the session. Large language
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:24,006'); seek(24.0)">
              models have captured the imagination for different software developers and customers
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:28,286'); seek(28.0)">
              who are interested in now integrating those models into their day to day workflows.
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:33,204'); seek(33.0)">
              Today I'll be talking about Amazon Bedrock, which is a managed service
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:37,484'); seek(37.0)">
              via which you can have access to different foundation models
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:40,660'); seek(40.0)">
              using a single API. We talk about the operational
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:44,324'); seek(44.0)">
              excellence best practices that you would like to consider when
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:47,644'); seek(47.0)">
              using Amazon bedrock. Customers are often
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:50,828'); seek(50.0)">
              looking for turnkey solution which can help integrate these llms
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:54,644'); seek(54.0)">
              into their existing applications. As part of
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:00:57,958'); seek(57.0)">
              the session, we will talk about an introduction to the operational
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:01:01,430'); seek(61.0)">
              excellence term from a well architected review perspective. We will
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:01:05,230'); seek(65.0)">
              talk about the llms and then we will go in depth in the bedrock.
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:01:09,734'); seek(69.0)">
              So let's start with operational excellence. If you look at the
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:01:13,286'); seek(73.0)">
              well architected review that AWS recommends,
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:01:16,902'); seek(76.0)">
              we have a pillar in there which says optional excellence.
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:20,334'); seek(80.0)">
              Operational excellence is basically an ability to support the development
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:24,006'); seek(84.0)">
              of your workloads, how to run your workloads, gain insight
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:28,200'); seek(88.0)">
              into your workloads, and essentially improve your process and procedures
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:32,712'); seek(92.0)">
              to deliver business value. Operational excellence is
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:36,584'); seek(96.0)">
              a practice which you develop over the course of time.
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:40,128'); seek(100.0)">
              It's not something which you will be able to get
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:43,824'); seek(103.0)">
              done overnight or just by adopting a particular solution.
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:47,624'); seek(107.0)">
              This is how your team is structured, this is how your people process and
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:51,586'); seek(111.0)">
              the technologies working together. Now,
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:54,522'); seek(114.0)">
              within operational excellence, there are different design principles
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:01:58,106'); seek(118.0)">
              that you should be considering. One of the key
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:02:01,490'); seek(121.0)">
              principles is performing operations as a code. We are all aware of infrastructure as
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:02:05,442'); seek(125.0)">
              code and we are aware of different tools and technologies
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:02:08,994'); seek(128.0)">
              which are there in the market. Try to adopt as much as possible
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:02:13,010'); seek(133.0)">
              from an operations perspective so that you can start executing these as code
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:02:17,186'); seek(137.0)">
              snippets, making frequent and small reversible changes.
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:02:21,478'); seek(141.0)">
              That's another key aspect of the design principle for operational excellence,
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:26,454'); seek(146.0)">
              refining your operational procedures, obviously anticipating
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:30,638'); seek(150.0)">
              failures and learning from your operational failures,
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:34,574'); seek(154.0)">
              and finally observability, which can help you get actionable insights.
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:39,654'); seek(159.0)">
              With respect to llms. Let's say you're using Amazon Bedrock,
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:43,262'); seek(163.0)">
              which is an API, access into different foundation models.
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:46,594'); seek(166.0)">
              You still have to follow all of these design principles in
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:49,858'); seek(169.0)">
              order to how to deploy the API. How do you start versioning
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:53,938'); seek(173.0)">
              the API? How do you have the different operational procedures
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:02:57,930'); seek(177.0)">
              working together? What kind of observability can be put in?
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:03:01,378'); seek(181.0)">
              So these are some of the factors that we will be talking about as we
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:03:04,898'); seek(184.0)">
              go more into the session. Now let's talk about some of
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:03:08,362'); seek(188.0)">
              the key terms that we keep hearing day in, day out DevOps,
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:03:12,946'); seek(192.0)">
              mlops and llmops.
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:15,834'); seek(195.0)">
              DevOps as a term has been around for a pretty long time.
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:19,522'); seek(199.0)">
              It's basically encouraging you to break down the silos,
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:22,858'); seek(202.0)">
              start having the organizational and the functional separation removed
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:26,362'); seek(206.0)">
              from the different teams, and have an ownership end to end. As to whatever
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:30,242'); seek(210.0)">
              you are building, you are also running and supporting that code.
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:33,954'); seek(213.0)">
              Mlogs is basically using the same set of processes and
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:38,534'); seek(218.0)">
              people and technology best practices within the scheme
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:41,750'); seek(221.0)">
              of machine learning solutions. So you consider DevOps
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:45,830'); seek(225.0)">
              is something which you would often use for microservices written in Java,
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:49,326'); seek(229.0)">
              Python or Golang. When you are trying to
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:52,534'); seek(232.0)">
              use the same set of technology stack, but now trying
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:03:56,534'); seek(236.0)">
              to solve a machine learning problem where you suddenly have a model,
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:04:00,158'); seek(240.0)">
              you need to train the model, you need to have the inference of the model,
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:04:03,566'); seek(243.0)">
              you need to have multi model endpoints.
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:04:07,582'); seek(247.0)">
              You want to incorporate these practices into how this model is getting trained,
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:04:11,510'); seek(251.0)">
              how it is getting deployed, how the approval process is going to be there,
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:15,118'); seek(255.0)">
              and ultimately how the inference is going to be there, be it a real time
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:18,182'); seek(258.0)">
              inference or a batch inference. So that's the mlops part in there.
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:22,614'); seek(262.0)">
              What about llmops? So far, mlops is
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:26,694'); seek(266.0)">
              mostly used for specific machine learning models which you have
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:30,494'); seek(270.0)">
              created to solve a single task. With large language
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:34,566'); seek(274.0)">
              models, you have a capability of using a single
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:37,686'); seek(277.0)">
              foundation model to solve different types of tasks.
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:41,166'); seek(281.0)">
              For example, a foundation model like something
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:45,046'); seek(285.0)">
              which you would be having access via
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:48,198'); seek(288.0)">
              the bedrock, you can use it for text summarization,
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:04:52,166'); seek(292.0)">
              you can use it as a chatbot, you can use it for
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:04:55,582'); seek(295.0)">
              question and answers. There are different business scenarios where
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:04:59,542'); seek(299.0)">
              you can use these models. Hence the llmops as a term
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:05:03,474'); seek(303.0)">
              is using that single model. That is
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:05:06,546'); seek(306.0)">
              the foundation model for different aspects of your business.
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:05:10,114'); seek(310.0)">
              So in all your best practices on the operational excellence which we spoke about
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:05:13,922'); seek(313.0)">
              in the previous slide, they remain quite consistent.
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:16,970'); seek(316.0)">
              But just the nature of these specific problems which you
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:20,138'); seek(320.0)">
              are solving would be differing depending on the machine learning solutions
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:23,834'); seek(323.0)">
              or the LLM solutions which you have. At its core,
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:27,830'); seek(327.0)">
              every solution that you are creating is going to be talking about
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:31,566'); seek(331.0)">
              people, process and technology.
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:34,614'); seek(334.0)">
              Now we say MLops is basically productionization
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:38,206'); seek(338.0)">
              of your ML solutions effectively. So let's say I deploy
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:41,974'); seek(341.0)">
              a solution into production which has the model go through its own
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:45,310'); seek(345.0)">
              set of training. Someone has provided an approval. Now it
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:05:48,998'); seek(348.0)">
              is an inference, be it a batch inference or a real time inference.
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:05:53,724'); seek(353.0)">
              There is a lot of overlap that happens with foundation model operations
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:05:58,252'); seek(358.0)">
              such as generative AI solutions using text, image,
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:06:01,532'); seek(361.0)">
              video and audio. And finally, when you talk about
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:06:05,436'); seek(365.0)">
              llms, these are basically large language models which are using,
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:06:08,996'); seek(368.0)">
              again for productionization. There are some of the attributes which would
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:12,436'); seek(372.0)">
              change in terms of the metrics that
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:16,028'); seek(376.0)">
              you're looking at, but the process more or less remains the same.
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:19,620'); seek(379.0)">
              And then there are some more additional customizations that you would incorporate
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:23,454'); seek(383.0)">
              into llms with say, rag or fine
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:06:27,350'); seek(387.0)">
              tuning, etcetera. We'll talk about it in a, in a few slides.
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:06:31,374'); seek(391.0)">
              At its core, it's still going to be people, process and technology,
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:06:35,614'); seek(395.0)">
              and that overlap is going to be consistent,
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:06:39,694'); seek(399.0)">
              irrespective of what kind of operational excellence you're going for,
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:43,614'); seek(403.0)">
              be it you using the best technology that is available in the market.
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:06:47,182'); seek(407.0)">
              You still need to train your people who can effectively use that technology
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:06:51,452'); seek(411.0)">
              to derive the business value. You need to have
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:06:54,804'); seek(414.0)">
              a close correlation between the team which is training your model,
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:06:58,212'); seek(418.0)">
              or maybe fine tuning that model, and ultimately the consumers who
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:07:01,676'); seek(421.0)">
              are going to be using that model. That aspect of people,
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:07:05,508'); seek(425.0)">
              process and technology. And obviously, Conway's law doesn't change much
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:07:09,788'); seek(429.0)">
              when it comes to deploying a software, whether you're deploying it using an
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:13,372'); seek(433.0)">
              LLM or you're deploying it in the traditional sense
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:07:16,882'); seek(436.0)">
              with microservices or even a monolith application.
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:07:20,194'); seek(440.0)">
              Now, let's talk about foundation models. And the first thing that we want
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:07:23,466'); seek(443.0)">
              to talk about is the model lifecycle. So in a
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:07:27,058'); seek(447.0)">
              typical machine learning use case, you will have a model
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:07:30,722'); seek(450.0)">
              lifecycle where you have a lot of data, and using that data now
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:07:34,130'); seek(454.0)">
              you're going to go into a processing stage where you're processing
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:07:37,634'); seek(457.0)">
              all of your information. That data has been labeled, maybe a supervised
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:07:41,594'); seek(461.0)">
              learning or unsupervised learning, whatever is your choice algorithm that you're
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:07:44,922'); seek(464.0)">
              doing. And then once the training has been done,
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:07:48,006'); seek(468.0)">
              now you have a hyper parameter tuning that you're doing to ultimately
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:07:52,198'); seek(472.0)">
              create a model. That model is going to go through the model validation,
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:07:56,494'); seek(476.0)">
              testing, and once that model is ready,
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:07:59,678'); seek(479.0)">
              you are going to be using for that specific task. That's the
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:08:02,918'); seek(482.0)">
              important part here. You're going to be using for this specific task because
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:08:06,566'); seek(486.0)">
              the model has been tuned and trained for that particular task.
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:08:10,750'); seek(490.0)">
              Tomorrow you have new set of data, you're going to do an iteration,
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:14,430'); seek(494.0)">
              and then you're going to do the training of the model and ultimately
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:08:18,414'); seek(498.0)">
              redeploy the model once the requisite approvals are available.
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:08:22,294'); seek(502.0)">
              So this is just one project. When it comes to
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:08:26,294'); seek(506.0)">
              foundation models or large language
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:08:30,206'); seek(510.0)">
              models, your data set is no longer just one data
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:08:33,766'); seek(513.0)">
              set. You're training that model using every possible
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:08:37,174'); seek(517.0)">
              data set. For example, the model which you
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:08:40,764'); seek(520.0)">
              have from meta, the Lama models you are,
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:08:44,660'); seek(524.0)">
              they have been trained, pre trained with large data sets,
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:08:48,164'); seek(528.0)">
              70 billion parameters, etcetera. Once that
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:08:51,764'); seek(531.0)">
              model has been made available. Now, from that foundation model,
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:08:56,428'); seek(536.0)">
              you can either do a fine tuning if you're interested in doing that.
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:08:59,812'); seek(539.0)">
              So that's the project b that you're looking at. But then from the
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:09:03,340'); seek(543.0)">
              same foundation model, you can directly use it for some task specific
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:09:07,060'); seek(547.0)">
              deployments. And then once you do a fine tuning or rag
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:09:10,194'); seek(550.0)">
              or something else, you can use that same model for a different
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:09:13,994'); seek(553.0)">
              use case. So that's the key difference here. You're using a single model
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:09:17,922'); seek(557.0)">
              for different projects and different scenarios with some
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:09:21,314'); seek(561.0)">
              alterations. And in the previous case you are having one model
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:09:24,874'); seek(564.0)">
              for each of it. Now, with Llmops,
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:09:28,034'); seek(568.0)">
              there can be different types of users that you're interacting with.
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:09:31,642'); seek(571.0)">
              And basically I want to talk about the generative AI user
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:09:35,002'); seek(575.0)">
              types and then the skills which are needed. Let's talk first about
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:09:38,582'); seek(578.0)">
              the providers. You have a provider, let's say someone is building an
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:09:41,958'); seek(581.0)">
              NLM from scratch. In this case we take a Lama model
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:09:45,262'); seek(585.0)">
              which has been built from scratch, and that model can
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:09:49,110'); seek(589.0)">
              be used for different use cases,
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:09:52,254'); seek(592.0)">
              NLP, data science, model deployment, inference,
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:09:55,582'); seek(595.0)">
              etcetera. So that's a provider. You have got a model from there internally,
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:09:59,830'); seek(599.0)">
              your team can decide to have a fine tuning on those
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:10:03,134'); seek(603.0)">
              models. So those are the people who are doing a fine tuning on the model
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:10:06,518'); seek(606.0)">
              to fit custom requirements. Maybe you have a business specific data
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:10:10,008'); seek(610.0)">
              which you want the model to be a little bit more aware of.
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:10:14,200'); seek(614.0)">
              So you're going to be training that model, fine tuning that model,
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:10:17,744'); seek(617.0)">
              using that business particular data, domain specific knowledge that you're
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:10:21,152'); seek(621.0)">
              having. And then the third group is basically consumers.
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:10:24,304'); seek(624.0)">
              They don't care about what the model has been trained
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:10:27,608'); seek(627.0)">
              on or how it has been fine tuned. They are more like consumers who are
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:10:30,368'); seek(630.0)">
              going to be just using that model. So consider someone who is using
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:10:33,456'); seek(633.0)">
              your chatbot, someone who has asked a question. They would like to
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:10:36,678'); seek(636.0)">
              get a response. They want to ensure that the response is not
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:10:40,462'); seek(640.0)">
              having any kind of bias, toxicity,
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:10:43,350'); seek(643.0)">
              or unrequired
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:10:48,134'); seek(648.0)">
              responses that you will be getting. So they don't really have much of the
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:10:51,510'); seek(651.0)">
              ML expertise, but they are basically using prompt engineering for getting
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:10:55,222'); seek(655.0)">
              a response from the model. Be mindful.
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:10:58,694'); seek(658.0)">
              These roles are transferable. So you can always have
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:11:02,040'); seek(662.0)">
              a provider who's also becoming a tuner, and you can always
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:11:05,296'); seek(665.0)">
              have a consumer who can also become a tuner.
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:11:08,176'); seek(668.0)">
              Essentially, this is the entire spectrum that you're having,
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:11:11,040'); seek(671.0)">
              where you have more on the MLov side, where the model
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:11:14,248'); seek(674.0)">
              is getting created, and then you have the other end of the spectrum, where people
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:11:17,240'); seek(677.0)">
              are directly incorporating this model into their day
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:11:20,600'); seek(680.0)">
              to day workflows when it comes to LLM selection,
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:11:24,448'); seek(684.0)">
              there are different aspects that you would want to consider.
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:11:27,844'); seek(687.0)">
              The three key ones that we have seen from the field is the speed,
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:11:31,388'); seek(691.0)">
              precision and the cost. Now, let's say you have three different llms
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:11:35,348'); seek(695.0)">
              and each one of them is good at one particular thing. So let's say
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:11:39,172'); seek(699.0)">
              we have LLM one, two and three. LLM one is
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:11:42,348'); seek(702.0)">
              the best when it comes to precision. Two is the best when it comes to
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:11:45,572'); seek(705.0)">
              cost, and we have one again
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:11:48,868'); seek(708.0)">
              to be the best when it comes to speed. Depending on
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:11:52,260'); seek(712.0)">
              the business scenario and the priority for a particular customer,
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:11:56,606'); seek(716.0)">
              they can choose one of those llms. Some customers are
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:12:00,702'); seek(720.0)">
              ready to sacrifice a bit of precision in
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:12:03,910'); seek(723.0)">
              order to pick up a low cost LLM
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:12:07,422'); seek(727.0)">
              because of the number of tokens that you'll be sending across and the large use
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:12:10,894'); seek(730.0)">
              that you'll be having. You always want to have a cost effective solution in
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:12:15,158'); seek(735.0)">
              terms of any software that you are deploying. Second is
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:12:19,126'); seek(739.0)">
              the response time. There are different ways in which you can surely improve the response
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:12:22,774'); seek(742.0)">
              times. Maybe you're using an embedded text,
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:12:25,946'); seek(745.0)">
              embeddings or something like that with a vector database by which you can cache it
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:12:29,330'); seek(749.0)">
              or you do something else. But essentially these are some of the key
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:12:32,826'); seek(752.0)">
              factors, the three key factors that I have seen with different customers when
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:12:36,850'); seek(756.0)">
              they are evaluating llms. And obviously this is a summarization of what
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:12:40,698'); seek(760.0)">
              I just spoke about, which is LLM one, two and three,
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:12:43,714'); seek(763.0)">
              how they compare. And then it's up to the customer how they want to
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:12:47,594'); seek(767.0)">
              pick up a particular LLM and what they want to use it against.
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:12:51,414'); seek(771.0)">
              Let's talk about customization. When it comes to customization of the llms,
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:12:56,494'); seek(776.0)">
              there are four different ways in which I have seen the customers
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:13:00,150'); seek(780.0)">
              to be using the llms. And one of the most common use
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:13:04,134'); seek(784.0)">
              cases that they have is prompt engineering. So that's when you are
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:13:07,966'); seek(787.0)">
              sending a request to the llms. For example, you're using an anthropic cloud
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:13:13,566'); seek(793.0)">
              model on Amazon Bedrock. You are going to be using one of
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:13:16,806'); seek(796.0)">
              the playgrounds and just send a request and ask for.
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:13:19,918'); seek(799.0)">
              Give me details of when was the
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:13:23,404'); seek(803.0)">
              last major incident which has happened in software
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:13:27,228'); seek(807.0)">
              engineering around the best practices or something like that. So that's
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:13:30,588'); seek(810.0)">
              a problem, engineering. Just asking a question. You're expecting a response from the LLC.
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:13:35,340'); seek(815.0)">
              A more nuanced one is a retrieval augmented generation, which is
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:13:39,068'); seek(819.0)">
              Rag, where you are able to use
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:13:43,452'); seek(823.0)">
              Rag, which as a better solution and as a better cost
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:13:47,300'); seek(827.0)">
              benefit, and you can use it for customizing your
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:13:51,274'); seek(831.0)">
              llms. Using rag then comes fine tuning,
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:13:55,266'); seek(835.0)">
              which is more time consuming, it is more complex.
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:13:58,882'); seek(838.0)">
              There is a lot of data and other things which would be needed.
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:14:01,866'); seek(841.0)">
              And compared to rag, fine tuning is a special
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:14:05,338'); seek(845.0)">
              case. I would say if you really want to have that level of
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:14:08,978'); seek(848.0)">
              control over the responses, then maybe you can think of fine
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:14:12,930'); seek(852.0)">
              tuning. And the last would be continued pre training where you
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:14:16,334'); seek(856.0)">
              are essentially loading the model and customizing
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:14:20,174'); seek(860.0)">
              it way more. And obviously the complexity increases as you go
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:14:24,174'); seek(864.0)">
              from prompt engineering to rack to fine tuning to
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:14:27,790'); seek(867.0)">
              ultimately continued pre training. One of
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:14:31,406'); seek(871.0)">
              the most common cases of a rush of
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:14:35,334'); seek(875.0)">
              LLMs that has been seen is everyone tries to
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:14:38,774'); seek(878.0)">
              start doing fine tuning, thinking that the LLMs can be made
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:14:42,730'); seek(882.0)">
              aware of specific knowledge and facts about the organization's
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:14:46,706'); seek(886.0)">
              code base or domain knowledge, etcetera. What has been observed
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:14:50,802'); seek(890.0)">
              is in majority of the cases, rag is good enough.
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:14:54,994'); seek(894.0)">
              It offers a better solution. It is more cost effective
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:14:59,026'); seek(899.0)">
              from in terms of cost benefit ratio between rag
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:15:02,234'); seek(902.0)">
              and fine tuning. And fine tuning requires considerably more computational
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:15:06,922'); seek(906.0)">
              resources and expertise. It introduces even more challenges around
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:15:11,240'); seek(911.0)">
              sensitivity and the proprietary data than rag.
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:15:15,512'); seek(915.0)">
              And there's obviously the risk of underfitting or overfitting if you
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:15:19,600'); seek(919.0)">
              don't have enough data which is available for fine
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:15:22,920'); seek(922.0)">
              tuning. So do have a very clear benchmarking
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:15:28,840'); seek(928.0)">
              to see how your model is performing with prompt engineering versus
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:15:32,880'); seek(932.0)">
              rag. And then think about whether fine tuning
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:15:36,572'); seek(936.0)">
              is the right solution that you want to go for without much
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:15:40,244'); seek(940.0)">
              evaluation. You may be jumping into a
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:15:43,524'); seek(943.0)">
              technology solution, but which may be a much more difficult thing to manage
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:15:47,348'); seek(947.0)">
              in the long term. Now customizing, now here we
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:15:50,868'); seek(950.0)">
              talk about customizing the business responses. So what's really going
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:15:54,548'); seek(954.0)">
              to help drive your business in generative AI is what's important
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:15:58,452'); seek(958.0)">
              for your customers, what's important for your products,
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:16:01,722'); seek(961.0)">
              which you're creating, and how you go about that. And you can
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:16:05,514'); seek(965.0)">
              leverage different mechanisms here. And this is where fine
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:16:09,442'); seek(969.0)">
              tuning and continued pre training comes into picture.
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:16:13,274'); seek(973.0)">
              You talk about the purpose, it's basically maximizing the accuracy of
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:16:16,858'); seek(976.0)">
              the specific tasks that you're having.
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:16:19,514'); seek(979.0)">
              And we have comparatively smaller number of
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:16:23,114'); seek(983.0)">
              label data. But then when it comes to continued pre
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:16:26,894'); seek(986.0)">
              training, that's where you want to maintain the model for
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:16:30,622'); seek(990.0)">
              a longer duration on your specific domain.
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:16:33,686'); seek(993.0)">
              That is hyper customizations and large number of unlabeled data
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:16:37,574'); seek(997.0)">
              sets that you will be using. Now, as I mentioned before,
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:16:41,494'); seek(1001.0)">
              Amazon Bedrock can help remove the heavy lifting for these
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:16:44,774'); seek(1004.0)">
              kind of model customization process. But be
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:16:48,254'); seek(1008.0)">
              very clear on your use case as to
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:16:51,902'); seek(1011.0)">
              when you would be using a rag versus a fine tuning or prompt
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:16:55,446'); seek(1015.0)">
              engineering, and why you would want to use a more complex customization
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:16:59,704'); seek(1019.0)">
              than the one that you're getting. So without that clarity at a business
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:17:03,272'); seek(1023.0)">
              level, it will be quite difficult for you to just adopt the LLM
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:17:06,952'); seek(1026.0)">
              and make sure that it is viable in the long term.
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:17:10,264'); seek(1030.0)">
              Now let's talk about Amazon Bedrock.
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:17:13,224'); seek(1033.0)">
              Amazon Bedrock is basically a way for simplifying
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:17:17,256'); seek(1037.0)">
              the access to foundation models and providing an integration
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:17:20,888'); seek(1040.0)">
              layer for you via a single API which is an invoke model API.
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:17:25,165'); seek(1045.0)">
              You get access to different models which are available within Amazon bedrock.
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:17:29,797'); seek(1049.0)">
              Some of the models which you have here is the
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:17:34,197'); seek(1054.0)">
              stability AI model. You have the Amazon Titan, you have cloud,
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:17:37,677'); seek(1057.0)">
              you have Lama models, etcetera. Customers have often told us
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:17:41,661'); seek(1061.0)">
              that one of the most important features of bedrock is how easy it
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:17:45,093'); seek(1065.0)">
              makes it to experiment with and select and combine different range of
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:17:49,157'); seek(1069.0)">
              foundation models. It's still very early days and we are
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:17:52,730'); seek(1072.0)">
              all just getting started and customers are moving extremely fast.
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:17:56,530'); seek(1076.0)">
              And the key aspect is customers want to
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:18:00,410'); seek(1080.0)">
              experiment, they want to deploy, they want to iterate on whatever
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:18:03,666'); seek(1083.0)">
              they have done. And today Bedrock provides access to
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:18:07,434'); seek(1087.0)">
              wide range of foundation models from different organizations
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:18:11,746'); seek(1091.0)">
              and as well as the Amazon Titan models that you have. So once you have
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:18:15,366'); seek(1095.0)">
              access to the bedrock API itself, invoking one
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:18:18,598'); seek(1098.0)">
              of these models is extremely straightforward. I'll talk about it
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:18:21,950'); seek(1101.0)">
              in a bit. Now let's talk about the architectural patterns
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:18:25,430'); seek(1105.0)">
              that you have when using Amazon Bedrock.
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:18:29,014'); seek(1109.0)">
              Obviously with Amazon Bedrock you have different knowledge bases
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:18:33,070'); seek(1113.0)">
              for Amazon Bedrock which you will be using. And to
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:18:37,102'); seek(1117.0)">
              equip a foundation model with an up to date proprietary
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:18:40,462'); seek(1120.0)">
              information organization often talk about retrieval augmented
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:18:44,342'); seek(1124.0)">
              generation. We spoke about it a little bit earlier when during
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:18:48,742'); seek(1128.0)">
              the customization slide. It's basically a technique where
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:18:52,630'); seek(1132.0)">
              you're fetching the data from the company's data sources,
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:18:56,462'); seek(1136.0)">
              enriching the prompt with that particular data and delivering
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:19:00,046'); seek(1140.0)">
              more relevant and accurate responses. We have knowledge
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:19:04,054'); seek(1144.0)">
              bases within Amazon Bedrock which helps you
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:19:07,606'); seek(1147.0)">
              in a fully managed rag capability and it allows
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:19:11,360'); seek(1151.0)">
              you to customize the foundation model responses with contextual
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:19:15,408'); seek(1155.0)">
              and relevant company data. So essentially it
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:19:19,520'); seek(1159.0)">
              helps you securely connect to your foundation models. It's a fully managed
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:19:23,736'); seek(1163.0)">
              rag and it's a built in session context management for
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:19:27,352'); seek(1167.0)">
              multi tone conversations. And obviously you
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:19:31,288'); seek(1171.0)">
              also have automated citations with retrievals to improve the transparency
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:19:35,544'); seek(1175.0)">
              that you get. So how does it work for you?
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:19:38,508'); seek(1178.0)">
              So let's say you have a user query someone has asked about
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:19:43,844'); seek(1183.0)">
              how can I get the latest details about my statement or something.
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:19:47,652'); seek(1187.0)">
              Now that information goes into Amazon Bedrock
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:19:51,532'); seek(1191.0)">
              and it has the knowledge basis which is associated
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:19:55,236'); seek(1195.0)">
              with that particular Amazon bedrock.
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:19:58,372'); seek(1198.0)">
              And it's an iterative process going to look at the knowledge basis for Amazon Bedrock
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:20:02,556'); seek(1202.0)">
              and based on that it's going to augment the prompt that
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:20:05,950'); seek(1205.0)">
              you have received and ultimately you are going to use one of the models,
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:20:10,574'); seek(1210.0)">
              which is the foundation models, be the Claude Llama,
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:20:14,358'); seek(1214.0)">
              Titan or Jurassic models, and ultimately provide a response
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:20:17,854'); seek(1217.0)">
              to your customer. All the information that you are retrieving
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:20:22,414'); seek(1222.0)">
              as part of this process comes from the source
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:20:25,734'); seek(1225.0)">
              citations and the source which you have within the knowledge base.
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:20:29,630'); seek(1229.0)">
              And ultimately it gives you the citations to the knowledge base. In order to improve
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:20:32,852'); seek(1232.0)">
              the transparency. You also have Amazon Q, which is
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:20:36,428'); seek(1236.0)">
              an which has a similar approach when it comes to integrating with Amazon Connect.
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:20:40,732'); seek(1240.0)">
              Not something which we are covering for this particular session, but it
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:20:44,772'); seek(1244.0)">
              has a similar aspect of being able to use your
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:20:48,516'); seek(1248.0)">
              knowledge bases and then give you customized responses.
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:20:52,644'); seek(1252.0)">
              Another architectural pattern is the fine tuning. We spoke about it
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:20:56,908'); seek(1256.0)">
              earlier. So let's say you want to have a very specific
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:21:01,068'); seek(1261.0)">
              task for which you need to have fine tuning. Simply point
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:21:05,084'); seek(1265.0)">
              to those examples of that particular data, which is an
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:21:08,804'); seek(1268.0)">
              S three and they have been labeled. And then Amazon
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:21:12,284'); seek(1272.0)">
              bedrock makes a copy of the base model,
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:21:15,964'); seek(1275.0)">
              trains it, and creates a private fine tuned model so you
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:21:19,668'); seek(1279.0)">
              can get tailored responses. So how does that
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:21:23,284'); seek(1283.0)">
              work? Essentially you're making use of one of
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:21:26,966'); seek(1286.0)">
              the foundation models, be it a Lama two model or a Titan
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:21:30,614'); seek(1290.0)">
              model. For these specific tasks you are keeping all of
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:21:34,334'); seek(1294.0)">
              your specific labeled data sets in Amazon
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:21:37,798'); seek(1297.0)">
              S three, and then you are using that
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:21:41,054'); seek(1301.0)">
              data set in order to make your model
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:21:44,486'); seek(1304.0)">
              better to get tailored responses. So today you will have fine tuning
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:21:48,510'); seek(1308.0)">
              available with Lama models cohere, command,
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:21:51,874'); seek(1311.0)">
              Titan and express Titan multi model and
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:21:55,850'); seek(1315.0)">
              Titan image generator. Fine tuning will be very soon coming
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:21:59,418'); seek(1319.0)">
              into anthropic cloud models, but today it is not available.
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:22:04,554'); seek(1324.0)">
              So this creates a copy. You have the label
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:22:08,242'); seek(1328.0)">
              dataset which is in Amazon S three, and from there you are able
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:22:11,562'); seek(1331.0)">
              to fine tune the model and get the generated responses.
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:22:15,034'); seek(1335.0)">
              Now let's talk about how do you invoke these models.
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:22:18,744'); seek(1338.0)">
              One of the most common patterns that you have with respect to invoking
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:22:22,184'); seek(1342.0)">
              these models is by using API gateway. So it's
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:22:25,888'); seek(1345.0)">
              a very well tested serverless pattern which has been there
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:22:29,832'); seek(1349.0)">
              in existence even before Amazon bedrock instead of bedrock. You would be
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:22:33,312'); seek(1353.0)">
              having, I don't know, ECS or EKS or
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:22:38,208'); seek(1358.0)">
              just something running on a compute somewhere.
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:22:41,536'); seek(1361.0)">
              And you can use Amazon Lambda AWS lambda for doing that invocation
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:22:45,712'); seek(1365.0)">
              with bedrock as well. You are able to use the same pattern and
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:22:49,128'); seek(1369.0)">
              it leverages the event driven architecture that you have
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:22:52,344'); seek(1372.0)">
              been using or maybe using with Amazon API gateway.
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:22:55,760'); seek(1375.0)">
              And it doesn't always have to be Amazon API gateway. You can use it with
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:22:59,184'); seek(1379.0)">
              any integration layer which can support AWS Lambda to invoke
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:23:03,040'); seek(1383.0)">
              the bedrock APIs. And finally, instead of AWS
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:23:07,472'); seek(1387.0)">
              lambda, you can also have the same behavior which let's say if
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:23:10,896'); seek(1390.0)">
              you're having a long running compute and EC,
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:23:13,934'); seek(1393.0)">
              two ecs or eks, and then you can invoke bedrock API
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:23:17,270'); seek(1397.0)">
              in the exact same way. For this particular
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:23:20,710'); seek(1400.0)">
              example, let's consider that you are having two models which you have created
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:23:24,230'); seek(1404.0)">
              for your request and response. Payload request is saying that
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:23:27,422'); seek(1407.0)">
              you need to have a prompt which is going in and response is saying that
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:23:31,294'); seek(1411.0)">
              you have a response that is coming back and a status code that is coming
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:23:34,598'); seek(1414.0)">
              back. When you want to invoke the Amazon
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:23:39,132'); seek(1419.0)">
              bedrock endpoint, you're going to be writing a very simple
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:23:43,012'); seek(1423.0)">
              lambda code which is going to be using the boto three API.
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:23:46,716'); seek(1426.0)">
              So let's walk through this API. So you guys basically
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:23:50,156'); seek(1430.0)">
              creating a client of bedrock using the bedrock runtime.
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:23:54,100'); seek(1434.0)">
              With boto three you are creating the body which
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:23:57,636'); seek(1437.0)">
              is the prompt, the max tokens that you need to get
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:24:01,492'); seek(1441.0)">
              as a sample response, the temperature, etcetera.
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:24:04,868'); seek(1444.0)">
              And then you're selecting the model id. So here I have
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:24:08,282'); seek(1448.0)">
              selected anthropic cloud model. You can also select any
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:24:11,850'); seek(1451.0)">
              of the other model like a Titan model, or you can select the
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:24:17,714'); seek(1457.0)">
              Lama model, any of the model that you want. And once
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:24:21,962'); seek(1461.0)">
              you select the model id and you select the payload structure
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:24:25,234'); seek(1465.0)">
              that you are sending. So be mindful that this particular payload structure can change
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:24:29,274'); seek(1469.0)">
              depending on the model that you are invoking. And you can just use
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:24:33,618'); seek(1473.0)">
              invoke model and give you a response. And that
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:24:37,034'); seek(1477.0)">
              response is how you would return it back by using the same model
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:24:40,834'); seek(1480.0)">
              structure that you have used earlier. Now this particular response
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:24:44,650'); seek(1484.0)">
              request payload is structure would be differing based on the
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:24:49,090'); seek(1489.0)">
              model that you are using and the model id will also change based
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:24:52,450'); seek(1492.0)">
              on the model that you are intending to use. So that's one of
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:24:55,858'); seek(1495.0)">
              the way of invoking it if you're using Lambda and API gateway, and even if
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:24:59,338'); seek(1499.0)">
              you're not using API gateway, anything else which can integrate with that
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:25:03,274'); seek(1503.0)">
              you can use. Now let's say you're not using any lambda you just
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:25:06,650'); seek(1506.0)">
              want to use from a generic application. You can essentially use boto
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:25:10,402'); seek(1510.0)">
              three and you can use a temporary credentials in order to gain access
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:25:13,818'); seek(1513.0)">
              and ultimately invoke the bedrock API. And for any reason
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:25:18,170'); seek(1518.0)">
              if the AWS SDK is not available to you.
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:25:22,002'); seek(1522.0)">
              You can also leverage AWS SIG V four for
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:25:25,498'); seek(1525.0)">
              constructing a valid request payload and invoking the bedrock
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:25:29,242'); seek(1529.0)">
              API. So this is a similar example,
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:25:33,562'); seek(1533.0)">
              quite similar to the one that has been shown earlier.
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:25:37,874'); seek(1537.0)">
              The only difference here is we don't have the lambda handler with
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:25:42,010'); seek(1542.0)">
              the event context and the event and the context. Here we are directly using the
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:25:46,058'); seek(1546.0)">
              Botox reap and we are getting a response from here.
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:25:48,810'); seek(1548.0)">
              So you can embed it in any of the applications which has
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:25:52,218'); seek(1552.0)">
              access to the temporary credentials and you should be able to access the bedrock API.
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:25:57,034'); seek(1557.0)">
              Talking about operational excellence, one of the things that I had spoken about earlier
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:26:01,082'); seek(1561.0)">
              is having good insight into your application.
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:26:04,386'); seek(1564.0)">
              So we spoke about how do you invoke the application, how do you have the
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:26:08,250'); seek(1568.0)">
              API driven approach so that you're able to have a versioning,
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:26:11,642'); seek(1571.0)">
              you're able to have visibility of what is invoking what,
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:26:15,506'); seek(1575.0)">
              and you're able to have temporary credentials, best practices, etc.
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:26:18,618'); seek(1578.0)">
              Etcetera. Now we talk about observability that you
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:26:22,010'); seek(1582.0)">
              would be getting with Amazon bedrock and that's the invocation login.
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:26:26,010'); seek(1586.0)">
              So customers want to know what was the invocation, what was the prompt
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:26:30,314'); seek(1590.0)">
              which was sent and what kind of response did I get.
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:26:33,890'); seek(1593.0)">
              You can enable it at the bedrock level and all of these
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:26:37,546'); seek(1597.0)">
              logs can go into Amazon s three or cloudwatch or both.
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:26:41,178'); seek(1601.0)">
              Here is a sample of a log structure where you
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:26:45,066'); seek(1605.0)">
              have the input body which was sent by the requester
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:26:49,474'); seek(1609.0)">
              either via lambda or any other way by which the API has been invoked.
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:26:53,954'); seek(1613.0)">
              And you can see that this is the input someone is asking
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:26:57,694'); seek(1617.0)">
              explain the three body problem. And here the response is coming in
            </span>
            
            <span id="chunk-429" class="transcript-chunks" onclick="console.log('00:27:01,654'); seek(1621.0)">
              in terms of the number of tokens that has been given. So you will notice
            </span>
            
            <span id="chunk-430" class="transcript-chunks" onclick="console.log('00:27:05,262'); seek(1625.0)">
              that because we had given a maximum token of 300, the response
            </span>
            
            <span id="chunk-431" class="transcript-chunks" onclick="console.log('00:27:09,310'); seek(1629.0)">
              token count is 296. For the purpose of the presentation I've
            </span>
            
            <span id="chunk-432" class="transcript-chunks" onclick="console.log('00:27:12,638'); seek(1632.0)">
              just truncated what is there in the completion response.
            </span>
            
            <span id="chunk-433" class="transcript-chunks" onclick="console.log('00:27:16,278'); seek(1636.0)">
              But here you will have a response coming from the model. In this case it
            </span>
            
            <span id="chunk-434" class="transcript-chunks" onclick="console.log('00:27:19,558'); seek(1639.0)">
              was a cloud model which had been used for that. So this logging will be
            </span>
            
            <span id="chunk-435" class="transcript-chunks" onclick="console.log('00:27:23,510'); seek(1643.0)">
              available for you directly within Cloudwatch. And then from Cloudwatch
            </span>
            
            <span id="chunk-436" class="transcript-chunks" onclick="console.log('00:27:26,774'); seek(1646.0)">
              onwards you can change it to let's say s three or
            </span>
            
            <span id="chunk-437" class="transcript-chunks" onclick="console.log('00:27:30,406'); seek(1650.0)">
              maybe use it for any kind of future use. Talking about metrics,
            </span>
            
            <span id="chunk-438" class="transcript-chunks" onclick="console.log('00:27:34,758'); seek(1654.0)">
              you have these metrics available out of the box for Cloudwatch
            </span>
            
            <span id="chunk-439" class="transcript-chunks" onclick="console.log('00:27:38,478'); seek(1658.0)">
              with Amazon Bedrock, which would be your number of invocations, your latency
            </span>
            
            <span id="chunk-440" class="transcript-chunks" onclick="console.log('00:27:42,342'); seek(1662.0)">
              that you're having, any kind of client and server side errors, any throttling that
            </span>
            
            <span id="chunk-441" class="transcript-chunks" onclick="console.log('00:27:46,238'); seek(1666.0)">
              you're having, and obviously the token count, the input and output,
            </span>
            
            <span id="chunk-442" class="transcript-chunks" onclick="console.log('00:27:49,758'); seek(1669.0)">
              you saw a sample of it in the previous log structure is going to be
            </span>
            
            <span id="chunk-443" class="transcript-chunks" onclick="console.log('00:27:52,718'); seek(1672.0)">
              the same. Now, talking about the
            </span>
            
            <span id="chunk-444" class="transcript-chunks" onclick="console.log('00:27:57,006'); seek(1677.0)">
              model evaluation bedrock currently has
            </span>
            
            <span id="chunk-445" class="transcript-chunks" onclick="console.log('00:28:00,142'); seek(1680.0)">
              in preview, I believe a way for you to evaluate
            </span>
            
            <span id="chunk-446" class="transcript-chunks" onclick="console.log('00:28:04,054'); seek(1684.0)">
              the models. Now the models can be evaluated for robustness,
            </span>
            
            <span id="chunk-447" class="transcript-chunks" onclick="console.log('00:28:08,198'); seek(1688.0)">
              it can be evaluated for toxicity and accuracy.
            </span>
            
            <span id="chunk-448" class="transcript-chunks" onclick="console.log('00:28:12,294'); seek(1692.0)">
              There is on the AWS console,
            </span>
            
            <span id="chunk-449" class="transcript-chunks" onclick="console.log('00:28:15,390'); seek(1695.0)">
              you you can can essentially evaluate the model using recommended metrics.
            </span>
            
            <span id="chunk-450" class="transcript-chunks" onclick="console.log('00:28:18,702'); seek(1698.0)">
              There's an automated evaluation, but you can also choose
            </span>
            
            <span id="chunk-451" class="transcript-chunks" onclick="console.log('00:28:22,718'); seek(1702.0)">
              what kind of task you're evaluating it for. For example,
            </span>
            
            <span id="chunk-452" class="transcript-chunks" onclick="console.log('00:28:26,518'); seek(1706.0)">
              this particular screenshot is from the AWS console, which allows you to
            </span>
            
            <span id="chunk-453" class="transcript-chunks" onclick="console.log('00:28:30,094'); seek(1710.0)">
              evaluate for a question and answer scenario for Amazon bedrock.
            </span>
            
            <span id="chunk-454" class="transcript-chunks" onclick="console.log('00:28:34,582'); seek(1714.0)">
              And we are using the anthropic cloud model and
            </span>
            
            <span id="chunk-455" class="transcript-chunks" onclick="console.log('00:28:38,062'); seek(1718.0)">
              these were the responses which we received on the
            </span>
            
            <span id="chunk-456" class="transcript-chunks" onclick="console.log('00:28:41,822'); seek(1721.0)">
              accuracy and the toxicity that was evaluated
            </span>
            
            <span id="chunk-457" class="transcript-chunks" onclick="console.log('00:28:45,166'); seek(1725.0)">
              against. And you can also bring your own prompt data set
            </span>
            
            <span id="chunk-458" class="transcript-chunks" onclick="console.log('00:28:48,574'); seek(1728.0)">
              or use built and curated prompt data sets for this purpose.
            </span>
            
            <span id="chunk-459" class="transcript-chunks" onclick="console.log('00:28:52,774'); seek(1732.0)">
              So these are some of the observability and insight
            </span>
            
            <span id="chunk-460" class="transcript-chunks" onclick="console.log('00:28:56,678'); seek(1736.0)">
              related details that you can potentially use when you
            </span>
            
            <span id="chunk-461" class="transcript-chunks" onclick="console.log('00:28:59,950'); seek(1739.0)">
              are thinking about using bedrock as your single API for different
            </span>
            
            <span id="chunk-462" class="transcript-chunks" onclick="console.log('00:29:03,614'); seek(1743.0)">
              foundation models. And finally, we want to talk about the
            </span>
            
            <span id="chunk-463" class="transcript-chunks" onclick="console.log('00:29:06,950'); seek(1746.0)">
              guardrails because as we talk about generative
            </span>
            
            <span id="chunk-464" class="transcript-chunks" onclick="console.log('00:29:10,930'); seek(1750.0)">
              AI, there are different challenges around undesirable,
            </span>
            
            <span id="chunk-465" class="transcript-chunks" onclick="console.log('00:29:14,418'); seek(1754.0)">
              irrelevant topic responses or controversial queries or
            </span>
            
            <span id="chunk-466" class="transcript-chunks" onclick="console.log('00:29:18,114'); seek(1758.0)">
              responses which you will be getting, toxicity of your responses,
            </span>
            
            <span id="chunk-467" class="transcript-chunks" onclick="console.log('00:29:22,434'); seek(1762.0)">
              privacy protection bias, stereotyping propagation
            </span>
            
            <span id="chunk-468" class="transcript-chunks" onclick="console.log('00:29:26,778'); seek(1766.0)">
              and all of those things. So as we talk about
            </span>
            
            <span id="chunk-469" class="transcript-chunks" onclick="console.log('00:29:29,978'); seek(1769.0)">
              these new challenges, you also want to talk about what
            </span>
            
            <span id="chunk-470" class="transcript-chunks" onclick="console.log('00:29:33,860'); seek(1773.0)">
              kind of guardrails you will be applying for your models.
            </span>
            
            <span id="chunk-471" class="transcript-chunks" onclick="console.log('00:29:37,244'); seek(1777.0)">
              One open source solution that you have is with Nvidia Nemo
            </span>
            
            <span id="chunk-472" class="transcript-chunks" onclick="console.log('00:29:41,684'); seek(1781.0)">
              guardrails. So this is basically for building
            </span>
            
            <span id="chunk-473" class="transcript-chunks" onclick="console.log('00:29:45,756'); seek(1785.0)">
              trustworthy, safe and secure llms. So you can define the
            </span>
            
            <span id="chunk-474" class="transcript-chunks" onclick="console.log('00:29:49,740'); seek(1789.0)">
              guardrails or rails and to guide and safeguard,
            </span>
            
            <span id="chunk-475" class="transcript-chunks" onclick="console.log('00:29:53,492'); seek(1793.0)">
              guide and have a safeguarded conversation. And you can
            </span>
            
            <span id="chunk-476" class="transcript-chunks" onclick="console.log('00:29:56,732'); seek(1796.0)">
              also choose to define the behavior of your LLM based application
            </span>
            
            <span id="chunk-477" class="transcript-chunks" onclick="console.log('00:30:00,452'); seek(1800.0)">
              for specific topics and prevent it from engaging in any
            </span>
            
            <span id="chunk-478" class="transcript-chunks" onclick="console.log('00:30:03,956'); seek(1803.0)">
              discussions which are in unwanted topics. You can also
            </span>
            
            <span id="chunk-479" class="transcript-chunks" onclick="console.log('00:30:07,908'); seek(1807.0)">
              start connecting different models using LAN chain and
            </span>
            
            <span id="chunk-480" class="transcript-chunks" onclick="console.log('00:30:11,652'); seek(1811.0)">
              other services which you can have. So it's kind of like a shim
            </span>
            
            <span id="chunk-481" class="transcript-chunks" onclick="console.log('00:30:15,236'); seek(1815.0)">
              layer which is sitting between your application which
            </span>
            
            <span id="chunk-482" class="transcript-chunks" onclick="console.log('00:30:18,500'); seek(1818.0)">
              is going to be invoking an LLM. So here you can define all your
            </span>
            
            <span id="chunk-483" class="transcript-chunks" onclick="console.log('00:30:22,364'); seek(1822.0)">
              programmable guardrails and then you
            </span>
            
            <span id="chunk-484" class="transcript-chunks" onclick="console.log('00:30:26,876'); seek(1826.0)">
              can kind of steer your llms in order to follow a predefined
            </span>
            
            <span id="chunk-485" class="transcript-chunks" onclick="console.log('00:30:30,452'); seek(1830.0)">
              conversation path and enforce standard operating
            </span>
            
            <span id="chunk-486" class="transcript-chunks" onclick="console.log('00:30:34,458'); seek(1834.0)">
              procedures. So these kind of standard operating procedures
            </span>
            
            <span id="chunk-487" class="transcript-chunks" onclick="console.log('00:30:37,970'); seek(1837.0)">
              are some of the core context
            </span>
            
            <span id="chunk-488" class="transcript-chunks" onclick="console.log('00:30:41,530'); seek(1841.0)">
              when it comes to building an operational excellence practice,
            </span>
            
            <span id="chunk-489" class="transcript-chunks" onclick="console.log('00:30:45,138'); seek(1845.0)">
              especially when you are building out the llms. So these are some
            </span>
            
            <span id="chunk-490" class="transcript-chunks" onclick="console.log('00:30:48,498'); seek(1848.0)">
              of the same points which I have mentioned. And you
            </span>
            
            <span id="chunk-491" class="transcript-chunks" onclick="console.log('00:30:52,346'); seek(1852.0)">
              can have a look at GitHub and there is, I'll give you
            </span>
            
            <span id="chunk-492" class="transcript-chunks" onclick="console.log('00:30:55,698'); seek(1855.0)">
              a link towards the end of the session as well,
            </span>
            
            <span id="chunk-493" class="transcript-chunks" onclick="console.log('00:30:59,178'); seek(1859.0)">
              where within Amazon bedrock samples
            </span>
            
            <span id="chunk-494" class="transcript-chunks" onclick="console.log('00:31:03,352'); seek(1863.0)">
              are available which you can take a look at it and you can see how
            </span>
            
            <span id="chunk-495" class="transcript-chunks" onclick="console.log('00:31:06,480'); seek(1866.0)">
              the guardrails have been incorporated, is basically a config
            </span>
            
            <span id="chunk-496" class="transcript-chunks" onclick="console.log('00:31:10,080'); seek(1870.0)">
              YAML file and you give an
            </span>
            
            <span id="chunk-497" class="transcript-chunks" onclick="console.log('00:31:13,368'); seek(1873.0)">
              input rail and the input rails are basically applied to
            </span>
            
            <span id="chunk-498" class="transcript-chunks" onclick="console.log('00:31:17,168'); seek(1877.0)">
              the inputs from the user and it can reject the input or
            </span>
            
            <span id="chunk-499" class="transcript-chunks" onclick="console.log('00:31:21,088'); seek(1881.0)">
              we can stop any additional processing. Then you have the dialogue
            </span>
            
            <span id="chunk-500" class="transcript-chunks" onclick="console.log('00:31:24,552'); seek(1884.0)">
              rails which is to influence how the LLM is prompted
            </span>
            
            <span id="chunk-501" class="transcript-chunks" onclick="console.log('00:31:28,320'); seek(1888.0)">
              and they operate on the canonical form messages. You have the
            </span>
            
            <span id="chunk-502" class="transcript-chunks" onclick="console.log('00:31:31,910'); seek(1891.0)">
              retrieval rails which are applied on the retrieval chunk. In the case of
            </span>
            
            <span id="chunk-503" class="transcript-chunks" onclick="console.log('00:31:35,622'); seek(1895.0)">
              say rag scenario, a retrieval rail can reject
            </span>
            
            <span id="chunk-504" class="transcript-chunks" onclick="console.log('00:31:38,886'); seek(1898.0)">
              a chunk or prevent it from being used to prompt the LLM. You have
            </span>
            
            <span id="chunk-505" class="transcript-chunks" onclick="console.log('00:31:42,438'); seek(1902.0)">
              the execution rail and finally you have the output rails. So these
            </span>
            
            <span id="chunk-506" class="transcript-chunks" onclick="console.log('00:31:45,518'); seek(1905.0)">
              are like five different levers which you can control
            </span>
            
            <span id="chunk-507" class="transcript-chunks" onclick="console.log('00:31:49,214'); seek(1909.0)">
              and you can write your config in a config yaml.
            </span>
            
            <span id="chunk-508" class="transcript-chunks" onclick="console.log('00:31:53,054'); seek(1913.0)">
              If you go into the GitHub for this particular guardrail,
            </span>
            
            <span id="chunk-509" class="transcript-chunks" onclick="console.log('00:31:56,830'); seek(1916.0)">
              the Nemo guardrails, you will be able to find more details in
            </span>
            
            <span id="chunk-510" class="transcript-chunks" onclick="console.log('00:32:00,400'); seek(1920.0)">
              there. But this is just an introduction of what kind of guardrails you can add
            </span>
            
            <span id="chunk-511" class="transcript-chunks" onclick="console.log('00:32:03,832'); seek(1923.0)">
              into your LLM invocation. So that you are
            </span>
            
            <span id="chunk-512" class="transcript-chunks" onclick="console.log('00:32:08,784'); seek(1928.0)">
              ensuring that it's safe and you're ensuring
            </span>
            
            <span id="chunk-513" class="transcript-chunks" onclick="console.log('00:32:12,552'); seek(1932.0)">
              the responsible AI best practices when using llms.
            </span>
            
            <span id="chunk-514" class="transcript-chunks" onclick="console.log('00:32:16,704'); seek(1936.0)">
              And finally, this is how it would look when you are using it with Amazon
            </span>
            
            <span id="chunk-515" class="transcript-chunks" onclick="console.log('00:32:20,464'); seek(1940.0)">
              bedrock where you would have the central layer of all the guardrails
            </span>
            
            <span id="chunk-516" class="transcript-chunks" onclick="console.log('00:32:24,110'); seek(1944.0)">
              that you are having. You have the invoker in here and ultimately the bedrock model
            </span>
            
            <span id="chunk-517" class="transcript-chunks" onclick="console.log('00:32:28,182'); seek(1948.0)">
              coming in here and the Neemo guardrails would apply at the central layer.
            </span>
            
            <span id="chunk-518" class="transcript-chunks" onclick="console.log('00:32:31,630'); seek(1951.0)">
              So that is the shim layer that is sitting between your LLM
            </span>
            
            <span id="chunk-519" class="transcript-chunks" onclick="console.log('00:32:35,182'); seek(1955.0)">
              which is exposed via the bedrock and then the invoker who is
            </span>
            
            <span id="chunk-520" class="transcript-chunks" onclick="console.log('00:32:39,262'); seek(1959.0)">
              giving that. And finally
            </span>
            
            <span id="chunk-521" class="transcript-chunks" onclick="console.log('00:32:42,830'); seek(1962.0)">
              the GitHub handle where you can find the details of Amazon bedrock workshop.
            </span>
            
            <span id="chunk-522" class="transcript-chunks" onclick="console.log('00:32:46,286'); seek(1966.0)">
              And this is a screenshot of the UI that you have that
            </span>
            
            <span id="chunk-523" class="transcript-chunks" onclick="console.log('00:32:50,180'); seek(1970.0)">
              closes every, all the topics that I wanted to cover for this session.
            </span>
            
            <span id="chunk-524" class="transcript-chunks" onclick="console.log('00:32:53,996'); seek(1973.0)">
              Talking all the way from what bedrock is what
            </span>
            
            <span id="chunk-525" class="transcript-chunks" onclick="console.log('00:32:57,620'); seek(1977.0)">
              it is offering. How do you invoke bedrock, what kind
            </span>
            
            <span id="chunk-526" class="transcript-chunks" onclick="console.log('00:33:00,924'); seek(1980.0)">
              of observability you're getting out of the box. And finally the guardrails
            </span>
            
            <span id="chunk-527" class="transcript-chunks" onclick="console.log('00:33:05,308'); seek(1985.0)">
              which you can apply for bedrock. Hope this helps.
            </span>
            
            <span id="chunk-528" class="transcript-chunks" onclick="console.log('00:33:08,788'); seek(1988.0)">
              Thank you so much for your time. It's been a pleasure.
            </span>
            
            </div>
          </div>
          
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/Suraj%20Muraleedharan%20-%20Conf42%20Large%20Language%20Models%20%28LLMs%29%202024.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/Suraj%20Muraleedharan%20-%20Conf42%20Large%20Language%20Models%20%28LLMs%29%202024.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #CCB87B;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/llms2024" class="btn btn-sm btn-danger shadow lift" style="background-color: #CCB87B;">
                <i class="fe fe-grid me-2"></i>
                See all 28 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Suraj%20Muraleedharan_llm.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Suraj Muraleedharan
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Principal Architect - Platform Engineering, Global Financial Services @ AWS
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/surajmuraleedharan/?profileId=ACoAAAYfIfcBl1o0F1Zhfm_FmmLBJpVLxX5WhVc" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Suraj Muraleedharan's LinkedIn account" />
                  </a>
                  
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by Suraj Muraleedharan"
                  data-url="https://www.conf42.com/llms2024"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/llms2024"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>






    <!-- WELCOME -->
    <section class="pt-8 pt-md-11 pb-10 pb-md-15 bg-info" id="register">

      <!-- Shape -->
      <div class="shape shape-blur-3 text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>      </div>

      <!-- Content -->
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 col-md-10 col-lg-8 text-center">

            <!-- Heading -->
            <h1 class="display-2 text-white">
              Join the community!
            </h1>

            <!-- Text -->
            <p class="lead text-white text-opacity-80 mb-6 mb-md-8">
              Learn for free, join the best tech learning community 
              for a <a class="text-white" href="https://www.reddit.com/r/sanfrancisco/comments/1bz90f6/why_are_coffee_shops_in_sf_so_expensive/" target="_blank">price of a pumpkin latte</a>.
            </p>

            <!-- Form -->
            <form class="d-flex align-items-center justify-content-center mb-7 mb-md-9">

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Annual
              </span>

              <!-- Switch -->
              <div class="form-check form-check-dark form-switch mx-3">
                <input class="form-check-input" type="checkbox" id="billingSwitch" data-toggle="price" data-target=".price">
              </div>

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Monthly
              </span>

            </form>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->

    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- PRICING -->
    <section class="mt-n8 mt-md-n15">
      <div class="container">
        <div class="row gx-4">
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-1">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Newsletter</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="0" data-monthly="0">0</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Event notifications, weekly newsletter
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Delayed access</b> to all content
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Immediate access to Keynotes & Panels
                  </p>
                </div>
              
              
              </div>
            </div>

            <!-- Card -->
            <div class="card shadow-lg mb-6 border border-success">
              <div class="card-body">

                <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
</script>

<!-- Form -->
<link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
<p class="emailoctopus-success-message text-success"></p>
<p class="emailoctopus-error-message text-danger"></p>
<form
    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
    method="post"
    data-message-success="Thanks! Check your email for further directions!"
    data-message-missing-email-address="Your email address is required."
    data-message-invalid-email-address="Your email address looks incorrect, please try again."
    data-message-bot-submission-error="This doesn't look like a human submission."
    data-message-consent-required="Please check the checkbox to indicate your consent."
    data-message-invalid-parameters-error="This form has missing or invalid fields."
    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
    class="emailoctopus-form"
    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
>
<div class="form-floating emailoctopus-form-row">
    <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
    <label for="field_0">Email address</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
    <label for="field_1">First Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
    <label for="field_2">Last Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
    <label for="field_4">Company</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
    <label for="field_5">Job Title</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
    <label for="field_3">Phone Number</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
    oninput="updateCountry()"
    >
    <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
    </select>
    <label for="field_7">Country</label>
</div>
<input id="country-destination" name="field_7" type="hidden">
<input id="tz-country" name="field_8" type="hidden">

<input
    name="field_6"
    type="hidden"
    value="Large Language Models"
>

<div class="emailoctopus-form-row-consent">
    <input
    type="checkbox"
    id="consent"
    name="consent"
    >
    <label for="consent">
    I consent to the following terms:
    </label>
    <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
    Terms and Conditions
    </a>
    &amp;
    <a href="./code-of-conduct" target="_blank">
    Code of Conduct
    </a>
</div>
<div
    aria-hidden="true"
    class="emailoctopus-form-row-hp"
>
    <input
    type="text"
    name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
    tabindex="-1"
    autocomplete="nope"
    >
</div>
<div class="mt-6 emailoctopus-form-row-subscribe">
    <input
    type="hidden"
    name="successRedirectUrl"
    >
    <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
    Subscribe to free newsletter <i class="fe fe-arrow-right ms-3"></i>
    </button>
</div>
</form>

<!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
<script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

              </div>
            </div>
          </div>
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-0">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Community</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="8.34" data-monthly="10">8.34</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Access to <a href="https://conf42.circle.so/">Circle community platform</a>
                  </p>
                </div>

                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Immediate access</b> to all content
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank"><b>Live events!</b></a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank">Regular office hours, Q&As, CV reviews</a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Courses, quizes & certificates
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Community chats
                  </p>
                </div>
                

                <!-- Button -->
                <a href="https://conf42.circle.so/checkout/subscribe" class="btn w-100 btn-primary">
                  Join the community (7 day free trial)<i class="fe fe-arrow-right ms-3"></i>
                </a>

              </div>
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2025">
                  Observability 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2025">
                  Quantum Computing 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2025">
                  Rustlang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2025">
                  Platform Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2025">
                  MLOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2025">
                  Incident Management 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2025">
                  Kube Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2025">
                  JavaScript 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2025">
                  Prompt Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/robotics2025">
                  Robotics 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2025">
                  DevSecOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2025">
                  Internet of Things (IoT) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>