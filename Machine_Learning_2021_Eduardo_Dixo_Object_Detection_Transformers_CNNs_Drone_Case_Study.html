<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Object Detection using Transformers and CNNs - A Drone Case Study</title>
    <meta name="description" content="Like the Machines need our help to take over the world">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/ml_eduardo.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Object Detection using Transformers and CNNs - A Drone Case Study | Conf42"/>
    <meta property="og:description" content="Drones with mounted cameras provide significant advantages when compared to fixed cameras for object detection and visual tracking scenarios. Given their recent adoption in the wild and late advances in computer vision models, many aerial datasets have been introduced.  In this talk, we’ll explore recent advances in object detection, comparing the challenges of natural images with those recorded by drones. Given the successes achieved by pretraining image classifiers on large datasets, and transferring the learned representations, a set of object detectors fine-tuned on publicly available aerial datasets will be presented and explained. We’ll highlight existing libraries that mitigate the cost of training large models from scratch, by including pretrained model weights and model variants found in the literature. Both Convolutional Neural Networks and the newly developed Transformers applied to vision will be covered and compared, outlining the main features of each architecture. The presentation will be accompanied by code snippets for aiding understanding and delivering practical examples.  This is aimed at a general audience familiar with Python. Knowledge of Computer Vision is a plus but not a requirement as we’ll introduce the necessary concepts. We’ll ground the presented model architectures and libraries on the task of object detection applied to aerial datasets and demonstrate that state-of-the-art methods are within everyone’s reach."/>
    <meta property="og:url" content="https://conf42.com/Machine_Learning_2021_Eduardo_Dixo_Object_Detection_Transformers_CNNs_Drone_Case_Study"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/ML2024_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Machine Learning 2024
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2024-05-30
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/ml2024" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.sreday.com/">
                            SREday San Francisco
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.sreday.com/">
                            SREday London
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.sreday.com/">
                            SREday Amsterdam
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="./support">
                            Support us
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Subscribe for FREE
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #198B91;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Machine Learning 2021 - Online
            </h1>

            <h2 class="text-white">
              
              <time datetime="2021-07-29">July 29 2021</time>
              
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- Like the Machines need our help to take over the world
 -->
              <script>
                const event_date = new Date("2021-07-29T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2021-07-29T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              <a href="#register" class="btn btn-primary shadow lift me-1 mb-3">
                <i class="fe fe-user-check me-2"></i>
                Subscribe to watch
              </a>
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "1s-hXnjHEHI"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <a class="btn btn-info lift mb-3" data-bigpicture='{"ytSrc": "h6kzwaBwrCY"}' href="#">
                <i class="fe fe-eye me-2"></i>
                Watch Premiere
              </a>
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://youtube.com/playlist?list=PLIuxSyKxlQrD02X7IKNNxFMy_K8oEejAu" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Hello, my name is Eduardo Dixo, I\u0027m a senior data scientist at Continental and", "timestamp": "00:00:22,010", "timestamp_s": 22.0}, {"text": "today I\u0027m going to talk to you about object detectors using cnns and", "timestamp": "00:00:25,644", "timestamp_s": 25.0}, {"text": "transformers applied to images recorded by datasets. First I\u0027m going to", "timestamp": "00:00:28,988", "timestamp_s": 28.0}, {"text": "introduce the task of object detectors and also the data set that", "timestamp": "00:00:32,972", "timestamp_s": 32.0}, {"text": "we\u0027ll be using. Next we\u0027ll see some common CNNS based architectures", "timestamp": "00:00:36,268", "timestamp_s": 36.0}, {"text": "like the faster RCNN and retinate before discussing", "timestamp": "00:00:40,258", "timestamp_s": 40.0}, {"text": "the transformer and seeing how the detection transformer performs on our data set.", "timestamp": "00:00:43,782", "timestamp_s": 43.0}, {"text": "So let\u0027s begin by first introducing the task of object detection taskoff.", "timestamp": "00:00:47,984", "timestamp_s": 47.0}, {"text": "Object detection can be regarded as given an input", "timestamp": "00:00:51,798", "timestamp_s": 51.0}, {"text": "image. We want to find all the objects that are present in that image.", "timestamp": "00:00:55,162", "timestamp_s": 55.0}, {"text": "So we need to spatially locate them using bounding", "timestamp": "00:00:58,842", "timestamp_s": 58.0}, {"text": "boxes and also we need to classify them into a set of", "timestamp": "00:01:03,578", "timestamp_s": 63.0}, {"text": "predefined classes. If we compare the task of object detection with", "timestamp": "00:01:07,048", "timestamp_s": 67.0}, {"text": "image classification, for example, in image classification we", "timestamp": "00:01:11,096", "timestamp_s": 71.0}, {"text": "usually have a single main target. In object detection we may", "timestamp": "00:01:14,824", "timestamp_s": 74.0}, {"text": "have different number of objects present in that image", "timestamp": "00:01:18,092", "timestamp_s": 78.0}, {"text": "with different poses, with different scales. And this makes the task", "timestamp": "00:01:22,194", "timestamp_s": 82.0}, {"text": "very challenging, more challenging than the image classification. For example,", "timestamp": "00:01:26,530", "timestamp_s": 86.0}, {"text": "the data set that we are going to use is those visron data set that", "timestamp": "00:01:30,124", "timestamp_s": 90.0}, {"text": "contains nearly 6000 training images and 500 validation", "timestamp": "00:01:33,168", "timestamp_s": 93.0}, {"text": "images. It also contains ten categories from which we are only interested", "timestamp": "00:01:36,582", "timestamp_s": 96.0}, {"text": "in cars. We are going to build an object detector for", "timestamp": "00:01:39,920", "timestamp_s": 99.0}, {"text": "detecting only one class, which will be cars. And this", "timestamp": "00:01:43,332", "timestamp_s": 103.0}, {"text": "data set is interesting because it records the", "timestamp": "00:01:47,092", "timestamp_s": 107.0}, {"text": "images and the different conditions like different weather,", "timestamp": "00:01:50,196", "timestamp_s": 110.0}, {"text": "different lighting, different object density of", "timestamp": "00:01:54,030", "timestamp_s": 114.0}, {"text": "the scenes, different scales of the objects. We have some fast motion artifacts", "timestamp": "00:01:57,304", "timestamp_s": 117.0}, {"text": "because of the movement of the cars or the movement of", "timestamp": "00:02:01,678", "timestamp_s": 121.0}, {"text": "those drone during flight. And also the bounding boxes are annotated for", "timestamp": "00:02:04,808", "timestamp_s": 124.0}, {"text": "occlusion and those truncation. Some applications of", "timestamp": "00:02:08,588", "timestamp_s": 128.0}, {"text": "training such object detector could be interesting", "timestamp": "00:02:12,332", "timestamp_s": 132.0}, {"text": "for road safety, traffic monitoring or", "timestamp": "00:02:16,572", "timestamp_s": 136.0}, {"text": "even driving assistance as finding free tracking", "timestamp": "00:02:20,656", "timestamp_s": 140.0}, {"text": "slots. First, we make a distinction between the one", "timestamp": "00:02:24,630", "timestamp_s": 144.0}, {"text": "stage and two stage object detectors. Two stage object", "timestamp": "00:02:28,592", "timestamp_s": 148.0}, {"text": "detectors contain a region proposal network", "timestamp": "00:02:32,196", "timestamp_s": 152.0}, {"text": "that will output high confidence region proposals", "timestamp": "00:02:36,282", "timestamp_s": 156.0}, {"text": "that should contain an object on it. So it\u0027s not concerned", "timestamp": "00:02:41,410", "timestamp_s": 161.0}, {"text": "what is the class of the object in it, it\u0027s only concerned if there is", "timestamp": "00:02:46,230", "timestamp_s": 166.0}, {"text": "an object or not. And then the object detector head that typically", "timestamp": "00:02:50,168", "timestamp_s": 170.0}, {"text": "drones bounding box regression for finding the position of the object", "timestamp": "00:02:53,646", "timestamp_s": 173.0}, {"text": "and object classification to find its class can attend to these", "timestamp": "00:02:57,580", "timestamp_s": 177.0}, {"text": "proposed regions and by doing so it will", "timestamp": "00:03:01,452", "timestamp_s": 181.0}, {"text": "have a much smaller set of candidate regions that might", "timestamp": "00:03:05,212", "timestamp_s": 185.0}, {"text": "have an object. And this will eliminate", "timestamp": "00:03:08,672", "timestamp_s": 188.0}, {"text": "many of the case positives that we would have otherwise. A one shot", "timestamp": "00:03:12,662", "timestamp_s": 192.0}, {"text": "detector, on the other hand, generates a dense sampling", "timestamp": "00:03:16,598", "timestamp_s": 196.0}, {"text": "of possible object locations. So it will generate lots of", "timestamp": "00:03:20,198", "timestamp_s": 200.0}, {"text": "object candidate locations with different shapes and", "timestamp": "00:03:24,164", "timestamp_s": 204.0}, {"text": "different aspect ratios, and it will process them directly to", "timestamp": "00:03:27,972", "timestamp_s": 207.0}, {"text": "learn the class labels and bounding boxes. The first model that we are", "timestamp": "00:03:31,444", "timestamp_s": 211.0}, {"text": "going to discuss is the faster RCNN. The faster RCNn", "timestamp": "00:03:34,904", "timestamp_s": 214.0}, {"text": "is a two stage object detector that employs two models,", "timestamp": "00:03:38,750", "timestamp_s": 218.0}, {"text": "a region proposal network, and also the classifier head", "timestamp": "00:03:42,062", "timestamp_s": 222.0}, {"text": "that has the bounding box, regression and object classification. We will start", "timestamp": "00:03:46,088", "timestamp_s": 226.0}, {"text": "by following those typical data flow of the image as", "timestamp": "00:03:49,980", "timestamp_s": 229.0}, {"text": "it goes through the architecture. So the image goes through the backbone.", "timestamp": "00:03:53,532", "timestamp_s": 233.0}, {"text": "The backbone goal is to extract eye", "timestamp": "00:03:57,026", "timestamp_s": 237.0}, {"text": "level semantic feature maps from the image. That will be", "timestamp": "00:04:00,450", "timestamp_s": 240.0}, {"text": "useful later for the region proposal network and for the classifier.", "timestamp": "00:04:03,728", "timestamp_s": 243.0}, {"text": "This can be typically achieved by any of the", "timestamp": "00:04:07,110", "timestamp_s": 247.0}, {"text": "shelf convolutional architectures like Rasnet or Vgg.", "timestamp": "00:04:10,368", "timestamp_s": 250.0}, {"text": "As the image goes through these several convolutional", "timestamp": "00:04:14,298", "timestamp_s": 254.0}, {"text": "layers, it gets downsampled", "timestamp": "00:04:18,154", "timestamp_s": 258.0}, {"text": "so it will have smaller width and smaller height,", "timestamp": "00:04:21,818", "timestamp_s": 261.0}, {"text": "but much more depth, meaning that the feature map of the", "timestamp": "00:04:25,710", "timestamp_s": 265.0}, {"text": "last stage of the backbone will have", "timestamp": "00:04:29,144", "timestamp_s": 269.0}, {"text": "many channels. Next we have this region proposal network.", "timestamp": "00:04:32,920", "timestamp_s": 272.0}, {"text": "This region proposal network will predict the object bounds as well", "timestamp": "00:04:36,786", "timestamp_s": 276.0}, {"text": "as the objectness cars. Meaning if it is", "timestamp": "00:04:40,828", "timestamp_s": 280.0}, {"text": "an object or not and it\u0027s a fully convolutional network,", "timestamp": "00:04:44,428", "timestamp_s": 284.0}, {"text": "it will receive as input the feature maps from the backbone.", "timestamp": "00:04:49,770", "timestamp_s": 289.0}, {"text": "It will slide those window over these feature maps. At each", "timestamp": "00:04:53,078", "timestamp_s": 293.0}, {"text": "point of those sliding window it will generate k anchor boxes.", "timestamp": "00:04:56,752", "timestamp_s": 296.0}, {"text": "The number of anchor boxes is parameterized by this k and", "timestamp": "00:05:00,662", "timestamp_s": 300.0}, {"text": "it will have two sibling networks for the outputs,", "timestamp": "00:05:04,516", "timestamp_s": 304.0}, {"text": "one that is two times the number of anchor boxes", "timestamp": "00:05:08,554", "timestamp_s": 308.0}, {"text": "for the score classification in foreground and", "timestamp": "00:05:13,970", "timestamp_s": 313.0}, {"text": "background, and the other one will be four times the", "timestamp": "00:05:17,172", "timestamp_s": 317.0}, {"text": "number of anchor boxes for the bounding box coordinates.", "timestamp": "00:05:20,548", "timestamp_s": 320.0}, {"text": "Finally, now we have a set of regions proposed", "timestamp": "00:05:24,186", "timestamp_s": 324.0}, {"text": "by this region proposal network module, and in a very naive way,", "timestamp": "00:05:28,866", "timestamp_s": 328.0}, {"text": "we could simply crop the image using these proposal regions", "timestamp": "00:05:32,652", "timestamp_s": 332.0}, {"text": "and feed it into another classifier just", "timestamp": "00:05:36,706", "timestamp_s": 336.0}, {"text": "to get the object class. However, we want to make those", "timestamp": "00:05:40,348", "timestamp_s": 340.0}, {"text": "end to end and to reuse the feature maps that we have", "timestamp": "00:05:43,776", "timestamp_s": 343.0}, {"text": "computed from the backbone. And for doing so, we are going", "timestamp": "00:05:47,008", "timestamp_s": 347.0}, {"text": "to map the feature maps to the proposals of the region proposal network using", "timestamp": "00:05:50,288", "timestamp_s": 350.0}, {"text": "this region of interest pulling layer that will extract", "timestamp": "00:05:54,452", "timestamp_s": 354.0}, {"text": "then fixed size features maps from each of these proposals", "timestamp": "00:05:58,202", "timestamp_s": 358.0}, {"text": "from the feature map. The reason these are fixed size", "timestamp": "00:06:02,330", "timestamp_s": 362.0}, {"text": "is because we are going to use fully connected layer that expects fixed size.", "timestamp": "00:06:05,476", "timestamp_s": 365.0}, {"text": "Then we have this classifier that will predict the object", "timestamp": "00:06:09,784", "timestamp_s": 369.0}, {"text": "class as well as the bounding box coordinates.", "timestamp": "00:06:13,640", "timestamp_s": 373.0}, {"text": "We are going to use the detection tool library which is a pytorch based", "timestamp": "00:06:17,218", "timestamp_s": 377.0}, {"text": "deep learning framework for object detectors and", "timestamp": "00:06:21,100", "timestamp_s": 381.0}, {"text": "also semantic segmentation. And we are using to use", "timestamp": "00:06:24,972", "timestamp_s": 384.0}, {"text": "faster RCNn with a Resnet 50 backbone", "timestamp": "00:06:30,190", "timestamp_s": 390.0}, {"text": "using fully features pyramid networks the reason we", "timestamp": "00:06:34,262", "timestamp_s": 394.0}, {"text": "are going to use these feature pyramid networks is because we have", "timestamp": "00:06:38,128", "timestamp_s": 398.0}, {"text": "images in our data set that have very", "timestamp": "00:06:41,536", "timestamp_s": 401.0}, {"text": "small scale so we have small cars and also large cars that we", "timestamp": "00:06:45,092", "timestamp_s": 405.0}, {"text": "want to detectors depending on the altitude that the drone is", "timestamp": "00:06:48,788", "timestamp_s": 408.0}, {"text": "flying. And by using these feature pyramid networks we", "timestamp": "00:06:52,500", "timestamp_s": 412.0}, {"text": "can improve the multiscale object detection because those goal", "timestamp": "00:06:56,008", "timestamp_s": 416.0}, {"text": "of the feature pyramid network is to build these eye level semantic", "timestamp": "00:06:59,358", "timestamp_s": 419.0}, {"text": "feature maps across all the pyramid levels from", "timestamp": "00:07:03,198", "timestamp_s": 423.0}, {"text": "a single image of a single", "timestamp": "00:07:07,436", "timestamp_s": 427.0}, {"text": "resolution. This is done by merging", "timestamp": "00:07:11,244", "timestamp_s": 431.0}, {"text": "the bottom pathway which is the feature maps", "timestamp": "00:07:15,218", "timestamp_s": 435.0}, {"text": "from our CNN backbone that then are upsampled through", "timestamp": "00:07:19,122", "timestamp_s": 439.0}, {"text": "those top line pathway and merged through lateral", "timestamp": "00:07:23,008", "timestamp_s": 443.0}, {"text": "connections in the feature pyramid network", "timestamp": "00:07:26,342", "timestamp_s": 446.0}, {"text": "architecture. For training the faster RCNN the first step is to register", "timestamp": "00:07:30,342", "timestamp_s": 450.0}, {"text": "our data set. We do this so that", "timestamp": "00:07:34,266", "timestamp_s": 454.0}, {"text": "the detectors two knows how to obtain it.", "timestamp": "00:07:38,244", "timestamp_s": 458.0}, {"text": "If we already have the annotations in adjacent cocoa", "timestamp": "00:07:41,810", "timestamp_s": 461.0}, {"text": "format, we can use the register cocoa instances directly.", "timestamp": "00:07:47,350", "timestamp_s": 467.0}, {"text": "In this case we have prepared the annotations in this format so we can use", "timestamp": "00:07:51,182", "timestamp_s": 471.0}, {"text": "those register cocoa instances and we also pass the", "timestamp": "00:07:54,408", "timestamp_s": 474.0}, {"text": "base path images so it knows where to fetch the images", "timestamp": "00:07:58,236", "timestamp_s": 478.0}, {"text": "from. Next, detectors two uses the key value", "timestamp": "00:08:01,666", "timestamp_s": 481.0}, {"text": "config system based on YaML files", "timestamp": "00:08:06,170", "timestamp_s": 486.0}, {"text": "that provide already some common functionality and operations.", "timestamp": "00:08:09,862", "timestamp_s": 489.0}, {"text": "If we require more advanced features, we can drop down to the Python\u0027s", "timestamp": "00:08:13,894", "timestamp_s": 493.0}, {"text": "API or also derive from a", "timestamp": "00:08:17,398", "timestamp_s": 497.0}, {"text": "base config file and implement the attributes. And in here", "timestamp": "00:08:20,528", "timestamp_s": 500.0}, {"text": "what we are going to do is first we load the default configuration", "timestamp": "00:08:24,100", "timestamp_s": 504.0}, {"text": "file. We then inherit from", "timestamp": "00:08:27,882", "timestamp_s": 507.0}, {"text": "the configuration file of the model that we want to fine tune.", "timestamp": "00:08:31,092", "timestamp_s": 511.0}, {"text": "We specify the training and test data sets that we already registered previously.", "timestamp": "00:08:34,170", "timestamp_s": 514.0}, {"text": "We specify the number of workers for the multiprocessing part", "timestamp": "00:08:38,542", "timestamp_s": 518.0}, {"text": "and we load the pretrained model weights from", "timestamp": "00:08:42,072", "timestamp_s": 522.0}, {"text": "the detectors two model zoo. Then we have the learning rate,", "timestamp": "00:08:45,912", "timestamp_s": 525.0}, {"text": "the maximum number of iterations, the batch size, and the steps at", "timestamp": "00:08:49,516", "timestamp_s": 529.0}, {"text": "which to decay the learning rate. All of these are very important parameters", "timestamp": "00:08:52,748", "timestamp_s": 532.0}, {"text": "that we should tune to get the best metrics,", "timestamp": "00:08:56,706", "timestamp_s": 536.0}, {"text": "but also to squeeze the best performance out", "timestamp": "00:09:00,918", "timestamp_s": 540.0}, {"text": "of the GPU. And then we specify the number of classes for", "timestamp": "00:09:04,304", "timestamp_s": 544.0}, {"text": "this particular architecture, which is one", "timestamp": "00:09:07,792", "timestamp_s": 547.0}, {"text": "because we are only interested in detectors cars.", "timestamp": "00:09:11,600", "timestamp_s": 551.0}, {"text": "Finally, we can launch the training using the default trainer class", "timestamp": "00:09:15,258", "timestamp_s": 555.0}, {"text": "that provide out of the box standard training logic.", "timestamp": "00:09:18,916", "timestamp_s": 558.0}, {"text": "If we require, we could also implement our own Python", "timestamp": "00:09:21,834", "timestamp_s": 561.0}, {"text": "training loop or also subclass this default trainer", "timestamp": "00:09:26,250", "timestamp_s": 566.0}, {"text": "in here. Since we are not loading from a checkpoint. We pass this resemb equal", "timestamp": "00:09:30,554", "timestamp_s": 570.0}, {"text": "false. Now we take a look at a one stage detector.", "timestamp": "00:09:33,914", "timestamp_s": 573.0}, {"text": "So retinate is a powerful one stage detector", "timestamp": "00:09:38,578", "timestamp_s": 578.0}, {"text": "that employs the feature pyramid network that we have seen before that helps", "timestamp": "00:09:42,498", "timestamp_s": 582.0}, {"text": "with a multiscale detection of", "timestamp": "00:09:46,338", "timestamp_s": 586.0}, {"text": "the objects and also two", "timestamp": "00:09:49,568", "timestamp_s": 589.0}, {"text": "civilian networks, one for classification and the other for bounding box", "timestamp": "00:09:53,392", "timestamp_s": 593.0}, {"text": "regression. The one stage detectors were typically regarded", "timestamp": "00:09:56,592", "timestamp_s": 596.0}, {"text": "as being faster than the two stage, but they were lagging the accuracy", "timestamp": "00:10:00,694", "timestamp_s": 600.0}, {"text": "of the two stage detectors. So the authors of retinate", "timestamp": "00:10:05,018", "timestamp_s": 605.0}, {"text": "attributed this to the eye class imbalance between", "timestamp": "00:10:09,082", "timestamp_s": 609.0}, {"text": "foreground and background that may happen.", "timestamp": "00:10:12,420", "timestamp_s": 612.0}, {"text": "And the reason is if you remember these", "timestamp": "00:10:15,380", "timestamp_s": 615.0}, {"text": "one stage detectors, they will examples", "timestamp": "00:10:20,072", "timestamp_s": 620.0}, {"text": "a large set of candidate regions, many of them will", "timestamp": "00:10:23,662", "timestamp_s": 623.0}, {"text": "be background, will be easy negatives and they will not contribute", "timestamp": "00:10:26,748", "timestamp_s": 626.0}, {"text": "with a useful learning signal for the network or they", "timestamp": "00:10:30,242", "timestamp_s": 630.0}, {"text": "can even overwhelm the training loss.", "timestamp": "00:10:33,788", "timestamp_s": 633.0}, {"text": "So what they propose is this nova loss called those focal loss that", "timestamp": "00:10:37,570", "timestamp_s": 637.0}, {"text": "adds this modulating factor to the standard cross entropy and", "timestamp": "00:10:42,144", "timestamp_s": 642.0}, {"text": "it will downweight the well classified examples so", "timestamp": "00:10:45,712", "timestamp_s": 645.0}, {"text": "that the model can focus more on the other examples. For the retinate", "timestamp": "00:10:50,608", "timestamp_s": 650.0}, {"text": "we are also going to use REsnet 50 backbone for", "timestamp": "00:10:54,774", "timestamp_s": 654.0}, {"text": "comparison with the faster RCNN. We also use the detectors", "timestamp": "00:10:59,252", "timestamp_s": 659.0}, {"text": "and two library for doing so. Registering the data", "timestamp": "00:11:02,714", "timestamp_s": 662.0}, {"text": "set requires no changes. Launching the training also requires no", "timestamp": "00:11:05,896", "timestamp_s": 665.0}, {"text": "changes, but we need to change the configuration file.", "timestamp": "00:11:09,032", "timestamp_s": 669.0}, {"text": "So in this case we need to inherit from the appropriate model. We also", "timestamp": "00:11:12,766", "timestamp_s": 672.0}, {"text": "need to load the appropriate models from the model zoo. And now", "timestamp": "00:11:16,376", "timestamp_s": 676.0}, {"text": "for setting the number of classes we need to access a", "timestamp": "00:11:20,332", "timestamp_s": 680.0}, {"text": "different attribute of the config which is under those retinate", "timestamp": "00:11:24,284", "timestamp_s": 684.0}, {"text": "num classes. After training both models we see that", "timestamp": "00:11:28,830", "timestamp_s": 688.0}, {"text": "they both have good cocoa evaluation", "timestamp": "00:11:32,640", "timestamp_s": 692.0}, {"text": "metrics. We are using the average precision which", "timestamp": "00:11:36,566", "timestamp_s": 696.0}, {"text": "basically penalizes missing detections", "timestamp": "00:11:40,692", "timestamp_s": 700.0}, {"text": "and also detecting having too many duplicate", "timestamp": "00:11:44,522", "timestamp_s": 704.0}, {"text": "detections for the same object. And we see", "timestamp": "00:11:48,298", "timestamp_s": 708.0}, {"text": "that the average precision is very similar for each model. In this case the", "timestamp": "00:11:51,876", "timestamp_s": 711.0}, {"text": "retina net is better at detecting larger", "timestamp": "00:11:55,928", "timestamp_s": 715.0}, {"text": "objects but worse at smaller objects. But if we look at the average precision,", "timestamp": "00:12:00,878", "timestamp_s": 720.0}, {"text": "they are very equally matched and also the inference results", "timestamp": "00:12:04,798", "timestamp_s": 724.0}, {"text": "as well. Another thing that is commonly employed", "timestamp": "00:12:08,418", "timestamp_s": 728.0}, {"text": "in computer vision is that augmentation for aiding in the generalization", "timestamp": "00:12:12,882", "timestamp_s": 732.0}, {"text": "of the network. And the reason is that we want our object detection", "timestamp": "00:12:18,170", "timestamp_s": 738.0}, {"text": "to work under different lighting, viewpoint scales, et cetera.", "timestamp": "00:12:22,358", "timestamp_s": 742.0}, {"text": "So we can generate an augmentation policy that will bake", "timestamp": "00:12:26,262", "timestamp_s": 746.0}, {"text": "these transformations there. And so", "timestamp": "00:12:29,862", "timestamp_s": 749.0}, {"text": "we pass our data set through this augmentation policy, enriching our data", "timestamp": "00:12:33,124", "timestamp_s": 753.0}, {"text": "set that we will then use for training our model.", "timestamp": "00:12:36,596", "timestamp_s": 756.0}, {"text": "In this example we have an horizontal flip and", "timestamp": "00:12:40,308", "timestamp_s": 760.0}, {"text": "also some we can see on the left the", "timestamp": "00:12:43,992", "timestamp_s": 763.0}, {"text": "augmentation policy used and also some random brightness,", "timestamp": "00:12:47,928", "timestamp_s": 767.0}, {"text": "some random saturation, some random contrast.", "timestamp": "00:12:51,262", "timestamp_s": 771.0}, {"text": "For using this augmentation policy we use those that takes", "timestamp": "00:12:55,190", "timestamp_s": 775.0}, {"text": "a data set. We use this data set mapper", "timestamp": "00:12:58,792", "timestamp_s": 778.0}, {"text": "that takes a data set in detection two and then we map", "timestamp": "00:13:02,178", "timestamp_s": 782.0}, {"text": "our data set into a format that will be used by the model,", "timestamp": "00:13:07,130", "timestamp_s": 787.0}, {"text": "which is this dictionary with the keys it", "timestamp": "00:13:10,160", "timestamp_s": 790.0}, {"text": "with image instances. So we", "timestamp": "00:13:13,936", "timestamp_s": 793.0}, {"text": "read the image, we transform it using the augmentation policy", "timestamp": "00:13:18,448", "timestamp_s": 798.0}, {"text": "that we have defined. We also need to be careful for transferring as", "timestamp": "00:13:22,016", "timestamp_s": 802.0}, {"text": "well the bounding boxes and then of generating those", "timestamp": "00:13:25,988", "timestamp_s": 805.0}, {"text": "data in the format the model expects. But we are not limited to", "timestamp": "00:13:29,972", "timestamp_s": 809.0}, {"text": "use representations only from detectors tool.", "timestamp": "00:13:33,332", "timestamp_s": 813.0}, {"text": "We can also integrate external libraries like algorithmations", "timestamp": "00:13:36,404", "timestamp_s": 816.0}, {"text": "or cornea and these libraries", "timestamp": "00:13:40,042", "timestamp_s": 820.0}, {"text": "have a very large collection of transformations", "timestamp": "00:13:43,374", "timestamp_s": 823.0}, {"text": "that are not readily available in", "timestamp": "00:13:46,926", "timestamp_s": 826.0}, {"text": "detectors tool like this random sandflower and that we can also use.", "timestamp": "00:13:50,808", "timestamp_s": 830.0}, {"text": "One comment is that we used data", "timestamp": "00:13:55,290", "timestamp_s": 835.0}, {"text": "augmentation for training the faster RCNN and the retina net,", "timestamp": "00:13:58,992", "timestamp_s": 838.0}, {"text": "but we didn\u0027t see improvements even", "timestamp": "00:14:02,800", "timestamp_s": 842.0}, {"text": "when training for more iteration steps.", "timestamp": "00:14:06,496", "timestamp_s": 846.0}, {"text": "Now we will discuss those transformers. The transformer", "timestamp": "00:14:10,326", "timestamp_s": 850.0}, {"text": "was originally proposed as a sequence to sequence model for machine translation", "timestamp": "00:14:14,330", "timestamp_s": 854.0}, {"text": "and it is now a standard in natural language processing.", "timestamp": "00:14:18,450", "timestamp_s": 858.0}, {"text": "But also it has found its way into computer vision and other tasks.", "timestamp": "00:14:21,322", "timestamp_s": 861.0}, {"text": "It\u0027s a very general purpose architectures that lacks the inductive biases", "timestamp": "00:14:25,726", "timestamp_s": 865.0}, {"text": "of cnns, for example the locality and translation invariance.", "timestamp": "00:14:29,790", "timestamp_s": 869.0}, {"text": "But given large enough scale data, it can learn this from the data", "timestamp": "00:14:33,438", "timestamp_s": 873.0}, {"text": "and perform on par or even surpassing the cnns.", "timestamp": "00:14:36,844", "timestamp_s": 876.0}, {"text": "The vanilla transformers uses", "timestamp": "00:14:40,834", "timestamp_s": 880.0}, {"text": "an encoder and a decoder. The encoder has", "timestamp": "00:14:44,322", "timestamp_s": 884.0}, {"text": "two modules and the decoder, the multi head self", "timestamp": "00:14:47,932", "timestamp_s": 887.0}, {"text": "attention and the feed forward network. And we employ around each models", "timestamp": "00:14:51,532", "timestamp_s": 891.0}, {"text": "a residual connection and also layer normalization.", "timestamp": "00:14:55,790", "timestamp_s": 895.0}, {"text": "The decoder also uses cross attentions. So in those", "timestamp": "00:14:59,414", "timestamp_s": 899.0}, {"text": "cross attention the keys and values come from", "timestamp": "00:15:02,772", "timestamp_s": 902.0}, {"text": "the encoder and the queries come from the decoder. And we", "timestamp": "00:15:06,772", "timestamp_s": 906.0}, {"text": "also have when we talk about differences", "timestamp": "00:15:10,132", "timestamp_s": 910.0}, {"text": "between applying these transformers from NLP to visions, we have", "timestamp": "00:15:14,286", "timestamp_s": 914.0}, {"text": "these differences in scale and resolution scale being", "timestamp": "00:15:17,432", "timestamp_s": 917.0}, {"text": "that in NLP the words serve as the basic elements of pre", "timestamp": "00:15:21,352", "timestamp_s": 921.0}, {"text": "processing. But when we\u0027re talking about object detection, those objects", "timestamp": "00:15:24,888", "timestamp_s": 924.0}, {"text": "may vary in scale, so they may be compared of a different number of pixels", "timestamp": "00:15:28,178", "timestamp_s": 928.0}, {"text": "and resolution. If we think that, for example,", "timestamp": "00:15:32,082", "timestamp_s": 932.0}, {"text": "those images are comprised of a", "timestamp": "00:15:35,996", "timestamp_s": 935.0}, {"text": "big number of a large number of pixels. Since the soft", "timestamp": "00:15:39,964", "timestamp_s": 939.0}, {"text": "attention is very central to the transformers, let\u0027s see what", "timestamp": "00:15:43,836", "timestamp_s": 943.0}, {"text": "makes it so appealing when compared to other layers.", "timestamp": "00:15:47,212", "timestamp_s": 947.0}, {"text": "We see that self attention. So in here, this table on", "timestamp": "00:15:50,954", "timestamp_s": 950.0}, {"text": "the bottom left, the t stands for the sequence", "timestamp": "00:15:54,388", "timestamp_s": 954.0}, {"text": "size and the d stands for the representation dimensionality", "timestamp": "00:15:58,058", "timestamp_s": 958.0}, {"text": "of each part of the sequence. And we see that self attention is", "timestamp": "00:16:01,658", "timestamp_s": 961.0}, {"text": "more parameter efficient and fully connected layers as well,", "timestamp": "00:16:05,108", "timestamp_s": 965.0}, {"text": "better at handling arbitrary variants input sizes.", "timestamp": "00:16:08,440", "timestamp_s": 968.0}, {"text": "And if we compare this to recurrent layers,", "timestamp": "00:16:12,670", "timestamp_s": 972.0}, {"text": "it\u0027s also more perimeter efficient if the size of the sequence", "timestamp": "00:16:16,290", "timestamp_s": 976.0}, {"text": "is smaller than the representation dimensionality when compared to", "timestamp": "00:16:19,698", "timestamp_s": 979.0}, {"text": "convolutional layers. Convolutional layers", "timestamp": "00:16:23,148", "timestamp_s": 983.0}, {"text": "for achieved a global receptive field, meaning that every pixel", "timestamp": "00:16:26,546", "timestamp_s": 986.0}, {"text": "would interact with every pixel, we typically", "timestamp": "00:16:30,786", "timestamp_s": 990.0}, {"text": "need to stack many of these convolutional layers on top of each other.", "timestamp": "00:16:34,342", "timestamp_s": 994.0}, {"text": "And in self attention, all parts of those sequence interact with each", "timestamp": "00:16:38,096", "timestamp_s": 998.0}, {"text": "other within a single layer. Let\u0027s take a look at how", "timestamp": "00:16:41,588", "timestamp_s": 1001.0}, {"text": "the self attention works. So the self attention relates", "timestamp": "00:16:45,188", "timestamp_s": 1005.0}, {"text": "different positions of a sequence to compute a different representation", "timestamp": "00:16:50,650", "timestamp_s": 1010.0}, {"text": "of that sequence. So we feed it as an input a sequence", "timestamp": "00:16:54,238", "timestamp_s": 1014.0}, {"text": "z, in this case of size t and dimensional", "timestamp": "00:16:57,742", "timestamp_s": 1017.0}, {"text": "td, and we compute three matrices, the queries,", "timestamp": "00:17:01,822", "timestamp_s": 1021.0}, {"text": "keys and values. We do so by multiplying the input with this", "timestamp": "00:17:05,230", "timestamp_s": 1025.0}, {"text": "matrix UQV and slicing along the last", "timestamp": "00:17:08,648", "timestamp_s": 1028.0}, {"text": "dimension, the dimension of the tree times the dimension of", "timestamp": "00:17:12,412", "timestamp_s": 1032.0}, {"text": "the head, and this will generate the queries, keys and values", "timestamp": "00:17:15,484", "timestamp_s": 1035.0}, {"text": "for us. Next, we compute the dot product between the queries and", "timestamp": "00:17:18,978", "timestamp_s": 1038.0}, {"text": "keys, so the queries and keys must have the same dimension and we", "timestamp": "00:17:22,768", "timestamp_s": 1042.0}, {"text": "divide by a scaling factor. To alleviate vanishing gradient problems,", "timestamp": "00:17:26,512", "timestamp_s": 1046.0}, {"text": "we apply a soft max in a row wise manner, and this will", "timestamp": "00:17:30,000", "timestamp_s": 1050.0}, {"text": "be our attention matrix that has size t", "timestamp": "00:17:34,212", "timestamp_s": 1054.0}, {"text": "by t. So it\u0027s quadratic to the size of the input", "timestamp": "00:17:38,260", "timestamp_s": 1058.0}, {"text": "sequence, which is one of the bottlenecks of the transformer. And then we multiply", "timestamp": "00:17:41,498", "timestamp_s": 1061.0}, {"text": "this by v, our matrix value to", "timestamp": "00:17:45,358", "timestamp_s": 1065.0}, {"text": "retrieve the final computation.", "timestamp": "00:17:48,968", "timestamp_s": 1068.0}, {"text": "However, the transformer", "timestamp": "00:17:52,870", "timestamp_s": 1072.0}, {"text": "doesn\u0027t use the regular self attention, it use a generalization of it,", "timestamp": "00:17:56,558", "timestamp_s": 1076.0}, {"text": "which is called a multi head self attention. Multi head self attention", "timestamp": "00:17:59,868", "timestamp_s": 1079.0}, {"text": "is an extension of the self attention in which we run case of attention", "timestamp": "00:18:03,490", "timestamp_s": 1083.0}, {"text": "operations in parallel. So we run", "timestamp": "00:18:07,362", "timestamp_s": 1087.0}, {"text": "many self attentions in parallel, we concatenate them,", "timestamp": "00:18:11,104", "timestamp_s": 1091.0}, {"text": "and then we do a linear projection again to", "timestamp": "00:18:14,640", "timestamp_s": 1094.0}, {"text": "the dimension d. To not explode the dimensionality.", "timestamp": "00:18:17,968", "timestamp_s": 1097.0}, {"text": "Let\u0027s now revisit the transformers after having seen how the self attention", "timestamp": "00:18:22,602", "timestamp_s": 1102.0}, {"text": "works. So in the original transformer we add", "timestamp": "00:18:25,946", "timestamp_s": 1105.0}, {"text": "an encoder and a recorded but we can also", "timestamp": "00:18:29,892", "timestamp_s": 1109.0}, {"text": "use only a part of the architecture. For example,", "timestamp": "00:18:33,176", "timestamp_s": 1113.0}, {"text": "architectures that only use those encoder part like vert,", "timestamp": "00:18:36,344", "timestamp_s": 1116.0}, {"text": "are important when we only want global representation", "timestamp": "00:18:40,870", "timestamp_s": 1120.0}, {"text": "of those sequence. And you want to build classification on top of it. For example", "timestamp": "00:18:44,958", "timestamp_s": 1124.0}, {"text": "for performing sentiment analysis. When we", "timestamp": "00:18:48,332", "timestamp_s": 1128.0}, {"text": "architectures that only use a decoder are used for", "timestamp": "00:18:52,316", "timestamp_s": 1132.0}, {"text": "language modeling like GPD two. And we also have architectures", "timestamp": "00:18:56,076", "timestamp_s": 1136.0}, {"text": "that use both encoder and decoder like detection transformer", "timestamp": "00:18:59,618", "timestamp_s": 1139.0}, {"text": "that we\u0027ll see next. A fact that is also important is that self attention is", "timestamp": "00:19:03,398", "timestamp_s": 1143.0}, {"text": "invariant to the position of those tokens. So it\u0027s very common to add", "timestamp": "00:19:07,408", "timestamp_s": 1147.0}, {"text": "these position encodings to the input so that those model can reason", "timestamp": "00:19:11,956", "timestamp_s": 1151.0}, {"text": "about the positions of the parts of the", "timestamp": "00:19:15,620", "timestamp_s": 1155.0}, {"text": "sequence during", "timestamp": "00:19:18,868", "timestamp_s": 1158.0}, {"text": "the self attention in the encoder and decoder", "timestamp": "00:19:22,868", "timestamp_s": 1162.0}, {"text": "blocks. And now we are going", "timestamp": "00:19:26,878", "timestamp_s": 1166.0}, {"text": "to talk about the detection transformer. The detection transformer is a", "timestamp": "00:19:30,088", "timestamp_s": 1170.0}, {"text": "very simple architectures that is based on a CNN and a", "timestamp": "00:19:34,008", "timestamp_s": 1174.0}, {"text": "transformer recorded architectures and it", "timestamp": "00:19:37,388", "timestamp_s": 1177.0}, {"text": "uses a CNN backbone. So we feed it an images", "timestamp": "00:19:40,652", "timestamp_s": 1180.0}, {"text": "and this image goes through the CNN backbone and", "timestamp": "00:19:44,402", "timestamp_s": 1184.0}, {"text": "generates a feature map with lower width", "timestamp": "00:19:48,048", "timestamp_s": 1188.0}, {"text": "and lower weights, but with a much deeper", "timestamp": "00:19:52,350", "timestamp_s": 1192.0}, {"text": "number of channels. And now we have this distancer", "timestamp": "00:19:56,190", "timestamp_s": 1196.0}, {"text": "of width, height and channels, but we want to feed it into the transformer", "timestamp": "00:19:59,734", "timestamp_s": 1199.0}, {"text": "encoder. But the transformer encoder is expecting a", "timestamp": "00:20:03,578", "timestamp_s": 1203.0}, {"text": "sequence. So the way we can do this is by flattening the spatial", "timestamp": "00:20:07,188", "timestamp_s": 1207.0}, {"text": "dimensions of the input, by multiplying the", "timestamp": "00:20:11,338", "timestamp_s": 1211.0}, {"text": "height and width and then we can feed it into the transformer encoder.", "timestamp": "00:20:14,968", "timestamp_s": 1214.0}, {"text": "Then we have the transformer decoder that has these object", "timestamp": "00:20:18,958", "timestamp_s": 1218.0}, {"text": "queries which are learned by the model as the input.", "timestamp": "00:20:22,584", "timestamp_s": 1222.0}, {"text": "And these object queries are the number of objects that we are trying to", "timestamp": "00:20:27,086", "timestamp_s": 1227.0}, {"text": "detect in an image. So it must be set to", "timestamp": "00:20:30,892", "timestamp_s": 1230.0}, {"text": "be larger than the largest number of objects", "timestamp": "00:20:34,332", "timestamp_s": 1234.0}, {"text": "that we have in an image to provide us some slack. And they will learn", "timestamp": "00:20:37,714", "timestamp_s": 1237.0}, {"text": "to attend to specific areas and specific bounding boxes", "timestamp": "00:20:41,152", "timestamp_s": 1241.0}, {"text": "sizes in an image. Then the decoder is", "timestamp": "00:20:45,062", "timestamp_s": 1245.0}, {"text": "also conditioned on the encoder output", "timestamp": "00:20:48,768", "timestamp_s": 1248.0}, {"text": "and we predict the classes,", "timestamp": "00:20:52,794", "timestamp_s": 1252.0}, {"text": "the object class and the bounding box through parallel decoding. So it\u0027s", "timestamp": "00:20:56,410", "timestamp_s": 1256.0}, {"text": "not in an autoregressive way. We output them in parallel", "timestamp": "00:21:00,074", "timestamp_s": 1260.0}, {"text": "and we are treating the object detectors problem as a direct set case", "timestamp": "00:21:03,370", "timestamp_s": 1263.0}, {"text": "prediction. So we need an appropriate loss for that. They use", "timestamp": "00:21:07,176", "timestamp_s": 1267.0}, {"text": "those bipartite matching loss based on the hungarian algorithm", "timestamp": "00:21:11,192", "timestamp_s": 1271.0}, {"text": "that is permutation invariant and also forces a unique", "timestamp": "00:21:15,374", "timestamp_s": 1275.0}, {"text": "assignment between the ground truth and the predicted objects.", "timestamp": "00:21:18,882", "timestamp_s": 1278.0}, {"text": "We are going to use the egging phase library that contains many", "timestamp": "00:21:22,722", "timestamp_s": 1282.0}, {"text": "transformers and they recently added those visual transformers", "timestamp": "00:21:26,332", "timestamp_s": 1286.0}, {"text": "for image classification like the visual transformer VIT and also", "timestamp": "00:21:30,710", "timestamp_s": 1290.0}, {"text": "this detection transformer for object detection. They added this", "timestamp": "00:21:33,952", "timestamp_s": 1293.0}, {"text": "to the library and we are going to use based on a REsnet 50", "timestamp": "00:21:37,728", "timestamp_s": 1297.0}, {"text": "backbone the reason we use this dilated convolutional", "timestamp": "00:21:41,380", "timestamp_s": 1301.0}, {"text": "is that the dilated convolution will increase", "timestamp": "00:21:44,906", "timestamp_s": 1304.0}, {"text": "the resolution by a factor of two at the expense of more computations,", "timestamp": "00:21:48,442", "timestamp_s": 1308.0}, {"text": "but it will help detecting small scale", "timestamp": "00:21:53,570", "timestamp_s": 1313.0}, {"text": "objects. Egin face provides a very comprehensive", "timestamp": "00:21:57,290", "timestamp_s": 1317.0}, {"text": "set of documentation. It also explains the internal part of those model and we also", "timestamp": "00:22:01,166", "timestamp_s": 1321.0}, {"text": "have these example notebooks by Niels rogue that are linked", "timestamp": "00:22:04,888", "timestamp_s": 1324.0}, {"text": "at the page and at the bottom of this slide. That explains how we", "timestamp": "00:22:08,510", "timestamp_s": 1328.0}, {"text": "can fine tune object object object object object object object object object", "timestamp": "00:22:11,868", "timestamp_s": 1331.0}, {"text": "object object detection transformers CNNS drone case", "timestamp": "00:22:16,060", "timestamp_s": 1336.0}, {"text": "feature extractor used for pre processing", "timestamp": "00:22:19,868", "timestamp_s": 1339.0}, {"text": "the input for the model or for post processing the output of the model in", "timestamp": "00:22:23,810", "timestamp_s": 1343.0}, {"text": "the cocoa notation format. For example for running the cocoa", "timestamp": "00:22:27,664", "timestamp_s": 1347.0}, {"text": "evaluation metrics we also have the data for object detection", "timestamp": "00:22:31,274", "timestamp_s": 1351.0}, {"text": "model that exposes those", "timestamp": "00:22:34,842", "timestamp_s": 1354.0}, {"text": "logits and the prediction boxes and also we have this data", "timestamp": "00:22:38,452", "timestamp_s": 1358.0}, {"text": "config that can be used for institiating", "timestamp": "00:22:42,660", "timestamp_s": 1362.0}, {"text": "data for object detection model. Through this", "timestamp": "00:22:46,474", "timestamp_s": 1366.0}, {"text": "configuration. The modifications that we do when", "timestamp": "00:22:49,832", "timestamp_s": 1369.0}, {"text": "compared to those notebook is that we use the REsnet with a little convolutions instead", "timestamp": "00:22:54,152", "timestamp_s": 1374.0}, {"text": "of the REsnet 50. We also set the maximum size of", "timestamp": "00:22:58,152", "timestamp_s": 1378.0}, {"text": "the image to 1100 to not eat good out of memory", "timestamp": "00:23:01,388", "timestamp_s": 1381.0}, {"text": "aircorse and we also use a smaller batch size of two instead", "timestamp": "00:23:04,818", "timestamp_s": 1384.0}, {"text": "of four because on v 100 gpu we", "timestamp": "00:23:08,432", "timestamp_s": 1388.0}, {"text": "use get out of memory aircraft. Otherwise, after training", "timestamp": "00:23:11,968", "timestamp_s": 1391.0}, {"text": "detectors transformers on our data set we see that average", "timestamp": "00:23:16,830", "timestamp_s": 1396.0}, {"text": "precision is very poor compared to the objects", "timestamp": "00:23:20,922", "timestamp_s": 1400.0}, {"text": "detections. Based on cnns we", "timestamp": "00:23:25,410", "timestamp_s": 1405.0}, {"text": "have seen previously, the model is able to detect large", "timestamp": "00:23:29,044", "timestamp_s": 1409.0}, {"text": "objects. It has a fairly good average precision", "timestamp": "00:23:32,596", "timestamp_s": 1412.0}, {"text": "for large objects, but it is very small for small and medium objects", "timestamp": "00:23:36,494", "timestamp_s": 1416.0}, {"text": "which can be attributed to the detection transformer", "timestamp": "00:23:40,710", "timestamp_s": 1420.0}, {"text": "not being suitable for these small scale", "timestamp": "00:23:44,814", "timestamp_s": 1424.0}, {"text": "object detectors problems and as feature pyramid", "timestamp": "00:23:48,690", "timestamp_s": 1428.0}, {"text": "networks did for cnns for", "timestamp": "00:23:52,242", "timestamp_s": 1432.0}, {"text": "helping addressing those multiscale object detectors problem.", "timestamp": "00:23:56,236", "timestamp_s": 1436.0}, {"text": "Similar approaches could also help improving the detection transformer", "timestamp": "00:24:00,272", "timestamp_s": 1440.0}, {"text": "further. We see in the inference results. We have some", "timestamp": "00:24:04,278", "timestamp_s": 1444.0}, {"text": "duplicate detections that could be probably removed", "timestamp": "00:24:10,756", "timestamp_s": 1450.0}, {"text": "by using non maximal suppression and we also have", "timestamp": "00:24:14,058", "timestamp_s": 1454.0}, {"text": "some missed detections. So how can", "timestamp": "00:24:18,036", "timestamp_s": 1458.0}, {"text": "we improve these results further? We can for example scale the", "timestamp": "00:24:21,348", "timestamp_s": 1461.0}, {"text": "backbone. In all of those experiments we use the resonate 50 but we could", "timestamp": "00:24:24,868", "timestamp_s": 1464.0}, {"text": "use a larger backbone like a REsnet 101.", "timestamp": "00:24:28,728", "timestamp_s": 1468.0}, {"text": "The results we had for the documentation didn\u0027t", "timestamp": "00:24:33,350", "timestamp_s": 1473.0}, {"text": "improve our results, but we could fine tune the probabilities", "timestamp": "00:24:39,042", "timestamp_s": 1479.0}, {"text": "or also change the augmentation transformations", "timestamp": "00:24:43,394", "timestamp_s": 1483.0}, {"text": "to find if we could get better results. Right now we also", "timestamp": "00:24:47,710", "timestamp_s": 1487.0}, {"text": "have more publicly available data sets", "timestamp": "00:24:51,376", "timestamp_s": 1491.0}, {"text": "recorded by drones like Miva,", "timestamp": "00:24:55,398", "timestamp_s": 1495.0}, {"text": "UAVDT and so on and we could use this to build", "timestamp": "00:24:58,086", "timestamp_s": 1498.0}, {"text": "a larger data set to see if we can get", "timestamp": "00:25:01,652", "timestamp_s": 1501.0}, {"text": "better results out of this. Also, we only used", "timestamp": "00:25:04,916", "timestamp_s": 1504.0}, {"text": "static images for the object detection part.", "timestamp": "00:25:08,724", "timestamp_s": 1508.0}, {"text": "But if we think about video object detection, we can exploit", "timestamp": "00:25:12,996", "timestamp_s": 1512.0}, {"text": "these temporal cues of the", "timestamp": "00:25:17,678", "timestamp_s": 1517.0}, {"text": "different frames to reduce the number of false positives.", "timestamp": "00:25:21,432", "timestamp_s": 1521.0}, {"text": "We also have different transformer architectures, for example", "timestamp": "00:25:25,182", "timestamp_s": 1525.0}, {"text": "the using transformer or the focal transformer", "timestamp": "00:25:30,330", "timestamp_s": 1530.0}, {"text": "that could be used and tested to see if", "timestamp": "00:25:34,098", "timestamp_s": 1534.0}, {"text": "they provide better results. To conclude,", "timestamp": "00:25:37,920", "timestamp_s": 1537.0}, {"text": "we see that CNNs make for very powerful baselines.", "timestamp": "00:25:41,702", "timestamp_s": 1541.0}, {"text": "We used off the shelf pre trained CNN", "timestamp": "00:25:45,462", "timestamp_s": 1545.0}, {"text": "architectures, the faster CNN and retinate and", "timestamp": "00:25:49,206", "timestamp_s": 1549.0}, {"text": "got very good average precision results in Visdron", "timestamp": "00:25:53,236", "timestamp_s": 1553.0}, {"text": "for detecting cars. The transformer architectures are being increasingly", "timestamp": "00:25:56,954", "timestamp_s": 1556.0}, {"text": "used in research and practice and we", "timestamp": "00:26:01,626", "timestamp_s": 1561.0}, {"text": "can see that they are being added to these mainstream libraries. Like egging", "timestamp": "00:26:04,808", "timestamp_s": 1564.0}, {"text": "case, for example, the detection transformer", "timestamp": "00:26:08,718", "timestamp_s": 1568.0}, {"text": "is better suited for medium to large to", "timestamp": "00:26:11,886", "timestamp_s": 1571.0}, {"text": "large objects. But developments similar to the feature pyramid", "timestamp": "00:26:16,316", "timestamp_s": 1576.0}, {"text": "network as it was", "timestamp": "00:26:20,002", "timestamp_s": 1580.0}, {"text": "used for CNNs can also help. The detection", "timestamp": "00:26:23,436", "timestamp_s": 1583.0}, {"text": "transformer and the transformers will continue to be used into", "timestamp": "00:26:27,298", "timestamp_s": 1587.0}, {"text": "downstream tasks like object detection, images classification", "timestamp": "00:26:32,110", "timestamp_s": 1592.0}, {"text": "and image representations. We can see many research papers coming from these", "timestamp": "00:26:35,510", "timestamp_s": 1595.0}, {"text": "areas and last but not least,", "timestamp": "00:26:39,472", "timestamp_s": 1599.0}, {"text": "transformers make from a unifying framework for different fields.", "timestamp": "00:26:42,788", "timestamp_s": 1602.0}, {"text": "So before we encode all of these inductive biases", "timestamp": "00:26:47,066", "timestamp_s": 1607.0}, {"text": "that we have for those CNNs and for the OSTMs.", "timestamp": "00:26:50,730", "timestamp_s": 1610.0}, {"text": "On the other hand, the transformer makes for a very general purpose architecture", "timestamp": "00:26:53,642", "timestamp_s": 1613.0}, {"text": "that lacks these inductive biases, but it can learn them from", "timestamp": "00:26:57,658", "timestamp_s": 1617.0}, {"text": "large scale data and it has", "timestamp": "00:27:02,068", "timestamp_s": 1622.0}, {"text": "given very good results for natural language processing", "timestamp": "00:27:06,252", "timestamp_s": 1626.0}, {"text": "and it\u0027s now also giving some state of the art results", "timestamp": "00:27:09,458", "timestamp_s": 1629.0}, {"text": "in image. And so it can maybe unify", "timestamp": "00:27:12,930", "timestamp_s": 1632.0}, {"text": "both fields and also unify the", "timestamp": "00:27:16,386", "timestamp_s": 1636.0}, {"text": "practitioners and researchers from both areas.", "timestamp": "00:27:20,332", "timestamp_s": 1640.0}, {"text": "So today, this concludes my presentation.", "timestamp": "00:27:23,746", "timestamp_s": 1643.0}, {"text": "I want to thank you for listening.", "timestamp": "00:27:27,218", "timestamp_s": 1647.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: '1s-hXnjHEHI',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Object Detection using Transformers and CNNs - A Drone Case Study
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>Drones with mounted cameras provide significant advantages when compared to fixed cameras for object detection and visual tracking scenarios. Given their recent adoption in the wild and late advances in computer vision models, many aerial datasets have been introduced.</p>
<p>In this talk, we’ll explore recent advances in object detection, comparing the challenges of natural images with those recorded by drones. Given the successes achieved by pretraining image classifiers on large datasets, and transferring the learned representations, a set of object detectors fine-tuned on publicly available aerial datasets will be presented and explained. We’ll highlight existing libraries that mitigate the cost of training large models from scratch, by including pretrained model weights and model variants found in the literature. Both Convolutional Neural Networks and the newly developed Transformers applied to vision will be covered and compared, outlining the main features of each architecture. The presentation will be accompanied by code snippets for aiding understanding and delivering practical examples.</p>
<p>This is aimed at a general audience familiar with Python. Knowledge of Computer Vision is a plus but not a requirement as we’ll introduce the necessary concepts. We’ll ground the presented model architectures and libraries on the task of object detection applied to aerial datasets and demonstrate that state-of-the-art methods are within everyone’s reach.</p>
<!-- End Text -->
          </div>

          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
              <li>
                Eduardo Dixo is a senior data scientist at Continental. He talks about object detectors using cnns and transformers applied to images recorded by datasets. Next we'll see some common CNNS based architectures like the faster RCNN and retinate.

              </li>
              
              <li>
                The transformer was originally proposed as a sequence to sequence model for machine translation. But also it has found its way into computer vision and other tasks. Given large enough scale data, it can learn this from the data and perform on par or even surpassing the cnns.

              </li>
              
              <li>
                The detection transformer is based on a CNN backbone. It has a fairly good average precision for large objects, but it is very small for small and medium objects. Similar approaches could also help improving the detection transformer further.
              </li>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/assemblyai/1s-hXnjHEHI.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:22,010'); seek(22.0)">
              Hello, my name is Eduardo Dixo, I'm a senior data scientist at Continental and
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:25,644'); seek(25.0)">
              today I'm going to talk to you about object detectors using cnns and
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:28,988'); seek(28.0)">
              transformers applied to images recorded by datasets. First I'm going to
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:32,972'); seek(32.0)">
              introduce the task of object detectors and also the data set that
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:36,268'); seek(36.0)">
              we'll be using. Next we'll see some common CNNS based architectures
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:40,258'); seek(40.0)">
              like the faster RCNN and retinate before discussing
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:43,782'); seek(43.0)">
              the transformer and seeing how the detection transformer performs on our data set.
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:47,984'); seek(47.0)">
              So let's begin by first introducing the task of object detection taskoff.
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:51,798'); seek(51.0)">
              Object detection can be regarded as given an input
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:55,162'); seek(55.0)">
              image. We want to find all the objects that are present in that image.
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:00:58,842'); seek(58.0)">
              So we need to spatially locate them using bounding
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:01:03,578'); seek(63.0)">
              boxes and also we need to classify them into a set of
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:01:07,048'); seek(67.0)">
              predefined classes. If we compare the task of object detection with
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:01:11,096'); seek(71.0)">
              image classification, for example, in image classification we
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:01:14,824'); seek(74.0)">
              usually have a single main target. In object detection we may
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:01:18,092'); seek(78.0)">
              have different number of objects present in that image
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:22,194'); seek(82.0)">
              with different poses, with different scales. And this makes the task
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:26,530'); seek(86.0)">
              very challenging, more challenging than the image classification. For example,
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:30,124'); seek(90.0)">
              the data set that we are going to use is those visron data set that
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:33,168'); seek(93.0)">
              contains nearly 6000 training images and 500 validation
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:36,582'); seek(96.0)">
              images. It also contains ten categories from which we are only interested
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:39,920'); seek(99.0)">
              in cars. We are going to build an object detector for
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:43,332'); seek(103.0)">
              detecting only one class, which will be cars. And this
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:47,092'); seek(107.0)">
              data set is interesting because it records the
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:50,196'); seek(110.0)">
              images and the different conditions like different weather,
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:54,030'); seek(114.0)">
              different lighting, different object density of
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:01:57,304'); seek(117.0)">
              the scenes, different scales of the objects. We have some fast motion artifacts
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:02:01,678'); seek(121.0)">
              because of the movement of the cars or the movement of
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:02:04,808'); seek(124.0)">
              those drone during flight. And also the bounding boxes are annotated for
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:02:08,588'); seek(128.0)">
              occlusion and those truncation. Some applications of
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:02:12,332'); seek(132.0)">
              training such object detector could be interesting
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:02:16,572'); seek(136.0)">
              for road safety, traffic monitoring or
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:02:20,656'); seek(140.0)">
              even driving assistance as finding free tracking
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:24,630'); seek(144.0)">
              slots. First, we make a distinction between the one
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:28,592'); seek(148.0)">
              stage and two stage object detectors. Two stage object
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:32,196'); seek(152.0)">
              detectors contain a region proposal network
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:36,282'); seek(156.0)">
              that will output high confidence region proposals
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:41,410'); seek(161.0)">
              that should contain an object on it. So it's not concerned
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:46,230'); seek(166.0)">
              what is the class of the object in it, it's only concerned if there is
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:50,168'); seek(170.0)">
              an object or not. And then the object detector head that typically
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:53,646'); seek(173.0)">
              drones bounding box regression for finding the position of the object
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:02:57,580'); seek(177.0)">
              and object classification to find its class can attend to these
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:03:01,452'); seek(181.0)">
              proposed regions and by doing so it will
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:03:05,212'); seek(185.0)">
              have a much smaller set of candidate regions that might
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:03:08,672'); seek(188.0)">
              have an object. And this will eliminate
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:03:12,662'); seek(192.0)">
              many of the case positives that we would have otherwise. A one shot
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:16,598'); seek(196.0)">
              detector, on the other hand, generates a dense sampling
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:20,198'); seek(200.0)">
              of possible object locations. So it will generate lots of
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:24,164'); seek(204.0)">
              object candidate locations with different shapes and
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:27,972'); seek(207.0)">
              different aspect ratios, and it will process them directly to
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:31,444'); seek(211.0)">
              learn the class labels and bounding boxes. The first model that we are
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:34,904'); seek(214.0)">
              going to discuss is the faster RCNN. The faster RCNn
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:38,750'); seek(218.0)">
              is a two stage object detector that employs two models,
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:42,062'); seek(222.0)">
              a region proposal network, and also the classifier head
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:46,088'); seek(226.0)">
              that has the bounding box, regression and object classification. We will start
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:49,980'); seek(229.0)">
              by following those typical data flow of the image as
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:53,532'); seek(233.0)">
              it goes through the architecture. So the image goes through the backbone.
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:03:57,026'); seek(237.0)">
              The backbone goal is to extract eye
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:04:00,450'); seek(240.0)">
              level semantic feature maps from the image. That will be
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:04:03,728'); seek(243.0)">
              useful later for the region proposal network and for the classifier.
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:04:07,110'); seek(247.0)">
              This can be typically achieved by any of the
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:04:10,368'); seek(250.0)">
              shelf convolutional architectures like Rasnet or Vgg.
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:14,298'); seek(254.0)">
              As the image goes through these several convolutional
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:18,154'); seek(258.0)">
              layers, it gets downsampled
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:21,818'); seek(261.0)">
              so it will have smaller width and smaller height,
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:25,710'); seek(265.0)">
              but much more depth, meaning that the feature map of the
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:29,144'); seek(269.0)">
              last stage of the backbone will have
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:32,920'); seek(272.0)">
              many channels. Next we have this region proposal network.
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:36,786'); seek(276.0)">
              This region proposal network will predict the object bounds as well
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:40,828'); seek(280.0)">
              as the objectness cars. Meaning if it is
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:44,428'); seek(284.0)">
              an object or not and it's a fully convolutional network,
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:49,770'); seek(289.0)">
              it will receive as input the feature maps from the backbone.
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:04:53,078'); seek(293.0)">
              It will slide those window over these feature maps. At each
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:04:56,752'); seek(296.0)">
              point of those sliding window it will generate k anchor boxes.
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:05:00,662'); seek(300.0)">
              The number of anchor boxes is parameterized by this k and
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:05:04,516'); seek(304.0)">
              it will have two sibling networks for the outputs,
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:05:08,554'); seek(308.0)">
              one that is two times the number of anchor boxes
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:05:13,970'); seek(313.0)">
              for the score classification in foreground and
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:05:17,172'); seek(317.0)">
              background, and the other one will be four times the
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:20,548'); seek(320.0)">
              number of anchor boxes for the bounding box coordinates.
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:24,186'); seek(324.0)">
              Finally, now we have a set of regions proposed
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:28,866'); seek(328.0)">
              by this region proposal network module, and in a very naive way,
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:32,652'); seek(332.0)">
              we could simply crop the image using these proposal regions
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:36,706'); seek(336.0)">
              and feed it into another classifier just
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:40,348'); seek(340.0)">
              to get the object class. However, we want to make those
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:43,776'); seek(343.0)">
              end to end and to reuse the feature maps that we have
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:47,008'); seek(347.0)">
              computed from the backbone. And for doing so, we are going
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:50,288'); seek(350.0)">
              to map the feature maps to the proposals of the region proposal network using
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:05:54,452'); seek(354.0)">
              this region of interest pulling layer that will extract
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:05:58,202'); seek(358.0)">
              then fixed size features maps from each of these proposals
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:06:02,330'); seek(362.0)">
              from the feature map. The reason these are fixed size
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:06:05,476'); seek(365.0)">
              is because we are going to use fully connected layer that expects fixed size.
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:06:09,784'); seek(369.0)">
              Then we have this classifier that will predict the object
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:06:13,640'); seek(373.0)">
              class as well as the bounding box coordinates.
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:17,218'); seek(377.0)">
              We are going to use the detection tool library which is a pytorch based
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:21,100'); seek(381.0)">
              deep learning framework for object detectors and
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:24,972'); seek(384.0)">
              also semantic segmentation. And we are using to use
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:30,190'); seek(390.0)">
              faster RCNn with a Resnet 50 backbone
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:06:34,262'); seek(394.0)">
              using fully features pyramid networks the reason we
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:06:38,128'); seek(398.0)">
              are going to use these feature pyramid networks is because we have
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:06:41,536'); seek(401.0)">
              images in our data set that have very
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:06:45,092'); seek(405.0)">
              small scale so we have small cars and also large cars that we
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:48,788'); seek(408.0)">
              want to detectors depending on the altitude that the drone is
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:06:52,500'); seek(412.0)">
              flying. And by using these feature pyramid networks we
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:06:56,008'); seek(416.0)">
              can improve the multiscale object detection because those goal
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:06:59,358'); seek(419.0)">
              of the feature pyramid network is to build these eye level semantic
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:07:03,198'); seek(423.0)">
              feature maps across all the pyramid levels from
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:07:07,436'); seek(427.0)">
              a single image of a single
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:07:11,244'); seek(431.0)">
              resolution. This is done by merging
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:07:15,218'); seek(435.0)">
              the bottom pathway which is the feature maps
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:19,122'); seek(439.0)">
              from our CNN backbone that then are upsampled through
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:07:23,008'); seek(443.0)">
              those top line pathway and merged through lateral
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:07:26,342'); seek(446.0)">
              connections in the feature pyramid network
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:07:30,342'); seek(450.0)">
              architecture. For training the faster RCNN the first step is to register
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:07:34,266'); seek(454.0)">
              our data set. We do this so that
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:07:38,244'); seek(458.0)">
              the detectors two knows how to obtain it.
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:07:41,810'); seek(461.0)">
              If we already have the annotations in adjacent cocoa
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:07:47,350'); seek(467.0)">
              format, we can use the register cocoa instances directly.
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:07:51,182'); seek(471.0)">
              In this case we have prepared the annotations in this format so we can use
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:07:54,408'); seek(474.0)">
              those register cocoa instances and we also pass the
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:07:58,236'); seek(478.0)">
              base path images so it knows where to fetch the images
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:08:01,666'); seek(481.0)">
              from. Next, detectors two uses the key value
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:08:06,170'); seek(486.0)">
              config system based on YaML files
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:08:09,862'); seek(489.0)">
              that provide already some common functionality and operations.
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:08:13,894'); seek(493.0)">
              If we require more advanced features, we can drop down to the Python's
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:08:17,398'); seek(497.0)">
              API or also derive from a
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:08:20,528'); seek(500.0)">
              base config file and implement the attributes. And in here
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:24,100'); seek(504.0)">
              what we are going to do is first we load the default configuration
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:08:27,882'); seek(507.0)">
              file. We then inherit from
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:08:31,092'); seek(511.0)">
              the configuration file of the model that we want to fine tune.
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:08:34,170'); seek(514.0)">
              We specify the training and test data sets that we already registered previously.
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:08:38,542'); seek(518.0)">
              We specify the number of workers for the multiprocessing part
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:08:42,072'); seek(522.0)">
              and we load the pretrained model weights from
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:08:45,912'); seek(525.0)">
              the detectors two model zoo. Then we have the learning rate,
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:08:49,516'); seek(529.0)">
              the maximum number of iterations, the batch size, and the steps at
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:08:52,748'); seek(532.0)">
              which to decay the learning rate. All of these are very important parameters
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:08:56,706'); seek(536.0)">
              that we should tune to get the best metrics,
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:09:00,918'); seek(540.0)">
              but also to squeeze the best performance out
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:09:04,304'); seek(544.0)">
              of the GPU. And then we specify the number of classes for
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:09:07,792'); seek(547.0)">
              this particular architecture, which is one
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:09:11,600'); seek(551.0)">
              because we are only interested in detectors cars.
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:09:15,258'); seek(555.0)">
              Finally, we can launch the training using the default trainer class
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:09:18,916'); seek(558.0)">
              that provide out of the box standard training logic.
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:09:21,834'); seek(561.0)">
              If we require, we could also implement our own Python
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:09:26,250'); seek(566.0)">
              training loop or also subclass this default trainer
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:09:30,554'); seek(570.0)">
              in here. Since we are not loading from a checkpoint. We pass this resemb equal
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:09:33,914'); seek(573.0)">
              false. Now we take a look at a one stage detector.
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:09:38,578'); seek(578.0)">
              So retinate is a powerful one stage detector
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:09:42,498'); seek(582.0)">
              that employs the feature pyramid network that we have seen before that helps
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:09:46,338'); seek(586.0)">
              with a multiscale detection of
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:09:49,568'); seek(589.0)">
              the objects and also two
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:09:53,392'); seek(593.0)">
              civilian networks, one for classification and the other for bounding box
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:09:56,592'); seek(596.0)">
              regression. The one stage detectors were typically regarded
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:10:00,694'); seek(600.0)">
              as being faster than the two stage, but they were lagging the accuracy
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:10:05,018'); seek(605.0)">
              of the two stage detectors. So the authors of retinate
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:10:09,082'); seek(609.0)">
              attributed this to the eye class imbalance between
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:10:12,420'); seek(612.0)">
              foreground and background that may happen.
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:10:15,380'); seek(615.0)">
              And the reason is if you remember these
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:10:20,072'); seek(620.0)">
              one stage detectors, they will examples
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:10:23,662'); seek(623.0)">
              a large set of candidate regions, many of them will
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:10:26,748'); seek(626.0)">
              be background, will be easy negatives and they will not contribute
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:10:30,242'); seek(630.0)">
              with a useful learning signal for the network or they
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:10:33,788'); seek(633.0)">
              can even overwhelm the training loss.
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:10:37,570'); seek(637.0)">
              So what they propose is this nova loss called those focal loss that
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:10:42,144'); seek(642.0)">
              adds this modulating factor to the standard cross entropy and
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:10:45,712'); seek(645.0)">
              it will downweight the well classified examples so
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:10:50,608'); seek(650.0)">
              that the model can focus more on the other examples. For the retinate
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:10:54,774'); seek(654.0)">
              we are also going to use REsnet 50 backbone for
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:10:59,252'); seek(659.0)">
              comparison with the faster RCNN. We also use the detectors
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:11:02,714'); seek(662.0)">
              and two library for doing so. Registering the data
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:11:05,896'); seek(665.0)">
              set requires no changes. Launching the training also requires no
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:11:09,032'); seek(669.0)">
              changes, but we need to change the configuration file.
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:11:12,766'); seek(672.0)">
              So in this case we need to inherit from the appropriate model. We also
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:11:16,376'); seek(676.0)">
              need to load the appropriate models from the model zoo. And now
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:11:20,332'); seek(680.0)">
              for setting the number of classes we need to access a
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:11:24,284'); seek(684.0)">
              different attribute of the config which is under those retinate
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:11:28,830'); seek(688.0)">
              num classes. After training both models we see that
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:11:32,640'); seek(692.0)">
              they both have good cocoa evaluation
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:11:36,566'); seek(696.0)">
              metrics. We are using the average precision which
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:11:40,692'); seek(700.0)">
              basically penalizes missing detections
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:11:44,522'); seek(704.0)">
              and also detecting having too many duplicate
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:11:48,298'); seek(708.0)">
              detections for the same object. And we see
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:11:51,876'); seek(711.0)">
              that the average precision is very similar for each model. In this case the
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:11:55,928'); seek(715.0)">
              retina net is better at detecting larger
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:12:00,878'); seek(720.0)">
              objects but worse at smaller objects. But if we look at the average precision,
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:12:04,798'); seek(724.0)">
              they are very equally matched and also the inference results
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:12:08,418'); seek(728.0)">
              as well. Another thing that is commonly employed
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:12:12,882'); seek(732.0)">
              in computer vision is that augmentation for aiding in the generalization
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:12:18,170'); seek(738.0)">
              of the network. And the reason is that we want our object detection
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:12:22,358'); seek(742.0)">
              to work under different lighting, viewpoint scales, et cetera.
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:12:26,262'); seek(746.0)">
              So we can generate an augmentation policy that will bake
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:12:29,862'); seek(749.0)">
              these transformations there. And so
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:12:33,124'); seek(753.0)">
              we pass our data set through this augmentation policy, enriching our data
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:12:36,596'); seek(756.0)">
              set that we will then use for training our model.
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:12:40,308'); seek(760.0)">
              In this example we have an horizontal flip and
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:12:43,992'); seek(763.0)">
              also some we can see on the left the
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:12:47,928'); seek(767.0)">
              augmentation policy used and also some random brightness,
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:12:51,262'); seek(771.0)">
              some random saturation, some random contrast.
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:12:55,190'); seek(775.0)">
              For using this augmentation policy we use those that takes
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:12:58,792'); seek(778.0)">
              a data set. We use this data set mapper
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:13:02,178'); seek(782.0)">
              that takes a data set in detection two and then we map
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:13:07,130'); seek(787.0)">
              our data set into a format that will be used by the model,
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:13:10,160'); seek(790.0)">
              which is this dictionary with the keys it
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:13:13,936'); seek(793.0)">
              with image instances. So we
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:13:18,448'); seek(798.0)">
              read the image, we transform it using the augmentation policy
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:13:22,016'); seek(802.0)">
              that we have defined. We also need to be careful for transferring as
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:13:25,988'); seek(805.0)">
              well the bounding boxes and then of generating those
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:13:29,972'); seek(809.0)">
              data in the format the model expects. But we are not limited to
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:13:33,332'); seek(813.0)">
              use representations only from detectors tool.
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:13:36,404'); seek(816.0)">
              We can also integrate external libraries like algorithmations
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:13:40,042'); seek(820.0)">
              or cornea and these libraries
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:13:43,374'); seek(823.0)">
              have a very large collection of transformations
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:13:46,926'); seek(826.0)">
              that are not readily available in
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:13:50,808'); seek(830.0)">
              detectors tool like this random sandflower and that we can also use.
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:13:55,290'); seek(835.0)">
              One comment is that we used data
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:13:58,992'); seek(838.0)">
              augmentation for training the faster RCNN and the retina net,
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:14:02,800'); seek(842.0)">
              but we didn't see improvements even
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:14:06,496'); seek(846.0)">
              when training for more iteration steps.
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:14:10,326'); seek(850.0)">
              Now we will discuss those transformers. The transformer
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:14:14,330'); seek(854.0)">
              was originally proposed as a sequence to sequence model for machine translation
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:14:18,450'); seek(858.0)">
              and it is now a standard in natural language processing.
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:14:21,322'); seek(861.0)">
              But also it has found its way into computer vision and other tasks.
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:14:25,726'); seek(865.0)">
              It's a very general purpose architectures that lacks the inductive biases
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:14:29,790'); seek(869.0)">
              of cnns, for example the locality and translation invariance.
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:14:33,438'); seek(873.0)">
              But given large enough scale data, it can learn this from the data
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:14:36,844'); seek(876.0)">
              and perform on par or even surpassing the cnns.
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:14:40,834'); seek(880.0)">
              The vanilla transformers uses
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:14:44,322'); seek(884.0)">
              an encoder and a decoder. The encoder has
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:14:47,932'); seek(887.0)">
              two modules and the decoder, the multi head self
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:14:51,532'); seek(891.0)">
              attention and the feed forward network. And we employ around each models
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:14:55,790'); seek(895.0)">
              a residual connection and also layer normalization.
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:14:59,414'); seek(899.0)">
              The decoder also uses cross attentions. So in those
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:15:02,772'); seek(902.0)">
              cross attention the keys and values come from
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:15:06,772'); seek(906.0)">
              the encoder and the queries come from the decoder. And we
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:15:10,132'); seek(910.0)">
              also have when we talk about differences
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:15:14,286'); seek(914.0)">
              between applying these transformers from NLP to visions, we have
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:15:17,432'); seek(917.0)">
              these differences in scale and resolution scale being
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:15:21,352'); seek(921.0)">
              that in NLP the words serve as the basic elements of pre
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:15:24,888'); seek(924.0)">
              processing. But when we're talking about object detection, those objects
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:15:28,178'); seek(928.0)">
              may vary in scale, so they may be compared of a different number of pixels
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:15:32,082'); seek(932.0)">
              and resolution. If we think that, for example,
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:15:35,996'); seek(935.0)">
              those images are comprised of a
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:15:39,964'); seek(939.0)">
              big number of a large number of pixels. Since the soft
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:15:43,836'); seek(943.0)">
              attention is very central to the transformers, let's see what
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:15:47,212'); seek(947.0)">
              makes it so appealing when compared to other layers.
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:15:50,954'); seek(950.0)">
              We see that self attention. So in here, this table on
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:15:54,388'); seek(954.0)">
              the bottom left, the t stands for the sequence
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:15:58,058'); seek(958.0)">
              size and the d stands for the representation dimensionality
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:16:01,658'); seek(961.0)">
              of each part of the sequence. And we see that self attention is
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:16:05,108'); seek(965.0)">
              more parameter efficient and fully connected layers as well,
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:16:08,440'); seek(968.0)">
              better at handling arbitrary variants input sizes.
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:16:12,670'); seek(972.0)">
              And if we compare this to recurrent layers,
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:16:16,290'); seek(976.0)">
              it's also more perimeter efficient if the size of the sequence
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:16:19,698'); seek(979.0)">
              is smaller than the representation dimensionality when compared to
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:16:23,148'); seek(983.0)">
              convolutional layers. Convolutional layers
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:16:26,546'); seek(986.0)">
              for achieved a global receptive field, meaning that every pixel
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:16:30,786'); seek(990.0)">
              would interact with every pixel, we typically
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:16:34,342'); seek(994.0)">
              need to stack many of these convolutional layers on top of each other.
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:16:38,096'); seek(998.0)">
              And in self attention, all parts of those sequence interact with each
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:16:41,588'); seek(1001.0)">
              other within a single layer. Let's take a look at how
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:16:45,188'); seek(1005.0)">
              the self attention works. So the self attention relates
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:16:50,650'); seek(1010.0)">
              different positions of a sequence to compute a different representation
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:16:54,238'); seek(1014.0)">
              of that sequence. So we feed it as an input a sequence
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:16:57,742'); seek(1017.0)">
              z, in this case of size t and dimensional
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:17:01,822'); seek(1021.0)">
              td, and we compute three matrices, the queries,
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:17:05,230'); seek(1025.0)">
              keys and values. We do so by multiplying the input with this
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:17:08,648'); seek(1028.0)">
              matrix UQV and slicing along the last
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:17:12,412'); seek(1032.0)">
              dimension, the dimension of the tree times the dimension of
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:17:15,484'); seek(1035.0)">
              the head, and this will generate the queries, keys and values
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:17:18,978'); seek(1038.0)">
              for us. Next, we compute the dot product between the queries and
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:17:22,768'); seek(1042.0)">
              keys, so the queries and keys must have the same dimension and we
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:17:26,512'); seek(1046.0)">
              divide by a scaling factor. To alleviate vanishing gradient problems,
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:17:30,000'); seek(1050.0)">
              we apply a soft max in a row wise manner, and this will
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:17:34,212'); seek(1054.0)">
              be our attention matrix that has size t
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:17:38,260'); seek(1058.0)">
              by t. So it's quadratic to the size of the input
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:17:41,498'); seek(1061.0)">
              sequence, which is one of the bottlenecks of the transformer. And then we multiply
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:17:45,358'); seek(1065.0)">
              this by v, our matrix value to
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:17:48,968'); seek(1068.0)">
              retrieve the final computation.
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:17:52,870'); seek(1072.0)">
              However, the transformer
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:17:56,558'); seek(1076.0)">
              doesn't use the regular self attention, it use a generalization of it,
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:17:59,868'); seek(1079.0)">
              which is called a multi head self attention. Multi head self attention
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:18:03,490'); seek(1083.0)">
              is an extension of the self attention in which we run case of attention
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:18:07,362'); seek(1087.0)">
              operations in parallel. So we run
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:18:11,104'); seek(1091.0)">
              many self attentions in parallel, we concatenate them,
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:18:14,640'); seek(1094.0)">
              and then we do a linear projection again to
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:18:17,968'); seek(1097.0)">
              the dimension d. To not explode the dimensionality.
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:18:22,602'); seek(1102.0)">
              Let's now revisit the transformers after having seen how the self attention
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:18:25,946'); seek(1105.0)">
              works. So in the original transformer we add
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:18:29,892'); seek(1109.0)">
              an encoder and a recorded but we can also
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:18:33,176'); seek(1113.0)">
              use only a part of the architecture. For example,
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:18:36,344'); seek(1116.0)">
              architectures that only use those encoder part like vert,
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:18:40,870'); seek(1120.0)">
              are important when we only want global representation
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:18:44,958'); seek(1124.0)">
              of those sequence. And you want to build classification on top of it. For example
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:18:48,332'); seek(1128.0)">
              for performing sentiment analysis. When we
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:18:52,316'); seek(1132.0)">
              architectures that only use a decoder are used for
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:18:56,076'); seek(1136.0)">
              language modeling like GPD two. And we also have architectures
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:18:59,618'); seek(1139.0)">
              that use both encoder and decoder like detection transformer
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:19:03,398'); seek(1143.0)">
              that we'll see next. A fact that is also important is that self attention is
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:19:07,408'); seek(1147.0)">
              invariant to the position of those tokens. So it's very common to add
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:19:11,956'); seek(1151.0)">
              these position encodings to the input so that those model can reason
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:19:15,620'); seek(1155.0)">
              about the positions of the parts of the
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:19:18,868'); seek(1158.0)">
              sequence during
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:19:22,868'); seek(1162.0)">
              the self attention in the encoder and decoder
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:19:26,878'); seek(1166.0)">
              blocks. And now we are going
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:19:30,088'); seek(1170.0)">
              to talk about the detection transformer. The detection transformer is a
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:19:34,008'); seek(1174.0)">
              very simple architectures that is based on a CNN and a
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:19:37,388'); seek(1177.0)">
              transformer recorded architectures and it
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:19:40,652'); seek(1180.0)">
              uses a CNN backbone. So we feed it an images
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:19:44,402'); seek(1184.0)">
              and this image goes through the CNN backbone and
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:19:48,048'); seek(1188.0)">
              generates a feature map with lower width
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:19:52,350'); seek(1192.0)">
              and lower weights, but with a much deeper
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:19:56,190'); seek(1196.0)">
              number of channels. And now we have this distancer
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:19:59,734'); seek(1199.0)">
              of width, height and channels, but we want to feed it into the transformer
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:20:03,578'); seek(1203.0)">
              encoder. But the transformer encoder is expecting a
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:20:07,188'); seek(1207.0)">
              sequence. So the way we can do this is by flattening the spatial
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:20:11,338'); seek(1211.0)">
              dimensions of the input, by multiplying the
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:20:14,968'); seek(1214.0)">
              height and width and then we can feed it into the transformer encoder.
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:20:18,958'); seek(1218.0)">
              Then we have the transformer decoder that has these object
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:20:22,584'); seek(1222.0)">
              queries which are learned by the model as the input.
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:20:27,086'); seek(1227.0)">
              And these object queries are the number of objects that we are trying to
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:20:30,892'); seek(1230.0)">
              detect in an image. So it must be set to
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:20:34,332'); seek(1234.0)">
              be larger than the largest number of objects
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:20:37,714'); seek(1237.0)">
              that we have in an image to provide us some slack. And they will learn
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:20:41,152'); seek(1241.0)">
              to attend to specific areas and specific bounding boxes
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:20:45,062'); seek(1245.0)">
              sizes in an image. Then the decoder is
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:20:48,768'); seek(1248.0)">
              also conditioned on the encoder output
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:20:52,794'); seek(1252.0)">
              and we predict the classes,
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:20:56,410'); seek(1256.0)">
              the object class and the bounding box through parallel decoding. So it's
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:21:00,074'); seek(1260.0)">
              not in an autoregressive way. We output them in parallel
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:21:03,370'); seek(1263.0)">
              and we are treating the object detectors problem as a direct set case
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:21:07,176'); seek(1267.0)">
              prediction. So we need an appropriate loss for that. They use
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:21:11,192'); seek(1271.0)">
              those bipartite matching loss based on the hungarian algorithm
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:21:15,374'); seek(1275.0)">
              that is permutation invariant and also forces a unique
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:21:18,882'); seek(1278.0)">
              assignment between the ground truth and the predicted objects.
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:21:22,722'); seek(1282.0)">
              We are going to use the egging phase library that contains many
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:21:26,332'); seek(1286.0)">
              transformers and they recently added those visual transformers
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:21:30,710'); seek(1290.0)">
              for image classification like the visual transformer VIT and also
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:21:33,952'); seek(1293.0)">
              this detection transformer for object detection. They added this
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:21:37,728'); seek(1297.0)">
              to the library and we are going to use based on a REsnet 50
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:21:41,380'); seek(1301.0)">
              backbone the reason we use this dilated convolutional
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:21:44,906'); seek(1304.0)">
              is that the dilated convolution will increase
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:21:48,442'); seek(1308.0)">
              the resolution by a factor of two at the expense of more computations,
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:21:53,570'); seek(1313.0)">
              but it will help detecting small scale
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:21:57,290'); seek(1317.0)">
              objects. Egin face provides a very comprehensive
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:22:01,166'); seek(1321.0)">
              set of documentation. It also explains the internal part of those model and we also
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:22:04,888'); seek(1324.0)">
              have these example notebooks by Niels rogue that are linked
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:22:08,510'); seek(1328.0)">
              at the page and at the bottom of this slide. That explains how we
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:22:11,868'); seek(1331.0)">
              can fine tune object object object object object object object object object
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:22:16,060'); seek(1336.0)">
              object object detection transformers CNNS drone case
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:22:19,868'); seek(1339.0)">
              feature extractor used for pre processing
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:22:23,810'); seek(1343.0)">
              the input for the model or for post processing the output of the model in
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:22:27,664'); seek(1347.0)">
              the cocoa notation format. For example for running the cocoa
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:22:31,274'); seek(1351.0)">
              evaluation metrics we also have the data for object detection
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:22:34,842'); seek(1354.0)">
              model that exposes those
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:22:38,452'); seek(1358.0)">
              logits and the prediction boxes and also we have this data
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:22:42,660'); seek(1362.0)">
              config that can be used for institiating
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:22:46,474'); seek(1366.0)">
              data for object detection model. Through this
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:22:49,832'); seek(1369.0)">
              configuration. The modifications that we do when
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:22:54,152'); seek(1374.0)">
              compared to those notebook is that we use the REsnet with a little convolutions instead
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:22:58,152'); seek(1378.0)">
              of the REsnet 50. We also set the maximum size of
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:23:01,388'); seek(1381.0)">
              the image to 1100 to not eat good out of memory
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:23:04,818'); seek(1384.0)">
              aircorse and we also use a smaller batch size of two instead
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:23:08,432'); seek(1388.0)">
              of four because on v 100 gpu we
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:23:11,968'); seek(1391.0)">
              use get out of memory aircraft. Otherwise, after training
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:23:16,830'); seek(1396.0)">
              detectors transformers on our data set we see that average
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:23:20,922'); seek(1400.0)">
              precision is very poor compared to the objects
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:23:25,410'); seek(1405.0)">
              detections. Based on cnns we
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:23:29,044'); seek(1409.0)">
              have seen previously, the model is able to detect large
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:23:32,596'); seek(1412.0)">
              objects. It has a fairly good average precision
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:23:36,494'); seek(1416.0)">
              for large objects, but it is very small for small and medium objects
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:23:40,710'); seek(1420.0)">
              which can be attributed to the detection transformer
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:23:44,814'); seek(1424.0)">
              not being suitable for these small scale
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:23:48,690'); seek(1428.0)">
              object detectors problems and as feature pyramid
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:23:52,242'); seek(1432.0)">
              networks did for cnns for
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:23:56,236'); seek(1436.0)">
              helping addressing those multiscale object detectors problem.
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:24:00,272'); seek(1440.0)">
              Similar approaches could also help improving the detection transformer
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:24:04,278'); seek(1444.0)">
              further. We see in the inference results. We have some
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:24:10,756'); seek(1450.0)">
              duplicate detections that could be probably removed
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:24:14,058'); seek(1454.0)">
              by using non maximal suppression and we also have
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:24:18,036'); seek(1458.0)">
              some missed detections. So how can
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:24:21,348'); seek(1461.0)">
              we improve these results further? We can for example scale the
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:24:24,868'); seek(1464.0)">
              backbone. In all of those experiments we use the resonate 50 but we could
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:24:28,728'); seek(1468.0)">
              use a larger backbone like a REsnet 101.
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:24:33,350'); seek(1473.0)">
              The results we had for the documentation didn't
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:24:39,042'); seek(1479.0)">
              improve our results, but we could fine tune the probabilities
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:24:43,394'); seek(1483.0)">
              or also change the augmentation transformations
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:24:47,710'); seek(1487.0)">
              to find if we could get better results. Right now we also
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:24:51,376'); seek(1491.0)">
              have more publicly available data sets
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:24:55,398'); seek(1495.0)">
              recorded by drones like Miva,
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:24:58,086'); seek(1498.0)">
              UAVDT and so on and we could use this to build
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:25:01,652'); seek(1501.0)">
              a larger data set to see if we can get
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:25:04,916'); seek(1504.0)">
              better results out of this. Also, we only used
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:25:08,724'); seek(1508.0)">
              static images for the object detection part.
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:25:12,996'); seek(1512.0)">
              But if we think about video object detection, we can exploit
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:25:17,678'); seek(1517.0)">
              these temporal cues of the
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:25:21,432'); seek(1521.0)">
              different frames to reduce the number of false positives.
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:25:25,182'); seek(1525.0)">
              We also have different transformer architectures, for example
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:25:30,330'); seek(1530.0)">
              the using transformer or the focal transformer
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:25:34,098'); seek(1534.0)">
              that could be used and tested to see if
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:25:37,920'); seek(1537.0)">
              they provide better results. To conclude,
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:25:41,702'); seek(1541.0)">
              we see that CNNs make for very powerful baselines.
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:25:45,462'); seek(1545.0)">
              We used off the shelf pre trained CNN
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:25:49,206'); seek(1549.0)">
              architectures, the faster CNN and retinate and
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:25:53,236'); seek(1553.0)">
              got very good average precision results in Visdron
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:25:56,954'); seek(1556.0)">
              for detecting cars. The transformer architectures are being increasingly
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:26:01,626'); seek(1561.0)">
              used in research and practice and we
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:26:04,808'); seek(1564.0)">
              can see that they are being added to these mainstream libraries. Like egging
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:26:08,718'); seek(1568.0)">
              case, for example, the detection transformer
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:26:11,886'); seek(1571.0)">
              is better suited for medium to large to
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:26:16,316'); seek(1576.0)">
              large objects. But developments similar to the feature pyramid
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:26:20,002'); seek(1580.0)">
              network as it was
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:26:23,436'); seek(1583.0)">
              used for CNNs can also help. The detection
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:26:27,298'); seek(1587.0)">
              transformer and the transformers will continue to be used into
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:26:32,110'); seek(1592.0)">
              downstream tasks like object detection, images classification
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:26:35,510'); seek(1595.0)">
              and image representations. We can see many research papers coming from these
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:26:39,472'); seek(1599.0)">
              areas and last but not least,
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:26:42,788'); seek(1602.0)">
              transformers make from a unifying framework for different fields.
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:26:47,066'); seek(1607.0)">
              So before we encode all of these inductive biases
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:26:50,730'); seek(1610.0)">
              that we have for those CNNs and for the OSTMs.
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:26:53,642'); seek(1613.0)">
              On the other hand, the transformer makes for a very general purpose architecture
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:26:57,658'); seek(1617.0)">
              that lacks these inductive biases, but it can learn them from
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:27:02,068'); seek(1622.0)">
              large scale data and it has
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:27:06,252'); seek(1626.0)">
              given very good results for natural language processing
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:27:09,458'); seek(1629.0)">
              and it's now also giving some state of the art results
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:27:12,930'); seek(1632.0)">
              in image. And so it can maybe unify
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:27:16,386'); seek(1636.0)">
              both fields and also unify the
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:27:20,332'); seek(1640.0)">
              practitioners and researchers from both areas.
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:27:23,746'); seek(1643.0)">
              So today, this concludes my presentation.
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:27:27,218'); seek(1647.0)">
              I want to thank you for listening.
            </span>
            
            </div>
          </div>
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/Conf42%20Machine%20Learning%202021%20Slides%20-%20Eduardo%20Dixo.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/Conf42%20Machine%20Learning%202021%20Slides%20-%20Eduardo%20Dixo.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #198B91;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/ml2021" class="btn btn-sm btn-danger shadow lift" style="background-color: #198B91;">
                <i class="fe fe-grid me-2"></i>
                See all 23 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/ml_eduardo.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Eduardo Dixo
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Senior Data Scientist @ Continental
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/eduardo-dixo/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Eduardo Dixo's LinkedIn account" />
                  </a>
                  
                  
                  <a href="https://twitter.com/@Eduardo_dixo" target="_blank">
                    <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="Eduardo Dixo's twitter account" />
                  </a>
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by @Eduardo_dixo"
                  data-url="https://www.conf42.com/ml2021"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/ml2021"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>





  <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
    </script>
    <!-- SUBSCRIBE -->
    <section class="pt-8 pt-md-11 bg-gradient-light-white" id="register">
        <div class="container">
          <div class="row align-items-center justify-content-between mb-8 mb-md-11">
            <div class="col-12 col-md-6 order-md-2" data-aos="fade-left">
  
              <!-- Heading -->
              <h2>
                Awesome tech events for <br>
                <span class="text-success"><span data-typed='{"strings": ["software engineers.", "tech leaders.", "SREs.", "DevOps.", "CTOs.",  "managers.", "architects.", "QAs.", "developers.", "coders.", "founders.", "CEOs.", "students.", "geeks.", "ethical hackers.", "educators.", "enthusiasts.", "directors.", "researchers.", "PHDs.", "evangelists.", "tech authors."]}'></span></span>
              </h2>
  
              <!-- Text -->
              <p class="fs-lg text-muted mb-6">
  
              </p>
  
              <!-- List -->
              <div class="row">
                <div class="col-12 col-lg-12">
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <!-- Text -->
                    <p class="text-success">
                      Priority access to all content
                    </p>
                  </div>
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Video hallway track
                    </p>
                  </div>

                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Community chat
                    </p>
                  </div>
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Exclusive promotions and giveaways
                    </p>
                  </div>
  
                </div>
              </div> <!-- / .row -->
  
            </div>
            <div class="col-12 col-md-6 col-lg-5 order-md-1">
  
              <!-- Card -->
              <div class="card shadow-light-lg">
  
                <!-- Body -->
                <div class="card-body">
  
                  <!-- Form -->
                  <link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
                  <p class="emailoctopus-success-message text-success"></p>
                  <p class="emailoctopus-error-message text-danger"></p>
                  <form
                    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
                    method="post"
                    data-message-success="Thanks! Check your email for further directions!"
                    data-message-missing-email-address="Your email address is required."
                    data-message-invalid-email-address="Your email address looks incorrect, please try again."
                    data-message-bot-submission-error="This doesn't look like a human submission."
                    data-message-consent-required="Please check the checkbox to indicate your consent."
                    data-message-invalid-parameters-error="This form has missing or invalid fields."
                    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
                    class="emailoctopus-form"
                    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
                  >
                    <div class="form-floating emailoctopus-form-row">
                      <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
                      <label for="field_0">Email address</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
                      <label for="field_1">First Name</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
                      <label for="field_2">Last Name</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
                      <label for="field_4">Company</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
                      <label for="field_5">Job Title</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
                      <label for="field_3">Phone Number</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
                        oninput="updateCountry()"
                      >
                        <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
                      </select>
                      <label for="field_7">Country</label>
                    </div>
                    <input id="country-destination" name="field_7" type="hidden">
                    <input id="tz-country" name="field_8" type="hidden">
                    
                    <input
                      name="field_6"
                      type="hidden"
                      value="Artificial Intelligence & Machine Learning"
                    >
                    
                    <div class="emailoctopus-form-row-consent">
                      <input
                        type="checkbox"
                        id="consent"
                        name="consent"
                      >
                      <label for="consent">
                        I consent to the following terms:
                      </label>
                      <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
                        Terms and Conditions
                      </a>
                      &amp;
                      <a href="./code-of-conduct" target="_blank">
                        Code of Conduct
                      </a>
                    </div>
                    <div
                      aria-hidden="true"
                      class="emailoctopus-form-row-hp"
                    >
                      <input
                        type="text"
                        name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
                        tabindex="-1"
                        autocomplete="nope"
                      >
                    </div>
                    <div class="mt-6 emailoctopus-form-row-subscribe">
                      <input
                        type="hidden"
                        name="successRedirectUrl"
                      >
                      <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
                        Subscribe
                      </button>
                    </div>
                  </form>
  
                </div>
  
              </div>
  
            </div>
  
          </div> <!-- / .row -->
        </div> <!-- / .container -->
      </section>

      <!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
      <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.sreday.com/">
                  SREday San Francisco 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.sreday.com/">
                  SREday London 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.sreday.com/">
                  SREday Amsterdam 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/@widgetbot/crate@3" async defer>
      new Crate({
        server: '814240231606714368',
        channel: '814240231788249115'
      })
    </script>
  </body>
</html>