<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Deploying ML solutions with low latency in Python</title>
    <meta name="description" content="Like the Machines need our help to take over the world">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/ml_aditya.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Deploying ML solutions with low latency in Python | Conf42"/>
    <meta property="og:description" content="When we aim for better accuracies, sometimes we forget that the algorithms become more massive and slower. This fact renders the algorithms unusable in real-time scenarios. How do you deploy your solution? Which framework to use? Can you use Python for deploying my solution? Can you use Jetson Nano for multi-stream inferencing? If you are curious to solve these questions, join me in this talk to discover TensorRT and DeepStream and how they reduce your algorithm's latency and memory footprint.   NVIDIA TensorRTâ„¢ is an SDK for high-performance deep learning inference. It includes a deep learning inference optimizer and runtime that delivers low latency and high-throughput for deep learning inference applications. DeepStream offers a multi-platform scalable framework with TLS security to deploy on edge and connect to any cloud. If you are using a GPU and CUDA/Tensor cores, you can leverage the SDK framework to deploy bigger and better algorithms for your real-time scenarios.  The main focus of this talk will be to demonstrate why, where, and how to use TensorRT and DeepStream. "/>
    <meta property="og:url" content="https://conf42.com/Machine_Learning_2021_Aditya_Lohia_Deploying_ML_solutions_low_latency"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/INCIDENT2024_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Incident Management 2024
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2024-10-17
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/im2024" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.sreday.com/">
                            SREday Amsterdam
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="./support">
                            Support us
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Subscribe for FREE
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #198B91;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Machine Learning 2021 - Online
            </h1>

            <h2 class="text-white">
              
              <time datetime="2021-07-29">July 29 2021</time>
              
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- Like the Machines need our help to take over the world
 -->
              <script>
                const event_date = new Date("2021-07-29T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2021-07-29T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              <a href="#register" class="btn btn-primary shadow lift me-1 mb-3">
                <i class="fe fe-user-check me-2"></i>
                Subscribe to watch
              </a>
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "ovLKssNavS4"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <a class="btn btn-info lift mb-3" data-bigpicture='{"ytSrc": "h6kzwaBwrCY"}' href="#">
                <i class="fe fe-eye me-2"></i>
                Watch Premiere
              </a>
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://youtube.com/playlist?list=PLIuxSyKxlQrD02X7IKNNxFMy_K8oEejAu" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Today we will talk about deploying ML solutions with low latency in python in.", "timestamp": "00:00:22,010", "timestamp_s": 22.0}, {"text": "You can find me at my LinkedIn address mentioned here.", "timestamp": "00:00:25,932", "timestamp_s": 25.0}, {"text": "So, as the world moves forward with these research in improving accuracies of", "timestamp": "00:00:29,130", "timestamp_s": 29.0}, {"text": "deep learning algorithms, we face an imminent problem of deploying", "timestamp": "00:00:32,828", "timestamp_s": 32.0}, {"text": "those algorithms. So, as many of my fellow", "timestamp": "00:00:36,338", "timestamp_s": 36.0}, {"text": "researchers might know, everyone here present is involved", "timestamp": "00:00:39,462", "timestamp_s": 39.0}, {"text": "in, directly or indirectly, in improving the algorithms that we currently", "timestamp": "00:00:43,622", "timestamp_s": 43.0}, {"text": "use today. However, these developers that are", "timestamp": "00:00:47,680", "timestamp_s": 47.0}, {"text": "using those algorithms often face the problem of making those algorithms", "timestamp": "00:00:51,472", "timestamp_s": 51.0}, {"text": "real time, even after doing a bunch of stuff that we\u0027ll be", "timestamp": "00:00:55,322", "timestamp_s": 55.0}, {"text": "discussing today. So what do I mean by low", "timestamp": "00:00:58,868", "timestamp_s": 58.0}, {"text": "latency? So, it is a term used", "timestamp": "00:01:02,532", "timestamp_s": 62.0}, {"text": "to describe the performance of can ML pipeline,", "timestamp": "00:01:06,392", "timestamp_s": 66.0}, {"text": "and it\u0027s basically the time taken to process data", "timestamp": "00:01:09,358", "timestamp_s": 69.0}, {"text": "by the ML algorithms. It\u0027s like a", "timestamp": "00:01:13,848", "timestamp_s": 73.0}, {"text": "time taken to process a single image or a single video,", "timestamp": "00:01:17,548", "timestamp_s": 77.0}, {"text": "and highthroughput is the time taken to process these entire data,", "timestamp": "00:01:20,892", "timestamp_s": 80.0}, {"text": "and latency is inversely proportional to throughput. And our", "timestamp": "00:01:24,970", "timestamp_s": 84.0}, {"text": "focus in this talk will be to convert", "timestamp": "00:01:28,348", "timestamp_s": 88.0}, {"text": "or deploy our ML pipelines in low latency. So as you can see", "timestamp": "00:01:31,622", "timestamp_s": 91.0}, {"text": "in this graph, entire ML pipeline is it starts with the", "timestamp": "00:01:35,072", "timestamp_s": 95.0}, {"text": "input data and it goes through several steps before we can get the output.", "timestamp": "00:01:38,928", "timestamp_s": 98.0}, {"text": "And these. The bottleneck we face most of the", "timestamp": "00:01:42,774", "timestamp_s": 102.0}, {"text": "time is the model inference. And there are several methods by which we", "timestamp": "00:01:46,468", "timestamp_s": 106.0}, {"text": "can improve the latency of the algorithms. First, one is weight quantization,", "timestamp": "00:01:50,132", "timestamp_s": 110.0}, {"text": "wherein you can deploy the model", "timestamp": "00:01:54,378", "timestamp_s": 114.0}, {"text": "in multiple quantization methods using multiple quantization", "timestamp": "00:01:57,848", "timestamp_s": 117.0}, {"text": "methods such as float 16 int eight. There are", "timestamp": "00:02:01,486", "timestamp_s": 121.0}, {"text": "two types of quantizations. One is post training quantization,", "timestamp": "00:02:05,048", "timestamp_s": 125.0}, {"text": "and one is quantization aware training. I cannot go in detail in", "timestamp": "00:02:08,526", "timestamp_s": 128.0}, {"text": "each of these methods as the time won\u0027t permit me to do so. But basically", "timestamp": "00:02:11,868", "timestamp_s": 131.0}, {"text": "you can use any of those to check which method", "timestamp": "00:02:15,964", "timestamp_s": 135.0}, {"text": "is giving you better accuracies. And quantization often involves", "timestamp": "00:02:19,874", "timestamp_s": 139.0}, {"text": "you to deal with accuracies and speed versus speed.", "timestamp": "00:02:23,862", "timestamp_s": 143.0}, {"text": "So in case you go with int eight, you will get much better speed.", "timestamp": "00:02:27,270", "timestamp_s": 147.0}, {"text": "However, there will be a drop in accuracy, and I generally", "timestamp": "00:02:30,342", "timestamp_s": 150.0}, {"text": "find that float 16 is the best method to go because it preserves", "timestamp": "00:02:33,718", "timestamp_s": 153.0}, {"text": "my accuracy and it also gives me like 1.5", "timestamp": "00:02:37,418", "timestamp_s": 157.0}, {"text": "or two times of what float 32 gives me. The second method that", "timestamp": "00:02:41,092", "timestamp_s": 161.0}, {"text": "is often used is model pruning. So the basic concept", "timestamp": "00:02:45,670", "timestamp_s": 165.0}, {"text": "is that you prune certain layers and connections of the models", "timestamp": "00:02:49,118", "timestamp_s": 169.0}, {"text": "based on several experiments. You run your", "timestamp": "00:02:52,542", "timestamp_s": 172.0}, {"text": "model with multiple and like a bunch of images", "timestamp": "00:02:56,152", "timestamp_s": 176.0}, {"text": "to see which layers can be omitted", "timestamp": "00:02:59,602", "timestamp_s": 179.0}, {"text": "and can be skipped so that these parameters won\u0027t be", "timestamp": "00:03:03,394", "timestamp_s": 183.0}, {"text": "calculated by this way. Sometimes you can remove like", "timestamp": "00:03:06,972", "timestamp_s": 186.0}, {"text": "99% of your connections and you can still preserve your accuracy,", "timestamp": "00:03:10,652", "timestamp_s": 190.0}, {"text": "but that is like the best case solution. Another method that is quite popular", "timestamp": "00:03:14,182", "timestamp_s": 194.0}, {"text": "today is knowledge distillation, and it is the concept wherein you are transferring", "timestamp": "00:03:17,702", "timestamp_s": 197.0}, {"text": "the knowledge of a bigger model into a smaller model. So suppose", "timestamp": "00:03:21,638", "timestamp_s": 201.0}, {"text": "you have a Stanford car data set and you have trained a Resnet 151 and", "timestamp": "00:03:25,258", "timestamp_s": 205.0}, {"text": "you get a testing classification accuracy of 95%.", "timestamp": "00:03:29,476", "timestamp_s": 209.0}, {"text": "Now when you train a REsnet 18 on the similar data on the same data", "timestamp": "00:03:33,396", "timestamp_s": 213.0}, {"text": "set, you might not find that it is giving you a 95% accuracy.", "timestamp": "00:03:36,696", "timestamp_s": 216.0}, {"text": "You often see that it is limited to 80% to 90% because", "timestamp": "00:03:40,606", "timestamp_s": 220.0}, {"text": "of the shallow depth of it. So with using", "timestamp": "00:03:44,312", "timestamp_s": 224.0}, {"text": "knowledge distillation, what you can do is you can transfer the knowledge learned by", "timestamp": "00:03:47,612", "timestamp_s": 227.0}, {"text": "Resnet 151 and", "timestamp": "00:03:51,436", "timestamp_s": 231.0}, {"text": "use Resnet 18 as if the like use resonate", "timestamp": "00:03:55,164", "timestamp_s": 235.0}, {"text": "18 with 95% of accuracies. And the fourth method", "timestamp": "00:03:58,978", "timestamp_s": 238.0}, {"text": "that is my topic of the talk today, that is framework based deployment.", "timestamp": "00:04:02,582", "timestamp_s": 242.0}, {"text": "So we will be seeing two frameworks, Tensorrt and deep", "timestamp": "00:04:06,694", "timestamp_s": 246.0}, {"text": "steam, and we\u0027ll be seeing how they help us in deploying these algorithms.", "timestamp": "00:04:10,870", "timestamp_s": 250.0}, {"text": "So what is Tensorrt? Tensorrt is", "timestamp": "00:04:15,178", "timestamp_s": 255.0}, {"text": "SDK for high performance deep learning inference. It is provided", "timestamp": "00:04:19,410", "timestamp_s": 259.0}, {"text": "by Nvidia and they have written the entire tensorrt in C", "timestamp": "00:04:23,114", "timestamp_s": 263.0}, {"text": "with Python bindings available. It includes a deep learning inference", "timestamp": "00:04:27,496", "timestamp_s": 267.0}, {"text": "optimizer and runtime. So the optimizer\u0027s job is to optimize", "timestamp": "00:04:31,278", "timestamp_s": 271.0}, {"text": "your model, like when you are converting it to tensorrt, it will optimize", "timestamp": "00:04:35,278", "timestamp_s": 275.0}, {"text": "the entire model and it will convert the layers using advanced", "timestamp": "00:04:39,282", "timestamp_s": 279.0}, {"text": "CUDA methods in C. And the runtime is responsible", "timestamp": "00:04:43,698", "timestamp_s": 283.0}, {"text": "for actually running your tensorrt engines. So Tensorrt often", "timestamp": "00:04:47,522", "timestamp_s": 287.0}, {"text": "delivers low latency and high throughput for several deep learning applications.", "timestamp": "00:04:52,048", "timestamp_s": 292.0}, {"text": "I have tried Tensorrt in the industry and it works great.", "timestamp": "00:04:56,102", "timestamp_s": 296.0}, {"text": "It supports both Python and C. And nowadays", "timestamp": "00:04:59,712", "timestamp_s": 299.0}, {"text": "Tensorrt supports conversion from multiple frameworks such as", "timestamp": "00:05:03,722", "timestamp_s": 303.0}, {"text": "Tensorflow, Pytorch, Mixnet, Theano,", "timestamp": "00:05:07,220", "timestamp_s": 307.0}, {"text": "Onnx, etc. For reference, I have", "timestamp": "00:05:09,946", "timestamp_s": 309.0}, {"text": "linked the tensorrt\u0027s official documentation and developer page", "timestamp": "00:05:13,172", "timestamp_s": 313.0}, {"text": "below. So how does Tensorrt do the", "timestamp": "00:05:17,124", "timestamp_s": 317.0}, {"text": "entire thing? So it is responsible for optimizing", "timestamp": "00:05:20,840", "timestamp_s": 320.0}, {"text": "your model, and it does so using the method shown here.", "timestamp": "00:05:24,718", "timestamp_s": 324.0}, {"text": "What it does is it does layer in tensorfusion,", "timestamp": "00:05:28,588", "timestamp_s": 328.0}, {"text": "it does kernel auto tuning, it does precision calibration, it does", "timestamp": "00:05:32,946", "timestamp_s": 332.0}, {"text": "dynamic tensor memory, it uses dynamic tensor memory,", "timestamp": "00:05:36,492", "timestamp_s": 336.0}, {"text": "and it is also possible to use multi steam execution with Tensorrt.", "timestamp": "00:05:39,634", "timestamp_s": 339.0}, {"text": "That is, you won\u0027t have to worry about like you can actually use batch processing", "timestamp": "00:05:43,202", "timestamp_s": 343.0}, {"text": "for this using Tensorrt. And we\u0027ll be talking about more of these", "timestamp": "00:05:47,238", "timestamp_s": 347.0}, {"text": "methods in further slides. So let\u0027s", "timestamp": "00:05:50,996", "timestamp_s": 350.0}, {"text": "talk about weight and activation precision calibration. So,", "timestamp": "00:05:55,034", "timestamp_s": 355.0}, {"text": "to quantize full precision information into intate while", "timestamp": "00:05:58,612", "timestamp_s": 358.0}, {"text": "minimizing accuracies loss, Tensorrt must perform a process called", "timestamp": "00:06:01,908", "timestamp_s": 361.0}, {"text": "calibration to determine how best to represent the weights", "timestamp": "00:06:05,512", "timestamp_s": 365.0}, {"text": "and activations as eight bit integers. These calibration step", "timestamp": "00:06:08,974", "timestamp_s": 368.0}, {"text": "requires you to provide tensorrt with a representative sample of the input", "timestamp": "00:06:12,456", "timestamp_s": 372.0}, {"text": "training data, and no additional fine tuning or restraining of the model is", "timestamp": "00:06:15,902", "timestamp_s": 375.0}, {"text": "necessary. Also, you don\u0027t need to have access to the entire training data", "timestamp": "00:06:19,612", "timestamp_s": 379.0}, {"text": "set, you just give it a sample. Calibration is a", "timestamp": "00:06:23,052", "timestamp_s": 383.0}, {"text": "completely automated and parameter free method for converting your model from", "timestamp": "00:06:26,892", "timestamp_s": 386.0}, {"text": "f float 32 to NTA eight. What is kernel autotuning?", "timestamp": "00:06:30,784", "timestamp_s": 390.0}, {"text": "So, during the optimization phase of Tensorrt,", "timestamp": "00:06:34,534", "timestamp_s": 394.0}, {"text": "it also chooses from hundreds of specialized kernels that are created by default,", "timestamp": "00:06:37,702", "timestamp_s": 397.0}, {"text": "and many of them are hand tuned and optimized for a range of parameters", "timestamp": "00:06:41,942", "timestamp_s": 401.0}, {"text": "and target platforms. So as an example, there are several different algorithms", "timestamp": "00:06:45,402", "timestamp_s": 405.0}, {"text": "to do convolutions. Tensorrt will pick the implementation from", "timestamp": "00:06:48,842", "timestamp_s": 408.0}, {"text": "a library of kernels that delivers the best performance for the target gpu,", "timestamp": "00:06:52,948", "timestamp_s": 412.0}, {"text": "input data size, filter size, tensorrt layout, batch size and other parameters.", "timestamp": "00:06:56,750", "timestamp_s": 416.0}, {"text": "This ensures that the developed model is highperformance tuned for the specific", "timestamp": "00:07:01,006", "timestamp_s": 421.0}, {"text": "development platform as well as for the specific neural network being deploying.", "timestamp": "00:07:04,984", "timestamp_s": 424.0}, {"text": "So also, TensorRt is supposed to convert", "timestamp": "00:07:08,798", "timestamp_s": 428.0}, {"text": "your models on a particular development platform. So you", "timestamp": "00:07:12,418", "timestamp_s": 432.0}, {"text": "cannot run a TensorRt engine converted on suppose", "timestamp": "00:07:15,948", "timestamp_s": 435.0}, {"text": "NVDiA\u0027s 1050 Ti and use it on a 2060 Ti.", "timestamp": "00:07:20,466", "timestamp_s": 440.0}, {"text": "You have to do it on that particular GPU to be used on that particular", "timestamp": "00:07:24,838", "timestamp_s": 444.0}, {"text": "GPU. So what is dynamic tensor memory?", "timestamp": "00:07:28,176", "timestamp_s": 448.0}, {"text": "Tensorrt also reduces memory footprint and improves", "timestamp": "00:07:31,574", "timestamp_s": 451.0}, {"text": "memory you reuse by designating memory for each tensor", "timestamp": "00:07:35,238", "timestamp_s": 455.0}, {"text": "only for the duration of its use, avoiding memory allocation overhead", "timestamp": "00:07:39,242", "timestamp_s": 459.0}, {"text": "for fast and efficient execution. So what", "timestamp": "00:07:43,082", "timestamp_s": 463.0}, {"text": "is multi stream execution? So, as I mentioned before, it is basically the ability of", "timestamp": "00:07:46,168", "timestamp_s": 466.0}, {"text": "tensorrt to process multiple input streams in parallel,", "timestamp": "00:07:49,928", "timestamp_s": 469.0}, {"text": "and it does so beautifully.", "timestamp": "00:07:53,310", "timestamp_s": 473.0}, {"text": "And what is layer and tensorfusion?", "timestamp": "00:07:56,862", "timestamp_s": 476.0}, {"text": "So, tensorrt calibrates, while in", "timestamp": "00:08:00,366", "timestamp_s": 480.0}, {"text": "the optimization step it calibrates your entire model,", "timestamp": "00:08:04,364", "timestamp_s": 484.0}, {"text": "it sees the entire model, and it basically fuses", "timestamp": "00:08:07,516", "timestamp_s": 487.0}, {"text": "several layers or tensors together, so that you", "timestamp": "00:08:10,998", "timestamp_s": 490.0}, {"text": "reduce the parameter calculation and you reduce these", "timestamp": "00:08:14,416", "timestamp_s": 494.0}, {"text": "multiple times. The data has to be passed from one layer to another. It is", "timestamp": "00:08:17,792", "timestamp_s": 497.0}, {"text": "a single block. So suppose you have a convolution layer,", "timestamp": "00:08:21,472", "timestamp_s": 501.0}, {"text": "activation function layer, and a fully connected layer in a", "timestamp": "00:08:25,434", "timestamp_s": 505.0}, {"text": "network. What tensorrt it will do is it will combine all those three into", "timestamp": "00:08:28,708", "timestamp_s": 508.0}, {"text": "a single module. So basically it reduces the", "timestamp": "00:08:32,148", "timestamp_s": 512.0}, {"text": "time to traverse the data, and it", "timestamp": "00:08:35,368", "timestamp_s": 515.0}, {"text": "also reduces some of the overhead that is incurred", "timestamp": "00:08:38,568", "timestamp_s": 518.0}, {"text": "by each layer calls. So here", "timestamp": "00:08:42,638", "timestamp_s": 522.0}, {"text": "you can see the difference between the optimizer network and", "timestamp": "00:08:45,996", "timestamp_s": 525.0}, {"text": "the tensor RT optimized network. This is an example of Google\u0027s", "timestamp": "00:08:49,980", "timestamp_s": 529.0}, {"text": "Leannet architecture, which won the imagenet competition in 2014.", "timestamp": "00:08:54,178", "timestamp_s": 534.0}, {"text": "Sorry, it\u0027s called Google Net. So as", "timestamp": "00:08:58,854", "timestamp_s": 538.0}, {"text": "shown here, after layer intensive fusion, what happens", "timestamp": "00:09:03,088", "timestamp_s": 543.0}, {"text": "is you can see on the left side you have multiple layers,", "timestamp": "00:09:06,800", "timestamp_s": 546.0}, {"text": "whereas on the right side you have quite a few amount, number of layers.", "timestamp": "00:09:10,582", "timestamp_s": 550.0}, {"text": "A deep learning framework, what it does is it does multiple function calls", "timestamp": "00:09:15,090", "timestamp_s": 555.0}, {"text": "for calling each layer, and as each layer is", "timestamp": "00:09:18,730", "timestamp_s": 558.0}, {"text": "on the GPU, it translates to multiple CudA kernel launches.", "timestamp": "00:09:22,228", "timestamp_s": 562.0}, {"text": "The kernel computation is often very fast relative to the kernel", "timestamp": "00:09:26,026", "timestamp_s": 566.0}, {"text": "launch overhead and the cost of reading and writing the tensor", "timestamp": "00:09:29,502", "timestamp_s": 569.0}, {"text": "data for each layer. This results in the memory bandwidth bottleneck and underutilization", "timestamp": "00:09:32,574", "timestamp_s": 572.0}, {"text": "of the available GPU resources. And Tensorrt addresses this by vertically", "timestamp": "00:09:37,118", "timestamp_s": 577.0}, {"text": "fusing kernels to perform the sequential operations. Together,", "timestamp": "00:09:41,346", "timestamp_s": 581.0}, {"text": "these layer fusion reduces kernel launches and avoids writing into", "timestamp": "00:09:44,332", "timestamp_s": 584.0}, {"text": "and reading from memory between layers. So in the", "timestamp": "00:09:47,836", "timestamp_s": 587.0}, {"text": "figure shown, the convolution bias and relu layers of various sizes can", "timestamp": "00:09:51,344", "timestamp_s": 591.0}, {"text": "be combined into a single layer called as CBR.", "timestamp": "00:09:55,168", "timestamp_s": 595.0}, {"text": "And a simple analogy is making three separate", "timestamp": "00:09:58,670", "timestamp_s": 598.0}, {"text": "trips to the supermarket to buy three items versus buying all the", "timestamp": "00:10:01,798", "timestamp_s": 601.0}, {"text": "three in a single trip. And Tensorrt also recognizes", "timestamp": "00:10:05,284", "timestamp_s": 605.0}, {"text": "layers that share the same input data and filter size, but have different", "timestamp": "00:10:09,098", "timestamp_s": 609.0}, {"text": "weights. Instead of three separate kernels, tensorrt fuses them", "timestamp": "00:10:12,292", "timestamp_s": 612.0}, {"text": "horizontally into a single wider kernel, as shown", "timestamp": "00:10:15,688", "timestamp_s": 615.0}, {"text": "as one into one CBR. And tensorrt also eliminates the", "timestamp": "00:10:19,358", "timestamp_s": 619.0}, {"text": "concatenation layer by preallocating output buffers and", "timestamp": "00:10:23,430", "timestamp_s": 623.0}, {"text": "writing into them into a styled fashion. And that reduces a lot of,", "timestamp": "00:10:26,872", "timestamp_s": 626.0}, {"text": "the lot of the overhead. So I actually performed", "timestamp": "00:10:30,732", "timestamp_s": 630.0}, {"text": "several calculations and I ran several networks", "timestamp": "00:10:34,098", "timestamp_s": 634.0}, {"text": "as shown. These are three networks that I tested with using tensorrt", "timestamp": "00:10:38,002", "timestamp_s": 638.0}, {"text": "on my 1050 Ti GPU. Sorry, not 1050 Ti", "timestamp": "00:10:42,134", "timestamp_s": 642.0}, {"text": "1650 Ti. You see the number of layers before", "timestamp": "00:10:45,814", "timestamp_s": 645.0}, {"text": "fusion and number of layers after fusion,", "timestamp": "00:10:49,760", "timestamp_s": 649.0}, {"text": "and those are quite diminished and those are quite less.", "timestamp": "00:10:52,234", "timestamp_s": 652.0}, {"text": "That is definitely a lot of less calls.", "timestamp": "00:10:56,210", "timestamp_s": 656.0}, {"text": "So how does tensorrt workers like, how do you make it work?", "timestamp": "00:10:59,738", "timestamp_s": 659.0}, {"text": "So suppose we are using a pytorch based model.", "timestamp": "00:11:03,252", "timestamp_s": 663.0}, {"text": "What you simply have to do is connect the pytos based", "timestamp": "00:11:06,568", "timestamp_s": 666.0}, {"text": "model into onnx and import the onnx into tensorrt.", "timestamp": "00:11:09,688", "timestamp_s": 669.0}, {"text": "You don\u0027t have to select anything else. Tensorrt will automatically generate these applications", "timestamp": "00:11:13,758", "timestamp_s": 673.0}, {"text": "and generate an engine. You can then use that engine to perform inference", "timestamp": "00:11:17,758", "timestamp_s": 677.0}, {"text": "on the GPU. This is these similar process even for", "timestamp": "00:11:21,474", "timestamp_s": 681.0}, {"text": "a tensorflow based model or an Mxnate based model.", "timestamp": "00:11:25,068", "timestamp_s": 685.0}, {"text": "Another way by which you can convert to tensorrt is by using", "timestamp": "00:11:28,284", "timestamp_s": 688.0}, {"text": "the network definition API provided with C and Python.", "timestamp": "00:11:32,432", "timestamp_s": 692.0}, {"text": "It does give you a benefit like you", "timestamp": "00:11:36,190", "timestamp_s": 696.0}, {"text": "do get better accuracy and a better speed. Those are", "timestamp": "00:11:39,840", "timestamp_s": 699.0}, {"text": "marginally better in some cases and exceptionally better in some others.", "timestamp": "00:11:43,828", "timestamp_s": 703.0}, {"text": "But you can try out both methods and the easiest one is to", "timestamp": "00:11:47,810", "timestamp_s": 707.0}, {"text": "directly use on Nx parser that is provided with Tensorrt. These are some", "timestamp": "00:11:51,012", "timestamp_s": 711.0}, {"text": "of the metrics that I ran with another gpu", "timestamp": "00:11:55,028", "timestamp_s": 715.0}, {"text": "that is on JTX 1080 and I used only", "timestamp": "00:11:59,022", "timestamp_s": 719.0}, {"text": "for one batch size and I used the Retina phase Resnet 50 based", "timestamp": "00:12:02,408", "timestamp_s": 722.0}, {"text": "model as well as the mobile net zero point 25.", "timestamp": "00:12:06,072", "timestamp_s": 726.0}, {"text": "So here you can see with FP 32 and an input shape of 640", "timestamp": "00:12:09,384", "timestamp_s": 729.0}, {"text": "by 480 I got an FPS of 81", "timestamp": "00:12:13,212", "timestamp_s": 733.0}, {"text": "and with int eight based model I got can FPS of 190.", "timestamp": "00:12:17,548", "timestamp_s": 737.0}, {"text": "So these are better than real time and you can basically use multistream", "timestamp": "00:12:21,984", "timestamp_s": 741.0}, {"text": "now with these models. And believe me, Retina phase won\u0027t do", "timestamp": "00:12:25,718", "timestamp_s": 745.0}, {"text": "this default by default on Pytorch or tensorflow. And the", "timestamp": "00:12:29,328", "timestamp_s": 749.0}, {"text": "Retina phase mobile net based model, even with a float 32 quantized", "timestamp": "00:12:32,768", "timestamp_s": 752.0}, {"text": "state, it gave me an FPS of 400. And if you", "timestamp": "00:12:37,130", "timestamp_s": 757.0}, {"text": "look at the object detection model Yolo V five, so you can", "timestamp": "00:12:40,948", "timestamp_s": 760.0}, {"text": "see the FPS metrics on the right and they are super awesome. And you", "timestamp": "00:12:44,404", "timestamp_s": 764.0}, {"text": "can basically even use Yolo V five large on a GTX 1080 for", "timestamp": "00:12:47,768", "timestamp_s": 767.0}, {"text": "doing real time processing.", "timestamp": "00:12:51,672", "timestamp_s": 771.0}, {"text": "What are the best practices to deploy the model?", "timestamp": "00:12:55,190", "timestamp_s": 775.0}, {"text": "Basically use multiple quantization methods. Try those out. Do not", "timestamp": "00:12:58,492", "timestamp_s": 778.0}, {"text": "discard intake as easily as possible. Do not build an engine", "timestamp": "00:13:02,092", "timestamp_s": 782.0}, {"text": "for each inference as that is an overhead. Save the model, serialize the model", "timestamp": "00:13:05,980", "timestamp_s": 785.0}, {"text": "on the disk and then reuse it for your inference. Do try out", "timestamp": "00:13:09,788", "timestamp_s": 789.0}, {"text": "different workspace sizes because that would reduce your memory", "timestamp": "00:13:13,472", "timestamp_s": 793.0}, {"text": "things to keep in mind while using tensorrt. So engines", "timestamp": "00:13:17,366", "timestamp_s": 797.0}, {"text": "generated are specific to the machine. Installation takes time without Docker.", "timestamp": "00:13:21,350", "timestamp_s": 801.0}, {"text": "I often go with Docker whenever I\u0027m installing Tensorrt because that\u0027s", "timestamp": "00:13:25,322", "timestamp_s": 805.0}, {"text": "quite easier. And there are multiple APIs for conversion.", "timestamp": "00:13:28,698", "timestamp_s": 808.0}, {"text": "That is Onnx passer, UFf passer network definition API", "timestamp": "00:13:32,154", "timestamp_s": 812.0}, {"text": "provided in C network definition API provided in Python.", "timestamp": "00:13:36,222", "timestamp_s": 816.0}, {"text": "So that was all that I wanted to talk about of Tensorrt.", "timestamp": "00:13:40,150", "timestamp_s": 820.0}, {"text": "It is a very broad topic and I would recommend you guys to go check", "timestamp": "00:13:43,918", "timestamp_s": 823.0}, {"text": "it out. Moving forward, we will be talking about again,", "timestamp": "00:13:47,148", "timestamp_s": 827.0}, {"text": "it involves Tensorrt, but it is a pipeline", "timestamp": "00:13:51,450", "timestamp_s": 831.0}, {"text": "provided by Nvidia specifically for deep learning solutions", "timestamp": "00:13:55,042", "timestamp_s": 835.0}, {"text": "and deploying ML solutions. So it is a multi scaled framework,", "timestamp": "00:13:59,314", "timestamp_s": 839.0}, {"text": "multiplatform, scalable framework with TLS security.", "timestamp": "00:14:03,462", "timestamp_s": 843.0}, {"text": "It can deploy on edge as well as on any cloud. It supports both", "timestamp": "00:14:06,750", "timestamp_s": 846.0}, {"text": "Python and C, and it uses GStreamer.", "timestamp": "00:14:10,272", "timestamp_s": 850.0}, {"text": "But Nvidia has custom developed the G Streamer objects", "timestamp": "00:14:13,718", "timestamp_s": 853.0}, {"text": "for the GPU, so that you often get low overhead in", "timestamp": "00:14:17,514", "timestamp_s": 857.0}, {"text": "the pre processing and post processing steps. So in Tensorrt", "timestamp": "00:14:21,188", "timestamp_s": 861.0}, {"text": "we saw that our target was the model inference phase,", "timestamp": "00:14:25,178", "timestamp_s": 865.0}, {"text": "which was a bottleneck. But after converting an engine to model inference,", "timestamp": "00:14:28,878", "timestamp_s": 868.0}, {"text": "even if we want to get more speed, we can use Deepstream, as it", "timestamp": "00:14:33,038", "timestamp_s": 873.0}, {"text": "will optimizer the entire pipeline. So the applications and", "timestamp": "00:14:36,828", "timestamp_s": 876.0}, {"text": "services of Deepstream are as shown below. You can use Python or", "timestamp": "00:14:39,948", "timestamp_s": 879.0}, {"text": "C Plus Plus. Deepstream SDK provides hardware", "timestamp": "00:14:43,548", "timestamp_s": 883.0}, {"text": "accelerated plugins, bi directional IoT messaging, Otmo model,", "timestamp": "00:14:47,122", "timestamp_s": 887.0}, {"text": "update, reference application, and helm charts.", "timestamp": "00:14:50,896", "timestamp_s": 890.0}, {"text": "Below that there is a CUDA layer which is used to deploy", "timestamp": "00:14:54,454", "timestamp_s": 894.0}, {"text": "your models, and you can use any of the Nvidia computing platforms", "timestamp": "00:14:57,958", "timestamp_s": 897.0}, {"text": "as shown here. So what is the process of a Deepstream pipeline?", "timestamp": "00:15:01,958", "timestamp_s": 901.0}, {"text": "So the first process is capturing your stream. It could be a", "timestamp": "00:15:05,258", "timestamp_s": 905.0}, {"text": "raw Deepstream RTSP stream, HTTP stream, or a video", "timestamp": "00:15:08,724", "timestamp_s": 908.0}, {"text": "recorded sent to a disk.", "timestamp": "00:15:12,116", "timestamp_s": 912.0}, {"text": "It is generally read using cpu, it\u0027s not read using GPU", "timestamp": "00:15:15,930", "timestamp_s": 915.0}, {"text": "right now. After that you have to often decode", "timestamp": "00:15:19,598", "timestamp_s": 919.0}, {"text": "the steam because it can be in multiple formats, and that decoding is actually done", "timestamp": "00:15:22,718", "timestamp_s": 922.0}, {"text": "on the GPU, so it\u0027s quite faster than on CPU, as you can", "timestamp": "00:15:26,648", "timestamp_s": 926.0}, {"text": "imagine. After that, Deepstream does image processing.", "timestamp": "00:15:30,108", "timestamp_s": 930.0}, {"text": "That is, in case you want any preprocessing steps such as scaling,", "timestamp": "00:15:33,778", "timestamp_s": 933.0}, {"text": "dwarping, cropping, et cetera. And all these steps are done on the", "timestamp": "00:15:37,746", "timestamp_s": 937.0}, {"text": "GPU. And with Deepstream you get automatic batching,", "timestamp": "00:15:40,944", "timestamp_s": 940.0}, {"text": "so you don\u0027t have to worry about batching together and then sending", "timestamp": "00:15:44,838", "timestamp_s": 944.0}, {"text": "it renders, like rewriting your pipeline to send it to the model.", "timestamp": "00:15:48,438", "timestamp_s": 948.0}, {"text": "Deepstream does this job on its own, while this job is done on", "timestamp": "00:15:52,160", "timestamp_s": 952.0}, {"text": "the cpu. After that you have several classifiers or", "timestamp": "00:15:55,844", "timestamp_s": 955.0}, {"text": "detect layers or segmentation layers that are on Tensorrt", "timestamp": "00:15:59,508", "timestamp_s": 959.0}, {"text": "or on the Triton inferencing server. Deepstream also", "timestamp": "00:16:03,242", "timestamp_s": 963.0}, {"text": "provides you with an option of by default, tracking.", "timestamp": "00:16:06,712", "timestamp_s": 966.0}, {"text": "It is done on both GPU and CPU. You can use that tracker,", "timestamp": "00:16:10,382", "timestamp_s": 970.0}, {"text": "and it\u0027s quite easy as it\u0027s already custom built in Deepstream.", "timestamp": "00:16:14,766", "timestamp_s": 974.0}, {"text": "After that you can do two things. Either you can visualize your results", "timestamp": "00:16:18,542", "timestamp_s": 978.0}, {"text": "on an on screen display and that conversion is done on the GPU,", "timestamp": "00:16:22,562", "timestamp_s": 982.0}, {"text": "or else you can store it to the cloud or send it to a disk.", "timestamp": "00:16:26,482", "timestamp_s": 986.0}, {"text": "And that can be done on an HDMI", "timestamp": "00:16:30,710", "timestamp_s": 990.0}, {"text": "cable, SATA cable, or using the Nvenc plugin. So these", "timestamp": "00:16:34,438", "timestamp_s": 994.0}, {"text": "are some of the models that Nvidia provides. These have", "timestamp": "00:16:38,352", "timestamp_s": 998.0}, {"text": "custom built all these for deep steam. So people, the use cases", "timestamp": "00:16:41,488", "timestamp_s": 1001.0}, {"text": "are pretty specific with the model names and you", "timestamp": "00:16:45,338", "timestamp_s": 1005.0}, {"text": "can see that you get an FPS of 1100", "timestamp": "00:16:49,028", "timestamp_s": 1009.0}, {"text": "on a t four inference server and you get a real time FPS on", "timestamp": "00:16:52,452", "timestamp_s": 1012.0}, {"text": "Jetson. Xavier this is quite awesome and", "timestamp": "00:16:56,472", "timestamp_s": 1016.0}, {"text": "you can basically use a these detection on Jetson Nano with a 95 FPS.", "timestamp": "00:17:00,488", "timestamp_s": 1020.0}, {"text": "And if you practically use model pruning and several", "timestamp": "00:17:04,430", "timestamp_s": 1024.0}, {"text": "other steps that we discussed before, you can get even better", "timestamp": "00:17:08,162", "timestamp_s": 1028.0}, {"text": "FPS on the models.", "timestamp": "00:17:11,756", "timestamp_s": 1031.0}, {"text": "These are several other resources on Tensorrt and these are several", "timestamp": "00:17:14,970", "timestamp_s": 1034.0}, {"text": "resources on the Deepstream. The slide deck is provided", "timestamp": "00:17:18,658", "timestamp_s": 1038.0}, {"text": "to you and I hope you guys will check this out using these two", "timestamp": "00:17:22,514", "timestamp_s": 1042.0}, {"text": "resources. Thank you and I\u0027m glad to be", "timestamp": "00:17:25,948", "timestamp_s": 1045.0}, {"text": "present here. Here the code for retina these tensorrt conversion is", "timestamp": "00:17:29,516", "timestamp_s": 1049.0}, {"text": "shown on the GitHub link. Please go and see how you can convert", "timestamp": "00:17:33,732", "timestamp_s": 1053.0}, {"text": "your retina these based model to Tensorrt and deploy", "timestamp": "00:17:37,498", "timestamp_s": 1057.0}, {"text": "it on any machine. Thank you.", "timestamp": "00:17:40,618", "timestamp_s": 1060.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'ovLKssNavS4',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Deploying ML solutions with low latency in Python
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>When we aim for better accuracies, sometimes we forget that the algorithms become more massive and slower. This fact renders the algorithms unusable in real-time scenarios. How do you deploy your solution? Which framework to use? Can you use Python for deploying my solution? Can you use Jetson Nano for multi-stream inferencing? If you are curious to solve these questions, join me in this talk to discover TensorRT and DeepStream and how they reduce your algorithm&rsquo;s latency and memory footprint. </p>
<p>NVIDIA TensorRTâ„¢ is an SDK for high-performance deep learning inference. It includes a deep learning inference optimizer and runtime that delivers low latency and high-throughput for deep learning inference applications. DeepStream offers a multi-platform scalable framework with TLS security to deploy on edge and connect to any cloud. If you are using a GPU and CUDA/Tensor cores, you can leverage the SDK framework to deploy bigger and better algorithms for your real-time scenarios. 
The main focus of this talk will be to demonstrate why, where, and how to use TensorRT and DeepStream. </p>
<!-- End Text -->
          </div>

          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
              <li>
                Today we will talk about deploying ML solutions with low latency in python in. Our focus will be to convert or deploy our ML pipelines in low latency. The bottleneck we face most of the time is the model inference. There are several methods by which we can improve the latency of the algorithms.

              </li>
              
              <li>
                Tensorrt is SDK for high performance deep learning inference. It is provided by Nvidia and they have written the entire tensorrt in C with Python bindings available. Tensorrt often delivers low latency and high throughput for several deep learning applications. And we'll be talking about more of these methods in further slides.

              </li>
              
              <li>
                Tensorrt must perform calibration to determine how best to represent the weights and activations as eight bit integers. Calibration is a completely automated and parameter free method for converting your model from f float 32 to NTA eight. This ensures that the developed model is highperformance tuned for the specific development platform.

              </li>
              
              <li>
                Tensorrt can process multiple input streams in parallel. It fuses several layers or tensors together. This avoids memory allocation overhead for fast and efficient execution. Here are some of the metrics that I ran using tensorrt.

              </li>
              
              <li>
                Deepstream is a pipeline provided by Nvidia specifically for deep learning solutions and deploying ML solutions. It is a multi scaled framework, multiplatform, scalable framework with TLS security. Can deploy on edge as well as on any cloud.
              </li>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./assemblyai/ovLKssNavS4.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:22,010'); seek(22.0)">
              Today we will talk about deploying ML solutions with low latency in python in.
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:25,932'); seek(25.0)">
              You can find me at my LinkedIn address mentioned here.
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:29,130'); seek(29.0)">
              So, as the world moves forward with these research in improving accuracies of
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:32,828'); seek(32.0)">
              deep learning algorithms, we face an imminent problem of deploying
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:36,338'); seek(36.0)">
              those algorithms. So, as many of my fellow
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:39,462'); seek(39.0)">
              researchers might know, everyone here present is involved
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:43,622'); seek(43.0)">
              in, directly or indirectly, in improving the algorithms that we currently
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:47,680'); seek(47.0)">
              use today. However, these developers that are
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:51,472'); seek(51.0)">
              using those algorithms often face the problem of making those algorithms
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:55,322'); seek(55.0)">
              real time, even after doing a bunch of stuff that we'll be
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:00:58,868'); seek(58.0)">
              discussing today. So what do I mean by low
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:01:02,532'); seek(62.0)">
              latency? So, it is a term used
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:01:06,392'); seek(66.0)">
              to describe the performance of can ML pipeline,
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:01:09,358'); seek(69.0)">
              and it's basically the time taken to process data
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:01:13,848'); seek(73.0)">
              by the ML algorithms. It's like a
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:01:17,548'); seek(77.0)">
              time taken to process a single image or a single video,
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:20,892'); seek(80.0)">
              and highthroughput is the time taken to process these entire data,
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:24,970'); seek(84.0)">
              and latency is inversely proportional to throughput. And our
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:28,348'); seek(88.0)">
              focus in this talk will be to convert
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:31,622'); seek(91.0)">
              or deploy our ML pipelines in low latency. So as you can see
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:35,072'); seek(95.0)">
              in this graph, entire ML pipeline is it starts with the
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:38,928'); seek(98.0)">
              input data and it goes through several steps before we can get the output.
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:42,774'); seek(102.0)">
              And these. The bottleneck we face most of the
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:46,468'); seek(106.0)">
              time is the model inference. And there are several methods by which we
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:50,132'); seek(110.0)">
              can improve the latency of the algorithms. First, one is weight quantization,
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:54,378'); seek(114.0)">
              wherein you can deploy the model
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:01:57,848'); seek(117.0)">
              in multiple quantization methods using multiple quantization
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:02:01,486'); seek(121.0)">
              methods such as float 16 int eight. There are
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:02:05,048'); seek(125.0)">
              two types of quantizations. One is post training quantization,
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:02:08,526'); seek(128.0)">
              and one is quantization aware training. I cannot go in detail in
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:02:11,868'); seek(131.0)">
              each of these methods as the time won't permit me to do so. But basically
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:02:15,964'); seek(135.0)">
              you can use any of those to check which method
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:02:19,874'); seek(139.0)">
              is giving you better accuracies. And quantization often involves
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:23,862'); seek(143.0)">
              you to deal with accuracies and speed versus speed.
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:27,270'); seek(147.0)">
              So in case you go with int eight, you will get much better speed.
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:30,342'); seek(150.0)">
              However, there will be a drop in accuracy, and I generally
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:33,718'); seek(153.0)">
              find that float 16 is the best method to go because it preserves
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:37,418'); seek(157.0)">
              my accuracy and it also gives me like 1.5
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:41,092'); seek(161.0)">
              or two times of what float 32 gives me. The second method that
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:45,670'); seek(165.0)">
              is often used is model pruning. So the basic concept
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:49,118'); seek(169.0)">
              is that you prune certain layers and connections of the models
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:02:52,542'); seek(172.0)">
              based on several experiments. You run your
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:02:56,152'); seek(176.0)">
              model with multiple and like a bunch of images
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:02:59,602'); seek(179.0)">
              to see which layers can be omitted
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:03:03,394'); seek(183.0)">
              and can be skipped so that these parameters won't be
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:03:06,972'); seek(186.0)">
              calculated by this way. Sometimes you can remove like
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:10,652'); seek(190.0)">
              99% of your connections and you can still preserve your accuracy,
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:14,182'); seek(194.0)">
              but that is like the best case solution. Another method that is quite popular
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:17,702'); seek(197.0)">
              today is knowledge distillation, and it is the concept wherein you are transferring
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:21,638'); seek(201.0)">
              the knowledge of a bigger model into a smaller model. So suppose
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:25,258'); seek(205.0)">
              you have a Stanford car data set and you have trained a Resnet 151 and
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:29,476'); seek(209.0)">
              you get a testing classification accuracy of 95%.
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:33,396'); seek(213.0)">
              Now when you train a REsnet 18 on the similar data on the same data
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:36,696'); seek(216.0)">
              set, you might not find that it is giving you a 95% accuracy.
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:40,606'); seek(220.0)">
              You often see that it is limited to 80% to 90% because
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:44,312'); seek(224.0)">
              of the shallow depth of it. So with using
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:47,612'); seek(227.0)">
              knowledge distillation, what you can do is you can transfer the knowledge learned by
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:03:51,436'); seek(231.0)">
              Resnet 151 and
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:03:55,164'); seek(235.0)">
              use Resnet 18 as if the like use resonate
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:03:58,978'); seek(238.0)">
              18 with 95% of accuracies. And the fourth method
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:04:02,582'); seek(242.0)">
              that is my topic of the talk today, that is framework based deployment.
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:04:06,694'); seek(246.0)">
              So we will be seeing two frameworks, Tensorrt and deep
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:10,870'); seek(250.0)">
              steam, and we'll be seeing how they help us in deploying these algorithms.
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:15,178'); seek(255.0)">
              So what is Tensorrt? Tensorrt is
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:19,410'); seek(259.0)">
              SDK for high performance deep learning inference. It is provided
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:23,114'); seek(263.0)">
              by Nvidia and they have written the entire tensorrt in C
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:27,496'); seek(267.0)">
              with Python bindings available. It includes a deep learning inference
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:31,278'); seek(271.0)">
              optimizer and runtime. So the optimizer's job is to optimize
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:35,278'); seek(275.0)">
              your model, like when you are converting it to tensorrt, it will optimize
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:39,282'); seek(279.0)">
              the entire model and it will convert the layers using advanced
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:43,698'); seek(283.0)">
              CUDA methods in C. And the runtime is responsible
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:47,522'); seek(287.0)">
              for actually running your tensorrt engines. So Tensorrt often
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:04:52,048'); seek(292.0)">
              delivers low latency and high throughput for several deep learning applications.
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:04:56,102'); seek(296.0)">
              I have tried Tensorrt in the industry and it works great.
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:04:59,712'); seek(299.0)">
              It supports both Python and C. And nowadays
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:05:03,722'); seek(303.0)">
              Tensorrt supports conversion from multiple frameworks such as
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:05:07,220'); seek(307.0)">
              Tensorflow, Pytorch, Mixnet, Theano,
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:05:09,946'); seek(309.0)">
              Onnx, etc. For reference, I have
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:05:13,172'); seek(313.0)">
              linked the tensorrt's official documentation and developer page
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:17,124'); seek(317.0)">
              below. So how does Tensorrt do the
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:20,840'); seek(320.0)">
              entire thing? So it is responsible for optimizing
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:24,718'); seek(324.0)">
              your model, and it does so using the method shown here.
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:28,588'); seek(328.0)">
              What it does is it does layer in tensorfusion,
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:32,946'); seek(332.0)">
              it does kernel auto tuning, it does precision calibration, it does
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:36,492'); seek(336.0)">
              dynamic tensor memory, it uses dynamic tensor memory,
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:39,634'); seek(339.0)">
              and it is also possible to use multi steam execution with Tensorrt.
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:43,202'); seek(343.0)">
              That is, you won't have to worry about like you can actually use batch processing
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:47,238'); seek(347.0)">
              for this using Tensorrt. And we'll be talking about more of these
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:05:50,996'); seek(350.0)">
              methods in further slides. So let's
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:05:55,034'); seek(355.0)">
              talk about weight and activation precision calibration. So,
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:05:58,612'); seek(358.0)">
              to quantize full precision information into intate while
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:06:01,908'); seek(361.0)">
              minimizing accuracies loss, Tensorrt must perform a process called
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:06:05,512'); seek(365.0)">
              calibration to determine how best to represent the weights
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:06:08,974'); seek(368.0)">
              and activations as eight bit integers. These calibration step
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:12,456'); seek(372.0)">
              requires you to provide tensorrt with a representative sample of the input
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:15,902'); seek(375.0)">
              training data, and no additional fine tuning or restraining of the model is
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:19,612'); seek(379.0)">
              necessary. Also, you don't need to have access to the entire training data
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:23,052'); seek(383.0)">
              set, you just give it a sample. Calibration is a
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:06:26,892'); seek(386.0)">
              completely automated and parameter free method for converting your model from
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:06:30,784'); seek(390.0)">
              f float 32 to NTA eight. What is kernel autotuning?
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:06:34,534'); seek(394.0)">
              So, during the optimization phase of Tensorrt,
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:06:37,702'); seek(397.0)">
              it also chooses from hundreds of specialized kernels that are created by default,
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:41,942'); seek(401.0)">
              and many of them are hand tuned and optimized for a range of parameters
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:06:45,402'); seek(405.0)">
              and target platforms. So as an example, there are several different algorithms
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:06:48,842'); seek(408.0)">
              to do convolutions. Tensorrt will pick the implementation from
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:06:52,948'); seek(412.0)">
              a library of kernels that delivers the best performance for the target gpu,
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:06:56,750'); seek(416.0)">
              input data size, filter size, tensorrt layout, batch size and other parameters.
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:07:01,006'); seek(421.0)">
              This ensures that the developed model is highperformance tuned for the specific
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:07:04,984'); seek(424.0)">
              development platform as well as for the specific neural network being deploying.
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:07:08,798'); seek(428.0)">
              So also, TensorRt is supposed to convert
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:12,418'); seek(432.0)">
              your models on a particular development platform. So you
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:07:15,948'); seek(435.0)">
              cannot run a TensorRt engine converted on suppose
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:07:20,466'); seek(440.0)">
              NVDiA's 1050 Ti and use it on a 2060 Ti.
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:07:24,838'); seek(444.0)">
              You have to do it on that particular GPU to be used on that particular
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:07:28,176'); seek(448.0)">
              GPU. So what is dynamic tensor memory?
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:07:31,574'); seek(451.0)">
              Tensorrt also reduces memory footprint and improves
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:07:35,238'); seek(455.0)">
              memory you reuse by designating memory for each tensor
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:07:39,242'); seek(459.0)">
              only for the duration of its use, avoiding memory allocation overhead
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:07:43,082'); seek(463.0)">
              for fast and efficient execution. So what
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:07:46,168'); seek(466.0)">
              is multi stream execution? So, as I mentioned before, it is basically the ability of
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:07:49,928'); seek(469.0)">
              tensorrt to process multiple input streams in parallel,
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:07:53,310'); seek(473.0)">
              and it does so beautifully.
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:07:56,862'); seek(476.0)">
              And what is layer and tensorfusion?
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:08:00,366'); seek(480.0)">
              So, tensorrt calibrates, while in
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:08:04,364'); seek(484.0)">
              the optimization step it calibrates your entire model,
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:08:07,516'); seek(487.0)">
              it sees the entire model, and it basically fuses
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:08:10,998'); seek(490.0)">
              several layers or tensors together, so that you
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:14,416'); seek(494.0)">
              reduce the parameter calculation and you reduce these
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:08:17,792'); seek(497.0)">
              multiple times. The data has to be passed from one layer to another. It is
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:08:21,472'); seek(501.0)">
              a single block. So suppose you have a convolution layer,
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:08:25,434'); seek(505.0)">
              activation function layer, and a fully connected layer in a
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:08:28,708'); seek(508.0)">
              network. What tensorrt it will do is it will combine all those three into
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:08:32,148'); seek(512.0)">
              a single module. So basically it reduces the
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:08:35,368'); seek(515.0)">
              time to traverse the data, and it
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:08:38,568'); seek(518.0)">
              also reduces some of the overhead that is incurred
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:08:42,638'); seek(522.0)">
              by each layer calls. So here
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:08:45,996'); seek(525.0)">
              you can see the difference between the optimizer network and
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:08:49,980'); seek(529.0)">
              the tensor RT optimized network. This is an example of Google's
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:08:54,178'); seek(534.0)">
              Leannet architecture, which won the imagenet competition in 2014.
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:08:58,854'); seek(538.0)">
              Sorry, it's called Google Net. So as
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:09:03,088'); seek(543.0)">
              shown here, after layer intensive fusion, what happens
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:09:06,800'); seek(546.0)">
              is you can see on the left side you have multiple layers,
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:09:10,582'); seek(550.0)">
              whereas on the right side you have quite a few amount, number of layers.
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:09:15,090'); seek(555.0)">
              A deep learning framework, what it does is it does multiple function calls
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:09:18,730'); seek(558.0)">
              for calling each layer, and as each layer is
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:09:22,228'); seek(562.0)">
              on the GPU, it translates to multiple CudA kernel launches.
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:09:26,026'); seek(566.0)">
              The kernel computation is often very fast relative to the kernel
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:09:29,502'); seek(569.0)">
              launch overhead and the cost of reading and writing the tensor
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:09:32,574'); seek(572.0)">
              data for each layer. This results in the memory bandwidth bottleneck and underutilization
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:09:37,118'); seek(577.0)">
              of the available GPU resources. And Tensorrt addresses this by vertically
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:09:41,346'); seek(581.0)">
              fusing kernels to perform the sequential operations. Together,
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:09:44,332'); seek(584.0)">
              these layer fusion reduces kernel launches and avoids writing into
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:09:47,836'); seek(587.0)">
              and reading from memory between layers. So in the
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:09:51,344'); seek(591.0)">
              figure shown, the convolution bias and relu layers of various sizes can
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:09:55,168'); seek(595.0)">
              be combined into a single layer called as CBR.
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:09:58,670'); seek(598.0)">
              And a simple analogy is making three separate
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:10:01,798'); seek(601.0)">
              trips to the supermarket to buy three items versus buying all the
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:10:05,284'); seek(605.0)">
              three in a single trip. And Tensorrt also recognizes
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:10:09,098'); seek(609.0)">
              layers that share the same input data and filter size, but have different
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:10:12,292'); seek(612.0)">
              weights. Instead of three separate kernels, tensorrt fuses them
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:10:15,688'); seek(615.0)">
              horizontally into a single wider kernel, as shown
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:10:19,358'); seek(619.0)">
              as one into one CBR. And tensorrt also eliminates the
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:10:23,430'); seek(623.0)">
              concatenation layer by preallocating output buffers and
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:10:26,872'); seek(626.0)">
              writing into them into a styled fashion. And that reduces a lot of,
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:10:30,732'); seek(630.0)">
              the lot of the overhead. So I actually performed
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:10:34,098'); seek(634.0)">
              several calculations and I ran several networks
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:10:38,002'); seek(638.0)">
              as shown. These are three networks that I tested with using tensorrt
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:10:42,134'); seek(642.0)">
              on my 1050 Ti GPU. Sorry, not 1050 Ti
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:10:45,814'); seek(645.0)">
              1650 Ti. You see the number of layers before
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:10:49,760'); seek(649.0)">
              fusion and number of layers after fusion,
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:10:52,234'); seek(652.0)">
              and those are quite diminished and those are quite less.
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:10:56,210'); seek(656.0)">
              That is definitely a lot of less calls.
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:10:59,738'); seek(659.0)">
              So how does tensorrt workers like, how do you make it work?
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:11:03,252'); seek(663.0)">
              So suppose we are using a pytorch based model.
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:11:06,568'); seek(666.0)">
              What you simply have to do is connect the pytos based
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:11:09,688'); seek(669.0)">
              model into onnx and import the onnx into tensorrt.
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:11:13,758'); seek(673.0)">
              You don't have to select anything else. Tensorrt will automatically generate these applications
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:11:17,758'); seek(677.0)">
              and generate an engine. You can then use that engine to perform inference
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:11:21,474'); seek(681.0)">
              on the GPU. This is these similar process even for
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:11:25,068'); seek(685.0)">
              a tensorflow based model or an Mxnate based model.
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:11:28,284'); seek(688.0)">
              Another way by which you can convert to tensorrt is by using
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:11:32,432'); seek(692.0)">
              the network definition API provided with C and Python.
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:11:36,190'); seek(696.0)">
              It does give you a benefit like you
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:11:39,840'); seek(699.0)">
              do get better accuracy and a better speed. Those are
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:11:43,828'); seek(703.0)">
              marginally better in some cases and exceptionally better in some others.
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:11:47,810'); seek(707.0)">
              But you can try out both methods and the easiest one is to
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:11:51,012'); seek(711.0)">
              directly use on Nx parser that is provided with Tensorrt. These are some
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:11:55,028'); seek(715.0)">
              of the metrics that I ran with another gpu
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:11:59,022'); seek(719.0)">
              that is on JTX 1080 and I used only
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:12:02,408'); seek(722.0)">
              for one batch size and I used the Retina phase Resnet 50 based
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:12:06,072'); seek(726.0)">
              model as well as the mobile net zero point 25.
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:12:09,384'); seek(729.0)">
              So here you can see with FP 32 and an input shape of 640
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:12:13,212'); seek(733.0)">
              by 480 I got an FPS of 81
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:12:17,548'); seek(737.0)">
              and with int eight based model I got can FPS of 190.
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:12:21,984'); seek(741.0)">
              So these are better than real time and you can basically use multistream
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:12:25,718'); seek(745.0)">
              now with these models. And believe me, Retina phase won't do
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:12:29,328'); seek(749.0)">
              this default by default on Pytorch or tensorflow. And the
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:12:32,768'); seek(752.0)">
              Retina phase mobile net based model, even with a float 32 quantized
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:12:37,130'); seek(757.0)">
              state, it gave me an FPS of 400. And if you
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:12:40,948'); seek(760.0)">
              look at the object detection model Yolo V five, so you can
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:12:44,404'); seek(764.0)">
              see the FPS metrics on the right and they are super awesome. And you
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:12:47,768'); seek(767.0)">
              can basically even use Yolo V five large on a GTX 1080 for
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:12:51,672'); seek(771.0)">
              doing real time processing.
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:12:55,190'); seek(775.0)">
              What are the best practices to deploy the model?
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:12:58,492'); seek(778.0)">
              Basically use multiple quantization methods. Try those out. Do not
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:13:02,092'); seek(782.0)">
              discard intake as easily as possible. Do not build an engine
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:13:05,980'); seek(785.0)">
              for each inference as that is an overhead. Save the model, serialize the model
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:13:09,788'); seek(789.0)">
              on the disk and then reuse it for your inference. Do try out
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:13:13,472'); seek(793.0)">
              different workspace sizes because that would reduce your memory
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:13:17,366'); seek(797.0)">
              things to keep in mind while using tensorrt. So engines
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:13:21,350'); seek(801.0)">
              generated are specific to the machine. Installation takes time without Docker.
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:13:25,322'); seek(805.0)">
              I often go with Docker whenever I'm installing Tensorrt because that's
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:13:28,698'); seek(808.0)">
              quite easier. And there are multiple APIs for conversion.
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:13:32,154'); seek(812.0)">
              That is Onnx passer, UFf passer network definition API
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:13:36,222'); seek(816.0)">
              provided in C network definition API provided in Python.
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:13:40,150'); seek(820.0)">
              So that was all that I wanted to talk about of Tensorrt.
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:13:43,918'); seek(823.0)">
              It is a very broad topic and I would recommend you guys to go check
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:13:47,148'); seek(827.0)">
              it out. Moving forward, we will be talking about again,
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:13:51,450'); seek(831.0)">
              it involves Tensorrt, but it is a pipeline
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:13:55,042'); seek(835.0)">
              provided by Nvidia specifically for deep learning solutions
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:13:59,314'); seek(839.0)">
              and deploying ML solutions. So it is a multi scaled framework,
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:14:03,462'); seek(843.0)">
              multiplatform, scalable framework with TLS security.
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:14:06,750'); seek(846.0)">
              It can deploy on edge as well as on any cloud. It supports both
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:14:10,272'); seek(850.0)">
              Python and C, and it uses GStreamer.
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:14:13,718'); seek(853.0)">
              But Nvidia has custom developed the G Streamer objects
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:14:17,514'); seek(857.0)">
              for the GPU, so that you often get low overhead in
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:14:21,188'); seek(861.0)">
              the pre processing and post processing steps. So in Tensorrt
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:14:25,178'); seek(865.0)">
              we saw that our target was the model inference phase,
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:14:28,878'); seek(868.0)">
              which was a bottleneck. But after converting an engine to model inference,
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:14:33,038'); seek(873.0)">
              even if we want to get more speed, we can use Deepstream, as it
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:14:36,828'); seek(876.0)">
              will optimizer the entire pipeline. So the applications and
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:14:39,948'); seek(879.0)">
              services of Deepstream are as shown below. You can use Python or
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:14:43,548'); seek(883.0)">
              C Plus Plus. Deepstream SDK provides hardware
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:14:47,122'); seek(887.0)">
              accelerated plugins, bi directional IoT messaging, Otmo model,
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:14:50,896'); seek(890.0)">
              update, reference application, and helm charts.
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:14:54,454'); seek(894.0)">
              Below that there is a CUDA layer which is used to deploy
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:14:57,958'); seek(897.0)">
              your models, and you can use any of the Nvidia computing platforms
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:15:01,958'); seek(901.0)">
              as shown here. So what is the process of a Deepstream pipeline?
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:15:05,258'); seek(905.0)">
              So the first process is capturing your stream. It could be a
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:15:08,724'); seek(908.0)">
              raw Deepstream RTSP stream, HTTP stream, or a video
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:15:12,116'); seek(912.0)">
              recorded sent to a disk.
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:15:15,930'); seek(915.0)">
              It is generally read using cpu, it's not read using GPU
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:15:19,598'); seek(919.0)">
              right now. After that you have to often decode
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:15:22,718'); seek(922.0)">
              the steam because it can be in multiple formats, and that decoding is actually done
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:15:26,648'); seek(926.0)">
              on the GPU, so it's quite faster than on CPU, as you can
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:15:30,108'); seek(930.0)">
              imagine. After that, Deepstream does image processing.
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:15:33,778'); seek(933.0)">
              That is, in case you want any preprocessing steps such as scaling,
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:15:37,746'); seek(937.0)">
              dwarping, cropping, et cetera. And all these steps are done on the
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:15:40,944'); seek(940.0)">
              GPU. And with Deepstream you get automatic batching,
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:15:44,838'); seek(944.0)">
              so you don't have to worry about batching together and then sending
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:15:48,438'); seek(948.0)">
              it renders, like rewriting your pipeline to send it to the model.
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:15:52,160'); seek(952.0)">
              Deepstream does this job on its own, while this job is done on
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:15:55,844'); seek(955.0)">
              the cpu. After that you have several classifiers or
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:15:59,508'); seek(959.0)">
              detect layers or segmentation layers that are on Tensorrt
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:16:03,242'); seek(963.0)">
              or on the Triton inferencing server. Deepstream also
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:16:06,712'); seek(966.0)">
              provides you with an option of by default, tracking.
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:16:10,382'); seek(970.0)">
              It is done on both GPU and CPU. You can use that tracker,
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:16:14,766'); seek(974.0)">
              and it's quite easy as it's already custom built in Deepstream.
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:16:18,542'); seek(978.0)">
              After that you can do two things. Either you can visualize your results
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:16:22,562'); seek(982.0)">
              on an on screen display and that conversion is done on the GPU,
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:16:26,482'); seek(986.0)">
              or else you can store it to the cloud or send it to a disk.
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:16:30,710'); seek(990.0)">
              And that can be done on an HDMI
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:16:34,438'); seek(994.0)">
              cable, SATA cable, or using the Nvenc plugin. So these
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:16:38,352'); seek(998.0)">
              are some of the models that Nvidia provides. These have
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:16:41,488'); seek(1001.0)">
              custom built all these for deep steam. So people, the use cases
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:16:45,338'); seek(1005.0)">
              are pretty specific with the model names and you
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:16:49,028'); seek(1009.0)">
              can see that you get an FPS of 1100
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:16:52,452'); seek(1012.0)">
              on a t four inference server and you get a real time FPS on
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:16:56,472'); seek(1016.0)">
              Jetson. Xavier this is quite awesome and
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:17:00,488'); seek(1020.0)">
              you can basically use a these detection on Jetson Nano with a 95 FPS.
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:17:04,430'); seek(1024.0)">
              And if you practically use model pruning and several
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:17:08,162'); seek(1028.0)">
              other steps that we discussed before, you can get even better
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:17:11,756'); seek(1031.0)">
              FPS on the models.
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:17:14,970'); seek(1034.0)">
              These are several other resources on Tensorrt and these are several
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:17:18,658'); seek(1038.0)">
              resources on the Deepstream. The slide deck is provided
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:17:22,514'); seek(1042.0)">
              to you and I hope you guys will check this out using these two
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:17:25,948'); seek(1045.0)">
              resources. Thank you and I'm glad to be
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:17:29,516'); seek(1049.0)">
              present here. Here the code for retina these tensorrt conversion is
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:17:33,732'); seek(1053.0)">
              shown on the GitHub link. Please go and see how you can convert
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:17:37,498'); seek(1057.0)">
              your retina these based model to Tensorrt and deploy
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:17:40,618'); seek(1060.0)">
              it on any machine. Thank you.
            </span>
            
            </div>
          </div>
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/Conf42%20Machine%20Learning%202021%20Slides%20-%20Aditya%20Lohia.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/Conf42%20Machine%20Learning%202021%20Slides%20-%20Aditya%20Lohia.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #198B91;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/ml2021" class="btn btn-sm btn-danger shadow lift" style="background-color: #198B91;">
                <i class="fe fe-grid me-2"></i>
                See all 23 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/ml_aditya.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Aditya Lohia
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Machine Learning Engineer @ Tod'Aers
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/aditya-lohia-ml/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Aditya Lohia's LinkedIn account" />
                  </a>
                  
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by Aditya Lohia"
                  data-url="https://www.conf42.com/ml2021"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/ml2021"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>





  <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
    </script>
    <!-- SUBSCRIBE -->
    <section class="pt-8 pt-md-11 bg-gradient-light-white" id="register">
        <div class="container">
          <div class="row align-items-center justify-content-between mb-8 mb-md-11">
            <div class="col-12 col-md-6 order-md-2" data-aos="fade-left">
  
              <!-- Heading -->
              <h2>
                Awesome tech events for <br>
                <span class="text-success"><span data-typed='{"strings": ["software engineers.", "tech leaders.", "SREs.", "DevOps.", "CTOs.",  "managers.", "architects.", "QAs.", "developers.", "coders.", "founders.", "CEOs.", "students.", "geeks.", "ethical hackers.", "educators.", "enthusiasts.", "directors.", "researchers.", "PHDs.", "evangelists.", "tech authors."]}'></span></span>
              </h2>
  
              <!-- Text -->
              <p class="fs-lg text-muted mb-6">
  
              </p>
  
              <!-- List -->
              <div class="row">
                <div class="col-12 col-lg-12">
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <!-- Text -->
                    <p class="text-success">
                      Priority access to all content
                    </p>
                  </div>
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Video hallway track
                    </p>
                  </div>

                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Community chat
                    </p>
                  </div>
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Exclusive promotions and giveaways
                    </p>
                  </div>
  
                </div>
              </div> <!-- / .row -->
  
            </div>
            <div class="col-12 col-md-6 col-lg-5 order-md-1">
  
              <!-- Card -->
              <div class="card shadow-light-lg">
  
                <!-- Body -->
                <div class="card-body">
  
                  <!-- Form -->
                  <link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
                  <p class="emailoctopus-success-message text-success"></p>
                  <p class="emailoctopus-error-message text-danger"></p>
                  <form
                    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
                    method="post"
                    data-message-success="Thanks! Check your email for further directions!"
                    data-message-missing-email-address="Your email address is required."
                    data-message-invalid-email-address="Your email address looks incorrect, please try again."
                    data-message-bot-submission-error="This doesn't look like a human submission."
                    data-message-consent-required="Please check the checkbox to indicate your consent."
                    data-message-invalid-parameters-error="This form has missing or invalid fields."
                    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
                    class="emailoctopus-form"
                    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
                  >
                    <div class="form-floating emailoctopus-form-row">
                      <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
                      <label for="field_0">Email address</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
                      <label for="field_1">First Name</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
                      <label for="field_2">Last Name</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
                      <label for="field_4">Company</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
                      <label for="field_5">Job Title</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
                      <label for="field_3">Phone Number</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
                        oninput="updateCountry()"
                      >
                        <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
                      </select>
                      <label for="field_7">Country</label>
                    </div>
                    <input id="country-destination" name="field_7" type="hidden">
                    <input id="tz-country" name="field_8" type="hidden">
                    
                    <input
                      name="field_6"
                      type="hidden"
                      value="Machine Learning"
                    >
                    
                    <div class="emailoctopus-form-row-consent">
                      <input
                        type="checkbox"
                        id="consent"
                        name="consent"
                      >
                      <label for="consent">
                        I consent to the following terms:
                      </label>
                      <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
                        Terms and Conditions
                      </a>
                      &amp;
                      <a href="./code-of-conduct" target="_blank">
                        Code of Conduct
                      </a>
                    </div>
                    <div
                      aria-hidden="true"
                      class="emailoctopus-form-row-hp"
                    >
                      <input
                        type="text"
                        name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
                        tabindex="-1"
                        autocomplete="nope"
                      >
                    </div>
                    <div class="mt-6 emailoctopus-form-row-subscribe">
                      <input
                        type="hidden"
                        name="successRedirectUrl"
                      >
                      <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
                        Subscribe
                      </button>
                    </div>
                  </form>
  
                </div>
  
              </div>
  
            </div>
  
          </div> <!-- / .row -->
        </div> <!-- / .container -->
      </section>

      <!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
      <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.sreday.com/">
                  SREday Amsterdam 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>