<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: AI in SRE: Unlocking Prometheus Insights with Natural Language</title>
    <meta name="description" content="Defend your systems against intergalactic invaders!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/David%20Asamu_sre.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="AI in SRE: Unlocking Prometheus Insights with Natural Language | Conf42"/>
    <meta property="og:description" content="While handling an incident, have you ever wished you could simply ask questions about the state of your systems and get immediate, actionable answers? I know I have.  As SREs, we know every second counts during an incident. So, what if we could skip over the complex queries and multiple dashboards to quick insights?  I set out to answer the simple question: “What if you could chat with your monitoring metrics?” In this presentation, I will be sharing what I learnt working building an open source project that allows you to do just that!"/>
    <meta property="og:url" content="https://conf42.com/Site_Reliability_Engineering_SRE_2025_David_Asamu_ai_sre_prometheus_nl"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/PLATFORM2025_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Platform Engineering 2025
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2025-09-04
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/platform2025" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2025">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2025">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2025">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2025">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2025">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2025">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2025">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2025">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2025">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/robotics2025">
                            Robotics
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2025">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2025">
                            Internet of Things (IoT)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="https://conf42.circle.so/">
                            <b>Community platform login</b>
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Join the community!
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #E36414;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Site Reliability Engineering (SRE) 2025 - Online
            </h1>

            <h2 class="text-white">
              
              <time datetime="2025-04-17">April 17 2025</time>
              
              - premiere 5PM GMT
              
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- Defend your systems against intergalactic invaders!
 -->
              <script>
                const event_date = new Date("2025-04-17T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2025-04-17T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              <a href="#register" class="btn btn-primary shadow lift me-1 mb-3">
                <i class="fe fe-user-check me-2"></i>
                Subscribe to watch
              </a>
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "DuWBp5KbfYM"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrCu4WV3_Ve7-ChGVhr_-DbU" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Hi, I am David.", "timestamp": "00:00:00,985", "timestamp_s": 0.0}, {"text": "Welcome to this session, AI in SRE, unlocking permits", "timestamp": "00:00:02,250", "timestamp_s": 2.0}, {"text": "Insights with Natural Language.", "timestamp": "00:00:06,390", "timestamp_s": 6.0}, {"text": "In this session, I\u0027ll be discussing a project I worked on that try to", "timestamp": "00:00:08,580", "timestamp_s": 8.0}, {"text": "answer the question, is it possible to chat with your mandatory metrics?", "timestamp": "00:00:13,020", "timestamp_s": 13.0}, {"text": "I\u0027m excited to share what I\u0027ve learned and I ask her if pay", "timestamp": "00:00:16,710", "timestamp_s": 16.0}, {"text": "attention as we go through it.", "timestamp": "00:00:20,100", "timestamp_s": 20.0}, {"text": "So the talk today is divided into five sections, we do", "timestamp": "00:00:22,300", "timestamp_s": 22.0}, {"text": "like a general introduction.", "timestamp": "00:00:25,200", "timestamp_s": 25.0}, {"text": "We look at the approach by which the tool was built.", "timestamp": "00:00:26,820", "timestamp_s": 26.0}, {"text": "The tool itself is com called Prompt Chart, so we\u0027ll spend some", "timestamp": "00:00:30,090", "timestamp_s": 30.0}, {"text": "time reviewing the tool itself.", "timestamp": "00:00:33,930", "timestamp_s": 33.0}, {"text": "Lastly, we\u0027ll discuss the result and then a closing out section.", "timestamp": "00:00:36,540", "timestamp_s": 36.0}, {"text": "Okay, moving straight on to the first section.", "timestamp": "00:00:41,430", "timestamp_s": 41.0}, {"text": "Introduction was a problem model solution.", "timestamp": "00:00:44,250", "timestamp_s": 44.0}, {"text": "So as SRE, we understand the importance of speed when it comes", "timestamp": "00:00:47,080", "timestamp_s": 47.0}, {"text": "to incident, and if you have a tool that allows you to be able to chat", "timestamp": "00:00:50,590", "timestamp_s": 50.0}, {"text": "with your metrics, it can speed up.", "timestamp": "00:00:54,340", "timestamp_s": 54.0}, {"text": "Your incident responds significantly.", "timestamp": "00:00:56,110", "timestamp_s": 56.0}, {"text": "For me, I think there are two ways to think about it.", "timestamp": "00:00:59,090", "timestamp_s": 59.0}, {"text": "The first is that it reduces cognitive load while responding to an incident where", "timestamp": "00:01:00,920", "timestamp_s": 60.0}, {"text": "you know that there is pressure to restore user experience as fast as possible.", "timestamp": "00:01:05,660", "timestamp_s": 65.0}, {"text": "So having a tool that.", "timestamp": "00:01:10,610", "timestamp_s": 70.0}, {"text": "Helps you write prompt care without you having to think about it, reduces the", "timestamp": "00:01:12,150", "timestamp_s": 72.0}, {"text": "cognitive load required with responding to incident on the other side as well.", "timestamp": "00:01:15,660", "timestamp_s": 75.0}, {"text": "You\u0027re able to get quick insight without having to go through multiple dashboards,", "timestamp": "00:01:20,370", "timestamp_s": 80.0}, {"text": "which is another way in which it can lead to faster incident response.", "timestamp": "00:01:24,210", "timestamp_s": 84.0}, {"text": "The order usefulness of having a tool like this is that it makes", "timestamp": "00:01:28,560", "timestamp_s": 88.0}, {"text": "data accessible to everybody.", "timestamp": "00:01:32,670", "timestamp_s": 92.0}, {"text": "Traditionally to expose data from your Es.", "timestamp": "00:01:34,910", "timestamp_s": 94.0}, {"text": "Instances, you\u0027ll build dashboards for people that are not capable of", "timestamp": "00:01:39,340", "timestamp_s": 99.0}, {"text": "writing like their own prom qls.", "timestamp": "00:01:43,540", "timestamp_s": 103.0}, {"text": "So you would build graph dashboard on things like graph, which they can go.", "timestamp": "00:01:46,150", "timestamp_s": 106.0}, {"text": "But of course that means for each type of.", "timestamp": "00:01:50,770", "timestamp_s": 110.0}, {"text": "Data, or each question that they have, you need to create the", "timestamp": "00:01:53,770", "timestamp_s": 113.0}, {"text": "dashboard, which they can go and see.", "timestamp": "00:01:56,200", "timestamp_s": 116.0}, {"text": "However, if they have a tool that they can basically ask questions,", "timestamp": "00:01:58,420", "timestamp_s": 118.0}, {"text": "then you don\u0027t need to keep coming up with the new dashboard.", "timestamp": "00:02:02,740", "timestamp_s": 122.0}, {"text": "Each time that there\u0027s a new query and a new question, they can basically", "timestamp": "00:02:06,520", "timestamp_s": 126.0}, {"text": "just interact with the tool and get.", "timestamp": "00:02:10,300", "timestamp_s": 130.0}, {"text": "It start answers back.", "timestamp": "00:02:13,405", "timestamp_s": 133.0}, {"text": "The other advantage of having a tool like this is that it uses", "timestamp": "00:02:15,335", "timestamp_s": 135.0}, {"text": "the learning curve for Prometheus.", "timestamp": "00:02:19,415", "timestamp_s": 139.0}, {"text": "Starting up it, it\u0027s easy to write simple promeus queries as you need", "timestamp": "00:02:21,405", "timestamp_s": 141.0}, {"text": "to get, write more completed queries.", "timestamp": "00:02:26,385", "timestamp_s": 146.0}, {"text": "Things can get a bit harder, especially we are just learning d. Language.", "timestamp": "00:02:28,755", "timestamp_s": 148.0}, {"text": "So having something like a tool like this that can essentially cooperate", "timestamp": "00:02:34,395", "timestamp_s": 154.0}, {"text": "with you, like copilot when you\u0027re trying to write ProQ is good \u0027cause", "timestamp": "00:02:39,515", "timestamp_s": 159.0}, {"text": "it helps you, is learning cover, especially via getting started.", "timestamp": "00:02:43,384", "timestamp_s": 163.0}, {"text": "And then I think one of the major.", "timestamp": "00:02:47,764", "timestamp_s": 167.0}, {"text": "Motivations for me to actually work on the project is I see a lot of projects", "timestamp": "00:02:49,684", "timestamp_s": 169.0}, {"text": "in the data space that allows you to be able to chat with your data.", "timestamp": "00:02:55,164", "timestamp_s": 175.0}, {"text": "So it terms Hey, ask questions about your database and things like that.", "timestamp": "00:02:58,885", "timestamp_s": 178.0}, {"text": "So I started wondering, is it possible to do the same thing for metrics,", "timestamp": "00:03:03,055", "timestamp_s": 183.0}, {"text": "especially in the monitoring sense.", "timestamp": "00:03:07,584", "timestamp_s": 187.0}, {"text": "So that\u0027s like the motivation in trying to solve the problem and answering the.", "timestamp": "00:03:09,774", "timestamp_s": 189.0}, {"text": "Question, is it possible to chat with your monitoring metrics?", "timestamp": "00:03:15,640", "timestamp_s": 195.0}, {"text": "So the solution that came up with that is discussed in this project works this way.", "timestamp": "00:03:19,280", "timestamp_s": 199.0}, {"text": "So the flow is the u We are building it too, that allows users to essentially ask", "timestamp": "00:03:25,880", "timestamp_s": 205.0}, {"text": "questions in natural language and then.", "timestamp": "00:03:31,220", "timestamp_s": 211.0}, {"text": "The AI agents take the natural language and generates a", "timestamp": "00:03:34,340", "timestamp_s": 214.0}, {"text": "corresponding prompt QL query.", "timestamp": "00:03:37,850", "timestamp_s": 217.0}, {"text": "The prompt QL query is then run on Prometheus to get the actual results", "timestamp": "00:03:40,760", "timestamp_s": 220.0}, {"text": "back from Prometheus, but then that result before it gets back to the user", "timestamp": "00:03:45,380", "timestamp_s": 225.0}, {"text": "is converted back into natural language.", "timestamp": "00:03:49,610", "timestamp_s": 229.0}, {"text": "So the flow is that it starts with natural language and it", "timestamp": "00:03:52,340", "timestamp_s": 232.0}, {"text": "ends with natural language.", "timestamp": "00:03:55,610", "timestamp_s": 235.0}, {"text": "The user asks questions in natural language, and then they", "timestamp": "00:03:56,900", "timestamp_s": 236.0}, {"text": "get a result presented back to them in natural language.", "timestamp": "00:03:59,960", "timestamp_s": 239.0}, {"text": "So the second section of the talk talks about the approach.", "timestamp": "00:04:04,400", "timestamp_s": 244.0}, {"text": "At this point, we\u0027ll dive deep into what.", "timestamp": "00:04:07,550", "timestamp_s": 247.0}, {"text": "How we implemented it the architecture, and then we review a couple of the", "timestamp": "00:04:11,210", "timestamp_s": 251.0}, {"text": "key components of the architecture.", "timestamp": "00:04:15,179", "timestamp_s": 255.0}, {"text": "So this diagram shows architecture for the system.", "timestamp": "00:04:17,649", "timestamp_s": 257.0}, {"text": "So on the far left side of it, we have the front end, which is", "timestamp": "00:04:21,339", "timestamp_s": 261.0}, {"text": "how you interact with the system.", "timestamp": "00:04:24,640", "timestamp_s": 264.0}, {"text": "So it is either you interacting with the tool via a web app or a", "timestamp": "00:04:26,800", "timestamp_s": 266.0}, {"text": "Slack app basically, which allows you to enter your user query.", "timestamp": "00:04:30,760", "timestamp_s": 270.0}, {"text": "And then that user query gets sent to a backend.", "timestamp": "00:04:36,109", "timestamp_s": 276.0}, {"text": "The backend has rest API that makes it easy for the front end to interact", "timestamp": "00:04:39,319", "timestamp_s": 279.0}, {"text": "and send questions down to it.", "timestamp": "00:04:44,679", "timestamp_s": 284.0}, {"text": "As part of the backend as well, we have the AI agent.", "timestamp": "00:04:47,419", "timestamp_s": 287.0}, {"text": "The AI agent basically handles the coordination with the LLM.", "timestamp": "00:04:50,419", "timestamp_s": 290.0}, {"text": "So when the user query comes, it basically sends the request to the LLM and then", "timestamp": "00:04:55,249", "timestamp_s": 295.0}, {"text": "the LLM as well uses what is called.", "timestamp": "00:05:00,679", "timestamp_s": 300.0}, {"text": "Two, calling a function, callings to be able to talk to Prometheus.", "timestamp": "00:05:03,974", "timestamp_s": 303.0}, {"text": "So on the Prometheus aspect, we need to be able to fetch metadata from Prometheus", "timestamp": "00:05:07,844", "timestamp_s": 307.0}, {"text": "and run queries from Prometheus as well.", "timestamp": "00:05:12,584", "timestamp_s": 312.0}, {"text": "As we go later into the talk, we\u0027ll talk more about the metadata such that you", "timestamp": "00:05:16,394", "timestamp_s": 316.0}, {"text": "have better understanding of what entails.", "timestamp": "00:05:20,684", "timestamp_s": 320.0}, {"text": "But this gives.", "timestamp": "00:05:23,139", "timestamp_s": 323.0}, {"text": "I an eye level overview of what happens when you\u0027re trying to answer questions.", "timestamp": "00:05:25,304", "timestamp_s": 325.0}, {"text": "So the flow is, the request comes in, let\u0027s say from the", "timestamp": "00:05:30,074", "timestamp_s": 330.0}, {"text": "web app, it goes to the backend.", "timestamp": "00:05:32,804", "timestamp_s": 332.0}, {"text": "The backend takes the question, sends it to the AI agent.", "timestamp": "00:05:35,174", "timestamp_s": 335.0}, {"text": "The AI agent takes the question, sends it to the LLM, so the user", "timestamp": "00:05:39,194", "timestamp_s": 339.0}, {"text": "query together with the metadata.", "timestamp": "00:05:44,384", "timestamp_s": 344.0}, {"text": "So the metadata in this case is you can think of it like the scheme of your data.", "timestamp": "00:05:45,824", "timestamp_s": 345.0}, {"text": "So it basically just contains information regarding.", "timestamp": "00:05:50,294", "timestamp_s": 350.0}, {"text": "What kind of metrics are available in your Promeus instance and", "timestamp": "00:05:53,909", "timestamp_s": 353.0}, {"text": "the descriptions for them.", "timestamp": "00:05:57,629", "timestamp_s": 357.0}, {"text": "So you send that together with the user query to the LLM.", "timestamp": "00:05:58,799", "timestamp_s": 358.0}, {"text": "The LLM then thinks about a prompt QL that would be able to", "timestamp": "00:06:02,819", "timestamp_s": 362.0}, {"text": "answer that question correctly.", "timestamp": "00:06:07,409", "timestamp_s": 367.0}, {"text": "Then when they ask the prompt, QL, send it back to the AI agent, which then runs", "timestamp": "00:06:09,719", "timestamp_s": 369.0}, {"text": "that query on Prometheus to get the.", "timestamp": "00:06:15,599", "timestamp_s": 375.0}, {"text": "Actual result.", "timestamp": "00:06:20,069", "timestamp_s": 380.0}, {"text": "And then once the result comes back from Prometheus, it sparks back to the", "timestamp": "00:06:21,090", "timestamp_s": 381.0}, {"text": "LLM such that the LLM can interpret the result in natural language.", "timestamp": "00:06:25,079", "timestamp_s": 385.0}, {"text": "And then the result goes all the way back to the front end for the user to see.", "timestamp": "00:06:29,999", "timestamp_s": 389.0}, {"text": "So that\u0027s what the.", "timestamp": "00:06:34,659", "timestamp_s": 394.0}, {"text": "Overview of the system.", "timestamp": "00:06:36,219", "timestamp_s": 396.0}, {"text": "Looks like we\u0027ll be looking at a couple of tools that allows, or the key", "timestamp": "00:06:37,389", "timestamp_s": 397.0}, {"text": "components of the architecture as well.", "timestamp": "00:06:41,319", "timestamp_s": 401.0}, {"text": "So the first one is Prometheus.", "timestamp": "00:06:43,669", "timestamp_s": 403.0}, {"text": "Again, if you\u0027re not familiar with Prometheus I won\u0027t spend too much time", "timestamp": "00:06:45,649", "timestamp_s": 405.0}, {"text": "diving into it, but you can check it out.", "timestamp": "00:06:50,019", "timestamp_s": 410.0}, {"text": "What you need to know is that it\u0027s a time series data store.", "timestamp": "00:06:51,969", "timestamp_s": 411.0}, {"text": "For Matrix and typically ma matrix stored in material have these", "timestamp": "00:06:55,359", "timestamp_s": 415.0}, {"text": "formats where you have the matrix name and then you have the label.", "timestamp": "00:07:01,189", "timestamp_s": 421.0}, {"text": "So in this case, the labels are method and handler, and then they have the", "timestamp": "00:07:04,879", "timestamp_s": 424.0}, {"text": "corresponding value of post and messages.", "timestamp": "00:07:09,049", "timestamp_s": 429.0}, {"text": "So this is an entry of Prometheus of a metric in Prometheus.", "timestamp": "00:07:11,209", "timestamp_s": 431.0}, {"text": "So this metric is then script by Prometheus.", "timestamp": "00:07:17,319", "timestamp_s": 437.0}, {"text": "Quite often based on whatever script interval you have.", "timestamp": "00:07:20,424", "timestamp_s": 440.0}, {"text": "As the time series data is going to recall the time that it was script", "timestamp": "00:07:23,534", "timestamp_s": 443.0}, {"text": "and then the value at that point in time, and that\u0027s how this part works.", "timestamp": "00:07:26,984", "timestamp_s": 446.0}, {"text": "So part of the things that make this architecture works is that premises", "timestamp": "00:07:32,604", "timestamp_s": 452.0}, {"text": "as this metadata API that allows you to get, again, as I described", "timestamp": "00:07:36,804", "timestamp_s": 456.0}, {"text": "earlier, like a scheme of your data.", "timestamp": "00:07:41,274", "timestamp_s": 461.0}, {"text": "In the protal systems.", "timestamp": "00:07:43,914", "timestamp_s": 463.0}, {"text": "So what that entails is that it shows you the list of all the metrics that are", "timestamp": "00:07:45,654", "timestamp_s": 465.0}, {"text": "available inside of your protal system.", "timestamp": "00:07:49,674", "timestamp_s": 469.0}, {"text": "So in our case, for example, you can see C device or this device usage to CPU", "timestamp": "00:07:53,669", "timestamp_s": 473.0}, {"text": "load average superior U system sequence.", "timestamp": "00:07:59,074", "timestamp_s": 479.0}, {"text": "These are examples of metrics that are being scripted by Prometheus and is", "timestamp": "00:08:01,804", "timestamp_s": 481.0}, {"text": "available in the Prometheus instance.", "timestamp": "00:08:06,994", "timestamp_s": 486.0}, {"text": "So that means you can write queries or ask questions regarding this metrics.", "timestamp": "00:08:09,634", "timestamp_s": 489.0}, {"text": "But not only is the name return, they also returns information,", "timestamp": "00:08:14,449", "timestamp_s": 494.0}, {"text": "helpful information about them, which is the type of the metric itself", "timestamp": "00:08:18,379", "timestamp_s": 498.0}, {"text": "based on the type of the metric.", "timestamp": "00:08:22,619", "timestamp_s": 502.0}, {"text": "Again, it depends the kind of operations that you can perform on The metric", "timestamp": "00:08:24,089", "timestamp_s": 504.0}, {"text": "depends on the type of the metric, right?", "timestamp": "00:08:29,199", "timestamp_s": 509.0}, {"text": "So that\u0027s something to take up that needs to be taken into account", "timestamp": "00:08:31,059", "timestamp_s": 511.0}, {"text": "while writing your Pro Creole.", "timestamp": "00:08:34,734", "timestamp_s": 514.0}, {"text": "And then the help also gives information, additional", "timestamp": "00:08:36,554", "timestamp_s": 516.0}, {"text": "information regarding each type.", "timestamp": "00:08:40,359", "timestamp_s": 520.0}, {"text": "Of metric, which allows you to be able to interpret the metric correctly.", "timestamp": "00:08:42,879", "timestamp_s": 522.0}, {"text": "Additionally, sometimes you can also add information about the labels.", "timestamp": "00:08:48,360", "timestamp_s": 528.0}, {"text": "So for example, for this 80 TTP request total, the description can be", "timestamp": "00:08:52,170", "timestamp_s": 532.0}, {"text": "something like, oh, you let you know the number of 80 TP requests that", "timestamp": "00:08:56,790", "timestamp_s": 536.0}, {"text": "has happened within a time period.", "timestamp": "00:08:59,706", "timestamp_s": 539.0}, {"text": "And he has following label like method.", "timestamp": "00:09:02,565", "timestamp_s": 542.0}, {"text": "And so this metadata is the, additional bits that is sent as context along", "timestamp": "00:09:04,885", "timestamp_s": 544.0}, {"text": "with the user query itself to the LLM.", "timestamp": "00:09:13,320", "timestamp_s": 553.0}, {"text": "So that\u0027s the first major component.", "timestamp": "00:09:17,190", "timestamp_s": 557.0}, {"text": "The second major component, as we explained, is the LLM.", "timestamp": "00:09:19,230", "timestamp_s": 559.0}, {"text": "And then he makes use of a. Technical tools calling or function calling, which", "timestamp": "00:09:22,920", "timestamp_s": 562.0}, {"text": "is basically he extends the LLM such that it\u0027s able to interact with the environment", "timestamp": "00:09:28,925", "timestamp_s": 568.0}, {"text": "that is outside of the LLM itself using tools and to make that happen,", "timestamp": "00:09:34,805", "timestamp_s": 574.0}, {"text": "we have built two tools in our example.", "timestamp": "00:09:39,005", "timestamp_s": 579.0}, {"text": "So we have the query promises tool, which basically allows the LLM to.", "timestamp": "00:09:42,275", "timestamp_s": 582.0}, {"text": "Execute Prometheus query on an instance and get response back before", "timestamp": "00:09:47,350", "timestamp_s": 587.0}, {"text": "returning a response back to the user.", "timestamp": "00:09:52,820", "timestamp_s": 592.0}, {"text": "The other tool also allows the LLM to be able to query Prometheus for the metadata", "timestamp": "00:09:55,980", "timestamp_s": 595.0}, {"text": "information similar to the metadata information shown in the previous.", "timestamp": "00:10:02,640", "timestamp_s": 602.0}, {"text": "In the previous page here.", "timestamp": "00:10:07,560", "timestamp_s": 607.0}, {"text": "So basically the first two allows Prometheus to be able to run it", "timestamp": "00:10:09,030", "timestamp_s": 609.0}, {"text": "degenerated from Q and get responses back from Prometheus while the second", "timestamp": "00:10:13,740", "timestamp_s": 613.0}, {"text": "two allows Prometheus to be able to fetch metadata results from Prometheus itself.", "timestamp": "00:10:18,300", "timestamp_s": 618.0}, {"text": "Metadata regarding the metrics available.", "timestamp": "00:10:26,250", "timestamp_s": 626.0}, {"text": "So that\u0027s the two major.", "timestamp": "00:10:30,220", "timestamp_s": 630.0}, {"text": "Component in our architecture.", "timestamp": "00:10:32,260", "timestamp_s": 632.0}, {"text": "So moving on to the third part, which is we are looking at the tool that was built.", "timestamp": "00:10:34,390", "timestamp_s": 634.0}, {"text": "So the tool itself is called Prompt chart, and basically we\u0027ll be exploring", "timestamp": "00:10:40,010", "timestamp_s": 640.0}, {"text": "the tool from two points of view.", "timestamp": "00:10:44,180", "timestamp_s": 644.0}, {"text": "So based on the data inside of the Prometheus instance, we\u0027ve", "timestamp": "00:10:46,260", "timestamp_s": 646.0}, {"text": "classified it into two examples.", "timestamp": "00:10:51,570", "timestamp_s": 651.0}, {"text": "The first one uses data from no exporter, and then the second.", "timestamp": "00:10:54,370", "timestamp_s": 654.0}, {"text": "Example of pro chats uses data from custom exporter to Promeus.", "timestamp": "00:10:59,620", "timestamp_s": 659.0}, {"text": "So both of them are chat.", "timestamp": "00:11:05,570", "timestamp_s": 665.0}, {"text": "You\u0027re chatting with Promeus.", "timestamp": "00:11:07,460", "timestamp_s": 667.0}, {"text": "So what\u0027s just different is the source of data in the Promeus instance and", "timestamp": "00:11:08,630", "timestamp_s": 668.0}, {"text": "which basically affects the type of queries that you can, or questions you", "timestamp": "00:11:13,550", "timestamp_s": 673.0}, {"text": "can ask, and the type of queries that you, that will be generated as well.", "timestamp": "00:11:17,360", "timestamp_s": 677.0}, {"text": "A little bit more about the node exporter.", "timestamp": "00:11:22,820", "timestamp_s": 682.0}, {"text": "So the node exporter is essentially.", "timestamp": "00:11:24,560", "timestamp_s": 684.0}, {"text": "Like a plugin that you can run on a vm.", "timestamp": "00:11:27,065", "timestamp_s": 687.0}, {"text": "So you can run it on a single vm, you can run it on your euca machine, for example.", "timestamp": "00:11:30,155", "timestamp_s": 690.0}, {"text": "Pretty much any like machine VM node, you can run it.", "timestamp": "00:11:36,165", "timestamp_s": 696.0}, {"text": "So what does is you expose system metrics to permit your such a protus", "timestamp": "00:11:39,675", "timestamp_s": 699.0}, {"text": "scan, script them at intervals.", "timestamp": "00:11:44,475", "timestamp_s": 704.0}, {"text": "So for example, things like the CPU seconds, the available storage.", "timestamp": "00:11:46,425", "timestamp_s": 706.0}, {"text": "Memory utilization, network traffic, all of that can be", "timestamp": "00:11:52,520", "timestamp_s": 712.0}, {"text": "exposed by the node exporter.", "timestamp": "00:11:55,910", "timestamp_s": 715.0}, {"text": "So there are standard system metrics and you don\u0027t need to configure anything.", "timestamp": "00:11:59,360", "timestamp_s": 719.0}, {"text": "The node exporter basically just makes those available and then those", "timestamp": "00:12:02,780", "timestamp_s": 722.0}, {"text": "data can be injected into premises.", "timestamp": "00:12:06,650", "timestamp_s": 726.0}, {"text": "So these are sample of some of the metrics that are available.", "timestamp": "00:12:09,890", "timestamp_s": 729.0}, {"text": "So as you can see, note CPU seconds to the the system available.", "timestamp": "00:12:13,740", "timestamp_s": 733.0}, {"text": "Five system available in bys.", "timestamp": "00:12:19,330", "timestamp_s": 739.0}, {"text": "And then this is on the number of network requests in bys that", "timestamp": "00:12:21,850", "timestamp_s": 741.0}, {"text": "have been received as well.", "timestamp": "00:12:28,190", "timestamp_s": 748.0}, {"text": "The average network traffic invites over the last minute.", "timestamp": "00:12:29,900", "timestamp_s": 749.0}, {"text": "So examples of the metrics and then some of the cray so you can", "timestamp": "00:12:32,960", "timestamp_s": 752.0}, {"text": "see, using like rates to get like averages over a time period of time.", "timestamp": "00:12:36,620", "timestamp_s": 756.0}, {"text": "So this is what a Prometheus query looks like as well.", "timestamp": "00:12:42,050", "timestamp_s": 762.0}, {"text": "So moving on.", "timestamp": "00:12:47,420", "timestamp_s": 767.0}, {"text": "So this is showing the actual tool called pro chart and then responses", "timestamp": "00:12:48,720", "timestamp_s": 768.0}, {"text": "that was generated by the system when it was asked certain questions.", "timestamp": "00:12:55,450", "timestamp_s": 775.0}, {"text": "So in this example, the prompt chart is connected to the", "timestamp": "00:12:58,570", "timestamp_s": 778.0}, {"text": "premises instance that has data.", "timestamp": "00:13:01,570", "timestamp_s": 781.0}, {"text": "From a note exporter.", "timestamp": "00:13:04,490", "timestamp_s": 784.0}, {"text": "So the information available or the kind of questions you can ask is", "timestamp": "00:13:06,710", "timestamp_s": 786.0}, {"text": "based on the kind of metrics that is expected by the note exporter.", "timestamp": "00:13:11,000", "timestamp_s": 791.0}, {"text": "So in this case we can see four exchanges or five questions into the, actually, so", "timestamp": "00:13:14,570", "timestamp_s": 794.0}, {"text": "the first talks about DCP utilization.", "timestamp": "00:13:21,030", "timestamp_s": 801.0}, {"text": "So this is a question.", "timestamp": "00:13:24,260", "timestamp_s": 804.0}, {"text": "So in black here, you have the user question itself.", "timestamp": "00:13:25,340", "timestamp_s": 805.0}, {"text": "While in white it is a response from the.", "timestamp": "00:13:29,390", "timestamp_s": 809.0}, {"text": "From chart AI system is itself.", "timestamp": "00:13:32,705", "timestamp_s": 812.0}, {"text": "So it looks first asking about the currency periodization on the node.", "timestamp": "00:13:35,375", "timestamp_s": 815.0}, {"text": "We get this.", "timestamp": "00:13:39,545", "timestamp_s": 819.0}, {"text": "He asked about the battery percentage on the node more around information", "timestamp": "00:13:40,265", "timestamp_s": 820.0}, {"text": "always running on the node.", "timestamp": "00:13:44,385", "timestamp_s": 824.0}, {"text": "So I did run this particular example on my laptop using the macros node", "timestamp": "00:13:45,495", "timestamp_s": 825.0}, {"text": "exporter, so you can see gets that.", "timestamp": "00:13:51,545", "timestamp_s": 831.0}, {"text": "The node is running macros, the digs.", "timestamp": "00:13:54,495", "timestamp_s": 834.0}, {"text": "Space that is available across all the instances of the nodes.", "timestamp": "00:13:57,510", "timestamp_s": 837.0}, {"text": "And then finally the memory utilization of the node is shown as well.", "timestamp": "00:14:00,840", "timestamp_s": 840.0}, {"text": "Alright, so we\u0027ll look at, in, in follow up, would look at later,", "timestamp": "00:14:08,190", "timestamp_s": 848.0}, {"text": "would actually see from the backend work queries are generated for these", "timestamp": "00:14:12,820", "timestamp_s": 852.0}, {"text": "examples and what they look like.", "timestamp": "00:14:17,470", "timestamp_s": 857.0}, {"text": "The second Prometheus instance, I\u0027ll be using.", "timestamp": "00:14:19,520", "timestamp_s": 859.0}, {"text": "Is thanks to our, the friends@promlabs.com.", "timestamp": "00:14:24,110", "timestamp_s": 864.0}, {"text": "So they have this Prometheus instance that is publicly accessible.", "timestamp": "00:14:28,790", "timestamp_s": 868.0}, {"text": "So is from the team that wrote Prometheus and they make it available", "timestamp": "00:14:32,880", "timestamp_s": 872.0}, {"text": "so that you can use it to experiment, to learn and interact with Prometheus.", "timestamp": "00:14:38,400", "timestamp_s": 878.0}, {"text": "On that Promis instance, we have some custom metrics.", "timestamp": "00:14:43,260", "timestamp_s": 883.0}, {"text": "So if you look at the first set of metrics that was exposed by, in our first example", "timestamp": "00:14:46,530", "timestamp_s": 886.0}, {"text": "using the node exporter, those metrics are system metrics, meaning that there are", "timestamp": "00:14:52,250", "timestamp_s": 892.0}, {"text": "metrics from regarding the machine itself.", "timestamp": "00:14:56,960", "timestamp_s": 896.0}, {"text": "But more often than not, when we are trying to set up monitoring,", "timestamp": "00:15:00,440", "timestamp_s": 900.0}, {"text": "we want to collect metrics about the state of our service or the", "timestamp": "00:15:03,530", "timestamp_s": 903.0}, {"text": "application we are running as well.", "timestamp": "00:15:06,740", "timestamp_s": 906.0}, {"text": "So in those cases.", "timestamp": "00:15:08,600", "timestamp_s": 908.0}, {"text": "You would come up with like your own custom metrics describing the different", "timestamp": "00:15:09,945", "timestamp_s": 909.0}, {"text": "states or events in your service.", "timestamp": "00:15:15,395", "timestamp_s": 915.0}, {"text": "So the Prometheus on prom lab, the demo Prometheus on prom lab.", "timestamp": "00:15:18,515", "timestamp_s": 918.0}, {"text": "So he runs a service called the demo service, and then the demo", "timestamp": "00:15:23,615", "timestamp_s": 923.0}, {"text": "service exposes the following.", "timestamp": "00:15:27,005", "timestamp_s": 927.0}, {"text": "Custom metrics which is what we based in a set of questions and", "timestamp": "00:15:30,055", "timestamp_s": 930.0}, {"text": "interactions with from chat on.", "timestamp": "00:15:34,835", "timestamp_s": 934.0}, {"text": "So a couple of interesting ones that you see is one is that there are batch jobs.", "timestamp": "00:15:37,305", "timestamp_s": 937.0}, {"text": "So you can essentially ask questions around success rate of batch jobs", "timestamp": "00:15:42,165", "timestamp_s": 942.0}, {"text": "that was done by the demo service.", "timestamp": "00:15:46,765", "timestamp_s": 946.0}, {"text": "Can see questions around the HCTP request duration as well.", "timestamp": "00:15:49,075", "timestamp_s": 949.0}, {"text": "Another interesting one is that the service itself as a matrix lets you", "timestamp": "00:15:53,785", "timestamp_s": 953.0}, {"text": "know whether today is an holiday or not.", "timestamp": "00:15:59,185", "timestamp_s": 959.0}, {"text": "So it\u0027s basically called demo is holiday and from the help information.", "timestamp": "00:16:01,195", "timestamp_s": 961.0}, {"text": "So again, if you look at the metadata information, this is GUI, not from", "timestamp": "00:16:06,295", "timestamp_s": 966.0}, {"text": "the API, but basically it\u0027s going to be similar to the metadata.", "timestamp": "00:16:10,945", "timestamp_s": 970.0}, {"text": "EPI response showed earlier where you can see the name of the metric.", "timestamp": "00:16:16,625", "timestamp_s": 976.0}, {"text": "The type of it, which is in this case for his early day.", "timestamp": "00:16:21,135", "timestamp_s": 981.0}, {"text": "We can see the type of set as gauge, and then the description takes that when", "timestamp": "00:16:24,545", "timestamp_s": 984.0}, {"text": "the value return is one, it means that the current days and early day, and when", "timestamp": "00:16:29,345", "timestamp_s": 989.0}, {"text": "the value rate return is zero, it means that the current days one early days.", "timestamp": "00:16:33,695", "timestamp_s": 993.0}, {"text": "So these are examples of custom metrics that has been made available", "timestamp": "00:16:36,965", "timestamp_s": 996.0}, {"text": "by the demo service on this instance.", "timestamp": "00:16:41,345", "timestamp_s": 1001.0}, {"text": "So all of this.", "timestamp": "00:16:43,715", "timestamp_s": 1003.0}, {"text": "Metrics has been script and is available inside of the Promeus instance.", "timestamp": "00:16:45,335", "timestamp_s": 1005.0}, {"text": "So that means we can connect our pro chart tool to this Prometheus instance and ask", "timestamp": "00:16:49,625", "timestamp_s": 1009.0}, {"text": "it questions and let the AI generate the corresponding queries and shows answers.", "timestamp": "00:16:54,665", "timestamp_s": 1014.0}, {"text": "So let\u0027s see how that looks like.", "timestamp": "00:17:01,025", "timestamp_s": 1021.0}, {"text": "So in the next chat here we can see is, the prom chart two is connected to", "timestamp": "00:17:04,745", "timestamp_s": 1024.0}, {"text": "the prom labs protal instance, and the first question here is today an holiday?", "timestamp": "00:17:10,505", "timestamp_s": 1030.0}, {"text": "So the AI system response are based on metrics that I was able", "timestamp": "00:17:15,495", "timestamp_s": 1035.0}, {"text": "to find from demo service Two.", "timestamp": "00:17:19,215", "timestamp_s": 1039.0}, {"text": "It reports that today is an holiday.", "timestamp": "00:17:21,165", "timestamp_s": 1041.0}, {"text": "Again, just in the next couple of slides, we\u0027ll be looking", "timestamp": "00:17:23,325", "timestamp_s": 1043.0}, {"text": "at the actual query and area.", "timestamp": "00:17:26,810", "timestamp_s": 1046.0}, {"text": "Got to this answer, but basically this just shows an overview", "timestamp": "00:17:29,360", "timestamp_s": 1049.0}, {"text": "of what the interaction from the web interface looks like.", "timestamp": "00:17:33,650", "timestamp_s": 1053.0}, {"text": "The second question we ask is how many items have been shipped to this?", "timestamp": "00:17:36,620", "timestamp_s": 1056.0}, {"text": "So if a go back cop would see that there is a custom metric exposed", "timestamp": "00:17:41,030", "timestamp_s": 1061.0}, {"text": "called items shipped to the, which is a counter that keeps track of the", "timestamp": "00:17:45,770", "timestamp_s": 1065.0}, {"text": "number of items that have been shipped.", "timestamp": "00:17:49,910", "timestamp_s": 1069.0}, {"text": "So making use of this, we expect that the AI will make use of this.", "timestamp": "00:17:51,800", "timestamp_s": 1071.0}, {"text": "Counter to be able to answer the question of how many items have been shipped today.", "timestamp": "00:17:55,775", "timestamp_s": 1075.0}, {"text": "And then lastly, there\u0027s a third question here where we are trying", "timestamp": "00:18:01,085", "timestamp_s": 1081.0}, {"text": "to ask questions around the demo API and if it\u0027s taken longer than usual", "timestamp": "00:18:03,515", "timestamp_s": 1083.0}, {"text": "unfortunately we realize, oh, as you can see from the screenshots, the ICM", "timestamp": "00:18:09,905", "timestamp_s": 1089.0}, {"text": "response, that there is no data fund.", "timestamp": "00:18:14,885", "timestamp_s": 1094.0}, {"text": "So we look at that and try to figure out, okay, why did that happen as well.", "timestamp": "00:18:17,555", "timestamp_s": 1097.0}, {"text": "Behind the scenes as promised.", "timestamp": "00:18:24,600", "timestamp_s": 1104.0}, {"text": "So this is basically the logs from the backend system letting", "timestamp": "00:18:25,980", "timestamp_s": 1105.0}, {"text": "us know exactly what transpired.", "timestamp": "00:18:30,090", "timestamp_s": 1110.0}, {"text": "So in the case of the first example message, the user", "timestamp": "00:18:32,250", "timestamp_s": 1112.0}, {"text": "query is a sedan holiday.", "timestamp": "00:18:36,180", "timestamp_s": 1116.0}, {"text": "Then the AI agents generated this corresponding prompt Q query,", "timestamp": "00:18:38,580", "timestamp_s": 1118.0}, {"text": "which is demo is holiday, right?", "timestamp": "00:18:43,530", "timestamp_s": 1123.0}, {"text": "And then.", "timestamp": "00:18:45,900", "timestamp_s": 1125.0}, {"text": "This generated prom QR was run by the AI agent using function,", "timestamp": "00:18:46,870", "timestamp_s": 1126.0}, {"text": "calling on the permitter sensor and permit to return this response.", "timestamp": "00:18:51,590", "timestamp_s": 1131.0}, {"text": "So if you pay attention to this, you\u0027ll see the instance name is demo service two,", "timestamp": "00:18:55,400", "timestamp_s": 1135.0}, {"text": "which is why the response talks about demo service two, or the most important stuff", "timestamp": "00:19:00,590", "timestamp_s": 1140.0}, {"text": "to pay attention to is the value here.", "timestamp": "00:19:05,905", "timestamp_s": 1145.0}, {"text": "So you see the value here is one, because this value is one that means", "timestamp": "00:19:08,465", "timestamp_s": 1148.0}, {"text": "based on the information that we are able to get from the metadata,", "timestamp": "00:19:13,145", "timestamp_s": 1153.0}, {"text": "we know that one means only day.", "timestamp": "00:19:16,445", "timestamp_s": 1156.0}, {"text": "So you can see in this example, the AI agent is able to both write", "timestamp": "00:19:18,365", "timestamp_s": 1158.0}, {"text": "the right corresponding query based on the metadata, but also", "timestamp": "00:19:22,805", "timestamp_s": 1162.0}, {"text": "based on the information provided.", "timestamp": "00:19:27,065", "timestamp_s": 1167.0}, {"text": "In the metadata, it\u0027s able to interpret that.", "timestamp": "00:19:30,275", "timestamp_s": 1170.0}, {"text": "One means that today is an holiday, and as such, based on that interpretation is able", "timestamp": "00:19:33,305", "timestamp_s": 1173.0}, {"text": "to reply the user query back in natural language saying yes based on the data that", "timestamp": "00:19:39,395", "timestamp_s": 1179.0}, {"text": "we can see from demo service to values.", "timestamp": "00:19:44,915", "timestamp_s": 1184.0}, {"text": "And that means that today it\u0027s an holiday.", "timestamp": "00:19:47,645", "timestamp_s": 1187.0}, {"text": "So this is essentially what\u0027s going on behind the scene.", "timestamp": "00:19:49,685", "timestamp_s": 1189.0}, {"text": "In the case of the first question.", "timestamp": "00:19:52,535", "timestamp_s": 1192.0}, {"text": "In the case of the second question, this is a way more interesting.", "timestamp": "00:19:54,815", "timestamp_s": 1194.0}, {"text": "So we asked about how many items has been shipped today.", "timestamp": "00:19:59,465", "timestamp_s": 1199.0}, {"text": "This is the user query again generated from ql.", "timestamp": "00:20:02,075", "timestamp_s": 1202.0}, {"text": "As expected, it makes use of the metrics demo item shipped total.", "timestamp": "00:20:05,375", "timestamp_s": 1205.0}, {"text": "So it sets the time period.", "timestamp": "00:20:09,665", "timestamp_s": 1209.0}, {"text": "So one day this is right, and then it looks at the increase", "timestamp": "00:20:11,495", "timestamp_s": 1211.0}, {"text": "over the period of one day.", "timestamp": "00:20:14,765", "timestamp_s": 1214.0}, {"text": "So this is generated from QL query.", "timestamp": "00:20:16,415", "timestamp_s": 1216.0}, {"text": "Which makes sense.", "timestamp": "00:20:19,325", "timestamp_s": 1219.0}, {"text": "So again, using two scrolling that this query is executed against the", "timestamp": "00:20:20,285", "timestamp_s": 1220.0}, {"text": "Prometheus instance and Promeus returns, this results back to the AI agent.", "timestamp": "00:20:25,415", "timestamp_s": 1225.0}, {"text": "So now the AI needs to figure out how to give, return this in natural language.", "timestamp": "00:20:31,385", "timestamp_s": 1231.0}, {"text": "And this, I think this is interesting.", "timestamp": "00:20:37,865", "timestamp_s": 1237.0}, {"text": "But this an a particularly interesting example because we would see that the demo", "timestamp": "00:20:40,460", "timestamp_s": 1240.0}, {"text": "service actually runs three copies of it.", "timestamp": "00:20:45,440", "timestamp_s": 1245.0}, {"text": "So as you can see here, there are three instances of the demo service.", "timestamp": "00:20:47,750", "timestamp_s": 1247.0}, {"text": "There\u0027s demo service zero, which is the first instance.", "timestamp": "00:20:51,110", "timestamp_s": 1251.0}, {"text": "There is demo service one, which is the second instance here, and then there is.", "timestamp": "00:20:54,140", "timestamp_s": 1254.0}, {"text": "Demo service two, which is the third instance.", "timestamp": "00:20:58,310", "timestamp_s": 1258.0}, {"text": "So for each of these instance, they have been processing orders through the day", "timestamp": "00:21:01,160", "timestamp_s": 1261.0}, {"text": "and each of them maintain a count of the number of orders that they\u0027ve processed.", "timestamp": "00:21:05,000", "timestamp_s": 1265.0}, {"text": "So if you look at the demo service zero, it returns around five, 455,000 orders", "timestamp": "00:21:09,410", "timestamp_s": 1269.0}, {"text": "have been processed by demo service one.", "timestamp": "00:21:17,090", "timestamp_s": 1277.0}, {"text": "And then if you look at demo service.", "timestamp": "00:21:19,550", "timestamp_s": 1279.0}, {"text": "No, the demo service zero rather, has processed 455,000.", "timestamp": "00:21:22,760", "timestamp_s": 1282.0}, {"text": "If you look at demo service one, the value four eight is 453,000.", "timestamp": "00:21:27,290", "timestamp_s": 1287.0}, {"text": "And then if you look at demo service two, the value process, so five a day", "timestamp": "00:21:31,910", "timestamp_s": 1291.0}, {"text": "is 453,000, close to 454,000 as well.", "timestamp": "00:21:36,710", "timestamp_s": 1296.0}, {"text": "Now.", "timestamp": "00:21:40,970", "timestamp_s": 1300.0}, {"text": "The LLM does something interesting because it gets this results back.", "timestamp": "00:21:41,735", "timestamp_s": 1301.0}, {"text": "It is intelligent enough to know that the value is interested in, is a,", "timestamp": "00:21:46,325", "timestamp_s": 1306.0}, {"text": "an aggregation of the value across each of these instances, and also", "timestamp": "00:21:51,845", "timestamp_s": 1311.0}, {"text": "applying the right grouping so it was able to figure out that it needs", "timestamp": "00:21:57,995", "timestamp_s": 1317.0}, {"text": "to sum the value from demo service.", "timestamp": "00:22:01,745", "timestamp_s": 1321.0}, {"text": "Zero demo service.", "timestamp": "00:22:04,355", "timestamp_s": 1324.0}, {"text": "One and demo service two.", "timestamp": "00:22:06,785", "timestamp_s": 1326.0}, {"text": "So if you look at this value, so you can do it later, but I\u0027ve confirmed it.", "timestamp": "00:22:08,915", "timestamp_s": 1328.0}, {"text": "If you sum this value return for demo service one, I plus the", "timestamp": "00:22:13,335", "timestamp_s": 1333.0}, {"text": "value sum there, demo service.", "timestamp": "00:22:18,225", "timestamp_s": 1338.0}, {"text": "One this is the value return from demo service two plus the value", "timestamp": "00:22:21,375", "timestamp_s": 1341.0}, {"text": "return from demo service, one plus value return from demo service zero.", "timestamp": "00:22:26,815", "timestamp_s": 1346.0}, {"text": "You\u0027re going to get.", "timestamp": "00:22:31,405", "timestamp_s": 1351.0}, {"text": "The total value.", "timestamp": "00:22:32,755", "timestamp_s": 1352.0}, {"text": "Yeah, so the LLM was actually able to figure out the right", "timestamp": "00:22:33,715", "timestamp_s": 1353.0}, {"text": "aggregation for the list of data that was returned in the results.", "timestamp": "00:22:37,765", "timestamp_s": 1357.0}, {"text": "Sum it appropriately and give the right number back in.", "timestamp": "00:22:41,875", "timestamp_s": 1361.0}, {"text": "Natural language back to the user.", "timestamp": "00:22:46,520", "timestamp_s": 1366.0}, {"text": "So this is another example that shows how it really shines to get instant.", "timestamp": "00:22:48,290", "timestamp_s": 1368.0}, {"text": "And this happens that can stand.", "timestamp": "00:22:54,030", "timestamp_s": 1374.0}, {"text": "So again, you\u0027re able to get instant insightful data", "timestamp": "00:22:55,470", "timestamp_s": 1375.0}, {"text": "while making use of the two.", "timestamp": "00:22:59,040", "timestamp_s": 1379.0}, {"text": "Now looking at the third example in the case, which we are", "timestamp": "00:23:00,660", "timestamp_s": 1380.0}, {"text": "unable to get any response back.", "timestamp": "00:23:04,260", "timestamp_s": 1384.0}, {"text": "So again, this is the user request or the user query.", "timestamp": "00:23:06,510", "timestamp_s": 1386.0}, {"text": "The user asks a request to the demo.", "timestamp": "00:23:11,335", "timestamp_s": 1391.0}, {"text": "With PI taken longer than usual.", "timestamp": "00:23:13,255", "timestamp_s": 1393.0}, {"text": "Then the AI agents generated this query.", "timestamp": "00:23:15,415", "timestamp_s": 1395.0}, {"text": "Now the query in terms of the ax is valid.", "timestamp": "00:23:20,385", "timestamp_s": 1400.0}, {"text": "There\u0027s nothing wrong with it.", "timestamp": "00:23:24,315", "timestamp_s": 1404.0}, {"text": "I returns and mt data set.", "timestamp": "00:23:25,420", "timestamp_s": 1405.0}, {"text": "And that\u0027s not because there is no data available for the demo API, but how?", "timestamp": "00:23:28,955", "timestamp_s": 1408.0}, {"text": "The LLM has interpreted the question and has tried to go about writing", "timestamp": "00:23:35,840", "timestamp_s": 1415.0}, {"text": "the queries incorrect, and as such, we get an empty data back.", "timestamp": "00:23:40,430", "timestamp_s": 1420.0}, {"text": "Although it\u0027s a valid ProQ query, it doesn\u0027t answer the", "timestamp": "00:23:44,720", "timestamp_s": 1424.0}, {"text": "question that the user has asked.", "timestamp": "00:23:49,500", "timestamp_s": 1429.0}, {"text": "And doesn\u0027t give us any response back.", "timestamp": "00:23:53,130", "timestamp_s": 1433.0}, {"text": "So that\u0027s one of the limitations that we\u0027ve discovered here.", "timestamp": "00:23:55,350", "timestamp_s": 1435.0}, {"text": "But again, looking at it there, where to get around this.", "timestamp": "00:23:59,400", "timestamp_s": 1439.0}, {"text": "So we\u0027ll discuss that in the next session.", "timestamp": "00:24:02,660", "timestamp_s": 1442.0}, {"text": "So in the next example, or as you can see here, so this is the", "timestamp": "00:24:04,790", "timestamp_s": 1444.0}, {"text": "prompt chart application itself.", "timestamp": "00:24:08,390", "timestamp_s": 1448.0}, {"text": "When you come here, you\u0027re able to modify the.", "timestamp": "00:24:10,430", "timestamp_s": 1450.0}, {"text": "LLM configurations where you can set what provider you want to use,", "timestamp": "00:24:15,220", "timestamp_s": 1455.0}, {"text": "what model you want to use as well.", "timestamp": "00:24:19,065", "timestamp_s": 1459.0}, {"text": "So in this case scenario is currently all the questions that we backed so far.", "timestamp": "00:24:21,795", "timestamp_s": 1461.0}, {"text": "We are using Google\u0027s Gemini model and then we are using the Google Gemini 1.5", "timestamp": "00:24:26,625", "timestamp_s": 1466.0}, {"text": "flash, which is their first free model.", "timestamp": "00:24:32,950", "timestamp_s": 1472.0}, {"text": "So that\u0027s what we have used so far to answer a, our questions.", "timestamp": "00:24:36,585", "timestamp_s": 1476.0}, {"text": "But we see that Gemini 1.5 in this example was unable to generate the", "timestamp": "00:24:40,195", "timestamp_s": 1480.0}, {"text": "correct prompt, clear query for us.", "timestamp": "00:24:45,175", "timestamp_s": 1485.0}, {"text": "So then by changing the model type to the thinking model.", "timestamp": "00:24:48,385", "timestamp_s": 1488.0}, {"text": "So if you check here based on the confirmation prompt chat", "timestamp": "00:24:52,255", "timestamp_s": 1492.0}, {"text": "so I flipped the model to.", "timestamp": "00:24:56,825", "timestamp_s": 1496.0}, {"text": "Use the thinking model provided by Google, which is the Gemini", "timestamp": "00:25:00,275", "timestamp_s": 1500.0}, {"text": "2.0 flash thinking model instead.", "timestamp": "00:25:03,425", "timestamp_s": 1503.0}, {"text": "So by making use of this model and asking exactly the same", "timestamp": "00:25:06,665", "timestamp_s": 1506.0}, {"text": "question, we are able to see that.", "timestamp": "00:25:09,905", "timestamp_s": 1509.0}, {"text": "Now if you look at this example, the AI agent is actually able to", "timestamp": "00:25:12,605", "timestamp_s": 1512.0}, {"text": "answer the question correctly.", "timestamp": "00:25:18,035", "timestamp_s": 1518.0}, {"text": "So we\u0027ll look at the backend and see what has changed.", "timestamp": "00:25:19,985", "timestamp_s": 1519.0}, {"text": "But in this case scenario, you basically ask the same question based on the newer.", "timestamp": "00:25:22,565", "timestamp_s": 1522.0}, {"text": "Thinking mod, he was able to provide this insight and say, oh, we can see", "timestamp": "00:25:28,235", "timestamp_s": 1528.0}, {"text": "that the slash API slash part seems to have a lot of 500 errors, and as", "timestamp": "00:25:32,855", "timestamp_s": 1532.0}, {"text": "such, latency is also significantly higher than the other part as well.", "timestamp": "00:25:38,875", "timestamp_s": 1538.0}, {"text": "Which suggest that the part has an issue.", "timestamp": "00:25:43,285", "timestamp_s": 1543.0}, {"text": "So as anr, you can imagine how insightful such a. And inside like", "timestamp": "00:25:47,245", "timestamp_s": 1547.0}, {"text": "this is when you are currently trying to debug an incident and you\u0027re", "timestamp": "00:25:54,800", "timestamp_s": 1554.0}, {"text": "trying to figure out what\u0027s going on.", "timestamp": "00:25:58,730", "timestamp_s": 1558.0}, {"text": "Maybe you get like a latency a lot or basically user start", "timestamp": "00:26:00,260", "timestamp_s": 1560.0}, {"text": "complain that your CM is slow.", "timestamp": "00:26:07,010", "timestamp_s": 1567.0}, {"text": "You can basically just.", "timestamp": "00:26:08,450", "timestamp_s": 1568.0}, {"text": "File the tool, ask and get insights for information like this.", "timestamp": "00:26:10,025", "timestamp_s": 1570.0}, {"text": "Now going back to the backend, we can see essentially what happened.", "timestamp": "00:26:14,475", "timestamp_s": 1574.0}, {"text": "So from the backend there, you would see pretty much the same thing, the same user", "timestamp": "00:26:18,435", "timestamp_s": 1578.0}, {"text": "request, but if you pay attention Yeah.", "timestamp": "00:26:23,385", "timestamp_s": 1583.0}, {"text": "To look at the.", "timestamp": "00:26:26,715", "timestamp_s": 1586.0}, {"text": "Generated prom ql.", "timestamp": "00:26:28,485", "timestamp_s": 1588.0}, {"text": "That this generated a different prom QL that is more appropriate", "timestamp": "00:26:30,045", "timestamp_s": 1590.0}, {"text": "is considering the rate now.", "timestamp": "00:26:33,975", "timestamp_s": 1593.0}, {"text": "And also based on the type of the data is able to use the Instagram counter.", "timestamp": "00:26:35,775", "timestamp_s": 1595.0}, {"text": "And as such, it was able to actually generate meaningful", "timestamp": "00:26:41,815", "timestamp_s": 1601.0}, {"text": "results this time around.", "timestamp": "00:26:45,805", "timestamp_s": 1605.0}, {"text": "So this is the full responses way much longer than this because there", "timestamp": "00:26:47,305", "timestamp_s": 1607.0}, {"text": "are a lot of metrics that match this.", "timestamp": "00:26:52,245", "timestamp_s": 1612.0}, {"text": "Essentially, based on all of this, you are able to get for each part you", "timestamp": "00:26:54,815", "timestamp_s": 1614.0}, {"text": "can get the status and then you can get the value, so based on that, the", "timestamp": "00:27:00,905", "timestamp_s": 1620.0}, {"text": "LLM was able to then go over all of this results similar to the first one.", "timestamp": "00:27:05,850", "timestamp_s": 1625.0}, {"text": "He was able to group them appropriately and then also identify the outlier", "timestamp": "00:27:11,860", "timestamp_s": 1631.0}, {"text": "in the list after the group.", "timestamp": "00:27:17,590", "timestamp_s": 1637.0}, {"text": "And as such was able to figure out that compared to the other", "timestamp": "00:27:19,120", "timestamp_s": 1639.0}, {"text": "guys, we can see that the slash a p slash par has 500 arrows.", "timestamp": "00:27:21,940", "timestamp_s": 1641.0}, {"text": "More, and then the viral latency as well is higher compared", "timestamp": "00:27:27,835", "timestamp_s": 1647.0}, {"text": "to the rest of the parts.", "timestamp": "00:27:32,485", "timestamp_s": 1652.0}, {"text": "So this shows basically how by changing the LLM model that is used, using a", "timestamp": "00:27:33,805", "timestamp_s": 1653.0}, {"text": "more powerful LM model, you\u0027re able to get a better result for queries that,", "timestamp": "00:27:41,525", "timestamp_s": 1661.0}, {"text": "or in cases where the simpler models were unable to write valid queries and", "timestamp": "00:27:47,585", "timestamp_s": 1667.0}, {"text": "compare the results that was written.", "timestamp": "00:27:52,805", "timestamp_s": 1672.0}, {"text": "So moving on to the fourth section where we basically just discuss a summary", "timestamp": "00:27:56,310", "timestamp_s": 1676.0}, {"text": "of everything that we have learned from interacting with Prompt Chat.", "timestamp": "00:28:00,300", "timestamp_s": 1680.0}, {"text": "So this is the first part basically talks about what are the lessons", "timestamp": "00:28:04,400", "timestamp_s": 1684.0}, {"text": "that we have learned, right?", "timestamp": "00:28:09,960", "timestamp_s": 1689.0}, {"text": "You would see throughout the, from the architecture and the rest", "timestamp": "00:28:11,580", "timestamp_s": 1691.0}, {"text": "of the presentation, there\u0027s no.", "timestamp": "00:28:14,820", "timestamp_s": 1694.0}, {"text": "Not at any point did we attempt to retrain any of the models", "timestamp": "00:28:17,220", "timestamp_s": 1697.0}, {"text": "or do any sort of fine tuning.", "timestamp": "00:28:21,240", "timestamp_s": 1701.0}, {"text": "So what that lets us know is that I\u0027m sure that they are LLM models", "timestamp": "00:28:22,950", "timestamp_s": 1702.0}, {"text": "today are actually capable of writing prompt QL queries on their own.", "timestamp": "00:28:27,540", "timestamp_s": 1707.0}, {"text": "The other thing to note as well is that, the only change we had to do actually", "timestamp": "00:28:33,570", "timestamp_s": 1713.0}, {"text": "was to use one shot prompting, which is basically adding an example to ensure", "timestamp": "00:28:39,240", "timestamp_s": 1719.0}, {"text": "that the output that we get from the LLM is formatted exactly how we want it.", "timestamp": "00:28:44,580", "timestamp_s": 1724.0}, {"text": "And that\u0027s important because you initially were on IT project.", "timestamp": "00:28:50,040", "timestamp_s": 1730.0}, {"text": "We are getting into issues where the response coming from the LLM", "timestamp": "00:28:54,720", "timestamp_s": 1734.0}, {"text": "model, it would add additional like characters or talking to it.", "timestamp": "00:28:59,250", "timestamp_s": 1739.0}, {"text": "And then once you pass it to.", "timestamp": "00:29:02,730", "timestamp_s": 1742.0}, {"text": "The Prometheus instance, it won\u0027t be valid.", "timestamp": "00:29:05,535", "timestamp_s": 1745.0}, {"text": "It would no longer be valid from cure, and that will lead to like", "timestamp": "00:29:09,845", "timestamp_s": 1749.0}, {"text": "crashes or issues because Prometheus cannot interpret the query.", "timestamp": "00:29:13,745", "timestamp_s": 1753.0}, {"text": "But after using one shot prompting where we are basically able to show.", "timestamp": "00:29:18,275", "timestamp_s": 1758.0}, {"text": "DNLM exactly the format of the response.", "timestamp": "00:29:23,435", "timestamp_s": 1763.0}, {"text": "He started returning exactly just the prompt QR required without any", "timestamp": "00:29:26,435", "timestamp_s": 1766.0}, {"text": "additional characters or tokens around it, and that we basically were", "timestamp": "00:29:31,415", "timestamp_s": 1771.0}, {"text": "able to avoid having to manually.", "timestamp": "00:29:35,915", "timestamp_s": 1775.0}, {"text": "Try to extract out the prom cure out of the LLM response.", "timestamp": "00:29:38,505", "timestamp_s": 1778.0}, {"text": "Based on the last example that you see as well would realize that obviously the", "timestamp": "00:29:43,255", "timestamp_s": 1783.0}, {"text": "thinking models are better when it comes to trying to write complicated ProQ.", "timestamp": "00:29:48,135", "timestamp_s": 1788.0}, {"text": "So the lighter models work.", "timestamp": "00:29:52,515", "timestamp_s": 1792.0}, {"text": "For most cases as well.", "timestamp": "00:29:55,515", "timestamp_s": 1795.0}, {"text": "But when you want to do, ask more completed questions, it\u0027s more useful to,", "timestamp": "00:29:57,165", "timestamp_s": 1797.0}, {"text": "the more powerful the models are and the more time you spend thinking, the better", "timestamp": "00:30:01,725", "timestamp_s": 1801.0}, {"text": "the prompt cure that they write as well.", "timestamp": "00:30:06,165", "timestamp_s": 1806.0}, {"text": "So we\u0027ve seen that again in the last example.", "timestamp": "00:30:08,835", "timestamp_s": 1808.0}, {"text": "So some of the limitations that we are so observed in the course of working on", "timestamp": "00:30:12,085", "timestamp_s": 1812.0}, {"text": "this project is that as you would see.", "timestamp": "00:30:16,345", "timestamp_s": 1816.0}, {"text": "The majority of being able to pull this off depends on", "timestamp": "00:30:19,255", "timestamp_s": 1819.0}, {"text": "the quality of documentation that you add to your metadata.", "timestamp": "00:30:23,365", "timestamp_s": 1823.0}, {"text": "So Promeus would always have the metadata, API available and it will tell you,", "timestamp": "00:30:26,875", "timestamp_s": 1826.0}, {"text": "okay, these are the metrics that I have.", "timestamp": "00:30:31,715", "timestamp_s": 1831.0}, {"text": "And then these are the types of those metrics.", "timestamp": "00:30:34,165", "timestamp_s": 1834.0}, {"text": "But if you don\u0027t add any help or information to interpret, for example,", "timestamp": "00:30:37,765", "timestamp_s": 1837.0}, {"text": "if, look at the case of when we had demo.", "timestamp": "00:30:41,965", "timestamp_s": 1841.0}, {"text": "Service is today all data metric.", "timestamp": "00:30:46,165", "timestamp_s": 1846.0}, {"text": "If the documentation do not contain information, saying A one means today\u0027s", "timestamp": "00:30:49,175", "timestamp_s": 1849.0}, {"text": "holiday, zero means today\u0027s an all day, then there would have not been any way for", "timestamp": "00:30:54,935", "timestamp_s": 1854.0}, {"text": "premature for the L to interpret correctly the premature response that I got.", "timestamp": "00:31:00,215", "timestamp_s": 1860.0}, {"text": "The other challenging bit as well is even for cases where", "timestamp": "00:31:06,155", "timestamp_s": 1866.0}, {"text": "you have the documentation.", "timestamp": "00:31:10,295", "timestamp_s": 1870.0}, {"text": "All the, all partex included as part of the documentation.", "timestamp": "00:31:13,710", "timestamp_s": 1873.0}, {"text": "Most of the time the labels are missing.", "timestamp": "00:31:16,770", "timestamp_s": 1876.0}, {"text": "Labels in this case are like, if you compare it to traditional databases,", "timestamp": "00:31:18,900", "timestamp_s": 1878.0}, {"text": "so maybe like the colons or the fields.", "timestamp": "00:31:23,250", "timestamp_s": 1883.0}, {"text": "So because you don\u0027t know what fields or what labels are available in your,", "timestamp": "00:31:26,490", "timestamp_s": 1886.0}, {"text": "in that particular matrix, it makes it harder for DLLM to be able to write.", "timestamp": "00:31:32,280", "timestamp_s": 1892.0}, {"text": "Correct queries, especially when you need to filter by things", "timestamp": "00:31:38,580", "timestamp_s": 1898.0}, {"text": "like the actual bill value.", "timestamp": "00:31:42,120", "timestamp_s": 1902.0}, {"text": "So that\u0027s one of the things where, again, that can easily be solved", "timestamp": "00:31:45,280", "timestamp_s": 1905.0}, {"text": "as is essentially going over the documentation and adding as much", "timestamp": "00:31:48,460", "timestamp_s": 1908.0}, {"text": "useful information there as possible.", "timestamp": "00:31:52,540", "timestamp_s": 1912.0}, {"text": "The other limitation notice as well is that sometimes you might feel you might", "timestamp": "00:31:55,300", "timestamp_s": 1915.0}, {"text": "having consistency in the results that you get because the query is generated.", "timestamp": "00:32:00,160", "timestamp_s": 1920.0}, {"text": "Differ define slightly.", "timestamp": "00:32:05,440", "timestamp_s": 1925.0}, {"text": "And that\u0027s important because if you frame your question slightly", "timestamp": "00:32:06,760", "timestamp_s": 1926.0}, {"text": "differently, then the LLM can interpret it differently and generate", "timestamp": "00:32:10,960", "timestamp_s": 1930.0}, {"text": "a correspondent different query, which would then give you a different result.", "timestamp": "00:32:15,400", "timestamp_s": 1935.0}, {"text": "But that can be eliminated by having more exact descriptions in your question.", "timestamp": "00:32:19,450", "timestamp_s": 1939.0}, {"text": "So for example, you\u0027re saying a, is there any endpoint currently between 500?", "timestamp": "00:32:25,650", "timestamp_s": 1945.0}, {"text": "If you don\u0027t put, a timeframe the first time, maybe you might", "timestamp": "00:32:31,910", "timestamp_s": 1951.0}, {"text": "do it over an hour or like maybe five minutes or maybe one minute.", "timestamp": "00:32:34,640", "timestamp_s": 1954.0}, {"text": "But if for example, you ask specifically with the timeframe, then generated", "timestamp": "00:32:38,540", "timestamp_s": 1958.0}, {"text": "prompt cure by the AI agents would contain that exact timeframe and", "timestamp": "00:32:43,070", "timestamp_s": 1963.0}, {"text": "then you get the same response back.", "timestamp": "00:32:47,600", "timestamp_s": 1967.0}, {"text": "That\u0027s another limitation.", "timestamp": "00:32:49,680", "timestamp_s": 1969.0}, {"text": "But again, how that can be improved upon is basically just puts in.", "timestamp": "00:32:51,120", "timestamp_s": 1971.0}, {"text": "More, the more exact your question is, the better the answers that you get.", "timestamp": "00:32:55,285", "timestamp_s": 1975.0}, {"text": "So in terms of the future improvement so better support for complex queries.", "timestamp": "00:33:00,235", "timestamp_s": 1980.0}, {"text": "So this better support includes things like being able to undo more completed", "timestamp": "00:33:05,085", "timestamp_s": 1985.0}, {"text": "queries, being able to even supply.", "timestamp": "00:33:10,005", "timestamp_s": 1990.0}, {"text": "Right now all the answers are coming back in texts in natural language, but of", "timestamp": "00:33:14,075", "timestamp_s": 1994.0}, {"text": "course it might be useful to maybe have a graph to look at from time to time.", "timestamp": "00:33:18,245", "timestamp_s": 1998.0}, {"text": "So for more competitive queries, for example, it might be useful to return", "timestamp": "00:33:22,355", "timestamp_s": 2002.0}, {"text": "both the natural language, but also some form of visualization for it.", "timestamp": "00:33:26,795", "timestamp_s": 2006.0}, {"text": "Also, right now the project is limited to just Prometheus, so", "timestamp": "00:33:31,775", "timestamp_s": 2011.0}, {"text": "that\u0027s the only matrix source that.", "timestamp": "00:33:36,005", "timestamp_s": 2016.0}, {"text": "It works with.", "timestamp": "00:33:38,360", "timestamp_s": 2018.0}, {"text": "So in terms of next steps, you\u0027re looking at expanding the project", "timestamp": "00:33:39,140", "timestamp_s": 2019.0}, {"text": "such that it supports more than just Prometheus as the source.", "timestamp": "00:33:42,560", "timestamp_s": 2022.0}, {"text": "And then lastly, I think another interesting improvement would", "timestamp": "00:33:46,640", "timestamp_s": 2026.0}, {"text": "be around the system being able to learn from user interactions.", "timestamp": "00:33:50,420", "timestamp_s": 2030.0}, {"text": "So imagine you ask a question and maybe for example, the, you didn\u0027t use the", "timestamp": "00:33:53,800", "timestamp_s": 2033.0}, {"text": "right labels, you and you, or it doesn\u0027t know the labels are available for that.", "timestamp": "00:33:59,350", "timestamp_s": 2039.0}, {"text": "Particular metrics.", "timestamp": "00:34:05,565", "timestamp_s": 2045.0}, {"text": "So he asks that, okay, I can answer this, right?", "timestamp": "00:34:06,465", "timestamp_s": 2046.0}, {"text": "And then you provide those labels.", "timestamp": "00:34:09,675", "timestamp_s": 2049.0}, {"text": "Now it would be useful if the, right now there is no memory in the system, so he", "timestamp": "00:34:11,835", "timestamp_s": 2051.0}, {"text": "actually doesn\u0027t store the information.", "timestamp": "00:34:17,115", "timestamp_s": 2057.0}, {"text": "So an extension would be such that next time you actually don\u0027t have to", "timestamp": "00:34:19,395", "timestamp_s": 2059.0}, {"text": "go back and get supply the same labels back to the system for you to answer.", "timestamp": "00:34:22,905", "timestamp_s": 2062.0}, {"text": "Correctly.", "timestamp": "00:34:29,000", "timestamp_s": 2069.0}, {"text": "So that\u0027s like a future improvement when it comes to, it\u0027s actually", "timestamp": "00:34:30,030", "timestamp_s": 2070.0}, {"text": "learning from user interaction.", "timestamp": "00:34:33,720", "timestamp_s": 2073.0}, {"text": "Another way that can go is when queries, for example, let\u0027s say the queries", "timestamp": "00:34:35,490", "timestamp_s": 2075.0}, {"text": "are wrong or like the wrong metrics was used and you correct it again,", "timestamp": "00:34:40,730", "timestamp_s": 2080.0}, {"text": "all those kind of interactions can be stored such that his user\u0027s context.", "timestamp": "00:34:46,415", "timestamp_s": 2086.0}, {"text": "Next time he\u0027s trying to answer questions, and as such, he can make use.", "timestamp": "00:34:51,065", "timestamp_s": 2091.0}, {"text": "Of that, and then the system gets better over time because it\u0027s", "timestamp": "00:34:55,905", "timestamp_s": 2095.0}, {"text": "learning from user interactions.", "timestamp": "00:34:59,265", "timestamp_s": 2099.0}, {"text": "So lastly, how can you contribute and join?", "timestamp": "00:35:01,615", "timestamp_s": 2101.0}, {"text": "The source code is available on our GitHub profile.", "timestamp": "00:35:03,685", "timestamp_s": 2103.0}, {"text": "So it\u0027s open the projects open source.", "timestamp": "00:35:07,569", "timestamp_s": 2107.0}, {"text": "If you\u0027ve go to next I HQ on GitHub you see the source code for the prom chat app.", "timestamp": "00:35:09,369", "timestamp_s": 2109.0}, {"text": "So issues pls are welcomed to, as a means of contributing to the project.", "timestamp": "00:35:14,899", "timestamp_s": 2114.0}, {"text": "Also the web interface that I was playing with or that I was shown in the sites", "timestamp": "00:35:21,744", "timestamp_s": 2121.0}, {"text": "is available from chat do co so you can basically visit that is available.", "timestamp": "00:35:26,424", "timestamp_s": 2126.0}, {"text": "You don\u0027t need like an identification payment or anything.", "timestamp": "00:35:33,124", "timestamp_s": 2133.0}, {"text": "The only issues might be because it\u0027s using my own personal, aPI key.", "timestamp": "00:35:36,484", "timestamp_s": 2136.0}, {"text": "So there are limits to the number of like daily requests or sometimes maybe", "timestamp": "00:35:41,294", "timestamp_s": 2141.0}, {"text": "even a lot of people have been playing around with it earlier in the day.", "timestamp": "00:35:45,534", "timestamp_s": 2145.0}, {"text": "You might get, you might not be able to get any responses back", "timestamp": "00:35:48,874", "timestamp_s": 2148.0}, {"text": "from the API it would tell you that the LLM credits is exhausted.", "timestamp": "00:35:54,344", "timestamp_s": 2154.0}, {"text": "But yeah.", "timestamp": "00:35:58,874", "timestamp_s": 2158.0}, {"text": "But you can clone the project locally.", "timestamp": "00:35:59,774", "timestamp_s": 2159.0}, {"text": "Put your own EPI keys and run it if.", "timestamp": "00:36:01,844", "timestamp_s": 2161.0}, {"text": "You want to, or if you want to use the web interface, you can visit from", "timestamp": "00:36:04,774", "timestamp_s": 2164.0}, {"text": "chat, do nest slide.co as well, and you would be able to interact with it.", "timestamp": "00:36:08,459", "timestamp_s": 2168.0}, {"text": "So that\u0027s it.", "timestamp": "00:36:13,989", "timestamp_s": 2173.0}, {"text": "That\u0027s it from me.", "timestamp": "00:36:15,939", "timestamp_s": 2175.0}, {"text": "So thank you very much for listening to the session.", "timestamp": "00:36:16,899", "timestamp_s": 2176.0}, {"text": "I hope you\u0027ve learned a lot and you have a better understanding.", "timestamp": "00:36:19,869", "timestamp_s": 2179.0}, {"text": "As regards how to implement something like this.", "timestamp": "00:36:24,819", "timestamp_s": 2184.0}, {"text": "And yes, the answer to the question is it is possible to", "timestamp": "00:36:27,399", "timestamp_s": 2187.0}, {"text": "chat with your mandatory metrics.", "timestamp": "00:36:31,569", "timestamp_s": 2191.0}, {"text": "And I do hope you enjoy the rest of the conference, but I thank you.", "timestamp": "00:36:33,789", "timestamp_s": 2193.0}, {"text": "I.", "timestamp": "00:36:40,214", "timestamp_s": 2200.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'DuWBp5KbfYM',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              AI in SRE: Unlocking Prometheus Insights with Natural Language
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>While handling an incident, have you ever wished you could simply ask questions about the state of your systems and get immediate, actionable answers? I know I have.</p>
<p>As SREs, we know every second counts during an incident. So, what if we could skip over the complex queries and multiple dashboards to quick insights?  I set out to answer the simple question: “What if you could chat with your monitoring metrics?” In this presentation, I will be sharing what I learnt working building an open source project that allows you to do just that!</p>
<!-- End Text -->
          </div>

          
          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./srt/sre2025_David_Asamu.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:00,985'); seek(0.0)">
              Hi, I am David.
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:02,250'); seek(2.0)">
              Welcome to this session, AI in SRE, unlocking permits
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:06,390'); seek(6.0)">
              Insights with Natural Language.
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:08,580'); seek(8.0)">
              In this session, I'll be discussing a project I worked on that try to
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:13,020'); seek(13.0)">
              answer the question, is it possible to chat with your mandatory metrics?
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:16,710'); seek(16.0)">
              I'm excited to share what I've learned and I ask her if pay
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:20,100'); seek(20.0)">
              attention as we go through it.
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:22,300'); seek(22.0)">
              So the talk today is divided into five sections, we do
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:25,200'); seek(25.0)">
              like a general introduction.
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:26,820'); seek(26.0)">
              We look at the approach by which the tool was built.
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:00:30,090'); seek(30.0)">
              The tool itself is com called Prompt Chart, so we'll spend some
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:00:33,930'); seek(33.0)">
              time reviewing the tool itself.
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:00:36,540'); seek(36.0)">
              Lastly, we'll discuss the result and then a closing out section.
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:00:41,430'); seek(41.0)">
              Okay, moving straight on to the first section.
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:00:44,250'); seek(44.0)">
              Introduction was a problem model solution.
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:00:47,080'); seek(47.0)">
              So as SRE, we understand the importance of speed when it comes
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:00:50,590'); seek(50.0)">
              to incident, and if you have a tool that allows you to be able to chat
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:00:54,340'); seek(54.0)">
              with your metrics, it can speed up.
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:00:56,110'); seek(56.0)">
              Your incident responds significantly.
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:00:59,090'); seek(59.0)">
              For me, I think there are two ways to think about it.
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:00,920'); seek(60.0)">
              The first is that it reduces cognitive load while responding to an incident where
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:05,660'); seek(65.0)">
              you know that there is pressure to restore user experience as fast as possible.
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:10,610'); seek(70.0)">
              So having a tool that.
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:12,150'); seek(72.0)">
              Helps you write prompt care without you having to think about it, reduces the
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:15,660'); seek(75.0)">
              cognitive load required with responding to incident on the other side as well.
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:20,370'); seek(80.0)">
              You're able to get quick insight without having to go through multiple dashboards,
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:01:24,210'); seek(84.0)">
              which is another way in which it can lead to faster incident response.
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:01:28,560'); seek(88.0)">
              The order usefulness of having a tool like this is that it makes
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:01:32,670'); seek(92.0)">
              data accessible to everybody.
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:01:34,910'); seek(94.0)">
              Traditionally to expose data from your Es.
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:01:39,340'); seek(99.0)">
              Instances, you'll build dashboards for people that are not capable of
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:01:43,540'); seek(103.0)">
              writing like their own prom qls.
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:01:46,150'); seek(106.0)">
              So you would build graph dashboard on things like graph, which they can go.
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:01:50,770'); seek(110.0)">
              But of course that means for each type of.
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:01:53,770'); seek(113.0)">
              Data, or each question that they have, you need to create the
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:01:56,200'); seek(116.0)">
              dashboard, which they can go and see.
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:01:58,420'); seek(118.0)">
              However, if they have a tool that they can basically ask questions,
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:02,740'); seek(122.0)">
              then you don't need to keep coming up with the new dashboard.
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:06,520'); seek(126.0)">
              Each time that there's a new query and a new question, they can basically
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:10,300'); seek(130.0)">
              just interact with the tool and get.
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:13,405'); seek(133.0)">
              It start answers back.
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:02:15,335'); seek(135.0)">
              The other advantage of having a tool like this is that it uses
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:02:19,415'); seek(139.0)">
              the learning curve for Prometheus.
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:02:21,405'); seek(141.0)">
              Starting up it, it's easy to write simple promeus queries as you need
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:02:26,385'); seek(146.0)">
              to get, write more completed queries.
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:02:28,755'); seek(148.0)">
              Things can get a bit harder, especially we are just learning d. Language.
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:02:34,395'); seek(154.0)">
              So having something like a tool like this that can essentially cooperate
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:02:39,515'); seek(159.0)">
              with you, like copilot when you're trying to write ProQ is good 'cause
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:02:43,384'); seek(163.0)">
              it helps you, is learning cover, especially via getting started.
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:02:47,764'); seek(167.0)">
              And then I think one of the major.
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:02:49,684'); seek(169.0)">
              Motivations for me to actually work on the project is I see a lot of projects
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:02:55,164'); seek(175.0)">
              in the data space that allows you to be able to chat with your data.
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:02:58,885'); seek(178.0)">
              So it terms Hey, ask questions about your database and things like that.
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:03,055'); seek(183.0)">
              So I started wondering, is it possible to do the same thing for metrics,
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:07,584'); seek(187.0)">
              especially in the monitoring sense.
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:09,774'); seek(189.0)">
              So that's like the motivation in trying to solve the problem and answering the.
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:15,640'); seek(195.0)">
              Question, is it possible to chat with your monitoring metrics?
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:03:19,280'); seek(199.0)">
              So the solution that came up with that is discussed in this project works this way.
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:03:25,880'); seek(205.0)">
              So the flow is the u We are building it too, that allows users to essentially ask
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:03:31,220'); seek(211.0)">
              questions in natural language and then.
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:03:34,340'); seek(214.0)">
              The AI agents take the natural language and generates a
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:03:37,850'); seek(217.0)">
              corresponding prompt QL query.
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:03:40,760'); seek(220.0)">
              The prompt QL query is then run on Prometheus to get the actual results
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:03:45,380'); seek(225.0)">
              back from Prometheus, but then that result before it gets back to the user
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:03:49,610'); seek(229.0)">
              is converted back into natural language.
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:03:52,340'); seek(232.0)">
              So the flow is that it starts with natural language and it
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:03:55,610'); seek(235.0)">
              ends with natural language.
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:03:56,900'); seek(236.0)">
              The user asks questions in natural language, and then they
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:03:59,960'); seek(239.0)">
              get a result presented back to them in natural language.
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:04,400'); seek(244.0)">
              So the second section of the talk talks about the approach.
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:07,550'); seek(247.0)">
              At this point, we'll dive deep into what.
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:11,210'); seek(251.0)">
              How we implemented it the architecture, and then we review a couple of the
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:04:15,179'); seek(255.0)">
              key components of the architecture.
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:04:17,649'); seek(257.0)">
              So this diagram shows architecture for the system.
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:04:21,339'); seek(261.0)">
              So on the far left side of it, we have the front end, which is
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:04:24,640'); seek(264.0)">
              how you interact with the system.
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:04:26,800'); seek(266.0)">
              So it is either you interacting with the tool via a web app or a
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:04:30,760'); seek(270.0)">
              Slack app basically, which allows you to enter your user query.
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:04:36,109'); seek(276.0)">
              And then that user query gets sent to a backend.
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:04:39,319'); seek(279.0)">
              The backend has rest API that makes it easy for the front end to interact
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:04:44,679'); seek(284.0)">
              and send questions down to it.
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:04:47,419'); seek(287.0)">
              As part of the backend as well, we have the AI agent.
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:04:50,419'); seek(290.0)">
              The AI agent basically handles the coordination with the LLM.
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:04:55,249'); seek(295.0)">
              So when the user query comes, it basically sends the request to the LLM and then
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:00,679'); seek(300.0)">
              the LLM as well uses what is called.
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:03,974'); seek(303.0)">
              Two, calling a function, callings to be able to talk to Prometheus.
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:07,844'); seek(307.0)">
              So on the Prometheus aspect, we need to be able to fetch metadata from Prometheus
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:12,584'); seek(312.0)">
              and run queries from Prometheus as well.
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:05:16,394'); seek(316.0)">
              As we go later into the talk, we'll talk more about the metadata such that you
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:05:20,684'); seek(320.0)">
              have better understanding of what entails.
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:05:23,139'); seek(323.0)">
              But this gives.
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:05:25,304'); seek(325.0)">
              I an eye level overview of what happens when you're trying to answer questions.
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:05:30,074'); seek(330.0)">
              So the flow is, the request comes in, let's say from the
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:05:32,804'); seek(332.0)">
              web app, it goes to the backend.
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:05:35,174'); seek(335.0)">
              The backend takes the question, sends it to the AI agent.
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:05:39,194'); seek(339.0)">
              The AI agent takes the question, sends it to the LLM, so the user
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:05:44,384'); seek(344.0)">
              query together with the metadata.
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:05:45,824'); seek(345.0)">
              So the metadata in this case is you can think of it like the scheme of your data.
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:05:50,294'); seek(350.0)">
              So it basically just contains information regarding.
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:05:53,909'); seek(353.0)">
              What kind of metrics are available in your Promeus instance and
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:05:57,629'); seek(357.0)">
              the descriptions for them.
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:05:58,799'); seek(358.0)">
              So you send that together with the user query to the LLM.
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:02,819'); seek(362.0)">
              The LLM then thinks about a prompt QL that would be able to
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:06:07,409'); seek(367.0)">
              answer that question correctly.
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:06:09,719'); seek(369.0)">
              Then when they ask the prompt, QL, send it back to the AI agent, which then runs
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:06:15,599'); seek(375.0)">
              that query on Prometheus to get the.
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:06:20,069'); seek(380.0)">
              Actual result.
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:06:21,090'); seek(381.0)">
              And then once the result comes back from Prometheus, it sparks back to the
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:06:25,079'); seek(385.0)">
              LLM such that the LLM can interpret the result in natural language.
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:06:29,999'); seek(389.0)">
              And then the result goes all the way back to the front end for the user to see.
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:06:34,659'); seek(394.0)">
              So that's what the.
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:06:36,219'); seek(396.0)">
              Overview of the system.
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:06:37,389'); seek(397.0)">
              Looks like we'll be looking at a couple of tools that allows, or the key
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:06:41,319'); seek(401.0)">
              components of the architecture as well.
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:06:43,669'); seek(403.0)">
              So the first one is Prometheus.
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:06:45,649'); seek(405.0)">
              Again, if you're not familiar with Prometheus I won't spend too much time
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:06:50,019'); seek(410.0)">
              diving into it, but you can check it out.
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:06:51,969'); seek(411.0)">
              What you need to know is that it's a time series data store.
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:06:55,359'); seek(415.0)">
              For Matrix and typically ma matrix stored in material have these
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:07:01,189'); seek(421.0)">
              formats where you have the matrix name and then you have the label.
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:07:04,879'); seek(424.0)">
              So in this case, the labels are method and handler, and then they have the
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:07:09,049'); seek(429.0)">
              corresponding value of post and messages.
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:07:11,209'); seek(431.0)">
              So this is an entry of Prometheus of a metric in Prometheus.
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:07:17,319'); seek(437.0)">
              So this metric is then script by Prometheus.
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:07:20,424'); seek(440.0)">
              Quite often based on whatever script interval you have.
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:07:23,534'); seek(443.0)">
              As the time series data is going to recall the time that it was script
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:07:26,984'); seek(446.0)">
              and then the value at that point in time, and that's how this part works.
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:07:32,604'); seek(452.0)">
              So part of the things that make this architecture works is that premises
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:07:36,804'); seek(456.0)">
              as this metadata API that allows you to get, again, as I described
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:07:41,274'); seek(461.0)">
              earlier, like a scheme of your data.
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:07:43,914'); seek(463.0)">
              In the protal systems.
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:07:45,654'); seek(465.0)">
              So what that entails is that it shows you the list of all the metrics that are
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:07:49,674'); seek(469.0)">
              available inside of your protal system.
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:07:53,669'); seek(473.0)">
              So in our case, for example, you can see C device or this device usage to CPU
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:07:59,074'); seek(479.0)">
              load average superior U system sequence.
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:08:01,804'); seek(481.0)">
              These are examples of metrics that are being scripted by Prometheus and is
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:08:06,994'); seek(486.0)">
              available in the Prometheus instance.
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:08:09,634'); seek(489.0)">
              So that means you can write queries or ask questions regarding this metrics.
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:08:14,449'); seek(494.0)">
              But not only is the name return, they also returns information,
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:08:18,379'); seek(498.0)">
              helpful information about them, which is the type of the metric itself
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:08:22,619'); seek(502.0)">
              based on the type of the metric.
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:08:24,089'); seek(504.0)">
              Again, it depends the kind of operations that you can perform on The metric
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:08:29,199'); seek(509.0)">
              depends on the type of the metric, right?
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:08:31,059'); seek(511.0)">
              So that's something to take up that needs to be taken into account
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:08:34,734'); seek(514.0)">
              while writing your Pro Creole.
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:08:36,554'); seek(516.0)">
              And then the help also gives information, additional
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:08:40,359'); seek(520.0)">
              information regarding each type.
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:08:42,879'); seek(522.0)">
              Of metric, which allows you to be able to interpret the metric correctly.
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:08:48,360'); seek(528.0)">
              Additionally, sometimes you can also add information about the labels.
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:08:52,170'); seek(532.0)">
              So for example, for this 80 TTP request total, the description can be
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:08:56,790'); seek(536.0)">
              something like, oh, you let you know the number of 80 TP requests that
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:08:59,706'); seek(539.0)">
              has happened within a time period.
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:09:02,565'); seek(542.0)">
              And he has following label like method.
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:09:04,885'); seek(544.0)">
              And so this metadata is the, additional bits that is sent as context along
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:09:13,320'); seek(553.0)">
              with the user query itself to the LLM.
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:09:17,190'); seek(557.0)">
              So that's the first major component.
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:09:19,230'); seek(559.0)">
              The second major component, as we explained, is the LLM.
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:09:22,920'); seek(562.0)">
              And then he makes use of a. Technical tools calling or function calling, which
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:09:28,925'); seek(568.0)">
              is basically he extends the LLM such that it's able to interact with the environment
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:09:34,805'); seek(574.0)">
              that is outside of the LLM itself using tools and to make that happen,
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:09:39,005'); seek(579.0)">
              we have built two tools in our example.
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:09:42,275'); seek(582.0)">
              So we have the query promises tool, which basically allows the LLM to.
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:09:47,350'); seek(587.0)">
              Execute Prometheus query on an instance and get response back before
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:09:52,820'); seek(592.0)">
              returning a response back to the user.
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:09:55,980'); seek(595.0)">
              The other tool also allows the LLM to be able to query Prometheus for the metadata
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:10:02,640'); seek(602.0)">
              information similar to the metadata information shown in the previous.
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:10:07,560'); seek(607.0)">
              In the previous page here.
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:10:09,030'); seek(609.0)">
              So basically the first two allows Prometheus to be able to run it
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:10:13,740'); seek(613.0)">
              degenerated from Q and get responses back from Prometheus while the second
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:10:18,300'); seek(618.0)">
              two allows Prometheus to be able to fetch metadata results from Prometheus itself.
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:10:26,250'); seek(626.0)">
              Metadata regarding the metrics available.
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:10:30,220'); seek(630.0)">
              So that's the two major.
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:10:32,260'); seek(632.0)">
              Component in our architecture.
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:10:34,390'); seek(634.0)">
              So moving on to the third part, which is we are looking at the tool that was built.
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:10:40,010'); seek(640.0)">
              So the tool itself is called Prompt chart, and basically we'll be exploring
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:10:44,180'); seek(644.0)">
              the tool from two points of view.
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:10:46,260'); seek(646.0)">
              So based on the data inside of the Prometheus instance, we've
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:10:51,570'); seek(651.0)">
              classified it into two examples.
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:10:54,370'); seek(654.0)">
              The first one uses data from no exporter, and then the second.
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:10:59,620'); seek(659.0)">
              Example of pro chats uses data from custom exporter to Promeus.
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:11:05,570'); seek(665.0)">
              So both of them are chat.
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:11:07,460'); seek(667.0)">
              You're chatting with Promeus.
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:11:08,630'); seek(668.0)">
              So what's just different is the source of data in the Promeus instance and
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:11:13,550'); seek(673.0)">
              which basically affects the type of queries that you can, or questions you
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:11:17,360'); seek(677.0)">
              can ask, and the type of queries that you, that will be generated as well.
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:11:22,820'); seek(682.0)">
              A little bit more about the node exporter.
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:11:24,560'); seek(684.0)">
              So the node exporter is essentially.
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:11:27,065'); seek(687.0)">
              Like a plugin that you can run on a vm.
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:11:30,155'); seek(690.0)">
              So you can run it on a single vm, you can run it on your euca machine, for example.
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:11:36,165'); seek(696.0)">
              Pretty much any like machine VM node, you can run it.
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:11:39,675'); seek(699.0)">
              So what does is you expose system metrics to permit your such a protus
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:11:44,475'); seek(704.0)">
              scan, script them at intervals.
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:11:46,425'); seek(706.0)">
              So for example, things like the CPU seconds, the available storage.
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:11:52,520'); seek(712.0)">
              Memory utilization, network traffic, all of that can be
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:11:55,910'); seek(715.0)">
              exposed by the node exporter.
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:11:59,360'); seek(719.0)">
              So there are standard system metrics and you don't need to configure anything.
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:12:02,780'); seek(722.0)">
              The node exporter basically just makes those available and then those
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:12:06,650'); seek(726.0)">
              data can be injected into premises.
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:12:09,890'); seek(729.0)">
              So these are sample of some of the metrics that are available.
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:12:13,740'); seek(733.0)">
              So as you can see, note CPU seconds to the the system available.
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:12:19,330'); seek(739.0)">
              Five system available in bys.
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:12:21,850'); seek(741.0)">
              And then this is on the number of network requests in bys that
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:12:28,190'); seek(748.0)">
              have been received as well.
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:12:29,900'); seek(749.0)">
              The average network traffic invites over the last minute.
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:12:32,960'); seek(752.0)">
              So examples of the metrics and then some of the cray so you can
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:12:36,620'); seek(756.0)">
              see, using like rates to get like averages over a time period of time.
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:12:42,050'); seek(762.0)">
              So this is what a Prometheus query looks like as well.
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:12:47,420'); seek(767.0)">
              So moving on.
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:12:48,720'); seek(768.0)">
              So this is showing the actual tool called pro chart and then responses
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:12:55,450'); seek(775.0)">
              that was generated by the system when it was asked certain questions.
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:12:58,570'); seek(778.0)">
              So in this example, the prompt chart is connected to the
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:13:01,570'); seek(781.0)">
              premises instance that has data.
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:13:04,490'); seek(784.0)">
              From a note exporter.
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:13:06,710'); seek(786.0)">
              So the information available or the kind of questions you can ask is
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:13:11,000'); seek(791.0)">
              based on the kind of metrics that is expected by the note exporter.
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:13:14,570'); seek(794.0)">
              So in this case we can see four exchanges or five questions into the, actually, so
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:13:21,030'); seek(801.0)">
              the first talks about DCP utilization.
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:13:24,260'); seek(804.0)">
              So this is a question.
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:13:25,340'); seek(805.0)">
              So in black here, you have the user question itself.
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:13:29,390'); seek(809.0)">
              While in white it is a response from the.
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:13:32,705'); seek(812.0)">
              From chart AI system is itself.
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:13:35,375'); seek(815.0)">
              So it looks first asking about the currency periodization on the node.
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:13:39,545'); seek(819.0)">
              We get this.
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:13:40,265'); seek(820.0)">
              He asked about the battery percentage on the node more around information
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:13:44,385'); seek(824.0)">
              always running on the node.
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:13:45,495'); seek(825.0)">
              So I did run this particular example on my laptop using the macros node
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:13:51,545'); seek(831.0)">
              exporter, so you can see gets that.
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:13:54,495'); seek(834.0)">
              The node is running macros, the digs.
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:13:57,510'); seek(837.0)">
              Space that is available across all the instances of the nodes.
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:14:00,840'); seek(840.0)">
              And then finally the memory utilization of the node is shown as well.
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:14:08,190'); seek(848.0)">
              Alright, so we'll look at, in, in follow up, would look at later,
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:14:12,820'); seek(852.0)">
              would actually see from the backend work queries are generated for these
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:14:17,470'); seek(857.0)">
              examples and what they look like.
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:14:19,520'); seek(859.0)">
              The second Prometheus instance, I'll be using.
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:14:24,110'); seek(864.0)">
              Is thanks to our, the friends@promlabs.com.
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:14:28,790'); seek(868.0)">
              So they have this Prometheus instance that is publicly accessible.
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:14:32,880'); seek(872.0)">
              So is from the team that wrote Prometheus and they make it available
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:14:38,400'); seek(878.0)">
              so that you can use it to experiment, to learn and interact with Prometheus.
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:14:43,260'); seek(883.0)">
              On that Promis instance, we have some custom metrics.
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:14:46,530'); seek(886.0)">
              So if you look at the first set of metrics that was exposed by, in our first example
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:14:52,250'); seek(892.0)">
              using the node exporter, those metrics are system metrics, meaning that there are
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:14:56,960'); seek(896.0)">
              metrics from regarding the machine itself.
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:15:00,440'); seek(900.0)">
              But more often than not, when we are trying to set up monitoring,
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:15:03,530'); seek(903.0)">
              we want to collect metrics about the state of our service or the
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:15:06,740'); seek(906.0)">
              application we are running as well.
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:15:08,600'); seek(908.0)">
              So in those cases.
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:15:09,945'); seek(909.0)">
              You would come up with like your own custom metrics describing the different
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:15:15,395'); seek(915.0)">
              states or events in your service.
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:15:18,515'); seek(918.0)">
              So the Prometheus on prom lab, the demo Prometheus on prom lab.
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:15:23,615'); seek(923.0)">
              So he runs a service called the demo service, and then the demo
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:15:27,005'); seek(927.0)">
              service exposes the following.
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:15:30,055'); seek(930.0)">
              Custom metrics which is what we based in a set of questions and
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:15:34,835'); seek(934.0)">
              interactions with from chat on.
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:15:37,305'); seek(937.0)">
              So a couple of interesting ones that you see is one is that there are batch jobs.
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:15:42,165'); seek(942.0)">
              So you can essentially ask questions around success rate of batch jobs
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:15:46,765'); seek(946.0)">
              that was done by the demo service.
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:15:49,075'); seek(949.0)">
              Can see questions around the HCTP request duration as well.
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:15:53,785'); seek(953.0)">
              Another interesting one is that the service itself as a matrix lets you
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:15:59,185'); seek(959.0)">
              know whether today is an holiday or not.
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:16:01,195'); seek(961.0)">
              So it's basically called demo is holiday and from the help information.
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:16:06,295'); seek(966.0)">
              So again, if you look at the metadata information, this is GUI, not from
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:16:10,945'); seek(970.0)">
              the API, but basically it's going to be similar to the metadata.
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:16:16,625'); seek(976.0)">
              EPI response showed earlier where you can see the name of the metric.
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:16:21,135'); seek(981.0)">
              The type of it, which is in this case for his early day.
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:16:24,545'); seek(984.0)">
              We can see the type of set as gauge, and then the description takes that when
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:16:29,345'); seek(989.0)">
              the value return is one, it means that the current days and early day, and when
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:16:33,695'); seek(993.0)">
              the value rate return is zero, it means that the current days one early days.
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:16:36,965'); seek(996.0)">
              So these are examples of custom metrics that has been made available
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:16:41,345'); seek(1001.0)">
              by the demo service on this instance.
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:16:43,715'); seek(1003.0)">
              So all of this.
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:16:45,335'); seek(1005.0)">
              Metrics has been script and is available inside of the Promeus instance.
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:16:49,625'); seek(1009.0)">
              So that means we can connect our pro chart tool to this Prometheus instance and ask
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:16:54,665'); seek(1014.0)">
              it questions and let the AI generate the corresponding queries and shows answers.
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:17:01,025'); seek(1021.0)">
              So let's see how that looks like.
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:17:04,745'); seek(1024.0)">
              So in the next chat here we can see is, the prom chart two is connected to
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:17:10,505'); seek(1030.0)">
              the prom labs protal instance, and the first question here is today an holiday?
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:17:15,495'); seek(1035.0)">
              So the AI system response are based on metrics that I was able
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:17:19,215'); seek(1039.0)">
              to find from demo service Two.
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:17:21,165'); seek(1041.0)">
              It reports that today is an holiday.
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:17:23,325'); seek(1043.0)">
              Again, just in the next couple of slides, we'll be looking
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:17:26,810'); seek(1046.0)">
              at the actual query and area.
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:17:29,360'); seek(1049.0)">
              Got to this answer, but basically this just shows an overview
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:17:33,650'); seek(1053.0)">
              of what the interaction from the web interface looks like.
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:17:36,620'); seek(1056.0)">
              The second question we ask is how many items have been shipped to this?
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:17:41,030'); seek(1061.0)">
              So if a go back cop would see that there is a custom metric exposed
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:17:45,770'); seek(1065.0)">
              called items shipped to the, which is a counter that keeps track of the
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:17:49,910'); seek(1069.0)">
              number of items that have been shipped.
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:17:51,800'); seek(1071.0)">
              So making use of this, we expect that the AI will make use of this.
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:17:55,775'); seek(1075.0)">
              Counter to be able to answer the question of how many items have been shipped today.
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:18:01,085'); seek(1081.0)">
              And then lastly, there's a third question here where we are trying
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:18:03,515'); seek(1083.0)">
              to ask questions around the demo API and if it's taken longer than usual
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:18:09,905'); seek(1089.0)">
              unfortunately we realize, oh, as you can see from the screenshots, the ICM
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:18:14,885'); seek(1094.0)">
              response, that there is no data fund.
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:18:17,555'); seek(1097.0)">
              So we look at that and try to figure out, okay, why did that happen as well.
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:18:24,600'); seek(1104.0)">
              Behind the scenes as promised.
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:18:25,980'); seek(1105.0)">
              So this is basically the logs from the backend system letting
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:18:30,090'); seek(1110.0)">
              us know exactly what transpired.
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:18:32,250'); seek(1112.0)">
              So in the case of the first example message, the user
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:18:36,180'); seek(1116.0)">
              query is a sedan holiday.
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:18:38,580'); seek(1118.0)">
              Then the AI agents generated this corresponding prompt Q query,
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:18:43,530'); seek(1123.0)">
              which is demo is holiday, right?
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:18:45,900'); seek(1125.0)">
              And then.
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:18:46,870'); seek(1126.0)">
              This generated prom QR was run by the AI agent using function,
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:18:51,590'); seek(1131.0)">
              calling on the permitter sensor and permit to return this response.
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:18:55,400'); seek(1135.0)">
              So if you pay attention to this, you'll see the instance name is demo service two,
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:19:00,590'); seek(1140.0)">
              which is why the response talks about demo service two, or the most important stuff
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:19:05,905'); seek(1145.0)">
              to pay attention to is the value here.
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:19:08,465'); seek(1148.0)">
              So you see the value here is one, because this value is one that means
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:19:13,145'); seek(1153.0)">
              based on the information that we are able to get from the metadata,
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:19:16,445'); seek(1156.0)">
              we know that one means only day.
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:19:18,365'); seek(1158.0)">
              So you can see in this example, the AI agent is able to both write
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:19:22,805'); seek(1162.0)">
              the right corresponding query based on the metadata, but also
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:19:27,065'); seek(1167.0)">
              based on the information provided.
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:19:30,275'); seek(1170.0)">
              In the metadata, it's able to interpret that.
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:19:33,305'); seek(1173.0)">
              One means that today is an holiday, and as such, based on that interpretation is able
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:19:39,395'); seek(1179.0)">
              to reply the user query back in natural language saying yes based on the data that
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:19:44,915'); seek(1184.0)">
              we can see from demo service to values.
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:19:47,645'); seek(1187.0)">
              And that means that today it's an holiday.
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:19:49,685'); seek(1189.0)">
              So this is essentially what's going on behind the scene.
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:19:52,535'); seek(1192.0)">
              In the case of the first question.
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:19:54,815'); seek(1194.0)">
              In the case of the second question, this is a way more interesting.
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:19:59,465'); seek(1199.0)">
              So we asked about how many items has been shipped today.
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:20:02,075'); seek(1202.0)">
              This is the user query again generated from ql.
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:20:05,375'); seek(1205.0)">
              As expected, it makes use of the metrics demo item shipped total.
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:20:09,665'); seek(1209.0)">
              So it sets the time period.
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:20:11,495'); seek(1211.0)">
              So one day this is right, and then it looks at the increase
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:20:14,765'); seek(1214.0)">
              over the period of one day.
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:20:16,415'); seek(1216.0)">
              So this is generated from QL query.
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:20:19,325'); seek(1219.0)">
              Which makes sense.
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:20:20,285'); seek(1220.0)">
              So again, using two scrolling that this query is executed against the
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:20:25,415'); seek(1225.0)">
              Prometheus instance and Promeus returns, this results back to the AI agent.
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:20:31,385'); seek(1231.0)">
              So now the AI needs to figure out how to give, return this in natural language.
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:20:37,865'); seek(1237.0)">
              And this, I think this is interesting.
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:20:40,460'); seek(1240.0)">
              But this an a particularly interesting example because we would see that the demo
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:20:45,440'); seek(1245.0)">
              service actually runs three copies of it.
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:20:47,750'); seek(1247.0)">
              So as you can see here, there are three instances of the demo service.
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:20:51,110'); seek(1251.0)">
              There's demo service zero, which is the first instance.
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:20:54,140'); seek(1254.0)">
              There is demo service one, which is the second instance here, and then there is.
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:20:58,310'); seek(1258.0)">
              Demo service two, which is the third instance.
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:21:01,160'); seek(1261.0)">
              So for each of these instance, they have been processing orders through the day
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:21:05,000'); seek(1265.0)">
              and each of them maintain a count of the number of orders that they've processed.
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:21:09,410'); seek(1269.0)">
              So if you look at the demo service zero, it returns around five, 455,000 orders
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:21:17,090'); seek(1277.0)">
              have been processed by demo service one.
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:21:19,550'); seek(1279.0)">
              And then if you look at demo service.
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:21:22,760'); seek(1282.0)">
              No, the demo service zero rather, has processed 455,000.
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:21:27,290'); seek(1287.0)">
              If you look at demo service one, the value four eight is 453,000.
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:21:31,910'); seek(1291.0)">
              And then if you look at demo service two, the value process, so five a day
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:21:36,710'); seek(1296.0)">
              is 453,000, close to 454,000 as well.
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:21:40,970'); seek(1300.0)">
              Now.
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:21:41,735'); seek(1301.0)">
              The LLM does something interesting because it gets this results back.
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:21:46,325'); seek(1306.0)">
              It is intelligent enough to know that the value is interested in, is a,
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:21:51,845'); seek(1311.0)">
              an aggregation of the value across each of these instances, and also
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:21:57,995'); seek(1317.0)">
              applying the right grouping so it was able to figure out that it needs
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:22:01,745'); seek(1321.0)">
              to sum the value from demo service.
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:22:04,355'); seek(1324.0)">
              Zero demo service.
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:22:06,785'); seek(1326.0)">
              One and demo service two.
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:22:08,915'); seek(1328.0)">
              So if you look at this value, so you can do it later, but I've confirmed it.
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:22:13,335'); seek(1333.0)">
              If you sum this value return for demo service one, I plus the
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:22:18,225'); seek(1338.0)">
              value sum there, demo service.
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:22:21,375'); seek(1341.0)">
              One this is the value return from demo service two plus the value
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:22:26,815'); seek(1346.0)">
              return from demo service, one plus value return from demo service zero.
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:22:31,405'); seek(1351.0)">
              You're going to get.
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:22:32,755'); seek(1352.0)">
              The total value.
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:22:33,715'); seek(1353.0)">
              Yeah, so the LLM was actually able to figure out the right
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:22:37,765'); seek(1357.0)">
              aggregation for the list of data that was returned in the results.
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:22:41,875'); seek(1361.0)">
              Sum it appropriately and give the right number back in.
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:22:46,520'); seek(1366.0)">
              Natural language back to the user.
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:22:48,290'); seek(1368.0)">
              So this is another example that shows how it really shines to get instant.
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:22:54,030'); seek(1374.0)">
              And this happens that can stand.
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:22:55,470'); seek(1375.0)">
              So again, you're able to get instant insightful data
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:22:59,040'); seek(1379.0)">
              while making use of the two.
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:23:00,660'); seek(1380.0)">
              Now looking at the third example in the case, which we are
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:23:04,260'); seek(1384.0)">
              unable to get any response back.
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:23:06,510'); seek(1386.0)">
              So again, this is the user request or the user query.
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:23:11,335'); seek(1391.0)">
              The user asks a request to the demo.
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:23:13,255'); seek(1393.0)">
              With PI taken longer than usual.
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:23:15,415'); seek(1395.0)">
              Then the AI agents generated this query.
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:23:20,385'); seek(1400.0)">
              Now the query in terms of the ax is valid.
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:23:24,315'); seek(1404.0)">
              There's nothing wrong with it.
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:23:25,420'); seek(1405.0)">
              I returns and mt data set.
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:23:28,955'); seek(1408.0)">
              And that's not because there is no data available for the demo API, but how?
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:23:35,840'); seek(1415.0)">
              The LLM has interpreted the question and has tried to go about writing
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:23:40,430'); seek(1420.0)">
              the queries incorrect, and as such, we get an empty data back.
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:23:44,720'); seek(1424.0)">
              Although it's a valid ProQ query, it doesn't answer the
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:23:49,500'); seek(1429.0)">
              question that the user has asked.
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:23:53,130'); seek(1433.0)">
              And doesn't give us any response back.
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:23:55,350'); seek(1435.0)">
              So that's one of the limitations that we've discovered here.
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:23:59,400'); seek(1439.0)">
              But again, looking at it there, where to get around this.
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:24:02,660'); seek(1442.0)">
              So we'll discuss that in the next session.
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:24:04,790'); seek(1444.0)">
              So in the next example, or as you can see here, so this is the
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:24:08,390'); seek(1448.0)">
              prompt chart application itself.
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:24:10,430'); seek(1450.0)">
              When you come here, you're able to modify the.
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:24:15,220'); seek(1455.0)">
              LLM configurations where you can set what provider you want to use,
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:24:19,065'); seek(1459.0)">
              what model you want to use as well.
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:24:21,795'); seek(1461.0)">
              So in this case scenario is currently all the questions that we backed so far.
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:24:26,625'); seek(1466.0)">
              We are using Google's Gemini model and then we are using the Google Gemini 1.5
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:24:32,950'); seek(1472.0)">
              flash, which is their first free model.
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:24:36,585'); seek(1476.0)">
              So that's what we have used so far to answer a, our questions.
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:24:40,195'); seek(1480.0)">
              But we see that Gemini 1.5 in this example was unable to generate the
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:24:45,175'); seek(1485.0)">
              correct prompt, clear query for us.
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:24:48,385'); seek(1488.0)">
              So then by changing the model type to the thinking model.
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:24:52,255'); seek(1492.0)">
              So if you check here based on the confirmation prompt chat
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:24:56,825'); seek(1496.0)">
              so I flipped the model to.
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:25:00,275'); seek(1500.0)">
              Use the thinking model provided by Google, which is the Gemini
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:25:03,425'); seek(1503.0)">
              2.0 flash thinking model instead.
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:25:06,665'); seek(1506.0)">
              So by making use of this model and asking exactly the same
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:25:09,905'); seek(1509.0)">
              question, we are able to see that.
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:25:12,605'); seek(1512.0)">
              Now if you look at this example, the AI agent is actually able to
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:25:18,035'); seek(1518.0)">
              answer the question correctly.
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:25:19,985'); seek(1519.0)">
              So we'll look at the backend and see what has changed.
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:25:22,565'); seek(1522.0)">
              But in this case scenario, you basically ask the same question based on the newer.
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:25:28,235'); seek(1528.0)">
              Thinking mod, he was able to provide this insight and say, oh, we can see
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:25:32,855'); seek(1532.0)">
              that the slash API slash part seems to have a lot of 500 errors, and as
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:25:38,875'); seek(1538.0)">
              such, latency is also significantly higher than the other part as well.
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:25:43,285'); seek(1543.0)">
              Which suggest that the part has an issue.
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:25:47,245'); seek(1547.0)">
              So as anr, you can imagine how insightful such a. And inside like
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:25:54,800'); seek(1554.0)">
              this is when you are currently trying to debug an incident and you're
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:25:58,730'); seek(1558.0)">
              trying to figure out what's going on.
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:26:00,260'); seek(1560.0)">
              Maybe you get like a latency a lot or basically user start
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:26:07,010'); seek(1567.0)">
              complain that your CM is slow.
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:26:08,450'); seek(1568.0)">
              You can basically just.
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:26:10,025'); seek(1570.0)">
              File the tool, ask and get insights for information like this.
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:26:14,475'); seek(1574.0)">
              Now going back to the backend, we can see essentially what happened.
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:26:18,435'); seek(1578.0)">
              So from the backend there, you would see pretty much the same thing, the same user
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:26:23,385'); seek(1583.0)">
              request, but if you pay attention Yeah.
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:26:26,715'); seek(1586.0)">
              To look at the.
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:26:28,485'); seek(1588.0)">
              Generated prom ql.
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:26:30,045'); seek(1590.0)">
              That this generated a different prom QL that is more appropriate
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:26:33,975'); seek(1593.0)">
              is considering the rate now.
            </span>
            
            <span id="chunk-429" class="transcript-chunks" onclick="console.log('00:26:35,775'); seek(1595.0)">
              And also based on the type of the data is able to use the Instagram counter.
            </span>
            
            <span id="chunk-430" class="transcript-chunks" onclick="console.log('00:26:41,815'); seek(1601.0)">
              And as such, it was able to actually generate meaningful
            </span>
            
            <span id="chunk-431" class="transcript-chunks" onclick="console.log('00:26:45,805'); seek(1605.0)">
              results this time around.
            </span>
            
            <span id="chunk-432" class="transcript-chunks" onclick="console.log('00:26:47,305'); seek(1607.0)">
              So this is the full responses way much longer than this because there
            </span>
            
            <span id="chunk-433" class="transcript-chunks" onclick="console.log('00:26:52,245'); seek(1612.0)">
              are a lot of metrics that match this.
            </span>
            
            <span id="chunk-434" class="transcript-chunks" onclick="console.log('00:26:54,815'); seek(1614.0)">
              Essentially, based on all of this, you are able to get for each part you
            </span>
            
            <span id="chunk-435" class="transcript-chunks" onclick="console.log('00:27:00,905'); seek(1620.0)">
              can get the status and then you can get the value, so based on that, the
            </span>
            
            <span id="chunk-436" class="transcript-chunks" onclick="console.log('00:27:05,850'); seek(1625.0)">
              LLM was able to then go over all of this results similar to the first one.
            </span>
            
            <span id="chunk-437" class="transcript-chunks" onclick="console.log('00:27:11,860'); seek(1631.0)">
              He was able to group them appropriately and then also identify the outlier
            </span>
            
            <span id="chunk-438" class="transcript-chunks" onclick="console.log('00:27:17,590'); seek(1637.0)">
              in the list after the group.
            </span>
            
            <span id="chunk-439" class="transcript-chunks" onclick="console.log('00:27:19,120'); seek(1639.0)">
              And as such was able to figure out that compared to the other
            </span>
            
            <span id="chunk-440" class="transcript-chunks" onclick="console.log('00:27:21,940'); seek(1641.0)">
              guys, we can see that the slash a p slash par has 500 arrows.
            </span>
            
            <span id="chunk-441" class="transcript-chunks" onclick="console.log('00:27:27,835'); seek(1647.0)">
              More, and then the viral latency as well is higher compared
            </span>
            
            <span id="chunk-442" class="transcript-chunks" onclick="console.log('00:27:32,485'); seek(1652.0)">
              to the rest of the parts.
            </span>
            
            <span id="chunk-443" class="transcript-chunks" onclick="console.log('00:27:33,805'); seek(1653.0)">
              So this shows basically how by changing the LLM model that is used, using a
            </span>
            
            <span id="chunk-444" class="transcript-chunks" onclick="console.log('00:27:41,525'); seek(1661.0)">
              more powerful LM model, you're able to get a better result for queries that,
            </span>
            
            <span id="chunk-445" class="transcript-chunks" onclick="console.log('00:27:47,585'); seek(1667.0)">
              or in cases where the simpler models were unable to write valid queries and
            </span>
            
            <span id="chunk-446" class="transcript-chunks" onclick="console.log('00:27:52,805'); seek(1672.0)">
              compare the results that was written.
            </span>
            
            <span id="chunk-447" class="transcript-chunks" onclick="console.log('00:27:56,310'); seek(1676.0)">
              So moving on to the fourth section where we basically just discuss a summary
            </span>
            
            <span id="chunk-448" class="transcript-chunks" onclick="console.log('00:28:00,300'); seek(1680.0)">
              of everything that we have learned from interacting with Prompt Chat.
            </span>
            
            <span id="chunk-449" class="transcript-chunks" onclick="console.log('00:28:04,400'); seek(1684.0)">
              So this is the first part basically talks about what are the lessons
            </span>
            
            <span id="chunk-450" class="transcript-chunks" onclick="console.log('00:28:09,960'); seek(1689.0)">
              that we have learned, right?
            </span>
            
            <span id="chunk-451" class="transcript-chunks" onclick="console.log('00:28:11,580'); seek(1691.0)">
              You would see throughout the, from the architecture and the rest
            </span>
            
            <span id="chunk-452" class="transcript-chunks" onclick="console.log('00:28:14,820'); seek(1694.0)">
              of the presentation, there's no.
            </span>
            
            <span id="chunk-453" class="transcript-chunks" onclick="console.log('00:28:17,220'); seek(1697.0)">
              Not at any point did we attempt to retrain any of the models
            </span>
            
            <span id="chunk-454" class="transcript-chunks" onclick="console.log('00:28:21,240'); seek(1701.0)">
              or do any sort of fine tuning.
            </span>
            
            <span id="chunk-455" class="transcript-chunks" onclick="console.log('00:28:22,950'); seek(1702.0)">
              So what that lets us know is that I'm sure that they are LLM models
            </span>
            
            <span id="chunk-456" class="transcript-chunks" onclick="console.log('00:28:27,540'); seek(1707.0)">
              today are actually capable of writing prompt QL queries on their own.
            </span>
            
            <span id="chunk-457" class="transcript-chunks" onclick="console.log('00:28:33,570'); seek(1713.0)">
              The other thing to note as well is that, the only change we had to do actually
            </span>
            
            <span id="chunk-458" class="transcript-chunks" onclick="console.log('00:28:39,240'); seek(1719.0)">
              was to use one shot prompting, which is basically adding an example to ensure
            </span>
            
            <span id="chunk-459" class="transcript-chunks" onclick="console.log('00:28:44,580'); seek(1724.0)">
              that the output that we get from the LLM is formatted exactly how we want it.
            </span>
            
            <span id="chunk-460" class="transcript-chunks" onclick="console.log('00:28:50,040'); seek(1730.0)">
              And that's important because you initially were on IT project.
            </span>
            
            <span id="chunk-461" class="transcript-chunks" onclick="console.log('00:28:54,720'); seek(1734.0)">
              We are getting into issues where the response coming from the LLM
            </span>
            
            <span id="chunk-462" class="transcript-chunks" onclick="console.log('00:28:59,250'); seek(1739.0)">
              model, it would add additional like characters or talking to it.
            </span>
            
            <span id="chunk-463" class="transcript-chunks" onclick="console.log('00:29:02,730'); seek(1742.0)">
              And then once you pass it to.
            </span>
            
            <span id="chunk-464" class="transcript-chunks" onclick="console.log('00:29:05,535'); seek(1745.0)">
              The Prometheus instance, it won't be valid.
            </span>
            
            <span id="chunk-465" class="transcript-chunks" onclick="console.log('00:29:09,845'); seek(1749.0)">
              It would no longer be valid from cure, and that will lead to like
            </span>
            
            <span id="chunk-466" class="transcript-chunks" onclick="console.log('00:29:13,745'); seek(1753.0)">
              crashes or issues because Prometheus cannot interpret the query.
            </span>
            
            <span id="chunk-467" class="transcript-chunks" onclick="console.log('00:29:18,275'); seek(1758.0)">
              But after using one shot prompting where we are basically able to show.
            </span>
            
            <span id="chunk-468" class="transcript-chunks" onclick="console.log('00:29:23,435'); seek(1763.0)">
              DNLM exactly the format of the response.
            </span>
            
            <span id="chunk-469" class="transcript-chunks" onclick="console.log('00:29:26,435'); seek(1766.0)">
              He started returning exactly just the prompt QR required without any
            </span>
            
            <span id="chunk-470" class="transcript-chunks" onclick="console.log('00:29:31,415'); seek(1771.0)">
              additional characters or tokens around it, and that we basically were
            </span>
            
            <span id="chunk-471" class="transcript-chunks" onclick="console.log('00:29:35,915'); seek(1775.0)">
              able to avoid having to manually.
            </span>
            
            <span id="chunk-472" class="transcript-chunks" onclick="console.log('00:29:38,505'); seek(1778.0)">
              Try to extract out the prom cure out of the LLM response.
            </span>
            
            <span id="chunk-473" class="transcript-chunks" onclick="console.log('00:29:43,255'); seek(1783.0)">
              Based on the last example that you see as well would realize that obviously the
            </span>
            
            <span id="chunk-474" class="transcript-chunks" onclick="console.log('00:29:48,135'); seek(1788.0)">
              thinking models are better when it comes to trying to write complicated ProQ.
            </span>
            
            <span id="chunk-475" class="transcript-chunks" onclick="console.log('00:29:52,515'); seek(1792.0)">
              So the lighter models work.
            </span>
            
            <span id="chunk-476" class="transcript-chunks" onclick="console.log('00:29:55,515'); seek(1795.0)">
              For most cases as well.
            </span>
            
            <span id="chunk-477" class="transcript-chunks" onclick="console.log('00:29:57,165'); seek(1797.0)">
              But when you want to do, ask more completed questions, it's more useful to,
            </span>
            
            <span id="chunk-478" class="transcript-chunks" onclick="console.log('00:30:01,725'); seek(1801.0)">
              the more powerful the models are and the more time you spend thinking, the better
            </span>
            
            <span id="chunk-479" class="transcript-chunks" onclick="console.log('00:30:06,165'); seek(1806.0)">
              the prompt cure that they write as well.
            </span>
            
            <span id="chunk-480" class="transcript-chunks" onclick="console.log('00:30:08,835'); seek(1808.0)">
              So we've seen that again in the last example.
            </span>
            
            <span id="chunk-481" class="transcript-chunks" onclick="console.log('00:30:12,085'); seek(1812.0)">
              So some of the limitations that we are so observed in the course of working on
            </span>
            
            <span id="chunk-482" class="transcript-chunks" onclick="console.log('00:30:16,345'); seek(1816.0)">
              this project is that as you would see.
            </span>
            
            <span id="chunk-483" class="transcript-chunks" onclick="console.log('00:30:19,255'); seek(1819.0)">
              The majority of being able to pull this off depends on
            </span>
            
            <span id="chunk-484" class="transcript-chunks" onclick="console.log('00:30:23,365'); seek(1823.0)">
              the quality of documentation that you add to your metadata.
            </span>
            
            <span id="chunk-485" class="transcript-chunks" onclick="console.log('00:30:26,875'); seek(1826.0)">
              So Promeus would always have the metadata, API available and it will tell you,
            </span>
            
            <span id="chunk-486" class="transcript-chunks" onclick="console.log('00:30:31,715'); seek(1831.0)">
              okay, these are the metrics that I have.
            </span>
            
            <span id="chunk-487" class="transcript-chunks" onclick="console.log('00:30:34,165'); seek(1834.0)">
              And then these are the types of those metrics.
            </span>
            
            <span id="chunk-488" class="transcript-chunks" onclick="console.log('00:30:37,765'); seek(1837.0)">
              But if you don't add any help or information to interpret, for example,
            </span>
            
            <span id="chunk-489" class="transcript-chunks" onclick="console.log('00:30:41,965'); seek(1841.0)">
              if, look at the case of when we had demo.
            </span>
            
            <span id="chunk-490" class="transcript-chunks" onclick="console.log('00:30:46,165'); seek(1846.0)">
              Service is today all data metric.
            </span>
            
            <span id="chunk-491" class="transcript-chunks" onclick="console.log('00:30:49,175'); seek(1849.0)">
              If the documentation do not contain information, saying A one means today's
            </span>
            
            <span id="chunk-492" class="transcript-chunks" onclick="console.log('00:30:54,935'); seek(1854.0)">
              holiday, zero means today's an all day, then there would have not been any way for
            </span>
            
            <span id="chunk-493" class="transcript-chunks" onclick="console.log('00:31:00,215'); seek(1860.0)">
              premature for the L to interpret correctly the premature response that I got.
            </span>
            
            <span id="chunk-494" class="transcript-chunks" onclick="console.log('00:31:06,155'); seek(1866.0)">
              The other challenging bit as well is even for cases where
            </span>
            
            <span id="chunk-495" class="transcript-chunks" onclick="console.log('00:31:10,295'); seek(1870.0)">
              you have the documentation.
            </span>
            
            <span id="chunk-496" class="transcript-chunks" onclick="console.log('00:31:13,710'); seek(1873.0)">
              All the, all partex included as part of the documentation.
            </span>
            
            <span id="chunk-497" class="transcript-chunks" onclick="console.log('00:31:16,770'); seek(1876.0)">
              Most of the time the labels are missing.
            </span>
            
            <span id="chunk-498" class="transcript-chunks" onclick="console.log('00:31:18,900'); seek(1878.0)">
              Labels in this case are like, if you compare it to traditional databases,
            </span>
            
            <span id="chunk-499" class="transcript-chunks" onclick="console.log('00:31:23,250'); seek(1883.0)">
              so maybe like the colons or the fields.
            </span>
            
            <span id="chunk-500" class="transcript-chunks" onclick="console.log('00:31:26,490'); seek(1886.0)">
              So because you don't know what fields or what labels are available in your,
            </span>
            
            <span id="chunk-501" class="transcript-chunks" onclick="console.log('00:31:32,280'); seek(1892.0)">
              in that particular matrix, it makes it harder for DLLM to be able to write.
            </span>
            
            <span id="chunk-502" class="transcript-chunks" onclick="console.log('00:31:38,580'); seek(1898.0)">
              Correct queries, especially when you need to filter by things
            </span>
            
            <span id="chunk-503" class="transcript-chunks" onclick="console.log('00:31:42,120'); seek(1902.0)">
              like the actual bill value.
            </span>
            
            <span id="chunk-504" class="transcript-chunks" onclick="console.log('00:31:45,280'); seek(1905.0)">
              So that's one of the things where, again, that can easily be solved
            </span>
            
            <span id="chunk-505" class="transcript-chunks" onclick="console.log('00:31:48,460'); seek(1908.0)">
              as is essentially going over the documentation and adding as much
            </span>
            
            <span id="chunk-506" class="transcript-chunks" onclick="console.log('00:31:52,540'); seek(1912.0)">
              useful information there as possible.
            </span>
            
            <span id="chunk-507" class="transcript-chunks" onclick="console.log('00:31:55,300'); seek(1915.0)">
              The other limitation notice as well is that sometimes you might feel you might
            </span>
            
            <span id="chunk-508" class="transcript-chunks" onclick="console.log('00:32:00,160'); seek(1920.0)">
              having consistency in the results that you get because the query is generated.
            </span>
            
            <span id="chunk-509" class="transcript-chunks" onclick="console.log('00:32:05,440'); seek(1925.0)">
              Differ define slightly.
            </span>
            
            <span id="chunk-510" class="transcript-chunks" onclick="console.log('00:32:06,760'); seek(1926.0)">
              And that's important because if you frame your question slightly
            </span>
            
            <span id="chunk-511" class="transcript-chunks" onclick="console.log('00:32:10,960'); seek(1930.0)">
              differently, then the LLM can interpret it differently and generate
            </span>
            
            <span id="chunk-512" class="transcript-chunks" onclick="console.log('00:32:15,400'); seek(1935.0)">
              a correspondent different query, which would then give you a different result.
            </span>
            
            <span id="chunk-513" class="transcript-chunks" onclick="console.log('00:32:19,450'); seek(1939.0)">
              But that can be eliminated by having more exact descriptions in your question.
            </span>
            
            <span id="chunk-514" class="transcript-chunks" onclick="console.log('00:32:25,650'); seek(1945.0)">
              So for example, you're saying a, is there any endpoint currently between 500?
            </span>
            
            <span id="chunk-515" class="transcript-chunks" onclick="console.log('00:32:31,910'); seek(1951.0)">
              If you don't put, a timeframe the first time, maybe you might
            </span>
            
            <span id="chunk-516" class="transcript-chunks" onclick="console.log('00:32:34,640'); seek(1954.0)">
              do it over an hour or like maybe five minutes or maybe one minute.
            </span>
            
            <span id="chunk-517" class="transcript-chunks" onclick="console.log('00:32:38,540'); seek(1958.0)">
              But if for example, you ask specifically with the timeframe, then generated
            </span>
            
            <span id="chunk-518" class="transcript-chunks" onclick="console.log('00:32:43,070'); seek(1963.0)">
              prompt cure by the AI agents would contain that exact timeframe and
            </span>
            
            <span id="chunk-519" class="transcript-chunks" onclick="console.log('00:32:47,600'); seek(1967.0)">
              then you get the same response back.
            </span>
            
            <span id="chunk-520" class="transcript-chunks" onclick="console.log('00:32:49,680'); seek(1969.0)">
              That's another limitation.
            </span>
            
            <span id="chunk-521" class="transcript-chunks" onclick="console.log('00:32:51,120'); seek(1971.0)">
              But again, how that can be improved upon is basically just puts in.
            </span>
            
            <span id="chunk-522" class="transcript-chunks" onclick="console.log('00:32:55,285'); seek(1975.0)">
              More, the more exact your question is, the better the answers that you get.
            </span>
            
            <span id="chunk-523" class="transcript-chunks" onclick="console.log('00:33:00,235'); seek(1980.0)">
              So in terms of the future improvement so better support for complex queries.
            </span>
            
            <span id="chunk-524" class="transcript-chunks" onclick="console.log('00:33:05,085'); seek(1985.0)">
              So this better support includes things like being able to undo more completed
            </span>
            
            <span id="chunk-525" class="transcript-chunks" onclick="console.log('00:33:10,005'); seek(1990.0)">
              queries, being able to even supply.
            </span>
            
            <span id="chunk-526" class="transcript-chunks" onclick="console.log('00:33:14,075'); seek(1994.0)">
              Right now all the answers are coming back in texts in natural language, but of
            </span>
            
            <span id="chunk-527" class="transcript-chunks" onclick="console.log('00:33:18,245'); seek(1998.0)">
              course it might be useful to maybe have a graph to look at from time to time.
            </span>
            
            <span id="chunk-528" class="transcript-chunks" onclick="console.log('00:33:22,355'); seek(2002.0)">
              So for more competitive queries, for example, it might be useful to return
            </span>
            
            <span id="chunk-529" class="transcript-chunks" onclick="console.log('00:33:26,795'); seek(2006.0)">
              both the natural language, but also some form of visualization for it.
            </span>
            
            <span id="chunk-530" class="transcript-chunks" onclick="console.log('00:33:31,775'); seek(2011.0)">
              Also, right now the project is limited to just Prometheus, so
            </span>
            
            <span id="chunk-531" class="transcript-chunks" onclick="console.log('00:33:36,005'); seek(2016.0)">
              that's the only matrix source that.
            </span>
            
            <span id="chunk-532" class="transcript-chunks" onclick="console.log('00:33:38,360'); seek(2018.0)">
              It works with.
            </span>
            
            <span id="chunk-533" class="transcript-chunks" onclick="console.log('00:33:39,140'); seek(2019.0)">
              So in terms of next steps, you're looking at expanding the project
            </span>
            
            <span id="chunk-534" class="transcript-chunks" onclick="console.log('00:33:42,560'); seek(2022.0)">
              such that it supports more than just Prometheus as the source.
            </span>
            
            <span id="chunk-535" class="transcript-chunks" onclick="console.log('00:33:46,640'); seek(2026.0)">
              And then lastly, I think another interesting improvement would
            </span>
            
            <span id="chunk-536" class="transcript-chunks" onclick="console.log('00:33:50,420'); seek(2030.0)">
              be around the system being able to learn from user interactions.
            </span>
            
            <span id="chunk-537" class="transcript-chunks" onclick="console.log('00:33:53,800'); seek(2033.0)">
              So imagine you ask a question and maybe for example, the, you didn't use the
            </span>
            
            <span id="chunk-538" class="transcript-chunks" onclick="console.log('00:33:59,350'); seek(2039.0)">
              right labels, you and you, or it doesn't know the labels are available for that.
            </span>
            
            <span id="chunk-539" class="transcript-chunks" onclick="console.log('00:34:05,565'); seek(2045.0)">
              Particular metrics.
            </span>
            
            <span id="chunk-540" class="transcript-chunks" onclick="console.log('00:34:06,465'); seek(2046.0)">
              So he asks that, okay, I can answer this, right?
            </span>
            
            <span id="chunk-541" class="transcript-chunks" onclick="console.log('00:34:09,675'); seek(2049.0)">
              And then you provide those labels.
            </span>
            
            <span id="chunk-542" class="transcript-chunks" onclick="console.log('00:34:11,835'); seek(2051.0)">
              Now it would be useful if the, right now there is no memory in the system, so he
            </span>
            
            <span id="chunk-543" class="transcript-chunks" onclick="console.log('00:34:17,115'); seek(2057.0)">
              actually doesn't store the information.
            </span>
            
            <span id="chunk-544" class="transcript-chunks" onclick="console.log('00:34:19,395'); seek(2059.0)">
              So an extension would be such that next time you actually don't have to
            </span>
            
            <span id="chunk-545" class="transcript-chunks" onclick="console.log('00:34:22,905'); seek(2062.0)">
              go back and get supply the same labels back to the system for you to answer.
            </span>
            
            <span id="chunk-546" class="transcript-chunks" onclick="console.log('00:34:29,000'); seek(2069.0)">
              Correctly.
            </span>
            
            <span id="chunk-547" class="transcript-chunks" onclick="console.log('00:34:30,030'); seek(2070.0)">
              So that's like a future improvement when it comes to, it's actually
            </span>
            
            <span id="chunk-548" class="transcript-chunks" onclick="console.log('00:34:33,720'); seek(2073.0)">
              learning from user interaction.
            </span>
            
            <span id="chunk-549" class="transcript-chunks" onclick="console.log('00:34:35,490'); seek(2075.0)">
              Another way that can go is when queries, for example, let's say the queries
            </span>
            
            <span id="chunk-550" class="transcript-chunks" onclick="console.log('00:34:40,730'); seek(2080.0)">
              are wrong or like the wrong metrics was used and you correct it again,
            </span>
            
            <span id="chunk-551" class="transcript-chunks" onclick="console.log('00:34:46,415'); seek(2086.0)">
              all those kind of interactions can be stored such that his user's context.
            </span>
            
            <span id="chunk-552" class="transcript-chunks" onclick="console.log('00:34:51,065'); seek(2091.0)">
              Next time he's trying to answer questions, and as such, he can make use.
            </span>
            
            <span id="chunk-553" class="transcript-chunks" onclick="console.log('00:34:55,905'); seek(2095.0)">
              Of that, and then the system gets better over time because it's
            </span>
            
            <span id="chunk-554" class="transcript-chunks" onclick="console.log('00:34:59,265'); seek(2099.0)">
              learning from user interactions.
            </span>
            
            <span id="chunk-555" class="transcript-chunks" onclick="console.log('00:35:01,615'); seek(2101.0)">
              So lastly, how can you contribute and join?
            </span>
            
            <span id="chunk-556" class="transcript-chunks" onclick="console.log('00:35:03,685'); seek(2103.0)">
              The source code is available on our GitHub profile.
            </span>
            
            <span id="chunk-557" class="transcript-chunks" onclick="console.log('00:35:07,569'); seek(2107.0)">
              So it's open the projects open source.
            </span>
            
            <span id="chunk-558" class="transcript-chunks" onclick="console.log('00:35:09,369'); seek(2109.0)">
              If you've go to next I HQ on GitHub you see the source code for the prom chat app.
            </span>
            
            <span id="chunk-559" class="transcript-chunks" onclick="console.log('00:35:14,899'); seek(2114.0)">
              So issues pls are welcomed to, as a means of contributing to the project.
            </span>
            
            <span id="chunk-560" class="transcript-chunks" onclick="console.log('00:35:21,744'); seek(2121.0)">
              Also the web interface that I was playing with or that I was shown in the sites
            </span>
            
            <span id="chunk-561" class="transcript-chunks" onclick="console.log('00:35:26,424'); seek(2126.0)">
              is available from chat do co so you can basically visit that is available.
            </span>
            
            <span id="chunk-562" class="transcript-chunks" onclick="console.log('00:35:33,124'); seek(2133.0)">
              You don't need like an identification payment or anything.
            </span>
            
            <span id="chunk-563" class="transcript-chunks" onclick="console.log('00:35:36,484'); seek(2136.0)">
              The only issues might be because it's using my own personal, aPI key.
            </span>
            
            <span id="chunk-564" class="transcript-chunks" onclick="console.log('00:35:41,294'); seek(2141.0)">
              So there are limits to the number of like daily requests or sometimes maybe
            </span>
            
            <span id="chunk-565" class="transcript-chunks" onclick="console.log('00:35:45,534'); seek(2145.0)">
              even a lot of people have been playing around with it earlier in the day.
            </span>
            
            <span id="chunk-566" class="transcript-chunks" onclick="console.log('00:35:48,874'); seek(2148.0)">
              You might get, you might not be able to get any responses back
            </span>
            
            <span id="chunk-567" class="transcript-chunks" onclick="console.log('00:35:54,344'); seek(2154.0)">
              from the API it would tell you that the LLM credits is exhausted.
            </span>
            
            <span id="chunk-568" class="transcript-chunks" onclick="console.log('00:35:58,874'); seek(2158.0)">
              But yeah.
            </span>
            
            <span id="chunk-569" class="transcript-chunks" onclick="console.log('00:35:59,774'); seek(2159.0)">
              But you can clone the project locally.
            </span>
            
            <span id="chunk-570" class="transcript-chunks" onclick="console.log('00:36:01,844'); seek(2161.0)">
              Put your own EPI keys and run it if.
            </span>
            
            <span id="chunk-571" class="transcript-chunks" onclick="console.log('00:36:04,774'); seek(2164.0)">
              You want to, or if you want to use the web interface, you can visit from
            </span>
            
            <span id="chunk-572" class="transcript-chunks" onclick="console.log('00:36:08,459'); seek(2168.0)">
              chat, do nest slide.co as well, and you would be able to interact with it.
            </span>
            
            <span id="chunk-573" class="transcript-chunks" onclick="console.log('00:36:13,989'); seek(2173.0)">
              So that's it.
            </span>
            
            <span id="chunk-574" class="transcript-chunks" onclick="console.log('00:36:15,939'); seek(2175.0)">
              That's it from me.
            </span>
            
            <span id="chunk-575" class="transcript-chunks" onclick="console.log('00:36:16,899'); seek(2176.0)">
              So thank you very much for listening to the session.
            </span>
            
            <span id="chunk-576" class="transcript-chunks" onclick="console.log('00:36:19,869'); seek(2179.0)">
              I hope you've learned a lot and you have a better understanding.
            </span>
            
            <span id="chunk-577" class="transcript-chunks" onclick="console.log('00:36:24,819'); seek(2184.0)">
              As regards how to implement something like this.
            </span>
            
            <span id="chunk-578" class="transcript-chunks" onclick="console.log('00:36:27,399'); seek(2187.0)">
              And yes, the answer to the question is it is possible to
            </span>
            
            <span id="chunk-579" class="transcript-chunks" onclick="console.log('00:36:31,569'); seek(2191.0)">
              chat with your mandatory metrics.
            </span>
            
            <span id="chunk-580" class="transcript-chunks" onclick="console.log('00:36:33,789'); seek(2193.0)">
              And I do hope you enjoy the rest of the conference, but I thank you.
            </span>
            
            <span id="chunk-581" class="transcript-chunks" onclick="console.log('00:36:40,214'); seek(2200.0)">
              I.
            </span>
            
            </div>
          </div>
          
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/David%20Asamu%20-%20Conf42%20Site%20Reliablity%20Engineering%20%28SRE%29%202025.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/David%20Asamu%20-%20Conf42%20Site%20Reliablity%20Engineering%20%28SRE%29%202025.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #E36414;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/sre2025" class="btn btn-sm btn-danger shadow lift" style="background-color: #E36414;">
                <i class="fe fe-grid me-2"></i>
                See all 109 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/David%20Asamu_sre.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  David Asamu
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Tech Lead Manager, SRE Team @ Nomba
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/david-asamu/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="David Asamu's LinkedIn account" />
                  </a>
                  
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by David Asamu"
                  data-url="https://www.conf42.com/sre2025"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/sre2025"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>






    <!-- WELCOME -->
    <section class="pt-8 pt-md-11 pb-10 pb-md-15 bg-info" id="register">

      <!-- Shape -->
      <div class="shape shape-blur-3 text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>      </div>

      <!-- Content -->
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 col-md-10 col-lg-8 text-center">

            <!-- Heading -->
            <h1 class="display-2 text-white">
              Join the community!
            </h1>

            <!-- Text -->
            <p class="lead text-white text-opacity-80 mb-6 mb-md-8">
              Learn for free, join the best tech learning community 
              for a <a class="text-white" href="https://www.reddit.com/r/sanfrancisco/comments/1bz90f6/why_are_coffee_shops_in_sf_so_expensive/" target="_blank">price of a pumpkin latte</a>.
            </p>

            <!-- Form -->
            <form class="d-flex align-items-center justify-content-center mb-7 mb-md-9">

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Annual
              </span>

              <!-- Switch -->
              <div class="form-check form-check-dark form-switch mx-3">
                <input class="form-check-input" type="checkbox" id="billingSwitch" data-toggle="price" data-target=".price">
              </div>

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Monthly
              </span>

            </form>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->

    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- PRICING -->
    <section class="mt-n8 mt-md-n15">
      <div class="container">
        <div class="row gx-4">
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-1">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Newsletter</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="0" data-monthly="0">0</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Event notifications, weekly newsletter
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Delayed access</b> to all content
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Immediate access to Keynotes & Panels
                  </p>
                </div>
              
              
              </div>
            </div>

            <!-- Card -->
            <div class="card shadow-lg mb-6 border border-success">
              <div class="card-body">

                <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
</script>

<!-- Form -->
<link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
<p class="emailoctopus-success-message text-success"></p>
<p class="emailoctopus-error-message text-danger"></p>
<form
    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
    method="post"
    data-message-success="Thanks! Check your email for further directions!"
    data-message-missing-email-address="Your email address is required."
    data-message-invalid-email-address="Your email address looks incorrect, please try again."
    data-message-bot-submission-error="This doesn't look like a human submission."
    data-message-consent-required="Please check the checkbox to indicate your consent."
    data-message-invalid-parameters-error="This form has missing or invalid fields."
    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
    class="emailoctopus-form"
    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
>
<div class="form-floating emailoctopus-form-row">
    <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
    <label for="field_0">Email address</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
    <label for="field_1">First Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
    <label for="field_2">Last Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
    <label for="field_4">Company</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
    <label for="field_5">Job Title</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
    <label for="field_3">Phone Number</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
    oninput="updateCountry()"
    >
    <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
    </select>
    <label for="field_7">Country</label>
</div>
<input id="country-destination" name="field_7" type="hidden">
<input id="tz-country" name="field_8" type="hidden">

<input
    name="field_6"
    type="hidden"
    value="Site Reliability Engineering"
>

<div class="emailoctopus-form-row-consent">
    <input
    type="checkbox"
    id="consent"
    name="consent"
    >
    <label for="consent">
    I consent to the following terms:
    </label>
    <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
    Terms and Conditions
    </a>
    &amp;
    <a href="./code-of-conduct" target="_blank">
    Code of Conduct
    </a>
</div>
<div
    aria-hidden="true"
    class="emailoctopus-form-row-hp"
>
    <input
    type="text"
    name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
    tabindex="-1"
    autocomplete="nope"
    >
</div>
<div class="mt-6 emailoctopus-form-row-subscribe">
    <input
    type="hidden"
    name="successRedirectUrl"
    >
    <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
    Subscribe to free newsletter <i class="fe fe-arrow-right ms-3"></i>
    </button>
</div>
</form>

<!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
<script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

              </div>
            </div>
          </div>
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-0">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Community</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="8.34" data-monthly="10">8.34</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Access to <a href="https://conf42.circle.so/">Circle community platform</a>
                  </p>
                </div>

                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Immediate access</b> to all content
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank"><b>Live events!</b></a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank">Regular office hours, Q&As, CV reviews</a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Courses, quizes & certificates
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Community chats
                  </p>
                </div>
                

                <!-- Button -->
                <a href="https://conf42.circle.so/checkout/subscribe" class="btn w-100 btn-primary">
                  Join the community (7 day free trial)<i class="fe fe-arrow-right ms-3"></i>
                </a>

              </div>
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2025">
                  Observability 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2025">
                  Quantum Computing 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2025">
                  Rustlang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2025">
                  Platform Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2025">
                  MLOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2025">
                  Incident Management 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2025">
                  Kube Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2025">
                  JavaScript 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2025">
                  Prompt Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/robotics2025">
                  Robotics 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2025">
                  DevSecOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2025">
                  Internet of Things (IoT) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>