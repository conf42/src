<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Build and orchestrate serverless generative AI applications</title>
    <meta name="description" content="Everything Cloud Native and Cloud Security. It came from the Cloud!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Mohammed%20Fazalullah_cloud.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Build and orchestrate serverless generative AI applications | Conf42"/>
    <meta property="og:description" content="In the journey to build versatile serverless generative AI applications, discover the roadmap from concept to realization, all within an efficient, model-driven design setup. From principles to prototyping with the tools, learn how to bring your ideas to your users in a scalable, efficient manner."/>
    <meta property="og:url" content="https://conf42.com/Cloud_Native_2024_Mohammed_Fazalullah_build_orchestrate_serverless"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/MLOPS2025_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        MLOps 2025
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2025-09-18
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/mlops2025" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2025">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2025">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2025">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2025">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2025">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2025">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2025">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2025">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2025">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/robotics2025">
                            Robotics
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2025">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2025">
                            Internet of Things (IoT)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="https://conf42.circle.so/">
                            <b>Community platform login</b>
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Join the community!
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #7B2726;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Cloud Native 2024 - Online
            </h1>

            <h2 class="text-white">
              
              Content unlocked! Welcome to the community!
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- Everything Cloud Native and Cloud Security. It came from the Cloud!
 -->
              <script>
                const event_date = new Date("2024-03-21T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2024-03-21T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "UhmOV51Hp10"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <a class="btn btn-info lift mb-3" data-bigpicture='{"ytSrc": "cMgNOs-0jzo"}' href="#">
                <i class="fe fe-eye me-2"></i>
                Watch Premiere
              </a>
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrDqcWhv2eREIUiv2_MqQatx" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Hi there, thanks for joining the session. Today I\u0027m going to be sharing how you", "timestamp": "00:00:24,570", "timestamp_s": 24.0}, {"text": "can get started on AWS for building and", "timestamp": "00:00:28,332", "timestamp_s": 28.0}, {"text": "orchestrating serverless workflows for generative AI generative", "timestamp": "00:00:31,644", "timestamp_s": 31.0}, {"text": "AI has taken the world by storm. We are seeing a massive shift in the", "timestamp": "00:00:35,762", "timestamp_s": 35.0}, {"text": "way applications are being built. A lot of this is through consumer facing", "timestamp": "00:00:38,924", "timestamp_s": 38.0}, {"text": "services that have come out like chat, JPT by Openei,", "timestamp": "00:00:42,738", "timestamp_s": 42.0}, {"text": "cloud by anthropic, and we are able to", "timestamp": "00:00:45,666", "timestamp_s": 45.0}, {"text": "see and experience how powerful latest machine learning models have", "timestamp": "00:00:48,748", "timestamp_s": 48.0}, {"text": "become. Generative AI is a type of AI that can create new", "timestamp": "00:00:52,612", "timestamp_s": 52.0}, {"text": "content and ideas, including conversations, stories, images,", "timestamp": "00:00:56,052", "timestamp_s": 56.0}, {"text": "videos and music. Like all AI, generative AI is powered", "timestamp": "00:00:59,290", "timestamp_s": 59.0}, {"text": "by machine learning models. But generative AI is powered by", "timestamp": "00:01:03,034", "timestamp_s": 63.0}, {"text": "very large models that are pretrained on vast amounts of data and", "timestamp": "00:01:06,292", "timestamp_s": 66.0}, {"text": "commonly referred to as foundational models. Now, throughout the session and", "timestamp": "00:01:10,056", "timestamp_s": 70.0}, {"text": "also in conversation that you\u0027ll have out there, you\u0027ll see foundational models", "timestamp": "00:01:13,288", "timestamp_s": 73.0}, {"text": "being interchangeably used with LLMs. Large language models", "timestamp": "00:01:17,026", "timestamp_s": 77.0}, {"text": "just to understand LLMs are a subset of foundational models where LLMs focus", "timestamp": "00:01:20,658", "timestamp_s": 80.0}, {"text": "on text. Specifically. There have been some amazing breakthroughs", "timestamp": "00:01:24,796", "timestamp_s": 84.0}, {"text": "through using foundational models in different industries. A couple", "timestamp": "00:01:28,978", "timestamp_s": 88.0}, {"text": "of these are where we see impacts in life sciences,", "timestamp": "00:01:32,128", "timestamp_s": 92.0}, {"text": "with drug discovery being powered by Genei. This has", "timestamp": "00:01:35,094", "timestamp_s": 95.0}, {"text": "enabled researchers to understand things like protein synthesis.", "timestamp": "00:01:38,368", "timestamp_s": 98.0}, {"text": "In financial services, we see Genei being used to help create", "timestamp": "00:01:42,110", "timestamp_s": 102.0}, {"text": "highly tailored investment strategies that are aligned to individuals,", "timestamp": "00:01:45,556", "timestamp_s": 105.0}, {"text": "their risk, appetite, and also financial goals that they want to achieve.", "timestamp": "00:01:48,954", "timestamp_s": 108.0}, {"text": "In healthcare, we have seen how physicians and", "timestamp": "00:01:52,770", "timestamp_s": 112.0}, {"text": "clinicians can use this to enhance medical images and", "timestamp": "00:01:55,928", "timestamp_s": 115.0}, {"text": "also to aid in better diagnosis. Think like a medical assistant.", "timestamp": "00:01:59,624", "timestamp_s": 119.0}, {"text": "And in the retail space we see teams generating high", "timestamp": "00:02:03,310", "timestamp_s": 123.0}, {"text": "quality product descriptions and listings based on product data that they already", "timestamp": "00:02:06,792", "timestamp_s": 126.0}, {"text": "have. Now you\u0027ll notice a lot of the use cases", "timestamp": "00:02:10,396", "timestamp_s": 130.0}, {"text": "for generative AI are for enhancing existing processes or experience", "timestamp": "00:02:13,778", "timestamp_s": 133.0}, {"text": "that are already there. A question that usually comes is we", "timestamp": "00:02:17,564", "timestamp_s": 137.0}, {"text": "already have services and applications that are out there. How do we take generative AI", "timestamp": "00:02:21,552", "timestamp_s": 141.0}, {"text": "and then add that to enhance the experience versus rewriting everything from", "timestamp": "00:02:25,014", "timestamp_s": 145.0}, {"text": "scratch? Now, to understand this, what you need to also", "timestamp": "00:02:28,464", "timestamp_s": 148.0}, {"text": "understand is how you view generative AI.", "timestamp": "00:02:32,020", "timestamp_s": 152.0}, {"text": "So from AWS\u0027s perspective, Gen AI", "timestamp": "00:02:35,146", "timestamp_s": 155.0}, {"text": "has three macro layers and these three are equally important to us and we are", "timestamp": "00:02:38,602", "timestamp_s": 158.0}, {"text": "investing in all of them. The bottom layer is the infrastructure. This is used to", "timestamp": "00:02:42,228", "timestamp_s": 162.0}, {"text": "train foundational models and then run these models in production.", "timestamp": "00:02:46,008", "timestamp_s": 166.0}, {"text": "Then you have the middle layer that provides access to these large language models", "timestamp": "00:02:50,510", "timestamp_s": 170.0}, {"text": "and other FMs that you need and the tools that you need to build and", "timestamp": "00:02:54,574", "timestamp_s": 174.0}, {"text": "scale generative AI applications which then use the LLMs under", "timestamp": "00:02:57,788", "timestamp_s": 177.0}, {"text": "the hood. Then at the top layer you have applications", "timestamp": "00:03:01,148", "timestamp_s": 181.0}, {"text": "that are built leveraging foundational models so they take advantage", "timestamp": "00:03:04,594", "timestamp_s": 184.0}, {"text": "of Gen AI quickly and you don\u0027t need to have any", "timestamp": "00:03:08,082", "timestamp_s": 188.0}, {"text": "specialized knowledge. Now, when you take this and map this against the", "timestamp": "00:03:11,968", "timestamp_s": 191.0}, {"text": "services that we provide from AWS, you kind of", "timestamp": "00:03:16,128", "timestamp_s": 196.0}, {"text": "see that the three stacks are kind of neatly segregated. At the", "timestamp": "00:03:19,568", "timestamp_s": 199.0}, {"text": "lowest layer of the stack is the infrastructure. This is basically where you get", "timestamp": "00:03:23,348", "timestamp_s": 203.0}, {"text": "to build cost effective foundational models. You train", "timestamp": "00:03:27,730", "timestamp_s": 207.0}, {"text": "them and then you can deploy them at scale. This gives you access to our", "timestamp": "00:03:31,172", "timestamp_s": 211.0}, {"text": "hardware, accelerators and GPUs. And also you get access to services", "timestamp": "00:03:34,628", "timestamp_s": 214.0}, {"text": "like Amazon Sagemaker that enables ML practitioners and your teams to", "timestamp": "00:03:38,376", "timestamp_s": 218.0}, {"text": "build, train and deploy LLMs and foundational models.", "timestamp": "00:03:42,216", "timestamp_s": 222.0}, {"text": "Then at the middle layer we have Amazon bedrock. This provides access to", "timestamp": "00:03:45,678", "timestamp_s": 225.0}, {"text": "all LLMs and other foundational models that you need to build", "timestamp": "00:03:49,628", "timestamp_s": 229.0}, {"text": "and scale generative AI applications without you managing the whole", "timestamp": "00:03:53,292", "timestamp_s": 233.0}, {"text": "infrastructure behind it, right? Without you actually managing the scale side of", "timestamp": "00:03:57,244", "timestamp_s": 237.0}, {"text": "things. Think serverless, but for machine learning models for FMS,", "timestamp": "00:04:00,668", "timestamp_s": 240.0}, {"text": "basically, then at the top layer are applications that help you to", "timestamp": "00:04:04,630", "timestamp_s": 244.0}, {"text": "take advantage of Genei quickly as part of your day to day operations.", "timestamp": "00:04:07,904", "timestamp_s": 247.0}, {"text": "This includes services like Amazon Q, our new generative AI", "timestamp": "00:04:11,206", "timestamp_s": 251.0}, {"text": "powered assistant that is tailored to your business. So think like Personas,", "timestamp": "00:04:14,618", "timestamp_s": 254.0}, {"text": "which are business users, data users, or even developers.", "timestamp": "00:04:18,138", "timestamp_s": 258.0}, {"text": "You could use Q as part of AWS, a plugin that\u0027s", "timestamp": "00:04:22,842", "timestamp_s": 262.0}, {"text": "already available for certain services, and then afterwards use that to get", "timestamp": "00:04:26,654", "timestamp_s": 266.0}, {"text": "an enhanced operation capability.", "timestamp": "00:04:30,408", "timestamp_s": 270.0}, {"text": "Each of these layers builds on the other, and you may need some or all", "timestamp": "00:04:33,918", "timestamp_s": 273.0}, {"text": "of these capabilities at different points in your generative AI journey. A lot", "timestamp": "00:04:37,128", "timestamp_s": 277.0}, {"text": "of what you see is in an organization, you\u0027ll have a mix of Personas that", "timestamp": "00:04:40,600", "timestamp_s": 280.0}, {"text": "would use all three layers. You use specific services from those three layers", "timestamp": "00:04:43,324", "timestamp_s": 283.0}, {"text": "to enhance productivity.", "timestamp": "00:04:46,978", "timestamp_s": 286.0}, {"text": "Now, Amazon Bedrock is the easiest way to build and scale generative AI applications", "timestamp": "00:04:50,170", "timestamp_s": 290.0}, {"text": "with foundational models. This is a fully managed service so you can get started", "timestamp": "00:04:54,358", "timestamp_s": 294.0}, {"text": "quickly and you can find the right model based on the use case that you", "timestamp": "00:04:58,016", "timestamp_s": 298.0}, {"text": "have. You can then also customize your model with your own data,", "timestamp": "00:05:01,408", "timestamp_s": 301.0}, {"text": "and you can do this privately. Nothing feeds your data back to the base models,", "timestamp": "00:05:04,660", "timestamp_s": 304.0}, {"text": "which then other customers would also have access to.", "timestamp": "00:05:08,234", "timestamp_s": 308.0}, {"text": "This doesn\u0027t happen and you have the tools that you need to combine the", "timestamp": "00:05:11,652", "timestamp_s": 311.0}, {"text": "power of foundational models with your organization data and execute", "timestamp": "00:05:15,048", "timestamp_s": 315.0}, {"text": "complex tasks. All of this is with security, privacy and", "timestamp": "00:05:18,542", "timestamp_s": 318.0}, {"text": "responsible EI safety, which you need to then put generative AI", "timestamp": "00:05:22,392", "timestamp_s": 322.0}, {"text": "into production for your users. Now, there\u0027s a lot of", "timestamp": "00:05:26,558", "timestamp_s": 326.0}, {"text": "models that are out there and from Amazon bedrock. These are", "timestamp": "00:05:30,108", "timestamp_s": 330.0}, {"text": "a couple of models that we provide and one of the reasons that we went", "timestamp": "00:05:33,692", "timestamp_s": 333.0}, {"text": "with this model is because everything\u0027s moving fast.", "timestamp": "00:05:36,812", "timestamp_s": 336.0}, {"text": "Experimenting and learning is the key right now and also generative.", "timestamp": "00:05:40,012", "timestamp_s": 340.0}, {"text": "AI as a technology is also evolving quickly with new developments.", "timestamp": "00:05:43,542", "timestamp_s": 343.0}, {"text": "Now when things are moving so fast, the ability to adapt is the most valuable", "timestamp": "00:05:47,222", "timestamp_s": 347.0}, {"text": "capability that you can have. There is not going to be one", "timestamp": "00:05:50,886", "timestamp_s": 350.0}, {"text": "model to rule them all, and certainly not one company providing the models that", "timestamp": "00:05:54,144", "timestamp_s": 354.0}, {"text": "everyone uses. So you don\u0027t want a cloud provider who is beholden", "timestamp": "00:05:57,652", "timestamp_s": 357.0}, {"text": "primarily to one model provider. You need to be able to try out", "timestamp": "00:06:01,322", "timestamp_s": 361.0}, {"text": "different models. You should be able to switch between them rapidly", "timestamp": "00:06:04,788", "timestamp_s": 364.0}, {"text": "based on the use cases, or even combine multiple models within a", "timestamp": "00:06:08,542", "timestamp_s": 368.0}, {"text": "certain use case. You need a real choice of model providers, AWS. You decide", "timestamp": "00:06:12,216", "timestamp_s": 372.0}, {"text": "who has the best technology. This is kind of like where we have", "timestamp": "00:06:15,982", "timestamp_s": 375.0}, {"text": "seen based on our building services that we want to provide the choice", "timestamp": "00:06:19,692", "timestamp_s": 379.0}, {"text": "to customers, which is you. This is why we provide", "timestamp": "00:06:23,202", "timestamp_s": 383.0}, {"text": "through Pedrog, access to wide range of foundational models from", "timestamp": "00:06:27,516", "timestamp_s": 387.0}, {"text": "leaders like AI 21 labs, anthropic coher stability AI,", "timestamp": "00:06:31,312", "timestamp_s": 391.0}, {"text": "also access to our own foundational models like Amazon", "timestamp": "00:06:36,270", "timestamp_s": 396.0}, {"text": "Titan. And the idea is that we provide an API as", "timestamp": "00:06:40,038", "timestamp_s": 400.0}, {"text": "part of this. So there is a layer, an API layer that provides", "timestamp": "00:06:43,812", "timestamp_s": 403.0}, {"text": "you access to the large language models under the hood or the foundational models.", "timestamp": "00:06:47,738", "timestamp_s": 407.0}, {"text": "And all you do is as a user or probably as a", "timestamp": "00:06:51,498", "timestamp_s": 411.0}, {"text": "developer, you create the prompts in a certain format based on what", "timestamp": "00:06:54,868", "timestamp_s": 414.0}, {"text": "the foundational model expects. You take that prompt or text embeddings", "timestamp": "00:06:58,168", "timestamp_s": 418.0}, {"text": "if you want to tune that model a bit more, and then afterwards send", "timestamp": "00:07:02,398", "timestamp_s": 422.0}, {"text": "that to the API layer and you can then get your responses", "timestamp": "00:07:05,928", "timestamp_s": 425.0}, {"text": "back and then use that as part of your applications. Now there are a couple", "timestamp": "00:07:09,438", "timestamp_s": 429.0}, {"text": "of ways you can use bedrock. One of the ways customers", "timestamp": "00:07:12,268", "timestamp_s": 432.0}, {"text": "usually start is by writing code. And the way you", "timestamp": "00:07:15,884", "timestamp_s": 435.0}, {"text": "integrate with Amazon bedrock is that you can use the SDK, right? So you", "timestamp": "00:07:19,228", "timestamp_s": 439.0}, {"text": "use the APIs and then afterwards access the foundational models.", "timestamp": "00:07:23,184", "timestamp_s": 443.0}, {"text": "So you load the libraries that has a bedrock API and then afterwards you", "timestamp": "00:07:26,406", "timestamp_s": 446.0}, {"text": "can also access data in other places like an", "timestamp": "00:07:30,880", "timestamp_s": 450.0}, {"text": "S three bucket. If you have data that\u0027s bigger than what\u0027s normal,", "timestamp": "00:07:34,544", "timestamp_s": 454.0}, {"text": "you can then access it in s three bucket for input and even for output.", "timestamp": "00:07:37,594", "timestamp_s": 457.0}, {"text": "You can then prepare the input and then handle the JSON to bring convert and", "timestamp": "00:07:41,338", "timestamp_s": 461.0}, {"text": "then afterwards decode the responses. If the return data", "timestamp": "00:07:44,888", "timestamp_s": 464.0}, {"text": "is image, it\u0027s an image of sorts. You can then store", "timestamp": "00:07:48,776", "timestamp_s": 468.0}, {"text": "that in an S three bucket. Then if you have retries,", "timestamp": "00:07:52,616", "timestamp_s": 472.0}, {"text": "then you\u0027ll have to do retry logic inside, and then afterwards,", "timestamp": "00:07:55,886", "timestamp_s": 475.0}, {"text": "if you have any errors, you may have to have a certain condition, so on", "timestamp": "00:07:59,598", "timestamp_s": 479.0}, {"text": "and so forth. You kind of get an ideas of what happens with code in", "timestamp": "00:08:02,444", "timestamp_s": 482.0}, {"text": "general. Now, this is what the code would look like, but how", "timestamp": "00:08:05,084", "timestamp_s": 485.0}, {"text": "do we actually look at providing simpler integration without", "timestamp": "00:08:09,500", "timestamp_s": 489.0}, {"text": "writing a lot of code? And for this, you need to also understand", "timestamp": "00:08:12,832", "timestamp_s": 492.0}, {"text": "the whole idea of sequencing. Right. How do you coordinate between multiple services?", "timestamp": "00:08:16,576", "timestamp_s": 496.0}, {"text": "Because a lot of organizations don\u0027t just have one specific app,", "timestamp": "00:08:20,704", "timestamp_s": 500.0}, {"text": "they would have probably a plethora of apps that power their business.", "timestamp": "00:08:23,760", "timestamp_s": 503.0}, {"text": "And you want to understand how these services are going to talk to each other", "timestamp": "00:08:27,812", "timestamp_s": 507.0}, {"text": "in a reliable and understandable way, because business processes", "timestamp": "00:08:31,012", "timestamp_s": 511.0}, {"text": "usually exhibit different patterns based on the inputs that are coming", "timestamp": "00:08:34,682", "timestamp_s": 514.0}, {"text": "in and what needs to be accomplished. Sometimes things need to be done", "timestamp": "00:08:38,312", "timestamp_s": 518.0}, {"text": "sequentially. So in this case, let\u0027s say you have a number of lambda functions.", "timestamp": "00:08:41,992", "timestamp_s": 521.0}, {"text": "So we\u0027ll use lambda as a proxy to understand this for different services.", "timestamp": "00:08:45,358", "timestamp_s": 525.0}, {"text": "So you have a lambda one, and then you have a lambda two. Now,", "timestamp": "00:08:49,000", "timestamp_s": 529.0}, {"text": "this is easy enough because you can have these in sequence.", "timestamp": "00:08:51,788", "timestamp_s": 531.0}, {"text": "So lambda one invokes lambda two. But what if you have more than two", "timestamp": "00:08:54,658", "timestamp_s": 534.0}, {"text": "lambda functions? What if instead of calling lambda two, you need", "timestamp": "00:08:58,348", "timestamp_s": 538.0}, {"text": "lambda one to also call lambda seven before calling another service, or before", "timestamp": "00:09:01,728", "timestamp_s": 541.0}, {"text": "calling a foundational model in this case. Now,", "timestamp": "00:09:05,472", "timestamp_s": 545.0}, {"text": "if one of these services or functions fail, there\u0027s no easy", "timestamp": "00:09:08,752", "timestamp_s": 548.0}, {"text": "recovery mechanism, and reprocessing previously executed", "timestamp": "00:09:12,436", "timestamp_s": 552.0}, {"text": "steps becomes difficult. So we add some persistence inside.", "timestamp": "00:09:15,498", "timestamp_s": 555.0}, {"text": "That\u0027s the next step. You have persistence because you have all these executions", "timestamp": "00:09:19,460", "timestamp_s": 559.0}, {"text": "happening behind the scenes. And this way we can", "timestamp": "00:09:23,098", "timestamp_s": 563.0}, {"text": "deal with state, right? Try to manage some kind of a coordination, try to understand", "timestamp": "00:09:26,776", "timestamp_s": 566.0}, {"text": "which service is being executed at this point of time for this", "timestamp": "00:09:30,870", "timestamp_s": 570.0}, {"text": "whole execution flow that\u0027s happening now. Because of this,", "timestamp": "00:09:34,264", "timestamp_s": 574.0}, {"text": "you have to also collaborate all these functions. You need to manage this persistence mechanism.", "timestamp": "00:09:37,752", "timestamp_s": 577.0}, {"text": "And there\u0027s no elegant way of coordinating flow or error handling between these", "timestamp": "00:09:41,442", "timestamp_s": 581.0}, {"text": "services. And not every process is sequential.", "timestamp": "00:09:44,988", "timestamp_s": 584.0}, {"text": "So, for example, you could also have certain processes that need to run in parallel,", "timestamp": "00:09:49,234", "timestamp_s": 589.0}, {"text": "or perhaps it can follow different paths based on the input or what happens in", "timestamp": "00:09:53,158", "timestamp_s": 593.0}, {"text": "an earlier step. That\u0027s a lot harder to do, and it gets even harder", "timestamp": "00:09:57,024", "timestamp_s": 597.0}, {"text": "the more successful you are, because more people want to use the flow processes you\u0027ve", "timestamp": "00:10:00,678", "timestamp_s": 600.0}, {"text": "built out. You need to be able to handle errors as they occur.", "timestamp": "00:10:04,538", "timestamp_s": 604.0}, {"text": "And this could be things like retrying calls, or it", "timestamp": "00:10:08,610", "timestamp_s": 608.0}, {"text": "could be something as simple as following a different path in your workflow.", "timestamp": "00:10:12,148", "timestamp_s": 612.0}, {"text": "All in said, this is all things that you can still do in code.", "timestamp": "00:10:15,834", "timestamp_s": 615.0}, {"text": "This is something that has been done in code for quite some time. But what", "timestamp": "00:10:19,528", "timestamp_s": 619.0}, {"text": "if your flow also needs a human as part of the process? For example,", "timestamp": "00:10:22,792", "timestamp_s": 622.0}, {"text": "you need a human to review the output of a previous task to see if", "timestamp": "00:10:26,712", "timestamp_s": 626.0}, {"text": "it\u0027s accurate, like a spot check for example. Or you\u0027ve", "timestamp": "00:10:29,628", "timestamp_s": 629.0}, {"text": "built out an application processing flow where the customer has requested a", "timestamp": "00:10:33,474", "timestamp_s": 633.0}, {"text": "credit limit that exceeds the specified auto approved threshold.", "timestamp": "00:10:36,828", "timestamp_s": 636.0}, {"text": "And then you need somebody else to come in and then afterwards review", "timestamp": "00:10:40,466", "timestamp_s": 640.0}, {"text": "that request, and then after say okay, yes or no, depending on other", "timestamp": "00:10:44,400", "timestamp_s": 644.0}, {"text": "data that they have. So that application needs to be routed to a", "timestamp": "00:10:47,472", "timestamp_s": 647.0}, {"text": "human for this to work, and this continues.", "timestamp": "00:10:51,184", "timestamp_s": 651.0}, {"text": "So as long as you have business processes that need to emulate what happens in", "timestamp": "00:10:54,262", "timestamp_s": 654.0}, {"text": "the real world, you\u0027re going to have this amount of complexity that you", "timestamp": "00:10:58,244", "timestamp_s": 658.0}, {"text": "need to build as part of your applications. So one approach to", "timestamp": "00:11:01,524", "timestamp_s": 661.0}, {"text": "manage this complexity is that you don\u0027t have to write a lot of code", "timestamp": "00:11:05,556", "timestamp_s": 665.0}, {"text": "and communication. Instead, try to visualize your sequences as", "timestamp": "00:11:09,016", "timestamp_s": 669.0}, {"text": "part of a workflow. And this is where AWS step functions comes", "timestamp": "00:11:13,128", "timestamp_s": 673.0}, {"text": "in. Step functions is service that allows you to create workflows. These are", "timestamp": "00:11:16,552", "timestamp_s": 676.0}, {"text": "workflows that allow you to move output of one step to the input of the", "timestamp": "00:11:20,108", "timestamp_s": 680.0}, {"text": "next step. You can arrange these in a workflow with conditional logic", "timestamp": "00:11:23,164", "timestamp_s": 683.0}, {"text": "branches, parallel states, tools, a map state, or even", "timestamp": "00:11:26,818", "timestamp_s": 686.0}, {"text": "specify wait states, like for example if you\u0027re running a job and then you need", "timestamp": "00:11:30,540", "timestamp_s": 690.0}, {"text": "to wait for a certain period. Over here", "timestamp": "00:11:33,568", "timestamp_s": 693.0}, {"text": "you can see a bit of an animation that shows you", "timestamp": "00:11:37,168", "timestamp_s": 697.0}, {"text": "how you can choose a service. You then can then", "timestamp": "00:11:41,392", "timestamp_s": 701.0}, {"text": "drag it from the left and then after put in the design view. Then the", "timestamp": "00:11:44,932", "timestamp_s": 704.0}, {"text": "logic gets added. Then each step or action the workflow is configured.", "timestamp": "00:11:48,324", "timestamp_s": 708.0}, {"text": "This also helps you to visualize how you can provide error handling", "timestamp": "00:11:52,090", "timestamp_s": 712.0}, {"text": "and also specify retry and backup strategy.", "timestamp": "00:11:55,998", "timestamp_s": 715.0}, {"text": "Step functions is serverless, so you only pay for what you use. It scales", "timestamp": "00:11:59,646", "timestamp_s": 719.0}, {"text": "automatically, which also means that you can scale to zero.", "timestamp": "00:12:03,118", "timestamp_s": 723.0}, {"text": "You\u0027re not paying when it\u0027s not being invoked. This is fully managed and provides", "timestamp": "00:12:06,712", "timestamp_s": 726.0}, {"text": "a visual building experience using a drag and drop interface called workflow Studio.", "timestamp": "00:12:10,578", "timestamp_s": 730.0}, {"text": "The visualization experience extends beyond building because when you", "timestamp": "00:12:14,754", "timestamp_s": 734.0}, {"text": "run your workflow you can also visualize its progress with each step,", "timestamp": "00:12:18,252", "timestamp_s": 738.0}, {"text": "changing colors as it moves forward and under", "timestamp": "00:12:21,616", "timestamp_s": 741.0}, {"text": "the hood. What happens is this is using code which is using Amazon", "timestamp": "00:12:24,832", "timestamp_s": 744.0}, {"text": "State\u0027s language, which is ESL. ESL is a domain specific language", "timestamp": "00:12:28,118", "timestamp_s": 748.0}, {"text": "and it\u0027s JsoN based. So you can then declaratively create your", "timestamp": "00:12:32,102", "timestamp_s": 752.0}, {"text": "workflows. So you provide that and we\u0027ll show some examples later.", "timestamp": "00:12:36,036", "timestamp_s": 756.0}, {"text": "You can then take that ESL and then add that as part of your deployment", "timestamp": "00:12:39,970", "timestamp_s": 759.0}, {"text": "pipelines so you can commit it to your repositories. You can also make pull requests", "timestamp": "00:12:42,938", "timestamp_s": 762.0}, {"text": "on this so that other team members can collaborate.", "timestamp": "00:12:46,618", "timestamp_s": 766.0}, {"text": "Now one of the things customers have told us with step functions, because step functions", "timestamp": "00:12:49,598", "timestamp_s": 769.0}, {"text": "has been there for a few years, is that it integrates natively with 220 services", "timestamp": "00:12:52,894", "timestamp_s": 772.0}, {"text": "and you can choose a service that you need to use as part of your", "timestamp": "00:12:57,590", "timestamp_s": 777.0}, {"text": "workflow and take advantage of the benefits. Now the", "timestamp": "00:13:00,508", "timestamp_s": 780.0}, {"text": "way step functions integrates with these services is through two ways.", "timestamp": "00:13:04,028", "timestamp_s": 784.0}, {"text": "First is SDK integrations and the second is optimized integrations.", "timestamp": "00:13:07,820", "timestamp_s": 787.0}, {"text": "SDK integrations, as the name applies, are provided", "timestamp": "00:13:11,870", "timestamp_s": 791.0}, {"text": "by step functions by directly integrating with the AWS SDK.", "timestamp": "00:13:15,798", "timestamp_s": 795.0}, {"text": "So that\u0027s over 10,000 API actions that you can use directly", "timestamp": "00:13:19,462", "timestamp_s": 799.0}, {"text": "from your workflow without the need to write any customer integration code.", "timestamp": "00:13:22,598", "timestamp_s": 802.0}, {"text": "Think blue code, which a lot of folks when they write serverless", "timestamp": "00:13:25,812", "timestamp_s": 805.0}, {"text": "applications with lambda you tend to write. You can remove a", "timestamp": "00:13:29,514", "timestamp_s": 809.0}, {"text": "lot of that just by using step functions. The other one is optimize", "timestamp": "00:13:33,044", "timestamp_s": 813.0}, {"text": "integrations. Now the way they differ from SDK integrations is that each", "timestamp": "00:13:36,554", "timestamp_s": 816.0}, {"text": "action has been customized to provide additional functionality for your workflow.", "timestamp": "00:13:40,376", "timestamp_s": 820.0}, {"text": "So beyond just the API call, you also get certain things like for", "timestamp": "00:13:44,334", "timestamp_s": 824.0}, {"text": "example where an API output is being converted from an", "timestamp": "00:13:47,608", "timestamp_s": 827.0}, {"text": "escape JSON to a json object. So depending on the kind of integration", "timestamp": "00:13:51,164", "timestamp_s": 831.0}, {"text": "that\u0027s bring, provided, those optimized integrations have that added value", "timestamp": "00:13:54,562", "timestamp_s": 834.0}, {"text": "needed so that you don\u0027t have to then write extra code for", "timestamp": "00:13:58,556", "timestamp_s": 838.0}, {"text": "maybe doing those manipulations. Now with any workflow", "timestamp": "00:14:02,176", "timestamp_s": 842.0}, {"text": "and orchestration around, you need to have certain patterns that are provided,", "timestamp": "00:14:06,006", "timestamp_s": 846.0}, {"text": "and these integration patterns by default are", "timestamp": "00:14:09,622", "timestamp_s": 849.0}, {"text": "something that API actions can be provided", "timestamp": "00:14:12,932", "timestamp_s": 852.0}, {"text": "with. So when you specify your workflow by default,", "timestamp": "00:14:16,698", "timestamp_s": 856.0}, {"text": "it is asynchronous so the workflow doesn\u0027t wait or block for the action", "timestamp": "00:14:20,378", "timestamp_s": 860.0}, {"text": "to complete. This is what you call as a standard request response call pattern.", "timestamp": "00:14:23,978", "timestamp_s": 863.0}, {"text": "So you start the task or the work to be done and the workflow doesn\u0027t", "timestamp": "00:14:28,186", "timestamp_s": 868.0}, {"text": "wait for complete, it moves on to the next step. This is great because it\u0027s", "timestamp": "00:14:31,374", "timestamp_s": 871.0}, {"text": "efficient. You can continue moving quickly, but sometimes", "timestamp": "00:14:34,638", "timestamp_s": 874.0}, {"text": "there are cases where you may need to wait until the request is complete and", "timestamp": "00:14:37,736", "timestamp_s": 877.0}, {"text": "then you progress. And there is an optimized integration pattern", "timestamp": "00:14:41,068", "timestamp_s": 881.0}, {"text": "called job run or also called sync. Because of", "timestamp": "00:14:45,122", "timestamp_s": 885.0}, {"text": "the word dot sync that\u0027s added to the end of the API action. Then you", "timestamp": "00:14:48,588", "timestamp_s": 888.0}, {"text": "also have a callback. This is what helps us to introduce a human into our", "timestamp": "00:14:52,124", "timestamp_s": 892.0}, {"text": "flow and we\u0027re bring to see a bit of that in the architecture later.", "timestamp": "00:14:55,552", "timestamp_s": 895.0}, {"text": "Now with these integrations that are available,", "timestamp": "00:14:59,710", "timestamp_s": 899.0}, {"text": "you then have an idea of how you can take a business process", "timestamp": "00:15:02,740", "timestamp_s": 902.0}, {"text": "and then afterwards integrate that across. But just to understand", "timestamp": "00:15:06,324", "timestamp_s": 906.0}, {"text": "why this is important, let\u0027s take an example of a standard", "timestamp": "00:15:10,484", "timestamp_s": 910.0}, {"text": "serverless application and show you why direct integration", "timestamp": "00:15:14,036", "timestamp_s": 914.0}, {"text": "actually makes more sense. So here\u0027s a classic example. You\u0027re querying", "timestamp": "00:15:17,310", "timestamp_s": 917.0}, {"text": "a database. So we have a lambda function that needs to get an item", "timestamp": "00:15:20,574", "timestamp_s": 920.0}, {"text": "from a dynamodb table. So from a code perspective,", "timestamp": "00:15:23,806", "timestamp_s": 923.0}, {"text": "what do I need to get started? I need the import AWs SDK", "timestamp": "00:15:27,198", "timestamp_s": 927.0}, {"text": "to interact with the table. Then I need to set up my parameters", "timestamp": "00:15:30,818", "timestamp_s": 930.0}, {"text": "to tell dynamodb what table I need to interact with. So this is", "timestamp": "00:15:34,082", "timestamp_s": 934.0}, {"text": "like the table name, the partition key, the sort key, and then", "timestamp": "00:15:37,468", "timestamp_s": 937.0}, {"text": "I set up my query so that there is a try catch block and then", "timestamp": "00:15:41,068", "timestamp_s": 941.0}, {"text": "I return any errors. Now above that I also need", "timestamp": "00:15:43,888", "timestamp_s": 943.0}, {"text": "to add lambda export handlers with my event object, my context", "timestamp": "00:15:47,568", "timestamp_s": 947.0}, {"text": "object, and then add another try catch block to catch other errors.", "timestamp": "00:15:51,062", "timestamp_s": 951.0}, {"text": "I may also need to convert data", "timestamp": "00:15:54,794", "timestamp_s": 954.0}, {"text": "structures like for example an object to a string, for example, for other reasons.", "timestamp": "00:15:57,972", "timestamp_s": 957.0}, {"text": "But you can see there\u0027s a lot of lines of code just to get one", "timestamp": "00:16:01,738", "timestamp_s": 961.0}, {"text": "item from a dynamodb table. Now each of these lines is an", "timestamp": "00:16:04,468", "timestamp_s": 964.0}, {"text": "area that something can go wrong. Because one thing", "timestamp": "00:16:07,832", "timestamp_s": 967.0}, {"text": "you have to understand is code is also a liability, right? When you write code,", "timestamp": "00:16:11,048", "timestamp_s": 971.0}, {"text": "you are responsible for the way it functions. You have to make sure that you\u0027re", "timestamp": "00:16:14,408", "timestamp_s": 974.0}, {"text": "writing it securely, you\u0027re using the right set of dependencies, ensuring that there\u0027s", "timestamp": "00:16:17,054", "timestamp_s": 977.0}, {"text": "no memory leaks and so on and so forth. Now when you look at it", "timestamp": "00:16:20,978", "timestamp_s": 980.0}, {"text": "from a step functions perspective, what you can do is you have a", "timestamp": "00:16:24,348", "timestamp_s": 984.0}, {"text": "single step that makes that item call to a dynamodb table and it\u0027s", "timestamp": "00:16:28,108", "timestamp_s": 988.0}, {"text": "just a scalable, right? I can still configure things like retries, I can still", "timestamp": "00:16:32,194", "timestamp_s": 992.0}, {"text": "catch any errors and then send that to a dead letter queue if I", "timestamp": "00:16:36,064", "timestamp_s": 996.0}, {"text": "need to so that I can do a retry later. And if you notice,", "timestamp": "00:16:39,424", "timestamp_s": 999.0}, {"text": "what happens is that this diagram isn\u0027t just a visual representation,", "timestamp": "00:16:42,422", "timestamp_s": 1002.0}, {"text": "this is actually showing how you can take a certain action", "timestamp": "00:16:46,042", "timestamp_s": 1006.0}, {"text": "and then after do that, take it from start till finish.", "timestamp": "00:16:49,338", "timestamp_s": 1009.0}, {"text": "And you can show this to other folks in your engineering team. You can also", "timestamp": "00:16:52,548", "timestamp_s": 1012.0}, {"text": "show this to business stakeholders so that they can understand what a flow looks like.", "timestamp": "00:16:55,608", "timestamp_s": 1015.0}, {"text": "So added value with of course the whole idea of errors and", "timestamp": "00:16:59,032", "timestamp_s": 1019.0}, {"text": "retries and the way it would look at when you", "timestamp": "00:17:03,000", "timestamp_s": 1023.0}, {"text": "actually add the nodes in the end with certain integrations is like this,", "timestamp": "00:17:06,712", "timestamp_s": 1026.0}, {"text": "right? So you have dynamodb, you have the getitem side, you have SQs send", "timestamp": "00:17:10,028", "timestamp_s": 1030.0}, {"text": "message, so on and so forth. One other thing during development,", "timestamp": "00:17:13,548", "timestamp_s": 1033.0}, {"text": "or even when you deploy a step function to production, is that", "timestamp": "00:17:17,058", "timestamp_s": 1037.0}, {"text": "you need to understand what\u0027s happening in the workflow and when things go wrong.", "timestamp": "00:17:20,268", "timestamp_s": 1040.0}, {"text": "And the way you do that is you have the execution flow where you can", "timestamp": "00:17:23,168", "timestamp_s": 1043.0}, {"text": "see different parts of the execution and then you can go within a specific execution,", "timestamp": "00:17:26,768", "timestamp_s": 1046.0}, {"text": "see the different states, what\u0027s happening within each state, what\u0027s the input", "timestamp": "00:17:30,742", "timestamp_s": 1050.0}, {"text": "and what\u0027s the output, and also look at things like how much time it takes", "timestamp": "00:17:34,198", "timestamp_s": 1054.0}, {"text": "to execute a certain state. And this is really critical when there are", "timestamp": "00:17:37,572", "timestamp_s": 1057.0}, {"text": "issues. So a great way to get all of that together and then see that", "timestamp": "00:17:41,348", "timestamp_s": 1061.0}, {"text": "in a single pane. Now let\u0027s dive into an", "timestamp": "00:17:44,308", "timestamp_s": 1064.0}, {"text": "actual use case, right? And we have a demo towards the end. I\u0027ll show a", "timestamp": "00:17:48,072", "timestamp_s": 1068.0}, {"text": "couple of demos in the middle, also about bedrock and integration,", "timestamp": "00:17:51,864", "timestamp_s": 1071.0}, {"text": "and then one where it looks at an application that uses all", "timestamp": "00:17:55,442", "timestamp_s": 1075.0}, {"text": "of this together. So let\u0027s say you", "timestamp": "00:17:59,004", "timestamp_s": 1079.0}, {"text": "have an application that has videos being", "timestamp": "00:18:02,732", "timestamp_s": 1082.0}, {"text": "uploaded, and then these videos need to be transcribed,", "timestamp": "00:18:06,572", "timestamp_s": 1086.0}, {"text": "right? So we already have a service that\u0027s available called Amazon", "timestamp": "00:18:10,546", "timestamp_s": 1090.0}, {"text": "transcribe. And in step functions, all I need to do is I can", "timestamp": "00:18:13,718", "timestamp_s": 1093.0}, {"text": "drag in a transcription job start node,", "timestamp": "00:18:17,104", "timestamp_s": 1097.0}, {"text": "so I can drag that in and then afterwards say, okay, fine, for any", "timestamp": "00:18:21,062", "timestamp_s": 1101.0}, {"text": "image that, and then trigger that step function for any video that comes in,", "timestamp": "00:18:24,640", "timestamp_s": 1104.0}, {"text": "for example, just kick in and then afterwards do a transcription of", "timestamp": "00:18:27,988", "timestamp_s": 1107.0}, {"text": "that video. So automatic speech recognition", "timestamp": "00:18:31,428", "timestamp_s": 1111.0}, {"text": "happens. And this makes it easy for developers to add speech to text capability", "timestamp": "00:18:34,762", "timestamp_s": 1114.0}, {"text": "to their applications. This integration is super powerful.", "timestamp": "00:18:38,526", "timestamp_s": 1118.0}, {"text": "This allows you to just have this without any code that\u0027s needed. Now let\u0027s", "timestamp": "00:18:42,382", "timestamp_s": 1122.0}, {"text": "say I want to also do something beyond this, right? So I want to take", "timestamp": "00:18:46,238", "timestamp_s": 1126.0}, {"text": "that transcription and I want to add some additional stuff.", "timestamp": "00:18:49,948", "timestamp_s": 1129.0}, {"text": "And this is where generative AI can help us. So I want to create multiple", "timestamp": "00:18:53,116", "timestamp_s": 1133.0}, {"text": "titles and descriptions for a video. I want to ask a human", "timestamp": "00:18:56,642", "timestamp_s": 1136.0}, {"text": "to provide feedback based on what choice they want to", "timestamp": "00:19:00,380", "timestamp_s": 1140.0}, {"text": "have from the titles and then also create an avatar for the video.", "timestamp": "00:19:04,128", "timestamp_s": 1144.0}, {"text": "So you have text also, and you have also image generation happening. And the", "timestamp": "00:19:07,472", "timestamp_s": 1147.0}, {"text": "way you do this with step functions is you can look at", "timestamp": "00:19:11,024", "timestamp_s": 1151.0}, {"text": "optimized integrations for Amazon bedrock. Now there are", "timestamp": "00:19:14,292", "timestamp_s": 1154.0}, {"text": "two new optimized integrations that we have provided, and there\u0027s more that\u0027s", "timestamp": "00:19:17,412", "timestamp_s": 1157.0}, {"text": "been added ever since where the first one is invoke", "timestamp": "00:19:20,618", "timestamp_s": 1160.0}, {"text": "model. And this invoke model API integration allows you to orchestrate", "timestamp": "00:19:24,042", "timestamp_s": 1164.0}, {"text": "interactions with foundational models. So you call the API directly", "timestamp": "00:19:27,742", "timestamp_s": 1167.0}, {"text": "through step functions. You give it the parameters that are needed, you provide the", "timestamp": "00:19:31,678", "timestamp_s": 1171.0}, {"text": "prompt that is needed and then that gets sent to the foundation model. You get", "timestamp": "00:19:34,888", "timestamp_s": 1174.0}, {"text": "the response back and then you can continue using that. The second one", "timestamp": "00:19:37,768", "timestamp_s": 1177.0}, {"text": "is the create model customization job. Now what this does", "timestamp": "00:19:41,212", "timestamp_s": 1181.0}, {"text": "is this supports the run a job, the dot sync call pattern that we saw", "timestamp": "00:19:44,348", "timestamp_s": 1184.0}, {"text": "earlier. And this means that it is waiting for the asynchronous", "timestamp": "00:19:47,772", "timestamp_s": 1187.0}, {"text": "job to complete before progressing to the next step in your workflow.", "timestamp": "00:19:52,262", "timestamp_s": 1192.0}, {"text": "So say for example, you\u0027re trying to create a certain customization on top of the", "timestamp": "00:19:55,446", "timestamp_s": 1195.0}, {"text": "foundational model. It\u0027ll wait for that and then it\u0027ll go to the next step and", "timestamp": "00:19:59,328", "timestamp_s": 1199.0}, {"text": "then afterwards continue with that process. This is useful especially in", "timestamp": "00:20:02,704", "timestamp_s": 1202.0}, {"text": "data processing pipelines because you are trying to do some kind of fine tuning to", "timestamp": "00:20:05,908", "timestamp_s": 1205.0}, {"text": "the model. I\u0027ll quickly jump into demo", "timestamp": "00:20:09,764", "timestamp_s": 1209.0}, {"text": "so that you can actually see what happens with standard", "timestamp": "00:20:13,460", "timestamp_s": 1213.0}, {"text": "implementation with bedrock. Just quickly to understand if", "timestamp": "00:20:17,590", "timestamp_s": 1217.0}, {"text": "you\u0027re getting started with bedrock, you need to make sure that you have access to", "timestamp": "00:20:21,224", "timestamp_s": 1221.0}, {"text": "the models. Right now you have access to foundational models", "timestamp": "00:20:24,024", "timestamp_s": 1224.0}, {"text": "in two regions, that\u0027s North Virginia and also Oregon.", "timestamp": "00:20:28,082", "timestamp_s": 1228.0}, {"text": "When you go to the bedrock screen you will actually see there\u0027s", "timestamp": "00:20:32,170", "timestamp_s": 1232.0}, {"text": "a section called the model access. And this gives you a list of all the", "timestamp": "00:20:35,634", "timestamp_s": 1235.0}, {"text": "models that are available right now in those two regions.", "timestamp": "00:20:39,228", "timestamp_s": 1239.0}, {"text": "And if you\u0027re doing it for the first time, you will have to go and", "timestamp": "00:20:42,886", "timestamp_s": 1242.0}, {"text": "manage your model access and then grant access to it. You\u0027ll get", "timestamp": "00:20:45,952", "timestamp_s": 1245.0}, {"text": "that immediately unless it\u0027s a brand new model that takes a bit of time", "timestamp": "00:20:49,088", "timestamp_s": 1249.0}, {"text": "where you may have to submit certain use cases. In my case right", "timestamp": "00:20:52,784", "timestamp_s": 1252.0}, {"text": "now I have clot three that\u0027s in the pipeline. I\u0027m waiting for the", "timestamp": "00:20:56,148", "timestamp_s": 1256.0}, {"text": "details to get approved so that I can get access to this clot three just", "timestamp": "00:20:59,812", "timestamp_s": 1259.0}, {"text": "got announced a few days ago, support in bedrock.", "timestamp": "00:21:02,948", "timestamp_s": 1262.0}, {"text": "So I have that immediately ready. Now let me jump in directly", "timestamp": "00:21:06,298", "timestamp_s": 1266.0}, {"text": "into a workflow. When you go to step function and you create a new step", "timestamp": "00:21:10,158", "timestamp_s": 1270.0}, {"text": "function, you\u0027re greeted with a blank canvas. You have a state box that\u0027s empty", "timestamp": "00:21:13,752", "timestamp_s": 1273.0}, {"text": "over here. In my case I already dragged in", "timestamp": "00:21:17,438", "timestamp_s": 1277.0}, {"text": "bedrock API and if you want to see", "timestamp": "00:21:20,764", "timestamp_s": 1280.0}, {"text": "the list of bedrock APIs that are currently available, you have much more right now", "timestamp": "00:21:24,188", "timestamp_s": 1284.0}, {"text": "where you can also manage operations on foundational models if", "timestamp": "00:21:27,692", "timestamp_s": 1287.0}, {"text": "you need to. Things like the custom models for example and listings,", "timestamp": "00:21:31,184", "timestamp_s": 1291.0}, {"text": "especially for processing pipelines, MLO Ops, so on and so forth.", "timestamp": "00:21:35,430", "timestamp_s": 1295.0}, {"text": "In our case I just want to do an invoke model. So I\u0027m going to", "timestamp": "00:21:38,982", "timestamp_s": 1298.0}, {"text": "just show you what the configuration looks like. I have foundation models already selected,", "timestamp": "00:21:43,024", "timestamp_s": 1303.0}, {"text": "and these are the list of foundation models that are already available. As you saw", "timestamp": "00:21:47,642", "timestamp_s": 1307.0}, {"text": "in the previous screen. In this case I have", "timestamp": "00:21:50,788", "timestamp_s": 1310.0}, {"text": "selected llama. So llama two is already selected in", "timestamp": "00:21:54,324", "timestamp_s": 1314.0}, {"text": "this case, and now you can configure what are the parameters that", "timestamp": "00:21:57,448", "timestamp_s": 1317.0}, {"text": "need to be sent. What I\u0027m doing over here is I\u0027m just hard coding the", "timestamp": "00:22:00,968", "timestamp_s": 1320.0}, {"text": "prompt in another demo. Quickly after this I\u0027m going to show", "timestamp": "00:22:04,824", "timestamp_s": 1324.0}, {"text": "where you can actually customize the prompts based on input that you", "timestamp": "00:22:08,040", "timestamp_s": 1328.0}, {"text": "may get from other applications or maybe from the user. In my case.", "timestamp": "00:22:11,628", "timestamp_s": 1331.0}, {"text": "All I\u0027m saying is, okay, there\u0027s a transcript from a video in a paragraph.", "timestamp": "00:22:14,892", "timestamp_s": 1334.0}, {"text": "This is the same video you\u0027re going to see in the last demo.", "timestamp": "00:22:17,746", "timestamp_s": 1337.0}, {"text": "This is an interview between Amazon\u0027s CTO Werner", "timestamp": "00:22:21,150", "timestamp_s": 1341.0}, {"text": "Vogels and ex Amazon CEO Jeff Bezos. This is from 2012,", "timestamp": "00:22:24,838", "timestamp_s": 1344.0}, {"text": "so eleven years old, and all it does is it", "timestamp": "00:22:28,256", "timestamp_s": 1348.0}, {"text": "uses this transcript, and then I\u0027m asking it to provide", "timestamp": "00:22:32,016", "timestamp_s": 1352.0}, {"text": "a summary of this transcript. So what I\u0027ll", "timestamp": "00:22:35,316", "timestamp_s": 1355.0}, {"text": "do quickly is I\u0027ll just do an execution, and we\u0027re going to", "timestamp": "00:22:38,618", "timestamp_s": 1358.0}, {"text": "see how it looks like when you do an execution. I\u0027m not passing any input,", "timestamp": "00:22:41,748", "timestamp_s": 1361.0}, {"text": "it\u0027s optional because I\u0027ve already hard coded the prompt over there.", "timestamp": "00:22:45,322", "timestamp_s": 1365.0}, {"text": "Once I run this, and within a certain execution history", "timestamp": "00:22:48,552", "timestamp_s": 1368.0}, {"text": "or a certain point of execution, you can see the actual path.", "timestamp": "00:22:52,104", "timestamp_s": 1372.0}, {"text": "You can see what are the different steps that are being executed. And with", "timestamp": "00:22:55,934", "timestamp_s": 1375.0}, {"text": "bedrock model already done in this case,", "timestamp": "00:22:59,612", "timestamp_s": 1379.0}, {"text": "you can see that the input just was an optional input that got", "timestamp": "00:23:03,436", "timestamp_s": 1383.0}, {"text": "sent out. And here is a summary that\u0027s come back from llama two. This is", "timestamp": "00:23:06,748", "timestamp_s": 1386.0}, {"text": "basically a summary of the transcript. It gives an example of what", "timestamp": "00:23:10,220", "timestamp_s": 1390.0}, {"text": "Jeff Bezos mentioned and what\u0027s the whole organization working", "timestamp": "00:23:13,904", "timestamp_s": 1393.0}, {"text": "on towards. This was eleven years ago. You also get other parameters like how much", "timestamp": "00:23:17,616", "timestamp_s": 1397.0}, {"text": "prompts were taken and generation token. All in all, without provisioning any", "timestamp": "00:23:21,248", "timestamp_s": 1401.0}, {"text": "large language models, without you actually managing the scaling side or", "timestamp": "00:23:24,852", "timestamp_s": 1404.0}, {"text": "even provisioning a large language model. So pretty cool. And the", "timestamp": "00:23:28,292", "timestamp_s": 1408.0}, {"text": "other thing, what you\u0027ll realize is with step functions you also are able to", "timestamp": "00:23:32,148", "timestamp_s": 1412.0}, {"text": "view the different states and how much time they took to execute.", "timestamp": "00:23:35,876", "timestamp_s": 1415.0}, {"text": "So really useful, especially if you want to debug certain things. If there\u0027s any failures", "timestamp": "00:23:39,742", "timestamp_s": 1419.0}, {"text": "that you get that. Also over here you can actually see those errors over there.", "timestamp": "00:23:43,582", "timestamp_s": 1423.0}, {"text": "Now another powerful way of showing what bedrock", "timestamp": "00:23:47,290", "timestamp_s": 1427.0}, {"text": "is capable of through step functions is chaining.", "timestamp": "00:23:50,818", "timestamp_s": 1430.0}, {"text": "And this is another demo application. What this does is this emulates", "timestamp": "00:23:53,810", "timestamp_s": 1433.0}, {"text": "a certain conversation that you can have with", "timestamp": "00:23:58,162", "timestamp_s": 1438.0}, {"text": "an LLM, with anything that\u0027s doing text, right? So, for example, you have a chat", "timestamp": "00:24:01,724", "timestamp_s": 1441.0}, {"text": "interface, and with any large language model, you have to always", "timestamp": "00:24:05,158", "timestamp_s": 1445.0}, {"text": "provide the context of, especially the history of the conversation that\u0027s happening,", "timestamp": "00:24:08,656", "timestamp_s": 1448.0}, {"text": "so that the next one can then understand the next conversation,", "timestamp": "00:24:11,872", "timestamp_s": 1451.0}, {"text": "or the response can be based on that conversation from before.", "timestamp": "00:24:14,902", "timestamp_s": 1454.0}, {"text": "So in our case, what we are doing is we\u0027re creating a chain, and in", "timestamp": "00:24:18,452", "timestamp_s": 1458.0}, {"text": "this case, I\u0027m leveraging another foundational model called", "timestamp": "00:24:21,924", "timestamp_s": 1461.0}, {"text": "command text from coher. And what this does is this is", "timestamp": "00:24:26,232", "timestamp_s": 1466.0}, {"text": "reading a prompt. So it\u0027s going to read for the prompt from the input.", "timestamp": "00:24:31,510", "timestamp_s": 1471.0}, {"text": "So when you invoke the step function, you can actually have a look", "timestamp": "00:24:35,022", "timestamp_s": 1475.0}, {"text": "at what are the different parameters that are there", "timestamp": "00:24:38,232", "timestamp_s": 1478.0}, {"text": "in the object, in the json body, and then afterwards you can pick that out.", "timestamp": "00:24:42,028", "timestamp_s": 1482.0}, {"text": "In our case, what I\u0027m doing is, I\u0027m just saying, okay, dollar prompt one,", "timestamp": "00:24:45,308", "timestamp_s": 1485.0}, {"text": "send this as a prompt, and these are the maximum tokens. Now, in this case,", "timestamp": "00:24:48,876", "timestamp_s": 1488.0}, {"text": "you\u0027ll see this is a different syntax based", "timestamp": "00:24:51,872", "timestamp_s": 1491.0}, {"text": "on this model versus what was there for llama two.", "timestamp": "00:24:55,232", "timestamp_s": 1495.0}, {"text": "And all I\u0027m doing is I\u0027m adding the result", "timestamp": "00:24:58,416", "timestamp_s": 1498.0}, {"text": "of this conversation back to the initial prompts", "timestamp": "00:25:01,600", "timestamp_s": 1501.0}, {"text": "that are coming in so that we have context throughout this conversation.", "timestamp": "00:25:05,594", "timestamp_s": 1505.0}, {"text": "And now if I just go in and execute this,", "timestamp": "00:25:08,922", "timestamp_s": 1508.0}, {"text": "I\u0027ll just copy this from a previous one, because I want to pass a similar", "timestamp": "00:25:12,210", "timestamp_s": 1512.0}, {"text": "input. I\u0027ll just do an execution over here.", "timestamp": "00:25:15,684", "timestamp_s": 1515.0}, {"text": "In my case, I\u0027m passing three prompts, if you notice, in the state", "timestamp": "00:25:18,950", "timestamp_s": 1518.0}, {"text": "also, I had three of them. And all I\u0027m doing is, I\u0027m saying, okay,", "timestamp": "00:25:22,392", "timestamp_s": 1522.0}, {"text": "name a random city from southeast Asia. Just want you to give some information,", "timestamp": "00:25:24,952", "timestamp_s": 1524.0}, {"text": "provide some description for it, and then provide some more description for it.", "timestamp": "00:25:28,652", "timestamp_s": 1528.0}, {"text": "So let\u0027s start the execution, and as you\u0027ll see,", "timestamp": "00:25:31,948", "timestamp_s": 1531.0}, {"text": "as the execution progresses, you\u0027re going to see all these states", "timestamp": "00:25:35,868", "timestamp_s": 1535.0}, {"text": "changing the colors based on how the", "timestamp": "00:25:39,760", "timestamp_s": 1539.0}, {"text": "foundational model is responding. So the first result is already", "timestamp": "00:25:43,312", "timestamp_s": 1543.0}, {"text": "in. So it says, okay, here is a random city from Southeast Asia.", "timestamp": "00:25:47,136", "timestamp_s": 1547.0}, {"text": "So it picks Ho Chi Minh from Vietnam.", "timestamp": "00:25:50,758", "timestamp_s": 1550.0}, {"text": "Packages that in as part of this result, one is already added", "timestamp": "00:25:53,810", "timestamp_s": 1553.0}, {"text": "in, and then afterwards sends it to the second conversation history. You\u0027ll see", "timestamp": "00:25:57,418", "timestamp_s": 1557.0}, {"text": "conversation result two. Here are two interesting aspects of the city,", "timestamp": "00:26:01,476", "timestamp_s": 1561.0}, {"text": "and it mentions certain parts of this. And then invoke model with", "timestamp": "00:26:04,888", "timestamp_s": 1564.0}, {"text": "three. And the output over here is that it takes in certain", "timestamp": "00:26:08,712", "timestamp_s": 1568.0}, {"text": "part. Now, with large language models being", "timestamp": "00:26:12,856", "timestamp_s": 1572.0}, {"text": "nondeterministic, a lot of times you have to be careful with how you send", "timestamp": "00:26:16,008", "timestamp_s": 1576.0}, {"text": "your prompts and then ensure that the context is remaining. Now, in a previous execution", "timestamp": "00:26:19,612", "timestamp_s": 1579.0}, {"text": "of the same workflow, I was able to get the third prompt and", "timestamp": "00:26:23,602", "timestamp_s": 1583.0}, {"text": "also make sure that it continues with the city, which was previously Ho", "timestamp": "00:26:27,948", "timestamp_s": 1587.0}, {"text": "Chi Minh. So what I would probably want to do is I would create my", "timestamp": "00:26:31,388", "timestamp_s": 1591.0}, {"text": "third prompt in such a way that I emphasize it clearly that this is", "timestamp": "00:26:34,688", "timestamp_s": 1594.0}, {"text": "the city that you\u0027re supposed to use. And probably the way I", "timestamp": "00:26:37,968", "timestamp_s": 1597.0}, {"text": "would do that is I would have certain parts in my inputs, which would probably", "timestamp": "00:26:41,424", "timestamp_s": 1601.0}, {"text": "take certain things like the city or other things and then enforce that as part", "timestamp": "00:26:45,232", "timestamp_s": 1605.0}, {"text": "of different prompts. But in a nutshell, you kind of see how you can", "timestamp": "00:26:48,388", "timestamp_s": 1608.0}, {"text": "do chaining in this case, and you can also bring that within", "timestamp": "00:26:52,228", "timestamp_s": 1612.0}, {"text": "this and have a bigger application that is using this. And we\u0027re going to talk", "timestamp": "00:26:56,036", "timestamp_s": 1616.0}, {"text": "about the architecture of this for the rest of the session.", "timestamp": "00:26:59,032", "timestamp_s": 1619.0}, {"text": "So let\u0027s continue with that use case of generating titles and descriptions", "timestamp": "00:27:02,790", "timestamp_s": 1622.0}, {"text": "for the videos in this case. What happens is that,", "timestamp": "00:27:07,294", "timestamp_s": 1627.0}, {"text": "like you saw earlier in the demo that I showed, you can select the", "timestamp": "00:27:10,204", "timestamp_s": 1630.0}, {"text": "large language model. In this case, Titan is selected.", "timestamp": "00:27:14,892", "timestamp_s": 1634.0}, {"text": "And what under the hood happens is that the ESL for Amazon bedrock", "timestamp": "00:27:18,258", "timestamp_s": 1638.0}, {"text": "looks something like this, right? So there\u0027s an invoke model action that\u0027s happening, and then", "timestamp": "00:27:21,858", "timestamp_s": 1641.0}, {"text": "there is a model that is being selected. It could be llama,", "timestamp": "00:27:25,312", "timestamp_s": 1645.0}, {"text": "it could be anything else. And then there is a dynamic input that\u0027s coming in.", "timestamp": "00:27:29,174", "timestamp_s": 1649.0}, {"text": "So dollar prompt, which basically means something else, is invoking the", "timestamp": "00:27:32,388", "timestamp_s": 1652.0}, {"text": "step function and then providing this prompt. Now you have also inference", "timestamp": "00:27:35,764", "timestamp_s": 1655.0}, {"text": "parameters that allow you to tweak the response", "timestamp": "00:27:39,498", "timestamp_s": 1659.0}, {"text": "that comes back from an LLM for various things like probability and other", "timestamp": "00:27:43,114", "timestamp_s": 1663.0}, {"text": "things. And when you look at", "timestamp": "00:27:46,552", "timestamp_s": 1666.0}, {"text": "invoking the model, you can also provide input and output.", "timestamp": "00:27:50,200", "timestamp_s": 1670.0}, {"text": "So for example, if your input is larger than 256 kb, because a", "timestamp": "00:27:53,598", "timestamp_s": 1673.0}, {"text": "step function can only take 256 kb of content text,", "timestamp": "00:27:56,888", "timestamp_s": 1676.0}, {"text": "usually in this case, what you can do is you can point to an S", "timestamp": "00:28:00,072", "timestamp_s": 1680.0}, {"text": "three bucket for input and for output. It\u0027s a good way to ensure", "timestamp": "00:28:03,308", "timestamp_s": 1683.0}, {"text": "that you\u0027re able to scale this application without facing", "timestamp": "00:28:07,138", "timestamp_s": 1687.0}, {"text": "the restrictions or the constraints by step functions.", "timestamp": "00:28:10,658", "timestamp_s": 1690.0}, {"text": "So this input and output is then used, and then you can change this and", "timestamp": "00:28:14,190", "timestamp_s": 1694.0}, {"text": "you can continue using this in different states within step", "timestamp": "00:28:17,584", "timestamp_s": 1697.0}, {"text": "function. One thing you\u0027ll realize is that in the first requirement,", "timestamp": "00:28:21,616", "timestamp_s": 1701.0}, {"text": "it was actually mentioned about creating multiple titles. Now,", "timestamp": "00:28:25,354", "timestamp_s": 1705.0}, {"text": "for example, we can continue using just the foundational models within AWS.", "timestamp": "00:28:29,028", "timestamp_s": 1709.0}, {"text": "But what if we want to access something that\u0027s outside, let\u0027s say for example,", "timestamp": "00:28:33,258", "timestamp_s": 1713.0}, {"text": "hugging phase, you want to access this foundational model from", "timestamp": "00:28:36,696", "timestamp_s": 1716.0}, {"text": "outside AWS. We want to then", "timestamp": "00:28:40,152", "timestamp_s": 1720.0}, {"text": "get the data, send that across, and then after get the response back and", "timestamp": "00:28:44,070", "timestamp_s": 1724.0}, {"text": "then continue in our execution. Now when you look at accessing", "timestamp": "00:28:47,544", "timestamp_s": 1727.0}, {"text": "a public API in general, it might look simple. Then the first question", "timestamp": "00:28:51,154", "timestamp_s": 1731.0}, {"text": "comes is what is the kind of authentication that you need, right? Is there basic", "timestamp": "00:28:54,492", "timestamp_s": 1734.0}, {"text": "authentication? Is there API keys? Is there oauth?", "timestamp": "00:28:58,194", "timestamp_s": 1738.0}, {"text": "Is there anything else token management for example. Then you", "timestamp": "00:29:01,746", "timestamp_s": 1741.0}, {"text": "also want to ensure that you\u0027re saving the secrets because you want to make sure", "timestamp": "00:29:05,472", "timestamp_s": 1745.0}, {"text": "maybe there\u0027s an access key for accessing the API. You want to keep that somewhere.", "timestamp": "00:29:08,768", "timestamp_s": 1748.0}, {"text": "Then you have input output handling. You also", "timestamp": "00:29:12,310", "timestamp_s": 1752.0}, {"text": "then have a graceful retry if something goes wrong. Then also", "timestamp": "00:29:15,812", "timestamp_s": 1755.0}, {"text": "rate control and so many other things. Now the way you would do that", "timestamp": "00:29:19,172", "timestamp_s": 1759.0}, {"text": "with AWS lambda for example, or maybe a container", "timestamp": "00:29:22,532", "timestamp_s": 1762.0}, {"text": "or virtual machine on EC two is that you would have your code running and", "timestamp": "00:29:26,058", "timestamp_s": 1766.0}, {"text": "then you would have these different services which would fetch the credentials, you would manage", "timestamp": "00:29:30,008", "timestamp_s": 1770.0}, {"text": "the token, you would then retrieve the request data and then afterwards", "timestamp": "00:29:33,272", "timestamp_s": 1773.0}, {"text": "invoke and get back the data, maybe store it somewhere else also if needed,", "timestamp": "00:29:36,606", "timestamp_s": 1776.0}, {"text": "this is what a resilient application would look like.", "timestamp": "00:29:40,492", "timestamp_s": 1780.0}, {"text": "One other way you can do this without writing code is by public", "timestamp": "00:29:43,850", "timestamp_s": 1783.0}, {"text": "HTTPs API integration on step functions. So step", "timestamp": "00:29:47,836", "timestamp_s": 1787.0}, {"text": "functions has the ability to call virtually any SaaS application from a workflow", "timestamp": "00:29:51,616", "timestamp_s": 1791.0}, {"text": "with the integration with HTTPS endpoints. So without", "timestamp": "00:29:55,462", "timestamp_s": 1795.0}, {"text": "using a lambda function, you can use huggingface for", "timestamp": "00:29:59,280", "timestamp_s": 1799.0}, {"text": "example, you can invoke an API and hugging face or maybe other APIs", "timestamp": "00:30:03,104", "timestamp_s": 1803.0}, {"text": "like stripe, Salesforce, GitHub, Adobe for example.", "timestamp": "00:30:06,858", "timestamp_s": 1806.0}, {"text": "And step functions now with this low code approach", "timestamp": "00:30:10,292", "timestamp_s": 1810.0}, {"text": "provides you a way to connect AWS services with services that are outside.", "timestamp": "00:30:13,914", "timestamp_s": 1813.0}, {"text": "And you can then take advantage of workflow studio because now you\u0027re dragging drop", "timestamp": "00:30:17,784", "timestamp_s": 1817.0}, {"text": "all of these things and then after putting that as part of the workflow together", "timestamp": "00:30:21,950", "timestamp_s": 1821.0}, {"text": "without changing or managing any code as part of", "timestamp": "00:30:25,096", "timestamp_s": 1825.0}, {"text": "this. So with such", "timestamp": "00:30:28,668", "timestamp_s": 1828.0}, {"text": "requests you can actually then put in your json object and then", "timestamp": "00:30:32,172", "timestamp_s": 1832.0}, {"text": "in the request body you can then mention okay, this is the kind of data", "timestamp": "00:30:35,676", "timestamp_s": 1835.0}, {"text": "that we are sending and this is what we are trying to retrieve back as", "timestamp": "00:30:38,652", "timestamp_s": 1838.0}, {"text": "part of that transformation. One of the ways that you can actually", "timestamp": "00:30:42,192", "timestamp_s": 1842.0}, {"text": "use this for integrating with HTTP APIs is that", "timestamp": "00:30:47,550", "timestamp_s": 1847.0}, {"text": "you can manage the errors also through step functions like we saw kind of you", "timestamp": "00:30:51,120", "timestamp_s": 1851.0}, {"text": "have that ability to do error handling. You can also", "timestamp": "00:30:54,516", "timestamp_s": 1854.0}, {"text": "manage authorization as part of that integration. You can", "timestamp": "00:30:58,308", "timestamp_s": 1858.0}, {"text": "also mention transformation of data because step functions already provides that for", "timestamp": "00:31:02,068", "timestamp_s": 1862.0}, {"text": "optimized integrations. So you can also leverage that if needed for things like", "timestamp": "00:31:05,268", "timestamp_s": 1865.0}, {"text": "URL encoding for request body and there\u0027s also a test", "timestamp": "00:31:09,128", "timestamp_s": 1869.0}, {"text": "state that allows you to execute that specific state", "timestamp": "00:31:12,472", "timestamp_s": 1872.0}, {"text": "without deploying that step function directly outside. So you can just", "timestamp": "00:31:17,510", "timestamp_s": 1877.0}, {"text": "execute that specific state as a test and then afterwards make sure", "timestamp": "00:31:20,732", "timestamp_s": 1880.0}, {"text": "that you\u0027re getting the kind of response that is needed. So with", "timestamp": "00:31:24,124", "timestamp_s": 1884.0}, {"text": "the task state that\u0027s available, you have that single unit of work. You can", "timestamp": "00:31:28,204", "timestamp_s": 1888.0}, {"text": "do an HTTP invoke and you can see that an existing", "timestamp": "00:31:31,664", "timestamp_s": 1891.0}, {"text": "resource field is available now and you also have the new", "timestamp": "00:31:34,998", "timestamp_s": 1894.0}, {"text": "option. And you can also then provide things like what methods", "timestamp": "00:31:38,832", "timestamp_s": 1898.0}, {"text": "are being invoked. For example, what\u0027s the authentication field that is there?", "timestamp": "00:31:43,114", "timestamp_s": 1903.0}, {"text": "The parameters block. This under the hood is actually using another", "timestamp": "00:31:46,724", "timestamp_s": 1906.0}, {"text": "service called Eventbridge. So Amazon Eventbridge is being used for API destinations because it", "timestamp": "00:31:50,756", "timestamp_s": 1910.0}, {"text": "has that ability to invoke or send requests", "timestamp": "00:31:54,808", "timestamp_s": 1914.0}, {"text": "to an API destination. So the same connection is actually being", "timestamp": "00:31:58,798", "timestamp_s": 1918.0}, {"text": "used as part of that. A lot of these parameters are actually optional.", "timestamp": "00:32:02,552", "timestamp_s": 1922.0}, {"text": "So when you\u0027re invoking a certain API, probably you\u0027re just getting a response back.", "timestamp": "00:32:05,982", "timestamp_s": 1925.0}, {"text": "You don\u0027t need to pass any query parameters. In this case, what we\u0027re doing", "timestamp": "00:32:09,980", "timestamp_s": 1929.0}, {"text": "is that we can add a request for headers and then anything", "timestamp": "00:32:13,308", "timestamp_s": 1933.0}, {"text": "else that\u0027s needed as part of the request. Now let\u0027s go", "timestamp": "00:32:17,052", "timestamp_s": 1937.0}, {"text": "back to a requirement directly. So in our case, since we want to generate", "timestamp": "00:32:20,288", "timestamp_s": 1940.0}, {"text": "multiple titles, we want to make sure that we\u0027re able to access", "timestamp": "00:32:23,878", "timestamp_s": 1943.0}, {"text": "one title from our model ourselves,", "timestamp": "00:32:28,272", "timestamp_s": 1948.0}, {"text": "and then after one from hugging phase. So we have a parallel state.", "timestamp": "00:32:32,038", "timestamp_s": 1952.0}, {"text": "Now through step functions, this allows us to use both the foundational", "timestamp": "00:32:35,572", "timestamp_s": 1955.0}, {"text": "models and you simply configure the endpoints on the right hand side.", "timestamp": "00:32:39,322", "timestamp_s": 1959.0}, {"text": "This way the parallel state will then execute and", "timestamp": "00:32:42,836", "timestamp_s": 1962.0}, {"text": "it will invoke each task in parallel. It then requires that", "timestamp": "00:32:46,808", "timestamp_s": 1966.0}, {"text": "each branch completes successfully for the parallel state to be considered successful.", "timestamp": "00:32:50,392", "timestamp_s": 1970.0}, {"text": "Now what happens if one of these branches doesn\u0027t complete successfully?", "timestamp": "00:32:55,038", "timestamp_s": 1975.0}, {"text": "Right? So what if something goes wrong? Maybe there\u0027s an issue in the call", "timestamp": "00:32:58,622", "timestamp_s": 1978.0}, {"text": "for one of our two FMs, and errors happen for", "timestamp": "00:33:01,932", "timestamp_s": 1981.0}, {"text": "various reasons. And if it\u0027s a transient issue such as a network interruption,", "timestamp": "00:33:04,988", "timestamp_s": 1984.0}, {"text": "you want to make sure that you\u0027re able to do a retry, and maybe you", "timestamp": "00:33:09,058", "timestamp_s": 1989.0}, {"text": "want to do that retry for a couple of times. And then you also configure", "timestamp": "00:33:12,768", "timestamp_s": 1992.0}, {"text": "something called as a backup rate to ensure that you don\u0027t overload the", "timestamp": "00:33:16,118", "timestamp_s": 1996.0}, {"text": "third party system. And for these momentary blips,", "timestamp": "00:33:19,728", "timestamp_s": 1999.0}, {"text": "it\u0027s just important. You just need to make sure that you have a retry mechanism", "timestamp": "00:33:23,286", "timestamp_s": 2003.0}, {"text": "of sorts. But what if that underlying error is actually something", "timestamp": "00:33:27,082", "timestamp_s": 2007.0}, {"text": "which requires a longer investigation, right, or a longer resolution? Time, because maybe", "timestamp": "00:33:30,980", "timestamp_s": 2010.0}, {"text": "it\u0027s not under your control, maybe it\u0027s independent of your team,", "timestamp": "00:33:34,548", "timestamp_s": 2014.0}, {"text": "and maybe it\u0027s somebody else who\u0027s managing it, or maybe even a third party.", "timestamp": "00:33:37,748", "timestamp_s": 2017.0}, {"text": "And what may happen is you may exhaust your retry strategy and then eventually", "timestamp": "00:33:40,702", "timestamp_s": 2020.0}, {"text": "that workflow step will actually fail. So you", "timestamp": "00:33:45,510", "timestamp_s": 2025.0}, {"text": "want to make sure that you\u0027re able to run this entire workflow, but then at", "timestamp": "00:33:49,112", "timestamp_s": 2029.0}, {"text": "the same time, if you can\u0027t, then you want to move it to an error", "timestamp": "00:33:52,524", "timestamp_s": 2032.0}, {"text": "state, or then move it somewhere else so that you can retry it later.", "timestamp": "00:33:54,978", "timestamp_s": 2034.0}, {"text": "So if you want to visualize this, this is basically what it looks", "timestamp": "00:33:59,450", "timestamp_s": 2039.0}, {"text": "like. So you have a success tip that then kicks off a parallel workflow.", "timestamp": "00:34:02,592", "timestamp_s": 2042.0}, {"text": "This parallel workflow has two branches, so you", "timestamp": "00:34:06,598", "timestamp_s": 2046.0}, {"text": "have bedrock on the left, hugging face on the left. And let\u0027s say we invoke", "timestamp": "00:34:09,904", "timestamp_s": 2049.0}, {"text": "the foundation model, we have some transformations we want to do using another", "timestamp": "00:34:13,018", "timestamp_s": 2053.0}, {"text": "AWS service, and that is an extra step. But let\u0027s", "timestamp": "00:34:16,612", "timestamp_s": 2056.0}, {"text": "say there is some failure in transformation because we have invoked something hugging", "timestamp": "00:34:20,714", "timestamp_s": 2060.0}, {"text": "face, and then when we get it back, something\u0027s not working.", "timestamp": "00:34:24,378", "timestamp_s": 2064.0}, {"text": "And this transcription job needs to continue. Right. There\u0027s some form that", "timestamp": "00:34:27,670", "timestamp_s": 2067.0}, {"text": "needs to happen, and we have stopped it over here before actually moving", "timestamp": "00:34:31,368", "timestamp_s": 2071.0}, {"text": "it to the next step, which is a human review.", "timestamp": "00:34:34,680", "timestamp_s": 2074.0}, {"text": "This is where you have the option of Redrive. Now, Redrive allows", "timestamp": "00:34:38,120", "timestamp_s": 2078.0}, {"text": "you to easily restart workflows, maybe because you have figured out,", "timestamp": "00:34:41,858", "timestamp_s": 2081.0}, {"text": "okay, there\u0027s a problem, and then maybe it\u0027s got resolved, and then you want to", "timestamp": "00:34:45,228", "timestamp_s": 2085.0}, {"text": "retry that workflow all over again so you can recover from failure", "timestamp": "00:34:47,804", "timestamp_s": 2087.0}, {"text": "faster, and you only pay for what you need. So you don\u0027t have", "timestamp": "00:34:51,154", "timestamp_s": 2091.0}, {"text": "to keep retrying it unless it\u0027s really necessary. So the way this", "timestamp": "00:34:54,384", "timestamp_s": 2094.0}, {"text": "works is you will have these two branches on", "timestamp": "00:34:58,032", "timestamp_s": 2098.0}, {"text": "the left and hugging face on the left. And let\u0027s say that when", "timestamp": "00:35:01,808", "timestamp_s": 2101.0}, {"text": "we invoke, we do the transformation, but it fails in the transformation step,", "timestamp": "00:35:04,948", "timestamp_s": 2104.0}, {"text": "so it gets fixed, and then after you come back again, and then you", "timestamp": "00:35:09,650", "timestamp_s": 2109.0}, {"text": "do a retry again once more, and this time the transcription actually kicks in", "timestamp": "00:35:13,188", "timestamp_s": 2113.0}, {"text": "because your transformations are already completed. And now it goes into the human review", "timestamp": "00:35:17,544", "timestamp_s": 2117.0}, {"text": "space if needed. So one of the other things", "timestamp": "00:35:21,192", "timestamp_s": 2121.0}, {"text": "you want to do also as a part of workflows is you want to have", "timestamp": "00:35:25,112", "timestamp_s": 2125.0}, {"text": "observability. Execution event history is very important as", "timestamp": "00:35:27,608", "timestamp_s": 2127.0}, {"text": "part of this because you have different states that are coming in, you have events", "timestamp": "00:35:30,812", "timestamp_s": 2130.0}, {"text": "being fired. You want to make sure that you\u0027re able to filter and", "timestamp": "00:35:33,698", "timestamp_s": 2133.0}, {"text": "drill down to what\u0027s actually happening within your workflow. This is kind", "timestamp": "00:35:37,276", "timestamp_s": 2137.0}, {"text": "of like where you can see execution Redriven and it also shows", "timestamp": "00:35:41,184", "timestamp_s": 2141.0}, {"text": "a count, a redrive count of how many times you are actually retrying that", "timestamp": "00:35:44,560", "timestamp_s": 2144.0}, {"text": "execution through Redrive. So cool. I think it\u0027s a great way", "timestamp": "00:35:48,448", "timestamp_s": 2148.0}, {"text": "to understand how you can actually manage events,", "timestamp": "00:35:52,068", "timestamp_s": 2152.0}, {"text": "especially errors in this case, and then ensure that your workflows", "timestamp": "00:35:55,162", "timestamp_s": 2155.0}, {"text": "are able to then continue properly.", "timestamp": "00:35:58,458", "timestamp_s": 2158.0}, {"text": "Now, with multiple titles out of the way,", "timestamp": "00:36:02,050", "timestamp_s": 2162.0}, {"text": "let\u0027s talk about asking a human to provide feedback.", "timestamp": "00:36:05,544", "timestamp_s": 2165.0}, {"text": "Now, having a human approval is an automated business process and it is super", "timestamp": "00:36:09,342", "timestamp_s": 2169.0}, {"text": "common. You have this as part of any approvals that are happening,", "timestamp": "00:36:13,192", "timestamp_s": 2173.0}, {"text": "probably in the banking space, in the financial space. You have", "timestamp": "00:36:17,416", "timestamp_s": 2177.0}, {"text": "probably also a human in the loop as part of maybe a foundational model that", "timestamp": "00:36:21,420", "timestamp_s": 2181.0}, {"text": "you have created or you have custom built, or maybe you\u0027re fine tuned", "timestamp": "00:36:25,244", "timestamp_s": 2185.0}, {"text": "and then you want to make sure that you\u0027re able to check the response that", "timestamp": "00:36:28,866", "timestamp_s": 2188.0}, {"text": "are coming in. Maybe you have an EB flow that\u0027s happening, right? For a few", "timestamp": "00:36:31,148", "timestamp_s": 2191.0}, {"text": "requests that need to come in, you want to have a human response that needs", "timestamp": "00:36:34,316", "timestamp_s": 2194.0}, {"text": "to happen, human review that needs to happen. So the requirement is super", "timestamp": "00:36:37,452", "timestamp_s": 2197.0}, {"text": "simple, but possibilities are endless when you need to do this.", "timestamp": "00:36:41,316", "timestamp_s": 2201.0}, {"text": "So step functions integrates with services in multiple ways,", "timestamp": "00:36:44,804", "timestamp_s": 2204.0}, {"text": "and one of the ways you can do this is through long running jobs of", "timestamp": "00:36:48,770", "timestamp_s": 2208.0}, {"text": "a service, and you want to wait for the job to complete and", "timestamp": "00:36:51,828", "timestamp_s": 2211.0}, {"text": "we\u0027ll use this integration pattern to achieve this requirement.", "timestamp": "00:36:56,100", "timestamp_s": 2216.0}, {"text": "So what you want to do is you want to make a call to a", "timestamp": "00:36:59,510", "timestamp_s": 2219.0}, {"text": "service integration. This passes a unique token and this token then gets", "timestamp": "00:37:01,624", "timestamp_s": 2221.0}, {"text": "embedded in an email. It goes to maybe a server or", "timestamp": "00:37:05,192", "timestamp_s": 2225.0}, {"text": "on premise and legacy server, or to a long running task", "timestamp": "00:37:08,668", "timestamp_s": 2228.0}, {"text": "in a container, for example. And then once that response", "timestamp": "00:37:12,434", "timestamp_s": 2232.0}, {"text": "is maybe it\u0027s reviewed, and then after they click on going ahead or", "timestamp": "00:37:16,498", "timestamp_s": 2236.0}, {"text": "not, it returns using the step functions API, send task success.", "timestamp": "00:37:20,128", "timestamp_s": 2240.0}, {"text": "And then the workflow continues from there. So as", "timestamp": "00:37:23,904", "timestamp_s": 2243.0}, {"text": "part of the send response and wait workflow, there will be", "timestamp": "00:37:28,128", "timestamp_s": 2248.0}, {"text": "a token that\u0027s sent out like I mentioned earlier, and this email notification is", "timestamp": "00:37:31,344", "timestamp_s": 2251.0}, {"text": "already there. Maybe as part of this use case at least,", "timestamp": "00:37:34,768", "timestamp_s": 2254.0}, {"text": "what will happen is there will be options that are being set. So choose the", "timestamp": "00:37:37,812", "timestamp_s": 2257.0}, {"text": "title that\u0027s being generated by Amazon Bedrock, or choose the title that\u0027s", "timestamp": "00:37:40,164", "timestamp_s": 2260.0}, {"text": "being generated by hugging face and then regenerate that.", "timestamp": "00:37:43,578", "timestamp_s": 2263.0}, {"text": "Now the last part of this requirement is creating an avatar for the video,", "timestamp": "00:37:47,192", "timestamp_s": 2267.0}, {"text": "which basically is an image in this case. And machine learning models,", "timestamp": "00:37:50,520", "timestamp_s": 2270.0}, {"text": "especially in the foundational model space, you have built in algorithms.", "timestamp": "00:37:55,590", "timestamp_s": 2275.0}, {"text": "We also have pre built ML solutions that are already available. You can probably invoke", "timestamp": "00:37:59,886", "timestamp_s": 2279.0}, {"text": "a third party API again for this case, and there are multiple", "timestamp": "00:38:03,474", "timestamp_s": 2283.0}, {"text": "ways you can do this as a part of bedrock. You also have access to", "timestamp": "00:38:06,898", "timestamp_s": 2286.0}, {"text": "stability diffusion models, so you", "timestamp": "00:38:09,868", "timestamp_s": 2289.0}, {"text": "can use that also as part of the step. What this does is in the", "timestamp": "00:38:13,024", "timestamp_s": 2293.0}, {"text": "end, once you have tried the feedback, you can then generate that video,", "timestamp": "00:38:16,208", "timestamp_s": 2296.0}, {"text": "sorry, the avatar for the video, and then you can store", "timestamp": "00:38:19,456", "timestamp_s": 2299.0}, {"text": "that in an S three bucket and then share that link later.", "timestamp": "00:38:22,816", "timestamp_s": 2302.0}, {"text": "Now, one of the things you\u0027ll realize when you want to create such a complex", "timestamp": "00:38:26,930", "timestamp_s": 2306.0}, {"text": "workflow is that especially with foundation models, you want to have this whole", "timestamp": "00:38:29,882", "timestamp_s": 2309.0}, {"text": "idea of creating chains of prompts, which we kind of saw in the demo.", "timestamp": "00:38:33,556", "timestamp_s": 2313.0}, {"text": "Now this is a technique of writing multiple prompts and responses. A sequence of steps.", "timestamp": "00:38:37,016", "timestamp_s": 2317.0}, {"text": "Step functions is a workflow is a great way to actually", "timestamp": "00:38:41,182", "timestamp_s": 2321.0}, {"text": "leverage chaining, so you can actually use this.", "timestamp": "00:38:44,520", "timestamp_s": 2324.0}, {"text": "And step function simplifies the way you invoke your foundational models,", "timestamp": "00:38:47,784", "timestamp_s": 2327.0}, {"text": "and you have state independency management already in place. You can then", "timestamp": "00:38:51,090", "timestamp_s": 2331.0}, {"text": "create chaining easily. You can also pass the state, as we saw earlier,", "timestamp": "00:38:54,252", "timestamp_s": 2334.0}, {"text": "pass that to the next state that\u0027s needed, the response of a", "timestamp": "00:38:57,362", "timestamp_s": 2337.0}, {"text": "state, and then pass that to the next one, maybe specific parts of the prompt", "timestamp": "00:39:00,908", "timestamp_s": 2340.0}, {"text": "also if you need to. And all of this is again serverless. So think of", "timestamp": "00:39:04,262", "timestamp_s": 2344.0}, {"text": "use cases like writing blogs and articles, response validation, conversational LLMs", "timestamp": "00:39:07,648", "timestamp_s": 2347.0}, {"text": "that we see a lot these days. Now, if you want to now take all", "timestamp": "00:39:11,718", "timestamp_s": 2351.0}, {"text": "of what we have seen and then put that in an architecture, this is what", "timestamp": "00:39:15,588", "timestamp_s": 2355.0}, {"text": "it looks like. So for example, you have an API gateway", "timestamp": "00:39:18,164", "timestamp_s": 2358.0}, {"text": "that a user would invoke through an application,", "timestamp": "00:39:21,498", "timestamp_s": 2361.0}, {"text": "and then that would then put in an event into", "timestamp": "00:39:25,560", "timestamp_s": 2365.0}, {"text": "a queue, and then this", "timestamp": "00:39:29,432", "timestamp_s": 2369.0}, {"text": "event in the queue then gets picked up by a lambda function, which then would", "timestamp": "00:39:32,712", "timestamp_s": 2372.0}, {"text": "trigger this step functions workflow. And in this case,", "timestamp": "00:39:35,448", "timestamp_s": 2375.0}, {"text": "what happens is that you have a lot of these steps already in place", "timestamp": "00:39:38,792", "timestamp_s": 2378.0}, {"text": "as part of the workflow. It sends the title and description to the user back,", "timestamp": "00:39:43,212", "timestamp_s": 2383.0}, {"text": "and then afterwards you can then send the chosen title and description", "timestamp": "00:39:46,972", "timestamp_s": 2386.0}, {"text": "as part of the human workflow, if needed, for the response,", "timestamp": "00:39:50,598", "timestamp_s": 2390.0}, {"text": "for the review part. Then as part of the final", "timestamp": "00:39:54,518", "timestamp_s": 2394.0}, {"text": "part where you have the generating the avatar,", "timestamp": "00:39:58,000", "timestamp_s": 2398.0}, {"text": "you actually get an S three presigned URL, because that avatar", "timestamp": "00:40:01,654", "timestamp_s": 2401.0}, {"text": "image gets created, generated, and then afterwards put in an S three bucket.", "timestamp": "00:40:05,338", "timestamp_s": 2405.0}, {"text": "So here\u0027s a demonstration of this final architecture.", "timestamp": "00:40:09,354", "timestamp_s": 2409.0}, {"text": "So there\u0027s a short video of an interview between Jeff Bezos and", "timestamp": "00:40:12,474", "timestamp_s": 2412.0}, {"text": "Werner Vogels. What\u0027s going to happen is that", "timestamp": "00:40:16,376", "timestamp_s": 2416.0}, {"text": "we want to generate a title and a description and an avatar for this video.", "timestamp": "00:40:19,768", "timestamp_s": 2419.0}, {"text": "So there\u0027s a simple UI that you saw earlier.", "timestamp": "00:40:23,848", "timestamp_s": 2423.0}, {"text": "This uses a websocket communication to talk to AWS", "timestamp": "00:40:27,538", "timestamp_s": 2427.0}, {"text": "iot core service. And once you select the button, it then sends", "timestamp": "00:40:30,898", "timestamp_s": 2430.0}, {"text": "that video\u0027s details. And then the workflow then gets executed", "timestamp": "00:40:34,802", "timestamp_s": 2434.0}, {"text": "from the lambda function. And then you see that that step starts kicking in.", "timestamp": "00:40:39,074", "timestamp_s": 2439.0}, {"text": "This gives a nice view of the execution. You have the color coding of the", "timestamp": "00:40:43,312", "timestamp_s": 2443.0}, {"text": "state. And with transcribe being used initially, you get the text", "timestamp": "00:40:46,528", "timestamp_s": 2446.0}, {"text": "back for the speech that is there in the video. And this transcribe", "timestamp": "00:40:50,432", "timestamp_s": 2450.0}, {"text": "job is asynchronous. So there is a wait loop that is there to make sure", "timestamp": "00:40:54,294", "timestamp_s": 2454.0}, {"text": "that we can wait for it to complete.", "timestamp": "00:40:57,668", "timestamp_s": 2457.0}, {"text": "Right, so that\u0027s the wait loop that\u0027s there in the beginning. And once that", "timestamp": "00:41:01,300", "timestamp_s": 2461.0}, {"text": "wait loop is done, wait loop is done. And using", "timestamp": "00:41:04,692", "timestamp_s": 2464.0}, {"text": "the get transcription job API, we get the final response", "timestamp": "00:41:08,312", "timestamp_s": 2468.0}, {"text": "from that transcription job. And then we read that transcript.", "timestamp": "00:41:11,950", "timestamp_s": 2471.0}, {"text": "And that transcript is available in an S three bucket", "timestamp": "00:41:15,678", "timestamp_s": 2475.0}, {"text": "already because transcription job will put it in over there. And then", "timestamp": "00:41:19,058", "timestamp_s": 2479.0}, {"text": "once that is read, it is then passed down to this parallel execution.", "timestamp": "00:41:22,716", "timestamp_s": 2482.0}, {"text": "As part of the parallel execution, we have this two calls that are being done.", "timestamp": "00:41:26,978", "timestamp_s": 2486.0}, {"text": "One to bedrock, one of the foundational models", "timestamp": "00:41:31,050", "timestamp_s": 2491.0}, {"text": "at bedrock, and then one to hugging face. In this case, we\u0027re just keeping it", "timestamp": "00:41:34,838", "timestamp_s": 2494.0}, {"text": "simple. So we want to just make sure that we\u0027re able to execute this back.", "timestamp": "00:41:38,208", "timestamp_s": 2498.0}, {"text": "And we want to then get user feedback now", "timestamp": "00:41:42,096", "timestamp_s": 2502.0}, {"text": "quickly, just to show you what the outputs look like.", "timestamp": "00:41:45,396", "timestamp_s": 2505.0}, {"text": "These are the inputs that are coming in. This is the transcript that\u0027s", "timestamp": "00:41:48,388", "timestamp_s": 2508.0}, {"text": "there, the prompts. You kind of notice that the models that are being invoked is", "timestamp": "00:41:51,802", "timestamp_s": 2511.0}, {"text": "also there as part of that. Here are the parameters,", "timestamp": "00:41:55,608", "timestamp_s": 2515.0}, {"text": "here\u0027s the conversation that\u0027s happening with the video and the", "timestamp": "00:41:58,950", "timestamp_s": 2518.0}, {"text": "s three bucket URL for the video and other things. And there\u0027s", "timestamp": "00:42:02,808", "timestamp_s": 2522.0}, {"text": "a task token that\u0027s already there. This is part of the review flow that", "timestamp": "00:42:06,638", "timestamp_s": 2526.0}, {"text": "is being invoked. So we have this task token that\u0027s being sent", "timestamp": "00:42:10,588", "timestamp_s": 2530.0}, {"text": "to the page. And this page is basically where someone", "timestamp": "00:42:14,444", "timestamp_s": 2534.0}, {"text": "can actually go in and then say, okay, do they want to select this title,", "timestamp": "00:42:18,156", "timestamp_s": 2538.0}, {"text": "the first one or the second one? So we select one of the titles,", "timestamp": "00:42:21,538", "timestamp_s": 2541.0}, {"text": "and then afterwards it goes down, and then it creates an avatar as part of", "timestamp": "00:42:25,542", "timestamp_s": 2545.0}, {"text": "that title that\u0027s created. It sends that as part of a prompt to", "timestamp": "00:42:28,768", "timestamp_s": 2548.0}, {"text": "one of the foundation models. So stability in this case,", "timestamp": "00:42:34,690", "timestamp_s": 2554.0}, {"text": "and once you have that, that gets displayed over here, and that\u0027s an", "timestamp": "00:42:38,036", "timestamp_s": 2558.0}, {"text": "avatar that is used now, for example, the team that uses it can then", "timestamp": "00:42:41,444", "timestamp_s": 2561.0}, {"text": "copy this image and then put it in because it\u0027s already there in s three", "timestamp": "00:42:44,868", "timestamp_s": 2564.0}, {"text": "bucket. Or probably it gets picked by another flow that then is used as part", "timestamp": "00:42:48,248", "timestamp_s": 2568.0}, {"text": "of their content publishing pipeline. Now, to know more about how", "timestamp": "00:42:51,912", "timestamp_s": 2571.0}, {"text": "you can build applications like this, there is a sample that\u0027s already available that has", "timestamp": "00:42:55,688", "timestamp_s": 2575.0}, {"text": "different use cases. Also covers things like the error retries.", "timestamp": "00:42:59,468", "timestamp_s": 2579.0}, {"text": "It covers prompt chaining and all", "timestamp": "00:43:03,346", "timestamp_s": 2583.0}, {"text": "the other parts of creating a complex workflow with step functions for generative", "timestamp": "00:43:06,652", "timestamp_s": 2586.0}, {"text": "AI. So have a look at this resource. A great way to do this,", "timestamp": "00:43:10,242", "timestamp_s": 2590.0}, {"text": "and also with our blog posts that are linked as part of this resource.", "timestamp": "00:43:13,228", "timestamp_s": 2593.0}, {"text": "So with that, I would like to thank you for attending the session and have", "timestamp": "00:43:17,210", "timestamp_s": 2597.0}, {"text": "a good day and rest of conf 42. Thank you so much.", "timestamp": "00:43:20,852", "timestamp_s": 2600.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'UhmOV51Hp10',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Build and orchestrate serverless generative AI applications
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>In the journey to build versatile serverless generative AI applications, discover the roadmap from concept to realization, all within an efficient, model-driven design setup. From principles to prototyping with the tools, learn how to bring your ideas to your users in a scalable, efficient manner.</p>
<!-- End Text -->
          </div>

          
          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
              <li>
                Today I'm going to be sharing how you can get started on AWS for building and orchestrating serverless workflows for generative AI. Generative AI is a type of AI that can create new content and ideas. There have been some amazing breakthroughs through using foundational models in different industries.

              </li>
              
              <li>
                AWS says Gen AI has three macro layers. The bottom layer is the infrastructure. The middle layer is access to large language models and other FMs. At the top layer are applications that take advantage of Gen AI quickly. The ability to adapt is the most valuable capability that you can have.

              </li>
              
              <li>
                AWS step functions is service that allows you to create workflows. These are workflows that allow you to move output of one step to the input of the next step. The way step functions integrates with these services is through two ways. First is SDK integrations and the second is optimized integrations.

              </li>
              
              <li>
                Step functions can be used to orchestrate interactions with foundational models. This makes it easy for developers to add speech to text capability to their applications. There are two new optimized integrations that we have provided. A demo shows how an application uses all of this together.

              </li>
              
              <li>
                Another powerful way of showing what bedrock is capable of through step functions is chaining. What this does is this emulates a certain conversation that you can have with an LLM. As the execution progresses, you'll see all these states changing the colors based on how the foundational model is responding.

              </li>
              
              <li>
                Step functions has the ability to call virtually any SaaS application from a workflow with the integration with HTTPS endpoints. It provides a way to connect AWS services with services that are outside. One other way you can do this without writing code is by public HTTPs API integration on step functions.

              </li>
              
              <li>
                Step functions is a great way to actually leverage chaining. It simplifies the way you invoke your foundational models. Once you have tried the feedback, you can then generate that video, sorry, the avatar for the video. All of this is serverless.
              </li>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./assemblyai/UhmOV51Hp10.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:24,570'); seek(24.0)">
              Hi there, thanks for joining the session. Today I'm going to be sharing how you
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:28,332'); seek(28.0)">
              can get started on AWS for building and
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:31,644'); seek(31.0)">
              orchestrating serverless workflows for generative AI generative
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:35,762'); seek(35.0)">
              AI has taken the world by storm. We are seeing a massive shift in the
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:38,924'); seek(38.0)">
              way applications are being built. A lot of this is through consumer facing
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:42,738'); seek(42.0)">
              services that have come out like chat, JPT by Openei,
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:45,666'); seek(45.0)">
              cloud by anthropic, and we are able to
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:48,748'); seek(48.0)">
              see and experience how powerful latest machine learning models have
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:52,612'); seek(52.0)">
              become. Generative AI is a type of AI that can create new
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:56,052'); seek(56.0)">
              content and ideas, including conversations, stories, images,
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:00:59,290'); seek(59.0)">
              videos and music. Like all AI, generative AI is powered
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:01:03,034'); seek(63.0)">
              by machine learning models. But generative AI is powered by
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:01:06,292'); seek(66.0)">
              very large models that are pretrained on vast amounts of data and
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:01:10,056'); seek(70.0)">
              commonly referred to as foundational models. Now, throughout the session and
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:01:13,288'); seek(73.0)">
              also in conversation that you'll have out there, you'll see foundational models
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:01:17,026'); seek(77.0)">
              being interchangeably used with LLMs. Large language models
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:20,658'); seek(80.0)">
              just to understand LLMs are a subset of foundational models where LLMs focus
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:24,796'); seek(84.0)">
              on text. Specifically. There have been some amazing breakthroughs
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:28,978'); seek(88.0)">
              through using foundational models in different industries. A couple
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:32,128'); seek(92.0)">
              of these are where we see impacts in life sciences,
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:35,094'); seek(95.0)">
              with drug discovery being powered by Genei. This has
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:38,368'); seek(98.0)">
              enabled researchers to understand things like protein synthesis.
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:42,110'); seek(102.0)">
              In financial services, we see Genei being used to help create
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:45,556'); seek(105.0)">
              highly tailored investment strategies that are aligned to individuals,
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:48,954'); seek(108.0)">
              their risk, appetite, and also financial goals that they want to achieve.
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:52,770'); seek(112.0)">
              In healthcare, we have seen how physicians and
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:01:55,928'); seek(115.0)">
              clinicians can use this to enhance medical images and
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:01:59,624'); seek(119.0)">
              also to aid in better diagnosis. Think like a medical assistant.
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:02:03,310'); seek(123.0)">
              And in the retail space we see teams generating high
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:02:06,792'); seek(126.0)">
              quality product descriptions and listings based on product data that they already
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:02:10,396'); seek(130.0)">
              have. Now you'll notice a lot of the use cases
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:02:13,778'); seek(133.0)">
              for generative AI are for enhancing existing processes or experience
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:02:17,564'); seek(137.0)">
              that are already there. A question that usually comes is we
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:21,552'); seek(141.0)">
              already have services and applications that are out there. How do we take generative AI
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:25,014'); seek(145.0)">
              and then add that to enhance the experience versus rewriting everything from
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:28,464'); seek(148.0)">
              scratch? Now, to understand this, what you need to also
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:32,020'); seek(152.0)">
              understand is how you view generative AI.
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:35,146'); seek(155.0)">
              So from AWS's perspective, Gen AI
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:38,602'); seek(158.0)">
              has three macro layers and these three are equally important to us and we are
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:42,228'); seek(162.0)">
              investing in all of them. The bottom layer is the infrastructure. This is used to
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:46,008'); seek(166.0)">
              train foundational models and then run these models in production.
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:02:50,510'); seek(170.0)">
              Then you have the middle layer that provides access to these large language models
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:02:54,574'); seek(174.0)">
              and other FMs that you need and the tools that you need to build and
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:02:57,788'); seek(177.0)">
              scale generative AI applications which then use the LLMs under
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:03:01,148'); seek(181.0)">
              the hood. Then at the top layer you have applications
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:03:04,594'); seek(184.0)">
              that are built leveraging foundational models so they take advantage
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:08,082'); seek(188.0)">
              of Gen AI quickly and you don't need to have any
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:11,968'); seek(191.0)">
              specialized knowledge. Now, when you take this and map this against the
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:16,128'); seek(196.0)">
              services that we provide from AWS, you kind of
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:19,568'); seek(199.0)">
              see that the three stacks are kind of neatly segregated. At the
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:23,348'); seek(203.0)">
              lowest layer of the stack is the infrastructure. This is basically where you get
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:27,730'); seek(207.0)">
              to build cost effective foundational models. You train
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:31,172'); seek(211.0)">
              them and then you can deploy them at scale. This gives you access to our
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:34,628'); seek(214.0)">
              hardware, accelerators and GPUs. And also you get access to services
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:38,376'); seek(218.0)">
              like Amazon Sagemaker that enables ML practitioners and your teams to
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:42,216'); seek(222.0)">
              build, train and deploy LLMs and foundational models.
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:45,678'); seek(225.0)">
              Then at the middle layer we have Amazon bedrock. This provides access to
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:03:49,628'); seek(229.0)">
              all LLMs and other foundational models that you need to build
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:03:53,292'); seek(233.0)">
              and scale generative AI applications without you managing the whole
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:03:57,244'); seek(237.0)">
              infrastructure behind it, right? Without you actually managing the scale side of
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:04:00,668'); seek(240.0)">
              things. Think serverless, but for machine learning models for FMS,
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:04:04,630'); seek(244.0)">
              basically, then at the top layer are applications that help you to
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:07,904'); seek(247.0)">
              take advantage of Genei quickly as part of your day to day operations.
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:11,206'); seek(251.0)">
              This includes services like Amazon Q, our new generative AI
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:14,618'); seek(254.0)">
              powered assistant that is tailored to your business. So think like Personas,
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:18,138'); seek(258.0)">
              which are business users, data users, or even developers.
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:22,842'); seek(262.0)">
              You could use Q as part of AWS, a plugin that's
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:26,654'); seek(266.0)">
              already available for certain services, and then afterwards use that to get
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:30,408'); seek(270.0)">
              an enhanced operation capability.
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:33,918'); seek(273.0)">
              Each of these layers builds on the other, and you may need some or all
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:37,128'); seek(277.0)">
              of these capabilities at different points in your generative AI journey. A lot
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:40,600'); seek(280.0)">
              of what you see is in an organization, you'll have a mix of Personas that
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:04:43,324'); seek(283.0)">
              would use all three layers. You use specific services from those three layers
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:04:46,978'); seek(286.0)">
              to enhance productivity.
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:04:50,170'); seek(290.0)">
              Now, Amazon Bedrock is the easiest way to build and scale generative AI applications
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:04:54,358'); seek(294.0)">
              with foundational models. This is a fully managed service so you can get started
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:04:58,016'); seek(298.0)">
              quickly and you can find the right model based on the use case that you
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:05:01,408'); seek(301.0)">
              have. You can then also customize your model with your own data,
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:05:04,660'); seek(304.0)">
              and you can do this privately. Nothing feeds your data back to the base models,
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:08,234'); seek(308.0)">
              which then other customers would also have access to.
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:11,652'); seek(311.0)">
              This doesn't happen and you have the tools that you need to combine the
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:15,048'); seek(315.0)">
              power of foundational models with your organization data and execute
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:18,542'); seek(318.0)">
              complex tasks. All of this is with security, privacy and
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:22,392'); seek(322.0)">
              responsible EI safety, which you need to then put generative AI
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:26,558'); seek(326.0)">
              into production for your users. Now, there's a lot of
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:30,108'); seek(330.0)">
              models that are out there and from Amazon bedrock. These are
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:33,692'); seek(333.0)">
              a couple of models that we provide and one of the reasons that we went
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:36,812'); seek(336.0)">
              with this model is because everything's moving fast.
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:05:40,012'); seek(340.0)">
              Experimenting and learning is the key right now and also generative.
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:05:43,542'); seek(343.0)">
              AI as a technology is also evolving quickly with new developments.
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:05:47,222'); seek(347.0)">
              Now when things are moving so fast, the ability to adapt is the most valuable
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:05:50,886'); seek(350.0)">
              capability that you can have. There is not going to be one
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:05:54,144'); seek(354.0)">
              model to rule them all, and certainly not one company providing the models that
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:05:57,652'); seek(357.0)">
              everyone uses. So you don't want a cloud provider who is beholden
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:01,322'); seek(361.0)">
              primarily to one model provider. You need to be able to try out
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:04,788'); seek(364.0)">
              different models. You should be able to switch between them rapidly
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:08,542'); seek(368.0)">
              based on the use cases, or even combine multiple models within a
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:12,216'); seek(372.0)">
              certain use case. You need a real choice of model providers, AWS. You decide
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:06:15,982'); seek(375.0)">
              who has the best technology. This is kind of like where we have
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:06:19,692'); seek(379.0)">
              seen based on our building services that we want to provide the choice
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:06:23,202'); seek(383.0)">
              to customers, which is you. This is why we provide
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:06:27,516'); seek(387.0)">
              through Pedrog, access to wide range of foundational models from
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:31,312'); seek(391.0)">
              leaders like AI 21 labs, anthropic coher stability AI,
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:06:36,270'); seek(396.0)">
              also access to our own foundational models like Amazon
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:06:40,038'); seek(400.0)">
              Titan. And the idea is that we provide an API as
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:06:43,812'); seek(403.0)">
              part of this. So there is a layer, an API layer that provides
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:06:47,738'); seek(407.0)">
              you access to the large language models under the hood or the foundational models.
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:06:51,498'); seek(411.0)">
              And all you do is as a user or probably as a
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:06:54,868'); seek(414.0)">
              developer, you create the prompts in a certain format based on what
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:06:58,168'); seek(418.0)">
              the foundational model expects. You take that prompt or text embeddings
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:02,398'); seek(422.0)">
              if you want to tune that model a bit more, and then afterwards send
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:07:05,928'); seek(425.0)">
              that to the API layer and you can then get your responses
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:07:09,438'); seek(429.0)">
              back and then use that as part of your applications. Now there are a couple
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:07:12,268'); seek(432.0)">
              of ways you can use bedrock. One of the ways customers
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:07:15,884'); seek(435.0)">
              usually start is by writing code. And the way you
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:07:19,228'); seek(439.0)">
              integrate with Amazon bedrock is that you can use the SDK, right? So you
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:07:23,184'); seek(443.0)">
              use the APIs and then afterwards access the foundational models.
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:07:26,406'); seek(446.0)">
              So you load the libraries that has a bedrock API and then afterwards you
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:07:30,880'); seek(450.0)">
              can also access data in other places like an
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:07:34,544'); seek(454.0)">
              S three bucket. If you have data that's bigger than what's normal,
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:07:37,594'); seek(457.0)">
              you can then access it in s three bucket for input and even for output.
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:07:41,338'); seek(461.0)">
              You can then prepare the input and then handle the JSON to bring convert and
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:07:44,888'); seek(464.0)">
              then afterwards decode the responses. If the return data
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:07:48,776'); seek(468.0)">
              is image, it's an image of sorts. You can then store
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:07:52,616'); seek(472.0)">
              that in an S three bucket. Then if you have retries,
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:07:55,886'); seek(475.0)">
              then you'll have to do retry logic inside, and then afterwards,
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:07:59,598'); seek(479.0)">
              if you have any errors, you may have to have a certain condition, so on
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:02,444'); seek(482.0)">
              and so forth. You kind of get an ideas of what happens with code in
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:08:05,084'); seek(485.0)">
              general. Now, this is what the code would look like, but how
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:08:09,500'); seek(489.0)">
              do we actually look at providing simpler integration without
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:08:12,832'); seek(492.0)">
              writing a lot of code? And for this, you need to also understand
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:08:16,576'); seek(496.0)">
              the whole idea of sequencing. Right. How do you coordinate between multiple services?
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:08:20,704'); seek(500.0)">
              Because a lot of organizations don't just have one specific app,
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:08:23,760'); seek(503.0)">
              they would have probably a plethora of apps that power their business.
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:08:27,812'); seek(507.0)">
              And you want to understand how these services are going to talk to each other
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:08:31,012'); seek(511.0)">
              in a reliable and understandable way, because business processes
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:08:34,682'); seek(514.0)">
              usually exhibit different patterns based on the inputs that are coming
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:08:38,312'); seek(518.0)">
              in and what needs to be accomplished. Sometimes things need to be done
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:08:41,992'); seek(521.0)">
              sequentially. So in this case, let's say you have a number of lambda functions.
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:08:45,358'); seek(525.0)">
              So we'll use lambda as a proxy to understand this for different services.
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:08:49,000'); seek(529.0)">
              So you have a lambda one, and then you have a lambda two. Now,
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:08:51,788'); seek(531.0)">
              this is easy enough because you can have these in sequence.
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:08:54,658'); seek(534.0)">
              So lambda one invokes lambda two. But what if you have more than two
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:08:58,348'); seek(538.0)">
              lambda functions? What if instead of calling lambda two, you need
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:09:01,728'); seek(541.0)">
              lambda one to also call lambda seven before calling another service, or before
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:09:05,472'); seek(545.0)">
              calling a foundational model in this case. Now,
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:09:08,752'); seek(548.0)">
              if one of these services or functions fail, there's no easy
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:09:12,436'); seek(552.0)">
              recovery mechanism, and reprocessing previously executed
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:09:15,498'); seek(555.0)">
              steps becomes difficult. So we add some persistence inside.
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:09:19,460'); seek(559.0)">
              That's the next step. You have persistence because you have all these executions
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:09:23,098'); seek(563.0)">
              happening behind the scenes. And this way we can
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:09:26,776'); seek(566.0)">
              deal with state, right? Try to manage some kind of a coordination, try to understand
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:09:30,870'); seek(570.0)">
              which service is being executed at this point of time for this
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:09:34,264'); seek(574.0)">
              whole execution flow that's happening now. Because of this,
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:09:37,752'); seek(577.0)">
              you have to also collaborate all these functions. You need to manage this persistence mechanism.
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:09:41,442'); seek(581.0)">
              And there's no elegant way of coordinating flow or error handling between these
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:09:44,988'); seek(584.0)">
              services. And not every process is sequential.
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:09:49,234'); seek(589.0)">
              So, for example, you could also have certain processes that need to run in parallel,
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:09:53,158'); seek(593.0)">
              or perhaps it can follow different paths based on the input or what happens in
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:09:57,024'); seek(597.0)">
              an earlier step. That's a lot harder to do, and it gets even harder
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:10:00,678'); seek(600.0)">
              the more successful you are, because more people want to use the flow processes you've
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:10:04,538'); seek(604.0)">
              built out. You need to be able to handle errors as they occur.
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:10:08,610'); seek(608.0)">
              And this could be things like retrying calls, or it
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:10:12,148'); seek(612.0)">
              could be something as simple as following a different path in your workflow.
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:10:15,834'); seek(615.0)">
              All in said, this is all things that you can still do in code.
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:10:19,528'); seek(619.0)">
              This is something that has been done in code for quite some time. But what
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:10:22,792'); seek(622.0)">
              if your flow also needs a human as part of the process? For example,
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:10:26,712'); seek(626.0)">
              you need a human to review the output of a previous task to see if
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:10:29,628'); seek(629.0)">
              it's accurate, like a spot check for example. Or you've
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:10:33,474'); seek(633.0)">
              built out an application processing flow where the customer has requested a
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:10:36,828'); seek(636.0)">
              credit limit that exceeds the specified auto approved threshold.
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:10:40,466'); seek(640.0)">
              And then you need somebody else to come in and then afterwards review
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:10:44,400'); seek(644.0)">
              that request, and then after say okay, yes or no, depending on other
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:10:47,472'); seek(647.0)">
              data that they have. So that application needs to be routed to a
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:10:51,184'); seek(651.0)">
              human for this to work, and this continues.
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:10:54,262'); seek(654.0)">
              So as long as you have business processes that need to emulate what happens in
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:10:58,244'); seek(658.0)">
              the real world, you're going to have this amount of complexity that you
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:11:01,524'); seek(661.0)">
              need to build as part of your applications. So one approach to
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:11:05,556'); seek(665.0)">
              manage this complexity is that you don't have to write a lot of code
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:11:09,016'); seek(669.0)">
              and communication. Instead, try to visualize your sequences as
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:11:13,128'); seek(673.0)">
              part of a workflow. And this is where AWS step functions comes
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:11:16,552'); seek(676.0)">
              in. Step functions is service that allows you to create workflows. These are
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:11:20,108'); seek(680.0)">
              workflows that allow you to move output of one step to the input of the
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:11:23,164'); seek(683.0)">
              next step. You can arrange these in a workflow with conditional logic
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:11:26,818'); seek(686.0)">
              branches, parallel states, tools, a map state, or even
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:11:30,540'); seek(690.0)">
              specify wait states, like for example if you're running a job and then you need
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:11:33,568'); seek(693.0)">
              to wait for a certain period. Over here
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:11:37,168'); seek(697.0)">
              you can see a bit of an animation that shows you
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:11:41,392'); seek(701.0)">
              how you can choose a service. You then can then
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:11:44,932'); seek(704.0)">
              drag it from the left and then after put in the design view. Then the
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:11:48,324'); seek(708.0)">
              logic gets added. Then each step or action the workflow is configured.
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:11:52,090'); seek(712.0)">
              This also helps you to visualize how you can provide error handling
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:11:55,998'); seek(715.0)">
              and also specify retry and backup strategy.
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:11:59,646'); seek(719.0)">
              Step functions is serverless, so you only pay for what you use. It scales
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:12:03,118'); seek(723.0)">
              automatically, which also means that you can scale to zero.
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:12:06,712'); seek(726.0)">
              You're not paying when it's not being invoked. This is fully managed and provides
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:12:10,578'); seek(730.0)">
              a visual building experience using a drag and drop interface called workflow Studio.
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:12:14,754'); seek(734.0)">
              The visualization experience extends beyond building because when you
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:12:18,252'); seek(738.0)">
              run your workflow you can also visualize its progress with each step,
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:12:21,616'); seek(741.0)">
              changing colors as it moves forward and under
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:12:24,832'); seek(744.0)">
              the hood. What happens is this is using code which is using Amazon
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:12:28,118'); seek(748.0)">
              State's language, which is ESL. ESL is a domain specific language
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:12:32,102'); seek(752.0)">
              and it's JsoN based. So you can then declaratively create your
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:12:36,036'); seek(756.0)">
              workflows. So you provide that and we'll show some examples later.
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:12:39,970'); seek(759.0)">
              You can then take that ESL and then add that as part of your deployment
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:12:42,938'); seek(762.0)">
              pipelines so you can commit it to your repositories. You can also make pull requests
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:12:46,618'); seek(766.0)">
              on this so that other team members can collaborate.
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:12:49,598'); seek(769.0)">
              Now one of the things customers have told us with step functions, because step functions
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:12:52,894'); seek(772.0)">
              has been there for a few years, is that it integrates natively with 220 services
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:12:57,590'); seek(777.0)">
              and you can choose a service that you need to use as part of your
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:13:00,508'); seek(780.0)">
              workflow and take advantage of the benefits. Now the
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:13:04,028'); seek(784.0)">
              way step functions integrates with these services is through two ways.
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:13:07,820'); seek(787.0)">
              First is SDK integrations and the second is optimized integrations.
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:13:11,870'); seek(791.0)">
              SDK integrations, as the name applies, are provided
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:13:15,798'); seek(795.0)">
              by step functions by directly integrating with the AWS SDK.
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:13:19,462'); seek(799.0)">
              So that's over 10,000 API actions that you can use directly
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:13:22,598'); seek(802.0)">
              from your workflow without the need to write any customer integration code.
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:13:25,812'); seek(805.0)">
              Think blue code, which a lot of folks when they write serverless
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:13:29,514'); seek(809.0)">
              applications with lambda you tend to write. You can remove a
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:13:33,044'); seek(813.0)">
              lot of that just by using step functions. The other one is optimize
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:13:36,554'); seek(816.0)">
              integrations. Now the way they differ from SDK integrations is that each
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:13:40,376'); seek(820.0)">
              action has been customized to provide additional functionality for your workflow.
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:13:44,334'); seek(824.0)">
              So beyond just the API call, you also get certain things like for
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:13:47,608'); seek(827.0)">
              example where an API output is being converted from an
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:13:51,164'); seek(831.0)">
              escape JSON to a json object. So depending on the kind of integration
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:13:54,562'); seek(834.0)">
              that's bring, provided, those optimized integrations have that added value
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:13:58,556'); seek(838.0)">
              needed so that you don't have to then write extra code for
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:14:02,176'); seek(842.0)">
              maybe doing those manipulations. Now with any workflow
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:14:06,006'); seek(846.0)">
              and orchestration around, you need to have certain patterns that are provided,
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:14:09,622'); seek(849.0)">
              and these integration patterns by default are
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:14:12,932'); seek(852.0)">
              something that API actions can be provided
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:14:16,698'); seek(856.0)">
              with. So when you specify your workflow by default,
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:14:20,378'); seek(860.0)">
              it is asynchronous so the workflow doesn't wait or block for the action
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:14:23,978'); seek(863.0)">
              to complete. This is what you call as a standard request response call pattern.
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:14:28,186'); seek(868.0)">
              So you start the task or the work to be done and the workflow doesn't
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:14:31,374'); seek(871.0)">
              wait for complete, it moves on to the next step. This is great because it's
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:14:34,638'); seek(874.0)">
              efficient. You can continue moving quickly, but sometimes
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:14:37,736'); seek(877.0)">
              there are cases where you may need to wait until the request is complete and
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:14:41,068'); seek(881.0)">
              then you progress. And there is an optimized integration pattern
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:14:45,122'); seek(885.0)">
              called job run or also called sync. Because of
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:14:48,588'); seek(888.0)">
              the word dot sync that's added to the end of the API action. Then you
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:14:52,124'); seek(892.0)">
              also have a callback. This is what helps us to introduce a human into our
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:14:55,552'); seek(895.0)">
              flow and we're bring to see a bit of that in the architecture later.
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:14:59,710'); seek(899.0)">
              Now with these integrations that are available,
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:15:02,740'); seek(902.0)">
              you then have an idea of how you can take a business process
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:15:06,324'); seek(906.0)">
              and then afterwards integrate that across. But just to understand
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:15:10,484'); seek(910.0)">
              why this is important, let's take an example of a standard
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:15:14,036'); seek(914.0)">
              serverless application and show you why direct integration
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:15:17,310'); seek(917.0)">
              actually makes more sense. So here's a classic example. You're querying
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:15:20,574'); seek(920.0)">
              a database. So we have a lambda function that needs to get an item
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:15:23,806'); seek(923.0)">
              from a dynamodb table. So from a code perspective,
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:15:27,198'); seek(927.0)">
              what do I need to get started? I need the import AWs SDK
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:15:30,818'); seek(930.0)">
              to interact with the table. Then I need to set up my parameters
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:15:34,082'); seek(934.0)">
              to tell dynamodb what table I need to interact with. So this is
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:15:37,468'); seek(937.0)">
              like the table name, the partition key, the sort key, and then
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:15:41,068'); seek(941.0)">
              I set up my query so that there is a try catch block and then
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:15:43,888'); seek(943.0)">
              I return any errors. Now above that I also need
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:15:47,568'); seek(947.0)">
              to add lambda export handlers with my event object, my context
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:15:51,062'); seek(951.0)">
              object, and then add another try catch block to catch other errors.
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:15:54,794'); seek(954.0)">
              I may also need to convert data
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:15:57,972'); seek(957.0)">
              structures like for example an object to a string, for example, for other reasons.
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:16:01,738'); seek(961.0)">
              But you can see there's a lot of lines of code just to get one
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:16:04,468'); seek(964.0)">
              item from a dynamodb table. Now each of these lines is an
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:16:07,832'); seek(967.0)">
              area that something can go wrong. Because one thing
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:16:11,048'); seek(971.0)">
              you have to understand is code is also a liability, right? When you write code,
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:16:14,408'); seek(974.0)">
              you are responsible for the way it functions. You have to make sure that you're
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:16:17,054'); seek(977.0)">
              writing it securely, you're using the right set of dependencies, ensuring that there's
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:16:20,978'); seek(980.0)">
              no memory leaks and so on and so forth. Now when you look at it
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:16:24,348'); seek(984.0)">
              from a step functions perspective, what you can do is you have a
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:16:28,108'); seek(988.0)">
              single step that makes that item call to a dynamodb table and it's
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:16:32,194'); seek(992.0)">
              just a scalable, right? I can still configure things like retries, I can still
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:16:36,064'); seek(996.0)">
              catch any errors and then send that to a dead letter queue if I
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:16:39,424'); seek(999.0)">
              need to so that I can do a retry later. And if you notice,
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:16:42,422'); seek(1002.0)">
              what happens is that this diagram isn't just a visual representation,
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:16:46,042'); seek(1006.0)">
              this is actually showing how you can take a certain action
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:16:49,338'); seek(1009.0)">
              and then after do that, take it from start till finish.
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:16:52,548'); seek(1012.0)">
              And you can show this to other folks in your engineering team. You can also
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:16:55,608'); seek(1015.0)">
              show this to business stakeholders so that they can understand what a flow looks like.
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:16:59,032'); seek(1019.0)">
              So added value with of course the whole idea of errors and
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:17:03,000'); seek(1023.0)">
              retries and the way it would look at when you
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:17:06,712'); seek(1026.0)">
              actually add the nodes in the end with certain integrations is like this,
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:17:10,028'); seek(1030.0)">
              right? So you have dynamodb, you have the getitem side, you have SQs send
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:17:13,548'); seek(1033.0)">
              message, so on and so forth. One other thing during development,
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:17:17,058'); seek(1037.0)">
              or even when you deploy a step function to production, is that
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:17:20,268'); seek(1040.0)">
              you need to understand what's happening in the workflow and when things go wrong.
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:17:23,168'); seek(1043.0)">
              And the way you do that is you have the execution flow where you can
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:17:26,768'); seek(1046.0)">
              see different parts of the execution and then you can go within a specific execution,
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:17:30,742'); seek(1050.0)">
              see the different states, what's happening within each state, what's the input
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:17:34,198'); seek(1054.0)">
              and what's the output, and also look at things like how much time it takes
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:17:37,572'); seek(1057.0)">
              to execute a certain state. And this is really critical when there are
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:17:41,348'); seek(1061.0)">
              issues. So a great way to get all of that together and then see that
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:17:44,308'); seek(1064.0)">
              in a single pane. Now let's dive into an
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:17:48,072'); seek(1068.0)">
              actual use case, right? And we have a demo towards the end. I'll show a
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:17:51,864'); seek(1071.0)">
              couple of demos in the middle, also about bedrock and integration,
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:17:55,442'); seek(1075.0)">
              and then one where it looks at an application that uses all
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:17:59,004'); seek(1079.0)">
              of this together. So let's say you
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:18:02,732'); seek(1082.0)">
              have an application that has videos being
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:18:06,572'); seek(1086.0)">
              uploaded, and then these videos need to be transcribed,
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:18:10,546'); seek(1090.0)">
              right? So we already have a service that's available called Amazon
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:18:13,718'); seek(1093.0)">
              transcribe. And in step functions, all I need to do is I can
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:18:17,104'); seek(1097.0)">
              drag in a transcription job start node,
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:18:21,062'); seek(1101.0)">
              so I can drag that in and then afterwards say, okay, fine, for any
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:18:24,640'); seek(1104.0)">
              image that, and then trigger that step function for any video that comes in,
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:18:27,988'); seek(1107.0)">
              for example, just kick in and then afterwards do a transcription of
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:18:31,428'); seek(1111.0)">
              that video. So automatic speech recognition
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:18:34,762'); seek(1114.0)">
              happens. And this makes it easy for developers to add speech to text capability
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:18:38,526'); seek(1118.0)">
              to their applications. This integration is super powerful.
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:18:42,382'); seek(1122.0)">
              This allows you to just have this without any code that's needed. Now let's
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:18:46,238'); seek(1126.0)">
              say I want to also do something beyond this, right? So I want to take
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:18:49,948'); seek(1129.0)">
              that transcription and I want to add some additional stuff.
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:18:53,116'); seek(1133.0)">
              And this is where generative AI can help us. So I want to create multiple
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:18:56,642'); seek(1136.0)">
              titles and descriptions for a video. I want to ask a human
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:19:00,380'); seek(1140.0)">
              to provide feedback based on what choice they want to
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:19:04,128'); seek(1144.0)">
              have from the titles and then also create an avatar for the video.
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:19:07,472'); seek(1147.0)">
              So you have text also, and you have also image generation happening. And the
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:19:11,024'); seek(1151.0)">
              way you do this with step functions is you can look at
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:19:14,292'); seek(1154.0)">
              optimized integrations for Amazon bedrock. Now there are
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:19:17,412'); seek(1157.0)">
              two new optimized integrations that we have provided, and there's more that's
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:19:20,618'); seek(1160.0)">
              been added ever since where the first one is invoke
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:19:24,042'); seek(1164.0)">
              model. And this invoke model API integration allows you to orchestrate
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:19:27,742'); seek(1167.0)">
              interactions with foundational models. So you call the API directly
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:19:31,678'); seek(1171.0)">
              through step functions. You give it the parameters that are needed, you provide the
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:19:34,888'); seek(1174.0)">
              prompt that is needed and then that gets sent to the foundation model. You get
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:19:37,768'); seek(1177.0)">
              the response back and then you can continue using that. The second one
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:19:41,212'); seek(1181.0)">
              is the create model customization job. Now what this does
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:19:44,348'); seek(1184.0)">
              is this supports the run a job, the dot sync call pattern that we saw
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:19:47,772'); seek(1187.0)">
              earlier. And this means that it is waiting for the asynchronous
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:19:52,262'); seek(1192.0)">
              job to complete before progressing to the next step in your workflow.
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:19:55,446'); seek(1195.0)">
              So say for example, you're trying to create a certain customization on top of the
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:19:59,328'); seek(1199.0)">
              foundational model. It'll wait for that and then it'll go to the next step and
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:20:02,704'); seek(1202.0)">
              then afterwards continue with that process. This is useful especially in
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:20:05,908'); seek(1205.0)">
              data processing pipelines because you are trying to do some kind of fine tuning to
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:20:09,764'); seek(1209.0)">
              the model. I'll quickly jump into demo
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:20:13,460'); seek(1213.0)">
              so that you can actually see what happens with standard
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:20:17,590'); seek(1217.0)">
              implementation with bedrock. Just quickly to understand if
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:20:21,224'); seek(1221.0)">
              you're getting started with bedrock, you need to make sure that you have access to
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:20:24,024'); seek(1224.0)">
              the models. Right now you have access to foundational models
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:20:28,082'); seek(1228.0)">
              in two regions, that's North Virginia and also Oregon.
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:20:32,170'); seek(1232.0)">
              When you go to the bedrock screen you will actually see there's
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:20:35,634'); seek(1235.0)">
              a section called the model access. And this gives you a list of all the
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:20:39,228'); seek(1239.0)">
              models that are available right now in those two regions.
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:20:42,886'); seek(1242.0)">
              And if you're doing it for the first time, you will have to go and
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:20:45,952'); seek(1245.0)">
              manage your model access and then grant access to it. You'll get
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:20:49,088'); seek(1249.0)">
              that immediately unless it's a brand new model that takes a bit of time
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:20:52,784'); seek(1252.0)">
              where you may have to submit certain use cases. In my case right
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:20:56,148'); seek(1256.0)">
              now I have clot three that's in the pipeline. I'm waiting for the
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:20:59,812'); seek(1259.0)">
              details to get approved so that I can get access to this clot three just
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:21:02,948'); seek(1262.0)">
              got announced a few days ago, support in bedrock.
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:21:06,298'); seek(1266.0)">
              So I have that immediately ready. Now let me jump in directly
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:21:10,158'); seek(1270.0)">
              into a workflow. When you go to step function and you create a new step
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:21:13,752'); seek(1273.0)">
              function, you're greeted with a blank canvas. You have a state box that's empty
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:21:17,438'); seek(1277.0)">
              over here. In my case I already dragged in
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:21:20,764'); seek(1280.0)">
              bedrock API and if you want to see
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:21:24,188'); seek(1284.0)">
              the list of bedrock APIs that are currently available, you have much more right now
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:21:27,692'); seek(1287.0)">
              where you can also manage operations on foundational models if
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:21:31,184'); seek(1291.0)">
              you need to. Things like the custom models for example and listings,
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:21:35,430'); seek(1295.0)">
              especially for processing pipelines, MLO Ops, so on and so forth.
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:21:38,982'); seek(1298.0)">
              In our case I just want to do an invoke model. So I'm going to
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:21:43,024'); seek(1303.0)">
              just show you what the configuration looks like. I have foundation models already selected,
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:21:47,642'); seek(1307.0)">
              and these are the list of foundation models that are already available. As you saw
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:21:50,788'); seek(1310.0)">
              in the previous screen. In this case I have
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:21:54,324'); seek(1314.0)">
              selected llama. So llama two is already selected in
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:21:57,448'); seek(1317.0)">
              this case, and now you can configure what are the parameters that
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:22:00,968'); seek(1320.0)">
              need to be sent. What I'm doing over here is I'm just hard coding the
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:22:04,824'); seek(1324.0)">
              prompt in another demo. Quickly after this I'm going to show
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:22:08,040'); seek(1328.0)">
              where you can actually customize the prompts based on input that you
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:22:11,628'); seek(1331.0)">
              may get from other applications or maybe from the user. In my case.
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:22:14,892'); seek(1334.0)">
              All I'm saying is, okay, there's a transcript from a video in a paragraph.
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:22:17,746'); seek(1337.0)">
              This is the same video you're going to see in the last demo.
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:22:21,150'); seek(1341.0)">
              This is an interview between Amazon's CTO Werner
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:22:24,838'); seek(1344.0)">
              Vogels and ex Amazon CEO Jeff Bezos. This is from 2012,
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:22:28,256'); seek(1348.0)">
              so eleven years old, and all it does is it
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:22:32,016'); seek(1352.0)">
              uses this transcript, and then I'm asking it to provide
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:22:35,316'); seek(1355.0)">
              a summary of this transcript. So what I'll
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:22:38,618'); seek(1358.0)">
              do quickly is I'll just do an execution, and we're going to
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:22:41,748'); seek(1361.0)">
              see how it looks like when you do an execution. I'm not passing any input,
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:22:45,322'); seek(1365.0)">
              it's optional because I've already hard coded the prompt over there.
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:22:48,552'); seek(1368.0)">
              Once I run this, and within a certain execution history
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:22:52,104'); seek(1372.0)">
              or a certain point of execution, you can see the actual path.
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:22:55,934'); seek(1375.0)">
              You can see what are the different steps that are being executed. And with
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:22:59,612'); seek(1379.0)">
              bedrock model already done in this case,
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:23:03,436'); seek(1383.0)">
              you can see that the input just was an optional input that got
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:23:06,748'); seek(1386.0)">
              sent out. And here is a summary that's come back from llama two. This is
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:23:10,220'); seek(1390.0)">
              basically a summary of the transcript. It gives an example of what
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:23:13,904'); seek(1393.0)">
              Jeff Bezos mentioned and what's the whole organization working
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:23:17,616'); seek(1397.0)">
              on towards. This was eleven years ago. You also get other parameters like how much
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:23:21,248'); seek(1401.0)">
              prompts were taken and generation token. All in all, without provisioning any
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:23:24,852'); seek(1404.0)">
              large language models, without you actually managing the scaling side or
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:23:28,292'); seek(1408.0)">
              even provisioning a large language model. So pretty cool. And the
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:23:32,148'); seek(1412.0)">
              other thing, what you'll realize is with step functions you also are able to
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:23:35,876'); seek(1415.0)">
              view the different states and how much time they took to execute.
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:23:39,742'); seek(1419.0)">
              So really useful, especially if you want to debug certain things. If there's any failures
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:23:43,582'); seek(1423.0)">
              that you get that. Also over here you can actually see those errors over there.
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:23:47,290'); seek(1427.0)">
              Now another powerful way of showing what bedrock
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:23:50,818'); seek(1430.0)">
              is capable of through step functions is chaining.
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:23:53,810'); seek(1433.0)">
              And this is another demo application. What this does is this emulates
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:23:58,162'); seek(1438.0)">
              a certain conversation that you can have with
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:24:01,724'); seek(1441.0)">
              an LLM, with anything that's doing text, right? So, for example, you have a chat
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:24:05,158'); seek(1445.0)">
              interface, and with any large language model, you have to always
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:24:08,656'); seek(1448.0)">
              provide the context of, especially the history of the conversation that's happening,
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:24:11,872'); seek(1451.0)">
              so that the next one can then understand the next conversation,
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:24:14,902'); seek(1454.0)">
              or the response can be based on that conversation from before.
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:24:18,452'); seek(1458.0)">
              So in our case, what we are doing is we're creating a chain, and in
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:24:21,924'); seek(1461.0)">
              this case, I'm leveraging another foundational model called
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:24:26,232'); seek(1466.0)">
              command text from coher. And what this does is this is
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:24:31,510'); seek(1471.0)">
              reading a prompt. So it's going to read for the prompt from the input.
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:24:35,022'); seek(1475.0)">
              So when you invoke the step function, you can actually have a look
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:24:38,232'); seek(1478.0)">
              at what are the different parameters that are there
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:24:42,028'); seek(1482.0)">
              in the object, in the json body, and then afterwards you can pick that out.
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:24:45,308'); seek(1485.0)">
              In our case, what I'm doing is, I'm just saying, okay, dollar prompt one,
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:24:48,876'); seek(1488.0)">
              send this as a prompt, and these are the maximum tokens. Now, in this case,
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:24:51,872'); seek(1491.0)">
              you'll see this is a different syntax based
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:24:55,232'); seek(1495.0)">
              on this model versus what was there for llama two.
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:24:58,416'); seek(1498.0)">
              And all I'm doing is I'm adding the result
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:25:01,600'); seek(1501.0)">
              of this conversation back to the initial prompts
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:25:05,594'); seek(1505.0)">
              that are coming in so that we have context throughout this conversation.
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:25:08,922'); seek(1508.0)">
              And now if I just go in and execute this,
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:25:12,210'); seek(1512.0)">
              I'll just copy this from a previous one, because I want to pass a similar
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:25:15,684'); seek(1515.0)">
              input. I'll just do an execution over here.
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:25:18,950'); seek(1518.0)">
              In my case, I'm passing three prompts, if you notice, in the state
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:25:22,392'); seek(1522.0)">
              also, I had three of them. And all I'm doing is, I'm saying, okay,
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:25:24,952'); seek(1524.0)">
              name a random city from southeast Asia. Just want you to give some information,
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:25:28,652'); seek(1528.0)">
              provide some description for it, and then provide some more description for it.
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:25:31,948'); seek(1531.0)">
              So let's start the execution, and as you'll see,
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:25:35,868'); seek(1535.0)">
              as the execution progresses, you're going to see all these states
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:25:39,760'); seek(1539.0)">
              changing the colors based on how the
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:25:43,312'); seek(1543.0)">
              foundational model is responding. So the first result is already
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:25:47,136'); seek(1547.0)">
              in. So it says, okay, here is a random city from Southeast Asia.
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:25:50,758'); seek(1550.0)">
              So it picks Ho Chi Minh from Vietnam.
            </span>
            
            <span id="chunk-429" class="transcript-chunks" onclick="console.log('00:25:53,810'); seek(1553.0)">
              Packages that in as part of this result, one is already added
            </span>
            
            <span id="chunk-430" class="transcript-chunks" onclick="console.log('00:25:57,418'); seek(1557.0)">
              in, and then afterwards sends it to the second conversation history. You'll see
            </span>
            
            <span id="chunk-431" class="transcript-chunks" onclick="console.log('00:26:01,476'); seek(1561.0)">
              conversation result two. Here are two interesting aspects of the city,
            </span>
            
            <span id="chunk-432" class="transcript-chunks" onclick="console.log('00:26:04,888'); seek(1564.0)">
              and it mentions certain parts of this. And then invoke model with
            </span>
            
            <span id="chunk-433" class="transcript-chunks" onclick="console.log('00:26:08,712'); seek(1568.0)">
              three. And the output over here is that it takes in certain
            </span>
            
            <span id="chunk-434" class="transcript-chunks" onclick="console.log('00:26:12,856'); seek(1572.0)">
              part. Now, with large language models being
            </span>
            
            <span id="chunk-435" class="transcript-chunks" onclick="console.log('00:26:16,008'); seek(1576.0)">
              nondeterministic, a lot of times you have to be careful with how you send
            </span>
            
            <span id="chunk-436" class="transcript-chunks" onclick="console.log('00:26:19,612'); seek(1579.0)">
              your prompts and then ensure that the context is remaining. Now, in a previous execution
            </span>
            
            <span id="chunk-437" class="transcript-chunks" onclick="console.log('00:26:23,602'); seek(1583.0)">
              of the same workflow, I was able to get the third prompt and
            </span>
            
            <span id="chunk-438" class="transcript-chunks" onclick="console.log('00:26:27,948'); seek(1587.0)">
              also make sure that it continues with the city, which was previously Ho
            </span>
            
            <span id="chunk-439" class="transcript-chunks" onclick="console.log('00:26:31,388'); seek(1591.0)">
              Chi Minh. So what I would probably want to do is I would create my
            </span>
            
            <span id="chunk-440" class="transcript-chunks" onclick="console.log('00:26:34,688'); seek(1594.0)">
              third prompt in such a way that I emphasize it clearly that this is
            </span>
            
            <span id="chunk-441" class="transcript-chunks" onclick="console.log('00:26:37,968'); seek(1597.0)">
              the city that you're supposed to use. And probably the way I
            </span>
            
            <span id="chunk-442" class="transcript-chunks" onclick="console.log('00:26:41,424'); seek(1601.0)">
              would do that is I would have certain parts in my inputs, which would probably
            </span>
            
            <span id="chunk-443" class="transcript-chunks" onclick="console.log('00:26:45,232'); seek(1605.0)">
              take certain things like the city or other things and then enforce that as part
            </span>
            
            <span id="chunk-444" class="transcript-chunks" onclick="console.log('00:26:48,388'); seek(1608.0)">
              of different prompts. But in a nutshell, you kind of see how you can
            </span>
            
            <span id="chunk-445" class="transcript-chunks" onclick="console.log('00:26:52,228'); seek(1612.0)">
              do chaining in this case, and you can also bring that within
            </span>
            
            <span id="chunk-446" class="transcript-chunks" onclick="console.log('00:26:56,036'); seek(1616.0)">
              this and have a bigger application that is using this. And we're going to talk
            </span>
            
            <span id="chunk-447" class="transcript-chunks" onclick="console.log('00:26:59,032'); seek(1619.0)">
              about the architecture of this for the rest of the session.
            </span>
            
            <span id="chunk-448" class="transcript-chunks" onclick="console.log('00:27:02,790'); seek(1622.0)">
              So let's continue with that use case of generating titles and descriptions
            </span>
            
            <span id="chunk-449" class="transcript-chunks" onclick="console.log('00:27:07,294'); seek(1627.0)">
              for the videos in this case. What happens is that,
            </span>
            
            <span id="chunk-450" class="transcript-chunks" onclick="console.log('00:27:10,204'); seek(1630.0)">
              like you saw earlier in the demo that I showed, you can select the
            </span>
            
            <span id="chunk-451" class="transcript-chunks" onclick="console.log('00:27:14,892'); seek(1634.0)">
              large language model. In this case, Titan is selected.
            </span>
            
            <span id="chunk-452" class="transcript-chunks" onclick="console.log('00:27:18,258'); seek(1638.0)">
              And what under the hood happens is that the ESL for Amazon bedrock
            </span>
            
            <span id="chunk-453" class="transcript-chunks" onclick="console.log('00:27:21,858'); seek(1641.0)">
              looks something like this, right? So there's an invoke model action that's happening, and then
            </span>
            
            <span id="chunk-454" class="transcript-chunks" onclick="console.log('00:27:25,312'); seek(1645.0)">
              there is a model that is being selected. It could be llama,
            </span>
            
            <span id="chunk-455" class="transcript-chunks" onclick="console.log('00:27:29,174'); seek(1649.0)">
              it could be anything else. And then there is a dynamic input that's coming in.
            </span>
            
            <span id="chunk-456" class="transcript-chunks" onclick="console.log('00:27:32,388'); seek(1652.0)">
              So dollar prompt, which basically means something else, is invoking the
            </span>
            
            <span id="chunk-457" class="transcript-chunks" onclick="console.log('00:27:35,764'); seek(1655.0)">
              step function and then providing this prompt. Now you have also inference
            </span>
            
            <span id="chunk-458" class="transcript-chunks" onclick="console.log('00:27:39,498'); seek(1659.0)">
              parameters that allow you to tweak the response
            </span>
            
            <span id="chunk-459" class="transcript-chunks" onclick="console.log('00:27:43,114'); seek(1663.0)">
              that comes back from an LLM for various things like probability and other
            </span>
            
            <span id="chunk-460" class="transcript-chunks" onclick="console.log('00:27:46,552'); seek(1666.0)">
              things. And when you look at
            </span>
            
            <span id="chunk-461" class="transcript-chunks" onclick="console.log('00:27:50,200'); seek(1670.0)">
              invoking the model, you can also provide input and output.
            </span>
            
            <span id="chunk-462" class="transcript-chunks" onclick="console.log('00:27:53,598'); seek(1673.0)">
              So for example, if your input is larger than 256 kb, because a
            </span>
            
            <span id="chunk-463" class="transcript-chunks" onclick="console.log('00:27:56,888'); seek(1676.0)">
              step function can only take 256 kb of content text,
            </span>
            
            <span id="chunk-464" class="transcript-chunks" onclick="console.log('00:28:00,072'); seek(1680.0)">
              usually in this case, what you can do is you can point to an S
            </span>
            
            <span id="chunk-465" class="transcript-chunks" onclick="console.log('00:28:03,308'); seek(1683.0)">
              three bucket for input and for output. It's a good way to ensure
            </span>
            
            <span id="chunk-466" class="transcript-chunks" onclick="console.log('00:28:07,138'); seek(1687.0)">
              that you're able to scale this application without facing
            </span>
            
            <span id="chunk-467" class="transcript-chunks" onclick="console.log('00:28:10,658'); seek(1690.0)">
              the restrictions or the constraints by step functions.
            </span>
            
            <span id="chunk-468" class="transcript-chunks" onclick="console.log('00:28:14,190'); seek(1694.0)">
              So this input and output is then used, and then you can change this and
            </span>
            
            <span id="chunk-469" class="transcript-chunks" onclick="console.log('00:28:17,584'); seek(1697.0)">
              you can continue using this in different states within step
            </span>
            
            <span id="chunk-470" class="transcript-chunks" onclick="console.log('00:28:21,616'); seek(1701.0)">
              function. One thing you'll realize is that in the first requirement,
            </span>
            
            <span id="chunk-471" class="transcript-chunks" onclick="console.log('00:28:25,354'); seek(1705.0)">
              it was actually mentioned about creating multiple titles. Now,
            </span>
            
            <span id="chunk-472" class="transcript-chunks" onclick="console.log('00:28:29,028'); seek(1709.0)">
              for example, we can continue using just the foundational models within AWS.
            </span>
            
            <span id="chunk-473" class="transcript-chunks" onclick="console.log('00:28:33,258'); seek(1713.0)">
              But what if we want to access something that's outside, let's say for example,
            </span>
            
            <span id="chunk-474" class="transcript-chunks" onclick="console.log('00:28:36,696'); seek(1716.0)">
              hugging phase, you want to access this foundational model from
            </span>
            
            <span id="chunk-475" class="transcript-chunks" onclick="console.log('00:28:40,152'); seek(1720.0)">
              outside AWS. We want to then
            </span>
            
            <span id="chunk-476" class="transcript-chunks" onclick="console.log('00:28:44,070'); seek(1724.0)">
              get the data, send that across, and then after get the response back and
            </span>
            
            <span id="chunk-477" class="transcript-chunks" onclick="console.log('00:28:47,544'); seek(1727.0)">
              then continue in our execution. Now when you look at accessing
            </span>
            
            <span id="chunk-478" class="transcript-chunks" onclick="console.log('00:28:51,154'); seek(1731.0)">
              a public API in general, it might look simple. Then the first question
            </span>
            
            <span id="chunk-479" class="transcript-chunks" onclick="console.log('00:28:54,492'); seek(1734.0)">
              comes is what is the kind of authentication that you need, right? Is there basic
            </span>
            
            <span id="chunk-480" class="transcript-chunks" onclick="console.log('00:28:58,194'); seek(1738.0)">
              authentication? Is there API keys? Is there oauth?
            </span>
            
            <span id="chunk-481" class="transcript-chunks" onclick="console.log('00:29:01,746'); seek(1741.0)">
              Is there anything else token management for example. Then you
            </span>
            
            <span id="chunk-482" class="transcript-chunks" onclick="console.log('00:29:05,472'); seek(1745.0)">
              also want to ensure that you're saving the secrets because you want to make sure
            </span>
            
            <span id="chunk-483" class="transcript-chunks" onclick="console.log('00:29:08,768'); seek(1748.0)">
              maybe there's an access key for accessing the API. You want to keep that somewhere.
            </span>
            
            <span id="chunk-484" class="transcript-chunks" onclick="console.log('00:29:12,310'); seek(1752.0)">
              Then you have input output handling. You also
            </span>
            
            <span id="chunk-485" class="transcript-chunks" onclick="console.log('00:29:15,812'); seek(1755.0)">
              then have a graceful retry if something goes wrong. Then also
            </span>
            
            <span id="chunk-486" class="transcript-chunks" onclick="console.log('00:29:19,172'); seek(1759.0)">
              rate control and so many other things. Now the way you would do that
            </span>
            
            <span id="chunk-487" class="transcript-chunks" onclick="console.log('00:29:22,532'); seek(1762.0)">
              with AWS lambda for example, or maybe a container
            </span>
            
            <span id="chunk-488" class="transcript-chunks" onclick="console.log('00:29:26,058'); seek(1766.0)">
              or virtual machine on EC two is that you would have your code running and
            </span>
            
            <span id="chunk-489" class="transcript-chunks" onclick="console.log('00:29:30,008'); seek(1770.0)">
              then you would have these different services which would fetch the credentials, you would manage
            </span>
            
            <span id="chunk-490" class="transcript-chunks" onclick="console.log('00:29:33,272'); seek(1773.0)">
              the token, you would then retrieve the request data and then afterwards
            </span>
            
            <span id="chunk-491" class="transcript-chunks" onclick="console.log('00:29:36,606'); seek(1776.0)">
              invoke and get back the data, maybe store it somewhere else also if needed,
            </span>
            
            <span id="chunk-492" class="transcript-chunks" onclick="console.log('00:29:40,492'); seek(1780.0)">
              this is what a resilient application would look like.
            </span>
            
            <span id="chunk-493" class="transcript-chunks" onclick="console.log('00:29:43,850'); seek(1783.0)">
              One other way you can do this without writing code is by public
            </span>
            
            <span id="chunk-494" class="transcript-chunks" onclick="console.log('00:29:47,836'); seek(1787.0)">
              HTTPs API integration on step functions. So step
            </span>
            
            <span id="chunk-495" class="transcript-chunks" onclick="console.log('00:29:51,616'); seek(1791.0)">
              functions has the ability to call virtually any SaaS application from a workflow
            </span>
            
            <span id="chunk-496" class="transcript-chunks" onclick="console.log('00:29:55,462'); seek(1795.0)">
              with the integration with HTTPS endpoints. So without
            </span>
            
            <span id="chunk-497" class="transcript-chunks" onclick="console.log('00:29:59,280'); seek(1799.0)">
              using a lambda function, you can use huggingface for
            </span>
            
            <span id="chunk-498" class="transcript-chunks" onclick="console.log('00:30:03,104'); seek(1803.0)">
              example, you can invoke an API and hugging face or maybe other APIs
            </span>
            
            <span id="chunk-499" class="transcript-chunks" onclick="console.log('00:30:06,858'); seek(1806.0)">
              like stripe, Salesforce, GitHub, Adobe for example.
            </span>
            
            <span id="chunk-500" class="transcript-chunks" onclick="console.log('00:30:10,292'); seek(1810.0)">
              And step functions now with this low code approach
            </span>
            
            <span id="chunk-501" class="transcript-chunks" onclick="console.log('00:30:13,914'); seek(1813.0)">
              provides you a way to connect AWS services with services that are outside.
            </span>
            
            <span id="chunk-502" class="transcript-chunks" onclick="console.log('00:30:17,784'); seek(1817.0)">
              And you can then take advantage of workflow studio because now you're dragging drop
            </span>
            
            <span id="chunk-503" class="transcript-chunks" onclick="console.log('00:30:21,950'); seek(1821.0)">
              all of these things and then after putting that as part of the workflow together
            </span>
            
            <span id="chunk-504" class="transcript-chunks" onclick="console.log('00:30:25,096'); seek(1825.0)">
              without changing or managing any code as part of
            </span>
            
            <span id="chunk-505" class="transcript-chunks" onclick="console.log('00:30:28,668'); seek(1828.0)">
              this. So with such
            </span>
            
            <span id="chunk-506" class="transcript-chunks" onclick="console.log('00:30:32,172'); seek(1832.0)">
              requests you can actually then put in your json object and then
            </span>
            
            <span id="chunk-507" class="transcript-chunks" onclick="console.log('00:30:35,676'); seek(1835.0)">
              in the request body you can then mention okay, this is the kind of data
            </span>
            
            <span id="chunk-508" class="transcript-chunks" onclick="console.log('00:30:38,652'); seek(1838.0)">
              that we are sending and this is what we are trying to retrieve back as
            </span>
            
            <span id="chunk-509" class="transcript-chunks" onclick="console.log('00:30:42,192'); seek(1842.0)">
              part of that transformation. One of the ways that you can actually
            </span>
            
            <span id="chunk-510" class="transcript-chunks" onclick="console.log('00:30:47,550'); seek(1847.0)">
              use this for integrating with HTTP APIs is that
            </span>
            
            <span id="chunk-511" class="transcript-chunks" onclick="console.log('00:30:51,120'); seek(1851.0)">
              you can manage the errors also through step functions like we saw kind of you
            </span>
            
            <span id="chunk-512" class="transcript-chunks" onclick="console.log('00:30:54,516'); seek(1854.0)">
              have that ability to do error handling. You can also
            </span>
            
            <span id="chunk-513" class="transcript-chunks" onclick="console.log('00:30:58,308'); seek(1858.0)">
              manage authorization as part of that integration. You can
            </span>
            
            <span id="chunk-514" class="transcript-chunks" onclick="console.log('00:31:02,068'); seek(1862.0)">
              also mention transformation of data because step functions already provides that for
            </span>
            
            <span id="chunk-515" class="transcript-chunks" onclick="console.log('00:31:05,268'); seek(1865.0)">
              optimized integrations. So you can also leverage that if needed for things like
            </span>
            
            <span id="chunk-516" class="transcript-chunks" onclick="console.log('00:31:09,128'); seek(1869.0)">
              URL encoding for request body and there's also a test
            </span>
            
            <span id="chunk-517" class="transcript-chunks" onclick="console.log('00:31:12,472'); seek(1872.0)">
              state that allows you to execute that specific state
            </span>
            
            <span id="chunk-518" class="transcript-chunks" onclick="console.log('00:31:17,510'); seek(1877.0)">
              without deploying that step function directly outside. So you can just
            </span>
            
            <span id="chunk-519" class="transcript-chunks" onclick="console.log('00:31:20,732'); seek(1880.0)">
              execute that specific state as a test and then afterwards make sure
            </span>
            
            <span id="chunk-520" class="transcript-chunks" onclick="console.log('00:31:24,124'); seek(1884.0)">
              that you're getting the kind of response that is needed. So with
            </span>
            
            <span id="chunk-521" class="transcript-chunks" onclick="console.log('00:31:28,204'); seek(1888.0)">
              the task state that's available, you have that single unit of work. You can
            </span>
            
            <span id="chunk-522" class="transcript-chunks" onclick="console.log('00:31:31,664'); seek(1891.0)">
              do an HTTP invoke and you can see that an existing
            </span>
            
            <span id="chunk-523" class="transcript-chunks" onclick="console.log('00:31:34,998'); seek(1894.0)">
              resource field is available now and you also have the new
            </span>
            
            <span id="chunk-524" class="transcript-chunks" onclick="console.log('00:31:38,832'); seek(1898.0)">
              option. And you can also then provide things like what methods
            </span>
            
            <span id="chunk-525" class="transcript-chunks" onclick="console.log('00:31:43,114'); seek(1903.0)">
              are being invoked. For example, what's the authentication field that is there?
            </span>
            
            <span id="chunk-526" class="transcript-chunks" onclick="console.log('00:31:46,724'); seek(1906.0)">
              The parameters block. This under the hood is actually using another
            </span>
            
            <span id="chunk-527" class="transcript-chunks" onclick="console.log('00:31:50,756'); seek(1910.0)">
              service called Eventbridge. So Amazon Eventbridge is being used for API destinations because it
            </span>
            
            <span id="chunk-528" class="transcript-chunks" onclick="console.log('00:31:54,808'); seek(1914.0)">
              has that ability to invoke or send requests
            </span>
            
            <span id="chunk-529" class="transcript-chunks" onclick="console.log('00:31:58,798'); seek(1918.0)">
              to an API destination. So the same connection is actually being
            </span>
            
            <span id="chunk-530" class="transcript-chunks" onclick="console.log('00:32:02,552'); seek(1922.0)">
              used as part of that. A lot of these parameters are actually optional.
            </span>
            
            <span id="chunk-531" class="transcript-chunks" onclick="console.log('00:32:05,982'); seek(1925.0)">
              So when you're invoking a certain API, probably you're just getting a response back.
            </span>
            
            <span id="chunk-532" class="transcript-chunks" onclick="console.log('00:32:09,980'); seek(1929.0)">
              You don't need to pass any query parameters. In this case, what we're doing
            </span>
            
            <span id="chunk-533" class="transcript-chunks" onclick="console.log('00:32:13,308'); seek(1933.0)">
              is that we can add a request for headers and then anything
            </span>
            
            <span id="chunk-534" class="transcript-chunks" onclick="console.log('00:32:17,052'); seek(1937.0)">
              else that's needed as part of the request. Now let's go
            </span>
            
            <span id="chunk-535" class="transcript-chunks" onclick="console.log('00:32:20,288'); seek(1940.0)">
              back to a requirement directly. So in our case, since we want to generate
            </span>
            
            <span id="chunk-536" class="transcript-chunks" onclick="console.log('00:32:23,878'); seek(1943.0)">
              multiple titles, we want to make sure that we're able to access
            </span>
            
            <span id="chunk-537" class="transcript-chunks" onclick="console.log('00:32:28,272'); seek(1948.0)">
              one title from our model ourselves,
            </span>
            
            <span id="chunk-538" class="transcript-chunks" onclick="console.log('00:32:32,038'); seek(1952.0)">
              and then after one from hugging phase. So we have a parallel state.
            </span>
            
            <span id="chunk-539" class="transcript-chunks" onclick="console.log('00:32:35,572'); seek(1955.0)">
              Now through step functions, this allows us to use both the foundational
            </span>
            
            <span id="chunk-540" class="transcript-chunks" onclick="console.log('00:32:39,322'); seek(1959.0)">
              models and you simply configure the endpoints on the right hand side.
            </span>
            
            <span id="chunk-541" class="transcript-chunks" onclick="console.log('00:32:42,836'); seek(1962.0)">
              This way the parallel state will then execute and
            </span>
            
            <span id="chunk-542" class="transcript-chunks" onclick="console.log('00:32:46,808'); seek(1966.0)">
              it will invoke each task in parallel. It then requires that
            </span>
            
            <span id="chunk-543" class="transcript-chunks" onclick="console.log('00:32:50,392'); seek(1970.0)">
              each branch completes successfully for the parallel state to be considered successful.
            </span>
            
            <span id="chunk-544" class="transcript-chunks" onclick="console.log('00:32:55,038'); seek(1975.0)">
              Now what happens if one of these branches doesn't complete successfully?
            </span>
            
            <span id="chunk-545" class="transcript-chunks" onclick="console.log('00:32:58,622'); seek(1978.0)">
              Right? So what if something goes wrong? Maybe there's an issue in the call
            </span>
            
            <span id="chunk-546" class="transcript-chunks" onclick="console.log('00:33:01,932'); seek(1981.0)">
              for one of our two FMs, and errors happen for
            </span>
            
            <span id="chunk-547" class="transcript-chunks" onclick="console.log('00:33:04,988'); seek(1984.0)">
              various reasons. And if it's a transient issue such as a network interruption,
            </span>
            
            <span id="chunk-548" class="transcript-chunks" onclick="console.log('00:33:09,058'); seek(1989.0)">
              you want to make sure that you're able to do a retry, and maybe you
            </span>
            
            <span id="chunk-549" class="transcript-chunks" onclick="console.log('00:33:12,768'); seek(1992.0)">
              want to do that retry for a couple of times. And then you also configure
            </span>
            
            <span id="chunk-550" class="transcript-chunks" onclick="console.log('00:33:16,118'); seek(1996.0)">
              something called as a backup rate to ensure that you don't overload the
            </span>
            
            <span id="chunk-551" class="transcript-chunks" onclick="console.log('00:33:19,728'); seek(1999.0)">
              third party system. And for these momentary blips,
            </span>
            
            <span id="chunk-552" class="transcript-chunks" onclick="console.log('00:33:23,286'); seek(2003.0)">
              it's just important. You just need to make sure that you have a retry mechanism
            </span>
            
            <span id="chunk-553" class="transcript-chunks" onclick="console.log('00:33:27,082'); seek(2007.0)">
              of sorts. But what if that underlying error is actually something
            </span>
            
            <span id="chunk-554" class="transcript-chunks" onclick="console.log('00:33:30,980'); seek(2010.0)">
              which requires a longer investigation, right, or a longer resolution? Time, because maybe
            </span>
            
            <span id="chunk-555" class="transcript-chunks" onclick="console.log('00:33:34,548'); seek(2014.0)">
              it's not under your control, maybe it's independent of your team,
            </span>
            
            <span id="chunk-556" class="transcript-chunks" onclick="console.log('00:33:37,748'); seek(2017.0)">
              and maybe it's somebody else who's managing it, or maybe even a third party.
            </span>
            
            <span id="chunk-557" class="transcript-chunks" onclick="console.log('00:33:40,702'); seek(2020.0)">
              And what may happen is you may exhaust your retry strategy and then eventually
            </span>
            
            <span id="chunk-558" class="transcript-chunks" onclick="console.log('00:33:45,510'); seek(2025.0)">
              that workflow step will actually fail. So you
            </span>
            
            <span id="chunk-559" class="transcript-chunks" onclick="console.log('00:33:49,112'); seek(2029.0)">
              want to make sure that you're able to run this entire workflow, but then at
            </span>
            
            <span id="chunk-560" class="transcript-chunks" onclick="console.log('00:33:52,524'); seek(2032.0)">
              the same time, if you can't, then you want to move it to an error
            </span>
            
            <span id="chunk-561" class="transcript-chunks" onclick="console.log('00:33:54,978'); seek(2034.0)">
              state, or then move it somewhere else so that you can retry it later.
            </span>
            
            <span id="chunk-562" class="transcript-chunks" onclick="console.log('00:33:59,450'); seek(2039.0)">
              So if you want to visualize this, this is basically what it looks
            </span>
            
            <span id="chunk-563" class="transcript-chunks" onclick="console.log('00:34:02,592'); seek(2042.0)">
              like. So you have a success tip that then kicks off a parallel workflow.
            </span>
            
            <span id="chunk-564" class="transcript-chunks" onclick="console.log('00:34:06,598'); seek(2046.0)">
              This parallel workflow has two branches, so you
            </span>
            
            <span id="chunk-565" class="transcript-chunks" onclick="console.log('00:34:09,904'); seek(2049.0)">
              have bedrock on the left, hugging face on the left. And let's say we invoke
            </span>
            
            <span id="chunk-566" class="transcript-chunks" onclick="console.log('00:34:13,018'); seek(2053.0)">
              the foundation model, we have some transformations we want to do using another
            </span>
            
            <span id="chunk-567" class="transcript-chunks" onclick="console.log('00:34:16,612'); seek(2056.0)">
              AWS service, and that is an extra step. But let's
            </span>
            
            <span id="chunk-568" class="transcript-chunks" onclick="console.log('00:34:20,714'); seek(2060.0)">
              say there is some failure in transformation because we have invoked something hugging
            </span>
            
            <span id="chunk-569" class="transcript-chunks" onclick="console.log('00:34:24,378'); seek(2064.0)">
              face, and then when we get it back, something's not working.
            </span>
            
            <span id="chunk-570" class="transcript-chunks" onclick="console.log('00:34:27,670'); seek(2067.0)">
              And this transcription job needs to continue. Right. There's some form that
            </span>
            
            <span id="chunk-571" class="transcript-chunks" onclick="console.log('00:34:31,368'); seek(2071.0)">
              needs to happen, and we have stopped it over here before actually moving
            </span>
            
            <span id="chunk-572" class="transcript-chunks" onclick="console.log('00:34:34,680'); seek(2074.0)">
              it to the next step, which is a human review.
            </span>
            
            <span id="chunk-573" class="transcript-chunks" onclick="console.log('00:34:38,120'); seek(2078.0)">
              This is where you have the option of Redrive. Now, Redrive allows
            </span>
            
            <span id="chunk-574" class="transcript-chunks" onclick="console.log('00:34:41,858'); seek(2081.0)">
              you to easily restart workflows, maybe because you have figured out,
            </span>
            
            <span id="chunk-575" class="transcript-chunks" onclick="console.log('00:34:45,228'); seek(2085.0)">
              okay, there's a problem, and then maybe it's got resolved, and then you want to
            </span>
            
            <span id="chunk-576" class="transcript-chunks" onclick="console.log('00:34:47,804'); seek(2087.0)">
              retry that workflow all over again so you can recover from failure
            </span>
            
            <span id="chunk-577" class="transcript-chunks" onclick="console.log('00:34:51,154'); seek(2091.0)">
              faster, and you only pay for what you need. So you don't have
            </span>
            
            <span id="chunk-578" class="transcript-chunks" onclick="console.log('00:34:54,384'); seek(2094.0)">
              to keep retrying it unless it's really necessary. So the way this
            </span>
            
            <span id="chunk-579" class="transcript-chunks" onclick="console.log('00:34:58,032'); seek(2098.0)">
              works is you will have these two branches on
            </span>
            
            <span id="chunk-580" class="transcript-chunks" onclick="console.log('00:35:01,808'); seek(2101.0)">
              the left and hugging face on the left. And let's say that when
            </span>
            
            <span id="chunk-581" class="transcript-chunks" onclick="console.log('00:35:04,948'); seek(2104.0)">
              we invoke, we do the transformation, but it fails in the transformation step,
            </span>
            
            <span id="chunk-582" class="transcript-chunks" onclick="console.log('00:35:09,650'); seek(2109.0)">
              so it gets fixed, and then after you come back again, and then you
            </span>
            
            <span id="chunk-583" class="transcript-chunks" onclick="console.log('00:35:13,188'); seek(2113.0)">
              do a retry again once more, and this time the transcription actually kicks in
            </span>
            
            <span id="chunk-584" class="transcript-chunks" onclick="console.log('00:35:17,544'); seek(2117.0)">
              because your transformations are already completed. And now it goes into the human review
            </span>
            
            <span id="chunk-585" class="transcript-chunks" onclick="console.log('00:35:21,192'); seek(2121.0)">
              space if needed. So one of the other things
            </span>
            
            <span id="chunk-586" class="transcript-chunks" onclick="console.log('00:35:25,112'); seek(2125.0)">
              you want to do also as a part of workflows is you want to have
            </span>
            
            <span id="chunk-587" class="transcript-chunks" onclick="console.log('00:35:27,608'); seek(2127.0)">
              observability. Execution event history is very important as
            </span>
            
            <span id="chunk-588" class="transcript-chunks" onclick="console.log('00:35:30,812'); seek(2130.0)">
              part of this because you have different states that are coming in, you have events
            </span>
            
            <span id="chunk-589" class="transcript-chunks" onclick="console.log('00:35:33,698'); seek(2133.0)">
              being fired. You want to make sure that you're able to filter and
            </span>
            
            <span id="chunk-590" class="transcript-chunks" onclick="console.log('00:35:37,276'); seek(2137.0)">
              drill down to what's actually happening within your workflow. This is kind
            </span>
            
            <span id="chunk-591" class="transcript-chunks" onclick="console.log('00:35:41,184'); seek(2141.0)">
              of like where you can see execution Redriven and it also shows
            </span>
            
            <span id="chunk-592" class="transcript-chunks" onclick="console.log('00:35:44,560'); seek(2144.0)">
              a count, a redrive count of how many times you are actually retrying that
            </span>
            
            <span id="chunk-593" class="transcript-chunks" onclick="console.log('00:35:48,448'); seek(2148.0)">
              execution through Redrive. So cool. I think it's a great way
            </span>
            
            <span id="chunk-594" class="transcript-chunks" onclick="console.log('00:35:52,068'); seek(2152.0)">
              to understand how you can actually manage events,
            </span>
            
            <span id="chunk-595" class="transcript-chunks" onclick="console.log('00:35:55,162'); seek(2155.0)">
              especially errors in this case, and then ensure that your workflows
            </span>
            
            <span id="chunk-596" class="transcript-chunks" onclick="console.log('00:35:58,458'); seek(2158.0)">
              are able to then continue properly.
            </span>
            
            <span id="chunk-597" class="transcript-chunks" onclick="console.log('00:36:02,050'); seek(2162.0)">
              Now, with multiple titles out of the way,
            </span>
            
            <span id="chunk-598" class="transcript-chunks" onclick="console.log('00:36:05,544'); seek(2165.0)">
              let's talk about asking a human to provide feedback.
            </span>
            
            <span id="chunk-599" class="transcript-chunks" onclick="console.log('00:36:09,342'); seek(2169.0)">
              Now, having a human approval is an automated business process and it is super
            </span>
            
            <span id="chunk-600" class="transcript-chunks" onclick="console.log('00:36:13,192'); seek(2173.0)">
              common. You have this as part of any approvals that are happening,
            </span>
            
            <span id="chunk-601" class="transcript-chunks" onclick="console.log('00:36:17,416'); seek(2177.0)">
              probably in the banking space, in the financial space. You have
            </span>
            
            <span id="chunk-602" class="transcript-chunks" onclick="console.log('00:36:21,420'); seek(2181.0)">
              probably also a human in the loop as part of maybe a foundational model that
            </span>
            
            <span id="chunk-603" class="transcript-chunks" onclick="console.log('00:36:25,244'); seek(2185.0)">
              you have created or you have custom built, or maybe you're fine tuned
            </span>
            
            <span id="chunk-604" class="transcript-chunks" onclick="console.log('00:36:28,866'); seek(2188.0)">
              and then you want to make sure that you're able to check the response that
            </span>
            
            <span id="chunk-605" class="transcript-chunks" onclick="console.log('00:36:31,148'); seek(2191.0)">
              are coming in. Maybe you have an EB flow that's happening, right? For a few
            </span>
            
            <span id="chunk-606" class="transcript-chunks" onclick="console.log('00:36:34,316'); seek(2194.0)">
              requests that need to come in, you want to have a human response that needs
            </span>
            
            <span id="chunk-607" class="transcript-chunks" onclick="console.log('00:36:37,452'); seek(2197.0)">
              to happen, human review that needs to happen. So the requirement is super
            </span>
            
            <span id="chunk-608" class="transcript-chunks" onclick="console.log('00:36:41,316'); seek(2201.0)">
              simple, but possibilities are endless when you need to do this.
            </span>
            
            <span id="chunk-609" class="transcript-chunks" onclick="console.log('00:36:44,804'); seek(2204.0)">
              So step functions integrates with services in multiple ways,
            </span>
            
            <span id="chunk-610" class="transcript-chunks" onclick="console.log('00:36:48,770'); seek(2208.0)">
              and one of the ways you can do this is through long running jobs of
            </span>
            
            <span id="chunk-611" class="transcript-chunks" onclick="console.log('00:36:51,828'); seek(2211.0)">
              a service, and you want to wait for the job to complete and
            </span>
            
            <span id="chunk-612" class="transcript-chunks" onclick="console.log('00:36:56,100'); seek(2216.0)">
              we'll use this integration pattern to achieve this requirement.
            </span>
            
            <span id="chunk-613" class="transcript-chunks" onclick="console.log('00:36:59,510'); seek(2219.0)">
              So what you want to do is you want to make a call to a
            </span>
            
            <span id="chunk-614" class="transcript-chunks" onclick="console.log('00:37:01,624'); seek(2221.0)">
              service integration. This passes a unique token and this token then gets
            </span>
            
            <span id="chunk-615" class="transcript-chunks" onclick="console.log('00:37:05,192'); seek(2225.0)">
              embedded in an email. It goes to maybe a server or
            </span>
            
            <span id="chunk-616" class="transcript-chunks" onclick="console.log('00:37:08,668'); seek(2228.0)">
              on premise and legacy server, or to a long running task
            </span>
            
            <span id="chunk-617" class="transcript-chunks" onclick="console.log('00:37:12,434'); seek(2232.0)">
              in a container, for example. And then once that response
            </span>
            
            <span id="chunk-618" class="transcript-chunks" onclick="console.log('00:37:16,498'); seek(2236.0)">
              is maybe it's reviewed, and then after they click on going ahead or
            </span>
            
            <span id="chunk-619" class="transcript-chunks" onclick="console.log('00:37:20,128'); seek(2240.0)">
              not, it returns using the step functions API, send task success.
            </span>
            
            <span id="chunk-620" class="transcript-chunks" onclick="console.log('00:37:23,904'); seek(2243.0)">
              And then the workflow continues from there. So as
            </span>
            
            <span id="chunk-621" class="transcript-chunks" onclick="console.log('00:37:28,128'); seek(2248.0)">
              part of the send response and wait workflow, there will be
            </span>
            
            <span id="chunk-622" class="transcript-chunks" onclick="console.log('00:37:31,344'); seek(2251.0)">
              a token that's sent out like I mentioned earlier, and this email notification is
            </span>
            
            <span id="chunk-623" class="transcript-chunks" onclick="console.log('00:37:34,768'); seek(2254.0)">
              already there. Maybe as part of this use case at least,
            </span>
            
            <span id="chunk-624" class="transcript-chunks" onclick="console.log('00:37:37,812'); seek(2257.0)">
              what will happen is there will be options that are being set. So choose the
            </span>
            
            <span id="chunk-625" class="transcript-chunks" onclick="console.log('00:37:40,164'); seek(2260.0)">
              title that's being generated by Amazon Bedrock, or choose the title that's
            </span>
            
            <span id="chunk-626" class="transcript-chunks" onclick="console.log('00:37:43,578'); seek(2263.0)">
              being generated by hugging face and then regenerate that.
            </span>
            
            <span id="chunk-627" class="transcript-chunks" onclick="console.log('00:37:47,192'); seek(2267.0)">
              Now the last part of this requirement is creating an avatar for the video,
            </span>
            
            <span id="chunk-628" class="transcript-chunks" onclick="console.log('00:37:50,520'); seek(2270.0)">
              which basically is an image in this case. And machine learning models,
            </span>
            
            <span id="chunk-629" class="transcript-chunks" onclick="console.log('00:37:55,590'); seek(2275.0)">
              especially in the foundational model space, you have built in algorithms.
            </span>
            
            <span id="chunk-630" class="transcript-chunks" onclick="console.log('00:37:59,886'); seek(2279.0)">
              We also have pre built ML solutions that are already available. You can probably invoke
            </span>
            
            <span id="chunk-631" class="transcript-chunks" onclick="console.log('00:38:03,474'); seek(2283.0)">
              a third party API again for this case, and there are multiple
            </span>
            
            <span id="chunk-632" class="transcript-chunks" onclick="console.log('00:38:06,898'); seek(2286.0)">
              ways you can do this as a part of bedrock. You also have access to
            </span>
            
            <span id="chunk-633" class="transcript-chunks" onclick="console.log('00:38:09,868'); seek(2289.0)">
              stability diffusion models, so you
            </span>
            
            <span id="chunk-634" class="transcript-chunks" onclick="console.log('00:38:13,024'); seek(2293.0)">
              can use that also as part of the step. What this does is in the
            </span>
            
            <span id="chunk-635" class="transcript-chunks" onclick="console.log('00:38:16,208'); seek(2296.0)">
              end, once you have tried the feedback, you can then generate that video,
            </span>
            
            <span id="chunk-636" class="transcript-chunks" onclick="console.log('00:38:19,456'); seek(2299.0)">
              sorry, the avatar for the video, and then you can store
            </span>
            
            <span id="chunk-637" class="transcript-chunks" onclick="console.log('00:38:22,816'); seek(2302.0)">
              that in an S three bucket and then share that link later.
            </span>
            
            <span id="chunk-638" class="transcript-chunks" onclick="console.log('00:38:26,930'); seek(2306.0)">
              Now, one of the things you'll realize when you want to create such a complex
            </span>
            
            <span id="chunk-639" class="transcript-chunks" onclick="console.log('00:38:29,882'); seek(2309.0)">
              workflow is that especially with foundation models, you want to have this whole
            </span>
            
            <span id="chunk-640" class="transcript-chunks" onclick="console.log('00:38:33,556'); seek(2313.0)">
              idea of creating chains of prompts, which we kind of saw in the demo.
            </span>
            
            <span id="chunk-641" class="transcript-chunks" onclick="console.log('00:38:37,016'); seek(2317.0)">
              Now this is a technique of writing multiple prompts and responses. A sequence of steps.
            </span>
            
            <span id="chunk-642" class="transcript-chunks" onclick="console.log('00:38:41,182'); seek(2321.0)">
              Step functions is a workflow is a great way to actually
            </span>
            
            <span id="chunk-643" class="transcript-chunks" onclick="console.log('00:38:44,520'); seek(2324.0)">
              leverage chaining, so you can actually use this.
            </span>
            
            <span id="chunk-644" class="transcript-chunks" onclick="console.log('00:38:47,784'); seek(2327.0)">
              And step function simplifies the way you invoke your foundational models,
            </span>
            
            <span id="chunk-645" class="transcript-chunks" onclick="console.log('00:38:51,090'); seek(2331.0)">
              and you have state independency management already in place. You can then
            </span>
            
            <span id="chunk-646" class="transcript-chunks" onclick="console.log('00:38:54,252'); seek(2334.0)">
              create chaining easily. You can also pass the state, as we saw earlier,
            </span>
            
            <span id="chunk-647" class="transcript-chunks" onclick="console.log('00:38:57,362'); seek(2337.0)">
              pass that to the next state that's needed, the response of a
            </span>
            
            <span id="chunk-648" class="transcript-chunks" onclick="console.log('00:39:00,908'); seek(2340.0)">
              state, and then pass that to the next one, maybe specific parts of the prompt
            </span>
            
            <span id="chunk-649" class="transcript-chunks" onclick="console.log('00:39:04,262'); seek(2344.0)">
              also if you need to. And all of this is again serverless. So think of
            </span>
            
            <span id="chunk-650" class="transcript-chunks" onclick="console.log('00:39:07,648'); seek(2347.0)">
              use cases like writing blogs and articles, response validation, conversational LLMs
            </span>
            
            <span id="chunk-651" class="transcript-chunks" onclick="console.log('00:39:11,718'); seek(2351.0)">
              that we see a lot these days. Now, if you want to now take all
            </span>
            
            <span id="chunk-652" class="transcript-chunks" onclick="console.log('00:39:15,588'); seek(2355.0)">
              of what we have seen and then put that in an architecture, this is what
            </span>
            
            <span id="chunk-653" class="transcript-chunks" onclick="console.log('00:39:18,164'); seek(2358.0)">
              it looks like. So for example, you have an API gateway
            </span>
            
            <span id="chunk-654" class="transcript-chunks" onclick="console.log('00:39:21,498'); seek(2361.0)">
              that a user would invoke through an application,
            </span>
            
            <span id="chunk-655" class="transcript-chunks" onclick="console.log('00:39:25,560'); seek(2365.0)">
              and then that would then put in an event into
            </span>
            
            <span id="chunk-656" class="transcript-chunks" onclick="console.log('00:39:29,432'); seek(2369.0)">
              a queue, and then this
            </span>
            
            <span id="chunk-657" class="transcript-chunks" onclick="console.log('00:39:32,712'); seek(2372.0)">
              event in the queue then gets picked up by a lambda function, which then would
            </span>
            
            <span id="chunk-658" class="transcript-chunks" onclick="console.log('00:39:35,448'); seek(2375.0)">
              trigger this step functions workflow. And in this case,
            </span>
            
            <span id="chunk-659" class="transcript-chunks" onclick="console.log('00:39:38,792'); seek(2378.0)">
              what happens is that you have a lot of these steps already in place
            </span>
            
            <span id="chunk-660" class="transcript-chunks" onclick="console.log('00:39:43,212'); seek(2383.0)">
              as part of the workflow. It sends the title and description to the user back,
            </span>
            
            <span id="chunk-661" class="transcript-chunks" onclick="console.log('00:39:46,972'); seek(2386.0)">
              and then afterwards you can then send the chosen title and description
            </span>
            
            <span id="chunk-662" class="transcript-chunks" onclick="console.log('00:39:50,598'); seek(2390.0)">
              as part of the human workflow, if needed, for the response,
            </span>
            
            <span id="chunk-663" class="transcript-chunks" onclick="console.log('00:39:54,518'); seek(2394.0)">
              for the review part. Then as part of the final
            </span>
            
            <span id="chunk-664" class="transcript-chunks" onclick="console.log('00:39:58,000'); seek(2398.0)">
              part where you have the generating the avatar,
            </span>
            
            <span id="chunk-665" class="transcript-chunks" onclick="console.log('00:40:01,654'); seek(2401.0)">
              you actually get an S three presigned URL, because that avatar
            </span>
            
            <span id="chunk-666" class="transcript-chunks" onclick="console.log('00:40:05,338'); seek(2405.0)">
              image gets created, generated, and then afterwards put in an S three bucket.
            </span>
            
            <span id="chunk-667" class="transcript-chunks" onclick="console.log('00:40:09,354'); seek(2409.0)">
              So here's a demonstration of this final architecture.
            </span>
            
            <span id="chunk-668" class="transcript-chunks" onclick="console.log('00:40:12,474'); seek(2412.0)">
              So there's a short video of an interview between Jeff Bezos and
            </span>
            
            <span id="chunk-669" class="transcript-chunks" onclick="console.log('00:40:16,376'); seek(2416.0)">
              Werner Vogels. What's going to happen is that
            </span>
            
            <span id="chunk-670" class="transcript-chunks" onclick="console.log('00:40:19,768'); seek(2419.0)">
              we want to generate a title and a description and an avatar for this video.
            </span>
            
            <span id="chunk-671" class="transcript-chunks" onclick="console.log('00:40:23,848'); seek(2423.0)">
              So there's a simple UI that you saw earlier.
            </span>
            
            <span id="chunk-672" class="transcript-chunks" onclick="console.log('00:40:27,538'); seek(2427.0)">
              This uses a websocket communication to talk to AWS
            </span>
            
            <span id="chunk-673" class="transcript-chunks" onclick="console.log('00:40:30,898'); seek(2430.0)">
              iot core service. And once you select the button, it then sends
            </span>
            
            <span id="chunk-674" class="transcript-chunks" onclick="console.log('00:40:34,802'); seek(2434.0)">
              that video's details. And then the workflow then gets executed
            </span>
            
            <span id="chunk-675" class="transcript-chunks" onclick="console.log('00:40:39,074'); seek(2439.0)">
              from the lambda function. And then you see that that step starts kicking in.
            </span>
            
            <span id="chunk-676" class="transcript-chunks" onclick="console.log('00:40:43,312'); seek(2443.0)">
              This gives a nice view of the execution. You have the color coding of the
            </span>
            
            <span id="chunk-677" class="transcript-chunks" onclick="console.log('00:40:46,528'); seek(2446.0)">
              state. And with transcribe being used initially, you get the text
            </span>
            
            <span id="chunk-678" class="transcript-chunks" onclick="console.log('00:40:50,432'); seek(2450.0)">
              back for the speech that is there in the video. And this transcribe
            </span>
            
            <span id="chunk-679" class="transcript-chunks" onclick="console.log('00:40:54,294'); seek(2454.0)">
              job is asynchronous. So there is a wait loop that is there to make sure
            </span>
            
            <span id="chunk-680" class="transcript-chunks" onclick="console.log('00:40:57,668'); seek(2457.0)">
              that we can wait for it to complete.
            </span>
            
            <span id="chunk-681" class="transcript-chunks" onclick="console.log('00:41:01,300'); seek(2461.0)">
              Right, so that's the wait loop that's there in the beginning. And once that
            </span>
            
            <span id="chunk-682" class="transcript-chunks" onclick="console.log('00:41:04,692'); seek(2464.0)">
              wait loop is done, wait loop is done. And using
            </span>
            
            <span id="chunk-683" class="transcript-chunks" onclick="console.log('00:41:08,312'); seek(2468.0)">
              the get transcription job API, we get the final response
            </span>
            
            <span id="chunk-684" class="transcript-chunks" onclick="console.log('00:41:11,950'); seek(2471.0)">
              from that transcription job. And then we read that transcript.
            </span>
            
            <span id="chunk-685" class="transcript-chunks" onclick="console.log('00:41:15,678'); seek(2475.0)">
              And that transcript is available in an S three bucket
            </span>
            
            <span id="chunk-686" class="transcript-chunks" onclick="console.log('00:41:19,058'); seek(2479.0)">
              already because transcription job will put it in over there. And then
            </span>
            
            <span id="chunk-687" class="transcript-chunks" onclick="console.log('00:41:22,716'); seek(2482.0)">
              once that is read, it is then passed down to this parallel execution.
            </span>
            
            <span id="chunk-688" class="transcript-chunks" onclick="console.log('00:41:26,978'); seek(2486.0)">
              As part of the parallel execution, we have this two calls that are being done.
            </span>
            
            <span id="chunk-689" class="transcript-chunks" onclick="console.log('00:41:31,050'); seek(2491.0)">
              One to bedrock, one of the foundational models
            </span>
            
            <span id="chunk-690" class="transcript-chunks" onclick="console.log('00:41:34,838'); seek(2494.0)">
              at bedrock, and then one to hugging face. In this case, we're just keeping it
            </span>
            
            <span id="chunk-691" class="transcript-chunks" onclick="console.log('00:41:38,208'); seek(2498.0)">
              simple. So we want to just make sure that we're able to execute this back.
            </span>
            
            <span id="chunk-692" class="transcript-chunks" onclick="console.log('00:41:42,096'); seek(2502.0)">
              And we want to then get user feedback now
            </span>
            
            <span id="chunk-693" class="transcript-chunks" onclick="console.log('00:41:45,396'); seek(2505.0)">
              quickly, just to show you what the outputs look like.
            </span>
            
            <span id="chunk-694" class="transcript-chunks" onclick="console.log('00:41:48,388'); seek(2508.0)">
              These are the inputs that are coming in. This is the transcript that's
            </span>
            
            <span id="chunk-695" class="transcript-chunks" onclick="console.log('00:41:51,802'); seek(2511.0)">
              there, the prompts. You kind of notice that the models that are being invoked is
            </span>
            
            <span id="chunk-696" class="transcript-chunks" onclick="console.log('00:41:55,608'); seek(2515.0)">
              also there as part of that. Here are the parameters,
            </span>
            
            <span id="chunk-697" class="transcript-chunks" onclick="console.log('00:41:58,950'); seek(2518.0)">
              here's the conversation that's happening with the video and the
            </span>
            
            <span id="chunk-698" class="transcript-chunks" onclick="console.log('00:42:02,808'); seek(2522.0)">
              s three bucket URL for the video and other things. And there's
            </span>
            
            <span id="chunk-699" class="transcript-chunks" onclick="console.log('00:42:06,638'); seek(2526.0)">
              a task token that's already there. This is part of the review flow that
            </span>
            
            <span id="chunk-700" class="transcript-chunks" onclick="console.log('00:42:10,588'); seek(2530.0)">
              is being invoked. So we have this task token that's being sent
            </span>
            
            <span id="chunk-701" class="transcript-chunks" onclick="console.log('00:42:14,444'); seek(2534.0)">
              to the page. And this page is basically where someone
            </span>
            
            <span id="chunk-702" class="transcript-chunks" onclick="console.log('00:42:18,156'); seek(2538.0)">
              can actually go in and then say, okay, do they want to select this title,
            </span>
            
            <span id="chunk-703" class="transcript-chunks" onclick="console.log('00:42:21,538'); seek(2541.0)">
              the first one or the second one? So we select one of the titles,
            </span>
            
            <span id="chunk-704" class="transcript-chunks" onclick="console.log('00:42:25,542'); seek(2545.0)">
              and then afterwards it goes down, and then it creates an avatar as part of
            </span>
            
            <span id="chunk-705" class="transcript-chunks" onclick="console.log('00:42:28,768'); seek(2548.0)">
              that title that's created. It sends that as part of a prompt to
            </span>
            
            <span id="chunk-706" class="transcript-chunks" onclick="console.log('00:42:34,690'); seek(2554.0)">
              one of the foundation models. So stability in this case,
            </span>
            
            <span id="chunk-707" class="transcript-chunks" onclick="console.log('00:42:38,036'); seek(2558.0)">
              and once you have that, that gets displayed over here, and that's an
            </span>
            
            <span id="chunk-708" class="transcript-chunks" onclick="console.log('00:42:41,444'); seek(2561.0)">
              avatar that is used now, for example, the team that uses it can then
            </span>
            
            <span id="chunk-709" class="transcript-chunks" onclick="console.log('00:42:44,868'); seek(2564.0)">
              copy this image and then put it in because it's already there in s three
            </span>
            
            <span id="chunk-710" class="transcript-chunks" onclick="console.log('00:42:48,248'); seek(2568.0)">
              bucket. Or probably it gets picked by another flow that then is used as part
            </span>
            
            <span id="chunk-711" class="transcript-chunks" onclick="console.log('00:42:51,912'); seek(2571.0)">
              of their content publishing pipeline. Now, to know more about how
            </span>
            
            <span id="chunk-712" class="transcript-chunks" onclick="console.log('00:42:55,688'); seek(2575.0)">
              you can build applications like this, there is a sample that's already available that has
            </span>
            
            <span id="chunk-713" class="transcript-chunks" onclick="console.log('00:42:59,468'); seek(2579.0)">
              different use cases. Also covers things like the error retries.
            </span>
            
            <span id="chunk-714" class="transcript-chunks" onclick="console.log('00:43:03,346'); seek(2583.0)">
              It covers prompt chaining and all
            </span>
            
            <span id="chunk-715" class="transcript-chunks" onclick="console.log('00:43:06,652'); seek(2586.0)">
              the other parts of creating a complex workflow with step functions for generative
            </span>
            
            <span id="chunk-716" class="transcript-chunks" onclick="console.log('00:43:10,242'); seek(2590.0)">
              AI. So have a look at this resource. A great way to do this,
            </span>
            
            <span id="chunk-717" class="transcript-chunks" onclick="console.log('00:43:13,228'); seek(2593.0)">
              and also with our blog posts that are linked as part of this resource.
            </span>
            
            <span id="chunk-718" class="transcript-chunks" onclick="console.log('00:43:17,210'); seek(2597.0)">
              So with that, I would like to thank you for attending the session and have
            </span>
            
            <span id="chunk-719" class="transcript-chunks" onclick="console.log('00:43:20,852'); seek(2600.0)">
              a good day and rest of conf 42. Thank you so much.
            </span>
            
            </div>
          </div>
          
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/Mohammed%20Fazalullah%20-%20Conf42%20Cloud%20Native%202024.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/Mohammed%20Fazalullah%20-%20Conf42%20Cloud%20Native%202024.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #7B2726;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/cloud2024" class="btn btn-sm btn-danger shadow lift" style="background-color: #7B2726;">
                <i class="fe fe-grid me-2"></i>
                See all 47 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Mohammed%20Fazalullah_cloud.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Mohammed Fazalullah
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Senior Developer Advocate @ AWS
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/mohammedfazalullah/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Mohammed Fazalullah's LinkedIn account" />
                  </a>
                  
                  
                  <a href="https://twitter.com/@_cloudranger" target="_blank">
                    <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="Mohammed Fazalullah's twitter account" />
                  </a>
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by @_cloudranger"
                  data-url="https://www.conf42.com/cloud2024"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/cloud2024"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>






    <!-- WELCOME -->
    <section class="pt-8 pt-md-11 pb-10 pb-md-15 bg-info" id="register">

      <!-- Shape -->
      <div class="shape shape-blur-3 text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>      </div>

      <!-- Content -->
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 col-md-10 col-lg-8 text-center">

            <!-- Heading -->
            <h1 class="display-2 text-white">
              Join the community!
            </h1>

            <!-- Text -->
            <p class="lead text-white text-opacity-80 mb-6 mb-md-8">
              Learn for free, join the best tech learning community 
              for a <a class="text-white" href="https://www.reddit.com/r/sanfrancisco/comments/1bz90f6/why_are_coffee_shops_in_sf_so_expensive/" target="_blank">price of a pumpkin latte</a>.
            </p>

            <!-- Form -->
            <form class="d-flex align-items-center justify-content-center mb-7 mb-md-9">

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Annual
              </span>

              <!-- Switch -->
              <div class="form-check form-check-dark form-switch mx-3">
                <input class="form-check-input" type="checkbox" id="billingSwitch" data-toggle="price" data-target=".price">
              </div>

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Monthly
              </span>

            </form>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->

    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- PRICING -->
    <section class="mt-n8 mt-md-n15">
      <div class="container">
        <div class="row gx-4">
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-1">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Newsletter</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="0" data-monthly="0">0</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Event notifications, weekly newsletter
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Delayed access</b> to all content
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Immediate access to Keynotes & Panels
                  </p>
                </div>
              
              
              </div>
            </div>

            <!-- Card -->
            <div class="card shadow-lg mb-6 border border-success">
              <div class="card-body">

                <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
</script>

<!-- Form -->
<link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
<p class="emailoctopus-success-message text-success"></p>
<p class="emailoctopus-error-message text-danger"></p>
<form
    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
    method="post"
    data-message-success="Thanks! Check your email for further directions!"
    data-message-missing-email-address="Your email address is required."
    data-message-invalid-email-address="Your email address looks incorrect, please try again."
    data-message-bot-submission-error="This doesn't look like a human submission."
    data-message-consent-required="Please check the checkbox to indicate your consent."
    data-message-invalid-parameters-error="This form has missing or invalid fields."
    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
    class="emailoctopus-form"
    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
>
<div class="form-floating emailoctopus-form-row">
    <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
    <label for="field_0">Email address</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
    <label for="field_1">First Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
    <label for="field_2">Last Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
    <label for="field_4">Company</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
    <label for="field_5">Job Title</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
    <label for="field_3">Phone Number</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
    oninput="updateCountry()"
    >
    <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
    </select>
    <label for="field_7">Country</label>
</div>
<input id="country-destination" name="field_7" type="hidden">
<input id="tz-country" name="field_8" type="hidden">

<input
    name="field_6"
    type="hidden"
    value="Cloud Native"
>

<div class="emailoctopus-form-row-consent">
    <input
    type="checkbox"
    id="consent"
    name="consent"
    >
    <label for="consent">
    I consent to the following terms:
    </label>
    <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
    Terms and Conditions
    </a>
    &amp;
    <a href="./code-of-conduct" target="_blank">
    Code of Conduct
    </a>
</div>
<div
    aria-hidden="true"
    class="emailoctopus-form-row-hp"
>
    <input
    type="text"
    name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
    tabindex="-1"
    autocomplete="nope"
    >
</div>
<div class="mt-6 emailoctopus-form-row-subscribe">
    <input
    type="hidden"
    name="successRedirectUrl"
    >
    <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
    Subscribe to free newsletter <i class="fe fe-arrow-right ms-3"></i>
    </button>
</div>
</form>

<!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
<script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

              </div>
            </div>
          </div>
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-0">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Community</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="8.34" data-monthly="10">8.34</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Access to <a href="https://conf42.circle.so/">Circle community platform</a>
                  </p>
                </div>

                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Immediate access</b> to all content
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank"><b>Live events!</b></a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank">Regular office hours, Q&As, CV reviews</a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Courses, quizes & certificates
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Community chats
                  </p>
                </div>
                

                <!-- Button -->
                <a href="https://conf42.circle.so/checkout/subscribe" class="btn w-100 btn-primary">
                  Join the community (7 day free trial)<i class="fe fe-arrow-right ms-3"></i>
                </a>

              </div>
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2025">
                  Observability 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2025">
                  Quantum Computing 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2025">
                  Rustlang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2025">
                  Platform Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2025">
                  MLOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2025">
                  Incident Management 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2025">
                  Kube Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2025">
                  JavaScript 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2025">
                  Prompt Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/robotics2025">
                  Robotics 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2025">
                  DevSecOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2025">
                  Internet of Things (IoT) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>