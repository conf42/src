<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Running an open source LLM</title>
    <meta name="description" content="One model, extra large, please!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Bongani%20Shongwe_llm.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Running an open source LLM | Conf42"/>
    <meta property="og:description" content="In this talk, we will explore the options available for harnessing LLMs, including deploying your own service or using existing LLM APIs. Attendees will learn best practices for leveraging LLMs effectively, whether by building or subscribing."/>
    <meta property="og:url" content="https://conf42.com/Large_Language_Models_LLMs_2024_Bongani_Shongwe_opensource_llm_running"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/KUBE2024_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Kube Native 2024
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2024-09-26
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/kubenative2024" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.sreday.com/">
                            SREday London
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.sreday.com/">
                            SREday Amsterdam
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="./support">
                            Support us
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Subscribe for FREE
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #CCB87B;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Large Language Models (LLMs) 2024 - Online
            </h1>

            <h2 class="text-white">
              
              Content unlocked! Welcome to the community!
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- One model, extra large, please!
 -->
              <script>
                const event_date = new Date("2024-04-11T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2024-04-11T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "XFSdVb479T4"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <a class="btn btn-info lift mb-3" data-bigpicture='{"ytSrc": "TQwxk0c4sh0"}' href="#">
                <i class="fe fe-eye me-2"></i>
                Watch Premiere
              </a>
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrBjR6ZR0g0LRq9Fp8c_4HrI" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Hello, I\u0027m Bongwani Shangwe and I\u0027m here to present to you", "timestamp": "00:00:20,760", "timestamp_s": 20.0}, {"text": "about running an open source large learning model on your own infrastructure.", "timestamp": "00:00:24,262", "timestamp_s": 24.0}, {"text": "I first want to kick off with a short introduction about the company I", "timestamp": "00:00:28,514", "timestamp_s": 28.0}, {"text": "work for at Aventa and what is it we do? Because a lot", "timestamp": "00:00:31,578", "timestamp_s": 31.0}, {"text": "of people might know about our secondhand on seller", "timestamp": "00:00:35,346", "timestamp_s": 35.0}, {"text": "marketplaces, but they do not know much about the", "timestamp": "00:00:38,970", "timestamp_s": 38.0}, {"text": "brand adventure behind them. Adavinte is", "timestamp": "00:00:42,330", "timestamp_s": 42.0}, {"text": "one of the world\u0027s leading online classified specialists with market", "timestamp": "00:00:45,690", "timestamp_s": 45.0}, {"text": "across three continents containing over 25", "timestamp": "00:00:49,594", "timestamp_s": 49.0}, {"text": "marketplace brands. Our marketplaces range from", "timestamp": "00:00:53,674", "timestamp_s": 53.0}, {"text": "consumer goods, vehicles, real estate and", "timestamp": "00:00:57,714", "timestamp_s": 57.0}, {"text": "jobs. Adventure consists of several marketplace", "timestamp": "00:01:01,114", "timestamp_s": 61.0}, {"text": "brands like Gleanzeigen in Germany, Mark Blatz in the", "timestamp": "00:01:04,490", "timestamp_s": 64.0}, {"text": "Netherlands, Lebencoin in France, Kijiji in Canada.", "timestamp": "00:01:07,850", "timestamp_s": 67.0}, {"text": "Adavinte is a champion for sustainable commerce,", "timestamp": "00:01:12,234", "timestamp_s": 72.0}, {"text": "making a positive impact on the environment, the economy and society", "timestamp": "00:01:15,178", "timestamp_s": 75.0}, {"text": "by the numbers we get about 2.5", "timestamp": "00:01:20,894", "timestamp_s": 80.0}, {"text": "billion monthly visits across our website.", "timestamp": "00:01:25,022", "timestamp_s": 85.0}, {"text": "We have 25 plus marketplaces in our digital portfolio", "timestamp": "00:01:28,974", "timestamp_s": 88.0}, {"text": "and over 5700 employees", "timestamp": "00:01:33,182", "timestamp_s": 93.0}, {"text": "across ten continents. Now at Delavinto,", "timestamp": "00:01:36,510", "timestamp_s": 96.0}, {"text": "we\u0027ve been working on conversational search assistant which is", "timestamp": "00:01:39,990", "timestamp_s": 99.0}, {"text": "geared to launch on Levin Quan, France for a b testing", "timestamp": "00:01:43,182", "timestamp_s": 103.0}, {"text": "during the second quarter of this year.", "timestamp": "00:01:46,934", "timestamp_s": 106.0}, {"text": "Conversational search is about building a smarter sharpening experience by allowing", "timestamp": "00:01:50,154", "timestamp_s": 110.0}, {"text": "users to ask arbitrary questions and being guided to relevant", "timestamp": "00:01:54,146", "timestamp_s": 114.0}, {"text": "recommendations and search results by an assistant in", "timestamp": "00:01:57,770", "timestamp_s": 117.0}, {"text": "order to serve a greater user experience.", "timestamp": "00:02:01,650", "timestamp_s": 121.0}, {"text": "Outside of the normal chat bots which are in use by", "timestamp": "00:02:05,082", "timestamp_s": 125.0}, {"text": "most websites, the conversational search assistance will", "timestamp": "00:02:08,834", "timestamp_s": 128.0}, {"text": "be backed by a large learning model service.", "timestamp": "00:02:12,338", "timestamp_s": 132.0}, {"text": "Conversational search can be defined as a chat interface", "timestamp": "00:02:15,494", "timestamp_s": 135.0}, {"text": "to enhance the user experience by allowing natural language interactions", "timestamp": "00:02:19,630", "timestamp_s": 139.0}, {"text": "with software agents or virtual assistants to retrieve information.", "timestamp": "00:02:23,350", "timestamp_s": 143.0}, {"text": "The product we envisioned is kind of like this.", "timestamp": "00:02:28,654", "timestamp_s": 148.0}, {"text": "This is an example I drew up and", "timestamp": "00:02:32,726", "timestamp_s": 152.0}, {"text": "with the conversational assistant, when it pops up or when you interact with it,", "timestamp": "00:02:37,262", "timestamp_s": 157.0}, {"text": "you ask a general question. In this case, this user is looking for", "timestamp": "00:02:41,854", "timestamp_s": 161.0}, {"text": "a Ford Focus or Fiesta and it\u0027s", "timestamp": "00:02:45,574", "timestamp_s": 165.0}, {"text": "for the conversational search assistance to assist", "timestamp": "00:02:49,110", "timestamp_s": 169.0}, {"text": "the user in narrowing down the search products and", "timestamp": "00:02:52,926", "timestamp_s": 172.0}, {"text": "also asking for more preferences and what the user would like", "timestamp": "00:02:56,326", "timestamp_s": 176.0}, {"text": "is specific. In this vehicle he\u0027s looking for", "timestamp": "00:02:59,710", "timestamp_s": 179.0}, {"text": "basic infrastructure overview of how it works.", "timestamp": "00:03:04,534", "timestamp_s": 184.0}, {"text": "The user would get of course", "timestamp": "00:03:09,514", "timestamp_s": 189.0}, {"text": "interact with the conversational search assistant", "timestamp": "00:03:13,082", "timestamp_s": 193.0}, {"text": "and which would in the background generate", "timestamp": "00:03:17,418", "timestamp_s": 197.0}, {"text": "a query to call a conversational large learning model.", "timestamp": "00:03:20,602", "timestamp_s": 200.0}, {"text": "The large learning model also gathers the history of", "timestamp": "00:03:23,978", "timestamp_s": 203.0}, {"text": "the conversation which has been happening and it extracts", "timestamp": "00:03:27,290", "timestamp_s": 207.0}, {"text": "that information in order to consolidate it and", "timestamp": "00:03:31,498", "timestamp_s": 211.0}, {"text": "push the information to a search API", "timestamp": "00:03:36,824", "timestamp_s": 216.0}, {"text": "to get relevant search results.", "timestamp": "00:03:39,952", "timestamp_s": 219.0}, {"text": "Everything here the conversation large render model and extraction large", "timestamp": "00:03:45,424", "timestamp_s": 225.0}, {"text": "learning model is backwards GPT 3.5", "timestamp": "00:03:49,296", "timestamp_s": 229.0}, {"text": "outside from OpenAI, the team has also been looking", "timestamp": "00:03:54,504", "timestamp_s": 234.0}, {"text": "at other providers for large learning model APIs.", "timestamp": "00:03:57,832", "timestamp_s": 237.0}, {"text": "However, we found that there are some downsides in having to", "timestamp": "00:04:02,214", "timestamp_s": 242.0}, {"text": "use a service provided largely model. Some points", "timestamp": "00:04:05,566", "timestamp_s": 245.0}, {"text": "are the readiness of the service as this is still a new", "timestamp": "00:04:09,518", "timestamp_s": 249.0}, {"text": "field and some providers are slow to", "timestamp": "00:04:13,150", "timestamp_s": 253.0}, {"text": "open up to more customers to a", "timestamp": "00:04:16,382", "timestamp_s": 256.0}, {"text": "larger scale. Thus it takes quite a while to get onboarded", "timestamp": "00:04:19,518", "timestamp_s": 259.0}, {"text": "onto these services. There\u0027s the cost factor.", "timestamp": "00:04:23,182", "timestamp_s": 263.0}, {"text": "Of course, this is a new additional cost for the team and", "timestamp": "00:04:27,824", "timestamp_s": 267.0}, {"text": "the company and we also had to think about", "timestamp": "00:04:30,976", "timestamp_s": 270.0}, {"text": "latency given that the services outside of adventure infrastructure,", "timestamp": "00:04:34,560", "timestamp_s": 274.0}, {"text": "there\u0027s an additional latency we have to account", "timestamp": "00:04:39,040", "timestamp_s": 279.0}, {"text": "for. Given some of these factors,", "timestamp": "00:04:42,320", "timestamp_s": 282.0}, {"text": "we decided to investigate the use of open large learning models", "timestamp": "00:04:45,464", "timestamp_s": 285.0}, {"text": "which we could possibly deploy into our own infrastructure.", "timestamp": "00:04:49,488", "timestamp_s": 289.0}, {"text": "So during the proof of concept phase where", "timestamp": "00:04:53,814", "timestamp_s": 293.0}, {"text": "we\u0027re using paid go to services, we also started exploring", "timestamp": "00:04:58,070", "timestamp_s": 298.0}, {"text": "looking at hosting a large learning model with", "timestamp": "00:05:02,974", "timestamp_s": 302.0}, {"text": "an enterprise service.", "timestamp": "00:05:07,366", "timestamp_s": 307.0}, {"text": "You get the top, best quality and the latest", "timestamp": "00:05:12,654", "timestamp_s": 312.0}, {"text": "large learning models which are being produced", "timestamp": "00:05:16,374", "timestamp_s": 316.0}, {"text": "and versus for", "timestamp": "00:05:20,724", "timestamp_s": 320.0}, {"text": "us. When you do a self hosted solution,", "timestamp": "00:05:24,724", "timestamp_s": 324.0}, {"text": "the benefit is that you complete control of your application", "timestamp": "00:05:29,324", "timestamp_s": 329.0}, {"text": "and your team is responsible for the system", "timestamp": "00:05:33,092", "timestamp_s": 333.0}, {"text": "versus using an external API. You\u0027re dependent on that", "timestamp": "00:05:37,164", "timestamp_s": 337.0}, {"text": "other system being up all the time for your service to run.", "timestamp": "00:05:40,972", "timestamp_s": 340.0}, {"text": "Some benefits in using", "timestamp": "00:05:45,704", "timestamp_s": 345.0}, {"text": "your own hosted large learning model is that you have greater privacy", "timestamp": "00:05:52,224", "timestamp_s": 352.0}, {"text": "and compliance and you also avoid vendor lock", "timestamp": "00:05:56,400", "timestamp_s": 356.0}, {"text": "in. We started exploring", "timestamp": "00:06:00,120", "timestamp_s": 360.0}, {"text": "models to use by going to", "timestamp": "00:06:04,120", "timestamp_s": 364.0}, {"text": "hugging face. Hugging face is currently the main platform", "timestamp": "00:06:07,720", "timestamp_s": 367.0}, {"text": "or a website for building and using machine learning based models", "timestamp": "00:06:11,514", "timestamp_s": 371.0}, {"text": "such as large render models. It also provides a platform to run these", "timestamp": "00:06:15,498", "timestamp_s": 375.0}, {"text": "models on a smaller scale. In our case, we considered", "timestamp": "00:06:19,266", "timestamp_s": 379.0}, {"text": "text generation based models. We first started", "timestamp": "00:06:23,434", "timestamp_s": 383.0}, {"text": "off with the Falcon 7 billion meter perimeter tuned", "timestamp": "00:06:27,018", "timestamp_s": 387.0}, {"text": "model to get familiar with deploying a large learning model.", "timestamp": "00:06:30,986", "timestamp_s": 390.0}, {"text": "It\u0027s a lightweight model and it\u0027s", "timestamp": "00:06:36,194", "timestamp_s": 396.0}, {"text": "quite easy to get started and set up. Though it is lightweight,", "timestamp": "00:06:40,780", "timestamp_s": 400.0}, {"text": "we did find at lack depth when answering", "timestamp": "00:06:45,284", "timestamp_s": 405.0}, {"text": "specific questions,", "timestamp": "00:06:49,196", "timestamp_s": 409.0}, {"text": "specifically when we\u0027re looking at to use it for as a conversational", "timestamp": "00:06:51,844", "timestamp_s": 411.0}, {"text": "search assistance. So then we started looking at other models", "timestamp": "00:06:55,860", "timestamp_s": 415.0}, {"text": "which were out there which we could use. We looked at the Falcon", "timestamp": "00:06:59,964", "timestamp_s": 419.0}, {"text": "40 millimeter chat tune model and the Loma 270", "timestamp": "00:07:03,548", "timestamp_s": 423.0}, {"text": "billion chat tune model.", "timestamp": "00:07:07,488", "timestamp_s": 427.0}, {"text": "Aside from being chat tuned, these models also provide multi", "timestamp": "00:07:11,024", "timestamp_s": 431.0}, {"text": "language support which was a requirement for us as marketplaces", "timestamp": "00:07:14,432", "timestamp_s": 434.0}, {"text": "across different countries with different language customer", "timestamp": "00:07:18,960", "timestamp_s": 438.0}, {"text": "customers who speak different languages. So on", "timestamp": "00:07:23,264", "timestamp_s": 443.0}, {"text": "deploying a model or hosting the large learner model. In this case,", "timestamp": "00:07:29,992", "timestamp_s": 449.0}, {"text": "we found text generation interface or TGI for", "timestamp": "00:07:34,754", "timestamp_s": 454.0}, {"text": "short. TGI is a fast optimized interface", "timestamp": "00:07:39,530", "timestamp_s": 459.0}, {"text": "solution built for deploying and serving large learning models.", "timestamp": "00:07:43,354", "timestamp_s": 463.0}, {"text": "TGI enables high performance text generation", "timestamp": "00:07:47,234", "timestamp_s": 467.0}, {"text": "using tensor parallelism, dynamic batching", "timestamp": "00:07:51,074", "timestamp_s": 471.0}, {"text": "for most popular and dynamic batch for most popular", "timestamp": "00:07:55,322", "timestamp_s": 475.0}, {"text": "open source larger models. TGI also has", "timestamp": "00:07:59,402", "timestamp_s": 479.0}, {"text": "a docker image which can be used to launch the text generation", "timestamp": "00:08:03,672", "timestamp_s": 483.0}, {"text": "service. One of some of", "timestamp": "00:08:07,320", "timestamp_s": 487.0}, {"text": "the benefits we found using TGI is that it\u0027s", "timestamp": "00:08:10,656", "timestamp_s": 490.0}, {"text": "a simple launcher service to host your model.", "timestamp": "00:08:13,800", "timestamp_s": 493.0}, {"text": "It\u0027s production ready as it provides tracing", "timestamp": "00:08:17,904", "timestamp_s": 497.0}, {"text": "with open telemetry and Prometheus, this token", "timestamp": "00:08:22,424", "timestamp_s": 502.0}, {"text": "streaming, you can have continuous batching", "timestamp": "00:08:26,592", "timestamp_s": 506.0}, {"text": "of incoming requests for increased total throughput. There\u0027s quantitization", "timestamp": "00:08:30,188", "timestamp_s": 510.0}, {"text": "with bits and bytes, stop sequences,", "timestamp": "00:08:34,868", "timestamp_s": 514.0}, {"text": "and you can have custom prompt generation,", "timestamp": "00:08:38,380", "timestamp_s": 518.0}, {"text": "and it also provides fine tuning support to", "timestamp": "00:08:43,244", "timestamp_s": 523.0}, {"text": "fine tune your models. The easiest way to get", "timestamp": "00:08:47,340", "timestamp_s": 527.0}, {"text": "started with the text generation interface", "timestamp": "00:08:51,132", "timestamp_s": 531.0}, {"text": "tick generation inference is to run", "timestamp": "00:08:57,204", "timestamp_s": 537.0}, {"text": "the simple command line, which is", "timestamp": "00:09:01,276", "timestamp_s": 541.0}, {"text": "you just point to the model which you\u0027d like to use.", "timestamp": "00:09:06,324", "timestamp_s": 546.0}, {"text": "You set up where to store the model and", "timestamp": "00:09:10,204", "timestamp_s": 550.0}, {"text": "all the dots, such as the model weights. And when you", "timestamp": "00:09:14,372", "timestamp_s": 554.0}, {"text": "run this, it\u0027s a docker command. It will launch the docker,", "timestamp": "00:09:17,860", "timestamp_s": 557.0}, {"text": "the TGI docker, and if", "timestamp": "00:09:22,052", "timestamp_s": 562.0}, {"text": "your model is not present on your machine, it will download it.", "timestamp": "00:09:27,280", "timestamp_s": 567.0}, {"text": "If it is present, it will just start. Yeah, it will start", "timestamp": "00:09:31,064", "timestamp_s": 571.0}, {"text": "running. And to get going you", "timestamp": "00:09:35,024", "timestamp_s": 575.0}, {"text": "can just, to test it out, you can use a simple curl", "timestamp": "00:09:38,408", "timestamp_s": 578.0}, {"text": "command and as you can see,", "timestamp": "00:09:42,064", "timestamp_s": 582.0}, {"text": "you submit the JSON and you get a response from the model.", "timestamp": "00:09:45,624", "timestamp_s": 585.0}, {"text": "Now our team based our", "timestamp": "00:09:50,124", "timestamp_s": 590.0}, {"text": "experimentations on GCP. Initially we", "timestamp": "00:09:53,852", "timestamp_s": 593.0}, {"text": "tried to run the deployments on our european region of GCP.", "timestamp": "00:09:57,564", "timestamp_s": 597.0}, {"text": "However, it appears that with the rise of GPU", "timestamp": "00:10:02,188", "timestamp_s": 602.0}, {"text": "use applications such as large rental models, there\u0027s a scarcity", "timestamp": "00:10:05,676", "timestamp_s": 605.0}, {"text": "of GPU variability and of GPU availability", "timestamp": "00:10:09,844", "timestamp_s": 609.0}, {"text": "in Europe and north american regions. Actually, if you", "timestamp": "00:10:14,612", "timestamp_s": 614.0}, {"text": "research this more, it seems to be occurring", "timestamp": "00:10:18,540", "timestamp_s": 618.0}, {"text": "across all the cloud providers because of", "timestamp": "00:10:22,548", "timestamp_s": 622.0}, {"text": "the current popularity of just GPU based applications.", "timestamp": "00:10:26,500", "timestamp_s": 626.0}, {"text": "So outside of, we scanned outside of Europe and North", "timestamp": "00:10:30,964", "timestamp_s": 630.0}, {"text": "America, and we found that there were two zones that could provide", "timestamp": "00:10:34,524", "timestamp_s": 634.0}, {"text": "GPU\u0027s, which we could gets in", "timestamp": "00:10:40,324", "timestamp_s": 640.0}, {"text": "for spot instances, and those were in", "timestamp": "00:10:44,408", "timestamp_s": 644.0}, {"text": "East Asia and also", "timestamp": "00:10:48,192", "timestamp_s": 648.0}, {"text": "in the Middle east one.", "timestamp": "00:10:52,584", "timestamp_s": 652.0}, {"text": "To be specific, large training models require high", "timestamp": "00:10:56,232", "timestamp_s": 656.0}, {"text": "GPU to train for inference. It can be less,", "timestamp": "00:11:00,344", "timestamp_s": 660.0}, {"text": "but it\u0027s still generating a large amount", "timestamp": "00:11:04,248", "timestamp_s": 664.0}, {"text": "of GPU memory required. The higher parameters", "timestamp": "00:11:07,664", "timestamp_s": 667.0}, {"text": "the model contains, the more GPU memory required to run the inference", "timestamp": "00:11:11,432", "timestamp_s": 671.0}, {"text": "of the model. There\u0027s also the possibility", "timestamp": "00:11:15,584", "timestamp_s": 675.0}, {"text": "to run the model inference in eight bits and four bits mode,", "timestamp": "00:11:19,512", "timestamp_s": 679.0}, {"text": "which decreases the amount of GPU memory required based", "timestamp": "00:11:23,232", "timestamp_s": 683.0}, {"text": "on current GPU availability in cloud environments, the best options", "timestamp": "00:11:27,608", "timestamp_s": 687.0}, {"text": "are machines that contain Nvidia a 180gb.", "timestamp": "00:11:31,048", "timestamp_s": 691.0}, {"text": "However, these gpu\u0027s are extremely scarce, even in the cloud", "timestamp": "00:11:35,976", "timestamp_s": 695.0}, {"text": "environment. We had then settled for", "timestamp": "00:11:39,978", "timestamp_s": 699.0}, {"text": "using machines that contain a Nvidia a", "timestamp": "00:11:43,658", "timestamp_s": 703.0}, {"text": "140gb. It is also possible to", "timestamp": "00:11:47,210", "timestamp_s": 707.0}, {"text": "run a model on multiple gpu\u0027s in order to meet the requirement", "timestamp": "00:11:50,842", "timestamp_s": 710.0}, {"text": "memory requirements of the model inference.", "timestamp": "00:11:54,290", "timestamp_s": 714.0}, {"text": "So there\u0027s two ways we could have", "timestamp": "00:11:57,834", "timestamp_s": 717.0}, {"text": "deployed these models", "timestamp": "00:12:01,274", "timestamp_s": 721.0}, {"text": "and TGI inference.", "timestamp": "00:12:04,594", "timestamp_s": 724.0}, {"text": "The first one was using Kubernetes or running", "timestamp": "00:12:08,874", "timestamp_s": 728.0}, {"text": "it on a virtual machine. For doing on kubernetes,", "timestamp": "00:12:12,770", "timestamp_s": 732.0}, {"text": "we just created a simple deployment yaml", "timestamp": "00:12:17,274", "timestamp_s": 737.0}, {"text": "and we allowed for autoscaling because we didn\u0027t want", "timestamp": "00:12:21,098", "timestamp_s": 741.0}, {"text": "to keep track of all the machines. So this", "timestamp": "00:12:24,530", "timestamp_s": 744.0}, {"text": "seemed a bit better because when", "timestamp": "00:12:29,186", "timestamp_s": 749.0}, {"text": "the models are not in use, it\u0027s easy to downscale as", "timestamp": "00:12:32,428", "timestamp_s": 752.0}, {"text": "versus for if we\u0027re going to do it using", "timestamp": "00:12:36,532", "timestamp_s": 756.0}, {"text": "a virtual machine. These were self managed machines", "timestamp": "00:12:39,852", "timestamp_s": 759.0}, {"text": "and we\u0027d have to keep track if on the capacity.", "timestamp": "00:12:43,260", "timestamp_s": 763.0}, {"text": "Basically, if machines were not being used, then we\u0027d have to shut them off ourselves.", "timestamp": "00:12:48,444", "timestamp_s": 768.0}, {"text": "So we went into the because of this, we specifically started looking", "timestamp": "00:12:53,364", "timestamp_s": 773.0}, {"text": "with doing on Kubernetes, on GKE.", "timestamp": "00:12:57,644", "timestamp_s": 777.0}, {"text": "This is kind of a high level overview of GKE setup.", "timestamp": "00:13:02,404", "timestamp_s": 782.0}, {"text": "You\u0027d have the text generation inference deployment", "timestamp": "00:13:07,284", "timestamp_s": 787.0}, {"text": "running in a docker image, pointing to", "timestamp": "00:13:11,636", "timestamp_s": 791.0}, {"text": "a volume where all the weights", "timestamp": "00:13:16,964", "timestamp_s": 796.0}, {"text": "were fetched from gcs.", "timestamp": "00:13:20,852", "timestamp_s": 800.0}, {"text": "But we actually came into an issue when we", "timestamp": "00:13:23,284", "timestamp_s": 803.0}, {"text": "started using GKe as", "timestamp": "00:13:27,588", "timestamp_s": 807.0}, {"text": "we started, of course, we started with the", "timestamp": "00:13:31,564", "timestamp_s": 811.0}, {"text": "Falcon seven, with the lightest Falcon model.", "timestamp": "00:13:35,092", "timestamp_s": 815.0}, {"text": "Even with that, we found that the deployment time took longer than expected.", "timestamp": "00:13:39,084", "timestamp_s": 819.0}, {"text": "GK GPU machines were also not really available", "timestamp": "00:13:44,644", "timestamp_s": 824.0}, {"text": "for use even with kubernetes. And the", "timestamp": "00:13:48,356", "timestamp_s": 828.0}, {"text": "highest or the highest", "timestamp": "00:13:52,668", "timestamp_s": 832.0}, {"text": "GPU machine, or in this case", "timestamp": "00:13:56,774", "timestamp_s": 836.0}, {"text": "node, GPU node which could be hosted with.", "timestamp": "00:14:00,774", "timestamp_s": 840.0}, {"text": "We were able to get with GKE was a twelve gigabyte", "timestamp": "00:14:04,822", "timestamp_s": 844.0}, {"text": "GPU, which was not enough to push on with", "timestamp": "00:14:09,014", "timestamp_s": 849.0}, {"text": "other large rental models. So given", "timestamp": "00:14:12,974", "timestamp_s": 852.0}, {"text": "that, we went back and said", "timestamp": "00:14:16,886", "timestamp_s": 856.0}, {"text": "well, we\u0027re going to do the virtual machine deployment strategy.", "timestamp": "00:14:20,694", "timestamp_s": 860.0}, {"text": "And with detection inference, it allows", "timestamp": "00:14:24,814", "timestamp_s": 864.0}, {"text": "you to host the model on a", "timestamp": "00:14:28,534", "timestamp_s": 868.0}, {"text": "vm with a single GPU, or parallelized", "timestamp": "00:14:32,830", "timestamp_s": 872.0}, {"text": "inference across multiple gpu\u0027s.", "timestamp": "00:14:36,806", "timestamp_s": 876.0}, {"text": "One of the benefits of parallelizing it across multiple gpu\u0027s", "timestamp": "00:14:41,134", "timestamp_s": 881.0}, {"text": "is that you can meet using multiple gpu\u0027s.", "timestamp": "00:14:44,942", "timestamp_s": 884.0}, {"text": "You can actually get have", "timestamp": "00:14:48,822", "timestamp_s": 888.0}, {"text": "more higher memory across all the GPU\u0027s for inference.", "timestamp": "00:14:52,556", "timestamp_s": 892.0}, {"text": "We started looking at running our", "timestamp": "00:14:57,884", "timestamp_s": 897.0}, {"text": "experiments and we set up our experiments.", "timestamp": "00:15:01,396", "timestamp_s": 901.0}, {"text": "When we deployed the models in a virtual machine,", "timestamp": "00:15:05,524", "timestamp_s": 905.0}, {"text": "we set up notebooks to run different experiments", "timestamp": "00:15:09,932", "timestamp_s": 909.0}, {"text": "so we can be able to track the data and compare the results", "timestamp": "00:15:14,300", "timestamp_s": 914.0}, {"text": "later on.", "timestamp": "00:15:18,332", "timestamp_s": 918.0}, {"text": "Yeah, so we looked at,", "timestamp": "00:15:20,964", "timestamp_s": 920.0}, {"text": "so we had different cases where we deployed the model.", "timestamp": "00:15:24,044", "timestamp_s": 924.0}, {"text": "The models on a single v, of a VM with", "timestamp": "00:15:28,372", "timestamp_s": 928.0}, {"text": "a single GPU and a VM with multiple gpu\u0027s.", "timestamp": "00:15:31,700", "timestamp_s": 931.0}, {"text": "Yeah, we had, for the single vm,", "timestamp": "00:15:35,884", "timestamp_s": 935.0}, {"text": "we had a short response latency, which was really perfect", "timestamp": "00:15:38,956", "timestamp_s": 938.0}, {"text": "for us. The deployment was quite quick,", "timestamp": "00:15:42,556", "timestamp_s": 942.0}, {"text": "but the number of max tokens which could be processed", "timestamp": "00:15:46,254", "timestamp_s": 946.0}, {"text": "was limited to the GPU memory. With a vm", "timestamp": "00:15:49,942", "timestamp_s": 949.0}, {"text": "with multiple gpu\u0027s, we increased the GPU", "timestamp": "00:15:53,862", "timestamp_s": 953.0}, {"text": "memory footprint so we had more GPU memory.", "timestamp": "00:15:58,102", "timestamp_s": 958.0}, {"text": "And this in turn increased the Max token processing.", "timestamp": "00:16:02,574", "timestamp_s": 962.0}, {"text": "The time it took for the model to be ready increased. So the deployment time", "timestamp": "00:16:06,774", "timestamp_s": 966.0}, {"text": "also increased and the response latency also", "timestamp": "00:16:10,334", "timestamp_s": 970.0}, {"text": "increased. There were several", "timestamp": "00:16:13,576", "timestamp_s": 973.0}, {"text": "factors which we looked into why latency?", "timestamp": "00:16:17,552", "timestamp_s": 977.0}, {"text": "The factors which came into latency with", "timestamp": "00:16:22,448", "timestamp_s": 982.0}, {"text": "this and some of the during our", "timestamp": "00:16:26,552", "timestamp_s": 986.0}, {"text": "experience, we found out that it", "timestamp": "00:16:30,256", "timestamp_s": 990.0}, {"text": "was due to prompt size and complexity,", "timestamp": "00:16:33,968", "timestamp_s": 993.0}, {"text": "so similar to if the", "timestamp": "00:16:37,424", "timestamp_s": 997.0}, {"text": "prompt was long and quite complex,", "timestamp": "00:16:41,064", "timestamp_s": 1001.0}, {"text": "it will take longer for any", "timestamp": "00:16:44,824", "timestamp_s": 1004.0}, {"text": "of the models to process.", "timestamp": "00:16:48,872", "timestamp_s": 1008.0}, {"text": "However, the less indirect and short prompt it is,", "timestamp": "00:16:51,424", "timestamp_s": 1011.0}, {"text": "it opens up the ability for side effects such as hallucinations,", "timestamp": "00:16:55,280", "timestamp_s": 1015.0}, {"text": "specifically with latency. With GPU", "timestamp": "00:17:01,184", "timestamp_s": 1021.0}, {"text": "setup, we tracked this down that", "timestamp": "00:17:04,944", "timestamp_s": 1024.0}, {"text": "using more GPU\u0027s across vmware,", "timestamp": "00:17:09,201", "timestamp_s": 1029.0}, {"text": "more GPU\u0027s increased latency.", "timestamp": "00:17:14,473", "timestamp_s": 1034.0}, {"text": "We figured out that this could be between that there\u0027s a", "timestamp": "00:17:18,673", "timestamp_s": 1038.0}, {"text": "lot of IO happening between the GPU\u0027s,", "timestamp": "00:17:22,137", "timestamp_s": 1042.0}, {"text": "so there\u0027s a lot of dot exchange happening versus when you", "timestamp": "00:17:26,849", "timestamp_s": 1046.0}, {"text": "do an inference on a single GPU machine.", "timestamp": "00:17:30,977", "timestamp_s": 1050.0}, {"text": "And another thing which we tracked", "timestamp": "00:17:34,404", "timestamp_s": 1054.0}, {"text": "with regards to latency is the max token", "timestamp": "00:17:38,564", "timestamp_s": 1058.0}, {"text": "output. So we were able to set the max", "timestamp": "00:17:41,756", "timestamp_s": 1061.0}, {"text": "token output of the deployment,", "timestamp": "00:17:45,764", "timestamp_s": 1065.0}, {"text": "and the higher token output length, the response from the model", "timestamp": "00:17:49,132", "timestamp_s": 1069.0}, {"text": "becomes more detailed and longer, which requires more processing time.", "timestamp": "00:17:52,412", "timestamp_s": 1072.0}, {"text": "But if the token length is extremely short,", "timestamp": "00:17:57,284", "timestamp_s": 1077.0}, {"text": "the response time might not be complete or make sense to a human.", "timestamp": "00:18:00,654", "timestamp_s": 1080.0}, {"text": "Thus, there has to be a balance between how detailed the model response", "timestamp": "00:18:04,974", "timestamp_s": 1084.0}, {"text": "is and the quickness of the response, and more", "timestamp": "00:18:09,022", "timestamp_s": 1089.0}, {"text": "detailed analysis on this.", "timestamp": "00:18:13,326", "timestamp_s": 1093.0}, {"text": "As I mentioned, it is possible to set the number of tokens used", "timestamp": "00:18:17,694", "timestamp_s": 1097.0}, {"text": "for the output. All the previous", "timestamp": "00:18:21,214", "timestamp_s": 1101.0}, {"text": "tests we\u0027ve done, we set the token outputs", "timestamp": "00:18:24,358", "timestamp_s": 1104.0}, {"text": "to 256, and once", "timestamp": "00:18:27,590", "timestamp_s": 1107.0}, {"text": "we doubled it, we were able to observe different", "timestamp": "00:18:30,762", "timestamp_s": 1110.0}, {"text": "effects from the models tested.", "timestamp": "00:18:34,458", "timestamp_s": 1114.0}, {"text": "So for the Falcon model with 256 tokens,", "timestamp": "00:18:38,154", "timestamp_s": 1118.0}, {"text": "maximum tokens, the model seemed to perform adequately. Once we", "timestamp": "00:18:42,290", "timestamp_s": 1122.0}, {"text": "increased the number of tokens, the response time initially", "timestamp": "00:18:45,970", "timestamp_s": 1125.0}, {"text": "seemed to be the same as before, but as the conversation", "timestamp": "00:18:49,138", "timestamp_s": 1129.0}, {"text": "continued with the user, the response time began to increase.", "timestamp": "00:18:52,778", "timestamp_s": 1132.0}, {"text": "Another effect was that the response became longer and", "timestamp": "00:18:56,724", "timestamp_s": 1136.0}, {"text": "the model started to hallucinate,", "timestamp": "00:19:00,548", "timestamp_s": 1140.0}, {"text": "appearing to have a conversation with itself.", "timestamp": "00:19:03,300", "timestamp_s": 1143.0}, {"text": "Llama two, we also again", "timestamp": "00:19:07,764", "timestamp_s": 1147.0}, {"text": "started with 256 maximum tokens and the models performed", "timestamp": "00:19:11,220", "timestamp_s": 1151.0}, {"text": "adequately. But we\u0027d noted some instances that detect response", "timestamp": "00:19:15,180", "timestamp_s": 1155.0}, {"text": "would appear to cut off and with an increase in the", "timestamp": "00:19:19,460", "timestamp_s": 1159.0}, {"text": "token size, delicacy also increased quite", "timestamp": "00:19:23,220", "timestamp_s": 1163.0}, {"text": "extremely and the response was more complete.", "timestamp": "00:19:27,192", "timestamp_s": 1167.0}, {"text": "But as the conversation would continue, we get", "timestamp": "00:19:31,008", "timestamp_s": 1171.0}, {"text": "some instances where the response would get cut off again.", "timestamp": "00:19:34,904", "timestamp_s": 1174.0}, {"text": "As mentioned again, we have marketplaces", "timestamp": "00:19:40,544", "timestamp_s": 1180.0}, {"text": "across different countries or different languages, so it", "timestamp": "00:19:44,520", "timestamp_s": 1184.0}, {"text": "is important for us to test how this performs, and since we\u0027re", "timestamp": "00:19:48,368", "timestamp_s": 1188.0}, {"text": "launching first in Devonkwan, we looked at", "timestamp": "00:19:52,240", "timestamp_s": 1192.0}, {"text": "using french. So we tested the Falcon and Loma", "timestamp": "00:19:55,926", "timestamp_s": 1195.0}, {"text": "two on its ability to do inference with", "timestamp": "00:19:59,854", "timestamp_s": 1199.0}, {"text": "french users. The main adjustment", "timestamp": "00:20:03,294", "timestamp_s": 1203.0}, {"text": "was with the system prompt, directing the model to respond only", "timestamp": "00:20:07,070", "timestamp_s": 1207.0}, {"text": "in French.", "timestamp": "00:20:11,334", "timestamp_s": 1211.0}, {"text": "For Falcon, if the users question was in English", "timestamp": "00:20:14,054", "timestamp_s": 1214.0}, {"text": "and the model proceeded to respond", "timestamp": "00:20:18,038", "timestamp_s": 1218.0}, {"text": "in English, but if the user asked questions", "timestamp": "00:20:22,674", "timestamp_s": 1222.0}, {"text": "in French, the model responded in French, and if the", "timestamp": "00:20:26,250", "timestamp_s": 1226.0}, {"text": "preceding user questions was in English, the model remained", "timestamp": "00:20:30,098", "timestamp_s": 1230.0}, {"text": "only speaking in French, and if all", "timestamp": "00:20:33,338", "timestamp_s": 1233.0}, {"text": "the user questions were in French, the model start to", "timestamp": "00:20:36,898", "timestamp_s": 1236.0}, {"text": "respond to French. However, there were some side effects we", "timestamp": "00:20:40,466", "timestamp_s": 1240.0}, {"text": "observed and some hallucinations", "timestamp": "00:20:43,826", "timestamp_s": 1243.0}, {"text": "within the model when using the french language.", "timestamp": "00:20:48,674", "timestamp_s": 1248.0}, {"text": "In the case of lama two, if the user", "timestamp": "00:20:52,654", "timestamp_s": 1252.0}, {"text": "question if the user\u0027s first question was in English, the model", "timestamp": "00:20:56,854", "timestamp_s": 1256.0}, {"text": "proceeded to respond in English. If the user\u0027s", "timestamp": "00:21:00,382", "timestamp_s": 1260.0}, {"text": "first question was in French, the model responded in", "timestamp": "00:21:04,454", "timestamp_s": 1264.0}, {"text": "French. However, if the preceding questions were switched", "timestamp": "00:21:08,510", "timestamp_s": 1268.0}, {"text": "to English, the model reverted to responding in English,", "timestamp": "00:21:11,798", "timestamp_s": 1271.0}, {"text": "and if all the user\u0027s questions were", "timestamp": "00:21:15,734", "timestamp_s": 1275.0}, {"text": "in French, the model stopped responding in French. And what", "timestamp": "00:21:19,018", "timestamp_s": 1279.0}, {"text": "we can conclude from this is that both models are quite adequate in", "timestamp": "00:21:23,090", "timestamp_s": 1283.0}, {"text": "using responding to users", "timestamp": "00:21:27,258", "timestamp_s": 1287.0}, {"text": "in French. One thing to note here is that", "timestamp": "00:21:31,426", "timestamp_s": 1291.0}, {"text": "within our discovery, we found out there\u0027s no one to one", "timestamp": "00:21:35,874", "timestamp_s": 1295.0}, {"text": "switch between using different models. It is not easy to", "timestamp": "00:21:39,250", "timestamp_s": 1299.0}, {"text": "switch using the same prompts or techniques in", "timestamp": "00:21:42,674", "timestamp_s": 1302.0}, {"text": "one conversational search model", "timestamp": "00:21:46,314", "timestamp_s": 1306.0}, {"text": "and use it with another without experiencing any type", "timestamp": "00:21:49,858", "timestamp_s": 1309.0}, {"text": "of side effects. For example, the prompts we used", "timestamp": "00:21:53,138", "timestamp_s": 1313.0}, {"text": "for these experiments had to be adjusted to what we\u0027re using with", "timestamp": "00:21:57,234", "timestamp_s": 1317.0}, {"text": "OpenAPI conversational search login model and", "timestamp": "00:22:01,690", "timestamp_s": 1321.0}, {"text": "these experiments show though these experiments show points of success", "timestamp": "00:22:06,178", "timestamp_s": 1326.0}, {"text": "for more adjustments will need it", "timestamp": "00:22:10,414", "timestamp_s": 1330.0}, {"text": "to get responses to the quality,", "timestamp": "00:22:13,622", "timestamp_s": 1333.0}, {"text": "we need to do more adjustments to get responses to the", "timestamp": "00:22:17,038", "timestamp_s": 1337.0}, {"text": "quality which we\u0027re using with OpenAI.", "timestamp": "00:22:20,350", "timestamp_s": 1340.0}, {"text": "We also adjust the properties to launching the model service and the", "timestamp": "00:22:23,574", "timestamp_s": 1343.0}, {"text": "properties of the API calls, but we did not do an in depth evaluation", "timestamp": "00:22:27,110", "timestamp_s": 1347.0}, {"text": "with these properties. It is evident that with some", "timestamp": "00:22:31,174", "timestamp_s": 1351.0}, {"text": "slight adjustments, the latency and quality of the larger model changes.", "timestamp": "00:22:35,044", "timestamp_s": 1355.0}, {"text": "But, you know, further investigation would", "timestamp": "00:22:39,284", "timestamp_s": 1359.0}, {"text": "have needed to be done by a team to find the best properties", "timestamp": "00:22:42,556", "timestamp_s": 1362.0}, {"text": "for the required results.", "timestamp": "00:22:47,452", "timestamp_s": 1367.0}, {"text": "So you might be the", "timestamp": "00:22:52,404", "timestamp_s": 1372.0}, {"text": "big question is, how much is this all going to cost you?", "timestamp": "00:22:56,820", "timestamp_s": 1376.0}, {"text": "In our experience, we\u0027re able to run the models on a single a", "timestamp": "00:23:00,704", "timestamp_s": 1380.0}, {"text": "140 gigabyte gpu,", "timestamp": "00:23:04,152", "timestamp_s": 1384.0}, {"text": "and the cost calculations for that, for that", "timestamp": "00:23:08,384", "timestamp_s": 1388.0}, {"text": "model, for using a model, would come around to", "timestamp": "00:23:11,824", "timestamp_s": 1391.0}, {"text": "around just under $3,000", "timestamp": "00:23:15,144", "timestamp_s": 1395.0}, {"text": "a month. But since we have discovered that having", "timestamp": "00:23:19,416", "timestamp_s": 1399.0}, {"text": "more GPU is an advantage,", "timestamp": "00:23:23,408", "timestamp_s": 1403.0}, {"text": "we would need to select the highest gpu", "timestamp": "00:23:27,584", "timestamp_s": 1407.0}, {"text": "available would be the Nvidia, a 180 gigabyte,", "timestamp": "00:23:31,800", "timestamp_s": 1411.0}, {"text": "and that would come down to a round of a cost of", "timestamp": "00:23:35,704", "timestamp_s": 1415.0}, {"text": "4200.", "timestamp": "00:23:39,200", "timestamp_s": 1419.0}, {"text": "This estimation is for a single instance. If we were to", "timestamp": "00:23:42,384", "timestamp_s": 1422.0}, {"text": "scale out using Kubernetes autopilot or manually adding more", "timestamp": "00:23:45,944", "timestamp_s": 1425.0}, {"text": "vms, the cost of the of running the open logic", "timestamp": "00:23:49,728", "timestamp_s": 1429.0}, {"text": "model would grow significantly. It\u0027s also good to note", "timestamp": "00:23:53,352", "timestamp_s": 1433.0}, {"text": "that this does not these costs do not include human costs or", "timestamp": "00:23:56,710", "timestamp_s": 1436.0}, {"text": "networking costs and maintenance costs.", "timestamp": "00:24:00,766", "timestamp_s": 1440.0}, {"text": "So in our exploration", "timestamp": "00:24:04,574", "timestamp_s": 1444.0}, {"text": "phase, we looked at hosting", "timestamp": "00:24:07,798", "timestamp_s": 1447.0}, {"text": "a large learning model and comparing it to an", "timestamp": "00:24:11,702", "timestamp_s": 1451.0}, {"text": "enterprise large revenue model. Some of the so", "timestamp": "00:24:16,190", "timestamp_s": 1456.0}, {"text": "the downsides we actually ended up coming out of is that it\u0027s difficult", "timestamp": "00:24:20,164", "timestamp_s": 1460.0}, {"text": "to get adequate gpu and that there\u0027s a", "timestamp": "00:24:23,644", "timestamp_s": 1463.0}, {"text": "lot of high cost associated with running", "timestamp": "00:24:27,428", "timestamp_s": 1467.0}, {"text": "this model. It also would require internal", "timestamp": "00:24:31,220", "timestamp_s": 1471.0}, {"text": "support or expertise, and also security maintenance.", "timestamp": "00:24:34,636", "timestamp_s": 1474.0}, {"text": "On the other side, while we\u0027re running this proof of concept,", "timestamp": "00:24:39,964", "timestamp_s": 1479.0}, {"text": "we had come to the discovery that,", "timestamp": "00:24:43,908", "timestamp_s": 1483.0}, {"text": "well, OpenAI actually announced", "timestamp": "00:24:46,532", "timestamp_s": 1486.0}, {"text": "that they were decreasing the cost of chat GPT 3.5", "timestamp": "00:24:50,340", "timestamp_s": 1490.0}, {"text": "turbo and this made it more promising to", "timestamp": "00:24:54,612", "timestamp_s": 1494.0}, {"text": "use. And as for the slow API", "timestamp": "00:24:58,612", "timestamp_s": 1498.0}, {"text": "response time, we are also doing some fine tuning", "timestamp": "00:25:03,220", "timestamp_s": 1503.0}, {"text": "on the model, and by doing some fine tuning, it actually", "timestamp": "00:25:06,292", "timestamp_s": 1506.0}, {"text": "sped up the response time.", "timestamp": "00:25:10,260", "timestamp_s": 1510.0}, {"text": "So my learnings and outcome from", "timestamp": "00:25:13,804", "timestamp_s": 1513.0}, {"text": "this, from deploying large", "timestamp": "00:25:17,492", "timestamp_s": 1517.0}, {"text": "learning model, yeah, it\u0027s definitely possible based", "timestamp": "00:25:20,924", "timestamp_s": 1520.0}, {"text": "on a use case, and based on our use case, it\u0027s best to start off", "timestamp": "00:25:25,700", "timestamp_s": 1525.0}, {"text": "with a lightweight model like the Falcon seven", "timestamp": "00:25:29,148", "timestamp_s": 1529.0}, {"text": "and start using that internally. Just discover,", "timestamp": "00:25:36,364", "timestamp_s": 1536.0}, {"text": "play with it and figure out", "timestamp": "00:25:40,252", "timestamp_s": 1540.0}, {"text": "what properties you can use to offload, or what functionality", "timestamp": "00:25:46,044", "timestamp_s": 1546.0}, {"text": "you can use to offload to your internal model, and use that", "timestamp": "00:25:49,668", "timestamp_s": 1549.0}, {"text": "and slowly start offloading to it until you", "timestamp": "00:25:53,380", "timestamp_s": 1553.0}, {"text": "get to a stage where you can either have a balance of a", "timestamp": "00:25:57,340", "timestamp_s": 1557.0}, {"text": "pay to go service or have,", "timestamp": "00:26:04,060", "timestamp_s": 1564.0}, {"text": "or having fully running it on your", "timestamp": "00:26:08,644", "timestamp_s": 1568.0}, {"text": "own infrastructure. Thank you and I", "timestamp": "00:26:12,556", "timestamp_s": 1572.0}, {"text": "hope you enjoyed the talk. If you have any questions,", "timestamp": "00:26:16,660", "timestamp_s": 1576.0}, {"text": "please feel free to get in touch with me.", "timestamp": "00:26:19,932", "timestamp_s": 1579.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'XFSdVb479T4',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Running an open source LLM
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>In this talk, we will explore the options available for harnessing LLMs, including deploying your own service or using existing LLM APIs. Attendees will learn best practices for leveraging LLMs effectively, whether by building or subscribing.</p>
<!-- End Text -->
          </div>

          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
              <li>
                Bongwani Shangwe talks about running an open source large learning model on your own infrastructure. Conversational search can be defined as a chat interface to enhance the user experience. When you do a self hosted solution, the benefit is that you complete control of your application.

              </li>
              
              <li>
                The less indirect and short prompt it is, it opens up the ability for side effects such as hallucinations, specifically with latency. There has to be a balance between how detailed the model response is and the quickness of the response. More adjustments will need it to get responses to the quality.
              </li>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./assemblyai/XFSdVb479T4.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:20,760'); seek(20.0)">
              Hello, I'm Bongwani Shangwe and I'm here to present to you
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:24,262'); seek(24.0)">
              about running an open source large learning model on your own infrastructure.
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:28,514'); seek(28.0)">
              I first want to kick off with a short introduction about the company I
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:31,578'); seek(31.0)">
              work for at Aventa and what is it we do? Because a lot
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:35,346'); seek(35.0)">
              of people might know about our secondhand on seller
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:38,970'); seek(38.0)">
              marketplaces, but they do not know much about the
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:42,330'); seek(42.0)">
              brand adventure behind them. Adavinte is
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:45,690'); seek(45.0)">
              one of the world's leading online classified specialists with market
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:49,594'); seek(49.0)">
              across three continents containing over 25
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:53,674'); seek(53.0)">
              marketplace brands. Our marketplaces range from
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:00:57,714'); seek(57.0)">
              consumer goods, vehicles, real estate and
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:01:01,114'); seek(61.0)">
              jobs. Adventure consists of several marketplace
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:01:04,490'); seek(64.0)">
              brands like Gleanzeigen in Germany, Mark Blatz in the
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:01:07,850'); seek(67.0)">
              Netherlands, Lebencoin in France, Kijiji in Canada.
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:01:12,234'); seek(72.0)">
              Adavinte is a champion for sustainable commerce,
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:01:15,178'); seek(75.0)">
              making a positive impact on the environment, the economy and society
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:20,894'); seek(80.0)">
              by the numbers we get about 2.5
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:25,022'); seek(85.0)">
              billion monthly visits across our website.
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:28,974'); seek(88.0)">
              We have 25 plus marketplaces in our digital portfolio
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:33,182'); seek(93.0)">
              and over 5700 employees
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:36,510'); seek(96.0)">
              across ten continents. Now at Delavinto,
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:39,990'); seek(99.0)">
              we've been working on conversational search assistant which is
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:43,182'); seek(103.0)">
              geared to launch on Levin Quan, France for a b testing
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:46,934'); seek(106.0)">
              during the second quarter of this year.
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:50,154'); seek(110.0)">
              Conversational search is about building a smarter sharpening experience by allowing
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:54,146'); seek(114.0)">
              users to ask arbitrary questions and being guided to relevant
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:01:57,770'); seek(117.0)">
              recommendations and search results by an assistant in
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:02:01,650'); seek(121.0)">
              order to serve a greater user experience.
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:02:05,082'); seek(125.0)">
              Outside of the normal chat bots which are in use by
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:02:08,834'); seek(128.0)">
              most websites, the conversational search assistance will
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:02:12,338'); seek(132.0)">
              be backed by a large learning model service.
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:02:15,494'); seek(135.0)">
              Conversational search can be defined as a chat interface
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:02:19,630'); seek(139.0)">
              to enhance the user experience by allowing natural language interactions
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:23,350'); seek(143.0)">
              with software agents or virtual assistants to retrieve information.
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:28,654'); seek(148.0)">
              The product we envisioned is kind of like this.
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:32,726'); seek(152.0)">
              This is an example I drew up and
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:37,262'); seek(157.0)">
              with the conversational assistant, when it pops up or when you interact with it,
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:41,854'); seek(161.0)">
              you ask a general question. In this case, this user is looking for
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:45,574'); seek(165.0)">
              a Ford Focus or Fiesta and it's
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:49,110'); seek(169.0)">
              for the conversational search assistance to assist
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:52,926'); seek(172.0)">
              the user in narrowing down the search products and
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:02:56,326'); seek(176.0)">
              also asking for more preferences and what the user would like
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:02:59,710'); seek(179.0)">
              is specific. In this vehicle he's looking for
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:03:04,534'); seek(184.0)">
              basic infrastructure overview of how it works.
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:03:09,514'); seek(189.0)">
              The user would get of course
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:03:13,082'); seek(193.0)">
              interact with the conversational search assistant
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:17,418'); seek(197.0)">
              and which would in the background generate
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:20,602'); seek(200.0)">
              a query to call a conversational large learning model.
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:23,978'); seek(203.0)">
              The large learning model also gathers the history of
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:27,290'); seek(207.0)">
              the conversation which has been happening and it extracts
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:31,498'); seek(211.0)">
              that information in order to consolidate it and
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:36,824'); seek(216.0)">
              push the information to a search API
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:39,952'); seek(219.0)">
              to get relevant search results.
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:45,424'); seek(225.0)">
              Everything here the conversation large render model and extraction large
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:49,296'); seek(229.0)">
              learning model is backwards GPT 3.5
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:54,504'); seek(234.0)">
              outside from OpenAI, the team has also been looking
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:57,832'); seek(237.0)">
              at other providers for large learning model APIs.
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:04:02,214'); seek(242.0)">
              However, we found that there are some downsides in having to
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:04:05,566'); seek(245.0)">
              use a service provided largely model. Some points
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:04:09,518'); seek(249.0)">
              are the readiness of the service as this is still a new
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:04:13,150'); seek(253.0)">
              field and some providers are slow to
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:04:16,382'); seek(256.0)">
              open up to more customers to a
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:19,518'); seek(259.0)">
              larger scale. Thus it takes quite a while to get onboarded
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:23,182'); seek(263.0)">
              onto these services. There's the cost factor.
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:27,824'); seek(267.0)">
              Of course, this is a new additional cost for the team and
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:30,976'); seek(270.0)">
              the company and we also had to think about
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:34,560'); seek(274.0)">
              latency given that the services outside of adventure infrastructure,
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:39,040'); seek(279.0)">
              there's an additional latency we have to account
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:42,320'); seek(282.0)">
              for. Given some of these factors,
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:45,464'); seek(285.0)">
              we decided to investigate the use of open large learning models
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:49,488'); seek(289.0)">
              which we could possibly deploy into our own infrastructure.
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:53,814'); seek(293.0)">
              So during the proof of concept phase where
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:04:58,070'); seek(298.0)">
              we're using paid go to services, we also started exploring
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:05:02,974'); seek(302.0)">
              looking at hosting a large learning model with
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:05:07,366'); seek(307.0)">
              an enterprise service.
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:05:12,654'); seek(312.0)">
              You get the top, best quality and the latest
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:05:16,374'); seek(316.0)">
              large learning models which are being produced
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:05:20,724'); seek(320.0)">
              and versus for
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:05:24,724'); seek(324.0)">
              us. When you do a self hosted solution,
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:29,324'); seek(329.0)">
              the benefit is that you complete control of your application
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:33,092'); seek(333.0)">
              and your team is responsible for the system
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:37,164'); seek(337.0)">
              versus using an external API. You're dependent on that
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:40,972'); seek(340.0)">
              other system being up all the time for your service to run.
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:45,704'); seek(345.0)">
              Some benefits in using
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:52,224'); seek(352.0)">
              your own hosted large learning model is that you have greater privacy
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:56,400'); seek(356.0)">
              and compliance and you also avoid vendor lock
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:06:00,120'); seek(360.0)">
              in. We started exploring
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:06:04,120'); seek(364.0)">
              models to use by going to
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:06:07,720'); seek(367.0)">
              hugging face. Hugging face is currently the main platform
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:06:11,514'); seek(371.0)">
              or a website for building and using machine learning based models
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:06:15,498'); seek(375.0)">
              such as large render models. It also provides a platform to run these
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:06:19,266'); seek(379.0)">
              models on a smaller scale. In our case, we considered
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:06:23,434'); seek(383.0)">
              text generation based models. We first started
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:06:27,018'); seek(387.0)">
              off with the Falcon 7 billion meter perimeter tuned
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:30,986'); seek(390.0)">
              model to get familiar with deploying a large learning model.
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:36,194'); seek(396.0)">
              It's a lightweight model and it's
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:40,780'); seek(400.0)">
              quite easy to get started and set up. Though it is lightweight,
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:45,284'); seek(405.0)">
              we did find at lack depth when answering
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:06:49,196'); seek(409.0)">
              specific questions,
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:06:51,844'); seek(411.0)">
              specifically when we're looking at to use it for as a conversational
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:06:55,860'); seek(415.0)">
              search assistance. So then we started looking at other models
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:06:59,964'); seek(419.0)">
              which were out there which we could use. We looked at the Falcon
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:07:03,548'); seek(423.0)">
              40 millimeter chat tune model and the Loma 270
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:07:07,488'); seek(427.0)">
              billion chat tune model.
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:07:11,024'); seek(431.0)">
              Aside from being chat tuned, these models also provide multi
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:07:14,432'); seek(434.0)">
              language support which was a requirement for us as marketplaces
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:07:18,960'); seek(438.0)">
              across different countries with different language customer
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:07:23,264'); seek(443.0)">
              customers who speak different languages. So on
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:07:29,992'); seek(449.0)">
              deploying a model or hosting the large learner model. In this case,
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:07:34,754'); seek(454.0)">
              we found text generation interface or TGI for
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:39,530'); seek(459.0)">
              short. TGI is a fast optimized interface
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:07:43,354'); seek(463.0)">
              solution built for deploying and serving large learning models.
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:07:47,234'); seek(467.0)">
              TGI enables high performance text generation
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:07:51,074'); seek(471.0)">
              using tensor parallelism, dynamic batching
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:07:55,322'); seek(475.0)">
              for most popular and dynamic batch for most popular
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:07:59,402'); seek(479.0)">
              open source larger models. TGI also has
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:08:03,672'); seek(483.0)">
              a docker image which can be used to launch the text generation
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:08:07,320'); seek(487.0)">
              service. One of some of
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:08:10,656'); seek(490.0)">
              the benefits we found using TGI is that it's
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:08:13,800'); seek(493.0)">
              a simple launcher service to host your model.
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:08:17,904'); seek(497.0)">
              It's production ready as it provides tracing
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:08:22,424'); seek(502.0)">
              with open telemetry and Prometheus, this token
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:08:26,592'); seek(506.0)">
              streaming, you can have continuous batching
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:08:30,188'); seek(510.0)">
              of incoming requests for increased total throughput. There's quantitization
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:08:34,868'); seek(514.0)">
              with bits and bytes, stop sequences,
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:08:38,380'); seek(518.0)">
              and you can have custom prompt generation,
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:08:43,244'); seek(523.0)">
              and it also provides fine tuning support to
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:47,340'); seek(527.0)">
              fine tune your models. The easiest way to get
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:08:51,132'); seek(531.0)">
              started with the text generation interface
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:08:57,204'); seek(537.0)">
              tick generation inference is to run
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:09:01,276'); seek(541.0)">
              the simple command line, which is
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:09:06,324'); seek(546.0)">
              you just point to the model which you'd like to use.
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:09:10,204'); seek(550.0)">
              You set up where to store the model and
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:09:14,372'); seek(554.0)">
              all the dots, such as the model weights. And when you
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:09:17,860'); seek(557.0)">
              run this, it's a docker command. It will launch the docker,
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:09:22,052'); seek(562.0)">
              the TGI docker, and if
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:09:27,280'); seek(567.0)">
              your model is not present on your machine, it will download it.
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:09:31,064'); seek(571.0)">
              If it is present, it will just start. Yeah, it will start
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:09:35,024'); seek(575.0)">
              running. And to get going you
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:09:38,408'); seek(578.0)">
              can just, to test it out, you can use a simple curl
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:09:42,064'); seek(582.0)">
              command and as you can see,
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:09:45,624'); seek(585.0)">
              you submit the JSON and you get a response from the model.
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:09:50,124'); seek(590.0)">
              Now our team based our
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:09:53,852'); seek(593.0)">
              experimentations on GCP. Initially we
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:09:57,564'); seek(597.0)">
              tried to run the deployments on our european region of GCP.
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:10:02,188'); seek(602.0)">
              However, it appears that with the rise of GPU
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:10:05,676'); seek(605.0)">
              use applications such as large rental models, there's a scarcity
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:10:09,844'); seek(609.0)">
              of GPU variability and of GPU availability
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:10:14,612'); seek(614.0)">
              in Europe and north american regions. Actually, if you
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:10:18,540'); seek(618.0)">
              research this more, it seems to be occurring
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:10:22,548'); seek(622.0)">
              across all the cloud providers because of
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:10:26,500'); seek(626.0)">
              the current popularity of just GPU based applications.
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:10:30,964'); seek(630.0)">
              So outside of, we scanned outside of Europe and North
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:10:34,524'); seek(634.0)">
              America, and we found that there were two zones that could provide
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:10:40,324'); seek(640.0)">
              GPU's, which we could gets in
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:10:44,408'); seek(644.0)">
              for spot instances, and those were in
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:10:48,192'); seek(648.0)">
              East Asia and also
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:10:52,584'); seek(652.0)">
              in the Middle east one.
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:10:56,232'); seek(656.0)">
              To be specific, large training models require high
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:11:00,344'); seek(660.0)">
              GPU to train for inference. It can be less,
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:11:04,248'); seek(664.0)">
              but it's still generating a large amount
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:11:07,664'); seek(667.0)">
              of GPU memory required. The higher parameters
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:11:11,432'); seek(671.0)">
              the model contains, the more GPU memory required to run the inference
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:11:15,584'); seek(675.0)">
              of the model. There's also the possibility
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:11:19,512'); seek(679.0)">
              to run the model inference in eight bits and four bits mode,
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:11:23,232'); seek(683.0)">
              which decreases the amount of GPU memory required based
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:11:27,608'); seek(687.0)">
              on current GPU availability in cloud environments, the best options
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:11:31,048'); seek(691.0)">
              are machines that contain Nvidia a 180gb.
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:11:35,976'); seek(695.0)">
              However, these gpu's are extremely scarce, even in the cloud
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:11:39,978'); seek(699.0)">
              environment. We had then settled for
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:11:43,658'); seek(703.0)">
              using machines that contain a Nvidia a
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:11:47,210'); seek(707.0)">
              140gb. It is also possible to
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:11:50,842'); seek(710.0)">
              run a model on multiple gpu's in order to meet the requirement
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:11:54,290'); seek(714.0)">
              memory requirements of the model inference.
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:11:57,834'); seek(717.0)">
              So there's two ways we could have
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:12:01,274'); seek(721.0)">
              deployed these models
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:12:04,594'); seek(724.0)">
              and TGI inference.
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:12:08,874'); seek(728.0)">
              The first one was using Kubernetes or running
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:12:12,770'); seek(732.0)">
              it on a virtual machine. For doing on kubernetes,
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:12:17,274'); seek(737.0)">
              we just created a simple deployment yaml
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:12:21,098'); seek(741.0)">
              and we allowed for autoscaling because we didn't want
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:12:24,530'); seek(744.0)">
              to keep track of all the machines. So this
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:12:29,186'); seek(749.0)">
              seemed a bit better because when
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:12:32,428'); seek(752.0)">
              the models are not in use, it's easy to downscale as
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:12:36,532'); seek(756.0)">
              versus for if we're going to do it using
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:12:39,852'); seek(759.0)">
              a virtual machine. These were self managed machines
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:12:43,260'); seek(763.0)">
              and we'd have to keep track if on the capacity.
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:12:48,444'); seek(768.0)">
              Basically, if machines were not being used, then we'd have to shut them off ourselves.
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:12:53,364'); seek(773.0)">
              So we went into the because of this, we specifically started looking
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:12:57,644'); seek(777.0)">
              with doing on Kubernetes, on GKE.
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:13:02,404'); seek(782.0)">
              This is kind of a high level overview of GKE setup.
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:13:07,284'); seek(787.0)">
              You'd have the text generation inference deployment
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:13:11,636'); seek(791.0)">
              running in a docker image, pointing to
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:13:16,964'); seek(796.0)">
              a volume where all the weights
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:13:20,852'); seek(800.0)">
              were fetched from gcs.
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:13:23,284'); seek(803.0)">
              But we actually came into an issue when we
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:13:27,588'); seek(807.0)">
              started using GKe as
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:13:31,564'); seek(811.0)">
              we started, of course, we started with the
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:13:35,092'); seek(815.0)">
              Falcon seven, with the lightest Falcon model.
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:13:39,084'); seek(819.0)">
              Even with that, we found that the deployment time took longer than expected.
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:13:44,644'); seek(824.0)">
              GK GPU machines were also not really available
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:13:48,356'); seek(828.0)">
              for use even with kubernetes. And the
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:13:52,668'); seek(832.0)">
              highest or the highest
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:13:56,774'); seek(836.0)">
              GPU machine, or in this case
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:14:00,774'); seek(840.0)">
              node, GPU node which could be hosted with.
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:14:04,822'); seek(844.0)">
              We were able to get with GKE was a twelve gigabyte
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:14:09,014'); seek(849.0)">
              GPU, which was not enough to push on with
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:14:12,974'); seek(852.0)">
              other large rental models. So given
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:14:16,886'); seek(856.0)">
              that, we went back and said
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:14:20,694'); seek(860.0)">
              well, we're going to do the virtual machine deployment strategy.
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:14:24,814'); seek(864.0)">
              And with detection inference, it allows
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:14:28,534'); seek(868.0)">
              you to host the model on a
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:14:32,830'); seek(872.0)">
              vm with a single GPU, or parallelized
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:14:36,806'); seek(876.0)">
              inference across multiple gpu's.
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:14:41,134'); seek(881.0)">
              One of the benefits of parallelizing it across multiple gpu's
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:14:44,942'); seek(884.0)">
              is that you can meet using multiple gpu's.
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:14:48,822'); seek(888.0)">
              You can actually get have
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:14:52,556'); seek(892.0)">
              more higher memory across all the GPU's for inference.
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:14:57,884'); seek(897.0)">
              We started looking at running our
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:15:01,396'); seek(901.0)">
              experiments and we set up our experiments.
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:15:05,524'); seek(905.0)">
              When we deployed the models in a virtual machine,
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:15:09,932'); seek(909.0)">
              we set up notebooks to run different experiments
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:15:14,300'); seek(914.0)">
              so we can be able to track the data and compare the results
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:15:18,332'); seek(918.0)">
              later on.
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:15:20,964'); seek(920.0)">
              Yeah, so we looked at,
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:15:24,044'); seek(924.0)">
              so we had different cases where we deployed the model.
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:15:28,372'); seek(928.0)">
              The models on a single v, of a VM with
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:15:31,700'); seek(931.0)">
              a single GPU and a VM with multiple gpu's.
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:15:35,884'); seek(935.0)">
              Yeah, we had, for the single vm,
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:15:38,956'); seek(938.0)">
              we had a short response latency, which was really perfect
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:15:42,556'); seek(942.0)">
              for us. The deployment was quite quick,
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:15:46,254'); seek(946.0)">
              but the number of max tokens which could be processed
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:15:49,942'); seek(949.0)">
              was limited to the GPU memory. With a vm
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:15:53,862'); seek(953.0)">
              with multiple gpu's, we increased the GPU
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:15:58,102'); seek(958.0)">
              memory footprint so we had more GPU memory.
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:16:02,574'); seek(962.0)">
              And this in turn increased the Max token processing.
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:16:06,774'); seek(966.0)">
              The time it took for the model to be ready increased. So the deployment time
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:16:10,334'); seek(970.0)">
              also increased and the response latency also
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:16:13,576'); seek(973.0)">
              increased. There were several
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:16:17,552'); seek(977.0)">
              factors which we looked into why latency?
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:16:22,448'); seek(982.0)">
              The factors which came into latency with
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:16:26,552'); seek(986.0)">
              this and some of the during our
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:16:30,256'); seek(990.0)">
              experience, we found out that it
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:16:33,968'); seek(993.0)">
              was due to prompt size and complexity,
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:16:37,424'); seek(997.0)">
              so similar to if the
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:16:41,064'); seek(1001.0)">
              prompt was long and quite complex,
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:16:44,824'); seek(1004.0)">
              it will take longer for any
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:16:48,872'); seek(1008.0)">
              of the models to process.
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:16:51,424'); seek(1011.0)">
              However, the less indirect and short prompt it is,
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:16:55,280'); seek(1015.0)">
              it opens up the ability for side effects such as hallucinations,
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:17:01,184'); seek(1021.0)">
              specifically with latency. With GPU
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:17:04,944'); seek(1024.0)">
              setup, we tracked this down that
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:17:09,201'); seek(1029.0)">
              using more GPU's across vmware,
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:17:14,473'); seek(1034.0)">
              more GPU's increased latency.
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:17:18,673'); seek(1038.0)">
              We figured out that this could be between that there's a
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:17:22,137'); seek(1042.0)">
              lot of IO happening between the GPU's,
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:17:26,849'); seek(1046.0)">
              so there's a lot of dot exchange happening versus when you
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:17:30,977'); seek(1050.0)">
              do an inference on a single GPU machine.
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:17:34,404'); seek(1054.0)">
              And another thing which we tracked
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:17:38,564'); seek(1058.0)">
              with regards to latency is the max token
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:17:41,756'); seek(1061.0)">
              output. So we were able to set the max
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:17:45,764'); seek(1065.0)">
              token output of the deployment,
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:17:49,132'); seek(1069.0)">
              and the higher token output length, the response from the model
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:17:52,412'); seek(1072.0)">
              becomes more detailed and longer, which requires more processing time.
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:17:57,284'); seek(1077.0)">
              But if the token length is extremely short,
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:18:00,654'); seek(1080.0)">
              the response time might not be complete or make sense to a human.
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:18:04,974'); seek(1084.0)">
              Thus, there has to be a balance between how detailed the model response
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:18:09,022'); seek(1089.0)">
              is and the quickness of the response, and more
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:18:13,326'); seek(1093.0)">
              detailed analysis on this.
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:18:17,694'); seek(1097.0)">
              As I mentioned, it is possible to set the number of tokens used
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:18:21,214'); seek(1101.0)">
              for the output. All the previous
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:18:24,358'); seek(1104.0)">
              tests we've done, we set the token outputs
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:18:27,590'); seek(1107.0)">
              to 256, and once
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:18:30,762'); seek(1110.0)">
              we doubled it, we were able to observe different
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:18:34,458'); seek(1114.0)">
              effects from the models tested.
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:18:38,154'); seek(1118.0)">
              So for the Falcon model with 256 tokens,
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:18:42,290'); seek(1122.0)">
              maximum tokens, the model seemed to perform adequately. Once we
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:18:45,970'); seek(1125.0)">
              increased the number of tokens, the response time initially
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:18:49,138'); seek(1129.0)">
              seemed to be the same as before, but as the conversation
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:18:52,778'); seek(1132.0)">
              continued with the user, the response time began to increase.
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:18:56,724'); seek(1136.0)">
              Another effect was that the response became longer and
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:19:00,548'); seek(1140.0)">
              the model started to hallucinate,
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:19:03,300'); seek(1143.0)">
              appearing to have a conversation with itself.
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:19:07,764'); seek(1147.0)">
              Llama two, we also again
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:19:11,220'); seek(1151.0)">
              started with 256 maximum tokens and the models performed
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:19:15,180'); seek(1155.0)">
              adequately. But we'd noted some instances that detect response
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:19:19,460'); seek(1159.0)">
              would appear to cut off and with an increase in the
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:19:23,220'); seek(1163.0)">
              token size, delicacy also increased quite
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:19:27,192'); seek(1167.0)">
              extremely and the response was more complete.
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:19:31,008'); seek(1171.0)">
              But as the conversation would continue, we get
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:19:34,904'); seek(1174.0)">
              some instances where the response would get cut off again.
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:19:40,544'); seek(1180.0)">
              As mentioned again, we have marketplaces
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:19:44,520'); seek(1184.0)">
              across different countries or different languages, so it
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:19:48,368'); seek(1188.0)">
              is important for us to test how this performs, and since we're
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:19:52,240'); seek(1192.0)">
              launching first in Devonkwan, we looked at
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:19:55,926'); seek(1195.0)">
              using french. So we tested the Falcon and Loma
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:19:59,854'); seek(1199.0)">
              two on its ability to do inference with
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:20:03,294'); seek(1203.0)">
              french users. The main adjustment
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:20:07,070'); seek(1207.0)">
              was with the system prompt, directing the model to respond only
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:20:11,334'); seek(1211.0)">
              in French.
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:20:14,054'); seek(1214.0)">
              For Falcon, if the users question was in English
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:20:18,038'); seek(1218.0)">
              and the model proceeded to respond
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:20:22,674'); seek(1222.0)">
              in English, but if the user asked questions
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:20:26,250'); seek(1226.0)">
              in French, the model responded in French, and if the
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:20:30,098'); seek(1230.0)">
              preceding user questions was in English, the model remained
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:20:33,338'); seek(1233.0)">
              only speaking in French, and if all
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:20:36,898'); seek(1236.0)">
              the user questions were in French, the model start to
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:20:40,466'); seek(1240.0)">
              respond to French. However, there were some side effects we
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:20:43,826'); seek(1243.0)">
              observed and some hallucinations
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:20:48,674'); seek(1248.0)">
              within the model when using the french language.
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:20:52,654'); seek(1252.0)">
              In the case of lama two, if the user
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:20:56,854'); seek(1256.0)">
              question if the user's first question was in English, the model
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:21:00,382'); seek(1260.0)">
              proceeded to respond in English. If the user's
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:21:04,454'); seek(1264.0)">
              first question was in French, the model responded in
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:21:08,510'); seek(1268.0)">
              French. However, if the preceding questions were switched
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:21:11,798'); seek(1271.0)">
              to English, the model reverted to responding in English,
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:21:15,734'); seek(1275.0)">
              and if all the user's questions were
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:21:19,018'); seek(1279.0)">
              in French, the model stopped responding in French. And what
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:21:23,090'); seek(1283.0)">
              we can conclude from this is that both models are quite adequate in
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:21:27,258'); seek(1287.0)">
              using responding to users
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:21:31,426'); seek(1291.0)">
              in French. One thing to note here is that
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:21:35,874'); seek(1295.0)">
              within our discovery, we found out there's no one to one
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:21:39,250'); seek(1299.0)">
              switch between using different models. It is not easy to
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:21:42,674'); seek(1302.0)">
              switch using the same prompts or techniques in
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:21:46,314'); seek(1306.0)">
              one conversational search model
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:21:49,858'); seek(1309.0)">
              and use it with another without experiencing any type
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:21:53,138'); seek(1313.0)">
              of side effects. For example, the prompts we used
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:21:57,234'); seek(1317.0)">
              for these experiments had to be adjusted to what we're using with
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:22:01,690'); seek(1321.0)">
              OpenAPI conversational search login model and
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:22:06,178'); seek(1326.0)">
              these experiments show though these experiments show points of success
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:22:10,414'); seek(1330.0)">
              for more adjustments will need it
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:22:13,622'); seek(1333.0)">
              to get responses to the quality,
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:22:17,038'); seek(1337.0)">
              we need to do more adjustments to get responses to the
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:22:20,350'); seek(1340.0)">
              quality which we're using with OpenAI.
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:22:23,574'); seek(1343.0)">
              We also adjust the properties to launching the model service and the
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:22:27,110'); seek(1347.0)">
              properties of the API calls, but we did not do an in depth evaluation
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:22:31,174'); seek(1351.0)">
              with these properties. It is evident that with some
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:22:35,044'); seek(1355.0)">
              slight adjustments, the latency and quality of the larger model changes.
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:22:39,284'); seek(1359.0)">
              But, you know, further investigation would
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:22:42,556'); seek(1362.0)">
              have needed to be done by a team to find the best properties
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:22:47,452'); seek(1367.0)">
              for the required results.
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:22:52,404'); seek(1372.0)">
              So you might be the
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:22:56,820'); seek(1376.0)">
              big question is, how much is this all going to cost you?
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:23:00,704'); seek(1380.0)">
              In our experience, we're able to run the models on a single a
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:23:04,152'); seek(1384.0)">
              140 gigabyte gpu,
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:23:08,384'); seek(1388.0)">
              and the cost calculations for that, for that
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:23:11,824'); seek(1391.0)">
              model, for using a model, would come around to
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:23:15,144'); seek(1395.0)">
              around just under $3,000
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:23:19,416'); seek(1399.0)">
              a month. But since we have discovered that having
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:23:23,408'); seek(1403.0)">
              more GPU is an advantage,
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:23:27,584'); seek(1407.0)">
              we would need to select the highest gpu
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:23:31,800'); seek(1411.0)">
              available would be the Nvidia, a 180 gigabyte,
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:23:35,704'); seek(1415.0)">
              and that would come down to a round of a cost of
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:23:39,200'); seek(1419.0)">
              4200.
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:23:42,384'); seek(1422.0)">
              This estimation is for a single instance. If we were to
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:23:45,944'); seek(1425.0)">
              scale out using Kubernetes autopilot or manually adding more
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:23:49,728'); seek(1429.0)">
              vms, the cost of the of running the open logic
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:23:53,352'); seek(1433.0)">
              model would grow significantly. It's also good to note
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:23:56,710'); seek(1436.0)">
              that this does not these costs do not include human costs or
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:24:00,766'); seek(1440.0)">
              networking costs and maintenance costs.
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:24:04,574'); seek(1444.0)">
              So in our exploration
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:24:07,798'); seek(1447.0)">
              phase, we looked at hosting
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:24:11,702'); seek(1451.0)">
              a large learning model and comparing it to an
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:24:16,190'); seek(1456.0)">
              enterprise large revenue model. Some of the so
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:24:20,164'); seek(1460.0)">
              the downsides we actually ended up coming out of is that it's difficult
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:24:23,644'); seek(1463.0)">
              to get adequate gpu and that there's a
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:24:27,428'); seek(1467.0)">
              lot of high cost associated with running
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:24:31,220'); seek(1471.0)">
              this model. It also would require internal
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:24:34,636'); seek(1474.0)">
              support or expertise, and also security maintenance.
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:24:39,964'); seek(1479.0)">
              On the other side, while we're running this proof of concept,
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:24:43,908'); seek(1483.0)">
              we had come to the discovery that,
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:24:46,532'); seek(1486.0)">
              well, OpenAI actually announced
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:24:50,340'); seek(1490.0)">
              that they were decreasing the cost of chat GPT 3.5
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:24:54,612'); seek(1494.0)">
              turbo and this made it more promising to
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:24:58,612'); seek(1498.0)">
              use. And as for the slow API
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:25:03,220'); seek(1503.0)">
              response time, we are also doing some fine tuning
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:25:06,292'); seek(1506.0)">
              on the model, and by doing some fine tuning, it actually
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:25:10,260'); seek(1510.0)">
              sped up the response time.
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:25:13,804'); seek(1513.0)">
              So my learnings and outcome from
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:25:17,492'); seek(1517.0)">
              this, from deploying large
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:25:20,924'); seek(1520.0)">
              learning model, yeah, it's definitely possible based
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:25:25,700'); seek(1525.0)">
              on a use case, and based on our use case, it's best to start off
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:25:29,148'); seek(1529.0)">
              with a lightweight model like the Falcon seven
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:25:36,364'); seek(1536.0)">
              and start using that internally. Just discover,
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:25:40,252'); seek(1540.0)">
              play with it and figure out
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:25:46,044'); seek(1546.0)">
              what properties you can use to offload, or what functionality
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:25:49,668'); seek(1549.0)">
              you can use to offload to your internal model, and use that
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:25:53,380'); seek(1553.0)">
              and slowly start offloading to it until you
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:25:57,340'); seek(1557.0)">
              get to a stage where you can either have a balance of a
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:26:04,060'); seek(1564.0)">
              pay to go service or have,
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:26:08,644'); seek(1568.0)">
              or having fully running it on your
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:26:12,556'); seek(1572.0)">
              own infrastructure. Thank you and I
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:26:16,660'); seek(1576.0)">
              hope you enjoyed the talk. If you have any questions,
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:26:19,932'); seek(1579.0)">
              please feel free to get in touch with me.
            </span>
            
            </div>
          </div>
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/Bongani%20Shongwe%20-%20Conf42%20Large%20Language%20Models%20%28LLMs%29%202024.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/Bongani%20Shongwe%20-%20Conf42%20Large%20Language%20Models%20%28LLMs%29%202024.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #CCB87B;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/llms2024" class="btn btn-sm btn-danger shadow lift" style="background-color: #CCB87B;">
                <i class="fe fe-grid me-2"></i>
                See all 28 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Bongani%20Shongwe_llm.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Bongani Shongwe
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Senior Data Engineer @ Adevinta / eBay
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/bonganishongwe/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Bongani Shongwe's LinkedIn account" />
                  </a>
                  
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by Bongani Shongwe"
                  data-url="https://www.conf42.com/llms2024"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/llms2024"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>





  <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
    </script>
    <!-- SUBSCRIBE -->
    <section class="pt-8 pt-md-11 bg-gradient-light-white" id="register">
        <div class="container">
          <div class="row align-items-center justify-content-between mb-8 mb-md-11">
            <div class="col-12 col-md-6 order-md-2" data-aos="fade-left">
  
              <!-- Heading -->
              <h2>
                Awesome tech events for <br>
                <span class="text-success"><span data-typed='{"strings": ["software engineers.", "tech leaders.", "SREs.", "DevOps.", "CTOs.",  "managers.", "architects.", "QAs.", "developers.", "coders.", "founders.", "CEOs.", "students.", "geeks.", "ethical hackers.", "educators.", "enthusiasts.", "directors.", "researchers.", "PHDs.", "evangelists.", "tech authors."]}'></span></span>
              </h2>
  
              <!-- Text -->
              <p class="fs-lg text-muted mb-6">
  
              </p>
  
              <!-- List -->
              <div class="row">
                <div class="col-12 col-lg-12">
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <!-- Text -->
                    <p class="text-success">
                      Priority access to all content
                    </p>
                  </div>
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Video hallway track
                    </p>
                  </div>

                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Community chat
                    </p>
                  </div>
  
                  <!-- Item -->
                  <div class="d-flex">
                    <!-- Check -->
                    <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                      <i class="fe fe-check"></i>
                    </div>
                    <p class="text-success">
                      Exclusive promotions and giveaways
                    </p>
                  </div>
  
                </div>
              </div> <!-- / .row -->
  
            </div>
            <div class="col-12 col-md-6 col-lg-5 order-md-1">
  
              <!-- Card -->
              <div class="card shadow-light-lg">
  
                <!-- Body -->
                <div class="card-body">
  
                  <!-- Form -->
                  <link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
                  <p class="emailoctopus-success-message text-success"></p>
                  <p class="emailoctopus-error-message text-danger"></p>
                  <form
                    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
                    method="post"
                    data-message-success="Thanks! Check your email for further directions!"
                    data-message-missing-email-address="Your email address is required."
                    data-message-invalid-email-address="Your email address looks incorrect, please try again."
                    data-message-bot-submission-error="This doesn't look like a human submission."
                    data-message-consent-required="Please check the checkbox to indicate your consent."
                    data-message-invalid-parameters-error="This form has missing or invalid fields."
                    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
                    class="emailoctopus-form"
                    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
                  >
                    <div class="form-floating emailoctopus-form-row">
                      <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
                      <label for="field_0">Email address</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
                      <label for="field_1">First Name</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
                      <label for="field_2">Last Name</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
                      <label for="field_4">Company</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
                      <label for="field_5">Job Title</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
                      <label for="field_3">Phone Number</label>
                    </div>
                    <div class="form-floating emailoctopus-form-row">
                      <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
                        oninput="updateCountry()"
                      >
                        <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
                      </select>
                      <label for="field_7">Country</label>
                    </div>
                    <input id="country-destination" name="field_7" type="hidden">
                    <input id="tz-country" name="field_8" type="hidden">
                    
                    <input
                      name="field_6"
                      type="hidden"
                      value="Large Language Models"
                    >
                    
                    <div class="emailoctopus-form-row-consent">
                      <input
                        type="checkbox"
                        id="consent"
                        name="consent"
                      >
                      <label for="consent">
                        I consent to the following terms:
                      </label>
                      <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
                        Terms and Conditions
                      </a>
                      &amp;
                      <a href="./code-of-conduct" target="_blank">
                        Code of Conduct
                      </a>
                    </div>
                    <div
                      aria-hidden="true"
                      class="emailoctopus-form-row-hp"
                    >
                      <input
                        type="text"
                        name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
                        tabindex="-1"
                        autocomplete="nope"
                      >
                    </div>
                    <div class="mt-6 emailoctopus-form-row-subscribe">
                      <input
                        type="hidden"
                        name="successRedirectUrl"
                      >
                      <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
                        Subscribe
                      </button>
                    </div>
                  </form>
  
                </div>
  
              </div>
  
            </div>
  
          </div> <!-- / .row -->
        </div> <!-- / .container -->
      </section>

      <!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
      <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.sreday.com/">
                  SREday London 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.sreday.com/">
                  SREday Amsterdam 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>