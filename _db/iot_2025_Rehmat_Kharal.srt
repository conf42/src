1
00:00:00,000 --> 00:00:04,320
Everyone, my name is Ramad Rell and
I'm here today to talk to you a little

2
00:00:04,320 --> 00:00:09,060
bit about iot data streaming and the
complexities that come alongside with it.

3
00:00:09,060 --> 00:00:11,340
So let's go ahead and get started.

4
00:00:11,790 --> 00:00:13,710
So let's talk about IO ot, right?

5
00:00:13,920 --> 00:00:18,300
The rise of IO OT is actually transformed
the way we think about data today.

6
00:00:18,540 --> 00:00:22,980
We've got billions of connected devices,
everything from wearables to industrial

7
00:00:22,980 --> 00:00:27,180
sensors that are generating streams
of events that must be captured,

8
00:00:27,180 --> 00:00:29,190
processed, and trusted in real time.

9
00:00:29,985 --> 00:00:35,295
For many engineering teams, moving from
devices, producing data to data producing

10
00:00:35,295 --> 00:00:37,815
value is where the hard problems begin.

11
00:00:38,025 --> 00:00:40,395
And in today's session, we're
gonna explore the hidden

12
00:00:40,395 --> 00:00:43,185
challenges of IOT data pipelines.

13
00:00:43,635 --> 00:00:45,555
Let's go ahead and jump in.

14
00:00:46,485 --> 00:00:50,625
So the challenges that we're seeing
today, they typically stem from the

15
00:00:50,625 --> 00:00:55,365
unique nature of the data source rather
than just standard big data problems.

16
00:00:56,055 --> 00:01:01,545
Key hidden challenges include ensuring
data integrity, managing device

17
00:01:01,575 --> 00:01:06,315
heterogeneity, and navigating complex
security and compliance issues.

18
00:01:06,975 --> 00:01:10,215
So how do you make
streaming data reliable?

19
00:01:10,215 --> 00:01:13,875
When devices are noisy,
unpredictable, or go offline?

20
00:01:14,505 --> 00:01:18,045
How do you enforce security,
governance and data quality

21
00:01:18,045 --> 00:01:19,120
without slowing innovation?

22
00:01:19,620 --> 00:01:23,820
And how can platform engineers give
developers self-service access to

23
00:01:23,820 --> 00:01:26,820
streaming data without creating ca chaos?

24
00:01:27,000 --> 00:01:29,310
So let's dig into it with
challenge number one.

25
00:01:29,700 --> 00:01:34,860
The first hidden challenge is that
sensor data is inherently unreliable.

26
00:01:35,250 --> 00:01:37,860
It's noisy, and it can be inaccurate.

27
00:01:37,890 --> 00:01:41,730
So sensors can produce erroneous,
irrelevant, or noisy data due

28
00:01:41,730 --> 00:01:46,480
to environmental factors such as
temperature of fluctuations vibrations.

29
00:01:46,780 --> 00:01:50,740
There could be hardware limitations
or even just simple malfunctions,

30
00:01:51,160 --> 00:01:54,070
but this creates a lot of
added complications, right?

31
00:01:54,310 --> 00:01:58,630
The data that comes off the sensor is
used for multiple business applications.

32
00:01:59,050 --> 00:02:04,090
So when the data that's coming
off those devices is unreliable,

33
00:02:04,150 --> 00:02:08,764
it's gonna have a entire domino
effect of negative consequences.

34
00:02:09,005 --> 00:02:12,454
So let's go ahead and look at the
manufacturing industry, for instance.

35
00:02:12,954 --> 00:02:16,644
In this environment, data could be
coming off of different robotic arms.

36
00:02:17,034 --> 00:02:19,614
One robot may be placing windows on cars.

37
00:02:19,644 --> 00:02:22,194
Another may be painting the
car, and a third would be

38
00:02:22,194 --> 00:02:23,575
placing the tires on the car.

39
00:02:23,964 --> 00:02:27,594
So what happens when the number of
windows placed is incorrect and that

40
00:02:27,594 --> 00:02:30,114
data that's being captured is incorrect?

41
00:02:30,804 --> 00:02:33,864
This throws off data accuracy completely.

42
00:02:34,284 --> 00:02:39,354
If the transmitted data says 500 windows
were placed on the car, but in reality

43
00:02:39,354 --> 00:02:41,754
is 5,000 windows were placed on the cars.

44
00:02:42,249 --> 00:02:45,519
The chain of command of the
rest of the robots is off.

45
00:02:45,729 --> 00:02:49,149
The data that goes into the procurement
and shipping department is off.

46
00:02:49,479 --> 00:02:53,100
The data that is exchanged
with the manufacturer is wrong.

47
00:02:53,459 --> 00:02:58,500
This calibration drift is throwing
off the entire supply chain.

48
00:02:59,160 --> 00:03:03,380
Then you've got not only calibration
issues but you also have missing

49
00:03:03,380 --> 00:03:06,830
data or packet loss, and they
can have a similar effect.

50
00:03:07,340 --> 00:03:11,860
This is typically, takes place when you
have inconsistent network connectivity

51
00:03:11,860 --> 00:03:17,320
issues, but this loss data leads to an
incomplete picture of what is actually

52
00:03:17,320 --> 00:03:21,940
taking place, and that missing data
can lead to significant disruptions

53
00:03:21,970 --> 00:03:26,260
all the way from production halts
to compromise quality control and

54
00:03:26,260 --> 00:03:30,725
safety issues, which in turn could
cost the missus millions of dollars.

55
00:03:31,225 --> 00:03:39,405
So now as teams moved away from, legacy
architectures and ET l and broken, or I

56
00:03:39,405 --> 00:03:44,955
guess you could say one-off integrations
and onto data streaming, they did it

57
00:03:44,955 --> 00:03:48,885
because there was new business models
that need to be fed this new data.

58
00:03:48,975 --> 00:03:52,635
And with the legacy systems, the
data was always getting stuck.

59
00:03:53,135 --> 00:03:56,315
You gotta ask yourself what's
worse, the data getting stuck

60
00:03:56,435 --> 00:03:58,895
or transmitting inaccurate data.

61
00:03:59,465 --> 00:04:03,065
So both of these things can cause
business disruption and it can

62
00:04:03,065 --> 00:04:04,865
cause the business a lot of money.

63
00:04:05,315 --> 00:04:08,885
So let's dig in, or we will be
digging in just a few minutes

64
00:04:08,885 --> 00:04:10,265
on how we can solve that.

65
00:04:10,295 --> 00:04:13,415
But first, let's get into the
second hidden challenge of

66
00:04:13,415 --> 00:04:16,385
managing iot streaming, right?

67
00:04:16,385 --> 00:04:20,935
And this next thing is about
managing device heterogeneity.

68
00:04:21,460 --> 00:04:25,900
So the primary goal of managing
this diversity is to ensure that all

69
00:04:25,900 --> 00:04:30,880
devices can work together seamlessly
and efficiently towards a common

70
00:04:30,880 --> 00:04:35,500
objective without being bottleneck
by the least capable component.

71
00:04:36,000 --> 00:04:36,840
What does that mean?

72
00:04:36,870 --> 00:04:42,210
While you need a multifaceted technical
approach here to tame the complexity

73
00:04:42,270 --> 00:04:43,710
of all these different systems.

74
00:04:44,210 --> 00:04:49,050
You would need to ensure interoperability,
allowing for different systems to

75
00:04:49,050 --> 00:04:53,520
communicate and exchange information
effectively using standardized interfaces

76
00:04:53,520 --> 00:04:58,560
and protocols, you would have to optimize
performance by assigning tasks of the most

77
00:04:58,560 --> 00:05:03,420
suitable processor, or adapt the workloads
to improve scalability and flexibility.

78
00:05:03,810 --> 00:05:06,900
And of course, you would have to handle
different challenges like addressing

79
00:05:06,900 --> 00:05:08,640
issues with varying performance.

80
00:05:09,180 --> 00:05:13,240
Security vulnerabilities arising
from complexity and power

81
00:05:13,240 --> 00:05:15,219
constraints in different devices.

82
00:05:15,719 --> 00:05:19,950
But the reality is that there is a
system, there are system management

83
00:05:20,000 --> 00:05:21,480
challenges as well, such as.

84
00:05:22,355 --> 00:05:25,535
Performance prediction accurately
predicting the performance of

85
00:05:25,535 --> 00:05:29,705
an application on a complex
heterogeneous system is very difficult.

86
00:05:30,125 --> 00:05:33,185
You have those increased security
vulnerabilities, the increased

87
00:05:33,185 --> 00:05:37,355
complexity in shared resources
and heterogeneous systems expands

88
00:05:37,355 --> 00:05:39,335
the potential attack service.

89
00:05:39,695 --> 00:05:44,390
And then you get data integration
and consistency in environments

90
00:05:44,390 --> 00:05:49,235
involving multiple data sources,
whether it's databases, iot, sensors.

91
00:05:49,610 --> 00:05:54,710
Challenges conclude everything from
schema mismatches, data inconsistency

92
00:05:54,710 --> 00:05:59,360
and ensuring data accuracy and
consistency across all platforms.

93
00:06:00,110 --> 00:06:06,010
So the final challenge of streaming
IOT data is, being able to look

94
00:06:06,100 --> 00:06:10,810
at data providence as enterprises
expand their footprint in terms of

95
00:06:10,810 --> 00:06:14,650
geography, programming, language
languages, deployment types, data

96
00:06:14,650 --> 00:06:16,900
types, and data points of entry.

97
00:06:16,900 --> 00:06:17,170
It's.

98
00:06:17,695 --> 00:06:20,545
Becoming harder to trust the
data's integrity, or even

99
00:06:20,545 --> 00:06:24,805
establish a variable chain of
custody in regulated environments.

100
00:06:25,165 --> 00:06:26,845
Who actually gathered that data?

101
00:06:26,905 --> 00:06:28,195
When was it gathered?

102
00:06:28,285 --> 00:06:29,815
Where was it gathered from?

103
00:06:29,995 --> 00:06:31,465
What did the handoff look like?

104
00:06:31,615 --> 00:06:33,565
What server was used?

105
00:06:33,565 --> 00:06:35,965
Was it was the server actually secure?

106
00:06:36,215 --> 00:06:37,805
What systems were involved?

107
00:06:37,835 --> 00:06:39,725
Which users had access to it?

108
00:06:39,755 --> 00:06:40,955
Did they transform it?

109
00:06:41,135 --> 00:06:43,655
Was the data tested who touched it last?

110
00:06:44,155 --> 00:06:50,475
It's crazy because data Pro, pro
providence data provenance is

111
00:06:50,475 --> 00:06:52,665
more necessary than ever before.

112
00:06:52,665 --> 00:06:56,895
It ensures that there is no
unauthorized access, alteration,

113
00:06:56,895 --> 00:06:58,665
or contamination of data.

114
00:06:58,965 --> 00:07:01,125
It verifies the origin of the data.

115
00:07:01,125 --> 00:07:06,105
It ensures that audits will be passed and
paints a clear picture of who is liable

116
00:07:06,105 --> 00:07:08,565
for the data at any given point in time.

117
00:07:08,685 --> 00:07:10,335
Talked about a lot of
different challenges.

118
00:07:10,935 --> 00:07:12,375
What are the solutions?

119
00:07:12,765 --> 00:07:16,725
How do you tame this iot giant that
has invaded the streaming world?

120
00:07:17,295 --> 00:07:21,915
As iot data grows and flows continuously
from an expanding universe of devices

121
00:07:21,915 --> 00:07:26,565
and system, the real challenge becomes
achieving true end-to-end visibility

122
00:07:26,565 --> 00:07:28,245
across your entire data stream.

123
00:07:28,785 --> 00:07:31,275
This is where discipline
data management stops being

124
00:07:31,335 --> 00:07:33,375
optional and becomes essential.

125
00:07:33,735 --> 00:07:37,395
It's where chaos and failover
testing turn into a routine practice.

126
00:07:38,205 --> 00:07:42,285
Where fast data exploration becomes a
critical workflow and where security

127
00:07:42,285 --> 00:07:46,815
and governance are designed into the
system, not patched on later, this

128
00:07:46,815 --> 00:07:48,255
is where the shift should happen.

129
00:07:48,375 --> 00:07:52,185
This is where your team shifts
from reacting to emergencies

130
00:07:52,275 --> 00:07:53,985
to starting to anticipate them.

131
00:07:54,555 --> 00:08:00,325
So what do you need in place to
be able to anticipate any of the

132
00:08:00,325 --> 00:08:01,825
emergencies that might come up?

133
00:08:02,410 --> 00:08:05,170
How do you deal with these
hidden challenges when it

134
00:08:05,170 --> 00:08:09,370
comes to data integrity, device
heterogeneity, and data provenance?

135
00:08:09,560 --> 00:08:12,140
You need an end-to-end solution.

136
00:08:12,560 --> 00:08:17,270
You need something that's going to
give you a unified look, a unified

137
00:08:17,270 --> 00:08:22,730
control plane to unlock all of Kafka's
full potential, a platform that

138
00:08:22,730 --> 00:08:27,110
provides that single pane of glass
so you can see the entire ecosystem.

139
00:08:27,530 --> 00:08:33,049
Transforming it into a managed, secure,
and efficient enterprise asset, and

140
00:08:33,049 --> 00:08:35,630
that's how Kafka streaming should be used.

141
00:08:35,870 --> 00:08:37,520
So what does that start with?

142
00:08:37,520 --> 00:08:41,299
You've got to have this centralized
management Kafka component.

143
00:08:41,689 --> 00:08:45,229
What that means is you can see
there's four components of managing

144
00:08:45,229 --> 00:08:49,175
data and there's intelligence
scale of how you use that data.

145
00:08:50,045 --> 00:08:53,765
So when we're talking about the core
components of managing data, that's

146
00:08:53,765 --> 00:08:58,834
everything that you want to see in one
central place from topics, schemas,

147
00:08:58,834 --> 00:09:01,324
consumer groups, Kafka Connect, et cetera.

148
00:09:02,045 --> 00:09:07,295
And at the same time, you want
to be able to add intelligence at

149
00:09:07,295 --> 00:09:08,854
scale because there is so much data.

150
00:09:09,380 --> 00:09:14,270
So you can have metadata and labels
to make resources easy to discover and

151
00:09:14,270 --> 00:09:19,070
organize while real time cluster and
broker monitoring provides that immediate

152
00:09:19,070 --> 00:09:21,230
insight into health and performance.

153
00:09:22,040 --> 00:09:25,520
You also need to be able to
give developers what they

154
00:09:25,520 --> 00:09:27,950
need so they can move quickly.

155
00:09:28,000 --> 00:09:29,170
So how do you do that?

156
00:09:29,200 --> 00:09:31,770
You need something that has
self-service tooling in it.

157
00:09:32,485 --> 00:09:36,565
So they can abstract away the
complexity that comes with Kafka, but

158
00:09:36,565 --> 00:09:40,015
you can provide developers with the
tools they need to build tests and

159
00:09:40,015 --> 00:09:46,375
debug applications quickly and safely
without needing to be experts on Kafka.

160
00:09:47,155 --> 00:09:51,385
Of course, it's important that we need
to have end to end zero trust framework

161
00:09:51,385 --> 00:09:54,145
when it comes our data in motion, right?

162
00:09:54,145 --> 00:09:56,005
You need data quality and governance.

163
00:09:56,005 --> 00:09:57,360
You need to be able to have traffic.

164
00:09:58,224 --> 00:10:01,885
Traffic and cluster protection
and make sure that you know who's

165
00:10:01,885 --> 00:10:06,444
entering and using the data and who
has access to what data when it comes

166
00:10:06,444 --> 00:10:10,255
to ensuring compliance and protecting
your most critical data streams.

167
00:10:11,125 --> 00:10:11,995
How can you do that?

168
00:10:11,995 --> 00:10:13,105
So how do you get that?

169
00:10:13,105 --> 00:10:13,915
Let's break it down.

170
00:10:13,915 --> 00:10:16,885
You need to make sure you
see who does what, right?

171
00:10:16,885 --> 00:10:20,665
So giving, having that granular
access control, auditing,

172
00:10:20,665 --> 00:10:22,445
and data centric security.

173
00:10:22,940 --> 00:10:27,940
So on the left hand side here, it's about,
identity and access management teams

174
00:10:27,940 --> 00:10:33,430
can now, define find grade role-based
access for different users and groups.

175
00:10:33,489 --> 00:10:38,800
You can integrate with SSO and
existing L DAP or ad systems and

176
00:10:38,800 --> 00:10:41,260
automatically manage users over time.

177
00:10:41,709 --> 00:10:42,265
Pretty cool.

178
00:10:42,790 --> 00:10:45,969
On the right hand side, you've
got data protection and auditing.

179
00:10:46,524 --> 00:10:50,844
Every user and application is
captured in immutable audit logs

180
00:10:50,844 --> 00:10:52,494
that can actually be exported.

181
00:10:52,974 --> 00:10:56,904
Data is protected end to end with
message encryption, including

182
00:10:56,904 --> 00:11:00,774
field level and schema based
encryption for sensitive data.

183
00:11:01,104 --> 00:11:06,414
And when teams need controlled access,
you've got dynamic data masking, ensuring

184
00:11:06,414 --> 00:11:12,279
that sensitive information is only visible
to those authorized users or obligations.

185
00:11:13,174 --> 00:11:13,924
Together.

186
00:11:13,924 --> 00:11:17,944
This allows organizations to meet
security and compliance requirements

187
00:11:18,154 --> 00:11:22,894
while still enabling teams to
safely self-service streaming data.

188
00:11:23,734 --> 00:11:25,294
No bottleneck, right?

189
00:11:25,474 --> 00:11:29,854
So how do you actually enforce data
quality before data ever becomes

190
00:11:29,854 --> 00:11:33,339
a problem, while also giving the
enterprise a single source of truth

191
00:11:33,519 --> 00:11:38,224
for all streaming data On the left
hand side, it's about what you can do.

192
00:11:38,224 --> 00:11:40,954
You can ensure data is valid
before it enters the system.

193
00:11:41,854 --> 00:11:45,574
Teams can define quality gates,
enforce schema usage, and

194
00:11:45,574 --> 00:11:49,534
apply advanced validation rules
to make sure only compliant,

195
00:11:49,834 --> 00:11:51,874
well-structured data is produced.

196
00:11:52,144 --> 00:11:56,494
If data doesn't meet those standards,
it's blocked at the source, preventing

197
00:11:56,494 --> 00:11:58,774
bad data from propagating downstream.

198
00:11:59,734 --> 00:12:03,844
On the right hand side, it's about
maintaining an enterprise wide catalog.

199
00:12:03,994 --> 00:12:08,644
This gives teams searchable, centralized
view of data streams, topics, and schemas.

200
00:12:09,094 --> 00:12:13,684
It also enables secure sharing through
data sharing in partner zones and

201
00:12:13,684 --> 00:12:19,684
supports chargeback by tracking resource
usage by app, by team or application.

202
00:12:20,254 --> 00:12:21,124
Together.

203
00:12:21,184 --> 00:12:25,204
This allows organizations to scale
streaming with confidence, protecting

204
00:12:25,204 --> 00:12:30,809
data quality, while improving visibility
governance and collaboration across teams.

205
00:12:31,784 --> 00:12:37,124
Finally, it's about protecting cluster
stability by putting automated guardrails

206
00:12:37,124 --> 00:12:42,344
around how data is produced, consumed,
and managed on the topic management

207
00:12:42,344 --> 00:12:48,084
side, all create or alter requests go
through a governed workflow, ensuring

208
00:12:48,084 --> 00:12:52,194
changes are reviewed and approved
before they impact the cluster.

209
00:12:52,524 --> 00:12:57,654
Topics can also be safely renamed without
disrupting producers or consumers.

210
00:12:58,404 --> 00:13:02,574
For producers policies enforce that
all messages have a valid schema

211
00:13:02,844 --> 00:13:07,284
and apply rate limiting to prevent
runaway producers from overwhelming

212
00:13:07,284 --> 00:13:09,414
brokers and causing outages.

213
00:13:09,714 --> 00:13:13,914
And on the consumer side, group
level policies control how consumers

214
00:13:13,914 --> 00:13:18,354
connects and behave limiting group
membership, join rates and commit

215
00:13:18,354 --> 00:13:23,304
offsets to prevent misbehaving consumers
from destabilizing the cluster.

216
00:13:23,889 --> 00:13:24,549
Together.

217
00:13:24,579 --> 00:13:30,249
These automated policies shift Kafka
operations from reactive firefighting

218
00:13:30,399 --> 00:13:35,829
to proactive protection, keeping
clusters stable as usage scales.

219
00:13:36,459 --> 00:13:39,759
So let me leave you with
this one final thought.

220
00:13:40,209 --> 00:13:43,074
What's your disciplined
approach to data management?

221
00:13:43,074 --> 00:13:45,324
Do you have a disciplined
approach to data management?

222
00:13:46,224 --> 00:13:48,444
Iot data streaming is only growing.

223
00:13:48,474 --> 00:13:54,074
There's more and more data that is
coming down that needs to be used in for

224
00:13:54,074 --> 00:13:58,124
different business models and be used
to actually drive different decisions.

225
00:13:58,694 --> 00:14:04,094
So as the IO OT bubble keeps expanding,
do you and your team have the right

226
00:14:04,094 --> 00:14:10,244
tools and foundation to ensure that this
increasingly fragile bubble doesn't burst?

227
00:14:10,844 --> 00:14:11,624
Thanks for your time.

