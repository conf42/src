<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Prompt Engineering: An Art, a Science, or Your Next Job Title?</title>
    <meta name="description" content="Master your prompt-fu!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Maxim%20Salnikov_prompt.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Prompt Engineering: An Art, a Science, or Your Next Job Title? | Conf42"/>
    <meta property="og:description" content="I will introduce prompt engineering as an emerging discipline with its own methodologies, tools, and best practices. Expect lots of examples that will help you write ideal prompts for all occasions."/>
    <meta property="og:url" content="https://conf42.com/Prompt_Engineering_2024_Maxim_Salnikov_promptengineering_ai_job"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/SRE2025_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Site Reliability Engineering (SRE) 2025
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2025-04-17
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/sre2025" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2025">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2025">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2025">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2025">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2025">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2025">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2025">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2025">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2025">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/robotics2025">
                            Robotics
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2025">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2025">
                            Internet of Things (IoT)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="https://conf42.circle.so/">
                            <b>Community platform login</b>
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Join the community!
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #749BC2;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Prompt Engineering 2024 - Online
            </h1>

            <h2 class="text-white">
              
              <time datetime="2024-11-14">November 14 2024</time>
              
              - premiere 5PM GMT
              
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- Master your prompt-fu!
 -->
              <script>
                const event_date = new Date("2024-11-14T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2024-11-14T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              <a href="#register" class="btn btn-primary shadow lift me-1 mb-3">
                <i class="fe fe-user-check me-2"></i>
                Subscribe to watch
              </a>
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "AtaSBySKKH4"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrAVCqb1r_qSxiIxgr09QRvk" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "hello everyone!", "timestamp": "00:00:00,000", "timestamp_s": 0.0}, {"text": "Let\u0027s talk about prompt engineering.", "timestamp": "00:00:00,930", "timestamp_s": 0.0}, {"text": "What is it?", "timestamp": "00:00:03,400", "timestamp_s": 3.0}, {"text": "Art or science or maybe your next job title?", "timestamp": "00:00:04,670", "timestamp_s": 4.0}, {"text": "I\u0027m Maxim Selnikov.", "timestamp": "00:00:08,550", "timestamp_s": 8.0}, {"text": "I\u0027m based in Oslo, Norway, where I work in Microsoft, where I help", "timestamp": "00:00:09,959", "timestamp_s": 9.0}, {"text": "developers to succeed with our cloud technologies, tools for developers.", "timestamp": "00:00:14,329", "timestamp_s": 14.0}, {"text": "And, everything related AI.", "timestamp": "00:00:19,575", "timestamp_s": 19.0}, {"text": "I personally, a developer myself, and I build applications", "timestamp": "00:00:22,795", "timestamp_s": 22.0}, {"text": "since, late 90s of last century.", "timestamp": "00:00:26,545", "timestamp_s": 26.0}, {"text": "And I\u0027m big fan of the developer communities.", "timestamp": "00:00:31,025", "timestamp_s": 31.0}, {"text": "In Oslo, where I based, I founded a couple of conferences and, run a few meetups.", "timestamp": "00:00:33,495", "timestamp_s": 33.0}, {"text": "My favorite topics to present about are web development, all kinds of", "timestamp": "00:00:39,164", "timestamp_s": 39.0}, {"text": "cloud development, and of course, AI.", "timestamp": "00:00:43,854", "timestamp_s": 43.0}, {"text": "Everything related to generative AI, including prompt engineering.", "timestamp": "00:00:45,894", "timestamp_s": 45.0}, {"text": "I found multiple estimations on, the number of people who", "timestamp": "00:00:51,144", "timestamp_s": 51.0}, {"text": "use generative AI on our planet.", "timestamp": "00:00:55,514", "timestamp_s": 55.0}, {"text": "And, the most conservative ones say that it\u0027s well over 1 billion people.", "timestamp": "00:00:58,364", "timestamp_s": 58.0}, {"text": "That\u0027s a number, right?", "timestamp": "00:01:05,464", "timestamp_s": 65.0}, {"text": "What do these people use generative AI for?", "timestamp": "00:01:07,414", "timestamp_s": 67.0}, {"text": "just for us to start the discussion.", "timestamp": "00:01:10,994", "timestamp_s": 70.0}, {"text": "I identified three large application areas.", "timestamp": "00:01:13,034", "timestamp_s": 73.0}, {"text": "First of all, everything related to productivity, your personal", "timestamp": "00:01:17,484", "timestamp_s": 77.0}, {"text": "productivity, your business productivity, everything related to usage of", "timestamp": "00:01:21,044", "timestamp_s": 81.0}, {"text": "generative AI in education and science, and many other areas where we can", "timestamp": "00:01:25,754", "timestamp_s": 85.0}, {"text": "really get super powered by gen AI.", "timestamp": "00:01:30,574", "timestamp_s": 90.0}, {"text": "Next creativity.", "timestamp": "00:01:33,974", "timestamp_s": 93.0}, {"text": "After all, it\u0027s called generative AI, right?", "timestamp": "00:01:35,914", "timestamp_s": 95.0}, {"text": "So it can generate for us many, very interesting things.", "timestamp": "00:01:38,184", "timestamp_s": 98.0}, {"text": "so we can really feel ourselves super creative, but, there should be someone", "timestamp": "00:01:41,414", "timestamp_s": 101.0}, {"text": "who is building these applications for us.", "timestamp": "00:01:46,544", "timestamp_s": 106.0}, {"text": "Someone who is, actually, Constructing this UIs that we can use both", "timestamp": "00:01:49,494", "timestamp_s": 109.0}, {"text": "for productivity and creativity.", "timestamp": "00:01:54,444", "timestamp_s": 114.0}, {"text": "And yeah, here I\u0027m talking about people who are actually building these AI", "timestamp": "00:01:56,844", "timestamp_s": 116.0}, {"text": "infused or intelligent applications.", "timestamp": "00:02:02,274", "timestamp_s": 122.0}, {"text": "And I\u0027ll do my best to make this particular session the most useful", "timestamp": "00:02:05,314", "timestamp_s": 125.0}, {"text": "for exactly this category of people.", "timestamp": "00:02:10,004", "timestamp_s": 130.0}, {"text": "people, but at the same time, I\u0027m sure that even if you don\u0027t have any intention", "timestamp": "00:02:12,534", "timestamp_s": 132.0}, {"text": "to build your, application, you can still get lots of useful information from", "timestamp": "00:02:18,124", "timestamp_s": 138.0}, {"text": "this session because, it contains some, general recommendations, some general,", "timestamp": "00:02:23,694", "timestamp_s": 143.0}, {"text": "practices for your improved Prompting.", "timestamp": "00:02:28,494", "timestamp_s": 148.0}, {"text": "And, after this session, when you open any kind of generative AI powered", "timestamp": "00:02:31,984", "timestamp_s": 151.0}, {"text": "service like ChatGPT or similar, you will know better how to communicate.", "timestamp": "00:02:36,384", "timestamp_s": 156.0}, {"text": "what is common thing in all these scenarios?", "timestamp": "00:02:42,594", "timestamp_s": 162.0}, {"text": "Let\u0027s look how we, for example, ask Meet Journey to create a new image for us, or", "timestamp": "00:02:48,004", "timestamp_s": 168.0}, {"text": "how we start conversation with Microsoft 365 Word, on, creating some very smart", "timestamp": "00:02:54,374", "timestamp_s": 174.0}, {"text": "and solid template for our document.", "timestamp": "00:03:01,084", "timestamp_s": 181.0}, {"text": "Or let\u0027s look how we communicate with a GitHub copilot for improving", "timestamp": "00:03:04,954", "timestamp_s": 184.0}, {"text": "our code, not even saying about the chat GPT as a service itself,", "timestamp": "00:03:11,234", "timestamp_s": 191.0}, {"text": "where we literally chat with this.", "timestamp": "00:03:17,014", "timestamp_s": 197.0}, {"text": "Of course, one common thing is the way we interact.", "timestamp": "00:03:20,444", "timestamp_s": 200.0}, {"text": "And, the fact that it all starts with the prompt with, some text that we write.", "timestamp": "00:03:24,534", "timestamp_s": 204.0}, {"text": "And, even though model landscape is emerging and there is, next amazing", "timestamp": "00:03:31,554", "timestamp_s": 211.0}, {"text": "model is, released while I speak, while you watch this session, and there are", "timestamp": "00:03:37,894", "timestamp_s": 217.0}, {"text": "already, multiple models available.", "timestamp": "00:03:43,144", "timestamp_s": 223.0}, {"text": "And, It\u0027s becoming more and more complex.", "timestamp": "00:03:45,694", "timestamp_s": 225.0}, {"text": "There are large language models.", "timestamp": "00:03:48,434", "timestamp_s": 228.0}, {"text": "There is new generation of small language models also multi modality is really", "timestamp": "00:03:50,254", "timestamp_s": 230.0}, {"text": "available now even though There are different kinds and types of models.", "timestamp": "00:03:57,854", "timestamp_s": 237.0}, {"text": "Some of them are general some of them specialized Still", "timestamp": "00:04:04,124", "timestamp_s": 244.0}, {"text": "we all go back to prompts", "timestamp": "00:04:07,864", "timestamp_s": 247.0}, {"text": "This is why I consider prompt engineering as a separate discipline or at least as", "timestamp": "00:04:11,544", "timestamp_s": 251.0}, {"text": "an essential skill of many people, not only developers, but, many technical", "timestamp": "00:04:18,544", "timestamp_s": 258.0}, {"text": "people, many people on business positions.", "timestamp": "00:04:23,754", "timestamp_s": 263.0}, {"text": "and, what is this after all, this is a process of, designing", "timestamp": "00:04:27,294", "timestamp_s": 267.0}, {"text": "prompts of, tuning these prompts.", "timestamp": "00:04:31,364", "timestamp_s": 271.0}, {"text": "And the further optimization, while we keep satisfaction on a satisfaction", "timestamp": "00:04:34,049", "timestamp_s": 274.0}, {"text": "level on the results that we get back from the large language models.", "timestamp": "00:04:41,149", "timestamp_s": 281.0}, {"text": "And of course, it\u0027s also important to, to follow cost efficiency, because in", "timestamp": "00:04:45,999", "timestamp_s": 285.0}, {"text": "many cases, we talk about some paid services when we talk specifically about", "timestamp": "00:04:51,039", "timestamp_s": 291.0}, {"text": "like high and large language models.", "timestamp": "00:04:57,369", "timestamp_s": 297.0}, {"text": "Let\u0027s look closer at the prompt components or if you wish we can call it prompt", "timestamp": "00:05:00,269", "timestamp_s": 300.0}, {"text": "anatomy in many cases Again, at least when we are the users who chat with", "timestamp": "00:05:05,549", "timestamp_s": 305.0}, {"text": "chat GPT We don\u0027t really think too much about How we structure our prompt.", "timestamp": "00:05:12,829", "timestamp_s": 312.0}, {"text": "We just communicate and as long as we are happy with the results.", "timestamp": "00:05:19,944", "timestamp_s": 319.0}, {"text": "We don\u0027t want to Dive deeper into all these nitty gritty details That\u0027s fine.", "timestamp": "00:05:24,754", "timestamp_s": 324.0}, {"text": "But when you build your AI infused application It\u0027s really good idea", "timestamp": "00:05:30,724", "timestamp_s": 330.0}, {"text": "for you to know how that works from inside and again the same ideas and", "timestamp": "00:05:34,684", "timestamp_s": 334.0}, {"text": "the techniques are still applicable to your like day to day conversations", "timestamp": "00:05:40,224", "timestamp_s": 340.0}, {"text": "with any generative AI powered service.", "timestamp": "00:05:44,464", "timestamp_s": 344.0}, {"text": "So yeah, let me, introduce how that works.", "timestamp": "00:05:47,214", "timestamp_s": 347.0}, {"text": "in many cases we have very clear and, concise and straightforward instruction.", "timestamp": "00:05:51,544", "timestamp_s": 351.0}, {"text": "What we want to get from this particular call to our, generative AI", "timestamp": "00:05:59,104", "timestamp_s": 359.0}, {"text": "service or, more technically precise.", "timestamp": "00:06:04,314", "timestamp_s": 364.0}, {"text": "The two large language model exposed by one of these services.", "timestamp": "00:06:06,769", "timestamp_s": 366.0}, {"text": "and yeah, let me illustrate it by this example what on the screen.", "timestamp": "00:06:10,809", "timestamp_s": 370.0}, {"text": "Let\u0027s imagine that we are building applications for marketing automation", "timestamp": "00:06:14,629", "timestamp_s": 374.0}, {"text": "that gives us Nice, at least drafts, maybe, or maybe ready to go emails", "timestamp": "00:06:19,609", "timestamp_s": 379.0}, {"text": "that, share details on some new products that we either produce or sell,", "timestamp": "00:06:25,439", "timestamp_s": 385.0}, {"text": "or, it\u0027s again, it\u0027s not obligatory as in role of developers, you can,", "timestamp": "00:06:30,619", "timestamp_s": 390.0}, {"text": "use the same, ideas and techniques when you just, use ready to ready", "timestamp": "00:06:34,969", "timestamp_s": 394.0}, {"text": "products, with, some prompts available.", "timestamp": "00:06:39,549", "timestamp_s": 399.0}, {"text": "Instruction.", "timestamp": "00:06:42,689", "timestamp_s": 402.0}, {"text": "Of course, we have to also provide some primary data about this product itself.", "timestamp": "00:06:43,679", "timestamp_s": 403.0}, {"text": "Also, we can provide some context or we can call it secondary", "timestamp": "00:06:48,449", "timestamp_s": 408.0}, {"text": "data about tone we expect.", "timestamp": "00:06:51,859", "timestamp_s": 411.0}, {"text": "In this particular situation, we want to be friendly and exciting, but for", "timestamp": "00:06:54,399", "timestamp_s": 414.0}, {"text": "different scenarios, there could be different ways of, exact, narrative", "timestamp": "00:06:59,529", "timestamp_s": 419.0}, {"text": "that we expect from the model.", "timestamp": "00:07:05,309", "timestamp_s": 425.0}, {"text": "Also, we, identify, we set the format, we define the format,", "timestamp": "00:07:07,139", "timestamp_s": 427.0}, {"text": "we want to get, answer back.", "timestamp": "00:07:12,889", "timestamp_s": 432.0}, {"text": "And in this particular situation, it\u0027s definitely something that, that is", "timestamp": "00:07:15,439", "timestamp_s": 435.0}, {"text": "in the middle of, overall, developer chain of, of this product, because", "timestamp": "00:07:19,149", "timestamp_s": 439.0}, {"text": "you see that we expect not just text, but, Jason, definitely this will", "timestamp": "00:07:24,109", "timestamp_s": 444.0}, {"text": "be somehow processed by the next steps of, our, for example, backend.", "timestamp": "00:07:28,319", "timestamp_s": 448.0}, {"text": "So we say that we want to get back.", "timestamp": "00:07:32,639", "timestamp_s": 452.0}, {"text": "Not just subject and the body, but JSON object with these fields.", "timestamp": "00:07:35,394", "timestamp_s": 455.0}, {"text": "Also, we provide an example and in this particular situation,", "timestamp": "00:07:40,844", "timestamp_s": 460.0}, {"text": "it plays at least two roles.", "timestamp": "00:07:44,464", "timestamp_s": 464.0}, {"text": "First of all, yeah, it demonstrates the model, that kind of text, maybe length", "timestamp": "00:07:46,904", "timestamp_s": 466.0}, {"text": "of the text and again tone and old structure That might be a good fit for us.", "timestamp": "00:07:54,804", "timestamp_s": 474.0}, {"text": "And also we double down on the format.", "timestamp": "00:08:00,744", "timestamp_s": 480.0}, {"text": "So this is why, we, put example exactly in the format that we", "timestamp": "00:08:03,254", "timestamp_s": 483.0}, {"text": "described on the line above.", "timestamp": "00:08:08,794", "timestamp_s": 488.0}, {"text": "Why do we need this duplication?", "timestamp": "00:08:10,854", "timestamp_s": 490.0}, {"text": "Stay tuned.", "timestamp": "00:08:13,124", "timestamp_s": 493.0}, {"text": "I will explain why that might be useful.", "timestamp": "00:08:13,824", "timestamp_s": 493.0}, {"text": "So this is how we, human or developers look at the prompt.", "timestamp": "00:08:16,484", "timestamp_s": 496.0}, {"text": "And this is how large language model or LLM understands", "timestamp": "00:08:21,684", "timestamp_s": 501.0}, {"text": "the same prompt on its end.", "timestamp": "00:08:26,444", "timestamp_s": 506.0}, {"text": "And that it\u0027s split into tokens.", "timestamp": "00:08:29,164", "timestamp_s": 509.0}, {"text": "And actually, tokenization is the first step.", "timestamp": "00:08:32,424", "timestamp_s": 512.0}, {"text": "procedure that happens when you send prompt to LLM.", "timestamp": "00:08:34,799", "timestamp_s": 514.0}, {"text": "So large language model understands your, your prompt in form of tokens.", "timestamp": "00:08:39,269", "timestamp_s": 519.0}, {"text": "And, when time has come to generate answer for you, it also generates", "timestamp": "00:08:45,109", "timestamp_s": 525.0}, {"text": "this recursively token by token.", "timestamp": "00:08:50,109", "timestamp_s": 530.0}, {"text": "On this example, that, many words.", "timestamp": "00:08:53,119", "timestamp_s": 533.0}, {"text": "are equal to one token, right?", "timestamp": "00:08:55,969", "timestamp_s": 535.0}, {"text": "One word equals one token.", "timestamp": "00:09:00,019", "timestamp_s": 540.0}, {"text": "But of course, the real situation is much more, complex, right?", "timestamp": "00:09:01,399", "timestamp_s": 541.0}, {"text": "So you see that, some words, take.", "timestamp": "00:09:06,059", "timestamp_s": 546.0}, {"text": "Multiple tokens and exact implementation of how that split", "timestamp": "00:09:09,259", "timestamp_s": 549.0}, {"text": "is up to implementation of a large language model, or it\u0027s a tokenizer.", "timestamp": "00:09:14,429", "timestamp_s": 554.0}, {"text": "There is no any kind of strict rule how many, Words equals to how many tokens", "timestamp": "00:09:20,669", "timestamp_s": 560.0}, {"text": "or how many characters equals to how many tokens we can very Approximately", "timestamp": "00:09:26,409", "timestamp_s": 566.0}, {"text": "say that at least for this generation of LLMs and for English texts 100 tokens", "timestamp": "00:09:31,289", "timestamp_s": 571.0}, {"text": "is Approximately, equal to 75 words.", "timestamp": "00:09:39,430", "timestamp_s": 579.0}, {"text": "so that means like one token is maybe around four characters in English text.", "timestamp": "00:09:44,309", "timestamp_s": 584.0}, {"text": "For different languages, the ratio is completely different.", "timestamp": "00:09:50,009", "timestamp_s": 590.0}, {"text": "You might ask me, why do we ever need to have this knowledge about tokens?", "timestamp": "00:09:53,599", "timestamp_s": 593.0}, {"text": "Because Definitely when we are in user role, this is hidden", "timestamp": "00:09:58,809", "timestamp_s": 598.0}, {"text": "completely under the hood for us.", "timestamp": "00:10:02,899", "timestamp_s": 602.0}, {"text": "We communicate with sentences in, sentences back.", "timestamp": "00:10:04,749", "timestamp_s": 604.0}, {"text": "when we are in developer role, Also, this is not that visible at the", "timestamp": "00:10:09,059", "timestamp_s": 609.0}, {"text": "beginning of, when you start building your AI application, but very soon as", "timestamp": "00:10:15,209", "timestamp_s": 615.0}, {"text": "a developer, you will understand super important meaning of tokenization.", "timestamp": "00:10:20,299", "timestamp_s": 620.0}, {"text": "And this is why, because number of tokens in your input and", "timestamp": "00:10:25,489", "timestamp_s": 625.0}, {"text": "expected output, first of", "timestamp": "00:10:31,039", "timestamp_s": 631.0}, {"text": "Cost of, this particular call, if we talk about some hosted,", "timestamp": "00:10:33,539", "timestamp_s": 633.0}, {"text": "LLMs hosted by some vendors.", "timestamp": "00:10:38,109", "timestamp_s": 638.0}, {"text": "And also there is technical limitation on number of tokens you can send and receive.", "timestamp": "00:10:40,219", "timestamp_s": 640.0}, {"text": "on this slide, there is some screenshot from, pricing for", "timestamp": "00:10:48,029", "timestamp_s": 648.0}, {"text": "Azure OpenAI, services for GPT 4.", "timestamp": "00:10:53,669", "timestamp_s": 653.0}, {"text": "0 model and 0.", "timestamp": "00:10:57,599", "timestamp_s": 657.0}, {"text": "1 preview model.", "timestamp": "00:10:58,909", "timestamp_s": 658.0}, {"text": "And for legacy purposes, I also listed price for GPT 4.", "timestamp": "00:11:00,269", "timestamp_s": 660.0}, {"text": "And, that, we are in good situation as developers because prices.", "timestamp": "00:11:04,639", "timestamp_s": 664.0}, {"text": "are going down.", "timestamp": "00:11:09,894", "timestamp_s": 669.0}, {"text": "And first of all, yeah, that it\u0027s priced by 1 million tokens.", "timestamp": "00:11:11,634", "timestamp_s": 671.0}, {"text": "So every single token for a few number of calls, maybe this is", "timestamp": "00:11:15,034", "timestamp_s": 675.0}, {"text": "not something really crucial.", "timestamp": "00:11:18,054", "timestamp_s": 678.0}, {"text": "The price becomes really different when we talk about some scale usage, but, Yeah.", "timestamp": "00:11:20,614", "timestamp_s": 680.0}, {"text": "Yeah.", "timestamp": "00:11:25,252", "timestamp_s": 685.0}, {"text": "Still very important to keep eye on it.", "timestamp": "00:11:25,919", "timestamp_s": 685.0}, {"text": "And that in GPT 4 it started for 60 per million and suddenly GPT 4.", "timestamp": "00:11:28,549", "timestamp_s": 688.0}, {"text": "0 that is more capable, more performant model.", "timestamp": "00:11:36,429", "timestamp_s": 696.0}, {"text": "It\u0027s many times more Cheaper and this is general trend.", "timestamp": "00:11:39,209", "timestamp_s": 699.0}, {"text": "there is newer and newer technologies from outside of providers of LLM services and", "timestamp": "00:11:44,639", "timestamp_s": 704.0}, {"text": "yeah, so it\u0027s it\u0027s very good, especially for example startups they have to", "timestamp": "00:11:52,289", "timestamp_s": 712.0}, {"text": "recalculate their economy and recalculate in a positive way and next column I want", "timestamp": "00:11:56,049", "timestamp_s": 716.0}, {"text": "to emphasize on this screenshot is context and this is exactly the Number of tokens", "timestamp": "00:12:02,599", "timestamp_s": 722.0}, {"text": "we can send in one particular request.", "timestamp": "00:12:09,834", "timestamp_s": 729.0}, {"text": "So we see it\u0027s not number of characters or number of words or bytes or whatever.", "timestamp": "00:12:12,754", "timestamp_s": 732.0}, {"text": "No, it\u0027s calculated in tokens.", "timestamp": "00:12:16,984", "timestamp_s": 736.0}, {"text": "This is why it\u0027s very important to understand, at least roughly estimate", "timestamp": "00:12:19,174", "timestamp_s": 739.0}, {"text": "number of tokens in your request.", "timestamp": "00:12:23,944", "timestamp_s": 743.0}, {"text": "prompts.", "timestamp": "00:12:25,504", "timestamp_s": 745.0}, {"text": "as you see, modern models are quite capable and we talk about a hundred", "timestamp": "00:12:26,194", "timestamp_s": 746.0}, {"text": "plus thousands of, of the token.", "timestamp": "00:12:31,824", "timestamp_s": 751.0}, {"text": "So we are not talking about every single, word or white space or, not even about", "timestamp": "00:12:34,464", "timestamp_s": 754.0}, {"text": "sentences, not even about paragraphs.", "timestamp": "00:12:38,924", "timestamp_s": 758.0}, {"text": "Maybe you can, currently send.", "timestamp": "00:12:41,484", "timestamp_s": 761.0}, {"text": "Pages of text as a prompt for these models and again trend is work", "timestamp": "00:12:43,964", "timestamp_s": 763.0}, {"text": "good for us models are Capable to accept more and more tokens, but", "timestamp": "00:12:49,954", "timestamp_s": 769.0}, {"text": "still there is a limitation, right?", "timestamp": "00:12:56,514", "timestamp_s": 776.0}, {"text": "So you cannot send maybe full book at least currently at least at this moment", "timestamp": "00:12:58,114", "timestamp_s": 778.0}, {"text": "So this is why, again, tokenization concept is extremely important and, we\u0027ll,", "timestamp": "00:13:03,259", "timestamp_s": 783.0}, {"text": "have a few more slides on this topic.", "timestamp": "00:13:07,909", "timestamp_s": 787.0}, {"text": "And of course, different providers of, these LLM services provide", "timestamp": "00:13:11,839", "timestamp_s": 791.0}, {"text": "different ways for you to save on, on this usage, all kinds of, caching.", "timestamp": "00:13:15,499", "timestamp_s": 795.0}, {"text": "All kinds of, batch calculations when you don\u0027t need output, here now, but", "timestamp": "00:13:20,424", "timestamp_s": 800.0}, {"text": "you can wait a bit and then, price for this call will be cheaper than regular.", "timestamp": "00:13:24,984", "timestamp_s": 804.0}, {"text": "All kinds of, some dedicated, capacities, that really depends vendor from vendor.", "timestamp": "00:13:29,654", "timestamp_s": 809.0}, {"text": "And you also noticed that, model selection makes real difference.", "timestamp": "00:13:34,744", "timestamp_s": 814.0}, {"text": "First of all on capabilities, on quality of the output, and second on the price.", "timestamp": "00:13:40,049", "timestamp_s": 820.0}, {"text": "And, I offer you to use this simple strategy on model selection.", "timestamp": "00:13:45,939", "timestamp_s": 825.0}, {"text": "First of all, try with the most capable, the most performant,", "timestamp": "00:13:50,769", "timestamp_s": 830.0}, {"text": "and, in many cases also the most expensive model on the market.", "timestamp": "00:13:55,369", "timestamp_s": 835.0}, {"text": "maybe you can, try different providers.", "timestamp": "00:13:58,979", "timestamp_s": 838.0}, {"text": "And, Identify your best prompt that gives you the best possible results", "timestamp": "00:14:01,679", "timestamp_s": 841.0}, {"text": "Then you can try to downgrade to the next cheapest one Next cheapest model", "timestamp": "00:14:06,809", "timestamp_s": 846.0}, {"text": "in some cases you might to might need to fine tune the prompt slightly but", "timestamp": "00:14:13,359", "timestamp_s": 853.0}, {"text": "check the results if Result or completion in technical terms when we talk about", "timestamp": "00:14:17,689", "timestamp_s": 857.0}, {"text": "prompt engineering is the same or better.", "timestamp": "00:14:23,384", "timestamp_s": 863.0}, {"text": "Maybe you can Do next step in this downgrade and try even", "timestamp": "00:14:26,374", "timestamp_s": 866.0}, {"text": "cheaper model Maybe again results will be the same or even better.", "timestamp": "00:14:33,074", "timestamp_s": 873.0}, {"text": "Sometimes that\u0027s also possible and it will give you a chance to save Some", "timestamp": "00:14:36,654", "timestamp_s": 876.0}, {"text": "dollars if no Then you just go up to the previous step where you downgraded from.", "timestamp": "00:14:40,834", "timestamp_s": 880.0}, {"text": "It\u0027s a simple and efficient model selection strategy.", "timestamp": "00:14:47,744", "timestamp_s": 887.0}, {"text": "Also, you can use multiple models and in many cases, if we talk about, not hello", "timestamp": "00:14:51,934", "timestamp_s": 891.0}, {"text": "world AI infused application, this is not just one call to the, LLM, it\u0027s a chain", "timestamp": "00:14:58,404", "timestamp_s": 898.0}, {"text": "of the calls, maybe to multiple models, maybe to even to multiple providers, maybe", "timestamp": "00:15:04,564", "timestamp_s": 904.0}, {"text": "it\u0027s mix between, some hosted services, model hosted by external vendor, model", "timestamp": "00:15:09,804", "timestamp_s": 909.0}, {"text": "hosted by yourself, and maybe even a model that is running straight on your", "timestamp": "00:15:14,634", "timestamp_s": 914.0}, {"text": "device or on your customer device.", "timestamp": "00:15:20,344", "timestamp_s": 920.0}, {"text": "So that, really depends on business scenario.", "timestamp": "00:15:22,284", "timestamp_s": 922.0}, {"text": "So if you have a chance to use multiple models in this particular feature", "timestamp": "00:15:24,614", "timestamp_s": 924.0}, {"text": "of your AI infused application, you can follow a simple rule.", "timestamp": "00:15:31,405", "timestamp_s": 931.0}, {"text": "for.", "timestamp": "00:15:35,694", "timestamp_s": 935.0}, {"text": "Let\u0027s say, complex tasks like generation, you can use expensive ones.", "timestamp": "00:15:36,784", "timestamp_s": 936.0}, {"text": "This is, where they really shine.", "timestamp": "00:15:41,284", "timestamp_s": 941.0}, {"text": "And, yeah, every new generation of the model provides better and", "timestamp": "00:15:43,124", "timestamp_s": 943.0}, {"text": "better results in generation.", "timestamp": "00:15:46,274", "timestamp_s": 946.0}, {"text": "While summarization, classification, categorization.", "timestamp": "00:15:47,844", "timestamp_s": 947.0}, {"text": "This is, again, our days, not that complex tasks anymore, at", "timestamp": "00:15:51,194", "timestamp_s": 951.0}, {"text": "least for a large language models.", "timestamp": "00:15:56,814", "timestamp_s": 956.0}, {"text": "And the cheap ones do them pretty good.", "timestamp": "00:15:58,604", "timestamp_s": 958.0}, {"text": "another strategy.", "timestamp": "00:16:02,564", "timestamp_s": 962.0}, {"text": "is chaining.", "timestamp": "00:16:04,254", "timestamp_s": 964.0}, {"text": "for example, you want to send to a large, expensive model some large amount of text.", "timestamp": "00:16:05,344", "timestamp_s": 965.0}, {"text": "This is how your prompt works, right?", "timestamp": "00:16:13,164", "timestamp_s": 973.0}, {"text": "but you can leverage cheaper and maybe, Like very fast one, maybe your, your", "timestamp": "00:16:15,614", "timestamp_s": 975.0}, {"text": "own fine tuned model to summarize this, large amount of text you are going to", "timestamp": "00:16:22,814", "timestamp_s": 982.0}, {"text": "send to expensive one also might work good for you with a minimal performance,", "timestamp": "00:16:28,184", "timestamp_s": 988.0}, {"text": "decrease, you will save a lot of time.", "timestamp": "00:16:34,174", "timestamp_s": 994.0}, {"text": "some, some budget,", "timestamp": "00:16:35,779", "timestamp_s": 995.0}, {"text": "let\u0027s go back to talking conversation.", "timestamp": "00:16:37,369", "timestamp_s": 997.0}, {"text": "So I hope that I convinced you that, keeping eye on number of tokens", "timestamp": "00:16:39,939", "timestamp_s": 999.0}, {"text": "you send is crucial for, economy of your application, not only for", "timestamp": "00:16:45,339", "timestamp_s": 1005.0}, {"text": "economy, but also you remember that, there was just technical limitation", "timestamp": "00:16:50,179", "timestamp_s": 1010.0}, {"text": "of a number of tokens you send.", "timestamp": "00:16:53,939", "timestamp_s": 1013.0}, {"text": "And after all the shorter prompt, normally the faster you get completion.", "timestamp": "00:16:55,859", "timestamp_s": 1015.0}, {"text": "So it\u0027s, like multiple reasons why you want to, minimize your", "timestamp": "00:17:01,764", "timestamp_s": 1021.0}, {"text": "prompts and how to do this exactly.", "timestamp": "00:17:07,484", "timestamp_s": 1027.0}, {"text": "First of all, very simple rule.", "timestamp": "00:17:10,074", "timestamp_s": 1030.0}, {"text": "have a closer look at.", "timestamp": "00:17:11,474", "timestamp_s": 1031.0}, {"text": "White spaces in your prompt.", "timestamp": "00:17:12,709", "timestamp_s": 1032.0}, {"text": "this is something that we can easily overlook because this is not", "timestamp": "00:17:14,869", "timestamp_s": 1034.0}, {"text": "something that is very visible, right?", "timestamp": "00:17:19,239", "timestamp_s": 1039.0}, {"text": "A couple of extra white spaces.", "timestamp": "00:17:21,289", "timestamp_s": 1041.0}, {"text": "We can just, okay, ignore this.", "timestamp": "00:17:23,389", "timestamp_s": 1043.0}, {"text": "But in reality, Some large language models treat every single", "timestamp": "00:17:25,179", "timestamp_s": 1045.0}, {"text": "whitespace as one extra token.", "timestamp": "00:17:29,634", "timestamp_s": 1049.0}, {"text": "Not a big deal.", "timestamp": "00:17:32,294", "timestamp_s": 1052.0}, {"text": "If you talk about one short request, but if we talk about, I don\u0027t know, hundreds,", "timestamp": "00:17:32,984", "timestamp_s": 1052.0}, {"text": "thousands, millions of requests that might bring some, some difference to", "timestamp": "00:17:36,784", "timestamp_s": 1056.0}, {"text": "your final bill and the end of the month.", "timestamp": "00:17:41,334", "timestamp_s": 1061.0}, {"text": "Next for, different data formats, try different, different ways", "timestamp": "00:17:43,909", "timestamp_s": 1063.0}, {"text": "to like, implement this data.", "timestamp": "00:17:49,089", "timestamp_s": 1069.0}, {"text": "What I mean exactly can be easily illustrated by example of, how", "timestamp": "00:17:52,109", "timestamp_s": 1072.0}, {"text": "we supply, Date in our prompts.", "timestamp": "00:17:56,859", "timestamp_s": 1076.0}, {"text": "I\u0027m technical mind and my first impression was okay The shorter string with the", "timestamp": "00:18:00,559", "timestamp_s": 1080.0}, {"text": "date the better chance that it will take fewer tokens In reality, not at all.", "timestamp": "00:18:05,939", "timestamp_s": 1085.0}, {"text": "So you see on the bottom line, short format of the date takes six tokens, while", "timestamp": "00:18:11,904", "timestamp_s": 1091.0}, {"text": "on the top line, it only three tokens.", "timestamp": "00:18:18,114", "timestamp_s": 1098.0}, {"text": "So sometimes it\u0027s counter intuitive.", "timestamp": "00:18:20,654", "timestamp_s": 1100.0}, {"text": "Again, this is example for one particular large language model.", "timestamp": "00:18:23,264", "timestamp_s": 1103.0}, {"text": "I don\u0027t even remember which one different models can do it differently.", "timestamp": "00:18:27,454", "timestamp_s": 1107.0}, {"text": "experimentation is the only way to really identify what\u0027s your optimal format", "timestamp": "00:18:32,134", "timestamp_s": 1112.0}, {"text": "for this or that type of, your data.", "timestamp": "00:18:37,724", "timestamp_s": 1117.0}, {"text": "if you talk about tabular data, this, straightforward tabular format is,", "timestamp": "00:18:40,984", "timestamp_s": 1120.0}, {"text": "pretty much, space efficient, and what\u0027s very important, understood by LLM.", "timestamp": "00:18:46,434", "timestamp_s": 1126.0}, {"text": "no need for you to, always reproduce JSON like format where you supply", "timestamp": "00:18:50,854", "timestamp_s": 1130.0}, {"text": "caption for every piece of the data.", "timestamp": "00:18:56,494", "timestamp_s": 1136.0}, {"text": "No, just provide some table headers, separated by pipes, or type, tabs, or", "timestamp": "00:18:58,424", "timestamp_s": 1138.0}, {"text": "you\u0027ll find your Personal separator and then rows with the data in vast majority", "timestamp": "00:19:03,124", "timestamp_s": 1143.0}, {"text": "of situations LLM will understand what you mean also Language makes real difference.", "timestamp": "00:19:08,179", "timestamp_s": 1148.0}, {"text": "I already mentioned that English is the most straightforward ways for way", "timestamp": "00:19:14,639", "timestamp_s": 1154.0}, {"text": "for Prompt engineering because it\u0027s the most efficient while we talk about", "timestamp": "00:19:20,969", "timestamp_s": 1160.0}, {"text": "tokenization at least again for the mainstream Large language models I", "timestamp": "00:19:26,509", "timestamp_s": 1166.0}, {"text": "suggest this because vast amount of data, all this, Wikipedia, public books,", "timestamp": "00:19:32,714", "timestamp_s": 1172.0}, {"text": "et cetera, et cetera, data that was used for LLM training was in English.", "timestamp": "00:19:37,924", "timestamp_s": 1177.0}, {"text": "it still understands other languages perfectly well, but if we talk about", "timestamp": "00:19:42,644", "timestamp_s": 1182.0}, {"text": "tokenization, the same, let\u0027s say, Concept same sentence in different", "timestamp": "00:19:46,804", "timestamp_s": 1186.0}, {"text": "language might take more tokens than the one in English i\u0027m not saying", "timestamp": "00:19:50,824", "timestamp_s": 1190.0}, {"text": "that you have to translate everything all single time But again experiment", "timestamp": "00:19:55,964", "timestamp_s": 1195.0}, {"text": "and I want to start introducing some tools and yeah in On next slides, you", "timestamp": "00:20:00,404", "timestamp_s": 1200.0}, {"text": "will see more and more very useful tools frameworks libraries that can", "timestamp": "00:20:06,494", "timestamp_s": 1206.0}, {"text": "simplify your life as a prompt engineer.", "timestamp": "00:20:10,344", "timestamp_s": 1210.0}, {"text": "the first one called LLM Lingua, and this is nothing by but prompt compressor.", "timestamp": "00:20:13,359", "timestamp_s": 1213.0}, {"text": "It\u0027s created by my colleagues from Microsoft, and it\u0027s open source.", "timestamp": "00:20:18,359", "timestamp_s": 1218.0}, {"text": "So you can take this tool and host it locally or on your own server.", "timestamp": "00:20:22,249", "timestamp_s": 1222.0}, {"text": "And yeah, as the description says, it takes your prompt and compresses,", "timestamp": "00:20:27,229", "timestamp_s": 1227.0}, {"text": "but it does it in a very smart way.", "timestamp": "00:20:33,879", "timestamp_s": 1233.0}, {"text": "of course We can try to do it ourselves, right?", "timestamp": "00:20:36,839", "timestamp_s": 1236.0}, {"text": "But, we do not, not always understand which parts of our prompt are crucial", "timestamp": "00:20:39,539", "timestamp_s": 1239.0}, {"text": "for, LLM to really understand what we mean and which parts we can skip.", "timestamp": "00:20:46,589", "timestamp_s": 1246.0}, {"text": "who can help us?", "timestamp": "00:20:52,789", "timestamp_s": 1252.0}, {"text": "Of course, another large language model, right?", "timestamp": "00:20:54,054", "timestamp_s": 1254.0}, {"text": "And in this particular case, this is compact one, maybe we can", "timestamp": "00:20:56,764", "timestamp_s": 1256.0}, {"text": "call it a small language model.", "timestamp": "00:21:00,474", "timestamp_s": 1260.0}, {"text": "And yeah, so this one is used to identify and remove non", "timestamp": "00:21:02,974", "timestamp_s": 1262.0}, {"text": "essential tokens in your prompt.", "timestamp": "00:21:08,054", "timestamp_s": 1268.0}, {"text": "And with some playing, some fine tuning, you can get up to 20 times compression", "timestamp": "00:21:10,954", "timestamp_s": 1270.0}, {"text": "with either zero or minimum performance.", "timestamp": "00:21:17,624", "timestamp_s": 1277.0}, {"text": "loss because definitely, of course, some time is needed for this, first step,", "timestamp": "00:21:21,374", "timestamp_s": 1281.0}, {"text": "LLM or SLM to compress your prompt.", "timestamp": "00:21:27,074", "timestamp_s": 1287.0}, {"text": "But on the other hand, prompt will be shorter and, it might happen that you save", "timestamp": "00:21:29,824", "timestamp_s": 1289.0}, {"text": "some milliseconds just because of that.", "timestamp": "00:21:35,154", "timestamp_s": 1295.0}, {"text": "if we take prompt from our first example about, email.", "timestamp": "00:21:37,874", "timestamp_s": 1297.0}, {"text": "about, new headphones and run it through this LLM Lingua.", "timestamp": "00:21:42,999", "timestamp_s": 1302.0}, {"text": "I did it without any fine tuning, just as it.", "timestamp": "00:21:47,459", "timestamp_s": 1307.0}, {"text": "So it took literally a couple of minutes for me to set everything", "timestamp": "00:21:50,309", "timestamp_s": 1310.0}, {"text": "up locally on my machine.", "timestamp": "00:21:53,459", "timestamp_s": 1313.0}, {"text": "I immediately got 17% Compression of this prompt and you see a prompt", "timestamp": "00:21:55,119", "timestamp_s": 1315.0}, {"text": "looks approximately the same, right?", "timestamp": "00:22:00,179", "timestamp_s": 1320.0}, {"text": "but definitely Something is removed, but the devil is in the details.", "timestamp": "00:22:02,179", "timestamp_s": 1322.0}, {"text": "LLM lingua knows exactly what is non essential for like other LLMs", "timestamp": "00:22:07,649", "timestamp_s": 1327.0}, {"text": "Yeah, so I really encourage you to try this tool Now, some general", "timestamp": "00:22:14,779", "timestamp_s": 1334.0}, {"text": "recommendations about prompts.", "timestamp": "00:22:19,709", "timestamp_s": 1339.0}, {"text": "Be specific and clear.", "timestamp": "00:22:22,339", "timestamp_s": 1342.0}, {"text": "The more concrete your order, your request, your ask to the prompt, the", "timestamp": "00:22:25,059", "timestamp_s": 1345.0}, {"text": "better chance you get nice results.", "timestamp": "00:22:30,149", "timestamp_s": 1350.0}, {"text": "At the same time, be descriptive and if possible, use examples.", "timestamp": "00:22:32,889", "timestamp_s": 1352.0}, {"text": "Again, you might need to educate LLM a bit on what, you expect as a completion.", "timestamp": "00:22:37,819", "timestamp_s": 1357.0}, {"text": "Order of the components.", "timestamp": "00:22:45,139", "timestamp_s": 1365.0}, {"text": "Of your prompt matters.", "timestamp": "00:22:47,164", "timestamp_s": 1367.0}, {"text": "There is no any kind of, strict rule or rules on that.", "timestamp": "00:22:49,914", "timestamp_s": 1369.0}, {"text": "only some recommendations, and it\u0027s on the next slide.", "timestamp": "00:22:54,044", "timestamp_s": 1374.0}, {"text": "Sometimes you need to double down.", "timestamp": "00:22:57,394", "timestamp_s": 1377.0}, {"text": "Sometimes you need to repeat either instruction or format, or", "timestamp": "00:22:59,884", "timestamp_s": 1379.0}, {"text": "any other component of the prompt.", "timestamp": "00:23:04,184", "timestamp_s": 1384.0}, {"text": "again, this is not a.", "timestamp": "00:23:05,884", "timestamp_s": 1385.0}, {"text": "Mandatory, absolutely, but if you are unhappy with, the results", "timestamp": "00:23:07,374", "timestamp_s": 1387.0}, {"text": "you get back, try it, experiment.", "timestamp": "00:23:12,524", "timestamp_s": 1392.0}, {"text": "And for the cases when we talk about classification and categorization, you", "timestamp": "00:23:15,764", "timestamp_s": 1395.0}, {"text": "better explain in your prompt explicitly.", "timestamp": "00:23:20,484", "timestamp_s": 1400.0}, {"text": "Like this.", "timestamp": "00:23:23,894", "timestamp_s": 1403.0}, {"text": "If you don\u0027t know which category to put this text in, you better say,", "timestamp": "00:23:24,604", "timestamp_s": 1404.0}, {"text": "I don\u0027t know, rather than force put it into the, one or another bucket", "timestamp": "00:23:29,304", "timestamp_s": 1409.0}, {"text": "that you provided in the prompt.", "timestamp": "00:23:33,034", "timestamp_s": 1413.0}, {"text": "It will save you time on, validation of the results.", "timestamp": "00:23:34,604", "timestamp_s": 1414.0}, {"text": "Based on this general recommendation some a bit more technical", "timestamp": "00:23:38,644", "timestamp_s": 1418.0}, {"text": "ones or more concrete ones.", "timestamp": "00:23:41,784", "timestamp_s": 1421.0}, {"text": "We can say like this Back to order matters Normally for your optimal", "timestamp": "00:23:43,864", "timestamp_s": 1423.0}, {"text": "prompt you start with clear instructions in some cases You might want to", "timestamp": "00:23:49,254", "timestamp_s": 1429.0}, {"text": "repeat instructions at the end.", "timestamp": "00:23:53,724", "timestamp_s": 1433.0}, {"text": "Again, this is not a requirement.", "timestamp": "00:23:55,274", "timestamp_s": 1435.0}, {"text": "You just need to experiment Don\u0027t be shy to provide very clear instructions", "timestamp": "00:23:57,084", "timestamp_s": 1437.0}, {"text": "Syntax, in your prompt, just, provide some separators between different, sections.", "timestamp": "00:24:01,329", "timestamp_s": 1441.0}, {"text": "You can even call these sections.", "timestamp": "00:24:06,279", "timestamp_s": 1446.0}, {"text": "the better chance you explain LLM what is what in your prompt,", "timestamp": "00:24:07,899", "timestamp_s": 1447.0}, {"text": "the better result you get back.", "timestamp": "00:24:11,679", "timestamp_s": 1451.0}, {"text": "Don\u0027t try to multitask.", "timestamp": "00:24:14,489", "timestamp_s": 1454.0}, {"text": "One prompt for one task, you better organize set of, the calls,", "timestamp": "00:24:16,174", "timestamp_s": 1456.0}, {"text": "like chain the calls if you need to, perform something complex.", "timestamp": "00:24:21,054", "timestamp_s": 1461.0}, {"text": "And also, many providers of the models and many models are capable to accept,", "timestamp": "00:24:25,014", "timestamp_s": 1465.0}, {"text": "some extra parameters, not only prompt itself, but some, parameters for,", "timestamp": "00:24:29,954", "timestamp_s": 1469.0}, {"text": "for C in case of GPD family these two parameters called temperature and Top", "timestamp": "00:24:34,644", "timestamp_s": 1474.0}, {"text": "probabilities and they both affect how creative this model is, how deterministic,", "timestamp": "00:24:40,854", "timestamp_s": 1480.0}, {"text": "you can, you want your answers to be.", "timestamp": "00:24:47,404", "timestamp_s": 1487.0}, {"text": "Of course, Models, model output are, non deterministic, right?", "timestamp": "00:24:50,694", "timestamp_s": 1490.0}, {"text": "but if you, for example, put temperature to zero and top probabilities, to zero,", "timestamp": "00:24:55,624", "timestamp_s": 1495.0}, {"text": "the better chance that you will get the same result for, the same prompt.", "timestamp": "00:24:59,954", "timestamp_s": 1499.0}, {"text": "Otherwise, if you put everything to the maximum, it will be", "timestamp": "00:25:05,114", "timestamp_s": 1505.0}, {"text": "as creative as possible.", "timestamp": "00:25:07,874", "timestamp_s": 1507.0}, {"text": "possible.", "timestamp": "00:25:09,129", "timestamp_s": 1509.0}, {"text": "Let me list a couple of techniques that is widely used in prompt engineering.", "timestamp": "00:25:10,059", "timestamp_s": 1510.0}, {"text": "And again, it might be very useful in your, in career of prompt engineer or", "timestamp": "00:25:15,949", "timestamp_s": 1515.0}, {"text": "AI engineer, or just a developer and, zero shot versus few short prompts by", "timestamp": "00:25:21,159", "timestamp_s": 1521.0}, {"text": "saying, short, we mean example here.", "timestamp": "00:25:27,499", "timestamp_s": 1527.0}, {"text": "and let\u0027s.", "timestamp": "00:25:30,429", "timestamp_s": 1530.0}, {"text": "Let\u0027s imagine that we are building, some automation tool for insurance", "timestamp": "00:25:31,239", "timestamp_s": 1531.0}, {"text": "company for first line of support.", "timestamp": "00:25:36,659", "timestamp_s": 1536.0}, {"text": "And we gather it, question from, Our customer via email automation or", "timestamp": "00:25:38,549", "timestamp_s": 1538.0}, {"text": "maybe a transcribed phone conversation and we want to pass it to this or", "timestamp": "00:25:43,589", "timestamp_s": 1543.0}, {"text": "that department of our company.", "timestamp": "00:25:47,979", "timestamp_s": 1547.0}, {"text": "It\u0027s either auto insurance or flat insurance.", "timestamp": "00:25:50,099", "timestamp_s": 1550.0}, {"text": "the prompt is pretty straightforward.", "timestamp": "00:25:52,829", "timestamp_s": 1552.0}, {"text": "We ask to categorize one, two, three, and this prompt illustrates one of the", "timestamp": "00:25:54,919", "timestamp_s": 1554.0}, {"text": "techniques I mentioned giving model and So if the question is not relevant, you", "timestamp": "00:25:59,379", "timestamp_s": 1559.0}, {"text": "better, say it\u0027s not relevant, just, mark it as three in this particular", "timestamp": "00:26:05,404", "timestamp_s": 1565.0}, {"text": "case, rather than force push it to either auto or home flood insurance.", "timestamp": "00:26:09,614", "timestamp_s": 1569.0}, {"text": "Again, we\u0027ll save some time on validation of the results.", "timestamp": "00:26:14,104", "timestamp_s": 1574.0}, {"text": "This prompt might work good.", "timestamp": "00:26:18,044", "timestamp_s": 1578.0}, {"text": "If it still fails in some, situations, you can educate this a bit.", "timestamp": "00:26:20,334", "timestamp_s": 1580.0}, {"text": "Use the same prompt, but somewhere in the middle, inject a couple of examples,", "timestamp": "00:26:25,874", "timestamp_s": 1585.0}, {"text": "maybe examples that use specific words from this specific, field, maybe it\u0027s", "timestamp": "00:26:31,514", "timestamp_s": 1591.0}, {"text": "like from your specific geo area, or I don\u0027t know, something that is from", "timestamp": "00:26:36,844", "timestamp_s": 1596.0}, {"text": "a real use cases, and, where a model, for example, failed last time, you", "timestamp": "00:26:41,044", "timestamp_s": 1601.0}, {"text": "can supply this as an example and you\u0027ll get definitely better results.", "timestamp": "00:26:46,274", "timestamp_s": 1606.0}, {"text": "Also, you have to find the balance.", "timestamp": "00:26:50,769", "timestamp_s": 1610.0}, {"text": "your prompt becomes a bit.", "timestamp": "00:26:52,969", "timestamp_s": 1612.0}, {"text": "Longer, right?", "timestamp": "00:26:54,769", "timestamp_s": 1614.0}, {"text": "That means a bit more expensive, maybe, slightly, longer time to", "timestamp": "00:26:55,629", "timestamp_s": 1615.0}, {"text": "process completion, but, content or like result is the king after all.", "timestamp": "00:27:00,209", "timestamp_s": 1620.0}, {"text": "We want, perfect, output.", "timestamp": "00:27:04,819", "timestamp_s": 1624.0}, {"text": "Another super powerful technique called chain of output.", "timestamp": "00:27:06,969", "timestamp_s": 1626.0}, {"text": "this particular example is solving math problem.", "timestamp": "00:27:10,374", "timestamp_s": 1630.0}, {"text": "First of all, maybe, I\u0027d say that LLMs are not perfect mathematicians at all.", "timestamp": "00:27:13,684", "timestamp_s": 1633.0}, {"text": "and, normally you don\u0027t use this for solving any kind of, math problems.", "timestamp": "00:27:19,834", "timestamp_s": 1639.0}, {"text": "But, I really want to illustrate this technique, by its, simpler", "timestamp": "00:27:24,604", "timestamp_s": 1644.0}, {"text": "of all to illustrate it by exactly this math problem.", "timestamp": "00:27:28,474", "timestamp_s": 1648.0}, {"text": "story.", "timestamp": "00:27:31,254", "timestamp_s": 1651.0}, {"text": "So just imagine that we, want to ask, for an answer on some simple math operation.", "timestamp": "00:27:31,734", "timestamp_s": 1651.0}, {"text": "So in the white, prompt in the light blue, there is, output or", "timestamp": "00:27:38,034", "timestamp_s": 1658.0}, {"text": "completion, more technical, term result is here and it\u0027s wrong.", "timestamp": "00:27:42,154", "timestamp_s": 1662.0}, {"text": "So 8 million liters per year is completely incorrect.", "timestamp": "00:27:46,944", "timestamp_s": 1666.0}, {"text": "I don\u0027t want to dive too deep.", "timestamp": "00:27:50,084", "timestamp_s": 1670.0}, {"text": "Why that happens in very simple words, LLMs.", "timestamp": "00:27:51,994", "timestamp_s": 1671.0}, {"text": "are trying to find the lowest hanging fruit in, in all", "timestamp": "00:27:55,129", "timestamp_s": 1675.0}, {"text": "this, kind of calculations.", "timestamp": "00:27:58,939", "timestamp_s": 1678.0}, {"text": "Of course, it\u0027s not real calculations.", "timestamp": "00:28:00,299", "timestamp_s": 1680.0}, {"text": "but I want to explain to you how to fix this and, possibly many other", "timestamp": "00:28:02,569", "timestamp_s": 1682.0}, {"text": "situations, not all, not, limited to any kind of, math or If we add one simple", "timestamp": "00:28:07,799", "timestamp_s": 1687.0}, {"text": "sentence, let\u0027s think step by step and explain calculations step by step.", "timestamp": "00:28:17,429", "timestamp_s": 1697.0}, {"text": "As a result, our completion will be a bit longer, that\u0027s fine.", "timestamp": "00:28:21,789", "timestamp_s": 1701.0}, {"text": "But the most important thing is the time.", "timestamp": "00:28:25,769", "timestamp_s": 1705.0}, {"text": "The answer is correct.", "timestamp": "00:28:28,754", "timestamp_s": 1708.0}, {"text": "And, also, by the way, this is not, exact wording.", "timestamp": "00:28:30,394", "timestamp_s": 1710.0}, {"text": "You have to use all the time.", "timestamp": "00:28:33,744", "timestamp_s": 1713.0}, {"text": "This let\u0027s think step by step.", "timestamp": "00:28:35,284", "timestamp_s": 1715.0}, {"text": "No, you can experiment with a longer, shorter description, something like", "timestamp": "00:28:36,594", "timestamp_s": 1716.0}, {"text": "customized description, but you have to, force model to literally think step", "timestamp": "00:28:41,574", "timestamp_s": 1721.0}, {"text": "by step or provide reasoning behind every step and results will be better.", "timestamp": "00:28:48,124", "timestamp_s": 1728.0}, {"text": "you will clearly see this.", "timestamp": "00:28:52,984", "timestamp_s": 1732.0}, {"text": "Another technique is not exactly about prompting itself, but how we construct", "timestamp": "00:28:54,484", "timestamp_s": 1734.0}, {"text": "overall communication with LLMs.", "timestamp": "00:29:00,994", "timestamp_s": 1740.0}, {"text": "And in many cases, we need to send multiple calls to", "timestamp": "00:29:04,314", "timestamp_s": 1744.0}, {"text": "complete one particular task.", "timestamp": "00:29:09,794", "timestamp_s": 1749.0}, {"text": "I already told you that.", "timestamp": "00:29:11,614", "timestamp_s": 1751.0}, {"text": "Not good idea to multitask one prompt for one operation, but, you can easily", "timestamp": "00:29:13,024", "timestamp_s": 1753.0}, {"text": "organize prompt chaining by orchestrating some your, some, backend tooling.", "timestamp": "00:29:20,054", "timestamp_s": 1760.0}, {"text": "And, you can use again, multiple models for multiple tasks, hosted", "timestamp": "00:29:24,594", "timestamp_s": 1764.0}, {"text": "model from one provider, another provider, your own model, local model,", "timestamp": "00:29:29,534", "timestamp_s": 1769.0}, {"text": "fine tuned model, general model.", "timestamp": "00:29:33,614", "timestamp_s": 1773.0}, {"text": "you decide, right?", "timestamp": "00:29:36,014", "timestamp_s": 1776.0}, {"text": "What works.", "timestamp": "00:29:37,144", "timestamp_s": 1777.0}, {"text": "Best for yourself.", "timestamp": "00:29:37,889", "timestamp_s": 1777.0}, {"text": "And the whole idea is very simple.", "timestamp": "00:29:39,619", "timestamp_s": 1779.0}, {"text": "You can use output of a previous call to the model or part of this output as", "timestamp": "00:29:41,239", "timestamp_s": 1781.0}, {"text": "a part of the input for your next call.", "timestamp": "00:29:46,919", "timestamp_s": 1786.0}, {"text": "So this way you achieve a very good, situation when you, literally, get", "timestamp": "00:29:49,439", "timestamp_s": 1789.0}, {"text": "what you want with, minimal efforts.", "timestamp": "00:29:55,209", "timestamp_s": 1795.0}, {"text": "Now I want to introduce Cambridge dictionary word of the year 2023.", "timestamp": "00:29:57,825", "timestamp_s": 1797.0}, {"text": "2023.", "timestamp": "00:30:03,477", "timestamp_s": 1803.0}, {"text": "And this is hallucination.", "timestamp": "00:30:04,448", "timestamp_s": 1804.0}, {"text": "Hallucination in context of our interactions with", "timestamp": "00:30:07,018", "timestamp_s": 1807.0}, {"text": "the large language models.", "timestamp": "00:30:09,998", "timestamp_s": 1809.0}, {"text": "And this is something that is really annoying in all kinds of AI engineering,", "timestamp": "00:30:11,748", "timestamp_s": 1811.0}, {"text": "in all kinds of prompt engineering, all kinds of building AI infused applications.", "timestamp": "00:30:17,498", "timestamp_s": 1817.0}, {"text": "So this is again, outcome of, how LMS were, designed,", "timestamp": "00:30:22,978", "timestamp_s": 1822.0}, {"text": "invented, and how they work.", "timestamp": "00:30:28,178", "timestamp_s": 1828.0}, {"text": "in, in summary, they are very good in making up facts and,", "timestamp": "00:30:30,178", "timestamp_s": 1830.0}, {"text": "making this in a very trusty way.", "timestamp": "00:30:34,748", "timestamp_s": 1834.0}, {"text": "So it\u0027s very, complex sometimes to identify what is, correct in this", "timestamp": "00:30:37,118", "timestamp_s": 1837.0}, {"text": "output and what is wrong if we talk about some, as some facts we requested.", "timestamp": "00:30:42,878", "timestamp_s": 1842.0}, {"text": "Fortunately, there are multiple ways for, if not removing completely,", "timestamp": "00:30:48,718", "timestamp_s": 1848.0}, {"text": "but reducing hallucination, mitigating its consequences.", "timestamp": "00:30:53,468", "timestamp_s": 1853.0}, {"text": "For example, you can explain model not only what you want, but also what you", "timestamp": "00:30:58,648", "timestamp_s": 1858.0}, {"text": "don\u0027t want to receive back from this.", "timestamp": "00:31:03,308", "timestamp_s": 1863.0}, {"text": "So you can limit number of use cases.", "timestamp": "00:31:05,298", "timestamp_s": 1865.0}, {"text": "Also, this is a recommendation that I already introduced, give, model and out.", "timestamp": "00:31:09,143", "timestamp_s": 1869.0}, {"text": "If you\u0027re not sure, say, I don\u0027t know.", "timestamp": "00:31:13,883", "timestamp_s": 1873.0}, {"text": "This is, this could, this simple statement could be your part of the prompt.", "timestamp": "00:31:16,573", "timestamp_s": 1876.0}, {"text": "As well as this one, sometimes it\u0027s, of course, it\u0027s super naive technique, right?", "timestamp": "00:31:21,113", "timestamp_s": 1881.0}, {"text": "But sometimes that, that works.", "timestamp": "00:31:25,403", "timestamp_s": 1885.0}, {"text": "Don\u0027t make up facts.", "timestamp": "00:31:27,063", "timestamp_s": 1887.0}, {"text": "As a part of your prompt, if there\u0027s a chance for you to organize, multiple", "timestamp": "00:31:28,513", "timestamp_s": 1888.0}, {"text": "short conversation with, your LLM, like chat, like maybe you can, ask every", "timestamp": "00:31:33,053", "timestamp_s": 1893.0}, {"text": "time, are you sure that you have all information to answer this question?", "timestamp": "00:31:38,053", "timestamp_s": 1898.0}, {"text": "If not, you better request some extra months.", "timestamp": "00:31:41,723", "timestamp_s": 1901.0}, {"text": "step by step reasoning and asking model to explain along with the answer.", "timestamp": "00:31:44,693", "timestamp_s": 1904.0}, {"text": "Basically, this is a chain of thoughts technique also helps,", "timestamp": "00:31:49,983", "timestamp_s": 1909.0}, {"text": "but all these points, what on the picture are nothing compared to this.", "timestamp": "00:31:54,313", "timestamp_s": 1914.0}, {"text": "You can dynamically find and inject relevant context in information", "timestamp": "00:32:00,623", "timestamp_s": 1920.0}, {"text": "straight into your prompt.", "timestamp": "00:32:06,093", "timestamp_s": 1926.0}, {"text": "And you can say, Use only this information for answer.", "timestamp": "00:32:07,843", "timestamp_s": 1927.0}, {"text": "I mean forget all your knowledge from wikipedia and the public books", "timestamp": "00:32:12,213", "timestamp_s": 1932.0}, {"text": "Discard this we only need you to use this data in its trade in the prompt.", "timestamp": "00:32:16,623", "timestamp_s": 1936.0}, {"text": "I can illustrate this by Example of us in the role of developers who", "timestamp": "00:32:22,253", "timestamp_s": 1942.0}, {"text": "build internal application for Say our employees to investigate what\u0027s possible", "timestamp": "00:32:30,523", "timestamp_s": 1950.0}, {"text": "in medical insurance coverage for them.", "timestamp": "00:32:37,128", "timestamp_s": 1957.0}, {"text": "And let\u0027s imagine that we got a question from employee, via chat bot, or again,", "timestamp": "00:32:40,038", "timestamp_s": 1960.0}, {"text": "via some automated email engine.", "timestamp": "00:32:44,898", "timestamp_s": 1964.0}, {"text": "Does my health plan cover annual eye exams?", "timestamp": "00:32:47,028", "timestamp_s": 1967.0}, {"text": "And at the top of the prompt, that we explain all the, prerequisites.", "timestamp": "00:32:50,258", "timestamp_s": 1970.0}, {"text": "We are intelligent assistant and, yeah, you can answer, questions about health", "timestamp": "00:32:53,708", "timestamp_s": 1973.0}, {"text": "care and use Only sources below for the answer and we also directly inject into", "timestamp": "00:32:59,958", "timestamp_s": 1979.0}, {"text": "this prompt These three sources that are relevant for answering this question.", "timestamp": "00:33:08,278", "timestamp_s": 1988.0}, {"text": "Sounds very simple, right?", "timestamp": "00:33:13,698", "timestamp_s": 1993.0}, {"text": "But of course we have to and yeah completion in that case", "timestamp": "00:33:15,258", "timestamp_s": 1995.0}, {"text": "will be nice and 100% Correct.", "timestamp": "00:33:20,178", "timestamp_s": 2000.0}, {"text": "But how do we exactly identify these sources?", "timestamp": "00:33:22,808", "timestamp_s": 2002.0}, {"text": "How do we find these sources?", "timestamp": "00:33:26,218", "timestamp_s": 2006.0}, {"text": "These few sentences in, for example, dozens of, PDFs or documents", "timestamp": "00:33:28,508", "timestamp_s": 2008.0}, {"text": "or somewhere in, in database.", "timestamp": "00:33:33,108", "timestamp_s": 2013.0}, {"text": "How we identify these sources?", "timestamp": "00:33:35,978", "timestamp_s": 2015.0}, {"text": "How we, shorten these sources?", "timestamp": "00:33:37,808", "timestamp_s": 2017.0}, {"text": "How we rank them after all to provide top 3 or 5 or 10,", "timestamp": "00:33:39,608", "timestamp_s": 2019.0}, {"text": "depending on your use case, items.", "timestamp": "00:33:43,718", "timestamp_s": 2023.0}, {"text": "And Answer to this question is Retrieval Augmented Generation Pattern or REG.", "timestamp": "00:33:46,238", "timestamp_s": 2026.0}, {"text": "as name says, it\u0027s about retrieve these sources, these data sources, augment", "timestamp": "00:33:52,958", "timestamp_s": 2032.0}, {"text": "them and, augment your promptery and after all generate completion.", "timestamp": "00:33:58,678", "timestamp_s": 2038.0}, {"text": "Generate completion is trivial.", "timestamp": "00:34:02,698", "timestamp_s": 2042.0}, {"text": "This is after, about, sending your final, Call to, final prompt", "timestamp": "00:34:04,188", "timestamp_s": 2044.0}, {"text": "to LLM augment is even simpler.", "timestamp": "00:34:08,858", "timestamp_s": 2048.0}, {"text": "This is just a string operation.", "timestamp": "00:34:11,738", "timestamp_s": 2051.0}, {"text": "This is, literally inserting sources of information into your prompt.", "timestamp": "00:34:13,558", "timestamp_s": 2053.0}, {"text": "Like we\u0027ve seen on this previous example, retrieve is the real magic.", "timestamp": "00:34:18,408", "timestamp_s": 2058.0}, {"text": "How?", "timestamp": "00:34:22,838", "timestamp_s": 2062.0}, {"text": "based on this particular question or on, question plus previous conversation, we", "timestamp": "00:34:23,658", "timestamp_s": 2063.0}, {"text": "really identify data we need from, again, back to, use case with, dozens of PDFs", "timestamp": "00:34:28,608", "timestamp_s": 2068.0}, {"text": "with all this insurance, information, and of course, here we can leverage.", "timestamp": "00:34:34,698", "timestamp_s": 2074.0}, {"text": "one more large language model, this one, say one that we use for the final", "timestamp": "00:34:39,523", "timestamp_s": 2079.0}, {"text": "call, or most likely in vast majority of situations, this will be completely", "timestamp": "00:34:43,773", "timestamp_s": 2083.0}, {"text": "different one, specialized, and also, you might want to, vectorize your", "timestamp": "00:34:47,473", "timestamp_s": 2087.0}, {"text": "request and, those you have to send this request to vectorize database.", "timestamp": "00:34:53,363", "timestamp_s": 2093.0}, {"text": "Unfortunately, it\u0027s all, out of our today\u0027s schedule.", "timestamp": "00:34:58,153", "timestamp_s": 2098.0}, {"text": "Scope this could be your homework to learn more about retrieve component of", "timestamp": "00:35:00,598", "timestamp_s": 2100.0}, {"text": "rag and in the real world It\u0027s even more complex because it could be multiple", "timestamp": "00:35:05,848", "timestamp_s": 2105.0}, {"text": "sources of this information, right?", "timestamp": "00:35:11,748", "timestamp_s": 2111.0}, {"text": "So before Constructing your final prompt that contains everything you might", "timestamp": "00:35:13,788", "timestamp_s": 2113.0}, {"text": "want to send requests to your Internal database to some external API and who", "timestamp": "00:35:18,748", "timestamp_s": 2118.0}, {"text": "knows where else so maybe you might Want to have some orchestration support.", "timestamp": "00:35:25,168", "timestamp_s": 2125.0}, {"text": "Surprise, surprise.", "timestamp": "00:35:32,568", "timestamp_s": 2132.0}, {"text": "In real world, it\u0027s even more complex than just, this orchestration.", "timestamp": "00:35:33,618", "timestamp_s": 2133.0}, {"text": "and if you talk not about this particular call to or chain of", "timestamp": "00:35:38,018", "timestamp_s": 2138.0}, {"text": "calls to, LLM, but also about.", "timestamp": "00:35:41,718", "timestamp_s": 2141.0}, {"text": "Full life cycle of our AI infused application or like", "timestamp": "00:35:43,928", "timestamp_s": 2143.0}, {"text": "a generative AI part of this.", "timestamp": "00:35:47,898", "timestamp_s": 2147.0}, {"text": "we want to get some tools, some, frameworks for ideation, for", "timestamp": "00:35:50,268", "timestamp_s": 2150.0}, {"text": "building, for making everything, real, how we deploy, how we, monitor.", "timestamp": "00:35:55,528", "timestamp_s": 2155.0}, {"text": "it\u0027s, let\u0027s say very specialized part, DevOps, but specifically", "timestamp": "00:36:00,763", "timestamp_s": 2160.0}, {"text": "for your LLM interactions.", "timestamp": "00:36:05,703", "timestamp_s": 2165.0}, {"text": "We can call this LLMOps.", "timestamp": "00:36:08,073", "timestamp_s": 2168.0}, {"text": "So let me introduce more tools that can help us both with, this,", "timestamp": "00:36:10,473", "timestamp_s": 2170.0}, {"text": "orchestration of our calls and the operationalization of all flow.", "timestamp": "00:36:15,203", "timestamp_s": 2175.0}, {"text": "First couple of tools I want to mention is LangChain and Semantic Kernel.", "timestamp": "00:36:20,923", "timestamp_s": 2180.0}, {"text": "And these are amazing orchestrators of everything you need to", "timestamp": "00:36:24,653", "timestamp_s": 2184.0}, {"text": "interact with any kind of LLM.", "timestamp": "00:36:29,703", "timestamp_s": 2189.0}, {"text": "So these two libraries or frameworks, you name it, are very similar.", "timestamp": "00:36:32,353", "timestamp_s": 2192.0}, {"text": "They support slightly different, Set of programming language slightly", "timestamp": "00:36:36,373", "timestamp_s": 2196.0}, {"text": "different set of Platforms but in a nutshell they are helping you to Get", "timestamp": "00:36:39,643", "timestamp_s": 2199.0}, {"text": "very nice abstraction over all your interactions with LLMs so you don\u0027t", "timestamp": "00:36:46,233", "timestamp_s": 2206.0}, {"text": "need to write all these calls all this ping pong from scratch also There are", "timestamp": "00:36:52,603", "timestamp_s": 2212.0}, {"text": "many more very interesting features included in both of the framework.", "timestamp": "00:36:58,233", "timestamp_s": 2218.0}, {"text": "So When you start your new AI infused application, don\u0027t ever", "timestamp": "00:37:03,993", "timestamp_s": 2223.0}, {"text": "start, like your, low level, communication with LLM from scratch.", "timestamp": "00:37:08,358", "timestamp_s": 2228.0}, {"text": "Find your perfect, level of abstraction, in these two or any other frameworks.", "timestamp": "00:37:12,838", "timestamp_s": 2232.0}, {"text": "And for overall orchestration or personalization of your LLM application,", "timestamp": "00:37:18,358", "timestamp_s": 2238.0}, {"text": "I recommend you to have a look at PromptFlow, another open source tool.", "timestamp": "00:37:23,968", "timestamp_s": 2243.0}, {"text": "this one also created by my colleagues from Microsoft that in code first", "timestamp": "00:37:28,858", "timestamp_s": 2248.0}, {"text": "way can help you again to keep a full control over full cycle of your LLM,", "timestamp": "00:37:34,348", "timestamp_s": 2254.0}, {"text": "development starting from experimentation.", "timestamp": "00:37:43,158", "timestamp_s": 2263.0}, {"text": "Up to monitoring", "timestamp": "00:37:45,813", "timestamp_s": 2265.0}, {"text": "from flow.", "timestamp": "00:37:47,773", "timestamp_s": 2267.0}, {"text": "You I could look like this.", "timestamp": "00:37:48,303", "timestamp_s": 2268.0}, {"text": "This particular short video is from, the hosted version.", "timestamp": "00:37:49,703", "timestamp_s": 2269.0}, {"text": "There is one on Azure, but you can get the same UI straight in", "timestamp": "00:37:53,333", "timestamp_s": 2273.0}, {"text": "your, VS code ID, via respective.", "timestamp": "00:37:59,063", "timestamp_s": 2279.0}, {"text": "extension called PromptFlow.", "timestamp": "00:38:02,103", "timestamp_s": 2282.0}, {"text": "basically it reproduces your interaction with LLMs as a graph where some", "timestamp": "00:38:04,203", "timestamp_s": 2284.0}, {"text": "nodes are pieces of Python code.", "timestamp": "00:38:12,573", "timestamp_s": 2292.0}, {"text": "Some nodes are calling of LLMs.", "timestamp": "00:38:14,973", "timestamp_s": 2294.0}, {"text": "Some nodes represent prompts where you can iterate through multiple variants,", "timestamp": "00:38:17,843", "timestamp_s": 2297.0}, {"text": "for example, to identify what is the best.", "timestamp": "00:38:23,403", "timestamp_s": 2303.0}, {"text": "And after all, you can host everything on the cloud of your choice.", "timestamp": "00:38:25,693", "timestamp_s": 2305.0}, {"text": "There are many learning resources available about prompt engineering,", "timestamp": "00:38:29,333", "timestamp_s": 2309.0}, {"text": "and, I hope that you will get PDF of, this session with all active links.", "timestamp": "00:38:34,763", "timestamp_s": 2314.0}, {"text": "What is the future of prompt engineering?", "timestamp": "00:38:41,113", "timestamp_s": 2321.0}, {"text": "I have set up the questions.", "timestamp": "00:38:43,753", "timestamp_s": 2323.0}, {"text": "I will not give you answers on.", "timestamp": "00:38:45,383", "timestamp_s": 2325.0}, {"text": "I encourage you, yourself find the answer.", "timestamp": "00:38:47,403", "timestamp_s": 2327.0}, {"text": "For example.", "timestamp": "00:38:51,193", "timestamp_s": 2331.0}, {"text": "Will it be a separate job title or just essential skill?", "timestamp": "00:38:52,283", "timestamp_s": 2332.0}, {"text": "Will it become simpler with tools like Longchain, Semantic Kernel, PromptFlow?", "timestamp": "00:38:56,013", "timestamp_s": 2336.0}, {"text": "Or they become more complex because there is multi modality, you", "timestamp": "00:39:01,483", "timestamp_s": 2341.0}, {"text": "have to closely work with vectors if you talk about React, etc.", "timestamp": "00:39:04,833", "timestamp_s": 2344.0}, {"text": "Doos, will it be democratized and pretty much everyone who ever sent a request", "timestamp": "00:39:10,113", "timestamp_s": 2350.0}, {"text": "to ChatGPT will call themselves Doos?", "timestamp": "00:39:15,553", "timestamp_s": 2355.0}, {"text": "Prompt engineer those, there\u0027ll be title inflation or it will be more gated,", "timestamp": "00:39:17,318", "timestamp_s": 2357.0}, {"text": "discipline and who will benefit the most people who understand how our language", "timestamp": "00:39:22,818", "timestamp_s": 2362.0}, {"text": "constructed linguists or people who understand how code works technologies.", "timestamp": "00:39:28,888", "timestamp_s": 2368.0}, {"text": "Or maybe people who know everything about this particular domain,", "timestamp": "00:39:33,788", "timestamp_s": 2373.0}, {"text": "where this application comes from and able to formulate the problem.", "timestamp": "00:39:38,778", "timestamp_s": 2378.0}, {"text": "And do we survive competition with LLM based prompt engineers?", "timestamp": "00:39:43,928", "timestamp_s": 2383.0}, {"text": "Because LLMs are also perfect in creating prompts.", "timestamp": "00:39:48,528", "timestamp_s": 2388.0}, {"text": "I invite you all to the Prompt Engineering Conference I organize, in November", "timestamp": "00:39:51,828", "timestamp_s": 2391.0}, {"text": "and this will be second edition.", "timestamp": "00:39:56,938", "timestamp_s": 2396.0}, {"text": "First of edition was, last year, super popular.", "timestamp": "00:39:58,558", "timestamp_s": 2398.0}, {"text": "and, if you watch this session before November 20, just go", "timestamp": "00:40:02,128", "timestamp_s": 2402.0}, {"text": "and register your ticket.", "timestamp": "00:40:07,038", "timestamp_s": 2407.0}, {"text": "If you watch this after, still go to the same URL.", "timestamp": "00:40:08,058", "timestamp_s": 2408.0}, {"text": "You will find all sessions recorded straight there.", "timestamp": "00:40:11,918", "timestamp_s": 2411.0}, {"text": "And it\u0027s online, it\u0027s free.", "timestamp": "00:40:16,418", "timestamp_s": 2416.0}, {"text": "It\u0027s free, open for everyone.", "timestamp": "00:40:17,748", "timestamp_s": 2417.0}, {"text": "Thank you very much for watching this session and my last prompt for everyone.", "timestamp": "00:40:19,878", "timestamp_s": 2419.0}, {"text": "Let\u0027s stay in touch.", "timestamp": "00:40:26,398", "timestamp_s": 2426.0}, {"text": "This is my LinkedIn profile.", "timestamp": "00:40:27,518", "timestamp_s": 2427.0}, {"text": "Find me, just scan this QR code or find Maxim Salnikov Microsoft on LinkedIn.", "timestamp": "00:40:28,988", "timestamp_s": 2428.0}, {"text": "And it\u0027s my great pleasure to stay connected with you.", "timestamp": "00:40:34,148", "timestamp_s": 2434.0}, {"text": "Ask me about, prompt engineering, AI engineering, any questions", "timestamp": "00:40:37,088", "timestamp_s": 2437.0}, {"text": "or, any questions about web development and cloud in general.", "timestamp": "00:40:41,068", "timestamp_s": 2441.0}, {"text": "Thank you very much.", "timestamp": "00:40:45,368", "timestamp_s": 2445.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: 'AtaSBySKKH4',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Prompt Engineering: An Art, a Science, or Your Next Job Title?
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>I will introduce prompt engineering as an emerging discipline with its own methodologies, tools, and best practices. Expect lots of examples that will help you write ideal prompts for all occasions.</p>
<!-- End Text -->
          </div>

          
          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./srt/prompt2024_Maxim_Salnikov.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:00,000'); seek(0.0)">
              hello everyone!
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:00,930'); seek(0.0)">
              Let's talk about prompt engineering.
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:03,400'); seek(3.0)">
              What is it?
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:04,670'); seek(4.0)">
              Art or science or maybe your next job title?
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:08,550'); seek(8.0)">
              I'm Maxim Selnikov.
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:09,959'); seek(9.0)">
              I'm based in Oslo, Norway, where I work in Microsoft, where I help
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:14,329'); seek(14.0)">
              developers to succeed with our cloud technologies, tools for developers.
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:19,575'); seek(19.0)">
              And, everything related AI.
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:22,795'); seek(22.0)">
              I personally, a developer myself, and I build applications
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:26,545'); seek(26.0)">
              since, late 90s of last century.
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:00:31,025'); seek(31.0)">
              And I'm big fan of the developer communities.
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:00:33,495'); seek(33.0)">
              In Oslo, where I based, I founded a couple of conferences and, run a few meetups.
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:00:39,164'); seek(39.0)">
              My favorite topics to present about are web development, all kinds of
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:00:43,854'); seek(43.0)">
              cloud development, and of course, AI.
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:00:45,894'); seek(45.0)">
              Everything related to generative AI, including prompt engineering.
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:00:51,144'); seek(51.0)">
              I found multiple estimations on, the number of people who
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:00:55,514'); seek(55.0)">
              use generative AI on our planet.
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:00:58,364'); seek(58.0)">
              And, the most conservative ones say that it's well over 1 billion people.
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:05,464'); seek(65.0)">
              That's a number, right?
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:07,414'); seek(67.0)">
              What do these people use generative AI for?
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:10,994'); seek(70.0)">
              just for us to start the discussion.
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:13,034'); seek(73.0)">
              I identified three large application areas.
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:17,484'); seek(77.0)">
              First of all, everything related to productivity, your personal
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:21,044'); seek(81.0)">
              productivity, your business productivity, everything related to usage of
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:25,754'); seek(85.0)">
              generative AI in education and science, and many other areas where we can
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:30,574'); seek(90.0)">
              really get super powered by gen AI.
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:01:33,974'); seek(93.0)">
              Next creativity.
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:01:35,914'); seek(95.0)">
              After all, it's called generative AI, right?
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:01:38,184'); seek(98.0)">
              So it can generate for us many, very interesting things.
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:01:41,414'); seek(101.0)">
              so we can really feel ourselves super creative, but, there should be someone
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:01:46,544'); seek(106.0)">
              who is building these applications for us.
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:01:49,494'); seek(109.0)">
              Someone who is, actually, Constructing this UIs that we can use both
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:01:54,444'); seek(114.0)">
              for productivity and creativity.
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:01:56,844'); seek(116.0)">
              And yeah, here I'm talking about people who are actually building these AI
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:02,274'); seek(122.0)">
              infused or intelligent applications.
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:05,314'); seek(125.0)">
              And I'll do my best to make this particular session the most useful
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:10,004'); seek(130.0)">
              for exactly this category of people.
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:12,534'); seek(132.0)">
              people, but at the same time, I'm sure that even if you don't have any intention
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:18,124'); seek(138.0)">
              to build your, application, you can still get lots of useful information from
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:23,694'); seek(143.0)">
              this session because, it contains some, general recommendations, some general,
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:28,494'); seek(148.0)">
              practices for your improved Prompting.
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:02:31,984'); seek(151.0)">
              And, after this session, when you open any kind of generative AI powered
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:02:36,384'); seek(156.0)">
              service like ChatGPT or similar, you will know better how to communicate.
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:02:42,594'); seek(162.0)">
              what is common thing in all these scenarios?
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:02:48,004'); seek(168.0)">
              Let's look how we, for example, ask Meet Journey to create a new image for us, or
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:02:54,374'); seek(174.0)">
              how we start conversation with Microsoft 365 Word, on, creating some very smart
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:01,084'); seek(181.0)">
              and solid template for our document.
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:04,954'); seek(184.0)">
              Or let's look how we communicate with a GitHub copilot for improving
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:11,234'); seek(191.0)">
              our code, not even saying about the chat GPT as a service itself,
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:17,014'); seek(197.0)">
              where we literally chat with this.
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:20,444'); seek(200.0)">
              Of course, one common thing is the way we interact.
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:24,534'); seek(204.0)">
              And, the fact that it all starts with the prompt with, some text that we write.
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:31,554'); seek(211.0)">
              And, even though model landscape is emerging and there is, next amazing
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:37,894'); seek(217.0)">
              model is, released while I speak, while you watch this session, and there are
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:43,144'); seek(223.0)">
              already, multiple models available.
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:45,694'); seek(225.0)">
              And, It's becoming more and more complex.
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:48,434'); seek(228.0)">
              There are large language models.
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:03:50,254'); seek(230.0)">
              There is new generation of small language models also multi modality is really
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:03:57,854'); seek(237.0)">
              available now even though There are different kinds and types of models.
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:04:04,124'); seek(244.0)">
              Some of them are general some of them specialized Still
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:04:07,864'); seek(247.0)">
              we all go back to prompts
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:04:11,544'); seek(251.0)">
              This is why I consider prompt engineering as a separate discipline or at least as
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:18,544'); seek(258.0)">
              an essential skill of many people, not only developers, but, many technical
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:23,754'); seek(263.0)">
              people, many people on business positions.
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:27,294'); seek(267.0)">
              and, what is this after all, this is a process of, designing
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:31,364'); seek(271.0)">
              prompts of, tuning these prompts.
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:34,049'); seek(274.0)">
              And the further optimization, while we keep satisfaction on a satisfaction
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:41,149'); seek(281.0)">
              level on the results that we get back from the large language models.
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:45,999'); seek(285.0)">
              And of course, it's also important to, to follow cost efficiency, because in
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:51,039'); seek(291.0)">
              many cases, we talk about some paid services when we talk specifically about
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:57,369'); seek(297.0)">
              like high and large language models.
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:05:00,269'); seek(300.0)">
              Let's look closer at the prompt components or if you wish we can call it prompt
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:05:05,549'); seek(305.0)">
              anatomy in many cases Again, at least when we are the users who chat with
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:05:12,829'); seek(312.0)">
              chat GPT We don't really think too much about How we structure our prompt.
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:05:19,944'); seek(319.0)">
              We just communicate and as long as we are happy with the results.
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:05:24,754'); seek(324.0)">
              We don't want to Dive deeper into all these nitty gritty details That's fine.
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:05:30,724'); seek(330.0)">
              But when you build your AI infused application It's really good idea
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:05:34,684'); seek(334.0)">
              for you to know how that works from inside and again the same ideas and
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:05:40,224'); seek(340.0)">
              the techniques are still applicable to your like day to day conversations
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:44,464'); seek(344.0)">
              with any generative AI powered service.
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:47,214'); seek(347.0)">
              So yeah, let me, introduce how that works.
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:51,544'); seek(351.0)">
              in many cases we have very clear and, concise and straightforward instruction.
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:59,104'); seek(359.0)">
              What we want to get from this particular call to our, generative AI
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:06:04,314'); seek(364.0)">
              service or, more technically precise.
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:06:06,769'); seek(366.0)">
              The two large language model exposed by one of these services.
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:06:10,809'); seek(370.0)">
              and yeah, let me illustrate it by this example what on the screen.
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:06:14,629'); seek(374.0)">
              Let's imagine that we are building applications for marketing automation
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:06:19,609'); seek(379.0)">
              that gives us Nice, at least drafts, maybe, or maybe ready to go emails
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:06:25,439'); seek(385.0)">
              that, share details on some new products that we either produce or sell,
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:06:30,619'); seek(390.0)">
              or, it's again, it's not obligatory as in role of developers, you can,
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:06:34,969'); seek(394.0)">
              use the same, ideas and techniques when you just, use ready to ready
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:06:39,549'); seek(399.0)">
              products, with, some prompts available.
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:06:42,689'); seek(402.0)">
              Instruction.
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:06:43,679'); seek(403.0)">
              Of course, we have to also provide some primary data about this product itself.
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:48,449'); seek(408.0)">
              Also, we can provide some context or we can call it secondary
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:51,859'); seek(411.0)">
              data about tone we expect.
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:54,399'); seek(414.0)">
              In this particular situation, we want to be friendly and exciting, but for
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:59,529'); seek(419.0)">
              different scenarios, there could be different ways of, exact, narrative
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:07:05,309'); seek(425.0)">
              that we expect from the model.
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:07:07,139'); seek(427.0)">
              Also, we, identify, we set the format, we define the format,
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:07:12,889'); seek(432.0)">
              we want to get, answer back.
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:07:15,439'); seek(435.0)">
              And in this particular situation, it's definitely something that, that is
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:07:19,149'); seek(439.0)">
              in the middle of, overall, developer chain of, of this product, because
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:07:24,109'); seek(444.0)">
              you see that we expect not just text, but, Jason, definitely this will
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:07:28,319'); seek(448.0)">
              be somehow processed by the next steps of, our, for example, backend.
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:07:32,639'); seek(452.0)">
              So we say that we want to get back.
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:07:35,394'); seek(455.0)">
              Not just subject and the body, but JSON object with these fields.
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:07:40,844'); seek(460.0)">
              Also, we provide an example and in this particular situation,
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:07:44,464'); seek(464.0)">
              it plays at least two roles.
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:07:46,904'); seek(466.0)">
              First of all, yeah, it demonstrates the model, that kind of text, maybe length
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:54,804'); seek(474.0)">
              of the text and again tone and old structure That might be a good fit for us.
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:08:00,744'); seek(480.0)">
              And also we double down on the format.
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:08:03,254'); seek(483.0)">
              So this is why, we, put example exactly in the format that we
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:08:08,794'); seek(488.0)">
              described on the line above.
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:08:10,854'); seek(490.0)">
              Why do we need this duplication?
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:08:13,124'); seek(493.0)">
              Stay tuned.
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:08:13,824'); seek(493.0)">
              I will explain why that might be useful.
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:08:16,484'); seek(496.0)">
              So this is how we, human or developers look at the prompt.
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:08:21,684'); seek(501.0)">
              And this is how large language model or LLM understands
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:08:26,444'); seek(506.0)">
              the same prompt on its end.
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:08:29,164'); seek(509.0)">
              And that it's split into tokens.
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:08:32,424'); seek(512.0)">
              And actually, tokenization is the first step.
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:08:34,799'); seek(514.0)">
              procedure that happens when you send prompt to LLM.
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:08:39,269'); seek(519.0)">
              So large language model understands your, your prompt in form of tokens.
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:08:45,109'); seek(525.0)">
              And, when time has come to generate answer for you, it also generates
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:08:50,109'); seek(530.0)">
              this recursively token by token.
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:08:53,119'); seek(533.0)">
              On this example, that, many words.
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:55,969'); seek(535.0)">
              are equal to one token, right?
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:09:00,019'); seek(540.0)">
              One word equals one token.
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:09:01,399'); seek(541.0)">
              But of course, the real situation is much more, complex, right?
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:09:06,059'); seek(546.0)">
              So you see that, some words, take.
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:09:09,259'); seek(549.0)">
              Multiple tokens and exact implementation of how that split
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:09:14,429'); seek(554.0)">
              is up to implementation of a large language model, or it's a tokenizer.
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:09:20,669'); seek(560.0)">
              There is no any kind of strict rule how many, Words equals to how many tokens
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:09:26,409'); seek(566.0)">
              or how many characters equals to how many tokens we can very Approximately
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:09:31,289'); seek(571.0)">
              say that at least for this generation of LLMs and for English texts 100 tokens
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:09:39,430'); seek(579.0)">
              is Approximately, equal to 75 words.
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:09:44,309'); seek(584.0)">
              so that means like one token is maybe around four characters in English text.
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:09:50,009'); seek(590.0)">
              For different languages, the ratio is completely different.
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:09:53,599'); seek(593.0)">
              You might ask me, why do we ever need to have this knowledge about tokens?
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:09:58,809'); seek(598.0)">
              Because Definitely when we are in user role, this is hidden
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:10:02,899'); seek(602.0)">
              completely under the hood for us.
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:10:04,749'); seek(604.0)">
              We communicate with sentences in, sentences back.
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:10:09,059'); seek(609.0)">
              when we are in developer role, Also, this is not that visible at the
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:10:15,209'); seek(615.0)">
              beginning of, when you start building your AI application, but very soon as
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:10:20,299'); seek(620.0)">
              a developer, you will understand super important meaning of tokenization.
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:10:25,489'); seek(625.0)">
              And this is why, because number of tokens in your input and
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:10:31,039'); seek(631.0)">
              expected output, first of
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:10:33,539'); seek(633.0)">
              Cost of, this particular call, if we talk about some hosted,
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:10:38,109'); seek(638.0)">
              LLMs hosted by some vendors.
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:10:40,219'); seek(640.0)">
              And also there is technical limitation on number of tokens you can send and receive.
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:10:48,029'); seek(648.0)">
              on this slide, there is some screenshot from, pricing for
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:10:53,669'); seek(653.0)">
              Azure OpenAI, services for GPT 4.
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:10:57,599'); seek(657.0)">
              0 model and 0.
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:10:58,909'); seek(658.0)">
              1 preview model.
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:11:00,269'); seek(660.0)">
              And for legacy purposes, I also listed price for GPT 4.
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:11:04,639'); seek(664.0)">
              And, that, we are in good situation as developers because prices.
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:11:09,894'); seek(669.0)">
              are going down.
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:11:11,634'); seek(671.0)">
              And first of all, yeah, that it's priced by 1 million tokens.
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:11:15,034'); seek(675.0)">
              So every single token for a few number of calls, maybe this is
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:11:18,054'); seek(678.0)">
              not something really crucial.
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:11:20,614'); seek(680.0)">
              The price becomes really different when we talk about some scale usage, but, Yeah.
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:11:25,252'); seek(685.0)">
              Yeah.
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:11:25,919'); seek(685.0)">
              Still very important to keep eye on it.
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:11:28,549'); seek(688.0)">
              And that in GPT 4 it started for 60 per million and suddenly GPT 4.
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:11:36,429'); seek(696.0)">
              0 that is more capable, more performant model.
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:11:39,209'); seek(699.0)">
              It's many times more Cheaper and this is general trend.
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:11:44,639'); seek(704.0)">
              there is newer and newer technologies from outside of providers of LLM services and
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:11:52,289'); seek(712.0)">
              yeah, so it's it's very good, especially for example startups they have to
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:11:56,049'); seek(716.0)">
              recalculate their economy and recalculate in a positive way and next column I want
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:12:02,599'); seek(722.0)">
              to emphasize on this screenshot is context and this is exactly the Number of tokens
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:12:09,834'); seek(729.0)">
              we can send in one particular request.
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:12:12,754'); seek(732.0)">
              So we see it's not number of characters or number of words or bytes or whatever.
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:12:16,984'); seek(736.0)">
              No, it's calculated in tokens.
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:12:19,174'); seek(739.0)">
              This is why it's very important to understand, at least roughly estimate
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:12:23,944'); seek(743.0)">
              number of tokens in your request.
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:12:25,504'); seek(745.0)">
              prompts.
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:12:26,194'); seek(746.0)">
              as you see, modern models are quite capable and we talk about a hundred
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:12:31,824'); seek(751.0)">
              plus thousands of, of the token.
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:12:34,464'); seek(754.0)">
              So we are not talking about every single, word or white space or, not even about
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:12:38,924'); seek(758.0)">
              sentences, not even about paragraphs.
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:12:41,484'); seek(761.0)">
              Maybe you can, currently send.
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:12:43,964'); seek(763.0)">
              Pages of text as a prompt for these models and again trend is work
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:12:49,954'); seek(769.0)">
              good for us models are Capable to accept more and more tokens, but
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:12:56,514'); seek(776.0)">
              still there is a limitation, right?
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:12:58,114'); seek(778.0)">
              So you cannot send maybe full book at least currently at least at this moment
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:13:03,259'); seek(783.0)">
              So this is why, again, tokenization concept is extremely important and, we'll,
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:13:07,909'); seek(787.0)">
              have a few more slides on this topic.
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:13:11,839'); seek(791.0)">
              And of course, different providers of, these LLM services provide
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:13:15,499'); seek(795.0)">
              different ways for you to save on, on this usage, all kinds of, caching.
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:13:20,424'); seek(800.0)">
              All kinds of, batch calculations when you don't need output, here now, but
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:13:24,984'); seek(804.0)">
              you can wait a bit and then, price for this call will be cheaper than regular.
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:13:29,654'); seek(809.0)">
              All kinds of, some dedicated, capacities, that really depends vendor from vendor.
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:13:34,744'); seek(814.0)">
              And you also noticed that, model selection makes real difference.
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:13:40,049'); seek(820.0)">
              First of all on capabilities, on quality of the output, and second on the price.
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:13:45,939'); seek(825.0)">
              And, I offer you to use this simple strategy on model selection.
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:13:50,769'); seek(830.0)">
              First of all, try with the most capable, the most performant,
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:13:55,369'); seek(835.0)">
              and, in many cases also the most expensive model on the market.
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:13:58,979'); seek(838.0)">
              maybe you can, try different providers.
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:14:01,679'); seek(841.0)">
              And, Identify your best prompt that gives you the best possible results
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:14:06,809'); seek(846.0)">
              Then you can try to downgrade to the next cheapest one Next cheapest model
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:14:13,359'); seek(853.0)">
              in some cases you might to might need to fine tune the prompt slightly but
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:14:17,689'); seek(857.0)">
              check the results if Result or completion in technical terms when we talk about
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:14:23,384'); seek(863.0)">
              prompt engineering is the same or better.
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:14:26,374'); seek(866.0)">
              Maybe you can Do next step in this downgrade and try even
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:14:33,074'); seek(873.0)">
              cheaper model Maybe again results will be the same or even better.
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:14:36,654'); seek(876.0)">
              Sometimes that's also possible and it will give you a chance to save Some
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:14:40,834'); seek(880.0)">
              dollars if no Then you just go up to the previous step where you downgraded from.
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:14:47,744'); seek(887.0)">
              It's a simple and efficient model selection strategy.
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:14:51,934'); seek(891.0)">
              Also, you can use multiple models and in many cases, if we talk about, not hello
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:14:58,404'); seek(898.0)">
              world AI infused application, this is not just one call to the, LLM, it's a chain
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:15:04,564'); seek(904.0)">
              of the calls, maybe to multiple models, maybe to even to multiple providers, maybe
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:15:09,804'); seek(909.0)">
              it's mix between, some hosted services, model hosted by external vendor, model
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:15:14,634'); seek(914.0)">
              hosted by yourself, and maybe even a model that is running straight on your
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:15:20,344'); seek(920.0)">
              device or on your customer device.
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:15:22,284'); seek(922.0)">
              So that, really depends on business scenario.
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:15:24,614'); seek(924.0)">
              So if you have a chance to use multiple models in this particular feature
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:15:31,405'); seek(931.0)">
              of your AI infused application, you can follow a simple rule.
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:15:35,694'); seek(935.0)">
              for.
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:15:36,784'); seek(936.0)">
              Let's say, complex tasks like generation, you can use expensive ones.
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:15:41,284'); seek(941.0)">
              This is, where they really shine.
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:15:43,124'); seek(943.0)">
              And, yeah, every new generation of the model provides better and
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:15:46,274'); seek(946.0)">
              better results in generation.
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:15:47,844'); seek(947.0)">
              While summarization, classification, categorization.
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:15:51,194'); seek(951.0)">
              This is, again, our days, not that complex tasks anymore, at
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:15:56,814'); seek(956.0)">
              least for a large language models.
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:15:58,604'); seek(958.0)">
              And the cheap ones do them pretty good.
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:16:02,564'); seek(962.0)">
              another strategy.
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:16:04,254'); seek(964.0)">
              is chaining.
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:16:05,344'); seek(965.0)">
              for example, you want to send to a large, expensive model some large amount of text.
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:16:13,164'); seek(973.0)">
              This is how your prompt works, right?
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:16:15,614'); seek(975.0)">
              but you can leverage cheaper and maybe, Like very fast one, maybe your, your
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:16:22,814'); seek(982.0)">
              own fine tuned model to summarize this, large amount of text you are going to
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:16:28,184'); seek(988.0)">
              send to expensive one also might work good for you with a minimal performance,
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:16:34,174'); seek(994.0)">
              decrease, you will save a lot of time.
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:16:35,779'); seek(995.0)">
              some, some budget,
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:16:37,369'); seek(997.0)">
              let's go back to talking conversation.
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:16:39,939'); seek(999.0)">
              So I hope that I convinced you that, keeping eye on number of tokens
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:16:45,339'); seek(1005.0)">
              you send is crucial for, economy of your application, not only for
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:16:50,179'); seek(1010.0)">
              economy, but also you remember that, there was just technical limitation
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:16:53,939'); seek(1013.0)">
              of a number of tokens you send.
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:16:55,859'); seek(1015.0)">
              And after all the shorter prompt, normally the faster you get completion.
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:17:01,764'); seek(1021.0)">
              So it's, like multiple reasons why you want to, minimize your
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:17:07,484'); seek(1027.0)">
              prompts and how to do this exactly.
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:17:10,074'); seek(1030.0)">
              First of all, very simple rule.
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:17:11,474'); seek(1031.0)">
              have a closer look at.
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:17:12,709'); seek(1032.0)">
              White spaces in your prompt.
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:17:14,869'); seek(1034.0)">
              this is something that we can easily overlook because this is not
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:17:19,239'); seek(1039.0)">
              something that is very visible, right?
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:17:21,289'); seek(1041.0)">
              A couple of extra white spaces.
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:17:23,389'); seek(1043.0)">
              We can just, okay, ignore this.
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:17:25,179'); seek(1045.0)">
              But in reality, Some large language models treat every single
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:17:29,634'); seek(1049.0)">
              whitespace as one extra token.
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:17:32,294'); seek(1052.0)">
              Not a big deal.
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:17:32,984'); seek(1052.0)">
              If you talk about one short request, but if we talk about, I don't know, hundreds,
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:17:36,784'); seek(1056.0)">
              thousands, millions of requests that might bring some, some difference to
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:17:41,334'); seek(1061.0)">
              your final bill and the end of the month.
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:17:43,909'); seek(1063.0)">
              Next for, different data formats, try different, different ways
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:17:49,089'); seek(1069.0)">
              to like, implement this data.
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:17:52,109'); seek(1072.0)">
              What I mean exactly can be easily illustrated by example of, how
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:17:56,859'); seek(1076.0)">
              we supply, Date in our prompts.
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:18:00,559'); seek(1080.0)">
              I'm technical mind and my first impression was okay The shorter string with the
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:18:05,939'); seek(1085.0)">
              date the better chance that it will take fewer tokens In reality, not at all.
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:18:11,904'); seek(1091.0)">
              So you see on the bottom line, short format of the date takes six tokens, while
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:18:18,114'); seek(1098.0)">
              on the top line, it only three tokens.
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:18:20,654'); seek(1100.0)">
              So sometimes it's counter intuitive.
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:18:23,264'); seek(1103.0)">
              Again, this is example for one particular large language model.
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:18:27,454'); seek(1107.0)">
              I don't even remember which one different models can do it differently.
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:18:32,134'); seek(1112.0)">
              experimentation is the only way to really identify what's your optimal format
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:18:37,724'); seek(1117.0)">
              for this or that type of, your data.
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:18:40,984'); seek(1120.0)">
              if you talk about tabular data, this, straightforward tabular format is,
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:18:46,434'); seek(1126.0)">
              pretty much, space efficient, and what's very important, understood by LLM.
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:18:50,854'); seek(1130.0)">
              no need for you to, always reproduce JSON like format where you supply
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:18:56,494'); seek(1136.0)">
              caption for every piece of the data.
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:18:58,424'); seek(1138.0)">
              No, just provide some table headers, separated by pipes, or type, tabs, or
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:19:03,124'); seek(1143.0)">
              you'll find your Personal separator and then rows with the data in vast majority
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:19:08,179'); seek(1148.0)">
              of situations LLM will understand what you mean also Language makes real difference.
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:19:14,639'); seek(1154.0)">
              I already mentioned that English is the most straightforward ways for way
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:19:20,969'); seek(1160.0)">
              for Prompt engineering because it's the most efficient while we talk about
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:19:26,509'); seek(1166.0)">
              tokenization at least again for the mainstream Large language models I
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:19:32,714'); seek(1172.0)">
              suggest this because vast amount of data, all this, Wikipedia, public books,
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:19:37,924'); seek(1177.0)">
              et cetera, et cetera, data that was used for LLM training was in English.
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:19:42,644'); seek(1182.0)">
              it still understands other languages perfectly well, but if we talk about
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:19:46,804'); seek(1186.0)">
              tokenization, the same, let's say, Concept same sentence in different
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:19:50,824'); seek(1190.0)">
              language might take more tokens than the one in English i'm not saying
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:19:55,964'); seek(1195.0)">
              that you have to translate everything all single time But again experiment
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:20:00,404'); seek(1200.0)">
              and I want to start introducing some tools and yeah in On next slides, you
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:20:06,494'); seek(1206.0)">
              will see more and more very useful tools frameworks libraries that can
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:20:10,344'); seek(1210.0)">
              simplify your life as a prompt engineer.
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:20:13,359'); seek(1213.0)">
              the first one called LLM Lingua, and this is nothing by but prompt compressor.
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:20:18,359'); seek(1218.0)">
              It's created by my colleagues from Microsoft, and it's open source.
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:20:22,249'); seek(1222.0)">
              So you can take this tool and host it locally or on your own server.
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:20:27,229'); seek(1227.0)">
              And yeah, as the description says, it takes your prompt and compresses,
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:20:33,879'); seek(1233.0)">
              but it does it in a very smart way.
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:20:36,839'); seek(1236.0)">
              of course We can try to do it ourselves, right?
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:20:39,539'); seek(1239.0)">
              But, we do not, not always understand which parts of our prompt are crucial
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:20:46,589'); seek(1246.0)">
              for, LLM to really understand what we mean and which parts we can skip.
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:20:52,789'); seek(1252.0)">
              who can help us?
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:20:54,054'); seek(1254.0)">
              Of course, another large language model, right?
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:20:56,764'); seek(1256.0)">
              And in this particular case, this is compact one, maybe we can
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:21:00,474'); seek(1260.0)">
              call it a small language model.
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:21:02,974'); seek(1262.0)">
              And yeah, so this one is used to identify and remove non
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:21:08,054'); seek(1268.0)">
              essential tokens in your prompt.
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:21:10,954'); seek(1270.0)">
              And with some playing, some fine tuning, you can get up to 20 times compression
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:21:17,624'); seek(1277.0)">
              with either zero or minimum performance.
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:21:21,374'); seek(1281.0)">
              loss because definitely, of course, some time is needed for this, first step,
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:21:27,074'); seek(1287.0)">
              LLM or SLM to compress your prompt.
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:21:29,824'); seek(1289.0)">
              But on the other hand, prompt will be shorter and, it might happen that you save
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:21:35,154'); seek(1295.0)">
              some milliseconds just because of that.
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:21:37,874'); seek(1297.0)">
              if we take prompt from our first example about, email.
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:21:42,999'); seek(1302.0)">
              about, new headphones and run it through this LLM Lingua.
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:21:47,459'); seek(1307.0)">
              I did it without any fine tuning, just as it.
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:21:50,309'); seek(1310.0)">
              So it took literally a couple of minutes for me to set everything
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:21:53,459'); seek(1313.0)">
              up locally on my machine.
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:21:55,119'); seek(1315.0)">
              I immediately got 17% Compression of this prompt and you see a prompt
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:22:00,179'); seek(1320.0)">
              looks approximately the same, right?
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:22:02,179'); seek(1322.0)">
              but definitely Something is removed, but the devil is in the details.
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:22:07,649'); seek(1327.0)">
              LLM lingua knows exactly what is non essential for like other LLMs
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:22:14,779'); seek(1334.0)">
              Yeah, so I really encourage you to try this tool Now, some general
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:22:19,709'); seek(1339.0)">
              recommendations about prompts.
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:22:22,339'); seek(1342.0)">
              Be specific and clear.
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:22:25,059'); seek(1345.0)">
              The more concrete your order, your request, your ask to the prompt, the
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:22:30,149'); seek(1350.0)">
              better chance you get nice results.
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:22:32,889'); seek(1352.0)">
              At the same time, be descriptive and if possible, use examples.
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:22:37,819'); seek(1357.0)">
              Again, you might need to educate LLM a bit on what, you expect as a completion.
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:22:45,139'); seek(1365.0)">
              Order of the components.
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:22:47,164'); seek(1367.0)">
              Of your prompt matters.
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:22:49,914'); seek(1369.0)">
              There is no any kind of, strict rule or rules on that.
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:22:54,044'); seek(1374.0)">
              only some recommendations, and it's on the next slide.
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:22:57,394'); seek(1377.0)">
              Sometimes you need to double down.
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:22:59,884'); seek(1379.0)">
              Sometimes you need to repeat either instruction or format, or
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:23:04,184'); seek(1384.0)">
              any other component of the prompt.
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:23:05,884'); seek(1385.0)">
              again, this is not a.
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:23:07,374'); seek(1387.0)">
              Mandatory, absolutely, but if you are unhappy with, the results
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:23:12,524'); seek(1392.0)">
              you get back, try it, experiment.
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:23:15,764'); seek(1395.0)">
              And for the cases when we talk about classification and categorization, you
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:23:20,484'); seek(1400.0)">
              better explain in your prompt explicitly.
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:23:23,894'); seek(1403.0)">
              Like this.
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:23:24,604'); seek(1404.0)">
              If you don't know which category to put this text in, you better say,
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:23:29,304'); seek(1409.0)">
              I don't know, rather than force put it into the, one or another bucket
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:23:33,034'); seek(1413.0)">
              that you provided in the prompt.
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:23:34,604'); seek(1414.0)">
              It will save you time on, validation of the results.
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:23:38,644'); seek(1418.0)">
              Based on this general recommendation some a bit more technical
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:23:41,784'); seek(1421.0)">
              ones or more concrete ones.
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:23:43,864'); seek(1423.0)">
              We can say like this Back to order matters Normally for your optimal
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:23:49,254'); seek(1429.0)">
              prompt you start with clear instructions in some cases You might want to
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:23:53,724'); seek(1433.0)">
              repeat instructions at the end.
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:23:55,274'); seek(1435.0)">
              Again, this is not a requirement.
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:23:57,084'); seek(1437.0)">
              You just need to experiment Don't be shy to provide very clear instructions
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:24:01,329'); seek(1441.0)">
              Syntax, in your prompt, just, provide some separators between different, sections.
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:24:06,279'); seek(1446.0)">
              You can even call these sections.
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:24:07,899'); seek(1447.0)">
              the better chance you explain LLM what is what in your prompt,
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:24:11,679'); seek(1451.0)">
              the better result you get back.
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:24:14,489'); seek(1454.0)">
              Don't try to multitask.
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:24:16,174'); seek(1456.0)">
              One prompt for one task, you better organize set of, the calls,
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:24:21,054'); seek(1461.0)">
              like chain the calls if you need to, perform something complex.
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:24:25,014'); seek(1465.0)">
              And also, many providers of the models and many models are capable to accept,
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:24:29,954'); seek(1469.0)">
              some extra parameters, not only prompt itself, but some, parameters for,
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:24:34,644'); seek(1474.0)">
              for C in case of GPD family these two parameters called temperature and Top
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:24:40,854'); seek(1480.0)">
              probabilities and they both affect how creative this model is, how deterministic,
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:24:47,404'); seek(1487.0)">
              you can, you want your answers to be.
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:24:50,694'); seek(1490.0)">
              Of course, Models, model output are, non deterministic, right?
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:24:55,624'); seek(1495.0)">
              but if you, for example, put temperature to zero and top probabilities, to zero,
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:24:59,954'); seek(1499.0)">
              the better chance that you will get the same result for, the same prompt.
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:25:05,114'); seek(1505.0)">
              Otherwise, if you put everything to the maximum, it will be
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:25:07,874'); seek(1507.0)">
              as creative as possible.
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:25:09,129'); seek(1509.0)">
              possible.
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:25:10,059'); seek(1510.0)">
              Let me list a couple of techniques that is widely used in prompt engineering.
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:25:15,949'); seek(1515.0)">
              And again, it might be very useful in your, in career of prompt engineer or
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:25:21,159'); seek(1521.0)">
              AI engineer, or just a developer and, zero shot versus few short prompts by
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:25:27,499'); seek(1527.0)">
              saying, short, we mean example here.
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:25:30,429'); seek(1530.0)">
              and let's.
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:25:31,239'); seek(1531.0)">
              Let's imagine that we are building, some automation tool for insurance
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:25:36,659'); seek(1536.0)">
              company for first line of support.
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:25:38,549'); seek(1538.0)">
              And we gather it, question from, Our customer via email automation or
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:25:43,589'); seek(1543.0)">
              maybe a transcribed phone conversation and we want to pass it to this or
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:25:47,979'); seek(1547.0)">
              that department of our company.
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:25:50,099'); seek(1550.0)">
              It's either auto insurance or flat insurance.
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:25:52,829'); seek(1552.0)">
              the prompt is pretty straightforward.
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:25:54,919'); seek(1554.0)">
              We ask to categorize one, two, three, and this prompt illustrates one of the
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:25:59,379'); seek(1559.0)">
              techniques I mentioned giving model and So if the question is not relevant, you
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:26:05,404'); seek(1565.0)">
              better, say it's not relevant, just, mark it as three in this particular
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:26:09,614'); seek(1569.0)">
              case, rather than force push it to either auto or home flood insurance.
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:26:14,104'); seek(1574.0)">
              Again, we'll save some time on validation of the results.
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:26:18,044'); seek(1578.0)">
              This prompt might work good.
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:26:20,334'); seek(1580.0)">
              If it still fails in some, situations, you can educate this a bit.
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:26:25,874'); seek(1585.0)">
              Use the same prompt, but somewhere in the middle, inject a couple of examples,
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:26:31,514'); seek(1591.0)">
              maybe examples that use specific words from this specific, field, maybe it's
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:26:36,844'); seek(1596.0)">
              like from your specific geo area, or I don't know, something that is from
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:26:41,044'); seek(1601.0)">
              a real use cases, and, where a model, for example, failed last time, you
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:26:46,274'); seek(1606.0)">
              can supply this as an example and you'll get definitely better results.
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:26:50,769'); seek(1610.0)">
              Also, you have to find the balance.
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:26:52,969'); seek(1612.0)">
              your prompt becomes a bit.
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:26:54,769'); seek(1614.0)">
              Longer, right?
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:26:55,629'); seek(1615.0)">
              That means a bit more expensive, maybe, slightly, longer time to
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:27:00,209'); seek(1620.0)">
              process completion, but, content or like result is the king after all.
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:27:04,819'); seek(1624.0)">
              We want, perfect, output.
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:27:06,969'); seek(1626.0)">
              Another super powerful technique called chain of output.
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:27:10,374'); seek(1630.0)">
              this particular example is solving math problem.
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:27:13,684'); seek(1633.0)">
              First of all, maybe, I'd say that LLMs are not perfect mathematicians at all.
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:27:19,834'); seek(1639.0)">
              and, normally you don't use this for solving any kind of, math problems.
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:27:24,604'); seek(1644.0)">
              But, I really want to illustrate this technique, by its, simpler
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:27:28,474'); seek(1648.0)">
              of all to illustrate it by exactly this math problem.
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:27:31,254'); seek(1651.0)">
              story.
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:27:31,734'); seek(1651.0)">
              So just imagine that we, want to ask, for an answer on some simple math operation.
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:27:38,034'); seek(1658.0)">
              So in the white, prompt in the light blue, there is, output or
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:27:42,154'); seek(1662.0)">
              completion, more technical, term result is here and it's wrong.
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:27:46,944'); seek(1666.0)">
              So 8 million liters per year is completely incorrect.
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:27:50,084'); seek(1670.0)">
              I don't want to dive too deep.
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:27:51,994'); seek(1671.0)">
              Why that happens in very simple words, LLMs.
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:27:55,129'); seek(1675.0)">
              are trying to find the lowest hanging fruit in, in all
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:27:58,939'); seek(1678.0)">
              this, kind of calculations.
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:28:00,299'); seek(1680.0)">
              Of course, it's not real calculations.
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:28:02,569'); seek(1682.0)">
              but I want to explain to you how to fix this and, possibly many other
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:28:07,799'); seek(1687.0)">
              situations, not all, not, limited to any kind of, math or If we add one simple
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:28:17,429'); seek(1697.0)">
              sentence, let's think step by step and explain calculations step by step.
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:28:21,789'); seek(1701.0)">
              As a result, our completion will be a bit longer, that's fine.
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:28:25,769'); seek(1705.0)">
              But the most important thing is the time.
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:28:28,754'); seek(1708.0)">
              The answer is correct.
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:28:30,394'); seek(1710.0)">
              And, also, by the way, this is not, exact wording.
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:28:33,744'); seek(1713.0)">
              You have to use all the time.
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:28:35,284'); seek(1715.0)">
              This let's think step by step.
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:28:36,594'); seek(1716.0)">
              No, you can experiment with a longer, shorter description, something like
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:28:41,574'); seek(1721.0)">
              customized description, but you have to, force model to literally think step
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:28:48,124'); seek(1728.0)">
              by step or provide reasoning behind every step and results will be better.
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:28:52,984'); seek(1732.0)">
              you will clearly see this.
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:28:54,484'); seek(1734.0)">
              Another technique is not exactly about prompting itself, but how we construct
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:29:00,994'); seek(1740.0)">
              overall communication with LLMs.
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:29:04,314'); seek(1744.0)">
              And in many cases, we need to send multiple calls to
            </span>
            
            <span id="chunk-429" class="transcript-chunks" onclick="console.log('00:29:09,794'); seek(1749.0)">
              complete one particular task.
            </span>
            
            <span id="chunk-430" class="transcript-chunks" onclick="console.log('00:29:11,614'); seek(1751.0)">
              I already told you that.
            </span>
            
            <span id="chunk-431" class="transcript-chunks" onclick="console.log('00:29:13,024'); seek(1753.0)">
              Not good idea to multitask one prompt for one operation, but, you can easily
            </span>
            
            <span id="chunk-432" class="transcript-chunks" onclick="console.log('00:29:20,054'); seek(1760.0)">
              organize prompt chaining by orchestrating some your, some, backend tooling.
            </span>
            
            <span id="chunk-433" class="transcript-chunks" onclick="console.log('00:29:24,594'); seek(1764.0)">
              And, you can use again, multiple models for multiple tasks, hosted
            </span>
            
            <span id="chunk-434" class="transcript-chunks" onclick="console.log('00:29:29,534'); seek(1769.0)">
              model from one provider, another provider, your own model, local model,
            </span>
            
            <span id="chunk-435" class="transcript-chunks" onclick="console.log('00:29:33,614'); seek(1773.0)">
              fine tuned model, general model.
            </span>
            
            <span id="chunk-436" class="transcript-chunks" onclick="console.log('00:29:36,014'); seek(1776.0)">
              you decide, right?
            </span>
            
            <span id="chunk-437" class="transcript-chunks" onclick="console.log('00:29:37,144'); seek(1777.0)">
              What works.
            </span>
            
            <span id="chunk-438" class="transcript-chunks" onclick="console.log('00:29:37,889'); seek(1777.0)">
              Best for yourself.
            </span>
            
            <span id="chunk-439" class="transcript-chunks" onclick="console.log('00:29:39,619'); seek(1779.0)">
              And the whole idea is very simple.
            </span>
            
            <span id="chunk-440" class="transcript-chunks" onclick="console.log('00:29:41,239'); seek(1781.0)">
              You can use output of a previous call to the model or part of this output as
            </span>
            
            <span id="chunk-441" class="transcript-chunks" onclick="console.log('00:29:46,919'); seek(1786.0)">
              a part of the input for your next call.
            </span>
            
            <span id="chunk-442" class="transcript-chunks" onclick="console.log('00:29:49,439'); seek(1789.0)">
              So this way you achieve a very good, situation when you, literally, get
            </span>
            
            <span id="chunk-443" class="transcript-chunks" onclick="console.log('00:29:55,209'); seek(1795.0)">
              what you want with, minimal efforts.
            </span>
            
            <span id="chunk-444" class="transcript-chunks" onclick="console.log('00:29:57,825'); seek(1797.0)">
              Now I want to introduce Cambridge dictionary word of the year 2023.
            </span>
            
            <span id="chunk-445" class="transcript-chunks" onclick="console.log('00:30:03,477'); seek(1803.0)">
              2023.
            </span>
            
            <span id="chunk-446" class="transcript-chunks" onclick="console.log('00:30:04,448'); seek(1804.0)">
              And this is hallucination.
            </span>
            
            <span id="chunk-447" class="transcript-chunks" onclick="console.log('00:30:07,018'); seek(1807.0)">
              Hallucination in context of our interactions with
            </span>
            
            <span id="chunk-448" class="transcript-chunks" onclick="console.log('00:30:09,998'); seek(1809.0)">
              the large language models.
            </span>
            
            <span id="chunk-449" class="transcript-chunks" onclick="console.log('00:30:11,748'); seek(1811.0)">
              And this is something that is really annoying in all kinds of AI engineering,
            </span>
            
            <span id="chunk-450" class="transcript-chunks" onclick="console.log('00:30:17,498'); seek(1817.0)">
              in all kinds of prompt engineering, all kinds of building AI infused applications.
            </span>
            
            <span id="chunk-451" class="transcript-chunks" onclick="console.log('00:30:22,978'); seek(1822.0)">
              So this is again, outcome of, how LMS were, designed,
            </span>
            
            <span id="chunk-452" class="transcript-chunks" onclick="console.log('00:30:28,178'); seek(1828.0)">
              invented, and how they work.
            </span>
            
            <span id="chunk-453" class="transcript-chunks" onclick="console.log('00:30:30,178'); seek(1830.0)">
              in, in summary, they are very good in making up facts and,
            </span>
            
            <span id="chunk-454" class="transcript-chunks" onclick="console.log('00:30:34,748'); seek(1834.0)">
              making this in a very trusty way.
            </span>
            
            <span id="chunk-455" class="transcript-chunks" onclick="console.log('00:30:37,118'); seek(1837.0)">
              So it's very, complex sometimes to identify what is, correct in this
            </span>
            
            <span id="chunk-456" class="transcript-chunks" onclick="console.log('00:30:42,878'); seek(1842.0)">
              output and what is wrong if we talk about some, as some facts we requested.
            </span>
            
            <span id="chunk-457" class="transcript-chunks" onclick="console.log('00:30:48,718'); seek(1848.0)">
              Fortunately, there are multiple ways for, if not removing completely,
            </span>
            
            <span id="chunk-458" class="transcript-chunks" onclick="console.log('00:30:53,468'); seek(1853.0)">
              but reducing hallucination, mitigating its consequences.
            </span>
            
            <span id="chunk-459" class="transcript-chunks" onclick="console.log('00:30:58,648'); seek(1858.0)">
              For example, you can explain model not only what you want, but also what you
            </span>
            
            <span id="chunk-460" class="transcript-chunks" onclick="console.log('00:31:03,308'); seek(1863.0)">
              don't want to receive back from this.
            </span>
            
            <span id="chunk-461" class="transcript-chunks" onclick="console.log('00:31:05,298'); seek(1865.0)">
              So you can limit number of use cases.
            </span>
            
            <span id="chunk-462" class="transcript-chunks" onclick="console.log('00:31:09,143'); seek(1869.0)">
              Also, this is a recommendation that I already introduced, give, model and out.
            </span>
            
            <span id="chunk-463" class="transcript-chunks" onclick="console.log('00:31:13,883'); seek(1873.0)">
              If you're not sure, say, I don't know.
            </span>
            
            <span id="chunk-464" class="transcript-chunks" onclick="console.log('00:31:16,573'); seek(1876.0)">
              This is, this could, this simple statement could be your part of the prompt.
            </span>
            
            <span id="chunk-465" class="transcript-chunks" onclick="console.log('00:31:21,113'); seek(1881.0)">
              As well as this one, sometimes it's, of course, it's super naive technique, right?
            </span>
            
            <span id="chunk-466" class="transcript-chunks" onclick="console.log('00:31:25,403'); seek(1885.0)">
              But sometimes that, that works.
            </span>
            
            <span id="chunk-467" class="transcript-chunks" onclick="console.log('00:31:27,063'); seek(1887.0)">
              Don't make up facts.
            </span>
            
            <span id="chunk-468" class="transcript-chunks" onclick="console.log('00:31:28,513'); seek(1888.0)">
              As a part of your prompt, if there's a chance for you to organize, multiple
            </span>
            
            <span id="chunk-469" class="transcript-chunks" onclick="console.log('00:31:33,053'); seek(1893.0)">
              short conversation with, your LLM, like chat, like maybe you can, ask every
            </span>
            
            <span id="chunk-470" class="transcript-chunks" onclick="console.log('00:31:38,053'); seek(1898.0)">
              time, are you sure that you have all information to answer this question?
            </span>
            
            <span id="chunk-471" class="transcript-chunks" onclick="console.log('00:31:41,723'); seek(1901.0)">
              If not, you better request some extra months.
            </span>
            
            <span id="chunk-472" class="transcript-chunks" onclick="console.log('00:31:44,693'); seek(1904.0)">
              step by step reasoning and asking model to explain along with the answer.
            </span>
            
            <span id="chunk-473" class="transcript-chunks" onclick="console.log('00:31:49,983'); seek(1909.0)">
              Basically, this is a chain of thoughts technique also helps,
            </span>
            
            <span id="chunk-474" class="transcript-chunks" onclick="console.log('00:31:54,313'); seek(1914.0)">
              but all these points, what on the picture are nothing compared to this.
            </span>
            
            <span id="chunk-475" class="transcript-chunks" onclick="console.log('00:32:00,623'); seek(1920.0)">
              You can dynamically find and inject relevant context in information
            </span>
            
            <span id="chunk-476" class="transcript-chunks" onclick="console.log('00:32:06,093'); seek(1926.0)">
              straight into your prompt.
            </span>
            
            <span id="chunk-477" class="transcript-chunks" onclick="console.log('00:32:07,843'); seek(1927.0)">
              And you can say, Use only this information for answer.
            </span>
            
            <span id="chunk-478" class="transcript-chunks" onclick="console.log('00:32:12,213'); seek(1932.0)">
              I mean forget all your knowledge from wikipedia and the public books
            </span>
            
            <span id="chunk-479" class="transcript-chunks" onclick="console.log('00:32:16,623'); seek(1936.0)">
              Discard this we only need you to use this data in its trade in the prompt.
            </span>
            
            <span id="chunk-480" class="transcript-chunks" onclick="console.log('00:32:22,253'); seek(1942.0)">
              I can illustrate this by Example of us in the role of developers who
            </span>
            
            <span id="chunk-481" class="transcript-chunks" onclick="console.log('00:32:30,523'); seek(1950.0)">
              build internal application for Say our employees to investigate what's possible
            </span>
            
            <span id="chunk-482" class="transcript-chunks" onclick="console.log('00:32:37,128'); seek(1957.0)">
              in medical insurance coverage for them.
            </span>
            
            <span id="chunk-483" class="transcript-chunks" onclick="console.log('00:32:40,038'); seek(1960.0)">
              And let's imagine that we got a question from employee, via chat bot, or again,
            </span>
            
            <span id="chunk-484" class="transcript-chunks" onclick="console.log('00:32:44,898'); seek(1964.0)">
              via some automated email engine.
            </span>
            
            <span id="chunk-485" class="transcript-chunks" onclick="console.log('00:32:47,028'); seek(1967.0)">
              Does my health plan cover annual eye exams?
            </span>
            
            <span id="chunk-486" class="transcript-chunks" onclick="console.log('00:32:50,258'); seek(1970.0)">
              And at the top of the prompt, that we explain all the, prerequisites.
            </span>
            
            <span id="chunk-487" class="transcript-chunks" onclick="console.log('00:32:53,708'); seek(1973.0)">
              We are intelligent assistant and, yeah, you can answer, questions about health
            </span>
            
            <span id="chunk-488" class="transcript-chunks" onclick="console.log('00:32:59,958'); seek(1979.0)">
              care and use Only sources below for the answer and we also directly inject into
            </span>
            
            <span id="chunk-489" class="transcript-chunks" onclick="console.log('00:33:08,278'); seek(1988.0)">
              this prompt These three sources that are relevant for answering this question.
            </span>
            
            <span id="chunk-490" class="transcript-chunks" onclick="console.log('00:33:13,698'); seek(1993.0)">
              Sounds very simple, right?
            </span>
            
            <span id="chunk-491" class="transcript-chunks" onclick="console.log('00:33:15,258'); seek(1995.0)">
              But of course we have to and yeah completion in that case
            </span>
            
            <span id="chunk-492" class="transcript-chunks" onclick="console.log('00:33:20,178'); seek(2000.0)">
              will be nice and 100% Correct.
            </span>
            
            <span id="chunk-493" class="transcript-chunks" onclick="console.log('00:33:22,808'); seek(2002.0)">
              But how do we exactly identify these sources?
            </span>
            
            <span id="chunk-494" class="transcript-chunks" onclick="console.log('00:33:26,218'); seek(2006.0)">
              How do we find these sources?
            </span>
            
            <span id="chunk-495" class="transcript-chunks" onclick="console.log('00:33:28,508'); seek(2008.0)">
              These few sentences in, for example, dozens of, PDFs or documents
            </span>
            
            <span id="chunk-496" class="transcript-chunks" onclick="console.log('00:33:33,108'); seek(2013.0)">
              or somewhere in, in database.
            </span>
            
            <span id="chunk-497" class="transcript-chunks" onclick="console.log('00:33:35,978'); seek(2015.0)">
              How we identify these sources?
            </span>
            
            <span id="chunk-498" class="transcript-chunks" onclick="console.log('00:33:37,808'); seek(2017.0)">
              How we, shorten these sources?
            </span>
            
            <span id="chunk-499" class="transcript-chunks" onclick="console.log('00:33:39,608'); seek(2019.0)">
              How we rank them after all to provide top 3 or 5 or 10,
            </span>
            
            <span id="chunk-500" class="transcript-chunks" onclick="console.log('00:33:43,718'); seek(2023.0)">
              depending on your use case, items.
            </span>
            
            <span id="chunk-501" class="transcript-chunks" onclick="console.log('00:33:46,238'); seek(2026.0)">
              And Answer to this question is Retrieval Augmented Generation Pattern or REG.
            </span>
            
            <span id="chunk-502" class="transcript-chunks" onclick="console.log('00:33:52,958'); seek(2032.0)">
              as name says, it's about retrieve these sources, these data sources, augment
            </span>
            
            <span id="chunk-503" class="transcript-chunks" onclick="console.log('00:33:58,678'); seek(2038.0)">
              them and, augment your promptery and after all generate completion.
            </span>
            
            <span id="chunk-504" class="transcript-chunks" onclick="console.log('00:34:02,698'); seek(2042.0)">
              Generate completion is trivial.
            </span>
            
            <span id="chunk-505" class="transcript-chunks" onclick="console.log('00:34:04,188'); seek(2044.0)">
              This is after, about, sending your final, Call to, final prompt
            </span>
            
            <span id="chunk-506" class="transcript-chunks" onclick="console.log('00:34:08,858'); seek(2048.0)">
              to LLM augment is even simpler.
            </span>
            
            <span id="chunk-507" class="transcript-chunks" onclick="console.log('00:34:11,738'); seek(2051.0)">
              This is just a string operation.
            </span>
            
            <span id="chunk-508" class="transcript-chunks" onclick="console.log('00:34:13,558'); seek(2053.0)">
              This is, literally inserting sources of information into your prompt.
            </span>
            
            <span id="chunk-509" class="transcript-chunks" onclick="console.log('00:34:18,408'); seek(2058.0)">
              Like we've seen on this previous example, retrieve is the real magic.
            </span>
            
            <span id="chunk-510" class="transcript-chunks" onclick="console.log('00:34:22,838'); seek(2062.0)">
              How?
            </span>
            
            <span id="chunk-511" class="transcript-chunks" onclick="console.log('00:34:23,658'); seek(2063.0)">
              based on this particular question or on, question plus previous conversation, we
            </span>
            
            <span id="chunk-512" class="transcript-chunks" onclick="console.log('00:34:28,608'); seek(2068.0)">
              really identify data we need from, again, back to, use case with, dozens of PDFs
            </span>
            
            <span id="chunk-513" class="transcript-chunks" onclick="console.log('00:34:34,698'); seek(2074.0)">
              with all this insurance, information, and of course, here we can leverage.
            </span>
            
            <span id="chunk-514" class="transcript-chunks" onclick="console.log('00:34:39,523'); seek(2079.0)">
              one more large language model, this one, say one that we use for the final
            </span>
            
            <span id="chunk-515" class="transcript-chunks" onclick="console.log('00:34:43,773'); seek(2083.0)">
              call, or most likely in vast majority of situations, this will be completely
            </span>
            
            <span id="chunk-516" class="transcript-chunks" onclick="console.log('00:34:47,473'); seek(2087.0)">
              different one, specialized, and also, you might want to, vectorize your
            </span>
            
            <span id="chunk-517" class="transcript-chunks" onclick="console.log('00:34:53,363'); seek(2093.0)">
              request and, those you have to send this request to vectorize database.
            </span>
            
            <span id="chunk-518" class="transcript-chunks" onclick="console.log('00:34:58,153'); seek(2098.0)">
              Unfortunately, it's all, out of our today's schedule.
            </span>
            
            <span id="chunk-519" class="transcript-chunks" onclick="console.log('00:35:00,598'); seek(2100.0)">
              Scope this could be your homework to learn more about retrieve component of
            </span>
            
            <span id="chunk-520" class="transcript-chunks" onclick="console.log('00:35:05,848'); seek(2105.0)">
              rag and in the real world It's even more complex because it could be multiple
            </span>
            
            <span id="chunk-521" class="transcript-chunks" onclick="console.log('00:35:11,748'); seek(2111.0)">
              sources of this information, right?
            </span>
            
            <span id="chunk-522" class="transcript-chunks" onclick="console.log('00:35:13,788'); seek(2113.0)">
              So before Constructing your final prompt that contains everything you might
            </span>
            
            <span id="chunk-523" class="transcript-chunks" onclick="console.log('00:35:18,748'); seek(2118.0)">
              want to send requests to your Internal database to some external API and who
            </span>
            
            <span id="chunk-524" class="transcript-chunks" onclick="console.log('00:35:25,168'); seek(2125.0)">
              knows where else so maybe you might Want to have some orchestration support.
            </span>
            
            <span id="chunk-525" class="transcript-chunks" onclick="console.log('00:35:32,568'); seek(2132.0)">
              Surprise, surprise.
            </span>
            
            <span id="chunk-526" class="transcript-chunks" onclick="console.log('00:35:33,618'); seek(2133.0)">
              In real world, it's even more complex than just, this orchestration.
            </span>
            
            <span id="chunk-527" class="transcript-chunks" onclick="console.log('00:35:38,018'); seek(2138.0)">
              and if you talk not about this particular call to or chain of
            </span>
            
            <span id="chunk-528" class="transcript-chunks" onclick="console.log('00:35:41,718'); seek(2141.0)">
              calls to, LLM, but also about.
            </span>
            
            <span id="chunk-529" class="transcript-chunks" onclick="console.log('00:35:43,928'); seek(2143.0)">
              Full life cycle of our AI infused application or like
            </span>
            
            <span id="chunk-530" class="transcript-chunks" onclick="console.log('00:35:47,898'); seek(2147.0)">
              a generative AI part of this.
            </span>
            
            <span id="chunk-531" class="transcript-chunks" onclick="console.log('00:35:50,268'); seek(2150.0)">
              we want to get some tools, some, frameworks for ideation, for
            </span>
            
            <span id="chunk-532" class="transcript-chunks" onclick="console.log('00:35:55,528'); seek(2155.0)">
              building, for making everything, real, how we deploy, how we, monitor.
            </span>
            
            <span id="chunk-533" class="transcript-chunks" onclick="console.log('00:36:00,763'); seek(2160.0)">
              it's, let's say very specialized part, DevOps, but specifically
            </span>
            
            <span id="chunk-534" class="transcript-chunks" onclick="console.log('00:36:05,703'); seek(2165.0)">
              for your LLM interactions.
            </span>
            
            <span id="chunk-535" class="transcript-chunks" onclick="console.log('00:36:08,073'); seek(2168.0)">
              We can call this LLMOps.
            </span>
            
            <span id="chunk-536" class="transcript-chunks" onclick="console.log('00:36:10,473'); seek(2170.0)">
              So let me introduce more tools that can help us both with, this,
            </span>
            
            <span id="chunk-537" class="transcript-chunks" onclick="console.log('00:36:15,203'); seek(2175.0)">
              orchestration of our calls and the operationalization of all flow.
            </span>
            
            <span id="chunk-538" class="transcript-chunks" onclick="console.log('00:36:20,923'); seek(2180.0)">
              First couple of tools I want to mention is LangChain and Semantic Kernel.
            </span>
            
            <span id="chunk-539" class="transcript-chunks" onclick="console.log('00:36:24,653'); seek(2184.0)">
              And these are amazing orchestrators of everything you need to
            </span>
            
            <span id="chunk-540" class="transcript-chunks" onclick="console.log('00:36:29,703'); seek(2189.0)">
              interact with any kind of LLM.
            </span>
            
            <span id="chunk-541" class="transcript-chunks" onclick="console.log('00:36:32,353'); seek(2192.0)">
              So these two libraries or frameworks, you name it, are very similar.
            </span>
            
            <span id="chunk-542" class="transcript-chunks" onclick="console.log('00:36:36,373'); seek(2196.0)">
              They support slightly different, Set of programming language slightly
            </span>
            
            <span id="chunk-543" class="transcript-chunks" onclick="console.log('00:36:39,643'); seek(2199.0)">
              different set of Platforms but in a nutshell they are helping you to Get
            </span>
            
            <span id="chunk-544" class="transcript-chunks" onclick="console.log('00:36:46,233'); seek(2206.0)">
              very nice abstraction over all your interactions with LLMs so you don't
            </span>
            
            <span id="chunk-545" class="transcript-chunks" onclick="console.log('00:36:52,603'); seek(2212.0)">
              need to write all these calls all this ping pong from scratch also There are
            </span>
            
            <span id="chunk-546" class="transcript-chunks" onclick="console.log('00:36:58,233'); seek(2218.0)">
              many more very interesting features included in both of the framework.
            </span>
            
            <span id="chunk-547" class="transcript-chunks" onclick="console.log('00:37:03,993'); seek(2223.0)">
              So When you start your new AI infused application, don't ever
            </span>
            
            <span id="chunk-548" class="transcript-chunks" onclick="console.log('00:37:08,358'); seek(2228.0)">
              start, like your, low level, communication with LLM from scratch.
            </span>
            
            <span id="chunk-549" class="transcript-chunks" onclick="console.log('00:37:12,838'); seek(2232.0)">
              Find your perfect, level of abstraction, in these two or any other frameworks.
            </span>
            
            <span id="chunk-550" class="transcript-chunks" onclick="console.log('00:37:18,358'); seek(2238.0)">
              And for overall orchestration or personalization of your LLM application,
            </span>
            
            <span id="chunk-551" class="transcript-chunks" onclick="console.log('00:37:23,968'); seek(2243.0)">
              I recommend you to have a look at PromptFlow, another open source tool.
            </span>
            
            <span id="chunk-552" class="transcript-chunks" onclick="console.log('00:37:28,858'); seek(2248.0)">
              this one also created by my colleagues from Microsoft that in code first
            </span>
            
            <span id="chunk-553" class="transcript-chunks" onclick="console.log('00:37:34,348'); seek(2254.0)">
              way can help you again to keep a full control over full cycle of your LLM,
            </span>
            
            <span id="chunk-554" class="transcript-chunks" onclick="console.log('00:37:43,158'); seek(2263.0)">
              development starting from experimentation.
            </span>
            
            <span id="chunk-555" class="transcript-chunks" onclick="console.log('00:37:45,813'); seek(2265.0)">
              Up to monitoring
            </span>
            
            <span id="chunk-556" class="transcript-chunks" onclick="console.log('00:37:47,773'); seek(2267.0)">
              from flow.
            </span>
            
            <span id="chunk-557" class="transcript-chunks" onclick="console.log('00:37:48,303'); seek(2268.0)">
              You I could look like this.
            </span>
            
            <span id="chunk-558" class="transcript-chunks" onclick="console.log('00:37:49,703'); seek(2269.0)">
              This particular short video is from, the hosted version.
            </span>
            
            <span id="chunk-559" class="transcript-chunks" onclick="console.log('00:37:53,333'); seek(2273.0)">
              There is one on Azure, but you can get the same UI straight in
            </span>
            
            <span id="chunk-560" class="transcript-chunks" onclick="console.log('00:37:59,063'); seek(2279.0)">
              your, VS code ID, via respective.
            </span>
            
            <span id="chunk-561" class="transcript-chunks" onclick="console.log('00:38:02,103'); seek(2282.0)">
              extension called PromptFlow.
            </span>
            
            <span id="chunk-562" class="transcript-chunks" onclick="console.log('00:38:04,203'); seek(2284.0)">
              basically it reproduces your interaction with LLMs as a graph where some
            </span>
            
            <span id="chunk-563" class="transcript-chunks" onclick="console.log('00:38:12,573'); seek(2292.0)">
              nodes are pieces of Python code.
            </span>
            
            <span id="chunk-564" class="transcript-chunks" onclick="console.log('00:38:14,973'); seek(2294.0)">
              Some nodes are calling of LLMs.
            </span>
            
            <span id="chunk-565" class="transcript-chunks" onclick="console.log('00:38:17,843'); seek(2297.0)">
              Some nodes represent prompts where you can iterate through multiple variants,
            </span>
            
            <span id="chunk-566" class="transcript-chunks" onclick="console.log('00:38:23,403'); seek(2303.0)">
              for example, to identify what is the best.
            </span>
            
            <span id="chunk-567" class="transcript-chunks" onclick="console.log('00:38:25,693'); seek(2305.0)">
              And after all, you can host everything on the cloud of your choice.
            </span>
            
            <span id="chunk-568" class="transcript-chunks" onclick="console.log('00:38:29,333'); seek(2309.0)">
              There are many learning resources available about prompt engineering,
            </span>
            
            <span id="chunk-569" class="transcript-chunks" onclick="console.log('00:38:34,763'); seek(2314.0)">
              and, I hope that you will get PDF of, this session with all active links.
            </span>
            
            <span id="chunk-570" class="transcript-chunks" onclick="console.log('00:38:41,113'); seek(2321.0)">
              What is the future of prompt engineering?
            </span>
            
            <span id="chunk-571" class="transcript-chunks" onclick="console.log('00:38:43,753'); seek(2323.0)">
              I have set up the questions.
            </span>
            
            <span id="chunk-572" class="transcript-chunks" onclick="console.log('00:38:45,383'); seek(2325.0)">
              I will not give you answers on.
            </span>
            
            <span id="chunk-573" class="transcript-chunks" onclick="console.log('00:38:47,403'); seek(2327.0)">
              I encourage you, yourself find the answer.
            </span>
            
            <span id="chunk-574" class="transcript-chunks" onclick="console.log('00:38:51,193'); seek(2331.0)">
              For example.
            </span>
            
            <span id="chunk-575" class="transcript-chunks" onclick="console.log('00:38:52,283'); seek(2332.0)">
              Will it be a separate job title or just essential skill?
            </span>
            
            <span id="chunk-576" class="transcript-chunks" onclick="console.log('00:38:56,013'); seek(2336.0)">
              Will it become simpler with tools like Longchain, Semantic Kernel, PromptFlow?
            </span>
            
            <span id="chunk-577" class="transcript-chunks" onclick="console.log('00:39:01,483'); seek(2341.0)">
              Or they become more complex because there is multi modality, you
            </span>
            
            <span id="chunk-578" class="transcript-chunks" onclick="console.log('00:39:04,833'); seek(2344.0)">
              have to closely work with vectors if you talk about React, etc.
            </span>
            
            <span id="chunk-579" class="transcript-chunks" onclick="console.log('00:39:10,113'); seek(2350.0)">
              Doos, will it be democratized and pretty much everyone who ever sent a request
            </span>
            
            <span id="chunk-580" class="transcript-chunks" onclick="console.log('00:39:15,553'); seek(2355.0)">
              to ChatGPT will call themselves Doos?
            </span>
            
            <span id="chunk-581" class="transcript-chunks" onclick="console.log('00:39:17,318'); seek(2357.0)">
              Prompt engineer those, there'll be title inflation or it will be more gated,
            </span>
            
            <span id="chunk-582" class="transcript-chunks" onclick="console.log('00:39:22,818'); seek(2362.0)">
              discipline and who will benefit the most people who understand how our language
            </span>
            
            <span id="chunk-583" class="transcript-chunks" onclick="console.log('00:39:28,888'); seek(2368.0)">
              constructed linguists or people who understand how code works technologies.
            </span>
            
            <span id="chunk-584" class="transcript-chunks" onclick="console.log('00:39:33,788'); seek(2373.0)">
              Or maybe people who know everything about this particular domain,
            </span>
            
            <span id="chunk-585" class="transcript-chunks" onclick="console.log('00:39:38,778'); seek(2378.0)">
              where this application comes from and able to formulate the problem.
            </span>
            
            <span id="chunk-586" class="transcript-chunks" onclick="console.log('00:39:43,928'); seek(2383.0)">
              And do we survive competition with LLM based prompt engineers?
            </span>
            
            <span id="chunk-587" class="transcript-chunks" onclick="console.log('00:39:48,528'); seek(2388.0)">
              Because LLMs are also perfect in creating prompts.
            </span>
            
            <span id="chunk-588" class="transcript-chunks" onclick="console.log('00:39:51,828'); seek(2391.0)">
              I invite you all to the Prompt Engineering Conference I organize, in November
            </span>
            
            <span id="chunk-589" class="transcript-chunks" onclick="console.log('00:39:56,938'); seek(2396.0)">
              and this will be second edition.
            </span>
            
            <span id="chunk-590" class="transcript-chunks" onclick="console.log('00:39:58,558'); seek(2398.0)">
              First of edition was, last year, super popular.
            </span>
            
            <span id="chunk-591" class="transcript-chunks" onclick="console.log('00:40:02,128'); seek(2402.0)">
              and, if you watch this session before November 20, just go
            </span>
            
            <span id="chunk-592" class="transcript-chunks" onclick="console.log('00:40:07,038'); seek(2407.0)">
              and register your ticket.
            </span>
            
            <span id="chunk-593" class="transcript-chunks" onclick="console.log('00:40:08,058'); seek(2408.0)">
              If you watch this after, still go to the same URL.
            </span>
            
            <span id="chunk-594" class="transcript-chunks" onclick="console.log('00:40:11,918'); seek(2411.0)">
              You will find all sessions recorded straight there.
            </span>
            
            <span id="chunk-595" class="transcript-chunks" onclick="console.log('00:40:16,418'); seek(2416.0)">
              And it's online, it's free.
            </span>
            
            <span id="chunk-596" class="transcript-chunks" onclick="console.log('00:40:17,748'); seek(2417.0)">
              It's free, open for everyone.
            </span>
            
            <span id="chunk-597" class="transcript-chunks" onclick="console.log('00:40:19,878'); seek(2419.0)">
              Thank you very much for watching this session and my last prompt for everyone.
            </span>
            
            <span id="chunk-598" class="transcript-chunks" onclick="console.log('00:40:26,398'); seek(2426.0)">
              Let's stay in touch.
            </span>
            
            <span id="chunk-599" class="transcript-chunks" onclick="console.log('00:40:27,518'); seek(2427.0)">
              This is my LinkedIn profile.
            </span>
            
            <span id="chunk-600" class="transcript-chunks" onclick="console.log('00:40:28,988'); seek(2428.0)">
              Find me, just scan this QR code or find Maxim Salnikov Microsoft on LinkedIn.
            </span>
            
            <span id="chunk-601" class="transcript-chunks" onclick="console.log('00:40:34,148'); seek(2434.0)">
              And it's my great pleasure to stay connected with you.
            </span>
            
            <span id="chunk-602" class="transcript-chunks" onclick="console.log('00:40:37,088'); seek(2437.0)">
              Ask me about, prompt engineering, AI engineering, any questions
            </span>
            
            <span id="chunk-603" class="transcript-chunks" onclick="console.log('00:40:41,068'); seek(2441.0)">
              or, any questions about web development and cloud in general.
            </span>
            
            <span id="chunk-604" class="transcript-chunks" onclick="console.log('00:40:45,368'); seek(2445.0)">
              Thank you very much.
            </span>
            
            </div>
          </div>
          
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/Maxim%20Salnikov%20-%20Conf42%20Prompt%20Engineering%202024.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/Maxim%20Salnikov%20-%20Conf42%20Prompt%20Engineering%202024.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #749BC2;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/prompt2024" class="btn btn-sm btn-danger shadow lift" style="background-color: #749BC2;">
                <i class="fe fe-grid me-2"></i>
                See all 40 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Maxim%20Salnikov_prompt.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Maxim Salnikov
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Digital and App Innovation Business Lead, Western Europe @ Microsoft
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/webmax/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Maxim Salnikov's LinkedIn account" />
                  </a>
                  
                  
                  <a href="https://twitter.com/webmaxru" target="_blank">
                    <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="Maxim Salnikov's twitter account" />
                  </a>
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by webmaxru"
                  data-url="https://www.conf42.com/prompt2024"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/prompt2024"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>






    <!-- WELCOME -->
    <section class="pt-8 pt-md-11 pb-10 pb-md-15 bg-info" id="register">

      <!-- Shape -->
      <div class="shape shape-blur-3 text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>      </div>

      <!-- Content -->
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 col-md-10 col-lg-8 text-center">

            <!-- Heading -->
            <h1 class="display-2 text-white">
              Join the community!
            </h1>

            <!-- Text -->
            <p class="lead text-white text-opacity-80 mb-6 mb-md-8">
              Learn for free, join the best tech learning community 
              for a <a class="text-white" href="https://www.reddit.com/r/sanfrancisco/comments/1bz90f6/why_are_coffee_shops_in_sf_so_expensive/" target="_blank">price of a pumpkin latte</a>.
            </p>

            <!-- Form -->
            <form class="d-flex align-items-center justify-content-center mb-7 mb-md-9">

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Annual
              </span>

              <!-- Switch -->
              <div class="form-check form-check-dark form-switch mx-3">
                <input class="form-check-input" type="checkbox" id="billingSwitch" data-toggle="price" data-target=".price">
              </div>

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Monthly
              </span>

            </form>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->

    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- PRICING -->
    <section class="mt-n8 mt-md-n15">
      <div class="container">
        <div class="row gx-4">
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-1">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Newsletter</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="0" data-monthly="0">0</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Event notifications, weekly newsletter
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Delayed access</b> to all content
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Immediate access to Keynotes & Panels
                  </p>
                </div>
              
              
              </div>
            </div>

            <!-- Card -->
            <div class="card shadow-lg mb-6 border border-success">
              <div class="card-body">

                <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
</script>

<!-- Form -->
<link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
<p class="emailoctopus-success-message text-success"></p>
<p class="emailoctopus-error-message text-danger"></p>
<form
    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
    method="post"
    data-message-success="Thanks! Check your email for further directions!"
    data-message-missing-email-address="Your email address is required."
    data-message-invalid-email-address="Your email address looks incorrect, please try again."
    data-message-bot-submission-error="This doesn't look like a human submission."
    data-message-consent-required="Please check the checkbox to indicate your consent."
    data-message-invalid-parameters-error="This form has missing or invalid fields."
    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
    class="emailoctopus-form"
    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
>
<div class="form-floating emailoctopus-form-row">
    <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
    <label for="field_0">Email address</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
    <label for="field_1">First Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
    <label for="field_2">Last Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
    <label for="field_4">Company</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
    <label for="field_5">Job Title</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
    <label for="field_3">Phone Number</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
    oninput="updateCountry()"
    >
    <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
    </select>
    <label for="field_7">Country</label>
</div>
<input id="country-destination" name="field_7" type="hidden">
<input id="tz-country" name="field_8" type="hidden">

<input
    name="field_6"
    type="hidden"
    value="Prompt Engineering"
>

<div class="emailoctopus-form-row-consent">
    <input
    type="checkbox"
    id="consent"
    name="consent"
    >
    <label for="consent">
    I consent to the following terms:
    </label>
    <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
    Terms and Conditions
    </a>
    &amp;
    <a href="./code-of-conduct" target="_blank">
    Code of Conduct
    </a>
</div>
<div
    aria-hidden="true"
    class="emailoctopus-form-row-hp"
>
    <input
    type="text"
    name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
    tabindex="-1"
    autocomplete="nope"
    >
</div>
<div class="mt-6 emailoctopus-form-row-subscribe">
    <input
    type="hidden"
    name="successRedirectUrl"
    >
    <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
    Subscribe to free newsletter <i class="fe fe-arrow-right ms-3"></i>
    </button>
</div>
</form>

<!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
<script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

              </div>
            </div>
          </div>
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-0">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Community</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="8.34" data-monthly="10">8.34</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Access to <a href="https://conf42.circle.so/">Circle community platform</a>
                  </p>
                </div>

                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Immediate access</b> to all content
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank"><b>Live events!</b></a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank">Regular office hours, Q&As, CV reviews</a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Courses, quizes & certificates
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Community chats
                  </p>
                </div>
                

                <!-- Button -->
                <a href="https://conf42.circle.so/checkout/subscribe" class="btn w-100 btn-primary">
                  Join the community (7 day free trial)<i class="fe fe-arrow-right ms-3"></i>
                </a>

              </div>
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2025">
                  Observability 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2025">
                  Quantum Computing 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2025">
                  Rustlang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2025">
                  Platform Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2025">
                  MLOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2025">
                  Incident Management 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2025">
                  Kube Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2025">
                  JavaScript 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2025">
                  Prompt Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/robotics2025">
                  Robotics 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2025">
                  DevSecOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2025">
                  Internet of Things (IoT) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>