<!doctype html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-77190356-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-77190356-3');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.css" />
    <link rel="stylesheet" href="./assets/css/libs.bundle.css" />
    <link rel="stylesheet" href="./assets/css/theme.bundle.css" />
    <link rel="stylesheet" href="./assets/css/various.css" />

    <title>Conf42: Mastering Generative AI: Harnessing AWS GenAI for Your Solutions</title>
    <meta name="description" content="Everything Cloud Native and Cloud Security. It came from the Cloud!">

    
    <meta name="image" property="og:image" content="https://www.conf42.com/assets/headshots/https://conf42.github.io/static/headshots/Samuel%20Baruffi_cloud.png">
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Mastering Generative AI: Harnessing AWS GenAI for Your Solutions | Conf42"/>
    <meta property="og:description" content="Unlock the power of Generative AI with AWS GenAI! Dive into Amazon Bedrock, Trainium, Sagemaker, and more. Discover how to build, scale, and optimize your AI applications efficiently, cost-effectively, and at scale. Don't just keep up with the AI revolution, lead it!"/>
    <meta property="og:url" content="https://conf42.com/Cloud_Native_2024_Samuel_Baruffi_mastering_generative_aws"/>
    

    <link rel="shortcut icon" href="./assets/favicon/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="./assets/favicon/site.webmanifest">

    

  <!-- Reddit Pixel -->
  <script>
  !function(w,d){if(!w.rdt){var p=w.rdt=function(){p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)};p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_e019g7ndfhrm', {"optOut":false,"useDecimalCurrencyValues":true,"aaid":"<AAID-HERE>"});rdt('track', 'PageVisit');
  </script>
  <!-- DO NOT MODIFY UNLESS TO REPLACE A USER IDENTIFIER -->
  <!-- End Reddit Pixel -->

  </head>
  <body>

    <!-- NAVBAR -->
    
    <!-- <nav class="navbar navbar-expand-lg navbar-light bg-light"> -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
    
      <div class="container">
    
        <!-- Brand -->
        <a class="navbar-brand" href="./">
          <img src="./assets/conf42/conf42_logo_black_small.png" class="navbar-brand-img" alt="...">
        </a>
    
        <!-- Toggler -->
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
    
        <!-- Collapse -->
        <div class="collapse navbar-collapse" id="navbarCollapse">
    
          <!-- Toggler -->
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
            <i class="fe fe-x"></i>
          </button>
    
          <!-- Navigation -->
          <ul class="navbar-nav ms-auto">

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Events
              </a>
              <div class="dropdown-menu dropdown-menu-xl p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-6">
                    <!-- <div class="dropdown-img-start" style="background-image: url(./assets/splash/RUST2025_Event_Splash.png);"> -->
                    <div class="dropdown-img-start">
                      <!-- Heading -->
                      <h4 class="fw-bold text-white mb-0">
                        Featured event
                      </h4>
                      <!-- Text -->
                      <p class="fs-sm text-white">
                        Rustlang 2025
                      </p>
                      <p class="fs-sm text-white">
                        Premiere 2025-08-21
                      </p>
                      <!-- Button -->
                      <a href="https://www.conf42.com/rustlang2025" class="btn btn-sm btn-white shadow-dark fonFt-size-sm">
                        Learn more
                      </a>
                    </div>
                  </div>
                  <div class="col-12 col-lg-6">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
    
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2025
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2025">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2025">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2025">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2025">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2025">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2025">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2025">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2025">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2025">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2025">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2025">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2025">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/mlops2025">
                            MLOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2025">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2025">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2025">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2025">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/robotics2025">
                            Robotics
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2025">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2025">
                            Internet of Things (IoT)
                          </a>
                          
                        
                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            2024
                          </h6>
                          <!-- List -->
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devops2024">
                            DevOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ce2024">
                            Chaos Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/python2024">
                            Python
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/cloud2024">
                            Cloud Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/llms2024">
                            Large Language Models (LLMs)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/golang2024">
                            Golang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/sre2024">
                            Site Reliability Engineering (SRE)
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/ml2024">
                            Machine Learning
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/obs2024">
                            Observability
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/quantum2024">
                            Quantum Computing
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/rustlang2024">
                            Rustlang
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/platform2024">
                            Platform Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/kubenative2024">
                            Kube Native
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/im2024">
                            Incident Management
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/js2024">
                            JavaScript
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/prompt2024">
                            Prompt Engineering
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/devsecops2024">
                            DevSecOps
                          </a>
                          
                          <a class="dropdown-item" href="https://www.conf42.com/iot2024">
                            Internet of Things (IoT)
                          </a>
                          
                        

                          <!-- Heading -->
                          <h6 class="dropdown-header mt-5">
                            Info
                          </h6>
                          <a class="dropdown-item" href="./code-of-conduct">
                            Code of Conduct
                          </a>
    
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" id="navbarLandings" data-bs-toggle="dropdown" href="#" aria-haspopup="true" aria-expanded="false">
                Community
              </a>
              <div class="dropdown-menu dropdown-menu-l p-0" aria-labelledby="navbarLandings">
                <div class="row gx-0">
                  <div class="col-12 col-lg-3">
                    <div class="dropdown-body">
                      <div class="row gx-0">
                        <div class="col-12">
                          <a class="dropdown-item" href="https://conf42.circle.so/">
                            <b>Community platform login</b>
                          </a>
                          <a class="dropdown-item" href="https://discord.gg/mvHyZzRGaQ" target="_blank">
                            Discord
                          </a>
                          <a class="dropdown-item" href="./hall-of-fame">
                            Hall of Fame
                          </a>
                          <a class="dropdown-item" href="./speakers">
                            Speakers
                          </a>
                          <a class="dropdown-item" href="https://www.papercall.io/events?cfps-scope=&keywords=conf42" target="_blank">
                            Become a speaker (CFPs)
                          </a>
                          <a class="dropdown-item" href="./testimonials">
                            Testimonials
                          </a>
                          <a class="dropdown-item" href="./about">
                            About the team
                          </a>
                        </div>
                      </div> <!-- / .row -->
                    </div>
                  </div>
                </div> <!-- / .row -->
              </div>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./podcast">
                Podcast
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./blog">
                Blog
              </a>
            </li>

            <li class="nav-item">
              <a class="nav-link" href="./sponsor">
                Sponsor
              </a>
            </li>
          </ul>
    
          <!-- Button -->
          <a class="navbar-btn btn btn-sm btn-primary lift ms-auto" href="#register">
            Join the community!
          </a>
    
        </div>
    
      </div>
    </nav>



<style>
.text-selected {
  background-color: #42ba96!important;
  color: white;
}
</style>
	

    <!-- WELCOME -->
    <section class="py-5 py-md-10" style="background-color: #7B2726;">

      <!-- Shape -->
      <div class="shape shape-blur-3 svg-shim text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>
      </div>

      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 text-center" data-aos="fade-up">

            <!-- Heading -->
            <h1 class="display-2 fw-bold text-white">
              Conf42 Cloud Native 2024 - Online
            </h1>

            <h2 class="text-white">
              
              <time datetime="2024-03-21">March 21 2024</time>
              
              
            </h2>

            <!-- Text -->
            <p class="lead mb-0 text-white-75">
              
              <!-- Everything Cloud Native and Cloud Security. It came from the Cloud!
 -->
              <script>
                const event_date = new Date("2024-03-21T17:00:00.000+00:00");
                const local_timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;
                const local_date = new Date("2024-03-21T17:00:00.000+00:00");
                // const local_offset = new Date().getTimezoneOffset() / 60;
                // local_date.setHours(local_date.getHours() + local_offset);
                document.getElementById("localtime").innerHTML = local_date + " in " + local_timezone
              </script>
            </p>

            <!-- Buttons -->
            <div class="text-center mt-5">
              
              <a href="#register" class="btn btn-primary shadow lift me-1 mb-3">
                <i class="fe fe-user-check me-2"></i>
                Subscribe to watch
              </a>
              
              
              <a class="btn btn-danger lift mb-3" data-bigpicture='{"ytSrc": "2awCEVyi_Lc"}' href="#">
                <i class="fe fe-youtube me-2"></i>
                Watch this talk
              </a>
              
              
              <a class="btn btn-info lift mb-3" data-bigpicture='{"ytSrc": "cMgNOs-0jzo"}' href="#">
                <i class="fe fe-eye me-2"></i>
                Watch Premiere
              </a>
              
              <!-- 
              <a class="btn btn-danger lift mb-3" href="https://www.youtube.com/playlist?list=PLIuxSyKxlQrDqcWhv2eREIUiv2_MqQatx" target="_blank">
                <i class="fe fe-youtube me-2"></i>
                Playlist
              </a>
               -->
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>
      </div>
    </div>

    
    <!-- VIDEO -->
    <section class="pt-2 sticky">
      <div class="container">
        <div class="row justify-content-center">

          <div id="video-container" class="col-9 col-lg-12 mb-5">

          <!-- Video -->

            <!-- 1. The <iframe> (and video player) will replace this <div> tag. -->
            <div id="player" class="sticky"></div>

            <script>
              
              var transcript = [{"text": "Hello everyone. Welcome to my session. My name is Samuel Baruffi", "timestamp": "00:00:24,650", "timestamp_s": 24.0}, {"text": "and today I\u0027m here to present a session called", "timestamp": "00:00:28,754", "timestamp_s": 28.0}, {"text": "Mastering Generative AI, harnessing AWS Genai", "timestamp": "00:00:32,748", "timestamp_s": 32.0}, {"text": "for your solutions. Let\u0027s look at a quick agenda.", "timestamp": "00:00:37,026", "timestamp_s": 37.0}, {"text": "What I\u0027m going to be covering on my session, I\u0027m going to start", "timestamp": "00:00:40,690", "timestamp_s": 40.0}, {"text": "a presentation talking at a very high level about generative AI", "timestamp": "00:00:44,412", "timestamp_s": 44.0}, {"text": "and the big impact in the world and also into", "timestamp": "00:00:48,658", "timestamp_s": 48.0}, {"text": "applications being built today. Then I\u0027m going to", "timestamp": "00:00:51,972", "timestamp_s": 51.0}, {"text": "talk about at the very infrastructure level, what AWS is", "timestamp": "00:00:55,508", "timestamp_s": 55.0}, {"text": "doing with our own chipsets called inferential", "timestamp": "00:00:59,572", "timestamp_s": 59.0}, {"text": "and trainium, and also talk about a wide variety", "timestamp": "00:01:03,306", "timestamp_s": 63.0}, {"text": "of EC two instances with Nvidia graphic", "timestamp": "00:01:07,166", "timestamp_s": 67.0}, {"text": "cards. After that I\u0027m going to start talking about more", "timestamp": "00:01:10,958", "timestamp_s": 70.0}, {"text": "on the application services and the platforms that you can build models", "timestamp": "00:01:14,248", "timestamp_s": 74.0}, {"text": "and use models. So I\u0027m going to talk about Amazon Sagemaker,", "timestamp": "00:01:18,482", "timestamp_s": 78.0}, {"text": "I\u0027m going to talk about Amazon Sagemaker Jumpstart and", "timestamp": "00:01:22,290", "timestamp_s": 82.0}, {"text": "then I\u0027m going to spend the majority of the time talking about", "timestamp": "00:01:25,548", "timestamp_s": 85.0}, {"text": "Bedrock. Bedrock is our foundational models", "timestamp": "00:01:29,056", "timestamp_s": 89.0}, {"text": "as a service is the ability for users and companies", "timestamp": "00:01:33,206", "timestamp_s": 93.0}, {"text": "and organizations to call a single API,", "timestamp": "00:01:36,784", "timestamp_s": 96.0}, {"text": "choose different models from large language models", "timestamp": "00:01:39,942", "timestamp_s": 99.0}, {"text": "to text generations from embedding models, and easily receive", "timestamp": "00:01:43,402", "timestamp_s": 103.0}, {"text": "the response and actually build solutions on top of that.", "timestamp": "00:01:47,978", "timestamp_s": 107.0}, {"text": "Bedrock is a very exciting service that has a lot of features", "timestamp": "00:01:51,492", "timestamp_s": 111.0}, {"text": "baked in and being shipped as we speak.", "timestamp": "00:01:56,014", "timestamp_s": 116.0}, {"text": "And I\u0027m going to cover some of those features as well.", "timestamp": "00:01:58,904", "timestamp_s": 118.0}, {"text": "Then I\u0027m going to jump into a single slide that talks", "timestamp": "00:02:02,088", "timestamp_s": 122.0}, {"text": "about vector databases. I\u0027m going to talk what is a vector database,", "timestamp": "00:02:05,918", "timestamp_s": 125.0}, {"text": "why it\u0027s important for generative AI solutions. And of course I\u0027m going to be", "timestamp": "00:02:09,762", "timestamp_s": 129.0}, {"text": "talking about the solutions within the AWS platform that allows", "timestamp": "00:02:13,468", "timestamp_s": 133.0}, {"text": "you to run, create and operate vector databases.", "timestamp": "00:02:17,378", "timestamp_s": 137.0}, {"text": "Then to finalize the presentation piece of", "timestamp": "00:02:22,270", "timestamp_s": 142.0}, {"text": "my session, I\u0027m going to quickly talk about code", "timestamp": "00:02:25,872", "timestamp_s": 145.0}, {"text": "Whisper, which is a very exciting tool for developers that", "timestamp": "00:02:29,696", "timestamp_s": 149.0}, {"text": "can actually have a companion helping with", "timestamp": "00:02:33,476", "timestamp_s": 153.0}, {"text": "generation of code on many different types of programming", "timestamp": "00:02:38,050", "timestamp_s": 158.0}, {"text": "languages and also infrastructure as a code.", "timestamp": "00:02:41,578", "timestamp_s": 161.0}, {"text": "And then I\u0027ve done quite a few talks", "timestamp": "00:02:44,980", "timestamp_s": 164.0}, {"text": "with comfort e two in the past. I always like to end the session with", "timestamp": "00:02:49,066", "timestamp_s": 169.0}, {"text": "a demo. So I\u0027m going to do a demo potentially using bedrock", "timestamp": "00:02:52,968", "timestamp_s": 172.0}, {"text": "and showing you how easy it is to use bedrock, some of the functionality", "timestamp": "00:02:56,686", "timestamp_s": 176.0}, {"text": "of bedrock, and hopefully showing you with a simple piece of python", "timestamp": "00:03:00,722", "timestamp_s": 180.0}, {"text": "code how easily you can call bedrock using", "timestamp": "00:03:04,482", "timestamp_s": 184.0}, {"text": "AWS SDK and receive a response from", "timestamp": "00:03:07,776", "timestamp_s": 187.0}, {"text": "a specific large language model that you\u0027re going to choose.", "timestamp": "00:03:11,712", "timestamp_s": 191.0}, {"text": "So let\u0027s just get started.", "timestamp": "00:03:14,990", "timestamp_s": 194.0}, {"text": "So what is generative AI? Right and how", "timestamp": "00:03:18,110", "timestamp_s": 198.0}, {"text": "are those generative AI powered? The most important thing to", "timestamp": "00:03:21,652", "timestamp_s": 201.0}, {"text": "be familiar with is, in order for this", "timestamp": "00:03:25,668", "timestamp_s": 205.0}, {"text": "generative AI explosion", "timestamp": "00:03:29,652", "timestamp_s": 209.0}, {"text": "and evolution, what is actually powering them behind the scenes", "timestamp": "00:03:33,198", "timestamp_s": 213.0}, {"text": "is what we call foundational model. Foundational model.", "timestamp": "00:03:36,862", "timestamp_s": 216.0}, {"text": "You can think about an AI model that", "timestamp": "00:03:40,648", "timestamp_s": 220.0}, {"text": "has been actually pretrained on vast amounts of instruction", "timestamp": "00:03:44,168", "timestamp_s": 224.0}, {"text": "data. Those state of art models that are", "timestamp": "00:03:48,078", "timestamp_s": 228.0}, {"text": "going to be talking later on in my presentation are known to be trained", "timestamp": "00:03:51,160", "timestamp_s": 231.0}, {"text": "across pretty much the whole Internet, right? Like, if you think about", "timestamp": "00:03:54,802", "timestamp_s": 234.0}, {"text": "it, like, all the amount of data", "timestamp": "00:03:58,128", "timestamp_s": 238.0}, {"text": "that the Internet has, that is actually all the data that those", "timestamp": "00:04:01,456", "timestamp_s": 241.0}, {"text": "models are being trained, then it", "timestamp": "00:04:04,992", "timestamp_s": 244.0}, {"text": "contains a large amount of parameters that", "timestamp": "00:04:08,128", "timestamp_s": 248.0}, {"text": "makes them capable. By learning all this data, which are the", "timestamp": "00:04:11,956", "timestamp_s": 251.0}, {"text": "parameters, they make their own word interpretations,", "timestamp": "00:04:15,588", "timestamp_s": 255.0}, {"text": "and they actually can learn very complex concepts by", "timestamp": "00:04:20,170", "timestamp_s": 260.0}, {"text": "that. And there is a technology that was", "timestamp": "00:04:23,672", "timestamp_s": 263.0}, {"text": "invented, I think, 2017, by a group of researchers", "timestamp": "00:04:28,710", "timestamp_s": 268.0}, {"text": "from the University of Toronto that", "timestamp": "00:04:32,462", "timestamp_s": 272.0}, {"text": "is called transformers, and that is the architecture using neural networks", "timestamp": "00:04:36,028", "timestamp_s": 276.0}, {"text": "that allows generative AI and large language models to", "timestamp": "00:04:40,562", "timestamp_s": 280.0}, {"text": "actually generate text. The good thing about those models,", "timestamp": "00:04:44,636", "timestamp_s": 284.0}, {"text": "those large language models, is they are generic", "timestamp": "00:04:48,854", "timestamp_s": 288.0}, {"text": "and general on their own. So it\u0027s", "timestamp": "00:04:52,662", "timestamp_s": 292.0}, {"text": "not only trained for a specific use case,", "timestamp": "00:04:56,118", "timestamp_s": 296.0}, {"text": "it can actually solve a lot of different use", "timestamp": "00:04:59,550", "timestamp_s": 299.0}, {"text": "cases. And I\u0027m going to talk in a moment some of the use cases that", "timestamp": "00:05:03,392", "timestamp_s": 303.0}, {"text": "companies are starting to build. And these number of use cases", "timestamp": "00:05:06,052", "timestamp_s": 306.0}, {"text": "are pretty much infinite because you can think about those", "timestamp": "00:05:10,122", "timestamp_s": 310.0}, {"text": "large language models as a model that can answer any type", "timestamp": "00:05:13,432", "timestamp_s": 313.0}, {"text": "of question. And of course, we need to evaluate", "timestamp": "00:05:17,352", "timestamp_s": 317.0}, {"text": "performance and how big those", "timestamp": "00:05:20,846", "timestamp_s": 320.0}, {"text": "models are and how smart they are.", "timestamp": "00:05:24,392", "timestamp_s": 324.0}, {"text": "Not every model is exactly the same as others.", "timestamp": "00:05:27,308", "timestamp_s": 327.0}, {"text": "So there is that. And we are going to talk about", "timestamp": "00:05:30,524", "timestamp_s": 330.0}, {"text": "how AWS will help you choose", "timestamp": "00:05:34,044", "timestamp_s": 334.0}, {"text": "different models for your use case.", "timestamp": "00:05:37,900", "timestamp_s": 337.0}, {"text": "Another thing that a lot of companies are doing now is customizing", "timestamp": "00:05:41,390", "timestamp_s": 341.0}, {"text": "foundational models for their own data. So let\u0027s say you", "timestamp": "00:05:44,982", "timestamp_s": 344.0}, {"text": "are a financial company and you really want to have a specific model", "timestamp": "00:05:48,608", "timestamp_s": 348.0}, {"text": "super well trainium, to know every single piece", "timestamp": "00:05:52,948", "timestamp_s": 352.0}, {"text": "of information that your company has. There are different strategies.", "timestamp": "00:05:56,212", "timestamp_s": 356.0}, {"text": "You can use retrieval, argument generation,", "timestamp": "00:06:00,690", "timestamp_s": 360.0}, {"text": "or you can use agents to actually retrieve the data,", "timestamp": "00:06:03,810", "timestamp_s": 363.0}, {"text": "or you can actually customize foundational models. Customizing foundational models", "timestamp": "00:06:07,208", "timestamp_s": 367.0}, {"text": "are also known as fine tuning. That is also a capability that", "timestamp": "00:06:11,214", "timestamp_s": 371.0}, {"text": "you retrain the model to recalculate", "timestamp": "00:06:14,648", "timestamp_s": 374.0}, {"text": "the weights of those models, to add", "timestamp": "00:06:18,306", "timestamp_s": 378.0}, {"text": "your data, that potentially your data was private, it was not", "timestamp": "00:06:22,092", "timestamp_s": 382.0}, {"text": "available on the Internet. And now we want to actually have those models", "timestamp": "00:06:25,628", "timestamp_s": 385.0}, {"text": "also expert on your own data. We are not", "timestamp": "00:06:29,602", "timestamp_s": 389.0}, {"text": "going to talk in depth about those strategies,", "timestamp": "00:06:32,928", "timestamp_s": 392.0}, {"text": "but that\u0027s something that you just need to be aware that on top of having", "timestamp": "00:06:36,182", "timestamp_s": 396.0}, {"text": "foundational models, you can actually customize those to", "timestamp": "00:06:39,536", "timestamp_s": 399.0}, {"text": "even be more expert on the data that you as", "timestamp": "00:06:42,692", "timestamp_s": 402.0}, {"text": "a person or potentially organization have", "timestamp": "00:06:46,692", "timestamp_s": 406.0}, {"text": "control of. So some of the use cases", "timestamp": "00:06:49,988", "timestamp_s": 409.0}, {"text": "that we\u0027ve seen the industry actually building.", "timestamp": "00:06:53,242", "timestamp_s": 413.0}, {"text": "So there are kind of three main categories, and this is just a very small", "timestamp": "00:06:56,472", "timestamp_s": 416.0}, {"text": "amount of use cases that companies are building. But the first category is", "timestamp": "00:07:00,168", "timestamp_s": 420.0}, {"text": "you want to enhance customer experience. So using", "timestamp": "00:07:04,568", "timestamp_s": 424.0}, {"text": "chat bots, virtual assistants,", "timestamp": "00:07:08,236", "timestamp_s": 428.0}, {"text": "conversational analytics, you can personalize a specific", "timestamp": "00:07:11,210", "timestamp_s": 431.0}, {"text": "user interaction, because those models are really good at", "timestamp": "00:07:14,972", "timestamp_s": 434.0}, {"text": "behaving and answering like humans, even though they are not", "timestamp": "00:07:18,652", "timestamp_s": 438.0}, {"text": "humans, but they answer like a human. It makes really", "timestamp": "00:07:22,064", "timestamp_s": 442.0}, {"text": "good to actually enhance customer experience, right.", "timestamp": "00:07:25,472", "timestamp_s": 445.0}, {"text": "Especially with chatbots. The other section is you can", "timestamp": "00:07:28,848", "timestamp_s": 448.0}, {"text": "boost employee productivity and creativity. So you", "timestamp": "00:07:32,452", "timestamp_s": 452.0}, {"text": "can think about conversational search. If an employee or", "timestamp": "00:07:35,748", "timestamp_s": 455.0}, {"text": "a customer wants to really retrieve an information from", "timestamp": "00:07:39,076", "timestamp_s": 459.0}, {"text": "a wide variety of data set, instead of", "timestamp": "00:07:42,676", "timestamp_s": 462.0}, {"text": "just doing a quick, simple search, you can actually have", "timestamp": "00:07:46,568", "timestamp_s": 466.0}, {"text": "conversational with those documents. You can do summarization,", "timestamp": "00:07:50,616", "timestamp_s": 470.0}, {"text": "you can do content creation, you can do code generation like we are", "timestamp": "00:07:54,398", "timestamp_s": 474.0}, {"text": "going to be talking about code Whisper. And the other category is", "timestamp": "00:07:58,184", "timestamp_s": 478.0}, {"text": "you can also optimize business processes. So let\u0027s say you", "timestamp": "00:08:01,612", "timestamp_s": 481.0}, {"text": "want to do a lot of document processing, you want to do data documentation.", "timestamp": "00:08:04,828", "timestamp_s": 484.0}, {"text": "Let\u0027s say you have a specific form that needs to be filled.", "timestamp": "00:08:08,322", "timestamp_s": 488.0}, {"text": "Given a specific data, you can join those two", "timestamp": "00:08:11,890", "timestamp_s": 491.0}, {"text": "sets of data and ask the model to potentially feel in", "timestamp": "00:08:16,270", "timestamp_s": 496.0}, {"text": "a specific way, right? So there is a lot of use cases", "timestamp": "00:08:19,888", "timestamp_s": 499.0}, {"text": "that companies are building or have built already to", "timestamp": "00:08:23,706", "timestamp_s": 503.0}, {"text": "enhance customer experience, boost employee productivity,", "timestamp": "00:08:28,930", "timestamp_s": 508.0}, {"text": "or optimize business processes. So that is just something you need to keep in", "timestamp": "00:08:32,826", "timestamp_s": 512.0}, {"text": "mind. Now, in order to run those", "timestamp": "00:08:36,824", "timestamp_s": 516.0}, {"text": "models, those models require a lot of computational,", "timestamp": "00:08:40,232", "timestamp_s": 520.0}, {"text": "specifically parallel computational. One good thing about", "timestamp": "00:08:44,430", "timestamp_s": 524.0}, {"text": "GPUs, graphic process units are that they are", "timestamp": "00:08:48,652", "timestamp_s": 528.0}, {"text": "inherent good at calculations that can be run", "timestamp": "00:08:53,196", "timestamp_s": 533.0}, {"text": "in parallel. So the reason why I\u0027m saying that", "timestamp": "00:08:56,588", "timestamp_s": 536.0}, {"text": "it\u0027s really important for us to understand how AWs", "timestamp": "00:08:59,936", "timestamp_s": 539.0}, {"text": "have a wide variety of selections of different instances.", "timestamp": "00:09:03,494", "timestamp_s": 543.0}, {"text": "You can choose to run those models. So AWS have,", "timestamp": "00:09:07,766", "timestamp_s": 547.0}, {"text": "since 2010, a very good partnership with Nvidia.", "timestamp": "00:09:11,920", "timestamp_s": 551.0}, {"text": "Nvidia have been the leader of having very powerful", "timestamp": "00:09:15,466", "timestamp_s": 555.0}, {"text": "GPUs. And those GPUs are really good for large language models.", "timestamp": "00:09:19,434", "timestamp_s": 559.0}, {"text": "You can see here, there are widely variety of instances that offer", "timestamp": "00:09:23,130", "timestamp_s": 563.0}, {"text": "specific instances, specific GPUs from Nvidia.", "timestamp": "00:09:26,824", "timestamp_s": 566.0}, {"text": "The most powerful one is the P five EC two instance that", "timestamp": "00:09:31,038", "timestamp_s": 571.0}, {"text": "comes with the Nvidia age 100 pencil car.", "timestamp": "00:09:34,712", "timestamp_s": 574.0}, {"text": "Those are really big GPUs that can", "timestamp": "00:09:38,236", "timestamp_s": 578.0}, {"text": "run very big, large language models, and they have been used", "timestamp": "00:09:41,548", "timestamp_s": 581.0}, {"text": "for many companies, specifically on the generative AI", "timestamp": "00:09:45,530", "timestamp_s": 585.0}, {"text": "solution. But apart from the good", "timestamp": "00:09:49,558", "timestamp_s": 589.0}, {"text": "partnership that AWS have with Nvidia,", "timestamp": "00:09:53,088", "timestamp_s": 593.0}, {"text": "AWS is also in the front of innovating our", "timestamp": "00:09:56,278", "timestamp_s": 596.0}, {"text": "own chipsets. So we have graviton,", "timestamp": "00:10:00,836", "timestamp_s": 600.0}, {"text": "which is a general CPU, but we also have", "timestamp": "00:10:04,522", "timestamp_s": 604.0}, {"text": "accelerators called trainium and infra.", "timestamp": "00:10:09,890", "timestamp_s": 609.0}, {"text": "So trainium is the GPU accelerator", "timestamp": "00:10:13,114", "timestamp_s": 613.0}, {"text": "that is focused on giving you better price performance,", "timestamp": "00:10:17,662", "timestamp_s": 617.0}, {"text": "up to 40% better price performance than other comparable", "timestamp": "00:10:21,726", "timestamp_s": 621.0}, {"text": "GPUs for training your models. So it\u0027s a very", "timestamp": "00:10:25,182", "timestamp_s": 625.0}, {"text": "specific car that has been built by AWS", "timestamp": "00:10:29,096", "timestamp_s": 629.0}, {"text": "to allow you to have a better performance and price when", "timestamp": "00:10:32,450", "timestamp_s": 632.0}, {"text": "you\u0027re training machine learning models. And then once you have", "timestamp": "00:10:35,948", "timestamp_s": 635.0}, {"text": "those models, you need to run inference on top of that. And AWS", "timestamp": "00:10:39,888", "timestamp_s": 639.0}, {"text": "have also created inferential chipsets. We have two different", "timestamp": "00:10:43,142", "timestamp_s": 643.0}, {"text": "chipsets, inference one and inferient two, that also give you a better", "timestamp": "00:10:47,408", "timestamp_s": 647.0}, {"text": "price performance to actually run inference on top of those", "timestamp": "00:10:51,296", "timestamp_s": 651.0}, {"text": "models. I\u0027ll highly encourage you to just quickly search", "timestamp": "00:10:54,912", "timestamp_s": 654.0}, {"text": "about those. There is a lot of good documentation. Feel free to reach out", "timestamp": "00:10:58,580", "timestamp_s": 658.0}, {"text": "to me on LinkedIn as well if you have any questions.", "timestamp": "00:11:02,068", "timestamp_s": 662.0}, {"text": "Now, let\u0027s move from the infrastructure level to actually", "timestamp": "00:11:04,870", "timestamp_s": 664.0}, {"text": "platform and services that AWS offers customers", "timestamp": "00:11:08,936", "timestamp_s": 668.0}, {"text": "to actually run those models. Right? But in a day,", "timestamp": "00:11:13,288", "timestamp_s": 673.0}, {"text": "organizations are looking, how can they run those models and", "timestamp": "00:11:16,184", "timestamp_s": 676.0}, {"text": "use those models for all the use cases we\u0027ve just discussed?", "timestamp": "00:11:19,436", "timestamp_s": 679.0}, {"text": "So, the first platform I want to talk is Sagemaker.", "timestamp": "00:11:24,010", "timestamp_s": 684.0}, {"text": "So, Sagemaker is the AWS", "timestamp": "00:11:28,470", "timestamp_s": 688.0}, {"text": "machine learning AI platform that encompass", "timestamp": "00:11:32,678", "timestamp_s": 692.0}, {"text": "a wide variety of features, from building new", "timestamp": "00:11:35,910", "timestamp_s": 695.0}, {"text": "data sets, cleaning new data sets, enriching new data", "timestamp": "00:11:40,292", "timestamp_s": 700.0}, {"text": "sets, training different models, different machine learning", "timestamp": "00:11:43,716", "timestamp_s": 703.0}, {"text": "models, neural networks, you name it. You can actually", "timestamp": "00:11:47,540", "timestamp_s": 707.0}, {"text": "run on Sagemaker once you run those models, you can", "timestamp": "00:11:51,604", "timestamp_s": 711.0}, {"text": "actually deploy those machine learning models at scale. You actually manage", "timestamp": "00:11:55,208", "timestamp_s": 715.0}, {"text": "the computational for you. You have monitoring, you can actually have evaluation.", "timestamp": "00:11:59,752", "timestamp_s": 719.0}, {"text": "You can do a roll of the automatically fine tuning and distributed", "timestamp": "00:12:04,950", "timestamp_s": 724.0}, {"text": "training for big models on top of that.", "timestamp": "00:12:08,738", "timestamp_s": 728.0}, {"text": "And you can actually, after you\u0027ve trained,", "timestamp": "00:12:11,852", "timestamp_s": 731.0}, {"text": "operating all those models on Sagemaker.", "timestamp": "00:12:14,530", "timestamp_s": 734.0}, {"text": "This is a very, in the last six years since Sagemaker", "timestamp": "00:12:18,430", "timestamp_s": 738.0}, {"text": "was launched, we have introduced many, many new", "timestamp": "00:12:22,598", "timestamp_s": 742.0}, {"text": "innovations like automatically model tuning. So you can deploy", "timestamp": "00:12:26,272", "timestamp_s": 746.0}, {"text": "and train the model. And as you train the model, we will find", "timestamp": "00:12:30,246", "timestamp_s": 750.0}, {"text": "the right parameters to actually fine tune and", "timestamp": "00:12:33,504", "timestamp_s": 753.0}, {"text": "tune the model for the better performance of what you\u0027re trying to achieve.", "timestamp": "00:12:36,708", "timestamp_s": 756.0}, {"text": "And when I mean model, I\u0027m not only talking about large language models,", "timestamp": "00:12:40,426", "timestamp_s": 760.0}, {"text": "I\u0027m talking about any type of machine learning model that you want to", "timestamp": "00:12:43,898", "timestamp_s": 763.0}, {"text": "actually build, train, and deploy.", "timestamp": "00:12:47,272", "timestamp_s": 767.0}, {"text": "Now, when it comes to generative AI,", "timestamp": "00:12:50,310", "timestamp_s": 770.0}, {"text": "Amazon Sagemaker has a very specific feature that really", "timestamp": "00:12:53,598", "timestamp_s": 773.0}, {"text": "helps developers to get quickly testing", "timestamp": "00:12:57,596", "timestamp_s": 777.0}, {"text": "different larger language models. The name of the feature is called Amazon", "timestamp": "00:13:01,202", "timestamp_s": 781.0}, {"text": "Sagemaker Jumpstart. Sagemaker Jumpstart", "timestamp": "00:13:05,506", "timestamp_s": 785.0}, {"text": "is a machine learning hub with foundational", "timestamp": "00:13:09,846", "timestamp_s": 789.0}, {"text": "models made available that you can literally just click and", "timestamp": "00:13:13,590", "timestamp_s": 793.0}, {"text": "deploy. It\u0027ll give you the recommended", "timestamp": "00:13:17,792", "timestamp_s": 797.0}, {"text": "instance types that those models should run, depending on the size", "timestamp": "00:13:21,366", "timestamp_s": 801.0}, {"text": "of those models. So right now there are more than hundreds of", "timestamp": "00:13:25,316", "timestamp_s": 805.0}, {"text": "different models that have built in algorithms that have", "timestamp": "00:13:29,172", "timestamp_s": 809.0}, {"text": "been pre trained with foundational models. So a lot", "timestamp": "00:13:33,092", "timestamp_s": 813.0}, {"text": "of those models are available on hugging face. Some of the Amazon Alexa", "timestamp": "00:13:36,648", "timestamp_s": 816.0}, {"text": "models are also available for you to deploy.", "timestamp": "00:13:40,382", "timestamp_s": 820.0}, {"text": "The good thing is this is all UI and API based with", "timestamp": "00:13:44,470", "timestamp_s": 824.0}, {"text": "a single click of buttons or simple API calls. You can actually", "timestamp": "00:13:48,172", "timestamp_s": 828.0}, {"text": "have a machine, an EC two machine", "timestamp": "00:13:51,756", "timestamp_s": 831.0}, {"text": "running on Sagemaker actually hosting that model,", "timestamp": "00:13:56,002", "timestamp_s": 836.0}, {"text": "and all the things that you need to do manually by running", "timestamp": "00:13:59,484", "timestamp_s": 839.0}, {"text": "to actually run that model and do inference on that model will be taken care", "timestamp": "00:14:03,648", "timestamp_s": 843.0}, {"text": "of for you. We have a lot of notebooks,", "timestamp": "00:14:07,248", "timestamp_s": 847.0}, {"text": "Jupyter notebooks that have examples on how you can actually do", "timestamp": "00:14:10,550", "timestamp_s": 850.0}, {"text": "that. And the good thing about Sagemaker Jumpstart, some models", "timestamp": "00:14:14,212", "timestamp_s": 854.0}, {"text": "that are available on Sagemaker Jumpstart also have", "timestamp": "00:14:18,058", "timestamp_s": 858.0}, {"text": "the capability for fine tuning. So if you want to customize", "timestamp": "00:14:21,732", "timestamp_s": 861.0}, {"text": "a model, let\u0027s say the Falcon model,", "timestamp": "00:14:25,150", "timestamp_s": 865.0}, {"text": "180,000,000,000 parameters you want to customize with your", "timestamp": "00:14:28,232", "timestamp_s": 868.0}, {"text": "own data, you can go on sage maker Jumpstart and", "timestamp": "00:14:32,168", "timestamp_s": 872.0}, {"text": "you have actually an ease walkthrough", "timestamp": "00:14:35,752", "timestamp_s": 875.0}, {"text": "way of fine tuning those models. One thing to note here,", "timestamp": "00:14:40,194", "timestamp_s": 880.0}, {"text": "that is going to be very important to differentiate it from other services like", "timestamp": "00:14:44,412", "timestamp_s": 884.0}, {"text": "bedrock that I\u0027m going to talk in a moment. Sagemaker Jumpstart", "timestamp": "00:14:48,076", "timestamp_s": 888.0}, {"text": "helps you run those models, but where you\u0027re", "timestamp": "00:14:52,150", "timestamp_s": 892.0}, {"text": "running those models is actually on a EC two instance", "timestamp": "00:14:55,558", "timestamp_s": 895.0}, {"text": "that you\u0027re paying every single minute or", "timestamp": "00:14:59,462", "timestamp_s": 899.0}, {"text": "second that you\u0027re running those, even though if you might not be using that,", "timestamp": "00:15:02,672", "timestamp_s": 902.0}, {"text": "you are actually paying for that instance. Those models are actually running", "timestamp": "00:15:05,892", "timestamp_s": 905.0}, {"text": "on your account, on your ECQ, on your sage maker", "timestamp": "00:15:10,530", "timestamp_s": 910.0}, {"text": "that behind the scenes runs ECQs, but you\u0027re paying for those.", "timestamp": "00:15:13,978", "timestamp_s": 913.0}, {"text": "So it\u0027s kind of pay as you go, but for", "timestamp": "00:15:17,400", "timestamp_s": 917.0}, {"text": "the whole instance that it requires, right? So you\u0027re not paying per tokens,", "timestamp": "00:15:20,968", "timestamp_s": 920.0}, {"text": "you\u0027re paying for the whole instance. That is something that you just want to watch", "timestamp": "00:15:24,990", "timestamp_s": 924.0}, {"text": "out because depending on the models, it can be very expensive.", "timestamp": "00:15:28,316", "timestamp_s": 928.0}, {"text": "But let\u0027s continue our journey regarding generative", "timestamp": "00:15:32,730", "timestamp_s": 932.0}, {"text": "AI. So when we look about", "timestamp": "00:15:36,146", "timestamp_s": 936.0}, {"text": "what customers are asking for generative AI is", "timestamp": "00:15:39,520", "timestamp_s": 939.0}, {"text": "which model should I use for a specific use case? How can", "timestamp": "00:15:43,056", "timestamp_s": 943.0}, {"text": "I move quickly? Most customers are", "timestamp": "00:15:46,448", "timestamp_s": 946.0}, {"text": "nodding to the exercise of training large", "timestamp": "00:15:50,128", "timestamp_s": 950.0}, {"text": "language models or fine tuning. They really have use cases that", "timestamp": "00:15:53,876", "timestamp_s": 953.0}, {"text": "they want to boost customer experience or increase employee", "timestamp": "00:15:57,748", "timestamp_s": 957.0}, {"text": "productivity. For example. They just want to reinterate run POCs", "timestamp": "00:16:01,898", "timestamp_s": 961.0}, {"text": "very quickly. And most important, how can they keep the", "timestamp": "00:16:05,498", "timestamp_s": 965.0}, {"text": "data that they\u0027re going to be running through those models? Secure and private,", "timestamp": "00:16:09,432", "timestamp_s": 969.0}, {"text": "right. That is a very important thing. You have your data. Your data shouldn\u0027t be", "timestamp": "00:16:12,926", "timestamp_s": 972.0}, {"text": "used to train new models if you don\u0027t want to.", "timestamp": "00:16:16,428", "timestamp_s": 976.0}, {"text": "And they should be kept secure encrypt by default.", "timestamp": "00:16:20,028", "timestamp_s": 980.0}, {"text": "So with all those three questions being asked by customer,", "timestamp": "00:16:24,578", "timestamp_s": 984.0}, {"text": "AWS have introduced last year a service called", "timestamp": "00:16:28,720", "timestamp_s": 988.0}, {"text": "Amazon Bedrock, which is the easiest way", "timestamp": "00:16:32,304", "timestamp_s": 992.0}, {"text": "to build and scale generative AI applications", "timestamp": "00:16:37,150", "timestamp_s": 997.0}, {"text": "with foundational models. And we talked about foundational models. Those are the models", "timestamp": "00:16:41,142", "timestamp_s": 1001.0}, {"text": "that, large language models that are very big and", "timestamp": "00:16:44,938", "timestamp_s": 1004.0}, {"text": "they can do a lot of general tasks. What does", "timestamp": "00:16:48,356", "timestamp_s": 1008.0}, {"text": "Amazon bedrock offers you? First, it offers", "timestamp": "00:16:52,468", "timestamp_s": 1012.0}, {"text": "you a choice, a democratization of", "timestamp": "00:16:56,398", "timestamp_s": 1016.0}, {"text": "leading foundational models with a single API. And this is one", "timestamp": "00:17:00,120", "timestamp_s": 1020.0}, {"text": "of the most amazing things about bedrock. You can", "timestamp": "00:17:03,528", "timestamp_s": 1023.0}, {"text": "use the same service and the same API,", "timestamp": "00:17:06,732", "timestamp_s": 1026.0}, {"text": "just choosing a parameter of your API", "timestamp": "00:17:10,866", "timestamp_s": 1030.0}, {"text": "by choosing what model you want. And the model list has", "timestamp": "00:17:15,130", "timestamp_s": 1035.0}, {"text": "been growing every single month. And you see in a moment", "timestamp": "00:17:18,812", "timestamp_s": 1038.0}, {"text": "what are the models and model providers that bedrock currently offers.", "timestamp": "00:17:22,416", "timestamp_s": 1042.0}, {"text": "But you can expect the model list and those capabilities to grow as", "timestamp": "00:17:27,446", "timestamp_s": 1047.0}, {"text": "we speak. You can also run retrieval", "timestamp": "00:17:31,168", "timestamp_s": 1051.0}, {"text": "augmented generation on top of that.", "timestamp": "00:17:34,850", "timestamp_s": 1054.0}, {"text": "And I\u0027m going to keep that on hold for now because I\u0027m going to be", "timestamp": "00:17:38,036", "timestamp_s": 1058.0}, {"text": "talking about a feature on bedrock that helps you do that.", "timestamp": "00:17:40,308", "timestamp_s": 1060.0}, {"text": "You can also have agents that execute multiple steps", "timestamp": "00:17:43,736", "timestamp_s": 1063.0}, {"text": "tasks by running lambda and calling your own APIs", "timestamp": "00:17:47,182", "timestamp_s": 1067.0}, {"text": "or outside APIs automatically. And most important,", "timestamp": "00:17:50,654", "timestamp_s": 1070.0}, {"text": "bedrock is security, private and safe.", "timestamp": "00:17:54,232", "timestamp_s": 1074.0}, {"text": "Every data that you put to bedrock is not going", "timestamp": "00:18:00,470", "timestamp_s": 1080.0}, {"text": "to be used to train your models. It\u0027s encrypted by default", "timestamp": "00:18:04,284", "timestamp_s": 1084.0}, {"text": "and nobody else has access. This is really important to", "timestamp": "00:18:07,970", "timestamp_s": 1087.0}, {"text": "keep in mind. You can also have vpc endpoints from bedrock", "timestamp": "00:18:11,776", "timestamp_s": 1091.0}, {"text": "so the data never leaves your VPC. It goes through your VPC to a", "timestamp": "00:18:15,926", "timestamp_s": 1095.0}, {"text": "vpc endpoint to bedrock where it hosts the service.", "timestamp": "00:18:19,504", "timestamp_s": 1099.0}, {"text": "One important thing to note about bedrock, different than Sagemaker", "timestamp": "00:18:22,932", "timestamp_s": 1102.0}, {"text": "Jumpstart, you pay AWS, you go. There are different pricing", "timestamp": "00:18:27,178", "timestamp_s": 1107.0}, {"text": "modes on bedrock, but you start with which is the most.", "timestamp": "00:18:31,370", "timestamp_s": 1111.0}, {"text": "I guess the way we start with bedrock, it\u0027s called on demand.", "timestamp": "00:18:34,692", "timestamp_s": 1114.0}, {"text": "So depending on the large language model, the foundational model", "timestamp": "00:18:38,430", "timestamp_s": 1118.0}, {"text": "that you pick, you\u0027re going to have a price per input", "timestamp": "00:18:41,976", "timestamp_s": 1121.0}, {"text": "token and output token. When you\u0027re talking about text", "timestamp": "00:18:45,422", "timestamp_s": 1125.0}, {"text": "you have a different pricing mechanisms for image generation.", "timestamp": "00:18:49,064", "timestamp_s": 1129.0}, {"text": "But for now let\u0027s just keep it simple. You\u0027re going to pay for that,", "timestamp": "00:18:52,498", "timestamp_s": 1132.0}, {"text": "right? So it\u0027s just the traditional AWS", "timestamp": "00:18:55,516", "timestamp_s": 1135.0}, {"text": "cloud approach of pay as you go. And that becomes very", "timestamp": "00:18:59,366", "timestamp_s": 1139.0}, {"text": "promising because instead of paying for big instances", "timestamp": "00:19:03,710", "timestamp_s": 1143.0}, {"text": "to run those large language models for you, you can experiment,", "timestamp": "00:19:07,222", "timestamp_s": 1147.0}, {"text": "iterate and create new products very easily by", "timestamp": "00:19:10,838", "timestamp_s": 1150.0}, {"text": "still keeping your application in your solutions very", "timestamp": "00:19:14,372", "timestamp_s": 1154.0}, {"text": "cost conscious. So now let\u0027s just", "timestamp": "00:19:18,996", "timestamp_s": 1158.0}, {"text": "quickly talk about some of the what are", "timestamp": "00:19:22,596", "timestamp_s": 1162.0}, {"text": "the model providers that are available on bedrock?", "timestamp": "00:19:26,408", "timestamp_s": 1166.0}, {"text": "So the way bedrock is architected is", "timestamp": "00:19:29,982", "timestamp_s": 1169.0}, {"text": "you have model providers. So those are companies that", "timestamp": "00:19:33,560", "timestamp_s": 1173.0}, {"text": "have trained foundational models. And each model", "timestamp": "00:19:37,164", "timestamp_s": 1177.0}, {"text": "provider, we have a different foundational models", "timestamp": "00:19:40,460", "timestamp_s": 1180.0}, {"text": "available on that. Right. So here you can see", "timestamp": "00:19:44,386", "timestamp_s": 1184.0}, {"text": "a list of seven different model providers that you can", "timestamp": "00:19:48,750", "timestamp_s": 1188.0}, {"text": "pick from on Amazon Bedrock AWS,", "timestamp": "00:19:52,432", "timestamp_s": 1192.0}, {"text": "the date of this presentation. So today is March", "timestamp": "00:19:55,552", "timestamp_s": 1195.0}, {"text": "eigth 2024. As I\u0027m recording this session,", "timestamp": "00:19:59,302", "timestamp_s": 1199.0}, {"text": "we currently have seven model providers available for you", "timestamp": "00:20:02,938", "timestamp_s": 1202.0}, {"text": "on Bedrock. So you have AI 21 that has the Jurassic two", "timestamp": "00:20:06,548", "timestamp_s": 1206.0}, {"text": "models available for you. Then you", "timestamp": "00:20:11,204", "timestamp_s": 1211.0}, {"text": "have entropic. And I\u0027m going to talk about entropic in a moment. But they are", "timestamp": "00:20:14,564", "timestamp_s": 1214.0}, {"text": "state of the art models with a very big performance.", "timestamp": "00:20:18,328", "timestamp_s": 1218.0}, {"text": "Then you have cohere. With cohere you have both text", "timestamp": "00:20:21,518", "timestamp_s": 1221.0}, {"text": "large language models and embedding models as well. So if", "timestamp": "00:20:25,850", "timestamp_s": 1225.0}, {"text": "you want to create embeddings for your vector database, Cohere also offers you", "timestamp": "00:20:30,508", "timestamp_s": 1230.0}, {"text": "with very performance embedding models. And it", "timestamp": "00:20:34,412", "timestamp_s": 1234.0}, {"text": "was just introduced I think a couple of weeks ago, Mistral AI.", "timestamp": "00:20:38,172", "timestamp_s": 1238.0}, {"text": "So you have two different models with Mistral AI. The Mistro", "timestamp": "00:20:41,222", "timestamp_s": 1241.0}, {"text": "seven D and the mixture mix of", "timestamp": "00:20:44,742", "timestamp_s": 1244.0}, {"text": "exports which is eight models that are put together", "timestamp": "00:20:48,352", "timestamp_s": 1248.0}, {"text": "into a single API. Very good performance. Then you also of", "timestamp": "00:20:52,180", "timestamp_s": 1252.0}, {"text": "course have meta with Leomachu which is an open", "timestamp": "00:20:55,828", "timestamp_s": 1255.0}, {"text": "science model. Then you have stability", "timestamp": "00:20:59,220", "timestamp_s": 1259.0}, {"text": "AI. So stability AI is one of the leaders", "timestamp": "00:21:02,906", "timestamp_s": 1262.0}, {"text": "research labs for image generation. So the stable", "timestamp": "00:21:06,718", "timestamp_s": 1266.0}, {"text": "diffusion XL 1.0 is a model that allows you to", "timestamp": "00:21:10,782", "timestamp_s": 1270.0}, {"text": "generate images. So instead of just generating text,", "timestamp": "00:21:14,312", "timestamp_s": 1274.0}, {"text": "it actually generates image. You input a text,", "timestamp": "00:21:17,736", "timestamp_s": 1277.0}, {"text": "a cat walking in the park. It will actually generate an image", "timestamp": "00:21:20,796", "timestamp_s": 1280.0}, {"text": "for you with a cat walking in the park. Then you also", "timestamp": "00:21:24,258", "timestamp_s": 1284.0}, {"text": "have our own models from Amazon. Those are", "timestamp": "00:21:27,776", "timestamp_s": 1287.0}, {"text": "called Titan. So Titan models offers you a text to", "timestamp": "00:21:31,408", "timestamp_s": 1291.0}, {"text": "text model, traditional large language models. It also offers embedding", "timestamp": "00:21:35,408", "timestamp_s": 1295.0}, {"text": "models and it also offers image generation models.", "timestamp": "00:21:39,462", "timestamp_s": 1299.0}, {"text": "Right? So it\u0027s a set of models available for", "timestamp": "00:21:43,082", "timestamp_s": 1303.0}, {"text": "you. The very important thing to keep in mind with", "timestamp": "00:21:46,772", "timestamp_s": 1306.0}, {"text": "bedrock is we are democratizing the ability for people", "timestamp": "00:21:50,468", "timestamp_s": 1310.0}, {"text": "to consume different models for a specific use case.", "timestamp": "00:21:54,472", "timestamp_s": 1314.0}, {"text": "And you can go right now on Bedrock webpage", "timestamp": "00:21:58,488", "timestamp_s": 1318.0}, {"text": "and click on the pricing and you see the different pricings for each model.", "timestamp": "00:22:03,326", "timestamp_s": 1323.0}, {"text": "Depending on the performance, the size of those models, you might be paying a", "timestamp": "00:22:07,448", "timestamp_s": 1327.0}, {"text": "specific price. And that is really important because you can now decide how", "timestamp": "00:22:11,148", "timestamp_s": 1331.0}, {"text": "you want to build your applications. Remember,", "timestamp": "00:22:15,452", "timestamp_s": 1335.0}, {"text": "it\u0027s the single API call and you do not need to manage", "timestamp": "00:22:18,490", "timestamp_s": 1338.0}, {"text": "any infrastructure. That is something I want to highlight as well. All the infrastructure", "timestamp": "00:22:21,840", "timestamp_s": 1341.0}, {"text": "and GPUs to actually run those models, which is very complex", "timestamp": "00:22:25,990", "timestamp_s": 1345.0}, {"text": "and it takes a lot of capacity, is taken care by AWS for", "timestamp": "00:22:29,750", "timestamp_s": 1349.0}, {"text": "you, and that is something very beautiful that", "timestamp": "00:22:33,508", "timestamp_s": 1353.0}, {"text": "you should be using and taking benefit of.", "timestamp": "00:22:38,530", "timestamp_s": 1358.0}, {"text": "Now, I really want to talk about the partnership that we have with entropic.", "timestamp": "00:22:42,050", "timestamp_s": 1362.0}, {"text": "So entropic is a longtime AWS customer, and entropic", "timestamp": "00:22:46,430", "timestamp_s": 1366.0}, {"text": "is one of the top leading research", "timestamp": "00:22:50,750", "timestamp_s": 1370.0}, {"text": "AI companies in the world. And I\u0027m", "timestamp": "00:22:55,290", "timestamp_s": 1375.0}, {"text": "going to talk about some of the models that they have published", "timestamp": "00:22:59,074", "timestamp_s": 1379.0}, {"text": "in the last years. But AWS,", "timestamp": "00:23:03,362", "timestamp_s": 1383.0}, {"text": "Amazon have invested heavily. I think we\u0027ve announced", "timestamp": "00:23:07,074", "timestamp_s": 1387.0}, {"text": "a $4 billion investment last year. So entropic", "timestamp": "00:23:10,902", "timestamp_s": 1390.0}, {"text": "has a very good partnership with Amazon. And the way we showed the", "timestamp": "00:23:14,678", "timestamp_s": 1394.0}, {"text": "results of that partnership is, well, first of all, let\u0027s talk about", "timestamp": "00:23:18,368", "timestamp_s": 1398.0}, {"text": "the story about entropic very quickly, right? So if", "timestamp": "00:23:23,410", "timestamp_s": 1403.0}, {"text": "you look here at the timeline, we are in a very fast paced", "timestamp": "00:23:26,644", "timestamp_s": 1406.0}, {"text": "environment. In 2019,", "timestamp": "00:23:30,410", "timestamp_s": 1410.0}, {"text": "GPT-2 was launched from OpenAI.", "timestamp": "00:23:33,670", "timestamp_s": 1413.0}, {"text": "Then some researchers have", "timestamp": "00:23:37,510", "timestamp_s": 1417.0}, {"text": "published some papers about the performance of transformers.", "timestamp": "00:23:41,256", "timestamp_s": 1421.0}, {"text": "And GPT-3 was launched sometime in 2020", "timestamp": "00:23:44,846", "timestamp_s": 1424.0}, {"text": "with Codex as well. Right?", "timestamp": "00:23:48,684", "timestamp_s": 1428.0}, {"text": "Most of the people that have founded entropic were", "timestamp": "00:23:52,010", "timestamp_s": 1432.0}, {"text": "employees from OpenAI. So they left on OpenAI", "timestamp": "00:23:55,964", "timestamp_s": 1435.0}, {"text": "in 2021 and they found Entropic.", "timestamp": "00:23:59,458", "timestamp_s": 1439.0}, {"text": "You can see how quickly they went from founding", "timestamp": "00:24:03,310", "timestamp_s": 1443.0}, {"text": "a new research lab and actually publishing very good and", "timestamp": "00:24:07,958", "timestamp_s": 1447.0}, {"text": "making available very good models. So they published some papers", "timestamp": "00:24:12,212", "timestamp_s": 1452.0}, {"text": "in 2021. Then in 2022", "timestamp": "00:24:16,378", "timestamp_s": 1456.0}, {"text": "they finished training clot, right? And they have", "timestamp": "00:24:21,970", "timestamp_s": 1461.0}, {"text": "something called Constitution AI. I would highly recommend you to search is", "timestamp": "00:24:25,720", "timestamp_s": 1465.0}, {"text": "their whole way how they take very important", "timestamp": "00:24:29,528", "timestamp_s": 1469.0}, {"text": "care on safety and alignment for those models.", "timestamp": "00:24:34,550", "timestamp_s": 1474.0}, {"text": "And then in 2023, they have released the first cloud one model.", "timestamp": "00:24:38,542", "timestamp_s": 1478.0}, {"text": "Then they have released one of the first companies to release 100,000.", "timestamp": "00:24:43,050", "timestamp_s": 1483.0}, {"text": "Context window. Context window just means how much text", "timestamp": "00:24:47,164", "timestamp_s": 1487.0}, {"text": "you can put per request. Then after that,", "timestamp": "00:24:50,592", "timestamp_s": 1490.0}, {"text": "in 2023, they have released cloud two, which was a big improvement", "timestamp": "00:24:54,096", "timestamp_s": 1494.0}, {"text": "from cloud one. Then a couple of months after they\u0027ve", "timestamp": "00:24:58,310", "timestamp_s": 1498.0}, {"text": "released cloud 2.1 with more improvements and performance.", "timestamp": "00:25:02,378", "timestamp_s": 1502.0}, {"text": "Then this year, actually last week or this week?", "timestamp": "00:25:05,866", "timestamp_s": 1505.0}, {"text": "To be honest, Monday this week. They have,", "timestamp": "00:25:10,450", "timestamp_s": 1510.0}, {"text": "I would say, shocked the industry with very performant", "timestamp": "00:25:13,800", "timestamp_s": 1513.0}, {"text": "and set of models of three different models", "timestamp": "00:25:17,614", "timestamp_s": 1517.0}, {"text": "called cloud tree. And I\u0027m going to talk about those. So cloud tree", "timestamp": "00:25:21,422", "timestamp_s": 1521.0}, {"text": "comes with three different models. The first one is cloudtree haiku.", "timestamp": "00:25:26,330", "timestamp_s": 1526.0}, {"text": "And you can see these on this graph is cloudtree haiku.", "timestamp": "00:25:30,498", "timestamp_s": 1530.0}, {"text": "You can see the intelligence is a very performance", "timestamp": "00:25:34,330", "timestamp_s": 1534.0}, {"text": "model, but most important is a very low cost and", "timestamp": "00:25:37,906", "timestamp_s": 1537.0}, {"text": "very fast inference model.", "timestamp": "00:25:42,032", "timestamp_s": 1542.0}, {"text": "Then they have also released cloud tree Sonnet,", "timestamp": "00:25:44,510", "timestamp_s": 1544.0}, {"text": "which is their mid tier model, which is very, very intelligent.", "timestamp": "00:25:47,782", "timestamp_s": 1547.0}, {"text": "It beats all the previous quad models in terms", "timestamp": "00:25:51,462", "timestamp_s": 1551.0}, {"text": "of benchmarks, and it\u0027s in the middle when it comes to", "timestamp": "00:25:54,932", "timestamp_s": 1554.0}, {"text": "cost. As a matter of fact, claw three sonnet is much more", "timestamp": "00:25:59,284", "timestamp_s": 1559.0}, {"text": "performance than any previous quad models, but is actually", "timestamp": "00:26:02,836", "timestamp_s": 1562.0}, {"text": "cheaper than cloud two and cloud 2.1 models.", "timestamp": "00:26:06,456", "timestamp_s": 1566.0}, {"text": "And then of course, client tree, opposite is the most intelligent model", "timestamp": "00:26:09,790", "timestamp_s": 1569.0}, {"text": "and has actually beat state of the art models", "timestamp": "00:26:13,992", "timestamp_s": 1573.0}, {"text": "on most benchmarks. And you can see this data just", "timestamp": "00:26:18,066", "timestamp_s": 1578.0}, {"text": "search cloud tree report paper, you see all those benchmarks", "timestamp": "00:26:21,932", "timestamp_s": 1581.0}, {"text": "that are available. So now how", "timestamp": "00:26:26,330", "timestamp_s": 1586.0}, {"text": "this entropic incredible performance", "timestamp": "00:26:30,080", "timestamp_s": 1590.0}, {"text": "and innovation we call three model impacts", "timestamp": "00:26:34,934", "timestamp_s": 1594.0}, {"text": "bedrock? Well, because the relationship that Amazon has with entropic", "timestamp": "00:26:38,342", "timestamp_s": 1598.0}, {"text": "claw three models are already available on bedrock", "timestamp": "00:26:43,338", "timestamp_s": 1603.0}, {"text": "as we speak right now. Claw three sonnet, which is their", "timestamp": "00:26:47,226", "timestamp_s": 1607.0}, {"text": "mid tier model, very big performance with", "timestamp": "00:26:50,852", "timestamp_s": 1610.0}, {"text": "very good price. All these models are multimodal.", "timestamp": "00:26:54,216", "timestamp_s": 1614.0}, {"text": "Let me just say that multimodal means you can input text,", "timestamp": "00:26:58,382", "timestamp_s": 1618.0}, {"text": "but also you can input images. Previous quad models could", "timestamp": "00:27:01,880", "timestamp_s": 1621.0}, {"text": "only receive text as inputs. Those models", "timestamp": "00:27:06,216", "timestamp_s": 1626.0}, {"text": "can actually receive image as input.", "timestamp": "00:27:10,402", "timestamp_s": 1630.0}, {"text": "So you can put an image and you can ask questions about that image.", "timestamp": "00:27:13,922", "timestamp_s": 1633.0}, {"text": "You can actually put multiple images per input. And all those three", "timestamp": "00:27:16,834", "timestamp_s": 1636.0}, {"text": "models actually have that capability. And all those", "timestamp": "00:27:20,512", "timestamp_s": 1640.0}, {"text": "models have now an even bigger context window.", "timestamp": "00:27:24,352", "timestamp_s": 1644.0}, {"text": "Not only they have bigger context window, but the claim", "timestamp": "00:27:28,102", "timestamp_s": 1648.0}, {"text": "is that with this bigger context window, doesn\u0027t matter", "timestamp": "00:27:32,058", "timestamp_s": 1652.0}, {"text": "where you put the text on those context window,", "timestamp": "00:27:35,812", "timestamp_s": 1655.0}, {"text": "the performance remains very similar and very good,", "timestamp": "00:27:38,714", "timestamp_s": 1658.0}, {"text": "which was not actually true in previous models and actually not in", "timestamp": "00:27:42,548", "timestamp_s": 1662.0}, {"text": "the industry as well. Claw three oppos and cloud", "timestamp": "00:27:46,088", "timestamp_s": 1666.0}, {"text": "three haiku are going to be made available on bedrock very", "timestamp": "00:27:49,672", "timestamp_s": 1669.0}, {"text": "soon. They are currently not available, but they\u0027re going to be made very", "timestamp": "00:27:53,432", "timestamp_s": 1673.0}, {"text": "soon. Now that I talked about it,", "timestamp": "00:27:56,796", "timestamp_s": 1676.0}, {"text": "let\u0027s just talk about some of the functionality that bedrock allows", "timestamp": "00:28:00,252", "timestamp_s": 1680.0}, {"text": "you to use. So first,", "timestamp": "00:28:03,538", "timestamp_s": 1683.0}, {"text": "you can actually use those foundational models as it is.", "timestamp": "00:28:06,970", "timestamp_s": 1686.0}, {"text": "But if you really want to fine tune and customize", "timestamp": "00:28:10,928", "timestamp_s": 1690.0}, {"text": "those models with your data, because you really believe you\u0027ve", "timestamp": "00:28:14,262", "timestamp_s": 1694.0}, {"text": "tried rag, you\u0027ve tried prompt engineering and you\u0027re", "timestamp": "00:28:17,462", "timestamp_s": 1697.0}, {"text": "not achieving the performance your use case require.", "timestamp": "00:28:21,334", "timestamp_s": 1701.0}, {"text": "In my opinion this should be the last resort. But if you need to privately", "timestamp": "00:28:24,298", "timestamp_s": 1704.0}, {"text": "customize models, you can actually use right now,", "timestamp": "00:28:27,930", "timestamp_s": 1707.0}, {"text": "bedrock supports to automatically customize those", "timestamp": "00:28:31,700", "timestamp_s": 1711.0}, {"text": "models. You put your data on s three in a private s three bucket.", "timestamp": "00:28:35,684", "timestamp_s": 1715.0}, {"text": "You connect that s three bucket to bedrock and bedrock will automatically", "timestamp": "00:28:39,310", "timestamp_s": 1719.0}, {"text": "fine tune and customize those models for you. Currently you can customize", "timestamp": "00:28:43,406", "timestamp_s": 1723.0}, {"text": "models with Titan, Cohere and Lemachu.", "timestamp": "00:28:47,646", "timestamp_s": 1727.0}, {"text": "Very soon we are going to open the ability to also customize", "timestamp": "00:28:50,830", "timestamp_s": 1730.0}, {"text": "cloud models and potentially other models from other model providers.", "timestamp": "00:28:54,706", "timestamp_s": 1734.0}, {"text": "So the good thing about this functionality from bedrock, if you have", "timestamp": "00:28:59,850", "timestamp_s": 1739.0}, {"text": "done some fine tuning customization from auto in the past, it actually", "timestamp": "00:29:03,712", "timestamp_s": 1743.0}, {"text": "requires a lot of science and it can be very complex. Bedrock completely", "timestamp": "00:29:07,296", "timestamp_s": 1747.0}, {"text": "removes that. You just put your data on s three,", "timestamp": "00:29:11,360", "timestamp_s": 1751.0}, {"text": "you go on bedrock and you point the data from bedrock on", "timestamp": "00:29:15,250", "timestamp_s": 1755.0}, {"text": "s three and you choose the model. And behind the scenes bedrock, you just customize", "timestamp": "00:29:19,332", "timestamp_s": 1759.0}, {"text": "new models and you notify when your specific model has", "timestamp": "00:29:23,082", "timestamp_s": 1763.0}, {"text": "been trained. No one else will be able to use this model.", "timestamp": "00:29:26,888", "timestamp_s": 1766.0}, {"text": "None of the data that you have actually provided from bedrock on", "timestamp": "00:29:30,248", "timestamp_s": 1770.0}, {"text": "s three will be used by anyone else or to train other models.", "timestamp": "00:29:33,848", "timestamp_s": 1773.0}, {"text": "It\u0027s just your model. And you can then consume that", "timestamp": "00:29:37,858", "timestamp_s": 1777.0}, {"text": "model and run inference on that model by", "timestamp": "00:29:41,532", "timestamp_s": 1781.0}, {"text": "making an API call. The same way you call API from", "timestamp": "00:29:46,410", "timestamp_s": 1786.0}, {"text": "Bedrock, you can call Bedrock API to", "timestamp": "00:29:49,872", "timestamp_s": 1789.0}, {"text": "use your own customizable model.", "timestamp": "00:29:53,088", "timestamp_s": 1793.0}, {"text": "Now another very good thing, if you don\u0027t need to", "timestamp": "00:29:56,270", "timestamp_s": 1796.0}, {"text": "customize your model is actually running retrieval augmented generation.", "timestamp": "00:29:59,712", "timestamp_s": 1799.0}, {"text": "Retrieval augmented generation, for folks that are not familiar is just", "timestamp": "00:30:04,042", "timestamp_s": 1804.0}, {"text": "the idea that, let\u0027s say you have a big data", "timestamp": "00:30:07,508", "timestamp_s": 1807.0}, {"text": "set of documents and those documents talk about the", "timestamp": "00:30:10,996", "timestamp_s": 1810.0}, {"text": "way your company operated and you want to have a chat bot", "timestamp": "00:30:15,768", "timestamp_s": 1815.0}, {"text": "that actually answer questions about those documentations,", "timestamp": "00:30:19,406", "timestamp_s": 1819.0}, {"text": "right? Well, those documents are likely private.", "timestamp": "00:30:22,750", "timestamp_s": 1822.0}, {"text": "So the foundational model, that model providers made it", "timestamp": "00:30:26,046", "timestamp_s": 1826.0}, {"text": "available on Bedrock, they don\u0027t know about your company operational", "timestamp": "00:30:29,692", "timestamp_s": 1829.0}, {"text": "procedures. But when you\u0027re creating a chatbot, you actually want to make that", "timestamp": "00:30:33,890", "timestamp_s": 1833.0}, {"text": "available for the", "timestamp": "00:30:37,788", "timestamp_s": 1837.0}, {"text": "model itself to actually consume. So what you can do,", "timestamp": "00:30:41,408", "timestamp_s": 1841.0}, {"text": "you can use what we call vector databases.", "timestamp": "00:30:45,040", "timestamp_s": 1845.0}, {"text": "And I\u0027m going to talk in a moment on what vector databases are", "timestamp": "00:30:48,486", "timestamp_s": 1848.0}, {"text": "made available on AWS. But Bedrock has a feature", "timestamp": "00:30:52,212", "timestamp_s": 1852.0}, {"text": "called knowledge base that makes all this process of running retrieval", "timestamp": "00:30:56,266", "timestamp_s": 1856.0}, {"text": "augmented generation very simple. The way it works is", "timestamp": "00:31:00,218", "timestamp_s": 1860.0}, {"text": "you go on bedrock, you first create an", "timestamp": "00:31:03,972", "timestamp_s": 1863.0}, {"text": "S three bucket and you put all your documents on this s three bucket.", "timestamp": "00:31:07,688", "timestamp_s": 1867.0}, {"text": "It\u0027s your s three bucket. Nobody has access.", "timestamp": "00:31:11,118", "timestamp_s": 1871.0}, {"text": "Then you go on bedrock you choose", "timestamp": "00:31:13,944", "timestamp_s": 1873.0}, {"text": "which model you actually want to run embeddings.", "timestamp": "00:31:18,300", "timestamp_s": 1878.0}, {"text": "So you can choose between Titan for now, Titan and", "timestamp": "00:31:21,490", "timestamp_s": 1881.0}, {"text": "cohere embeddings are just going through those documents,", "timestamp": "00:31:24,732", "timestamp_s": 1884.0}, {"text": "converting those texts into vector numerical,", "timestamp": "00:31:28,550", "timestamp_s": 1888.0}, {"text": "vector vector representations. And then finally you choose a vector", "timestamp": "00:31:32,246", "timestamp_s": 1892.0}, {"text": "database. And right now you have a variety of databases", "timestamp": "00:31:36,022", "timestamp_s": 1896.0}, {"text": "that you can select from. I think there are four options right now", "timestamp": "00:31:40,006", "timestamp_s": 1900.0}, {"text": "that you can select and those numbers are going to be increasing in the future.", "timestamp": "00:31:43,428", "timestamp_s": 1903.0}, {"text": "But you can select, for example, the open search serverless vector database.", "timestamp": "00:31:46,564", "timestamp_s": 1906.0}, {"text": "Then automatically bedrock will run the embeddings", "timestamp": "00:31:50,826", "timestamp_s": 1910.0}, {"text": "on the data that is on s three will store the vectors on your open", "timestamp": "00:31:54,938", "timestamp_s": 1914.0}, {"text": "search vector database. And finally, which is,", "timestamp": "00:31:58,936", "timestamp_s": 1918.0}, {"text": "let\u0027s say you want to run this chatbot. When you ask", "timestamp": "00:32:02,280", "timestamp_s": 1922.0}, {"text": "a question, let\u0027s say you ask a question, what is the", "timestamp": "00:32:05,692", "timestamp_s": 1925.0}, {"text": "HR policy for vacation", "timestamp": "00:32:08,780", "timestamp_s": 1928.0}, {"text": "in New York as an example, right? What bedrock", "timestamp": "00:32:12,594", "timestamp_s": 1932.0}, {"text": "can do, it can then retrieve your vector database by running what", "timestamp": "00:32:16,418", "timestamp_s": 1936.0}, {"text": "we call semantic search. It can find the specific", "timestamp": "00:32:19,808", "timestamp_s": 1939.0}, {"text": "chunks of text that are very likely to respond my question.", "timestamp": "00:32:23,104", "timestamp_s": 1943.0}, {"text": "You copy those chunks of text into bedrock and", "timestamp": "00:32:26,672", "timestamp_s": 1946.0}, {"text": "then you run your question, plus the combination", "timestamp": "00:32:30,752", "timestamp_s": 1950.0}, {"text": "of chunks of text that has been retrieved from the", "timestamp": "00:32:34,618", "timestamp_s": 1954.0}, {"text": "database, from the vector database, and you send that to your", "timestamp": "00:32:38,324", "timestamp_s": 1958.0}, {"text": "foundational model. Then the foundational model, let\u0027s say cloud three,", "timestamp": "00:32:41,492", "timestamp_s": 1961.0}, {"text": "will see all the chunks of text that talks about vacation policy,", "timestamp": "00:32:44,964", "timestamp_s": 1964.0}, {"text": "New York. And you see your question. And then based on the information that you", "timestamp": "00:32:48,184", "timestamp_s": 1968.0}, {"text": "have provided, because now the model has access to", "timestamp": "00:32:52,168", "timestamp_s": 1972.0}, {"text": "the chunks of data that has the answer, will be able to provide an answer", "timestamp": "00:32:55,528", "timestamp_s": 1975.0}, {"text": "for you. That is what is called retrieval augmented generation.", "timestamp": "00:32:59,276", "timestamp_s": 1979.0}, {"text": "And you can actually run very simple with", "timestamp": "00:33:02,754", "timestamp_s": 1982.0}, {"text": "knowledge bases. So that is one capability that", "timestamp": "00:33:08,128", "timestamp_s": 1988.0}, {"text": "you can run. It\u0027s all managed for you and you can choose different", "timestamp": "00:33:12,352", "timestamp_s": 1992.0}, {"text": "models to actually run. Another functionality is the", "timestamp": "00:33:16,016", "timestamp_s": 1996.0}, {"text": "ability to enable generative AI applications to execute", "timestamp": "00:33:19,728", "timestamp_s": 1999.0}, {"text": "steps outside your model. So let\u0027s say you have an", "timestamp": "00:33:23,498", "timestamp_s": 2003.0}, {"text": "API where if", "timestamp": "00:33:27,732", "timestamp_s": 2007.0}, {"text": "someone on your chat bot asks the question about what is the current", "timestamp": "00:33:31,892", "timestamp_s": 2011.0}, {"text": "price of this stock, right? The model is not going to be able to answer", "timestamp": "00:33:36,120", "timestamp_s": 2016.0}, {"text": "that question. Or probably if he answered that question, it\u0027s going to hallucinate,", "timestamp": "00:33:40,216", "timestamp_s": 2020.0}, {"text": "meaning it\u0027s not going to be accurate, right? Because the training data", "timestamp": "00:33:43,918", "timestamp_s": 2023.0}, {"text": "from that model was probably months ago or years ago. What you", "timestamp": "00:33:47,432", "timestamp_s": 2027.0}, {"text": "can do on bedrock, you can use agents for bedrock. What allows you to", "timestamp": "00:33:51,004", "timestamp_s": 2031.0}, {"text": "do is to provide. So you select a model, let\u0027s say cloud model,", "timestamp": "00:33:54,972", "timestamp_s": 2034.0}, {"text": "you provide the basic of set instructions,", "timestamp": "00:33:58,636", "timestamp_s": 2038.0}, {"text": "then you choose different data sources, maybe different APIs,", "timestamp": "00:34:02,278", "timestamp_s": 2042.0}, {"text": "and then you specify the actions that it can take.", "timestamp": "00:34:06,670", "timestamp_s": 2046.0}, {"text": "So the example I provided, right, you can say if someone asks you", "timestamp": "00:34:09,536", "timestamp_s": 2049.0}, {"text": "about the pricing of a stock,", "timestamp": "00:34:13,972", "timestamp_s": 2053.0}, {"text": "you need to call this API. Here is the open", "timestamp": "00:34:17,090", "timestamp_s": 2057.0}, {"text": "API spec of my API and this is how you can call the API.", "timestamp": "00:34:21,204", "timestamp_s": 2061.0}, {"text": "So what agents for bedrock do you ask? A question", "timestamp": "00:34:25,454", "timestamp_s": 2065.0}, {"text": "for your model. Your model realizes it needs to actually", "timestamp": "00:34:29,510", "timestamp_s": 2069.0}, {"text": "make a action, take an action on that request.", "timestamp": "00:34:32,840", "timestamp_s": 2072.0}, {"text": "Behind the scenes, what bedrock will do, we will actually call Lambda,", "timestamp": "00:34:36,386", "timestamp_s": 2076.0}, {"text": "which is a serverless compute platform", "timestamp": "00:34:39,954", "timestamp_s": 2079.0}, {"text": "with lambda. The model will actually trigger a lambda.", "timestamp": "00:34:43,244", "timestamp_s": 2083.0}, {"text": "The lambda code will already be prebuilt. Behind the scenes you call", "timestamp": "00:34:47,810", "timestamp_s": 2087.0}, {"text": "the API that you have told bedrock to do", "timestamp": "00:34:51,952", "timestamp_s": 2091.0}, {"text": "and then that API will come with a response, let\u0027s say the", "timestamp": "00:34:55,920", "timestamp_s": 2095.0}, {"text": "value of your stock. And then you return to the larger language model to provide", "timestamp": "00:34:59,488", "timestamp_s": 2099.0}, {"text": "you with the response. This is just one example, but what you can do with", "timestamp": "00:35:03,332", "timestamp_s": 2103.0}, {"text": "bedrock, you can break down and orchestrate tasks.", "timestamp": "00:35:06,708", "timestamp_s": 2106.0}, {"text": "You can invoke whatever API on your behalf so", "timestamp": "00:35:10,850", "timestamp_s": 2110.0}, {"text": "you can do a lot of automation. And the capabilities", "timestamp": "00:35:14,644", "timestamp_s": 2114.0}, {"text": "here are really infinite. It\u0027s just you configuring those", "timestamp": "00:35:18,062", "timestamp_s": 2118.0}, {"text": "agents properly so you can do a lot of chain of thought as well", "timestamp": "00:35:22,056", "timestamp_s": 2122.0}, {"text": "on top of that. So moving on, on the ability that,", "timestamp": "00:35:25,772", "timestamp_s": 2125.0}, {"text": "what are the ability that we have for making", "timestamp": "00:35:30,890", "timestamp_s": 2130.0}, {"text": "the responses very secure and safe? On bedrock", "timestamp": "00:35:35,052", "timestamp_s": 2135.0}, {"text": "we have a functionality that is currently in preview, but it\u0027s called guardrails.", "timestamp": "00:35:38,742", "timestamp_s": 2138.0}, {"text": "What guardrails allows you to do is to create consistently", "timestamp": "00:35:42,966", "timestamp_s": 2142.0}, {"text": "safeguards, including on your models. Doesn\u0027t matter", "timestamp": "00:35:47,398", "timestamp_s": 2147.0}, {"text": "if they are fine tuned or agents what it does,", "timestamp": "00:35:51,328", "timestamp_s": 2151.0}, {"text": "you can create filters for harmful content both on", "timestamp": "00:35:54,500", "timestamp_s": 2154.0}, {"text": "the input that you\u0027re sending to bedrock and also the output that bedrock", "timestamp": "00:35:57,828", "timestamp_s": 2157.0}, {"text": "will tell you, right? So I\u0027ll give you an example,", "timestamp": "00:36:01,898", "timestamp_s": 2161.0}, {"text": "right, let\u0027s say the example you", "timestamp": "00:36:05,224", "timestamp_s": 2165.0}, {"text": "see here on the screen. Let\u0027s say someone asks you about investment", "timestamp": "00:36:08,408", "timestamp_s": 2168.0}, {"text": "and device on your chat bot and you don\u0027t want to have that", "timestamp": "00:36:12,462", "timestamp_s": 2172.0}, {"text": "input and actually output to be sent to the customer.", "timestamp": "00:36:16,840", "timestamp_s": 2176.0}, {"text": "Right? So what you can do, you can create those filters and you can", "timestamp": "00:36:20,204", "timestamp_s": 2180.0}, {"text": "say these topics deny and then this", "timestamp": "00:36:23,548", "timestamp_s": 2183.0}, {"text": "is the response you should be giving back if someone is trying to", "timestamp": "00:36:26,988", "timestamp_s": 2186.0}, {"text": "ask questions about investment advice. So you don\u0027t get into legal", "timestamp": "00:36:30,960", "timestamp_s": 2190.0}, {"text": "complaints or problems that you might get into the future,", "timestamp": "00:36:34,582", "timestamp_s": 2194.0}, {"text": "right? So this is one of the capabilities that is available", "timestamp": "00:36:37,984", "timestamp_s": 2197.0}, {"text": "on bedrock and it\u0027s called guardrails. Another functionality", "timestamp": "00:36:41,792", "timestamp_s": 2201.0}, {"text": "that I\u0027m going to talk about it is batch.", "timestamp": "00:36:46,186", "timestamp_s": 2206.0}, {"text": "So everything I\u0027ve talked about so far is you just run an", "timestamp": "00:36:49,146", "timestamp_s": 2209.0}, {"text": "API call and you receive a response. Pretty much", "timestamp": "00:36:52,468", "timestamp_s": 2212.0}, {"text": "synchronous, right? API call goes in, API call", "timestamp": "00:36:56,200", "timestamp_s": 2216.0}, {"text": "comes back. There are some use cases that don\u0027t require", "timestamp": "00:37:00,216", "timestamp_s": 2220.0}, {"text": "live interaction, but you want to run a lot of", "timestamp": "00:37:04,510", "timestamp_s": 2224.0}, {"text": "inference for a lot of documents in a batch mode.", "timestamp": "00:37:07,804", "timestamp_s": 2227.0}, {"text": "So what Bedrock can do, its batch", "timestamp": "00:37:11,426", "timestamp_s": 2231.0}, {"text": "mode allows you to efficiently run inference on the large volumes", "timestamp": "00:37:15,266", "timestamp_s": 2235.0}, {"text": "of data. So you can put the data on s three, you can put different", "timestamp": "00:37:19,218", "timestamp_s": 2239.0}, {"text": "json files with the prompt and the data you want to run behind the scenes.", "timestamp": "00:37:22,496", "timestamp_s": 2242.0}, {"text": "Bedrock, you grab those files, you run the inference, you save the", "timestamp": "00:37:26,262", "timestamp_s": 2246.0}, {"text": "results of those inference in another s three as the result,", "timestamp": "00:37:30,528", "timestamp_s": 2250.0}, {"text": "and it\u0027s completely managed for you. And once", "timestamp": "00:37:34,228", "timestamp_s": 2254.0}, {"text": "the batch is completed, you can get notified and you can do a lot", "timestamp": "00:37:37,540", "timestamp_s": 2257.0}, {"text": "of different automation. So you don\u0027t need to write any code for", "timestamp": "00:37:41,124", "timestamp_s": 2261.0}, {"text": "handling failures or restarts. Bedrock would", "timestamp": "00:37:44,692", "timestamp_s": 2264.0}, {"text": "take care of that for you. And you can run that with base foundational", "timestamp": "00:37:48,244", "timestamp_s": 2268.0}, {"text": "models or your custom trainium models", "timestamp": "00:37:51,966", "timestamp_s": 2271.0}, {"text": "as well. One last nice feature", "timestamp": "00:37:55,214", "timestamp_s": 2275.0}, {"text": "about Bedrock is model evaluation. It\u0027s still in preview,", "timestamp": "00:37:58,930", "timestamp_s": 2278.0}, {"text": "but model evaluation is a really good feature of Bedrock.", "timestamp": "00:38:02,754", "timestamp_s": 2282.0}, {"text": "As you saw, Bedrock offers you a wide variety of", "timestamp": "00:38:06,450", "timestamp_s": 2286.0}, {"text": "model providers and models available. From those models providers,", "timestamp": "00:38:09,968", "timestamp_s": 2289.0}, {"text": "it can be really complex to evaluate those", "timestamp": "00:38:14,358", "timestamp_s": 2294.0}, {"text": "foundational models and select the best one. So what model evaluation on", "timestamp": "00:38:17,648", "timestamp_s": 2297.0}, {"text": "bedrock allows you to do is to choose different", "timestamp": "00:38:21,668", "timestamp_s": 2301.0}, {"text": "tests. And those evaluation tests can be either automatic", "timestamp": "00:38:25,460", "timestamp_s": 2305.0}, {"text": "benchmarks that the industry use and bedrock", "timestamp": "00:38:29,642", "timestamp_s": 2309.0}, {"text": "makes available, but you can also create your own human evaluation.", "timestamp": "00:38:34,330", "timestamp_s": 2314.0}, {"text": "You can have actually humans evaluating the response from different models", "timestamp": "00:38:38,030", "timestamp_s": 2318.0}, {"text": "and rating those models without actually knowing which model it is.", "timestamp": "00:38:42,030", "timestamp_s": 2322.0}, {"text": "So there is no bias into the place. And you can have", "timestamp": "00:38:45,688", "timestamp_s": 2325.0}, {"text": "your own data sets of questions and you can create your", "timestamp": "00:38:49,132", "timestamp_s": 2329.0}, {"text": "own custom metrics or use", "timestamp": "00:38:52,412", "timestamp_s": 2332.0}, {"text": "the metrics that comes with it. So some of the metrics that are there", "timestamp": "00:38:56,410", "timestamp_s": 2336.0}, {"text": "are the accuracy, are the toxicity and the robustness", "timestamp": "00:39:00,384", "timestamp_s": 2340.0}, {"text": "of the response. And you can see here a screenshot of a", "timestamp": "00:39:04,326", "timestamp_s": 2344.0}, {"text": "human evaluation report across different models being tested", "timestamp": "00:39:07,648", "timestamp_s": 2347.0}, {"text": "and automatic evaluation report. So I\u0027ve", "timestamp": "00:39:11,878", "timestamp_s": 2351.0}, {"text": "talked a lot about the different features", "timestamp": "00:39:15,082", "timestamp_s": 2355.0}, {"text": "that bedrock makes available for you. But one of", "timestamp": "00:39:18,138", "timestamp_s": 2358.0}, {"text": "the things that is important to highlight is right now,", "timestamp": "00:39:21,364", "timestamp_s": 2361.0}, {"text": "thousands and thousands of customers are using bedrock", "timestamp": "00:39:25,030", "timestamp_s": 2365.0}, {"text": "because the capability, the democratization,", "timestamp": "00:39:29,086", "timestamp_s": 2369.0}, {"text": "the flexibility and the feature set that bedrock allows", "timestamp": "00:39:32,174", "timestamp_s": 2372.0}, {"text": "them to build generative AI on top of pretty much every single", "timestamp": "00:39:35,998", "timestamp_s": 2375.0}, {"text": "industry, right? So you can see big names like Adidas,", "timestamp": "00:39:40,220", "timestamp_s": 2380.0}, {"text": "you can see names like the BMW group,", "timestamp": "00:39:44,114", "timestamp_s": 2384.0}, {"text": "Salesforce and many, many others so highly encourage you to", "timestamp": "00:39:47,148", "timestamp_s": 2387.0}, {"text": "test bedrock because it\u0027s a really cool feature. Two more", "timestamp": "00:39:50,688", "timestamp_s": 2390.0}, {"text": "things before we go to the demo is we talked about the", "timestamp": "00:39:54,352", "timestamp_s": 2394.0}, {"text": "retrieval, augmented generation and the need for vector databases.", "timestamp": "00:39:58,832", "timestamp_s": 2398.0}, {"text": "And I just want to quickly tell you the story about vector databases on", "timestamp": "00:40:03,114", "timestamp_s": 2403.0}, {"text": "AWS. AWS has a wide variety of", "timestamp": "00:40:06,692", "timestamp_s": 2406.0}, {"text": "different databases that support vectors. As you can see here,", "timestamp": "00:40:10,116", "timestamp_s": 2410.0}, {"text": "we have six databases that are now", "timestamp": "00:40:14,232", "timestamp_s": 2414.0}, {"text": "supporting vector databases and depending on the use case,", "timestamp": "00:40:17,912", "timestamp_s": 2417.0}, {"text": "you might choose one versus the other. The important thing here", "timestamp": "00:40:21,416", "timestamp_s": 2421.0}, {"text": "is to understand that AWS is giving", "timestamp": "00:40:24,904", "timestamp_s": 2424.0}, {"text": "the flexibility to pick and choose from the", "timestamp": "00:40:28,460", "timestamp_s": 2428.0}, {"text": "database that makes the most sense for you. So a", "timestamp": "00:40:31,708", "timestamp_s": 2431.0}, {"text": "very popular database on AWS for vector is opensearch.", "timestamp": "00:40:35,292", "timestamp_s": 2435.0}, {"text": "So OpenSearch has a functionality for vectors", "timestamp": "00:40:39,634", "timestamp_s": 2439.0}, {"text": "and you can actually even run OpenSearch serverless for Vector", "timestamp": "00:40:42,966", "timestamp_s": 2442.0}, {"text": "database that have a very good performance and price. But you can see here documentDB", "timestamp": "00:40:46,806", "timestamp_s": 2446.0}, {"text": "now has support for vector. MemoryDB for", "timestamp": "00:40:51,322", "timestamp_s": 2451.0}, {"text": "redis has also support for vector RDS for postgres.", "timestamp": "00:40:54,788", "timestamp_s": 2454.0}, {"text": "So if you\u0027re running a SQL database and you have a relational", "timestamp": "00:40:58,314", "timestamp_s": 2458.0}, {"text": "use case and you also want to run specific vectors,", "timestamp": "00:41:02,202", "timestamp_s": 2462.0}, {"text": "you can run pgvector which is a plugin library for", "timestamp": "00:41:06,398", "timestamp_s": 2466.0}, {"text": "postgres that can run also on top of both RDs", "timestamp": "00:41:09,960", "timestamp_s": 2469.0}, {"text": "and Aurora postgres. If you\u0027re doing graph databases,", "timestamp": "00:41:13,646", "timestamp_s": 2473.0}, {"text": "you can actually run on top of Netun. And as I talked about it", "timestamp": "00:41:16,974", "timestamp_s": 2476.0}, {"text": "right now, the direct integration for knowledge databases on bedrock supports", "timestamp": "00:41:20,764", "timestamp_s": 2480.0}, {"text": "open search, redis, enterprise, cloud and Pinecone.", "timestamp": "00:41:25,122", "timestamp_s": 2485.0}, {"text": "But very soon Aurora, Amazon, Aurora and MongoDB are", "timestamp": "00:41:28,466", "timestamp_s": 2488.0}, {"text": "going to be made available as well. So that is", "timestamp": "00:41:32,368", "timestamp_s": 2492.0}, {"text": "about vector database. The last thing I want to talk about it is the", "timestamp": "00:41:35,888", "timestamp_s": 2495.0}, {"text": "capability for code generation and code assistant for", "timestamp": "00:41:40,450", "timestamp_s": 2500.0}, {"text": "developers. So AWS has a service called code Whisper", "timestamp": "00:41:43,972", "timestamp_s": 2503.0}, {"text": "which is AI powered code suggestion, as you see here", "timestamp": "00:41:47,882", "timestamp_s": 2507.0}, {"text": "on the small video that has actually let me play", "timestamp": "00:41:52,470", "timestamp_s": 2512.0}, {"text": "it again, the small video that is", "timestamp": "00:41:55,768", "timestamp_s": 2515.0}, {"text": "demonstrating here, in this case it\u0027s a JavaScript code.", "timestamp": "00:41:59,272", "timestamp_s": 2519.0}, {"text": "You can provide a single comment, in this case, parse the CSV", "timestamp": "00:42:04,488", "timestamp_s": 2524.0}, {"text": "string and return a list of songs with positional or position", "timestamp": "00:42:08,002", "timestamp_s": 2528.0}, {"text": "original chart date, artist title and ignore lines.", "timestamp": "00:42:12,092", "timestamp_s": 2532.0}, {"text": "We\u0027re starting with hashtag, right?", "timestamp": "00:42:15,778", "timestamp_s": 2535.0}, {"text": "Then you just click tab and it automatically returns the code generation.", "timestamp": "00:42:18,972", "timestamp_s": 2538.0}, {"text": "This is pretty cool. And the way it works is you just have your", "timestamp": "00:42:23,494", "timestamp_s": 2543.0}, {"text": "ID and there is support for a variety of vs", "timestamp": "00:42:27,536", "timestamp_s": 2547.0}, {"text": "code, jetbrains, cloud, nine, lambda,", "timestamp": "00:42:31,402", "timestamp_s": 2551.0}, {"text": "Jupyter notebooks. There are supports for pretty much all the popular", "timestamp": "00:42:35,034", "timestamp_s": 2555.0}, {"text": "IDs out there. Install the plugin from AWS", "timestamp": "00:42:39,866", "timestamp_s": 2559.0}, {"text": "that has code whisper support, then you can receive code suggestions,", "timestamp": "00:42:43,946", "timestamp_s": 2563.0}, {"text": "and code whisper can actually do more than that. You receive real", "timestamp": "00:42:48,030", "timestamp_s": 2568.0}, {"text": "time code suggestions for a variety of programming languages like", "timestamp": "00:42:52,136", "timestamp_s": 2572.0}, {"text": "Java, JavaScript, go.", "timestamp": "00:42:56,316", "timestamp_s": 2576.0}, {"text": "Net, and many, many others, but not also programming", "timestamp": "00:42:59,210", "timestamp_s": 2579.0}, {"text": "languages. If you\u0027re building infrastructure, AWS, a code terraform or cloud formation,", "timestamp": "00:43:02,738", "timestamp_s": 2582.0}, {"text": "you can also have suggestions for those. On top of being", "timestamp": "00:43:06,674", "timestamp_s": 2586.0}, {"text": "an assistant for developers and improving productivity quite significantly,", "timestamp": "00:43:10,576", "timestamp_s": 2590.0}, {"text": "you can also have a security scam. So the code that is being suggested for", "timestamp": "00:43:15,542", "timestamp_s": 2595.0}, {"text": "you can actually give you security suggestions", "timestamp": "00:43:19,632", "timestamp_s": 2599.0}, {"text": "to make sure you\u0027re writing actually secure code. And you can also have reference", "timestamp": "00:43:23,882", "timestamp_s": 2603.0}, {"text": "tracker for different licenses on open source on", "timestamp": "00:43:28,282", "timestamp_s": 2608.0}, {"text": "the data that has been trained. So if whatever suggestion is being given", "timestamp": "00:43:32,308", "timestamp_s": 2612.0}, {"text": "to you has been trained from an open", "timestamp": "00:43:35,992", "timestamp_s": 2615.0}, {"text": "source repository that has a potentially prohibitive", "timestamp": "00:43:40,120", "timestamp_s": 2620.0}, {"text": "license, you can actually have that warning telling", "timestamp": "00:43:44,142", "timestamp_s": 2624.0}, {"text": "you. And if you are an enterprise version of code whisper, you could say", "timestamp": "00:43:48,722", "timestamp_s": 2628.0}, {"text": "developers should never receive recommendation for code that has been", "timestamp": "00:43:52,188", "timestamp_s": 2632.0}, {"text": "generated on this specific license that is prohibitive", "timestamp": "00:43:55,692", "timestamp_s": 2635.0}, {"text": "for my business use case one of the great things about", "timestamp": "00:43:59,046", "timestamp_s": 2639.0}, {"text": "code Whisper is code whisper for individuals are", "timestamp": "00:44:02,912", "timestamp_s": 2642.0}, {"text": "free. We are only one of the only companies that have an", "timestamp": "00:44:07,136", "timestamp_s": 2647.0}, {"text": "enterprise grade product that if you\u0027re using", "timestamp": "00:44:12,050", "timestamp_s": 2652.0}, {"text": "for an individual user like not", "timestamp": "00:44:15,828", "timestamp_s": 2655.0}, {"text": "a company, you can install codebisper created", "timestamp": "00:44:19,012", "timestamp_s": 2659.0}, {"text": "an AWS builder account. You don\u0027t even need to have an AWS", "timestamp": "00:44:23,242", "timestamp_s": 2663.0}, {"text": "account, you just need to have a login with build", "timestamp": "00:44:27,278", "timestamp_s": 2667.0}, {"text": "Id we call builder ID. You can use it for free.", "timestamp": "00:44:31,422", "timestamp_s": 2671.0}, {"text": "Some features are only for enterprise, but most features and most", "timestamp": "00:44:34,936", "timestamp_s": 2674.0}, {"text": "important feature which is code suggestions are actually available", "timestamp": "00:44:38,332", "timestamp_s": 2678.0}, {"text": "for free. So I highly encourage everyone to take a look on this and", "timestamp": "00:44:41,740", "timestamp_s": 2681.0}, {"text": "then hopefully this was a good overview of the", "timestamp": "00:44:46,508", "timestamp_s": 2686.0}, {"text": "offerings on AWS for generative AI, most important for", "timestamp": "00:44:51,152", "timestamp_s": 2691.0}, {"text": "bedrock. So I\u0027ll pause here and I\u0027ll come back", "timestamp": "00:44:55,152", "timestamp_s": 2695.0}, {"text": "sharing my screen to actually do a presentation and a demo on", "timestamp": "00:44:58,784", "timestamp_s": 2698.0}, {"text": "how can you utilize some of those functionalities in", "timestamp": "00:45:03,970", "timestamp_s": 2703.0}, {"text": "the real world? Actually clicking buttons and making API calls", "timestamp": "00:45:08,820", "timestamp_s": 2708.0}, {"text": "and writing some code. Awesome. So let\u0027s quickly", "timestamp": "00:45:12,554", "timestamp_s": 2712.0}, {"text": "jump into the demo. Very simple. I have logged", "timestamp": "00:45:16,360", "timestamp_s": 2716.0}, {"text": "in into my AWS account. I can search here the bedrock", "timestamp": "00:45:19,838", "timestamp_s": 2719.0}, {"text": "service. I\u0027ll go and I\u0027ll jump inside the bedrock", "timestamp": "00:45:23,742", "timestamp_s": 2723.0}, {"text": "service. Right now bedrock", "timestamp": "00:45:27,202", "timestamp_s": 2727.0}, {"text": "is available in a few AWS regions. In this example we are", "timestamp": "00:45:31,218", "timestamp_s": 2731.0}, {"text": "using North Virginia US east one region. If I click here", "timestamp": "00:45:34,748", "timestamp_s": 2734.0}, {"text": "on my left side you can see the menu, you can", "timestamp": "00:45:38,476", "timestamp_s": 2738.0}, {"text": "see some examples how to get start. You can just open those on", "timestamp": "00:45:42,032", "timestamp_s": 2742.0}, {"text": "playground here. If you click on the provider you can see the", "timestamp": "00:45:45,792", "timestamp_s": 2745.0}, {"text": "providers that I just actually showed to you on the", "timestamp": "00:45:49,328", "timestamp_s": 2749.0}, {"text": "presentation. You can see some of the base models.", "timestamp": "00:45:52,628", "timestamp_s": 2752.0}, {"text": "So each provider, for example entropic here have", "timestamp": "00:45:56,138", "timestamp_s": 2756.0}, {"text": "the cloud models. You can see all the different cloud models that are currently available.", "timestamp": "00:45:59,412", "timestamp_s": 2759.0}, {"text": "So I have for example cloud three sonnet, which is the median model that have", "timestamp": "00:46:03,652", "timestamp_s": 2763.0}, {"text": "just got released this week. Have cloud 2.1, cloud two", "timestamp": "00:46:07,672", "timestamp_s": 2767.0}, {"text": "and cloud 1.2 instant.", "timestamp": "00:46:11,800", "timestamp_s": 2771.0}, {"text": "In this case, I don\u0027t have any custom model,", "timestamp": "00:46:15,990", "timestamp_s": 2775.0}, {"text": "but if I had trainium before a custom model,", "timestamp": "00:46:18,792", "timestamp_s": 2778.0}, {"text": "I would see the list of training models here. If I wanted to customize a", "timestamp": "00:46:22,556", "timestamp_s": 2782.0}, {"text": "new model, could just go here, create a new fine tune job,", "timestamp": "00:46:26,444", "timestamp_s": 2786.0}, {"text": "or continue fine tune job. But the thing I want", "timestamp": "00:46:29,724", "timestamp_s": 2789.0}, {"text": "to show you, you can actually get started and play around and test some of", "timestamp": "00:46:33,264", "timestamp_s": 2793.0}, {"text": "the models by just going to the playground. So if you look here, the playground,", "timestamp": "00:46:37,008", "timestamp_s": 2797.0}, {"text": "you have the chat option. And what I really like about the", "timestamp": "00:46:40,630", "timestamp_s": 2800.0}, {"text": "chat option, I\u0027ll just give you first example how you can actually talk to a", "timestamp": "00:46:44,048", "timestamp_s": 2804.0}, {"text": "model. Let\u0027s say you want to talk to Claude and you want to talk to", "timestamp": "00:46:47,748", "timestamp_s": 2807.0}, {"text": "the new Claude tree model, which is one of the most performance", "timestamp": "00:46:50,692", "timestamp_s": 2810.0}, {"text": "in the industry. So let\u0027s just say, write me a poem", "timestamp": "00:46:54,574", "timestamp_s": 2814.0}, {"text": "about AWS and its ecosystem.", "timestamp": "00:46:58,862", "timestamp_s": 2818.0}, {"text": "Just a simple poem here.", "timestamp": "00:47:02,750", "timestamp_s": 2822.0}, {"text": "So I can put the entry here. Because this is a multi modality", "timestamp": "00:47:06,024", "timestamp_s": 2826.0}, {"text": "model, I can also put an image, I\u0027ll do a demo of an image", "timestamp": "00:47:09,598", "timestamp_s": 2829.0}, {"text": "in a moment. You can see all the configurations of the hyperparameters,", "timestamp": "00:47:13,154", "timestamp_s": 2833.0}, {"text": "like temperature, top P, top K. The length", "timestamp": "00:47:16,626", "timestamp_s": 2836.0}, {"text": "of the output can be controlled here. In this case I\u0027m just", "timestamp": "00:47:20,230", "timestamp_s": 2840.0}, {"text": "keeping for 2000 tokens maximum.", "timestamp": "00:47:23,888", "timestamp_s": 2843.0}, {"text": "You can see this on demand. If I click run,", "timestamp": "00:47:27,398", "timestamp_s": 2847.0}, {"text": "it\u0027s actually calling the model and then it\u0027s actually generating,", "timestamp": "00:47:31,550", "timestamp_s": 2851.0}, {"text": "in this case generating a poem for AWS and its ecosystem.", "timestamp": "00:47:35,462", "timestamp_s": 2855.0}, {"text": "Right. You can see that it\u0027s pretty cool. One thing that I really like", "timestamp": "00:47:39,146", "timestamp_s": 2859.0}, {"text": "about the playground are the following, as it\u0027s finishing", "timestamp": "00:47:42,836", "timestamp_s": 2862.0}, {"text": "generating here, if we click on the three dots on the top menu,", "timestamp": "00:47:46,382", "timestamp_s": 2866.0}, {"text": "you can export it as JSON and you can see streaming preference because", "timestamp": "00:47:51,110", "timestamp_s": 2871.0}, {"text": "you are streaming. But the other thing that I like, you can", "timestamp": "00:47:54,728", "timestamp_s": 2874.0}, {"text": "go down and you can see some model metrics. So you can see this actually", "timestamp": "00:47:58,428", "timestamp_s": 2878.0}, {"text": "took 15,000", "timestamp": "00:48:02,060", "timestamp_s": 2882.0}, {"text": "milliseconds. You tell me how many input tokens, how many", "timestamp": "00:48:05,852", "timestamp_s": 2885.0}, {"text": "output tokens, and this is the cost, it\u0027s 0.0.", "timestamp": "00:48:09,756", "timestamp_s": 2889.0}, {"text": "Because it\u0027s less than 0.0, there is like be a zero point", "timestamp": "00:48:12,848", "timestamp_s": 2892.0}, {"text": "something that this will cost. Right. What I really like about", "timestamp": "00:48:16,816", "timestamp_s": 2896.0}, {"text": "it on the chat AWS, well, we can compare models. So let\u0027s say I want", "timestamp": "00:48:20,592", "timestamp_s": 2900.0}, {"text": "to compare claw three versus claw 2.1,", "timestamp": "00:48:23,828", "timestamp_s": 2903.0}, {"text": "right? And I\u0027m going to talk about it here. Let\u0027s see.", "timestamp": "00:48:27,460", "timestamp_s": 2907.0}, {"text": "Talk about the", "timestamp": "00:48:31,010", "timestamp_s": 2911.0}, {"text": "word economy in the 99", "timestamp": "00:48:34,760", "timestamp_s": 2914.0}, {"text": "year, right? And I can go and I can run. So it\u0027s", "timestamp": "00:48:39,304", "timestamp_s": 2919.0}, {"text": "going to run both models at the same time and I will", "timestamp": "00:48:42,654", "timestamp_s": 2922.0}, {"text": "be able to compare the performance of both models, this is just like", "timestamp": "00:48:46,248", "timestamp_s": 2926.0}, {"text": "by reading them. So let\u0027s just wait a little bit.", "timestamp": "00:48:49,932", "timestamp_s": 2929.0}, {"text": "So you can see here he has outputted. So cloud 2.1", "timestamp": "00:48:53,452", "timestamp_s": 2933.0}, {"text": "has outputted. I can see here, compare the response,", "timestamp": "00:48:57,052", "timestamp_s": 2937.0}, {"text": "but I can see down below here how many tokens each one of", "timestamp": "00:49:00,758", "timestamp_s": 2940.0}, {"text": "them had and so forth. Now you also have", "timestamp": "00:49:03,968", "timestamp_s": 2943.0}, {"text": "a text playground instead of a chat. It\u0027s just like you", "timestamp": "00:49:07,248", "timestamp_s": 2947.0}, {"text": "send one request and you get the response. What I like about this,", "timestamp": "00:49:10,752", "timestamp_s": 2950.0}, {"text": "you go here and you select the model. Let me show you what I like", "timestamp": "00:49:14,756", "timestamp_s": 2954.0}, {"text": "about it. And let\u0027s say write a small poem about", "timestamp": "00:49:17,652", "timestamp_s": 2957.0}, {"text": "New York City, right? Let\u0027s just run this.", "timestamp": "00:49:21,620", "timestamp_s": 2961.0}, {"text": "What I really like about this. So it\u0027s streaming back. But the best", "timestamp": "00:49:24,612", "timestamp_s": 2964.0}, {"text": "thing about it, if I click on the three dots, I can actually see the", "timestamp": "00:49:28,200", "timestamp_s": 2968.0}, {"text": "API request. And this is actually how I", "timestamp": "00:49:31,832", "timestamp_s": 2971.0}, {"text": "would actually call this model through API. Right? In this", "timestamp": "00:49:35,528", "timestamp_s": 2975.0}, {"text": "case it\u0027s using AWS CLI, but you can see the message here and", "timestamp": "00:49:39,612", "timestamp_s": 2979.0}, {"text": "all the formats get properly configured for me.", "timestamp": "00:49:43,228", "timestamp_s": 2983.0}, {"text": "And in a moment I\u0027ll show you some python code on how you can actually", "timestamp": "00:49:46,252", "timestamp_s": 2986.0}, {"text": "do that. Few more things I want to quickly show.", "timestamp": "00:49:50,000", "timestamp_s": 2990.0}, {"text": "If you want to generate images, you can actually generate images with", "timestamp": "00:49:52,944", "timestamp_s": 2992.0}, {"text": "stable diffusion stability, AI and Amazon Titan. So I can say", "timestamp": "00:49:57,710", "timestamp_s": 2997.0}, {"text": "create an image of a cat in the moon.", "timestamp": "00:50:01,748", "timestamp_s": 3001.0}, {"text": "Let\u0027s just ask this for the", "timestamp": "00:50:05,890", "timestamp_s": 3005.0}, {"text": "model. Let\u0027s see what actually outputs for me.", "timestamp": "00:50:09,108", "timestamp_s": 3009.0}, {"text": "And then you could do whatever you want with that", "timestamp": "00:50:12,630", "timestamp_s": 3012.0}, {"text": "image, right? So you can see it\u0027s cat with the moon. There\u0027s very simple", "timestamp": "00:50:16,072", "timestamp_s": 3016.0}, {"text": "image. We can say create picture", "timestamp": "00:50:20,280", "timestamp_s": 3020.0}, {"text": "of a cat that is super realistic.", "timestamp": "00:50:25,950", "timestamp_s": 3025.0}, {"text": "Let\u0027s see if it does more like instead of you saw, there was", "timestamp": "00:50:30,018", "timestamp_s": 3030.0}, {"text": "more like a paint with the moon in the background.", "timestamp": "00:50:33,452", "timestamp_s": 3033.0}, {"text": "Let\u0027s see if this does. And this is what I\u0027m doing here is just prompt", "timestamp": "00:50:36,674", "timestamp_s": 3036.0}, {"text": "engineering. I\u0027m not using anything specific. There you go. You can see an image", "timestamp": "00:50:39,558", "timestamp_s": 3039.0}, {"text": "here that is a more realistic cat image,", "timestamp": "00:50:43,398", "timestamp_s": 3043.0}, {"text": "right? Remember I talked about some of the other features", "timestamp": "00:50:46,806", "timestamp_s": 3046.0}, {"text": "like guardrails. You can see the guardrails here, you can create the", "timestamp": "00:50:50,838", "timestamp_s": 3050.0}, {"text": "guardrails, you can create the knowledge base, you can", "timestamp": "00:50:54,144", "timestamp_s": 3054.0}, {"text": "create all the agents. Those are the things you can do. One of the things", "timestamp": "00:50:57,588", "timestamp_s": 3057.0}, {"text": "I want to highlight, if you were to start using Bedrock for the first time,", "timestamp": "00:51:01,012", "timestamp_s": 3061.0}, {"text": "the first thing I would recommend you doing is actually going model access", "timestamp": "00:51:04,536", "timestamp_s": 3064.0}, {"text": "and enabling those models to have access on your account.", "timestamp": "00:51:08,488", "timestamp_s": 3068.0}, {"text": "You don\u0027t pay anything for just enabling them, but if you", "timestamp": "00:51:13,590", "timestamp_s": 3073.0}, {"text": "don\u0027t enable them, you can\u0027t use, and it\u0027s as simple as just going on manage", "timestamp": "00:51:16,808", "timestamp_s": 3076.0}, {"text": "model access, selecting the models you want. In my account you can see", "timestamp": "00:51:20,460", "timestamp_s": 3080.0}, {"text": "I have access to all those models, right? So it\u0027s pretty straightforward.", "timestamp": "00:51:23,708", "timestamp_s": 3083.0}, {"text": "But now let\u0027s jump in into some code, right?", "timestamp": "00:51:27,530", "timestamp_s": 3087.0}, {"text": "Likely most people that are here watching", "timestamp": "00:51:30,704", "timestamp_s": 3090.0}, {"text": "my session are probably developers or people that do code.", "timestamp": "00:51:34,304", "timestamp_s": 3094.0}, {"text": "So how can I actually call those models on bedrock using", "timestamp": "00:51:38,384", "timestamp_s": 3098.0}, {"text": "a programmatic way. So the first example here I\u0027ll show you is just calling", "timestamp": "00:51:42,750", "timestamp_s": 3102.0}, {"text": "cloud, right? So you can see I import bodo tree which is the", "timestamp": "00:51:46,948", "timestamp_s": 3106.0}, {"text": "AWS ssdk for python. And then I", "timestamp": "00:51:50,788", "timestamp_s": 3110.0}, {"text": "instantiated the bedrock runtime from the SDK. You can", "timestamp": "00:51:54,632", "timestamp_s": 3114.0}, {"text": "see the bedrock runtime in the region. Here is the payload.", "timestamp": "00:51:58,024", "timestamp_s": 3118.0}, {"text": "So I\u0027m providing the model version.", "timestamp": "00:52:01,406", "timestamp_s": 3121.0}, {"text": "So this is quadrisoned then the body.", "timestamp": "00:52:05,582", "timestamp_s": 3125.0}, {"text": "Each model have a specific body and format that the model", "timestamp": "00:52:08,984", "timestamp_s": 3128.0}, {"text": "providers have configured. And you can see that in the documentation.", "timestamp": "00:52:13,020", "timestamp_s": 3133.0}, {"text": "I can show you the link in a moment. But once you have actually this,", "timestamp": "00:52:16,610", "timestamp_s": 3136.0}, {"text": "you create a model. In this case you create a prompt. In this case just", "timestamp": "00:52:22,590", "timestamp_s": 3142.0}, {"text": "saying write a text about going to the moon", "timestamp": "00:52:26,224", "timestamp_s": 3146.0}, {"text": "and its technical challenges. Right? Then I create", "timestamp": "00:52:29,558", "timestamp_s": 3149.0}, {"text": "that payload into JSON and then finally call the single API that", "timestamp": "00:52:33,970", "timestamp_s": 3153.0}, {"text": "I\u0027ve talked to you about, bedrock. So we always call bedrock", "timestamp": "00:52:38,004", "timestamp_s": 3158.0}, {"text": "invoke model. And this is not using streaming. I\u0027ll show any", "timestamp": "00:52:41,946", "timestamp_s": 3161.0}, {"text": "streaming version in a moment. And then I wait for the response.", "timestamp": "00:52:45,752", "timestamp_s": 3165.0}, {"text": "I parse the response into JSON and then I print a response", "timestamp": "00:52:49,870", "timestamp_s": 3169.0}, {"text": "where exactly the text from the", "timestamp": "00:52:53,422", "timestamp_s": 3173.0}, {"text": "response is. So if I go on and run pythoncloud", "timestamp": "00:52:56,968", "timestamp_s": 3176.0}, {"text": "py, I\u0027m going to call behind the scenes that\u0027s actually", "timestamp": "00:53:00,674", "timestamp_s": 3180.0}, {"text": "calling bedrock, sending the payload that I request to claw", "timestamp": "00:53:04,396", "timestamp_s": 3184.0}, {"text": "tree. Running the prompt. Remember my prompt is talked about the", "timestamp": "00:53:08,274", "timestamp_s": 3188.0}, {"text": "technical challenges about going to the moon. I write a text about", "timestamp": "00:53:12,224", "timestamp_s": 3192.0}, {"text": "that. So once he actually returns the text from", "timestamp": "00:53:15,632", "timestamp_s": 3195.0}, {"text": "bedrock then I will be able to just see the text. And there", "timestamp": "00:53:19,550", "timestamp_s": 3199.0}, {"text": "you go. You see it wasn\u0027t streaming. So it\u0027s a pretty long", "timestamp": "00:53:23,264", "timestamp_s": 3203.0}, {"text": "text saying embarking on a journey to the moon. Present multitude of", "timestamp": "00:53:26,436", "timestamp_s": 3206.0}, {"text": "technical challenges. Not going to evaluate this but you get the", "timestamp": "00:53:29,972", "timestamp_s": 3209.0}, {"text": "gist, right. So this is example one. The second example is calling", "timestamp": "00:53:33,588", "timestamp_s": 3213.0}, {"text": "the same bedrock API but for a different model.", "timestamp": "00:53:37,480", "timestamp_s": 3217.0}, {"text": "So you can see here I\u0027m also invocating a bedrock.", "timestamp": "00:53:40,840", "timestamp_s": 3220.0}, {"text": "And then I\u0027m just saying can you write me a poem about apples?", "timestamp": "00:53:45,750", "timestamp_s": 3225.0}, {"text": "Right? So let\u0027s just call this python", "timestamp": "00:53:49,250", "timestamp_s": 3229.0}, {"text": "three titan Ui. So now this is calling the Amazon", "timestamp": "00:53:52,338", "timestamp_s": 3232.0}, {"text": "Titan text model. And you can see it was very simple poem", "timestamp": "00:53:55,842", "timestamp_s": 3235.0}, {"text": "about apple. Now there might be applications that", "timestamp": "00:54:00,082", "timestamp_s": 3240.0}, {"text": "you\u0027re trying to build that require streaming. Like a chatbot, you don\u0027t want to make", "timestamp": "00:54:03,744", "timestamp_s": 3243.0}, {"text": "the user waiting for, I don\u0027t know, like a minute to get", "timestamp": "00:54:07,296", "timestamp_s": 3247.0}, {"text": "a response back. Sometimes those models take a while to finalize", "timestamp": "00:54:11,152", "timestamp_s": 3251.0}, {"text": "the whole text completion so you can do streaming. So in this case,", "timestamp": "00:54:15,066", "timestamp_s": 3255.0}, {"text": "very similar to what I\u0027ve done before. This is a demonstration of", "timestamp": "00:54:18,724", "timestamp_s": 3258.0}, {"text": "using clotry sonnet, but with a streaming right.", "timestamp": "00:54:21,972", "timestamp_s": 3261.0}, {"text": "So I\u0027m not using multimodality yet. This is just text. So I have", "timestamp": "00:54:25,192", "timestamp_s": 3265.0}, {"text": "an input text and you see this just creating", "timestamp": "00:54:29,064", "timestamp_s": 3269.0}, {"text": "the input payload. Then you can see here the API", "timestamp": "00:54:33,590", "timestamp_s": 3273.0}, {"text": "that I call is just a little bit different. It is the invoke model with", "timestamp": "00:54:37,358", "timestamp_s": 3277.0}, {"text": "response streaming. So what bedrock does,", "timestamp": "00:54:40,748", "timestamp_s": 3280.0}, {"text": "as soon as you start receiving some chunks of text from the model,", "timestamp": "00:54:43,932", "timestamp_s": 3283.0}, {"text": "we actually output back for you. And then here it\u0027s just like,", "timestamp": "00:54:47,324", "timestamp_s": 3287.0}, {"text": "as you get the response, just display that for me, just do a print", "timestamp": "00:54:50,556", "timestamp_s": 3290.0}, {"text": "on the console for me. And then on my main here, I\u0027m finally", "timestamp": "00:54:54,262", "timestamp_s": 3294.0}, {"text": "providing some model IDs and providing the prompt. In this case,", "timestamp": "00:54:58,048", "timestamp_s": 3298.0}, {"text": "what can you tell me about the, what can you tell me about", "timestamp": "00:55:01,828", "timestamp_s": 3301.0}, {"text": "brazilian economy? And then I\u0027m", "timestamp": "00:55:06,130", "timestamp_s": 3306.0}, {"text": "just starting the border tree with bedrock and then calling the", "timestamp": "00:55:10,058", "timestamp_s": 3310.0}, {"text": "function that I created above. So if you go here,", "timestamp": "00:55:13,928", "timestamp_s": 3313.0}, {"text": "clean the screen and you go cloud streaming,", "timestamp": "00:55:17,080", "timestamp_s": 3317.0}, {"text": "and we try to run, oh,", "timestamp": "00:55:23,350", "timestamp_s": 3323.0}, {"text": "sorry, python three, apologies for that. Python three cloud", "timestamp": "00:55:26,120", "timestamp_s": 3326.0}, {"text": "streaming. So it\u0027s invoking my row and you can see now it\u0027s streaming the", "timestamp": "00:55:29,848", "timestamp_s": 3329.0}, {"text": "response back and it\u0027s actually getting the response about the brazilian", "timestamp": "00:55:33,788", "timestamp_s": 3333.0}, {"text": "economy. And you can see here it finalized.", "timestamp": "00:55:37,138", "timestamp_s": 3337.0}, {"text": "So I even predicting, like, okay, why it stopped", "timestamp": "00:55:40,034", "timestamp_s": 3340.0}, {"text": "because it was end of the turn. It finalized the response and also how", "timestamp": "00:55:43,318", "timestamp_s": 3343.0}, {"text": "many output tokens you got. So that is just one example. The other example", "timestamp": "00:55:46,688", "timestamp_s": 3346.0}, {"text": "is I want to use clotrisonet because it\u0027s a multimodal", "timestamp": "00:55:50,928", "timestamp_s": 3350.0}, {"text": "large language model that also accepts", "timestamp": "00:55:55,250", "timestamp_s": 3355.0}, {"text": "images as inputs. So what I want to do, I have this image", "timestamp": "00:55:58,410", "timestamp_s": 3358.0}, {"text": "of a very cute cat. I want to provide this image to the model.", "timestamp": "00:56:02,122", "timestamp_s": 3362.0}, {"text": "And you see here what I\u0027m doing. Very similar again,", "timestamp": "00:56:05,752", "timestamp_s": 3365.0}, {"text": "but now what I\u0027m actually doing, I\u0027m receiving the cat image.", "timestamp": "00:56:09,830", "timestamp_s": 3369.0}, {"text": "I\u0027m encoding that on base 64. Then I\u0027m providing", "timestamp": "00:56:13,998", "timestamp_s": 3373.0}, {"text": "on my messages for clot tree as a content. You can see I\u0027m", "timestamp": "00:56:18,142", "timestamp_s": 3378.0}, {"text": "providing now an image and a text. So the first I\u0027m providing the image", "timestamp": "00:56:21,874", "timestamp_s": 3381.0}, {"text": "AWS base 64 mastering. And then I\u0027m saying as the", "timestamp": "00:56:25,922", "timestamp_s": 3385.0}, {"text": "prompt, write me a detailed description of this photo and", "timestamp": "00:56:29,564", "timestamp_s": 3389.0}, {"text": "then upon talking about it. So that\u0027s the request. And if you see down", "timestamp": "00:56:32,768", "timestamp_s": 3392.0}, {"text": "below here, I\u0027m invoking the model. So this is not using", "timestamp": "00:56:36,352", "timestamp_s": 3396.0}, {"text": "streaming. Remember the example before was using streaming. In this case it\u0027s", "timestamp": "00:56:40,000", "timestamp_s": 3400.0}, {"text": "not using streaming. So it\u0027s going to send everything, it\u0027s going to process the", "timestamp": "00:56:44,038", "timestamp_s": 3404.0}, {"text": "response. Once the response is finalized it\u0027s actually going to show me and", "timestamp": "00:56:47,428", "timestamp_s": 3407.0}, {"text": "it\u0027s going to just print the result. Let\u0027s just quickly do", "timestamp": "00:56:51,252", "timestamp_s": 3411.0}, {"text": "this prod multimodality again.", "timestamp": "00:56:54,728", "timestamp_s": 3414.0}, {"text": "I keep using Python two instead of Python three.", "timestamp": "00:56:58,968", "timestamp_s": 3418.0}, {"text": "Apologies for that. Now I\u0027m going to run Python three.", "timestamp": "00:57:02,216", "timestamp_s": 3422.0}, {"text": "Hopefully this is going to start printing", "timestamp": "00:57:06,170", "timestamp_s": 3426.0}, {"text": "the whole description. And you can see here the image show", "timestamp": "00:57:11,074", "timestamp_s": 3431.0}, {"text": "a close up portrait of a cat with striking green eyes and a sweet", "timestamp": "00:57:14,732", "timestamp_s": 3434.0}, {"text": "brownish gray fur coat. The cat says face a", "timestamp": "00:57:18,882", "timestamp_s": 3438.0}, {"text": "slightly stern, yet alert and yet alert and", "timestamp": "00:57:22,272", "timestamp_s": 3442.0}, {"text": "attentive expression. So it talks about the cat in the image. It\u0027s very accurate.", "timestamp": "00:57:26,032", "timestamp_s": 3446.0}, {"text": "And then like I said, he writes a poem, emerald depth gaze.", "timestamp": "00:57:30,006", "timestamp_s": 3450.0}, {"text": "So king blah blah blah blah blah, he talks about it. So you can see", "timestamp": "00:57:33,478", "timestamp_s": 3453.0}, {"text": "bedrock is amazing because with very simple API", "timestamp": "00:57:36,980", "timestamp_s": 3456.0}, {"text": "calls, I can call different models with different configurations with different", "timestamp": "00:57:40,282", "timestamp_s": 3460.0}, {"text": "I parameters. And this is pay as you go. All what I\u0027ve done", "timestamp": "00:57:43,892", "timestamp_s": 3463.0}, {"text": "here is probably less than a penny because it\u0027s all", "timestamp": "00:57:47,352", "timestamp_s": 3467.0}, {"text": "on demand. I\u0027m not paying for any provisioned capacity because I don\u0027t need", "timestamp": "00:57:50,488", "timestamp_s": 3470.0}, {"text": "in this example. Last thing is I\u0027ll recommend if you want", "timestamp": "00:57:54,296", "timestamp_s": 3474.0}, {"text": "to look for some of the code that I\u0027ve used. I based myself", "timestamp": "00:57:57,708", "timestamp_s": 3477.0}, {"text": "on this GitHub public repository called Amazon", "timestamp": "00:58:01,740", "timestamp_s": 3481.0}, {"text": "bedrock samples. You can go here introduction to bedrock and", "timestamp": "00:58:05,874", "timestamp_s": 3485.0}, {"text": "you can see some examples. For example cloud tree. You can see the", "timestamp": "00:58:09,708", "timestamp_s": 3489.0}, {"text": "example for cloud know with the image. This is the one that I\u0027ve used", "timestamp": "00:58:13,008", "timestamp_s": 3493.0}, {"text": "so highly recommend you get in there. And last, if you really want to", "timestamp": "00:58:17,008", "timestamp_s": 3497.0}, {"text": "look in more in detail into each model and the hyperparameters on", "timestamp": "00:58:20,624", "timestamp_s": 3500.0}, {"text": "how you call, you can go on the AWS bedrock documentation.", "timestamp": "00:58:23,908", "timestamp_s": 3503.0}, {"text": "Within the foundational model submenu we have the model inference", "timestamp": "00:58:27,738", "timestamp_s": 3507.0}, {"text": "parameters and when you click on specific models,", "timestamp": "00:58:31,722", "timestamp_s": 3511.0}, {"text": "for example cloud, you can see the different cloud completion and", "timestamp": "00:58:34,938", "timestamp_s": 3514.0}, {"text": "cloud messages API. On the messages you can see here, you can see", "timestamp": "00:58:38,548", "timestamp_s": 3518.0}, {"text": "some code examples. So you have a very descriptive documentation for", "timestamp": "00:58:42,372", "timestamp_s": 3522.0}, {"text": "you as a developer to actually take a look and deep dive.", "timestamp": "00:58:46,156", "timestamp_s": 3526.0}, {"text": "So that is all I had to show for today. Hopefully it was very", "timestamp": "00:58:49,986", "timestamp_s": 3529.0}, {"text": "useful. Feel free to connect with me via LinkedIn on Twitter if you have any", "timestamp": "00:58:53,356", "timestamp_s": 3533.0}, {"text": "questions. And happy coding, happy genai applications", "timestamp": "00:58:56,988", "timestamp_s": 3536.0}, {"text": "and I hope you find this useful. Thank you so much.", "timestamp": "00:59:00,786", "timestamp_s": 3540.0}];
              

              var tag = document.createElement('script');

              tag.src = "https://www.youtube.com/iframe_api";
              var firstScriptTag = document.getElementsByTagName('script')[0];
              firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

              // 3. This function creates an <iframe> (and YouTube player)
              //    after the API code downloads.
              var player;
              function onYouTubeIframeAPIReady() {
                player = new YT.Player('player', {
                  height: '100%',
                  width: '100%',
                  videoId: '2awCEVyi_Lc',
                  playerVars: {
                    'playsinline': 1
                  },
                  events: {
                    'onReady': onPlayerReady,
                    // 'onStateChange': onPlayerStateChange
                  }
                });
              }
              function onPlayerReady(event) {
                console.log("Player ready");
                var sec = Number(location.href.split("#")[1]);
                if (sec){
                  player.seekTo(sec, true);
                }
                player.playVideo();
                highlightParagraph();
              }
              // find the number of the paragraph
              function findParagraph(sec){
                for (var i = 1; i < transcript.length; i++) {
                  if (transcript[i].timestamp_s > sec){
                    return i - 1;
                  }
                }
                return transcript.length - 1;
              }
              // move the video to the desired second
              function seek(sec){
                if(player){
                  player.playVideo();
                  player.seekTo(sec, true);
                }
                location.href = location.href.split("#")[0] + "#" + sec;
                highlightParagraph(sec);
              }
              // highlight the right paragraph
              var prevParagraph;
              function highlightParagraph(sec) {
                var currentTime = sec;
                if (!currentTime && player) {
                  currentTime = player.getCurrentTime();
                }
                if (!currentTime){
                  console.log("No current time")
                  return;
                }
                var currentParagraph = findParagraph(currentTime);
                if (currentParagraph !== prevParagraph){
                  prevParagraph = currentParagraph;
                  Array.from(document.getElementsByClassName("transcript-chunks")).forEach((e) => {
                    e.classList.remove('text-selected');
                  });
                  var body = document.getElementById("chunk-"+currentParagraph);
                  body.classList.add('text-selected');
                }
              }
              time_update_interval = setInterval(highlightParagraph, 1000);
            </script>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>
    

    <!-- CONTENT -->
    <section class="pt-2">
      <div class="container">
        <div class="row justify-content-center">

          <div class="col-12 mb-5">
            <h1>
              Mastering Generative AI: Harnessing AWS GenAI for Your Solutions
            </h1>
            
            <h3 class="bg-white">
              Video size:
              <a href="javascript:void(0);" onclick="resizeVideo(25)"><i class="fe fe-zoom-out me-2"></i></a>
              <a href="javascript:void(0);" onclick="resizeVideo(50)"><i class="fe fe-zoom-in me-2"></i></a>
            </h3>
            
          </div>

          <div class="col-12 mb-5">
            <h3>
              Abstract
            </h3>
<!-- Text -->
<p>Unlock the power of Generative AI with AWS GenAI! Dive into Amazon Bedrock, Trainium, Sagemaker, and more. Discover how to build, scale, and optimize your AI applications efficiently, cost-effectively, and at scale. Don&rsquo;t just keep up with the AI revolution, lead it!</p>
<!-- End Text -->
          </div>

          
          

          <div class="col-12 mb-5">
            <h3>
              Summary
            </h3>
            <ul>
              
              <li>
                Samuel Baruffi will present a session called Mastering Generative AI, harnessing AWS Genai for your solutions. He will talk about the solutions within the AWS platform that allows you to run, create and operate vector databases. And he always like to end the session with a demo.

              </li>
              
              <li>
                In order for this generative AI explosion and evolution, what is actually powering them behind the scenes is what we call foundational model. The good thing about those models is they are generic and general on their own. And we are going to talk about how AWS will help you choose different models for your use case.

              </li>
              
              <li>
                Another thing that a lot of companies are doing now is customizing foundational models for their own data. There are different strategies. You can use retrieval, argument generation, or you can use agents to actually retrieve the data. On top of having foundational models, you can also customize those to even be more expert on the data that you have control of.

              </li>
              
              <li>
                Companies are building use cases to enhance customer experience, boost employee productivity, or optimize business processes. In order to run those models, those models require a lot of computational, specifically parallel computational. AWS is also in the front of innovating our own chipsets.

              </li>
              
              <li>
                Amazon Sagemaker is the AWS machine learning AI platform. Once you run those models, you can actually deploy those machine learning models at scale. Sagemaker Jumpstart also has the capability for fine tuning. It's kind of pay as you go, but for the whole instance that it requires.

              </li>
              
              <li>
                Most customers are nodding to the exercise of training large language models or fine tuning. They really have use cases that they want to boost customer experience or increase employee productivity. How can they keep the data that they're going to be running through those models? Secure and private, right.

              </li>
              
              <li>
                AWS have introduced last year a service called Amazon Bedrock. It is the easiest way to build and scale generative AI applications with foundational models. There are different pricing modes on bedrock, but you start with which is the most.

              </li>
              
              <li>
                We currently have seven model providers available for you on Bedrock. The very important thing to keep in mind with bedrock is we are democratizing the ability for people to consume different models for a specific use case. And that is really important because you can decide how you want to build your applications.

              </li>
              
              <li>
                Entropic is one of the top leading research AI companies in the world. They have shocked the industry with very performant and set of models of three different models called cloud tree. Claw three models are already available on bedrock as we speak right now.

              </li>
              
              <li>
                Bedrock has a feature called knowledge base that makes all this process of running retrieval augmented generation very simple. Another functionality is the ability to enable generative AI applications to execute steps outside your model.

              </li>
              
              <li>
                On bedrock we have a functionality that is currently in preview, but it's called guardrails. It allows you to create consistently safeguards, including on your models. You can create filters for harmful content both on the input that you're sending to bedrock and also the output that bedrock will tell you.

              </li>
              
              <li>
                Bedrock's batch mode allows you to efficiently run inference on large volumes of data. One last nice feature about Bedrock is model evaluation. Thousands of customers are using bedrock to build generative AI on top of pretty much every single industry.

              </li>
              
              <li>
                AWS has a wide variety of different databases that support vectors. Depending on the use case, you might choose one versus the other. Very soon Aurora, Amazon, Aurora and MongoDB are going to be made available as well.

              </li>
              
              <li>
                AWS has a service called code Whisper which is AI powered code suggestion. You receive real time code suggestions for a variety of programming languages. Some features are only for enterprise, but most features are available for free.

              </li>
              
              <li>
                You can actually get started and play around and test some of the models by just going to the playground. On the chat AWS, well, we can compare models. If you want to generate images, you can actually generate images quickly. Few more things I want to show you.

              </li>
              
              <li>
                How can I call those models on bedrock using a programmatic way. Each model have a specific body and format that the model providers have configured. And then I wait for the response. I parse the response into JSON and then I print a response. Now there might be applications that you're trying to build that require streaming.

              </li>
              
              <li>
                 AWS bedrock is amazing because with very simple API calls, I can call different models with different configurations with different I parameters. All what I've done here is probably less than a penny because it's all on demand. Last thing is I'll recommend if you want to look for some of the code that I've used.
              </li>
              
            </ul>
          </div>

          <div class="col-12 mb-5">
            <h3>
              Transcript
            </h3>
            <span class="text-muted">
              This transcript was autogenerated. To make changes, <a href="https://github.com/conf42/src/edit/main/./assemblyai/2awCEVyi_Lc.srt" target="_blank">submit a PR</a>.
            </span>
            <div>
            
            <span id="chunk-0" class="transcript-chunks" onclick="console.log('00:00:24,650'); seek(24.0)">
              Hello everyone. Welcome to my session. My name is Samuel Baruffi
            </span>
            
            <span id="chunk-1" class="transcript-chunks" onclick="console.log('00:00:28,754'); seek(28.0)">
              and today I'm here to present a session called
            </span>
            
            <span id="chunk-2" class="transcript-chunks" onclick="console.log('00:00:32,748'); seek(32.0)">
              Mastering Generative AI, harnessing AWS Genai
            </span>
            
            <span id="chunk-3" class="transcript-chunks" onclick="console.log('00:00:37,026'); seek(37.0)">
              for your solutions. Let's look at a quick agenda.
            </span>
            
            <span id="chunk-4" class="transcript-chunks" onclick="console.log('00:00:40,690'); seek(40.0)">
              What I'm going to be covering on my session, I'm going to start
            </span>
            
            <span id="chunk-5" class="transcript-chunks" onclick="console.log('00:00:44,412'); seek(44.0)">
              a presentation talking at a very high level about generative AI
            </span>
            
            <span id="chunk-6" class="transcript-chunks" onclick="console.log('00:00:48,658'); seek(48.0)">
              and the big impact in the world and also into
            </span>
            
            <span id="chunk-7" class="transcript-chunks" onclick="console.log('00:00:51,972'); seek(51.0)">
              applications being built today. Then I'm going to
            </span>
            
            <span id="chunk-8" class="transcript-chunks" onclick="console.log('00:00:55,508'); seek(55.0)">
              talk about at the very infrastructure level, what AWS is
            </span>
            
            <span id="chunk-9" class="transcript-chunks" onclick="console.log('00:00:59,572'); seek(59.0)">
              doing with our own chipsets called inferential
            </span>
            
            <span id="chunk-10" class="transcript-chunks" onclick="console.log('00:01:03,306'); seek(63.0)">
              and trainium, and also talk about a wide variety
            </span>
            
            <span id="chunk-11" class="transcript-chunks" onclick="console.log('00:01:07,166'); seek(67.0)">
              of EC two instances with Nvidia graphic
            </span>
            
            <span id="chunk-12" class="transcript-chunks" onclick="console.log('00:01:10,958'); seek(70.0)">
              cards. After that I'm going to start talking about more
            </span>
            
            <span id="chunk-13" class="transcript-chunks" onclick="console.log('00:01:14,248'); seek(74.0)">
              on the application services and the platforms that you can build models
            </span>
            
            <span id="chunk-14" class="transcript-chunks" onclick="console.log('00:01:18,482'); seek(78.0)">
              and use models. So I'm going to talk about Amazon Sagemaker,
            </span>
            
            <span id="chunk-15" class="transcript-chunks" onclick="console.log('00:01:22,290'); seek(82.0)">
              I'm going to talk about Amazon Sagemaker Jumpstart and
            </span>
            
            <span id="chunk-16" class="transcript-chunks" onclick="console.log('00:01:25,548'); seek(85.0)">
              then I'm going to spend the majority of the time talking about
            </span>
            
            <span id="chunk-17" class="transcript-chunks" onclick="console.log('00:01:29,056'); seek(89.0)">
              Bedrock. Bedrock is our foundational models
            </span>
            
            <span id="chunk-18" class="transcript-chunks" onclick="console.log('00:01:33,206'); seek(93.0)">
              as a service is the ability for users and companies
            </span>
            
            <span id="chunk-19" class="transcript-chunks" onclick="console.log('00:01:36,784'); seek(96.0)">
              and organizations to call a single API,
            </span>
            
            <span id="chunk-20" class="transcript-chunks" onclick="console.log('00:01:39,942'); seek(99.0)">
              choose different models from large language models
            </span>
            
            <span id="chunk-21" class="transcript-chunks" onclick="console.log('00:01:43,402'); seek(103.0)">
              to text generations from embedding models, and easily receive
            </span>
            
            <span id="chunk-22" class="transcript-chunks" onclick="console.log('00:01:47,978'); seek(107.0)">
              the response and actually build solutions on top of that.
            </span>
            
            <span id="chunk-23" class="transcript-chunks" onclick="console.log('00:01:51,492'); seek(111.0)">
              Bedrock is a very exciting service that has a lot of features
            </span>
            
            <span id="chunk-24" class="transcript-chunks" onclick="console.log('00:01:56,014'); seek(116.0)">
              baked in and being shipped as we speak.
            </span>
            
            <span id="chunk-25" class="transcript-chunks" onclick="console.log('00:01:58,904'); seek(118.0)">
              And I'm going to cover some of those features as well.
            </span>
            
            <span id="chunk-26" class="transcript-chunks" onclick="console.log('00:02:02,088'); seek(122.0)">
              Then I'm going to jump into a single slide that talks
            </span>
            
            <span id="chunk-27" class="transcript-chunks" onclick="console.log('00:02:05,918'); seek(125.0)">
              about vector databases. I'm going to talk what is a vector database,
            </span>
            
            <span id="chunk-28" class="transcript-chunks" onclick="console.log('00:02:09,762'); seek(129.0)">
              why it's important for generative AI solutions. And of course I'm going to be
            </span>
            
            <span id="chunk-29" class="transcript-chunks" onclick="console.log('00:02:13,468'); seek(133.0)">
              talking about the solutions within the AWS platform that allows
            </span>
            
            <span id="chunk-30" class="transcript-chunks" onclick="console.log('00:02:17,378'); seek(137.0)">
              you to run, create and operate vector databases.
            </span>
            
            <span id="chunk-31" class="transcript-chunks" onclick="console.log('00:02:22,270'); seek(142.0)">
              Then to finalize the presentation piece of
            </span>
            
            <span id="chunk-32" class="transcript-chunks" onclick="console.log('00:02:25,872'); seek(145.0)">
              my session, I'm going to quickly talk about code
            </span>
            
            <span id="chunk-33" class="transcript-chunks" onclick="console.log('00:02:29,696'); seek(149.0)">
              Whisper, which is a very exciting tool for developers that
            </span>
            
            <span id="chunk-34" class="transcript-chunks" onclick="console.log('00:02:33,476'); seek(153.0)">
              can actually have a companion helping with
            </span>
            
            <span id="chunk-35" class="transcript-chunks" onclick="console.log('00:02:38,050'); seek(158.0)">
              generation of code on many different types of programming
            </span>
            
            <span id="chunk-36" class="transcript-chunks" onclick="console.log('00:02:41,578'); seek(161.0)">
              languages and also infrastructure as a code.
            </span>
            
            <span id="chunk-37" class="transcript-chunks" onclick="console.log('00:02:44,980'); seek(164.0)">
              And then I've done quite a few talks
            </span>
            
            <span id="chunk-38" class="transcript-chunks" onclick="console.log('00:02:49,066'); seek(169.0)">
              with comfort e two in the past. I always like to end the session with
            </span>
            
            <span id="chunk-39" class="transcript-chunks" onclick="console.log('00:02:52,968'); seek(172.0)">
              a demo. So I'm going to do a demo potentially using bedrock
            </span>
            
            <span id="chunk-40" class="transcript-chunks" onclick="console.log('00:02:56,686'); seek(176.0)">
              and showing you how easy it is to use bedrock, some of the functionality
            </span>
            
            <span id="chunk-41" class="transcript-chunks" onclick="console.log('00:03:00,722'); seek(180.0)">
              of bedrock, and hopefully showing you with a simple piece of python
            </span>
            
            <span id="chunk-42" class="transcript-chunks" onclick="console.log('00:03:04,482'); seek(184.0)">
              code how easily you can call bedrock using
            </span>
            
            <span id="chunk-43" class="transcript-chunks" onclick="console.log('00:03:07,776'); seek(187.0)">
              AWS SDK and receive a response from
            </span>
            
            <span id="chunk-44" class="transcript-chunks" onclick="console.log('00:03:11,712'); seek(191.0)">
              a specific large language model that you're going to choose.
            </span>
            
            <span id="chunk-45" class="transcript-chunks" onclick="console.log('00:03:14,990'); seek(194.0)">
              So let's just get started.
            </span>
            
            <span id="chunk-46" class="transcript-chunks" onclick="console.log('00:03:18,110'); seek(198.0)">
              So what is generative AI? Right and how
            </span>
            
            <span id="chunk-47" class="transcript-chunks" onclick="console.log('00:03:21,652'); seek(201.0)">
              are those generative AI powered? The most important thing to
            </span>
            
            <span id="chunk-48" class="transcript-chunks" onclick="console.log('00:03:25,668'); seek(205.0)">
              be familiar with is, in order for this
            </span>
            
            <span id="chunk-49" class="transcript-chunks" onclick="console.log('00:03:29,652'); seek(209.0)">
              generative AI explosion
            </span>
            
            <span id="chunk-50" class="transcript-chunks" onclick="console.log('00:03:33,198'); seek(213.0)">
              and evolution, what is actually powering them behind the scenes
            </span>
            
            <span id="chunk-51" class="transcript-chunks" onclick="console.log('00:03:36,862'); seek(216.0)">
              is what we call foundational model. Foundational model.
            </span>
            
            <span id="chunk-52" class="transcript-chunks" onclick="console.log('00:03:40,648'); seek(220.0)">
              You can think about an AI model that
            </span>
            
            <span id="chunk-53" class="transcript-chunks" onclick="console.log('00:03:44,168'); seek(224.0)">
              has been actually pretrained on vast amounts of instruction
            </span>
            
            <span id="chunk-54" class="transcript-chunks" onclick="console.log('00:03:48,078'); seek(228.0)">
              data. Those state of art models that are
            </span>
            
            <span id="chunk-55" class="transcript-chunks" onclick="console.log('00:03:51,160'); seek(231.0)">
              going to be talking later on in my presentation are known to be trained
            </span>
            
            <span id="chunk-56" class="transcript-chunks" onclick="console.log('00:03:54,802'); seek(234.0)">
              across pretty much the whole Internet, right? Like, if you think about
            </span>
            
            <span id="chunk-57" class="transcript-chunks" onclick="console.log('00:03:58,128'); seek(238.0)">
              it, like, all the amount of data
            </span>
            
            <span id="chunk-58" class="transcript-chunks" onclick="console.log('00:04:01,456'); seek(241.0)">
              that the Internet has, that is actually all the data that those
            </span>
            
            <span id="chunk-59" class="transcript-chunks" onclick="console.log('00:04:04,992'); seek(244.0)">
              models are being trained, then it
            </span>
            
            <span id="chunk-60" class="transcript-chunks" onclick="console.log('00:04:08,128'); seek(248.0)">
              contains a large amount of parameters that
            </span>
            
            <span id="chunk-61" class="transcript-chunks" onclick="console.log('00:04:11,956'); seek(251.0)">
              makes them capable. By learning all this data, which are the
            </span>
            
            <span id="chunk-62" class="transcript-chunks" onclick="console.log('00:04:15,588'); seek(255.0)">
              parameters, they make their own word interpretations,
            </span>
            
            <span id="chunk-63" class="transcript-chunks" onclick="console.log('00:04:20,170'); seek(260.0)">
              and they actually can learn very complex concepts by
            </span>
            
            <span id="chunk-64" class="transcript-chunks" onclick="console.log('00:04:23,672'); seek(263.0)">
              that. And there is a technology that was
            </span>
            
            <span id="chunk-65" class="transcript-chunks" onclick="console.log('00:04:28,710'); seek(268.0)">
              invented, I think, 2017, by a group of researchers
            </span>
            
            <span id="chunk-66" class="transcript-chunks" onclick="console.log('00:04:32,462'); seek(272.0)">
              from the University of Toronto that
            </span>
            
            <span id="chunk-67" class="transcript-chunks" onclick="console.log('00:04:36,028'); seek(276.0)">
              is called transformers, and that is the architecture using neural networks
            </span>
            
            <span id="chunk-68" class="transcript-chunks" onclick="console.log('00:04:40,562'); seek(280.0)">
              that allows generative AI and large language models to
            </span>
            
            <span id="chunk-69" class="transcript-chunks" onclick="console.log('00:04:44,636'); seek(284.0)">
              actually generate text. The good thing about those models,
            </span>
            
            <span id="chunk-70" class="transcript-chunks" onclick="console.log('00:04:48,854'); seek(288.0)">
              those large language models, is they are generic
            </span>
            
            <span id="chunk-71" class="transcript-chunks" onclick="console.log('00:04:52,662'); seek(292.0)">
              and general on their own. So it's
            </span>
            
            <span id="chunk-72" class="transcript-chunks" onclick="console.log('00:04:56,118'); seek(296.0)">
              not only trained for a specific use case,
            </span>
            
            <span id="chunk-73" class="transcript-chunks" onclick="console.log('00:04:59,550'); seek(299.0)">
              it can actually solve a lot of different use
            </span>
            
            <span id="chunk-74" class="transcript-chunks" onclick="console.log('00:05:03,392'); seek(303.0)">
              cases. And I'm going to talk in a moment some of the use cases that
            </span>
            
            <span id="chunk-75" class="transcript-chunks" onclick="console.log('00:05:06,052'); seek(306.0)">
              companies are starting to build. And these number of use cases
            </span>
            
            <span id="chunk-76" class="transcript-chunks" onclick="console.log('00:05:10,122'); seek(310.0)">
              are pretty much infinite because you can think about those
            </span>
            
            <span id="chunk-77" class="transcript-chunks" onclick="console.log('00:05:13,432'); seek(313.0)">
              large language models as a model that can answer any type
            </span>
            
            <span id="chunk-78" class="transcript-chunks" onclick="console.log('00:05:17,352'); seek(317.0)">
              of question. And of course, we need to evaluate
            </span>
            
            <span id="chunk-79" class="transcript-chunks" onclick="console.log('00:05:20,846'); seek(320.0)">
              performance and how big those
            </span>
            
            <span id="chunk-80" class="transcript-chunks" onclick="console.log('00:05:24,392'); seek(324.0)">
              models are and how smart they are.
            </span>
            
            <span id="chunk-81" class="transcript-chunks" onclick="console.log('00:05:27,308'); seek(327.0)">
              Not every model is exactly the same as others.
            </span>
            
            <span id="chunk-82" class="transcript-chunks" onclick="console.log('00:05:30,524'); seek(330.0)">
              So there is that. And we are going to talk about
            </span>
            
            <span id="chunk-83" class="transcript-chunks" onclick="console.log('00:05:34,044'); seek(334.0)">
              how AWS will help you choose
            </span>
            
            <span id="chunk-84" class="transcript-chunks" onclick="console.log('00:05:37,900'); seek(337.0)">
              different models for your use case.
            </span>
            
            <span id="chunk-85" class="transcript-chunks" onclick="console.log('00:05:41,390'); seek(341.0)">
              Another thing that a lot of companies are doing now is customizing
            </span>
            
            <span id="chunk-86" class="transcript-chunks" onclick="console.log('00:05:44,982'); seek(344.0)">
              foundational models for their own data. So let's say you
            </span>
            
            <span id="chunk-87" class="transcript-chunks" onclick="console.log('00:05:48,608'); seek(348.0)">
              are a financial company and you really want to have a specific model
            </span>
            
            <span id="chunk-88" class="transcript-chunks" onclick="console.log('00:05:52,948'); seek(352.0)">
              super well trainium, to know every single piece
            </span>
            
            <span id="chunk-89" class="transcript-chunks" onclick="console.log('00:05:56,212'); seek(356.0)">
              of information that your company has. There are different strategies.
            </span>
            
            <span id="chunk-90" class="transcript-chunks" onclick="console.log('00:06:00,690'); seek(360.0)">
              You can use retrieval, argument generation,
            </span>
            
            <span id="chunk-91" class="transcript-chunks" onclick="console.log('00:06:03,810'); seek(363.0)">
              or you can use agents to actually retrieve the data,
            </span>
            
            <span id="chunk-92" class="transcript-chunks" onclick="console.log('00:06:07,208'); seek(367.0)">
              or you can actually customize foundational models. Customizing foundational models
            </span>
            
            <span id="chunk-93" class="transcript-chunks" onclick="console.log('00:06:11,214'); seek(371.0)">
              are also known as fine tuning. That is also a capability that
            </span>
            
            <span id="chunk-94" class="transcript-chunks" onclick="console.log('00:06:14,648'); seek(374.0)">
              you retrain the model to recalculate
            </span>
            
            <span id="chunk-95" class="transcript-chunks" onclick="console.log('00:06:18,306'); seek(378.0)">
              the weights of those models, to add
            </span>
            
            <span id="chunk-96" class="transcript-chunks" onclick="console.log('00:06:22,092'); seek(382.0)">
              your data, that potentially your data was private, it was not
            </span>
            
            <span id="chunk-97" class="transcript-chunks" onclick="console.log('00:06:25,628'); seek(385.0)">
              available on the Internet. And now we want to actually have those models
            </span>
            
            <span id="chunk-98" class="transcript-chunks" onclick="console.log('00:06:29,602'); seek(389.0)">
              also expert on your own data. We are not
            </span>
            
            <span id="chunk-99" class="transcript-chunks" onclick="console.log('00:06:32,928'); seek(392.0)">
              going to talk in depth about those strategies,
            </span>
            
            <span id="chunk-100" class="transcript-chunks" onclick="console.log('00:06:36,182'); seek(396.0)">
              but that's something that you just need to be aware that on top of having
            </span>
            
            <span id="chunk-101" class="transcript-chunks" onclick="console.log('00:06:39,536'); seek(399.0)">
              foundational models, you can actually customize those to
            </span>
            
            <span id="chunk-102" class="transcript-chunks" onclick="console.log('00:06:42,692'); seek(402.0)">
              even be more expert on the data that you as
            </span>
            
            <span id="chunk-103" class="transcript-chunks" onclick="console.log('00:06:46,692'); seek(406.0)">
              a person or potentially organization have
            </span>
            
            <span id="chunk-104" class="transcript-chunks" onclick="console.log('00:06:49,988'); seek(409.0)">
              control of. So some of the use cases
            </span>
            
            <span id="chunk-105" class="transcript-chunks" onclick="console.log('00:06:53,242'); seek(413.0)">
              that we've seen the industry actually building.
            </span>
            
            <span id="chunk-106" class="transcript-chunks" onclick="console.log('00:06:56,472'); seek(416.0)">
              So there are kind of three main categories, and this is just a very small
            </span>
            
            <span id="chunk-107" class="transcript-chunks" onclick="console.log('00:07:00,168'); seek(420.0)">
              amount of use cases that companies are building. But the first category is
            </span>
            
            <span id="chunk-108" class="transcript-chunks" onclick="console.log('00:07:04,568'); seek(424.0)">
              you want to enhance customer experience. So using
            </span>
            
            <span id="chunk-109" class="transcript-chunks" onclick="console.log('00:07:08,236'); seek(428.0)">
              chat bots, virtual assistants,
            </span>
            
            <span id="chunk-110" class="transcript-chunks" onclick="console.log('00:07:11,210'); seek(431.0)">
              conversational analytics, you can personalize a specific
            </span>
            
            <span id="chunk-111" class="transcript-chunks" onclick="console.log('00:07:14,972'); seek(434.0)">
              user interaction, because those models are really good at
            </span>
            
            <span id="chunk-112" class="transcript-chunks" onclick="console.log('00:07:18,652'); seek(438.0)">
              behaving and answering like humans, even though they are not
            </span>
            
            <span id="chunk-113" class="transcript-chunks" onclick="console.log('00:07:22,064'); seek(442.0)">
              humans, but they answer like a human. It makes really
            </span>
            
            <span id="chunk-114" class="transcript-chunks" onclick="console.log('00:07:25,472'); seek(445.0)">
              good to actually enhance customer experience, right.
            </span>
            
            <span id="chunk-115" class="transcript-chunks" onclick="console.log('00:07:28,848'); seek(448.0)">
              Especially with chatbots. The other section is you can
            </span>
            
            <span id="chunk-116" class="transcript-chunks" onclick="console.log('00:07:32,452'); seek(452.0)">
              boost employee productivity and creativity. So you
            </span>
            
            <span id="chunk-117" class="transcript-chunks" onclick="console.log('00:07:35,748'); seek(455.0)">
              can think about conversational search. If an employee or
            </span>
            
            <span id="chunk-118" class="transcript-chunks" onclick="console.log('00:07:39,076'); seek(459.0)">
              a customer wants to really retrieve an information from
            </span>
            
            <span id="chunk-119" class="transcript-chunks" onclick="console.log('00:07:42,676'); seek(462.0)">
              a wide variety of data set, instead of
            </span>
            
            <span id="chunk-120" class="transcript-chunks" onclick="console.log('00:07:46,568'); seek(466.0)">
              just doing a quick, simple search, you can actually have
            </span>
            
            <span id="chunk-121" class="transcript-chunks" onclick="console.log('00:07:50,616'); seek(470.0)">
              conversational with those documents. You can do summarization,
            </span>
            
            <span id="chunk-122" class="transcript-chunks" onclick="console.log('00:07:54,398'); seek(474.0)">
              you can do content creation, you can do code generation like we are
            </span>
            
            <span id="chunk-123" class="transcript-chunks" onclick="console.log('00:07:58,184'); seek(478.0)">
              going to be talking about code Whisper. And the other category is
            </span>
            
            <span id="chunk-124" class="transcript-chunks" onclick="console.log('00:08:01,612'); seek(481.0)">
              you can also optimize business processes. So let's say you
            </span>
            
            <span id="chunk-125" class="transcript-chunks" onclick="console.log('00:08:04,828'); seek(484.0)">
              want to do a lot of document processing, you want to do data documentation.
            </span>
            
            <span id="chunk-126" class="transcript-chunks" onclick="console.log('00:08:08,322'); seek(488.0)">
              Let's say you have a specific form that needs to be filled.
            </span>
            
            <span id="chunk-127" class="transcript-chunks" onclick="console.log('00:08:11,890'); seek(491.0)">
              Given a specific data, you can join those two
            </span>
            
            <span id="chunk-128" class="transcript-chunks" onclick="console.log('00:08:16,270'); seek(496.0)">
              sets of data and ask the model to potentially feel in
            </span>
            
            <span id="chunk-129" class="transcript-chunks" onclick="console.log('00:08:19,888'); seek(499.0)">
              a specific way, right? So there is a lot of use cases
            </span>
            
            <span id="chunk-130" class="transcript-chunks" onclick="console.log('00:08:23,706'); seek(503.0)">
              that companies are building or have built already to
            </span>
            
            <span id="chunk-131" class="transcript-chunks" onclick="console.log('00:08:28,930'); seek(508.0)">
              enhance customer experience, boost employee productivity,
            </span>
            
            <span id="chunk-132" class="transcript-chunks" onclick="console.log('00:08:32,826'); seek(512.0)">
              or optimize business processes. So that is just something you need to keep in
            </span>
            
            <span id="chunk-133" class="transcript-chunks" onclick="console.log('00:08:36,824'); seek(516.0)">
              mind. Now, in order to run those
            </span>
            
            <span id="chunk-134" class="transcript-chunks" onclick="console.log('00:08:40,232'); seek(520.0)">
              models, those models require a lot of computational,
            </span>
            
            <span id="chunk-135" class="transcript-chunks" onclick="console.log('00:08:44,430'); seek(524.0)">
              specifically parallel computational. One good thing about
            </span>
            
            <span id="chunk-136" class="transcript-chunks" onclick="console.log('00:08:48,652'); seek(528.0)">
              GPUs, graphic process units are that they are
            </span>
            
            <span id="chunk-137" class="transcript-chunks" onclick="console.log('00:08:53,196'); seek(533.0)">
              inherent good at calculations that can be run
            </span>
            
            <span id="chunk-138" class="transcript-chunks" onclick="console.log('00:08:56,588'); seek(536.0)">
              in parallel. So the reason why I'm saying that
            </span>
            
            <span id="chunk-139" class="transcript-chunks" onclick="console.log('00:08:59,936'); seek(539.0)">
              it's really important for us to understand how AWs
            </span>
            
            <span id="chunk-140" class="transcript-chunks" onclick="console.log('00:09:03,494'); seek(543.0)">
              have a wide variety of selections of different instances.
            </span>
            
            <span id="chunk-141" class="transcript-chunks" onclick="console.log('00:09:07,766'); seek(547.0)">
              You can choose to run those models. So AWS have,
            </span>
            
            <span id="chunk-142" class="transcript-chunks" onclick="console.log('00:09:11,920'); seek(551.0)">
              since 2010, a very good partnership with Nvidia.
            </span>
            
            <span id="chunk-143" class="transcript-chunks" onclick="console.log('00:09:15,466'); seek(555.0)">
              Nvidia have been the leader of having very powerful
            </span>
            
            <span id="chunk-144" class="transcript-chunks" onclick="console.log('00:09:19,434'); seek(559.0)">
              GPUs. And those GPUs are really good for large language models.
            </span>
            
            <span id="chunk-145" class="transcript-chunks" onclick="console.log('00:09:23,130'); seek(563.0)">
              You can see here, there are widely variety of instances that offer
            </span>
            
            <span id="chunk-146" class="transcript-chunks" onclick="console.log('00:09:26,824'); seek(566.0)">
              specific instances, specific GPUs from Nvidia.
            </span>
            
            <span id="chunk-147" class="transcript-chunks" onclick="console.log('00:09:31,038'); seek(571.0)">
              The most powerful one is the P five EC two instance that
            </span>
            
            <span id="chunk-148" class="transcript-chunks" onclick="console.log('00:09:34,712'); seek(574.0)">
              comes with the Nvidia age 100 pencil car.
            </span>
            
            <span id="chunk-149" class="transcript-chunks" onclick="console.log('00:09:38,236'); seek(578.0)">
              Those are really big GPUs that can
            </span>
            
            <span id="chunk-150" class="transcript-chunks" onclick="console.log('00:09:41,548'); seek(581.0)">
              run very big, large language models, and they have been used
            </span>
            
            <span id="chunk-151" class="transcript-chunks" onclick="console.log('00:09:45,530'); seek(585.0)">
              for many companies, specifically on the generative AI
            </span>
            
            <span id="chunk-152" class="transcript-chunks" onclick="console.log('00:09:49,558'); seek(589.0)">
              solution. But apart from the good
            </span>
            
            <span id="chunk-153" class="transcript-chunks" onclick="console.log('00:09:53,088'); seek(593.0)">
              partnership that AWS have with Nvidia,
            </span>
            
            <span id="chunk-154" class="transcript-chunks" onclick="console.log('00:09:56,278'); seek(596.0)">
              AWS is also in the front of innovating our
            </span>
            
            <span id="chunk-155" class="transcript-chunks" onclick="console.log('00:10:00,836'); seek(600.0)">
              own chipsets. So we have graviton,
            </span>
            
            <span id="chunk-156" class="transcript-chunks" onclick="console.log('00:10:04,522'); seek(604.0)">
              which is a general CPU, but we also have
            </span>
            
            <span id="chunk-157" class="transcript-chunks" onclick="console.log('00:10:09,890'); seek(609.0)">
              accelerators called trainium and infra.
            </span>
            
            <span id="chunk-158" class="transcript-chunks" onclick="console.log('00:10:13,114'); seek(613.0)">
              So trainium is the GPU accelerator
            </span>
            
            <span id="chunk-159" class="transcript-chunks" onclick="console.log('00:10:17,662'); seek(617.0)">
              that is focused on giving you better price performance,
            </span>
            
            <span id="chunk-160" class="transcript-chunks" onclick="console.log('00:10:21,726'); seek(621.0)">
              up to 40% better price performance than other comparable
            </span>
            
            <span id="chunk-161" class="transcript-chunks" onclick="console.log('00:10:25,182'); seek(625.0)">
              GPUs for training your models. So it's a very
            </span>
            
            <span id="chunk-162" class="transcript-chunks" onclick="console.log('00:10:29,096'); seek(629.0)">
              specific car that has been built by AWS
            </span>
            
            <span id="chunk-163" class="transcript-chunks" onclick="console.log('00:10:32,450'); seek(632.0)">
              to allow you to have a better performance and price when
            </span>
            
            <span id="chunk-164" class="transcript-chunks" onclick="console.log('00:10:35,948'); seek(635.0)">
              you're training machine learning models. And then once you have
            </span>
            
            <span id="chunk-165" class="transcript-chunks" onclick="console.log('00:10:39,888'); seek(639.0)">
              those models, you need to run inference on top of that. And AWS
            </span>
            
            <span id="chunk-166" class="transcript-chunks" onclick="console.log('00:10:43,142'); seek(643.0)">
              have also created inferential chipsets. We have two different
            </span>
            
            <span id="chunk-167" class="transcript-chunks" onclick="console.log('00:10:47,408'); seek(647.0)">
              chipsets, inference one and inferient two, that also give you a better
            </span>
            
            <span id="chunk-168" class="transcript-chunks" onclick="console.log('00:10:51,296'); seek(651.0)">
              price performance to actually run inference on top of those
            </span>
            
            <span id="chunk-169" class="transcript-chunks" onclick="console.log('00:10:54,912'); seek(654.0)">
              models. I'll highly encourage you to just quickly search
            </span>
            
            <span id="chunk-170" class="transcript-chunks" onclick="console.log('00:10:58,580'); seek(658.0)">
              about those. There is a lot of good documentation. Feel free to reach out
            </span>
            
            <span id="chunk-171" class="transcript-chunks" onclick="console.log('00:11:02,068'); seek(662.0)">
              to me on LinkedIn as well if you have any questions.
            </span>
            
            <span id="chunk-172" class="transcript-chunks" onclick="console.log('00:11:04,870'); seek(664.0)">
              Now, let's move from the infrastructure level to actually
            </span>
            
            <span id="chunk-173" class="transcript-chunks" onclick="console.log('00:11:08,936'); seek(668.0)">
              platform and services that AWS offers customers
            </span>
            
            <span id="chunk-174" class="transcript-chunks" onclick="console.log('00:11:13,288'); seek(673.0)">
              to actually run those models. Right? But in a day,
            </span>
            
            <span id="chunk-175" class="transcript-chunks" onclick="console.log('00:11:16,184'); seek(676.0)">
              organizations are looking, how can they run those models and
            </span>
            
            <span id="chunk-176" class="transcript-chunks" onclick="console.log('00:11:19,436'); seek(679.0)">
              use those models for all the use cases we've just discussed?
            </span>
            
            <span id="chunk-177" class="transcript-chunks" onclick="console.log('00:11:24,010'); seek(684.0)">
              So, the first platform I want to talk is Sagemaker.
            </span>
            
            <span id="chunk-178" class="transcript-chunks" onclick="console.log('00:11:28,470'); seek(688.0)">
              So, Sagemaker is the AWS
            </span>
            
            <span id="chunk-179" class="transcript-chunks" onclick="console.log('00:11:32,678'); seek(692.0)">
              machine learning AI platform that encompass
            </span>
            
            <span id="chunk-180" class="transcript-chunks" onclick="console.log('00:11:35,910'); seek(695.0)">
              a wide variety of features, from building new
            </span>
            
            <span id="chunk-181" class="transcript-chunks" onclick="console.log('00:11:40,292'); seek(700.0)">
              data sets, cleaning new data sets, enriching new data
            </span>
            
            <span id="chunk-182" class="transcript-chunks" onclick="console.log('00:11:43,716'); seek(703.0)">
              sets, training different models, different machine learning
            </span>
            
            <span id="chunk-183" class="transcript-chunks" onclick="console.log('00:11:47,540'); seek(707.0)">
              models, neural networks, you name it. You can actually
            </span>
            
            <span id="chunk-184" class="transcript-chunks" onclick="console.log('00:11:51,604'); seek(711.0)">
              run on Sagemaker once you run those models, you can
            </span>
            
            <span id="chunk-185" class="transcript-chunks" onclick="console.log('00:11:55,208'); seek(715.0)">
              actually deploy those machine learning models at scale. You actually manage
            </span>
            
            <span id="chunk-186" class="transcript-chunks" onclick="console.log('00:11:59,752'); seek(719.0)">
              the computational for you. You have monitoring, you can actually have evaluation.
            </span>
            
            <span id="chunk-187" class="transcript-chunks" onclick="console.log('00:12:04,950'); seek(724.0)">
              You can do a roll of the automatically fine tuning and distributed
            </span>
            
            <span id="chunk-188" class="transcript-chunks" onclick="console.log('00:12:08,738'); seek(728.0)">
              training for big models on top of that.
            </span>
            
            <span id="chunk-189" class="transcript-chunks" onclick="console.log('00:12:11,852'); seek(731.0)">
              And you can actually, after you've trained,
            </span>
            
            <span id="chunk-190" class="transcript-chunks" onclick="console.log('00:12:14,530'); seek(734.0)">
              operating all those models on Sagemaker.
            </span>
            
            <span id="chunk-191" class="transcript-chunks" onclick="console.log('00:12:18,430'); seek(738.0)">
              This is a very, in the last six years since Sagemaker
            </span>
            
            <span id="chunk-192" class="transcript-chunks" onclick="console.log('00:12:22,598'); seek(742.0)">
              was launched, we have introduced many, many new
            </span>
            
            <span id="chunk-193" class="transcript-chunks" onclick="console.log('00:12:26,272'); seek(746.0)">
              innovations like automatically model tuning. So you can deploy
            </span>
            
            <span id="chunk-194" class="transcript-chunks" onclick="console.log('00:12:30,246'); seek(750.0)">
              and train the model. And as you train the model, we will find
            </span>
            
            <span id="chunk-195" class="transcript-chunks" onclick="console.log('00:12:33,504'); seek(753.0)">
              the right parameters to actually fine tune and
            </span>
            
            <span id="chunk-196" class="transcript-chunks" onclick="console.log('00:12:36,708'); seek(756.0)">
              tune the model for the better performance of what you're trying to achieve.
            </span>
            
            <span id="chunk-197" class="transcript-chunks" onclick="console.log('00:12:40,426'); seek(760.0)">
              And when I mean model, I'm not only talking about large language models,
            </span>
            
            <span id="chunk-198" class="transcript-chunks" onclick="console.log('00:12:43,898'); seek(763.0)">
              I'm talking about any type of machine learning model that you want to
            </span>
            
            <span id="chunk-199" class="transcript-chunks" onclick="console.log('00:12:47,272'); seek(767.0)">
              actually build, train, and deploy.
            </span>
            
            <span id="chunk-200" class="transcript-chunks" onclick="console.log('00:12:50,310'); seek(770.0)">
              Now, when it comes to generative AI,
            </span>
            
            <span id="chunk-201" class="transcript-chunks" onclick="console.log('00:12:53,598'); seek(773.0)">
              Amazon Sagemaker has a very specific feature that really
            </span>
            
            <span id="chunk-202" class="transcript-chunks" onclick="console.log('00:12:57,596'); seek(777.0)">
              helps developers to get quickly testing
            </span>
            
            <span id="chunk-203" class="transcript-chunks" onclick="console.log('00:13:01,202'); seek(781.0)">
              different larger language models. The name of the feature is called Amazon
            </span>
            
            <span id="chunk-204" class="transcript-chunks" onclick="console.log('00:13:05,506'); seek(785.0)">
              Sagemaker Jumpstart. Sagemaker Jumpstart
            </span>
            
            <span id="chunk-205" class="transcript-chunks" onclick="console.log('00:13:09,846'); seek(789.0)">
              is a machine learning hub with foundational
            </span>
            
            <span id="chunk-206" class="transcript-chunks" onclick="console.log('00:13:13,590'); seek(793.0)">
              models made available that you can literally just click and
            </span>
            
            <span id="chunk-207" class="transcript-chunks" onclick="console.log('00:13:17,792'); seek(797.0)">
              deploy. It'll give you the recommended
            </span>
            
            <span id="chunk-208" class="transcript-chunks" onclick="console.log('00:13:21,366'); seek(801.0)">
              instance types that those models should run, depending on the size
            </span>
            
            <span id="chunk-209" class="transcript-chunks" onclick="console.log('00:13:25,316'); seek(805.0)">
              of those models. So right now there are more than hundreds of
            </span>
            
            <span id="chunk-210" class="transcript-chunks" onclick="console.log('00:13:29,172'); seek(809.0)">
              different models that have built in algorithms that have
            </span>
            
            <span id="chunk-211" class="transcript-chunks" onclick="console.log('00:13:33,092'); seek(813.0)">
              been pre trained with foundational models. So a lot
            </span>
            
            <span id="chunk-212" class="transcript-chunks" onclick="console.log('00:13:36,648'); seek(816.0)">
              of those models are available on hugging face. Some of the Amazon Alexa
            </span>
            
            <span id="chunk-213" class="transcript-chunks" onclick="console.log('00:13:40,382'); seek(820.0)">
              models are also available for you to deploy.
            </span>
            
            <span id="chunk-214" class="transcript-chunks" onclick="console.log('00:13:44,470'); seek(824.0)">
              The good thing is this is all UI and API based with
            </span>
            
            <span id="chunk-215" class="transcript-chunks" onclick="console.log('00:13:48,172'); seek(828.0)">
              a single click of buttons or simple API calls. You can actually
            </span>
            
            <span id="chunk-216" class="transcript-chunks" onclick="console.log('00:13:51,756'); seek(831.0)">
              have a machine, an EC two machine
            </span>
            
            <span id="chunk-217" class="transcript-chunks" onclick="console.log('00:13:56,002'); seek(836.0)">
              running on Sagemaker actually hosting that model,
            </span>
            
            <span id="chunk-218" class="transcript-chunks" onclick="console.log('00:13:59,484'); seek(839.0)">
              and all the things that you need to do manually by running
            </span>
            
            <span id="chunk-219" class="transcript-chunks" onclick="console.log('00:14:03,648'); seek(843.0)">
              to actually run that model and do inference on that model will be taken care
            </span>
            
            <span id="chunk-220" class="transcript-chunks" onclick="console.log('00:14:07,248'); seek(847.0)">
              of for you. We have a lot of notebooks,
            </span>
            
            <span id="chunk-221" class="transcript-chunks" onclick="console.log('00:14:10,550'); seek(850.0)">
              Jupyter notebooks that have examples on how you can actually do
            </span>
            
            <span id="chunk-222" class="transcript-chunks" onclick="console.log('00:14:14,212'); seek(854.0)">
              that. And the good thing about Sagemaker Jumpstart, some models
            </span>
            
            <span id="chunk-223" class="transcript-chunks" onclick="console.log('00:14:18,058'); seek(858.0)">
              that are available on Sagemaker Jumpstart also have
            </span>
            
            <span id="chunk-224" class="transcript-chunks" onclick="console.log('00:14:21,732'); seek(861.0)">
              the capability for fine tuning. So if you want to customize
            </span>
            
            <span id="chunk-225" class="transcript-chunks" onclick="console.log('00:14:25,150'); seek(865.0)">
              a model, let's say the Falcon model,
            </span>
            
            <span id="chunk-226" class="transcript-chunks" onclick="console.log('00:14:28,232'); seek(868.0)">
              180,000,000,000 parameters you want to customize with your
            </span>
            
            <span id="chunk-227" class="transcript-chunks" onclick="console.log('00:14:32,168'); seek(872.0)">
              own data, you can go on sage maker Jumpstart and
            </span>
            
            <span id="chunk-228" class="transcript-chunks" onclick="console.log('00:14:35,752'); seek(875.0)">
              you have actually an ease walkthrough
            </span>
            
            <span id="chunk-229" class="transcript-chunks" onclick="console.log('00:14:40,194'); seek(880.0)">
              way of fine tuning those models. One thing to note here,
            </span>
            
            <span id="chunk-230" class="transcript-chunks" onclick="console.log('00:14:44,412'); seek(884.0)">
              that is going to be very important to differentiate it from other services like
            </span>
            
            <span id="chunk-231" class="transcript-chunks" onclick="console.log('00:14:48,076'); seek(888.0)">
              bedrock that I'm going to talk in a moment. Sagemaker Jumpstart
            </span>
            
            <span id="chunk-232" class="transcript-chunks" onclick="console.log('00:14:52,150'); seek(892.0)">
              helps you run those models, but where you're
            </span>
            
            <span id="chunk-233" class="transcript-chunks" onclick="console.log('00:14:55,558'); seek(895.0)">
              running those models is actually on a EC two instance
            </span>
            
            <span id="chunk-234" class="transcript-chunks" onclick="console.log('00:14:59,462'); seek(899.0)">
              that you're paying every single minute or
            </span>
            
            <span id="chunk-235" class="transcript-chunks" onclick="console.log('00:15:02,672'); seek(902.0)">
              second that you're running those, even though if you might not be using that,
            </span>
            
            <span id="chunk-236" class="transcript-chunks" onclick="console.log('00:15:05,892'); seek(905.0)">
              you are actually paying for that instance. Those models are actually running
            </span>
            
            <span id="chunk-237" class="transcript-chunks" onclick="console.log('00:15:10,530'); seek(910.0)">
              on your account, on your ECQ, on your sage maker
            </span>
            
            <span id="chunk-238" class="transcript-chunks" onclick="console.log('00:15:13,978'); seek(913.0)">
              that behind the scenes runs ECQs, but you're paying for those.
            </span>
            
            <span id="chunk-239" class="transcript-chunks" onclick="console.log('00:15:17,400'); seek(917.0)">
              So it's kind of pay as you go, but for
            </span>
            
            <span id="chunk-240" class="transcript-chunks" onclick="console.log('00:15:20,968'); seek(920.0)">
              the whole instance that it requires, right? So you're not paying per tokens,
            </span>
            
            <span id="chunk-241" class="transcript-chunks" onclick="console.log('00:15:24,990'); seek(924.0)">
              you're paying for the whole instance. That is something that you just want to watch
            </span>
            
            <span id="chunk-242" class="transcript-chunks" onclick="console.log('00:15:28,316'); seek(928.0)">
              out because depending on the models, it can be very expensive.
            </span>
            
            <span id="chunk-243" class="transcript-chunks" onclick="console.log('00:15:32,730'); seek(932.0)">
              But let's continue our journey regarding generative
            </span>
            
            <span id="chunk-244" class="transcript-chunks" onclick="console.log('00:15:36,146'); seek(936.0)">
              AI. So when we look about
            </span>
            
            <span id="chunk-245" class="transcript-chunks" onclick="console.log('00:15:39,520'); seek(939.0)">
              what customers are asking for generative AI is
            </span>
            
            <span id="chunk-246" class="transcript-chunks" onclick="console.log('00:15:43,056'); seek(943.0)">
              which model should I use for a specific use case? How can
            </span>
            
            <span id="chunk-247" class="transcript-chunks" onclick="console.log('00:15:46,448'); seek(946.0)">
              I move quickly? Most customers are
            </span>
            
            <span id="chunk-248" class="transcript-chunks" onclick="console.log('00:15:50,128'); seek(950.0)">
              nodding to the exercise of training large
            </span>
            
            <span id="chunk-249" class="transcript-chunks" onclick="console.log('00:15:53,876'); seek(953.0)">
              language models or fine tuning. They really have use cases that
            </span>
            
            <span id="chunk-250" class="transcript-chunks" onclick="console.log('00:15:57,748'); seek(957.0)">
              they want to boost customer experience or increase employee
            </span>
            
            <span id="chunk-251" class="transcript-chunks" onclick="console.log('00:16:01,898'); seek(961.0)">
              productivity. For example. They just want to reinterate run POCs
            </span>
            
            <span id="chunk-252" class="transcript-chunks" onclick="console.log('00:16:05,498'); seek(965.0)">
              very quickly. And most important, how can they keep the
            </span>
            
            <span id="chunk-253" class="transcript-chunks" onclick="console.log('00:16:09,432'); seek(969.0)">
              data that they're going to be running through those models? Secure and private,
            </span>
            
            <span id="chunk-254" class="transcript-chunks" onclick="console.log('00:16:12,926'); seek(972.0)">
              right. That is a very important thing. You have your data. Your data shouldn't be
            </span>
            
            <span id="chunk-255" class="transcript-chunks" onclick="console.log('00:16:16,428'); seek(976.0)">
              used to train new models if you don't want to.
            </span>
            
            <span id="chunk-256" class="transcript-chunks" onclick="console.log('00:16:20,028'); seek(980.0)">
              And they should be kept secure encrypt by default.
            </span>
            
            <span id="chunk-257" class="transcript-chunks" onclick="console.log('00:16:24,578'); seek(984.0)">
              So with all those three questions being asked by customer,
            </span>
            
            <span id="chunk-258" class="transcript-chunks" onclick="console.log('00:16:28,720'); seek(988.0)">
              AWS have introduced last year a service called
            </span>
            
            <span id="chunk-259" class="transcript-chunks" onclick="console.log('00:16:32,304'); seek(992.0)">
              Amazon Bedrock, which is the easiest way
            </span>
            
            <span id="chunk-260" class="transcript-chunks" onclick="console.log('00:16:37,150'); seek(997.0)">
              to build and scale generative AI applications
            </span>
            
            <span id="chunk-261" class="transcript-chunks" onclick="console.log('00:16:41,142'); seek(1001.0)">
              with foundational models. And we talked about foundational models. Those are the models
            </span>
            
            <span id="chunk-262" class="transcript-chunks" onclick="console.log('00:16:44,938'); seek(1004.0)">
              that, large language models that are very big and
            </span>
            
            <span id="chunk-263" class="transcript-chunks" onclick="console.log('00:16:48,356'); seek(1008.0)">
              they can do a lot of general tasks. What does
            </span>
            
            <span id="chunk-264" class="transcript-chunks" onclick="console.log('00:16:52,468'); seek(1012.0)">
              Amazon bedrock offers you? First, it offers
            </span>
            
            <span id="chunk-265" class="transcript-chunks" onclick="console.log('00:16:56,398'); seek(1016.0)">
              you a choice, a democratization of
            </span>
            
            <span id="chunk-266" class="transcript-chunks" onclick="console.log('00:17:00,120'); seek(1020.0)">
              leading foundational models with a single API. And this is one
            </span>
            
            <span id="chunk-267" class="transcript-chunks" onclick="console.log('00:17:03,528'); seek(1023.0)">
              of the most amazing things about bedrock. You can
            </span>
            
            <span id="chunk-268" class="transcript-chunks" onclick="console.log('00:17:06,732'); seek(1026.0)">
              use the same service and the same API,
            </span>
            
            <span id="chunk-269" class="transcript-chunks" onclick="console.log('00:17:10,866'); seek(1030.0)">
              just choosing a parameter of your API
            </span>
            
            <span id="chunk-270" class="transcript-chunks" onclick="console.log('00:17:15,130'); seek(1035.0)">
              by choosing what model you want. And the model list has
            </span>
            
            <span id="chunk-271" class="transcript-chunks" onclick="console.log('00:17:18,812'); seek(1038.0)">
              been growing every single month. And you see in a moment
            </span>
            
            <span id="chunk-272" class="transcript-chunks" onclick="console.log('00:17:22,416'); seek(1042.0)">
              what are the models and model providers that bedrock currently offers.
            </span>
            
            <span id="chunk-273" class="transcript-chunks" onclick="console.log('00:17:27,446'); seek(1047.0)">
              But you can expect the model list and those capabilities to grow as
            </span>
            
            <span id="chunk-274" class="transcript-chunks" onclick="console.log('00:17:31,168'); seek(1051.0)">
              we speak. You can also run retrieval
            </span>
            
            <span id="chunk-275" class="transcript-chunks" onclick="console.log('00:17:34,850'); seek(1054.0)">
              augmented generation on top of that.
            </span>
            
            <span id="chunk-276" class="transcript-chunks" onclick="console.log('00:17:38,036'); seek(1058.0)">
              And I'm going to keep that on hold for now because I'm going to be
            </span>
            
            <span id="chunk-277" class="transcript-chunks" onclick="console.log('00:17:40,308'); seek(1060.0)">
              talking about a feature on bedrock that helps you do that.
            </span>
            
            <span id="chunk-278" class="transcript-chunks" onclick="console.log('00:17:43,736'); seek(1063.0)">
              You can also have agents that execute multiple steps
            </span>
            
            <span id="chunk-279" class="transcript-chunks" onclick="console.log('00:17:47,182'); seek(1067.0)">
              tasks by running lambda and calling your own APIs
            </span>
            
            <span id="chunk-280" class="transcript-chunks" onclick="console.log('00:17:50,654'); seek(1070.0)">
              or outside APIs automatically. And most important,
            </span>
            
            <span id="chunk-281" class="transcript-chunks" onclick="console.log('00:17:54,232'); seek(1074.0)">
              bedrock is security, private and safe.
            </span>
            
            <span id="chunk-282" class="transcript-chunks" onclick="console.log('00:18:00,470'); seek(1080.0)">
              Every data that you put to bedrock is not going
            </span>
            
            <span id="chunk-283" class="transcript-chunks" onclick="console.log('00:18:04,284'); seek(1084.0)">
              to be used to train your models. It's encrypted by default
            </span>
            
            <span id="chunk-284" class="transcript-chunks" onclick="console.log('00:18:07,970'); seek(1087.0)">
              and nobody else has access. This is really important to
            </span>
            
            <span id="chunk-285" class="transcript-chunks" onclick="console.log('00:18:11,776'); seek(1091.0)">
              keep in mind. You can also have vpc endpoints from bedrock
            </span>
            
            <span id="chunk-286" class="transcript-chunks" onclick="console.log('00:18:15,926'); seek(1095.0)">
              so the data never leaves your VPC. It goes through your VPC to a
            </span>
            
            <span id="chunk-287" class="transcript-chunks" onclick="console.log('00:18:19,504'); seek(1099.0)">
              vpc endpoint to bedrock where it hosts the service.
            </span>
            
            <span id="chunk-288" class="transcript-chunks" onclick="console.log('00:18:22,932'); seek(1102.0)">
              One important thing to note about bedrock, different than Sagemaker
            </span>
            
            <span id="chunk-289" class="transcript-chunks" onclick="console.log('00:18:27,178'); seek(1107.0)">
              Jumpstart, you pay AWS, you go. There are different pricing
            </span>
            
            <span id="chunk-290" class="transcript-chunks" onclick="console.log('00:18:31,370'); seek(1111.0)">
              modes on bedrock, but you start with which is the most.
            </span>
            
            <span id="chunk-291" class="transcript-chunks" onclick="console.log('00:18:34,692'); seek(1114.0)">
              I guess the way we start with bedrock, it's called on demand.
            </span>
            
            <span id="chunk-292" class="transcript-chunks" onclick="console.log('00:18:38,430'); seek(1118.0)">
              So depending on the large language model, the foundational model
            </span>
            
            <span id="chunk-293" class="transcript-chunks" onclick="console.log('00:18:41,976'); seek(1121.0)">
              that you pick, you're going to have a price per input
            </span>
            
            <span id="chunk-294" class="transcript-chunks" onclick="console.log('00:18:45,422'); seek(1125.0)">
              token and output token. When you're talking about text
            </span>
            
            <span id="chunk-295" class="transcript-chunks" onclick="console.log('00:18:49,064'); seek(1129.0)">
              you have a different pricing mechanisms for image generation.
            </span>
            
            <span id="chunk-296" class="transcript-chunks" onclick="console.log('00:18:52,498'); seek(1132.0)">
              But for now let's just keep it simple. You're going to pay for that,
            </span>
            
            <span id="chunk-297" class="transcript-chunks" onclick="console.log('00:18:55,516'); seek(1135.0)">
              right? So it's just the traditional AWS
            </span>
            
            <span id="chunk-298" class="transcript-chunks" onclick="console.log('00:18:59,366'); seek(1139.0)">
              cloud approach of pay as you go. And that becomes very
            </span>
            
            <span id="chunk-299" class="transcript-chunks" onclick="console.log('00:19:03,710'); seek(1143.0)">
              promising because instead of paying for big instances
            </span>
            
            <span id="chunk-300" class="transcript-chunks" onclick="console.log('00:19:07,222'); seek(1147.0)">
              to run those large language models for you, you can experiment,
            </span>
            
            <span id="chunk-301" class="transcript-chunks" onclick="console.log('00:19:10,838'); seek(1150.0)">
              iterate and create new products very easily by
            </span>
            
            <span id="chunk-302" class="transcript-chunks" onclick="console.log('00:19:14,372'); seek(1154.0)">
              still keeping your application in your solutions very
            </span>
            
            <span id="chunk-303" class="transcript-chunks" onclick="console.log('00:19:18,996'); seek(1158.0)">
              cost conscious. So now let's just
            </span>
            
            <span id="chunk-304" class="transcript-chunks" onclick="console.log('00:19:22,596'); seek(1162.0)">
              quickly talk about some of the what are
            </span>
            
            <span id="chunk-305" class="transcript-chunks" onclick="console.log('00:19:26,408'); seek(1166.0)">
              the model providers that are available on bedrock?
            </span>
            
            <span id="chunk-306" class="transcript-chunks" onclick="console.log('00:19:29,982'); seek(1169.0)">
              So the way bedrock is architected is
            </span>
            
            <span id="chunk-307" class="transcript-chunks" onclick="console.log('00:19:33,560'); seek(1173.0)">
              you have model providers. So those are companies that
            </span>
            
            <span id="chunk-308" class="transcript-chunks" onclick="console.log('00:19:37,164'); seek(1177.0)">
              have trained foundational models. And each model
            </span>
            
            <span id="chunk-309" class="transcript-chunks" onclick="console.log('00:19:40,460'); seek(1180.0)">
              provider, we have a different foundational models
            </span>
            
            <span id="chunk-310" class="transcript-chunks" onclick="console.log('00:19:44,386'); seek(1184.0)">
              available on that. Right. So here you can see
            </span>
            
            <span id="chunk-311" class="transcript-chunks" onclick="console.log('00:19:48,750'); seek(1188.0)">
              a list of seven different model providers that you can
            </span>
            
            <span id="chunk-312" class="transcript-chunks" onclick="console.log('00:19:52,432'); seek(1192.0)">
              pick from on Amazon Bedrock AWS,
            </span>
            
            <span id="chunk-313" class="transcript-chunks" onclick="console.log('00:19:55,552'); seek(1195.0)">
              the date of this presentation. So today is March
            </span>
            
            <span id="chunk-314" class="transcript-chunks" onclick="console.log('00:19:59,302'); seek(1199.0)">
              eigth 2024. As I'm recording this session,
            </span>
            
            <span id="chunk-315" class="transcript-chunks" onclick="console.log('00:20:02,938'); seek(1202.0)">
              we currently have seven model providers available for you
            </span>
            
            <span id="chunk-316" class="transcript-chunks" onclick="console.log('00:20:06,548'); seek(1206.0)">
              on Bedrock. So you have AI 21 that has the Jurassic two
            </span>
            
            <span id="chunk-317" class="transcript-chunks" onclick="console.log('00:20:11,204'); seek(1211.0)">
              models available for you. Then you
            </span>
            
            <span id="chunk-318" class="transcript-chunks" onclick="console.log('00:20:14,564'); seek(1214.0)">
              have entropic. And I'm going to talk about entropic in a moment. But they are
            </span>
            
            <span id="chunk-319" class="transcript-chunks" onclick="console.log('00:20:18,328'); seek(1218.0)">
              state of the art models with a very big performance.
            </span>
            
            <span id="chunk-320" class="transcript-chunks" onclick="console.log('00:20:21,518'); seek(1221.0)">
              Then you have cohere. With cohere you have both text
            </span>
            
            <span id="chunk-321" class="transcript-chunks" onclick="console.log('00:20:25,850'); seek(1225.0)">
              large language models and embedding models as well. So if
            </span>
            
            <span id="chunk-322" class="transcript-chunks" onclick="console.log('00:20:30,508'); seek(1230.0)">
              you want to create embeddings for your vector database, Cohere also offers you
            </span>
            
            <span id="chunk-323" class="transcript-chunks" onclick="console.log('00:20:34,412'); seek(1234.0)">
              with very performance embedding models. And it
            </span>
            
            <span id="chunk-324" class="transcript-chunks" onclick="console.log('00:20:38,172'); seek(1238.0)">
              was just introduced I think a couple of weeks ago, Mistral AI.
            </span>
            
            <span id="chunk-325" class="transcript-chunks" onclick="console.log('00:20:41,222'); seek(1241.0)">
              So you have two different models with Mistral AI. The Mistro
            </span>
            
            <span id="chunk-326" class="transcript-chunks" onclick="console.log('00:20:44,742'); seek(1244.0)">
              seven D and the mixture mix of
            </span>
            
            <span id="chunk-327" class="transcript-chunks" onclick="console.log('00:20:48,352'); seek(1248.0)">
              exports which is eight models that are put together
            </span>
            
            <span id="chunk-328" class="transcript-chunks" onclick="console.log('00:20:52,180'); seek(1252.0)">
              into a single API. Very good performance. Then you also of
            </span>
            
            <span id="chunk-329" class="transcript-chunks" onclick="console.log('00:20:55,828'); seek(1255.0)">
              course have meta with Leomachu which is an open
            </span>
            
            <span id="chunk-330" class="transcript-chunks" onclick="console.log('00:20:59,220'); seek(1259.0)">
              science model. Then you have stability
            </span>
            
            <span id="chunk-331" class="transcript-chunks" onclick="console.log('00:21:02,906'); seek(1262.0)">
              AI. So stability AI is one of the leaders
            </span>
            
            <span id="chunk-332" class="transcript-chunks" onclick="console.log('00:21:06,718'); seek(1266.0)">
              research labs for image generation. So the stable
            </span>
            
            <span id="chunk-333" class="transcript-chunks" onclick="console.log('00:21:10,782'); seek(1270.0)">
              diffusion XL 1.0 is a model that allows you to
            </span>
            
            <span id="chunk-334" class="transcript-chunks" onclick="console.log('00:21:14,312'); seek(1274.0)">
              generate images. So instead of just generating text,
            </span>
            
            <span id="chunk-335" class="transcript-chunks" onclick="console.log('00:21:17,736'); seek(1277.0)">
              it actually generates image. You input a text,
            </span>
            
            <span id="chunk-336" class="transcript-chunks" onclick="console.log('00:21:20,796'); seek(1280.0)">
              a cat walking in the park. It will actually generate an image
            </span>
            
            <span id="chunk-337" class="transcript-chunks" onclick="console.log('00:21:24,258'); seek(1284.0)">
              for you with a cat walking in the park. Then you also
            </span>
            
            <span id="chunk-338" class="transcript-chunks" onclick="console.log('00:21:27,776'); seek(1287.0)">
              have our own models from Amazon. Those are
            </span>
            
            <span id="chunk-339" class="transcript-chunks" onclick="console.log('00:21:31,408'); seek(1291.0)">
              called Titan. So Titan models offers you a text to
            </span>
            
            <span id="chunk-340" class="transcript-chunks" onclick="console.log('00:21:35,408'); seek(1295.0)">
              text model, traditional large language models. It also offers embedding
            </span>
            
            <span id="chunk-341" class="transcript-chunks" onclick="console.log('00:21:39,462'); seek(1299.0)">
              models and it also offers image generation models.
            </span>
            
            <span id="chunk-342" class="transcript-chunks" onclick="console.log('00:21:43,082'); seek(1303.0)">
              Right? So it's a set of models available for
            </span>
            
            <span id="chunk-343" class="transcript-chunks" onclick="console.log('00:21:46,772'); seek(1306.0)">
              you. The very important thing to keep in mind with
            </span>
            
            <span id="chunk-344" class="transcript-chunks" onclick="console.log('00:21:50,468'); seek(1310.0)">
              bedrock is we are democratizing the ability for people
            </span>
            
            <span id="chunk-345" class="transcript-chunks" onclick="console.log('00:21:54,472'); seek(1314.0)">
              to consume different models for a specific use case.
            </span>
            
            <span id="chunk-346" class="transcript-chunks" onclick="console.log('00:21:58,488'); seek(1318.0)">
              And you can go right now on Bedrock webpage
            </span>
            
            <span id="chunk-347" class="transcript-chunks" onclick="console.log('00:22:03,326'); seek(1323.0)">
              and click on the pricing and you see the different pricings for each model.
            </span>
            
            <span id="chunk-348" class="transcript-chunks" onclick="console.log('00:22:07,448'); seek(1327.0)">
              Depending on the performance, the size of those models, you might be paying a
            </span>
            
            <span id="chunk-349" class="transcript-chunks" onclick="console.log('00:22:11,148'); seek(1331.0)">
              specific price. And that is really important because you can now decide how
            </span>
            
            <span id="chunk-350" class="transcript-chunks" onclick="console.log('00:22:15,452'); seek(1335.0)">
              you want to build your applications. Remember,
            </span>
            
            <span id="chunk-351" class="transcript-chunks" onclick="console.log('00:22:18,490'); seek(1338.0)">
              it's the single API call and you do not need to manage
            </span>
            
            <span id="chunk-352" class="transcript-chunks" onclick="console.log('00:22:21,840'); seek(1341.0)">
              any infrastructure. That is something I want to highlight as well. All the infrastructure
            </span>
            
            <span id="chunk-353" class="transcript-chunks" onclick="console.log('00:22:25,990'); seek(1345.0)">
              and GPUs to actually run those models, which is very complex
            </span>
            
            <span id="chunk-354" class="transcript-chunks" onclick="console.log('00:22:29,750'); seek(1349.0)">
              and it takes a lot of capacity, is taken care by AWS for
            </span>
            
            <span id="chunk-355" class="transcript-chunks" onclick="console.log('00:22:33,508'); seek(1353.0)">
              you, and that is something very beautiful that
            </span>
            
            <span id="chunk-356" class="transcript-chunks" onclick="console.log('00:22:38,530'); seek(1358.0)">
              you should be using and taking benefit of.
            </span>
            
            <span id="chunk-357" class="transcript-chunks" onclick="console.log('00:22:42,050'); seek(1362.0)">
              Now, I really want to talk about the partnership that we have with entropic.
            </span>
            
            <span id="chunk-358" class="transcript-chunks" onclick="console.log('00:22:46,430'); seek(1366.0)">
              So entropic is a longtime AWS customer, and entropic
            </span>
            
            <span id="chunk-359" class="transcript-chunks" onclick="console.log('00:22:50,750'); seek(1370.0)">
              is one of the top leading research
            </span>
            
            <span id="chunk-360" class="transcript-chunks" onclick="console.log('00:22:55,290'); seek(1375.0)">
              AI companies in the world. And I'm
            </span>
            
            <span id="chunk-361" class="transcript-chunks" onclick="console.log('00:22:59,074'); seek(1379.0)">
              going to talk about some of the models that they have published
            </span>
            
            <span id="chunk-362" class="transcript-chunks" onclick="console.log('00:23:03,362'); seek(1383.0)">
              in the last years. But AWS,
            </span>
            
            <span id="chunk-363" class="transcript-chunks" onclick="console.log('00:23:07,074'); seek(1387.0)">
              Amazon have invested heavily. I think we've announced
            </span>
            
            <span id="chunk-364" class="transcript-chunks" onclick="console.log('00:23:10,902'); seek(1390.0)">
              a $4 billion investment last year. So entropic
            </span>
            
            <span id="chunk-365" class="transcript-chunks" onclick="console.log('00:23:14,678'); seek(1394.0)">
              has a very good partnership with Amazon. And the way we showed the
            </span>
            
            <span id="chunk-366" class="transcript-chunks" onclick="console.log('00:23:18,368'); seek(1398.0)">
              results of that partnership is, well, first of all, let's talk about
            </span>
            
            <span id="chunk-367" class="transcript-chunks" onclick="console.log('00:23:23,410'); seek(1403.0)">
              the story about entropic very quickly, right? So if
            </span>
            
            <span id="chunk-368" class="transcript-chunks" onclick="console.log('00:23:26,644'); seek(1406.0)">
              you look here at the timeline, we are in a very fast paced
            </span>
            
            <span id="chunk-369" class="transcript-chunks" onclick="console.log('00:23:30,410'); seek(1410.0)">
              environment. In 2019,
            </span>
            
            <span id="chunk-370" class="transcript-chunks" onclick="console.log('00:23:33,670'); seek(1413.0)">
              GPT-2 was launched from OpenAI.
            </span>
            
            <span id="chunk-371" class="transcript-chunks" onclick="console.log('00:23:37,510'); seek(1417.0)">
              Then some researchers have
            </span>
            
            <span id="chunk-372" class="transcript-chunks" onclick="console.log('00:23:41,256'); seek(1421.0)">
              published some papers about the performance of transformers.
            </span>
            
            <span id="chunk-373" class="transcript-chunks" onclick="console.log('00:23:44,846'); seek(1424.0)">
              And GPT-3 was launched sometime in 2020
            </span>
            
            <span id="chunk-374" class="transcript-chunks" onclick="console.log('00:23:48,684'); seek(1428.0)">
              with Codex as well. Right?
            </span>
            
            <span id="chunk-375" class="transcript-chunks" onclick="console.log('00:23:52,010'); seek(1432.0)">
              Most of the people that have founded entropic were
            </span>
            
            <span id="chunk-376" class="transcript-chunks" onclick="console.log('00:23:55,964'); seek(1435.0)">
              employees from OpenAI. So they left on OpenAI
            </span>
            
            <span id="chunk-377" class="transcript-chunks" onclick="console.log('00:23:59,458'); seek(1439.0)">
              in 2021 and they found Entropic.
            </span>
            
            <span id="chunk-378" class="transcript-chunks" onclick="console.log('00:24:03,310'); seek(1443.0)">
              You can see how quickly they went from founding
            </span>
            
            <span id="chunk-379" class="transcript-chunks" onclick="console.log('00:24:07,958'); seek(1447.0)">
              a new research lab and actually publishing very good and
            </span>
            
            <span id="chunk-380" class="transcript-chunks" onclick="console.log('00:24:12,212'); seek(1452.0)">
              making available very good models. So they published some papers
            </span>
            
            <span id="chunk-381" class="transcript-chunks" onclick="console.log('00:24:16,378'); seek(1456.0)">
              in 2021. Then in 2022
            </span>
            
            <span id="chunk-382" class="transcript-chunks" onclick="console.log('00:24:21,970'); seek(1461.0)">
              they finished training clot, right? And they have
            </span>
            
            <span id="chunk-383" class="transcript-chunks" onclick="console.log('00:24:25,720'); seek(1465.0)">
              something called Constitution AI. I would highly recommend you to search is
            </span>
            
            <span id="chunk-384" class="transcript-chunks" onclick="console.log('00:24:29,528'); seek(1469.0)">
              their whole way how they take very important
            </span>
            
            <span id="chunk-385" class="transcript-chunks" onclick="console.log('00:24:34,550'); seek(1474.0)">
              care on safety and alignment for those models.
            </span>
            
            <span id="chunk-386" class="transcript-chunks" onclick="console.log('00:24:38,542'); seek(1478.0)">
              And then in 2023, they have released the first cloud one model.
            </span>
            
            <span id="chunk-387" class="transcript-chunks" onclick="console.log('00:24:43,050'); seek(1483.0)">
              Then they have released one of the first companies to release 100,000.
            </span>
            
            <span id="chunk-388" class="transcript-chunks" onclick="console.log('00:24:47,164'); seek(1487.0)">
              Context window. Context window just means how much text
            </span>
            
            <span id="chunk-389" class="transcript-chunks" onclick="console.log('00:24:50,592'); seek(1490.0)">
              you can put per request. Then after that,
            </span>
            
            <span id="chunk-390" class="transcript-chunks" onclick="console.log('00:24:54,096'); seek(1494.0)">
              in 2023, they have released cloud two, which was a big improvement
            </span>
            
            <span id="chunk-391" class="transcript-chunks" onclick="console.log('00:24:58,310'); seek(1498.0)">
              from cloud one. Then a couple of months after they've
            </span>
            
            <span id="chunk-392" class="transcript-chunks" onclick="console.log('00:25:02,378'); seek(1502.0)">
              released cloud 2.1 with more improvements and performance.
            </span>
            
            <span id="chunk-393" class="transcript-chunks" onclick="console.log('00:25:05,866'); seek(1505.0)">
              Then this year, actually last week or this week?
            </span>
            
            <span id="chunk-394" class="transcript-chunks" onclick="console.log('00:25:10,450'); seek(1510.0)">
              To be honest, Monday this week. They have,
            </span>
            
            <span id="chunk-395" class="transcript-chunks" onclick="console.log('00:25:13,800'); seek(1513.0)">
              I would say, shocked the industry with very performant
            </span>
            
            <span id="chunk-396" class="transcript-chunks" onclick="console.log('00:25:17,614'); seek(1517.0)">
              and set of models of three different models
            </span>
            
            <span id="chunk-397" class="transcript-chunks" onclick="console.log('00:25:21,422'); seek(1521.0)">
              called cloud tree. And I'm going to talk about those. So cloud tree
            </span>
            
            <span id="chunk-398" class="transcript-chunks" onclick="console.log('00:25:26,330'); seek(1526.0)">
              comes with three different models. The first one is cloudtree haiku.
            </span>
            
            <span id="chunk-399" class="transcript-chunks" onclick="console.log('00:25:30,498'); seek(1530.0)">
              And you can see these on this graph is cloudtree haiku.
            </span>
            
            <span id="chunk-400" class="transcript-chunks" onclick="console.log('00:25:34,330'); seek(1534.0)">
              You can see the intelligence is a very performance
            </span>
            
            <span id="chunk-401" class="transcript-chunks" onclick="console.log('00:25:37,906'); seek(1537.0)">
              model, but most important is a very low cost and
            </span>
            
            <span id="chunk-402" class="transcript-chunks" onclick="console.log('00:25:42,032'); seek(1542.0)">
              very fast inference model.
            </span>
            
            <span id="chunk-403" class="transcript-chunks" onclick="console.log('00:25:44,510'); seek(1544.0)">
              Then they have also released cloud tree Sonnet,
            </span>
            
            <span id="chunk-404" class="transcript-chunks" onclick="console.log('00:25:47,782'); seek(1547.0)">
              which is their mid tier model, which is very, very intelligent.
            </span>
            
            <span id="chunk-405" class="transcript-chunks" onclick="console.log('00:25:51,462'); seek(1551.0)">
              It beats all the previous quad models in terms
            </span>
            
            <span id="chunk-406" class="transcript-chunks" onclick="console.log('00:25:54,932'); seek(1554.0)">
              of benchmarks, and it's in the middle when it comes to
            </span>
            
            <span id="chunk-407" class="transcript-chunks" onclick="console.log('00:25:59,284'); seek(1559.0)">
              cost. As a matter of fact, claw three sonnet is much more
            </span>
            
            <span id="chunk-408" class="transcript-chunks" onclick="console.log('00:26:02,836'); seek(1562.0)">
              performance than any previous quad models, but is actually
            </span>
            
            <span id="chunk-409" class="transcript-chunks" onclick="console.log('00:26:06,456'); seek(1566.0)">
              cheaper than cloud two and cloud 2.1 models.
            </span>
            
            <span id="chunk-410" class="transcript-chunks" onclick="console.log('00:26:09,790'); seek(1569.0)">
              And then of course, client tree, opposite is the most intelligent model
            </span>
            
            <span id="chunk-411" class="transcript-chunks" onclick="console.log('00:26:13,992'); seek(1573.0)">
              and has actually beat state of the art models
            </span>
            
            <span id="chunk-412" class="transcript-chunks" onclick="console.log('00:26:18,066'); seek(1578.0)">
              on most benchmarks. And you can see this data just
            </span>
            
            <span id="chunk-413" class="transcript-chunks" onclick="console.log('00:26:21,932'); seek(1581.0)">
              search cloud tree report paper, you see all those benchmarks
            </span>
            
            <span id="chunk-414" class="transcript-chunks" onclick="console.log('00:26:26,330'); seek(1586.0)">
              that are available. So now how
            </span>
            
            <span id="chunk-415" class="transcript-chunks" onclick="console.log('00:26:30,080'); seek(1590.0)">
              this entropic incredible performance
            </span>
            
            <span id="chunk-416" class="transcript-chunks" onclick="console.log('00:26:34,934'); seek(1594.0)">
              and innovation we call three model impacts
            </span>
            
            <span id="chunk-417" class="transcript-chunks" onclick="console.log('00:26:38,342'); seek(1598.0)">
              bedrock? Well, because the relationship that Amazon has with entropic
            </span>
            
            <span id="chunk-418" class="transcript-chunks" onclick="console.log('00:26:43,338'); seek(1603.0)">
              claw three models are already available on bedrock
            </span>
            
            <span id="chunk-419" class="transcript-chunks" onclick="console.log('00:26:47,226'); seek(1607.0)">
              as we speak right now. Claw three sonnet, which is their
            </span>
            
            <span id="chunk-420" class="transcript-chunks" onclick="console.log('00:26:50,852'); seek(1610.0)">
              mid tier model, very big performance with
            </span>
            
            <span id="chunk-421" class="transcript-chunks" onclick="console.log('00:26:54,216'); seek(1614.0)">
              very good price. All these models are multimodal.
            </span>
            
            <span id="chunk-422" class="transcript-chunks" onclick="console.log('00:26:58,382'); seek(1618.0)">
              Let me just say that multimodal means you can input text,
            </span>
            
            <span id="chunk-423" class="transcript-chunks" onclick="console.log('00:27:01,880'); seek(1621.0)">
              but also you can input images. Previous quad models could
            </span>
            
            <span id="chunk-424" class="transcript-chunks" onclick="console.log('00:27:06,216'); seek(1626.0)">
              only receive text as inputs. Those models
            </span>
            
            <span id="chunk-425" class="transcript-chunks" onclick="console.log('00:27:10,402'); seek(1630.0)">
              can actually receive image as input.
            </span>
            
            <span id="chunk-426" class="transcript-chunks" onclick="console.log('00:27:13,922'); seek(1633.0)">
              So you can put an image and you can ask questions about that image.
            </span>
            
            <span id="chunk-427" class="transcript-chunks" onclick="console.log('00:27:16,834'); seek(1636.0)">
              You can actually put multiple images per input. And all those three
            </span>
            
            <span id="chunk-428" class="transcript-chunks" onclick="console.log('00:27:20,512'); seek(1640.0)">
              models actually have that capability. And all those
            </span>
            
            <span id="chunk-429" class="transcript-chunks" onclick="console.log('00:27:24,352'); seek(1644.0)">
              models have now an even bigger context window.
            </span>
            
            <span id="chunk-430" class="transcript-chunks" onclick="console.log('00:27:28,102'); seek(1648.0)">
              Not only they have bigger context window, but the claim
            </span>
            
            <span id="chunk-431" class="transcript-chunks" onclick="console.log('00:27:32,058'); seek(1652.0)">
              is that with this bigger context window, doesn't matter
            </span>
            
            <span id="chunk-432" class="transcript-chunks" onclick="console.log('00:27:35,812'); seek(1655.0)">
              where you put the text on those context window,
            </span>
            
            <span id="chunk-433" class="transcript-chunks" onclick="console.log('00:27:38,714'); seek(1658.0)">
              the performance remains very similar and very good,
            </span>
            
            <span id="chunk-434" class="transcript-chunks" onclick="console.log('00:27:42,548'); seek(1662.0)">
              which was not actually true in previous models and actually not in
            </span>
            
            <span id="chunk-435" class="transcript-chunks" onclick="console.log('00:27:46,088'); seek(1666.0)">
              the industry as well. Claw three oppos and cloud
            </span>
            
            <span id="chunk-436" class="transcript-chunks" onclick="console.log('00:27:49,672'); seek(1669.0)">
              three haiku are going to be made available on bedrock very
            </span>
            
            <span id="chunk-437" class="transcript-chunks" onclick="console.log('00:27:53,432'); seek(1673.0)">
              soon. They are currently not available, but they're going to be made very
            </span>
            
            <span id="chunk-438" class="transcript-chunks" onclick="console.log('00:27:56,796'); seek(1676.0)">
              soon. Now that I talked about it,
            </span>
            
            <span id="chunk-439" class="transcript-chunks" onclick="console.log('00:28:00,252'); seek(1680.0)">
              let's just talk about some of the functionality that bedrock allows
            </span>
            
            <span id="chunk-440" class="transcript-chunks" onclick="console.log('00:28:03,538'); seek(1683.0)">
              you to use. So first,
            </span>
            
            <span id="chunk-441" class="transcript-chunks" onclick="console.log('00:28:06,970'); seek(1686.0)">
              you can actually use those foundational models as it is.
            </span>
            
            <span id="chunk-442" class="transcript-chunks" onclick="console.log('00:28:10,928'); seek(1690.0)">
              But if you really want to fine tune and customize
            </span>
            
            <span id="chunk-443" class="transcript-chunks" onclick="console.log('00:28:14,262'); seek(1694.0)">
              those models with your data, because you really believe you've
            </span>
            
            <span id="chunk-444" class="transcript-chunks" onclick="console.log('00:28:17,462'); seek(1697.0)">
              tried rag, you've tried prompt engineering and you're
            </span>
            
            <span id="chunk-445" class="transcript-chunks" onclick="console.log('00:28:21,334'); seek(1701.0)">
              not achieving the performance your use case require.
            </span>
            
            <span id="chunk-446" class="transcript-chunks" onclick="console.log('00:28:24,298'); seek(1704.0)">
              In my opinion this should be the last resort. But if you need to privately
            </span>
            
            <span id="chunk-447" class="transcript-chunks" onclick="console.log('00:28:27,930'); seek(1707.0)">
              customize models, you can actually use right now,
            </span>
            
            <span id="chunk-448" class="transcript-chunks" onclick="console.log('00:28:31,700'); seek(1711.0)">
              bedrock supports to automatically customize those
            </span>
            
            <span id="chunk-449" class="transcript-chunks" onclick="console.log('00:28:35,684'); seek(1715.0)">
              models. You put your data on s three in a private s three bucket.
            </span>
            
            <span id="chunk-450" class="transcript-chunks" onclick="console.log('00:28:39,310'); seek(1719.0)">
              You connect that s three bucket to bedrock and bedrock will automatically
            </span>
            
            <span id="chunk-451" class="transcript-chunks" onclick="console.log('00:28:43,406'); seek(1723.0)">
              fine tune and customize those models for you. Currently you can customize
            </span>
            
            <span id="chunk-452" class="transcript-chunks" onclick="console.log('00:28:47,646'); seek(1727.0)">
              models with Titan, Cohere and Lemachu.
            </span>
            
            <span id="chunk-453" class="transcript-chunks" onclick="console.log('00:28:50,830'); seek(1730.0)">
              Very soon we are going to open the ability to also customize
            </span>
            
            <span id="chunk-454" class="transcript-chunks" onclick="console.log('00:28:54,706'); seek(1734.0)">
              cloud models and potentially other models from other model providers.
            </span>
            
            <span id="chunk-455" class="transcript-chunks" onclick="console.log('00:28:59,850'); seek(1739.0)">
              So the good thing about this functionality from bedrock, if you have
            </span>
            
            <span id="chunk-456" class="transcript-chunks" onclick="console.log('00:29:03,712'); seek(1743.0)">
              done some fine tuning customization from auto in the past, it actually
            </span>
            
            <span id="chunk-457" class="transcript-chunks" onclick="console.log('00:29:07,296'); seek(1747.0)">
              requires a lot of science and it can be very complex. Bedrock completely
            </span>
            
            <span id="chunk-458" class="transcript-chunks" onclick="console.log('00:29:11,360'); seek(1751.0)">
              removes that. You just put your data on s three,
            </span>
            
            <span id="chunk-459" class="transcript-chunks" onclick="console.log('00:29:15,250'); seek(1755.0)">
              you go on bedrock and you point the data from bedrock on
            </span>
            
            <span id="chunk-460" class="transcript-chunks" onclick="console.log('00:29:19,332'); seek(1759.0)">
              s three and you choose the model. And behind the scenes bedrock, you just customize
            </span>
            
            <span id="chunk-461" class="transcript-chunks" onclick="console.log('00:29:23,082'); seek(1763.0)">
              new models and you notify when your specific model has
            </span>
            
            <span id="chunk-462" class="transcript-chunks" onclick="console.log('00:29:26,888'); seek(1766.0)">
              been trained. No one else will be able to use this model.
            </span>
            
            <span id="chunk-463" class="transcript-chunks" onclick="console.log('00:29:30,248'); seek(1770.0)">
              None of the data that you have actually provided from bedrock on
            </span>
            
            <span id="chunk-464" class="transcript-chunks" onclick="console.log('00:29:33,848'); seek(1773.0)">
              s three will be used by anyone else or to train other models.
            </span>
            
            <span id="chunk-465" class="transcript-chunks" onclick="console.log('00:29:37,858'); seek(1777.0)">
              It's just your model. And you can then consume that
            </span>
            
            <span id="chunk-466" class="transcript-chunks" onclick="console.log('00:29:41,532'); seek(1781.0)">
              model and run inference on that model by
            </span>
            
            <span id="chunk-467" class="transcript-chunks" onclick="console.log('00:29:46,410'); seek(1786.0)">
              making an API call. The same way you call API from
            </span>
            
            <span id="chunk-468" class="transcript-chunks" onclick="console.log('00:29:49,872'); seek(1789.0)">
              Bedrock, you can call Bedrock API to
            </span>
            
            <span id="chunk-469" class="transcript-chunks" onclick="console.log('00:29:53,088'); seek(1793.0)">
              use your own customizable model.
            </span>
            
            <span id="chunk-470" class="transcript-chunks" onclick="console.log('00:29:56,270'); seek(1796.0)">
              Now another very good thing, if you don't need to
            </span>
            
            <span id="chunk-471" class="transcript-chunks" onclick="console.log('00:29:59,712'); seek(1799.0)">
              customize your model is actually running retrieval augmented generation.
            </span>
            
            <span id="chunk-472" class="transcript-chunks" onclick="console.log('00:30:04,042'); seek(1804.0)">
              Retrieval augmented generation, for folks that are not familiar is just
            </span>
            
            <span id="chunk-473" class="transcript-chunks" onclick="console.log('00:30:07,508'); seek(1807.0)">
              the idea that, let's say you have a big data
            </span>
            
            <span id="chunk-474" class="transcript-chunks" onclick="console.log('00:30:10,996'); seek(1810.0)">
              set of documents and those documents talk about the
            </span>
            
            <span id="chunk-475" class="transcript-chunks" onclick="console.log('00:30:15,768'); seek(1815.0)">
              way your company operated and you want to have a chat bot
            </span>
            
            <span id="chunk-476" class="transcript-chunks" onclick="console.log('00:30:19,406'); seek(1819.0)">
              that actually answer questions about those documentations,
            </span>
            
            <span id="chunk-477" class="transcript-chunks" onclick="console.log('00:30:22,750'); seek(1822.0)">
              right? Well, those documents are likely private.
            </span>
            
            <span id="chunk-478" class="transcript-chunks" onclick="console.log('00:30:26,046'); seek(1826.0)">
              So the foundational model, that model providers made it
            </span>
            
            <span id="chunk-479" class="transcript-chunks" onclick="console.log('00:30:29,692'); seek(1829.0)">
              available on Bedrock, they don't know about your company operational
            </span>
            
            <span id="chunk-480" class="transcript-chunks" onclick="console.log('00:30:33,890'); seek(1833.0)">
              procedures. But when you're creating a chatbot, you actually want to make that
            </span>
            
            <span id="chunk-481" class="transcript-chunks" onclick="console.log('00:30:37,788'); seek(1837.0)">
              available for the
            </span>
            
            <span id="chunk-482" class="transcript-chunks" onclick="console.log('00:30:41,408'); seek(1841.0)">
              model itself to actually consume. So what you can do,
            </span>
            
            <span id="chunk-483" class="transcript-chunks" onclick="console.log('00:30:45,040'); seek(1845.0)">
              you can use what we call vector databases.
            </span>
            
            <span id="chunk-484" class="transcript-chunks" onclick="console.log('00:30:48,486'); seek(1848.0)">
              And I'm going to talk in a moment on what vector databases are
            </span>
            
            <span id="chunk-485" class="transcript-chunks" onclick="console.log('00:30:52,212'); seek(1852.0)">
              made available on AWS. But Bedrock has a feature
            </span>
            
            <span id="chunk-486" class="transcript-chunks" onclick="console.log('00:30:56,266'); seek(1856.0)">
              called knowledge base that makes all this process of running retrieval
            </span>
            
            <span id="chunk-487" class="transcript-chunks" onclick="console.log('00:31:00,218'); seek(1860.0)">
              augmented generation very simple. The way it works is
            </span>
            
            <span id="chunk-488" class="transcript-chunks" onclick="console.log('00:31:03,972'); seek(1863.0)">
              you go on bedrock, you first create an
            </span>
            
            <span id="chunk-489" class="transcript-chunks" onclick="console.log('00:31:07,688'); seek(1867.0)">
              S three bucket and you put all your documents on this s three bucket.
            </span>
            
            <span id="chunk-490" class="transcript-chunks" onclick="console.log('00:31:11,118'); seek(1871.0)">
              It's your s three bucket. Nobody has access.
            </span>
            
            <span id="chunk-491" class="transcript-chunks" onclick="console.log('00:31:13,944'); seek(1873.0)">
              Then you go on bedrock you choose
            </span>
            
            <span id="chunk-492" class="transcript-chunks" onclick="console.log('00:31:18,300'); seek(1878.0)">
              which model you actually want to run embeddings.
            </span>
            
            <span id="chunk-493" class="transcript-chunks" onclick="console.log('00:31:21,490'); seek(1881.0)">
              So you can choose between Titan for now, Titan and
            </span>
            
            <span id="chunk-494" class="transcript-chunks" onclick="console.log('00:31:24,732'); seek(1884.0)">
              cohere embeddings are just going through those documents,
            </span>
            
            <span id="chunk-495" class="transcript-chunks" onclick="console.log('00:31:28,550'); seek(1888.0)">
              converting those texts into vector numerical,
            </span>
            
            <span id="chunk-496" class="transcript-chunks" onclick="console.log('00:31:32,246'); seek(1892.0)">
              vector vector representations. And then finally you choose a vector
            </span>
            
            <span id="chunk-497" class="transcript-chunks" onclick="console.log('00:31:36,022'); seek(1896.0)">
              database. And right now you have a variety of databases
            </span>
            
            <span id="chunk-498" class="transcript-chunks" onclick="console.log('00:31:40,006'); seek(1900.0)">
              that you can select from. I think there are four options right now
            </span>
            
            <span id="chunk-499" class="transcript-chunks" onclick="console.log('00:31:43,428'); seek(1903.0)">
              that you can select and those numbers are going to be increasing in the future.
            </span>
            
            <span id="chunk-500" class="transcript-chunks" onclick="console.log('00:31:46,564'); seek(1906.0)">
              But you can select, for example, the open search serverless vector database.
            </span>
            
            <span id="chunk-501" class="transcript-chunks" onclick="console.log('00:31:50,826'); seek(1910.0)">
              Then automatically bedrock will run the embeddings
            </span>
            
            <span id="chunk-502" class="transcript-chunks" onclick="console.log('00:31:54,938'); seek(1914.0)">
              on the data that is on s three will store the vectors on your open
            </span>
            
            <span id="chunk-503" class="transcript-chunks" onclick="console.log('00:31:58,936'); seek(1918.0)">
              search vector database. And finally, which is,
            </span>
            
            <span id="chunk-504" class="transcript-chunks" onclick="console.log('00:32:02,280'); seek(1922.0)">
              let's say you want to run this chatbot. When you ask
            </span>
            
            <span id="chunk-505" class="transcript-chunks" onclick="console.log('00:32:05,692'); seek(1925.0)">
              a question, let's say you ask a question, what is the
            </span>
            
            <span id="chunk-506" class="transcript-chunks" onclick="console.log('00:32:08,780'); seek(1928.0)">
              HR policy for vacation
            </span>
            
            <span id="chunk-507" class="transcript-chunks" onclick="console.log('00:32:12,594'); seek(1932.0)">
              in New York as an example, right? What bedrock
            </span>
            
            <span id="chunk-508" class="transcript-chunks" onclick="console.log('00:32:16,418'); seek(1936.0)">
              can do, it can then retrieve your vector database by running what
            </span>
            
            <span id="chunk-509" class="transcript-chunks" onclick="console.log('00:32:19,808'); seek(1939.0)">
              we call semantic search. It can find the specific
            </span>
            
            <span id="chunk-510" class="transcript-chunks" onclick="console.log('00:32:23,104'); seek(1943.0)">
              chunks of text that are very likely to respond my question.
            </span>
            
            <span id="chunk-511" class="transcript-chunks" onclick="console.log('00:32:26,672'); seek(1946.0)">
              You copy those chunks of text into bedrock and
            </span>
            
            <span id="chunk-512" class="transcript-chunks" onclick="console.log('00:32:30,752'); seek(1950.0)">
              then you run your question, plus the combination
            </span>
            
            <span id="chunk-513" class="transcript-chunks" onclick="console.log('00:32:34,618'); seek(1954.0)">
              of chunks of text that has been retrieved from the
            </span>
            
            <span id="chunk-514" class="transcript-chunks" onclick="console.log('00:32:38,324'); seek(1958.0)">
              database, from the vector database, and you send that to your
            </span>
            
            <span id="chunk-515" class="transcript-chunks" onclick="console.log('00:32:41,492'); seek(1961.0)">
              foundational model. Then the foundational model, let's say cloud three,
            </span>
            
            <span id="chunk-516" class="transcript-chunks" onclick="console.log('00:32:44,964'); seek(1964.0)">
              will see all the chunks of text that talks about vacation policy,
            </span>
            
            <span id="chunk-517" class="transcript-chunks" onclick="console.log('00:32:48,184'); seek(1968.0)">
              New York. And you see your question. And then based on the information that you
            </span>
            
            <span id="chunk-518" class="transcript-chunks" onclick="console.log('00:32:52,168'); seek(1972.0)">
              have provided, because now the model has access to
            </span>
            
            <span id="chunk-519" class="transcript-chunks" onclick="console.log('00:32:55,528'); seek(1975.0)">
              the chunks of data that has the answer, will be able to provide an answer
            </span>
            
            <span id="chunk-520" class="transcript-chunks" onclick="console.log('00:32:59,276'); seek(1979.0)">
              for you. That is what is called retrieval augmented generation.
            </span>
            
            <span id="chunk-521" class="transcript-chunks" onclick="console.log('00:33:02,754'); seek(1982.0)">
              And you can actually run very simple with
            </span>
            
            <span id="chunk-522" class="transcript-chunks" onclick="console.log('00:33:08,128'); seek(1988.0)">
              knowledge bases. So that is one capability that
            </span>
            
            <span id="chunk-523" class="transcript-chunks" onclick="console.log('00:33:12,352'); seek(1992.0)">
              you can run. It's all managed for you and you can choose different
            </span>
            
            <span id="chunk-524" class="transcript-chunks" onclick="console.log('00:33:16,016'); seek(1996.0)">
              models to actually run. Another functionality is the
            </span>
            
            <span id="chunk-525" class="transcript-chunks" onclick="console.log('00:33:19,728'); seek(1999.0)">
              ability to enable generative AI applications to execute
            </span>
            
            <span id="chunk-526" class="transcript-chunks" onclick="console.log('00:33:23,498'); seek(2003.0)">
              steps outside your model. So let's say you have an
            </span>
            
            <span id="chunk-527" class="transcript-chunks" onclick="console.log('00:33:27,732'); seek(2007.0)">
              API where if
            </span>
            
            <span id="chunk-528" class="transcript-chunks" onclick="console.log('00:33:31,892'); seek(2011.0)">
              someone on your chat bot asks the question about what is the current
            </span>
            
            <span id="chunk-529" class="transcript-chunks" onclick="console.log('00:33:36,120'); seek(2016.0)">
              price of this stock, right? The model is not going to be able to answer
            </span>
            
            <span id="chunk-530" class="transcript-chunks" onclick="console.log('00:33:40,216'); seek(2020.0)">
              that question. Or probably if he answered that question, it's going to hallucinate,
            </span>
            
            <span id="chunk-531" class="transcript-chunks" onclick="console.log('00:33:43,918'); seek(2023.0)">
              meaning it's not going to be accurate, right? Because the training data
            </span>
            
            <span id="chunk-532" class="transcript-chunks" onclick="console.log('00:33:47,432'); seek(2027.0)">
              from that model was probably months ago or years ago. What you
            </span>
            
            <span id="chunk-533" class="transcript-chunks" onclick="console.log('00:33:51,004'); seek(2031.0)">
              can do on bedrock, you can use agents for bedrock. What allows you to
            </span>
            
            <span id="chunk-534" class="transcript-chunks" onclick="console.log('00:33:54,972'); seek(2034.0)">
              do is to provide. So you select a model, let's say cloud model,
            </span>
            
            <span id="chunk-535" class="transcript-chunks" onclick="console.log('00:33:58,636'); seek(2038.0)">
              you provide the basic of set instructions,
            </span>
            
            <span id="chunk-536" class="transcript-chunks" onclick="console.log('00:34:02,278'); seek(2042.0)">
              then you choose different data sources, maybe different APIs,
            </span>
            
            <span id="chunk-537" class="transcript-chunks" onclick="console.log('00:34:06,670'); seek(2046.0)">
              and then you specify the actions that it can take.
            </span>
            
            <span id="chunk-538" class="transcript-chunks" onclick="console.log('00:34:09,536'); seek(2049.0)">
              So the example I provided, right, you can say if someone asks you
            </span>
            
            <span id="chunk-539" class="transcript-chunks" onclick="console.log('00:34:13,972'); seek(2053.0)">
              about the pricing of a stock,
            </span>
            
            <span id="chunk-540" class="transcript-chunks" onclick="console.log('00:34:17,090'); seek(2057.0)">
              you need to call this API. Here is the open
            </span>
            
            <span id="chunk-541" class="transcript-chunks" onclick="console.log('00:34:21,204'); seek(2061.0)">
              API spec of my API and this is how you can call the API.
            </span>
            
            <span id="chunk-542" class="transcript-chunks" onclick="console.log('00:34:25,454'); seek(2065.0)">
              So what agents for bedrock do you ask? A question
            </span>
            
            <span id="chunk-543" class="transcript-chunks" onclick="console.log('00:34:29,510'); seek(2069.0)">
              for your model. Your model realizes it needs to actually
            </span>
            
            <span id="chunk-544" class="transcript-chunks" onclick="console.log('00:34:32,840'); seek(2072.0)">
              make a action, take an action on that request.
            </span>
            
            <span id="chunk-545" class="transcript-chunks" onclick="console.log('00:34:36,386'); seek(2076.0)">
              Behind the scenes, what bedrock will do, we will actually call Lambda,
            </span>
            
            <span id="chunk-546" class="transcript-chunks" onclick="console.log('00:34:39,954'); seek(2079.0)">
              which is a serverless compute platform
            </span>
            
            <span id="chunk-547" class="transcript-chunks" onclick="console.log('00:34:43,244'); seek(2083.0)">
              with lambda. The model will actually trigger a lambda.
            </span>
            
            <span id="chunk-548" class="transcript-chunks" onclick="console.log('00:34:47,810'); seek(2087.0)">
              The lambda code will already be prebuilt. Behind the scenes you call
            </span>
            
            <span id="chunk-549" class="transcript-chunks" onclick="console.log('00:34:51,952'); seek(2091.0)">
              the API that you have told bedrock to do
            </span>
            
            <span id="chunk-550" class="transcript-chunks" onclick="console.log('00:34:55,920'); seek(2095.0)">
              and then that API will come with a response, let's say the
            </span>
            
            <span id="chunk-551" class="transcript-chunks" onclick="console.log('00:34:59,488'); seek(2099.0)">
              value of your stock. And then you return to the larger language model to provide
            </span>
            
            <span id="chunk-552" class="transcript-chunks" onclick="console.log('00:35:03,332'); seek(2103.0)">
              you with the response. This is just one example, but what you can do with
            </span>
            
            <span id="chunk-553" class="transcript-chunks" onclick="console.log('00:35:06,708'); seek(2106.0)">
              bedrock, you can break down and orchestrate tasks.
            </span>
            
            <span id="chunk-554" class="transcript-chunks" onclick="console.log('00:35:10,850'); seek(2110.0)">
              You can invoke whatever API on your behalf so
            </span>
            
            <span id="chunk-555" class="transcript-chunks" onclick="console.log('00:35:14,644'); seek(2114.0)">
              you can do a lot of automation. And the capabilities
            </span>
            
            <span id="chunk-556" class="transcript-chunks" onclick="console.log('00:35:18,062'); seek(2118.0)">
              here are really infinite. It's just you configuring those
            </span>
            
            <span id="chunk-557" class="transcript-chunks" onclick="console.log('00:35:22,056'); seek(2122.0)">
              agents properly so you can do a lot of chain of thought as well
            </span>
            
            <span id="chunk-558" class="transcript-chunks" onclick="console.log('00:35:25,772'); seek(2125.0)">
              on top of that. So moving on, on the ability that,
            </span>
            
            <span id="chunk-559" class="transcript-chunks" onclick="console.log('00:35:30,890'); seek(2130.0)">
              what are the ability that we have for making
            </span>
            
            <span id="chunk-560" class="transcript-chunks" onclick="console.log('00:35:35,052'); seek(2135.0)">
              the responses very secure and safe? On bedrock
            </span>
            
            <span id="chunk-561" class="transcript-chunks" onclick="console.log('00:35:38,742'); seek(2138.0)">
              we have a functionality that is currently in preview, but it's called guardrails.
            </span>
            
            <span id="chunk-562" class="transcript-chunks" onclick="console.log('00:35:42,966'); seek(2142.0)">
              What guardrails allows you to do is to create consistently
            </span>
            
            <span id="chunk-563" class="transcript-chunks" onclick="console.log('00:35:47,398'); seek(2147.0)">
              safeguards, including on your models. Doesn't matter
            </span>
            
            <span id="chunk-564" class="transcript-chunks" onclick="console.log('00:35:51,328'); seek(2151.0)">
              if they are fine tuned or agents what it does,
            </span>
            
            <span id="chunk-565" class="transcript-chunks" onclick="console.log('00:35:54,500'); seek(2154.0)">
              you can create filters for harmful content both on
            </span>
            
            <span id="chunk-566" class="transcript-chunks" onclick="console.log('00:35:57,828'); seek(2157.0)">
              the input that you're sending to bedrock and also the output that bedrock
            </span>
            
            <span id="chunk-567" class="transcript-chunks" onclick="console.log('00:36:01,898'); seek(2161.0)">
              will tell you, right? So I'll give you an example,
            </span>
            
            <span id="chunk-568" class="transcript-chunks" onclick="console.log('00:36:05,224'); seek(2165.0)">
              right, let's say the example you
            </span>
            
            <span id="chunk-569" class="transcript-chunks" onclick="console.log('00:36:08,408'); seek(2168.0)">
              see here on the screen. Let's say someone asks you about investment
            </span>
            
            <span id="chunk-570" class="transcript-chunks" onclick="console.log('00:36:12,462'); seek(2172.0)">
              and device on your chat bot and you don't want to have that
            </span>
            
            <span id="chunk-571" class="transcript-chunks" onclick="console.log('00:36:16,840'); seek(2176.0)">
              input and actually output to be sent to the customer.
            </span>
            
            <span id="chunk-572" class="transcript-chunks" onclick="console.log('00:36:20,204'); seek(2180.0)">
              Right? So what you can do, you can create those filters and you can
            </span>
            
            <span id="chunk-573" class="transcript-chunks" onclick="console.log('00:36:23,548'); seek(2183.0)">
              say these topics deny and then this
            </span>
            
            <span id="chunk-574" class="transcript-chunks" onclick="console.log('00:36:26,988'); seek(2186.0)">
              is the response you should be giving back if someone is trying to
            </span>
            
            <span id="chunk-575" class="transcript-chunks" onclick="console.log('00:36:30,960'); seek(2190.0)">
              ask questions about investment advice. So you don't get into legal
            </span>
            
            <span id="chunk-576" class="transcript-chunks" onclick="console.log('00:36:34,582'); seek(2194.0)">
              complaints or problems that you might get into the future,
            </span>
            
            <span id="chunk-577" class="transcript-chunks" onclick="console.log('00:36:37,984'); seek(2197.0)">
              right? So this is one of the capabilities that is available
            </span>
            
            <span id="chunk-578" class="transcript-chunks" onclick="console.log('00:36:41,792'); seek(2201.0)">
              on bedrock and it's called guardrails. Another functionality
            </span>
            
            <span id="chunk-579" class="transcript-chunks" onclick="console.log('00:36:46,186'); seek(2206.0)">
              that I'm going to talk about it is batch.
            </span>
            
            <span id="chunk-580" class="transcript-chunks" onclick="console.log('00:36:49,146'); seek(2209.0)">
              So everything I've talked about so far is you just run an
            </span>
            
            <span id="chunk-581" class="transcript-chunks" onclick="console.log('00:36:52,468'); seek(2212.0)">
              API call and you receive a response. Pretty much
            </span>
            
            <span id="chunk-582" class="transcript-chunks" onclick="console.log('00:36:56,200'); seek(2216.0)">
              synchronous, right? API call goes in, API call
            </span>
            
            <span id="chunk-583" class="transcript-chunks" onclick="console.log('00:37:00,216'); seek(2220.0)">
              comes back. There are some use cases that don't require
            </span>
            
            <span id="chunk-584" class="transcript-chunks" onclick="console.log('00:37:04,510'); seek(2224.0)">
              live interaction, but you want to run a lot of
            </span>
            
            <span id="chunk-585" class="transcript-chunks" onclick="console.log('00:37:07,804'); seek(2227.0)">
              inference for a lot of documents in a batch mode.
            </span>
            
            <span id="chunk-586" class="transcript-chunks" onclick="console.log('00:37:11,426'); seek(2231.0)">
              So what Bedrock can do, its batch
            </span>
            
            <span id="chunk-587" class="transcript-chunks" onclick="console.log('00:37:15,266'); seek(2235.0)">
              mode allows you to efficiently run inference on the large volumes
            </span>
            
            <span id="chunk-588" class="transcript-chunks" onclick="console.log('00:37:19,218'); seek(2239.0)">
              of data. So you can put the data on s three, you can put different
            </span>
            
            <span id="chunk-589" class="transcript-chunks" onclick="console.log('00:37:22,496'); seek(2242.0)">
              json files with the prompt and the data you want to run behind the scenes.
            </span>
            
            <span id="chunk-590" class="transcript-chunks" onclick="console.log('00:37:26,262'); seek(2246.0)">
              Bedrock, you grab those files, you run the inference, you save the
            </span>
            
            <span id="chunk-591" class="transcript-chunks" onclick="console.log('00:37:30,528'); seek(2250.0)">
              results of those inference in another s three as the result,
            </span>
            
            <span id="chunk-592" class="transcript-chunks" onclick="console.log('00:37:34,228'); seek(2254.0)">
              and it's completely managed for you. And once
            </span>
            
            <span id="chunk-593" class="transcript-chunks" onclick="console.log('00:37:37,540'); seek(2257.0)">
              the batch is completed, you can get notified and you can do a lot
            </span>
            
            <span id="chunk-594" class="transcript-chunks" onclick="console.log('00:37:41,124'); seek(2261.0)">
              of different automation. So you don't need to write any code for
            </span>
            
            <span id="chunk-595" class="transcript-chunks" onclick="console.log('00:37:44,692'); seek(2264.0)">
              handling failures or restarts. Bedrock would
            </span>
            
            <span id="chunk-596" class="transcript-chunks" onclick="console.log('00:37:48,244'); seek(2268.0)">
              take care of that for you. And you can run that with base foundational
            </span>
            
            <span id="chunk-597" class="transcript-chunks" onclick="console.log('00:37:51,966'); seek(2271.0)">
              models or your custom trainium models
            </span>
            
            <span id="chunk-598" class="transcript-chunks" onclick="console.log('00:37:55,214'); seek(2275.0)">
              as well. One last nice feature
            </span>
            
            <span id="chunk-599" class="transcript-chunks" onclick="console.log('00:37:58,930'); seek(2278.0)">
              about Bedrock is model evaluation. It's still in preview,
            </span>
            
            <span id="chunk-600" class="transcript-chunks" onclick="console.log('00:38:02,754'); seek(2282.0)">
              but model evaluation is a really good feature of Bedrock.
            </span>
            
            <span id="chunk-601" class="transcript-chunks" onclick="console.log('00:38:06,450'); seek(2286.0)">
              As you saw, Bedrock offers you a wide variety of
            </span>
            
            <span id="chunk-602" class="transcript-chunks" onclick="console.log('00:38:09,968'); seek(2289.0)">
              model providers and models available. From those models providers,
            </span>
            
            <span id="chunk-603" class="transcript-chunks" onclick="console.log('00:38:14,358'); seek(2294.0)">
              it can be really complex to evaluate those
            </span>
            
            <span id="chunk-604" class="transcript-chunks" onclick="console.log('00:38:17,648'); seek(2297.0)">
              foundational models and select the best one. So what model evaluation on
            </span>
            
            <span id="chunk-605" class="transcript-chunks" onclick="console.log('00:38:21,668'); seek(2301.0)">
              bedrock allows you to do is to choose different
            </span>
            
            <span id="chunk-606" class="transcript-chunks" onclick="console.log('00:38:25,460'); seek(2305.0)">
              tests. And those evaluation tests can be either automatic
            </span>
            
            <span id="chunk-607" class="transcript-chunks" onclick="console.log('00:38:29,642'); seek(2309.0)">
              benchmarks that the industry use and bedrock
            </span>
            
            <span id="chunk-608" class="transcript-chunks" onclick="console.log('00:38:34,330'); seek(2314.0)">
              makes available, but you can also create your own human evaluation.
            </span>
            
            <span id="chunk-609" class="transcript-chunks" onclick="console.log('00:38:38,030'); seek(2318.0)">
              You can have actually humans evaluating the response from different models
            </span>
            
            <span id="chunk-610" class="transcript-chunks" onclick="console.log('00:38:42,030'); seek(2322.0)">
              and rating those models without actually knowing which model it is.
            </span>
            
            <span id="chunk-611" class="transcript-chunks" onclick="console.log('00:38:45,688'); seek(2325.0)">
              So there is no bias into the place. And you can have
            </span>
            
            <span id="chunk-612" class="transcript-chunks" onclick="console.log('00:38:49,132'); seek(2329.0)">
              your own data sets of questions and you can create your
            </span>
            
            <span id="chunk-613" class="transcript-chunks" onclick="console.log('00:38:52,412'); seek(2332.0)">
              own custom metrics or use
            </span>
            
            <span id="chunk-614" class="transcript-chunks" onclick="console.log('00:38:56,410'); seek(2336.0)">
              the metrics that comes with it. So some of the metrics that are there
            </span>
            
            <span id="chunk-615" class="transcript-chunks" onclick="console.log('00:39:00,384'); seek(2340.0)">
              are the accuracy, are the toxicity and the robustness
            </span>
            
            <span id="chunk-616" class="transcript-chunks" onclick="console.log('00:39:04,326'); seek(2344.0)">
              of the response. And you can see here a screenshot of a
            </span>
            
            <span id="chunk-617" class="transcript-chunks" onclick="console.log('00:39:07,648'); seek(2347.0)">
              human evaluation report across different models being tested
            </span>
            
            <span id="chunk-618" class="transcript-chunks" onclick="console.log('00:39:11,878'); seek(2351.0)">
              and automatic evaluation report. So I've
            </span>
            
            <span id="chunk-619" class="transcript-chunks" onclick="console.log('00:39:15,082'); seek(2355.0)">
              talked a lot about the different features
            </span>
            
            <span id="chunk-620" class="transcript-chunks" onclick="console.log('00:39:18,138'); seek(2358.0)">
              that bedrock makes available for you. But one of
            </span>
            
            <span id="chunk-621" class="transcript-chunks" onclick="console.log('00:39:21,364'); seek(2361.0)">
              the things that is important to highlight is right now,
            </span>
            
            <span id="chunk-622" class="transcript-chunks" onclick="console.log('00:39:25,030'); seek(2365.0)">
              thousands and thousands of customers are using bedrock
            </span>
            
            <span id="chunk-623" class="transcript-chunks" onclick="console.log('00:39:29,086'); seek(2369.0)">
              because the capability, the democratization,
            </span>
            
            <span id="chunk-624" class="transcript-chunks" onclick="console.log('00:39:32,174'); seek(2372.0)">
              the flexibility and the feature set that bedrock allows
            </span>
            
            <span id="chunk-625" class="transcript-chunks" onclick="console.log('00:39:35,998'); seek(2375.0)">
              them to build generative AI on top of pretty much every single
            </span>
            
            <span id="chunk-626" class="transcript-chunks" onclick="console.log('00:39:40,220'); seek(2380.0)">
              industry, right? So you can see big names like Adidas,
            </span>
            
            <span id="chunk-627" class="transcript-chunks" onclick="console.log('00:39:44,114'); seek(2384.0)">
              you can see names like the BMW group,
            </span>
            
            <span id="chunk-628" class="transcript-chunks" onclick="console.log('00:39:47,148'); seek(2387.0)">
              Salesforce and many, many others so highly encourage you to
            </span>
            
            <span id="chunk-629" class="transcript-chunks" onclick="console.log('00:39:50,688'); seek(2390.0)">
              test bedrock because it's a really cool feature. Two more
            </span>
            
            <span id="chunk-630" class="transcript-chunks" onclick="console.log('00:39:54,352'); seek(2394.0)">
              things before we go to the demo is we talked about the
            </span>
            
            <span id="chunk-631" class="transcript-chunks" onclick="console.log('00:39:58,832'); seek(2398.0)">
              retrieval, augmented generation and the need for vector databases.
            </span>
            
            <span id="chunk-632" class="transcript-chunks" onclick="console.log('00:40:03,114'); seek(2403.0)">
              And I just want to quickly tell you the story about vector databases on
            </span>
            
            <span id="chunk-633" class="transcript-chunks" onclick="console.log('00:40:06,692'); seek(2406.0)">
              AWS. AWS has a wide variety of
            </span>
            
            <span id="chunk-634" class="transcript-chunks" onclick="console.log('00:40:10,116'); seek(2410.0)">
              different databases that support vectors. As you can see here,
            </span>
            
            <span id="chunk-635" class="transcript-chunks" onclick="console.log('00:40:14,232'); seek(2414.0)">
              we have six databases that are now
            </span>
            
            <span id="chunk-636" class="transcript-chunks" onclick="console.log('00:40:17,912'); seek(2417.0)">
              supporting vector databases and depending on the use case,
            </span>
            
            <span id="chunk-637" class="transcript-chunks" onclick="console.log('00:40:21,416'); seek(2421.0)">
              you might choose one versus the other. The important thing here
            </span>
            
            <span id="chunk-638" class="transcript-chunks" onclick="console.log('00:40:24,904'); seek(2424.0)">
              is to understand that AWS is giving
            </span>
            
            <span id="chunk-639" class="transcript-chunks" onclick="console.log('00:40:28,460'); seek(2428.0)">
              the flexibility to pick and choose from the
            </span>
            
            <span id="chunk-640" class="transcript-chunks" onclick="console.log('00:40:31,708'); seek(2431.0)">
              database that makes the most sense for you. So a
            </span>
            
            <span id="chunk-641" class="transcript-chunks" onclick="console.log('00:40:35,292'); seek(2435.0)">
              very popular database on AWS for vector is opensearch.
            </span>
            
            <span id="chunk-642" class="transcript-chunks" onclick="console.log('00:40:39,634'); seek(2439.0)">
              So OpenSearch has a functionality for vectors
            </span>
            
            <span id="chunk-643" class="transcript-chunks" onclick="console.log('00:40:42,966'); seek(2442.0)">
              and you can actually even run OpenSearch serverless for Vector
            </span>
            
            <span id="chunk-644" class="transcript-chunks" onclick="console.log('00:40:46,806'); seek(2446.0)">
              database that have a very good performance and price. But you can see here documentDB
            </span>
            
            <span id="chunk-645" class="transcript-chunks" onclick="console.log('00:40:51,322'); seek(2451.0)">
              now has support for vector. MemoryDB for
            </span>
            
            <span id="chunk-646" class="transcript-chunks" onclick="console.log('00:40:54,788'); seek(2454.0)">
              redis has also support for vector RDS for postgres.
            </span>
            
            <span id="chunk-647" class="transcript-chunks" onclick="console.log('00:40:58,314'); seek(2458.0)">
              So if you're running a SQL database and you have a relational
            </span>
            
            <span id="chunk-648" class="transcript-chunks" onclick="console.log('00:41:02,202'); seek(2462.0)">
              use case and you also want to run specific vectors,
            </span>
            
            <span id="chunk-649" class="transcript-chunks" onclick="console.log('00:41:06,398'); seek(2466.0)">
              you can run pgvector which is a plugin library for
            </span>
            
            <span id="chunk-650" class="transcript-chunks" onclick="console.log('00:41:09,960'); seek(2469.0)">
              postgres that can run also on top of both RDs
            </span>
            
            <span id="chunk-651" class="transcript-chunks" onclick="console.log('00:41:13,646'); seek(2473.0)">
              and Aurora postgres. If you're doing graph databases,
            </span>
            
            <span id="chunk-652" class="transcript-chunks" onclick="console.log('00:41:16,974'); seek(2476.0)">
              you can actually run on top of Netun. And as I talked about it
            </span>
            
            <span id="chunk-653" class="transcript-chunks" onclick="console.log('00:41:20,764'); seek(2480.0)">
              right now, the direct integration for knowledge databases on bedrock supports
            </span>
            
            <span id="chunk-654" class="transcript-chunks" onclick="console.log('00:41:25,122'); seek(2485.0)">
              open search, redis, enterprise, cloud and Pinecone.
            </span>
            
            <span id="chunk-655" class="transcript-chunks" onclick="console.log('00:41:28,466'); seek(2488.0)">
              But very soon Aurora, Amazon, Aurora and MongoDB are
            </span>
            
            <span id="chunk-656" class="transcript-chunks" onclick="console.log('00:41:32,368'); seek(2492.0)">
              going to be made available as well. So that is
            </span>
            
            <span id="chunk-657" class="transcript-chunks" onclick="console.log('00:41:35,888'); seek(2495.0)">
              about vector database. The last thing I want to talk about it is the
            </span>
            
            <span id="chunk-658" class="transcript-chunks" onclick="console.log('00:41:40,450'); seek(2500.0)">
              capability for code generation and code assistant for
            </span>
            
            <span id="chunk-659" class="transcript-chunks" onclick="console.log('00:41:43,972'); seek(2503.0)">
              developers. So AWS has a service called code Whisper
            </span>
            
            <span id="chunk-660" class="transcript-chunks" onclick="console.log('00:41:47,882'); seek(2507.0)">
              which is AI powered code suggestion, as you see here
            </span>
            
            <span id="chunk-661" class="transcript-chunks" onclick="console.log('00:41:52,470'); seek(2512.0)">
              on the small video that has actually let me play
            </span>
            
            <span id="chunk-662" class="transcript-chunks" onclick="console.log('00:41:55,768'); seek(2515.0)">
              it again, the small video that is
            </span>
            
            <span id="chunk-663" class="transcript-chunks" onclick="console.log('00:41:59,272'); seek(2519.0)">
              demonstrating here, in this case it's a JavaScript code.
            </span>
            
            <span id="chunk-664" class="transcript-chunks" onclick="console.log('00:42:04,488'); seek(2524.0)">
              You can provide a single comment, in this case, parse the CSV
            </span>
            
            <span id="chunk-665" class="transcript-chunks" onclick="console.log('00:42:08,002'); seek(2528.0)">
              string and return a list of songs with positional or position
            </span>
            
            <span id="chunk-666" class="transcript-chunks" onclick="console.log('00:42:12,092'); seek(2532.0)">
              original chart date, artist title and ignore lines.
            </span>
            
            <span id="chunk-667" class="transcript-chunks" onclick="console.log('00:42:15,778'); seek(2535.0)">
              We're starting with hashtag, right?
            </span>
            
            <span id="chunk-668" class="transcript-chunks" onclick="console.log('00:42:18,972'); seek(2538.0)">
              Then you just click tab and it automatically returns the code generation.
            </span>
            
            <span id="chunk-669" class="transcript-chunks" onclick="console.log('00:42:23,494'); seek(2543.0)">
              This is pretty cool. And the way it works is you just have your
            </span>
            
            <span id="chunk-670" class="transcript-chunks" onclick="console.log('00:42:27,536'); seek(2547.0)">
              ID and there is support for a variety of vs
            </span>
            
            <span id="chunk-671" class="transcript-chunks" onclick="console.log('00:42:31,402'); seek(2551.0)">
              code, jetbrains, cloud, nine, lambda,
            </span>
            
            <span id="chunk-672" class="transcript-chunks" onclick="console.log('00:42:35,034'); seek(2555.0)">
              Jupyter notebooks. There are supports for pretty much all the popular
            </span>
            
            <span id="chunk-673" class="transcript-chunks" onclick="console.log('00:42:39,866'); seek(2559.0)">
              IDs out there. Install the plugin from AWS
            </span>
            
            <span id="chunk-674" class="transcript-chunks" onclick="console.log('00:42:43,946'); seek(2563.0)">
              that has code whisper support, then you can receive code suggestions,
            </span>
            
            <span id="chunk-675" class="transcript-chunks" onclick="console.log('00:42:48,030'); seek(2568.0)">
              and code whisper can actually do more than that. You receive real
            </span>
            
            <span id="chunk-676" class="transcript-chunks" onclick="console.log('00:42:52,136'); seek(2572.0)">
              time code suggestions for a variety of programming languages like
            </span>
            
            <span id="chunk-677" class="transcript-chunks" onclick="console.log('00:42:56,316'); seek(2576.0)">
              Java, JavaScript, go.
            </span>
            
            <span id="chunk-678" class="transcript-chunks" onclick="console.log('00:42:59,210'); seek(2579.0)">
              Net, and many, many others, but not also programming
            </span>
            
            <span id="chunk-679" class="transcript-chunks" onclick="console.log('00:43:02,738'); seek(2582.0)">
              languages. If you're building infrastructure, AWS, a code terraform or cloud formation,
            </span>
            
            <span id="chunk-680" class="transcript-chunks" onclick="console.log('00:43:06,674'); seek(2586.0)">
              you can also have suggestions for those. On top of being
            </span>
            
            <span id="chunk-681" class="transcript-chunks" onclick="console.log('00:43:10,576'); seek(2590.0)">
              an assistant for developers and improving productivity quite significantly,
            </span>
            
            <span id="chunk-682" class="transcript-chunks" onclick="console.log('00:43:15,542'); seek(2595.0)">
              you can also have a security scam. So the code that is being suggested for
            </span>
            
            <span id="chunk-683" class="transcript-chunks" onclick="console.log('00:43:19,632'); seek(2599.0)">
              you can actually give you security suggestions
            </span>
            
            <span id="chunk-684" class="transcript-chunks" onclick="console.log('00:43:23,882'); seek(2603.0)">
              to make sure you're writing actually secure code. And you can also have reference
            </span>
            
            <span id="chunk-685" class="transcript-chunks" onclick="console.log('00:43:28,282'); seek(2608.0)">
              tracker for different licenses on open source on
            </span>
            
            <span id="chunk-686" class="transcript-chunks" onclick="console.log('00:43:32,308'); seek(2612.0)">
              the data that has been trained. So if whatever suggestion is being given
            </span>
            
            <span id="chunk-687" class="transcript-chunks" onclick="console.log('00:43:35,992'); seek(2615.0)">
              to you has been trained from an open
            </span>
            
            <span id="chunk-688" class="transcript-chunks" onclick="console.log('00:43:40,120'); seek(2620.0)">
              source repository that has a potentially prohibitive
            </span>
            
            <span id="chunk-689" class="transcript-chunks" onclick="console.log('00:43:44,142'); seek(2624.0)">
              license, you can actually have that warning telling
            </span>
            
            <span id="chunk-690" class="transcript-chunks" onclick="console.log('00:43:48,722'); seek(2628.0)">
              you. And if you are an enterprise version of code whisper, you could say
            </span>
            
            <span id="chunk-691" class="transcript-chunks" onclick="console.log('00:43:52,188'); seek(2632.0)">
              developers should never receive recommendation for code that has been
            </span>
            
            <span id="chunk-692" class="transcript-chunks" onclick="console.log('00:43:55,692'); seek(2635.0)">
              generated on this specific license that is prohibitive
            </span>
            
            <span id="chunk-693" class="transcript-chunks" onclick="console.log('00:43:59,046'); seek(2639.0)">
              for my business use case one of the great things about
            </span>
            
            <span id="chunk-694" class="transcript-chunks" onclick="console.log('00:44:02,912'); seek(2642.0)">
              code Whisper is code whisper for individuals are
            </span>
            
            <span id="chunk-695" class="transcript-chunks" onclick="console.log('00:44:07,136'); seek(2647.0)">
              free. We are only one of the only companies that have an
            </span>
            
            <span id="chunk-696" class="transcript-chunks" onclick="console.log('00:44:12,050'); seek(2652.0)">
              enterprise grade product that if you're using
            </span>
            
            <span id="chunk-697" class="transcript-chunks" onclick="console.log('00:44:15,828'); seek(2655.0)">
              for an individual user like not
            </span>
            
            <span id="chunk-698" class="transcript-chunks" onclick="console.log('00:44:19,012'); seek(2659.0)">
              a company, you can install codebisper created
            </span>
            
            <span id="chunk-699" class="transcript-chunks" onclick="console.log('00:44:23,242'); seek(2663.0)">
              an AWS builder account. You don't even need to have an AWS
            </span>
            
            <span id="chunk-700" class="transcript-chunks" onclick="console.log('00:44:27,278'); seek(2667.0)">
              account, you just need to have a login with build
            </span>
            
            <span id="chunk-701" class="transcript-chunks" onclick="console.log('00:44:31,422'); seek(2671.0)">
              Id we call builder ID. You can use it for free.
            </span>
            
            <span id="chunk-702" class="transcript-chunks" onclick="console.log('00:44:34,936'); seek(2674.0)">
              Some features are only for enterprise, but most features and most
            </span>
            
            <span id="chunk-703" class="transcript-chunks" onclick="console.log('00:44:38,332'); seek(2678.0)">
              important feature which is code suggestions are actually available
            </span>
            
            <span id="chunk-704" class="transcript-chunks" onclick="console.log('00:44:41,740'); seek(2681.0)">
              for free. So I highly encourage everyone to take a look on this and
            </span>
            
            <span id="chunk-705" class="transcript-chunks" onclick="console.log('00:44:46,508'); seek(2686.0)">
              then hopefully this was a good overview of the
            </span>
            
            <span id="chunk-706" class="transcript-chunks" onclick="console.log('00:44:51,152'); seek(2691.0)">
              offerings on AWS for generative AI, most important for
            </span>
            
            <span id="chunk-707" class="transcript-chunks" onclick="console.log('00:44:55,152'); seek(2695.0)">
              bedrock. So I'll pause here and I'll come back
            </span>
            
            <span id="chunk-708" class="transcript-chunks" onclick="console.log('00:44:58,784'); seek(2698.0)">
              sharing my screen to actually do a presentation and a demo on
            </span>
            
            <span id="chunk-709" class="transcript-chunks" onclick="console.log('00:45:03,970'); seek(2703.0)">
              how can you utilize some of those functionalities in
            </span>
            
            <span id="chunk-710" class="transcript-chunks" onclick="console.log('00:45:08,820'); seek(2708.0)">
              the real world? Actually clicking buttons and making API calls
            </span>
            
            <span id="chunk-711" class="transcript-chunks" onclick="console.log('00:45:12,554'); seek(2712.0)">
              and writing some code. Awesome. So let's quickly
            </span>
            
            <span id="chunk-712" class="transcript-chunks" onclick="console.log('00:45:16,360'); seek(2716.0)">
              jump into the demo. Very simple. I have logged
            </span>
            
            <span id="chunk-713" class="transcript-chunks" onclick="console.log('00:45:19,838'); seek(2719.0)">
              in into my AWS account. I can search here the bedrock
            </span>
            
            <span id="chunk-714" class="transcript-chunks" onclick="console.log('00:45:23,742'); seek(2723.0)">
              service. I'll go and I'll jump inside the bedrock
            </span>
            
            <span id="chunk-715" class="transcript-chunks" onclick="console.log('00:45:27,202'); seek(2727.0)">
              service. Right now bedrock
            </span>
            
            <span id="chunk-716" class="transcript-chunks" onclick="console.log('00:45:31,218'); seek(2731.0)">
              is available in a few AWS regions. In this example we are
            </span>
            
            <span id="chunk-717" class="transcript-chunks" onclick="console.log('00:45:34,748'); seek(2734.0)">
              using North Virginia US east one region. If I click here
            </span>
            
            <span id="chunk-718" class="transcript-chunks" onclick="console.log('00:45:38,476'); seek(2738.0)">
              on my left side you can see the menu, you can
            </span>
            
            <span id="chunk-719" class="transcript-chunks" onclick="console.log('00:45:42,032'); seek(2742.0)">
              see some examples how to get start. You can just open those on
            </span>
            
            <span id="chunk-720" class="transcript-chunks" onclick="console.log('00:45:45,792'); seek(2745.0)">
              playground here. If you click on the provider you can see the
            </span>
            
            <span id="chunk-721" class="transcript-chunks" onclick="console.log('00:45:49,328'); seek(2749.0)">
              providers that I just actually showed to you on the
            </span>
            
            <span id="chunk-722" class="transcript-chunks" onclick="console.log('00:45:52,628'); seek(2752.0)">
              presentation. You can see some of the base models.
            </span>
            
            <span id="chunk-723" class="transcript-chunks" onclick="console.log('00:45:56,138'); seek(2756.0)">
              So each provider, for example entropic here have
            </span>
            
            <span id="chunk-724" class="transcript-chunks" onclick="console.log('00:45:59,412'); seek(2759.0)">
              the cloud models. You can see all the different cloud models that are currently available.
            </span>
            
            <span id="chunk-725" class="transcript-chunks" onclick="console.log('00:46:03,652'); seek(2763.0)">
              So I have for example cloud three sonnet, which is the median model that have
            </span>
            
            <span id="chunk-726" class="transcript-chunks" onclick="console.log('00:46:07,672'); seek(2767.0)">
              just got released this week. Have cloud 2.1, cloud two
            </span>
            
            <span id="chunk-727" class="transcript-chunks" onclick="console.log('00:46:11,800'); seek(2771.0)">
              and cloud 1.2 instant.
            </span>
            
            <span id="chunk-728" class="transcript-chunks" onclick="console.log('00:46:15,990'); seek(2775.0)">
              In this case, I don't have any custom model,
            </span>
            
            <span id="chunk-729" class="transcript-chunks" onclick="console.log('00:46:18,792'); seek(2778.0)">
              but if I had trainium before a custom model,
            </span>
            
            <span id="chunk-730" class="transcript-chunks" onclick="console.log('00:46:22,556'); seek(2782.0)">
              I would see the list of training models here. If I wanted to customize a
            </span>
            
            <span id="chunk-731" class="transcript-chunks" onclick="console.log('00:46:26,444'); seek(2786.0)">
              new model, could just go here, create a new fine tune job,
            </span>
            
            <span id="chunk-732" class="transcript-chunks" onclick="console.log('00:46:29,724'); seek(2789.0)">
              or continue fine tune job. But the thing I want
            </span>
            
            <span id="chunk-733" class="transcript-chunks" onclick="console.log('00:46:33,264'); seek(2793.0)">
              to show you, you can actually get started and play around and test some of
            </span>
            
            <span id="chunk-734" class="transcript-chunks" onclick="console.log('00:46:37,008'); seek(2797.0)">
              the models by just going to the playground. So if you look here, the playground,
            </span>
            
            <span id="chunk-735" class="transcript-chunks" onclick="console.log('00:46:40,630'); seek(2800.0)">
              you have the chat option. And what I really like about the
            </span>
            
            <span id="chunk-736" class="transcript-chunks" onclick="console.log('00:46:44,048'); seek(2804.0)">
              chat option, I'll just give you first example how you can actually talk to a
            </span>
            
            <span id="chunk-737" class="transcript-chunks" onclick="console.log('00:46:47,748'); seek(2807.0)">
              model. Let's say you want to talk to Claude and you want to talk to
            </span>
            
            <span id="chunk-738" class="transcript-chunks" onclick="console.log('00:46:50,692'); seek(2810.0)">
              the new Claude tree model, which is one of the most performance
            </span>
            
            <span id="chunk-739" class="transcript-chunks" onclick="console.log('00:46:54,574'); seek(2814.0)">
              in the industry. So let's just say, write me a poem
            </span>
            
            <span id="chunk-740" class="transcript-chunks" onclick="console.log('00:46:58,862'); seek(2818.0)">
              about AWS and its ecosystem.
            </span>
            
            <span id="chunk-741" class="transcript-chunks" onclick="console.log('00:47:02,750'); seek(2822.0)">
              Just a simple poem here.
            </span>
            
            <span id="chunk-742" class="transcript-chunks" onclick="console.log('00:47:06,024'); seek(2826.0)">
              So I can put the entry here. Because this is a multi modality
            </span>
            
            <span id="chunk-743" class="transcript-chunks" onclick="console.log('00:47:09,598'); seek(2829.0)">
              model, I can also put an image, I'll do a demo of an image
            </span>
            
            <span id="chunk-744" class="transcript-chunks" onclick="console.log('00:47:13,154'); seek(2833.0)">
              in a moment. You can see all the configurations of the hyperparameters,
            </span>
            
            <span id="chunk-745" class="transcript-chunks" onclick="console.log('00:47:16,626'); seek(2836.0)">
              like temperature, top P, top K. The length
            </span>
            
            <span id="chunk-746" class="transcript-chunks" onclick="console.log('00:47:20,230'); seek(2840.0)">
              of the output can be controlled here. In this case I'm just
            </span>
            
            <span id="chunk-747" class="transcript-chunks" onclick="console.log('00:47:23,888'); seek(2843.0)">
              keeping for 2000 tokens maximum.
            </span>
            
            <span id="chunk-748" class="transcript-chunks" onclick="console.log('00:47:27,398'); seek(2847.0)">
              You can see this on demand. If I click run,
            </span>
            
            <span id="chunk-749" class="transcript-chunks" onclick="console.log('00:47:31,550'); seek(2851.0)">
              it's actually calling the model and then it's actually generating,
            </span>
            
            <span id="chunk-750" class="transcript-chunks" onclick="console.log('00:47:35,462'); seek(2855.0)">
              in this case generating a poem for AWS and its ecosystem.
            </span>
            
            <span id="chunk-751" class="transcript-chunks" onclick="console.log('00:47:39,146'); seek(2859.0)">
              Right. You can see that it's pretty cool. One thing that I really like
            </span>
            
            <span id="chunk-752" class="transcript-chunks" onclick="console.log('00:47:42,836'); seek(2862.0)">
              about the playground are the following, as it's finishing
            </span>
            
            <span id="chunk-753" class="transcript-chunks" onclick="console.log('00:47:46,382'); seek(2866.0)">
              generating here, if we click on the three dots on the top menu,
            </span>
            
            <span id="chunk-754" class="transcript-chunks" onclick="console.log('00:47:51,110'); seek(2871.0)">
              you can export it as JSON and you can see streaming preference because
            </span>
            
            <span id="chunk-755" class="transcript-chunks" onclick="console.log('00:47:54,728'); seek(2874.0)">
              you are streaming. But the other thing that I like, you can
            </span>
            
            <span id="chunk-756" class="transcript-chunks" onclick="console.log('00:47:58,428'); seek(2878.0)">
              go down and you can see some model metrics. So you can see this actually
            </span>
            
            <span id="chunk-757" class="transcript-chunks" onclick="console.log('00:48:02,060'); seek(2882.0)">
              took 15,000
            </span>
            
            <span id="chunk-758" class="transcript-chunks" onclick="console.log('00:48:05,852'); seek(2885.0)">
              milliseconds. You tell me how many input tokens, how many
            </span>
            
            <span id="chunk-759" class="transcript-chunks" onclick="console.log('00:48:09,756'); seek(2889.0)">
              output tokens, and this is the cost, it's 0.0.
            </span>
            
            <span id="chunk-760" class="transcript-chunks" onclick="console.log('00:48:12,848'); seek(2892.0)">
              Because it's less than 0.0, there is like be a zero point
            </span>
            
            <span id="chunk-761" class="transcript-chunks" onclick="console.log('00:48:16,816'); seek(2896.0)">
              something that this will cost. Right. What I really like about
            </span>
            
            <span id="chunk-762" class="transcript-chunks" onclick="console.log('00:48:20,592'); seek(2900.0)">
              it on the chat AWS, well, we can compare models. So let's say I want
            </span>
            
            <span id="chunk-763" class="transcript-chunks" onclick="console.log('00:48:23,828'); seek(2903.0)">
              to compare claw three versus claw 2.1,
            </span>
            
            <span id="chunk-764" class="transcript-chunks" onclick="console.log('00:48:27,460'); seek(2907.0)">
              right? And I'm going to talk about it here. Let's see.
            </span>
            
            <span id="chunk-765" class="transcript-chunks" onclick="console.log('00:48:31,010'); seek(2911.0)">
              Talk about the
            </span>
            
            <span id="chunk-766" class="transcript-chunks" onclick="console.log('00:48:34,760'); seek(2914.0)">
              word economy in the 99
            </span>
            
            <span id="chunk-767" class="transcript-chunks" onclick="console.log('00:48:39,304'); seek(2919.0)">
              year, right? And I can go and I can run. So it's
            </span>
            
            <span id="chunk-768" class="transcript-chunks" onclick="console.log('00:48:42,654'); seek(2922.0)">
              going to run both models at the same time and I will
            </span>
            
            <span id="chunk-769" class="transcript-chunks" onclick="console.log('00:48:46,248'); seek(2926.0)">
              be able to compare the performance of both models, this is just like
            </span>
            
            <span id="chunk-770" class="transcript-chunks" onclick="console.log('00:48:49,932'); seek(2929.0)">
              by reading them. So let's just wait a little bit.
            </span>
            
            <span id="chunk-771" class="transcript-chunks" onclick="console.log('00:48:53,452'); seek(2933.0)">
              So you can see here he has outputted. So cloud 2.1
            </span>
            
            <span id="chunk-772" class="transcript-chunks" onclick="console.log('00:48:57,052'); seek(2937.0)">
              has outputted. I can see here, compare the response,
            </span>
            
            <span id="chunk-773" class="transcript-chunks" onclick="console.log('00:49:00,758'); seek(2940.0)">
              but I can see down below here how many tokens each one of
            </span>
            
            <span id="chunk-774" class="transcript-chunks" onclick="console.log('00:49:03,968'); seek(2943.0)">
              them had and so forth. Now you also have
            </span>
            
            <span id="chunk-775" class="transcript-chunks" onclick="console.log('00:49:07,248'); seek(2947.0)">
              a text playground instead of a chat. It's just like you
            </span>
            
            <span id="chunk-776" class="transcript-chunks" onclick="console.log('00:49:10,752'); seek(2950.0)">
              send one request and you get the response. What I like about this,
            </span>
            
            <span id="chunk-777" class="transcript-chunks" onclick="console.log('00:49:14,756'); seek(2954.0)">
              you go here and you select the model. Let me show you what I like
            </span>
            
            <span id="chunk-778" class="transcript-chunks" onclick="console.log('00:49:17,652'); seek(2957.0)">
              about it. And let's say write a small poem about
            </span>
            
            <span id="chunk-779" class="transcript-chunks" onclick="console.log('00:49:21,620'); seek(2961.0)">
              New York City, right? Let's just run this.
            </span>
            
            <span id="chunk-780" class="transcript-chunks" onclick="console.log('00:49:24,612'); seek(2964.0)">
              What I really like about this. So it's streaming back. But the best
            </span>
            
            <span id="chunk-781" class="transcript-chunks" onclick="console.log('00:49:28,200'); seek(2968.0)">
              thing about it, if I click on the three dots, I can actually see the
            </span>
            
            <span id="chunk-782" class="transcript-chunks" onclick="console.log('00:49:31,832'); seek(2971.0)">
              API request. And this is actually how I
            </span>
            
            <span id="chunk-783" class="transcript-chunks" onclick="console.log('00:49:35,528'); seek(2975.0)">
              would actually call this model through API. Right? In this
            </span>
            
            <span id="chunk-784" class="transcript-chunks" onclick="console.log('00:49:39,612'); seek(2979.0)">
              case it's using AWS CLI, but you can see the message here and
            </span>
            
            <span id="chunk-785" class="transcript-chunks" onclick="console.log('00:49:43,228'); seek(2983.0)">
              all the formats get properly configured for me.
            </span>
            
            <span id="chunk-786" class="transcript-chunks" onclick="console.log('00:49:46,252'); seek(2986.0)">
              And in a moment I'll show you some python code on how you can actually
            </span>
            
            <span id="chunk-787" class="transcript-chunks" onclick="console.log('00:49:50,000'); seek(2990.0)">
              do that. Few more things I want to quickly show.
            </span>
            
            <span id="chunk-788" class="transcript-chunks" onclick="console.log('00:49:52,944'); seek(2992.0)">
              If you want to generate images, you can actually generate images with
            </span>
            
            <span id="chunk-789" class="transcript-chunks" onclick="console.log('00:49:57,710'); seek(2997.0)">
              stable diffusion stability, AI and Amazon Titan. So I can say
            </span>
            
            <span id="chunk-790" class="transcript-chunks" onclick="console.log('00:50:01,748'); seek(3001.0)">
              create an image of a cat in the moon.
            </span>
            
            <span id="chunk-791" class="transcript-chunks" onclick="console.log('00:50:05,890'); seek(3005.0)">
              Let's just ask this for the
            </span>
            
            <span id="chunk-792" class="transcript-chunks" onclick="console.log('00:50:09,108'); seek(3009.0)">
              model. Let's see what actually outputs for me.
            </span>
            
            <span id="chunk-793" class="transcript-chunks" onclick="console.log('00:50:12,630'); seek(3012.0)">
              And then you could do whatever you want with that
            </span>
            
            <span id="chunk-794" class="transcript-chunks" onclick="console.log('00:50:16,072'); seek(3016.0)">
              image, right? So you can see it's cat with the moon. There's very simple
            </span>
            
            <span id="chunk-795" class="transcript-chunks" onclick="console.log('00:50:20,280'); seek(3020.0)">
              image. We can say create picture
            </span>
            
            <span id="chunk-796" class="transcript-chunks" onclick="console.log('00:50:25,950'); seek(3025.0)">
              of a cat that is super realistic.
            </span>
            
            <span id="chunk-797" class="transcript-chunks" onclick="console.log('00:50:30,018'); seek(3030.0)">
              Let's see if it does more like instead of you saw, there was
            </span>
            
            <span id="chunk-798" class="transcript-chunks" onclick="console.log('00:50:33,452'); seek(3033.0)">
              more like a paint with the moon in the background.
            </span>
            
            <span id="chunk-799" class="transcript-chunks" onclick="console.log('00:50:36,674'); seek(3036.0)">
              Let's see if this does. And this is what I'm doing here is just prompt
            </span>
            
            <span id="chunk-800" class="transcript-chunks" onclick="console.log('00:50:39,558'); seek(3039.0)">
              engineering. I'm not using anything specific. There you go. You can see an image
            </span>
            
            <span id="chunk-801" class="transcript-chunks" onclick="console.log('00:50:43,398'); seek(3043.0)">
              here that is a more realistic cat image,
            </span>
            
            <span id="chunk-802" class="transcript-chunks" onclick="console.log('00:50:46,806'); seek(3046.0)">
              right? Remember I talked about some of the other features
            </span>
            
            <span id="chunk-803" class="transcript-chunks" onclick="console.log('00:50:50,838'); seek(3050.0)">
              like guardrails. You can see the guardrails here, you can create the
            </span>
            
            <span id="chunk-804" class="transcript-chunks" onclick="console.log('00:50:54,144'); seek(3054.0)">
              guardrails, you can create the knowledge base, you can
            </span>
            
            <span id="chunk-805" class="transcript-chunks" onclick="console.log('00:50:57,588'); seek(3057.0)">
              create all the agents. Those are the things you can do. One of the things
            </span>
            
            <span id="chunk-806" class="transcript-chunks" onclick="console.log('00:51:01,012'); seek(3061.0)">
              I want to highlight, if you were to start using Bedrock for the first time,
            </span>
            
            <span id="chunk-807" class="transcript-chunks" onclick="console.log('00:51:04,536'); seek(3064.0)">
              the first thing I would recommend you doing is actually going model access
            </span>
            
            <span id="chunk-808" class="transcript-chunks" onclick="console.log('00:51:08,488'); seek(3068.0)">
              and enabling those models to have access on your account.
            </span>
            
            <span id="chunk-809" class="transcript-chunks" onclick="console.log('00:51:13,590'); seek(3073.0)">
              You don't pay anything for just enabling them, but if you
            </span>
            
            <span id="chunk-810" class="transcript-chunks" onclick="console.log('00:51:16,808'); seek(3076.0)">
              don't enable them, you can't use, and it's as simple as just going on manage
            </span>
            
            <span id="chunk-811" class="transcript-chunks" onclick="console.log('00:51:20,460'); seek(3080.0)">
              model access, selecting the models you want. In my account you can see
            </span>
            
            <span id="chunk-812" class="transcript-chunks" onclick="console.log('00:51:23,708'); seek(3083.0)">
              I have access to all those models, right? So it's pretty straightforward.
            </span>
            
            <span id="chunk-813" class="transcript-chunks" onclick="console.log('00:51:27,530'); seek(3087.0)">
              But now let's jump in into some code, right?
            </span>
            
            <span id="chunk-814" class="transcript-chunks" onclick="console.log('00:51:30,704'); seek(3090.0)">
              Likely most people that are here watching
            </span>
            
            <span id="chunk-815" class="transcript-chunks" onclick="console.log('00:51:34,304'); seek(3094.0)">
              my session are probably developers or people that do code.
            </span>
            
            <span id="chunk-816" class="transcript-chunks" onclick="console.log('00:51:38,384'); seek(3098.0)">
              So how can I actually call those models on bedrock using
            </span>
            
            <span id="chunk-817" class="transcript-chunks" onclick="console.log('00:51:42,750'); seek(3102.0)">
              a programmatic way. So the first example here I'll show you is just calling
            </span>
            
            <span id="chunk-818" class="transcript-chunks" onclick="console.log('00:51:46,948'); seek(3106.0)">
              cloud, right? So you can see I import bodo tree which is the
            </span>
            
            <span id="chunk-819" class="transcript-chunks" onclick="console.log('00:51:50,788'); seek(3110.0)">
              AWS ssdk for python. And then I
            </span>
            
            <span id="chunk-820" class="transcript-chunks" onclick="console.log('00:51:54,632'); seek(3114.0)">
              instantiated the bedrock runtime from the SDK. You can
            </span>
            
            <span id="chunk-821" class="transcript-chunks" onclick="console.log('00:51:58,024'); seek(3118.0)">
              see the bedrock runtime in the region. Here is the payload.
            </span>
            
            <span id="chunk-822" class="transcript-chunks" onclick="console.log('00:52:01,406'); seek(3121.0)">
              So I'm providing the model version.
            </span>
            
            <span id="chunk-823" class="transcript-chunks" onclick="console.log('00:52:05,582'); seek(3125.0)">
              So this is quadrisoned then the body.
            </span>
            
            <span id="chunk-824" class="transcript-chunks" onclick="console.log('00:52:08,984'); seek(3128.0)">
              Each model have a specific body and format that the model
            </span>
            
            <span id="chunk-825" class="transcript-chunks" onclick="console.log('00:52:13,020'); seek(3133.0)">
              providers have configured. And you can see that in the documentation.
            </span>
            
            <span id="chunk-826" class="transcript-chunks" onclick="console.log('00:52:16,610'); seek(3136.0)">
              I can show you the link in a moment. But once you have actually this,
            </span>
            
            <span id="chunk-827" class="transcript-chunks" onclick="console.log('00:52:22,590'); seek(3142.0)">
              you create a model. In this case you create a prompt. In this case just
            </span>
            
            <span id="chunk-828" class="transcript-chunks" onclick="console.log('00:52:26,224'); seek(3146.0)">
              saying write a text about going to the moon
            </span>
            
            <span id="chunk-829" class="transcript-chunks" onclick="console.log('00:52:29,558'); seek(3149.0)">
              and its technical challenges. Right? Then I create
            </span>
            
            <span id="chunk-830" class="transcript-chunks" onclick="console.log('00:52:33,970'); seek(3153.0)">
              that payload into JSON and then finally call the single API that
            </span>
            
            <span id="chunk-831" class="transcript-chunks" onclick="console.log('00:52:38,004'); seek(3158.0)">
              I've talked to you about, bedrock. So we always call bedrock
            </span>
            
            <span id="chunk-832" class="transcript-chunks" onclick="console.log('00:52:41,946'); seek(3161.0)">
              invoke model. And this is not using streaming. I'll show any
            </span>
            
            <span id="chunk-833" class="transcript-chunks" onclick="console.log('00:52:45,752'); seek(3165.0)">
              streaming version in a moment. And then I wait for the response.
            </span>
            
            <span id="chunk-834" class="transcript-chunks" onclick="console.log('00:52:49,870'); seek(3169.0)">
              I parse the response into JSON and then I print a response
            </span>
            
            <span id="chunk-835" class="transcript-chunks" onclick="console.log('00:52:53,422'); seek(3173.0)">
              where exactly the text from the
            </span>
            
            <span id="chunk-836" class="transcript-chunks" onclick="console.log('00:52:56,968'); seek(3176.0)">
              response is. So if I go on and run pythoncloud
            </span>
            
            <span id="chunk-837" class="transcript-chunks" onclick="console.log('00:53:00,674'); seek(3180.0)">
              py, I'm going to call behind the scenes that's actually
            </span>
            
            <span id="chunk-838" class="transcript-chunks" onclick="console.log('00:53:04,396'); seek(3184.0)">
              calling bedrock, sending the payload that I request to claw
            </span>
            
            <span id="chunk-839" class="transcript-chunks" onclick="console.log('00:53:08,274'); seek(3188.0)">
              tree. Running the prompt. Remember my prompt is talked about the
            </span>
            
            <span id="chunk-840" class="transcript-chunks" onclick="console.log('00:53:12,224'); seek(3192.0)">
              technical challenges about going to the moon. I write a text about
            </span>
            
            <span id="chunk-841" class="transcript-chunks" onclick="console.log('00:53:15,632'); seek(3195.0)">
              that. So once he actually returns the text from
            </span>
            
            <span id="chunk-842" class="transcript-chunks" onclick="console.log('00:53:19,550'); seek(3199.0)">
              bedrock then I will be able to just see the text. And there
            </span>
            
            <span id="chunk-843" class="transcript-chunks" onclick="console.log('00:53:23,264'); seek(3203.0)">
              you go. You see it wasn't streaming. So it's a pretty long
            </span>
            
            <span id="chunk-844" class="transcript-chunks" onclick="console.log('00:53:26,436'); seek(3206.0)">
              text saying embarking on a journey to the moon. Present multitude of
            </span>
            
            <span id="chunk-845" class="transcript-chunks" onclick="console.log('00:53:29,972'); seek(3209.0)">
              technical challenges. Not going to evaluate this but you get the
            </span>
            
            <span id="chunk-846" class="transcript-chunks" onclick="console.log('00:53:33,588'); seek(3213.0)">
              gist, right. So this is example one. The second example is calling
            </span>
            
            <span id="chunk-847" class="transcript-chunks" onclick="console.log('00:53:37,480'); seek(3217.0)">
              the same bedrock API but for a different model.
            </span>
            
            <span id="chunk-848" class="transcript-chunks" onclick="console.log('00:53:40,840'); seek(3220.0)">
              So you can see here I'm also invocating a bedrock.
            </span>
            
            <span id="chunk-849" class="transcript-chunks" onclick="console.log('00:53:45,750'); seek(3225.0)">
              And then I'm just saying can you write me a poem about apples?
            </span>
            
            <span id="chunk-850" class="transcript-chunks" onclick="console.log('00:53:49,250'); seek(3229.0)">
              Right? So let's just call this python
            </span>
            
            <span id="chunk-851" class="transcript-chunks" onclick="console.log('00:53:52,338'); seek(3232.0)">
              three titan Ui. So now this is calling the Amazon
            </span>
            
            <span id="chunk-852" class="transcript-chunks" onclick="console.log('00:53:55,842'); seek(3235.0)">
              Titan text model. And you can see it was very simple poem
            </span>
            
            <span id="chunk-853" class="transcript-chunks" onclick="console.log('00:54:00,082'); seek(3240.0)">
              about apple. Now there might be applications that
            </span>
            
            <span id="chunk-854" class="transcript-chunks" onclick="console.log('00:54:03,744'); seek(3243.0)">
              you're trying to build that require streaming. Like a chatbot, you don't want to make
            </span>
            
            <span id="chunk-855" class="transcript-chunks" onclick="console.log('00:54:07,296'); seek(3247.0)">
              the user waiting for, I don't know, like a minute to get
            </span>
            
            <span id="chunk-856" class="transcript-chunks" onclick="console.log('00:54:11,152'); seek(3251.0)">
              a response back. Sometimes those models take a while to finalize
            </span>
            
            <span id="chunk-857" class="transcript-chunks" onclick="console.log('00:54:15,066'); seek(3255.0)">
              the whole text completion so you can do streaming. So in this case,
            </span>
            
            <span id="chunk-858" class="transcript-chunks" onclick="console.log('00:54:18,724'); seek(3258.0)">
              very similar to what I've done before. This is a demonstration of
            </span>
            
            <span id="chunk-859" class="transcript-chunks" onclick="console.log('00:54:21,972'); seek(3261.0)">
              using clotry sonnet, but with a streaming right.
            </span>
            
            <span id="chunk-860" class="transcript-chunks" onclick="console.log('00:54:25,192'); seek(3265.0)">
              So I'm not using multimodality yet. This is just text. So I have
            </span>
            
            <span id="chunk-861" class="transcript-chunks" onclick="console.log('00:54:29,064'); seek(3269.0)">
              an input text and you see this just creating
            </span>
            
            <span id="chunk-862" class="transcript-chunks" onclick="console.log('00:54:33,590'); seek(3273.0)">
              the input payload. Then you can see here the API
            </span>
            
            <span id="chunk-863" class="transcript-chunks" onclick="console.log('00:54:37,358'); seek(3277.0)">
              that I call is just a little bit different. It is the invoke model with
            </span>
            
            <span id="chunk-864" class="transcript-chunks" onclick="console.log('00:54:40,748'); seek(3280.0)">
              response streaming. So what bedrock does,
            </span>
            
            <span id="chunk-865" class="transcript-chunks" onclick="console.log('00:54:43,932'); seek(3283.0)">
              as soon as you start receiving some chunks of text from the model,
            </span>
            
            <span id="chunk-866" class="transcript-chunks" onclick="console.log('00:54:47,324'); seek(3287.0)">
              we actually output back for you. And then here it's just like,
            </span>
            
            <span id="chunk-867" class="transcript-chunks" onclick="console.log('00:54:50,556'); seek(3290.0)">
              as you get the response, just display that for me, just do a print
            </span>
            
            <span id="chunk-868" class="transcript-chunks" onclick="console.log('00:54:54,262'); seek(3294.0)">
              on the console for me. And then on my main here, I'm finally
            </span>
            
            <span id="chunk-869" class="transcript-chunks" onclick="console.log('00:54:58,048'); seek(3298.0)">
              providing some model IDs and providing the prompt. In this case,
            </span>
            
            <span id="chunk-870" class="transcript-chunks" onclick="console.log('00:55:01,828'); seek(3301.0)">
              what can you tell me about the, what can you tell me about
            </span>
            
            <span id="chunk-871" class="transcript-chunks" onclick="console.log('00:55:06,130'); seek(3306.0)">
              brazilian economy? And then I'm
            </span>
            
            <span id="chunk-872" class="transcript-chunks" onclick="console.log('00:55:10,058'); seek(3310.0)">
              just starting the border tree with bedrock and then calling the
            </span>
            
            <span id="chunk-873" class="transcript-chunks" onclick="console.log('00:55:13,928'); seek(3313.0)">
              function that I created above. So if you go here,
            </span>
            
            <span id="chunk-874" class="transcript-chunks" onclick="console.log('00:55:17,080'); seek(3317.0)">
              clean the screen and you go cloud streaming,
            </span>
            
            <span id="chunk-875" class="transcript-chunks" onclick="console.log('00:55:23,350'); seek(3323.0)">
              and we try to run, oh,
            </span>
            
            <span id="chunk-876" class="transcript-chunks" onclick="console.log('00:55:26,120'); seek(3326.0)">
              sorry, python three, apologies for that. Python three cloud
            </span>
            
            <span id="chunk-877" class="transcript-chunks" onclick="console.log('00:55:29,848'); seek(3329.0)">
              streaming. So it's invoking my row and you can see now it's streaming the
            </span>
            
            <span id="chunk-878" class="transcript-chunks" onclick="console.log('00:55:33,788'); seek(3333.0)">
              response back and it's actually getting the response about the brazilian
            </span>
            
            <span id="chunk-879" class="transcript-chunks" onclick="console.log('00:55:37,138'); seek(3337.0)">
              economy. And you can see here it finalized.
            </span>
            
            <span id="chunk-880" class="transcript-chunks" onclick="console.log('00:55:40,034'); seek(3340.0)">
              So I even predicting, like, okay, why it stopped
            </span>
            
            <span id="chunk-881" class="transcript-chunks" onclick="console.log('00:55:43,318'); seek(3343.0)">
              because it was end of the turn. It finalized the response and also how
            </span>
            
            <span id="chunk-882" class="transcript-chunks" onclick="console.log('00:55:46,688'); seek(3346.0)">
              many output tokens you got. So that is just one example. The other example
            </span>
            
            <span id="chunk-883" class="transcript-chunks" onclick="console.log('00:55:50,928'); seek(3350.0)">
              is I want to use clotrisonet because it's a multimodal
            </span>
            
            <span id="chunk-884" class="transcript-chunks" onclick="console.log('00:55:55,250'); seek(3355.0)">
              large language model that also accepts
            </span>
            
            <span id="chunk-885" class="transcript-chunks" onclick="console.log('00:55:58,410'); seek(3358.0)">
              images as inputs. So what I want to do, I have this image
            </span>
            
            <span id="chunk-886" class="transcript-chunks" onclick="console.log('00:56:02,122'); seek(3362.0)">
              of a very cute cat. I want to provide this image to the model.
            </span>
            
            <span id="chunk-887" class="transcript-chunks" onclick="console.log('00:56:05,752'); seek(3365.0)">
              And you see here what I'm doing. Very similar again,
            </span>
            
            <span id="chunk-888" class="transcript-chunks" onclick="console.log('00:56:09,830'); seek(3369.0)">
              but now what I'm actually doing, I'm receiving the cat image.
            </span>
            
            <span id="chunk-889" class="transcript-chunks" onclick="console.log('00:56:13,998'); seek(3373.0)">
              I'm encoding that on base 64. Then I'm providing
            </span>
            
            <span id="chunk-890" class="transcript-chunks" onclick="console.log('00:56:18,142'); seek(3378.0)">
              on my messages for clot tree as a content. You can see I'm
            </span>
            
            <span id="chunk-891" class="transcript-chunks" onclick="console.log('00:56:21,874'); seek(3381.0)">
              providing now an image and a text. So the first I'm providing the image
            </span>
            
            <span id="chunk-892" class="transcript-chunks" onclick="console.log('00:56:25,922'); seek(3385.0)">
              AWS base 64 mastering. And then I'm saying as the
            </span>
            
            <span id="chunk-893" class="transcript-chunks" onclick="console.log('00:56:29,564'); seek(3389.0)">
              prompt, write me a detailed description of this photo and
            </span>
            
            <span id="chunk-894" class="transcript-chunks" onclick="console.log('00:56:32,768'); seek(3392.0)">
              then upon talking about it. So that's the request. And if you see down
            </span>
            
            <span id="chunk-895" class="transcript-chunks" onclick="console.log('00:56:36,352'); seek(3396.0)">
              below here, I'm invoking the model. So this is not using
            </span>
            
            <span id="chunk-896" class="transcript-chunks" onclick="console.log('00:56:40,000'); seek(3400.0)">
              streaming. Remember the example before was using streaming. In this case it's
            </span>
            
            <span id="chunk-897" class="transcript-chunks" onclick="console.log('00:56:44,038'); seek(3404.0)">
              not using streaming. So it's going to send everything, it's going to process the
            </span>
            
            <span id="chunk-898" class="transcript-chunks" onclick="console.log('00:56:47,428'); seek(3407.0)">
              response. Once the response is finalized it's actually going to show me and
            </span>
            
            <span id="chunk-899" class="transcript-chunks" onclick="console.log('00:56:51,252'); seek(3411.0)">
              it's going to just print the result. Let's just quickly do
            </span>
            
            <span id="chunk-900" class="transcript-chunks" onclick="console.log('00:56:54,728'); seek(3414.0)">
              this prod multimodality again.
            </span>
            
            <span id="chunk-901" class="transcript-chunks" onclick="console.log('00:56:58,968'); seek(3418.0)">
              I keep using Python two instead of Python three.
            </span>
            
            <span id="chunk-902" class="transcript-chunks" onclick="console.log('00:57:02,216'); seek(3422.0)">
              Apologies for that. Now I'm going to run Python three.
            </span>
            
            <span id="chunk-903" class="transcript-chunks" onclick="console.log('00:57:06,170'); seek(3426.0)">
              Hopefully this is going to start printing
            </span>
            
            <span id="chunk-904" class="transcript-chunks" onclick="console.log('00:57:11,074'); seek(3431.0)">
              the whole description. And you can see here the image show
            </span>
            
            <span id="chunk-905" class="transcript-chunks" onclick="console.log('00:57:14,732'); seek(3434.0)">
              a close up portrait of a cat with striking green eyes and a sweet
            </span>
            
            <span id="chunk-906" class="transcript-chunks" onclick="console.log('00:57:18,882'); seek(3438.0)">
              brownish gray fur coat. The cat says face a
            </span>
            
            <span id="chunk-907" class="transcript-chunks" onclick="console.log('00:57:22,272'); seek(3442.0)">
              slightly stern, yet alert and yet alert and
            </span>
            
            <span id="chunk-908" class="transcript-chunks" onclick="console.log('00:57:26,032'); seek(3446.0)">
              attentive expression. So it talks about the cat in the image. It's very accurate.
            </span>
            
            <span id="chunk-909" class="transcript-chunks" onclick="console.log('00:57:30,006'); seek(3450.0)">
              And then like I said, he writes a poem, emerald depth gaze.
            </span>
            
            <span id="chunk-910" class="transcript-chunks" onclick="console.log('00:57:33,478'); seek(3453.0)">
              So king blah blah blah blah blah, he talks about it. So you can see
            </span>
            
            <span id="chunk-911" class="transcript-chunks" onclick="console.log('00:57:36,980'); seek(3456.0)">
              bedrock is amazing because with very simple API
            </span>
            
            <span id="chunk-912" class="transcript-chunks" onclick="console.log('00:57:40,282'); seek(3460.0)">
              calls, I can call different models with different configurations with different
            </span>
            
            <span id="chunk-913" class="transcript-chunks" onclick="console.log('00:57:43,892'); seek(3463.0)">
              I parameters. And this is pay as you go. All what I've done
            </span>
            
            <span id="chunk-914" class="transcript-chunks" onclick="console.log('00:57:47,352'); seek(3467.0)">
              here is probably less than a penny because it's all
            </span>
            
            <span id="chunk-915" class="transcript-chunks" onclick="console.log('00:57:50,488'); seek(3470.0)">
              on demand. I'm not paying for any provisioned capacity because I don't need
            </span>
            
            <span id="chunk-916" class="transcript-chunks" onclick="console.log('00:57:54,296'); seek(3474.0)">
              in this example. Last thing is I'll recommend if you want
            </span>
            
            <span id="chunk-917" class="transcript-chunks" onclick="console.log('00:57:57,708'); seek(3477.0)">
              to look for some of the code that I've used. I based myself
            </span>
            
            <span id="chunk-918" class="transcript-chunks" onclick="console.log('00:58:01,740'); seek(3481.0)">
              on this GitHub public repository called Amazon
            </span>
            
            <span id="chunk-919" class="transcript-chunks" onclick="console.log('00:58:05,874'); seek(3485.0)">
              bedrock samples. You can go here introduction to bedrock and
            </span>
            
            <span id="chunk-920" class="transcript-chunks" onclick="console.log('00:58:09,708'); seek(3489.0)">
              you can see some examples. For example cloud tree. You can see the
            </span>
            
            <span id="chunk-921" class="transcript-chunks" onclick="console.log('00:58:13,008'); seek(3493.0)">
              example for cloud know with the image. This is the one that I've used
            </span>
            
            <span id="chunk-922" class="transcript-chunks" onclick="console.log('00:58:17,008'); seek(3497.0)">
              so highly recommend you get in there. And last, if you really want to
            </span>
            
            <span id="chunk-923" class="transcript-chunks" onclick="console.log('00:58:20,624'); seek(3500.0)">
              look in more in detail into each model and the hyperparameters on
            </span>
            
            <span id="chunk-924" class="transcript-chunks" onclick="console.log('00:58:23,908'); seek(3503.0)">
              how you call, you can go on the AWS bedrock documentation.
            </span>
            
            <span id="chunk-925" class="transcript-chunks" onclick="console.log('00:58:27,738'); seek(3507.0)">
              Within the foundational model submenu we have the model inference
            </span>
            
            <span id="chunk-926" class="transcript-chunks" onclick="console.log('00:58:31,722'); seek(3511.0)">
              parameters and when you click on specific models,
            </span>
            
            <span id="chunk-927" class="transcript-chunks" onclick="console.log('00:58:34,938'); seek(3514.0)">
              for example cloud, you can see the different cloud completion and
            </span>
            
            <span id="chunk-928" class="transcript-chunks" onclick="console.log('00:58:38,548'); seek(3518.0)">
              cloud messages API. On the messages you can see here, you can see
            </span>
            
            <span id="chunk-929" class="transcript-chunks" onclick="console.log('00:58:42,372'); seek(3522.0)">
              some code examples. So you have a very descriptive documentation for
            </span>
            
            <span id="chunk-930" class="transcript-chunks" onclick="console.log('00:58:46,156'); seek(3526.0)">
              you as a developer to actually take a look and deep dive.
            </span>
            
            <span id="chunk-931" class="transcript-chunks" onclick="console.log('00:58:49,986'); seek(3529.0)">
              So that is all I had to show for today. Hopefully it was very
            </span>
            
            <span id="chunk-932" class="transcript-chunks" onclick="console.log('00:58:53,356'); seek(3533.0)">
              useful. Feel free to connect with me via LinkedIn on Twitter if you have any
            </span>
            
            <span id="chunk-933" class="transcript-chunks" onclick="console.log('00:58:56,988'); seek(3536.0)">
              questions. And happy coding, happy genai applications
            </span>
            
            <span id="chunk-934" class="transcript-chunks" onclick="console.log('00:59:00,786'); seek(3540.0)">
              and I hope you find this useful. Thank you so much.
            </span>
            
            </div>
          </div>
          
          

          
          <div class="col-12 mb-5">
            <h3>
              Slides
            </h3>
            <iframe src="https://conf42.github.io/static/slides/Samuel%20Baruffi%20-%20Conf42%20Cloud%20Native%202024.pdf" width="100%" height="500px"></iframe>
            <a href="https://conf42.github.io/static/slides/Samuel%20Baruffi%20-%20Conf42%20Cloud%20Native%202024.pdf" class="btn btn-xs btn-info shadow lift" style="background-color: #7B2726;" target="_blank">
              <i class="fe fe-paperclip me-2"></i>
              Download slides (PDF)
            </a>
          </div>
          

          <div class="col-12 mb-2 text-center">
            <div class="text-center mb-5">
              <a href="https://www.conf42.com/cloud2024" class="btn btn-sm btn-danger shadow lift" style="background-color: #7B2726;">
                <i class="fe fe-grid me-2"></i>
                See all 47 talks at this event!
              </a>
            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- PHOTO -->
    <section class="pt-8 pb-6">
      <div class="container">

        <div class="row align-items-center">
          <div class="col-12 col-md-6 col-lg-7">

            <div class="mb-8 mb-md-0">

              <!-- Image -->
              <img src="https://conf42.github.io/static/headshots/Samuel%20Baruffi_cloud.png" alt="..." class="screenshot img-fluid mw-md-110 float-end me-md-6 mb-6 mb-md-0">

            </div>

          </div>
          <div class="col-12 col-md-6 col-lg-5">

            <!-- List -->
            <div class="d-flex">

              <!-- Body -->
              <div class="ms-5">

                <!-- Author 1 -->
                <h2 class="me-2">
                  Samuel Baruffi
                </h2>
                <h3 class="me-2">
                  <span class="text-muted">
                    Senior Global Solutions Architect @ AWS
                  </span>
                </h3>

                <p class="text-uppercase text-muted me-2 mb-3">
                  
                  <a href="https://www.linkedin.com/in/samuelbaruffi/" target="_blank" class="mr-3">
                    <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="Samuel Baruffi's LinkedIn account" />
                  </a>
                  
                  
                  <a href="https://twitter.com/@SamuelBaruffi" target="_blank">
                    <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="Samuel Baruffi's twitter account" />
                  </a>
                  
                </p>
                

                <br />

                <a
                  href="https://twitter.com/share?ref_src=twsrc%5Etfw"
                  class="twitter-share-button"

                  data-text="Check out this talk by @SamuelBaruffi"
                  data-url="https://www.conf42.com/cloud2024"
                  data-via="conf42com"
                  data-related=""
                  data-show-count="false"
                >
                  Tweet
                </a>
                <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

                <br />

                <script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
                <script type="IN/Share" data-url="https://www.conf42.com/cloud2024"></script>
              </div>

            </div>
          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>






    <!-- WELCOME -->
    <section class="pt-8 pt-md-11 pb-10 pb-md-15 bg-info" id="register">

      <!-- Shape -->
      <div class="shape shape-blur-3 text-white">
        <svg viewBox="0 0 1738 487" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h1420.92s713.43 457.505 0 485.868C707.502 514.231 0 0 0 0z" fill="url(#paint0_linear)"/><defs><linearGradient id="paint0_linear" x1="0" y1="0" x2="1049.98" y2="912.68" gradientUnits="userSpaceOnUse"><stop stop-color="currentColor" stop-opacity=".075"/><stop offset="1" stop-color="currentColor" stop-opacity="0"/></linearGradient></defs></svg>      </div>

      <!-- Content -->
      <div class="container">
        <div class="row justify-content-center">
          <div class="col-12 col-md-10 col-lg-8 text-center">

            <!-- Heading -->
            <h1 class="display-2 text-white">
              Join the community!
            </h1>

            <!-- Text -->
            <p class="lead text-white text-opacity-80 mb-6 mb-md-8">
              Learn for free, join the best tech learning community 
              for a <a class="text-white" href="https://www.reddit.com/r/sanfrancisco/comments/1bz90f6/why_are_coffee_shops_in_sf_so_expensive/" target="_blank">price of a pumpkin latte</a>.
            </p>

            <!-- Form -->
            <form class="d-flex align-items-center justify-content-center mb-7 mb-md-9">

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Annual
              </span>

              <!-- Switch -->
              <div class="form-check form-check-dark form-switch mx-3">
                <input class="form-check-input" type="checkbox" id="billingSwitch" data-toggle="price" data-target=".price">
              </div>

              <!-- Label -->
              <span class="text-white text-opacity-80">
                Monthly
              </span>

            </form>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->

    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x text-light">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- PRICING -->
    <section class="mt-n8 mt-md-n15">
      <div class="container">
        <div class="row gx-4">
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-1">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Newsletter</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="0" data-monthly="0">0</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Event notifications, weekly newsletter
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Delayed access</b> to all content
                  </p>
                </div>
              
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Immediate access to Keynotes & Panels
                  </p>
                </div>
              
              
              </div>
            </div>

            <!-- Card -->
            <div class="card shadow-lg mb-6 border border-success">
              <div class="card-body">

                <script>
    function gtag_report_conversion(url) {
      var callback = function () {
        if (typeof(url) != 'undefined') {
          window.location = url;
        }
      };
      gtag('event', 'conversion', {
          'send_to': 'AW-882275635/jLVTCPbt1N8CELPq2aQD',
          'event_callback': callback
      });
      return false;
    }
</script>

<!-- Form -->
<link rel="stylesheet" href="https://emailoctopus.com/bundles/emailoctopuslist/css/1.6/form.css">
<p class="emailoctopus-success-message text-success"></p>
<p class="emailoctopus-error-message text-danger"></p>
<form
    action="https://emailoctopus.com/lists/a3ba0cb5-7524-11eb-a3d0-06b4694bee2a/members/embedded/1.3/add"
    method="post"
    data-message-success="Thanks! Check your email for further directions!"
    data-message-missing-email-address="Your email address is required."
    data-message-invalid-email-address="Your email address looks incorrect, please try again."
    data-message-bot-submission-error="This doesn't look like a human submission."
    data-message-consent-required="Please check the checkbox to indicate your consent."
    data-message-invalid-parameters-error="This form has missing or invalid fields."
    data-message-unknown-error="Sorry, an unknown error has occurred. Please try again later."
    class="emailoctopus-form"
    data-sitekey="6LdYsmsUAAAAAPXVTt-ovRsPIJ_IVhvYBBhGvRV6"
>
<div class="form-floating emailoctopus-form-row">
    <input type="email" class="form-control form-control-flush" name="field_0" id="field_0" placeholder="Email" required>
    <label for="field_0">Email address</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_1" id="field_1" placeholder="First Name" required>
    <label for="field_1">First Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_2" id="field_2" placeholder="Last Name" required>
    <label for="field_2">Last Name</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_4" id="field_4" placeholder="Company" required>
    <label for="field_4">Company</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_5" id="field_5" placeholder="Job Title" required>
    <label for="field_5">Job Title</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <input type="text" class="form-control form-control-flush" name="field_3" id="field_3" placeholder="Phone">
    <label for="field_3">Phone Number</label>
</div>
<div class="form-floating emailoctopus-form-row">
    <select type="text" class="form-control form-control-flush" name="field_7" id="country-source" required
    oninput="updateCountry()"
    >
    <!-- Country names and Country Name -->
    <option value="">Please select your country</option>
    <option value="Afghanistan">Afghanistan</option>
    <option value="Aland Islands">Aland Islands</option>
    <option value="Albania">Albania</option>
    <option value="Algeria">Algeria</option>
    <option value="American Samoa">American Samoa</option>
    <option value="Andorra">Andorra</option>
    <option value="Angola">Angola</option>
    <option value="Anguilla">Anguilla</option>
    <option value="Antarctica">Antarctica</option>
    <option value="Antigua and Barbuda">Antigua and Barbuda</option>
    <option value="Argentina">Argentina</option>
    <option value="Armenia">Armenia</option>
    <option value="Aruba">Aruba</option>
    <option value="Australia">Australia</option>
    <option value="Austria">Austria</option>
    <option value="Azerbaijan">Azerbaijan</option>
    <option value="Bahamas">Bahamas</option>
    <option value="Bahrain">Bahrain</option>
    <option value="Bangladesh">Bangladesh</option>
    <option value="Barbados">Barbados</option>
    <option value="Belarus">Belarus</option>
    <option value="Belgium">Belgium</option>
    <option value="Belize">Belize</option>
    <option value="Benin">Benin</option>
    <option value="Bermuda">Bermuda</option>
    <option value="Bhutan">Bhutan</option>
    <option value="Bolivia">Bolivia</option>
    <option value="Bonaire, Sint Eustatius and Saba">Bonaire, Sint Eustatius and Saba</option>
    <option value="Bosnia and Herzegovina">Bosnia and Herzegovina</option>
    <option value="Botswana">Botswana</option>
    <option value="Bouvet Island">Bouvet Island</option>
    <option value="Brazil">Brazil</option>
    <option value="British Indian Ocean Territory">British Indian Ocean Territory</option>
    <option value="Brunei Darussalam">Brunei Darussalam</option>
    <option value="Bulgaria">Bulgaria</option>
    <option value="Burkina Faso">Burkina Faso</option>
    <option value="Burundi">Burundi</option>
    <option value="Cambodia">Cambodia</option>
    <option value="Cameroon">Cameroon</option>
    <option value="Canada">Canada</option>
    <option value="Cape Verde">Cape Verde</option>
    <option value="Cayman Islands">Cayman Islands</option>
    <option value="Central African Republic">Central African Republic</option>
    <option value="Chad">Chad</option>
    <option value="Chile">Chile</option>
    <option value="China">China</option>
    <option value="Christmas Island">Christmas Island</option>
    <option value="Cocos (Keeling) Islands">Cocos (Keeling) Islands</option>
    <option value="Colombia">Colombia</option>
    <option value="Comoros">Comoros</option>
    <option value="Congo">Congo</option>
    <option value="Congo, Democratic Republic of the Congo">Congo, Democratic Republic of the Congo</option>
    <option value="Cook Islands">Cook Islands</option>
    <option value="Costa Rica">Costa Rica</option>
    <option value="Cote D'Ivoire">Cote D'Ivoire</option>
    <option value="Croatia">Croatia</option>
    <option value="Cuba">Cuba</option>
    <option value="Curacao">Curacao</option>
    <option value="Cyprus">Cyprus</option>
    <option value="Czech Republic">Czech Republic</option>
    <option value="Denmark">Denmark</option>
    <option value="Djibouti">Djibouti</option>
    <option value="Dominica">Dominica</option>
    <option value="Dominican Republic">Dominican Republic</option>
    <option value="Ecuador">Ecuador</option>
    <option value="Egypt">Egypt</option>
    <option value="El Salvador">El Salvador</option>
    <option value="Equatorial Guinea">Equatorial Guinea</option>
    <option value="Eritrea">Eritrea</option>
    <option value="Estonia">Estonia</option>
    <option value="Ethiopia">Ethiopia</option>
    <option value="Falkland Islands (Malvinas)">Falkland Islands (Malvinas)</option>
    <option value="Faroe Islands">Faroe Islands</option>
    <option value="Fiji">Fiji</option>
    <option value="Finland">Finland</option>
    <option value="France">France</option>
    <option value="French Guiana">French Guiana</option>
    <option value="French Polynesia">French Polynesia</option>
    <option value="French Southern Territories">French Southern Territories</option>
    <option value="Gabon">Gabon</option>
    <option value="Gambia">Gambia</option>
    <option value="Georgia">Georgia</option>
    <option value="Germany">Germany</option>
    <option value="Ghana">Ghana</option>
    <option value="Gibraltar">Gibraltar</option>
    <option value="Greece">Greece</option>
    <option value="Greenland">Greenland</option>
    <option value="Grenada">Grenada</option>
    <option value="Guadeloupe">Guadeloupe</option>
    <option value="Guam">Guam</option>
    <option value="Guatemala">Guatemala</option>
    <option value="Guernsey">Guernsey</option>
    <option value="Guinea">Guinea</option>
    <option value="Guinea-Bissau">Guinea-Bissau</option>
    <option value="Guyana">Guyana</option>
    <option value="Haiti">Haiti</option>
    <option value="Heard Island and Mcdonald Islands">Heard Island and Mcdonald Islands</option>
    <option value="Holy See (Vatican City State)">Holy See (Vatican City State)</option>
    <option value="Honduras">Honduras</option>
    <option value="Hong Kong">Hong Kong</option>
    <option value="Hungary">Hungary</option>
    <option value="Iceland">Iceland</option>
    <option value="India">India</option>
    <option value="Indonesia">Indonesia</option>
    <option value="Iran, Islamic Republic of">Iran, Islamic Republic of</option>
    <option value="Iraq">Iraq</option>
    <option value="Ireland">Ireland</option>
    <option value="Isle of Man">Isle of Man</option>
    <option value="Israel">Israel</option>
    <option value="Italy">Italy</option>
    <option value="Jamaica">Jamaica</option>
    <option value="Japan">Japan</option>
    <option value="Jersey">Jersey</option>
    <option value="Jordan">Jordan</option>
    <option value="Kazakhstan">Kazakhstan</option>
    <option value="Kenya">Kenya</option>
    <option value="Kiribati">Kiribati</option>
    <option value="Korea, Democratic People's Republic of">Korea, Democratic People's Republic of</option>
    <option value="Korea, Republic of">Korea, Republic of</option>
    <option value="Kosovo">Kosovo</option>
    <option value="Kuwait">Kuwait</option>
    <option value="Kyrgyzstan">Kyrgyzstan</option>
    <option value="Lao People's Democratic Republic">Lao People's Democratic Republic</option>
    <option value="Latvia">Latvia</option>
    <option value="Lebanon">Lebanon</option>
    <option value="Lesotho">Lesotho</option>
    <option value="Liberia">Liberia</option>
    <option value="Libyan Arab Jamahiriya">Libyan Arab Jamahiriya</option>
    <option value="Liechtenstein">Liechtenstein</option>
    <option value="Lithuania">Lithuania</option>
    <option value="Luxembourg">Luxembourg</option>
    <option value="Macao">Macao</option>
    <option value="Macedonia, the Former Yugoslav Republic of">Macedonia, the Former Yugoslav Republic of</option>
    <option value="Madagascar">Madagascar</option>
    <option value="Malawi">Malawi</option>
    <option value="Malaysia">Malaysia</option>
    <option value="Maldives">Maldives</option>
    <option value="Mali">Mali</option>
    <option value="Malta">Malta</option>
    <option value="Marshall Islands">Marshall Islands</option>
    <option value="Martinique">Martinique</option>
    <option value="Mauritania">Mauritania</option>
    <option value="Mauritius">Mauritius</option>
    <option value="Mayotte">Mayotte</option>
    <option value="Mexico">Mexico</option>
    <option value="Micronesia, Federated States of">Micronesia, Federated States of</option>
    <option value="Moldova, Republic of">Moldova, Republic of</option>
    <option value="Monaco">Monaco</option>
    <option value="Mongolia">Mongolia</option>
    <option value="Montenegro">Montenegro</option>
    <option value="Montserrat">Montserrat</option>
    <option value="Morocco">Morocco</option>
    <option value="Mozambique">Mozambique</option>
    <option value="Myanmar">Myanmar</option>
    <option value="Namibia">Namibia</option>
    <option value="Nauru">Nauru</option>
    <option value="Nepal">Nepal</option>
    <option value="Netherlands">Netherlands</option>
    <option value="Netherlands Antilles">Netherlands Antilles</option>
    <option value="New Caledonia">New Caledonia</option>
    <option value="New Zealand">New Zealand</option>
    <option value="Nicaragua">Nicaragua</option>
    <option value="Niger">Niger</option>
    <option value="Nigeria">Nigeria</option>
    <option value="Niue">Niue</option>
    <option value="Norfolk Island">Norfolk Island</option>
    <option value="Northern Mariana Islands">Northern Mariana Islands</option>
    <option value="Norway">Norway</option>
    <option value="Oman">Oman</option>
    <option value="Pakistan">Pakistan</option>
    <option value="Palau">Palau</option>
    <option value="Palestinian Territory, Occupied">Palestinian Territory, Occupied</option>
    <option value="Panama">Panama</option>
    <option value="Papua New Guinea">Papua New Guinea</option>
    <option value="Paraguay">Paraguay</option>
    <option value="Peru">Peru</option>
    <option value="Philippines">Philippines</option>
    <option value="Pitcairn">Pitcairn</option>
    <option value="Poland">Poland</option>
    <option value="Portugal">Portugal</option>
    <option value="Puerto Rico">Puerto Rico</option>
    <option value="Qatar">Qatar</option>
    <option value="Reunion">Reunion</option>
    <option value="Romania">Romania</option>
    <option value="Russian Federation">Russian Federation</option>
    <option value="Rwanda">Rwanda</option>
    <option value="Saint Barthelemy">Saint Barthelemy</option>
    <option value="Saint Helena">Saint Helena</option>
    <option value="Saint Kitts and Nevis">Saint Kitts and Nevis</option>
    <option value="Saint Lucia">Saint Lucia</option>
    <option value="Saint Martin">Saint Martin</option>
    <option value="Saint Pierre and Miquelon">Saint Pierre and Miquelon</option>
    <option value="Saint Vincent and the Grenadines">Saint Vincent and the Grenadines</option>
    <option value="Samoa">Samoa</option>
    <option value="San Marino">San Marino</option>
    <option value="Sao Tome and Principe">Sao Tome and Principe</option>
    <option value="Saudi Arabia">Saudi Arabia</option>
    <option value="Senegal">Senegal</option>
    <option value="Serbia">Serbia</option>
    <option value="Serbia and Montenegro">Serbia and Montenegro</option>
    <option value="Seychelles">Seychelles</option>
    <option value="Sierra Leone">Sierra Leone</option>
    <option value="Singapore">Singapore</option>
    <option value="Sint Maarten">Sint Maarten</option>
    <option value="Slovakia">Slovakia</option>
    <option value="Slovenia">Slovenia</option>
    <option value="Solomon Islands">Solomon Islands</option>
    <option value="Somalia">Somalia</option>
    <option value="South Africa">South Africa</option>
    <option value="South Georgia and the South Sandwich Islands">South Georgia and the South Sandwich Islands</option>
    <option value="South Sudan">South Sudan</option>
    <option value="Spain">Spain</option>
    <option value="Sri Lanka">Sri Lanka</option>
    <option value="Sudan">Sudan</option>
    <option value="Suriname">Suriname</option>
    <option value="Svalbard and Jan Mayen">Svalbard and Jan Mayen</option>
    <option value="Swaziland">Swaziland</option>
    <option value="Sweden">Sweden</option>
    <option value="Switzerland">Switzerland</option>
    <option value="Syrian Arab Republic">Syrian Arab Republic</option>
    <option value="Taiwan, Province of China">Taiwan, Province of China</option>
    <option value="Tajikistan">Tajikistan</option>
    <option value="Tanzania, United Republic of">Tanzania, United Republic of</option>
    <option value="Thailand">Thailand</option>
    <option value="Timor-Leste">Timor-Leste</option>
    <option value="Togo">Togo</option>
    <option value="Tokelau">Tokelau</option>
    <option value="Tonga">Tonga</option>
    <option value="Trinidad and Tobago">Trinidad and Tobago</option>
    <option value="Tunisia">Tunisia</option>
    <option value="Turkey">Turkey</option>
    <option value="Turkmenistan">Turkmenistan</option>
    <option value="Turks and Caicos Islands">Turks and Caicos Islands</option>
    <option value="Tuvalu">Tuvalu</option>
    <option value="Uganda">Uganda</option>
    <option value="Ukraine">Ukraine</option>
    <option value="United Arab Emirates">United Arab Emirates</option>
    <option value="United Kingdom">United Kingdom</option>
    <option value="United States">United States</option>
    <option value="United States Minor Outlying Islands">United States Minor Outlying Islands</option>
    <option value="Uruguay">Uruguay</option>
    <option value="Uzbekistan">Uzbekistan</option>
    <option value="Vanuatu">Vanuatu</option>
    <option value="Venezuela">Venezuela</option>
    <option value="Viet Nam">Viet Nam</option>
    <option value="Virgin Islands, British">Virgin Islands, British</option>
    <option value="Virgin Islands, U.s.">Virgin Islands, U.s.</option>
    <option value="Wallis and Futuna">Wallis and Futuna</option>
    <option value="Western Sahara">Western Sahara</option>
    <option value="Yemen">Yemen</option>
    <option value="Zambia">Zambia</option>
    <option value="Zimbabwe">Zimbabwe</option>
    </select>
    <label for="field_7">Country</label>
</div>
<input id="country-destination" name="field_7" type="hidden">
<input id="tz-country" name="field_8" type="hidden">

<input
    name="field_6"
    type="hidden"
    value="Cloud Native"
>

<div class="emailoctopus-form-row-consent">
    <input
    type="checkbox"
    id="consent"
    name="consent"
    >
    <label for="consent">
    I consent to the following terms:
    </label>
    <a href="https://www.conf42.com/terms-and-conditions.pdf" target="_blank">
    Terms and Conditions
    </a>
    &amp;
    <a href="./code-of-conduct" target="_blank">
    Code of Conduct
    </a>
</div>
<div
    aria-hidden="true"
    class="emailoctopus-form-row-hp"
>
    <input
    type="text"
    name="hpc4b27b6e-eb38-11e9-be00-06b4694bee2a"
    tabindex="-1"
    autocomplete="nope"
    >
</div>
<div class="mt-6 emailoctopus-form-row-subscribe">
    <input
    type="hidden"
    name="successRedirectUrl"
    >
    <button class="btn w-100 btn-success lift" type="submit" onclick="gtag_report_conversion(); rdt('track', 'SignUp');">
    Subscribe to free newsletter <i class="fe fe-arrow-right ms-3"></i>
    </button>
</div>
</form>

<!-- <script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-recaptcha.js"></script> -->
<script src="https://emailoctopus.com/bundles/emailoctopuslist/js/1.6/form-embed.js"></script>

              </div>
            </div>
          </div>
          <div class="col-12 col-md-6">

            <!-- Card -->
            <div class="card shadow-lg mb-6 mb-md-0">
              <div class="card-body">

                <!-- Preheading -->
                <div class="text-center mb-3">
                  <span class="badge rounded-pill bg-primary-soft">
                    <span class="h6 text-uppercase">Community</span>
                  </span>
                </div>

                <!-- Price -->
                <div class="d-flex justify-content-center">
                  <span class="h2 mb-0 mt-2">$</span>
                  <span class="price display-2 mb-0" data-annual="8.34" data-monthly="10">8.34</span>
                  <span class="h2 align-self-end mb-1">/mo</span>
                </div>

                <!-- Text -->
                <p class="text-center text-muted mb-5">
                </p>

                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Access to <a href="https://conf42.circle.so/">Circle community platform</a>
                  </p>
                </div>

                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <b>Immediate access</b> to all content
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank"><b>Live events!</b></a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    <a href="https://conf42.circle.so/c/live-events/" target="_blank">Regular office hours, Q&As, CV reviews</a>
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Courses, quizes & certificates
                  </p>
                </div>
                
                <div class="d-flex">
                  <div class="badge badge-rounded-circle bg-success-soft mt-1 me-4">
                    <i class="fe fe-check"></i>
                  </div>
                  <p>
                    Community chats
                  </p>
                </div>
                

                <!-- Button -->
                <a href="https://conf42.circle.so/checkout/subscribe" class="btn w-100 btn-primary">
                  Join the community (7 day free trial)<i class="fe fe-arrow-right ms-3"></i>
                </a>

              </div>
            </div>

          </div>
        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </section>

    <!-- SHAPE -->
    <div class="position-relative">
      <div class="shape shape-bottom shape-fluid-x svg-shim text-dark">
        <svg viewBox="0 0 2880 48" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 48h2880V0h-720C1442.5 52 720 0 720 0H0v48z" fill="currentColor"/></svg>      </div>
    </div>

    <!-- FOOTER -->
    <footer class="py-8 py-md-11 bg-dark">
      <div class="container">
        <div class="row">

          <div class="col-12 col-md-4 col-lg-3">
            <!-- Brand -->
            <img src="./assets/conf42/conf42_logo_white_small.png" alt="..." class="footer-brand img-fluid mb-2">
    
            <!-- Text -->
            <p class="text-gray-700 mb-2">
              Online tech events
            </p>
    
            <!-- Social -->
            <ul class="list-unstyled list-inline list-social mb-5">
              <li class="list-inline-item list-social-item me-3">
                <a href="https://www.linkedin.com/company/49110720/" class="text-decoration-none">
                  <img src="./assets/img/icons/social/linkedin.svg" class="list-social-icon" alt="...">
                </a>
              </li>
              <li class="list-inline-item list-social-item me-3">
                <a href="https://twitter.com/conf42com" class="text-decoration-none">
                  <img src="./assets/img/icons/social/twitter.svg" class="list-social-icon" alt="...">
                </a>
              </li>
            </ul>

            <!-- QR Code -->
            <img src="./assets/conf42/CONF42.QR.png" style="width: 100px;" class="mb-5 img-fluid" />
          </div>


          <div class="col-12 col-md-4 col-lg-3">
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2025
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2025">
                  DevOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2025">
                  Python 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2025">
                  Chaos Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2025">
                  Cloud Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2025">
                  Large Language Models (LLMs) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2025">
                  Golang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2025">
                  Site Reliability Engineering (SRE) 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2025">
                  Machine Learning 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2025">
                  Observability 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2025">
                  Quantum Computing 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2025">
                  Rustlang 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2025">
                  Platform Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mlops2025">
                  MLOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2025">
                  Incident Management 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2025">
                  Kube Native 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2025">
                  JavaScript 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2025">
                  Prompt Engineering 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/robotics2025">
                  Robotics 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2025">
                  DevSecOps 2025
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2025">
                  Internet of Things (IoT) 2025
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2024
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2024">
                  DevOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2024">
                  Chaos Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2024">
                  Python 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2024">
                  Cloud Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/llms2024">
                  Large Language Models (LLMs) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2024">
                  Golang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2024">
                  Site Reliability Engineering (SRE) 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2024">
                  Machine Learning 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2024">
                  Observability 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2024">
                  Quantum Computing 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2024">
                  Rustlang 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2024">
                  Platform Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2024">
                  Kube Native 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2024">
                  Incident Management 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2024">
                  JavaScript 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/prompt2024">
                  Prompt Engineering 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2024">
                  DevSecOps 2024
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2024">
                  Internet of Things (IoT) 2024
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2023
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devops2023">
                  DevOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2023">
                  Chaos Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2023">
                  Python 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2023">
                  Cloud Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2023">
                  Golang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2023">
                  Site Reliability Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2023">
                  Machine Learning 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/obs2023">
                  Observability 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2023">
                  Quantum Computing 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2023">
                  Rustlang 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/platform2023">
                  Platform Engineering 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2023">
                  Kube Native 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2023">
                  Incident Management 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2023">
                  JavaScript 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2023">
                  DevSecOps 2023
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/iot2023">
                  Internet of Things (IoT) 2023
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2022
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2022">
                  Python 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/mobile2022">
                  Mobile 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2022">
                  Chaos Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2022">
                  Golang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2022">
                  Cloud Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2022">
                  Machine Learning 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2022">
                  Site Reliability Engineering 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/quantum2022">
                  Quantum Computing 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/rustlang2022">
                  Rustlang 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/im2022">
                  Incident Management 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/kubenative2022">
                  Kube Native 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2022">
                  JavaScript 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2022">
                  DevSecOps 2022
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/web2022">
                  Web 3.0 2022
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2021
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2021">
                  Chaos Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/enterprise2021">
                  Enterprise Software 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/cloud2021">
                  Cloud Native 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/python2021">
                  Python 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/golang2021">
                  Golang 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ml2021">
                  Machine Learning 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2021">
                  Site Reliability Engineering 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2021">
                  JavaScript 2021
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/devsecops2021">
                  DevSecOps 2021
                </a>
              </li>
            
            </ul>
          
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Events 2020
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-6 mb-md-8 mb-lg-0">
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/ce2020">
                  Chaos Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/oss2020">
                  Open Source Showcase 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/sre2020">
                  Site Reliability Engineering 2020
                </a>
              </li>
            
              <li class="mb-3">
                <a class="text-reset" href="https://www.conf42.com/js2020">
                  JavaScript 2020
                </a>
              </li>
            
            </ul>
          
          </div>

          
          <div class="col-12 col-md-4 offset-md-4 col-lg-3 offset-lg-0">

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Community
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./support" class="text-reset">
                  Support us
                </a>
              </li>
              <li class="mb-3">
                <a href="./speakers" class="text-reset">
                  Speakers
                </a>
              </li>
              <li class="mb-3">
                <a href="./hall-of-fame" class="text-reset">
                  Hall of fame
                </a>
              </li>
              <li class="mb-3">
                <a href="https://discord.gg/DnyHgrC7jC" class="text-reset" target="_blank">
                  Discord
                </a>
              </li>
              <li class="mb-3">
                <a href="./about" class="text-reset">
                  About the team
                </a>
              </li>
            </ul>

            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Sponsors
            </h6>
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./sponsor" class="text-reset" target="_blank">
                  Sponsorship
                </a>
              </li>
              <li class="mb-3">
                <a href="mailto:mark@conf42.com?subject=We would like to sponsor" class="text-reset" target="_blank">
                  Request the Prospectus
                </a>
              </li>
              <li class="mb-3">
                <a href="https://drive.google.com/drive/folders/1tT2lspLQgj3sdfxG9FwDVkBUt-TYSPGe?usp=sharing" class="text-reset" target="_blank">
                  Media kit
                </a>
              </li>
            </ul>
    
          </div>


          <div class="col-12 col-md-4 col-lg-3">
    
            <!-- Heading -->
            <h6 class="fw-bold text-uppercase text-gray-700">
              Legal
            </h6>
    
            <!-- List -->
            <ul class="list-unstyled text-muted mb-0">
              <li class="mb-3">
                <a href="./code-of-conduct" class="text-reset">
                  Code of Conduct
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/terms-and-conditions.pdf" class="text-reset" target="_blank">
                  Terms and Conditions
                </a>
              </li>
              <li class="mb-3">
                <a href="https://www.conf42.com/privacy-policy.pdf" class="text-reset" target="_blank">
                  Privacy policy
                </a>
              </li>
            </ul>
          </div>


        </div> <!-- / .row -->
      </div> <!-- / .container -->
    </footer>

    <!-- JAVASCRIPT -->
    <!-- Map JS -->
    <script src='https://api.mapbox.com/mapbox-gl-js/v0.53.0/mapbox-gl.js'></script>
    
    <!-- Vendor JS -->
    <script src="./assets/js/vendor.bundle.js"></script>
    
    <!-- Theme JS -->
    <script src="./assets/js/theme.bundle.js"></script>

    <!-- Various JS -->
    <script src="./assets/js/various.js"></script>

    <script src='https://cdn.jsdelivr.net/npm/@widgetbot/crate@3' async defer>
      new Crate({
          notifications: true,
          indicator: true,
          server: '814240231606714368', // Conf42.com
          channel: '814240231788249115' // #community
      })
    </script>
  </body>
</html>